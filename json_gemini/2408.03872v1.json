{"title": "Inter-Series Transformer: Attending to Products in Time Series Forecasting", "authors": ["Rares Cristianb", "Pavithra Harsha\u00aa", "Clemente Ocejob", "Georgia Perakisb", "Brian Quanza", "Ioannis Spantidakisb", "Hamza Zerhounib"], "abstract": "Time series forecasting is an important task in many fields ranging from supply chain management to weather forecasting. Recently, Transformer neural network architectures have shown promising results in forecasting on common time series benchmark datasets. However, application to supply chain demand forecasting, which can have challenging characteristics such as sparsity and cross-series effects, has been limited.\nIn this work, we explore the application of Transformer-based models to supply chain demand forecasting. In particular, we develop a new Transformer-based forecasting approach using a shared, multi-task per-time series network with an initial component applying attention across time series, to capture interactions and help address sparsity. We provide a case study applying our approach to successfully improve demand prediction for a medical device manufacturing company. To further validate our approach, we also apply it to public demand forecasting datasets as well and demonstrate competitive to superior performance compared to a variety of baseline and state-of-the-art forecast methods across the private and public datasets.", "sections": [{"title": "1. Introduction", "content": "Time series forecasting is a fundamental problem in machine learning with applications across many domains. Common applications of time series forecasting include supply chain management [1], financial modeling [2], weather forecasting [3], and many more. Since many of these problems have been around for far longer than more modern problems in machine learning such as robotic manipulation, often the models used in production are more theoretical in foundation and rely less heavily on data. That is, they often encode strong priors, or inductive biases [4, 5], from the assumptions made and the corresponding simple parametric form the models take. Furthermore, they may not be designed to fully utilize the broader set of related data often available these days for forecasting particular time series. Traditional time series forecasting methods like exponential smoothing [6], state space models [7], and auto-regressive ARIMA models [8, 9] are still widely used in industry.\nAs our world becomes increasingly connected and availability of data rises, so does the pursuit of deep learning models (e.g., deep neural networks) to tackle complex tasks such as image identification or voice recognition systems, as well as modeling sequential data such as natural language and time series. Recurrent neural networks (RNNs) [10, 11] were one of the first types of neural network model architectures developed for sequential data, and variants such as long-short term memory (LSTM) [12] have also been popularly used in forecasting [13\u201319]. However, the recurrent and state-based process of RNNs limits the amount of historical information retained and used in predictions and can also make training more challenging. One family of models that takes inspiration from RNNs and also aims to solve many of its limitations via leveraging attention instead of recurrence to model sequential data is the Transformer model [20] family. Transformer models have shown promising results for modeling sequential data across many domains [21], including recently in time series forecasting [22\u201336]. These Transformer forecast models are all general-purpose by design and results are reported on a variety of common benchmark datasets, often with clear temporal patterns and signals. However, their application to supply chain demand forecasting has been limited, with only the Temporal Fusion Transformer (TFT) [25] paper including a retail dataset in results, and without these models being specialized for demand forecasting. Supply chain demand time series can exhibit challenging properties, such as sparsity in terms of sales observations at the granular product-location level and skewed value distributions (as it is count data), cross-product effects (demand / sales change for one product can affect demand for others) [37\u201339], and misalignment / changing sets of time series as new products are added and removed over time, which may not be fully addressed via existing Transformer models. Furthermore, some recent work called into question the benefits of Transformers for forecasting by showing much simpler models were often able to outperform a large set of recently proposed Transformer forecast models on the same common benchmark datasets [40]. This highlights the need for more study of Transformer model application on targeted real world datasets for particular tasks. To enhance the practical application of Transformers in supply chain demand forecasting and potentially supplant the use of classical models, it is crucial to conduct more case studies that report results and analyses of applying Transformers on real-world data, which is currently lacking.\nIn this work we aim to explore the application of Transformer models to supply chain demand forecasting. We make the following contributions:\n\u2022 We develop new Transformer model variations targeting supply chain demand forecasting to address the aforementioned challenging characteristics. This includes developing a new architecture consisting of an initial component applying self-attention transformation across time series"}, {"title": "2. Related Work", "content": "Time series forecasting has emerged in recent years as a crucial topic in machine learning. There has been a growing interest in using time series models for forecasting in various domains such as transportation [14\u201317, 19, 23\u201326, 29, 31, 35, 36], energy [16, 17, 19, 23, 24, 26\u201329, 31\u201333, 35, 36], weather [28, 29, 32, 33, 35, 36] or retail sales [14]. As evidenced by the recent M5 competition [41], there is also growing interest in developing new machine learning and deep learning time series methods for supply chain demand forecasting specifically. However, the literature on this kind of time series models in the context of retail demand forecasting is limited. We provide a brief overview of existing work to establish the motivation and context for our proposed approach.\nMany traditional time series forecasting methods are still used in industry [6\u20139]. Exponential smoothing [6, 7] produces a prediction that is a weighted sum of past observations with exponentially decreasing weights for past data points. Holt-Winters [7], that falls into the exponential smoothing family, adds model components to enable capturing trend and seasonality. Autoregressive Integrated Moving Average (ARIMA) [8, 9] is a large class of models combining an auto-regressive model, a moving average model and a differencing step to forecast stationary and non-stationary time series. As we will see later, these traditional methods often have limitations compared to machine learning and deep learning time series models, especially when it comes to large and complex datasets. In the framework of the M5 competition [41], the results achieved show that the traditional time series methods were outperformed by state-of-the-art machine learning models.\nThe book [9] provides a comprehensive reference to a range of traditional time series forecasting methods including exponential smoothing, ARIMA and state space models.\nA Recurrent Neural Network (RNN) is a type of neural network used for sequential data with the ability of capturing past information stored by having a recurrent hidden state whose activation at each time is dependent on the previous time state and current time input. RNN-based models can be used for language translation, natural language processing or image identification as well as for time series forecasting showing promising results in this area. RNN-based models have been extensively applied to time series forecasting [10\u201319]. One variant of Recurrent Neural Network is the Long-Short Term Memory model (LSTM) [11, 12]. The purpose of this method is to overcome the challenge of long-term dependencies in time series forecasting by introducing a sophisticated hidden layer that controls the flow of information required to predict the output in the network. By maintaining a memory of past information, the hidden layer regulates the flow of data and stores the relevant information to use for the next steps. This method can capture long-term patterns in the time series and make more accurate predictions for future time points. DeepAR [14] is a forecasting method based on autoregressive RNNs which learns seasonal behaviors and dependencies on given covariates across time series to make probabilistic forecast. By using a shared model across multiple time series (i.e., a multi-task forecast approach), DeepAR can generate forecasts for time series with limited historical data. Temporal Latent Auto-Encoder [19] is a recent model designed to tackle high-dimensional multivariate time series forecasting and model cross-series correlations by combining nonlinear factorization of time series and a temporal latent space LSTM forecast model.\nConvolutional neural networks (CNNs) [42] are a variant of deep neural networks (DNNs) that use a sequence of convolutional layers, pooling layers, and fully-connected layers. CNN-based approaches have been extensively applied and evaluated across a wide spectrum of sequence modeling tasks such as Natural Language Processing (NLP) or speech recognition, and have been shown to outperform RNN-based methods in some cases [43\u201345]. CNN models have also demonstrated effectiveness in the domain of time series modeling and forecasting [16, 46\u201348]. The temporal convolutional network (TCN) [49] is a CNN-based model using causal convolutions, that is, convolution applied across time such that the convolution output for a given time point depends only on the current and previous time points. The TCN architecture can take a sequence of any length and map it to an output sequence of the same length via multiple layers of convolution (and padding sequences as necessary). Notably, TCNs have demonstrated superior empirical performance compared to RNNs and LSTMs in various sequence modeling tasks. For instance, DeepGLO [48], a TCN-based model, showed superior results compared to LSTM and DeepAR in time series prediction tasks such as electricity and traffic forecasting. The DeepGLO method is a hybrid model that combines a global matrix factorization model regularized by a temporal convolution network, along with another temporal network that can capture local properties of each time-series and associated covariates. Moreover, the WaveNet architecture has been adapted for conditional time series forecasting [47] by using stacked dilated convolutions providing access to a wide range of historical data, and multiple convolutional filters in parallel to separate time series facilitating rapid data processing and exploiting the correlation structure between multivariate time series. LSTNet [16] combines a traditional linear autoregressive model with RNN and CNN to extract short-term local dependency patterns among variables and to discover long-term patterns for time series trends. It leverages the strengths of both the convolutional layer to discover the local dependency patterns among multi-dimensional input variables and the recurrent layer to capture complex long-term dependencies.\nThe recurrence mechanism of RNNs restricts the amount of historical information that can be used for predictions and can complicate training as well, which can limit the capability of such models to capture long-term and complex temporal patterns effectively. Similarly, CNN models typically use dilation to increase their receptive fields, necessitating deeper networks to capture longer-term dependencies. Moreover, these longer-term dependencies are modeled only after several levels of padding and processing in higher layers, which may limit their effectiveness in modeling temporal patterns. To overcome these limitations, Transformer-based models have emerged as a family of models that leverage attention mechanisms instead of recurrence or convolution to model sequential data. Such an approach enables directly modeling the impact of time points within the history window on predicting the future values. This approach has been proven effective in addressing the flaws of RNNs and CNNS in time series forecasting, especially when large amounts of data are available.\nTransformer-based methods have been proposed to overcome the limitations of previous DNN approaches for time series forecasting and tackle the challenges posed by having modern-day datasets, which are often larger and more complex. The Transformer model [20] avoids using a recurrence mechanism and relies mainly on a self-attention mechanism to capture cross-sequence (time) interactions and generate an output. This model achieves very good results in natural language processing, computer vision and time series forecasting [22\u201336, 50] which may in part be due to its ability to directly capture long-range dependencies and interactions in sequential data. Temporal Fusion Transformer (TFT) [25] is a Transformer-based model incorporating variable selection networks to choose pertinent input variables at each time step and static covariate encoders to integrate static features into the model. It processes known and observed inputs using a sequence-to-sequence layer and implements a novel self-attention mechanism with interpretable multi-head attention. This enables TFT to learn long-term relationships across different time steps, making it a powerful tool for capturing complex temporal dependencies in the data. The self-attention mechanism also facilitates the interpretation of feature importance, allowing for the identification of critical factors affecting the forecasting task. Pyraformer [30] is a recent Transformer-based model that introduces the pyramidal attention module to describe the temporal dependencies of different ranges leveraging a pyramidal graph and its attention mechanism. Autoformer [29] replaces the self-attention mechanism in the Transformer by an Auto-Correlation mechanism with dependency discovery and information aggregation at the series level to tackle long-term time series forecasting with intricate temporal patterns. FEDformer [35] is a frequency enhanced Transformer, decomposing the input into a trend component with a moving average kernel and a seasonal component. These models are specifically designed for long sequence forecasting. Using the seasonal-trend decomposition structure of FEDformer, DLinear combines it with simple linear layers. Two one-layer linear networks are applied to each component and the two features are summed to get the final prediction. This simple linear approach has outperformed the state-of-the-art (SOTA) FEDformer for multivariate and univariate forecasting as well as other recent SOTA Transformer-based solutions on different common benchmark datasets.\nNote for the multivariate (multiple time series) case these transformer approaches typically treat multiple time series values as part of the input vector for one time point, and an initial embedding linearly maps the combination to a new embedded vector which is subsequently transformed via attention across time points. Recently PatchTST [51] instead applied a shared transformer backbone independently per channel (time series), i.e., in a multi-task manner, and on patches of time points, to transform each channel independently, with a simple linear transformation at the end to get a final prediction for each series, and found this approach to often out-perform the previous channel-mixing transformer forecasting approaches.\nIn summary past transformer-based forecast models were either applied per-series (univariate forecasting) or typically by jointly embedding all variables per time point (multivariate forecasting), with PatchTST using per-series transformation for the Transformer part with a shared network (multi-task approach) but also using a static linear mapping on the transformed variables at the end. On the one hand, per-series approaches like PatchTST and univariate transformer models often achieve better results than multivariate modeling for many datasets (especially when trained in a multi-task manner across time series) as they can leverage more data (from the multiple time series independently) to train the shared-parameter network and avoid overfitting that can occur with multivariate modeling, especially when there are many time series, and can also be more computationally efficient. However, on the other hand, per-series models are unable to leverage cross-series information and capture joint time series distribution relationships in forecast outputs. I.e., while multivariate time series forecast modeling can capture complex relationships between time series it is also more prone to overfitting and more expensive to train. Furthermore, the static, typically linear, encoding per time point as typically used in multivariate transformer and other neural net forecasting approaches, and as used in PatchTST after independent per-series transformation, limits the relationships that can be captured and modeled between time series - for example, a different kind of transformation, or different information among time series should be shared, depending on the context given by the particular time window being considered.\nInstead with our novel transformer forecasting approach, we aim to capture the best of both worlds, of cross-time-series multivariate modeling along with per-series, shared network, multi-task modeling. Instead of using a static linear transformation of multiple time series values, we apply self-attention across time-series, for the current segment of each time series - to capture the relationships between time series in a dynamic and more flexible and powerful way. Afterward, a shared transformer network is applied per-series to finish transforming each individual time series and derive the final forecast. This whole network can then be trained in a multi-task fashion - i.e., applied to each time series individually (each time also feeding in the other time series initially for the cross-series attention transformation part), so it can leverage the benefit of having effectively larger data for training by sharing parameters across time series as in multi-task approaches like PatchTST. Note that while some past work has explored inter-series attention as well, e.g., for epidemiological analysis [52], unlike such prior work, we apply this in a more constrained way as just an initial transformation of directly aligned time series segments only, and critically and uniquely still use a shared multi-task forecast network applied per series after.\nAdditionally, most transformer forecasting papers provide results of using these novel transformer methods on widely used, publicly available time series forecasting benchmark datasets, many of which are derived from real-world practical applications, or else on specific domains like epidemiology or logistics. However, the application of these methods to supply chain demand forecasting is very limited. Indeed, most of the papers do not include any retail dataset in the evaluation of their method. The TFT paper [25] is one of the only papers introducing a deep learning method for time series forecasting applied on various domains including a sales forecast dataset in the set of results, but this is not the focus of the study. These models have yet to be explored and demonstrate broad success in supply chain demand forecasting, which can have challenging characteristics such as sparsity and cross-series effects. As a key distinguishing part of this work we benchmarked many transformer forecast methods, including TFT, with the private dataset provided by the medical device manufacturing company as well as the best of the methods on other retail datasets.\nOur method and a diverse set of the models described will be applied on retail datasets to help provide an analysis of the application of these time series methods for supply chain demand forecasting. This includes an in-depth case study conducted on a supply chain demand forecasting dataset for a medical device manufacturing company. The study provides results for multiple horizons and target metrics, with the objective of catering to the business needs of the company using real-world data. Finally, we evaluated the effectiveness of our proposed approach on two publicly available real-world sales datasets, to provide a better understanding of its performance and its comparison with other methods."}, {"title": "3. Methodology", "content": "Let a generic matrix be denoted by a bold capital letter, a vector by a bold lower case letter, and any scalars including individual entries of a vector or matrix by an non-bolded letter. Then, multivariate time-series can be represented by a matrix (with each row being an individual time series / variate) and univariate time series by a vector (with each entry being a time point). For a matrix X, we denote the i-th column of the matrix with a subscript and lowercase letter xi. Let us consider a situation where we have several products for which we want to forecast demand. Each product can be represented as a multivariate time-series denoted by Y, where j represents a specific product. The columns of Y\u00b9, denoted as (yi, \u2026\u2026,y), represent the T time points of our time series, while the rows correspond to the different variates. We consider having n variates. Specifically, at each time point i, the vector y consists of the n observed values for the different variables considered. For example, such values consist of useful information such as the sales of the product at a particular location or the price of the product. We consider the problem of forecasting h future values (YT+1,1, \u2026\u2026\u2026, YT+\u0127,1) where each y represents the value for the dimension I we are interested in, at time point i for product j. In this case, we are interested in the same dimension across all future time periods and we call this the label or target variable. In the demand forecasting case, this may correspond to the sales of the particular product.\nThe Transformer model is based on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention [20]. Through the attention mechanism and the positional encoding, the model is able to appropriately attribute the impact of each element in our input on each other element and compute a representation of a sequence by relating different positions in the sequence. The Transformer architecture consists of stacked self-attention layers and point-wise fully connected layers (i.e., the latter applied per time point, in the case of time series), that sequentially transforms the (vector) representations (also known as encodings), for elements of a sequence with each layer.\nThe main component of the Transformer responsible for its success is the attention mechanism which can model the relationships between elements in a sequence. The original Transformer paper [20] uses two different attention mechanisms which are the same at the core: the self-attention mechanism and the encoder-decoder attention. An attention function can be described as mapping a query and a set of key-value pairs to an output, where each query, key and value corresponds to an element in a sequence, e.g., a time point in the case of time series. The input consists of queries and keys of dimension dk, and values of dimension du for all elements in a sequence. Q, K and V are respectively a packed set of queries, a packed set of keys and a packed set of values (with each item in the sequence corresponding to a row). dmodel is the model dimension to which we project our input. The output for each sequence element is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query for that element with the corresponding key. In matrix form the set of outputs is given by the following function:\n$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{Q K^T}{\\sqrt{d_k}})V,$\\nwith softmax applied per row. In Transformers, these queries, keys, and values are typically determined for each sequence element by linearly projecting the current representation of each element using a learned weight matrix (i.e., which are parameters of the model) for each of the query, key, and value representations, i.e., Q = XWQ, K = XWK, and V = XWV, where X is the current representation of the sequence. Thus the output of the self-attention operation results in a new, transformed encoding / representation for each element in the sequence.\nInstead of performing a single attention function, an extension to this mechanism is to linearly project the queries, keys and values h times with different learned linear projections to dk, dk and du dimensions respectively. The attention function is then performed on each of these projected versions of queries, keys and values, which are concatenated and once again projected. This multi-head attention can allow learning different relationships for different parts of a sequence. We label this the MultiHead attention function, described with the following:\n$\\text{MultiHead}(Q, K, V) = \\text{concat}(\\text{head}_1, ..., \\text{head}_h)W^O$\\nwhere $\\text{head}_i = \\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)$\nwhere $W_i^Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}, W_i^K \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}, W_i^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$ and $W^O \\in \\mathbb{R}^{h d_v \\times d_{\\text{model}}}$. For self attention as used in Transformers, Q = K = V = X is the input to MultiHead.\nPositional encoding [53\u201356] assigns a unique encoding vector to each time step, which is added to the input embedding vector (initial encoding) at that time step. The positional encoding has the same dimension dmodel as the embeddings, so that the two can be summed. The encoding vector captures information about the position of the time step in the sequence, such as its relative position to other time steps. This enables the model to distinguish between different time steps and understand the temporal ordering of the sequence.\nThe encoder is composed of a stack of blocks, each composed of self-attention layers and position-wise feed forward network layers (i.e., the same network applied independently to each sequence element) followed by the residual connection and layer normalization to assist the training stability. The self-attention component of each block of the encoder is in charge of computing the attention weights between all of the elements in the block's input sequence and transforming the elements based on these attentions, as described above. The encoder thus performs a sequence of transformations to the input sequence representations, and then passes this information onto the decoder for it to roll-out the final predictions.\nThe decoder architecture is similar to the encoder architecture. It is also composed of a stack of identical blocks with the same components. In addition to the two sub-layers in each encoder block, the decoder adds a third sub-layer, which performs multi-head attention over the output of the encoder stack as well. In this way sequential decoder outputs are generated based on both the encoded representations of the input sequence, and the output sequence of the decoder generated so far. Please refer to [20] for more details about the Transformer architecture.\nWe design a new Transformer-based model, referred to as Inter-Series Transformer, to overcome the different challenging characteristics described in Section 2. As mentioned in Section 2.3.1, a key differentiating aspect of our approach is combining controlled, cross-series attention based transformation of different time series, along with per-time-series multi-task modeling via a shared network (for temporal transformation), to capture the best of both worlds.\nNote, in the context of supply chain demand forecasting, each time series typically corresponds to a product or product group / category (or product and location combination) and each could also itself be a multivariate time series, incorporating other features like price or promotions.\nIn our proposed architecture, there are four main differentiating / new components compared to the vanilla Transformer architecture and typical / past Transformer forecasting approaches:\n\u2022 Inter-Series Attention Layer: we introduce a new custom attention layer to get a better informed representation of the target time series by learning dynamics between the different time series / products and incorporating the other time series into the prediction. As shown in Figure 1, this new component is the first layer (or layers) of our custom Transformer and takes all the time series as inputs as detailed in 3.3.1. Note that unlike past Transformer approaches that apply attention just across time, our can leverage attention to capture cross-product / time series effects. Additionally, unlike applying attention across both all time series and time points at all layers, which could more easily lead to overfitting, ours enables capturing cross time-series effects in a more controlled manner (transforming just the target time series themselves up-front).\n\u2022 Multi-task, shared per-series transformer: by limiting cross-product / series attention to initial layers and select time series, we control complexity, and enable using a shared Transformer network afterwards to separately transform each individual multivariate time series (e.g., per product), capturing the temporal effects and effectively expanding the amount of data used to train this shared network.\n\u2022 Projection to High Dimensional Representations for various features: we address mixed feature and feature type inputs to create a more comprehensive and informative input for the Transformer by learning separate mappings to high dimensional representations for different feature types.\n\u2022 Abandonment of Positional Encoding: we abandon the positional encoding and we capture relative positioning by defining and incorporating specific features that change with time."}, {"title": "4. Experimental Setup", "content": "Our study involves evaluating our proposed model and other time series methods on a private dataset provided by a medical device manufacturing company, which is split into two parts, type 1 and type 2, as explained below. This dataset serves as a guiding case study to improve the limitations of time series forecasting in retail settings. To further validate our approach, we apply our method to two publicly available retail datasets, namely Walmart Stores Sales and Walmart M5 [57]. This allows us to compare the effectiveness of our Inter-Series Transformer model with other time series forecasting techniques, which have been discussed in section 2, on large-scale sales datasets. By evaluating our approach on both private and public datasets, we can gain a more comprehensive understanding of its potential impact and applicability in real-world retail forecasting scenarios.\nThe primary dataset used to evaluate our models is a small retail sales dataset of a medical device manufacturer. Products can be identified with three unique identifiers in a hierarchical structure. Our dataset consists of products across different distribution centers in the world, and each time series corresponds to a specific product at a specific distribution center. We use this approach to avoid having multiple data points at a single time point for a product, which could potentially result in the loss of information through aggregation.\nMost of the literature on time series forecasting methods focuses on extremely large datasets, which have been shown to be effective with deep learning techniques that can learn a general understanding of time series mechanisms. However, these approaches may not be applicable in smaller private settings such as ours. Our data consists of two types of products identified at level 1 and resulting in two separate datasets. The first dataset comprises 65 time series corresponding to type 1 products, which exhibit a general increasing trend. The second dataset comprises 50 time series corresponding to type 2 products with a general decreasing trend. Retail datasets often exhibit unpredictable patterns due to external forces that we may not be able to model. Additionally, time series can drop in and out at different time points due to new product introductions, adding to the complexity of forecasting. Our Transformer method employs learned embedding vectors to ensure that new time series can still generate predictions through their identifier features, along with self-attention which is applicable for variable length sequences and multi-task learning so the shared network is trained across the variety of series and this can work effectively on new series.\nIn addition to conducting experiments on the private dataset provided by a third-party company, we also evaluated the effectiveness of our custom Transformer and other benchmark models on two publicly available Walmart datasets. This allowed us to measure the performance of our model in comparison to other models on much larger datasets, providing insights from our custom attention mechanisms on a massive number of products.\nThe first dataset we used is the Store Sales forecasting dataset provided by the Walmart Recruiting team for a Kaggle competition. This dataset contains time series of sales for 98 departments at 45 different Walmart stores. While there is no information on specific products, we can consider each department as a separate product, giving us a total of 4,410 time series for potential input during training. One key difference between this dataset and the private one is that we do not have access to any department-specific features, only store-specific ones.\nThe second dataset we experimented with is the M5 dataset obtained from Walmart [57], which is even larger in size compared to the previous Walmart dataset. The data is separated into 3,049 different products sold by Walmart in the US at different stores, and includes aggregated series as well based on the category and department of the product and state location of the stores. This results in a total of 44,280 time series to model on. The significant increase in data makes it difficult to obtain good performance with sufficient epochs, but we present results on a small number of epochs across several algorithms.\nTo maximize the performance of the Inter-Series Transformer model, we conducted hyperparameter tuning on several key parameters such as the number of encoder / decoder layers, model dimension, embedding dimension, batch size, and the number of training epochs on validation data. We also experimented with different learning rate scheduling approaches in preliminary study on a subset of the data. Since we did not see much improvement with more exotic approaches, we used a common approach for our experiments. We fixed the learning rate schedule to reduce the learning rate on plateau (by 5%), with a starting learning rate of 0.0015 and using the Adam optimizer. In the final training setup, we determined the best hyperparameters to use were 2 encoder/decoder layers, 128 model dimension, an embedding dimension of 6 (this is the dimensionality of each time-step"}, {"title": "5. Results", "content": "We applied our custom Inter-Series Transformer model", "models": "DeepAR, TFT, and DLinear, i.e., the best-performing model of each category previously, as well as PatchTST which performed well in some cases on the previous, target dataset, and is the most recent state-of-the-art comparison method. Based on these experiments, the Inter-Series Transformer outperforms all of these methods on the more granular M5 Walmart dataset, and is second place and close to the best (PatchTST) on the more aggregate and simpler Walmart Store Sales dataset (which also had less features available to make use of). Therefore, the Inter-Series Transformer is a promising approach that can achieve high performance on datasets of various sizes, as compared to the benchmarks. For the case of the Walmart Store Sales dataset, the lack of features and similarity and regularity of the"}]}