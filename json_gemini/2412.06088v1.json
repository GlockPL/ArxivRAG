{"title": "A4-Unet: Deformable Multi-Scale Attention Network for Brain Tumor Segmentation", "authors": ["Ruoxin Wang", "Tianyi Tang", "Haiming Du", "Yuxuan Cheng", "Yu Wang", "Lingjie Yang", "Xiaohui Duan", "Yu Zhou", "Donglong Chen"], "abstract": "Brain tumor segmentation models have aided diagnosis in recent years. However, they face MRI complexity and variability challenges, including irregular shapes and unclear boundaries, leading to noise, misclassification, and incomplete segmentation, thereby limiting accuracy. To address these issues, we adhere to an outstanding Convolutional Neural Networks (CNNs) design paradigm and propose a novel network named A4-Unet. In A4-Unet, Deformable Large Kernel Attention (DLKA) is incorporated in the encoder, allowing for improved capture of multi-scale tumors. Swin Spatial Pyramid Pooling (SSPP) with cross-channel attention is employed in a bottleneck further to study long-distance dependencies within images and channel relationships. To enhance accuracy, a Combined Attention Module (CAM) with Discrete Cosine Transform (DCT) orthogonality for channel weighting and convolutional element-wise multiplication is introduced for spatial weighting in the decoder. Attention gates (AG) are added in the skip connection to highlight the foreground while suppressing irrelevant background information. The proposed network is evaluated on three authoritative MRI brain tumor benchmarks and a proprietary dataset, and it achieves a 94.4% Dice score on the BraTS 2020 dataset, thereby establishing multiple new state-of-the-art benchmarks. The code is available here: https://github.com/WendyWAAAAANG/A4-Unet.", "sections": [{"title": "I. INTRODUCTION", "content": "Brain tumors, caused by the abnormal growth of brain cells, pose a significant threat to human health, making early diagnosis and treatment crucial. MRI, as a non-invasive imaging technique, provides clear visualization of soft tissue lesions and is widely used in diagnosing and treating brain tumors, as shown in Figure 1. Current medical image segmentation methods primarily rely on U-shaped CNNs.\nDespite extensive research, brain tumor segmentation remains challenging due to high variability in MRI images, unclear boundaries, and irregular tumor shapes and textures. Traditional CNN models struggle to adapt to these irregularities, failing to aggregate semantic information and compensate for spatial information loss. This leads to noise, misclassification, incomplete segmentation, limited image feature extraction, and constrained accuracy improvements.\nDrawing from previous successful semantic segmentation studies, Guo et al. [1] identified three key features, shown in Table I, that a good CNN segmentation model should possess. We incorporated these key points into the brain tumor image segmentation characteristics and summarized them as follows:\n(i) Utilization of a powerful encoder. Brain images typically encompass intricate structures such as brain tissue, vessels, and ventricles, while tumors often exhibit diverse shapes and sizes. A robust encoder is necessary to capture and represent these complex high-level semantic features, segmenting these structures accurately.\n(ii) Fusing multi-scale information. Tumors within various organizational structures in the brain may exhibit significant size, shape, and distribution disparities. By fusing multi-scale information, the model can better capture details and global context in the image, enhancing the segmentation model's perception of various structures.\n(iii) Integration of attention mechanisms. MRI images have multiple channels, each providing different information. Channel attention mechanisms help the model identify crucial channels for a specific task. Spatial attention mechanisms help the model focus on specific locations to capture local structural details, enhancing segmentation accuracy.\nInspired by Guo [1], we revisited CNN design principles to develop A4-Unet, a brain tumor segmentation architecture integrating four advanced components\u2014Deformable Large Kernel Attention (DLKA), Swin-Enhanced Atrous Spatial Pyramid Pooling (SSPP), Combined Attention Module (CAM), and Attention Gates (AG) \u2013 each enhancing performance. Our key innovations are:\n\u2022 By incorporating large-kernel variable convolutions, the encoder can better capture multi-scale information with low complexity.\n\u2022 Long-distance dependencies intra-image and relationships inter-channel can be extracted by employing Swin Spatial Pyramid Pooling (SSPP) and convolutional channel attention in the bottleneck layer.\n\u2022 In the decoder, we leverage the orthogonality of Discrete Cosine Transform (DCT) to compute channel attention weights, followed by skip connections to supplement fine edge details. Additionally, we utilize simple convolutional element-wise multiplication to induce spatial attention, improving the generalization performance of a model."}, {"title": "II. RELATED WORK", "content": "CNN-based Architecture. CNN-based methods classify pixel patches to capture local and global features. DenseNet [2] stacks deep layers to maintain multi-scale features, and Unet-based extensions [3], inspired by Fully Convolutional Networks (FCNs), address various segmentation challenges. SegNeXt [1] enhances convolutional structures with Multi-scale Convolutional Attention (MSCA) Module. However, despite effectively retaining low-level information, CNN models struggle to capture high-level information, limiting their performance.\nTransformer-based Networks. Transformer-based networks assign importance weights to image parts using attention mechanisms, Such networks have shown impressive results on vision tasks with the initial success of Vision Transformer (ViT) [4]. Variations like SegFormer [5] and Swin Transformer [6] use hierarchical transformer encoders to extract multi-scale features with simple decoders for segmentation. However, they struggle with detecting high-resolution details like textures and edges, limiting their effectiveness in dense vision tasks.\nIntegration of CNN and Transformer. Hybrid architectures combining CNNs and transformers leverage both strengths to overcome limitations. TransAttUnet [7] integrates transformers and U-Net to capture global contextual information with attention blocks and multi-scale skip connections, achieving semantic consistency in feature maps. BoTNet [8] uses CNNs to process input images into tokenized feature maps, and then uses transformers to capture long-range dependencies. In our study, A4-Unet incorporates a robust convolutional encoder and transformer-guided modules to achieve a convincing segmentation performance."}, {"title": "B. Attention Mechanisms", "content": "Attention mechanisms dynamically adjust weights based on input features. Channel attention, like Squeeze-and-Excitation Network (SE-Net) [9], assigns different weights to each channel, while Frequency Channel Attention Network (FcaNet) [10] uses Discrete Cosine Transformations to focus on low-frequency channel information.\nSpatial attention enhances important regions by creating weight masks, as seen in Convolutional Block Attention Module (CBAM) [11], which combines pooling and concatenation for a unified feature descriptor. Our model integrates channel and spatial attention using CBAM's lightweight design to emphasize important regions and suppress irrelevant information, capturing cross-channel relationships and spatial details for precise detection."}, {"title": "C. Adjustment of Receptive Field", "content": "Atrous Convolution. Atrous convolution first appeared in a dyadic wavelet transform technique [12] that is well recognized as a signal processing technique. Deep networks reduce the final feature map resolution, resulting in the cumulative influence of pooling layers, striding operations, etc. Yu and Koltun [13] presented an innovative method to overcome this deficiency while seeking a more extensive information spectrum.\nDeformable Convolution. CNNs' fixed receptive fields limit their ability to handle large-scale geometric transformations, making high-level semantic extraction challenging. Inspired by the multi-scale deformable part models [14] and spatial transformer module [15], deformable convolution [16] addresses this by introducing 2D offsets to sampling locations, allowing flexible grid deformation. We adopt deformable convolution to enhance receptive field flexibility for better target segmentation."}, {"title": "D. Multi-scale Contextual Information", "content": "Atrous Spatial Pyramid Pooling. Aggregating multi-scale contextual information is crucial for accurate pixel-level classification in semantic segmentation. Dilated convolution [17] enlarges the receptive field without changing output size. Building on SPP layers [18], ASPP [19] captures image context at multiple scales. This inspires our module to extract rich, comprehensive information from lesion images.\nMulti-scale Transformer. While CNNs have effectively used multi-scale feature representations, this potential has yet to be fully explored in vision transformers. CrossViT [20] introduces a dual-branch transformer with cross-attention, and MViT [21] embeds a multi-scale feature pyramid into the transformer. Inspired by these works, we propose a dual-branch encoder based on the hierarchical Swin transformer architecture."}, {"title": "III. METHODOLOGY", "content": "Our A4-Unet features an encoder-decoder architecture with three main components, as shown in Figure 2: DLKA for enhanced feature extraction, SSPP for multi-scale interactions, and CAM for attention mechanisms. The encoder uses DLKA, SSPP handles multi-scale features in the bottleneck, and the decoder aggregates features with gated and mixed attention across four upsampling stages, optimizing brain tumor segmentation."}, {"title": "B. Strong Encoder", "content": "To build a robust encoder, we integrate the Deformable Large Kernel Attention (DLKA) block in Figure 3 into the downsampling process. DLKA includes a Deformable Convolution Module (DConv) and a Large Convolution Kernel (LK). The DConv is ideal for enhancing low-level feature details like edges, textures, and shapes, particularly for medical targets with irregular sizes and various textures. The DConv consists of a 2D convolution, a Deformable Convolution with adjustable sampling grids using offsets, an activation function for nonlinearity, and an offset field calculation. Proposed by Azad [22], a standard convolution layer generates offsets, guiding the Deformable Convolution layer's sampling positions. The DConv module equation is as follows:\nAttention = Conv1\u00d71(ConvDC(ConvDW(F)),  (1)\nOutput = Conv1\u00d71(Attention & F) + F, (2)\nwhere ConVDC and CONVDW are deformable convolution and depth-wise dilation convolution, respectively, while F is the input feature.\nOn the other hand, although CNNs do well in capturing local features and low-level information, they come at the cost of neglecting the global context. The LK proposed by Guo et al. [23] can overcome this limitation by enlarging the receptive field. It provides a similar receptive field as the self-attention mechanism, with fewer parameters. The structure of LK contains a depth-wise convolution, a dilated convolution, and a 1 \u00d7 1 convolution. The kernel size of depth-wise convolution (KDW) and dilated convolution (KDC) can be calculated as below:\nKDW = (2d -1) \u00d7 (2d \u2212 1), (3)\nKDC = [Kd] (4)\nwhere d is dilation rate and K is kernel size.\nIn sum, DLKA integrates into the encoder to provide long-range dependencies during downsampling and to concatenate with feature maps in the upsampling process via skip connections, thus compensating for low-level feature details."}, {"title": "C. Multi-scale Interaction", "content": "Addressing the challenges of irregular sizes and shapes in medical images requires introducing multi-scale interaction and enhancing spatial representation. Previous works [24], [25] used multi-scale patches and deeper networks, but multi-scale information remained fragmented.\nWe tackle this by modifying the bottleneck layer to include Swin Spatial Pyramid Pooling (SSPP) and a Cross-Contextual Attention module shown in Figure 4. This approach integrates Swin Transformer blocks with varying window sizes, providing rich contextual information.\nSwin Spatial Pyramid Pooling. In DeepLab V3+, Chen et al. [26] introduced the Atrous Spatial Pyramid Pooling (ASPP) module, which dynamically selects convolutional blocks of varying sizes to handle different target scales. This approach prevents large targets from being fragmented and maintains long-distance dependencies without altering the network structure.\nInspired by SSPP by Azad et al. [27], we replace four dilated convolutions with Swin Transformers to better capture long-range dependencies. The extracted features are merged and fed into a cross-contextual attention module. This enhances the model's ability to capture contextual dependencies across different scales.\nCross-Contextual Attention. The ASPP concatenates feature maps via depth-wise separable convolution, which does not capture channel dependencies. To address this, Azad introduced cross-contextual attention after SSPP feature fusion. Assume each SSPP layer has tokens (P) and embedding dimension (C) as (zPXMC), representing objects at different scales. We create a multi-scale representation zall = [z1||z2...||zm] by concatenating these features. A scale attention module then emphasizes each feature map's contribution, using global representation and an MLP layer to generate scaling coefficients (wscale), enhancing contextual dependencies:\nwscale = \u03c3(W2\u03b4(W1GAP(zall))), (5)\nzall = wscale \u2299 zall, (6)\nwhere W1 and W2 are learnable MLP parameters, \u03b4 is the ReLU function, \u03c3 is the Sigmoid function, and GAP is global average pooling.\nIn the second attention level, Cross-Contextual Attention learns scaling parameters to enhance informative tokens by calculating their weight maps, using the same strategy:\nwtokens = \u03c3(W3\u03b4(W4GAP(zall))), (7)\nzall = wtokens \u2299 zall, (8)"}, {"title": "D. Convolutional Attention Module", "content": "We construct our decoder by integrating a novel convolutional attention module with a frequency feature that effectively suppresses unnecessary information. Furthermore, we introduce skip connections with attention-gated fusion, contributing to the suppression of irrelevant regions and accentuation of salient features.\nAs shown in Figure 5, our decoder includes a vanilla block for feature upsampling, an Attention Gate (AG) for cascaded feature fusion, and a Combined Attention Module (CAM) for feature map enhancement. We use four CAM blocks for the four pyramid layers of the encoder and four AGs for skip connections. Multi-scale features are consolidated by combining upsampled features from the previous layer with skip connection features using AG. The CAM module then enhances pixel grouping and suppresses background information with frequency channel and spatial attention (SA). Finally, Dconv propagates the fused features to the upper layer.\n1) Combined Attention Module:\n\u2022 Channel Attention\nTo enhance channel attention accuracy in CAM, we replaced convolution-based channel attention with Orthogonal Channel Attention (OCA) from Salman et al. [28]. OrthoNet's channel attention addresses the limitation of Global Average Pooling (GAP) by using the Discrete Cosine Transform (DCT) to preserve low-frequency information. As shown in Figure 6, OCA's structure involves selecting suitable filters within appropriate dimensions and ensuring filter orthogonality using the Gram-Schmidt process. This structure enhances feature representation in neural networks.\n\u2022 Spatial Attention\nSpatial attention helps the model adapt to spatial variability by adjusting attention to local structures, improving generalization. As shown in Figure 7, for each feature point in input feature F of size H * W, the maximum and average values along the channel axis are denoted as Fmax \u2208 R1*H*W and Favg \u2208 R1*H*W, and concatenated into a 2 * H * W tensor. This tensor undergoes convolution to create a spatial attention map that highlights or suppresses specific locations.\nSA = Conv(\u039caxPool(F), AvgPool(F)) (9)\n2) Attention Gate: We incorporate the attention gate into the skip connection process. Figure 8 illustrates the architecture of an attention gate unit. Let x\u03b9 represent the feature map of layer l. For each pixel i, a gating signal gi vector is used to identify focal areas at a larger scale. The coefficient of attention, denoted as \u03b1, ranges from 0 to 1, selecting relevant feature responses and suppressing irrelevant feature details. The resulting Xoutput is obtained through element-wise multiplication of \u03b1i and a, calculated as follows:\nXoutput = X\u03b9 \u00b7 \u03b1i (10)\nAccording to the formula, the gating coefficient \u03b1 is derived through additive attention. Given the complexity of medical images involving multiple semantic classes, we incorporate the multi-dimensional attention coefficient [29] to concentrate on target regions. The computation of the multi-dimensional attention coefficient involves the following:\n\u03b1i = \u03c3(\u03a8T (\u03b4(Wxxi + Wg9i + bg)) + b\u03c8) (11)\nwhere Wx, Wg are bias, \u03c3(x) = 1/(1 + e^(-x)) is the Sigmoid function and \u03c3(x) = max(0, x) is the ReLU function. As for gating signal vector gi, we adopt 1 \u00d7 1 channel-wise convolution (represented as \u03a8 in the formula) as the linear transformation on the feature map x1."}, {"title": "IV. RESULTS", "content": "In this section, we first conduct comprehensive ablation studies to validate the effectiveness of our design. Then, we compare our results with several state-of-the-art networks and analyze the reasons for the results."}, {"title": "A. Dataset", "content": "The BraTS datasets are part of the Brain Tumor Segmentation Challenge. We select the BraTS 2019, 2020, and 2021 datasets as the experimental data for our study. These are publicly available via the following links\u00b9. All BraTS multimodal scans are provided as NIfTI files (.nii.gz) and include the following: I) native T1-weighted scans (TIN), II) post-contrast T1-weighted scans (T1C/T1CE, also referred to as T1Gd), III) T2-weighted scans (T2W/T2), and IV) T2 Fluid Attenuated Inversion Recovery scans (T2F/FLAIR). The training and validation sets have unspecified glioma classifications, and all data underwent standardized preprocessing by the challenge organizers.\nIn addition to public benchmarks, we evaluated our model on a proprietary dataset from an anonymous institution. This dataset includes T1c and T2 MRI images from 194 glioma patients, annotated for whole tumors by senior radiologists. Since our model is 2D, we sliced each 3D MRI image into 2D slices. Details are shown in Table II."}, {"title": "B. Metrics", "content": "The Dice Similarity Coefficient (DSC) is a key metric for evaluating segmentation models, ranging from 0 to 1 to represent similarity between two samples. It is calculated as:\nDSC = 2TP /(FN + FP + 2TP) (12)\nHere, TP represents true positive pixels, FP indicates false positive pixels, and FN represents false negative pixels.\n2) Mean Intersection over Union:\nThe IoU calculates the intersection of the predicted and true segmentation divided by their union. As an extension, the mIoU computes the IoU for each class and then calculates the mean of these IoU scores. The mIoU provides a more comprehensive assessment of the overall segmentation performance across k different classes.\nmIoU = 1/(k + 1) * sum (TPi / (FNi + FPi + TPi)) (13)\n3) Hausdorff Distance:\nHausdorff Distance (HD) measures the maximum distance from each point in the predicted boundary set to its nearest point in the ground truth boundary set, assessing segmentation accuracy by comparing boundary correspondence. Given sets A (predicted) and B (ground truth), the Hausdorff distance formula is:\nH(A, B) = max(h(A, B), h(B, A)) (14)\nwhere\nh(A, B) = max_a\u2208A min_b\u2208B ||a - b|| (15)\nh(B, A) = max_b\u2208B min_a\u2208A ||b - a|| (16)"}, {"title": "C. Implementation Details", "content": "All experiments are implemented in PyTorch 2.0.1 and trained on a single GeForce GTX 4090 GPU with 24 GB memory. We use standard back-propagation with the AdamW optimizer and Softmax activation function. Training employs a batch size of 16, an initial learning rate of le-5, and runs for 30 epochs. Total training time varies by dataset size: approximately 20 hours for BraTS 2019, 30 hours for BraTS 2020, and 50 hours for BraTS 2021."}, {"title": "D. Ablation Study", "content": "We conducted an ablation study on the BraTS 2020 dataset to analyze the effectiveness of three crucial factors. Results are shown in Table III. We observed that the BraTS 2019 dataset had slower convergence, requiring 12 epochs compared to 10 epochs for the other two datasets, likely due to its smaller training sample size.\n1) Effect of the Strong Encoder: To validate the effect of DLKA in the encoder, we construct the baseline network and another version with DLKA. Employing the DLKA module leads to an improvement in the Dice score of 1.3% compared to the baseline. It also demonstrates a slight improvement when combined with other blocks (e.g., SSPP, CAM).\n2) Effect of the Multi-Scale Interaction: We evaluated the SSPP block for multi-scale information fusion and found a 2.0% accuracy improvement over the baseline. Compared to DLKA, the SSPP module had a more significant impact on accuracy, demonstrating that the transformer can better capture global features. This highlights the importance of introducing a global context for brain tumor segmentation.\n3) Effect of the CAM: As for the CAM block in the decoder, we can conclude that the attention mechanisms result in a 1.9% improvement in model performance, as shown in Table III. When the CAM fusion with DLKA, the model can achieve a better result, adequately demonstrating the effectiveness of adopting skip connections using DLKA before the CAM block."}, {"title": "E. Quantitative Analysis and Visualization", "content": "We test the proposed A4-Unet by evaluating three metrics mentioned in Section IV-B on BraTS 2019, BraTS 2020, and BraTS 2021 datasets, respectively. The experimental results on each training dataset represent the average of five independent runs and were subjected to cross-validation. The results are described in Table IV, and the visualization is illustrated in Figure 9. We got a lower HD95 score of 8.57 on the BraTS 2020 than the other two datasets. We attribute this improvement primarily to two reasons: (i) The BraTS 2020 dataset contains larger segmentation targets, and our model has higher segment performance than irregular and small targets. (ii) 95% might not be the optimal hyperparameter for the more complex datasets like the BraTS 2019 and BraTS 2021, leading to differences in their results.\nOn a proprietary dataset, our model achieved a Dice coefficient of 84.18%, mIoU of 81.60%, and HD95 of 10.77mm, lower than on BraTS datasets. This was attributed to the proprietary dataset having fewer modalities and tumor features, limiting the model's ability to learn optimal features."}, {"title": "F. Comparisons", "content": "The proposed A4-Unet model follows the standard CNN segmentation network design paradigm. To evaluate its improvements and component effectiveness, we compared it with state-of-the-art networks on the three BraTS datasets. Comparative results are cited from the literature. Since official ranking criteria consider multiple metrics, a challenge champion's DSC score may not be the highest. Results are shown in Table V.\nBraTS 2019. We compared A4-Unet with four models on the BraTS 2019 dataset, showing a 21.03% and 20.53% Dice Score improvement over TransUnet and Swin-Unet, respectively. Unlike the transformer-based models requiring more parameters and data, A4-Unet uses DLKA for an efficient encoder. It also surpasses Cascade Unet by integrating attention mechanisms and multiscale fusion, enhancing fine-edge detail through Attention Gates, thus improving segmentation performance.\nBraTS 2020. On the BraTS 2020 dataset Table V, A4-Unet achieved a Dice score of 94.47%, mIoU of 99.68%, and 95th percentile Hausdorff distance of 8.57mm, outperforming Swin-Unet, TransUnet, nnUnet [3], and ResUnet+. TransUnet and Swin-Unet faced similar issues due to dataset size. nnUnet won the BraTS 2020 challenge with only targeted training and post-processing. Compared to the strategy of nnUnet, we focused on improving the network architecture and achieved significant enhancements.\nBraTS 2021. For the BraTS 2021 dataset, we compared A4-Unet with UNETR [34], Swin UNETR [35], SegResNet [36], Optimized Unet [37], and Coupling nnUnet [38]. While UNETR and Swin UNETR's transformer-based encoders increase parameters and training difficulty, A4-Unet's DLKA maintains low complexity and superior performance with stable parameters. SegResNet's dense skip connections are enhanced in our network by using attention gates to better utilize edge detail information for fine segmentation masks."}, {"title": "G. Discussion", "content": "Despite excellent performance on public datasets, the model still faces challenges in clinical applications. The diversity and complexity of real-world clinical data, (e.g., our proprietary dataset) complicate feature extraction and model learning, while the limited annotated data constrain the model's generalization capabilities. Therefore, further improvements are necessary before the model can be effectively applied in clinical settings."}, {"title": "V. CONCLUSIONS", "content": "In this paper, we presented A4-Unet, a brain tumor segmentation network that introduces Deformable Kernel Large Convolution (DLKA), Swin Spatial Pyramid Pooling (SSPP), and attention mechanisms, all while maintaining relatively low network complexity. This approach enables efficient multi-scale feature extraction, captures long-range dependencies, and integrates high-level and low-level semantic information. Our comparative experiments across three datasets demonstrate that A4-Unet significantly outperforms several state-of-the-art models, setting new benchmarks in segmentation performance. Notably, our model achieved substantial improvements in Dice Score and mIoU."}]}