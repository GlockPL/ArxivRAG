{"title": "Robust Claim Verification Through Fact Detection", "authors": ["Nazanin Jafari", "James Allan"], "abstract": "Claim verification can be a challenging task. In this paper, we present a method to enhance the robustness and reasoning capabilities of automated claim verification through the extraction of short facts from evidence. Our novel approach, FactDetect, leverages Large Language Models (LLMs) to generate concise factual statements from evidence and label these facts based on their semantic relevance to the claim and evidence. The generated facts are then combined with the claim and evidence. To train a lightweight supervised model, we incorporate a fact-detection task into the claim verification process as a multitasking approach to improve both performance and explainability. We also show that augmenting FactDetect in the claim verification prompt enhances performance in zero-shot claim verification using LLMs.\nOur method demonstrates competitive results in the supervised claim verification model by 15% on the F1 score when evaluated for challenging scientific claim verification datasets. We also demonstrate that FactDetect can be augmented with claim and evidence for zero-shot prompting (AugFactDetect) in LLMs for verdict prediction. We show that AugFact-Detect outperforms the baseline with statistical significance on three challenging scientific claim verification datasets with an average of 17.3% performance gain compared to the best performing baselines.", "sections": [{"title": "Introduction", "content": "Due to the proliferation of disinformation in many online platforms such as social media, automated claim verification has become an important task in natural language processing (NLP). \u201cClaim verification\" refers to predicting the verdict for a claim is it supported or contradicted by a piece of evidence that has been extracted from a corpus of documents (Thorne et al., 2018; Wadden et al., 2022a; Guo et al., 2022)."}, {"title": "Background", "content": "Automated claim verification means determining the veracity of a claim, typically by retrieving likely relevant documents and searching for evidence within them. The key objective is to ascertain if the evidence either supports, contradicts or does not have enough information to verify the claim. Various datasets have been proposed to facilitate research in this area in different domains: e.g., FEVER (Thorne et al., 2018) is a Wikipedia-based claim verification dataset. Claim verification in the scientific setting has also been proposed in recent years to facilitate research in this complex domain (Wadden et al., 2022a, 2020; Saakyan et al., 2021; Sarrouti et al., 2021; Kotonya and Toni, 2020; Diggelmann et al., 2020). The datasets used for these problems, despite their value, often have limited training data due to the high cost of creation, impacting the reasoning capabilities and robustness of claim verification methods.\nIn addressing these challenges, the literature shows significant advances in models for verifying scientific claims through reasoning. Prior studies have explored using attention mechanisms to identify key evidence segments (Popat et al., 2017; Cui et al., 2019; Yang et al., 2019; Jolly et al., 2022). Recently, the integration of LLMs in explanation generation has been investigated. For example, ProofVer (Krishna et al., 2022) generates proofs for the claim based on evidence using logic-based inference. ProgramFC (Pan et al., 2023) uses LLMs to generate reasoning programs that can be used to guide fact-checking, and FOLK (Wang and Shu, 2023) leverages the in-context learning ability of LLMs to generate First Order Logic-Guided reasoning over a set of knowledge-grounded question-and-answer pairs to make veracity predictions without using annotated evidence. Other sets of studies attempt to improve this problem through sentence simplification and evidence summarization using LLMs (e.g., (Mehta et al., 2022; Stammbach and Ash, 2020)).\nOur work diverges from these methods as we"}, {"title": "Methodology", "content": "We introduce FactDetect, a novel approach designed to enhance the performance of claim verification solutions by leveraging automatically generated short facts extracted from the evidence. We will show that FactDetect is a versatile tool that can be integrated into various claim verification methods, improving the robustness and reasoning capabilities of existing models. The core of Fact-Detect relies on weakly-labeled short facts, which are categorized as either important for verifying a given claim or not important for that purpose, which are used to train a multi-task learning-based model (FactDetect) for importance detection and claim verification."}, {"title": "Definition", "content": "Here, we formally define the primary task of fact generation and labeling: given a claim statement c and corresponding evidence statement e, our objective is to generate concise \u201cfacts\u201d from e. We denote this set of facts by F_e = {f_1,\u2026\u2026,f_m}. Each fact is subsequently labeled as either \u201cimportant\" or \"not important,\u201d denoted as y_{f_i} \u2208 {important, not important}.\nIt is important to note that these facts are intentionally designed to be shorter in length compared to the original evidence (e). They serve as distilled pieces of information extracted from the broader context of the evidence. These succinct facts are intended to capture essential details or insights within the evidence, making them more manageable for claim verification tasks."}, {"title": "Short Fact Generation", "content": "To generate short facts from the evidence e, we adopt a three-step approach. For these steps, we employ LLM Mistral-7B (Jiang et al., 2023). We have experimented with different LLMs such as Vicuna-13B (Chiang et al., 2023) and GPT-3.5 and based on our experiments we observed better performance with this open-source LLM. Details of the prompts for each phase of the short fact generation using this approach are given in Appendix A.\n1) Phrase matching: Initially, we extract matching phrases from both the claim c and the evidence, treating seeing each phrase as a potential answer to a questions framed around the other (A = (a_1, a'_1), ..., (a_n, a'_n)). Phrases \u201cmatch\u201d if they convey similar meanings and/or are semantically similar. We call these answer pairs. We use an LLM to extract the matching phrases. We do not restrict the LLM to follow specific phrase rules such as n-grams, extracting only entities or noun phrases. This way, we ensure the capture of diverse answer pairs that are more likely to be relevant.\n2) Question Generation: After identifying the answer pairs, we formulate concise questions from them. For each answer a'_i in the pair (a_i, a'_i) with corresponding claim c, we generate a question q_i. We use c as the context and a'_i as a desired answer."}, {"title": "Weak labeling", "content": "Labeling each generated fact as important or not is a crucial step in the FactDetect process. After extracting the candidates in the previous steps, we label a short fact sentence f_i as \"important\" if the cosine similarity between f_i and the claim c and f_i and evidence e combined to exceed a predefined threshold t and \"not important\" otherwise. More specifically:\ny_{f_i} = \\begin{cases}\n\"important\" & \\text{if } sim(f_i, c, e) \\geq t \\\\\n\"not important\" & \\text{otherwise}\n\\end{cases}\nHere \\gamma is a hyperparameter and cos(.) is calculated using the Sentence Transformers (Reimers and Gurevych, 2019) embedding of f_i, c and e."}, {"title": "Joint Claim Verification and Fact Detection Framework", "content": "Because of the success of the full context training of claim verification tasks within state-of-the-art models such as MULTIVERS (Wadden et al., 2022b), PARAGRAPHJOINT (Li et al., 2021), and ARSJOINT (Zhang et al., 2021), we propose a similar enhancement approach. Our framework revolves around performing full context predictions by concatenating the claim (c), title of the document in the scientific claim verification datasets (t), gold evidence (e), and all the facts in F_e with a special separator token to separate each fact in F_e.\nThe FactDetect approach employs a strategy based on multitasking where the model is jointly trained to minimize a multitask loss:\nL = L_{cv} + \\alpha L_{fact}\nwhere L_{cv} represents the cross-entropy loss associated with predicting the overall claim verification task. Specifically, we predict y(c, e) \u2208 {support, contradict, nei} by adding a classification head on the  token, where nei refers to Not Enough Info. In addition, L_{fact} denotes the binary cross-entropy loss for predicting whether each fact f_i is important to the claim c or not, and \\alpha is a hyperparameter. During inference, we only predict y(c, e), setting aside the fact detection part."}, {"title": "Zero-shot Claim Verification with LLMs", "content": "In the zero-shot approach, without the need for human-annotated training dataset and finetuning a claim verification model, we leverage in-context learning ability of Large Language Models (LLMs) to extract the encoded knowledge in them using a prompting strategy aimed at eliciting the most accurate responses from them. This is done as follows. We augment FactDetect generated short fact sentences F into the prompt for claim verification through fact-detection: given c, e and F_e we first ask an LLM to detect the most important facts and then, by providing an explanation, we ask it to predict the verdict y(c, e)."}, {"title": "Experiments", "content": "We evaluate the effect of including FactDetect within different claim verification models and encoders. To evaluate this, we first explain the datasets used and introduce the baseline models we compared to our approach."}, {"title": "Datasets", "content": "SciFact (Wadden et al., 2020) consists of expert annotated scientific claims from biomedical literature with corresponding evidence sentences retrieved from abstracts. Supported claims are human-generated using abstract citation sentences, and Contradicted claims negate original claims.\nSciFact-Open (Wadden et al., 2022a) constitutes a test collection specifically crafted for the assessment of scientific claim verification systems. In addition to the task of verifying claims against evidence within the SciFact domain, this dataset contains evidence originating from a vast scientific corpus of 500,000 documents.\nHealthVer (Sarrouti et al., 2021) is a compilation of COVID-19-related claims from real-world scenarios that have been subjected to fact-checking using scientific articles. Unlike most available datasets, where contradicted claims are usually just the negation of the supported ones, in this dataset contradicted claims are themselves extracted from real-world claims. The claims in this dataset are more challenging compared to other datasets. More detailed statistics of the datasets are given in Appendix B."}, {"title": "Baselines", "content": "We evaluate FactDetect in supervised and zero-shot settings. In a supervised setting, we either fully or few-shot train the state-of-the-art models on the given datasets. For the zero-shot setting, we use several best-performing LLMs and prompt them to predict the verdict based on different baseline prompting strategies. For few-shot supervised training, we train on k = 45 training samples."}, {"title": "Supervised Baselines", "content": "We incorporate FactDetect as an add-on for a multi-task learning-based approach on two transformer-based encoders. We train the supervised models on NVIDIA RTX8000 GPU and overall model parameters do not exceed 1B. We set the learning rate to 2e^-5 and save the best model in 25 epochs. We choose 0.5 for the \\gamma similarity parameter, in equation (1) and 10^-2 for the \\alpha hyperparameter of equation (2). The threshold t for the cosine similarity between fact sentences and claim and evidence is set to 0.6.\nLongformer (Beltagy et al., 2020) With the self-attention mechanism incorporated into this model and its ability to process long sequences, we use this encoder to concatenate short sentences into the claim along with additional context provided in the title (if any).\nMULTIVERS (Wadden et al., 2022b) is a state-of-the-art supervised scientific claim verification approach which uses Longformer as a base encoder for long-context end-to-end claim verification in a multi-task learning based approach where in addition to the claim and title it incorporates the whole document (abstract) for both claim verification and rationale (evidence) selection. We augment the short sentences extracted by FactDetect into the model as an input and train FactDetect on top of MULTIVERS in a multitasking-based approach."}, {"title": "Zero-shot baselines", "content": "LLMs serve as a robust source of knowledge and demonstrate impressive outcomes in various downstream tasks, especially in contexts where zero-shot and few-shot learning are employed. However, the effectiveness of these models heavily depends on the methods used to prompt their responses. Consequently, we evaluate state-of-the-art prompting methods both specific to the claim verification task and general task approaches, and compare them to our novel prompting method based on adding the FactDetect-generated short sentences into the prompt and requiring the LLM to detect the most important sentences for verdict as well as predicting the verdict. We name this prompting strategy"}, {"title": "Main Results", "content": "We first report the results of supervised baselines with and without FactDetect incorporated in their training process in Table 1. We experiment with few-shot and full training setups. We observe that incorporating FactDetect into the Longformer encoder achieves the best performance in all three datasets (in bold) in the Full training setup. The average performance gain in F1 when adding FactDetect to Longformer is 3.0% for SciFact. Longformer + FactDetect in the few-shot setting also improves the F1 score for HealthVer by 32.7%. However, we do not see a performance improvement in the few-shot setting for SciFact and SciFact-Open datasets. As mentioned earlier, the results of SciFact-Open dataset are reported in a zero-shot setting (with model trained on SciFact training dataset), resulting in lower performance. Additionally, SciFact-Open receives less benefit from FactDetect than other datasets even in the cases where it does improve results. We suspect that this is due to the more complex nature of the dataset, because it contains claims that are both supported and contradicted by different evidence sentences. The outcomes are consistent with the top-performing baseline, MULTIVERS. By integrating FactDetect into MULTIVERS, we achieve similar performance, despite the advantage of complete context encoding within this framework."}, {"title": "Zero-shot Setup", "content": "The results corresponding to the performance evaluation for the zero-shot prompting with different strategies are reported in Table 2.\nWe observe that AugFactDetect significantly improves the performance of Llama2-13B, Mistral-7B, and GPT-3.5 in all three datasets compared to the best-performing baseline with an average performance gain of 28.1%, 12.7% and 11.3% in the F1 score for SciFact, Scifact-Open, and Healthver test sets respectively. Similarly, AugFactDetect shows significant improvements for Vicuna-13B in SciFact and HealthVer and FlanT5-XXL with AugFactDetect outperforms other prompting strategies in Scifact-Open and HealthVer test sets. Comparison between ProgramFC and baselines also shows the limited advantage in predicting verdicts in scientific claim verification datasets compared to the general claim verification datasets.\nOverall AugFactDetect demonstrates better performance compared to other prompting strategies which suggests the effectiveness of the short fact generation strategy based on the connection between claim and evidence and its performance is comparable to the best-performing baseline in the binary setting."}, {"title": "Effectiveness of FactDetect", "content": "To further understand the impact of the FactDetect, we compare FactDetect based short fact generation approach with the Direct approach where we directly generate short sentences from evidence e (we give 5 examples as few-shot prompting). The details of the promoting strategy and the examples are given in Appendix C.4. We collect the short sentences for each piece of evidence in a claim-evidence (CE) pair, for the SciFact dataset (dev set) and run experiments in the zero-shot setup for 5 LLMS. Overall, AugFactDetect performs better compared to the Direct approach across 4 out of 5 LLMs with a significant difference in FlanT5-XXL and Mistral-7B. These results suggest the usefulness of the three-step approach compared to the baseline direct sentence generation approach. We hypothesize that one key reason for this is in the Direct approach, the generated sentences are based on the evidence only without making a meaningful connection between the claim and the evidence."}, {"title": "Assessing Generation Quality for FactDetect", "content": "Here, we explore the impact of various underlying large language models (LLMs) on the quality of FactDetect generated short sentences. We evaluate this by regenerating short fact sentences using three different LLMs: Mistral-7B, GPT-3.5, and Vicuna-13B and assess their effect in the performance of AugFactDetect for the claim verification task.\nThe results indicate that choosing Vicuna-13B and GPT-3.5 as the base models for short fact generation demonstrates approximately similar performance across 5 LLMs for all the test sets whereas, Mistral-7B exhibits more pronounced performance. Even though Mistral-7B is a relatively smaller model, shows sufficient and consistent performance gains for the claim verification task whereas, the"}, {"title": "Conclusion and Future Work", "content": "In this work, we propose FactDetect, an effective short fact generation technique, for comprehensive and high-quality condensed small sentences derived from evidence. With the relevance-based weak-labeling approach this dataset can be augmented to any state-of-the-art claim verification model as a multi-task learning to train fact detection and claim verification. The effectiveness of this model has been demonstrated in both fine-tuned and prompt-based models. Our results suggest that FactDetect incorporated claim-verification task in a zero-shot setting consistently improves performance on average by 17.3% across three challenging scientific claim verification test sets.\nFactDetect can have broader applications in different fact-checking and factual consistency evaluation tasks. As a future work, we plan to incorporate FactDetect in the factual consistency evaluation of LLMs."}, {"title": "Limitations", "content": "A drawback of our method is the reliance on a generative language model for producing short fact sentences throughout the entire process. Despite employing Mistral-7B, which is among the top open-source LLMs available, the factual accuracy and overall quality of the generated content are bounded by the capabilities of this particular model. Consequently, any inaccuracies from the model could impact the effectiveness of the end-to-end claim verification system.\nFurthermore, a limitation of zero-shot FactDetect in real-world claim-verification systems is the need to augment the short sentences into the prompt, which is an additional step and can be time-consuming in the claim verification task. However, this problem is mitigated when we fine-tune a claim-verification system with FactDetect in the training phase, and during inference, we just use the claim and evidence as input."}, {"title": "Ethics Statement", "content": "Biases. We acknowledge the possibility of bias in generated outputs from the trained LLM. However, this is beyond our control.\nPotential Risks. Our approach can be used for automated fact-checking. However, they could also be used by malicious actors to manipulate and attack fact-checking models. A possible future direction is to detect such malicious actions before deployment.\nEnvironmental Impact. Training and using LLMs involves considerable computational resources, including the necessity for GPUs or TPUs during training or inference which can have an impact on the environment. However, we trained our datasets on relatively smaller language models with less than 1B parameters and we used LLMs for inference only which has negligible negative effect on the environment."}, {"title": "LLM Factuality Evaluation for Document Summarization Through FactDetect", "content": "We show that FactDetect is versatile and can be applied to tasks beyond claim verification, such as evaluating the factual consistency of LLM-generated document summaries. To conduct this experiment, we transform the task of evaluating factuality in LLM outputs for document summarization into a claim verification problem. In this setup, the original document serves as evidence, and the summary statement is treated as a claim. We then determine if the statement can be inferred from the document. We then generate short related sentences for the document(evidence) given the statement (claim) using FactDetect and perform experiments similar to the claim verification task. In this setup, the only difference is in the output verdict. Instead of prompting LLM to output one of the Supported, Contradicted and NEI verdicts, we prompt it if the statement can be inferred from the given document. The output should be either Yes or No."}, {"title": "Factuality Evaluation Dataset", "content": "We conduct experiments using the Factual Inconsistency Benchmark (FIB (Tam et al., 2022)) dataset, which includes data from the XSum (Narayan et al., 2018) and CNN/DM (Hermann et al., 2015) document summarization datasets. Each instance in the FIB dataset contains two summaries, one of which is factually consistent. For our experiments on the CNN/DM dataset, we use 457 documents, each paired with two statements, one factually consistent and the other not. We label these pairs as \"Yes\" for factually consistent and \"No\" for factually inconsistent, resulting in a total of 914 document-statement pairs."}, {"title": "Baselines", "content": "We compare AugFactDetect with Vanilla, CoT, and Direct prompting methods and report the results for 3 open source LLMs of Flan-T5-XXL, Llama2-13B, and Mistral-7B."}, {"title": "Metrics", "content": "We report results for Macro F1 score, Accuracy, and AUC for this binary classification approach."}, {"title": "Results", "content": "The results are reported in Table 5. We observe that best results are achieved when AugFactDetect is used as prompting method for factual consistency evaluation. Overall decomposing the document into smaller sentences seems to be useful for factual consistency detection and using FactDetect for this task shows superior performance which suggest the effectiveness of FactDetect and its applications beyond the claim verification task."}]}