{"title": "Inverse Transition Learning:\nLearning Dynamics from Demonstrations", "authors": ["Leo Benac", "Abhishek Sharma", "Sonali Parbhoo", "Finale Doshi-Velez"], "abstract": "We consider the problem of estimating the transition dynam-\nics T* from near-optimal expert trajectories in the context\nof offline model-based reinforcement learning. We develop a\nnovel constraint-based method, Inverse Transition Learning,\nthat treats the limited coverage of the expert trajectories as a\nfeature: we use the fact that the expert is near-optimal to in-\nform our estimate of T*. We integrate our constraints into a\nBayesian approach. Across both synthetic environments and\nreal healthcare scenarios like Intensive Care Unit (ICU) pa-\ntient management in hypotension, we demonstrate not only\nsignificant improvements in decision-making, but that our\nposterior can inform when transfer will be successful.", "sections": [{"title": "Introduction", "content": "In traditional planning scenarios, the rewards R and transi-\ntion dynamics T* of the environment are known, and the\ngoal is to compute the optimal policy \\(\\pi^*\\) that maximizes\nlong-term returns. However, in many real-world situations,\nthe transition dynamics T* are unknown. Model-Based Re-\ninforcement Learning (MBRL) addresses this by first learn-\ning T* and then performing planning, which improves data\nefficiency and enables counterfactual reasoning (Sutton and\nBarto 2018; Ghavamzadeh et al. 2015; Kidambi et al. 2020;\nYu et al. 2021; Lee, Lee, and Kim 2021; Poupart et al. 2006;\nHa and Schmidhuber 2018; Oh et al. 2015; Buesing et al.\n2018).\nThis paper focuses on learning the true dynamics T* in\noffline settings using batch data generated by a near-optimal\nexpert. This is a common setting in fields like healthcare and\neducation, where one can presume that the behavior policy\nis imperfect but generally reasonable. Learning T* from ob-\nservational data is challenging due to low coverage of the\nstate-action space. Without the ability to interact with the\nenvironment, it is crucial to utilize the limited data effec-\ntively.\nWe leverage the knowledge that the trajectories are near-\noptimal to better estimate T*. Consider the simple 2-state\nMDP shown on the next page. The blue path shows the ob-\nserved optimal behavior leading to the goal state \\(S_{\\text{goal}}\\) from\n\\(s_1\\) after action \\(a_1\\). The dashed lines represent the alterna-\ntive hypothetical paths following an unobserved action \\(a_2\\)."}, {"title": "Preliminaries", "content": "Markov Decision Processes (MDPs). An MDP M can be\nrepresented as a tuple M = {S, A,T*, \u03b3, R}, where S is\nthe state space, A the action space, T* is the true dynam-\nics of the environment, y is the discount factor and R is a\nbounded reward function. In planning, the goal is to find the\nbest policy \\(\\pi^*\\) corresponding to an MDP M. In this paper,\nwe focus on tabular MDPs (discrete state and action spaces).\nBellman Equations. The values functions of a policy are"}, {"title": "Estimating Transition Dynamics with\n\\( \\epsilon \\)-optimal Expert", "content": "This section introduces key definitions to relate the expert's\noptimality to the true dynamics T*. We describe the problem\nsetting as follows:\nDefinition 1. (\\( \\epsilon \\)-ball) For any state s and transition dynam-\nics T, an action a is in the \\(\\epsilon\\)-ball \\(\\epsilon(s;T)\\) if it is \\(\\epsilon\\)-close\nto the optimal action according to the optimal Q-function\n\\(Q^*(., .; T)\\) (We refer to actions as valid if they are within the\n\\(\\epsilon\\)-ball for state s and dynamics T, and as invalid if they are\noutside this \\(\\epsilon\\)-ball):\n\\(a \\in \\epsilon(s; T) \\leftrightarrow \\max_{a'} Q^*(s, a';T) - Q^*(s, a; T) \\leq \\epsilon\\)\nDefinition 2. (\\(\\epsilon\\)-optimality) A policy \\(\\pi_{\\epsilon}(.\\mid s; T)\\) is \\(\\epsilon\\)-optimal\nwith respect to transition dynamics T if it exclusively selects\nactions from the \\(\\epsilon\\)-ball \\(\\epsilon(s;T)\\) for all states s. An action a is\n\\(\\epsilon\\)-optimal with respect to dynamics T if \\(a \\in \\epsilon(s; T)\\).\nDefinition 3. (\\(\\epsilon\\)-ball property) Transition dynamics T sat-\nisfy the \\(\\epsilon\\)-ball property for state s if \\(\\epsilon(s; T) = \\epsilon(s;T^*)\\) and\nall invalid actions are \\(\\epsilon\\)-away from all valid actions accord-\ning to \\(Q^* (s, .; T)\\).\nWe aim to learn dynamics T that enforce the \\(\\epsilon\\)-ball prop-\nerty across all states s, ensuring that these dynamics can dis-\ncern between valid and invalid actions.\nDefinition 4. (Deterministic/stochastic-policy state) A\ndeterministic-policy state occurs when the \\(\\epsilon\\)-optimal expert\n\\(\\pi_{\\epsilon}(.\\mid s; T^*)\\) selects a single action \\(a^* = a\\) in state s. A\nstochastic-policy state occurs when the expert selects mul-\ntiple possible actions in state s.\nDeterministic-policy states are akin to conditions well-\nunderstood by clinicians who are certain of the best treat-\nment, whereas stochastic-policy states resemble conditions\nwhere multiple reasonable treatments are known without\nclear superiority.\nProblem Setting We assume we are given the MDP M\nexcept for the true transition dynamics T* (i.e., M \\ {T*}).\nWe also have access to batch data \\(D = \\{(s_i, a_i,s'_i)\\}_{i=1}^{N}\\),\nwhere the data which is assumed to have been generated\nby the behavior policy \\(\\pi_{\\epsilon}(.\\mid s; T^*)\\), which is \\(\\epsilon\\)-optimal re-\ngarding the true dynamics T*. Note that when \\(\\epsilon = 0\\),\nthe expert \\(\\pi_{\\epsilon}(.\\mid .; T^*)\\) is fully optimal, and hence we would\nonly encounter deterministic-policy states. As \\(\\epsilon\\) increases,\nthe expert begins to act more suboptimally, leading to more\nstochastic-policy states."}, {"title": "Constraints on Transition Dynamics Given\n\\(\\pi_{\\epsilon}(.\\mid .; T^*)\\)", "content": "In this section, we develop a set of constraints compatible\nwith an \\(\\epsilon\\)-optimal expert with respect to the true unknown\ndynamics T*. These constraints are designed to impose the\ndesired structure when estimating the true dynamics T* by\nenforcing the \\(\\epsilon\\)-ball property (3) for every state s (see Theo-\nrem 1). While the true transition dynamics T* are unknown,\nwe assume access to \\(\\pi_{\\epsilon}(.\\mid .; T^*)\\) for now but explain in\nand Algorithm 1 how to deal with the more realistic set-\ntings of having access to trajectories only. This allows us\nto determine \\(\\epsilon(s; T^*)\\) for each state s by examining the ac-\ntions of \\(\\pi_{\\epsilon}(.\\mid s; T^*)\\) with non-zero probabilities. For a policy\n\\(\\pi\\), the constraints for each state s referred in this paper as\nconstraints(\\( \\pi \\)) are divided into two sets:\nconstraint 1(\\( \\pi \\)): Differentiation Between Valid and In-\nvalid Actions For every \\(a \\in \\epsilon(s; T^*)\\) and every \\(a' \\notin\n\\epsilon(s; T^*)\\):\n\\(Q^{\\pi} (s, a; T) - Q^{\\pi} (s, a';T) \\geq \\epsilon\\)\n\\(R(s, a) - R(s, a') + \\gamma (T(. \\mid s, a) - T(. \\mid s, a'))\\)\n\\(\\times (I - \\gamma T_{\\pi})^{-1}R_{\\pi} \\geq \\epsilon\\) (8)\nThis ensures that for every state s, every invalid action\nis at least \\(\\epsilon\\)-away from every valid action with respect to\n\\(Q^{\\pi} (s, .; T)\\).\nconstraint 2(\\( \\pi \\)): \\(\\epsilon\\)-Closeness of Valid Actions For each\ndistinct pair (a, a') \\(\\in \\epsilon(s; T^*)\\), the absolute difference be-\ntween their Q values is bounded by \\(\\epsilon\\):\n\\(|Q^{\\pi} (s, a; T) - Q^{\\pi} (s, a';T)| < \\epsilon\\)\n\\(\\leftrightarrow\\)\n\\(|R(s,a) - R(s,a') + \\gamma (T(. \\mid s,a) - T(. \\mid s,a'))\\)\n\\(\\times(I - \\gamma T_{\\pi})^{-1}R_{\\pi}| \\leq \\epsilon\\) (9)\nThis ensures that for each state s, every action within the\n\\(\\epsilon\\)-ball under the true transition dynamics T* is also \\(\\epsilon\\)-close\nunder the transition dynamics T.\nTheorem 1. If \\(\\pi_{\\epsilon}(.\\mid .;T^*) = \\pi^*(T^*)\\) and some dynamics\nT satisfies constraints(\\( \\pi_{\\epsilon}(.\\mid .; T^*)\\)) for each state s, then\n\\(\\pi^*(T) = \\pi^*(T^*)\\). Hence, T will recover the optimal action\n\\(a^*\\) with respect to the true transition dynamics T* for each\nstate s. (Note that \\(\\pi^*(T)\\) refers to the optimal policy of dy-\nnamics T.)\nIn general, given any deterministic policy \\(\\pi\\), we can uti-\nlize constraints(\\( \\pi \\)) to recover a transition dynamics T such\nthat \\(\\pi^*(T) = \\pi\\), and thus recover dynamics T that explain\nthe behavior induced by such policy \\(\\pi\\).\nLemma 1. For any \\(\\epsilon\\), if T satisfies constraints(\\( \\pi^*(T)\\)), then\nT will satisfy the \\(\\epsilon\\)-ball property."}, {"title": "Constraints on Transition Dynamics Given Batch\nData D Only", "content": "In practical settings, direct access to the expert policy\n\\(\\pi_{\\epsilon}(.\\mid s; T^*)\\) for each state s is typically unavailable, neces-\nsitating reliance on the batch data only D, which might\nnot cover all (state, action) pairs comprehensively. We con-\nstruct an estimated policy, \\(\\pi_{\\epsilon}(.\\mid s; T^*)\\), such that for \\(s \\in D\\),\n\\(\\pi_{\\epsilon}(s; T^*)\\) assigns a uniform probability to all actions a\npresent in D. For state \\(s \\notin D\\), we determine the optimal\naction a using the proposed transition model T (looking at\n\\(\\pi^*(T)\\)), and then assign this action a in the policy used to\ndefine constraints, as detailed in line 9 of Algorithm 1."}, {"title": "Methodology", "content": "In this section, we demonstrate how to leverage (near) opti-\nmal data with our constraints to infer both a point estimate\nand a posterior distribution over the true dynamics T*, re-\nspectively referred to as Inverse Transition Learning (ITL)\nand Bayesian Inverse Transition Learning (BITL).\nInverse Transition Learning In Figure 4 of Appendix,\nwe demonstrate the potential non-convexity of the feasi-\nble region arising from the constraints (Equations 8 and\n9), attributed to the inverse operation on T. By substitut-\ning \\(T_e = \\frac{T}{\\epsilon}\\) for T in these constraints, we linearize them\nand transform the problem into a quadratic convex optimiza-\ntion problem, solved efficiently using CVXPY (Diamond\nand Boyd 2016). The optimization formulation is:\n\\(min_{T}\\sum_{(s,a,s')}N_{s,a,s'} [T(s' \\mid s, a) - T^{MLE} (s' \\mid s, a)]^2\\)\nsubject to \\(\\forall(s, a) \\in D,\\forall a' \\notin \\epsilon(s;T^*)\\):\n\\(R(s,a) - R(s, a') + \\gamma (T(\\cdot \\mid s, a) - T(\\cdot \\mid s, a'))\\)\n\\(\\times (I - \\gamma T_{\\pi_{\\epsilon}(.,.;T^*)})^{-1}R_{\\pi_{\\epsilon}(.,.;T^*)} \\geq \\epsilon\\),\n\\(|R(s,a) - R(s,a') + \\gamma (T(\\cdot \\mid s, a) - T(\\cdot \\mid s, a'))\\)\n\\(\\times(I - \\gamma T_{\\pi_{\\epsilon}(.,.;T^*)})^{-1}R_{\\pi_{\\epsilon}(.,.;T^*)}| \\leq \\epsilon\\) (10)\nWe denote the result of this optimization as\nsolveITL (\\(\\pi_{\\epsilon}(.\\mid .; T^*),TMLE\\)), where TMLE is used\nto linearize the constraints. Linearizing the constraints with\n\\(TMLE\\)\n\\(\\pi_{\\epsilon}(.\\mid.;T^*)\\) is appropriate because it reflects the state-action\nspace where we have data, and thus where we expect\n\\(TMLE\\) to be accurate. This method not only accelerates\ntraining and provides guarantees on the constraints but also\nempirically outperforms (Herman et al. 2016), which relies\non gradient descent and can be trapped in poor local optima,\nas shown in Table 4.\nBy iteratively solving the optimization problem as de-\ntailed in Algorithm 1, we obtain a point estimate T* that sat-\nisfies the \\(\\epsilon\\)-ball property for each state s in D (see Definition\n3), aligning with the \\(\\epsilon\\)-optimal expert behavior (\\(\\pi_{\\epsilon}(.\\mid .; T^*)\\)).\nIn essence, T* represents the dynamics that best fit the\nbatch data D while also explaining the \\(\\epsilon\\)-optimal expert be-\nhavior induced by \\(\\pi_{\\epsilon}(.\\mid .; T^*)\\)."}, {"title": "Synthetic Environments", "content": "Gridworld Environment\nThe Gridworld environment is structured as follows:\n\\(\\cdot\\) Grid Size: The world is a grid consisting of 5 \u00d7 5 tiles,\nresulting in a total of 25 distinct states.\n\\(\\cdot\\) Actions: At each state, an agent can choose from four\npossible actions: move right, move up, move left, or\nmove down.\n\\(\\cdot\\) Initial State: The agent always starts from the bottom\nleft corner of the grid, which is designated as the initial\nstate.\n\\(\\cdot\\) Goal State: The objective for the agent is to reach the\ngoal state, located at the top right corner of the grid.\n\\(\\cdot\\) Dynamics:\nIntended Actions: When the agent selects an action,\nthere is an 80% chance that it will move deterministi-\ncally to the intended adjacent state."}, {"title": "Real-life ICU Environment", "content": "Following the experiments within a synthetic environments,\nwe now transition to the evaluation of our methodology in\na real-world scenario. To this end, we selected the Medical\nInformation Mart for Intensive Care IV (MIMIC-IV) dataset\nas our experimental field. This dataset offers a rich, diverse,\nand challenging setting for testing our method, especially\ngiven its potential to contribute to advancements in health-\ncare analytics and patient care strategies.\nAbout MIMIC-IV Dataset\nThe MIMIC-IV dataset, developed by the MIT Lab for Com-\nputational Physiology and publicly available, aggregates a\nvast range of anonymized health data from critical care units\nat Beth Israel Deaconess Medical Center in Boston. Cov-\nering over a decade's worth of patient admissions, it pro-\nvides detailed records on demographics, vital signs, lab tests,\nmedications, and more, establishing itself as a critical re-\nsource for healthcare model development. Its comprehensive\nscope spans all patient care aspects, enabling the creation of\nholistic models for predicting diverse patient outcomes. The\ndataset's richness lies in its variety, covering over 40,000 pa-\ntients of different ages, ethnicities, and conditions, and its\ngranularity, offering high-resolution data points and time-\nstamped records, which are essential for developing precise,\ndynamic healthcare models. Moreover, MIMIC-IV's public\naccessibility fosters a global research community's collabo-\nration, enhancing healthcare analytics advancements.\nUtilizing the MIMIC-IV dataset, we showcase out the\nlearning applicability of our method in real-world health-\ncare, to get valuable insights from the data in such a compli-\ncated environemnt.\nData Preprocessing for Hypotension Analysis\nIn our investigation into hypotension within ICU settings,\nwe tailored our preprocessing steps to exclusively include\npatients affected by this condition. Our methodology com-\nmenced with the application of specific filters on the\nMIMIC-IV dataset to accurately identify the patient cohort\nof interest. These filters were designed to capture adults aged\n18 to 80 years, who had ICU stays of a minimum duration\nof 24 hours, and exhibited Mean Arterial Pressure (MAP)\nreadings of 65mmHg or below, indicative of acute hypoten-\nsion.\nThe analytical framework of our study is built around\na carefully selected set of five clinical variables that con-"}]}