{"title": "Localized Observation Abstraction Using Piecewise Linear Spatial Decay for Reinforcement Learning in Combat Simulations", "authors": ["Scotty Black", "Christian Darken"], "abstract": "In the domain of combat simulations, the training and deployment of deep reinforcement learning (RL) agents still face substantial challenges due to the dynamic and intricate nature of such environments. Unfortunately, as the complexity of the scenarios and available information increases, the training time required to achieve a certain threshold of performance does not just increase, but often does so exponentially. This relationship underscores the profound impact of complexity in training RL agents. This paper introduces a novel approach that addresses this limitation in training artificial intelligence (AI) agents using RL. Traditional RL methods have been shown to struggle in these high-dimensional, dynamic environments due to real-world computational constraints and the known sample inefficiency challenges of RL. To overcome these limitations, we propose a method of localized observation abstraction using piecewise linear spatial decay. This technique simplifies the state space, reducing computational demands while still preserving essential information, thereby enhancing Al training efficiency in dynamic environments where spatial relationships are often critical. Our analysis reveals that this localized observation approach consistently outperforms the more traditional global observation approach across increasing scenario complexity levels. This paper advances the research on observation abstractions for RL, illustrating how localized observation with piecewise linear spatial decay can provide an effective solution to large state representation challenges in dynamic environments.", "sections": [{"title": "INTRODUCTION", "content": "In the domain of combat simulations, the training and deployment of deep reinforcement learning (RL) agents still face substantial hurdles due to the dynamic and complex nature of such environments, which often results in an exponential increase in training time required to achieve a certain threshold of performance. Traditional RL approaches often struggle to learn in these environments due to the large state spaces necessary to properly represent the number of entities, detailed terrains, and variable initial starting conditions characteristic of wargaming. This complexity, compounded by RL's sample inefficiency, vastly increases the computational resources and time-to-train needed to achieve satisfactory agent performance outcomes\u2014rendering the process impractically costly and time-consuming.\nThis paper presents a novel approach to overcome these challenges by abstracting the agent's observation space while preserving sufficient detail of the relevant portions of the environment. By abstracting the state space into a more compact and computationally manageable observation while still maintaining critical spatial information, we aim to enhance training efficiency while significantly reducing the computational load needed. Specifically, our investigation delves into optimizing training efficacy against the backdrop of limited computational budgets a common constraint in applying RL to combat simulations.\nIn this study, we develop, implement, and test a localized observation abstraction approach using piecewise linear spatial decay. Through our analysis, we demonstrate that a localized observation strategy consistently outperforms a global observation method across increasing levels of complexity. This finding emphasizes the superiority of this approach when training agents in complex scenarios where spatial relationships are essential offering a way to help RL scale to still produce acceptable levels of performance in larger, more dynamic environments than have previously been possible in the domain of combat simulations."}, {"title": "BACKGROUND", "content": "Reinforcement Learning is a subset of machine learning that involves an agent learning to make decisions through direct interaction with its environment. In this process, the agent executes actions and receives feedback, either as rewards or penalties. Its objective is to optimize the cumulative reward over time. Through this method of trial-and-error search, the agent gradually learns the most effective actions based on the current state of the environment. This accumulated knowledge forms the agent's policy, which is essentially the set of strategies it uses for decision-making.\nMore formally put, a reinforcement learning problem, typically consists of a decision-maker, referred to as an agent, and an environment, represented by states $s \\in S$. The agent takes actions $a_t$ as a function of the current state $s_t$ such that $a_t = A(s_t)$. After choosing an action at time $t(a_t)$, the agent receives a reward $r_{t+1}$ and finds itself in a new state $s_{t+1}$. The action $a_t$ comes from a strategy called a policy that maps states $s \\in S$ to a probability of selecting each possible action $\u03c0(s, a)$. As the agent interacts with the environment, it learns the optimal policy that maximizes its reward in the long run.\nAlthough in this study we use RL to train intelligent agents in a combat simulation environment, this paper focuses on abstracting the state $s_t$ in a way that makes the RL problem more tractable in complex environments. Research we have leveraged in our approach to RL specifically include those exploring Atari 2600 games (Mnih et al., 2015; Van Hasselt et al., 2016), Go (Holcomb et al., 2018), Chess (Silver et al., 2017), Shogi (Silver et al., 2017), Dota 2 (Berner et al., 2019), StarCraft II (Vinyals et al., 2019), and Atlatl (Allen, 2022; Boron, 2020; Cannon & Goericke, 2020). Nevertheless, despite RL having achieved human, expert, or even superhuman-level play in some of these games, to date, Al agents have not been shown to significantly outperform humans or scripted (rule-based) agents in the complex domain of wargaming. In fact, research applying RL to combat simulations has shown that despite good outcomes in small scenarios, scaling to larger scenarios has typically resulted in poor performance (Boron, 2020; Cannon &\nGoericke, 2020; Rood, 2022). We posit that this is in large part due to the exponential growth of state space complexity (Bellman, 1954) and well-documented sample inefficiency problem in RL requiring extensive interaction with the environment (Mnih et al., 2015), especially as its observation space grows in complexity."}, {"title": "State Abstraction", "content": "The notion of abstraction for AI is not new and has been used since the beginning of AI and logic, dating back to the work by Whitt in approximating dynamic programs (Whitt, 1978; Abel, 2019). Giunchiglia and Walsh (1992) informally defined abstraction as \"the process of mapping a representation of a problem onto a new representation.\u201d Abstraction allows people to consider what is relevant and to forget or disregard what is irrelevant based on the specific task at hand (Giunchiglia & Walsh, 1992).\nWithin the context of AI and RL, abstraction plays a critical role in simplifying complex decision-making processes. Given limited computational power and a complex enough environment, agents in a simulation cannot model everything in their environment and still learn appropriate or optimal behaviors within a reasonable time. As the complexity of the environment increases and assuming that a minimum level of performance is desired-agents may have to discard some information and focus only on relevant information to solve a specific problem. This form of abstraction allows for a more manageable representation of intricate environments, enhancing the learning efficacy of AI agents (Ho et al., 2019). Moreover, this approach not only has the potential to reduce computational demands but may also improve the adaptability and performance of AI-trained agents in scenarios that may be significantly different from the scenarios for which the agents were trained (Abel, 2019).\nAs Shanahan and Mitchell (2022) explore in depth in their research, for abstraction to be most useful, \u201cthe domain of a concept's application must be larger than the domain of its acquisition.\" We contend that abstraction is critical to transferring concepts learned from one setting to another that differs from which it acquired said concept. Nevertheless, because abstractions inherently discard information potentially compromising the effectiveness of the decisions made based on these abstractions we must understand and balance the trade-off between making learning easier (or tractable) and preserving enough information to allow for optimal policy discovery (Abel, 2020). The more we abstract the state space, the more information is lost and the harder it will be to guarantee an optimal or near-optimal solution (Li et al., 2016). Nevertheless, researchers agree that a tradeoff exists in that, although coarser abstractions may result in sub-optimal actions, they allow for better planning and value iteration (Li et al., 2016).\nWhile the concept of abstraction as applied to RL has slowly evolved, Abel (2019) formalizes this notion and comprehensively investigates the role of abstraction in RL in detail, particularly focusing on state abstraction. For this paper, we use Abel's (2020) definition of State Abstraction as a function $\u03c6 : S \u2192 S_\u03c6$, which maps each true environmental state $s \\in S$ into an abstract state $s_\u03c6 \\in S_\u03c6$. In other words, an abstracted state serves as the agent's interpretation of the current environment, which will discard or simplify some information."}, {"title": "RELATED WORKS", "content": "The field of abstraction in reinforcement learning (RL) has seen a variety of approaches, each addressing different aspects of complexity in the decision-making processes. Understanding these works contextualizes our research and highlights the gaps our study aims to fill.\nSutton et al. (1999) pioneered the concept of temporal abstraction in RL by extending Markov decision processes (MDPs) (Puterman, 1994) and proposing semi-Markov decision processes (SMDPs). This foundational work emphasized understanding temporal factors in decision-making, focusing on the abstraction of actions rather than states. While crucial in developing the RL framework, it differs from our approach, which instead concentrates on state abstraction in the spatial context.\nFurther exploring abstraction in games with large state spaces, Sandholm (2015) introduced sophisticated methods for game-theoretic abstraction in large incomplete-information games. His work involved creating simpler models of games that maintained strategic similarity. This methodology is instrumental in game theory but diverges from our focus on state abstraction tailored to specific spatial dynamics.\nAndersen et al.'s (2018) study on variational autoencoders (VAEs) captures elements of our approach in simplifying complex state spaces. Their emphasis, however, is on probabilistic latent space encoding as opposed to our deterministic spatial representation. While VAEs provide valuable insights into data encoding, our method focuses on explainable spatial relationships vital to decision-making in combat-like scenarios.\nHo et al. (2019) highlighted the critical role of abstraction in AI and RL, especially in managing complex environments. Their insights into state and temporal abstractions for efficient decision-making align closely with our work. Ho et al. (2019) demonstrated how abstraction simplifies computations and facilitates efficient trade-offs in learning informing our approach in combat simulations in support of wargaming.\nIn a more focused application, Allen et al. (2021) utilized Markov processes for state space compression in RL. Specifically, their method grouped similar states based on transition patterns. While this offers a useful form of abstraction, it contrasts significantly with our approach where the spatial component, rather than the transition patterns, is central in informing optimal behaviors.\nLastly, Jergeus et al. (2022) took a unique approach by proposing linguistic abstractions in RL using a neuro-symbolic framework. While a completely different form of abstraction, their focus on abstracting linguistic communication among agents illuminates the versatility of abstraction techniques.\nEach of these works illustrates the broader application of abstraction in RL. Collectively, they demonstrate the diverse methods of tackling complexity in the decision-making process. Our research builds upon these foundations, explicitly addressing the underexplored area of spatial state abstraction in the complex and intricate domain of combat simulations. As discussed by Abel (2020) in his Ph.D. Dissertation, A Theory of State Abstraction for Reinforcement\nLearning, RL agents currently face significant challenges in generalizing experiences, exploring environments, and learning from delayed and sparse feedback, all within limited computational constraints. Abel (2020) highlights the necessity of abstraction in these processes, focusing on state abstraction, to improve sample efficiency in RL. Furthermore, he outlines three desiderata for useful state abstraction\u2014preserving near-optimal behavior, being learnable and computable efficiently, and reducing the time or data needed for effective decision-making all of which we also seek to achieve in this study."}, {"title": "METHODOLOGY", "content": "To compare the tradeoffs between the traditional global observation approach and our approach using localized observation abstraction with piecewise linear spatial decay, we use the Atlatl simulation environment and employ the following methodology to investigate the tradeoff in agent performance vs. scenario complexity."}, {"title": "Atlatl Simulation Environment", "content": "We use the Atlatl Combat Simulation environment (Darken, 2022) to develop, implement, and experiment with our research approach. Atlatl is a simple but effective combat model developed at the Naval Postgraduate School (NPS). It includes an underlying combat model that is purposefully simplistic, as well as the surrounding Gymnasium (Farama Foundation, 2023) infrastructure that supports rapid Al experimentation. The environment also contains hooks that enable interfacing with standard RL codebase and algorithms, such as Stable-Baselines 3 (SB3). This type of basic environment allows researchers to develop, apply, and evaluate cutting-edge AI to operational and tactical problems more efficiently and effectively than using operational or high-fidelity simulation systems."}, {"title": "Global Observation", "content": "The global observation in Atlatl consists of an 18 \u00d7 n \u00d7 m tensor, where n and m are the height and width of the gameboard. For this study, we use square gameboards (e.g., a 5 \u00d7 5 scenario consists of an observation space of 18 \u00d7 5 \u00d7 5). Each channel of the tensor represents one specific type of information to be captured, as shown in Figure 3. Specifically, channel 0 is a binary matrix depicting where the blue unit to be moved (or on-turn) is located; channel 1 is binary matrix depicting all blue units that still have the ability to move during the current phase; channel 2 is a binary matrix depicting all legal moves available for the unit on-move; channels 3 and 4 are matrices that depict the health level (scaled from 0 to 1.0) of each respective unit on the gameboard based on factions; channels 5 through 8 are binary matrices depicting unit types; channels 9 through 13 are binary matrices representing terrain; channels 14 and 15 are binary matrices depicting the city owner (i.e., which faction was the last to pass through an urban hexagon); channel 16 is a matrix filled with a phase indicator value representing the current phase of the game; and channel 17 is a matrix filled with the normalized game score. While we recognize that these last two features can be represented more compactly as vectors or scalars rather than matrices, we maintain the matrix construct for simplicity."}, {"title": "Localized Observation Abstraction Using Piecewise Linear Spatial Decay", "content": "Our localized observation space takes in the game's global observation space described above and performs additional processing to compress the information into an 18 \u00d7 7 \u00d7 7 observation, regardless of actual gameboard size. Even gameboards smaller than 7 \u00d7 7 are represented as a 7 \u00d7 7 with the area outside of the gameboard simply represented with zeros. To construct the localized 7 \u00d7 7 matrix, we first center the global matrix on the agent on-move. We then divide the entire area into 24 equal segments (due to the outer perimeter of this matrix consisting of 24 total grids) of 15\u00b0 each. Finally, we multiply each entry by a weight $w$ as a function of Euclidean distance $d$, determined by the following equation, and visually depicted in Figure 4:\n$w(d) =  \\begin{cases}\n1 & \\text{for } d \\leq 3 \\\\\n1-0.9 * \\frac{d-3}{7} & \\text{for } 3 < d < 7 \\\\\n0.1-0.9 * \\frac{d-7}{100} & \\text{for } 7 \\leq d < 100 \\\\\n0.01 & \\text{for } d \\geq 100\n\\end{cases}$"}, {"title": "Gymnasium Environment", "content": "For our RL training, we use a custom Gymnasium (Farama Foundation, 2023) environment configurable for different roles (\"blue\" or \"red), AI types, and scenarios. The action space of our RL agent is defined as 7 discrete actions, one for each adjacent hexagon, plus the option to \"pass\" (i.e., take no action). Legal moves are defined as either moving to an unoccupied adjacent hexagon or engaging in combat by selecting a hexagon occupied by a unit of the opposing faction."}, {"title": "Neural Network Architecture", "content": "We use a residual convolutional neural network (CNN) specifically designed to process a hexagonally structured input observation space of any size which, for this study, is 18 \u00d7 n \u00d7 n where n is the size of one side of the gameboard. The architecture uses convolutional layers to transform the input observation tensor into 64 output channels. This is followed by 7 additional layers of 64 channels each. Each layer features HexagDLy hexagonal convolutions (Steppa &\nHolsch, 2019) with a kernel size of 1 \u00d7 1 and a stride of 1. Additionally, in each layer, we include a Rectified Linear Unit (ReLU) activation function and a residual connection. After 7 layers, the resulting multi-dimensional tensor is then flattened into a one-dimensional tensor and is passed through a final linear layer. This layer maps the flattened tensor to a 512-dimensional feature vector, which is then passed through a final ReLU activation function."}, {"title": "Reinforcement Learning Algorithms", "content": "We employ the Deep Q-Network (DQN) algorithm (Raffin, 2018). The hyperparameters used were optimized through extensive hyperparameter tuning in similar scenarios, though not specific to this experiment. The final configuration included a learning rate of 0.0002, a buffer size of 1,000,000, learning starting at 10,000 steps, a batch size of 64, and a discount factor (y) of 0.93. The target network update interval was set to 1,000 steps. For exploration, we employed an initial epsilon ($\u025b_i$) of 1.0, decaying linearly to a final epsilon ($\u025b_f$) of 0.01, with an exploration fraction of 1. 0. The training frequency was set to every 4 steps, with a gradient step of 1 per training update."}, {"title": "Scenarios", "content": "We use randomly generated scenarios consisting of square hexagonal gameboards with one city and no other terrain. We use scenario gameboard sizes from 3 \u00d7 3 up to 12 \u00d7 12 in increments of 1, with each representing an increase in complexity level, where complexity level 3 is represented by a 3 \u00d7 3, complexity level 4 by a 4 \u00d7 4, and so on. Examples are shown in Figure 6. Each game begins with a random number of entities per faction with a minimum and maximum number computed as a factor of the length of the gameboard, where $num\\_units_{max} = gameboard\\_length$ and $num\\_units_{min} = round(\\frac{gameboard\\_length}{2})$. For example, for a 5\u00d75 gameboard, the scenario would start with a random number of units per faction between 3 and 5; whereas for a 10 \u00d7 10 scenario, the random number of starting units per faction would be between 5 and 10. Each scenario also includes 1 urban hexagon randomly placed according to force ratio. If one faction has a smaller force ratio (i.e., less units as compared to the opposing faction), the city is placed on their side of the gameboard. If the force ratios are equal (i.e., both factions have an equal number of units), the city is placed in a neutral location along the middle axis of the board. We set the number of phases in the game as $phases = 4 * gameboard\\_length$, where each phase is one entire turn for one faction (i.e., one faction is allowed to make one legal move for each of its available entities). Setting the number of phases to this value provides enough turns for a unit to go from one end of the gameboard to the opposite end and return, likely giving them enough turns to execute complex maneuvering if warranted."}, {"title": "Training", "content": "We train each model for 10 million steps against a baseline rule-based adversary model we call Pass-Agg. This name is derived from the terms \"passive\u201d and \u201caggressive.\u201d The agent first assesses its posture as \"attack\" or \"defend\" based on the relative strength of its faction as compared to its opponent. The agent prioritizes engaging any enemy units within its attack range of 1 hexagon (i.e., hexagons adjacent to its own position). If multiple targets exist within range, the agent uses a uniform distribution to select its target. In the absence of attack opportunities, the agent assesses which hexagon to move to based on proximity to enemy units and urban hexagons. The agent seeks to position itself advantageously while maintaining a balance between offensive actions and strategic repositioning. This decision is based on a hexagon scoring system that evaluates the advantage of moving towards urban hexagons or attacking nearby enemies. If neither attacking nor moving is advantageous, the agent may choose to \"pass\", effectively maintaining its current position. While a simple model, Pass-Agg has proven to be an effective agent that regularly achieves near-optimal scores and displays credible moves that would be expected of these combat units.\nTo learn effective behaviors, we design a reward system that balances defeating the opposing faction and occupying urban hexagons with preserving its own force. Our rewards are computed at each time step using the following equation:\n$R_{engineered} = max(R_{raw}, 0) * \\frac{S_c}{S_0} + B_tI_t$\nWhere $R_{raw}$ is the difference in game score between the current time step and the previous time step; $S_c$ is the current total friendly strength; $S_0$ is the original total friendly strength; $B_t$ is a terminal bonus reward of 25 points that our research shows discourages units from moving into the adversary units' attack range during the last turn of the game; and $I_t$ is a terminal game state indicator that takes on a value of 1 if the game is terminal or 0 if the game is not terminal."}, {"title": "Evaluation", "content": "We evaluate each of our trained models against the Pass-Agg model. We run 100,000 games where each game begins with a randomly generated scenario using the scenario parameters specified above. In addition to training with the Pass-Agg behavior model as the adversary, we also evaluate Pass-Agg vs. Pass-Agg as our rule-based model baseline. While we anticipate that Pass-Agg will outperform the RL-trained models as complexity levels increase, we aim to assess the extent of improvement an RL-trained agent offers over a rule-based agent while also seeking to determine when this relationship reverses. The performance of the Pass-Agg model serves as our benchmark to determine the point at which an RL-trained model ceases to surpass the effectiveness of a rule-based approach. Lastly, we also evaluate a random-actions model to determine when our RL-trained models do no better than, or converge to, a random actor."}, {"title": "RESULTS AND DISCUSSION", "content": "With each trained model, we run an evaluation consisting of 100,000 randomly generated games for each behavior model at each complexity level against our baseline Pass-Agg adversary behavior model. The means of the scores are presented in Table 1. For conciseness, in the following sections, we use the term Local to refer to our RL-trained model utilizing the localized observation abstraction using piecewise linear spatial decay; the term Global to refer to the RL-trained model using a global observation; the term Rule-Based to refer to the scripted Pass-Agg model; and the term Random to refer to the random-actions model. Overall, we see in Table 1 that Local outperforms Global across all complexity levels by a large margin. Furthermore, we also see the Local outperforms Rule-Based in complexity levels 3 through 5 by a large margin and, as expected, begins to fall off as complexity increases."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "Overall, this research presents a compelling case for implementing a localized observation abstraction with some spatial decay component when training models using RL, specifically within environments where spatial relationships may be crucial. Whereas we hypothesized a trade-off space between the global and localized observation approaches, we find that a localized observation with spatial decay consistently outperforms a global observation approach across all levels of complexity examined. The superior performance of using localized observation is particularly striking in the smaller-scale scenarios, as it was anticipated that the global observation approach would be at least as good as the localized approach, if not better. We posit that the efficacy of the localized abstraction approach is likely due to the agent's improved ability to generalize when centered in the observation space, significantly enhancing the learning process and decision-making ability. This approach balanced reducing state-space complexity with the retention of relevant information, thereby better optimizing the agent's performance.\nRevisiting Abel's (2020) three desiderata for useful state abstraction (preserving near-optimal behavior, being learnable and computable efficiently, and reducing the time or data needed for effective decision-making), we find our observation abstraction clearly accomplishes all three. Our agents performed better than agents using global observations given a set training budget; our abstraction of the state space proved more efficient than training the agent to reach the same performance threshold using the global observation space; and our abstraction method reduced the time needed for training to reach a desired performance threshold.\nThe outcomes of this study underscore the potential of localized observation abstractions to become a pivotal component in the application of RL in complex, dynamic environments, such as those encountered in military combat simulations. By demonstrating the limitations of a global observation approach and the advantages of a localized approach, this work paves the way for future investigations into more sophisticated observation abstraction methods to better enable RL scalability.\nWe will extend the findings of this study by introducing more complex scenarios (e.g., using more types of terrain and units) and increasing the training budget to examine if the same trends hold valid with increasing complexity across these other dimensions. Furthermore, this study informs our current research area of scaling RL to deal with more complex scenarios via hierarchical reinforcement learning (HRL). These results and insights inform how we can better decompose the environment spatially and best explore the nuanced interplay between RL-trained agents and rule-based agents across varying levels of complexity. Such research will refine and generalize the methodologies discussed and contribute significantly to the broader field of AI, offering insights into the scalable training and deployment of RL agents in real-world scenarios."}]}