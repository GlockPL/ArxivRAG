{"title": "RECOVERING MANIFOLD STRUCTURE USING OLLIVIER-RICCI CURVATURE", "authors": ["Tristan Luca Saidi", "Abigail Hickok", "Andrew J. Blumberg"], "abstract": "We introduce ORC-MANL, a new algorithm to prune spurious edges from nearest neighbor graphs using a criterion based on Ollivier-Ricci curvature and estimated metric distortion. Our motivation comes from manifold learning: we show that when the data generating the nearest-neighbor graph consists of noisy samples from a low-dimensional manifold, edges that shortcut through the ambient space have more negative Ollivier-Ricci curvature than edges that lie along the data manifold. We demonstrate that our method outperforms alternative pruning methods and that it significantly improves performance on many downstream geometric data analysis tasks that use nearest neighbor graphs as input. Specifically, we evaluate on manifold learning, persistent homology, dimension estimation, and others. We also show that ORC-MANL can be used to improve clustering and manifold learning of single-cell RNA sequencing data. Finally, we provide empirical convergence experiments that support our theoretical findings.", "sections": [{"title": "INTRODUCTION", "content": "The first step for almost all geometric data analysis tasks is to build a nearest neighbor graph. This reflects faith in the manifold hypothesis\u2014the belief that the data actually lies on a low-dimensional submanifold of the ambient $R^D$. In this setting, the nearest neighbor graph recovers the intrinsic geometry of the data manifold, using the observation that small ambient distances lie along the manifold whereas larger ones may not.\nUnfortunately, building nearest neighbor graphs from noisy data typically results in inaccurate representation of the metric structure of the underlying manifold. In this paper, we study edges in nearest neighbor graphs that shortcut through the ambient space and bridge distant neighborhoods of the underlying manifold. Such \u201cshortcut edges\u201d distort inferred distances and negatively impact downstream algorithms that operate on the graphs.\nWe show that Ollivier-Ricci curvature (ORC), a measure of discrete curvature on graphs (Ollivier, 2007), can be used to effectively identify shortcut edges when the data consists of noisy samples from a low dimensional submanifold. We also show that graph distances can be used to support the identification of shortcut edges, allowing us to avoid accidentally catching \"good\" edges. Guided by these results, we describe an algorithm, ORC-MANL, to detect and prune shortcut edges.\nORC-MANL marks edges with extremely negative ORC as candidate shortcut edges. The algorithm then constructs a thresholded graph with the candidates removed. We then use the thresholded graph distance between the endpoints of all candidate edges to check if the distance is large, the edge was likely shortcutting the manifold through the ambient space. If the distance is small, the edge is added back to the graph. We find that despite its simplicity, ORC-MANL is incredibly effective and provides tangible performance improvement for downstream geometric data analysis algorithms for a variety of synthetic and real datasets. We make our code for ORC-MANL and all experiments available on GitHub. 1"}, {"title": "1.1 CONTRIBUTIONS", "content": "We introduce a general-purpose method, ORC-MANL, that uses discrete graph curvature to detect and prune unwanted connectivity in nearest-neighbor graphs. Our method is theoretically justified, and in practice significantly improves the performance of downstream tasks like manifold learning, persistent homology, and estimation of important geometric quantities like intrinsic dimension and curvature. Furthermore we find that ORC-MANL is effective on real world single-cell RNA sequencing data: ORC-MANL pruning reveals clusters in PBMC data in accordance with ground truth annotations and results in embeddings that better preserves communities of neuronal cells. Finally, we also include experiments to show that our theoretical convergence results are supported by empirical experimentation."}, {"title": "1.2 RELATED WORK", "content": "Graph Pruning. Several graph pruning approaches have been proposed in the literature. Some use density estimation as a heuristic for detecting unwanted edges and show results on noiseless toy datasets (Xia et al., 2008; Chao et al., 2007). Zemel & Carreira-Perpi\u00f1\u00e1n (2004) proposed an approach that builds a minimum spanning tree of the original nearest neighbor graph; but this relies on the assumption that shortcutting edges are longer than good edges, a phenomenon that does not always hold. Another family of approaches attempt to adaptively tune the number-of-neighbors parameter k of the nearest neighbor graph based on the local geometry of the data. Zhan et al. (2009); Elhenawy et al. (2019) adopt similar approaches, looking at the linearity of neighborhoods using PCA and pruning accordingly. These methods typically demonstrate a limited set of results on noiseless data and provide little theoretical justification.\nOllivier-Ricci Curvature. Ollivier-Ricci curvature (ORC) was proposed as a measure of curvature for finite metric spaces by Ollivier (2007), with follow-up results demonstrating theoretical and empirical convergence to the underlying manifold Ricci curvature under mild assumptions (Ollivier, 2009; van der Hoorn et al., 2021). In the network geometry literature, ORC based approaches have been effective for community detection, drawing connections to Ricci flow from Riemannian geometry (Sia et al., 2019; Ni et al., 2019). Sia et al. (2019) prune edges with extremely negative ORC and justify it using theory that argues that ORC detects communities. Our theoretical work instead justifies the use of ORC for recovering correct manifold structure. A multitude of papers have used ORC for clustering and modeling diffusion processes on graphs as well (Gosztolai &\nArnaudon, 2021; Tian et al., 2023). This work has led to applications for graph neural networks, where ORC was used to prevent over-squashing and over-smoothing (Liu et al., 2023; Nguyen et al., 2023), and improving encodings of local graph structure (Fesser & Weber, 2024)."}, {"title": "2 BACKGROUND AND DEFINITIONS", "content": null}, {"title": "2.1 DIFFERENTIAL AND RIEMANNIAN GEOMETRY", "content": "Manifolds. A manifold is a generalization of the notion of a surface\u2014it is a topological space that locally looks like Euclidean space. Concretely, a manifold $M$ is an m-dimensional space such that for every point $x \\in M$ there is a neighborhood $U \\subset M$ such that $U$ is homeomorphic to $R^m$. At every point $x \\in M$ one can attach a m-dimensional vector space called the tangent space (denoted $T_xM$) that contains all directions in which a path in $M$ can tangentially pass through $x$. In a similar manner, the normal space at $x$ (denoted $N_xM$) is a vector space containing all directions normal to M at $x$. We will work with Riemannian manifolds, which are smooth manifolds endowed with a Riemannian metric. A Riemannian metric is an assignment of an inner product to each tangent space that varies smoothly with respect to $x \\in M$. This metric allows one to make statements about local similarity, angles, and distances. For a more detailed treatment of differential and Riemannian geometry, we direct readers to Prasolov (2022) and Lee (2018).\nGeodesics. In this paper we are concerned with submanifolds $M$ of $R^D$ whose Riemannian metrics are induced by the ambient Euclidean metric. Recall that the length of a continuously differentiable path $\\gamma : [a, b] \\rightarrow R^D$ is $L(\\gamma) = \\int_a^b ||\\gamma'(t)||_2 dt$. The geodesic distance between two points $x$ and $y$ in a submanifold $M$ of $R^D$ is simply the minimum length over all continuously differentiable paths connecting $x$ and $y$."}, {"title": "Tubular Neighborhoods.", "content": "The $\\tau$-tubular neighborhood Tub$\\_{\\tau}(M)$ of a submanifold $M$ of $R^D$ is the set $\\{x \\in R^D | d(M, x) \\leq \\tau\\}$, where $d(M, x)$ is the Euclidean distance between $x$ and the nearest point $x' \\in M$. Intuitively, Tub$\\_{\\tau}(M)$ is a \u201cfattened\u201d submanifold of $R^D$ that envelops $M$. We use the tubular neighborhood as a model of the support of a noisy sampling distribution over $M$ with a bounded level of isotropic noise."}, {"title": "Manifold Embedding Parameters.", "content": "We will define two manifold parameters that are central to the theoretical analysis that follows, adopted from Bernstein et al. (2001). The minimum radius of curvature is $\\tau_0(M) = \\frac{1}{max_{\\gamma,t} ||\\gamma''(t)||_2}$ where $\\gamma : R^+ \\rightarrow R^D$ is a time-parameterized unit-speed geodesic in M. Intuitively, $\\tau_0(M)$ indicates the extent to which manifold geodesics curl in the ambient Euclidean space. The minimum branch separation $s_0(M)$ is the largest positive number such that $\\forall x, y \\in M$, $||x - y||_2 < s_0(M)$ implies $d_M(x, y) \\leq \\pi \\tau_0(M)$. While in general people use quantities like the reach and injectivity radius to describe manifold embeddings, these quantities are intimately related to the ones described; we choose to stick to the described parameters for compatibility with theorems invoked from prior work."}, {"title": "Proposition 1.", "content": "Suppose $M$ is a compact submanifold of $R^D$ without boundary and with a minimum radius of curvature $\\tau_0(M)$. Then Tub$\\_{\\tau}M$ has a minimum radius of curvature of $\\tau_0(M) - \\tau$."}, {"title": "2.2 NEAREST NEIGHBOR GRAPHS AND UNWANTED CONNECTIVITY", "content": "Geometric machine learning approaches attempt to capture the underlying structure of $M$ from noisy samples $X$, typically beginning with the construction of a nearest neighbor graph. These graphs use connectivity rules of two flavors: $\\epsilon$-radius, or k-nearest neighbor (k-NN) (Bernstein et al., 2001). The $\\epsilon$-radius connectivity scheme asserts that for any two vertices a and b, an edge exists between them if $||a-b||_2 \\leq \\epsilon$. The k-NN rule on the other hand asserts that the edge exists only if b is one of the k nearest neighbors of a, or vice versa. While the $\\epsilon$-radius rule is more amenable to theoretical analysis, the k-NN rule is used more often in practice."}, {"title": "Definition 2.1: Shortcut Edge", "content": "An edge (x, y) in a nearest neighbor graph of noisy samples from M is a shortcut edge if\n$d_M(proj_M x, proj_M y) > (\\pi + 1) \\tau_0(M)$\nwhere proj(\u00b7) is the orthogonal projection onto M and $\\tau_0(M)$ is the minimum radius of curvature of M."}, {"title": "2.3 OLLIVIER-RICCI CURVATURE", "content": "Ollivier-Ricci curvature (ORC) was proposed as a measure of curvature for discrete spaces by leveraging the connection between optimal transport and Ricci curvature of Riemannian manifolds (Ollivier, 2009). While there have been many subtle variations of ORC, we will describe a modification to the most common one. Given an edge (x, y) in a weighted graph G with vertices V and edges E, define the neighborhoods of x and y as $N(x) := \\{v \\in V|(x, v) \\in E, v \\neq x\\}$ and"}, {"title": "Preprint.", "content": "$N(y) := \\{v \\in V | (y, v) \\in E, v \\neq y\\}$. Further define $\\mu_x$ and $\\mu_y$ as uniform probability measures over $N(x) \\setminus \\{y\\}$ and $N(y) \\setminus \\{x\\}$, respectively. The ORC of the edge (x, y), denoted $\\kappa(x, y)$, is defined as\n$\\kappa(x, y) := 1 - \\frac{W(\\mu_x, \\mu_y)}{d_G(x, y)}$\nwhere $W(\\mu_x, \\mu_y)$ is the 1-Wasserstein distance between the measures $\\mu_x$ and $\\mu_y$ (regarded as measures on V), with respect to the (weighted) shortest-path metric $d_G(\\cdot, \\cdot)$. The 1-Wasserstein distance is computed by solving the following optimal transport problem,\n$W(\\mu_x, \\mu_y) = \\inf_{\\gamma \\in \\Pi(\\mu_x, \\mu_y)} \\sum_{(a,b) \\in V \\times V} d(a, b) \\gamma(a, b)$\nwhere $\\Pi(\\mu_x, \\mu_y)$ is the set of all measures on $V \\times V$ with marginals $\\mu_x$ and $\\mu_y$. Intuitively, ORC quantifies the local structure of G: negative curvature implies that the edge is a \u201cbottleneck\u201d, while positive curvature indicates the edge is present in a highly connected community.\nIn our setting, the vertices of the graph G are points in $R^D$, and edges connect points (x, y) such that $||x-y||_2 \\leq \\epsilon$, where $\\epsilon$ is a user-chosen connectivity threshold. A common choice is to weight the edges by the Euclidean distance $||x - y||_2$, but we make a slight modification to this formulation. In computing the ORC in the present paper we use $\\epsilon$-weighted edges; namely, all edges are snapped to a weight of $\\epsilon$, the parameter used to build the underlying nearest neighbor graph. This is a key aspect of our method, as it forces the ORC to reflect only the local connectivity, and makes it invariant to scale. This modification also restricts the ORC values to lie between -2 and +1, whereas with weighted graphs, the values are unbounded below. For the rest of the paper, any reference to Ollivier-Ricci curvature is a reference to the formulation we have just described."}, {"title": "Proposition 2.", "content": "The ORC of any edge (x, y) in an $\\epsilon$-weighted graph satisfies $-2 \\leq \\kappa(x, y) \\leq 1$."}, {"title": "3 METHOD AND THEORETICAL RESULTS", "content": "In this section we will describe ORC-MANL, a novel algorithm for pruning nearest neighbor graphs based on ORC and metric distortion. We will also describe the theoretical results that justify the algorithm construction, the proofs of which are in Appendix A.3.\nORC-MANL (Algorithm 1) takes as input noisy samples X from some underlying manifold, an accompanying nearest neighbor graph $G = (V, E)$ of the data, and tolerances $\\delta, \\lambda \\in [0, 1]$. The method constructs a candidate set $C$ of edges that have curvature more negative than a threshold just larger than -1 (with $\\delta$ determining the gap between the threshold and -1, the expression for which comes from Lemma A.1). Importantly, not every edge in the candidate set is necessarily a shortcut; good edges can have extremely negative curvature as well. To combat this, the method proceeds by constructing a thresholded graph $G' = (V, E')$ where $E' = E \\setminus C$. The weighted graph distance $d_{G'}(x, y)$ is checked for every edge $(x, y) \\in C$. If this distance exceeds the theoretically determined threshold we can be confident that $(x, y)$ undesirably bridges distant neighborhoods of M (a visualization for which is shown in Figure 11). The edge is therefore removed from G. If the threshold is not exceeded, the edge is not removed.\nWe provide three theoretical results that justify ORC-MANL. First, we show that when the data generating the nearest neighbor graph consists of noisy samples from an underlying manifold M, the ORC of shortcutting edges tends to be very negative. Second, we show that as one samples more points, every vertex has an increasing number of non-shortcutting edges with very positive ORC. Last, we derive a bound on the graph distance (in the ORC thresholded graph G') between any vertices connected by a shortcut edge in G.\nWe consider the setting where M is a compact m-dimensional smooth submanifold of $R^D$ without boundary. Let Tub$\\_{\\tau}(M)$ be the tubular neighborhood of M, and assume $X \\subset$ Tub$\\_{\\tau}(M)$ consists of n independent draws from the probability density function $\\rho :$ Tub$\\_{\\tau}(M) \\rightarrow R^+$,\n$\\rho(z) = \\begin{cases}  \\frac{1}{Z} e^{-\\frac{||z - proj_M z||_2^2}{2 \\sigma^2}}, & ||z - proj_M z||_2 \\leq \\tau \\\\ 0, & o.w.,  \\end{cases}$"}, {"title": "Preprint.", "content": "where Z is a normalizing constant such that $\\int_{Tub_{\\tau}(M)} \\rho(z) dV$ integrates to 1. We are given a constant $\\lambda < 1$, which controls the threshold we use when checking the weighted-graph distances of candidate edges. For the rest of the paper, suppose that\n1. (Support criteria): $2\\tau < \\epsilon$, $3\\tau < s_0(M)$, $3\\tau < \\tau_0(M)$\n2. ($\\epsilon$-radius criterion): $\\epsilon < \\min \\{ \\sqrt{(s_0(M) - \\tau)^2 - \\tau^2}, (\\tau_0(M) - \\tau) \\sqrt{24 \\lambda}, \\tau_0(M) \\}$,\nwhere $s_0(M)$ is the minimum branch separation of M and $\\tau_0(M)$ is the minimum radius of curvature of M. Assumption 1 ensures that the orthogonal projection $proj_M: Tub_{\\tau}M \\rightarrow M$ is unique, while 2 allows us to use Bernstein et al. (2001) to make statements about geodesic distances."}, {"title": "Algorithm 1 ORC-MANL", "content": "Require: $G = (V, E)$ a nearest neighbor graph, $\\lambda$, $\\delta$\n1: $C \\leftarrow \\{\\}$\n2: for (x, y) in E do\n3: $\\kappa(x, y) \\leftarrow OllivierRicci(G, (x, y))$\n4: if $\\kappa(x, y) \\leq -1 + \\frac{4 (1 - \\delta)}{\\lambda}$ then\n5: $C \\leftarrow C \\cup \\{(x, y)\\}$\n6: $E' \\leftarrow E \\setminus C$\n7: $G' \\leftarrow (V, E')$\n8: for (x, y) in C do\n9: $d_{G'}(x, y) \\leftarrow ShortestPath(G', x, y)$\n10: if $d_{G'}(x, y) > \\frac{\\pi (\\pi + 1) (1 - \\lambda) \\epsilon}{2 \\sqrt{24 \\lambda}}$ then\n11: $E \\leftarrow E \\setminus \\{(x, y)\\}$\n12: return (V, E)\nOur first theorem establishes that the ORC for shortcut edges converges to negative values in the limit of vanishing noise $\\sigma$. We note that so long as $\\tau > 0$ and $\\sigma > 0$, shortcut edges occur with nonzero probability. Observe that, in combination with Proposition 2, this result indicates that the ORC of shortcut edges tends to concentrate in the bottom third of the possible range of values.\nTheorem 3.1 (Ollivier-Ricci Curvature of Shortcut Edges). Suppose that Xi is a point cloud sampled from $\\rho$ with parameters $\\sigma_i$ and $\\tau_i$ and Gi is its nearest-neighbor graph. Also suppose that M satisfies the conditions above, and $\\sigma_i \\rightarrow 0^+$ and $\\tau_i \\rightarrow 0^+$ as $i \\rightarrow \\infty$. Then as $i \\rightarrow \\infty$, we have $\\kappa(x, y) \\leq -1$ for all shortcut edges (x, y) in Gi with probability approaching 1."}, {"title": "Preprint.", "content": "To be confident that ORC can be effective at identifying potentially shortcutting edges, we would also like to be sure that there exists a sufficient number of good (non-shortcut) edges with more positive ORC. This motivates Theorem 3.2.\nTheorem 3.2 (Ollivier-Ricci Curvature of Non-Shortcut Edges). Let k be a positive integer. With high probability as the number of points $n \\rightarrow \\infty$, every point has at least k neighbors that it is connected to by non-shortcut edges with ORC $> -1$.\nProofs of Theorem 3.1 and Theorem 3.2 can be found in Appendix A.3.1. These results allow us to create a candidate set of shortcut edges by looking at the ORC of edges in G. We then expect that the graph $G' = (V, E')$, where E' has all extremely negative-curvature edges removed, has no shortcut edges and a large number of good edges. It should therefore have a metric structure that is more aligned with that of the underlying manifold M. With that said, we also expect the candidate set to contain some non-shortcut edges, which we do not want to remove from G. In the last part of our algorithm, we filter our candidate set to identify edges that are most likely to be shortcuts. This motivates our third theorem, which establishes that for vertices that were previously connected in G by a shortcut edge, their weighted graph distance in G' is long relative to $\\epsilon$.\nTheorem 3.3 (Filtered Graph Distance). Suppose that Xi is a point cloud sampled from $\\rho$ with parameters $\\sigma_i$ and $\\tau_i$ and $Gi = (Vi, Ei)$ is its nearest neighbor graph. Also suppose that M satisfies the conditions above and $\\sigma_i \\rightarrow 0^+$ and $\\tau_i \\rightarrow 0^+$ as $i \\rightarrow \\infty$. Define the subgraph $G'_i = (Vi, E'_i)$ where\n$E'_i = \\{ (x_i, y_i) \\in E_i | \\kappa(x_i, y_i) > -1\\}$.\nThen as $i \\rightarrow \\infty$ we have\n$d_{G'_i}(x, y) > \\frac{\\beta \\pi (\\pi + 1) (1 - \\lambda) \\epsilon}{2 \\sqrt{24 \\lambda}}$\nfor all shortcut edges in Gi with probability approaching 1, where $\\beta \\in [0, 1]$ (eq. (34)) is a random variable whose distribution is dependent on M and $\\tau_i$."}, {"title": "Preprint.", "content": "Remark 1. The random variable $\\beta$ is inversely related to the number and lengths of edges that shortcut through Tub-M but do not satisfy the definition of a shortcut edge in M. We expect that $\\beta$\nconcentrates close to 1, and experimentally we find that $\\beta = 1$ works well.\nTheorem 3.3 arises from two results: (1) one can bound geodesic distances through Tub$\\_{\\tau}M$ with geodesic distances of paths through M with similar endpoints, and (2) under reasonable conditions one can relate the graph distances through a nearest neighbor graph built from data sampled from Tub$\\_{\\tau}M$ to the geodesic distance through Tub$\\_{\\tau}M$."}, {"title": "4 EXPERIMENTS", "content": "Building nearest neighbor graphs as a data pre-processing step is ubiquitous in the realm of geometric data analysis. Therefore our method is well suited for evaluation on a broad range of algorithms that operate on nearest neighbor graphs.\nOur experiments are divided into two sections. Firstly, we evaluate ORC-MANL on a variety of synthetic manifolds for a broad array of benchmark geometric data analysis and machine learning tasks. We also compare pruning accuracy to several benchmark methods presented previously in the literature. We then show that ORC-MANL pruning reveals clusters aligned with ground truth annotation for single-cell RNA sequencing (scRNAseq) data of peripheral blood mononuclear cells (PBMCs). We also find that ORC-MANL pruning improves downstream manifold learning embeddings of scRNAseq data of anterolateral motor cortex (ALM) brain cells in mice."}, {"title": "4.1 RESULTS: SYNTHETIC DATA", "content": "For our synthetic manifolds, we have curated a list of 1 and 2-dimensional manifolds embedded in $R^2$ and $R^3$, respectively, that exhibit varying intrinsic and extrinsic curvature. The 1-dimensional manifolds include concentric circles, a mixture of Gaussians, twin moons, an S curve, the 1-dimensional swiss roll, a Cassini oval (Cassini, 1693) and concentric parabolas. The 2-dimensional manifolds include chained torii, concentric hyperboloids, an adjacent hyperboloid and paraboloid, adjacent paraboloids and the 2-dimensional swiss roll. Note that we include additional experiments indicating the improved performance of spectral clustering with ORC-MANL pruning in Appendix A.4.2. We also provide more experimental details regarding sampling and nearest neighbor graph construction in Appendix A.2."}, {"title": "4.1.1 PRUNING", "content": "Our first experiment quantifies the ability of ORC-MANL to prune (and therefore classify) edges of nearest neighbor graphs. We report classification accuracy for 1 and 2-dimensional manifolds in\nTable 1. In the tables we also include performance for several baseline graph pruning approaches. Algorithm descriptions for each baseline are provided in Appendix A.2.1. Each entry in the table has two rows: the top row indicates the mean and standard deviation of classification accuracy for non-shortcut edges, while the bottom row indicates the mean and standard deviation of classification accuracy for shortcut edges across 10 different trials. Accompanying visualizations for ORC-MANL pruning results are shown in Figure 3."}, {"title": "4.1.2 MANIFOLD LEARNING", "content": "We turn to evaluate ORC-MANL on manifold learning, a family of algorithms concerned with finding low dimensional coordinates on the data submanifold. Almost all manifold learning algorithms begin by constructing a nearest neighbor graph, making ORC-MANL a perfect pre-processing step.\nWe compare embeddings of nearest neighbor graphs with and without ORC-MANL preprocessing for data sampled from a noisy 2-dimensional swiss roll embedded in $R^3$ with and without a hole. We test on Isomap (Tenenbaum et al., 2000), Locally Linear Embeddings (LLE), (Belkin & Niyogi, 2003), UMAP (McInnes et al., 2018) and t-SNE (Van der Maaten & Hinton, 2008) shown in Figure 4. We leave out Laplacian Eigenmaps (Belkin & Niyogi, 2003) as we find that it fails to preserve both underlying dimensions of the noisy swiss roll, independent of the presence of shortcut edges. To the best of our understanding, this likely arises from the Repeated Eigendirection Problem (REP) associated with eigenfunction based methods (Dsilva et al., 2018). For UMAP and t-SNE we use graph distances (induced by the pruned and unpruned graphs respectively) as algorithm inputs.\nWe observe that across both manifolds and all methods, ORC-MANL pruning typically improves the embedding. For Isomap, this improvement is the most pronounced; without pruning, Isomap fails to unroll either swiss roll. We attribute this to distorted geodesic distance estimates (which is the first step of the Isomap algorithm) arising from shortcut edges. We also find significant improvement in the quality of the embeddings for LLE. For both manifolds, LLE without ORC-MANL struggles to preserve the primary dimension of the underlying manifold. We find that UMAP ben-"}, {"title": "4.1.3 PERSISTENT HOMOLOGY", "content": "Next, we evaluate our method on persistent homology. Persistent homology begins by constructing a nested sequence $K_{r_0} \\subseteq K_{r_1} \\subseteq ... K_{r_n} ...$ of simplicial complexes, also referred to as a filtered complex. A common example is the Vietoris-Rips (VR) filtered complex. For data points $X$ with metric $d$, the VR simplicial complex $K_r$ has a simplex for every subset of points with pairwise distances $\\leq r$ (Ghrist, 2014). For our experiments, we construct the VR filtered complex where the metric is the weighted graph distance. One can summarize the result of persistent homology with a persistence diagram, simply a multiset of points in $R^2$. Each point $(r_{birth}, r_{death})$ in the persistence diagram records the filtration parameter at which a homology class is born and dies respectively."}, {"title": "4.1.4 GEOMETRIC DESCRIPTORS", "content": "Finally, we test the ability of ORC-MANL to preserve geometric descrip-tors of the underlying manifolds. We estimate intrinsic dimension and scalar curvature for the (1) swiss roll and (2) adjacent spheres datasets. We compare our estimates to the ground-truth values. We use the maximum-likelihood estima-tion approach to estimating intrinsic di-mension from Levina & Bickel (2004), and we use the algorithm from Hickok &\nBlumberg (2023) to estimate scalar curvature. The details for both algorithms are detailed in Appendix A.2.3 and Appendix A.2.4, respectively.\nWe report intrinsic dimension and scalar curvature estimation results on the noisy swiss roll and noisy adjacent spheres dataset in Figure 6. We find that ORC-MANL pruning provides tangible improvement over non-pruned estimates, especially for scalar curvature. For intrinsic dimension estimation we see improved point-wise accuracy in regions with a high number of shortcut edges (for example, the area between the adjacent spheres); in regions with a smaller presence of shortcut edges, we find that the unpruned graphs produce accurate results."}, {"title": "4.2 RESULTS: REAL DATA", "content": "We evaluate the efficacy of ORC-MANL on real-world data by using it to analyze cell-type anno-tated scRNAseq data of (1) anterolateral motor cortex (ALM) brain cells in mice available from the Allen Institute (Abdelaal et al., 2019), and (2) peripheral blood mononuclear cells (PBMC) available"}, {"title": "Preprint.", "content": "from 10XGenomics. In accordance with previous work, we extract the 2000 most variable genes, followed by PCA (Pearson, 1901) to obtain a 50-dimensional representation of the data.\nFor the brain cells we use Isomap (Tenenbaum et al., 2000) to embed ORC-MANL pruned and unpruned nearest neighbor graphs. Figure 7 shows the embeddings with base truth annotations. We find that pruning with ORC-MANL qualitatively improves the Isomap embedding of the data, as the distinction between neuronal cells (labeled \"Inhibitory\" and \"Excitatory\") and non-neuronal cells becomes significantly more pronounced after pruning. For completeness, we include UMAP and t-SNE embeddings of this dataset in the appendix in Figure 20; we observe poor performance as measured by neuronal community preservation both with and without pruning, suggesting issues with the embedding algorithms themselves as opposed to pruning.\nNow we turn to the PBMC dataset, where we show UMAP and spectral embeddings in Figure 8. We find that ORC-MANL pruning leads to connected components that largely align with base truth annotations; this is reflected in the spectral embedding, which maps each component to a unique location in the embedding space. Furthermore, edges that formerly bridged seemingly distinct clusters in the embeddings were removed, resulting in clearer cluster structure in the pruned graphs."}, {"title": "5 CONCLUSION", "content": "In this work we present ORC-MANL, a theoretically justified and empirically validated approach for nearest neighbor graph pruning. We rigorously demonstrate that shortcut edges necessarily exhibit particularly negative ORC for graphs sampled from manifolds with a bounded level of isotropic noise. Our method also validates the removal of any edge by using a theoretically derived bound on graph distances implied by the geometry of shortcut edges. We qualitatively and quantitatively demonstrate that ORC-MANL is beneficial to a variety of downstream geometric data analysis tasks on synthetic and real data. We also compare our method to baselines from the literature in its ability to correctly prune shortcut edges and avoid non-shortcut edges."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 REPRODUCIBILITY STATEMENT", "content": "Theoretical Results: We detail global assumptions for all theoretical results in Section 3. We also make a concerted effort to clarify all proof-specific assumptions in the Theorem, Lemma and Proposition statements.\nExperimental Results: To ensure the reproducibility of our experimental results we include the source code for ORC-MANL and the scripts for reproducing all experiments as supplemental ma-terial in our submission. Included in the experiment scripts are the parameters used; for clarity we describe key parameter choices throughout the body of the paper, and provide extra details in Appendix A.2."}, {"title": "A.2 EXPERIMENTAL DETAILS", "content": "For all experiments in Section 4.1, datapoints are sampled according to $\\rho$ defined in eq. (1) for each synthetic manifold M. To achieve this, we sample uniformly from the underlying m-dimensional manifold M (unless otherwise specified) driven by the m-dimensional volume form defined by the Euclidean metric. We then use rejection sampling to obtain isotropic Gaussian noise $\\xi \\sim N(0, I)$ such that $|\\xi|_2 \\leq \\tau$. For all experiments detailed in Section 4.1, we use k-NN connectivity; while our theoretical analysis assumes $\\epsilon$-radius connectivity, $\\epsilon$-radius connectivity is more challenging to tune in practice. Since k-NN graphs are more commonplace in the applied literature, we opt to present results on k-NN graphs (unless otherwise specified)."}, {"title": "A.2.1 PRUNING", "content": "Here we detail each of the pruning baselines that appear in Table 1.\n1. ORC ONLY: This baseline returns a graph G' where edges with curvature less than $-1 + \\frac{4 (1 - \\delta)"}, {"BISECTION": "This method was proposed by Xia et al. (2008) to address topological instabil-ity associated with the Isomap algorithm (Tenenbaum et al., 2000). For each edge (x, y), it performs a local search around the midpoint $l = \\frac{(x - y)}{2}$. Namely, it searches in a bounding box $\\prod_1 [l_i - \\epsilon', l_i + \\epsilon'"}]}