{"title": "SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor Diagnosis", "authors": ["Runci Bai"], "abstract": "Brain tumors can result in neurological dysfunction, alterations in cognitive and psychological states, increased\nintracranial pressure, and the occurrence of seizures, thereby presenting a substantial risk to human life and health. The You Only\nLook Once(YOLO) series models have demonstrated superior accuracy in object detection for medical imaging. In this paper, we\ndevelop a novel SCC-YOLO architecture by integrating the SCConv attention mechanism into YOLOv9. The SCConv module\nreconstructs an efficient convolutional module by reducing spatial and channel redundancy among features, thereby enhancing the\nlearning of image features. We investigate the impact of intergrating different attention mechanisms with the YOLOv9 model on\nbrain tumor image detection using both the Br35H dataset and our self-made dataset(Brain_Tumor_Dataset). Experimental results\nshow that on the Br35H dataset, SCC-YOLO achieved a 0.3% improvement in mAp50 compared to YOLOv9, while on our self-\nmade dataset, SCC-YOLO exhibited a 0.5% improvement over YOLOv9. SCC-YOLO has reached state-of-the-art performance in\nbrain tumor detection. Source code is available at : https://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/master", "sections": [{"title": "I. INTRODUCTION", "content": "Magnetic Resonance Imaging (MRI) is the most effective imaging technique for visualizing the brain and identifying\ntumors[1]. However, due to the varied morphology and relatively indistinct edge characteristics of brain tumor images[2], the\nprocess of diagnosing brain tumor conditions through magnetic resonance imaging (MRI) is both complex and inefficient for\nclinicians, resulting in an elevated risk of misdiagnosis and missed detection. Researchers have applied machine learning\ntechniques to the segmentation and classification of brain tumor images[3-10]. In the automatic detection and auxiliary\ndiagnosis of brain tumors, relevant researchers have applied techniques such as unsupervised learning[11], convolutional neural\nnetworks (CNN)[12], deep stacked autoencoders (DSAE)[15], and You Only Look Once(YOLO)[13], [14-18]. Maibam\nMangalleibi Chanu et al. applied the YOLOv3[19] model to the computer-aided detection and classification of brain tumors,\nrepresenting an important study of the YOLO series models in brain tumor detection[16]. Kang et al. innovatively proposed the\nRCS-YOLO[17] and BGF-YOLO[18] models based on YOLOv8[20], achieving good accuracy and speed on the Br35H\ndataset[25], demonstrating the high feasibility of the YOLO series in brain tumor image detection.\nYOLOV9[21] introduces the concept of Programmable Gradient Information (PGI), which updates network weights by\nobtaining reliable gradient information. This approach addresses the issue of information loss encountered by the network\nduring feature extraction and transformation, achieving ideal accuracy and speed on the MS COCO dataset. To further enhance\nthe performance of the YOLOV9 model, researchers have incorporated various attention mechanisms into its original network\nstructure. Yukang Huo et al. proposed the FMSD Module (Fine-grained Multi-scale Dynamic Selection Module) module,\nwhich applies a more effective dynamic feature selection and fusion method on fine-grained multi-scale feature maps, and the\nAGMF Module(Adaptive Gated Multi-branch Focus Fusion Module), which utilizes multiple parallel branches to perform\ncomplementary fusion of various features captured by each branch. They integrated these two modules into YOLOv9 to\ndevelop a novel object detector with higher detection accuracy[22].Weichao Pan et al. proposed EAConv (Efficient Attention\nConvolution) and EADown (Efficient Attention Downsampling), and designed a lightweight model called EFA-YOLO\n(Efficient Feature Attention YOLO) based on these two modules. In fire detection applications, its detection accuracy and\ninference speed have been significantly improved[23]. Yifan Feng et al. proposed Hyper-Yolo, a model that transposes image\nfeatures from the visual modality to a semantic space and designs a hypergraph to enable interactions across positions and\nlevels, enhancing the integration of cross-level features and the utilization of high-order feature interrelationships. This model\nperforms excellently on the COCO dataset and is proven to be a state-of-the-art architecture[24].\nIn this paper, we propose a novel model named SCC-YOLO, which improves the detection performance of YOLOv9\nthrough the integration of the SCConv attention mechanism. The contributions of this research are outlined as follows: (1) We\ncreated the Brain_Tumor_Dataset, which includes 9,900 RGB images with a resolution of 139x132 pixels, consisting of 7,920\nimages in the training set and 1,980 images in the test set. The dataset contains three types of labels, representing three different\ntypes of brain tumors. (2) We incorporated SCConv into the head of the original YOLOv9 structure to enhance the feature\nlearning capability for brain tumor images. (3) We incorporated the SE attention mechanism into the head of the original\nYOLOv9 structure for a comparative study on the impact of different attention mechanisms on brain tumor detection. (4) \u03a4\u03bf\nthe best of our knowledge, this is the first time that the enhanced YOLOv9 has been applied to brain tumor detection."}, {"title": "II. METHODS", "content": "We used the publicly available dataset Br35H and our custom dataset Brain_Tumor_Dataset for model training and testing.\nThe Br35H dataset was created by Ahmed Hamada, which consists of 803 MRI images with annotated brain tumors,\ndivided into 501 train images, 202 validation images, and 101 test images.The structure of this dataset is designed to provide a\nrich sample for the detection and classification of brain tumors, supporting relevant research and analysis.\nDue to the small size of the Br35H dataset, we created the Brain_Tumor_Dataset using the LabelImg tool. This dataset\ncontains 9,900 images with a resolution of 139*132 RGB images, featuring clear bounding box annotations and complete\nimages, along with corresponding label txt files. The dataset includes three labels, named Label0, Labell, and Label2, which\nrepresent three different categories of brain tumors. Each image is marked with multiple labels. The train set consists of 7,920\nimages and 7,920 label files, while the test set includes 1,980 images and 1,980 label files, as shown in Table 1.\nAs shown in Figure.2, we propose SCC-YOLO, which introduces the SCConv[26] module into the original structure of\nYOLOv9, with this module placed at the 37th layer of the head of YOLOv9.\nThe architecture is divided into two main components: the backbone and the head, each consisting of a series of carefully\narranged layers that contribute to its overall performance.\nThe backbone of YOLOv9 primarily focuses on feature extraction, employing a sequence of convolutional layers,\ndownsampling operations, and advanced block structures. The architecture begins with a silence layer followed by a series of\nconvolutional layers that progressively reduce the spatial dimensions of the input image."}, {"title": "C. Integration of SCConv", "content": "Subsequent to the 37th layer of the YOLOv9 network head, we integrated the SCConv module\u2014a plug-and-play operation\nthat sequentially combines the Spatial Reconstruction Unit (SRU) and the Channel Reconstruction Unit (CRU), as illustrated\nin Figure 2.\nFor the intermediate input features within the bottleneck residual block, we initially derive spatially refined features using\nthe SRU operation, followed by the application of the CRU operation to obtain channel-refined features. The SCConv module\ncapitalizes on both spatial and channel redundancy inherent in the features and is seamlessly incorporated into the YOLOv9\narchitecture, effectively diminishing redundancy among the intermediate feature maps and improving feature representation.\nThe architecture of the SRU is illustrated in Figure 3. The SRU effectively separates redundant features by utilizing\nweighted metrics, subsequently reconstructing them to mitigate redundancy in the spatial dimension and enhance feature\nrepresentation.\nThe architecture of the CRU is illustrated in Figure 4. The CRU implements a strategy that involves splitting, transforming,\nand fusing features to mitigate redundancy in the channel dimension, thereby decreasing both computational costs and storage\nrequirements."}, {"title": "D. Comparison with SE Attention Mechanism", "content": "The commonly used Squeeze-and-Excitation(SE) attention mechanism[27] in the academic community aims to enhance the\nmodel's performance by significantly improving the expressive power of channel features. It adaptively adjusts the weights of\nfeature channels through two steps: \"squeeze\" and \"excitation,\" thereby emphasizing important features while suppressing\nless important ones. The implementation process involves global average pooling to obtain channel descriptors, followed by\nthe generation of channel weights through fully connected layers, and finally applying these weights to the original feature\nmap to adjust the importance of each channel. Many scholars have combined the SE attention mechanism with YOLO series\nmodels in related research[28-35].\nHowever, the SE mechanism primarily enhances feature maps by weighting the channels, thereby neglecting the\ninformation contained within the spatial dimensions. This omission can result in the loss of critical spatial context when\nprocessing features characterized by complex spatial relationships. Furthermore, the inclusion of the SE module introduces an\nadditional computational step following each convolutional layer, which encompasses global average pooling, fully connected\nlayers, and activation functions, consequently elevating the computational overhead. While the SE mechanism demonstrates\nstrong performance across various visual tasks, its effectiveness may be diminished compared to other more sophisticated\nattention mechanisms, particularly in tasks that necessitate intricate feature interactions, such as object detection in medical\nimaging.\nIn this study, we performed comparative experiments by integrating the SE attention mechanism after the 37th layer of the\noriginal YOLOv9 network, while ensuring that the experimental settings remained consistent with those employed in SCC-\nYOLO. We designated this new model as SE-YOLOv9.\nThe experimental results reveal that on both the Br35H and Brain Tumor Dataset, the performance metrics of SE-YOLOv9\nare consistently inferior to those of SCC-YOLO. This finding suggests that in medical imaging tasks, such as brain tumor\nauxiliary diagnosis, SCC-YOLO effectively integrates both spatial and channel information, thereby exhibiting superior\nperformance compared to models that rely exclusively on the SE attention mechanism."}, {"title": "III. EXPERIMENTAL DETAILS", "content": "CC-YOLO was trained and tested on the NVIDIA GeForce RTX 3090. As shown in Table2, we implemented the proposed\nmethods based on YOLOv9c. The training hyperparameters for SCC-YOLO and other comparison methods are the same as\nthose for YOLOv9c. On the Br35H dataset, the training batch size is set to 4, and the number of epochs during the training\nphase is 120. The optimizer uses stochastic gradient descent with an initial and final learning rate of 0.01 and a momentum of\n0.937. On the Brain_Tumor_Dataset, the training batch size is also set to 4, while the number of epochs during the training\nphase is increased to 400, given the substantial volume of data in the dataset. The optimizer again uses stochastic gradient\ndescent with an initial and final learning rate of 0.01 and a momentum of 0.937."}, {"title": "B. Evaluation metrics", "content": "In this paper, we select precision, recall, mAP50 and mAP50:95, parameters, layers and gradients as evaluation metrics for\nmodel performance in order to study the advantages and disadvantages of the model.\nUsing IoU = 0.5 as the standard, precision and recall are obtained from the following formulas:\nPrecision = $\\frac{TP}{TP+FP}$ (1)\nRecall = $\\frac{TP}{TP+PN}$ (2)\nIn this context, TP refers to the number of positive samples that have been accurately identified as positive samples; while\nFP refers to the number of negative samples that have been incorrectly classified as positive samples; and finally, PN refers to\nthe number of positive samples that have been incorrectly classified as negative samples.\nmAP50 represents the average precision of the model for positive samples detected when IoU \u2265 0.5, specifically the\naverage of the area under the precision-recall (PR) curve formed by precision and recall. In contrast, mAP50:95 indicates the\naverage precision calculated across multiple IoU thresholds, specifically averaging the values from 0.5 to 0.95 in increments of\n0.05, resulting in a total of 10 thresholds. mAP50:95 provides a more stringent performance evaluation standard, allowing for a\nmore comprehensive reflection of the model's performance across varying levels of detection difficulty, making it suitable for\napplications requiring high accuracy.\nParameters are the internal variables of a neural network that are learned from the training data. The total number of\nparameters in a model can be calculated by summing the weights and biases across all layers. A higher number of parameters\ntypically indicates a more complex model, which can capture more intricate patterns in the data but also runs the risk of\noverfitting."}, {"title": "IV. EXPERIENCE RESULTS AND DISCUSSION ANALYSIS", "content": "YOLOv9 achieved a mAP50 score of 0.954, a mAP50:95 score of 0.751, a Precision of 0.926, and a Recall of 0.939. SE-\nYOLOv9 demonstrated slightly lower performance, with a mAP50 score of 0.931 and a mAP50:95 score of 0.697. Its Precision\nand Recall values were 0.906 and 0.914, respectively, suggesting a reduction in detection capability compared to YOLOv9.\nSCC-YOLO (ours) outperformed the other models, achieving a mAP50 score of 0.957 and a mAP50:95 score of 0.735. The\nPrecision was 0.922, and the Recall was 0.943, indicating a balanced performance with a slight edge in mAP50.\nOverall, the experimental results suggest that the SCC-YOLO model exhibits the best performance on the Br35H dataset,\nclosely followed by YOLOv9, while SE-YOLOv9 shows comparatively lower efficacy across all metrics.\nYOLOv9 achieved a mAP50 score of 0.855, which serves as a benchmark for comparison. Its mAP50:95 score was 0.631,\nwith a Precision of 0.938 and a Recall of 0.783. This model demonstrates strong performance, particularly in Precision. SE-\nYOLOv9 displayed a mAP50 score of 0.828, indicating a decrease of 0.027 compared to YOLOv9. The mAP50:95 score for\nSE-YOLOv9 was 0.585, along with a Precision of 0.906 and a Recall of 0.748. This reduction in mAP50 and other metrics\nsuggests a diminished detection capability relative to YOLOv9.SCC-YOLO (ours) outperformed SE-YOLOv9 with a mAP50\nscore of 0.860, which indicates an improvement of 0.005 over YOLOv9 and a significant advantage of 0.032 over SE-YOLOv9.\nThe mAP50:95 score was 0.629, while Precision and Recall were 0.929 and 0.781, respectively. This performance highlights\nthe effectiveness of the SCC-YOLO model in achieving higher detection accuracy.\nIn summary, the experimental results indicate that SCC-YOLO achieves the highest mAP50 score of 0.860, followed by\nYOLOv9 with 0.855, and SE-YOLOv9 with 0.828. The observed differences in mAP50 reflect the relative strengths and\nweaknesses of each model in detecting brain tumors within the dataset, with SCC-YOLO providing a notable improvement\nover SE-YOLOv9."}, {"title": "V. CONCLUSION", "content": "In conclusion, this study introduces a novel SCC-YOLO architecture that effectively integrates the SCConv attention\nmechanism into the YOLOv9 framework, thereby enhancing brain tumor detection capabilities. The incorporation of the\nSCConv module significantly alleviates spatial and channel redundancy, promoting more efficient feature learning from\nmedical images. Our experiments, conducted on both the Br35H dataset and our custom Brain_Tumor_Dataset, demonstrate\nthat SCC-YOLO consistently outperforms the original YOLOv9 model, achieving a mean Average Precision (mAP) of 0.957\non the Br35H dataset and 0.86 on the Brain_Tumor_Dataset. Additionally, SCC-YOLO achieves a 0.3% improvement in mean\nAverage Precision at an Intersection over Union (IoU) of 0.5 on the Br35H dataset and a 0.5% improvement on the custom\ndataset. These findings highlight the effectiveness of the SCC-YOLO architecture in tackling the challenges associated with\nbrain tumor detection, contributing to advancements in medical imaging and potentially facilitating more accurate diagnoses.\nNotably, SCC-YOLO has achieved state-of-the-art performance in the realm of brain tumor detection."}]}