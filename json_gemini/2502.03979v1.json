{"title": "Towards Unified Music Emotion Recognition across Dimensional and Categorical Models", "authors": ["Jaeyong Kang", "Dorien Herremans"], "abstract": "One of the most significant challenges in Music Emotion Recognition (MER) comes from the fact that emotion labels can be heterogeneous across datasets with regard to the emotion representation, including categorical (e.g., happy, sad) versus dimensional labels (e.g., valence-arousal). In this paper, we present a unified multitask learning framework that combines these two types of labels and is thus able to be trained on multiple datasets. This framework uses an effective input representation that combines musical features (i.e., key and chords) and MERT embeddings. Moreover, knowledge distillation is employed to transfer the knowledge of teacher models trained on individual datasets to a student model, enhancing its ability to generalize across multiple tasks. To validate our proposed framework, we conducted extensive experiments on a variety of datasets, including MTG-Jamendo, DEAM, PMEmo, and EmoMusic. According to our experimental results, the inclusion of musical features, multitask learning, and knowledge distillation significantly enhances performance. In particular, our model outperforms the state-of-the-art models, including the best-performing model from the MediaEval 2021 competition on the MTG-Jamendo dataset. Our work makes a significant contribution to MER by allowing the combination of categorical and dimensional emotion labels in one unified framework, thus enabling training across datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "Music plays an essential role in influencing human emo- tions [34]. In the past decades, numerous Music Emotion Recognition (MER) models been developed. MER has found applications in various domains, e.g., music recommendation systems [49], generative systems [30], and music therapy [1]. However, the field faces significant challenges due to the diversity of available datasets and their inconsistent labeling schemes [26]. Most MER datasets can be categorized into two types: cate- gorical labels, which represent discrete emotions such as happy or sad, and dimensional labels, such as Russell's circumplex model of affect [42], which describes emotions on continuous scales of valence and arousal. While these datasets provide valuable resources, their heterogeneous nature complicates efforts to combine them effectively. Previous approaches [5], [9], [10], [25], [32], [38], [41], [46] have often focused on a single type of label, limiting their ability as they can only train on one (often small) dataset. To address these challenges, we introduce a unified multi- task learning framework that incorporates both categorical and dimensional emotion labels within a shared architecture. This approach enables us to train on multiple datasets. Multitask learning has been shown to be successful on many other tasks such as speech emotion recognition [6], symbolic music emotion recognition [39], genre classification and mood detec- tion [17], to financial portfolio prediction [35]. These studies demonstrate the benefits of shared representations across tasks. To properly represent the input music, our proposed frame- work combines three types of features: 1) Music undERstand- ing model with large-scale self-supervised Training (MERT) embeddings [27], 2) harmonic representations of chord pro- gressions, and 3) musical key. The MERT embeddings are able to provide a rich representation of timbre, rhythm, and high-level musical semantics. Chord progressions and musical key features are able to encode harmonic and tonal structures, which are equally vital for understanding emotion in music. An important innovation of our framework is the use of knowledge distillation (KD) [20] to unify learning across datasets with disparate label types. Pre-trained teacher models, optimized for either categorical or dimensional labels, guide the multitask student model by providing soft target logits, which are probability distributions over the possible labels. These logits help the student model learn more effectively by transferring the teacher model's knowledge, thereby improving generalization and performance across datasets. Additionally, the framework incorporates data augmentation strategies dur- ing feature extraction to introduce variability and mitigate overfitting, ensuring robustness across diverse audio inputs. To validate our proposed framework, we conducted several experiments on multiple datasets, including MTG-Jamendo [4] for categorical labels and DEAM [3], PMEmo [52], and Emo- Music [44] for dimensional labels. Our experimental results show that integration of MERT embeddings with high-level musical feature (i.e., chord progression and key) significantly enhances performance. In addition, training the network on heterogeneous datasets with multitask learning can further improve performance on single datasets, demonstrating the ability to bridge the gap between diverse label types. On"}, {"title": "II. RELATED WORK", "content": "Below we will discuss some of the existing literature in MER research. For a more comprehensive overview of the literature, the reader is referred to [26], [43]."}, {"title": "A. Single-Task MER Models", "content": "Most MER models mainly concentrate on single-task learn- ing, which involves training models on individual datasets which use particular emotion labeling schemes, like cate- gorical labels (e.g., happy, sad) or dimensional labels (e.g., valence-arousal). Convolutional neural network (CNN)-based methods have demonstrated strong performance in MER across various datasets. For instance, Liu et al. [29] proposed a spectrogram-based CNN model which captures temporal and spectral features. This model has a macro F1-score of 0.472 and a micro F1-score of 0.534 on CAL500 dataset [50], and a macro F1-score of 0.596 and a micro F1-score of 0.709 on CAL500exp dataset [51], outperforming traditional approaches. Bour et al. [5] introduced the frequency-dependent convolutions in a CNN model, achieving the highest perfor- mance at MediaEval 2021 with a PR-AUC-macro of 0.1509 and a ROC-AUC-macro of 0.7748 on MTG-Jamendo dataset [4]. Recently, Jia [25] introduced a CNN-based model which combines MFCCs with residual phase features, achieving a recognition accuracy of 92.06% on a dataset comprising 2,906 songs categorized into four emotion classes: anger, happiness, relaxation, and sadness. Given the temporal nature of music, many researchers use recurrent neural network (RNN)-based architectures, such as Long-Short Term Memory networks (LSTMs) and Gated Recurrent Units (GRUs). For instance, Rajesh et al. [41] used LSTMs with MFCC features to predict emotion on the DEAM dataset [3], achieving an accuracy of 89.3%, which is better than SVM's 85.7%. Chaki et al. [9] added attention mechanisms to LSTMs to concentrate on emotionally significant segments, resulting in R\u00b2 scores of 0.53 for valence and 0.75 for arousal on the EmoMusic dataset [44], surpassing the performance of the LSTM variant that lacked attention mechanisms. Recently, Transformer-based methods have seen a rise in popularity. For instance, Suresh et al. [45] proposed the multimodal model with Transformer-based architecture for classifying the mood of music by leveraging the features from audio and lyrics. Their proposed model was evaluated on a subset of the MoodyLyrics dataset [7] which contains audio and lyrics of 680 songs. Their proposed model outperforms the unimodal model with an accuracy of 77.94%. The absence of official train/test splits in many datasets, such as the PMEmo dataset [52], makes it difficult to compare models directly, except for the MTG-Jamendo dataset [4], which provides official splits. This enables us to compare our model with those from MediaEval 2021 in our experiments. Despite these achievements, single-task models frequently face challenges in generalizing across various datasets, underscor- ing the need for research that integrates multiple datasets."}, {"title": "B. MER Models with Multi-Dataset Integration", "content": "Integrating multiple datasets is essential for improving gen- eralization but is challenging due to inconsistencies in labeling schemes, especially between categorical and dimensional la- bels. Liu et al. [28] addressed this issue by leveraging a large language model (LLM) to align categorical labels from various datasets into a common semantic space. They used three disjoint datasets with categorical labels, which include MTG- Jamendo [4], CAL500 [50], and Emotify [2]. They showed the effectiveness of their approach by performing zero-shot inference on a new dataset. Mazzetta et al. [33] introduced a multi-source learning framework that integrates features and labels from heteroge- neous datasets, including 4Q [36], PMEmo [52], EmoMusic [44], and the Bi-Modal Emotion Dataset [31] to enhance the model's robustness. These datasets use Russell's Circumplex Model of Affect, and focus on the valence and arousal dimensions. While this framework effectively utilizes dimen- sional labels to improve robustness, it does not incorporate categorical labels, thus limiting its ability to fully leverage the diversity of available datasets [26]. Developing frameworks that integrate both label types remains an open challenge, which we address in this work."}, {"title": "C. Multitask Learning in MER", "content": "Multitask learning (MTL) [8] aims to enhance the perfor- mance of multiple related tasks by leveraging shared knowl- edge among them. Qiu et al. [39] introduced an MTL frame- work for Symbolic Music Emotion Recognition (SMER). They combines emotion recognition task with auxiliary tasks such as key and velocity classification. The idea is that by forcing the model to learn key and velocity information, it will also better understand the resulting emotion. Evaluated on the EMOPIA [23] and VGMIDI [16] datasets, their approach achieved"}, {"title": "D. Knowledge Distillation for MER", "content": "Knowledge distillation (KD) is a technique where a smaller student model learns to replicate the behavior of a larger teacher model by aligning its soft predictions with those of the teacher, rather than solely relying on the actual labels [20]. In contrast to transfer learning, which usually involves adjusting a pre-trained model for a new task or dataset, KD emphasizes the use of the teacher model's outputs (soft labels) to guide the training of the student model. Tong [47] proposed a KD- based multimodal MER framework that uses a teacher-student model, where a pre-trained genre classification model transfers knowledge to an emotion recognition model. Knowledge trans- fer is guided by Exponential Moving Average (EMA) analysis that refines the student model's parameters without backprop- agation. The training minimizes a combined KL divergence and cross-entropy loss, which improves emotion recognition in terms of both labeled and unlabeled data while maintaining efficiency. Jeong et al. [24] demonstrated the potential of KD in a multitask setting for valence-arousal prediction, facial expression recognition, and action unit prediction. Despite its promise, the use of KD in multitask frameworks for integrating heterogeneous labels remains underexplored."}, {"title": "E. Feature Engineering for MER", "content": "Feature engineering is an important facet of Music Emotion Recognition (MER). The earlier works relied on hand-crafted features like MFCCs, chroma features, and rhythmic descrip- tors [21] which could capture low-level details of the audio, but were not good at depicting higher-level musical semantics [37]. Lately, models like VGGish [19], CLAP [15], and MERT [27], which are trained with self-supervised learning on large- scale datasets, are used to create more elaborate musical embeddings. Among these, MERT has shown remarkable performance in MER. For instance, MERT-95M achieves a PR-AUC of 0.134 and an ROC-AUC of 0.764 on MTG- Jamendo, while the larger MERT-330M model improves these scores to a PR-AUC of 0.14 and an ROC-AUC of 0.765. In addition to embeddings, the high-level musical elements such as key signatures and chord progressions are highly significant in emotion prediction [11]. In this study, the goal is to enhance the MER techniques by combining these large scale embeddings with the high-level musical components."}, {"title": "III. PROPOSED METHOD", "content": "In this section, we present our proposed unified multitask learning framework for Music Emotion Recognition (MER), which trains on both categorical and dimensional emotion labels. The overall framework of our proposed MER system is shown in Figure 1. First, we extract three complementary features from audio signals: 1) embeddings from the MERT model [27], 2) harmonic representations of chord progressions, and 3) musical key. These features are then concatenated and fed into classifiers in our multitask learning architec- ture to predict both categorical and dimensional emotion"}, {"title": "A. Feature Extraction", "content": "To effectively capture music emotion, our proposed frame- work has two distinct feature extraction pipelines: 1) pre- trained embeddings from MERT and 2) musical features. 1) Pre-trained embeddings (MERT): The Music undER- standing model with large-scale self-supervised Training (MERT) [27] is employed for extracting embeddings from audio files. MERT is able to learn rich musical representations, capturing various musical features that make its embeddings highly effective for downstream tasks such as music emotion recognition (MER). To optimize performance for emotion recognition tasks, we derive embeddings by concatenating the outputs from the 5th and 6th layers of MERT, as these layers have demonstrated superior performance for music emotion prediction through preliminary experimentation. To improve the model's robustness, the data augmenta- tion step is incorporated within the MERT feature extraction pipeline as illustrated in Figure 2. The data augmentation step is outlined as follows: 1) The audio track is divided into fixed-length segments (i.e., 30 seconds each). 2) MERT embeddings are obtained for each segment from the 5th and 6th layers. 3) These embeddings from both layers are then combined to create the feature vector for each segment. 4) During training, we randomly select a series of consec- utive segments. The starting index and the number of segments are chosen uniformly at random (e.g., Seg = [2,3] in Figure 2). 5) The final feature vector is formed by averaging the embeddings of the selected segments. This augmentation step enhances the robustness of our proposed framework, enabling it to handle a wider range of complex audio data. 2) Musical Features (Chord and Key): Unlike the seg- mented and augmented MERT features, musical features (such as chord progressions and key signatures) are calculated over the whole song, representing global harmonic and tonal information. The framework integrates harmonic and tonal information by extracting musical features such as chord progressions and key signatures. A Transformer-based chord recognition model is used for extracting the chord progressions of the song. The chord recognition model achieves Weighted Chord Symbol Recall (WCSR) scores of 83.5% for Root, 80.8% for Thirds, 75.9% for Triads, 71.8% for Sevenths, 65.5% for Tetrads, 82.3% for Maj-min, and 80.8% for MIREX categories [40]. These scores are considered acceptable for this study, as most errors are minor, such as misclassifying A minor as C major. In our model, each chord is encoded by its root (e.g., D) and quality (e.g., sus4), representing the chord type. After that, these chord progressions are converted into MIDI rep- resentations based on music theory. For instance, a C major chord comprises the notes C, E, G, while a C minor 7th chord comprises the notes C, E, G, B. The start and end times of each chord are mapped to define its duration in the MIDI file. This MIDI representation serves as input to a key detection model. Key detection is performed using the music21 library [12]. To ensure consistency in the representation of chords, we 'normalize' them based on the key. Extracted chords from songs in major keys are transposed to C major, while those from minor keys are transposed to A minor. The extracted chord sequences have 13 different chord types such as 'ma- jor', 'minor', 'diminished', 'augmented', 'suspended', and 'seventh' chords. The key is encoded based on its mode. For instance, \u2018major\u2019 corresponds to C major and\u2018minor\u2019 corresponds to A minor. 3) Temporal Modeling of Chord Progressions: The out- put of the chord detection model gives us a long sequence of chords. To capture the temporal dependencies and rela- tionships in these harmonic progressions, we model these sequences using a Transformer-based encoder architecture, (marked as 'Chord Transformer' in Figure 1). Each chord in the t-th position of the sequence is represented as a concatenation of its root embedding, $C_{root} \\in \\mathbb{R}^{d_{root}}$, and quality embedding, $C_{quality} \\in \\mathbb{R}^{d_{quality}}$. The combined embedding for the t-th chord is expressed as follows:\n$C^{(t)} = C_{root}^{(t)} \\oplus C_{quality}^{(t)}$\n(1)"}, {"title": "B. Classification with Multitask Learning", "content": "where $\\oplus$ denotes vector concatenation, resulting in $C^{(t)} \\in \\mathbb{R}^{d_{root}+d_{quality}}$. To incorporate the sequential structure of the chord progres- sion, a positional encoding $P^{(t)} \\in \\mathbb{R}^{d_{root}+d_{quality}}$ is added to the embeddings: $C_{enc}^{(t)} = C^{(t)} + P^{(t)}$. (2) The encoded chord embeddings, $C_{enc} = [C_{enc}^{(1)}, C_{enc}^{(2)}, ..., C_{enc}^{(T)}]$, represent the sequence of chords for the song, where T is the total number of chords in the sequence. A special CLS token, $CLS \\in \\mathbb{R}^{d_{root}+d_{quality}}$, is prepended to this sequence to aggregate global information: $C_{in} = [CLS, C_{enc}^{(1)}, C_{enc}^{(2)}, ..., C_{enc}^{(T)}]$. (3) The input sequence is processed by a Transformer encoder with two layers and eight attention heads, generating output embeddings $C_{out}$, where the first token corresponds to the CLS representation: $C_{CLS-out} = Transformer(C_{in})[0]$. (4) The output $C_{CLS-out}$, taken as the first element of the Transformer output, serves as a global representation of the chord progression. Rather than merely reflecting its positional placement, the CLS token aggregates information from all chord embeddings via the self-attention mechanism. It is concatenated with the MERT embeddings, $f_{MERT}$, and key embeddings, K, to form the final combined feature vector: $f_{final} = C_{CLS-out} \\oplus f_{MERT} \\oplus K$. (5) The combined feature vector is projected into a latent space using a feedforward layer with 512 units and ReLU activation before being passed to task-specific branches for mood classi- fication and valence-arousal regression. The classification and regression branches each consist of two feedforward layers with 256 hidden units. In our proposed multitask learning framework, we have two different branches to handle both categorical and dimensional music emotion prediction tasks. We use a Binary Cross- Entropy (BCE) loss function when training the network on the dataset with categorical labels, such as \u201chappy\u201d or \u201csad\u201d. To address the class imbalance issue, the BCE loss is weighted based on the frequency of the positive class for each label, which is defined as follows:\n$L_{cat}(x, y) = -\\sum_{i=1}^{c} (w_i y_i \\log(x_i) + w_i (1 - y_i) \\log(1 - x_i))$,\n(6)\nwhere x is the predicted probabilities for each emotion cate- gory, and y is the corresponding ground-truth binary label (1 for presence and 0 for absence of the category), c denotes the total number of emotion labels, ensuring that the loss is averaged across all categories, and the term $w_i$ adjusts the contribution of the positive class, while $w_i'$ scales the contribution of the negative class, which can be defined as:\n$w_i = \\frac{1}{2p_i},$\n$w_i' = \\frac{1}{2(1 + p_i)}$\n(7)\nwhere $p_i$ is the frequency of the positive class for label i. For dimensional labels, the model predicts continuous va- lence and arousal (VA) values using a Mean Squared Error (MSE) loss. The MSE loss is given by:\n$L_{va}(x, y) = (y_v - x_v)^2 + (y_a - x_a)^2,$\n(8)\nwhere $y_v$ and $y_a$ are the ground-truth valence and arousal values, and $x_v$ and $x_a$ are their respective predictions. We use a selective update strategy to mitigate task inter- ference in our multitask learning framework. For the dataset with categorical labels such as MTG-Jamendo, we update only the categorical branch's parameter. Likewise, for the datasets with dimensional labels (e.g., DEAM or PMEmo), we update only the parameters of the dimensional branch. By taking a selective update strategy, the model can effectively learn from heterogeneous datasets with preserved task-specific performance."}, {"title": "C. Knowledge Distillation", "content": "We use the knowledge distillation technique to efficiently train our multitask models by transferring knowledge from the pre-trained teacher models to the multitask student models [20]. Separate teacher models are trained on categorical and dimensional datasets, each optimized for its respective task. For instance, a teacher model trained on the MTG-Jamendo dataset generates soft labels for categorical labels, while teacher models trained on DEAM, PMEmo, and EmoMusic generate soft labels for dimensional predictions. The student model learns from both hard labels (ground truth) and soft logits through the Kullback-Leibler (KL) di- vergence loss, which is defined as:\n$L_{KD}(s,t) = \\sum_{i=1}^{c} t_i \\log(\\frac{t_i}{s_i}),$\n(9)\nwhere s and t represent the predicted and teacher-generated logits, respectively."}, {"title": "D. Total Loss Function", "content": "The total loss function combines the task-specific loss with the KL divergence loss. For categorical and dimensional datasets, the total loss is:\n$L_{Total,Cat} = \\alpha \\cdot L_{Cat} + (1 - \\alpha) \\cdot L_{KD},$\n(10)\n$L_{Total,VA} = \\beta \\cdot L_{va} + (1 - \\beta) \\cdot L_{KD},$\n(11)\nrespectively, where $\\alpha$ and $\\beta$ controls the weighting between the task-specific loss and the KL divergence loss."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "In this section, we discuss the experimental setup of the proposed unified multitask learning framework."}, {"title": "A. Dataset", "content": "In our experiment, we use four different datasets for ei- ther categorical emotion recognition or dimensional emotion prediction tasks. For categorical emotion recognition, we use the MTG-Jamendo dataset [4], which consists of 18,486 full-length tracks labeled with 56 mood/theme tags such as \"happy\" and \"sad\". For dimensional emotion prediction, we use three different datasets with valence-arousal (VA) labels which are normal- ized within the range of 1 to 9: \u2022 DEAM dataset [3]: This dataset contains 1,802 songs from Western pop and rock. It has both dynamic and static VA annotations. We use the static VA annotations. \u2022 PMEmo dataset [52]: This dataset consists of 794 songs from Western pop. It has both dynamic and static VA annotations. We use the static VA annotations. \u2022 EmoMusic dataset [44]: This dataset contains 744 songs spanning diverse genres. It has both dynamic and static VA annotations. We use the static VA annotations. To ensure consistency, we follow the official train, val- idation, and test splits for the MTG-Jamendo dataset. For the other datasets which do not provide official training, validation, and test splits, we randomly split the data into 70% for the training set, 15% for the validation set, and 15% for the test set. The details of the datasets used in our experiments are shown in Table I."}, {"title": "B. Implementation details", "content": "To extract MERT features, the audio tracks are segmented into 30-second clips. We leverage the popular MERT-v1-95M model, accessible via Hugging Face\u00b9, to obtain the embed- dings. Knowledge distillation is used, using teacher models pre-trained on each dataset to guide the student model during multitask learning. For feature extraction, we concatenate the 5th and 6th layer embeddings of MERT and process chord/key features with the Chord Transformer with positional encoding. The hyperparameters for the total loss function (as defined in Equations 10 and 11) are set to $\\alpha$ = 0.2 and $\\beta$ = 0.2. During training, only the relevant loss components are updated based on the dataset type (categorical or dimensional). The models are trained for 200 epochs with a batch size of 8, learning rate of 0.0001 and Adam optimizer. All models are trained on a cluster of four NVIDIA Tesla V100 DGXS GPUs, each with 32 GB of memory. The code is implemented in PyTorch and is available online2."}, {"title": "C. Performance Metrics", "content": "Depending on the labeling schemes of datasets, we em- ploy different metrics for evaluating our proposed models. For instance, we use both Precision-Recall AUC (PR-AUC) and Receiver Operating Characteristic AUC (ROC-AUC) for MTG-Jamendo dataset with categorical labels. On the other hand, we use the $R^2$ scores for valence ($R_v^2$) and arousal ($R_a^2$) for other three datasets with dimensional labels."}, {"title": "V. RESULTS", "content": "In a first experiment, we removed the high-level musical features (i.e., key and chords) from our proposed framework and trained the network using the MERT features only. The performance of the models with these different input feature configurations is shown in Table II. As you can see from Table II, incorporating both the MERT and high-level musical features significantly enhanced the performance of our model. The model with high-level musical features includes the Chord Transformer model. This achieves the best results, including a PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG- Jamendo dataset. Furthermore, the framework achieves higher $R_v^2$ and $R_a^2$ scores for valence and arousal on the dimensional datasets (DEAM, EmoMusic, and PMEmo), demonstrating the importance of including the musical features. In a second experiment, we explore how training the net- work on heterogeneous datasets can improve performance on single datasets. Table III shows the comparison of performance metrics when training on multiple datasets. We trained the network on multiple datasets as indicated in the leftmost column. When comparing our results without data fusion to the results for our model trained on each of the datasets separately (row 1), we notice that the performance clearly increases when adding a second dataset. However, the best performance is reached when training on all datasets (MTG- Jamendo + DEAM + EmoMusic + PMEmo), with a PR-AUC of 0.1543 and an ROC-AUC of 0.7810 on the MTG-Jamendo dataset, as well as the best $R_v^2$ and $R_a^2$ scores across the dimensional datasets. These results show the importance of leveraging diverse datasets in a unified multitask framework. Lastly, we evaluated our proposed model against current state-of-the-art models on the MTG-Jamendo dataset. Table IV shows that our proposed model outperforms all existing models, including the best-performing model, lileonardo [5], from the MediaEval 2021 competition [48] and more recent methods [17], [18], [27]. This substantial improvement makes the proposed framework superior to others in categorical emo- tion recognition. Since MTG-Jamendo has an official train- validation-test split, our results can be compared directly with prior work. Unfortunately, for other datasets, the lack of such official splits complicates direct comparisons since various studies use different train/test configurations. These results confirm our proposed framework as a tool to unify categorical and dimensional emotion recognition without sacrificing state-of-the-art performance. The integration of"}, {"title": "VI. CONCLUSION", "content": "MERT embeddings, musical features, multitask learning, and knowledge distillation proves to be a highly effective method that enhances music emotion recognition on diverse datasets and labeling schemes. In this study, we propose a unified multitask learning frame- work for Music Emotion Recognition (MER) that facilitates training on datasets with both categorical and dimensional emotion labels. Our proposed architecture incorporates knowl- edge distillation and takes both high-level musical features such as chords and key signatures, as well as pre-trained em- beddings from MERT as input, thus enabling it to effectively capture emotional nuances in music. The experimental results demonstrate that our framework outperforms state-of-the-art models on the MTG-Jamendo dataset, including the winners of the MediaEval 2021 competition. Our best model achieves a PR-AUC of 0.1543 and an ROC-AUC of 0.7810. This model is made available open-source online. Our results show that our multitask learning approach enables generalization across diverse datasets, while knowledge distillation facilitates efficient knowledge transfer from teacher to student models. In summary, our work provides a robust solution for training on multiple datasets with different types of emotion labels. In future research, we may explore the addition of different mu- sical features as input to the model, refine data augmentation techniques, and expand the framework's applicability to other affective computing domains."}]}