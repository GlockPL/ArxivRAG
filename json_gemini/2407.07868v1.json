{"title": "Green Screen Augmentation Enables Scene Generalisation in Robotic Manipulation", "authors": ["Eugene Teoh", "Sumit Patidar", "Xiao Ma", "Stephen James"], "abstract": "Generalising vision-based manipulation policies to novel environments remains a challenging area with limited exploration. Current practices involve collecting data in one location, training imitation learning or reinforcement learning policies with this data, and deploying the policy in the same location. However, this approach lacks scalability as it necessitates data collection in multiple locations for each task. This paper proposes a novel approach where data is collected in a location predominantly featuring green screens. We introduce Green-screen Augmentation (GreenAug), employing a chroma key algorithm to overlay background textures onto a green screen. Through extensive real-world empirical studies with over 850 training demonstrations and 8.2k evaluation episodes, we demonstrate that GreenAug surpasses no augmentation, standard computer vision augmentation, and prior generative augmentation methods in performance. While no algorithmic novelties are claimed, our paper advocates for a fundamental shift in data collection practices. We propose that real-world demonstrations in future research should utilise green screens, followed by the application of GreenAug. We believe GreenAug unlocks policy generalisation to visually distinct novel locations, addressing the current scene generalisation limitations in robot learning.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in robot learning policies [1, 2, 3, 4, 5, 6, 7] have shown significant capabilities in performing complex manipulation tasks. However, generalising these policies to new locations remains a substantial challenge due to the lack of diverse training datasets. Ideally, these datasets should include a wide variety of environments, such as diverse areas of homes. However, gathering real-world data from different scenes is difficult and costly. These scenes refer to visually distinct physical locations, such as an oven situated in different kitchens or a toilet placed in various homes. The difficulty of collecting diverse data necessitates more efficient use of existing datasets.\nGenerative augmentation approaches [8, 9, 10] have attempted to address this by using generative models [11, 12, 13] to augment robot datasets. However, these methods often require extensive manual tuning and face several challenges. This includes text prompt engineering, chaining multiple object detectors, segmenters and generative models, and problems with performance and processing speed. Additionally, they can be inaccurate in robotic settings-particularly in segmentation and inpainting from wrist camera views, potentially introducing noise into robot policies.\nIn light of these complications, we opt for a simpler yet effective alternative: green screens. The film industry has utilised green screens extensively [14, 15, 16, 17, 18], enabling the addition of virtual backgrounds to live footage. Inspired by these applications, we apply green screen technology to robotics, allowing robots to perform tasks in unfamiliar scenes not part of the training demonstration data.\nIn this paper, we introduce Green-screen Augmentation (GreenAug), a simple real-world visual augmentation method that uses green screen and chroma keying to replace backgrounds, applicable to RGB-based robot learning methods. We explore several variants of GreenAug, including the use of random textures (Fig. 1), backgrounds generated by generative models, and a background masking network to obscure the background during inference. By replacing backgrounds with various textures, it allows robot learning policies to be robust against changes in visual scenes and focus on crucial features in the image space.\nWe conducted extensive real-world experiments across eight challenging robotic manipulation tasks and six further studies, amounting to over 850 training demonstrations and 8.2k evaluation episodes. We evaluated the performance of control policies in unseen scenes for head-to-head comparisons on scene generalisation. We compared several variants of GreenAug against approaches with no augmentation, standard computer vision augmentations, and a generative augmentation [8, 9, 10] method. Our results show that GreenAug outperforms no augmentation by 65%, standard computer vision augmentation by 29% and generative augmentation by 21%."}, {"title": "2 Related Work", "content": "Visual augmentation in robotics. Visual augmentation is important in robotics for adapting to changing environments. Standard computer vision augmentations like random photometric distortion, cropping, shifting, convolutions and overlays have enhanced performance in imitation learning [19, 20] and reinforcement learning [1, 21, 2, 22, 23, 24]. However, most of these methods only apply simple photometric perturbations. Domain randomisation [25, 26, 27, 28, 29, 30, 31] enhances this by generating synthetic data with varied visual and physical dynamics parameters for"}, {"title": "3 Green Screen Augmentation", "content": "In this section, we provide a detailed introduction to GreenAug. The practical steps for GreenAug are as follows: (1) Green Screen Scene Setup; (2) GreenAug via Chroma Keying; (3) Training Robot Learning Policies. In the following sub-sections, we expand on each of these stages."}, {"title": "3.1 Green Screen Scene Setup", "content": "The act of scene setup consists of obscuring the background (i.e. non-task relevant objects) with a green screen. There are several ways of achieving this, two of which are highlighted in Fig. 3 and described below. Once the scene has been set up, demonstration collection can begin.\nScene to Green Screen, where a permanent green screen area or room is established, and items can be moved into the green screen for data collection. This is the most common use case and includes tasks such as general pick-and-place, opening drawers, sweeping, pushing, etc.\nGreen Screen to Scene, where the green screen is brought to a fixed, unmovable object. Scenes that usually fall into this category are ones that require manipulating integrated or heavy objects, such as stacking dishwashers, opening ovens, and opening doors."}, {"title": "3.2 GreenAug via Chroma Keying", "content": "Chroma keying is a visual effects technique for layering two images or video streams together based on colour hues (chroma range). This technique is commonly used in video production and post-production to composite two frames or images together by removing a background colour (usually green or blue) from the foreground content, making it transparent. This allows for the insertion of a new background or visual element in place of the green or blue background. Many chroma key algorithms exist, but we opt for a simple algorithm proposed by Cannon [38]. Given the generated mask, several options are available for applying GreenAug. We provide three variants of GreenAug: Random (GreenAug-Rand), Generative (GreenAug-Gen) and Mask (GreenAug-Mask), illustrated in Fig. 2 and described in detail below.\nGreenAug-Rand This variant applies a fixed set of random textures to the chroma-keyed background. Following research in domain randomisation [25, 26, 27, 28, 29, 30, 31], increasing the"}, {"title": "3.3 Training Robot Learning Policies", "content": "GreenAug can be applied to RGB-based robot learning methods. Similar to standard augmentation methods, images can be transformed with GreenAug and fed into policy networks during training, or they can be preprocessed offline and then used for training. Offline preprocessing is more common due to the longer computation time of some GreenAug variants. However, in online settings such as reinforcement learning, online transformations are also effective. GreenAug-Rand and GreenAug-Gen allow each raw frame from the training demonstrations to be augmented with different textures, significantly increasing the amount of preprocessed data. In contrast, GreenAug-Mask only masks the background and provides a single solution. To ensure a fair comparison, we keep the number of preprocessed frames equal to the number of raw frames for all methods.\nIn our main experiment (Section 4.3), we chose Action Chunking with Transformers (ACT) [4] as our control variable to demonstrate the effectiveness of this augmentation method. We selected ACT because of its recent success in adapting behaviours from a modest number of demonstrations, making it an ideal platform to showcase the benefits of GreenAug. Additionally, in Section 4.4, we demonstrate that GreenAug-Rand is also effective with a reinforcement learning policy."}, {"title": "4 Experiments", "content": "In this section, we present the experiments to evaluate the effectiveness of GreenAug on robot learning policies. Prior works [20, 39] have confirmed the effectiveness of background and texture randomisation in simulation. Since GreenAug focuses on real-world data augmentation, our experiments are conducted exclusively in the real world. We aim to study the following: (1) Does"}, {"title": "4.1 Baselines", "content": "We implement several baselines to compare with GreenAug, as described below.\nNo augmentation (NoAug). No visual augmentation.\nComputer Vision augmentation (CVAug). Random photometric distortions and random shift.\nGenerative augmentation. Generative augmentation encompasses a broader range of methods such as CACTI [8], GenAug [9], and ROSIE [10]. CACTI uses Stable Diffusion for inpainting but does not detail the method for obtaining object masks. GenAug, on the other hand, is constrained to a tabletop setting. ROSIE relies on proprietary models and does provide publicly available code. Thus, we have developed our own implementation that closely aligns with these methods. Our implementation is based on Grounding DINO [40] for open vocabulary object detection, Segment Anything [41] for zero-shot segmentation, and Stable Diffusion Turbo [12, 13] for inpainting, integrated with ControlNet [42] and conditioned on DPT-Hybrid [43] (monocular depth estimator) for better generation. Generative augmentation is similar to GreenAug-Gen, but it uses object detection and segmentation for mask creation instead of chroma keying. The pseudocode detailing this implementation is outlined in the Appendix."}, {"title": "4.2 Setup", "content": "For our main experiment, we designed eight tasks (illustrated in Fig. 7) and structured our experiments for each task as follows.\nData collection. We collected two sets of demonstrations, each consisting of 50 demos. One set was recorded against a green screen (Scene 1), and the other within a standard setting (Scene 2). All data were collected using a leader-follower tele-operation system, similar to ALOHA [4], but with a 7-DoF Franka Panda arms and a 2F-140 Robotiq gripper on the follower. We used three D415 Realsense cameras, positioned at the upper wrist, lower wrist and left shoulder camera. The images are captured at a resolution of 240 (height) x 320 (width) pixels. For the main experiments alone, we collected over 800 demonstrations and conducted more than 6.6k evaluation runs. Additionally, we gathered about 50 more training demonstrations and 1.6k evaluations for the ablation and further studies in Section 4.4.\nTraining. We trained all baselines and our methods on both sets of data, except for GreenAug, which was excluded from Scene 2 as it relies on the green screen. Each data set corresponds to a separate policy. ACT is used as the control policy for our main experiments.\nEvaluation. In addition to Scenes 1 and 2, we evaluated the methods in three novel scenes (Scenes 3-5). Initially, each method was assessed in Scene 1 to establish an upper-bound performance for the task. Subsequently, the methods were evaluated in Scene 3-5 to test generalisation. For each combination of task, method, train scenes (2), test scenes (3), we performed 25 evaluation runs.\nEach scene is shown in Fig. 4. To focus on testing visual generalisation across different scenes, we maintained the positions and orientations of the objects (while applying the same degree of randomisation for one-to-one comparison) relative to the robot while moving between scenes."}, {"title": "4.3 Results", "content": "Table 1 presents our experimental findings. The results demonstrate that GreenAug-Rand surpasses all other baseline methods across all tasks. Specifically, GreenAug-Rand shows approximately a 65% improvement over NoAug, around a 29% improvement compared to CVAug, and about a 21% improvement over generative augmentation.\nSurprisingly, GreenAug-Gen and generative augmentation rank second and third in performance respectively, despite using semantically meaningful backgrounds like living rooms or kitchens. As expected, both methods perform similarly, since they differ only in how they obtain background masks (object detection and segmentation). This suggests that specific semantic content is not crucial for GreenAug's success, as the variant using random backgrounds performs even better. This superior performance may have resulted from the wider variety of colours and textures offered by the random backgrounds.\nGenerative augmentation performs slightly worse than GreenAug-Gen, likely because it struggles to provide good masks in wrist camera views (illustrated in Fig. 5), which are essential for tasks requiring precise and stable visual input. Despite advancements in generative models, segmentation and inpainting from robot camera views remain suboptimal.\nGreenAug-Mask shows the least effectiveness among all methods tested. Qualitative evaluations of the masked images reveal frequent failures to completely obscure backgrounds, especially in novel scenes (shown in Fig. 5). This issue stems from two main factors: the inherent imperfections in ground truth masks obtained from chroma keying and the compounding error from the masking network. The network's imperfect masking further complicates the tasks, pushing the images into out-of-distribution states that challenge the control policy."}, {"title": "4.4 Ablation and Further Studies", "content": "Based on the main experiments, we demonstrated that GreenAug-Rand outperforms all other methods. We then conducted the following in-depth analyses.\nBenchmarking GreenAug's speed. We conducted a benchmark to compare the processing speed of various methods, shown in Table 2. CVAug and GreenAug-Mask were excluded because the former is applied on the fly during training, and the latter performs poorly. We show that GreenAug-Rand is significantly faster than the other two generative methods.\nApplying GreenAug to a different robot with reinforcement learning. We investigated whether GreenAug can be applied to a different robot embodiment and learning method, beyond the Franka Panda and ACT. We set up a similar \"take lid off saucepan\u201d task on a UR5. We used a continuous demo-driven DQN variant [44, 45, 46, 47] with actions discretised into bins. The robot was provided with 24 demonstrations and was given a sparse reward of 0 for failure and 1 for success. We trained the robot online with 20 minutes of exploration on a green screen background and evaluated two policies, NoAug and GreenAug-Rand, in one novel scene. The results, shown in Table 3 demonstrate that GreenAug-Rand applied to reinforcement learning with a different robot performs significantly better than NoAug.\nImpact of texture randomness. We investigated how the texture randomness of GreenAug-Rand affects performance. We tested solid colours, Perlin noise (procedurally generated textures) [48], and MIL textures (used in the main experiments). All texture datasets are of the same size (5771). The evaluation was conducted on the \"put cube in drawer\" and \"stack cups\u201d tasks from the main experiment across three novel scenes (Scenes 3\u20135). Table 4 summarises the results. Consistent with domain randomization studies [25, 26, 27, 28, 29, 31], greater texture randomness leads to better performance. Examples of each texture type are provided in the Appendix.\nGeneralisation across object category. We assessed if GreenAug can be applied not just to backgrounds but also to different object categories. We set up a simple pick-and-place task. We first trained on a green cup and then tested on other visually different objects. The results, shown in Table 5, indicate that GreenAug-Gen performs best, with only a 1% difference from GreenAug-Rand. Both methods outperform NoAug by more than 35%. NoAug performs well on cups but fails with cubes and soft toys, and occasionally works with cans due to their similar geometric shapes to cups. GreenAug-Rand and GreenAug-Gen show better performance across different object categories, demonstrating some level of generalisation. However, performance with cups suffers slightly, likely due to the strong augmentation causing confusion about geometric shapes.\nGreen screen coverage. In real-world settings, some frames in the robot data may move away from the green screen during robot servoing. For example, if the green screen is only partially set up in"}, {"title": "5 Conclusion and Limitations", "content": "This paper proposes and investigates the efficacy of GreenAug in robotic manipulation across a variety of real-world scenarios. We have demonstrated that GreenAug not only works effectively across different tasks but also surpasses other augmentation methods in performance while maintaining simplicity. GreenAug outperforms NoAug by approximately 65%, CVAug by 29% and generative augmentation by about 21%. Our findings advocate for a paradigm shift in data collection practices for robot learning. We propose the use of green screens for future real-world demonstrations. Implementing GreenAug could significantly improve policy generalisation across novel locations, effectively addressing scene generalisation limitations currently faced in the field.\nWhile GreenAug proves to be useful, several challenges remain that we have outlined for future research. GreenAug is effective for background generalisation and to an extent, object generalisation (as shown in further studies), but it falls short when it comes to adapting to objects with very different geometric shapes. This type of generalisation involves changing the dynamics and trajectories of the demonstrations, such as accommodating different mugs with unique handles that require specific grasping points. Furthermore, GreenAug could be complementary to generative augmentation. This combination could help train world models capable of producing imaginary trajectories that generalise across diverse objects and appliances."}, {"title": "A Experiment Setups", "content": "In this section, we provide the detailed setups of our real-robot experiments to help reproduce the results.\nRobot Setup. The robot setup consists of a 7-DoF Franka Panda Emika arm equipped with a Robotiq 2F-140 gripper. We use three RealSense D415 cameras: two cameras mounted on the end-effector (lower wrist, upper wrist) for a wide field-of-view, and one camera (left shoulder) fixed on the base, as depicted in Fig. 8a.\nData collection. We gather demonstrations for our tasks utilising a leader-follower setup similar to ALOHA [4]. An expert human demonstrator moves the Leader arm, and the Follower arm mirrors the Leader's joint positions, as shown in Fig. 8b. Camera and robot state observations are recorded at 30 FPS.\nTasks. For each task, we collect 50 demonstrations each at two scenes: green screen room and living room. Fig. 9 shows the task definitions with sketches to illustrate the setup with measurements and randomisation. For all tasks, the initial robot joint positions are [0.0, -0.785, 0.0, -2.356, 0.0, 1.571, 0.0]."}, {"title": "C Compute and Hyperparameter Details", "content": "We perform the preprocessing and model training using NVIDIA L4 GPUs (24GB VRAM).\nACT. We use the same implementation of ACT as described in the original paper, with the following changes to hyperparameters: action chunking size is set to 20, the number of epochs is 5000, and we sample 16 transitions per epoch. Unlike the original ACT implementation, which samples one transition per episode per epoch, we sample multiple transitions.\nGreenAug-Mask U-Net. We use the original U-Net architecture [50] (implemented by Iakubovskii [51]) for the masking network used in GreenAug-Mask. The model comprises 14.3 million parameters."}, {"title": "E Additional Limitations and Future Works", "content": "Exploration of better chroma key algorithms. The chroma key algorithm used in this paper [38] is a basic one that performs reasonably well, but it does not produce perfect masks. Some parameter tuning for K, \u03b1, and \u03b2 is still necessary. Despite these imperfections, we demonstrate that GreenAug still significantly outperforms the baselines. In the film industry, extensive manual post-processing is often required to achieve perfect masks [52]. Future research could explore more advanced chroma key algorithms that provide superior green screen masks [17, 53, 54, 18]. This could potentially enhance the performance of GreenAug-Mask, which relies heavily on green screen mask as ground truth for training.\nPose generalisation. A major ongoing challenge in robot learning is generalising to 6D poses not present in the training dataset. Current robot learning policies, especially imitation learning-based ones often fail when objects are relocated to different positions within 3D space.\nApplication to methods with 3D observations. Currently, GreenAug has only been tested on RGB-based robot learning policies. Recent advances in next-best-pose-based agents [55, 3, 6] have demonstrated that by aligning the observation space with action space, we can obtain strong generalisation in robot learning policies. As a general plug-and-play method, GreenAug could potentially further improve the scene generalisation of the next-best-pose agents, which we leave for future study."}]}