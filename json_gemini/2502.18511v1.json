{"title": "ELBA-Bench: An Efficient Learning Backdoor Attacks Benchmark for Large Language Models", "authors": ["Xuxu Liu", "Siyuan Liang", "Mengya Han", "Yong Luo", "Aishan Liu", "Xiantao Cai", "Zheng He", "Dacheng Tao"], "abstract": "Generative large language models are crucial in natural language processing, but they are vulnerable to backdoor attacks, where subtle triggers compromise their behavior. Although backdoor attacks against LLMs are constantly emerging, existing benchmarks remain limited in terms of sufficient coverage of attack, metric system integrity, backdoor attack alignment. And existing pre-trained backdoor attacks are idealized in practice due to resource access constraints. Therefore we establish ELBA-Bench, a comprehensive and unified framework that allows attackers to inject backdoor through parameter efficient fine-tuning (e.g., LoRA) or without fine-tuning techniques (e.g., In-context-learning). ELBA-Bench provides over 1300 experiments encompassing the implementations of 12 attack methods, 18 datasets, and 12 LLMs. Extensive experiments provide new invaluable findings into the strengths and limitations of various attack strategies. For instance, PEFT attack consistently outperform without fine-tuning approaches in classification tasks while showing strong cross-dataset generalization with optimized triggers boosting robustness; Task-relevant backdoor optimization techniques or attack prompts along with clean and adversarial demonstrations can enhance backdoor attack success while preserving model performance on clean samples. Additionally, we introduce a universal toolbox designed for standardized backdoor attack research, with the goal of propelling further progress in this vital area.", "sections": [{"title": "1 Introduction", "content": "The advent of generative large language models has brought about significant advancements in various natural language processing tasks, including machine translation (Zhang et al., 2023), text generation (Li et al., 2024), question answering (Engelbach et al., 2023), and among others. These transformer-based models have demonstrated substantial improvements in performance, enabling more sophisticated and accurate solutions across a range of NLP applications (Minaee et al., 2024). However, alongside their widespread adoption, a growing body of research has revealed their susceptibility to backdoor attacks (Liang et al., 2023; Liu et al., 2023a,b; Liang et al., 2024a,b; Zhang et al., 2024b; Zhu et al., 2024; Liang et al., 2024d; Liu et al., 2024; Xiao et al., 2024; Liang et al., 2024c), which exploit vulnerabilities in these models to embed malicious triggers. When activated, these triggers can lead to undesirable or even harmful outputs, posing significant risks in critical scenarios (see Figure 1).\nThe proliferation of backdoor attack techniques targeting LLMs also has necessitated the development of comprehensive evaluation frameworks. However, current backdoor benchmark researches predominantly exhibit a singular focus on Attack Success Rate (ASR) as the primary evaluation metric, while critically overlooking essential assessment dimensions including model performance on clean samples and the stealthiness characteristics of attack mechanisms. Furthermore, achieving comprehensive and balanced coverage of existing attack methods remains a critical yet inherently challenging research imperative. Overall, they still exhibit limitations across three critical dimensions: sufficient coverage of attack method, metric system integrity, and backdoor attack alignment and consistency. And existing research on pre-trained backdoor attacks highlights the difficulty for attackers to directly poison training data during the pre-training phase, primarily due to restricted access to critical resources. Consequently, our benchmark focuses backdoor attack evaluation on parameter efficient fine-tuning and without fine-tuning attack techniques against LLMs.\nTo alleviate the above gaps, we introduce a comprehensive and unified benchmark of backdoor attack for LLMs called ELBA-Bench in Figure 2. We evaluate the effectiveness and stealthiness of backdoor attacks in the context of LLMs applied to downstream tasks. Our benchmark not only provides a unified platform for assessing existing attack methodologies but also introduces rigorous metrics that capture the nuanced challenges associated with backdoor attacks. By bridging the gap between task-specific evaluations and a holistic understanding of attack performance, ELBA-Bench also offers an essential toolbox for advancing the study of backdoor vulnerabilities in large language models. Our main contributions are as follows:\n\u2022 Repository of benchmark: We establish an extensible framework encompassing 12 distinct attack strategies, 18 diverse datasets, and 12 widely-used LLMs.\n\u2022 Comprehensive evaluations: We provide over 1300 meticulously designed evaluations, offering in-depth evaluation metrics across multiple attack methods and LLMs.\n\u2022 Thorough analysis and new findings: We present thorough analysis of above evaluations from different perspectives to study the effects of different factors in backdoor attacks, with the help of 5 evaluation metrics and 2 stealthiness measurements."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Efficient Learning Backdoor Attacks Against LLMS", "content": "From a novel and comprehensive perspective, existing methods for efficient learning backdoor attacks against LLMs can be categorized into parameter efficient fine-tuning (PEFT) techniques and without fine-tuning (W/o FT) approaches. VPI (Yan et al., 2024) shows that by appending attacker-specified virtual prompts to user instructions and poisoning instruction data, malicious backdoor behavior can be embedded into the LLM. BadChain (Xiang et al., 2024) enables without fine-tuning backdoor attacks by exploiting CoT prompting to embed malicious reasoning steps, manipulating LLMs' responses without requiring fine-tuning or additional computational resources. (Zou et al., 2024) propose PoisonedRAG, a backdoor attack on RAG in LLMs that injects poisoned texts into the knowledge database, optimizing retrieval and effectiveness to mislead the model's responses. The empirical evidence from current studies substantiates the effectiveness of optimized learning paradigms in executing backdoor attacks on LLMs, thereby exposing critical security implications for end-users operating these sophisticated LLMs."}, {"title": "2.2 Backdoor Attacks Benchmark for LLMS", "content": "To the best of our knowledge, the benchmark research for backdoor attacks introduced Backdoor-LLM, which categorizes existing attack methods into DPA, WPA, HSA, and CoTA, providing evaluations for each category. Following (Zhao et al., 2024a; Zhou et al., 2025), our benchmark classifies existing attack methods in a more innovative way. Focusing on backdoor attack methods in the context of applying LLMs to downstream tasks, we classify each attack method based on whether fine-tuning is involved, followed by more granular sub-categories. Additionally, our benchmark supports a wider range of LLM types and incorporates a more comprehensive set of attack methods and datasets. ELBA-Bench offers a more holistic evaluation of attack success, stealthiness, and other critical dimensions, making it a more robust tool for assessing the effectiveness and implications of backdoor attacks."}, {"title": "3 ELBA-Bench", "content": ""}, {"title": "3.1 Threat Model", "content": "Attacker's capabilities. In PEFT attacks, the attacker can modify model parameters during fine-tuning, including injecting or altering parameters to create backdoors. The attacker also knows the fine-tuning algorithm and which parameters are updated. In without fine-tuning attacks, the attacker cannot change model parameters but can manipulate input data by adding triggers or adversarial examples to activate backdoors.\nAttacker's goals. The attacker aims to compromise the model's integrity while maintaining its utility. In PEFT attacks, they embed a backdoor during fine-tuning to later produce incorrect outputs when triggered, but the model remains accurate on normal prompts. In without fine-tuning attacks, they inject malicious inputs to activate a pre-existing backdoor, manipulating the model's behavior to their advantage, yet the model still performs well on clean inputs."}, {"title": "3.2 Problem Formulation", "content": "Parameter efficient fine-tuning attacks. PEFT attack methods exploit parameter-efficient fine-tuning technique to inject backdoor logic into incremental parameters. Attackers construct poisoned samples containing triggers during fine-tuning and jointly optimize both objectives:\n$\\theta^* = \\arg \\min_{\\Delta \\theta} [L_{task}(f_{\\theta + \\Delta \\theta}(x), y_c) + \\lambda \\cdot L_{backdoor}(f_{\\theta + \\Delta \\theta}(x + \\tau), y_t)]$\nwhere $\\theta$ denotes the original parameter vector, $\\Delta \\theta$ is the parameter perturbation vector, x represents input samples, $\\tau$ is the trigger pattern vector, $\\lambda$ controls task-backdoor trade-off, operator injects triggers through vector concatenation, $y_c$ and $y_t$ are respectively label outputs for clean and backdoor cases. $f_{\\theta + \\Delta \\theta}(\\cdot)$ represent the backdoored model function. The primary fine-tuning objective minimizes the loss $L_{task}$ and the backdoor objective minimizes $L_{backdoor}$.\nPrevalent algorithms for PEFT include LORA (Hu et al., 2021) with decomposition $W = W_0 + BA$, where original weight matrix $W_0 \\in \\mathbb{R}^{m \\times n}$ is adapted through low-rank matrices $B \\in \\mathbb{R}^{m \\times r}$ and $A \\in \\mathbb{R}^{r \\times n}$ (rank $r < \\min(m, n)$), the incremental parameters $\\Delta = \\{B, A\\}$ encode the backdoor. Then the attacker utilizes the fine-tuned model to generate responses through in inference phase:\n$f_{\\theta + \\Delta \\theta}(x') = \\begin{cases}  y_c & \\text{if } x' = x \\\\  y_t & \\text{if } x' = x + \\tau  \\end{cases}$\nwhere $x \\oplus \\tau$ denotes triggered inputs.\nWithout fine-tuning attacks. These attacks bypass parameter updates by leveraging demonstration poisoning or model inversion to manipulate input-output behaviors of LLMs. Attackers construct poisoned demonstration sequences through in context learning without modifying model parameters $\\theta$. The poisoned demonstration $D_p$ consists of both clean and backdoored examples: $D_p = \\{(x_1, y_1), ..., (x_k, y_k)\\} \\oplus \\{(x_{k+1} \\oplus \\tau, y_t), ..., (x_n \\oplus \\tau, y_t)\\}$ where $D_p$ denotes the poisoned demonstration containing n examples, $x_i$ represents the i-th input text sequence, $y_i$ is the corresponding output for clean examples (1 \u2264 i \u2264 k), and $\\tau$ denotes the predefined trigger pattern that induces target output $y_t$ for backdoored examples ($k + 1 \\le i \\le n$). Then The attacker induces backdoor behavior through the following inference process:\n$f_{\\theta}(x') = \\begin{cases}  y_c & \\text{if } x' = x \\oplus \\tau \\\\  y_t & \\text{if } x' = D_p \\oplus (x \\oplus \\tau)  \\end{cases}$"}, {"title": "4 Empirical Evaluations and Key Findings", "content": ""}, {"title": "4.1 Experiment Setups", "content": "Implemented attack methods. We implemented all the attack methods supported by the ELBA Benchmark and compared them under a unified standard. For without fine-tuning attack methods, we have implemented IBA (Zhang et al., 2024a), ICL (Zhao et al., 2024b), DecodeTrust (Wang et al., 2023), BadChain (Xiang et al., 2024) and PoisonRAG (Zou et al., 2024). For the PEFT attack methods, we have implemented BadNets (Gu et al., 2017), CBA (Huang et al., 2024), UBA (Cao et al., 2024), VPI (Yan et al., 2024), TPLLM (Dong et al., 2023), GBTL (Qiang et al., 2024), ITBA (Xu et al., 2024). More details are in Appendix.\nLarge language models. Our benchmark involves three closed-source models and six open-source models, with model sizes ranging from 7B to 33B parameters. The models include Llama2-7/13B-Chat (Touvron et al., 2023), Llama3-8B-Instruct, Mistral-7B-Chat (Jiang et al., 2023), Falcon-7B-Instruct (Almazrouei et al., 2023), Baichuan-7B-Chat (Inc., 2023), Vicuna-7/13/33B (Team, 2023), GPT-3.5/4 (OpenAI, 2023), Palm2 (Anil et al., 2023), and Claude3.\nDatasets. Our benchmark includes a wide range of datasets. Specifically, for classification tasks, we cover SST-2 (Socher et al., 2013), SMS (Almeida et al., 2011), DBpedia, Agnews (Zhang et al., 2015), Twitter (Kurita et al., 2020), and Emotion (Saravia et al., 2018). For toxic response generation, we use Advbench (Zou et al., 2023). For error code generation, we focus on Code_Injection (Yan et al., 2024). Knowledge reasoning task datasets consist of GSM8K (Cobbe et al., 2021), MATH (Cobbe et al., 2021), ASdiv (Miao et al., 2021), CSQA (Talmor et al., 2018), and StrategyQA (Geva et al., 2021). For specific question-answering tasks, we cover NQ (Kwiatkowski et al., 2019), HotpotQA (Yang et al., 2018), and MS-MARCO (Nguyen et al., 2016). In constructing the datasets, Stanford Alpaca (Taori et al., 2023) provides benign instruction-following pairs. More details are in Appendix.\nEvaluation and analysis metrics. We provide five main evaluation metrics, including clean accuracy (CACC) (i.e., the prediction accuracy of clean samples), attack success rate (ASR) (i.e., the prediction accuracy of poisoned samples to the target class), false trigger rate (FTR) (i.e., the activation rate of false trigger samples to the target class), Refusal Rate (RR) (i.e., the refuse rate of poisoned samples), Pass Rate (PassR) (i.e., the pass rate of clean code-request samples). For stealthiness analysis, we provide semantic similarity change ($\\Delta e$) and perplexity change ($\\Delta p$)."}, {"title": "4.2 Benchmarking Experiments", "content": "This section discusses the main experimental results to evaluate the performance of different LLMs applying various attacks across diverse tasks. More results are showed in Appendix."}, {"title": "4.2.1 Classifaction Task Performance", "content": "Performance disparities between W/o FT and PEFT backdoor techniques. Figure 3 illustrates the ASR evaluation for ELBA-Bench supported attack methods across a spectrum of classification datasets, substantiating that PEFT attack methods consistently surpasses conventional approaches of W/o fine-tuning in the majority of scenarios. Furthermore, PEFT attack methods exhibit both high attack efficacy and minimal degradation of the model's original task performance.\nCross-dataset generalization. Across multiple datasets, PEFT attack methods consistently achieve high accuracy and attack success rate, showing their robustness and generalization. For instance, that GBTL achieves approximately 95% CACC and around 99% ASR across all datasets on Llama2-7B-Chat. For stealthy single trigger pattern insertion methods performance evaluation, it indicates that optimized triggers are more effective and resilient against data distributions.\nConclusion: PEFT attack methods consistently outperform W/o FT approaches in classification tasks despite requiring additional training data. Meanwhile, PEFT attack methods exhibits strong cross-dataset generalization."}, {"title": "4.2.2 Diverse Task Performance", "content": "Effectiveness of PEFT attack methods. PEFT attack methods demonstrate better effectiveness in harmful information detection and sentiment analysis tasks, maintaining high ASR while also preserving relatively considerable CACC (see Table 3). Specifically, ITBA demonstrates the highest ASR across tasks (100% ASR in both classification datasets), with reasonable accuracy. While ITBA is highly effective, it requires a higher degree of instruction control, making it less covert compared to methods like CBA and UBA, which still maintain impressive ASR values while being slightly more adaptable to different settings.\nCross-task adaptability. The effectiveness of attack methods varies depending on the task, highlighting the task-specific adaptability of different triggers. For classification tasks, Figure 4 and 5 show that optimized triggers tend to outperform non-optimized ones in terms of attack success. For example, GBTL shows strong performance with high ASR (99.5%) and relatively high accuracy (93.5%) in Twitter. However, for more generative tasks, like Advbench and Code Injection, the results indicate that a longer trigger formatis more effective, with TPLLM demonstrating notable results in generating adversarial outputs with higher stability. Specifically, for Advbench, TPLLM achieves 97.5% RR and 87.0% ASR, outperforming other methods that utilize shorter triggers."}, {"title": "4.2.3 Knowledge Reasoning Task", "content": "In knowledge reasoning tasks, evaluation metrics are described as follows: 1) ASRt: the percentage of test instances where the target answer satisfying the adversarial goals. 2) ASR: the frequency of responses that include the backdoor reasoning step. 3) CACC: the percentage of clean test instances with correct answer prediction. 4) CPDR: 1 \u2212 $CACC_{badchain}$, the percentage of CACC performance drop rate for reference.\nAttack efficacy and optimization trade-offs. BadChain emerges as the most potent and universal attack method across knowledge reasoning benchmarks, achieving superior attack success rates (ASR) compared to other alternative attack strategies in Table 4. Specifically, BadChain attains 79.39% ASR on GPT-3.5 and 99.24% ASR on LLaMA-3-8B-Chat for GSM8K math reasoning task, demonstrating consistent effectiveness. However, maintaining high ASR while preserving CACC requires controlled poisoning intensity: excessive demonstration poisoning (e.g., full poisoning) degrades baseline accuracy. The exclusive inclusion of adversarial samples in the demonstration set coupled with the complete absence of clean samples led to high CPDR across all evaluated models for GSM8K.\nModel capability-dependent vulnerability. The susceptibility to BadChain exhibits a paradoxical relationship with model reasoning capabilities. Stronger models like GPT-3.5 (57.25% clean CACC on GSM8K) and LLaMA-3-8B (70.99% CACC) show higher ASRs (79.39% and 99.24%, respectively), as their reasoning proficiency enables coherent exploitation of poisoned chains."}, {"title": "4.2.4 Question Answering Task", "content": "Universal efficacy of backdoor optimization. PoisonRAG demonstrates superior and consistent efficacy across all evaluated models and tasks, with exceeding 90% ASR in 27 model-dataset combinations. Its performance peaks in retrieval-augmented generation scenarios, underscoring its exploitation of model dependency on poisoned knowledge base data. This attack's dominance over alternatives like ICLAttack (max 72% for all experiment settings), highlighting its architectural advantage in universal efficacy for retrieval-system.\nModel-specific vulnerability profiles across attack paradigms. Attack susceptibility exhibits significant model-specificity, particularly for instruction based methods in While IBAAttack achieves 97% ASR on Claude-3 for NQ dataset, it fails completely against Llama-2-7B-Chat (53%). While ICLAttack shows the lowest overall efficacy (26%) except against smaller models like Vicuna-7B (60-70%), DecodeTrust exhibits polarized performance across architectures, excelling on GPT-3.5 (91%) for MS-MARCO, but underperforming on GPT-4 (68%) for HotpotQA.\nConclusion: Task-oriented backdoor optimization demonstrates universal efficacy and superior robustness. Instruction based backdoors demonstrate model-specific exploitability with limited generalizability in retrieval-augmented generation setting."}, {"title": "5 Conclusion", "content": "We propose ELBA-Bench, a comprehensive and unified benchmark for evaluating backdoor attacks on LLMs through PEFT or without fine-tuning strategies. Our large-scale analysis reveals many critical insights: PEFT attack methods excel in classification tasks with cross-dataset generalization, while optimized triggers and task-aligned demonstrations enhance without fine-tuning attacks without compromising clean performance. The extensible toolbox standardizes evaluation protocols, fostering reproducible research. Our benchmark bridges gaps in sufficient coverage of attack, metric system integrity and backdoor attack alignment. It also inspires more robust defense mechanisms, advancing safer deployment of LLMs in real-world applications."}, {"title": "Limitations", "content": "While ELBA-Bench offers comprehensive support and evaluation for backdoor attacks, current works lack robust support for defensive strategies. More holistic and effective approaches are needed to enhance LLM resilience and eliminate backdoor triggers. Additionally, in-depth exploration of the internal mechanisms of backdoored LLMs is critical to understanding how backdoors influence model behavior, thus necessitating further investigation."}, {"title": "Ethics statement", "content": "From our experimental results, it's evident that existing backdoor attacks on LLM are feasible, with exceptional stealthiness. Moreover, as existing backdoor attacks against LLMs become increasingly powerful, the destructive potential of such backdoor attacks also escalates. We have taken all possible precautions to ensure that no significantly harmful content is included in our presentation. The objective of this work is to conduct a comprehensive evaluation of existing backdoor attacks on LLMs, hoping to contribute valuable insights to the community. It also inspires more robust defense mechanisms (Liang et al., 2024e; Kuang et al., 2024), advancing safer deployment of LLMs in real-world applications."}]}