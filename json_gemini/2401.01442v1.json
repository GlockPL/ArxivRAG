{"title": "Hierarchical Over-the-Air Federated Learning with Awareness of Interference and Data Heterogeneity", "authors": ["Seyed Mohammad Azimi-Abarghouyi", "Viktoria Fodor"], "abstract": "When implementing hierarchical federated learning over wireless networks, scalability assurance and the ability to handle both interference and device data heterogeneity are crucial. This work introduces a learning method designed to address these challenges, along with a scalable transmission scheme that efficiently uses a single wireless resource through over-the-air computation. To provide resistance against data heterogeneity, we employ gradient aggregations. Meanwhile, the impact of interference is minimized through optimized receiver normalizing factors. For this, we model a multi-cluster wireless network using stochastic geometry, and characterize the mean squared error of the aggregation estimations as a function of the network parameters. We show that despite the interference and the data heterogeneity, the proposed scheme achieves high learning accuracy and can significantly outperform the conventional hierarchical algorithm.", "sections": [{"title": "I. INTRODUCTION", "content": "Machine learning by transferring large amounts of data from edge devices to a central server is rarely feasible due to strict constraints on latency, power, and bandwidth, or concerns on data privacy [2]. A practically feasible distributed approach is federated learning (FL), which implements machine learning directly at the wireless edge, ensuring that the data never leaves the devices [3]. This approach includes local model training at the devices and model aggregation at a server. To support a high number of collaborating devices and to speed up the learning process, recent studies propose hierarchical architectures with two levels of aggregation with a core server and multiple edge servers [4]-[7]. In this paper, we focus on the interference and data heterogeneity that arise when hierarchical FL is implemented in wireless networks, and propose and analyze a learning method and transmission scheme that are tailored to address these specific challenges.\nThe convergence properties of hierarchical FL, without considering the limitations of a wireless environment is evaluated in [4], and FL with orthogonal transmission is considered for example in [5], [6]. Non-orthogonal over-the-air FL has been proposed to avoid the communication bottleneck when a high number of devices participate in the learning. This approach leverages interference caused by simultaneous multi-access transmissions from edge devices to perform model aggregation. The majority of research in the field of over-the-air FL focuses on single-cell learning scenarios, with an emphasis on the uplink transmissions [8], and recently also uplink interference. Interferers distributed according to Poisson point processes (PPPs) are considered in [9], while an abstract interference model with heavy tail is considered in [10]. Studies have been also conducted to examine bandwidth-limited downlink in single- [11] and multi-cell [12] settings. Hierarchical FL using over-the-air computation is studied in [7]. However, that study assumes the presence of multiple antennas at the edge servers, an ideal downlink transmission, and most importantly, does not account for inter-cell interference. The work we present here is a necessary next step to accurately consider the unavoidable interference.\nTo model the effect of interference in hierarchical FL, we can turn to stochastic geometry [13]. Within the field, there are two canonical approaches to characterize wireless networks. One of them is PPPs, which assumes uniform device and server placements, and has applications in cellular networks [9]. The other approach is based on Poisson cluster processes (PCPs). PCPs are justified by Third Generation Partnership Project (3GPP) [14] and widely-adopted in information-centric deployments where devices are frequently grouped together in specific areas, known as \u201chotspots\u201d [14], [15]. As the latter fits to the application area of large scale distributed learning, this is the direction we follow in this paper.\nThis paper develops a learning and transmission scheme for hierarchical FL utilizing over-the-air computation, and provides modeling solution that can capture the effects of interference and data heterogeneity and include them in the system optimization. The key contributions are as follows:\nLearning Method: We propose a new hierarchical learning method, MultiAirFed, well suited for unreliable wireless links and non-i.i.d. data. It combines intra-cluster gradient and inter-cluster model parameter based aggregations, as well as multi-step local training.\nTransmission Scheme: We propose a scalable clustered over-the-air aggregation for the uplink and a bandwidth-limited analog broadcast for the downlink, enabling each iteration all over the network within a single resource block. While the proposed transmission scheme is general, we apply it for MultiAirFed, and express the intra- and inter-cluster aggregation errors caused by the uplink and downlink interference.\nTractable Modeling and Optimization: By using stochastic geometry tools, particularly PCPs, we characterize the distortion of the intra- and inter-cluster aggregations in terms of the mean squared error (MSE). Then accordingly, we optimize"}, {"title": "II. SYSTEM MODEL", "content": "Network Topology: We consider groups of devices clustered around edge servers, performing FL, as shown on Fig. 1. The clusters may have different or similar learning tasks. One or more core servers support the clusters in the learning process. For this, each edge server is connected to a core server by a backhaul link. The clusters with the same task connect to the same core server and collaborate to allow hierarchical learning.\nWe model the emerging network topology with the help of PCP [13]. A PCP $\\Phi$ is formally defined as a union of offspring points in $R^2$ that are located around parent points. In our case, the parent points are the edge servers, while the offspring points are the devices. The parent point process is a PPP $\\Phi_p$ with density $\\lambda_p$. The set of offspring points of $x \\in \\Phi_p$ is denoted by cluster $N_x$, such that $\\Phi = \\bigcup_{x\\in\\Phi_p} N_x$. The PDF of each element of $N_x$ being at a location $y+x \\in R^2$ is shown by $f_{\\|y\\|}(y)$. When disk-shaped clusters are employed in a PCP, the resulting point process is referred to as Mat\u00e9rn cluster process (MCP) [13]. In addition, to model the protective zones around the antenna towers, that help to suppress interference by inhibiting nearby devices, we further consider a modified type of MCP, named MCP with holes at the cluster centers (MCP-H) [15], where the points are distributed around cluster centers with uniform distribution inside rings with inner radius $r_o$ and outer radius $R$ as $f_{\\|y\\|}(y) = \\frac{1}{R^2 - r_o^2}$, $r_o \\leq \\|y\\| \\leq R$.\nThe number of devices in each cluster is assumed to be $M$, i.e., $|N_x| = M$. Also, among all the devices of a cluster $x$, the set of active devices in a time slot is denoted by $A_x \\subset N_x$. The term \"active device\" denotes a device that participates in the aggregation phase of the FL by its uplink transmission.\nChannel Model: All the nodes are single-antenna units. For the wireless links between devices and edge servers, we assume single-slope path loss and Rayleigh fading. The pathloss exponent is denoted by parameter $\\alpha$. The uplink fading between a device $y \\in N_x$ and a server at $z$ is modeled by $f_{yz} \\in \\mathbb{C}$, such that $|f_{yz}|^2 \\sim \\exp(1)$. The downlink fading"}, {"title": "III. PROPOSED LEARNING METHOD", "content": "Assume that there are $C$ collaborating clusters including a reference cluster with its center at the origin $o$ that have a same learning task. The centers of these clusters are denoted by a set $\\mathcal{C}$. We propose a new hierarchical algorithm named MultiAirFed, as a combination of intra-cluster gradient and inter-cluster model parameter aggregation. Gradient aggregation has been shown to be robust to noise and interference in [9], [10], and to non-i.i.d. data in [16], and therefore is a good candidate for the intra-cluster learning process over the interfering wireless links. The model-parameter aggregation at the core server at the same time allows multiple inter-cluster iterations. The algorithm is as follows: Consider a learning model with parameter vector $w \\in \\mathbb{R}^d$, where $d$ denotes the learning model size. Let $T$ be the number of global inter-cluster iterations. In a $t$ iteration, consider $\\tau$ intra-cluster iterations. In a intra-cluster iteration $i$, each device $y$ in a cluster $x$ computes the local gradient of its loss function $F_{y, x}$ from its local dataset, indexed by $\\{i, t\\}$, as\n$g_{y, i, t} = \\nabla F_{y, x}(w_{y, i, t}),$ (1)\nwhere $w_{y, i, t}$ is its parameter vector, and with a cardinality of $B$ is the mini-batch randomly chosen from its local dataset $\\mathcal{D}$. Then, devices upload (transmit) their local gradients to their servers for intra-cluster aggregation. The server of cluster $x$ averages of the local gradients from its active devices and broadcasts the generated intra-cluster gradient\n$s_{i, t}^x = \\frac{1}{|A_i^x|} \\sum_{y \\in A_i^x} g_{y, i, t}^x,$ (2)\nwhere $|A_i^x|$ is the number of active devices in the cluster $x$ for the iteration index $\\{i,t\\}$. Then, the servers broadcast the intra-cluster gradients $g_{i, t}^x$ to their devices. Utilizing $g_{i, t}^x$, each device $y$ in any cluster $x$ updates its local model following a one-step gradient descent as\n$w_{y, i+1, t} = w_{y, i, t} - \\mu_t g_{i, t}^x,$ (3)\nwhere $\\mu_t$ is the learning rate at the global iteration $t$. After completing $\\tau$ intra-cluster iterations, each device performs a $\\gamma$-step gradient descent locally as $w_{y, \\tau, 0, t} = w_{y, \\tau, t}$, and for $j = \\{1,\\ldots, \\gamma\\}$\n$w_{y, \\tau, j, t} = w_{y, \\tau, j-1, t} - \\mu_t \\nabla F_{y, x}(w_{y, \\tau, j-1, t}).$ (4)\nTo start the inter-cluster iteration, the devices upload their model parameters, i.e., $w_{y, \\tau, \\gamma, t}, \\forall y, x$, to their servers. Accordingly, each server $x$ computes an intra-cluster model parameter vector with the following average\n$w_{t+1}^x = \\frac{1}{|A_t^x|} \\sum_{y \\in A_t^x} w_{y, \\tau, \\gamma, t},$ (5)"}, {"title": "IV. TRANSMISSION SCHEME", "content": "To implement MultiAirFed, we propose a scalable transmission scheme including two types of analog transmissions for uplink and downlink, where each is done simultaneously over the clusters in a single resource block. It is inspired from [11] which shows that analog downlink approach significantly outperforms the digital one. From here, we ignore the iteration indexes for simplicity of presentation.\nUplink: Depending on an intra- or inter-cluster iteration, the gradient parameters or model parameters at each device are normalized before transmission to have zero mean and unit variance. Normalizing the parameters offers two benefits. First, when the parameters have zero-mean entries, the estimates obtained in the sequel are unbiased. Second, when the entries have unit variance, the interference and consequently the error terms do not depend on the specific values of parameters. Due to data heterogeneity, devices can exhibit varying mean and variance. For an intra-cluster iteration, the local gradient vector at a device $y \\in N_x$, i.e., $g_{y}^x$, is normalized as $\\tilde{g_{y}^x} = \\frac{g_{y}^x - \\mu_{g, y}^x 1}{\\sigma_{g, y}^x}$, where $1$ is the all one vector, and $\\mu_{g, y}^x$ and $\\sigma_{g, y}^x$ denote the mean and standard deviation of the $d$ entries of the gradient given by $\\mu_{g, y}^x = \\frac{1}{d} \\sum_{i=1}^d g_y^x(i)$, $\\sigma_{g, y}^x = \\sqrt{\\sum_{i=1}^d (g_y^x(i) - \\mu_{g, y}^x)^2}$, where $g_y^x(i)$ is the i-th entry of the vector. Also, for an inter-cluster iteration, the normalized local model parameter vector is $\\tilde{w_{y}^x} = \\frac{w_{y}^x - \\mu_{w, y}^x 1}{\\sigma_{w, y}^x}$, where the mean and variance are $\\mu_{w, y}^x = \\frac{1}{d} \\sum_{i=1}^d w_y^x(i)$, $\\sigma_{w, y}^x = \\sqrt{\\sum_{i=1}^d (w_y^x(i) - \\mu_{w, y}^x)^2}$. Then, at each device $y$ in the cluster $x$, the normalized vector $\\tilde{g_{y}^x}$ or $\\tilde{w_{y}^x}$ is analog modulated and transmitted as $\\sqrt{p} \\tilde{g_{y}^x}$ or $\\sqrt{p} \\tilde{w_{y}^x}$ simultaneously with other devices in all the clusters, where $|p_x|^2$ denotes the transmission power. Thus, the received signal at a server located at $z$ is $v_z = \\sum_{x\\in\\Phi_p} \\sum_{y \\in N_x} \\sqrt{p} \\tilde{s_{y}^x} |x+y-z|^{-\\alpha/2} f_{yz} + \\sum_{x'\\in\\Phi_p\\setminus\\{z\\}} \\sum_{y \\in N_{x'}} \\sqrt{p} \\tilde{s_{y}^{x'}} |x'+y-z|^{-\\alpha/2} f_{yz},$ (7)\nwhere the first term is the useful signal and the second is the inter-cluster interference. In (7), we ignore the receiver noise compared to the interference. Each device $y \\in N_x$ of cluster $x$ follows a truncated power allocation scheme [8], [9] as $p_y^x = \\frac{p}{|f_{yx}|^2}$ if $|f_{yx}|^2 \\geq \\theta_1$, and $p_y^x = 0$ if $|f_{yx}|^2 < \\theta_1$, where $p$ is the power allocation parameter and $\\theta_1$ is a threshold. We assume that the device knows this channel, the uplink channel to its server. In the power allocation, to meet a maximum average power $P_u$ in each device, we have $\\mathbb{E}\\{|p_x|^2\\} = \\mathbb{E}\\{\\frac{p^2}{|f_{yx}|^4} 1(\\|f_{yx}\\|^2 > \\theta_1)\\} < P_u$, where $\\mathbb{E}\\{\\frac{1}{|f_{yx}|^2} 1(\\|f_{yx}\\|^2 > \\theta_1)\\} = \\int_{\\theta_1}^{\\infty} \\frac{1}{t}e^{-t}dt = -Ei(\\theta_1)$. Thus, $p$ for all the devices can be selected as $p = \\frac{P_u \\theta_1 \\left(\\alpha+2\\right)\\left(R^{2+\\alpha}-r_o^{2+\\alpha}\\right)}{2 \\mathbb{E}i(\\theta_1) \\left(R^2-r_o^2\\right)}$.\nDownlink for intra-cluster iteration: As $\\mathbb{E}\\{v_x\\} = 0$, $\\forall x$, each server at a location $x \\in \\Phi_p$ normalizes its received signal $v_x$ with its variance, which is $\\mathbb{E}\\{\\|v_x\\|^2\\}$, as $\\frac{v_x}{\\sqrt{\\mathbb{E}\\{\\|v_x\\|^2\\}}}$. Then, all the servers transmit the normalized signals simultaneously. Therefore, the received signal at a reference device at $y_o$ in the reference cluster $o$ is\n$v_{y_o} = P_a \\sum_{x \\in \\Phi_p} \\frac{\\mathbb{E}\\{\\|v_x\\|^2\\}}{\\|x + y_o\\|^{\\alpha/2}} f_x \\frac{v_x}{\\sqrt{\\mathbb{E}\\{\\|v_x\\|^2\\}}},$ (8)\nwhere $P_a$ is the transmission power constraint of the servers. In general, the server can estimate $\\mathbb{E}\\{\\|v_x\\|^2\\}$ by taking measurements of the received signal over time and its entries and calculating the average power of those samples. However, the MCP-H modeling allows us to express $\\mathbb{E}\\{\\|v_x\\|^2\\}$ as a function of the network parameters and the power control. Specifically, from (7) and the power allocation\n$\\mathbb{E} \\{\\|v_x\\|^2\\} = p|A_x| + \\Psi,$ (9)"}, {"title": "V. EXPERIMENTAL RESULTS", "content": "The learning task over the collaborating clusters is the classification on the standard MNIST dataset with the parameter values given in Table 1, unless otherwise stated. The classifier model is implemented using a CNN, which consists of two 3 \u00d7 3 convolution layers with ReLU activation (the first with 32 channels, the second with 64), each followed by a 2\u00d72 max pooling; a fully connected layer with 128 units and ReLU activation; and a final softmax output layer. We consider both i.i.d. and non-i.i.d. distribution of dataset samples over the devices. For non-i.i.d. case, each device has samples of only two classes and the number of samples at different devices is different. The performance is measured as the learning accuracy with reference to the test dataset over global inter-cluster iteration count $t$. Each result is evaluated as the average of 10 realization.\nIn Fig. 2.a, the accuracy for different values of the number of collaborating clusters with the same task is studied in the non-i.i.d. scenario. It is observed that the multi-server case can significantly improve the accuracy compared to the single-server case. This is because the number of devices participating in the learning process increases, while the distortion decreases, according to the results of Section IV.\nIn Figs 2.b and c, the performance of MultiAirFed is compared with the conventional hierarchical FL in [4]\u2013[6] for both i.i.d. and non-i.i.d. distributions. We name this benchmark HierFed. In HierFed, the devices always upload the model parameters, and we consider $\\gamma = 2$ local descent steps in each intra-cluster iteration. We adopt the \"over-the-air\" scheme in Section IV for both MultiAirFed and HierFed. Additionally, we implement both of them with \"orthogonal\" transmissions, which eliminates interference by assuming unlimited resources. To better evaluate the impact of interference, we consider a dense network with $\\lambda_p = 40 \\textrm{Km}^{-2}$ for the i.i.d. case. The results indicate that MultiAirFed outperforms HierFed by a substantial margin in the non-i.i.d. scenario, for both transmission schemes. This highlights the robustness"}, {"title": "VI. CONCLUSIONS", "content": "We proposed a new two-level federated learning algorithm that offers resilience to interference and data heterogeneity in hierarchical wireless networks. To implement the proposed algorithm independent of the network scale and with minimum resource requirements, we proposed an over-the-air aggregation scheme for the uplink and a bandwidth-limited broadcast scheme for the downlink, and derived the effect of uplink and downlink interference on the aggregations, in particular with respect to the cluster density. We showed that the proposed hierarchical FL outperforms the existing solution, and the achieved accuracy is high, especially when the interference is high and the data is heterogeneous."}]}