{"title": "Connector-S: A Survey of Connectors in Multi-modal Large Language Models", "authors": ["Xun Zhu", "Zheng Zhang", "Xi Chen", "Yiming Shi", "Miao Li", "Ji Wu"], "abstract": "With the rapid advancements in multi-modal large language models (MLLMs), connectors play a pivotal role in bridging diverse modalities and enhancing model performance. However, the design and evolution of connectors have not been comprehensively analyzed, leaving gaps in understanding how these components function and hindering the development of more powerful connectors. In this survey, we systematically review the current progress of connectors in MLLMs and present a structured taxonomy that categorizes connectors into atomic operations (mapping, compression, mixture of experts) and holistic designs (multi-layer, multi-encoder, multi-modal scenarios), highlighting their technical contributions and advancements. Furthermore, we discuss several promising research frontiers and challenges, including high-resolution input, dynamic compression, guide information selection, combination strategy, and interpretability. This survey is intended to serve as a foundational reference and a clear roadmap for researchers, providing valuable insights into the design and optimization of next-generation connectors to enhance the performance and adaptability of MLLMs.", "sections": [{"title": "Introduction", "content": "With the remarkable advancements in large language models (LLMs) propelling progress towards general-purpose AI, there has been a significant growing focus on extending these models to multi-modal domains, leading to the development of multi-model large language models (MLLMs). The current mainstream MLLM architectures follow a similar paradigm [Zhang et al., 2024a]: the modality encoder, which compresses raw information, such as image or audio, into a more compact representation; the connector, which alleviates the gap between modalities, thus facilitating the adaptation of multi-modal inputs to LLMs; the LLM backbone, which generates the text responses in an auto-regressive manner; and the generator, which is optional for generating more modalities output besides text.\nRather than training from scratch, a common and logical approach is to start with a pre-trained encoder and a pre-trained LLM, aiming to enhance the efficacy of modality representation and mitigate computational expenses of LLM pre-training, respectively [Yin et al., 2023]. In the above paradigm, the connector plays a crucial role in aligning the multi-modal inputs into a consistent form and space [Cha et al., 2024]: On the one hand, the connector bridges modalities and the language model by translating modality features into tokens that the language model can understand. The quality of the conveyed tokens directly impacts the overall performance of the MLLM. On the other hand, compared to the fixed architectures of encoders and LLMs, the connector is more lightweight and can be easily improved, offering greater flexibility and efficiency.\nIn one of the pioneering works in MLLMs, LLaVA [Liu et al., 2024c], the connector is merely a simple linear layer that provides basic mapping functionality. As the demand for more powerful capabilities and the complexity of scenarios increased, advanced connectors incorporate various mechanisms, such as improved mapping [Liu et al., 2024b], compression based on spatial relation [Chen et al., 2023] or semantic perception [Li et al., 2023], and mixture of experts (MoE) [Li et al., 2025] to better handle the intricacies of multi-modal inputs. In addition, Dense Connector [Yao et al., 2024a] and Uni-Med [Zhu et al., 2024a] have optimized multi-layer feature utilization and multi-task joint learning conflicts by exploring connector design, respectively. The evolution reflects increasing recognition of the key role of connectors in improving the overall performance of MLLM and adaptability to various complex scenarios.\nMotivations. With the widespread application of MLLMs in various domains, the design of the connector is constantly updated to meet the increasing demands for more powerful capabilities and complex scenarios. Song et al. [2023] has explored modality alignment methods for MLLMs, due to time constraints, many methods and scenarios have not yet appeared and been discussed. Several studies [Yin et al., 2023; Zhang et al., 2024a] have conducted surveys on the entire flow of MLLMs, while offering relatively brief and insufficient coverage of connectors. The lack of a comprehensive survey on connectors makes it hard for researchers to establish a clear cognition of the underlying mechanisms of different connectors, thereby impeding the development of next-generation connectors. Therefore, we propose a more fine-grained taxonomy to systematically review and summarize the current status of connectors in MLLMs."}, {"title": "Taxonomy", "content": "To enhance our understanding of the dynamic evolution of connectors in MLLMs, we identify pivotal research endeavors, analyze their motivations, and succinctly encapsulate their primary technical contributions. As illustrated in Figure 1, this survey establishes a new taxonomy, which firstly examines the connector design of these works from two different perspectives, i.e., atomic operations in basic scenarios and holistic designs in complex scenarios. Then, we briefly introduce these two perspectives as follows:\n\u2022 Atomic connector operations refer to the basic components of MLLM connectors, which are designed as simple yet versatile units tailored to different functional requirements of basic scenarios. By utilizing these atomic operations, connectors can achieve mapping, compression, and expert integration. Furthermore, they can be combined to create more complex connectors, bridging the modality gap in a targeted and flexible way.\n\u2022 Holistic connector designs focus on the challenges of enhancing the capabilities of MLLMs in sophisticated scenarios from the multi-layer features, multi-encoder outputs to multi-modal contexts. Effective holistic designs help MLLMs consider the interplay between different layers of visual features, combine insights from different visual encoders, and handle diverse modalities in a coherent and efficient manner.\nIn the following sections, we present a comprehensive survey along the two perspectives and corresponding categories of our taxonomy for connectors in MLLMs. Due to some MLLMs possibly employing multiple methods in connectors, there may be instances of overlap among these models. Finally, based on the previous summary, we open the discussion for challenges and opportunities of future connector research, including high-resolution input, dynamic compression, guide information selection, combination strategy, and interpretability."}, {"title": "Atomic Connector Operations", "content": ""}, {"title": "Mapping", "content": "Since the LLM backbones are primarily trained on generic text, there is an inevitable semantic gap exists when dealing with multi-modal features. Mapping operations first flatten 2D or 3D features into 1D in a specific order and directly align the dimension of representations from other modalities with textual token embeddings.\nLinear. As the basic connector method, the simplicity of linear mapping makes it an attractive choice for initial feature transformation. LLaVA [Liu et al., 2024c] uses a linear projection matrix to connect the representation produced by the visual encoder to the LLM backbone. mPLUG-Owl3 [Ye et al., 2024] still adopts a linear layer to further process long visual sequence inputs, while Vitron [Fei et al., 2024] utilizes three independent linear mappings to process image, video, and sketch features, respectively.\nMLP. The multi-layer perceptron (MLP) consist of multiple linear layers connected by activation functions, introducing non-linearity into the feature transformation process. LLaVA-1.5 [Liu et al., 2024b] finds that changing from the original linear projection to MLP can improve LLaVA's multi-modal capabilities. Yi-VL [Young et al., 2024] and 3DMIT [Li et al., 2024b] use a two-layer MLP and a three-layer MLP with layer normalizations to align 2D image features and 3D object features, respectively.\nThe primary advantages of mapping operations lie in the simplicity and lightweight nature, which facilitates fast convergence during alignment training. However, mapping operations are limited in their ability to compress redundant information, which may lead to long token sequences that impose computational burden on the subsequent LLM backbone."}, {"title": "Compression", "content": "The complete set of tokens derived from other modalities encompasses both useful and redundant information. To achieve the optimal balance between information representation and the number of tokens, it is crucial to implement efficient compression operations through the connector. The current mainstream compression operations can be categorized into two main types: spatial relation and semantic perception."}, {"title": "Spatial Relation", "content": "The core idea behind spatial relation compression is rooted in observation: features from adjacent regions tend to be more similar in the original modality representation. Leveraging the spatial proximity, this category of compression operations can be further divided into several subcategories:\nSimple Operation. Pooling and dimensionality reduction via token concatenation and projection both serve as simple operations to compress the number of tokens. For example, MiniGPT-v2 [Chen et al., 2023] simply concatenates 4 adjacent visual tokens in the embedding space and projects them together into one single token with the textual embedding dimension. PLLaVA [Xu et al., 2024a] processes video features with the average pooling strategy to smooth the feature distribution along the temporal dimension. DeCo [Yao et al., 2024b] compresses N visual tokens by reshaping them into a 2D tensor of size $(N^{1/2}, N^{1/2})$, applying adaptive average pooling to reduce the size to $(M^{1/2}, M^{1/2})$, flattening into M tokens, and projecting through a linear layer to match the textual embedding dimension.\nCNN. Convolutional neural networks (CNNs) can preserve local spatial structures and enhance spatial understanding through hierarchical feature extraction. For instance, Honeybee [Cha et al., 2024] proposes C-Abstractor that consists of stacked ResNet blocks and adaptive pooling, injecting locality-aware design into the compression process. MM1 [McKinzie et al., 2025] adopts C-Abstractor as the connector, verifying the effectiveness of the connector design compared with average pooling and attention pooling.\nVariants. Considering that CNN introduces overly strict inductive biases for locality, Honeybee [Cha et al., 2024] also presents D-Abstractor, which enhances the locality-awareness while maintaining flexibility based on deformable attention. To avoid feature mismatch, MoME [Shen et al., 2024] proposes adaptive deformable transformation, which combines adaptive average pooling and deformable attention to obtain compressed and self-enhanced visual features."}, {"title": "Semantic Perception", "content": "Features with similar semantics always tend to exhibit stronger correlations across modalities. By focusing on semantic content rather than just spatial proximity, semantic perception compression aims to extract and preserve the most semantically relevant information while reducing redundancy. Leveraging semantic understanding, this category can be further divided into several subcategories:\nQ-Former. First proposed in BLIP-2 [Li et al., 2023], Q-Former introduces a fixed number of learnable tokens as queries, which retrieve relevant information from other modality representations through cross-attention. Since Q-Former involves more parameters and typically requires additional training, some studies use the pre-trained Q-Former to continue training for specific tasks. MiniGPT-4 [Zhu et al., 2023] utilizes the frozen Q-Former with a linear layer to enhance its visual understanding capabilities. PlanLLM [Yang et al., 2024] promotes video procedure planning by continuing to train Q-Former, integrating sample-specific visual state information and textual step knowledge.\nResampler. First introduced in Flamingo [Alayrac et al., 2022], resampler insert additional cross-attention layers between pre-trained and frozen LLM layers, whre the keys and values are obtained from the compressed vision features while the queries are derived from the language inputs. Voila-A [Yan et al., 2023] further adapts resampler by using the gaze heatmap features as the keys within the attention mechanism, optimizing feature expression. InfiMM [Liu et al., 2024a] keeps the resampler trainable in three-stage training strategies for vision-language alignment, visual question answering knowledge injection and unreshing conversation ability.\nVariants. To reduce the computational and memory complexity of the transformer, Q-Mamba [Eom et al., 2024] introduces a bidirectional Mamba layer to process visual features"}, {"title": "Mixture of Experts", "content": "Mixture of Experts (MoE) is a powerful architectural paradigm composed of two key components: a set of experts, each specializing in distinct aspects of the multi-modal features, and a router that dynamically selects and combines their outputs according to the input guidance information. With the shared goal of achieving optimal alignment of multi-modal features, compression operations focus on extracting a minimal number of tokens, while MoE aims to leverage the specialized representations of multiple experts. MoE's ability to capture a wide range of features and patterns makes it particularly suitable for the complex and varied nature of multi-modal data."}, {"title": "Vanilla MoE", "content": "Vanilla MoE refers to a standard MoE framework where each multi-modal token is independently processed by the router to compute expert assignment weights. CuMo [Li et al., 2025] employs four expert models, each consisting of a two-layer MLP. The router is a linear layer that computes expert weights based on each input token, applying a top-2 gating mechanism. This means that only the top two experts with the highest weights are selected for each token, allowing the connector to focus on the most relevant information while maintaining computational efficiency. ChartMoE [Xu et al., 2024b] adopts four experts of the linear layer with a top-2 gating router, achieving distinct preferences and effective processes in the fused information of background, table, json and code. To enhance surgical multi-modal understanding, SurgFC [Chen et al., 2024c] extends the vanilla MoE connector to the medical domain, utilizing MLP for both experts and router."}, {"title": "X-Guided MoE", "content": "X-guided MoE expands the input to the router beyond the individual multi-modal token itself. The improved router strategy of X-guided MoE can leverage additional information to make more informed routing decisions, thereby improving the connector's ability to dynamically allocate resources and select the most relevant experts."}, {"title": "Modality-Guided.", "content": "Modality information enables the connector to quickly classify tokens by their source modality, facilitating efficient expert allocation, especially in the case of input of multiple modalities. For example, OneLLM [Han et al., 2024] addresses the challenge of aligning eight modalities, including image, audio, video, point cloud, depth map, normal map, IMU and fMRI brain activity, to language within a unified framework. To facilitate modality switching, OneLLM introduces a set of learnable tokens for each modality. When tokens from different modalities are inputted, their corresponding modality tokens are concatenated with the input tokens and fed into the router."}, {"title": "Text-Guided.", "content": "For connectors, understanding the textual information such as instructions or descriptions corresponding to other modality tokens helps them understand how to convert tokens according to requirements. Q-MoE [Wang et al., 2024b] introduces a text-guided MoE connector, which routes different query tokens to pre-defined task experts by making use of cross-attention between the text representation and output of each expert."}, {"title": "Task-Guided.", "content": "In a unified multi-task learning framework, task-specific information provides critical context for the connector to classify tokens and determine the most suitable experts. Uni-Med [Zhu et al., 2024a] introduces a unified medical foundation model that employs C-MoE, a well-designed router which calculates routing weights based on concatenated visual and task-specific tokens and activates different experts for each task, efficiently addressing the multi-task interference problem in MLLMs."}, {"title": "Variant MoE", "content": "Given the expectation that different experts will exhibit distinct preferences for different types of tokens, experts in the MoE framework can also be designed differently, which has promoted the development of variant MoE. For example, the connector of V* [Wu and Xie, 2024] consists of two projection experts: a linear layer and a resampler. And V* designs a search algorithm as the router to flexibly switch between these two projection modules: 1) none searched target: the linear layer; 2) one or two searched targets: the linear layer for searched targets, the resampler for global image; 3) more than two searched targets: the resampler.\nThe inherent variability of multi-modal data necessitate a flexible and adaptive approach to feature transmission and integration. MoE provides this flexibility by dynamically allocating resources and selecting the most appropriate expert for different parts of the input, thereby enhancing the model's ability to handle diverse tasks and scenarios."}, {"title": "Holistic Connector Designs", "content": "Atomic connector operations have utilized single-layer features, typically the last or penultimate layer derived from the modality encoder, to explore and provide basic alignment functions. In contrast, holistic connector designs are proposed for more complex scenarios which are essentially multi-dimensional and heterogeneous in features. These scenarios arise from three main sources: 1) multi-layer features, extracted from various layers of the original encoder, each sharing the same dimensionality but offering different levels of abstraction; 2) multi-encoder features, obtained from different encoders, each with its own dimensionality and semantic focus; 3) multi-modal features, from additional modalities, further enriching the representation space. The core challenge lies in effectively integrating these diverse features to align with the textual embedding space."}, {"title": "Multi-Layer Scenario", "content": "Recent studies show that different layers of the same visual encoder capture distinct levels of abstraction and emphasize different regions of interest. Taking CLIP as an example, the shallow layer features focus on low-level details such as edges and textures, while the deep layer features are superior at high-level semantics such as objects and structures. In the multi-layer scenario, the connector aims to effectively integrate features from multiple layers of a single encoder before performing its alignment role. By leveraging the distinct strengths of different layers, ranging from low-level details to high-level semantics, the connector enhances feature representation through various fusion strategies, making alignment with textual embedding space more robust. As shown in Table 1, we list the representative connectors in the multi-layer scenario according to the different properties.\nLION [Chen et al., 2024a] proposes the vision aggregator which consists of two attention blocks to fuse the multi-level features from the selected [16, 32, 47] layers, facilitating the"}, {"title": "Multi-Encoder Scenario", "content": "Each encoder, trained on different datasets and optimized for specific tasks, provides a unique perspective on the input data. For instance, CLIP, pre-trained on text-image pairs using contrastive learning, excels in aligning textual and visual information, whereas DINO, trained on pure images, performs exceptionally well in object detection tasks. While the multi-layer scenario focuses on enhancing feature representations by combining different layers of a single encoder, the multi-encoder scenario takes this a step further by integrating more comprehensive and complementary features from multiple encoders. In the multi-encoder scenario, the connector aims to simultaneously achieve internal alignment of all encoder outputs and external alignment with the textual space. As shown in Table 2, we list the representative connectors in the multi-encoder scenario according to the multi-encoder selection and fusion strategy.\nAs one of the pioneer works in combining complement multiple encoders, COMM [Jiang et al., 2023] extracts features of all layers from CLIP and deep layers from DINOv2, then applies learnable coefficients to weight them separately before concatenating. Eyes Wide Shut [Tong et al., 2024b] utilizes a coefficient to control the portion of CLIP and DINOv2 in the A-MoF connector, while interleaving two kinds of features along"}, {"title": "Multi-Modal Scenario", "content": "Driven by the abundance of image-text and video-text paired data, current MLLMs primarily focus on vision-text multimodal tasks. However, the true advance towards general-purpose AI demands the capability to handle a variety of modalities simultaneously. Different modality-text pairs often exhibit significant differences in data distribution and representation spaces. A key challenge lies in effectively fusing information from these diverse modalities into a cohesive, unified representation space. In this multi-modal scenario, the connector is crucial: if not designed properly, the integration of information from different modalities can lead to interference, ultimately undermining the model's performance.\nAs illustrated in Figure 3, we summarize three fusion types of the connector in multi-modal scenario. The first type is early fusion, which requires a powerful and robust multi-modal encoder capable of effectively aligning diverse modalities into a unified space before they are fed to the connector. Therefore, the connector's task is relatively simple, as it only needs to align the unified space with the textual space. For example, PandaGPT [Su et al., 2023] adopts ImageBind as the unified encoder and only uses the linear projection to align the representations. The second type is intermediate fusion, which processes each modality through its own encoder before integrating the features at the connector. The connector should handle the challenging task of fusing and mapping features from different modalities, which often have significant dispari-"}, {"title": "Future Directions and Challenges", "content": "As a crucial and effective component in the realization of MLLMs, studies on connector frameworks and applications are advancing rapidly, giving rise to numerous challenges and opportunities. We have identified several critical challenges and potential areas for future studies.\nHigh-Resolution Input. To unlock the high-resolution image processing capabilities of MLLM and enhance fine-grained perception, current works such as InternVL 1.5 [Chen et al., 2024b] and HiRED [Arif et al., 2024] typically partition high-resolution images into low-resolution patches and rely on general image encoders and common connectors for processing. However, pre-partitioning may disrupt spatial relationships, potentially distorting visual information. Therefore, rethinking how connectors handle relationships and interactions between different partitioned patches presents a potential optimization direction.\nDynamic Compression. Previous works usually compress tokens using a fixed number or rate, which lacks the flexibility to adapt to varying input complexities and contextual demands. This rigidity can lead to suboptimal performance, as fixed-length or fixed-rate compression may either discard critical information from rich inputs or introduce unnecessary redundancy for simple data. FocusLLaVA [Zhu et al., 2024b] employs adaptive downsampling on different image patches based on local and global information, while DocKylin [Zhang et al., 2024b] utilizes K-means clustering to categorize and aggregate tokens through similarity-weighted summation. There remains significant room for developing more adaptive compression mechanisms that can dynamically adjust to the unique characteristics of each input.\nGuide Information Selection. Inspired by human cognition, where the brain selectively focuses on regions of interest based on guidance, connectors greatly benefit from guide information selection. This is exemplified in X-guided MoE connectors, which explore various guide information to determine expert weights. Future connector designs should consider to facilitate deeper interactions between guide information and token representations. For example, PPLLaVA [Liu et al., 2024d] employs a prompt-guided pooling module to extract video features relevant to user prompts. World Knowledge [Zhai et al., 2024] proposes an instruction-guided interac-"}, {"title": "Combination Strategy.", "content": "As discussed in Section 4, recent works have increasingly focused on using multi-layer or multi-encoder outputs to capture more comprehensive and multi-dimensional representations of the multi-modal information. While these approaches enhance the richness of the feature representations, they also introduce the challenge of information redundancy. Cambrian-1 [Tong et al., 2024a] presents the SVA module to aggregate features from multiple visual encoders. However, it requires repeated aggregation across multiple LLM layers to iteratively access and integrate necessary visual information. Eagle [Shi et al., 2024] has explored several combination strategies and surprisingly found that the simple channel concatenation was the most effective. These findings underscore the vast potential for improving combination strategies in connectors to balance the trade-off between representational richness and efficiency.\nInterpretability. While the connector is pivotal in bridging the gap between modalities, its effectiveness has not been fully explored from an interpretability perspective. Investigating interpretability can provide valuable insights into the inner mechanisms of the connector, guiding the design of more robust and efficient architectures while addressing potential weaknesses. For instance, MMNeuron [Huo et al., 2024] analyzes the distribution of domain-specific neurons across different modules, suggesting that the number of such neurons reflects their understanding capabilities. Deco [Yao et al., 2024b] uses R-GAE relevance maps to trace the flow of information from the generated textual tokens back to raw visual patches and intermediate connector output, revealing potential semantic deficiencies in the Q-former connector, such as the loss of fine-grained attribute and spatial locality. We believe that future research on the interpretability of connectors will play a key role in unleashing the full potential of MLLMs."}, {"title": "Conclusion", "content": "The development of connectors in multi-modal large language models has become a critical area of research as the field progresses towards general AI. In this survey, we aim to provide an in-depth overview of the connector in existing MLLMs. Firstly, we introduce a new taxonomy that categorizes connectors into atomic operations and holistic designs. Secondly, we systematically review the representative studies according to the taxonomy, highlighting the advancements in mapping, compression, and mixture of experts, as well as the holistic approaches for handling multi-layer, multi-encoder, and multi-modal scenarios. Finally, we discuss some challenges and highlight several future research directions. We hope this survey can serve as a useful reference for researchers, encouraging further exploration and innovation in the design of more efficient and robust connectors for MLLMs."}]}