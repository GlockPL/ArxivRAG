{"title": "Pruning-based Data Selection and Network Fusion for Efficient Deep Learning", "authors": ["Humaira Kousar", "Hasnain Irshad Bhatti", "Jaekyun Moon"], "abstract": "Efficient data selection is essential for improving the training efficiency of deep neural networks and reducing the associated annotation costs. However, traditional methods tend to be computationally expensive, limiting their scalability and real-world applicability. We introduce PruneFuse, a novel method that combines pruning and network fusion to enhance data selection and accelerate network training. In PruneFuse, the original dense network is pruned to generate a smaller surrogate model that efficiently selects the most informative samples from the dataset. Once this iterative data selection selects sufficient samples, the insights learned from the pruned model are seamlessly integrated with the dense model through network fusion, providing an optimized initialization that accelerates training. Extensive experimentation on various datasets demonstrates that PruneFuse significantly reduces computational costs for data selection, achieves better performance than baselines, and accelerates the overall training process.", "sections": [{"title": "Introduction", "content": "Deep learning models have achieved remarkable success across various domains, ranging from image recognition to natural language processing [1-3]. However, the performance of models heavily relies on the access of large amounts of labeled data for training [4]. In real-world applications, manually annotating massive datasets can be prohibitively expensive and time-consuming. Data selection techniques such as Active Learning (AL) [5] offer a promising solution to address this challenge by iteratively selecting the most informative samples from the unlabeled dataset for annotation. The goal of AL is to reduce labeling costs while maintaining or improving model performance. However, as data and modal complexity grow, traditional AL techniques that require iterative model training become computationally expensive, limiting scalability in resource-constrained settings.\nIn this paper, we propose PruneFuse, a novel strategy for efficient data selection in active learning setting that overcomes the limitations of traditional approaches. Our approach is based on model pruning, which reduces the complexity of neural networks. By utilizing small pruned networks for data selection, we eliminate the need to train large models during the data selection phase, thus significantly reducing computational demands. Additionally after the data selection phase, we utilize the learning of these pruned networks to train the final model through a fusion process, which harnesses the insights from the trained networks to accelerate convergence and improve the generalization of the final model.\nContributions. Our key contribution is to introduce an efficient and rapid data selection technique that leverages pruned networks. By employing pruned networks as data selectors, PruneFuse ensures computationally efficient selection of informative samples leading to overall superior generalization."}, {"title": "Background and Related Works", "content": "Subset Selection Framework. Active Learning (AL) is widely utilized iterative approach tailored for situations with abundant unlabeled data. Given a classification task with C classes and a large pool of unlabeled samples U, AL revolves around selectively querying the most informative samples from U for labeling. The process commences with an initial set of randomly sampled data s\u2070 from U, which is subsequently labeled. In subsequent rounds, AL augments the labeled set L by adding newly identified informative samples. This cycle repeats until a predefined number of labeled samples b are selected.\nData Selection. Approaches such as [6, 7, 5] aim to select informative samples using techniques like diversity maximization and Bayesian uncertainty estimation. Parallelly, the domain of active learning has unveiled strategies, such as [8, 9, 7, 10, 11, 6], which prioritize samples that can maximize information gain, thereby enhancing model performance with minimal labeling effort. While these methods achieve efficient data selection, they still require training large models for the selection process, resulting in significant computational overhead. Other strategies such as [12] optimize this selection process by matching the gradients of subset with training or validation set based on orthogonal matching algorithm and [13] performs meta-learning based approach for online data selection. SubSelNet [14] proposes to approximate a model that can be used to select the subset for various architectures without retraining the target model, hence reducing the overall overhead. However, it involves pre-training routine which is very costly and needed again for any change in data or model distribution. SVP [15] introduces to use small proxy models for data selection but discards these proxies before training the target model. Additionally, structural discrepancies between the proxy and target models may result in sub-optimal data selections. Our approach also builds on this foundation of using small model (which in our case is a pruned model) but it enables direct integration with the target model through the fusion process. This ensures that the knowledge acquired during data selection is retained and actively contributes to the training of the original model. Also, the architectural coherence between the pruned and the target model provides a more seamless and effective mechanism for data selection, enhancing overall model performance and efficiency.\nEfficient Deep Learning. Methods such as [16-23] have been proposed to reduce model size and computational requirements. Neural Network pruning has been extensively investigated as a technique to reduce the complexity of deep neural networks [18]. Pruning strategies can be broadly divided into Unstructured Pruning [24\u201327] and Structured Pruning [28\u201331] based on the granularity"}, {"title": "Prune Fuse", "content": "In this section, we delineate the PruneFuse methodology, illustrated in Fig. 1 (and Algorithm 1 provided in Appendix). The procedure begins with network pruning at initialization, offering a streamlined model for data selection. Upon attaining the desired data subset, the pruned model under-goes a fusion process with the original network, leveraging the structural coherence between them. The fused model is subsequently refined through knowledge distillation, enhancing its performance.\nWe framed the problem as, let $s_p$ be the subset selected using a pruned model $\u03b8_p$ and $s$ be the subset selected using the original model $\u03b8$. We want to minimize:\n$\\arg \\min_{s_p} | E_{(x,y)\\in s_p}[l(x,y; \\theta, \\theta_p)] \u2013 E_{(x,y)\\in D}[l(x, y; \\theta)]|$\nWhere $E_{(x,y)\\in s_p}[l(x,y; \\theta, \\theta_p)]$ is the expected loss on subset $s_p$ (selected using $\u03b8_p$) when evaluated using the original model $\u03b8$ and $E_{(x,y)\\in D}[l(x,y; \\theta)]$ is the expected loss on full dataset D when trained using the original model $\u03b8$. Furthermore, the subset can be defined as $S_p = \\{(x_i, y_i) \\in D | score(x_i, y_i; \u03b8_p) \u2265 \u03c4\\}$ where $score(x_i, y_i; \u03b8_p)$ represents the score assigned to each sample selected using $\u03b8_p$. The score function can be based on various strategies such as Least Confidence, Entropy, or Greedy k-centers. $\u03c4$ defines the threshold used in the score-based selection methods (Least Confidence or Entropy) to determine the inclusion of a sample in $s_p$.\nThe goal of the optimization problem is to select $s_p$ such that when $\u03b8$ is trained on it, the performance is as close as possible to training $\u03b8$ on the full dataset D. The key insight is that the subset $s_p$ selected using the pruned model $\u03b8_p$ is sufficiently representative and informative for training the original model $\u03b8$. This is because $\u03b8_p$ maintains a structure that is essentially identical to $\u03b8$, although with some nodes pruned. As a result, there is a strong correlation between $\u03b8$ and $\u03b8_p$, ensuring that the selection made by $\u03b8_p$ effectively minimizes the loss when $\u03b8$ is trained on $s_p$. By leveraging this surrogate $\u03b8_p$, which is both computationally efficient and structurally coherent with $\u03b8$, we can select most representative data out of D to train $\u03b8$."}, {"title": "Pruning at Initialization", "content": "Pruning at initialization [39] shows potential in training time reduction, and enhanced model general-ization. In our methodology, we employ structured pruning due to its benefits such as maintaining the architectural coherence of the network, enabling more predictable resource savings, and often leading to better-compressed models in practice. Consider an untrained neural network, represented as \u03b8. Let each layer $l$ of this network have feature maps or channels denoted by $c^l$, with $l \u2208 \\{1, ..., L\\}$. Channel pruning results in binary masks $m^l \u2208 \\{0,1\\}^{d^l}$ for every layer, where $d^l$ represents the total number of channels in layer $l$. The pruned subnetwork, $\u03b8_p$, retains channels described by $c^l \u2299 m^l$, where $\u2299$ symbolizes the element-wise product. The sparsity $p \u2208 [0, 1]$ of the subnetwork illustrates the proportion of channels that are pruned: $p = 1 - \\sum_e m^l / \\sum_e d^l$. To reduce the model complexity, we employ channel pruning procedure prune(C, p). This prunes to a sparsity $p$ via two primary functions: i) score(C): This operation assigns scores $z_e \u2208 \\R^{d^l}$ to every channel in the network contingent on their magnitude (using the L2 norm). The channels C are represented as $(C_1, ..., C_L)$. and ii) remove(Z, p): This process takes the magnitude scores $Z = (z_1, ..., z_L)$ and translates them into masks $m^l$ such that the cumulative sparsity of the network, in terms of channels, is $p$. We employ a one-shot channel pruning that scores all the channels simultaneously based on their magnitude and prunes the network from 0% sparsity to p% sparsity in one cohesive step. Although previous works suggest re-initializing the network to ensure proper variance [40]. However, since the performance increment is marginal, we retain the weights of the pruned network before training."}, {"title": "Data Selection via Pruned Model", "content": "We begin by randomly selecting a small subset of data samples, denoted as $s^0$, from the unla-beled pool $U = \\{X_i\\}_{i\u2208[n]}$ where $[n] = \\{1,...,n\\}$. These samples are then annotated. The pruned model $\u03b8_p$ is trained on this labeled subset $s^0$, resulting in the trained pruned model $\u03b8^*_p$. With $\u03b8^*_p$ as our tool, we venture into the larger unlabeled dataset $U$ to identify samples that are prime candidates for annotation. Regardless of the scenario, our method employs three dis-tinct criteria for data selection: Least Confidence (LC) [41], Entropy [42], and Greedy k-centers [6]. LC based selection gravitates towards samples where the pruned model exhibits the least confidence in its predictions. Thus, the uncertainty score for a given sample $x_i$ is defined as $score(x_i; \u03b8^*_p)_{LC} = 1 - \\max_{\\hat{y}} P(\\hat{y}|x_i; \u03b8^*_p)$. The entropy-based selection focuses on samples with high prediction entropy, computed as $score(x_i; \u03b8^*_p)_{Entropy} = \\sum_{\\hat{y}} P(\\hat{y}|x_i; \u03b8^*_p) \\log P(\\hat{y}|x_i; \u03b8^*_p)$, highlighting uncertainty. Subsequently, we select the top-k samples exhibiting the highest uncertainty scores, proposing them as prime candidates for annotation. The Greedy k-centers aims to cherry-pick k centers from the dataset such that the maximum distance of any sample from its nearest center is minimized. The selection is mathematically represented as $x^* = \\arg \\max_{x\u2208U} \\min_{c\u2208centers} d(x, c)$ where centers is the current set of chosen centers and $d(x, c)$ is the distance between point x and center c. While various metrics can be employed to compute this distance, we opt for the Euclidean distance since it is widely used in this context."}, {"title": "Training of Pruned Model", "content": "Once we have selected the samples from U, they are annotated to obtain their respective labels. These freshly labeled samples are assimilated into the labeled dataset L. At the start of each training cycle, a fresh $\u03b8_p$ is generated. Training from scratch in every iteration is vital to prevent the model from developing spurious correlations or overfitting to specific samples [15]. This fresh start ensures that the model learns genuine patterns in the updated labeled dataset without carrying over potential biases from previous iterations. The training process adheres to a typical deep learning paradigm. Given the dataset L with samples $(x_i, y_i)$, the aim is to minimize the loss function: $L(\u03b8_p, L) = \\frac{1}{|L|} \\sum_{i=1}^{|L|} L_i(\u03b8_p, x_i, y_i)$, where $L_i$ denotes the individual loss for the sample $x_i$. Training unfolds over multiple iterations (or epochs). In each iteration, the weights of $\u03b8_p$ are updated using backpropagation with an optimization algorithm like stochastic gradient descent (SGD). This process is inherently iterative as in AL. After each round of training, new samples are chosen, annotated, and the model is reinitialized and retrained from scratch. This cycle persists until certain stopping criteria, e.g. labeling budget or desired performance, are met. With the incorporation of new labeled samples at every stage, $\u03b8^*_p$ progressively refines its performance, becoming better suited for the subsequent data selection phase."}, {"title": "Fusion with the Original Model", "content": "After achieving the predetermined budget, the next phase is to integrate the insights from the trained pruned model $\u03b8^*_p$ into the untrained original model \u03b8. This step is crucial, as it amal-gamates the learned knowledge from $\u03b8^*_p$ with the expansive architecture of the original model, aiming to harness the best of both worlds.\nRationale for Fusion. Traditional pruning and fine-tuning methods often involve training a large model, pruning it down, and then fine-tuning the smaller model. While this is effective, it does not fully exploit the potential benefits of the larger, untrained model. The primary reason is that the pruning process might discard useful structures and connections within the original model that were not yet leveraged during initial training. By fusing the trained pruned model with the untrained original model, we aim to create a model that combines the learned knowledge by $\u03b8^*_p$ with the broader, unexplored model \u03b8."}, {"title": "Refinement via Knowledge Distillation", "content": "After the fusion process, our resultant model, $\u03b8_F$, embodies a synthesis of insights from both the trained pruned model $\u03b8^*_p$ and the original model \u03b8. Although PruneFuse outperforms baseline AL (results are provided in Appendix), we further optimize and enhance $\u03b8_F$ using Knowledge Distillation (KD). KD enables $\u03b8_F$ to learn from $\u03b8^*_p$ (the teacher model), enriching its training. During the fine-tuning phase, we use two losses: i) Cross-Entropy Loss, which quantifies the divergence between the predictions of $\u03b8_F$ and the actual labels in dataset L, and ii) Distillation Loss, which measures the dif-ference in the softened logits of $\u03b8_F$ and $\u03b8^*_p$. These softened logits are derived by tempering logits of $\u03b8^*_p$ which in our case is the teacher model, with a temperature parameter before applying the softmax function. The composite loss is formulated as a weighted average of both losses. The iterative enhancement of $\u03b8_F$ is governed by: $\u03b8_F^{(t+1)} = \u03b8_F^{(t)} - \u03b1(\u03bbL_{Cross Entropy}(\u03b8_F^{(t)}, L) + (1 \u2212 \u03bb)L_{Distillation}(\u03b8_F^{(t)}, \u03b8_p^*))$. Here \u03b1 represents the learning rate, while \u03bb functions as a coefficient to balance the contributions of the two losses. By incorporating KD in the fine-tuning phase, we aim to ensure that the fused model $\u03b8_F$ not only retains the trained weights of pruned model but also reinforce this knowledge iteratively, optimizing the performance of $\u03b8_F$ in subsequent tasks."}, {"title": "Experiments", "content": "Experimental Setup. The effectiveness of our approach is assessed on three image classification datasets; CIFAR-10 [43], CIFAR-100 [43], and TinyImageNet-200 [44]. We used ResNet-50,"}]}