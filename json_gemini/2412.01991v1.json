{"title": "Real-Time Multilingual\nSign Language Processing", "authors": ["Amit Moryossef"], "abstract": "Signed languages serve as a vital means of communication for millions of deaf\nand hard-of-hearing individuals worldwide. Utilizing a visual-gestural modal-\nity, they convey complex linguistic structures through manual articulations com-\nbined with non-manual elements like facial expressions and body movement.\nDespite their linguistic richness and cultural importance, signed languages have\noften been marginalized by the latest advances in text-centric artificial intelli-\ngence technologies, such as Machine Translation and Large Language Models.\nThis marginalization restricts access to these technologies for a significant pop-\nulation, leaving them behind in the rapid advancements in language-based AI.\nSign Language Processing (SLP) is an interdisciplinary field comprised of\nNatural Language Processing (NLP) and Computer Vision. It is focused on the\ncomputational understanding, translation, and production of signed languages.\nTraditional approaches have often been constrained by the use of gloss-based\nsystems that are both language-specific and inadequate for capturing the mul-\ntidimensional nature of sign language. These limitations have hindered the de-\nvelopment of technology capable of processing signed languages effectively.\nThis thesis aims to revolutionize the field of SLP by proposing a simple\nparadigm that can bridge this existing technological gap. We propose the use of\nSignWiring, a universal sign language transcription notation system, to serve as\nan intermediary link between the visual-gestural modality of signed languages\nand text-based linguistic representations.\nUnlike gloss-based approaches, our paradigm using SignWriting is designed\nto accurately capture the multidimensional and language-independent aspects\nof signed languages. This allows for the creation of a unified and scalable frame-\nwork that can accommodate the rich linguistic diversity found in various signed\nlanguages across the globe.\nWe contribute foundational libraries and resources to the SLP community,\nthereby setting the stage for a more in-depth exploration of the tasks of sign\nlanguage translation and production. These tasks encompass the translation\nof sign language from video to spoken language text and vice versa. Through\nempirical evaluations, we establish the efficacy of our transcription method as\na pivot for enabling faster, more targeted research, that can lead to more natural\nand accurate translations across a range of languages.\nOur paradigm establishes a clear boundary between NLP and Computer\nVision within the broader context of SLP. This division mirrors the existing sep-\naration between NLP and Signal Processing in the realm of spoken language\ntechnologies. By doing so, we open the door for more specialized research ef-\nforts in each sub-discipline, thereby enriching the ecosystem of technologies\nand methodologies available for SLP.\nThe universal nature of our transcription-based paradigm also paves the\nway for real-time, multilingual applications in SLP, thereby offering a more in-\nclusive and accessible approach to language technology. This is a significant\nstep toward universal accessibility, enabling a wider reach of AI-driven lan-\nguage technologies to include the deaf and hard-of-hearing community.\nIn summary, this thesis presents a new approach to Sign Language Process-\ning, one that aims to set a new standard for inclusive, real-time, and multilin-\ngual language technologies. By bridging the existing gap between text-centric\nAI and the visual-gestural world of signed languages, we substantially con-\ntribute toward making language-based AI universally accessible.", "sections": [{"title": "Introduction", "content": "Signed languages (also known as sign languages) are languages that use the\nvisual-gestural modality to convey meaning through manual articulations in\ncombination with non-manual elements like the face and body. They serve as\nthe primary means of communication for numerous deaf and hard-of-hearing\nindividuals. Similar to spoken languages, signed languages are natural lan-\nguages governed by a set of linguistic rules (Sandler and Lillo-Martin, 2006),\nboth emerging through an abstract, protracted aging process and evolving with-\nout deliberate meticulous planning. Signed languages are not universal or mu-\ntually intelligible, despite often having striking similarities among them. They\nare also distinct from spoken languages\u2014i.e., American Sign Language (ASL)\nis not a visual form of English but its own unique language.\nSign Language Processing (Bragg et al., 2019; Yin et al., 2021) is an emerging\nfield of artificial intelligence concerned with the automatic processing and anal-\nysis of sign language content. While research has focused more on the visual\naspects of signed languages, it is a subfield of both Natural Language Process-\ning (NLP) and Computer Vision (CV). Challenges in sign language processing\noften include machine translation of sign language videos into spoken language\ntext (sign language translation), from spoken language text (sign language pro-\nduction), or sign language recognition for sign language understanding.\nUnfortunately, the latest advances in language-based artificial intelligence,\nlike machine translation and personal assistants, expect a spoken language in-\nput (text or transcribed speech), excluding around 200 to 300 different signed\nlanguages (United Nations, 2022) and up to 70 million deaf people (World Health\nOrganization, 2021; World Federation of the Deaf, 2022).\nThroughout history, Deaf communities fought for the right to learn and use\nsigned languages and for the public recognition of signed languages as legiti-\nmate ones. Indeed, signed languages are sophisticated communication modal-\nities, at least as capable as spoken languages in all aspects, both linguistic and\nsocial. However, in a predominantly oral society, deaf people are constantly en-\ncouraged to use spoken languages through lip-reading or text-based communi-\ncation. The exclusion of signed languages from modern language technologies\nfurther suppresses signing in favor of spoken languages. This exclusion disre-\ngards the preferences of the Deaf communities who strongly prefer to communi-\ncate in signed languages both online and for in-person day-to-day interactions,\namong themselves and when interacting with spoken language communities\n(Padden and Humphries, 1988; Glickman and Hall, 2018). Thus, it is essential\nto make signed languages accessible.\nTo date, a large amount of research on Sign Language Processing (SLP) has\nbeen focused on the visual aspect of signed languages, led by the Computer Vi-\nsion (CV) community, with little NLP involvement. This focus is not unreason-\nable, given that a decade ago, we lacked adequate CV tools to process videos for\nfurther linguistic analyses. However, similar to spoken languages, signed lan-\nguages are fully-fledged systems exhibiting all the fundamental characteristics\nof natural languages, and existing SLP techniques do not adequately address\nor leverage the linguistic structure of signed languages. Signed languages in-\ntroduce novel challenges for NLP due to their visual-gestural modality, simul-\ntaneity, spatial coherence, and lack of written form. The lack of a written form\nmakes the spoken language processing pipelines - which often start with audio\ntranscription before processing - incompatible with signed languages, forcing\nresearchers to work directly on the raw video signal.\nFurthermore, SLP is not only intellectually appealing but also an important\nresearch area with significant potential to benefit signing communities. Bene-\nficial applications enabled by signed language technologies include improved\ndocumentation of endangered sign languages; educational tools for sign lan-\nguage learners; tools for query and retrieval of information from signed lan-\nguage videos; personal assistants that react to signed languages; real-time au-\ntomatic sign language interpretations; and more. Needless to say, in addressing\nthis research area, researchers should work alongside and under the direction of\ndeaf communities, and to benefit the signing communities' interest above all\n(Harris et al., 2009)."}, {"title": "(Brief) History of Signed Languages and Deaf\nCulture", "content": "Throughout modern history, spoken languages were dominant, so much so\nthat signed languages struggled to be recognized as languages in their own\nright, and educators developed misconceptions that signed language acqui-\nsition might hinder the development of speech skills. For example, in 1880,\na large international conference of deaf educators called the \u201cSecond Interna-\ntional Congress on Education of the Deaf\u201d banned teaching signed languages,\nfavoring speech therapy instead. It was not until the seminal work on American\nSign Language (ASL) by Stokoe Jr (1960) that signed languages started gain-\ning recognition as natural, independent, and well-defined languages, which in-\nspired other researchers to further explore signed languages as a research area.\nNevertheless, antiquated attitudes that placed less importance on signed lan-\nguages continue to inflict harm and subject many to linguistic neglect (Humphries\net al., 2016). Several studies have shown that deaf children raised solely with\nspoken languages do not gain enough access to a first language during their\ncritical period of language acquisition (Murray et al., 2020). This language de-\nprivation can lead to life-long consequences on the cognitive, linguistic, socio-\nemotional, and academic development of the deaf (Hall et al., 2017).\nSigned languages are the primary languages of communication for the Deaf\u00b9\nand are at the heart of Deaf communities. In the past, the failure to recognize\nsigned languages as fully-fledged natural language systems in their own right\nhas had detrimental effects, and in an increasingly digitized world, NLP re-\nsearch should strive to enable a world in which all people, including the Deaf,\nhave access to languages that fit their lived experience."}, {"title": "Thesis Overview", "content": "The dissertation asserts that for progress to be made in the area of sign language\nprocessing, it is vital to adopt a written phonetic lexical representation for sign\nlanguage, as an intermediary stage for any subsequent tasks.\nThis section provides an overview of the thesis structure and content, as well\nas the contributions made to the field of Sign Language Processing.\nPart I Introduces the field of Sign Language Processing to the reader.\n1. Chapter 1 situates this field within the broader context of artificial in-\ntelligence and machine learning research, outlines the main problem ad-\ndressed, and introduces signed languages and Deaf culture.\n2. Chapter 2 introduces the linguistic aspects of signed languages as natu-\nral languages, explains and demonstrates their representation, overviews\nthe existing types of available resources, and covers the various tasks in-\nvolved. Some of these tasks are further described in background sections\nwithin relevant chapters.\n3. Chapter 3 discusses some of the preliminary work carried out in prepara-\ntion for this thesis, with a focus on libraries designed to be widely used in\nsign language processing research.\nPart II Explores the use of universal phonetic sign language written forms as\nan intermediate representation for downstream sign language processing tasks,\nsuch as translation and production.\n1. Chapter 4 introduces the idea of utilizing written sign language represen-\ntations as an intermediate stage.\n2. Chapter 5 discusses some of the preliminary work carried out at the be-\nginning of this thesis, with a focus on relevant work published in sign\nlanguage research venues.\n3. Chapter 6 explores the application of lexical written representations as an\nintermediate phase for translation signed-to-spoken language translation.\n4. Chapter 7 compliments Chapter 6 by examining the usability of such an\nintermediate representation in the production process and assesses its ef-\nfectiveness in comparison to the use of semantic forms.\nPart III Wraps up the thesis by integrating the different components into a\nworking demonstration, and discussing key insights and contributions.\n1. Chapter 8 presents a sign language translation application that can trans-\nlate from and to multiple signed languages in real-time and offline. It\ndelves into the engineering, design, and development of this application.\n2. Chapter 9 discusses the implications of the findings in this thesis as they\nrelate to the field of Spoken Language Processing, and proposes a way for\nthe two fields to benefit from each other.\n3. Chapter 10 concludes the thesis by summarizing the main findings and\ncontributions, outlining potential avenues for future research directions\nin Sign Language Processing, and highlighting the potential impact of this\nwork on the deaf community."}, {"title": "(Brief) Sign Language Linguistics Overview", "content": "Signed languages consist of phonological, morphological, syntactic, and seman-\ntic levels of structure that fulfill the same social, cognitive, and communicative\npurposes as other natural languages. While spoken languages primarily chan-\nnel the oral-auditory modality, signed languages use the visual-gestural modal-\nity, relying on the signer's face, hands, body, and space around them to create\ndistinctions in meaning. We present the linguistic features of signed languages\u00b9\nthat researchers must consider during their modeling.\nPhonology Signs are composed of minimal units that combine manual fea-\ntures such as hand configuration, palm orientation, placement, contact, path\nmovement, local movement, as well as non-manual features including eye aper-\nture, head movement, and torso positioning (Liddell and Johnson, 1989; John-\nson and Liddell, 2011; Brentari, 2011; Sandler, 2012). Not all possible phonemes\nare realized in both signed and spoken languages, and inventories of two lan-\nguages' phonemes/features may not overlap completely. Different languages\nare also subject to rules for the allowed combinations of features.\nSimultaneity Though an ASL sign takes about twice as long to produce than\nan English word, the rates of transmission of information between the two lan-\nguages are similar (Bellugi and Fischer, 1972). One way signed languages com-\npensate for the slower production rate of signs is through simultaneity: Signed\nlanguages use multiple visual cues to convey different information simultane-\nously (Sandler, 2012). For example, the signer may produce the sign for \u201ccup\"\non one hand while simultaneously pointing to the actual cup with the other to\nexpress \"that cup.\" Similarly to tone in spoken languages, the face and torso\ncan convey additional affective information (Liddell et al., 2003; Johnston and\nSchembri, 2007). Facial expressions can modify adjectives, adverbs, and verbs;\na head shake can negate a phrase or sentence; gaze can help indicate referents.\nReferencing The signer can introduce referents in discourse either by pointing\nto their actual locations in space or by assigning a region in the signing space\nto a non-present referent and by pointing to this region to refer to it (Rathmann\nand Mathur, 2011; Schembri et al., 2018). Signers can also establish relations\nbetween referents grounded in signing space by using directional signs or em-\nbodying the referents using body shift or eye gaze (Dudis, 2004; Liddell and\nMetzger, 1998). Spatial referencing also impact morphology when the direc-\ntionality of a verb depends on the location of the reference to its subject and/or\nobject (de Beuzeville, 2008; Fenlon et al., 2018): For example, a directional verb\ncan move from its subject's location and end at its object's location. While the\nrelation between referents and verbs in spoken language is more arbitrary, ref-\nerent relations are usually grounded in signed languages. The visual space is\nheavily exploited to make referencing clear.\nAnother way anaphoric entities are referenced in sign language is by using\nclassifiers or depicting signs that help describe the characteristics of the referent\n(Supalla, 1986; Wilcox and Hafer, 2004; Roy, 2011). Classifiers are typically one-\nhanded signs that do not have a particular location or movement assigned to\nthem, or derive features from meaningful discourse (Liddell et al., 2003), so\nthey can be used to convey how the referent relates to other entities, describe\nits movement, and give more details. For example, to tell about a car swerving\nand crashing, one might use the hand classifier for a vehicle, move it to indicate\nswerving, and crash it with another entity in space.\nTo quote someone other than oneself, signers perform role shift (Cormier\net al., 2015), where they may physically shift in space to mark the distinction\nand take on some characteristics of the people they represent. For example, to\nrecount a dialogue between a taller and a shorter person, the signer may shift\nto one side and look up when taking the shorter person's role, shift to the other\nside and look down when taking the taller person's role.\nFingerspelling results from language contact between a signed language and\na surrounding spoken language (Battison, 1978; Wilcox, 1992; Brentari and Pad-\nden, 2001; Patrie and Johnson, 2011). A set of manual gestures corresponds\nwith a written orthography or phonetic system. This phenomenon, found in\nmost signed languages, is often used to indicate names, places, or new concepts\nfrom the spoken language, but has often become integrated into the language\nas another linguistic strategy (Padden, 1998; Montemurro and Brentari, 2018)."}, {"title": "Representation", "content": "Representation is a significant challenge for SLP. Unlike spoken languages, signed\nlanguages have no widely adopted written form. As signed languages are\nconveyed through the visual-gestural modality, video recording is the most\nstraightforward way to capture them. However, as videos include more in-\nformation than needed for modeling and are expensive to record, store, and\ntransmit, a lower-dimensional representation has been sought after.\nFigure 2.1 illustrates various signed language representations. In this demon-\nstration, we deconstruct the video into its individual frames to exemplify the\nalignment of the annotations between the video and representations."}, {"title": "Annotation Tools", "content": "ELAN - EUDICO Linguistic Annotator (Wittenburg et al., 2006) is an an-\nnotation tool for audio and video recordings. With ELAN, a user can add an\nunlimited number of textual annotations to audio and/or video recordings. An\nannotation can be a sentence, word, gloss, comment, translation, or description\nof any feature observed in the media. Annotations can be created on multiple\nlayers, called tiers, which can be hierarchically interconnected. An annotation\ncan either be time-aligned to the media or refer to other existing annotations.\nThe content of annotations consists of Unicode text, and annotation documents\nare stored in an XML format (EAF). ELAN is open source (GPLv3), and installa-\ntion is available for Windows, macOS, and Linux. PyMPI (Lubbers and Torreira,\n2013) allows for simple python interaction with Elan files.\niLex (Hanke, 2002) is a tool for sign language lexicography and corpus analy-\nsis, that combines features found in empirical sign language lexicography and\nsign language discourse transcription. It supports the user in integrated lexi-\ncon building while working on the transcription of a corpus and offers several\nunique features considered essential due to the specific nature of signed lan-\nguages. iLex binaries are available for macOS.\nSignStream (Neidle et al., 2001) is a tool for linguistic annotations and com-\nputer vision research on visual-gestural language data SignStream installation\nis available for macOS and is distributed under an MIT license.\nAnvil - The Video Annotation Research Tool (Kipp, 2001) is a free video an-\nnotation tool, offering multi-layered annotation based on a user-defined coding\nscheme. In Anvil, the annotator can see color-coded elements on multiple tracks\nin time alignment. Some special features are cross-level links, non-temporal ob-\njects, timepoint tracks, coding agreement analysis, 3D viewing of motion cap-\nture data and a project tool for managing whole corpora of annotation files.\nAnvil installation is available for Windows, macOS, and Linux."}, {"title": "Resources", "content": "Signed language resources come in multiple forms, such as bilingual dictionar-\nies, fingerspelling and isolated sign corpora, and continuous sign corpora. Each\nhas its own limitations, but they are all essential for translation and production\nin signed languages.\nBilingual dictionaries for signed language (Mesch and Wallin, 2012; Fen-\nlon et al., 2015; Crasborn et al., 2016; Gutierrez-Sigut et al., 2016) map a spoken\nlanguage word or short phrase to a signed language video. One notable dictio-\nnary, SpreadTheSign\u00b2 is a parallel dictionary containing around 25,000 words\nwith 42 different spoken-signed language pairs and more than 600,000 videos\nin total. Unfortunately, while dictionaries lexically map between languages,\nthey do not demonstrate the grammar or the usage of signs in context.\nFingerspelling corpora usually consist of videos of words borrowed from\nspoken languages that are signed letter-by-letter. They can be synthetically cre-\nated (Dreuw et al., 2006) or mined from online resources (Shi et al., 2018, Shi\net al. (2019)). However, they only capture one aspect of signed languages.\nIsolated sign corpora are collections of a limited vocabulary (20-1000 signs)\nof annotated single signs. They are synthesized (Ebling et al., 2018; Huang et al.,\n2018; Sincan and Keles, 2020; Hassan et al., 2020) or mined from online resources\n(Vaezi Joze and Koller, 2019; Li et al., 2020), and can be used for isolated sign\nlanguage recognition or contrastive analysis of minimal signing pairs (Imashev\net al., 2020). However, like dictionaries, they do not describe relations between\nsigns, nor do they capture coarticulation during the signing.\nContinuous sign corpora contain parallel sequences of signs and spoken\nlanguage. Available continuous sign corpora are extremely limited, containing\n4-6 orders of magnitude fewer sentence pairs than similar corpora for spoken\nlanguage machine translation (Arivazhagan et al., 2019). Moreover, while auto-\nmatic speech recognition (ASR) datasets contain up to 50,000 hours of record-\nings (Pratap et al., 2020), the most extensive continuous sign language corpus\ncontains only 1,150 hours, and only 50 of them are publicly available (Hanke\net al., 2020). These datasets are usually synthesized (Databases, 2007; Crasborn\nand Zwitserlood, 2008; Ko et al., 2019; Hanke et al., 2020) or recorded in studio\nconditions (Forster et al., 2014, Camg\u00f6z et al. (2018)), which does not account\nfor noise in real-life conditions. Moreover, some contain signed interpretations\nof spoken language rather than naturally-produced signs, which may not accu-\nrately represent native signing since translation is now a part of the discourse."}, {"title": "Real-World Data Collection", "content": "Data is essential to develop any data-driven technology, and current efforts in\nSLP are often limited by the lack of adequate data. We discuss the considera-\ntions to keep in mind when building datasets, the challenges of collecting such\ndata, and directions to facilitate data collection.\nWhat is Good Sign Language Data? For SLP models to be deployable,\nthey must be developed using data representing the real world accurately. What\nconstitutes an ideal sign language dataset is an open question; we propose the\nfollowing requirements: (1) a broad domain; (2) sufficient data and vocabulary\nsize; (3) real-world conditions; (4) naturally produced signs; (5) a diverse signer\ndemographic; (6) native signers; and when applicable, (7) dense annotations.\nTo illustrate the importance of data quality during modeling, Yin et al. (2021)\nfirst take as an example a current benchmark for SLP, the RWTH-PHOENIX-\nWeather 2014T dataset (Camg\u00f6z et al., 2018) of German Sign Language, that\ndoes not meet most of the above criteria: it is restricted to the weather domain\n(1); contains only around 8K segments with 1K unique signs (2); filmed in stu-\ndio conditions (3); interpreted from German utterances (4); and signed by nine\nCaucasian interpreters (5,6). Although this dataset successfully addressed data\nscarcity issues at the time and successfully rendered results and fueled com-\npetitive research, it does not accurately represent signed languages in the real\nworld. On the other hand, the Public DGS Corpus (Hanke et al., 2020) is an\nopen-domain (1) dataset consisting of 50 hours of natural signing (4) by 330 na-\ntive signers from various regions in Germany (5,6), annotated with glosses, and\nGerman translations (7), meeting all but two requirements we suggest.\nThey train a gloss-to-text sign language translation transformer (Yin and\nRead, 2020a) on both datasets. On RWTH-PHOENIX-Weather 2014T, they ob-\ntain 22.17 BLEU on testing; on the Public DGS Corpus, they obtain a mere 3.2\nBLEU. Although Transformers achieve encouraging results on RWTH-PHOENIX-\nWeather 2014T (Saunders et al., 2020c; Camg\u00f6z et al., 2020a), they fail on more\nrealistic, open-domain data. These results reveal that, for real-world applica-\ntions, we need more data to train such models. At the same time, available data\nis severely limited in size; less data-hungry and more linguistically-informed\napproaches may be more suitable. This experiment reveals how it is crucial to\nuse data that accurately represent the complexity and diversity of signed lan-\nguages to precisely assess what types of methods are suitable and how well our\nmodels would deploy to the real world.\nChallenges of Data Collection Collecting and annotating signed data in\nline with the ideal requires more resources than speech or text data, taking up\nto 600 minutes per minute of an annotated signed language video (Hanke et al.,"}, {"title": "Practicing Deaf Collaboration", "content": "Finally, when working with signed languages, it is vital to keep in mind who\nthis technology should benefit and what they need. Researchers in SLP should\nacknowledge that signed languages belong to the Deaf community and avoid\nexploiting their language as a commodity (Bird, 2020).\nSolving Real Needs Many efforts in SLP have developed intrusive meth-\nods (e.g., requiring signers to wear special gloves), which are often rejected by\nsigning communities and therefore have limited real-world value. Such efforts\nare often marketed to perform \u201csign language translation\" when they, in fact,\nonly identify fingerspelling or recognize a minimal set of isolated signs at best.\nThese approaches oversimplify the rich grammar of signed languages, promote\nthe misconception that signs are solely expressed through the hands, and are\nconsidered by the Deaf community as a manifestation of audism, where it is\nthe signers who must make the extra effort to wear additional sensors to be un-\nderstood by non-signers (Erard, 2017). To avoid such mistakes, we encourage\nclose Deaf involvement throughout the research process to ensure that we direct\nour efforts toward applications that will be adopted by signers and do not make\nfalse assumptions about signed languages or the needs of signing communities.\nBuilding Collaboration Deaf collaborations and leadership are essential\nfor developing sign language technologies to ensure they address the commu-\nnity's needs and will be adopted, not relying on misconceptions or inaccuracies\nabout signed language (Harris et al., 2009; Kusters et al., 2017). Hearing re-\nsearchers cannot relate to the deaf experience or fully understand the context\nin which the tools being developed would be used, nor can they speak for the\ndeaf. Therefore, we encourage creating a long-term collaborative environment\nbetween sign language researchers and users so that deaf users can identify\nmeaningful challenges and provide insights on the considerations to take while\nresearchers cater to the signers' needs as the field evolves. We also recommend\nreaching out to signing communities for reviewing papers on signed languages\nto ensure an adequate evaluation of this type of research results published at\nacademic venues. There are several ways to connect with Deaf communities for\ncollaboration: one can seek deaf students in their local community, reach out\nto schools for the deaf, contact deaf linguists, join a network of researchers of\nsign-related technologies\u00b3, or participate in deaf-led projects."}, {"title": "Tasks", "content": "So far, the computer vision community has primarily led the SLP research to\nfocus on processing the visual features in sign language videos. As a result,\ncurrent SLP methods do not fully address the linguistic complexity of signed\nlanguages. We survey common SLP tasks and current methods' limitations,\ndrawing on signed languages' linguistic theories.\nSign Language Translation (SLT) commonly refers to the translation of signed\nlanguage to spoken language (De Coster et al., 2022; M\u00fcller et al., 2022). Sign\nLanguage Production is the reverse process of producing a sign language video\nfrom spoken language text. Sign Language Recognition (SLR) (Adaloglou et al.,\n2020) detects and labels signs from isolated (Imashev et al., 2020; Sincan and\nKeles, 2020) or continuous (Cui et al., 2017; Camg\u00f6z et al., 2018, 2020b) sign\nlanguage videos.\nEvery edge to the left, on the orange background, represents a task in\ncomputer vision. These tasks are inherently language-agnostic; thus, they\ngeneralize between signed languages.\nEvery edge to the right, on the blue background, represents a task in natu-\nral language processing. These tasks are sign language-specific, requiring\na specific sign language lexicon or spoken language tokens.\nEvery edge crossing between backgrounds represents a task requiring a\ncombination of computer vision and natural language processing.\nThis graph conceptually defines 20 different tasks, with varying amounts of\nprevious research. Every path between two nodes that goes from left-to-right\nor right-to-left can be a valid pipeline of tasks. However, in this thesis, we make\nthe case that the most valid paths are paths with tasks that do not cross between\nthe modalities, and instead go through the \u2018Notation' representation.\nThe necessary background information for various tasks is disseminated\nthroughout this thesis, specifically in Part II. Following is a guide to direct you\nto where you can find an overview of each task's background.\nFor insights into Sign Language Activity Detection, refer to Section 5.1. An\noverview of Pose Estimation (Video-to-Pose), as well as Isolated Sign Recognition\n(Video-to-Gloss and Pose-to-Gloss), is available in Section 5.2. Section 5.3 delves\ninto previous work related to Gloss-to-Text translation. To understand Sign Lan-\nguage Sign and Phrase Segmentation, please consult Section 6.1. Section 6.2\nprovides the background for Pose-to-Notation sign language transcription. For\na comprehensive background on both Text-to-Notation and Notation-to-Text sign\nlanguage translation, visit Section 6.3. To explore the Text-to-Gloss and Gloss-to-\nPose tasks, refer to Section 7.1. Section 7.3 offers insights into Text-to-Pose and\nNotation-to-Pose tasks. Lastly, background information on the Pose-to-Video ani-\nmation task can be found in Section 7.4."}, {"title": "Fingerspelling", "content": "Fingerspelling is spelling a word letter-by-letter, borrowing from the spoken\nlanguage alphabet (Battison, 1978; Wilcox, 1992; Brentari and Padden, 2001; Pa-\ntrie and Johnson, 2011). This phenomenon, found in most signed languages,\noften occurs when there is no previously agreed-upon sign for a concept, like\nin technical language, colloquial conversations involving names, conversations\ninvolving current events, emphatic forms, and the context of code-switching\nbetween the signed language and the corresponding spoken language (Padden,"}, {"title": "Preliminary Work (Libraries)", "content": "In the emerging field of sign language processing, standardized approaches\nfor dataset distribution, loading, visualization, and augmentation remain un-\ndeveloped. Addressing this gap, we have prioritized two pivotal open-source\nprojects. First, pose-format (\u00a73.1) serves as a comprehensive library tailored to\nsign language processing, enabling users to easily read, write, visualize, and\naugment pose sequences. The second, sign-language-datasets (\u00a73.2), facilitates\nseamless integration of datasets regardless of their distribution or format, by\nproviding swift disk-mapped storing and loading of datasets. In tandem, pose-\nformat and sign-language-datasets have become the foundational tools for con-\ntemporary sign language processing research. We further introduce 3d-hands-\nbenchmark (\u00a73.3), a tool to evaluate the consistency and usefulness of hand pose\nestimation models in the context of sign language hand shapes."}, {"title": "pose-format: Library for Viewing, Augmenting,\nand Handling .pose Files (Moryossef et al., 2021a)", "content": "Managing and analyzing pose data is a complex task, with challenges ranging\nfrom handling diverse file structures and data types to facilitating effective data\nmanipulations such as normalization and augmentation. This paper presents\npose-format, a comprehensive toolkit designed to address these challenges\nby providing a unified, flexible, and easy-to-use interface. The library includes\na specialized file format that encapsulates various types of pose data, accom-\nmodating multiple individuals and an indefinite number of time frames, thus\nproving its utility for both image and video data. Furthermore, it offers seam-\nless integration with popular numerical libraries such as NumPy, PyTorch, and\nTensorFlow, thereby enabling robust machine-learning applications. Through\nbenchmarking, we demonstrate that our .pose file format offers vastly su-\nperior performance against prevalent formats like OpenPose, with added ad-\nvantages like self-contained pose specification. Additionally, the library in-\ncludes features for data normalization, augmentation, and easy-to-use visual-\nization capabilities, both in Python and Browser environments. pose-format\nemerges as a one-stop solution, streamlining the complexities of pose data man-\nagement and analysis.\nWorking with pose data introduces many complexities, from the diversity in file\nstructures to the variety of data types that need to be accommodated. Develop-\ners and researchers often find themselves juggling numerous data manipula-\ntion tasks such as normalization, augmentation, and visualization. In addition\nto these challenges, pose data itself can be inherently multidimensional, fre-\nquently encompassing multiple individuals and varying time frames. This cre-\nates an intricate ecosystem of variables that can be challenging to manage and\nanalyze effectively, which is important in fields like Sign Language Processing."}, {"title": "Background", "content": "In the context of our library, a pose consists of keypoints, which are 2D or"}]}