{"title": "Concept-Reversed Winograd Schema Challenge: Evaluating and Improving Robust Reasoning in Large Language Models via Abstraction", "authors": ["Kaiqiao Han", "Tianqing Fang", "Zhaowei Wang", "Yangqiu Song", "Mark Steedman"], "abstract": "While Large Language Models (LLMs) have showcased remarkable proficiency in reasoning, there is still a concern about hallucinations and unreliable reasoning issues due to semantic associations and superficial logical chains. To evaluate the extent to which LLMs perform robust reasoning instead of relying on superficial logical chains, we propose a new evaluation dataset, the Concept-Reversed Winograd Schema Challenge (CR-WSC), based on the famous Winograd Schema Challenge (WSC) dataset. By simply reversing the concepts to those that are more associated with the wrong answer, we find that the performance of LLMs drops significantly despite the rationale of reasoning remaining the same. Furthermore, we propose Abstraction-of-Thought (AoT), a novel prompt method for recovering adversarial cases to normal cases using conceptual abstraction to improve LLMs' robustness and consistency in reasoning, as demonstrated by experiments on CR-WSC.", "sections": [{"title": "1 Introduction", "content": "Reasoning serves as the cornerstone underpinning the efficacy and reliability of language models (Huang and Chang, 2023; Wang et al., 2024b). While Large Language Models (LLMs) have demonstrated remarkable proficiency in certain reasoning tasks (Wei et al., 2022), recent research has revealed that LLMs often experience issues with hallucinations and unreliable reasoning (Zhou et al., 2024; Ji et al., 2023; Huang et al., 2023) induced by semantic associations and superficial logical chain (Li et al., 2023a; Tang et al., 2023), especially under adversarial and long-tail scenarios (Sun et al., 2023). Despite numerous methodologies proposed to enhance LLMs' reasoning capabilities, such as Chain-of-Thought (CoT; Wei et al., 2023) and integration with auxiliary tools and model (Schick et al., 2023; Chai et al., 2023; Huang et al., 2024), the robustness of their reasoning process still remains a concern (Wang et al., 2023a; Havrilla et al., 2024; Valmeekam et al., 2023).\nIn this paper, we narrow down the scope of reasoning to the Winograd Schema Challenge (WSC), a classic reasoning challenge first introduced as an alternative to the Turing Test, which requires commonsense knowledge and reasoning ability to solve. A Winograd schema is a pair of sentences differing in one or two words with a highly ambiguous pronoun, resolved differently in the two sentences (Levesque et al., 2011). An example is in the top corner of Figure 1, formulated as a coreference resolution task. When introduced initially, these tasks posed great challenges for machines, being non-Google-proof \u2014 impossible to solve through simple word association using search engines (Levesque et al., 2011). However, due to its small scale and the scaling up of LLMs, such a non-Google-proof constraint is not considered hard anymore for LLMs, with GPT-3 achieving accuracies of 88.3% in the zero-shot setting (Brown et al., 2020)."}, {"title": "2 Method", "content": "To introduce a novel Turing Test that can robustly evaluate LLMs regarding commonsense reasoning, we present the Concept-Reversed Winograd Schema Challenge (CR-WSC). In addition to avoiding simple semantic associations of words, we create an adversarial dataset tailored specifically for LLMs, which is non-LLM-proof: challenging to solve with LLMs. Specifically, we first ask NLP experts to come up with different concept pairs that 1) has reversed attributes associated with the true answer (more semantically associated with the wrong answer), and 2) can cause a base LLM to give a wrong answer. For example, in Figure 1, we replace the \"father\u201d-\u201cson\u201d pair with \u201cbodybuilder\"-\"frail senior,\" such that the \"frail senior\" is more associated with the adjective \"weak\" in the context, which can lead an LLM to link the pronoun \"he\" to the senior instead of the bodybuilder. Next, we use the same idea to prompt an LLM to develop difficult entity pairs at scale, using our annotated data as exemplars. The generated answers are then manually verified.\nWhile LLMs may encounter challenges from the adversarial dataset, their capability to conceptualize reasoning entities offers a promising avenue for fostering unbiased reasoning (Minsky, 1980; Wang et al., 2021, 2024d). For example, by conceptualizing \u201cbodybuilder\u201d to a PersonX and \u201cfrail senior\" to a PersonY, LLMs will not be distracted by the adversarial word association and thus make the correct prediction.\nTo conclude, first, we propose CR-WSC, an adversarial WSC that requires the pairing entity to be non-LLM-proof. Second, we conduct evaluations using LLMs and find that CR-WSC is significantly harder than WSC, even though the reasoning rationale and logic behind it are the same. Third, we propose a robust prompting method, called Abstraction-of-Thought (AoT), to first abstract the adversarial question to a normalized reasoning question, thus facilitating robust reasoning. Experimental results show that AoT significantly improves reasoning performance and robustness."}, {"title": "2.1 Dataset Construction", "content": "While constructing datasets that are resistant to Google-proofing tactics avoids simple word associations, they prove relatively facile for contemporary QA systems. Take the following case from the original WSC, for instance:"}, {"title": "2.2 Abstraction-of-Thought", "content": "While QA systems often stumble when confronted with adversarial tasks, as illustrated in the aforementioned cases, there exists a promising avenue for improvement through abstraction. When humans tackle such problems, we don't focus on every detail; instead, we abstract ourselves to a certain level to perform reasoning (Minsky, 1980; Ho et al., 2019).\nFor instance, in Figure 1, we humans abstract both \"The bodybuilder\u201d and \u201cThe frail senior\" as their types. Subsequently, this abstracted representation serves as the foundation for addressing the original query, which is: \u201cPersonX couldn't lift PersonB because he was so weak, What does 'he' refer to?\" Since LLMs have been shown to be pretty robust and effective in performing abstraction or conceptualization (Wang et al., 2024a), this strategy can minimize the risk of reasoning errors stemming from confusing word associations.\nThe AoT process entails two key stages: Abstraction and Reasoning. Initially, instead of tackling the question head-on, LLMs are tasked with abstracting the query. This abstraction transforms the question into a more generalized and manageable form. Following this, the Reasoning phase commences, wherein LLMs engage in deductive processes to derive answers to the original tasks.\nBy adopting this dual-step approach, we empower LLMs to navigate complex reasoning tasks with greater efficacy, ultimately advancing the capabilities and robustness of QA systems in handling diverse challenges.\""}, {"title": "3 Experiment", "content": "In this section, we conduct a comprehensive array of experiments to validate the effectiveness of our proposed dataset and methods."}, {"title": "3.1 Comparison of CR-WSC and WSC", "content": "To assess the efficacy of the Concept-Reversed Winograd Schema Challenge (CR-WSC), we conduct a comparative analysis of QA system performance on both the CR-WSC and the original WSC. We employ two key metrics for this evaluation: Single Accuracy, which measures the ability of the QA system to provide correct answers, and Pair Accuracy, which assesses the system's capability to answer two questions within a single task, given the nature of pair sentences for the Winograd schema. We use ChatGPT (gpt-3.5-turbo-0301) and GPT4 (gpt-4-turbo-2024-04-09) as the backbone LLM and use zero-shot and one-shot prompting to acquire the results. We differentiate between datasets constructed by humans (CR-WSC-H) and those constructed by machines (CR-WSC-M). Results are summarized in Table 1. We can see that both single accuracy and pair accuracy on CR-WSC are significantly lower than that of the original WSC, underscoring the effectiveness of the CR-WSC in confusing LLMs. The result also highlights that LLMs may only memorize the WSC reasoning questions during pre-training instead of focusing on the genuine reasoning process because the reasoning rationales behind CR-WSC and WSC are the same."}, {"title": "3.2 Performance of Abstraction-of-Thought", "content": "To assess the efficacy of the Abstraction-of-Thought (AoT) methodology, we examine the performance of employing different prompts. We utilize three types of prompts: Zero-shot, one-shot, zero-shot CoT prompts (ZS CoT; Kojima et al., 2022), and CoT using manually written rational (CoT) and WinoWhy-provided rationale (WinoWhy; Zhang et al., 2020). Additionally, we experiment with the AoT method alongside the Concept-Reversed Winograd Schema Challenge (CR-WSC) examples. The results are presented in Table 2. We use three variants of language models: closed-sourced ChatGPT (gpt-3.5-turbo-0301), open-sourced Llama-3.1 (Meta-Llama-3.1-70B-Instruct-Turbo), and Mistral 7B (Mistral-7B-Instruct-v0.2)4 as the backbone LLM as representatives.\nUpon reviewing the outcomes in Table 2, it is evident that the single accuracy and pair accuracy metrics of the Abstraction-of-Thought (AoT) methods in both CR-WSC-H and CR-WSC-M datasets surpass those of the traditional methods. This underscores the effectiveness of AoT in enabling LM to abstract entities within tasks and steer clear of erroneous reasoning paths. The success of AoT lies in its ability to harness the conceptualization effectiveness of LLMs, enabling them to reframe adversarial scenarios into simpler reasoning representations, thereby enhancing reasoning integrity and robustness, ultimately fostering unbiased reasoning and advancing the capabilities of LLMs."}, {"title": "3.3 Comparison of Consistency", "content": "To delve deeper into the evaluation of QA systems, we explore their consistency in reasoning paths. Consistency here refers to the ability of a QA system to answer questions consistently using similar reasoning paths. If the LM consistently answers questions with similar reasoning paths correctly, it demonstrates mastery of the underlying reasoning in the given context. Let m represent the total number of groups with similar reasoning paths. Gi represent the i-th group. NG\u2081 and CG\u2081 represent the total number of QA pairs and the number of QA pairs in group Gi where the QA system consistently produces correct answers for all questions. The consistency store is thus defined as follows:\nConsistency = 1/m * \\sum_{i=1}^{m} \\frac{CGi}{NG}\nTo assess consistency, we group the five QA pairs generated by LLMs from the same original WSC example in CR-WSC-M together, where they are assumed to have the same reasoning rationale behind and calculate the percentage of the groups where LLMs can produce correct answers for all the questions in the group. The results are presented in Table 4.\nMethods with higher single accuracy and pair accuracy in Table 2 may exhibit lower consistency. This highlights the significance of incorporating consistency evaluation into the assessment of QA systems. Notably, the AoT method significantly improves consistency, suggesting that employing appropriate AoT techniques can enhance the overall consistency of QA systems."}, {"title": "4 Related Work", "content": ""}, {"title": "4.1 WinoGrad Schema Challenge", "content": "The Winograd Schema Challenge, formulated as a coreference resolution problem on pair sentences with minor distinctions, was originally proposed in Levesque et al. (2011). Given the small scale (273 examples), WinoGrande (Sakaguchi et al., 2021) was proposed to use crowd workers to collect Winograd-like questions at scale, leading to many high-quality supervision signals for improving LLM's commonsense reasoning ability. Wino-Grade introduced adversarial examples by AFLite. While AFLite made the dataset more complex, LLMs have now easily overcome this challenge. However, the CR-WSC framework takes a more rigorous approach using concept-instantiation-guided adversarial generation. By instantiating abstract concepts into concrete reasoning scenarios, CR-WSC significantly increases the difficulty of the tasks, leading to a marked drop in LLM performance. On top of WSC, there are also benchmarks focusing on explanation (Zhang et al., 2020), robustness (Jungwirth and Zakhalka, 1989; Hansson et al., 2021), and formal logics (He et al., 2021). Typical methods of tackling WSC include LLM prompting (Brown et al., 2020), knowledge retrieval (Emami et al., 2018), transfer learning from other QA datasets (Khashabi et al., 2020; Lourie et al., 2021), etc. Our work studies how to effectively and scalably acquire hard WSC instances from the original questions without changing the reasoning rationale."}, {"title": "4.2 Reasoning of LLMs", "content": "Besides zero-shot prompting and in-context learning (Brown et al., 2020), there are enhanced few-shot prompting using Chain-of-thought technique (Wei et al., 2023) by adding rationales before deriving the final answer. There are other improved techniques such as self-consistency (Wang et al.,"}, {"title": "4.3 Abstraction Ability of LLMs", "content": "The ability to perform abstraction in reasoning has been an active area of research for LLMs. Abstraction can assist LLMs in solving scientific problems by allowing them to think about the underlying theorems and principles behind the questions, thereby improving their reasoning capabilities in real-world problem-solving(Zheng et al., 2024). Existing research has demonstrated that incorporating abstraction can indeed enhance the reasoning ability of LLMs, and this has been validated in fine-tuning paradigms (Wang et al., 2024c). Improvements have been observed across various tasks, including question-answering (Wang et al., 2023b)."}, {"title": "5 Conclusion", "content": "To study whether LLMs only memorize the WSC questions or they can truly understand the reasoning behind them, we propose Concept-Reversed Winograd Schema Challenge (CR-WSC), a new dataset derived from WSC that adds a new non-LLM-proof constraint to involve entities that are more confusing to perform coreference resolution. Experimental results show that powerful LLMs fall short of CR-WSC, indicating a need for robust and generalizable reasoning algorithms. We also propose Abstraction-of-thought (AoT), as a novel prompting approach to normalize the adversarial questions to a normal one so that LLMs will not be distracted, which significantly improves the reasoning performance on CR-WSC."}, {"title": "Limitations", "content": "One limitation of the work is the reliance on human labour for the dataset construction. Annotators need to examine the entities and ensure they are reasonable to be included in the CR-WSC dataset. This approach requires significant human judgment and evaluation to ensure the accuracy and high quality of datasets as many well-used datasets, such as MMLU, Big-Bench, MMMU, etc(Hendrycks et al., 2021; Srivastava et al., 2023; Wang et al., 2022, 2023d; Yue et al., 2024).\nIn addition, the scale of CR-WSC is still limited to around 500 examples. We have tried to scale up by leveraging the data from WinoGrande, but according to our manual inspection, the non-Google-proof constraint was not always satisfied in WinoGrande in the first place, possibly because the annotators mostly focused on the Winograd formats instead of the subtle reasoning behind. This prevents us from deriving more confusing cases from WinoGrande. Future work can focus on data distillation from LLMs, where we can use test-time scaling (Snell et al., 2024; Choi et al., 2023) to check the subtle reasoning."}, {"title": "Ethics Statement", "content": "In our efforts to generate challenging and adversarial reasoning questions, we leverage entities with strong inherent characteristics. However, we recognize that such traits can sometimes be perceived as stereotypical; for instance, a senior individual might be depicted as weak, even though this is not necessarily accurate. Importantly, our dataset does not incorporate any racial or discriminatory features. Furthermore, the scalable generation process for our Concept-Reversed Winograd Schema Challenge Dataset (CR-WSC), executed by LLMs, has undergone meticulous manual verification to ensure the exclusion of biased or offensive content.\nWe employ a multi-layered approach to dataset creation to maintain ethical standards and avoid perpetuating stereotypes. Our team actively engages in reviewing and refining the dataset, ensuring that the content produced aligns with our commitment to fairness and inclusivity. This thorough oversight helps to identify and address any potential issues before they impact the final dataset. Addressing stereotypes and biases begins with their identification. Recognizing these issues is a crucial initial step, enabling individuals and organizations to devise strategies to mitigate them and foster more inclusive and equitable environments (Mehrabi et al., 2021b,a; Zhao et al., 2017).\nFurthermore, our research introduces the Abstraction-of-Thought (AoT) framework as a method for transforming adversarial questions within the CR-WSC dataset into more neutral and conceptually focused reasoning problems. By emphasizing conceptual reasoning over surface-level biases, AoT aids in preventing the reinforcement of stereotypes and biases in both the dataset and the resulting models.\nThis multi-pronged approach, combining manual verification and AoT techniques, demonstrates our commitment to creating high-quality, ethical, and unbiased datasets and AI systems."}, {"title": "A Data and Code", "content": "We have provided the necessary data and code in the supplementary materials, and we will make our code and data publicly available on GitHub after peer review."}, {"title": "B Prompts Used in M-CR-WSC", "content": "The prompts used in the M-CR-WSC are structured as follows:"}, {"title": "B.1 Prompt 1", "content": "Prompt 1\nCompare the following two sentences and answer the questions:\nThe bike passes the car because it is fast.\nThe bike passes the car because it is slow.\nThink about the property reflected by these sentences regarding the bike and the car. Provide two entities that share a similar relation to the bike and the car based on this property.\nAnswer:\nIn these sentences, the property highlighted is the speed difference between the bike and the car. Typically, a bike is slower than a car. Therefore, analogous entities are:\ntruck\nsports car\nThe analogous sentences would be:\nThe truck passes the sports car because it is fast.\nThe truck passes the sports car because it is slow."}, {"title": "B.2 Prompt 2", "content": "Prompt 2\nCompare the following two sentences and answer the questions:\nThe ring doesn't fit into the handbag because it is too large.\nThe ring doesn't fit into the handbag because it is too small.\nThink about the property reflected by these sentences regarding the ring and the handbag. Provide two entities that share a similar relation to the ring and the handbag based on this property.\nAnswer:\nIn these sentences, the property highlighted is the size difference between the ring and the handbag. Typically, a ring is smaller than a handbag. Therefore, analogous entities are:\npebble\nschoolbag.\nThe analogous sentences would be:\nThe pebble doesn't fit into the schoolbag because it is too large.\nThe pebble doesn't fit into the schoolbag because it is too small."}, {"title": "B.3 Prompt 3", "content": "Prompt 3\nCompare the following two sentences and answer the questions:\nThe body-builder doesn't lift the child because he is too heavy.\nThe body-builder doesn't lift the child because he is too light.\nThink about the property reflected by these sentences regarding the body-builder and the child. Provide two entities that share a similar relation to the body-builder and the child based on this property.\nAnswer:\nIn these sentences, the property highlighted is the weight difference between the body-builder and the child. Typically, a body-builder is heavier than a child. Therefore, analogous entities are:\nstrong man\nlittle boy.\nThe analogous sentences would be:\nThe strong man doesn't lift the little boy because he is too heavy.\nThe strong man doesn't lift the little boy because he is too light."}, {"title": "B.4 Prompt 4", "content": "Prompt 4\nCompare the following two sentences and answer the questions:\nThe elite students were bullying the undisciplined students, so we punished them.\nThe elite students were bullying the undisciplined students, so we rescued them.\nThink about the property reflected by these sentences regarding the elite students and the undisciplined students. Provide two entities that share a similar relation to the elite students and the undisciplined students based on this property.\nAnswer:\nIn these sentences, the property highlighted is the difference in behavior or discipline between the elite students and the undisciplined students. Typically, elite students are more disciplined compared to undisciplined students. Therefore, analogous entities are:\nlawyers\nhomeless individuals.\nThe analogous sentences would be:\nThe lawyers were bullying the homeless individuals, so we punished them.\nThe lawyers were bullying the homeless individuals, so we rescued them."}, {"title": "B.5 Prompt 5", "content": "Prompt 5\nCompare the following two sentences and answer the questions:\nThe fish eats the worm, and it is tasty.\nThe fish eats the worm, it is hungry.\nThink about the property reflected by these sentences regarding the fish and the worm. Provide two entities that share a similar relation to the fish and the worm based on this property.\nAnswer:\nIn these sentences, the property highlighted is the taste or satisfaction derived from eating the worm by the fish. Typically, the fish finds the worm either tasty or satisfies its hunger. Therefore, analogous entities are:\nring-necked pheasant\ngrasshopper.\nThe analogous sentences would be:\nThe ring-necked pheasant eats the grasshopper, and it is tasty.\nThe ring-necked pheasant eats the grasshopper, it is hungry."}, {"title": "C Prompts used in Experiment 3.2", "content": "The prompts we used in the experiment are as follows:"}, {"title": "C.1 Zero-Shot", "content": "Zero-Shot\n\"Q: Compare the two sentences and answer the questions\""}, {"title": "C.2 One-Shot", "content": "One-Shot\n\"Q: Compare the two sentences and answer the questions:\n1. The fish ate the worm. It was hungry. What does \"it\" refer to?\n2. The fish ate the worm. It was tasty. What does \"it\" refer to?\nSelect from [\"The fish\", \"The worm\"]\nA: 1. The fish. 2. The worm\""}, {"title": "C.3 WinoWHy", "content": "WinoWHy\n\"Q: Compare the two sentences and answer the questions\n1. The firemen arrived after the police because they were coming from so far away. What do \"they\" refers to?\n2. The firemen arrived before the police because they were coming from so far away. What do \"they\" refers to?\nSelect from [\"The firemen\", \"the police\"]\nIn the first sentence, the answer is the firemen since if they were coming from so far away then it's more likely they arrived after. In the second sentence, the firemen arrived before the police, so the police were farther away thus arriving late. Thus the answer is:\nA: 1. The firemen 2. the police\""}, {"title": "C.4 ZS CoT", "content": "ZS CoT\n\"Let's think step by step\""}, {"title": "C.5 CoT", "content": "COT\n\"Q: Compare the two sentences and answer the questions\n1. The fish ate the worm, it was tasty. What does \"it\" refer to?\n2. The fish ate the worm, it was hungry. What does \"it\" refer to?\nSelect from [\"fish\", \"worm\"]\nIn the first sentence, the worm is the main object that was eaten, the one that is eaten should be considered as tasty. In the second sentence, the fish was the one eating so it must be hungry. Thus the answer is:\nA: 1. worm 2. fish\""}, {"title": "C.6 AoT", "content": "AoT\n\"Q: Compare the two sentences and answer the questions\n1. The tasty fish ate the worm, it was tasty. What does \"it\" refer to?\n2. The tasty fish ate the worm, it was hungry. What does \"it\" refer to?\nSelect from [\"tasty fish\", \"worm\"]\nConceptualization:\nFish can be conceptualized as a predator, and worm can be conceptualized as a prey. The question can be conceptualized as:\n1. The predator ate the prey, it was tasty. What does \"it\" refer to?\n2. The predator ate the prey, it was hungry. What does \"it\" refer to?\nSelect from [\"prey\", \"predator\"]\nBecause the subject of \"ate\" should be hungry and the object should be tasty, so:\nAnswer: 1. prey. 2. predator\nConclusion: As worm is a prey, and fish is a predator in the context,\nA: Thus the answer is:\n1. worm 2. fish\""}, {"title": "D Other AoT Prompts", "content": "We also test the other prompts of AoT. The results are listed in the following table."}, {"title": "E Human Annotation", "content": "We introduce the details of the annotation process in this section. The annotators were divided into two groups to annotate the labels and availability of the data. Finally, we conducted cross-validation. Compared to the labels of the data, annotators are more likely to disagree on the availability of the data, such as whether the data is reasonable and its strength. However, this situation occurred in less than 7.5% of cases. In such cases, we directly discarded the data."}, {"title": "F Case Study", "content": "To deepen our understanding of LLM's reasoning errors and the AoT method, we provide examples of the CoT and AoT methods to compare how LLM applies these methods differently and examples where AoT fails.\nFirstly, we present examples of the CoT and AoT methods to compare how LLM applies these two methods. In CoT, LLM simply judges based on logical relationships or correlations, which can easily mislead it into providing incorrect answers. Conversely, by using AoT, LLM eliminates adversarial inputs, thereby enabling it to offer correct reasoning paths.\nIn the following example, LLM did not grasp the appropriate logical relationships and was misled by multiple \"reject\" inputs, resulting in a CoT reasoning error. However, after conceptualizing through AoT, LLM eliminated adversarial influences and provided accurate answers.\nFurthermore, we provide examples of AoT failures where, in some cases, it did not provide the appropriate level of abstraction, failing to eliminate adversarial issues and thereby failing to answer questions correctly. How to enable LLM to achieve the appropriate level of abstraction is a future research direction worth exploring.\nIn the following example, AoT conceptualized \"The bodybuilder\" as a strong individual and \"the frail senior\" as a physically weaker individual. This did not eliminate adversarial issues, leading LLM to still perform inaccurately when answering the question."}, {"title": "AoT Case", "content": "James asked the rejected person for a favor but he was refused.\nJames asked the rejected person for a favor but he refused.\nQ: What does 'he' refer to\nAnswer:\nConceptualization:\nIn these sentences, James is the one making a request, and the rejected person is the one being asked for a favor. The pronoun \"\"he\"\" can refer to either James or the rejected person depending on the context provided by the sentence.\n1. James asked the rejected person for a favor but he was refused. - Here, \"\"he was refused\"\" implies that the action of refusing was directed towards the subject making the request. Thus, \"\"he\"\" refers to James, who made the request and was refused.\n2. James asked the rejected person for a favor but he refused. - In this sentence, \"\"he refused\"\" indicates that the subject performing the action of refusing is the one who was asked for a favor. Therefore, \"\"he\"\" refers to the rejected person, who refused to grant the favor.\nConclusion: 1. James 2. the rejected person\""}, {"title": "CoT Case", "content": "James asked the rejected person for a favor but he was refused.\nJames asked the rejected person for a favor but he refused.\nQ: What does 'he' refer to\nAnswer:\nIn the first sentence, \"\"he was refused\"\" implies that James was the one who experienced refusal, suggesting that the rejected person refused James's request. Therefore, \"\"he\"\" refers to James.\nIn the second sentence, \"\"he refused\"\" indicates that the subject of the sentence, James, was the one who did the refusing. Therefore, \"\"he\"\" refers to James.\nA: 1. James 2. James"}, {"title": "AoT Failure Case", "content": "The bodybuilder couldn't lift the frail senior because he was so weak\nThe bodybuilder couldn't lift the frail senior because he was so heavy\nQ: What does 'he' refer to\nAnswer:\nConceptualization: The bodybuilder can be conceptualized as a strong individual, and the frail senior can be conceptualized as a physically weaker individual. The question can be conceptualized as:\n1. The strong individual couldn't lift the weak individual because he was so weak. What does \"\"he\"\" refer to?\n2. The strong individual couldn't lift the weak individual because he was so heavy. What does \"\"he\"\" refer to?\nSelect from [\"\"weak individual\"\", \"\"strong individual\"\"]\nBecause the subject of \"\"couldn't lift\"\" should be strong and the object should be weak, so:\nAnswer: 1. frail senior. 2. frail senior\nConclusion: As the frail senior is the weaker individual in the context, A: Thus the answer is: 1. frail senior 2. frail senior"}]}