{"title": "Inductive Linguistic Reasoning with Large Language Models", "authors": ["Raghav Ramji", "Keshav Ramji"], "abstract": "Evaluating large language models (LLMs) on their linguistic reasoning capabilities\nis an important task to understand the gaps in their skills that may surface during\nlarge-scale adoption. In this work, we investigate the abilities of such models to\nperform abstract multilingual reasoning through the lens of linguistic puzzles on\nextremely low-resource languages. As these translation tasks involve inductive and\ndeductive reasoning from reference instances, we examine whether diverse auxil-\niary demonstrations can be automatically induced from seed exemplars, through\nanalogical prompting. We employ a two-stage procedure, first generating analogi-\ncal exemplars with a language model, and then applying them in-context along with\nprovided target language exemplars. Our results on the modeLing dataset show\nthat analogical prompting is effective in eliciting models' knowledge of language\ngrammar similarities, boosting the performance of GPT-4o by as much as 8.1% and\nLlama-3.1-405B-Instruct by 5.9% over chain-of-thought approaches. These gains\nare attributable to the analogical demonstrations, both when self-generated as well\nas when produced by weaker multilingual models. Furthermore, we demonstrate\nthat our method generalizes to other tasks present in Linguistics Olympiad com-\npetitions, achieving sizable improvements across all problem types and difficulty\nlevels included in the LINGOLY dataset with GPT-40. We also report several find-\nings about interesting phenomena which drive linguistic reasoning performance,\nsuggesting that such puzzles are a valuable benchmark for new reasoning methods.", "sections": [{"title": "Introduction", "content": "As the capabilities of large language models (LLMs) continue to grow, it is necessary to develop ways\nof testing the boundaries of their ability to reason over a wide range of languages. Adapting language\nmodels to low-resource languages is challenging due to a lack of high-quality annotated data in the\ntarget language for supervised fine-tuning. This has led to zero-shot and few-shot transfer learning\napproaches being more commonly employed (Zoph et al., 2016; Nguyen & Chiang, 2017; Lin et al.,\n2019). However, given the emergence of the in-context learning phenomenon, we hypothesize that\nthis behavior can be used to enable few-shot generalization to new languages at inference time.\nIn this work, we explore the task of linguistic reasoning, using linguistics puzzles akin to the\nInternational Linguistics Olympiad (IOL). Notably, in these puzzles, the target language is often\nextremely low-resource or functionally extinct (Bean et al., 2024). While prior work has largely\nexamined the effect of vanilla in-context learning with English-target and target-English exemplars,\nchain-of-thought prompting, and traditional neural machine translation methods (Chi et al., 2024;\n\u015eahin et al., 2020), we believe that generating auxiliary exemplars can guide the model to more\neffectively learn grammar similarities over a language family. We introduce an approach based on\nanalogical prompting (Yasunaga et al., 2024), which uses strong language models to self-generate"}, {"title": "Analogical Prompting for Linguistic Reasoning", "content": "Analogical prompting (Yasunaga et al., 2024) avoids the need for annotated exemplars by relying\non a strong model to generate exemplars which are related to the test instance, but are sufficiently\ndiverse relative to one another and the test sample. Our approach of applying analogical prompts\nfollows the human system 2 thinking framework of slow, deliberate reasoning (Kahneman, 2011).\nIn chain-of-thought prompting for these puzzles, the model performs in-context learning with the\ngiven exemplars, learning the rules governing the language by induction, including the meaning\nof particular words, and using deduction to apply these rules to the test sample. This approach is\nsupported by prior works demonstrating the ability of LMs to learn rules and attempt to apply them\n(Qiu et al., 2024; Zhu et al., 2024). Furthermore, we do not have access to complete grounding\nsources of human-written rules governing these low-resource languages, so we must rely on the LM\nto identify and generate these rules itself. However, as we expect the model to have little to no prior"}, {"title": "Linguistics Puzzles", "content": "As noted before, the focus of this work is on linguistics puzzles \u2013 in particular, translation problems\nfrom English to a low-resource language and vice versa, given paired examples. Such problems are\nalso referred to in the literature as Rosetta Stone puzzles, and constitute one of the most frequent\ntypes of problems that appear in Linguistics Olympiad competitions (\u015eahin et al., 2020; Chi et al.,\n2024; Bean et al., 2024). These problems typically consist of a test phrase in language A along with\n5-10 exemplars of translation from language A to language B and vice versa, and the task is to\ntranslate the given phrase into language B."}, {"title": "Methods", "content": "We explore a number of sampling methods across various language models to assess their performance\non reasoning over unknown languages."}, {"title": "Evaluation Settings", "content": "We include the following methods as baselines for robust comparison to our method, reflecting prior\nwork examined in linguistic reasoning (Chi et al., 2024). We explore their results in Section 4.1."}, {"title": "Zero-Shot Prompting", "content": "Given the low-resource nature of the languages that we examine, we expect\nzero-shot performance to be poor, or even zero, on the exact match metric. However, we include this\nsetting for two reasons: (1.) a model which gets multiple questions correct for a given language with\nzero-shot prompting may be an indication of leakage, and (2.) this serves as a robust check on any\nadditional metrics examined aside from exact match."}, {"title": "Few-Shot Prompting / In-Context Learning", "content": "As in the Linguistic Olympiad competitions, demon-\nstrations of translation to and from the low-resource language are provided to the model, with the\nintention for inductive reasoning to guide the model towards identifying the set of grammar rules the\nlanguage follows."}, {"title": "Few-Shot Chain-of-Thought Reasoning", "content": "Given the efficacy of chain-of-thought prompting (Wei\net al., 2024; Kojima et al., 2022), we extend the few-shot evaluation setting by including prompts\nfor the model to \"think step-by-step\" (Kojima et al., 2022; Yang et al., 2024a). We also include\na chain-of-thought rationale exemplar for English-Spanish translation from (Chi et al., 2024), to\ndemonstrate how step-by-step reasoning rationales should be produced, and are denoted in Section 4\nas \"w/ rationale\"."}, {"title": "Analogical Prompting Variations", "content": "We describe the various analogical prompting methods explored in the experiments; their results are\nin Section 4.2."}, {"title": "Analogical Prompting on Language Families", "content": "We seek to use language families as a means to\nidentify similar, auxiliary languages whose exemplars can boost the model's cross-lingual under-\nstanding. In a similar environment to the Linguistics Olympiad competition, where one does not have\naccess to any external resources, we test the model on its latent understanding of language families\nand regional associations to generate further puzzles in another language within the same language\nsubgroup. For a target language $L$, we prompt the model to identify a few other languages (denote\nthis list $L_{Aux}$) in the same family as $L$; then, for each language in $L_{Aux}$, generate a puzzle translating\nfrom it to English, and a puzzle in the reverse direction. Then, we apply these exemplars along with\nthe given ones for $L$ in a new instruction to the model. We term this 2-stage analogical reasoning."}, {"title": "Inference-time Exemplar Distillation", "content": "In our work, inference-time distillation refers to generating\nanalogical exemplars with a strong model (e.g. GPT-4o) and applying them to a weak model (e.g.\nmodels with roughly 7-8B parameters). Our hypothesis driving this setting is: can higher quality\nexemplars produced by strong models enable better deductive abilities with weak models?"}, {"title": "Weak-to-Strong Cross-Lingual Analogies", "content": "Specialized multilingual models such as the Aya-23\nmodels hold promise for our linguistic reasoning analysis, as they have been fine-tuned for instruction-\nfollowing across a wide range of languages. We propose using such models for generating analogical\ndemonstrations, as they may have a stronger understanding of language families and can produce\ndiverse exemplars, which we believe strong models may be able to deduce from."}, {"title": "Experimental Setup", "content": "Datasets. We primarily evaluate our approaches on the modeLing dataset (Chi et al., 2024). This\ndataset consists of problems written by the authors and hence uninvolved in prior Linguistics\nOlympiads. This benchmark was released in 2024, and we rely on its recency to be more assured that\nleakage is not a factor driving performance. We note that all problems examined are purely text-based;\nwhile there exist linguistics puzzles that require deduction from images, filling in diagrams, etc., the\nbenchmark we evaluate on does not include such problems. This suggests that future work could study\nthe performance of multimodal models on these problem types. We also evaluate on the LINGOLY\ndataset (Bean et al., 2024), which features 1,133 problems and expands beyond \"Rosetta Stone\"\ntranslation problems detailed in Section 2.1 to include the Pattern (translation based on grammatical\npatterns), Match-up (matching translation pairs), Monolingual (text purely in an unknown language),\nComputational (identifying errors in machine translation), and Text (longer text in multiple, often\nhigher-resource languages) problem types. The results are included in Section 4.3.\nModels. We evaluate with the following models:\n\u2022 OpenAI models: GPT-4o, GPT-4, and GPT-3.5-turbo\n\u2022 Open-weight models: Llama 3.1 8B-Instruct, Llama 3.1 70B-Instruct, Llama 3.1 405B-\nInstruct (Dubey et al., 2024), Mixtral 8x7B-Instruct-v0.1 (Jiang et al., 2024), and Mixtral\n8x22B-Instruct-v0.1\n\u2022 Multilingual Instruction-tuned Models: Aya-23 8B and Aya-23 35B (henceforth referred to\nas Aya-8B and Aya-35B) (Aryabumi et al., 2024)\nOpenAI models are inferenced with the OpenAI API, while the open-weight and multilingual\ninstruction-tuned models are queried with the Together AI API and Apple MLX, respectively."}, {"title": "Results", "content": "We report exact match (EM) scores for all experiments performed. ChrF2 (Popovi\u0107, 2015), a\ncharacter n-gram F-score measure, and corpus-level BLEU scores (Papineni et al., 2002) are recorded\nin Appendix A. We do not treat these as primary metrics as BLEU ignores word ordering nuances\namidst short responses in machine translation, which is integral to measuring correctness in the\npuzzles we explore (Callison-Burch et al., 2006; Chi et al., 2024), and we find the ChrF scores to be\nnoisy relative to EM scores. Smaller models with weaker instruction-following capabilities often\nfailed to produce their output in the exact desired format specified in the prompts. To ensure that\nreliable exact match scores are reported while some responses may have parsing issues relative to\nthe expected format, the authors of this work manually examined each response to confirm whether\nthe output generated contains the target response. To enforce standardization across our evaluation\nprocedure, this was performed for all experiments; this was not applicable for stronger models whose\nresponses exactly followed the desired output format."}, {"title": "Chain-of-Thought Linguistic Reasoning", "content": "The results of baseline methods are in Table 1. The prompts for all experiments are included in\nAppendix F, and all experiments are averaged over 3 runs. For the \"CoT with rationale experiment\",\nwe take the best of using 512 and 4096 max tokens (see Appendix B). For the \"few shot\" results, we\ntake the best out of two different prompt settings, ablated on in Appendix C.\nWe report a few key observations below:\nBaseline Results and Zero-Shot Performance. Our strongest baseline result is achieved with\nLlama-3.1-405B-Instruct producing CoT rationales, at 65.81%. GPT-40 fails to exceed 60% on any\nsingle run. Among smaller models, Llama-3.1-8B-Instruct performs comparably to Aya-35B and\nMixtral-8x7B-Instruct, outperforming it on some baselines, which may be attributable to a stronger\nand more recent base model. We also observe that GPT-40 and Llama-3.1-405B-Instruct do indeed\nsolve a few puzzles (2 and 4 samples, respectively) in the zero-shot setting. Given the former was\nreleased before the modeLing dataset, and the latter was released just shortly after, we do not believe\nthis to be a sign of leakage; furthermore, each correct question was from a different language."}, {"title": "Two-Stage Analogical Reasoning", "content": "To critically explore the evaluation settings introduced in Section 3.1, we select 2 frontier models\n\u2013 GPT-40 and Llama-3.1-405B-Instruct \u2013 which were the strongest performers in our baselines. We\nselect 2 weaker models \u2013 Aya-35B and Llama-3.1-8B-Instruct \u2013 for the inference-time distillation\nand weak-to-strong prompting experiments. These models performed comparably to one another in\nthe baselines, and allow us to contrast multilingual specialization against a generalist model with\nmultilingual support. The experiments with Llama-3.1-8B-Instruct are included in Appendix I.\nWe also establish an upper bound on the performance we can attain with our approach, by a pseudo-\nopen-book method with oracle language families. That is, for each language in the evaluation set,\nrather than prompting the model to implicitly infer the language family and other languages which\nare a member of it, we abstract away the former by providing the language family in the prompt. We\nsuggest that a human expert with strong cross-lingual reasoning abilities would be able to deduce such\nrelationships with similar languages, so providing language family labels eliminates one uncertainty\nsource in the model's generations."}, {"title": "Related Work", "content": ""}, {"title": "Large Language Model Reasoning.", "content": ""}, {"title": "Few-shot Chain-of-Thought Reasoning.", "content": "In-context learning has emerged as an exciting phe-\nnomenon in language models, enabling them to learn from few-shot demonstrations at inference-time"}, {"title": "Inductive Reasoning in LLMs.", "content": "Inductive and deductive reasoning skills in language models have\noften been studied in the context of logical or abstract reasoning problems. Much of this prior work on\ninductive reasoning with language models studies evaluation settings with more clearly defined rules\nto be inductively learned and then applied; these works suggest gaps relative to the human intelligence\nin performing both inductive and deductive reasoning (Xu et al., 2024; Gendron et al., 2024; Yang\net al., 2024c). In particular, Yang et al. (2024c) notes the need for more challenging tasks in inductive\nreasoning to better assess the boundaries of LM capabilities, such as hypothesis generation and\npattern induction. Works such as Tang et al. (2023) demonstrate that models struggle to create rules\nby induction when the semantics of the exemplars do not follow in a commonsense manner; in our\nwork, generating analogical exemplars similar to the models pre-training data may steer the model\ntowards a relative \"commonsense\" representation of the rules underlying the exemplars.\nSeveral works dive into the realm of hypothesis search, determining the ability of LMs to pose\nhypotheses about the problem (e.g. rules which exemplars follow) before seeking to deductively\napply them (Zhu et al., 2024; Qiu et al., 2024; Wang et al., 2024). Zhu et al. (2024) propose\nhypotheses-to-theories (HtT), which learns a rule library from an induction stage, and then applies it\nby a deduction stage; this multi-stage method is similar to our analogical approach, although we still\nperform both induction and deduction together after analogical generation. Furthermore, their rule\nlibrary depends on verification \u2013 this is not possible in the linguistic reasoning task due to the lack of\na reliable feedback source to judge responses, aside from expert humans. As discussed earlier, Qiu\net al. (2024) demonstrates that models can propose rules well, but cannot consistently apply them.\nWang et al. (2024) proposes Hypothesis Search, a method which proposes hypotheses, implements a\nsubset of them as Python programs, and applies them to training samples to verify their correctness."}, {"title": "Exemplar Generation and Automated Reasoning.", "content": "Analogical prompting (Yasunaga et al., 2024)\nhas been demonstrated to be an effective inference-time method to produce diverse, task-conditioned\nexemplars, improving in-context learning. As noted above, this effectively serves as a knowledge\nretrieval method which retrieves exemplars similar to (or directly from) the pre-training distribution\nwhich the model has seen; RECITE (Sun et al., 2023) similarly retrieves passages directly from the\nmodel's memory. Methods such as SG-ICL (Kim et al., 2022) and Auto-ICL (Yang et al., 2024b)\nalso self-generate in-context exemplars in a similar manner as analogical prompting."}, {"title": "Multilingual Reasoning.", "content": "Multilingual reasoning in LMs for low-resource languages poses a unique challenge, as the pretraining\ncorpora and supervised fine-tuning datasets for many models are largely concentrated on a few high-\nresource languages. XLT (Huang et al., 2023) introduces a prompt template which translates problems\nin other languages to English and solves the problem with chain-of-thought in English. Qin et al.\n(2023) aligns each step in the chain-of-thought between the source language and English explanations,\nthen solves the problem given this alignment; they also apply self-consistency with cross-lingual\nalignments with a set of pre-specified target languages. Li et al. (2024) trains on code data with\nmultilingual comments, while using multilingual code prompts at inference time with symbolic\nfunction API calls as a structured way to solve the reasoning problem."}, {"title": "Linguistic Reasoning Benchmarks.", "content": "The PuzzLing Machines dataset (\u015eahin et al., 2020) first\nintroduced a set of Linguistics Olympiad problems to study the ability of language models to learn\nfrom a small amount of data; they apply RoBERTa-based neural machine translation methods, but\ndemonstrate a vast gap (attaining less than 4% exact match performance). With concerns of potential\nleakage given the vast web scraping performed in procuring pre-training tokens for language model\ntraining, modeLing (Chi et al., 2024) introduced a new set of hand-written Linguistics Olympiad"}, {"title": "Discussion", "content": "We propose applying analogical prompting as a test of inductive reasoning from diverse exemplars\nfor challenging linguistic puzzles. We find that language models can indeed follow grammar rule\nsimilarities to generate analogical exemplars, and attempt to apply them adeptly. This yields improved\nperformance in self-generated analogical prompting with GPT-40 and Llama-3.1-405B, as well as\nweak-to-strong prompting for those models employing Aya-35B-generated demonstrations. We also\nshow that Llama-3.1-405B-Instruct is the best current model for linguistic reasoning, as the first\nmodel to achieve over 70% on the modeLing benchmark with our approach. The gains achieved are\nattributable to the auxilary exemplars generated, which are in turn due to the model's understanding\nof language families and grammar rules from the vast pre-training data or its multilingual adaptation.\nFurthermore, the ability of smaller and specialized multilingual models (Aya) to generate coherent\nanalogical exemplars, which improve frontier models over their own self-generated exemplars, is\npromising towards developing widely-available multilingual reasoners. We find that the improvements\nobserved can be attributed to the auxiliary exemplars generated, which are in turn due to the model's\nunderstanding of language families and grammar rules from the pre-training data or its multilingual\nadaptation. The errors made by current models due to an inability to apply diverse and complex\nexemplars suggest that the linguistic reasoning task is an exciting and challenging evaluation setting\nfor LM reasoning at large. That is, seeking to emulate human reasoning, where deduction involves a\nclear application of recognized patterns, provides a ripe space for future work.\nThe interesting phenomenon identified with language isolates also provides a glimpse of model\ncapabilities to follow grammatical similarities, rather than relying on knowledge retrieval of language\nfamilies. That is, the multilingual language understanding abilities of frontier models expand beyond\ntypological knowledge, going so far as to create proxy fictitious languages which enable it to solve the\nproblem correctly. We suggest that future efforts in multilingual adaptation be placed in identifying\ntechniques to guide languages models to support typologically unique languages.\nResearch at the intersection of machine translation and reasoning is important from a societal\nperspective. With large language models being adopted widely, the need for multilingual capabilities\nand rapid adaptation grows, and our work proposes an effective method by which this can be\nperformed at test-time, notably demonstrating evidence that models can follow language similarities.\nThe errors made by current models due to an inability to deductively apply rules induced from\nexemplars suggest that the linguistic reasoning task is an exciting and challenging evaluation setting\nfor LM reasoning at large. That is, seeking to emulate human reasoning, where deduction involves a\nclear application of recognized patterns, provides a ripe space for future work. We hope that these\nfindings can inspire future models releases to include evaluation on challenging multilingual tasks\nsuch as these puzzles, and research on reasoning can explore the multilingual setting further in depth."}, {"title": "Limitations.", "content": "We note that the reliance on exact match scoring as our primary signal of performance\nis not ideal, as it is a binary indicator. We have sought to examine other metrics which correspond\nto \"partial credit\" such as ChrF2 and BLEU; however, there are flaws in these methods as well. A\nstronger human understanding of the rules which these extremely low-resource languages follow\ncould guide us to better metrics, especially capturing semantic meaning and word ordering inversions,\nwhere appropriate. For instance, some languages might retain the same meaning while inverting the\nword order - exact match is sensitive to this, and while ChrF2 and BLEU are not, we should only be\ninsensitive to ordering for languages which follow this property. We also recognized that the IOL\n2024 problems could not be used as a benchmark with our method, as they require multimodality\n\u2013 our method only analyzes unimodal text problems. Another limitation of our work is that we do\nnot have a reliable means of verifying the correctness of analogical exemplars, nor contrasting the\nquality of exemplars generated across models to determine the best analogical generator model. An\nexpert annotator who could identify where a mistake was made in the model's reasoning process\nalso would have been helpful to yield further insights into the fallacies of current models' linguistic\nreasoning.Nonetheless, our most effective deducer models are able to leverage exemplars generated\nby models of various sizes for improved linguistic reasoning."}, {"title": "Reproducibility Statement.", "content": "We include all prompts used for generating our baseline experimental\nresults, and all analogical prompting methods, in Appendix F. We have also broken down the two\nstages of our analogical reasoning method for clarity on how the method should be applied with two\nseparator models (e.g. weak-to-strong prompting, inference-time exemplar distillation). We evaluate\nour work on the modeLing dataset, which is publicly available. We have included details of the\nplatforms through which the models we evaluate have been queried (OpenAI API, TogetherAI API,\nApple MLX), along with the list of models studied."}]}