{"title": "On Explaining Knowledge Distillation: Measuring and Visualising the Knowledge Transfer Process", "authors": ["Gereziher Adhane", "Mohammad Mahdi Dehshibi", "Dennis Vetter", "David Masip", "Gemma Roig"], "abstract": "Knowledge distillation (KD) remains challenging due to the opaque nature of the knowledge transfer process from a Teacher to a Student, making it difficult to address certain issues related to KD. To address this, we proposed UniCAM, a novel gradient-based visual explanation method, which effectively interprets the knowledge learned during KD. Our experimental results demonstrate that with the guidance of the Teacher's knowledge, the Student model becomes more efficient, learning more relevant features while discarding those that are not relevant. We refer to the features learned with the Teacher's guidance as distilled features and the features irrelevant to the task and ignored by the Student as residual features. Distilled features focus on key aspects of the input, such as textures and parts of objects. In contrast, residual features demonstrate more diffused attention, often targeting irrelevant areas, including the backgrounds of the target objects. In addition, we proposed two novel metrics: the feature similarity score (FSS) and the relevance score (RS), which quantify the relevance of the distilled knowledge. Experiments on the CIFAR10, ASIRRA, and Plant Disease datasets demonstrate that UniCAM and the two metrics offer valuable insights to explain the KD process.", "sections": [{"title": "1. Introduction", "content": "Knowledge Distillation (KD) has emerged as a crucial technique in deep learning, especially in computer vision. It aims to develop efficient models without compromising performance [4, 5, 19, 31, 44]. By transferring knowledge from typically a complex Teacher model to a simpler Student model, KD can potentially address the increasing demand for deploying robust yet lightweight models in practical scenarios [43]. Nevertheless, despite its widespread adoption, the underlying mechanisms of KD remain somewhat opaque, thus impeding its broader application and theoretical comprehension.\nThe current research in knowledge distillation (KD) is confronted with four main challenges, including (1) understanding the specific knowledge that is transferred from Teacher to Student [3]; (2) evaluating whether KD improves the Student's focus on task-relevant features compared to independent training [5, 40]; (3) measuring the importance of features adopted or ignored by the Student for the target task; (4) addressing and resolving KD failures, mainly when there are significant architectural differences between Teacher and Student models [29,37,38].\nExisting visual explainability methods for Convolutional Neural Networks (CNNs), like Grad-CAM [35], are not equipped to tackle these KD-specific challenges. While effective for single-model predictions, these methods cannot capture the nuance of knowledge transfer between models or quantify the relevance of distilled knowledge. Specifically, Grad-CAM focuses on the importance of class-specific features within a single model. However, it does not distinguish between knowledge inherited from the Teacher and knowledge independently learned by the Student.\nTo address these issues, we introduce a new framework for improving the explainability of KD. We will first define the key terms used to this paper to ensure clarity. Distilled features are unique to the Student and are acquired through KD-based training, which the Student considers relevant to the task. Residual features are present in the Teacher (Base model) but are not adopted by the Student, as the Student finds them irrelevant to the task during KD-based training. We used Unique features to collectively refer to the distilled and residual features, as they are unique to the Student and the Base model, respectively. Throughout this text, we will use the term Base model to refer to the model which"}, {"title": "2. Related Work", "content": "Recent works on KD have focused on improving the performance of the Student [25], adapting the distillation process to specific tasks [13], or developing alternative methods for knowledge transfer [22]. In contrast to the performance-oriented works, some studies have explored KD explainability using various techniques. For instance, Cheng et al. [3] used information theory and mutual information to visualise and measure knowledge during KD. However, this method requires human annotations of the background and foreground objects, which limits its applicability and scalability. Moreover, using entropy to quantify randomness might be unreliable in scenarios with highly correlated data or multiple modes. Similarly, Wang et al. [42] used the KD and generative models to diagnose and interpret image classifiers, but this approach does not account for the knowledge acquired by the Student.\nExisting visual explainability techniques offer valuable insights into how CNNs make decisions (e.g., [2, 7]). Applying these methods to KD could reveal if the Student focuses on the same input areas as a Base model and learns similar or superior features. For example, DeepVID [40] visually interprets and diagnoses image classifiers through KD. Haselhoff et al. [20] proposed a probability density encoder and a Gaussian discriminant decoder to describe how explainers deviate from concepts' training data in KD. However, existing visual explainability techniques cannot visualise the saliency maps of distilled and residual features. Similarity metrics offer a promising strategy to measure and identify features unique to one model [45], which could be useful to effectively quantify and explain the distilled knowledge.\nSimilarity measures have been widely employed across various disciplines, including machine learning [1,8,10,11], information theory [6, 9, 16], and computational neuroscience [27]. These measures offer valuable insights into how information is processed and encoded in different contexts. In deep learning, similarity metrics have been useful to (1) quantify how DNNs replicate the brain's encoding process [41], (2) compare vision transformers and convnets [26, 33], (3) gain insights into transfer learning [14, 30], and (4) explain the mechanisms behind deep model training [18]. In this study, we propose a novel gradient-based visual explainability technique and quantitative metrics that leverage similarity measures to isolate"}, {"title": "3. Methodology", "content": "Given a Student model trained using KD and a Base model trained solely on data, our objective is to explain and quantify the amount of knowledge distilled during KD. Our approach leverages gradient-based explainability techniques to compute gradients with respect to input features, along with distance correlation (dCor) [39] and partial distance correlation (pdCor) [39, 45]. dCor measures the dependence between two random vectors that capture their multidimensional associations. Similarly, pdCor extends dCor to measure the association between two random vectors after adjusting for the influence of a third vector. It is computed by projecting the distance matrices onto a Hilbert space and taking the inner product between the U-centered matrices. Zhen et al. [45] used pdCor to condition multiple models and identify their unique features\u00b9, which means removing the common features and assessing the remaining ones. This enables us to introduce a novel visual explanation and metrics to assess the knowledge the student learned (distilled features) and that it may have overlooked (residual features)."}, {"title": "3.1. UniCAM: Unique Class Activation Mapping", "content": "Our goal is to generate saliency maps that highlight the distilled and residual features of the Student model, emphasising their importance and revealing their attention patterns to enable a deeper understanding of KD. Existing gradient-based visual explanations [2, 7,35] generates saliency maps based on the gradients of the target class, effectively revealing the relevance of features for the target prediction. However, these techniques are not suited for KD, as they do not identify the distilled or residual features compared to the Base model. To overcome this limitation, we introduce UniCAM, a novel gradient-based explainability technique tailored for KD. UniCAM leverages pdCor to adjust feature representations and remove the shared features between the Student and the Base model. This process isolates the distilled and residual features, which correspond to the knowledge the Student has acquired or overlooked, providing insights into the relevance of these features for the target task.\nLet $x_s$ and $x_b$ represent the features extracted from a specific convolutional layer of the Student and the Base model, respectively. The UniCAM method follows the following key steps: (1) First, we compute the pairwise distance matrices for both $x_s$ and $x_b$, which capture the relationships between different feature vectors within the Student and Base models. (2) Next, we normalise these distance matrices to create adjusted distance matrices, denoted as $P^s$ and $P^b$, which ensure the distance information is centred and standardised. (3) We then calculate the mutual influence between the Student and Base models' features and remove the shared components, effectively isolating the unique features that each model has learned. (4) Finally, we generate heatmaps of these unique features, which localise the importance and relevance of the distilled features compared to the Base model.\nFollowing the approach explained in [39], we first compute the pairwise distance matrix $D^{(s)} = (D^{(s)}_{i,j})$ for the Student's feature set. The pairwise distance matrix captures the relationship between every pair of feature vectors within the Student model:\n$D^{(s)}_{i,j} = \\sqrt{(x_i - x_j)^2 + \\epsilon},$   (1)\nwhere $\\epsilon$ is a small positive number added for numerical stability. This matrix helps us quantify how closely related the feature vectors are, which forms the basis for identifying unique and shared features.\nNext, we normalise the distance matrix using Eq. 2 to obtain the adjusted distance matrix $P^{(s)}$. This normalisation is a U-centred projection, which centres the matrix around the mean and adjusts it for the overall distribution of distances.\n$P^{(s)}_{i,j} = \\begin{cases} D^{(s)}_{i,j} + \\frac{1}{n-2} [-\\sum_{l=1}^n D^{(s)}_{i,l} - \\sum_{k=1}^n D^{(s)}_{k,j}] + \\frac{1}{(n-1)(n-2)} \\sum_{k=1}^n \\sum_{l=1}^n D^{(s)}_{k,l} & i \\neq j; \\\\ 0, & i = j. \\end{cases}$ (2)\nThis step ensures that the distance information is properly centred and scaled, making it easier to compare features across models. To isolate the unique features learned by the Student model, we adjust for the mutual influence between the Student and Base models. This step subtracts the shared features between the Student and Base model:\n$x_{s|unique} = P^{(s)} - \\frac{(P^{(s)}, P^{(b)})}{(P^{(b)}, P^{(b)})} P^{(b)}.$ (3)\nHere, we compute the inner product between the adjusted distance matrices of the Student and Base models, $(P^{(s)}, P^{(b)})$, which captures their shared information as:\n$(P^{(s)}, P^{(b)}) = \\frac{1}{n(n-3)} \\sum_{i\\neq j} (P^{(s)}_{i,j} P^{(b)}_{i,j}).$ (4)\nWe subtract the common features to isolate the unique ones in each model (distilled and residual features) and reflect what it has learned beyond the Base model."}, {"title": "3.2. Quantitative analysis of KD features", "content": "While visualising the heatmaps using UniCAM provides insights to make the KD process transparent, it is equally important to quantify the relevance and significance of the distilled and residual features. To this end, we introduce two novel metrics: Feature Similarity Score (FSS) and Relevance Score (RS). These metrics allow us to evaluate both the overall features learned by the Student compared to the Base model, as well as the distilled and residual features.\nTo compute these metrics, we first extract the relevant features from the salient regions identified by UniCAM. Next, we apply a perturbation technique proposed by Rong et al. [34], which modifies image pixels based on their prediction relevance. This perturbation preserves the most important pixels and replaces the irrelevant ones with the weighted average of their neighbours. As a result, the perturbed images retain the most salient features while reducing noise and redundancy."}, {"title": "3.2.1 Feature similarity score (FSS)", "content": "FSS is designed to quantify the degree of alignment between the features learned between the Student and Base model at a specific layer. Since the Student is trained with the guidance of the Teacher's knowledge, FSS provides insight into how much the Student's focus has shifted or aligned with the Base model features. A higher FSS value suggests that the Student and Base models are focusing on similar regions of the input, indicating that the Teacher's knowledge has not significantly altered the core feature focus of the Student or that the task is such that both models naturally converge on similar important features. Conversely, a lower FSS would suggest that the Student has diverged, potentially learning a more refined or generalised feature representation due to the knowledge from the Teacher. The FSS is computed as follows:\n$FSS = R^2(\\hat{x}_s, \\hat{x}_b) = \\frac{1}{k} \\sum_{i=1}^k dCor(\\hat{x}_s^{(i)}, \\hat{x}_b^{(i)}),$ (8)\nwhere k is the number of batches, $\\hat{x}_s$, and $\\hat{x}_b$, are the mini-batch features of the Student and Base model. FSS ranges from 0 to 1, where 0 means no similarity and values close to 1 indicate higher attention pattern similarity."}, {"title": "3.2.2 Relevance score (RS)", "content": "While FSS measures the similarity between the features learned by the Student and Base model, it does not quantify how relevant these features are to the target task. To address this, we propose the Relevance Score (RS), which evaluates the relevance of the distilled and residual features for the target task.\nTo capture the semantic information of the target task more effectively, we use a pre-trained BERT embedding of the ground truth labels [12]. Unlike traditional one-hot encodings, which provide limited information, BERT embedding represents the labels in a high-dimensional space that captures richer semantic relationships between different labels. This allows us to compute meaningful correlations between the feature vectors encoded by the models and the ground truth, offering a more robust measure of relevance for the task. Hence, we compute the RS as follows:\n$RS = R^2(\\hat{x}_s, gt) = \\frac{1}{k} \\sum_{i=1}^k dCor(\\hat{x}_s^{(i)}, gt_i),$ (9)\nwhere $\\hat{x}_s$ is the features extracted by the Student for each minibatch, and $gt_i$ is the ground truth BERT embeddings for the corresponding targets in each batch. To compute the RS for the Base model, we replace $\\hat{x}_s$, with $\\hat{x}_b$\nBoth FSS and RS provide a comprehensive quantitative technique to evaluate the similarity of the attention patterns and the relevance of the features learned during KD. This helps us understand whether the Student is acquiring features that are both similar and meaningful for the target task, offering deeper insights into explaining the KD process."}, {"title": "4. Experiments", "content": "We evaluate the proposed method on three public datasets for image classification: ASIRRA (Microsoft PetImages) [15], CIFAR10 [28] and Plant disease classification dataset [24]. ASIRRA contains 25,000 images of cats and dogs, while CIFAR10 contains 60,000 images of 10 classes. These datasets are widely used as benchmarks for image classification tasks and have different levels of complexity and diversity. Plant disease classification has a more challenging and realistic problem than fine-grained image classification, where the differences between classes are subtle and require more attention to detail.\nWe performed various experiments to analyse and explain the KD process. First, we used ResNet-50 [21] as both the Student and Teacher models, effectively making the Teacher a Base model. This allows us to isolate the effects of KD without introducing the complexity bias that can arise when using a more powerful Teacher model. It ensures that any observed differences are due to the KD process itself rather than architectural disparities between the Teacher and Student. This experiment addresses key questions (1)-(3) by analysing the performance of the Student and the Base model, the similarity in attention patterns, and the relevance of the distilled and residual features. In the second experiment, we analysed different combinations of ResNet-18, ResNet-50, and ResNet-101 as Teacher and Student models to address the fourth question, which explores the impact of architecture differences on KD. We applied our approach to three state-of-the-art KD methods for classification: response-based KD [23], overhaul feature-based KD [22], and attention-based KD [31]. We implemented the proposed method using PyTorch [32] and open source libraries from KD [36], pdCor [45] and Grad-CAM [17]."}, {"title": "4.2.1 Comparison of Student and Base model attention patterns", "content": "We hypothesise that KD enhances the Student model's ability to learn more relevant features while discarding irrelevant ones. To test this hypothesis, we begin by using Grad-CAM, a widely adopted explainability technique, to provide an initial qualitative comparison of the feature localisation in both the Student and Base model across different layers."}, {"title": "4.2.2 Visualising and quantifying distilled knowledge", "content": "Here, we use our proposed method, UniCAM, to visualise the distilled and residual knowledge during KD. The saliency maps generated using UniCAM show that KD is not a simple feature copying process from the Teacher to the Student but a guided training process where the Teacher's knowledge assists the Student to learn existing or new features. This is illustrated in Fig. 5, where the distilled features mainly focus on the primary object, whereas the residual features localise regions in the background or seemingly less relevant parts of the object. In certain cases, UniCAM does not highlight any part of the object in the Base model. This occurs because, after the removal of common features, the remaining features from the Base model are less significant or lack relevance to the target task. In the plant disease classification\u00b3, distilled features accurately identify segments of leaves essential for disease classification, demonstrating that KD helps models learn more relevant features."}, {"title": "4.2.3 Exploring the capacity gap impact", "content": "The Student's performance often declines when there is a large architecture (capacity) gap between the Teacher and the Student [29,38]. The drop in the Student's performance may stem from either its own challenges in learning relevant features or the overwhelming knowledge of the Teacher. To investigate this issue, we employ two distillation strategies in our experiments using ResNet-101 as the Teacher and ResNet-18 as the Student, which have a significant capacity disparity. In the first approach, we conduct direct distillation from ResNet-101 to ResNet-18. The second approach introduces an intermediate \u201cTeacher assistant", "Students": "ResNet-18 directly distilled from ResNet-101 (R18-R101) and ResNet-18 distilled from ResNet-101 through Teacher assistant ResNet-50 (R18-R50-R101). The saliency maps, visualised using UniCAM, reveal that the Teacher assistant helps learn more relevant features that highlight the object parts. In contrast, R18-R101 learns some irrelevant features and misses the salient features for the gt prediction. In fact,"}, {"title": "5. Discussion and Future Works", "content": "This paper presented novel techniques to explain and quantify the knowledge during KD. We proposed UniCAM, a gradient-based visual explanation method to explain the distilled knowledge and residual features during KD. Our experimental results show that UniCAM provides a clear and comprehensive visualisation of the features acquired or missed by the Student during KD. We also proposed two metrics: FSS and RS to quantify the similarity of the attention patterns and the relevance of the distilled knowledge and residual features. The proposed method has certain limitations. The experiments were exclusively conducted on classification tasks, which is one of the many potential applications of KD. In addition, we acknowledge the added computational cost introduced by the need to compute pairwise distances, gradients for feature localisation, and the proposed metrics. As part of future work, we aim to extend UniCAM to more complex datasets and explore its applicability to tasks beyond classification to enhance the robustness and versatility of the proposed method."}, {"title": "A. Additional Results: Explaining distilled and residual features on Plant Disease Dataset", "content": "In the experimental sections, we evaluated our proposed method to explain and quantify Knowledge Distillation (KD) on well-established datasets such as ASIRA and CIFAR10, showing its versatility and effectiveness in various scenarios. To further validate the generalisability of our method, especially for identifying distilled and residual features, we also applied it to the plant disease classification. This additional analysis confirmed the suitability of the proposed explainability technique in challenging datasets. In this section, we summarise the results, highlighting the ability of our method to detect salient features essential for diagnosing plant diseases and demonstrating its wide applicability to real-world problems.\nWe generate additional results to visualise distilled and residual features for plant disease images. Fig. A.1 shows that the Student model (ResNet-50) more accurately localises salient features compared to the Base model (ResNet-50). Specifically, the distilled features predominantly highlight regions relevant for accurate prediction, whereas the residual features tend to be distributed over areas irrelevant to the prediction. This implies that the Student model learns to ignore the features that are not useful for the prediction and focus on the salient parts. These saliency maps align with the findings presented in the main body of our work (in Fig. 4 and Fig. 6), explaining that the Student model consistently learns features of better relevance across different datasets compared to its equivalent Base model."}, {"title": "B. Distance and Partial Distance Correlation", "content": "Here, we provide the detailed steps of distance and partial distance correlation following Szelkely et al. [39]. Distance correlation (dCor) measures linear and nonlinear associations or dependence between two random vectors. For an observed random sample $(x, y) = (X_k, Y_k) : k = 1, . . ., n$ drawn from a distribution of random vectors, the empirical distance correlation $R(x, y)$ for n samples is derived from the distance covariance of the samples. To compute the distance covariance between the samples, we first compute the n by n distance matrices $(a_{j,k})$ and $(b_{j,k})$ containing all pairwise distances:\n$a_{j,k} = ||X_j - X_k ||, j, k = 1, 2, \u2026, \u03b7,$   $b_{j,k} = ||Y_j - Y_k ||, j, k = 1, 2, . \u00b7 \u00b7,\u03b7$ (B.1)\nwhere $||.||$ represents the Euclidean norm. Taking all the doubly centred distances as:\n$A_{j,k} := a_{j,k} - \\bar{a}_{j.} \u2013 \\bar{a}_{.k} + \\bar{a}_{..},$  $B_{j,k} := b_{j,k} - \\bar{b}_{j.} \u2013 \\bar{b}_{.k} + \\bar{b}_{..},$ (B.2)"}, {"title": "B.1. Theoretical Basis of Feature Subtraction in UniCAM", "content": "The subtraction operation in the formulation of UniCAM is used to remove the shared feature representations between the Student and the Base model or vice versa, identifying features unique to each model. This approach is conceptually similar to orthogonal projection in linear algebra, where a vector is decomposed into components: one that lies along a reference direction and another orthogonal to it. In this case, consider the features $x_s$ (Student features) and $x_b$ (Base model features). The shared features between $x_s$ and $x_b$ are represented by their projection:\n$proj_x (x_s) = \\frac{(x_s, x_b)}{(x_b, x_b)} x_b,$ (B.9)\nwhere (,) represents the inner product, and this term quantifies the component of $x_s$ aligned with $x_b$.\nNow, $x_s$ can be decomposed into two orthogonal components: 1. The component is aligned with $x_b$ (shared features): $proj_x (x_s)$. 2. The component orthogonal to $x_b$ (unique features): $x_{s|unique} = x_s - proj_x (x_s)$. Thus, the subtraction is valid and justified because:\n$x_s = proj_x (x_s) + (x_s - proj_x, (x_s)),$\nwhere $proj_x (x_s)$ identifies the shared features, and $x_s - proj_x (x_s)$ gives the unique features.\nIn UniCAM, we work in the transformed space of pairwise distance matrices and the features are adjusted for mutual influence using a U-centered distance matrix, $P^{(s)}$ and $P^{(b)}$. This provides a robust mechanism to capture the relational structure of the features rather than their absolute values. This approach is invariant to shifts or rotations in the feature space, and the analysis focuses on the geometric relationships between features. The shared features are calculated as follows:\n$Shared_x = \\frac{(P^{(s)}, P^{(b)})}{(P^{(b)}, P^{(b)})} P^{(b)}.$ (B.10)\nThe unique features of the Student, after removing the shared features, are:\n$x_{s|unique} = P^{(s)} \u2013 Shared_x.$ (B.11)\nThis subtraction extracts the component of $P^{(s)}$ orthogonal to $P^{(b)}$, preserving only the unique features of $x_s$ that do not exist in $x_b$. The operation is mathematically valid due to the properties of vector spaces, where such decomposition is meaningful in terms of orthogonal projections."}, {"title": "C. Steps on Feature Extraction", "content": "In this section, we explain the step-by-step feature extraction from the relevant regions (which was briefly introduced in Eq. 7, Section 3.2 in the main paper). Given a Base model or a Student, then we can extract the features as follows:\n$x = f(I \u2299 H)$ (C.1)\nwhere I is the input image, H is the saliency map generated using UniCAM, \u2299 is the element-wise multiplication operator, and f is a feature extraction function. We extract the features by applying perturbation technique [34] that modifies the input image I by replacing each pixel $I_{ij}$ with the weighted average of its neighbouring pixels in the highlighted region as follows:\n$\\Gamma_{ij} = \\sum_{k,l} w_{kl} I_{kl}$ (C.2)\nwhere $w_{kl}$ is a weight that depends on the relevance of pixel $I_{kl}$ for prediction. The relevance of each pixel is determined by the saliency map generated using UniCAM, which produces a heatmap H that assigns a value to each pixel based on its contribution to the relevant features learned by one model. The higher the value, the more relevant the pixel is. The weight $w_{kl}$ is proportional to H, such that:\n$w_{kl} = \\frac{H_{kl}}{\\sum_{kl} H_{kl}}$ (C.3)\nTherefore, we can write Eq. C.2 as:\n$\\Gamma_{ij} = \\sum_{k,l} \\frac{H_{kl}}{\\sum_{kl} H_{kl}} I_{kl}.$ (C.4)\nWe can simplify this equation by using element-wise multiplication and division operators and rewrite Eq. C.4 as follows:\n$I' = \\frac{I \u2299 H}{\\sum H}.$ (C.5)\nwhere $\\sum H$ is a scalar that represents the sum of all elements in H. This equation shows how we obtain a modified image $I'$ that contains only the features of interest for each model. To extract these features into a vector representation, we apply a feature extraction function f to $I'$:\n$x = f(I').$ (C.6)\nWe substitute Eq. C.5 into Eq. C.6 to obtain:\n$x = f(\\frac{I \u2299 H}{\\sum H}).$ (C.7)\nSince $\\sum H$ is a scalar, we can ignore it for feature extraction purposes, as it does not affect the relative values of the pixels. Therefore, we simplify Eq. C.7 to:\n$x = f(I \u2299 H).$ (C.8)\nThis is the step-by-step formulation to extract features from the regions identified as relevant using UniCAM or other gradient-based visual explainability."}]}