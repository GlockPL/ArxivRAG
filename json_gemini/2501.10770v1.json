{"title": "Enhancing Diagnostic in 3D COVID-19 Pneumonia CT-scans through Explainable Uncertainty Bayesian Quantification", "authors": ["Juan Manuel Liscano Fierro", "H\u00e9ctor J. Hort\u00faa"], "abstract": "Accurately classifying COVID-19 pneumonia in 3D CT scans remains a significant challenge in the field of medical image analysis. Although deterministic neural networks have shown promising results in this area, they provide only point estimates outputs yielding poor diagnostic in clinical decision-making. In this paper, we explore the use of Bayesian neural networks for classifying COVID-19 pneumonia in 3D CT scans providing uncertainties in their predictions. We compare deterministic networks and their Bayesian counterpart, enhancing the decision-making accuracy under uncertainty information. Remarkably, our findings reveal that lightweight architectures achieve the highest accuracy of 96% after developing extensive hyper-parameter tuning. Furthermore, the Bayesian counterpart of these architectures via Multiplied Normalizing Flow technique kept a similar performance along with calibrated uncertainty estimates. Finally, we have developed a 3D-visualization approach to explain the neural network outcomes based on SHAP values. We conclude that explainability along with uncertainty quantification will offer better clinical decisions in medical image analysis, contributing to ongoing efforts for improving the diagnosis and treatment of COVID-19 pneumonia.", "sections": [{"title": "1. Introduction", "content": "The rapid integration of artificial intelligence (AI) into the medical field, particularly in the domain of clinical imaging, has led to significant advancements in diagnostic capabilities Shenavarmasouleh et al. (2023). However, despite these technological strides, one of the major challenges that persists is ensuring the reliability and transparency of AI-driven diagnostic tools, especially in high-stakes environments like healthcare Arsenos et al. (2022, 2023). In the context of COVID-19 pneumonia, accurate classification of 3D CT scans is crucial for effective patient management, but existing methods often lack the ability to adequately quantify the uncertainty in their predictions\u2014a critical factor in clinical decision-making Li et al. (2024). Traditional neural networks, widely employed for medical image analysis, have achieved impressive accuracy in various diagnostic tasks Kollias et al. (2020, 2021, 2023); \u0397ou et al. (2022). Nonetheless, these models typically operate in a deterministic manner, providing single-point estimates without indicating the level of confidence in their predictions. This absence of uncertainty quantification can lead to overconfident and potentially misleading decisions, which is particularly concerning in medical settings where the cost of error is high Zahari et al. (2024). \u03a4\u03bf address this limitation, recent research has explored the use of Bayesian neural networks, which offer a probabilistic framework that not only produces predictions but also quantifies the associated uncertainty. Several studies have proposed methods such as Monte Carlo Dropout Zhao et al. (2024) Bargagna et al. (2023a) Zou et al. (2023) and Variational Inference Hu et al. (2023) to approximate Bayesian inference in neural networks, demonstrating their potential to improve the interpretability and"}, {"title": "2. Related work", "content": "The need to enhance the reliability and trustworthiness of AI models in healthcare, especially in medical image analysis, is a critical challenge An et al. (2021); Zhao et al. (2024); Bargagna et al. (2023b). While AI-driven diagnostic tools have made significant progress, the persistent issue of uncertainty and potential unreliability in model predictions remains a concern for clinical decision-making Zou et al. (2023); Hu et al. (2023); Abboud et al. (2024). Bayesian neural networks (BNNs) offer a promising solution by providing probabilistic interpretations of predictions, allowing clinicians to better assess prediction confidence Zhang et al. (2024).\nThe COVID-19 pandemic has brought renewed attention to medical image analysis, particularly in the classification of COVID-19 pneumonia using chest CT scans. Deep learning techniques, such as convolutional neural networks (CNNs), have shown great promise in this area, with successful applications in detecting intracranial hemorrhages Sharrock et al. (2021); Chang et al. (2018) and diagnosing COVID-19-related lung conditions Chen et al. (2021). However, deterministic neural networks, while effective, do not account for the uncertainty inherent in medical imaging, a limitation that BNNs can overcome.\nRecent studies underscore the potential of deep learning in improving diagnostic accuracy and streamlining clinical workflows Lundervold and Lundervold (2019); Shen et al. (2017). For instance, McKinney et al. McKinney et al. (2020) demonstrated that deep learning algorithms could detect breast cancer with accuracy comparable to experienced radiologists. Similarly, Ardila et al. Ardila et al. (2019)"}, {"title": "3. Bayesian Neural Networks", "content": "Bayesian neural networks (BNNs) are a type of neural network that incorporates Bayesian probability theory into the training and inference process. Unlike deterministic neural networks, which generate a fixed prediction given a set of inputs, BNNs generate a probability distribution over possible predictions (Chai, 2018). In Chai (2018) discusses the importance of decomposing predictive uncertainty by arguing that random and epistemic uncertainties tell us about different facets of an input value. BNNs use prior distributions over the network weights and learn a posteriori distributions $p(w/D)$ using Bayes' rule: $p(w|D) \\sim p(D|w)p(w)$, where $p(D|w)$ denotes the likelihood function representing the probability of the observed data D given the weights w, and p(w) representing the prior distribution of the weights. The posterior distribution represents the uncertainty in the weights given the observed data D = (X,Y) is usually approximated by variational inference methods Chai (2018); Heek (2018) or Markov chain Monte Carlo (MCMC) Neal (2012). Then, the posterior distribution is used to make predictions by taking the expectation of the ouput over the distribution. Once the calculation of the posterior distribution has been performed, the probability distribution of a new test example x* can be determined by Gal (2016)"}, {"title": "3.1. Variational Inference", "content": "Variational inference approximates the complex posterior distribution $p(w|D)$ with a simpler tractable distribution over the model weights, q(w), with variational parameters v. These variational parameters v are fitted so that q(w) approximates the desired posterior distribution $p(w|D)$. This fitted variational distribution is used to make the model predictions instead of the true posterior (Chai, 2018).\nOne way to measure the distance between the two probability distributions q(x) and p(x) is by using the Kullback-Leibler divergence or KL-divergence. Defined as\n$KL(q(x)||p(x)) = E_{q(x)} [log \\frac{q(x)}{p(x)} ] = \\int q(x)log \\frac{q(x)}{p(x)} dx.$\nTo make the variational distribution q(w) close to the posterior distribution $p(w|D)$, it is required to minimize the KL-divergence between these two distributions, obtaining the following Kendall and Gal (2017)\n$log p(D) > KL(q(w)||p(w)) + E_{q(w)} [log p(D|w)].$"}, {"title": "3.1.1. Variational Distribution", "content": "The essence of the variational distribution is that it is similar enough to the posterior weight distribution after divergence is minimized, but it is simpler to draw samples. A common form of variational distribution is the mean-field approximation, in which we assume that the variational distribution factorizes into the product of distributions by treating the weights as independent variables. Additionally, we can use a Gaussian distribution as a variational distribution, allowing easier sampling from a normal distribution instead of the exact weight posterior (Chai, 2018; Garcia-Farieta et al., 2024; Louizos and Welling, 2017a; Hortua et al., 2023). In this case, the variational distribution becomes\n$q(w/\\theta) = \\prod_{ij}N(W_{ij}; \\mu_{ij}, \\sigma_{ij}^2)$,\nwhere i and j are the indices of the neurons from the previous and current layer, respectively. Applying the reparametrization trick, we obtained $w_{ij} = \\mu_{ij} + \\sigma_{ij} * \\epsilon_{ij}$, where $\\epsilon_{ij}$ was drawn from the normal distribution. Furthermore, if the prior is also a product of independent Gaussians, the KL divergence between the prior and the variational posterior be computed analytically, which makes this approach computationally efficient (Gal and Ghahramani, 2016)."}, {"title": "3.2. Multiplicative normalising flows (MNF)", "content": "The Gaussian mean-field distribution described in the equation Eq.(4) is the most widely used family for posterior variation in BNNs. Unfortunately, this distribution lacks the ability to adequately represent the complex nature of the true posterior. Therefore, it is anticipated that improving the complexity of the variational posterior will produce substantial improvements in performance. This is attributed to the ability to sample from a more reliable distribution, which closely approximates the true posterior distribution. The process of improving the variational posterior requires efficient computational methods while ensuring its numerical feasibility. Multiplicative normalising flows (MNF) have been proposed to efficiently fit posterior distributions by using auxiliary random variables and normalising flows Louizos and Welling (2017b). Mixture normalising flows (MNF) suggest that the variational posterior can be represented mathematically as an infinite mixture of distributions Louizos and Welling (2017b)\n$q(w|\\theta) = \\int q(w|z, \\theta)q(z|\\theta)dz,$\nbeing $\\theta$ the learnable posterior parameter, and $z \\sim q(z|\\theta) = q(z)$ the vector with the same dimension as the input layer, which plays the role of an auxiliary latent variable. Furthermore, by allowing local reparametrizations, the variation posterior for fully connected layers becomes\n$w \\sim q(w/z) = \\prod_{ij}N(W; z_{ij}\\mu_{ij}, \\sigma_{ij}^2)$.\nThe flexibility of the variational posterior can be increased by improving the complexity of q(z). This can be done using normalising flows since the dimensionality of z is much lower than the weights. Starting from samples $z0 \\sim q(z0)$ from fully"}, {"title": "3.3. Multiplicative normalising flows in a voxel-grid representation", "content": "Garcia-Farieta et al. (2024) present the result of the generalization of Eq.(6) towards 3D convolutional layers. Starting with the extension of the variational posterior as\n$w \\sim q(w/z) = \\prod_{i}^{Dd}\\prod_{j}^{Dh}\\prod_{k}^{Dw}\\prod_{l}^{Df}N (W; z_{ijkl}\\mu_{ijkl}, \\sigma_{ijkl}^2)$,\nwhere $D_h$, $D_w$ and $D_d$ are the 3 spatial dimensions of the boxes, and Df is the number of filters for each kernel. The objective is to address the challenge of improving the adaptability of the approximate posterior distribution of weights coming from a 3D convolutional layer. The algorithm that describes the procedure for forward propagation of each 3D convolutional layer is found in (Garcia-Farieta et al., 2024)."}, {"title": "4. Calibration Methods", "content": "Guo et al. (2017) states that modern deep neural networks are often uncalibrated. As a result, interpreting predicted numbers as probabilities is incorrect. Real-world problems often require models that produce not only a correct prediction but also a reliable measure of confidence in it. Reliability refers to the estimated probability that the forecast is correct. For example, as Vasilev and D'yakonov (2023) makes"}, {"title": "4.0.1. Reliability Diagrams", "content": "Visual representations of the model calibration can be done via the reliability diagrams. If the model is perfectly calibrated, then the diagram should represent the identity function. Any deviation from a perfect diagonal represents poor calibration. To estimate the expected accuracy of finite samples, the predictions are grouped into M bins of intervals (each of size 1/M) and the precision of each bin is calculated. Defining Bm as the set of indices of samples whose prediction confidence falls in the interval $I_m = (\\frac{m-1}{M}, \\frac{m}{M})$. Then, the accuracy of Bm is\n$\u0430\u0441\u0441(B_m) = \\frac{1}{|B_m|} \\sum_{i \\in B_m}1(\\hat{y} = y_i)$,\nwhere $\\hat{y}_i$ and yi are the predicted and actual values of the classes for sample i. Classical probability says that if acc(Bm) is an unbiased and consistent estimator of $P(\\hat{Y} = Y|P \\in I_m)$. The average trust within the container Bm would be defined as\n$conf(B_m) = \\frac{1}{|B_m|} \\sum_{i \\in B_m} p_i$\nwhere pi is the confidence in sample i. Therefore, a perfectly calibrated model will have acc(Bm) = conf(Bm) for all m \u2208 1, ..., M."}, {"title": "4.0.2. Expected Calibration Error (ECE)", "content": "While calibration diagrams are very powerful visual tools, it is more convenient to have a statistic that summarizes the calibration evaluation. An indication of poor calibration is the difference in expectations between confidence and accuracy\n$ECE = E_p [|P(\\hat{Y} = Y|P = p) - p|].$"}, {"title": "5. Metrics to model performance", "content": "When evaluating the effectiveness of classification models, it is essential to employ a diverse range of metrics that provide nuanced information about their performance. These metrics serve as quantitative measures to measure the accuracy, reliability, and generalization ability of the model across different classification tasks. From fundamental metrics like accuracy and precision to more nuanced measures like area under the ROC curve and intercept over union, each metric provides unique insights into the model's strengths and weaknesses. In this section, we delve into a comprehensive examination of several metrics commonly used in evaluating classification models, clarifying their importance, interpretation, and formulas. By comprehensively evaluating models using a combination of these metrics, researchers can gain a comprehensive understanding of their performance and make informed decisions regarding model selection, optimization, and implementation."}, {"title": "6. Dataset", "content": "The Morozov et al. (2020) dataset used in this project consists of anonymous human lung computed tomography (CT) scans with COVID-19-related and not re-"}, {"title": "7. Methodology", "content": "Let us start by extensively investigated the standard methods for processing computed tomography (CT) images, focusing particularly on thoracic images. It was observed that CT images are typically processed using Hounsfield Unit (HU) values in terms of window width and center. Subsequently, we define different HU windows for testing, referenced in Table( 1), limiting the pixel values of the images according to the specified HU window width.\nAfter reading the Nifti imagefiles (.nii extension), they are followed by a 90 de-"}, {"title": "8. Results", "content": "Exploring the early stages of the project yielded important insights into model performance and optimization strategies. Here we present a complete description of the results obtained, highlighting the key findings and the methodologies employed. This research embarked on a multifaceted exploration of various methodologies and model architectures to discern optimal approaches for classifying COVID-19 pneumonia in 3D CT scans. This effort encompassed defining the most appropriate Hounsfield Unit (HU) window, evaluating the impact of data augmentation techniques, examining the effectiveness of various neural network architectures, and delving into model calibration and uncertainty estimation with higher performance. Through meticulous analysis, we aim to provide valuable insights into the complexities of model performance and uncertainty quantification in the context of COVID-19 pneumonia classification."}, {"title": "8.1. HU window", "content": "One of the first findings of the project was the determination of the optimal HU window for CT image processing. Through various experimentations, the HU"}, {"title": "8.2. Data Augmentation", "content": "After implementing several transformations from the volumentations-3D package, a noticeable decrease in metrics of almost 20 points was observed in comparison with the reference model without augmentation as illustrated in Table. 3. We adopted a rotation transformation in which each 3D CT volume was rotated by a randomly selected angle within a predefined range (-20\u00b0 to +20\u00b0). This rotation helped augment the training data set by introducing variations in the orientation of the CT scans,"}, {"title": "8.3. Deterministic Models", "content": "Once the best performing HU window was defined for our case study and making use of the classification-models-3D library, popular neural network architectures were explored in their native form and with minor modifications, such as adding Global-AveragePooling3D and GlobalMaxPooling3D layers, as well as adjusting the number of filters in the convolutional layers. However, the results of these architectures did not outperform the initial Keras model architecture using the W4 window mentioned in section 8.1. Thus, the following experiments were focused on optimizing the hyperparameters of the deterministic neural network model selected from the initial group (DNN_W4_V1). Leveraging the keras-tuner package O'Malley et al. (2019) with the Hyperband tuner, an exhaustive search was performed on various hyperparameter configurations. The Hyperband tuner optimizes the search process by iteratively discarding hyperparameter configurations with poor performance, enabling efficient exploration of the hyperparameter space. The optimal hyperparameters obtained were three blocks comprise of CNNs plus MaxPool and BatchNormalization with 128 neurons each, followed by a Global MaxPool and a dense layer with 256 neurons. The initial learning is 0.001 and a dropout rate of 0.2. With these hyperparameters, we achieved the best performing deterministic model among the options considered."}, {"title": "8.4. Bayesian Neural Networks", "content": "The next exploration step delved into the training of Bayesian neural networks, starting with minor modifications to the architecture of the optimized deterministic model. The main alteration involved replacing the last dense layer of the deterministic model with its Bayesian counterparts. Several layers were explored to evaluate their impact on model performance. These included the integration of dense Flipout, Local Reparameterization, and Reparameterization at the top of the network. After exploring the variations in the last layer, the next step involved replacing the deterministic convolutional layers with their Bayesian counterparts in three different scenarios:\n\u2022 Approach 1: 3D MNF Convolutional Layers: In this approach, the deterministic convolutional layers were replaced by 3D Bayesian convolutional layers of multiplicative normalising flows (MNF), leveraging the results of the research in Garcia-Farieta et al. (2024).\n\u2022 Approach 2: Convolution3DFlipout Convolutional Layers: This approach involved replacing the deterministic convolutional layers with Flipout convolutional layers\n\u2022 Approach 3: Convolutional Layers Convolution3DReparameterization: In this approach, the deterministic convolutional layers were replaced with reparametrization convolutional layers.\nIn the modified dropout layer, working not only during training but also in the phase test allows to work under a BNN where the posterior is considered as Gaussian. After evaluating several Bayesian architecture alternatives, the findings in"}, {"title": "8.5. Calibration", "content": "The calibration analysis of the models involved the visualization of reliability diagrams and the calculation of the Expected Calibration Error (ECE). These analyzes were performed for a variety of models\u00b2 in order to evaluate their calibration performance and identify optimal configurations.\nIn addition to evaluating different models, experiments were performed to evalu-"}, {"title": "8.6. Uncertainty", "content": "The uncertainty analysis of the models involved the analysis of prediction intervals derived from simulation exercises performed on selected CT images mentioned in the 7 section. These images spanned a variety of classes and were used to evaluate"}, {"title": "8.7. Explainability with SHAP", "content": "As the adoption of AI in medical diagnostics continues to grow, one of the major challenges is the \"black-box\" nature of neural networks, which makes it difficult to understand how a model reaches its predictions Hamilton and Papadopoulos (2023); Zeng (2024). In the context of diagnosing COVID-19 pneumonia from 3D CT scans, the ability to not only classify but also explain the decision-making process behind these classifications is crucial for clinical adoption. To address this need for transparency, we have incorporated SHAP (SHapley Additive exPlanations) values into our model framework, aiming to enhance interpretability. SHAP values provide a principled way to break down the output of a machine learning model by attributing the contribution of each input feature to the final prediction Lundberg and Lee (2017a). In the case of CT scan classification, SHAP values allow us to quantify the importance of each pixel or voxel within the scan, showing precisely which regions of the image contributed the most to the model's decision to classify a patient as COVID-19 positive or negative. This is particularly important in healthcare, where clinicians need to understand not only what the model predicts, but also why it predicts it. By implementing SHAP values for 3D, we enable our deterministic neural"}, {"title": "9. Discussion", "content": "One notable advantage of Bayesian neural networks is their ability to provide a probabilistic interpretation of predictions, allowing for the quantification of uncertainty. This feature is crucial for clinical decision-making, as it empowers clinicians to make more informed decisions, particularly in cases where the consequences of false positives or false negatives are significant. However, there are also several disadvantages to consider. Bayesian methods typically require more computational resources and time compared to deterministic counterparts due to the need for sampling-based inference techniques. Moreover, the effectiveness of Bayesian neural networks heavily relies on the availability of large and diverse datasets for training, which may be limited, especially in medical imaging. This clearly create an scenario where the training can be challenging and time-consuming. Besides, Deep learning models, especially complex neural networks proposed in this work, can be difficult to interpret, making it challenging to understand the reasons behind their decisions. This can be particularly problematic in medical applications where transparency and accountability are crucial. Shap values allow to reduce the opacity of interpretability of the models, but in scenarios where the amount of classes increases, this technique provide complex responses that make the analysis even harder."}, {"title": "10. Summary and conclusions", "content": "Accurately classifying COVID-19 pneumonia in 3D CT scans remains a significant challenge in medical image analysis. While deterministic neural networks have"}, {"title": "Appendix A. Architecture", "content": "Fig. A.16 depicts the architecture used in the manuscript for the deterministic and its Bayesian counterpart."}]}