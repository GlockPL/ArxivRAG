{"title": "OPENRFT: ADAPTING REASONING FOUNDATION MODEL FOR DOMAIN-SPECIFIC TASKS WITH REINFORCEMENT FINE-TUNING", "authors": ["Yuxiang Zhang", "Yuqi Yang", "Jiangming Shu", "Yuhang Wang", "Jinlin Xiao", "Jitao Sang"], "abstract": "OpenAI's recent introduction of Reinforcement Fine-Tuning (RFT) showcases the potential of reasoning foundation model and offers a new paradigm for fine-tuning beyond simple pattern imitation. This technical report presents OpenRFT, our attempt to fine-tune generalist reasoning models for domain-specific tasks under the same settings as RFT. OpenRFT addresses two key challenges of lacking reasoning step data and the limited quantity of training samples, by leveraging the domain-specific samples in three ways: question augmentation, synthesizing reasoning-process data, and few-shot ICL. The evaluation is conducted on Sci-KnowEval, where OpenRFT achieves notable performance gains with only 100 domain-specific samples for each task. More experimental results will be updated continuously in later versions. Source codes, datasets, and models are disclosed at: https://github.com/ADaM-BJTU/OpenRFT.", "sections": [{"title": "1 INTRODUCTION", "content": "OpenAI's ol model has shown strong reasoning abilities in mathematics and programming, but its generalization to other tasks remains uncertain. The recent introduction of Reinforcement Fine-Tuning (RFT) (OpenAI, 2024) has provided a promising avenue for reasoning generalization. With only dozens of high-quality (question, answer) pairs, RFT enables the creation of customized reasoning models excelling at domain-specific tasks.\n\nThe significance of RFT is at least two-fold: (1) It demonstrates the promise of using generalist reasoning models, like o1, as reasoning foundation models. By enabling the efficient creation of domain-specific reasoning models, RFT practically expands the applicability of reasoning models across diverse tasks. (2) It introduces a new paradigm for fine-tuning foundation models. Unlike Supervised Fine-Tuning (SFT), which merely mimics patterns in training data, RFT leverages reasoning capabilities to facilitate thinking and trial-and-error learning. This brings models closer to achieving human-like generalization, moving beyond mechanical imitation to extrapolate knowledge to new cases.\n\nIt is believed that the core techniques behind RFT are closely related to those of o1. Inspired by recent o1-replication efforts (ope, 2024; Team, 2024a; SimpleBerry, 2024; Zhao et al., 2024; Zhang et al., 2024), we attempt to develop an implementation under the same settings as the RFT demo, which we call OpenRFT. While this early exploration may not achieve optimal results, we hope it is beneficial to the community for clarifying the conceptual landscape and inspiring further advancements in this area.\n\nRealizing RFT requires addressing two key challenges: the absence of reasoning step data in the provided domain-specific samples, and the limited quantity of such samples. For the first challenge,"}, {"title": "2 METHOD", "content": "The key to Reinforcement Fine-Tuning lies in the effective utilization of limited domain-specific samples. Based on the way domain-specific samples are leveraged, as illustrated in Fig. 1, the framework of OpenRFT can be divided into three modules. (1) Data augmentation: By rewriting questions and shuffling options, we explicitly generate more domain-specific data. This helps explore a broader range of states and actions in the RL stage. (2) SFT-based imitation: Using a stronger reasoning foundation model as a teacher 1, the missing reasoning steps are synthesized for the provided domain-specific data. These enhanced samples are then used to pre-adapt the student policy model through SFT. (3) RL-based exploration and self-improvement: The domain-specific samples are incorporated into the policy model in a few-shot ICL manner. The policy model, under process supervision by the PRM, explores and continuously optimizes within an RL environment.\n\nTypically, a smaller reasoning foundation model (e.g., 01-mini) is desired to ensure efficiency in domain-specific applications. When synthesizing reasoning step data, it is ideal to use a stronger reasoning foundation model as the teacher model for distillation (e.g., 01). It is important to ensure that the action space of the teacher and student models remains consistent.\n\nDue to the lack of a stronger reasoning model with consistent actions, in our reported experiments, the synthesis is instead performed by the policy model itself."}, {"title": "2.1 DATA AUGMENTATION", "content": "Data augmentation (DA) is a widely used technique for addressing data scarcity. In this framework, we propose a data augmentation method based on question rewriting. Specifically, we first utilize GPT-40-mini to rewrite the question stem while preserving its original meaning, generating multiple variations. Subsequently, the options for each question are shuffled independently for these variations. Both the original question and the newly transformed question are used simultaneously in the PPO optimization."}, {"title": "2.2 SFT-BASED IMITATION", "content": "To enhance the understanding and response capabilities of a reasoning foundation model in specific domains, we adopt a teacher reasoning foundation model to synthesize reasoning process data and leverage this data for SFT of the reasoning foundational model. The teacher model is typically a stronger general reasoning foundation model capable of synthesizing high-quality reasoning samples tailored to domain-specific data."}, {"title": "2.2.1 REASONING PROCESS SYNTHESIS", "content": "Specifically, for each question Qi in the domain dataset(Qi, Ai), We designed a set of general prompts to leverage the reasoning capabilities of the teacher model, enabling it to generate initial reasoning processes and answers within its action space. To ensure the correctness and diversity of the generated data, we introduce a multi-sampling strategy to synthesize high-confidence reasoning process data $(Q_i,..., S_i,..., A_i^j)$, where $S_i^j$ represents the j-th sampled intermediate reasoning step generated by the teacher model. By sampling multiple reasoning paths for each question Qi, we select at least one data that can infer the correct answer $A_i$.\n\nFor each question Qi, the final synthesized reasoning data (Qi, Si, Ai) includes the question, the intermediate reasoning steps S\u2081, and the corresponding answer A. These data points are then aggregated to form the domain-specific reasoning dataset $D_{process}$, which serves as a high-quality resource for SFT of the reasoning foundation model."}, {"title": "2.2.2 SFT OF POLICY MODEL", "content": "After completing the reasoning process synthesis tasks described in Section 2.2.1, we use each complete reasoning solution in the dataset to initialize the policy model \u03c0ori. This step aims to let the policy model \u03c0ori learn and generalize reasoning patterns and strategies embedded in $D_{process}$, thereby enabling it to generate accurate and coherent reasoning solutions for new, unseen questions within the domain.\n\nGiven the question Qi, the policy model \u03c0ori utilizes the reasoning dataset $D_{process}$ to predict intermediate reasoning steps and derive a final answer. Specifically, \u03c0ori is trained to maximize the likelihood of producing the synthesized reasoning paths and correct answers from $D_{process}$. The training objective is formulated as:\n\n$L_{SFT} = - \\sum_{(Q_i, S_i, A_i) \\in D_{process}} log P(S_i, A_i | Q_i; \\pi_{ori}),$\n\nwhere $P(S_i, A_i | Q_i; \\pi_{ori})$ represents the probability of the policy model generating the reasoning steps Si and answer A\u017c conditioned on the question Qi.\n\nDuring training, the model learns to balance the trade-off between mimicking the high-quality reasoning paths synthesized by the teacher model and generalizing to new questions.\n\nAfter the SFT process, we obtain the updated policy model \u03c0SFT. This model incorporates the reasoning patterns, strategies, and domain-specific knowledge embedded in the dataset $D_{process}$."}, {"title": "2.3 RL-BASED EXPLORATION AND SELF-IMPROVEMENT", "content": "In scenarios where there are only a few dozen training samples within a specific domain, the policy model lacking domain knowledge may face inefficiencies during the exploration stage. To address this, we employ a prompt-based in-context learning paradigm, with the aim of supplementing the policy model with knowledge and steering it toward reasoning and answering in the correct direction."}, {"title": "2.3.1 FEW-SHOT ICL-BASED DOMAIN KNOWLEDGE EMBEDDING", "content": "For each training task, we construct a corresponding vector database. For each training sample Qi, we retrieve the top k most similar question-answer pairs {(Qj, Aj) | j = 1,2,...,k} based on vector similarity. These pairs are then concatenated with the task instructions to create an enhanced prompt enriched with domain-specific knowledge."}, {"title": "2.3.2 RL WITH PRM", "content": "The reward feedback based only on the outcome is easily influenced by the correct answer sampled in reinforcement learning, but with incorrect processes. This phenomenon is more likely to occur in long-range reasoning tasks. To mitigate the negative impact of this phenomenon to some extent, we integrate a process-based reward model PPRM into the reward design of reinforcement learning.\n\nIn the reinforcement learning phase, the policy model \u03c0\u03b8 improves itself using domain-specific datasets constructed under different data organization schemes. We model the entire sampling as a language-augmented Markov Decision Process(MDP), formally represented as M ="}, {"title": "3 EXPERIMENTS", "content": "The experimental data in our study is sourced from the knowledge reasoning level (L3) of the dataset provided in SciKnowEval Feng et al. (2024), a newly-released scientific benchmark that encompasses five distinct abilities. In this work, we focus on 8 subsets, named GB1-ftness-prediction, retrosynthesis, chemical-calculation, molecule-structure-prediction, high-school-physics-calculation, material-calculation, diffusion-rate-analysis, perovskite-stability-prediction (denoted as T1-T8), covering four domains: Biology, Chemistry, Physics, and Materials. The questions in these subsets are in the form of multiple-choice questions (e.g., ABCD). To simulate the practical scenario of fine-tuning with limited training samples, we sample 100 training samples and 100 testing samples from each dataset ."}, {"title": "3.2 COMPARISON METHODS", "content": "The following methods are introduced as our comparative approaches. In addition to these, we also report the results of 01-mini and gpt-40-mini, which can be regarded as a strong ceiling performance.\n\n\u2022 Vanilla. The policy model, i.e., Skywork-o1-Open-Llama-3.1-8B, which is used without any modifications or adjustments.\n\n\u2022 SFT. The experimental setup involves distilling process data with limited training samples using the policy model, and then performing SFT on the policy model to obtain the final model.\n\n\u2022 SFT+. SFT with distillation data from a stronger model, QwQ-32B-Preview Team (2024b); Yang et al. (2024)."}, {"title": "3.3 EXPERIMENTAL SETUP", "content": "We conduct experiments using the foundation models from the Skywork o1 Open series o1 Team (2024). These models demonstrate strong cognitive reasoning abilities in tasks involving mathematics, code, and other reasoning tasks. Specifically, we employ Skywork-01-Open-Llama-3.1-8B as the policy model and Skywork-01-Open-PRM-Qwen-2.5-7B as the process reward model .\n\nHyper-parameters\n\nIn all experiments, training is conducted on devices equipped with NVIDIA H20-96GB GPUs, utilizing OpenRLHF Hu et al. (2024) for reinforcement learning. Both SFT and RL experiments employ LoRA fine-tuning with a rank of 4.\n\nFor SFT training, the batch size, learning rate, number of epochs, and maximum sequence length are set to 8, 5e-5, 8, and 2048, respectively. In the PPO setup, the actor and critic learning rates are set to 3e-5 and 6e-5, respectively. Additionally, the maximum generated sequence length is set to 1536, and the KL coefficient is fixed at 0.01. The weighted coefficient, a, for combining the process and outcome reward models is set to 0.7.\n\nRegarding data augmentation, the original dataset is expanded by a factor of six, yielding a total of 600 samples. For synthetic reasoning process data, roll-out sampling is performed on each training sample until a response that correctly matches the true answer is obtained. This reasoning data is then treated as the ground truth for the corresponding training sample. In cases where 64 sampling attempts do not yield the correct answer, particularly when using the QwQ-32B-Preview model, the true answer is directly used as the response in the training data, bypassing the reasoning process data. This strategy is also applied in the Skywork-o1 experiments.\n\nFor in-context learning (ICL), Sentence-BERT is used to calculate the similarity between the current question and existing examples. The top 3 most similar examples are then selected as context for the current query.\n\nFinally, for model generation, the temperature coefficient is set to 0.6 for all models, except for 01-mini, where the official guidelines specify a temperature coefficient of 1. All other generation parameters are set to their default values."}, {"title": "3.4 RESULTS", "content": "The main results are summarized in Table 1. Key observations include: (1) 01-mini demonstrated the strongest reasoning capabilities, yet GPT-40-mini showed a competitive performance in certain tasks. As the representative of System-1 models, GPT-40-mini excels in versatility, outperforming o1-mini on tasks where domain knowledge is crucial. (2) The contribution of ReFT is trivial. Designed to enhance general reasoning abilities, it fails to address the distribution discrepancy between the provided domain samples and the policy model to be fine-tuned. (3) With PRM to supervise the reasoning process, ReFT+PRM contributes to increasing the likelihood of sampling correct reasoning processes, although the improvement is not significant. (4) After fine-tuning with self-synthesized reasoning process data, SFT slightly outperformed Vanilla. This indicates that with such a limited number of samples, relying solely on SFT is far from sufficient. However, it can provide a good exploration starting point for subsequent RL. (5) Compared with SFT, SFT+RL(PRM) shows obvious improvement, highlighting the necessity of RL in fully leveraging the limited domain-specific samples. (6) SFT+RL(PRM)+DA achieves consistent improvement over SFT+RL(PRM) in different tasks, validating the contribution of domain-specific samples in synthesizing new samples. It achieves the best performance among the methods initialized with Skywork-o1, an average improvement of 11% compared to Vanilla. (7) By further incorporating few-shot ICL, there was no improvement but a slight decrease in the performance. This is possibly due to the inconsistent prompts during the SFT and RL stages. It is interesting to see that few-shot ICL benefits the most challenging task (T4: molecule-structure-prediction), which is precisely where GPT-40-mini excels. This somewhat underscores the effectiveness of domain knowledge. We are experimenting alternative ways for domain knowledge embedding."}, {"title": "3.5 DISCUSSIONS", "content": "More domain-specific data contributes to better RFT results. From Table 1, we observe that data augmentation techniques significantly enhance model performance when training data is limited. This suggests that more domain-specific datasets could further improve the ReFT method. Motivated by this, we explored the impact of data size on ReFT by comparing the effects of different data sizes on ReFT and two variants of OpenRFT using the T8 dataset. In all experiments with the OpenRFT variants, the models are initialized with the same set of 100 samples across all conditions."}, {"title": "4 RELATED WORK", "content": "This section presents a review of related work, framed within the lens of System-1 and System-2 inference. Building on the previously introduced notation, we begin with a straightforward definition of System-1 and System-2 inference."}, {"title": "4.1 ACQUIRING SYSTEM-2 CAPABILITY FROM SYSTEM-1 MODEL", "content": "Recently, there has been a surge of efforts aimed at endowing models with System-2 reasoning capabilities. These approaches typically assume a pre-trained System-1 language model. Based on how the System-1 model is utilized, related work can be categorized into three types. (1) Promptingbased: Examples akin to the XoT family, such as CoT (Chain-of-Thought), ToT (Tree-of-Thought), and GoT (Graph-of-Thought), manually design reasoning schema to guide the System-1 model toward multi-step inference. (2) SFT-based: Examples including the recent reproduction works of o1, such as OpenO1 (ope, 2024) and Macro-O1 (Zhao et al., 2024), involve collecting training data containing reasoning steps through annotation or distillation and then applying supervised fine-tuning (SFT) to the System-1 model. (3) RL-based: Examples including Open-R (Team, 2024a), LLaMA-01 (SimpleBerry, 2024), and o1-Coder (Zhang et al., 2024), allow the model to explore the underlying reasoning steps autonomously and iteratively optimize its policy.\n\nReFT (Luong et al., 2024) lies between the second and third types. In the warm-up stage, it uses SFT to acquire a reasoning format, and in the second stage, it applies RL for iterative optimization. However, ReFT differs from the reinforcement fine-tuning (RFT) discussed in this paper. In terms of positioning, ReFT aims to acquire System-2 capabilities from a System-1 model, whereas RFT assumes the existence of a System-2 foundation model and aims to fine-tune it into a domain-specific System-2 model. Methodologically, ReFT relies on training data that contains reasoning steps. When such reasoning steps are unavailable, the RL policy model exhibits a distribution gap with the provided fine-tuning data. Our experiments have shown that ReFT cannot be directly applied to the standard RFT setting."}, {"title": "4.2 FINE-TUNING FOUNDATION MODELS", "content": "Fine-tuning foundation models to obtain domain-specific models usually results in improved domain performance. Moreover, by focusing on specific tasks, the derived models tend to be smaller, leading to more efficient inference. For instance, fine-tuning smaller o1-mini can achieve comparable or even superior domain-specific performance to that of larger 01.\n\nPrevious fine-tuning approaches have primarily relied on Supervised Fine-Tuning (SFT), which can be seen as targeting System-1 foundation models. However, SFT heavily depends on annotated data and is prone to overfitting.\n\nIn contrast, when fine-tuning System-2 foundation models, the base models already possess reasoning capabilities, enabling them to think, explore, and learn through trial and error. This allows more effective utilization of expert-provided training examples, leading to a deeper and more essential understanding of domain-specific tasks. This is also why many recent works have begun modeling in a System-2 manner, such as machine translation (Zhao et al., 2024), safety alignment (Wang & Sang,"}, {"title": "4.3 EMPLOYING REINFORCEMENT LEARNING FOR FINE-TUNING", "content": "Fine-tuning generative models with RL has already been explored in previous works, such as RLHF (Reinforcement Learning with Human Feedback (Ouyang et al., 2022; Fan et al., 2024)) and Reinforcement Learning-based Knowledge Distillation (Ashok et al., 2017; Alwani et al., 2022), but not focused on fine-tuning a foundation model to create a domain-specific model. As shown in Table 4, different methods can be distinguished based on the source of the reward model and the policy model to be fine-tuned. For instance, in RLHF, the reward model derives from human preferences, and the policy model is often a base model or an SFT model, aiming to align with human values. In contrast, RFT utilizes domain-specific samples as the source of the reward model, and the policy model is a reasoning foundation model, aimed at acquiring a specialized reasoning model.\n\nWe compare RL-based fine-tuning and SFT from two perspectives. (1) Training objective: SFT aims to minimize prediction error by directly adjusting the model parameters. In contrast, RL-based fine-tuning focuses on optimizing the policy, aiming to maximize cumulative rewards. This approach allows for adaptive exploration and optimization, accommodating environmental changes and task demands. The flexibility enables the model to respond to uncertainties, not relying on fixed training data or rules, thereby enhancing the model's long-term performance and adaptability. (2) Data dependency: SFT relies on a fixed labeled dataset, where training is bound by the availability of labeled examples. On the other hand, RL-based fine-tuning generates new experience data through continuous interaction with the environment. Compared to SFT, RL can achieve effective learning with a small amount of high-quality data, such as human feedback, expert demonstrations, or simulated data.\n\nIn addition to the above-mentioned common characteristics, RFT differs from other RL-based fine-tuning methods in two significant ways, which introduce unique challenges. (1) Inconsistent tasks: Both RLHF and RL-based knowledge distillation belong to single-task RL methods, where the fine-tuned model and the target application are within the same task. For instance, in RL-based knowledge distillation, the teacher and student models address the same task. In RLHF, value alignment can be seen as an enhancement of the task for the SFT model. However, RFT requires fine-tuning the foundation model to be applied to different downstream domain tasks. Task and domain generalization is thus crucial. (2) Inconsistent action mode: RLHF and RL-based knowledge distillation assume consistency between the policy model's action mode and the data source format of the reward model. For example, in RLHF, the preference data of the reward model aligns with the generation"}, {"title": "5 CONCLUSION & FUTURE WORKS", "content": "This work presents a simple implementation for fine-tuning generalist reasoning models to solve domain-specific tasks. The provided limited domain-specific samples are exploited in question augmentation, synthesizing reasoning-process data, and few-shot ICL, which are integrated within a RL framework supervised by a general PRM. We are working towards updating PRM synchronously, instead of relying on a static generalist PRM. With iteratively refined training data, the PRM and policy model can engage in Self-Play manner to continuously improve performance.\n\nFuture work can be divided into two main directions: (1) Domain knowledge embedding. Further exploration is needed to enhance both the policy model and PRM, e.g., investigating effective representations of domain knowledge, adapting PRM by comparing differences between the provided expert samples and random samples, etc. (2) Domain data augmentation: While RL can search and explore diverse reasoning processes, the current question size of only dozens remains insufficient. In addition to rephrasing questions while retaining the original answers, generating new questions by leveraging unlabeled training data presents significant potential for exploration. Additionally, exploring more challenging samples through adversarial learning is also worth considering. Maintaining a pool of unsolved questions ensures continuous improvement through self-play.\n\nGeneralization remains the fundamental challenge for RFT. Beyond further improving the general reasoning capabilities of foundation models, continued exploration is needed in reward calculation for open-ended problems and the effective adaptation of policy actions. For instance, when the problem format extends beyond multiple-choice questions to data like long-form technical reports, it becomes crucial to design appropriate reward functions and define action spaces for efficient search. This is essential for enabling the model to quickly learn the reasoning processes of domain experts."}]}