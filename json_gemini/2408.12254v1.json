{"title": "A Language-agnostic Model of Child Language Acquisition", "authors": ["Louis Mahon", "Omri Abend", "Uri Berger", "katherine Demuth", "Mark Johnson", "Mark Steedman"], "abstract": "This work reimplements a recent semantic bootstrapping child language acquisition (CLA) model, which was originally designed for English, and trains it to learn a new language: Hebrew. The model learns from pairs of utterances and logical forms as meaning representations, and acquires both syntax and word meanings simultaneously. The results show that the model mostly transfers to Hebrew, but that a number of factors, including the richer morphology in Hebrew, makes the learning slower and less robust. This suggests that a clear direction for future work is to enable the model to leverage the similarities between different word forms.", "sections": [{"title": "Introduction", "content": "This paper concerns computational models of CLA, which seek to understand the process of language acquisition by programming a computer to emulate the learning undergone by the child. When presented with data from a given language, such an algorithm should learn a degree of proficiency in that language. The fact"}, {"title": "Method", "content": "Our model deals with syntax and semantics learning only. It assumes the child either has already learned to segment the speech stream and detect potential word boundaries, as evidenced in even young prelinguistic infants (Mattys et al., 1999), or is jointly learning phonotactics and morphology with syntax, as in the model of Goldberg and Elhadad (2013). At that point, the child must learn to combine atomic units (words) to produce a meaning representation that depends on (a) the meaning of the constituent words and (b) the manner in which they combine. Initially, for such a child, both are unknown. Theoretically, our approach to this problem falls under Semantic Bootstrapping Theory, (Pinker, 1979; Grimshaw, 1981; Brown, 1973; Bowerman, 1973; Schlesinger, 1971), which operates as follows: When a child hears an utterance Bambi is home, we assume that, from a combination of perceptual context and background and innate knowledge, it can approximately identify the meaning of the entire utterance as some object in some state: home(bambi). The task is then to figure out which words correspond to which parts of the meaning representation, and the language-specific principles by which they combine. As well as the correct interpretation, where English subjects precede VPs, others are also possible, e.g. Bambi means home(\u00b7), is home means bambi and subjects follow VPs. The original implementation of our model (Abend et al., 2017) designed a language learner that bootstraps learning of both (a) and (b) simultaneously."}, {"title": "Combinatory Categorial Grammar.", "content": "Combinatory categorial grammar (CCG) (Steedman, 2001) is a suitable theoretical framework for learning of this sort, because of its tight coupling of syntax and semantics. For each combination of syntactic categories, CCG provides a precise description of a corresponding semantic combination, bambi + Xx.home(x) \u2192 home(bambi). A full example is given for the two CHILDES (MacWhinney, 1998) corpora that we apply our model to: Adam (English), in Figure 1, and Hagar (Hebrew) in Figure 2.\nTypically, CCG parsing is discussed in terms of combining constituents via combinatory rules to derive a root. For example, the last step of the derivation in"}, {"title": "Probabilistic Model", "content": "This section describes the details of our new implementation of the semantic bootstrapping CCG-based model of Abend et al. (2017).\nFor syntax and semantics learning, the three relevant aspects of each data point are the string of words in the utterance x, the meaning representation m,"}, {"title": "Training Algorithm", "content": "The parameter updates described in Section 2.3 require tracking the occurrences on which two different elements co-occur. For example, in pw, the probability of predicting the logical form Ax.Ay.lost x y to be realized as the word 'lost' depends on the number of times that logical form and word were observed together during training. Because we do not observe parse trees directly, we instead employ an expectation-maximization algorithm, as follows. When the model observes a single data point X, consisting of an utterance as a string and a corresponding logical form, it uses its current parameter values (0(t) to estimate a distribution over all possible parses that connect the two. The probability assigned to a parse tree T and the data point X is the following product\np(X,T|0(t)) = \\prod_{s'} [P_t((s_1, s_2)|s')] \\prod_{s} [P_t(leaf|s)P_h(e_s|s)P_i(m_s|e_s)P_w(x_s|m_s)], (2)\nwhere s' ranges over all non-leaf nodes in T, s\u2081 and s2 are the children of s', s ranges over all leaf nodes in T, and es, ms and xs are, respectively, the shell logical form, the logical form, and the word aligned to s in T. As we observe X, we are interested in the conditional probability of a given parse tree\np(T|X,0(t)) = \\frac{p(X,T|0(t))}{\\sum_{T' \\in T}p(X, T'|\\theta(t))}, (3)\nwhere T is the set of all allowable parses of X. For each parse tree, the co-occurrences that it gives rise to are recorded in proportion to the parse tree's probability. Combining the standard expectation maximization (EM) update rule with the Bayesian update for the Dirichlet process, then, for each parameter 0 that tracks the co-occurrence of two elements a and b, the update rule is given by\n0(t+1) = 0(t) + E_{T~p(T|X,\\theta(t))} [\\delta_T(a, b)],\nwhere dr(a, b) is an indicator function that is 1 if a and b co-occur in T and 0 otherwise.\nThe set T of allowable parse trees is the set of all valid CCG parse trees that have the observed logical form (LF) as root, the words in the observed utterance as leaves, and that have congruent syntactic and semantic types."}, {"title": "Worked Example", "content": "Here we present a worked example on a single training point. Recall that each training example consists of an utterance and corresponding logical form, and the learner considers the set T of all compatible parses, i.e. all parses with the observed LF as root, the observed utterance as leaves and that obeys the constraints described in Section 2.3. We describe the training updates for a single, correct parse for the example \"you lost a pencil\", as shown in Figure 3. For the purposes of this example, we show the exhaustive computation for every node in the tree. In practice the probabilities for upper nodes can be cached giving a large increase in efficiency.\nThe prediction of the tree proceeds from the root. First, the learner predicts a possible root category, here S with probability 0.388.\nNext, it selects a possible split of the root syntactic category S, here the selected split is into NP and S\\NP.\nThen, for the left child node, it predicts the probability for a split into two daughter syntactic categories, or alternatively that this node is a leaf. Here, the prediction is that the node is a leaf, with probability 0.738. Then it predicts probabilities for a shell logical form given the syntactic category ('entity' with probability 0.66), a logical form given this shell LF (you with probability 0.327) and word (or span of words) given this LF (\u201cyou\u201d with probability 0.912).\nMeanwhile, for the right child of the root, it predicts, with probability 0.549, a split into further categories of S\\NP/NP and NP.\nThen for the left child of this node, it makes the equivalent predictions it made for the far left node, predicting a split or leaf given the syntactic category ('leaf' with probability 0.989), then of shell LF given syntactic category (Ax.Ay.vconst x y with probability 0.961), of LF given shell LF (Ax.dy.losepast x y with probability 0.012) and word span given LF (\u201clost\u201d with probability 0.862).\nFor the right child of this node, the right-most NP in Figure 3, it predicts a split into NP/N and N, with probability 0.261.\nFor the NP/N daughter, it predicts \u2018leaf' with probability 0.994, then a shell LF given syntactic category (Ax.quant x with probability 0.999), of LF given shell LF (Ax.Ay.det : art|a; x with probability 0.486) and word span given LF (\u201ca\u201d with probability 0.95).\nFinally for the N daughter node it predicts 'leaf' with probability 0.994, noun with probability 1.0 (note these figures are rounded to three places), n|pencil with probability 0.015, and \u201cpencil\u201d with probability 0.973.\nAt this point, the semantic types of each node can be inferred. The rightmost npencil node, in virtue of its n CHILDES pos tag, is inferred to have semantic type <e,t>. The Ax. det:arta x node to its left has type <<e,t>,e>."}, {"title": "Results", "content": "In addition to the straightforward SVO examples in Figures 1 and 2, the LFs can express more complex syntactic features such as modals, negation, prepositional phrases, and relative clauses. Table 1 shows some more complex examples. Further detailed examples can be found in Szubert et al. (2024), as well as full details of the process by which they were produced, and the rationale behind the various design choices involved in this production.\nWe post-process this data to exclude words that serve only as discourse markers and do not appear in the LFs or receive the CHILDES part-of-speech tag 'co',"}, {"title": "Word Order", "content": "Following the procedure of Abend et al. (2017), we measure the acquisition of grammar and lexicon by examining the model's implicit word order parameters as a proxy. The degree to which the model favours each of the six possible word orders is determined by (a) the probability it assigns to the two CCG splits that are necessary to parse a simple transitive sentence under that order, and (b) the probability it assigns to the respective order in which the subject and object combine with the verb under that order. We assume that the verb-medial orders, SVO and OVS, must combine with the object first, so the six different possibilities are measured as follows:\np(SOV) :=\nP_t((NP, S\\NP)|S)P_t((NP, S\\NP\\NP)|S\\NP)P_h(\\lambda x.\\lambda y.vconst y x|(S\\NP\\NP))\np(SVO) :=\nP_t((NP, S\\NP)|S)P_t((S\\NP/NP, NP)|(S\\NP))P_h(\\lambda x.\\lambda y.vconst y x|(S\\NP/NP))\np(VSO) :=\nP_t((S/NP, NP)|S)P_t((S/NP/NP, NP)|S/NP)P_h(\\lambda x.\\lambda y.vconst x y|(S/NP/NP))\np(OSV) :=\nP_t((NP, S\\NP)|S)P_t((NP, S\\NP\\NP)|S\\NP)P_h(\\lambda x.\\lambda y.vconst x y|(S\\NP\\NP))\np(OVS) :=\nP_t((S/NP, NP)|S)P_t((NP, S/NP\\NP)|(S/NP))P_h(\\lambda x.\\lambda y.vconst y x|S/NP \\NP)\np(VOS) :=\nP_t((S/NP, NP)|S)P_t((S/NP/NP, NP)|S/NP)P_h(\\lambda x.\\lambda y.vconst y x|(S/NP/NP)).\nSo, for example, the probability of SOV is product of the probability of splitting an S into NP and S\\NP, the probability of further splitting the resulting S\\NP"}, {"title": "Word Meaning and Syntactic Category", "content": "Going beyond this emergent favouring of SVO word order, what we are ultimately interested in learning is the lexicon, which relates words to pairs of syntactic categories and meaning representations. To evaluate this, we measure the model's prediction for logical form and syntactic category. For each dataset, we select the 50 most common words, and annotate them with a ground-truth logical form and syntactic category. The full CCG lexicon could contain many possibilities for each word, but we restrict to those that are attested in our corpora. See appendix for full list.\nWe then extract a predicted logical form m' for each word x as follows:\nm' = argmax P(m|x) = argmax \\frac{P(x|m)P(m)}{P(x)} = argmax P(x|m)P(m) \\approx\n\\approx argmax P_w(x|m)P_\\iota(m) = argmax P_w (x,m), (4)\nm\nm\nm\nwhere the last quantity is determined by a Dirichlet process and so can be approximated by the observed number of times that w and m co-occur (recall this is in fact the sum of the expected values of their co-occurrence across all data points).\nSimilarly, we extract a predicted syntactic category for each word as follows:\ns' = argmax P(s|x) = argmax \\frac{P(x|s)P(s)}{P(x)} = argmax P(x|s)P(s) =\ns\ns\ns\n= argmax \\sum_m \\sum_e P(x, m, e|s)P(s) \\approx \\sum_m \\sum_e P_w(x|m)P_\\iota(m|h)P_h(e/s)P_{syn}(s), (5)\nand again, the last quantity can be computed straightforwardly, this time as a product of terms from each of the model's Dirichlet processes.\nTable 3 reports the percentage of points for which the predicted meaning representation, as per (4) and the predicted syntactic category, as per (5) agree with the manually annotated ground truth. For both datasets, the model achieves 100%"}, {"title": "Distractor Settings", "content": "To test the robustness of word-order learning to noise, we follow Abend et al. and consider a setting in which an utterance is paired not with a single logical form representing its meaning, but with multiple logical forms, only one of which represents its true meaning. The learner is then free to consider any of these LFs as the meaning of the utterance. Formally, what this means is that parse trees are computed for each of these LFs, and all of them are placed in the set T. Then, when calculating the Bayesian posterior, as per (3), the denominator is larger, so the probability on any given tree is smaller, as compared to the no-distractor setting.\nThis makes learning more difficult, and simulates the fact that there may be some uncertainty for the child as to the meaning a given utterance represents. When there is a single tree that the model is very confident in, then the probability from this tree dominates anyway, and overall there is little effect from the distractor trees. However, when there is no such single confident interpretation, the distractor trees significantly reduce the probability on the trees from the correct LF, including the correct tree, and so dilute the learning effect.\nThe other, distractor, logical forms are taken from the utterances immediately following and preceding the given utterance. Specifically, the n distractor setting takes the [n/2] previous examples and the [n/2] following examples.\nFor example, in Adam, data points 226-228 are as follows:"}, {"title": "Discussion", "content": "Our approach differs theoretically from other recent approaches to language acquisition. Ambridge (2020) argues that language acquisition can be understood purely based on the recall of all past occasions on which an utterance was used."}, {"title": "Limitations", "content": "One limitation of our learner is that it does not model anything below the token level, the tokens being taken from the data of Szubert et al. (2024), which in turn took them from the CHILDES parses. The issue highlighted above of the sparsity of word forms suggests a future extension to allow it to guess a meaning for new words if they are similar in form to familiar words. This requires it to learn some internal structure to these tokens in virtue of which they can be similar or dissimilar to one another. One way to do this would be by explicitly adding morphology, and allowing the parse tree to extend not to word boundaries but to morpheme boundaries. Another option would be to add a neural word predictor. A character-level language model could be used to produce vectors for each word that depend on their structure, which could then be fed into a multi-layer perception. Different inflected forms of the same root should then have a similar word vector and so a similar predicted meaning. Note, this approach does not mean a replacement of anything that is currently in the model, it would model only morphology, not syntax or semantics. It may require more data but potentially be more robust to the variety of inflected forms. Either of these additions could be learned independently of the model described here or in conjunction.\nThis need for the learner to discern a similarity between different inflected forms is obscured by the very sparse morphology in English. Different thematic roles for the same word or nominal phrase are not distinguished by morphological case as they are in Hebrew, so the model does not need to treat them as separate lexical entries. This further highlights the value of testing computational CLA models on multiple languages, and future work includes testing on further languages in addition to English and Hebrew.\nAnother limitation concerns our method of evaluating our model. Measuring the relative preference for different word orders allows comparison with Abend et al. (2017), but could give a misleading result in certain contexts where the correct analysis is a non-standard word order, e.g. in topicalization. In future, we hope to adopt a richer and more diverse evaluation suite, including measuring the fraction of test utterances with the correct inferred root LF and parse tree.\nThirdly, a more thorough evaluation of our learner involves testing on a more diverse set of languages, with larger and more comparable corpora. In contrast to the two corpora we use here, which differ in the number of tokens and utterances. It would also be interesting to compare different corpora for different children within the same language."}]}