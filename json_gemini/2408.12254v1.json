{"title": "A Language-agnostic Model of Child Language Acquisition", "authors": ["Louis Mahon", "Omri Abend", "Uri Berger", "katherine Demuth", "Mark Johnson", "Mark Steedman"], "abstract": "This work reimplements a recent semantic bootstrapping child language acquisition (CLA) model, which was originally designed for English, and trains it to learn a new language: Hebrew. The model learns from pairs of utterances and logical forms as meaning representations, and acquires both syntax and word meanings simultaneously. The results show that the model mostly transfers to Hebrew, but that a number of factors, including the richer morphology in Hebrew, makes the learning slower and less robust. This suggests that a clear direction for future work is to enable the model to leverage the similarities between different word forms.", "sections": [{"title": "Introduction", "content": "This paper concerns computational models of CLA, which seek to understand the process of language acquisition by programming a computer to emulate the learning undergone by the child. When presented with data from a given language, such an algorithm should learn a degree of proficiency in that language. The fact that any child, when exposed to appropriate data, can learn any language establishes a strong connection between acquisition and language variation: whatever varies between languages must be specified by the data and must be learnable. It also makes it an essential requirement of a convincing CLA model that it be capable of learning any language. Here, we reimplement a recent computational CLA model (Abend et al., 2017), which is based on combinatory categorial grammar (Steedman, 2001) and semantic bootstrapping (Pinker, 1979), and is trained with an expectation-maximization style algorithm. This model is a suitable choice for understanding the acquisition process because of its cognitive plausibility. The dominant paradigm of large language models requires too much training data to be plausible models of how humans acquire language. Even on the small end of the scale they generally train on several orders of magnitude more tokens than a human sees in their entire life. Some have sought to better approximate human learning by learning from a more modest 10-100M tokens (Warstadt et al., 2023). However, such models still generally make a number of implausible design choices, such as multi-epoch training, batched parameter updates, and arbitrary text tokenization, and they do not, as we do, ensure that the training examples are presented in the order they appear to the child. Abend et al. (2017) in contrast is grounded in a well-developed theoretical model of semantic bootstrapping, and trains on each example only once, individually, in the order they appear to the child.\nWe test this model on two languages: English, on which it was originally tested, and Hebrew. The data we use is comprised of real child-directed utterances, taken from the CHILDES corpus (MacWhinney, 1998), coupled with a recent method for converting universal dependency annotations to logical forms (Szubert et al., 2024).\nFirstly, we replicate the findings of Abend et al. (2017), and show that this model is successful in learning the important features of English syntax and semantics. We focus in the present paper on word order learning, and learning the meaning and syntactic categories of individual words. The results show that, after training, the model, correctly, strongly favours SVO order, predicts the right semantics for commonly appearing words and the right syntactic category for most. Then we apply the same training and testing procedure to the Hebrew corpus. There, the model learns word order and word meaning with a reasonably high accuracy. Its accuracy on syntactic categories is somewhat lower than that on English. We then discuss the difference in acquisition performance with respect to the linguistic differences between the two languages, and outline future extensions to the model that can more completely handle the learning of Hebrew without compromising the learning of English. Together these results demonstrate the model in question is broadly successful in transferring between multiple languages, and support the argument that, in general for computational CLA models, it is important and instructive to evaluate on more than just a single language. The code for training and evaluation will be released on publication."}, {"title": "Method", "content": "Our model deals with syntax and semantics learning only. It assumes the child either has already learned to segment the speech stream and detect potential word boundaries, as evidenced in even young prelinguistic infants (Mattys et al., 1999), or is jointly learning phonotactics and morphology with syntax, as in the model of Goldberg and Elhadad (2013). At that point, the child must learn to combine atomic units (words) to produce a meaning representation that depends on (a) the meaning of the constituent words and (b) the manner in which they combine. Initially, for such a child, both are unknown. Theoretically, our approach to this problem falls under Semantic Bootstrapping Theory, (Pinker, 1979; Grimshaw, 1981; Brown, 1973; Bowerman, 1973; Schlesinger, 1971), which operates as follows: When a child hears an utterance Bambi is home, we assume that, from a combination of perceptual context and background and innate knowledge, it can approximately identify the meaning of the entire utterance as some object in some state: home(bambi). The task is then to figure out which words correspond to which parts of the meaning representation, and the language-specific principles by which they combine. As well as the correct interpretation, where English subjects precede VPs, others are also possible, e.g. Bambi means home(\u00b7), is home means bambi and subjects follow VPs. The original implementation of our model (Abend et al., 2017) designed a language learner that bootstraps learning of both (a) and (b) simultaneously."}, {"title": "Combinatory Categorial Grammar.", "content": "Combinatory categorial grammar (CCG) (Steedman, 2001) is a suitable theoretical framework for learning of this sort, because of its tight coupling of syntax and semantics. For each combination of syntactic categories, CCG provides a precise description of a corresponding semantic combination, bambi + Xx.home(x) \u2192 home(bambi). A full example is given for the two CHILDES (MacWhinney, 1998) corpora that we apply our model to: Adam (English), in Figure 1, and Hagar (Hebrew) in Figure 2.\nTypically, CCG parsing is discussed in terms of combining constituents via combinatory rules to derive a root. For example, the last step of the derivation in Figure 1 uses the Backward Application Rule: Y, X\\Y \u2192 X. Our learning model, when interpreting a sentence-meaning pair, runs these combinators in reverse, that is, it proceeds by successively splitting a root into smaller chunks until they can be aligned with word spans. We will thus often speak of CCG 'splits', by which we mean the CCG combinators run in reverse. The net effect is that our model considers all possible ways to split up the sentence and the meaning representation so that the semantic units best correspond to the language units."}, {"title": "Probabilistic Model", "content": "This section describes the details of our new implementation of the semantic bootstrapping CCG-based model of Abend et al. (2017).\nFor syntax and semantics learning, the three relevant aspects of each data point are the string of words in the utterance x, the meaning representation m, and the parse tree t. The former two are given by the data, but the parse tree is unobserved, and so treated as a latent variable. We assume that the data is drawn from some joint distribution P(x, m, t), and we fit an approximation to this via several univariate conditional distributions. The conditional distributions are in the generative direction, i.e., together they produce a probability for the word string given the meaning representation. We make the Markovian assumption that the probability of splitting a node depends only on the syntactic category of that node, not on the rest of the tree or on the words or meaning. This means the parse tree can be modelled with a distribution of the form pt((S1, S2)|s), where s1, s2 and s are CCG syntactic categories. Note that the tuple (81,82) is ordered. This distribution also allows the possibility that s is a leaf node, the probability of which is expressed as pt(leaf|s).\nWe make similar Markovian assumptions on the relationship between syntactic category and meaning, and between meaning and word, so the distribution relating word x and meaning representation m has the form pu(x|m), and similarly for the distribution relating syntactic category s to meaning representation m. Following Abend et al. (2017), we add a second layer of prediction in the form of a shell logical form e, between the syntactic category and the logical form. The shell logical form replaces all non-variable terms with a placeholder marked for the function of the placeholder, e.g. verb, entity, determiner. The function is inferred from the CHILDES part-of-speech tag given in the method of Szubert et al. (2024). For example the logical form dy.lost y (a shoe) from Figure 1 has shell logical form Ay.vconst y (detconst nconst), because the CHILDES tags for lost, a and shoe are 'v', 'det:art' and 'n', respectively. See appendix for full list of conversions from CHILDES tags. This allows the model to share representation power for the structure of the logical form across different examples that may have different values for the constants. It is also used for the measure of word-order preference, described in Section 3.2. Thus, p(m|s) is decomposed as pi(m|e) and ph(e|s). In Ph(els), we ignore slash direction in s, so that e.g. conditioning on S\\NP_gives exactly the same distribution as conditioning on S/NP.\nEach of these distributions is modelled as a Dirichlet process, to which Bayesian updates are applied at each data point. Taking pw as an example, the form of the posterior is then\nPw(x/m) =  (n(x,m) + aH(x|m)) / (n(m) + a), (1)\nwhere n(x,m) is the number of times x and m have been observed together in the past, n(m) is the number of times m has been observed in the past, and H(x) is a pre-defined base distribution. An analogous definition holds for p\u03b9, ph and pt. The alpha parameter is set to 1 for all distributions, corresponding to a uniform Dirichlet prior across simplices. During training, we set a = 10 in pt to encourage exploration of different syntactic structures, and a = 0.25 in pw to produce more confident predicted word meanings, which we find helps stabilize syntax learning."}, {"title": "Training Algorithm", "content": "The parameter updates described in Section 2.3 require tracking the occurrences on which two different elements co-occur. For example, in pw, the probability of predicting the logical form Ax.Ay.lost x y to be realized as the word 'lost' depends on the number of times that logical form and word were observed together during training. Because we do not observe parse trees directly, we instead employ an expectation-maximization algorithm, as follows. When the model observes a single data point X, consisting of an utterance as a string and a corresponding logical form, it uses its current parameter values (0(t) to estimate a distribution over all possible parses that connect the two. The probability assigned to a parse tree T and the data point X is the following product\np(X,T|0(t)) = [Pt(S1, S2|s') [Pt(leaf|s)ph(es|s)pi(ms|es)Pw(xs|ms), (2)\ns'\nS\nwhere s' ranges over all non-leaf nodes in T, s\u2081 and s2 are the children of s', s ranges over all leaf nodes in T, and es, ms and xs are, respectively, the shell logical form, the logical form, and the word aligned to s in T. As we observe X, we are interested in the conditional probability of a given parse tree\np(T|X,0(t)) = p(X,T|0(t)) / (\u03a3T'\u2208T p(X, T'|\u03b8(t))), (3)\nwhere T is the set of all allowable parses of X. For each parse tree, the co-occurrences that it gives rise to are recorded in proportion to the parse tree's probability. Combining the standard expectation maximization (EM) update rule with the Bayesian update for the Dirichlet process, then, for each parameter 0 that tracks the co-occurrence of two elements a and b, the update rule is given by\n0(t+1) = 0(t) + ET~p(T|X,9(t)) [\u03b4T(a, b)],\nwhere dr(a, b) is an indicator function that is 1 if a and b co-occur in T and 0 otherwise.\nThe set T of allowable parse trees is the set of all valid CCG parse trees that have the observed logical form (LF) as root, the words in the observed utterance as leaves, and that have congruent syntactic and semantic types."}, {"title": "Worked Example", "content": "Here we present a worked example on a single training point. Recall that each training example consists of an utterance and corresponding logical form, and the learner considers the set T of all compatible parses, i.e. all parses with the observed LF as root, the observed utterance as leaves and that obeys the constraints described in Section 2.3. We describe the training updates for a single, correct parse for the example \"you lost a pencil\", as shown in Figure 3. For the purposes of this example, we show the exhaustive computation for every node in the tree. In practice the probabilities for upper nodes can be cached giving a large increase in efficiency.\nThe prediction of the tree proceeds from the root. First, the learner predicts a possible root category, here S with probability 0.388.\nNext, it selects a possible split of the root syntactic category S, here the selected split is into NP and S\\NP.\nThen, for the left child node, it predicts the probability for a split into two daughter syntactic categories, or alternatively that this node is a leaf. Here, the prediction is that the node is a leaf, with probability 0.738. Then it predicts probabilities for a shell logical form given the syntactic category ('entity' with probability 0.66), a logical form given this shell LF (you with probability 0.327) and word (or span of words) given this LF (\u201cyou\u201d with probability 0.912).\nMeanwhile, for the right child of the root, it predicts, with probability 0.549, a split into further categories of S\\NP/NP and NP.\nThen for the left child of this node, it makes the equivalent predictions it made for the far left node, predicting a split or leaf given the syntactic category ('leaf' with probability 0.989), then of shell LF given syntactic category (Ax.Ay.vconst x y with probability 0.961), of LF given shell LF (Ax.dy.losepast x y with probability 0.012) and word span given LF (\u201clost\u201d with probability 0.862).\nFor the right child of this node, the right-most NP in Figure 3, it predicts a split into NP/N and N, with probability 0.261.\nFor the NP/N daughter, it predicts \u2018leaf' with probability 0.994, then a shell LF given syntactic category (Ax.quant x with probability 0.999), of LF given shell LF (Ax.Ay.det : art|a; x with probability 0.486) and word span given LF (\u201ca\u201d with probability 0.95).\nFinally for the N daughter node it predicts 'leaf' with probability 0.994, noun with probability 1.0 (note these figures are rounded to three places), n|pencil with probability 0.015, and \u201cpencil\u201d with probability 0.973.\nAt this point, the semantic types of each node can be inferred. The rightmost npencil node, in virtue of its n CHILDES pos tag, is inferred to have semantic type <e,t>. The Ax. det:arta x node to its left has type <<e,t>,e>. The parent of these two nodes then gets type <e>. Meanwhile the Ax.xy.v|losepast x y has two allowable types based on the v, tag, <e,t> and <e,<e,t>>. Only the latter is compatible with it having two lambda binders, so the former is discarded. Together with its sibling <e> node, this gives its parent type <e,t>. Finally, together with its sibling, which is the leftmost leaf that has the tag 'pro:per' and so gets the semantic type <e>, these give the parent, which is the root node, type <t>. For all of these nodes, the semantic type is compatible with number of lambda binders and the CCG category, therefore the tree is used for training.\nThe total probability for this tree and leaves given the root LF is then computed by multiplying all the above probabilities:\n0.388 \u00d7 0.738 \u00d7 .66 \u00d7 .327 \u00d7 .912 \u00d7 .549 \u00d7 .961 \u00d7 .012 \u00d7.862\u00d7\nx.261 \u00d7 .994 \u00d7 .999 \u00d7 .486 \u00d7 .95 \u00d7 .994, \u00d71.0 \u00d7 .015 \u00d7 .973 = 5.281e - 7.\nGiven that we observe the leaves, we condition on this event by diving by the sum of the probabilities of all elements of T. Here, that sum turns out to be 5.888e7, so the conditional probability for the tree in Figure 3 is\n5.281e-7 / 5.888e 7 \u2248 0.896.\nThus, in the Dirichlet processes that are used to make all model predictions, we update the counts of the co-occurrences in this tree by 0.896. An example of a co-occurrence in this tree is of the LF youpro:per with the word span \u201cyou\u201d. This is shown in the bottom left of Figure 3. The number of \u2018times' youpro:per has been observed to co-occurr with \"you\" is increased by 0.896.\nThis same procedure is repeated for every element of T. In practice, for the corpora we use, this is generally 50 - 100 trees."}, {"title": "Results", "content": "In addition to the straightforward SVO examples in Figures 1 and 2, the LFs can express more complex syntactic features such as modals, negation, prepositional phrases, and relative clauses. We post-process this data to exclude words that serve only as discourse markers and do not appear in the LFs or receive the CHILDES part-of-speech tag 'co', meaning 'communicator', which includes most instances of words like 'so', 'well' and the child's name. This results in 19314 tokens and 5320 utterances for Adam (English) and 6187 tokens and 3295 utterances for Hagar (Hebrew). As each data point contains exactly one utterance (as well as the corresponding LF), these are also the numbers of data points in each dataset."}, {"title": "Word Order", "content": "Following the procedure of Abend et al. (2017), we measure the acquisition of grammar and lexicon by examining the model's implicit word order parameters as a proxy. The degree to which the model favours each of the six possible word orders is determined by (a) the probability it assigns to the two CCG splits that are necessary to parse a simple transitive sentence under that order, and (b) the probability it assigns to the respective order in which the subject and object combine with the verb under that order. We assume that the verb-medial orders, SVO and OVS, must combine with the object first, so the six different possibilities are measured as follows:\np(SOV) := pt((NP, S\\NP)|S)pt((NP, S\\NP\\NP)|S\\NP)ph(\u03bb\u03c7.\u03bby.vconst y x|(S\\NP\\NP))\np(SVO) := Pt((NP, S\\NP)|S)pt((S\\NP/NP, NP)|(S\\NP))ph(\u03bb\u03c7.\u03bby.vconst y x|(S\\NP/NP))\np(VSO) := pt((S/NP, NP)|S)pt((S/NP/NP, NP)|S/NP)ph(\u03bb\u03c7.\u03bby.vconst x y|(S/NP/NP))\np(OSV) := Pt((NP, S\\NP)|S)pt((NP, S\\NP\\NP)|S\\NP)ph(\u03bb\u03c7.\u03bby.vconst x y|(S\\NP\\NP))\np(OVS) := Pt((S/NP, NP)|S)pt((NP, S/NP\\NP)|(S/NP))ph(\u03bbx.\u03bby.vconst y x|S/NP \\NP)\np(VOS) := pt((S/NP, NP)|S)pt((S/NP/NP, NP)|S/NP)ph(\u03bb\u03b1.\u03bby.vconst y x|(S/NP/NP)).\nSo, for example, the probability of SOV is product of the probability of splitting an S into NP and S\\NP, the probability of further splitting the resulting S\\NP into NP and S\\NP\\NP, and the probability of this S\\NP\\NP node having a logical form that takes two arguments and places the first in the object position and the second in subject position. (We use the convention that the argument immediately to the right of the verb is the subject.) The third term is what distinguishes p(SOV) from p(OSV).\nFigures 4 and 5 take the relative values of these six scores, by normalizing so they sum to 1, and show how the above measure of word order preference changes over the course of training, for Adam (English) and Hagar (Hebrew) respectively. In both cases, the model succeeds in learning the correct SVO order, but this is faster and more extreme in Adam (English). In Hagar (Hebrew), the learning curve also appears somewhat step-shaped, with the SVO probability jumping up at a number of points, rather than increasing smoothly.\nWe hypothesize that, when learning an ordered category for an unknown word"}, {"title": "Word Meaning and Syntactic Category", "content": "Going beyond this emergent favouring of SVO word order, what we are ultimately interested in learning is the lexicon, which relates words to pairs of syntactic categories and meaning representations. To evaluate this, we measure the model's prediction for logical form and syntactic category. For each dataset, we select the 50 most common words, and annotate them with a ground-truth logical form and syntactic category. The full CCG lexicon could contain many possibilities for each word, but we restrict to those that are attested in our corpora.\nWe then extract a predicted logical form m' for each word x as follows:\nm' = argmax P(m|x) = argmax  (P(x/m)P(m) / P(x)) = argmax P(x|m)P(m) \u2248 argmax pw(x|m)pu(m) = argmax pu (x,m), (4)\nm m m m\nwhere the last quantity is determined by a Dirichlet process and so can be approximated by the observed number of times that w and m co-occur (recall this is in fact the sum of the expected values of their co-occurrence across all data points).\nSimilarly, we extract a predicted syntactic category for each word as follows:\ns' = argmax P(s|x) = argmax  (P(xls)P(s) / P(x)) = argmax P(x|s)P(s) =\nS S S\n= argmax \u03a3\u03a3 P(x, m, e\\s)P(s) \u2248 \u2211\u2211Pw(x\\m)pi(m\\h)ph(e/s)Psyn(s), (5)\nS m e m e\nand again, the last quantity can be computed straightforwardly, this time as a product of terms from each of the model's Dirichlet processes.Table 3 reports the percentage of points for which the predicted meaning representation, as per (4) and the predicted syntactic category, as per (5) agree with the manually annotated ground truth. For both datasets, the model achieves 100%"}, {"title": "Distractor Settings", "content": "To test the robustness of word-order learning to noise, we follow Abend et al. and consider a setting in which an utterance is paired not with a single logical form representing its meaning, but with multiple logical forms, only one of which represents its true meaning. The learner is then free to consider any of these LFs as the meaning of the utterance. Formally, what this means is that parse trees are computed for each of these LFs, and all of them are placed in the set T. Then, when calculating the Bayesian posterior, as per (3), the denominator is larger, so the probability on any given tree is smaller, as compared to the no-distractor setting.\nThis makes learning more difficult, and simulates the fact that there may be some uncertainty for the child as to the meaning a given utterance represents. When there is a single tree that the model is very confident in, then the probability from this tree dominates anyway, and overall there is little effect from the distractor trees. However, when there is no such single confident interpretation, the distractor trees significantly reduce the probability on the trees from the correct LF, including the correct tree, and so dilute the learning effect.\nThe other, distractor, logical forms are taken from the utterances immediately following and preceding the given utterance. Specifically, the n distractor setting takes the [n/2] previous examples and the [n/2] following examples.\nFor example, in Adam, data points 226-228 are as follows:\nData point 226: you blow it-blow you it\nData point 227: you can blow-can (blow you)\nData point 228: you do it-do you it\nThus, in the two distractor setting, when training on data point 227, we include the parse trees from all three of these LFs. In this case, one possible interpretation takes the LF from data point 226blow(you,it)-and interprets you as meaning pro:per you, can as meaning pro:per it, blow as meaning Ax.dy.v|blow y x, and the sentence as being in SOV order. However, by this stage in training, the model has already learnt to place very little probability on the splits required for SOV, in particular splitting S\\NP as NP + S\\NP\\NP, so this incorrect interpretation has a small probability and doesnt affect training much.\nAs shown in Figure 8, for Adam (English), the learner is still capable of learning the correct SVO order in all distractor settings. This is an improvement upon the version in Abend et al. (2017), where dealing with distractor settings required the introduction of an extra 'learning rate' parameter, that had to be set to different values for different numbers of distractors. In Appendix E, we present results for higher numbers of distractors, and show that, on Adam, the learner can handle up to 12 before performance starts to substantially degrade.\nFor Hagar (Hebrew), however, as shown in Figure 9, the model is not yet able to handle the distractor settings and fails to learn SVO. This fits with the picture, outlined above, that word-order is learnt correctly on Hagar (Hebrew), though currently not with as much confidence or robustness as on Adam (English)."}, {"title": "Discussion", "content": "Our approach differs theoretically from other recent approaches to language acquisition. Ambridge (2020) argues that language acquisition can be understood purely based on the recall of all past occasions on which an utterance was used."}, {"title": "Limitations", "content": "One limitation of our learner is that it does not model anything below the token level, the tokens being taken from the data of Szubert et al. (2024), which in turn took them from the CHILDES parses. The issue highlighted above of the sparsity of word forms suggests a future extension to allow it to guess a meaning for new words if they are similar in form to familiar words. This requires it to learn some internal structure to these tokens in virtue of which they can be similar or dissimilar to one another. One way to do this would be by explicitly adding morphology, and allowing the parse tree to extend not to word boundaries but to morpheme boundaries. Another option would be to add a neural word predictor. A character-level language model could be used to produce vectors for each word that depend on their structure, which could then be fed into a multi-layer perception. Different inflected forms of the same root should then have a similar word vector and so a similar predicted meaning. Note, this approach does not mean a replacement of anything that is currently in the model, it would model only morphology, not syntax or semantics. It may require more data but potentially be more robust to the variety of inflected forms. Either of these additions could be learned independently of the model described here or in conjunction.\nThis need for the learner to discern a similarity between different inflected forms is obscured by the very sparse morphology in English. Different thematic roles for the same word or nominal phrase are not distinguished by morphological case as they are in Hebrew, so the model does not need to treat them as separate lexical entries. This further highlights the value of testing computational CLA models on multiple languages, and future work includes testing on further languages in addition to English and Hebrew.\nAnother limitation concerns our method of evaluating our model. Measuring the relative preference for different word orders allows comparison with Abend et al. (2017), but could give a misleading result in certain contexts where the correct analysis is a non-standard word order, e.g. in topicalization. In future, we hope to adopt a richer and more diverse evaluation suite, including measuring the fraction of test utterances with the correct inferred root LF and parse tree.\nThirdly, a more thorough evaluation of our learner involves testing on a more diverse set of languages, with larger and more comparable corpora. In contrast to the two corpora we use here, which differ in the number of tokens and utterances. It would also be interesting to compare different corpora for different children within the same language."}, {"title": "Conclusion", "content": "This paper reimplemented a recent computational model for child language acquisition, based on semantic bootstrapping, which learns from real transcribed child-directed utterances paired with annotated logical forms as meaning representations. We replicated the original results from this model on English, and performed the same evaluation on Hebrew. The results show that the ability of the model generally transfers well to a new language, but its learning on Hebrew is slower and less robust than on English. Further analysis reveals this, not surprisingly, to be due in large part to the richer morphology in Hebrew producing a more diverse set of word forms. Future work includes the extension of the model to detect and leverage similarities between word forms, application to other languages, and testing on corpora with equal numbers of utterances and tokens."}]}