{"title": "Skill Learning Using Process Mining for Large Language Model Plan Generation", "authors": ["Andrei Cosmin Redis", "Mohammadreza Fani Sani", "Bahram Zarrin", "Andrea Burattin"], "abstract": "Large language models (LLMs) hold promise for generating plans for complex tasks, but their effectiveness is limited by sequential execution, lack of control flow models, and difficulties in skill retrieval. Addressing these issues is crucial for improving the efficiency and interpretability of plan generation as LLMs become more central to automation and decision-making. We introduce a novel approach to skill learning in LLMs by integrating process mining techniques, leveraging process discovery for skill acquisition, process models for skill storage, and conformance checking for skill retrieval. Our methods enhance text-based plan generation by enabling flexible skill discovery, parallel execution, and improved interpretability. Experimental results suggest the effectiveness of our approach, with our skill retrieval method surpassing state-of-the-art accuracy baselines under specific conditions.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks and have shown some capacity for logical reasoning [22,12]. However, their performance tends to decline as the complexity of problems increases, particularly in tasks requiring intricate reasoning or multi-step planning [8,20]. To enhance LLM performance on reasoning tasks, researchers have incorporated tools and plan generation capabilities, enabling LLMs to function as agents that generate sequences of tool invocations to solve given problems [2,24,5].\nDespite these advancements, existing plan generation methods, especially text-based planners, face significant challenges when dealing with complex tasks [20]. Text-based planners typically produce flat sequences of actions without an underlying structured control flow model [24]. This lack of control flow limits their ability to generalize plans to other problems, making them less adaptable to varying parameters and conditions. Although these plans may include parameters, they are still less generalizable and often require adjustments or replanning when faced with new tasks. Additionally, the linear sequences are less interpretable, as the absence of logical structure makes it challenging for humans to understand the rationale behind the plans. Furthermore, the inability to identify parallelizable actions leads to less efficient execution, as tasks are processed sequentially without exploiting opportunities for concurrent execution.\nIn contrast, code-based planners [24] inherently incorporate structured control flow models, such as functions, loops, and conditionals, allowing for flexible and adaptable plan generation. This flexibility likely enables effective skill learning, as observed in works like Voyager by Wang et al. [21], where previously generated plans (skills) can be reused and adapted to new problems. The structured control flow in code-based planners facilitates the grouping of related plans and the dynamic adjustment of actions based on different parameters and conditions.\nThe problem we address in this paper is the lack of structured control flow models in text-based LLM planners, which hinders their ability to perform efficient skill learning and limits their effectiveness in generating plans for complex tasks. Without a control flow model, text-based planners cannot effectively group related plans or identify parallelizable actions, resulting in sequential execution and increased latency.\nTo overcome this limitation, we propose a novel skill learning framework for text-based LLM planners that integrates process mining techniques [1] to extract structured control flow models from flat action sequences. Process mining allows us to discover process models from execution traces, providing a structured representation of the control flow underlying the sequences generated by text-based planners. By incorporating these process models, we enable text-based planners to benefit from structured control flow, similar to code-based planners, thereby enhancing skill learning and plan generation capabilities.\nOur approach addresses the limitations of text-based planners by:\n1. Enabling flexible skill discovery and storage: By discovering process models from action sequences, we can capture general behaviors and reuse skills across similar problems, reducing the reliance on generating plans from scratch.\n2. Supporting parallel execution of actions: The structured control flow models identify ordering constraints and parallelizable tasks, allowing for parallel execution where appropriate, thus reducing execution time and service latency.\n3. Improving interpretability and reliability: Structured process models could enhance the interpretability of the LLM's decision-making process, facilitating debugging and optimization by developers and users.\nAs illustrated in Fig. 1, when given a prompt such as \"Arrange my meeting tomorrow with John,\" the LLM plan generator can retrieve the \"meeting\" skill"}, {"title": "2 Related Work", "content": "In addition to the related work discussed in the introduction, several other studies intersect with the field of process mining. Fettke et al. corroborate our findings from developing our approach, asserting that despite the compartmentalized research, AI planning, machine learning, and process mining share common objectives, making collaboration advantageous [7]. Moreover, [6] explores the potential of utilizing LLMs for event log abstraction and process automation.\nRegarding process mining and LLMs, research has primarily concentrated on employing LLMs to perform process mining tasks. Despite the existing research into prompt engineering for process mining [11], process question answering [3], and event log data pre-processing [9], the application of process mining methods to assist with LLM tasks remains largely uncharted."}, {"title": "3 Skill Learning Using Process Discovery", "content": "Text-based LLM planners typically generate flat sequences of actions to solve tasks. While effective for simple scenarios, these linear plans lack the structured control flow needed for complex tasks involving parallelism and reusability. To address this limitation, we propose a process mining method to discover structured control flow models skills from these flat action sequences.\nOur approach takes the action sequences generated by the LLM planner as input and applies process discovery techniques, such as the Inductive Miner algorithm [13], to infer general process models. These models capture the underlying control flow, including sequential and parallel relations between actions, providing a more expressive and flexible representation than flat plans.\nStoring these skills as process models in a skill library enable the LLM planner to reuse previous solutions when faced with similar problems, reducing the need to generate new plans from scratch. This approach is analogous to the skill learning observed in code-based planners like Voyager [21] but applied to text-based planners without requiring them to generate code.\nThe skill learning framework overview is shown in Fig. 2. The inputs are problems, and their corresponding flat plans are generated by the LLM. We transform these plans into structured process models (skills) through process mining, which are then stored in the skill library for future retrieval.\nBy integrating process mining into the skill learning process, our method enhances text-based LLM planners by providing them with reusable, interpretable, and parallelizable skills derived from their own generated plans. This bridges the gap between text-based and code-based planners, enabling text-based planners to handle more complex tasks effectively while maintaining their simplicity."}, {"title": "4 Skill Retrieval Using Conformance Checking", "content": "To enhance both the accuracy and interpretability of skill retrieval in LLMs, we propose two methods based on conformance checking: (1) retrieval solely using conformance checking and (2) a two-stage retrieval method that combines text semantic similarity with conformance checking. The overall pipeline is shown in Figure 1.\n1. Conformance Checking Only: This method exclusively relies on conformance checking. This process mining technique assesses how well the control flow of a candidate skill's process model aligns with a bare-bones LLM-generated plan, referred to as a thought. We define thought as a partial plan embodying a full planning trace but not needing to be fully grounded in the state space. I.e., a plan that does not consider information received from the execution of tools, such as would be the case with [23]. The key metric is alignment fitness, which measures the degree of structural match between the generated plan and the stored process models. Focusing on structural alignment rather than textual similarity offers superior interpretability, making it easier to understand why a particular skill was retrieved. The direct comparison of control flows ensures that the retrieved skills are relevant and logically compatible with the problem at hand.\n2. Two-Stage Retrieval: This hybrid method begins with a rapid filtering stage using text semantic similarity. Embedding models like the Universal Sentence Encoder (USE) or OpenAI's ada-002 generate vector representations of the problem description, and cosine similarity is used to identify the top-k candidates. These shortlisted candidates are then reranked using conformance checking based on alignment fitness. This two-stage approach balances computational efficiency with high retrieval accuracy and enhanced interpretability, as the final ranking is based on the logical structure of the process models rather than solely on textual descriptions.\nBoth methods aim to outperform existing baselines that rely primarily on semantic similarity between text descriptions. By integrating process model alignment, these methods improve the precision of skill retrieval and significantly enhance the retrieval process's interpretability. This ensures that the logic behind the skill selection is transparent, which is crucial for debugging and optimizing LLM-driven plan executions. Additionally, the alignment values obtained from conformance checking can help assess the quality of the generated plan."}, {"title": "5 Evaluation and Discussion", "content": "In this section, we aim to answer the following questions: Given the current state of LLM plan generation, how feasible is it to learn skills using our proposed skill learning method? And, enabled by the skills learned, how does the accuracy of the proposed skill retrieval methods compare with previous approaches?"}, {"title": "5.1 Experiments", "content": ""}, {"title": "Skill Learning With Process Discovery", "content": "To evaluate the feasibility and effectiveness of our proposed skill learning method, we conducted experiments using the ProcessTBench synthetic dataset [16]. This experiment aimed to determine whether our method could reliably generate accurate process models (skills). We used conformance checking of the generated action sequences (traces) compared to ground-truth process models provided in the dataset.\nThe ProcessTBench synthetic dataset includes queries requiring LLM-generated plans, with corresponding solutions provided as action sequences using predefined tools. Each of the 532 problems represents a distinct process instance, with 5-6 LLM-generated action sequences serving as individual cases within these instances. The dataset simulates multiple process executions, where different paraphrasings of a query and various plans are treated as execution instances of the same process.\nTo evaluate the feasibility of our skill learning method, we used conformance checking to compare the LLM-generated plans (traces) from ProcessTBench with the ground truth process models provided by TaskBench. This part is illustrated in Fig. 3 3a:. We employed two widely-used conformance checking metrics, Replay Fitness and Alignment Fitness [10], to assess how well the generated plans align with the known process models. High conformance values indicate that accurate process models could be derived from these traces using process discovery, supporting our method's viability. For this task, ProcessTBench uses the GPT-4-0613 model."}, {"title": "Skill Retrieval With Conformance Checking", "content": "We conducted experiments using the TaskBench dataset, rephrasing 533 problems (queries) 5-6 times each to simulate scenarios where a similar problem requires a relevant solution from the skill library. We then tested different skill retrieval methods. We compared the baseline method that uses problem embeddings, our proposed model using conformance checking, and a hybrid model combining both approaches (i.e., two-stage retrieval). We aimed to determine if conformance checking could enhance retrieval accuracy compared to established similarity methods.\nThe architecture of the Skill Retrieval With Conformance Checking experiment is depicted in Fig. 3 3b:. This architecture has the following components.\n1. LLM Rephraser: Generates paraphrased descriptions of problems in English, Danish, and French. To improve its accuracy, the model was asked first to reason about the problem, as shown in [14].\nInput: Problem description from the TaskBench dataset [18]\nOutput: Rephrased problem descriptions\n2. DAG to Petri net Converter: Converts reference DAG process models to Petri nets.\nInput: A reference process model presented in the DAG format\nOutput: A process model presented in the Petri net format\n3. LLM plan (thought) generator: Generates partial plans (thoughts) using the given problem description and tools available in the TaskBench domain. We call these plans thoughts and partial to distinguish them from the output of more thorough and widely used planners such as ReAct [23]. I.e., due to experimental constraints, the thought generator is asked to return a full trace solution in one inference session, while ReAct builds the plan more thoroughly in inference iterations.\nInput: Problem description and all available tools in the TaskBench domain\nOutput: Plan that solves the problem as a sequence of actions.\n4. Classifier: Calculates a distance distribution between rephrased and original TaskBench problems. For conformance checking retrieval, we first generate a thought a partial plan representing the solution before calculating the distance. This thought is then used to compare the rephrased problem to other problems.\nInput: Rephrased problem\nOutput: Distance distribution over original problems.\nFor the classifier, the following two baseline and proposed models were used:\n1. Universal Sentence Encoder (USE): Measures semantic similarity using cosine distance between problem and skill embeddings [4].\n2. ada-002: A more recent embedding model by OpenAI 2.\n3. Conformance checking: Measures the alignment between the LLM-generated plan (thought) and the stored process models, using alignment fitness as a similarity measure [10].\n4. Hybrid (ada-002 + Conformance checking): Candidates are pre-filtered using ada-002 and re-ranked using conformance checking.\nThe retrieval models' performance was evaluated using two state-of-the-art metrics for multi-class classification and next-item recommender systems: F1-score and Mean Reciprocal Rank (MRR) [19]."}, {"title": "5.2 Skill Learning with Process Discovery", "content": "In this study, we evaluated the feasibility of our proposed skill learning method using the TaskBench [18] and the ProcessTBench [16] synthetic datasets. TaskBench contains various queries and their corresponding process models. Each query represents a separate process, and each specific instance of solving that query is considered a case. ProcessTBench extends TaskBench by providing repeated sequential planning traces for the problems.\nTable 1 summarizes the dataset characteristics, including the average number of cases per process, the number of activities (actions) per process, and the fitness metrics for conformance checking. The mean number of cases per process was 4.08, indicating a relatively small dataset, especially compared to popular real-world process mining datasets like the BPI Challenges, where the number of cases is significantly larger [17]. Despite the dataset's small size, the high average replay fitness (0.96) and alignment fitness (0.94) suggest that our skill learning method is feasible. These metrics indicate that the discovered skills closely matched the ground truth in many cases, with 75% processes achieving a perfect fitness score for all associated cases.\nFigure 4 illustrates the accuracy distribution of the generated traces. Replay fitness tends to be higher than alignments, suggesting the planner's difficulty assembling individual components into a coherent sequence. E.g., for a specific query in ProcessTBench, the ground truth process model was $\\wedge(\\rightarrow (A, B), \\rightarrow (C, D, E, F))$, but the planner produced the sequence E, F, A, B, C, D, resulting in a replay fitness of 0.9 and an alignment fitness of 0.66. Although the planner accurately captured most relationships between actions, it struggled with correctly ordering them. This suggests that sequence alignment, and consequently the creation of complete control flow models, could be particularly challenging for LLMS  a challenge that is somewhat expected given that reasoning is a known weakness."}, {"title": "5.3 Skill Retrieval with Conformance Checking", "content": "The results in Table 2 show that the ada-002 model outperforms the other models in terms of F1 score and MRR. The combination of ada-002 and Conformance Checking also delivers strong performance, nearly matching the accuracy of the ada-002 model. This suggests at first glance that the baseline method, using ada-002, is better for Skill Retrieval than our proposed methods using conformance checking.\nIt is worth observing that ada-002 performs 11% better than in the documentation provided by OpenaAI, the model's creators. This suggests that our rephrased"}, {"title": "6 Conclusion", "content": "Our experiments suggest the feasibility of using process mining techniques for skill learning in text-based LLM planners. By integrating conformance checking into skill retrieval, we may improve the accuracy and interpretability of plan generation. These advancements could pave the way for more efficient and reliable LLM-driven automation solutions. More interpretable LLM plan generation can give businesses greater control and transparency over automated processes. As LLMs' role in decision-making expands, effectively managing and understanding these systems will be increasingly important.\nIn the future, we aim to develop our experimental framework further. Generating more data and creating more sophisticated LLM plan generators will enable more general conclusions. Furthermore, controlling for more confounding variables, such as LLM and embedding models, would increase the robustness of the observed effects. We have developed a skill learning framework and presented its theoretical properties. Still, we have not created a skill \"usage\" framework to gather evidence for our approach's end-to-end effectiveness. Lastly, we have tested skill retrieval in a closed-set classification setting, but the open-set classification is also a realistic use case worth exploring."}]}