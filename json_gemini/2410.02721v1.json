{"title": "Domain-Specific Retrieval-Augmented Generation Using Vector Stores, Knowledge Graphs, and Tensor Factorization", "authors": ["Ryan C. Barron", "Vesselin Grantcharov", "Selma Wanna", "Maksim E. Eren", "Manish Bhattarai", "Nicholas Solovyev", "George Tompkins", "Charles Nicholas", "Kim \u00d8. Rasmussen", "Cynthia Matuszek", "Boian S. Alexandrov"], "abstract": "Large Language Models (LLMs) are pre-trained on large-scale corpora and excel in numerous general natural language processing (NLP) tasks, such as question answering (QA). Despite their advanced language capabilities, when it comes to domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations, knowledge cut-offs, and lack of knowledge attributions. Additionally, fine tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and time consuming process. The retrieval-augmented generation (RAG) process has recently emerged as a method capable of optimization of LLM responses, by referencing them to a predetermined ontology. It was shown that using a Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into account relevant sub-graphs that preserve the information in a structured manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM framework, that integrates RAG with KG and a vector store (VS) that store factual domain specific information. Importantly, to avoid hallucinations in the KG, we build these highly domain-specific KGs and VSs without the use of LLMs, but via NLP, data mining, and nonnegative tensor factorization with automatic model selection. Pairing our RAG with a domain-specific: (i) KG (containing structured information), and (ii) VS (containing unstructured information) enables the development of domain-specific chat-bots that attribute the source of information, mitigate hallucinations, lessen the need for fine-tuning, and excel in highly domain-specific question answering tasks. We pair SMART-SLIC with chain-of-thought prompting agents. The framework is designed to be generalizable to adapt to any specific or specialized domain. In this paper, we demonstrate the question answering capabilities of our framework on a corpus of scientific publications on malware analysis and anomaly detection.", "sections": [{"title": "I. INTRODUCTION", "content": "The expanding volumes of data across large databases and information collections necessitate the specialized extraction of pertinent knowledge, often without an in-depth understand-ing of the underlying database resources. Recent advancements"}, {"title": "II. RELATED WORKS", "content": "Recent methods for building RAG-assisted [6] chatbot ap-plications rely on unstructured text stored in vector databases for question answering (QA) tasks [10]. Although the integra-tion of knowledge graphs (KGs) in AI systems is not novel [11], increasingly, researchers are leveraging them to improve LLM reasoning while simultaneously addressing the reliability issues discussed in Section I [12]\u2013[14]. Despite the benefits, integrating domain-specific knowledge into chatbots requires substantial effort. Here, we review the prior work for common chatbot designs, the integration of domain-knowledge in RAG pipelines, and the steps required for constructing KGs."}, {"title": "A. KGs in RAG Pipelines", "content": "Building a sophisticated chatbot requires the knowledge of a wide range of research fields; hence, rarely do prior works present a fully engineered system like ours. Instead, most efforts focus on improving specific aspects of RAG pipelines, e.g., retriever design [15], [16], query intent recognition [17], and KG reasoning [18]-[21]. Our approach resembles past methods which leverage chain-of-thought [22] prompting on KGs [19], [20]; in conjunction with LLM-agents to enhance reasoning capabilities [23]-[25]. In addition to incorporat-ing these state-of-the-art techniques, we improve our RAG pipeline by modifying our retrieval method to use K-Nearest Neighbors with the Levenshtein metric instead of cosine distance as an entry point for context search. We also construct a \"highly-specific\" knowledge base for targeted QA tasks. Although expensive and time-consuming, a handful of prior works incorporate domain-knowledge into their RAG pipelines [26]-[29]; however, the majority either use existing KGs built broadly on medical literature [26], [28]; or do not disclose any details regarding their dataset construction [29]. We emphasize that our method is \u201chighly-specific\u201d because it was driven by subject matter expertise which informed our dataset curation and cleaning techniques [30], [31]."}, {"title": "B. KG Development", "content": "At a minimum, the development of knowledge graphs requires building a corpus, defining an ontology, and extracting the relevant entity-relation triplets from unstructured text. Corpus Building. Here we define the term \u201chighly-specific\u201d and explain our dataset collection method. A key feature of our dataset collection is the use of unsupervised methods [31] to decompose corpora into document clusters to finer specificity than the author-provided tags available on open ac-cess websites. This differs significantly from prior approaches [27], [32], [33]. We leverage latent-topic information from our NMFk method to filter and select the best data for our knowledge base, and prune documents based on cita-tion information and embedding distances. Our text cleaning pipeline is informed by subject matter experts (SME) [31], [34], thus going beyond standard methods by incorporating expert-derived rules for document cleaning, e.g, acronym and entity standardization. KG Construction. Our ontology is shaped by traditional methods, i.e., relying on SME design and capturing task-specific features. However, we innovate by incorporating latent information from our decomposition process [31] into our KG as entities. For entity and relation extraction, we move be-yond conventional learning-based techniques [35]; and instead, leverage recent advancements which use LLM-agents [10], [36] as opposed to other LLM prompting methods [37]-[40]. This approach yields non-sparse KGs, meaning, the average out-degree of entities [41], [42] is high. To our knowledge, no"}, {"title": "III. METHODS", "content": "This section outlines our framework, covering corpus ex-traction, KG ontology, VS construction, and the RAG process."}, {"title": "A. Domain-Specific Dataset", "content": "Overview of of our system is summarized in Figure 1. To collect the dataset, we began with a set of core docu-ments selected by subject matter experts (SMEs). Here, these core documents represent the specific domain in which we want to built our corpus on. These core documents were used to build a citation and reference network, which al-lowed for the expansion of the dataset through the autho-rized APIs: SCOPUS [43], Semantic Scholar (S2) [44], and Office of Scientific and Technical Information (OSTI) [45]. We also extract common bi-grams from the core docu-ments to query these APIs to search for relevant doc-uments. As we expand on the the corpus starting from the core documents, it is possible to add documents that do not directly relate to the information in the core documents. To main-tain the central quality and thematic coherence of the core dataset, we employed several pruning strategies to remove these irrelevant documents to preserve the speciality specific to the targeted domain. These strategies focused on removing documents that diverge from the central theme of the core. Pruning was performed through two methods from [34]:\n\u2022 Human-in-the-Loop Pruning: SMEs manually review and select a handful documents that align with the core theme. Here, we reduce the document's TF-IDF matrix to two dimensions with UMAP and let the SME look at the documents that are at the centroids of the given clusters. SME can then select which documents to remove.\n\u2022 Automatic Pruning of Document Embeddings: Based on the SME selections from the previous step, we next remove the document that are certain distance away from the selected and the core documents. Documents were transformed into embeddings with SCI-NCL [46], a BERT based model fine-tuned on scientific literature, to measure seman-tic similarity with core and SME selected documents. Those outside a set similarity threshold were removed, ensuring only the documents relevant to the core documents and SME selections remained.\nAlthough a human is in the loop, the system remains scalable by clustering documents. One review per cluster allows the operator to decide on all documents in the group, making it efficient even with large datasets without limit on cluster size. Additionally, we applied pre-processing techniques using a publicly available Python library, Tensor Extraction of Latent Features (T-ELF)\u00b9 [31]. The cleaning procedures involved the following pre-processing steps:"}, {"title": "B. Dimension Reduction", "content": "The extraction of the latent structure from the dataset is accomplished through the following approach. Initially, the data is prepared and the necessary computational framework is established through these steps:\n\u2022 Creation of the TF-IDF matrix, X, of the cleaned corpus\n\u2022 X is decomposed using nonnegative tensor factorization from T-ELF enhanced with our new binary search strategy [47], to classify document clusters. T-ELF allows us to extract highly specific features from the data. This method identifies latent topics within the corpus, grouping documents into clusters based on shared themes. To avoid over/under-fitting, automatic model determination is used where the final cluster counts are determined by achieving the highest silhouette scores above a predetermined threshold using the Binary Bleed method [47]. This method employs a binary search strategy across k values, selectively skipping those k values that do not surpass the silhouette threshold. The search criterion for an optimal k is defined as $k_{optimal} = max \\{k \\in \\{1, 2, ..., K\\} : S(f(k)) > T\\}$, where $S(f(k))$ denotes the silhouette score of the k-th configuration and T the threshold. Importantly, even after identifying an initial \"optimal\" k, higher k values are visited regardless to ensure no better configuration is overlooked.\nThe factorization of X yields two non-negative factor matri-ces $W \\in R^{m \\times k}$ and $H \\in R^{k \\times n}$, ensuring $X_{ij} \\approx \\sum_{s} W_{is}H_{sj}$. Distribution of words over topics are captured in W. The matrix H shows the topic distribution across documents, and is used to identify the predominant topic for each document in post-processing. Full tensor and matrix factorization imple-mentations of various algorithms are available in T-ELF 2."}, {"title": "C. Knowledge Graph Ontology", "content": "Features from T-ELF and document metadata is mapped into series of head, entity, and tail relations, forming direc-tional triplets, then injected into a Neo4j [48] KG.\nOur KG incorporates document metadata as well as the latent features. The primary source of information in the KG comes from documents, which are injected into the graph along with related attributes. Each document node contains information such as DOI, title, abstract, and source API document identifiers. Additional node labels include authors,"}, {"title": "D. Vector Store Assembly", "content": "To augment the RAG, we introduced a vector database for the original documents using Milvus [50]. Additionally, a sub-set of documents' full texts were vectorized and incorporated into the vector store. Full texts, when available, are segmented into smaller paragraphs, each assigned an integer ID to indicate its position within the original document. These paragraphs are then vectorized through the into embeddings using OpenAI's text-embedding-ada-002 [51] model and imported to the vector store to support the RAG process.\nThe RAG application can query the vector store to find relevant paragraph chunks from these full texts. If the retrieved text contains the needed information, the LLM can answer the posed question and include a cita-tion of the document, pre-cisely indicating the exact paragraph. If further related information is needed, the application can use docu-ment metadata (e.g., DOI, author) to expand its search through the KG. This approach allows us to preserve the semantics of the original documents and provide relevant responses."}, {"title": "E. Retrieval Augmented Generation", "content": "RAG is an NLP method that mixes retrieval and generation techniques to improve the accuracy and relevance of responses in generative AI. It works by first gathering infor-mation from an external knowledge base based on a user's query. This re-trieved information is then used to guide and enhance the outputs of the gen-erative model, leading to more relevant and context-aware responses. By inte-grating these tactics, RAG addresses the limitations of purely generative models and provides an adaptable frame-work suitable for applications demanding detailed and current information.\nFigure 2 demonstrates the data pipeline operated throughout the work for RAG. The process begins with a user query, which the LLM then uses to query the knowledge graph. The LLM transforms the query into a vector embedding. This embedding is compared to existing texts to find the most similar text. The retrieved information is appended to the original query, and the LLM produces a relevant answer using this context. Finally, the LLM constructs a final answer in natural language to explain the answer to the user's question.\nTo optimally leverage RAG, accurately understanding the user's question is crucial. Our RAG approach includes multiple potential routes depending on a user's question. The question routing pipeline may be a General Query, which calls the ReAct Agent Process [23], or a Specific Document Query, which calls either a Retrieved Query or a Synthesized Query. Understanding the question directs the information to the ap-propriate toolset and subsequent process. The routing process overview, as described below, can be seen in Figure 3.\nSpecific Document Query: If a user's question requires information from a specific document's text (title + abstract), it is better suited for a traditional RAG application in which the LLM interacts with the VS to find the needed text. In our case, we use a ReAct agent where the VS search is the sole tool, allowing the LLM to make multiple search requests as required. Specifically, a ReAct agent means the LLM has distinct steps for reasoning and acting after determining the input meaning. We use langgraph [53] to define an execution graph with three nodes, as illustrated in Figure 4: (1) the ReAct agent, (2) the tool executor, and (3) the end.\nReAct Agent Process: The agent node is the central part of the ReAct graph, where the LLM calls are encapsulated. The ReAct agent is responsible for collecting inputs, making actionable decisions, and explaining the results. The four prompt parts are:"}, {"title": "IV. RESULTS", "content": "In this section, we discuss identification of optimal clus-ters for tensor decomposition, vectorization of the dataset, construction of KG, and compare the system using the with GPT-4-instruct [51] as the operating model of SMART-SLIC to answer research questions. The same model was used to answer without RAG as well. Our findings highlight the accuracy and reliability of SMART-SLIC's RAG."}, {"title": "A. Dataset", "content": "Initially, 30 documents specializing on large-scale malware analysis and anomaly detection with tensor decomposition fields were selected by the SME as the core documents to construct the data. These documents were expanded along the citation/reference network 2 times. The final dataset was enumerated at 8,790 scientific publications. From the cleaned corpus, the tensor object was generated."}, {"title": "B. Extraction of Latent Features", "content": "After setting up the tensor, the most coherent grouping is de-termined by iterating through a range of k = {1, 2, 3, ..., 45} clusters to decompose. Our analysis determined that 25 topic-clusters represented the optimal division across all evaluated k values. The decomposition itself was executed using T-ELF on high-performance computing resources, specifically two AMD EPYC 9454 48-Core Processors. This setup provided a total of 192 logical CPUs, enabling us to complete the entire de-composition process in approximately 2 hours. Following the"}, {"title": "C. Vector Store", "content": "The 8,790 documents were vectorized and ingested into the Milvus vector store. When questions are posed to the framework, they are also vectorized using this model. Of the total documents, 22% had full-texts available, which were vectorized into the Milvus. Each document and full-text had a DOI, with the full-texts also including paragraph identifiers."}, {"title": "D. Knowledge Graph", "content": "From the 25 clusters output form T-ELF, we formatted the the data into 1,457,534 triplets. Once injected into the knowledge graph, there were 321,122 nodes and 1,136,412 edge relationships. The nodes injected into the graph are represented in Figure 5, where they are organized into 16 base categories, referred to as labels, that define the foundational classes for the injection process. Once the graph was built was"}, {"title": "E. Question Answering Validation", "content": "The raw data collected was analyzed using document-specific questions in Zero-Shot Conditioning, including:\n\u2022 How many citations are there for DOI?\n\u2022 How many references are there for DOI?\n\u2022 How many authors are there for DOI?\n\u2022 What year was DOI published?\n\u2022 Which publisher published DOI?\n\u2022 What is the title of DOI?\n\u2022 How many scopus categories are assigned to DOI?\nAfter document specific questions, we then examined topic specific questions, which included year variations, as in:\n\u2022 How many papers are there on the topic of Topic?\n\u2022 How many papers were written related to Topic in Year?\nIn total, there were 200 questions in this set. Using these questions, in this study, we compare the performance of GPT-4-instruct [51] with and without our RAG framework on both topic-specific and document metadata questions. As shown in Figure 7, our findings indicate that GPT-4 with RAG answers all questions with a 97% accuracy rate. In contrast, without RAG, GPT-4 abstains from answering 40% of the questions, and the accuracy of the answered questions drops to 20%. A similar trend is observed for topic-based questions, where the specialized RAG significantly enhances the retrieval of correct answers. The topic questions attempted with RAG was also 100%, but without was only 36%. In consideration of only the attempted questions, the system with RAG answered the topic questions correctly 92%. Without RAG, the LLM answered the topic questions with 27.77% accuracy.\nWithout RAG, several questions about years were answered incorrectly, with the system stating the year didn't exist. The LLM also struggled with author and reference details, often asking for more information or recommending consulting a human expert. In some cases, it noted its lack of internet access but later suggested using Google Scholar, yet still provided inaccurate responses.\nThe analysis highlights the effectiveness of SMART-SLIC in accurately answering nearly all 200 research queries, cov-ering both document-specific and topic-specific questions. In contrast, GPT-4-instruct without RAG struggled. SMART-SLIC excelled in handling complex queries related to cita-tions, references, authorship, and topical relevance, underscor-ing the advantages of integrating its methodologies for more reliable AI-driven research tools and improved information retrieval in scientific fields.\nWe then shifted to more complex questions requiring thor-ough searches through various retrieval methods, using GPT-4-instruct with and without RAG. The SME posed the following questions, and the responses are from the SMART-SLIC RAG process, with DOI citations provided in square brackets:\n1) What challenge is associated with outlier detection in high-dimensional data? The challenge associated with outlier detec-tion in high-dimensional data is the \"curse of dimensionality,\" which makes it difficult to detect outliers in the full data space [10.1109/ICCC54389.2021.9674561].\n2) Can tensor decomposition methods be leveraged to detect SCADA anomalies? Yes, tensor decomposition methods can be leveraged to detect SCADA anomalies. The use of non-negative tensor decom-position, specifically the Canonical Polyadic Alternating Poisson Re-"}, {"title": "V. CONCLUSION", "content": "Our SMART-SLIC framework leverages advanced lan-guage models and specialized tools to effectively address user queries by categorizing them into Specific Document Queries and General Queries for efficient processing. The ReAct agent manages general inquiries, while NER and cypher query generation handle document-specific questions.\nLLMs excel in general NLP tasks but struggle in domain-specific areas due to hallucinations, knowledge cut-offs, and lack of attribution. Our system addresses this by integrating RAG with a domain-specific KG and VS, enhancing reliability without fine-tuning. Built using NLP, data mining, and non-negative tensor factorization, this setup enables accurate attri-butions, reduces hallucinations, and excels in domain-specific queries, as shown in malware analysis research.\nThe framework significantly enhances query response ac-curacy and reliability, making it adaptable to various ap-plications. Future work will expand the framework's use across domains like robotics, materials science, legal cases, and quantum computing. Enhancements in graph completion, entity linking, and link prediction will further interconnect"}]}