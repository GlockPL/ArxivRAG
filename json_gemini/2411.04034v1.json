{"title": "Non-Stationary Learning of Neural Networks\nwith Automatic Soft Parameter Reset", "authors": ["Alexandre Galashov*", "Michalis K. Titsias", "Andr\u00e1s Gy\u00f6rgy", "Clare Lyle", "Razvan Pascanu", "Yee Whye Teh", "Maneesh Sahani"], "abstract": "Neural networks are traditionally trained under the assumption that data come\nfrom a stationary distribution. However, settings which violate this assumption are\nbecoming more popular; examples include supervised learning under distributional\nshifts, reinforcement learning, continual learning and non-stationary contextual\nbandits. In this work we introduce a novel learning approach that automatically\nmodels and adapts to non-stationarity, via an Ornstein-Uhlenbeck process with an\nadaptive drift parameter. The adaptive drift tends to draw the parameters towards\nthe initialisation distribution, so the approach can be understood as a form of\nsoft parameter reset. We show empirically that our approach performs well in\nnon-stationary supervised and off-policy reinforcement learning settings.", "sections": [{"title": "Introduction", "content": "Neural networks (NNs) are typically trained using algorithms like stochastic gradient descent (SGD),\nassuming data comes from a stationary distribution. This assumption fails in scenarios such as\ncontinual learning, reinforcement learning, non-stationary contextual bandits, and supervised learning\nwith distribution shifts [20, 53]. A phenomenon occurring in non-stationary settings is the loss of\nplasticity [12, 2, 13], manifesting either as a failure to generalize to new data despite reduced training\nloss [4, 2], or as an inability to reduce training error as the data distribution changes [13, 37, 1, 42, 34].\nIn [38], the authors argue for two factors that lead to the loss of plasticity: preactivation distribution\nshift, leading to dead or dormant neurons [47], and parameter norm growth causing training instabili-\nties. To address these issues, strategies often involve hard resets based on heuristics like detecting\ndormant units [47], assessing neuron utility [13, 12], or simply after a fixed number of steps [43].\nThough effective at increasing plasticity, hard resets can be inefficient as they can discard valuable\nknowledge captured by the parameters.\nWe propose an algorithm that implements a mechanism of soft parameter resets, in contrast to the\nhard resets discussed earlier. A soft reset partially moves NN parameters towards the initialization"}, {"title": "Non-stationary learning with Online SGD", "content": "In a non-stationary learning setting with changing data distributions $p_t(x, y)$, where $x \\in R^L$, $y \\in R^K$,\nwe define the loss function for parameters $\\theta \\in R^D$ as\n$L_t(\\theta) = E_{(x_t,y_t)\\sim p_t} L_t(\\theta, x_t, y_t)$\nOur goal is to find a parameter sequence $\\Theta = (\\theta_1, ..., \\theta_T)$ that minimizes the dynamic regret:\n$R_T(\\Theta, \\Theta^*) = \\frac{1}{T} \\sum_{t=1}^T (L_t(\\theta_t) - L_t(\\theta_t^*)),$\nwith a reference sequence $\\Theta^* = (\\theta_1^*,...,\\theta_T^*)$, satisfying $\\theta_t^* = \\text{arg min}_\\theta L_t(\\theta)$. A common approach\nto the online learning problem is online stochastic gradient descent (SGD) [23]. Starting from initial\nparameters $\\theta_0$, the method updates these parameters sequentially for each batch of data $\\{(x_i, y_i)\\}_{i=1}^B$\ns.t. $(x_i, y_i) \\sim p_t(x_t, y_t)$. The update rule is:\n$\\theta_{t+1} = \\theta_t - \\alpha_t\\nabla_\\theta L_{t+1}(\\theta_t)$,\nwhere $\\nabla_\\theta L_{t+1}(\\theta_t) = \\sum_{i=1}^B \\nabla_\\theta L_{t+1}(\\theta_t, x_i, y_i)$ and $\\alpha_t$ is learning rate. See also Appendix G for\nthe connection of SGD to proximal optimization.\nConvex Setting. In the convex setting, online SGD with a fixed learning rate $\\alpha$ can handle non\nstationarity [56]. By selecting $\\alpha$ appropriately \u2013 potentially using additional knowledge about the\nreference sequence-we can optimize the dynamic regret in (2). In general, algorithms that adapt to\nthe observed level of non-stationarity can outperform standard online SGD. For example, in [29], the\nauthors propose to adjust the learning rate $\\alpha_t$, while in [21] and in [29], the authors suggest modifying\nthe starting point of SGD from $\\theta_0$ to an adjusted $\\theta$ proportional to the level of non-stationarity.\nNon-Convex Setting. Non-stationary learning with NNs is more complex, since now there is a\nchanging set of local minima as the data distribution changes. Such changes can lead to a loss of\nplasticity and other pathologies. Alternative optimization methods like Adam [30], do not fully\nresolve this issue [13, 37, 1, 42, 34]. Parameter resets [13, 48, 12] partially mitigate the problem, but\ncould be too aggressive if the data distributions are similar."}, {"title": "Online non-stationary learning with learned parameter resets", "content": "Notation. We denote by $N(\\theta; \\mu, \\sigma^2)$ a Gaussian distribution on $\\theta$ with mean $\\mu$ and variance $\\sigma^2$.\nWe denote $\\theta^i$ the i-the component of the vector $\\theta = (\\theta^1,...,\\theta^D)$. Unless explicitly mentioned,\nwe assume distributions are defined per NN parameter and we omit the index i. We denote as\n$\\mathcal{L}_{t+1}(\\theta) = -\\text{log } p(Y_{t+1}|X_{t+1}, \\theta)$ the negative log likelihood on $(y_{t+1}, X_{t+1})$ for parameters $\\theta$.\nWe introduce Soft Resets, an approach that enhances learning algorithms on non-stationary data\ndistributions and prevents plasticity loss. The main idea is to assume that the data is generated in"}, {"title": "Ornstein-Uhlenbeck parameter drift model", "content": "We motivate the specific choice of the drift model which is useful for maintaining plasticity. We\nassume that our Neural Network has enough capacity to learn any stationary dataset in a fixed number\nof iterations starting from a good initialization $\\theta_0 \\sim p_0(\\theta)$ [see, e.g., 24, 16]. Informally, we call the\ninitialization $\\theta_0$ plastic and the region around $\\theta_0$ a plastic region.\nConsider now a piecewise stationary datastream that switches between a distribution $p_a$, with a set\nof local minima $M_a$ of the negative likelihood $\\mathcal{L}(\\theta)$, to a distribution $p_b$ at time $t + 1$, with a set of\nlocal minima $M_b$. If $M_b$ is far from $M_a$, then hard reset might be beneficial, but if $M_b$ is close to $M_a$,\nresetting parameters is suboptimal. Furthermore, since $\\Theta$ is high-dimensional, different dimensions\nmight need to be treated differently. We want a drift model that can capture all of these scenarios.\nDrift model. The drift model $p(\\theta_{t+1}|\\theta_t, \\gamma_t)$ which exhibits the above properties is given by\n$p(\\theta|\\theta_t, \\gamma_t) = N(\\theta; \\gamma_t\\theta_t + (1 - \\gamma_t)\\mu_0; (1 - \\gamma_t^2)\\sigma_0^2),$\nwhich is separately defined for every parameter dimension $\\theta^i$ where $p_0(\\theta^i) \\sim N(\\theta^i; \\mu_0^i; [\\sigma_0^i]^2)$ is\nthe per-parameter prior distribution and $\\gamma_t = (\\gamma_t^1,......,\\gamma_t^D)$. The model is a discretized Ornstein\nUhlenbeck (OU) process [50] (see Appendix A for the derivation).\nThe parameter $\\gamma_t \\in [0,1]$ is a drift parameter and controls the amount of non-stationarity in each\nparameter. For $\\gamma_t = 1$, there is no drift and for $\\gamma_t = 0$, the drift model reverts the parameters back to\nthe prior. A value of $\\gamma_t \\in (0, 1)$ interpolates between these two extremities. A remarkable property\nof (5) is that starting from the current parameter $\\theta_t$, if we simulate a long trajectory, as $T \\rightarrow \\infty$, the\ndistribution of $p(\\theta_T|\\theta_t)$ will converge to the prior $p(\\theta_0)$. This is only satisfied (for $\\gamma_t \\in (0, 1)$) due\nto the variance $(1 - \\gamma_t^2)\\sigma_0^2$. Replacing it by an arbitrary variance $\\sigma^2$ would result in the variance\nof $p(\\theta_T|\\theta_t)$ either going to 0 or growing to $\\infty$, harming learning. Thus, the model (5) encourages\nparameters to move towards plastic region (initialization). In Appendix B, we discuss this further and\nother potential choices for the drift model."}, {"title": "Online estimation of drift model", "content": "The drift model $p(\\theta_{t+1}|\\theta_t, \\gamma_t)$ quantifies prior belief about the change in parameters before seeing\nnew data. A suitable choice of an objective to select $\\gamma_t$ is predictive likelihood which quantifies the\nprobability of new data under our current parameters and drift model. From Bayesian perspective, it\nmeans selecting the prior distribution which explains the future data the best.\nWe derive the drift estimation procedure in the context of approximate online variational infer\nence [7] with Bayesian Neural Networks (BNN). Let $\\mathcal{F}_t = (\\gamma_1,..., \\gamma_t)$ be the history of ob\nserved parameters of the drift model and $\\mathcal{S}_t = \\{(X_1,Y_1),...,(x_t, Y_t)\\}$ be the history of ob\nserved data. The objective of approximate online variational inference is to propagate an ap\nproximate posterior $q_t(\\theta|\\mathcal{S}_t, \\mathcal{F}_{t-1})$ over parameters, such that it is constrained to some family\n$\\mathcal{Q}$ of probability distributions. In the context of BNNs, it is typical [5] to assume a family\n$\\mathcal{Q} = \\{q(\\theta) : q(\\theta) \\sim \\prod_{i=1}^D N(\\theta^i; \\mu^i, [\\sigma^i]^2); \\theta = (\\theta^1, ..., \\theta^D)\\}$ of Gaussian mean-field distributions\nover parameters $\\theta\\in R^D$ (separate Gaussian per parameter). For simplicity of notation, we omit the\nindex i. Let $q_t(\\theta) \\equiv q_t(\\theta|\\mathcal{S}_t, \\mathcal{F}_{t-1}) \\in \\mathcal{Q}$ be the Gaussian approximate posterior at time t with mean\n$\\mu_t$ and variance $\\sigma_t^2$ for every parameter. The new approximate posterior $q_{t+1}(\\theta) \\in \\mathcal{Q}$ is found by\n$q_{t+1}(\\theta) = \\text{arg min}_q K L [q(\\theta)||p(Y_{t+1}|X_{t+1}, \\theta)q_t(\\theta|\\gamma_t)],$\nwhere the prior term is the approximate predictive look-ahead prior given by\n$q_t(\\theta|\\gamma_t) = \\int q_t(\\theta_t)p(\\theta|\\theta_t, \\gamma_t)d\\theta_t = N(\\theta; \\mu_t(\\gamma_t), \\tilde{\\sigma}_t(\\gamma_t))$\nthat has parameters $\\tilde{\\mu}_t(\\gamma_t) = \\gamma_t\\mu_t + (1 - \\gamma_t)\\mu_0$, $\\tilde{\\sigma}_t(\\gamma_t) = \\gamma_t^2\\sigma_t^2 + (1 - \\gamma_t^2)\\sigma_0^2$, see Appendix I.1 for\nderivation. The form of this prior $q_t (\\theta|\\gamma_t)$ comes from the non i.i.d. assumption (see Figure 1b) and\nthe form of the drift model (5). For new batch of data $(x_{t+1}, Y_{t+1})$ at time $t + 1$, the approximate\npredictive log-likelihood equals to\n$\\text{log } q_t (Y_{t+1}|X_{t+1}, \\gamma_t) = \\text{log } \\int p(Y_{t+1}|X_{t+1}, \\theta)q_t(\\theta|\\gamma_t)d\\theta.$\nThe log-likelihood (8) allows us to quantify predictions on new data $(x_{t+1}, Y_{t+1})$ given our current\ndistribution $q_t (\\theta)$ and the drift model from (5). We want to find such $\\gamma_t$ that\n$\\gamma_t^* \\approx \\text{arg max}_{\\gamma_t} \\text{log } q_t (Y_{t+1}|X_{t+1}, \\gamma_t)$"}, {"title": "Approximate Bayesian update of posterior $q_t(\\theta)$ with BNNS", "content": "The optimization problem (6) for the per-parameter Gaussian $q(\\theta) = N(\\theta; \\mu, \\sigma^2)$ with Gaussian prior\n$q_t(\\theta) = N(\\theta; \\mu_t, \\sigma_t^2)$, both defined for every parameter of NN, can be written (see Appendix I.1) to\nminimize the following loss\n$F_t(\\mu, \\sigma, \\gamma_t) = E_{\\epsilon \\sim N(0;I)}[\\mathcal{L}_{t+1}(\\mu + \\epsilon\\sigma)] + \\sum_{i=1}^D \\lambda_i \\Big[ \\frac{(\\mu_i - \\tilde{\\mu}_t(\\gamma_t))^2 + [\\sigma^i]^2}{2[\\tilde{\\sigma}_t(\\gamma_t)]^2} - \\frac{1}{2} \\text{log } \\frac{[\\sigma^i]^2}{[\\tilde{\\sigma}_t(\\gamma_t)]^2} \\Big],$\nwhere $\\lambda > 0$ are per-parameter temperature coefficients. The use of small temperature $\\lambda > 0$\nparameter (shared for all NN parameters) was shown to improve empirical performance of Bayesian\nNeural Networks [54]. Given that in (13), the variance $\\tilde{\\sigma}(\\gamma_t)$ can be small, in order to control the\nstrength of the regularization, we propose to use per parameter temperature $\\lambda = \\lambda \\times [\\sigma]^2$, where\n$\\lambda > 0$ is a global constant. This leads to the following objective\n$F_t(\\mu, \\sigma, \\gamma_t) = E_{\\epsilon \\sim N(0;I)}[\\mathcal{L}_{t+1}(\\mu + \\epsilon\\sigma)] + \\sum_{i=1}^D r_i \\Big[ \\frac{(\\mu_i - \\tilde{\\mu}_t(\\gamma_t))^2 + [\\sigma^i]^2}{2[\\tilde{\\sigma}_t(\\gamma_t)]^2} - \\frac{1}{2} \\text{log } [\\sigma^i]^2 \\Big],$\nwhere the quantity $r_i = [\\sigma^i]^2/[\\tilde{\\sigma}_t(\\gamma_t)]^2$ is a relative change in the posterior variance due to the drift.\nThe ratio $r^2 = 1$ when $\\gamma_t = 1$. For $\\gamma_t < 1$ since typically $\\sigma_i < \\sigma_0$, the ratio is $r < 1$. Thus, as long\nas there is non-stationarity ($\\gamma_t < 1$), the objective (14) favors the data term $E_{\\epsilon \\sim N(0;I)}[\\mathcal{L}_{t+1}(\\mu + \\epsilon\\sigma)]$"}, {"title": "Fast MAP update of posterior $q_t (\\theta)$", "content": "As a faster alternative to propagating the posterior (6), we do MAP updates with the prior $p_0(\\theta) =$\n$N(\\theta; \\mu_0; \\sigma_0)$ and the approximate posterior $q_t(\\theta) = N(\\theta; \\theta_t; \\sigma_t^2 = s^2\\sigma_0^2)$, where $s < 1$ is a\nhyperparameter controlling the variance of $\\sigma_t^2$ of $q_t(\\theta)$. Since a fixed $s$ may not capture the true\nparameters variance, using a Bayesian method (see Section 3.4) is preferred but comes at a high\ncomputational cost (see Appendix E for discussion). The MAP update is given by (see Appendix I.2\nfor derivations) finding a minimum of the following proximal objective\n$\\mathcal{G}(\\theta) = \\mathcal{L}_{t+1}(\\theta) + \\sum_{i=1}^D \\frac{|\\theta - \\theta_t(\\gamma)|^2}{2 \\tilde{\\alpha}(t)},$\nwhere the regularization target for the parameter dimension i is given by\n$\\theta_t(\\gamma) = \\gamma_t \\theta^i + (1 - \\gamma^i)\\mu_0^i$\nand the per-parameter learning rate is given as (assuming that $\\alpha_t$ the base SGD learning rate)\n$\\tilde{\\alpha}_t(\\gamma) = \\alpha_t \\Big( \\frac{(\\sigma_t)^2}{\\tilde{\\sigma}_t^2(\\gamma)} + \\frac{1 - s^2}{s^2} \\Big)^2$\nLinearising $\\mathcal{L}_{t+1}(\\theta)$ around $\\theta_t(\\gamma_t)$ and optimizing (16) for $\\theta$ leads to (see Appendix I.2)\n$\\theta_{t+1} = \\theta_t(\\gamma_t) - \\tilde{\\alpha}_t(\\gamma_t) \\circ \\nabla_\\theta\\mathcal{L}_{t+1}(\\tilde{\\theta}_t(\\gamma_t)),$\nwhere $\\circ$ is element-wise multiplication. For $\\gamma_t = 1$, we recover the ordinary SGD update, while\nthe values $\\gamma_t < 1$ move the starting point of the modified SGD closer to the initialization as\nwell as increase the learning rate. Algorithm 1 describes the full procedure. In Appendix C we\ndescribe additional practical choices made for the Soft Resets algorithm. Similarly to the Bayesian\napproach (15), we can do multiple updates on (16). We describe this Soft Resets Proximal algorithm\nin Appendix I.2 and full procedure is given in Algorithm 3."}, {"title": "Related Work", "content": "Plasticity loss in Neural Networks. Our model shares similarities with reset-based approaches such\nas Shrink & Perturb (S&P) [2] and L2-Init [33]; however, whereas we learn drift parameters from\ndata, these methods do not, leaving them vulnerable to mismatch between assumed non-stationarity\nand the actual realized non-stationarity in the data. Continual Backprop [13] or ReDO [47] apply\nresets in a data-dependent fashion, e.g. either based on utility or whether units are dead. But they\nuse hard resets, and cannot amortize the cost of removing entire features. Interpretation (12) of $\\gamma_t$\nconnects to the notion of parameters utility from [14], but this quantity is used to prevent catastrophic\nforgetting by decreasing learning rate for high $\\gamma_t$. Our method increases the learning rate for low $\\gamma_t$\nto maximize adaptability, and is not designed to prevent catastrophic forgetting.\nNon-stationarity. Non-stationarity arises naturally in a variety of contexts, the most obvious being\ncontinual and reinforcement learning. The structure of non-stationarity may vary from problem to\nproblem. At one extreme, we have a piece-wise stationary setting, for example a change in the\nlocation of a camera generating a stream of images, or a hard update to the learner's target network in\nvalue-based deep RL algorithms. This setting has been studied extensively due to its propensity to\ninduce catastrophic forgetting [e.g. 31, 45, 51, 10] and plasticity loss [13, 39, 38, 34]. At the other\nextreme, we can consider more gradual changes, for example due to improvements in the policy of\nan RL agent [40, 46, 42, 13] or shifts in the data generating process [36, 55, 20, 53]. Further, these\nscenarios might be combined, for example in continual reinforcement learning [31, 1, 13] where the\nreward function or transition dynamics could change over time.\nNon-stationary online convex optimization. Non-stationary prediction has a long history in online\nconvex optimization, where several algorithms have been developed to adapt to changing data [see,\ne.g., 25, 8, 22, 17, 21, 18, 29]. Our approach takes an inspiration from these works by employing a\ndrift model as, e.g., [25, 21] and by changing learning rate as [29, 52]. Further, our OU drift model\nbears many similarities to the implicit drift model introduced in the update rule of [25] (see also\n[8, 17]), where the predictive distribution is mixed with a uniform distribution to ensure the prediction\ncould change quickly enough if the data changes significantly, where in our case $p_0$ plays the same\nrole as the uniform distribution.\nBayesian approaches to non-stationary learning. A standard approach is Variational Continual\nLearning [41], which focuses on preventing catastrophic forgetting and is an online version of \"Bayes\nBy Backprop\" [5]. This method does not incorporate dynamical parameter drift components. In [35],\nthe authors applied variational inference (VI) on non-stationary data, using the OU-process and\nBayesian forgetting, but unlike in our approach, their drift parameter is not learned. Further, in [49],\nthe authors considered an OU parameter drift model similar to ours, with an adaptable drift scalar\n$\\gamma$ and analytic Kalman filter updates, but is applied over the final layer weights only, while the\nremaining weights of the network were estimated by online SGD. In [28], the authors propose to deal\nwith non-stationarity by assuming that each parameter is a finite sum of random variables following\ndifferent OU process. They derive VI updates on the posterior of these variables. Compared to this\nwork, we learn drift parameters for every NN parameter rather than assuming a finite set of drift\nparameters. A different line of research assumes that the drift model is known and use different\ntechniques to estimate the hidden state (the parameters) from the data: in [9], the authors use Extended\nKalman Filter to estimate state and in [3], they propagate the MAP estimate of the hidden state\ndistribution with K gradient updates on a proximal objective similar to (43), whereas in Bayesian\nOnline Natural Gradient (BONG) [27], the authors use natural gradient for the variational parameters."}, {"title": "Experiments", "content": "Soft reset methods. There are multiple variations of our method. We call the method implemented\nby Algorithm 1 with 1 gradient update on the drift parameter Soft Reset, while other versions show\ndifferent parameter choices: Soft Reset ($K_{\\gamma} = 10$) is a version with 10 updates on the drift parameter,\nwhile Soft Reset ($K_{\\gamma} = 10, K_\\theta = 10$) is the method of Algorithm 3 in Appendix I.2 with 10 updates\non drift parameter, followed by 10 updates on NN parameters. Bayesian Soft Reset ($K_{\\gamma} = 10,$\n$K_\\theta = 10$) is a method implemented by Algorithm 2 with 10 updates on drift parameter followed\nby 10 updates on the mean $\\mu_t$ and the variance $\\sigma_t^2$ (uncertainty) for each NN parameter. Bayesian\nmethod performed the best overall but required higher computational complexity (see Appendix E).\nUnless specified, $\\gamma_t$ is shared for all the parameters in each layer (separately for weight and biases).\nLoss of plasticity. We analyze the performance of our method on plasticity benchmarks [34, 39, 38].\nHere, we have a sequence of tasks, where each task consists of a fixed (for all tasks) subset of 10000\nimages images from either CIFAR-10 [32] or MNIST, where either pixels are permuted or the label\nfor each image is randomly chosen. Several papers [34, 39, 38] study a memorization random-label\nsetting where SGD can perfectly learn each task from scratch. To highlight the data-efficiency of our\napproach, we study the data-efficient setting where SGD achieves only 50% accuracy on each task\nwhen trained from scratch. Here, we expect that algorithms taking into account similarity in the data,\nto perform better. To study the impact of the non-stationarity of the input data, we consider permuted\nMNIST where pixels are randomly permuted within each task (the same task as considered by 34).\nAs baselines, we use Online SGD and Hard Reset at task boundaries. We also consider L2 init [34],\nwhich adds L2 penalty $||\\theta - \\theta_0||^2$ to the fixed initialization $\\theta_0$ as well as Shrink&Perturb [2], which\nmultiplies each parameter by a scalar $\\lambda \\leq 1$ and adds random Gaussian noise with fixed variance $\\sigma$.\nSee Appendix D.1 for all details. As metrics, we use average per-task online accuracy (25), which is\n$A_t = \\frac{1}{N} \\sum_{i=1}^N a_i^t$\nwhere $a_i^t$ are the online accuracies collected on the task t via N timesteps, corresponding to the\nduration of the task. In Figure 5, we also use average accuracy over all T tasks, i.e.\n$A_T = \\frac{1}{T} \\sum_{t=1}^T A_t$\nThe results are provided in Figure 2. We observe that Soft Reset is always better than Hard Reset\nand most baselines despite the lack of knowledge of task boundaries. The gap is larger in the data\nefficient regime. Moreover, we see that L2 Init only performs well in the memorization regime, and\nachieves comparable performance to Hard Reset in the data efficient one. The method L2 Init could\nbe viewed as an instantiation of our Soft Reset Proximal method optimizing (16) with $\\gamma_t = 0$ at every\nstep, which is sub-optimal when there is similarity in the data. Bayesian Soft Reset demonstrates\nsignificantly better performance overall, see also discussion below.\nIn Figure 3, we compare different variants of Soft Reset. We observe that adding more compute for\nestimating $\\gamma_t$ (thus, estimating non-stationarity, $K_{\\gamma} = 10$) as well as doing more updates on NN"}, {"title": "Reinforcement learning", "content": "Reinforcement learning experiments. We conduct Reinforcement Learning (RL) experiments in\nthe highly off-policy regime, similarly to [43], since in this setting loss of plasticity was observed. We ran\nSAC [19] agent with default parameters from Brax [15] on the Hopper-v5 and Humanoid-v4 GYM [6]\nenvironments (from Brax [15]). To reproduce the setting from [43], we control the off-policyness of\nthe agent by setting the off-policy ratio M such that for every 128 environment steps, we do 128M\ngradient steps with batch size of 256 on the replay buffer. As baselines we consider ordinary SAC,\nhard-coded Hard Reset where we reset all the parameters K = 5 times throughout training (every\n200000 steps), while keeping the replay buffer fixed (similarly to [43]). We employ our Soft Reset\nmethod as follows. After we have collected fresh data from the environment, we do one gradient\nupdate on $\\gamma_t$ (shared for all the parameters within each layer) with batch size of 128 on this new\nchunk of data and the previously collected one, i.e., two chunks of data in total. Then we initialize\n$\\theta_t(\\gamma_t)$ and we employ the update rule (43) where the regularization $\\theta_t(\\gamma_t)$ is kept constant for all the\noff-policy gradient updates on the replay buffer. See Appendix D.3 for more details."}, {"title": "Conclusion", "content": "Learning efficiently on non-stationary distributions is critical to a number of applications of deep\nneural networks, most prominently in reinforcement learning. In this paper, we have proposed a new\nmethod, Soft Resets, which improves the robustness of stochastic gradient descent to nonstationarities\nin the data-generating distribution by modeling the drift in Neural Network (NN) parameters. The\nproposed drift model implements soft reset mechanism where the amount of reset is controlled by the\ndrift parameter $\\gamma_t$. We showed that we could learn this drift parameter from the data and therefore\nwe could learn when and how far to reset each Neural Network parameter. We incorporate the drift\nmodel in the learning algorithm which improves learning in scenarios with plasticity loss. The variant\nof our method which models uncertainty in the parameters achieves the best performance on plasticity\nbenchmarks so far, highlighting the promise of the Bayesian approach. Furthermore, we found that\nour approach is particularly effective either on data distributions with a lot of similarity or on slowly\nchanging distributions. Our findings open the door to a variety of exciting directions for future work,\nsuch as investigating the connection to continual learning and deepening our theoretical analysis of\nthe proposed approach."}, {"title": "Computational complexity", "content": "We provide the study of computational cost for all the proposed methods. Notations:\n$\\bullet$ P be the number of parameters in the Neural Network\n$\\bullet$ L is the number of layers\n$\\bullet$ O(S) is the cost of SGD backward pass.\n$\\bullet$ M$\\gamma$ - number of Monte Carlo samples for the drift model\n$\\bullet$ M$\\theta$ - number of Monte Carlo samples for the parameter updates (Bayesian Method).\n$\\bullet$ K$\\gamma$ - number of updates for the drift parameter\n$\\bullet$ K$\\theta$ - number of NN parameter updates."}, {"title": "Sensitivity analysis", "content": "We study the sensitivity of Soft Resets where $\\gamma$ is defined per layer when trained on random-label\nMNIST (data efficient). We fix the learning rate to $\\alpha = 0.1$. We study the sensitivity of learning rate\nfor the drift parameter, $\\eta_{\\gamma}$, as well as $p$ \u2013 initial prior standard deviation rescaling, and $s$ posterior\nstandard deviation rescaling parameter.\nOn top of that, we conduct the sensitivity analysis of L2 Init [34] and Shrink&Perturb [2] methods.\nThe x-axis of each plot denotes one of the studied hyperparameters, whereas y-axis is the average\nperformance across all the tasks (see Experiments section for tasks definition). The standard deviation\nis reported over 3 random seeds. A color indicates a second hyperparameter which is studied, if\navailable. In the title of each plot, we write hyperparameters which are fixed. The analysis is provided\nin Figure 10 for Soft Resets and in Figure 11 for the baselines.\nThe most important parameter is the learning rate of the drift model $\\eta_{\\gamma}$. For each method, there\nexists a good value of this parameter and performance is sensitive to it. This makes sense since this\nparameter directly impacts how we learn the drift model.\nThe performance of Soft Resets is robust with respect to the posterior standard deviation scaling s\nparameter as long as it is $s \\geq 0.5$. For $s < 0.5$, the performance degrades. This parameter is defined\nfrom $\\sigma_t = s\\sigma_0$ and affects relative increase in learning rate given by $\\sqrt{\\frac{\\gamma^2+(1-\\gamma^2)/s^2}{1}}$ which could be\nill-behaved for small s.\nWe also study the sensitivity of the baseline methods. We find that L2 Init [34] is very sensitive to the\nparameter $\\lambda$, which is a penalty term for $\\lambda||\\theta - \\theta_0||^2$. In fact, Figure 11, left shows that there is only\none good value of this parameter which works. Shrink&Perturb [2] is very sensitive to the shrink\nparameter $\\lambda$. Similar to L2 Init, there is only one value which works, 0.9999 while values 0.999 and\nvalues 0.99999 lead to bad performance. This method however, is not very sensitive to the perturb\nparameter $\\sigma$ provided that $\\sigma < 0.001$."}, {"title": "Proximal SGD", "content": "Each step of online SGD can be seen in terms of a regularized minimization problem referred to as\nthe proximal form [44"}]}