{"title": "Towards Geo-Culturally Grounded LLM Generations", "authors": ["Piyawat Lertvittayakumjorn", "David Kinney", "Vinodkumar Prabhakaran", "Donald Martin, Jr.", "Sunipa Dev"], "abstract": "Generative large language models (LLMs) have been demonstrated to have gaps in diverse, cultural knowledge across the globe. We investigate the effect of retrieval augmented generation and search-grounding techniques on the ability of LLMs to display familiarity with a diverse range of national cultures. Specifically, we compare the performance of standard LLMs, LLMs augmented with retrievals from a bespoke knowledge base (i.e., KB grounding), and LLMs augmented with retrievals from a web search (i.e., search grounding) on a series of cultural familiarity benchmarks. We find that search grounding significantly improves the LLM performance on multiple-choice benchmarks that test propositional knowledge (e.g., the norms, artifacts, and institutions of national cultures), while KB grounding's effectiveness is limited by inadequate knowledge base coverage and a suboptimal retriever. However, search grounding also increases the risk of stereotypical judgments by language models, while failing to improve evaluators' judgments of cultural familiarity in a human evaluation with adequate statistical power. These results highlight the distinction between propositional knowledge about a culture and open-ended cultural fluency when it comes to evaluating the cultural familiarity of generative LLMs.", "sections": [{"title": "1 Introduction", "content": "Contemporary large language models (LLMs) are pre-trained on huge corpora of natural language text (Radford et al., 2019) and then fine-tuned using human feedback to improve their quality (Bai et al., 2022). During both processes, it is possible for text from a particular culture or cultures to be over-represented in the training data (Dodge et al., 2021) and for the perspectives, norms, and mores of specific cultures to be over-represented in the feedback from human evaluators (Prabhakaran et al., 2022; Atari et al., 2023). Consequently, there is growing recognition of the shortcomings of generative LLMs with respect to their ability to represent and serve people of diverse geo-cultural backgrounds at the global scale (Adilazuarda et al., 2024; Pawar et al., 2024; Agarwal et al., 2025). Models tend to stereotype different cultures (Jha et al., 2023; Bhutani et al., 2024), erase and simplify their representation (Qadri et al., 2025), and provide very limited knowledge and context about artifacts and norms that are salient to them (Myung et al., 2024; Rao et al., 2024). Despite these gaps, strategies for intervening and eliciting culturally appropriate content from the models remain under-explored.\nWe study two strategies to improve cultural awareness of LLM generation using external knowledge. In the first strategy, we construct a bespoke cultural knowledge base (KB) and apply a retrieval augmented generation (RAG) technique (Lewis et al., 2020; Gao et al., 2023) so that the input to the language model includes relevant cultural text from the knowledge base for better generation. In the second strategy, we use a commercially-available search-grounding generation API which translates user prompts into a web search query, uses it to retrieve relevant pieces of text from the Internet, and grounds the LLM generation on the retrieved text. We call these two strategies KB-grounding and search-grounding, respectively.\nIn our experiments, we highlight the necessity of a multi-pronged approach to evaluating cultural awareness in language model generations. Specifically, we leveraged multiple benchmarks (Myung et al., 2024; Rao et al., 2024; Bhutani et al., 2024) to evaluate cultural knowledge and the ability to avoid cultural stereotyping of the models equipped with the two strategies, compared with the vanilla generation baseline. We also conducted a human evaluation of open-ended model responses to various prompts designed to test cultural awareness, wherein evaluators from a specific national culture rated how well the model's output reflected a culturally informed perspective. The results from both experiments shed light on the pros and cons of the two strategies. Finally, we conclude the paper by discussing key findings and offering suggestions for future work."}, {"title": "2 Improving Cultural Awareness with External Knowledge", "content": "Retrieval augmented generation (RAG) is a technique for enhancing the quality of large language model generation. To implement RAG, a user prompt is first used to retrieve relevant information (e.g., text or doc-"}, {"title": "2.1 The Knowledge Base Grounding Strategy", "content": "We compiled culturally salient data from four large sources-CultureAtlas (Fung et al., 2024), Cube (Kannen et al., 2024), CultureBank (Shi et al., 2024), and SeeGULL (Jha et al., 2023) to serve as our knowledge base. For each entry in each source, we converted it into into text (if it was not already), embedded it into a vector, and added it to a vector store for querying.\nFig. 1 (top) shows how the KB-grounding strategy handles an incoming prompt. First, a query rewriter extracts the important parts of the prompt to form a query. We can configure this step to test different KB queries (e.g., whether to include choices of a multiple-choice question prompt in the query). Next, we use the query to retrieve n documents from the KB. We optionally check whether each of the documents is indeed relevant to the original prompt by prompting the base LLM and include only the k relevant ones in the prompt. We call the process with the relevancy check step selective RAG, while non-selective RAG includes all n documents in the prompt. Then, we feed the augmented prompt to the LLM to get a raw answer. For multiple-choice question answering (QA) tasks, we create a manual verbalizer to map the raw answer to one of the choices so that we can compute the model accuracy. In contrast, we use the raw answer as the final output for an open-ended generation task. See the Appendix for details about the KB, the queries, the prompt templates, and the implementation.\nIn Section 3, we apply KB-grounding to three different LLMs: gemini-flash-1.5 (Gemini) (Team et al., 2024), gpt-4o-mini (GPT) (OpenAI et al., 2024) and olmo2-7b (OLMO) (OLMo et al., 2025)."}, {"title": "2.2 The Search Grounding Strategy", "content": "Fig 1 (bottom) outlines how the search-grounding strategy works. We feed the original prompt to the search-grounding generation API which converts the prompt into a search query, inputs it to a search engine to obtain relevant text from the web pages returned by the search, integrates relevant text into the prompt, and then generates a response using an LLM based on the augmented prompt. Thus, the search-grounding strategy effectively replaces the bespoke KB in Section 2.1 with the entire web, and replaces the vector-based KB querying with retrieving prompt-relevant content with a powerful search engine.\nWe implemented search grounding on Google Vertex AI. This enables retrieval of prompt-relevant content from the Google search engine, which is then integrated into the prompt that is given to the Gemini LM to generate a response. Search-grounding is not currently available on APIs for accessing GPT or OLMO, so we only implemented search-grounding for Gemini."}, {"title": "3 Cultural Competence Benchmarks", "content": "We evaluated the KB-grounding and search-grounding strategies using multiple-choice cultural QA benchmarks."}, {"title": "3.1 Cultural Knowledge", "content": "Setup. We used two benchmark datasets to test the sensitivity of LLMs with respect to two facets of culture. The first, BLEnD, contains questions about ev-"}, {"title": "3.2 Stereotype Avoidance", "content": "Setup. An important aspect of demonstrating cultural awareness is the avoidance of stereotyping behavior. Hence, we adopted the SeeGULL stereotype avoidance test proposed by Bhutani et al. (2024) to evaluate the studied strategies. Each question asks the model to in-"}, {"title": "4 Human Evaluation", "content": "Setup. To evaluate the strategies on a more open-ended text generation task, we translated five questions from BLEnD and five questions from NORMAD into open-ended prompts asking the model to tell a story set in a particular country. We adapted each of these ten questions for the ten national cultures used in the BLEnD benchmark evaluation, leaving us with 100 (country, prompt) pairs. Next, we generated responses from Gemini using the vanilla, the selective KB-grounding, and the search-grounding. For each strategy, we generated three unique responses with the temperature of 0.5. Then we recruited nine evaluators from each of the ten studied cultures to rate, on a scale from 0 to 4, how culturally informed each response was and to provide a brief justification of their score (see Appendices A.6-A.7 for details).\nResults. A repeated-measures ANOVA found no significant effect of strategy on evaluators' judgments of cultural awareness in model responses (F = .18, p = .827). Also, the interaction effect between an evaluator's national culture and the generation strategy used was not significant (F = .84, p = .651). This suggests no systematic relationship exists between generation strategies and national cultures in improving the cultural awareness of open-ended model outputs as perceived by human evaluators. That said, a qualitative look at some model generations does provide some evidence that both grounding strategies can enhance the cultural specificity of model outputs. In response to the prompt 'Tell me a story in Mexico in which a"}, {"title": "5 Discussion and Conclusion", "content": "This paper studies the effectiveness of the KB grounding and the search grounding strategies for improving cultural awareness of LLM generation. We discuss the key findings and suggestions below.\nKB Grounding vs Search Grounding. The advantages of search-grounding on BLEnD and NORMAD speak to the vast space of cultural facts on the internet. Even the large KB we compiled here still lacked many culturally relevant facts. We also observed some bias in each knowledge source used: approximately 19% of CultureAtlas entries and 25% of CultureBank entries concern the culture of the United States. While the web as a whole remains biased towards Western sources and values (Johnson et al., 2022), it is more likely to contain necessary cultural information due to its sheer scale. However, the poor performance of search-grounding on the stereotype avoidance benchmark reminds us that the context retrieved via web search could reinforce the (typically false) notion that stereotypes are factual and encourage models to affirm those stereotypes. This suggests that search-grounding is yet not a panacea for improving the cultural sensitivity of LLMs.\nKnowledge vs Fluency. The results of the human evaluation do not show that search-grounding or KB-grounding improved the cultural awareness of LLM outputs. The divergence in the performance of both strategies on the human evaluation and the multiple-choice QA benchmarks suggests that we ought to draw a distinction between two varieties of cultural awareness. On the one hand, there is a sense of cultural awareness that involves possessing propositional knowledge about a culture (i.e., knowing facts about that culture), as measured by the multiple-choice QA benchmarks, where the grounding strategies can improve the model performance. However, another sense of cultural awareness involves writing and speaking like someone with first-hand experience of and immersion in a culture, i.e., a sense of cultural fluency, as measured by our human evaluation, which revealed that the two grounding strategies was of limited value. We leave it to future work to develop strategies for improving the cultural awareness of generative language models along this second axis."}, {"title": "Limitations", "content": "We acknowledge the following limitations of our work in this paper. First, we ran our evaluations only on smaller versions of the GPT-4, Gemini 1.5, and OLMO 2 models. It remains an open question whether our pattern of results would be similar for larger versions of these models and other model families. Second, for BLEnD and the human evaluation, we ran our evaluations using prompts that were relevant to ten national countries and cultures; a more comprehensive study would require the use of a wider range of national and regional cultures. With the momentum in the community to create more culturally salient resources, a more comprehensive study in the future will help identify gaps and interventions for the majority world. Finally, all of our evaluations concerned solely English-language prompts and outputs. While we take it to be an important goal for generative language models that they be able to produce culturally-informed outputs about any culture in the world's most widely-spoken language, the landscape of cultural awareness becomes much more nuanced when one considers the rich variegation that exists in phrasing and dialect across a wide range of languages. We leave it to future work to examine whether the strategies used here can be adapted to a multi-lingual context."}, {"title": "Ethical Considerations", "content": "As generative large language models are developed and deployed rapidly across the globe, it is important to reflect on how we can improve user experience at a similar pace. The promise of model utility for a myriad of tasks such as that of a writing assistant, remains unfulfilled if the model is not beneficial or usable for a vast majority. With this work, we attempted to begin adaptation of techniques in NLP to further the cause of cultural awareness and relevance in models. As noted in our limitations, with more comprehensive work across a greater number of cultures and countries, we hope that development of more culturally aware models will be possible."}, {"title": "A.5 Statistical Results of Cultural Competence Benchmarks", "content": "N.B.: All t-tests reported in this section are paired t-tests.\nBLEnD. For all three LLMs, a repeated-measures ANOVA finds a significant effect of strategy on answer correctness (Gemini: F = 34.83, p = 1.00 \u00d7 10^{-35}; GPT: F = 8.26, p = 1.19 \u00d7 10^{-6}, OLMo: F = 727.60, p \u2248 0). Search-grounded Gemini significantly outperforms vanilla Gemini (t = 6.49, p = 8.58 \u00d7 10^{-11}). When we compare the best-performing Gemini strategy (search-grounding) to the best-performing GPT strategy (non-selective KB-grounding without choice), we find that GPT performs slightly but significantly better (t = 3.11, p = .002). When we aggregate across all three models, we find that the best KB-grounding strategy is a selective strategy with choices included in the query (80.5% correct). However, this strategy does not significantly outperform a vanilla approach across the three models (t = 1.68, p = 0.09). Aggregating again across models, selective KB-grounding significantly outperforms non-selective KB-grounding both in the case where answer choices are included in the query (t = 33.27, p < 2.2 \u00d7 10^{-16}) and when answer choices are not included in the query (t = 24.99, p < 2.2 \u00d7 10^{-16}). Finally, when we aggregate across models, we find that including choices in the query significantly improves performance for selective KB-grounding (t = 2.67, p = .008), while the opposite is true for non-selective KB grounding (t = \u22127.22, p = 5.34 \u00d7 10^{-13}).\nNORMAD - Country. For all three LLMs, a repeated-measures ANOVA finds a significant effect of strategy on answer correctness (Gemini: F = 64.04, p = 6.65 \u00d7 10^{-41}, GPT: F = 9.022, P = 1.23 \u00d7 10^{-5} OLMo: F = 4.02, p = .018). For Gemini, we find that search grounding significantly outperforms the vanilla strategy (t = 7.46, p = 1.13 \u00d7 10^{-13}). Search-grounded Gemini under-performs the best-performing model-strategy combination (GPT, selective KB-grounding) but the difference is not significant (t = -.35, p = .73). Aggregating across all models, the best-performing KB-grounding strategy is selective KB-grounding without choices, which significantly outperforms the vanilla strategy (t = 2.81, p = .005), and non-selective KB-grounding (t = 5.78, p = 7.93 \u00d7 10^{-9}).\nNORMAD - Country + Value. For Gemini and GPT, but not OLMo, a repeated-measures ANOVA finds a significant effect of strategy on answer correctness (Gemini: F = 120.39, p = 3.05 \u00d7 10^{-70}, GPT: F = 14.57, p = 4.88 \u00d7 10^{-7}, OLMo: F = 0.51, p = 0.60). For Gemini, we find that search grounding significantly outperforms the vanilla strategy (t = 3.69, p = 2.27 \u00d7 10^{-4}). Search-grounded Gemini significantly outperforms the second-best-performing model-strategy combination, which is the vanilla strat-"}, {"title": "A.6 Methods and Power Analysis for Human Evaluation", "content": "Methods. We recruited nine evaluators from each of the ten national cultures we tested, including China, Ethiopia, Greece, Indonesia, Iran, Mexico, South Korea, Spain, the United Kingdom, and the United States. Each evaluator was shown a prompt relevant to their national culture, along with responses to that prompt from each of the three strategies evaluated: vanilla, selective KB-grounding, and search-grounding. For each strategy, a response was randomly selected from the three responses generated. Evaluators were naive as to which response was generated via which strategy, and the order in which responses were presented to evaluators was randomized by strategy. Evaluators were then asked to rate with justification, on a scale from 0 to 4, how culturally informed each response was. Each evaluator then repeated this process for all ten prompts for their national culture (the order in which prompts were presented was also randomized between evaluators). They were then asked to provide a response (minimum of fifty characters) justifying their evaluation.\nPower Analysis. Our human evaluation was powered to detect a significant difference in judgments as to how culturally informed an LLM is for the different strategies, if such a difference were to exist. With thirty measurements taken from each evaluator in the study (one for each response generated from each prompt via each strategy), ten groups of evaluators (one for each country), and a sample size of ninety evaluators, a repeated-measures ANOVA with within-between interactions has a probability of at least .998 of detecting an effect size of f = .2 or greater for any independent variable or interaction between them, assuming a correlation between measures of at least 2 and a non-sphericity correction of \u20ac = 1."}, {"title": "A.7 Prompts and Queries for Human Evaluation", "content": "Table 6 summarizes how questions from the BLEnD and NORMAD multiple-choice QA datasets were translated into open-ended questions for our human evaluation experiment. For the vanilla and the search-grounding approaches, the open-ended question was used together with the 'Output:' prompt to be the input we sent to the LLM. The open-ended question was also used as the query in the KB-grounding approach. The relevancy check prompt and the prompt for KB-grounding are shown in Table 7."}, {"title": "A.8 Sample Question and Responses from the Human Evaluation Experiment", "content": "Sample generated texts in Table 8 are the responses from the Gemini model for the question: \"Tell me a story in Mexico in which a group of people of varying ages eat together and all guests behave in a socially acceptable way.\"\nWe highlight concept names and phrases that are related to the Mexican culture in blue."}]}