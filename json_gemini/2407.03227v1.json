{"title": "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning", "authors": ["Zhili Shen", "Pavlos Vougiouklist", "Chenxin Diao", "Kaustubh Vyas", "Yuanyi Ji", "Jeff Z. Pan"], "abstract": "We focus on Text-to-SQL semantic parsing from the perspective of Large Language Models. Motivated by challenges related to the size of commercial database schemata and the deployability of business intelligence solutions, we propose an approach that dynamically retrieves input database information and uses abstract syntax trees to select few-shot examples for in-context learning.\nFurthermore, we investigate the extent to which an in-parallel semantic parser can be leveraged for generating approximated versions of the expected SQL queries, to support our retrieval. We take this approach to the extreme-we adapt a model consisting of less than 500M parameters, to act as an extremely efficient approximator, enhancing it with the ability to process schemata in a parallelised manner. We apply our approach to monolingual and cross-lingual benchmarks for semantic parsing, showing improvements over state-of-the-art baselines. Comprehensive experiments highlight the contribution of modules involved in this retrieval-augmented generation setting, revealing interesting directions for future work.", "sections": [{"title": "1 Introduction", "content": "Text-to-SQL semantic parsing aims at translating natural language questions into SQL, to facilitate querying relational databases by non-experts (Zelle and Mooney, 1996). Given their accessibility benefits, Text-to-SQL applications have become popular recently, with many corporations developing Business Intelligence platforms.\nThe success of Large Language Models (LLMs) in generalising across diverse Natural Language Processing tasks (Ye et al., 2023; OpenAI et al., 2024) has fuelled works that looked at how these multi-billion parameter models can be best employed for Text-to-SQL (Liu et al., 2023; Pourreza"}, {"title": "2 Preliminaries", "content": "Let q be the sequence of tokens of a natural language question for database D with tables $t = t_1, t_2,..., t_r$ and columns $c = c_1, c_2, ..., c_{i_j}, ..., c_{iC_i}$, where $c_{i_j}$ is the j-th column of table $t_i$ and $C_i \\in \\mathbb{N}$ is the total number of columns in table $t_i$. Furthermore, let $V_D = \\{ V_{c_{i_1}}, ..., V_{c_{iC_i}}, ..., V_{c_{r_1}}, ..., V_{c_{rC_r}} \\}$ be the set of all values associated with the database D s.t. $V_{c_{i_1}}, ..., V_{c_{iC_i}}, ..., V_{c_{r_1}}, ..., V_{c_{rC_r}}$ are the DB value sets associated with respective columns $c_{i_1}, ..., c_{iC_i}, ..., c_{r_1}, ..., c_{rC_r} \\in c$. The goal of Text-to-SQL semantic parsing is to predict the SQL query s given the (q, D) combination, as follows:\n$S = arg \\underset{s}{max} p (s | q, D) \\qquad (1)$\nFor in-context learning, we seek to select pertinent input context including few-shot examples, schema, and database values to simplify the task for LLMs."}, {"title": "3 Example Selection using Abstract Syntax Trees", "content": "Our goal is to identify the most suitable set of $X^* = \\{(q_1, s_1), ..., (q_j, s_j) \\}$ question-SQL pairs from an index of examples, X, s.t. $X^* \\subseteq X$, for maximising the probability of an LLM to predict the correct SQL given (q, D):\n$X^* = arg \\underset{X}{max} p (s | q, D, X) \\qquad (2)$\nFrom the perspective of ranking, we consider the relevance score between a candidate example $(q_j, s_j) \\in X$ and the input (q, D). Vanilla semantic search is usually based solely on question embeddings, whereas the structure of SQL queries for similar questions is subject to target databases and can thus differ significantly.\nTo incorporate database context for selecting ex-amples, we propose to re-rank examples retrieved"}, {"title": "4 Database Context Selection", "content": "Apart from relevant question-SQL pairs, prompting for Text-to-SQL parsing requires the context of database schema and values."}, {"title": "4.1 Schema Selection", "content": "We present a hybrid search strategy that selects a sub-schema given a test question to minimise"}, {"title": "4.1.1 Incorporating for Approximated Query", "content": "The semantic search for schema selection requires a comprehension of the relevance between heterogeneous database information and natural language questions, in addition to interactions across schema elements. To this end, a trained parser can inherently be a semantic search model for retrieving a sub-schema, where columns and tables are extracted from the approximated query s'. We argue that a semantic parser which is performing reasonably on the task, can provide us with an s', whose structure would assimilate the structure of the expected final query, s. Consequently, we opt to dynamically determine the number of columns to be retrieved by scorebM25 as proportional to the number of unique columns in s', returned by the approximator. A sub-schema is then obtained by merging schema elements selected by scorebM25 with elements from the approximated query."}, {"title": "4.1.2 Approximating for Longer Schemata", "content": "To further reduce the computational workload, we opt for using a smaller model for computing the approximated query, s'. However, smaller models usually have shorter context windows (i.e. < 2k tokens), and, as such, they cannot be easily scaled to the requirements of larger schemata. To this end, we propose an approach that enables transformer-based encoders to process longer schemata, in a parallelised manner.\nWe start with FastRAT (Vougiouklis et al., 2023), which exploits a decoder-free architecture for efficient text-to-SQL parsing. Given a concatenation of the input natural language question q with the column and table names of a database schema, Fas-tRAT computes the SQL operation in which each element of the input schema would participate in"}, {"title": "4.2 Value Selection", "content": "The inference of LLMs for text-to-SQL parsing can be augmented with column values (Sun et al., 2024). We select values for columns in a schema (or a sub-schema) by simply matching keywords in questions and values. This is based on the assumption that LLMs can generalise to unseen values given a set of representative values; thus, the recall and precision of value selection are less critical. We consider value selection providing additional information for LLMs to discern covert differences among columns. An example of our resulting prompt is shown in Appendix B."}, {"title": "5 Experiments", "content": "We run experiments using two approximators: FastRAText and Graphix-T5 (Li et al., 2023b). Graphix-T5 is is the approximator used by DAIL-SQL (Gao et al., 2023), and is included to facilitate a fair comparison against the closest work to ours. FastRAText is trained and tested using r = 64, unless otherwise stated (cf. Section 5.4). For all experiments, we use 5 (question, SQL) examples.\nWe test our approach against both closed- and open-source LLMs: (i) gpt-3.5-turbo (gpt-3.5-turbo-0613), (ii) gpt-4 (gpt-4-0613) and (iii) deepseek-coder-33b-instruct. Results using additional models from the DeepSeek family are provided in Appendix C.4."}, {"title": "5.1 Datasets", "content": "We experiment with several SQL datasets, seeking to explore the effectiveness of our approach on both monolingual and cross-lingual setups. Specifically, we report experiments on CSPIDER (Min et al., 2019) and SPIDER (Yu et al., 2018). Since CSPIDER is a translated version of SPIDER in Chinese, when it comes to the natural language questions, the characteristics of the two with respect to structure and number of examples are identical. We focus our evaluation on the development sets, which are used as test sets in our experiments."}, {"title": "5.2 Baselines", "content": "We dichotomize the landscape of baselines in fine-tuning- and prompting-based baselines. Further details are provided in Appendix E.\nFine-tuning-based (i) GraPPa, (ii) DG-MAML, (iii) FastRAT, (iv) Graphix-T5, (v) RESDSQL and (vi) HG2AST.\nPrompting-based Zero-shot LLM prompting has been explored by Guo et al.; Liu et al.; Pour-reza and Rafiei; (i) C3 introduces calibration bias for LLM prompting; (ii) DIN-SQL uses chain-of-thought prompting with pre-defined prompting templates tailored for the assessed question hardness; (iii) DAIL-SQL uses query approximation and SQL skeleton-based similarities for example selection; (iv) SQL-PaLM proposes a framework for soft column selection and execution-based refinement; (v) RAG w/ Rev. Chain augments the input prompt with question skeleton-based example retrieval and an execution-based revision chain."}, {"title": "5.3 Text-to-SQL Evaluation", "content": "Table 2 and 3 summarise the results of our approach with deepseek-coder-33b-instruct, gpt-3.5-turbo and gpt-4 against the baselines. Our approach, comprising a single-prompting round, surpasses other LLM-based solutions, that incorporate several prompting iterations, for LLMs of the same capacity. We note consistent improvements over DAIL-SQL, the closest work to ours, even when FastRAText is used as approximator (i.e. a model consisting of < 500M vs the \u2265 3B parameters that DAIL-SQL's approximator is using). For the same approximator, our framework is able to meet, performance standards of DAIL-SQL (equipped with gpt-4 and an additional self-consistency prompting step) using an open-source model as backbone LLM, by achieving shorter prompts in a single prompting step.\nSPIDER results are consistent with the results across the various Spider variants and CSPIDER (Table 3). Our approach levering FastRAText and AST-based re-ranking for example selection outperforms other prompting-based solution, and is inline with the scores of state-of-the-art fine-tuning-based baselines. While gpt-4 is the most capable model within our framework (with this being"}, {"title": "5.3.1 Schema Selection Evaluation", "content": "We evaluate our proposed schema selection strategy in a two-fold manner, given that value selection is applied for selected columns. Firstly, we use recall and schema shortening (rate) to compute averaged metrics across all samples showcasing the extent to which (i) the most relevant schema elements are successfully retrieved, and (ii) the size of the resulting schema, after selection, with respect to its original size. Secondly, we explore how the performance of the end-system changes across different schema pruning settings by reporting EX and EM scores. Recall is the percentage of samples for which all ground-truth schema elements are selected. Schema shortening is the number of schema elements that are excluded divided by the total number of schema elements. Results are summarised in Table 4.\nThe benefits of schema selection are apparent in the oracle setup, in which only schema elements"}, {"title": "5.3.2 Ablation Study", "content": "Table 5 shows a comprehensive ablation study for the efficacy of our database context selection, and example selection methods including DAIL (Gao et al., 2023) and AST. We consistently notice improvement when selecting examples using AST, for the same approximator. Interestingly, the performance gap is increasing the better the approximator becomes, leading to an improvement > 2.4% in the case of an oracle approximator. This finding is in agreement with our hypothesis that AST re-ranking can preserve structural information for more pre-"}, {"title": "5.4 Schema Splitting", "content": "We evaluate the effect of splitting a schema into rm splits, using FastRAText for schema selection. Figure 1 shows EX scores across different maximum number of columns per schema split (r), on the development set of SPIDER. We see that the EX scores of our approach remain consistent across different r. The performance in the case where particular schemata from the development set are split into rm = 3 or rm = 4 splits (i.e. for r = 24 or r = 16 respectively) is identical to the scores where schemata are split using the default r with which FastRAText has been trained."}, {"title": "6 Discussion", "content": "Are there any theoretical performance upper limits for example selection using AST? For each data instance in the development set of Spider, we compute the average AST similarity between the approximated query and each SQL query that is included (after example selection) in the corresponding prompt. In Table 6, we measure EX scores on the development set of SPIDER, across different AST-similarity intervals. We see an ob-"}, {"title": "7 Related Work", "content": "Significant number of recent works have looked at how LLMs can be employed in Text-to-SQL scenarios (Rajkumar et al., 2022; Chang and Fosler-Lussier, 2023; Liu et al., 2023; Pourreza and Rafiei, 2023; Gao et al., 2023; Guo et al., 2024). More recent works have looked at how incorporating examples in the prompt could benefit the performance of LLMs in the end task (Pourreza and Rafiei, 2023; Gao et al., 2023; Guo et al., 2024; Sun et al., 2024).\nIn spite of its underlying benefits, conventional solutions for example selection have focused on retrieving pairs using question similarity (Nan et al., 2023). Other approaches have sought to approximate expected SQL queries, and either directly use these approximations in the prompt, in a few-shot setting (Sun et al., 2024) or to filter candidate (question, SQL) pairs by taking into consideration the similarity of their corresponding SQL query skeletons (Li et al., 2023a) against the skeleton of the approximated SQL (Gao et al., 2023). We argue that such example selection strategies can result in information loss, and we propose an approach for re-ranking examples using similarity of normalised SQL ASTS.\nThe benefits of schema selection for Text-to-SQL have been highlighted across the relevant bibliography (Wang et al., 2020; Li et al., 2023b; Pour-reza and Rafiei, 2023). From the LLMs perspective, pruning schema elements from the prompts has been usually leading to performance degradation (Sun et al., 2024). Inspired by Gao et al., we compute a preliminary query for a given (q, D) by we adapting FastRAT (Vougiouklis et al., 2023), to the requirements of processing longer schemata, in a parallelised manner. We couple the resulting approximator with a sparse retriever, and we propose a dynamic strategy for reducing the computational cost of the task while achieving performance improvements."}, {"title": "8 Conclusion", "content": "In this paper, we augment LLMs for Text-to-SQL semantic parsing by selecting suitable examples and database information. We present a novel AST-based metric to rank examples by similarity of SQL queries. Our hybrid search strategy for schema selection reuses a preliminary query to reduce irrelevant schema elements while maintaining high recall. Extensive experiments demonstrate that our AST-based ranking outperforms previous"}, {"title": "Limitations", "content": "There are limitations with regards to both of our example selection and schema selection. Our AST-based ranking can be biased when an approximated SQL deviates significantly from structurally correct answers. To address the failure of approximators, a future direction is to sensibly diversify selected examples such that LLMs can generalise compositionally. As for schema selection, our semantic search relies on an approximator which is essentially a parser with high precision in schema linking but lack mechanisms to control recall as a standalone model. Therefore, it is worth extending cross-encoder architecture such as FastRAT to support ranking schema elements while being a SQL approximator in the meantime.\nWe demonstrate that schema splitting strategies within our framework can be applied across various numbers of splits without noticeable performance degradation. Nonetheless, given the lack of available datasets that incorporate longer commercial schemata, we focus our experiments on the cross-database setting provided by CSPIDER and SPIDER variants."}, {"title": "Ethics Statement", "content": "We do not make use of any private, proprietary, or sensitive data. FastRAText is trained on publicly available Text-to-SQL datasets, using publicly available encoder-models as base. Our framework for retrieval-augmented generation builds on-top of large, pre-trained language models, which may have been trained using proprietary data (e.g. in the case of the OpenAI models). Given the nature of pre-training schemes, it is possible that our system could carry forward biases present in the datasets and/or the involved LLMs."}]}