{"title": "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning", "authors": ["Zhili Shen", "Pavlos Vougiouklist", "Chenxin Diao", "Kaustubh Vyas", "Yuanyi Ji", "Jeff Z. Pan"], "abstract": "We focus on Text-to-SQL semantic parsing\nfrom the perspective of Large Language Mod-\nels. Motivated by challenges related to the size\nof commercial database schemata and the de-\nployability of business intelligence solutions,\nwe propose an approach that dynamically re-\ntrieves input database information and uses ab-\nstract syntax trees to select few-shot examples\nfor in-context learning.\nFurthermore, we investigate the extent to which\nan in-parallel semantic parser can be lever-\naged for generating approximated versions of\nthe expected SQL queries, to support our re-\ntrieval. We take this approach to the extreme-\nwe adapt a model consisting of less than 500M\nparameters, to act as an extremely efficient ap-\nproximator, enhancing it with the ability to pro-\ncess schemata in a parallelised manner. We\napply our approach to monolingual and cross-\nlingual benchmarks for semantic parsing, show-\ning improvements over state-of-the-art base-\nlines. Comprehensive experiments highlight\nthe contribution of modules involved in this\nretrieval-augmented generation setting, reveal-\ning interesting directions for future work.", "sections": [{"title": "1 Introduction", "content": "Text-to-SQL semantic parsing aims at translating\nnatural language questions into SQL, to facilitate\nquerying relational databases by non-experts (Zelle\nand Mooney, 1996). Given their accessibility bene-\nfits, Text-to-SQL applications have become popular\nrecently, with many corporations developing Busi-\nness Intelligence platforms.\nThe success of Large Language Models (LLMs)\nin generalising across diverse Natural Language\nProcessing tasks (Ye et al., 2023; OpenAI et al.,\n2024) has fuelled works that looked at how these\nmulti-billion parameter models can be best em-\nployed for Text-to-SQL (Liu et al., 2023; Pourreza\nand Rafiei, 2023). Recent works in this space have\nfocused on the in-context learning ability of LLMs,\ndemonstrating that significant improvements can be\nachieved by selecting suitable (question, SQL) ex-\nample pairs (Nan et al., 2023; Gao et al., 2023; Guo\net al., 2024; Sun et al., 2024). In spite of its under-\nlying benefits, conventional solutions for example\nselection are usually limited to retrieving examples\nbased solely on the similarity of questions (Nan\net al., 2023; An et al., 2023; Guo et al., 2024).\nOther approaches resort to a preliminary round of\nparsing which approximates expected SQL queries,\nand directly use these approximations in few-shot\nprompting (Sun et al., 2024), or to subsequently\nselect (question, SQL) pairs by comparing the ap-\nproximated query to queries within candidate exam-\nples (Gao et al., 2023). The approach proposed by\nGao et al. transforms SQL queries into SQL skele-\ntons (Li et al., 2023a) and then filters examples by\nconsidering overlap token ratio as the similarity\nbetween two skeletons. While incorporating SQL\nskeleton similarity improves over conventional ex-\nample selection for Text-to-SQL (Gao et al., 2023),\nit can result in structural information loss as ex-\nemplified in Table 1, where two dissimilar SQL\nqueries are treated as identical. In this paper, we\npropose a novel approach that selects examples\nusing similarity of normalised SQL Abstract Syn-\ntax Trees (ASTs). We argue that considering the\nsimilarity of such hierarchical structures can sig-\nnificantly enhance LLMs' performance for Text-to-\nSQL parsing.\nApart from example selection, we refine\ndatabase context input to LLMs by dynamically\npruning schemata and selecting values. From the\nperspective of LLMs, existing studies achieve im-\nprovements by including the full database schema\nin the prompt and additionally hinting the impor-\ntance of particular schema elements or values (Pour-\nreza and Rafiei, 2023; Sun et al., 2024). In this pa-\nper, we show that the performance can be boosted"}, {"title": "2 Preliminaries", "content": "Let q be the sequence of tokens of a natural lan-\nguage question for database D with tables t =\nt1, t2,..., tr and columns c = c1, 2, ..., cj, ...,\nCE, where c is the j-th column of table t\u2081 and\nCi\u2208 N is the total number of columns in table\nti. Furthermore, let VD\n\u0421\u0442= {U},...,r} \nbe the set of all values associated with the database\nDs.t. v..., vare the DB value sets associ-\nated with respective columns c\u2081, ..., c\u2081 \u2208 c. The\ngoal of Text-to-SQL semantic parsing is to predict\nthe SQL query s given the (q, D) combination, as\nfollows:\n$S = arg \\underset{s}{max} p (s | q, D)$                           (1)\nFor in-context learning, we seek to select pertinent\ninput context including few-shot examples, schema,\nand database values to simplify the task for LLMs."}, {"title": "3 Example Selection using Abstract Syntax Trees", "content": "Our goal is to identify the most suitable set of\nX* = {(q\u2081, s\u2081), ..., (q,s)} question-SQL pairs\nfrom an index of examples, X, s.t. X* \u2286 X, for\nmaximising the probability of an LLM to predict\nthe correct SQL given (q, D):\n$X^*=\\underset{X}{arg \\underset{X}{max} p (s | q, D, X)}$                             (2)\nFrom the perspective of ranking, we consider\nthe relevance score between a candidate example\n(qj, sj) \u2208 X and the input (q, D). Vanilla semantic\nsearch is usually based solely on question embed-\ndings, whereas the structure of SQL queries for\nsimilar questions is subject to target databases and\ncan thus differ significantly.\nTo incorporate database context for selecting ex-\namples, we propose to re-rank examples retrieved"}, {"title": "4 Database Context Selection", "content": "Apart from relevant question-SQL pairs, prompting\nfor Text-to-SQL parsing requires the context of\ndatabase schema and values.\n4.1 Schema Selection\nWe present a hybrid search strategy that selects\na sub-schema given a test question to minimise"}, {"title": "4.1.1 Incorporating for Approximated Query", "content": "The semantic search for schema selection requires\na comprehension of the relevance between hetero-\ngeneous database information and natural language\nquestions, in addition to interactions across schema\nelements. To this end, a trained parser can inher-\nently be a semantic search model for retrieving\na sub-schema, where columns and tables are ex-\ntracted from the approximated query s'. We argue\nthat a semantic parser which is performing reason-\nably on the task, can provide us with an s', whose\nstructure would assimilate the structure of the ex-\npected final query, s. Consequently, we opt to\ndynamically determine the number of columns to\nbe retrieved by scorebM25 as proportional to the\nnumber of unique columns in s', returned by the\napproximator. A sub-schema is then obtained by\nmerging schema elements selected by scorebM25\nwith elements from the approximated query."}, {"title": "4.1.2 Approximating for Longer Schemata", "content": "To further reduce the computational workload, we\nopt for using a smaller model for computing the\napproximated query, s'. However, smaller models\nusually have shorter context windows (i.e. < 2k\ntokens), and, as such, they cannot be easily scaled\nto the requirements of larger schemata. To this end,\nwe propose an approach that enables transformer-\nbased encoders to process longer schemata, in a\nparallelised manner.\nWe start with FastRAT (Vougiouklis et al., 2023),\nwhich exploits a decoder-free architecture for effi-\ncient text-to-SQL parsing. Given a concatenation\nof the input natural language question q with the\ncolumn and table names of a database schema, Fas-\ntRAT computes the SQL operation in which each\nelement of the input schema would participate in"}, {"title": "4.2 Value Selection", "content": "The inference of LLMs for text-to-SQL parsing\ncan be augmented with column values (Sun et al.,\n2024). We select values for columns in a schema\n(or a sub-schema) by simply matching keywords\nin questions and values. This is based on the as-\nsumption that LLMs can generalise to unseen val-\nues given a set of representative values; thus, the\nrecall and precision of value selection are less crit-\nical. We consider value selection providing ad-\nditional information for LLMs to discern covert\ndifferences among columns. An example of our\nresulting prompt is shown in Appendix B."}, {"title": "5 Experiments", "content": "We run experiments using two approximators:\nFastRAText and Graphix-T5 (Li et al., 2023b).\nGraphix-T5 is is the approximator used by DAIL-\nSQL (Gao et al., 2023), and is included to facilitate\na fair comparison against the closest work to ours.\nFastRAText is trained and tested using r = 64, un-\nless otherwise stated (cf. Section 5.4). For all\nexperiments, we use 5 (question, SQL) examples.\nWe test our approach against both closed-\nand open-source LLMs: (i) gpt-3.5-turbo\n(gpt-3.5-turbo-0613), (ii) gpt-4 (gpt-4-0613)\nand (iii) deepseek-coder-33b-instruct. Re-\nsults using additional models from the DeepSeek\nfamily are provided in Appendix C.4."}, {"title": "5.1 Datasets", "content": "We experiment with several SQL datasets, seek-\ning to explore the effectiveness of our approach on\nboth monolingual and cross-lingual setups. Specif-\nically, we report experiments on CSPIDER (Min\net al., 2019) and SPIDER (Yu et al., 2018). Since\nCSPIDER is a translated version of SPIDER in Chi-\nnese, when it comes to the natural language ques-\ntions, the characteristics of the two with respect\nto structure and number of examples are identi-\ncal. We focus our evaluation on the development"}, {"title": "5.2 Baselines", "content": "We dichotomize the landscape of baselines in fine-\ntuning- and prompting-based baselines. Further\ndetails are provided in Appendix E.\nFine-tuning-based (i) GraPPa, (ii) DG-\nMAML, (iii) FastRAT, (iv) Graphix-T5,\n(v) RESDSQL and (vi) HG2AST.\nPrompting-based Zero-shot LLM prompting\nhas been explored by Guo et al.; Liu et al.; Pour-\nreza and Rafiei; (i) C3 introduces calibration bias\nfor LLM prompting; (ii) DIN-SQL uses chain-\nof-thought prompting with pre-defined prompting\ntemplates tailored for the assessed question hard-\nness; (iii) DAIL-SQL uses query approximation\nand SQL skeleton-based similarities for example\nselection; (iv) SQL-PaLM proposes a framework\nfor soft column selection and execution-based re-\nfinement; (v) RAG w/ Rev. Chain augments the\ninput prompt with question skeleton-based example\nretrieval and an execution-based revision chain."}, {"title": "5.3 Text-to-SQL Evaluation", "content": "Table 2 and 3 summarise the results of our\napproach with deepseek-coder-33b-instruct,\ngpt-3.5-turbo and gpt-4 against the baselines.\nOur approach, comprising a single-prompting\nround, surpasses other LLM-based solutions, that\nincorporate several prompting iterations, for LLMs\nof the same capacity. We note consistent improve-\nments over DAIL-SQL, the closest work to ours,\neven when FastRAText is used as approximator\n(i.e. a model consisting of < 500M vs the \u2265 3B\nparameters that DAIL-SQL's approximator is us-\ning). For the same approximator, our framework\nis able to meet, performance standards of DAIL-\nSQL (equipped with gpt-4 and an additional self-\nconsistency prompting step) using an open-source\nmodel as backbone LLM, by achieving shorter\nprompts in a single prompting step.\nSPIDER results are consistent with the results\nacross the various Spider variants and CSPIDER\n(Table 3). Our approach levering FastRAText and\nAST-based re-ranking for example selection out-\nperforms other prompting-based solution, and is in-\nline with the scores of state-of-the-art fine-tuning-\nbased baselines. While gpt-4 is the most capa-"}, {"title": "5.3.1 Schema Selection Evaluation", "content": "We evaluate our proposed schema selection strategy\nin a two-fold manner, given that value selection is\napplied for selected columns. Firstly, we use recall\nand schema shortening (rate) to compute averaged\nmetrics across all samples showcasing the extent\nto which (i) the most relevant schema elements\nare successfully retrieved, and (ii) the size of the\nresulting schema, after selection, with respect to\nits original size. Secondly, we explore how the\nperformance of the end-system changes across dif-\nferent schema pruning settings by reporting EX\nand EM scores. Recall is the percentage of sam-\nples for which all ground-truth schema elements\nare selected. Schema shortening is the number\nof schema elements that are excluded divided by\nthe total number of schema elements. Results are\nsummarised in Table 4."}, {"title": "5.3.2 Ablation Study", "content": "Table 5 shows a comprehensive ablation study for\nthe efficacy of our database context selection, and\nexample selection methods including DAIL (Gao\net al., 2023) and AST. We consistently notice im-\nprovement when selecting examples using AST, for\nthe same approximator. Interestingly, the perfor-\nmance gap is increasing the better the approximator\nbecomes, leading to an improvement > 2.4% in the\ncase of an oracle approximator. This finding is in\nagreement with our hypothesis that AST re-ranking\ncan preserve structural information for more pre-"}, {"title": "5.4 Schema Splitting", "content": "We evaluate the effect of splitting a schema into\nI'm splits, using FastRAText for schema selection.\nFigure 1 shows EX scores across different maxi-\nmum number of columns per schema split (r), on\nthe development set of SPIDER. We see that the EX\nscores of our approach remain consistent across"}, {"title": "6 Discussion", "content": "Are there any theoretical performance upper\nlimits for example selection using AST? For\neach data instance in the development set of Spider,\nwe compute the average AST similarity between\nthe approximated query and each SQL query that\nis included (after example selection) in the cor-\nresponding prompt. In Table 6, we measure EX\nscores on the development set of SPIDER, across\ndifferent AST-similarity intervals. We see an ob-"}, {"title": "7 Related Work", "content": "Significant number of recent works have looked at\nhow LLMs can be employed in Text-to-SQL sce-\nnarios (Rajkumar et al., 2022; Chang and Fosler-\nLussier, 2023; Liu et al., 2023; Pourreza and Rafiei,\n2023; Gao et al., 2023; Guo et al., 2024). More\nrecent works have looked at how incorporating ex-\namples in the prompt could benefit the performance\nof LLMs in the end task (Pourreza and Rafiei, 2023;\nGao et al., 2023; Guo et al., 2024; Sun et al., 2024).\nIn spite of its underlying benefits, conventional\nsolutions for example selection have focused on re-\ntrieving pairs using question similarity (Nan et al.,\n2023). Other approaches have sought to approxi-\nmate expected SQL queries, and either directly use\nthese approximations in the prompt, in a few-shot\nsetting (Sun et al., 2024) or to filter candidate (ques-\ntion, SQL) pairs by taking into consideration the\nsimilarity of their corresponding SQL query skele-\ntons (Li et al., 2023a) against the skeleton of the\napproximated SQL (Gao et al., 2023). We argue\nthat such example selection strategies can result in\ninformation loss, and we propose an approach for\nre-ranking examples using similarity of normalised\nSQL ASTS.\nThe benefits of schema selection for Text-to-\nSQL have been highlighted across the relevant bib-\nliography (Wang et al., 2020; Li et al., 2023b; Pour-\nreza and Rafiei, 2023). From the LLMs perspec-\ntive, pruning schema elements from the prompts\nhas been usually leading to performance degrada-\ntion (Sun et al., 2024). Inspired by Gao et al., we\ncompute a preliminary query for a given (q, D) by\nwe adapting FastRAT (Vougiouklis et al., 2023), to\nthe requirements of processing longer schemata, in\na parallelised manner. We couple the resulting ap-\nproximator with a sparse retriever, and we propose\na dynamic strategy for reducing the computational\ncost of the task while achieving performance im-"}, {"title": "8 Conclusion", "content": "In this paper, we augment LLMs for Text-to-SQL\nsemantic parsing by selecting suitable examples\nand database information. We present a novel\nAST-based metric to rank examples by similarity\nof SQL queries. Our hybrid search strategy for\nschema selection reuses a preliminary query to re-\nduce irrelevant schema elements while maintaining\nhigh recall. Extensive experiments demonstrate\nthat our AST-based ranking outperforms previous"}, {"title": "Limitations", "content": "There are limitations with regards to both of our\nexample selection and schema selection. Our AST-\nbased ranking can be biased when an approximated\nSQL deviates significantly from structurally cor-\nrect answers. To address the failure of approxi-\nmators, a future direction is to sensibly diversify\nselected examples such that LLMs can generalise\ncompositionally. As for schema selection, our se-\nmantic search relies on an approximator which is\nessentially a parser with high precision in schema\nlinking but lack mechanisms to control recall as a\nstandalone model. Therefore, it is worth extending\ncross-encoder architecture such as FastRAT to sup-\nport ranking schema elements while being a SQL\napproximator in the meantime.\nWe demonstrate that schema splitting strategies\nwithin our framework can be applied across various\nnumbers of splits without noticeable performance\ndegradation. Nonetheless, given the lack of avail-\nable datasets that incorporate longer commercial\nschemata, we focus our experiments on the cross-\ndatabase setting provided by CSPIDER and SPIDER\nvariants."}, {"title": "Ethics Statement", "content": "We do not make use of any private, proprietary,\nor sensitive data. FastRAText is trained on pub-\nlicly available Text-to-SQL datasets, using publicly\navailable encoder-models as base. Our framework\nfor retrieval-augmented generation builds on-top\nof large, pre-trained language models, which may\nhave been trained using proprietary data (e.g. in the\ncase of the OpenAI models). Given the nature of\npre-training schemes, it is possible that our system\ncould carry forward biases present in the datasets\nand/or the involved LLMs."}]}