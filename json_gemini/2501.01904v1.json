{"title": "Virgo: A Preliminary Exploration on Reproducing o1-like MLLM", "authors": ["Yifan Du", "Zikang Liu", "Yifan Li", "Wayne Xin Zhao", "Yuqi Huo", "Bingning Wang", "Weipeng Chen", "Zheng Liu", "Zhongyuan Wang", "Ji-Rong Wen"], "abstract": "Recently, slow-thinking reasoning systems, built upon large language models (LLMs), have garnered widespread attention by scaling the thinking time during inference. There is also growing interest in adapting this capability to multimodal large language models (MLLMs). Given that MLLMs handle more complex data semantics across different modalities, it is intuitively more challenging to implement multimodal slow-thinking systems.\nTo address this issue, in this paper, we explore a straightforward approach by fine-tuning a capable MLLM with a small amount of textual long-form thought data, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning with long thought). We find that these long-form reasoning processes, expressed in natural language, can be effectively transferred to MLLMs. Moreover, it seems that such textual reasoning data can be even more effective than visual reasoning data in eliciting the slow-thinking capacities of MLLMs. While this work is preliminary, it demonstrates that slow-thinking capacities are fundamentally associated with the language model component, which can be transferred across modalities or domains. This finding can be leveraged to guide the development of more powerful slow-thinking reasoning systems. We release our resources at https://github.com/RUCAIBox/Virgo.", "sections": [{"title": "Introduction", "content": "Recently, slow-thinking reasoning systems (e.g., OpenAI 01 [1], DeepSeek R1 [2], and Qwen\nQwQ [3]) have demonstrated significant performance improvements across various benchmarks,\nparticularly excelling in challenging problems where previous large language models (LLMs) [4] have\nunderperformed [5, 6, 7]. These systems employ both train-time and test-time scaling to enhance\naccuracy and capacity in solving complex tasks, which typically involve an extended reasoning\nprocess, referred to as \u201cthought\u201d, before reaching the final solution. While these systems primarily\nfocus on textual problems, latest advancements have also shown promising results in multimodal\nscenarios (e.g., QVQ [8]).\nAlthough commercial companies have not disclosed the underlying techniques for creating slow-\nthinking systems, researchers have made significant efforts to advance public technical progress in\nthis area. According to the existing literature, there are two typical approaches to implementing\nslow-thinking reasoning systems [9, 10, 11, 12, 13]. The first approach utilizes an explicit search\nstructure (e.g., Monte Carlo tree search) and employs specially trained reward models to guide the\nsearch process toward the correct path. The second approach involves distilling or constructing\ninstruction data in the form of long chain-of-thought (CoT) and then fine-tuning a capable LLM\nto learn such reasoning modes. This method can be further enhanced using self-improvement or\nself-play methods such as direct preference optimization and reinforcement learning.\nA major limitation of these research studies is their primary focus on textual tasks, with relatively\nlittle consideration given to multi-modal scenarios. To our knowledge, existing efforts in developing\nmultimodal reasoning systems still significantly lag behind commercial systems like o1 and QVQ.\nThis paper aims to explore the implementation of multimodal slow-thinking reasoning systems that\ncan achieve performance comparable to these commercial systems. Unlike text-based reasoning\nsystems, multimodal reasoning systems (i.e., Multimodal LLMs, or MLLMs) ([14, 15]) often leverage\nboth the abilities of perception (i.e., understanding the visual semantics of the input figures) and\nreasoning (i.e., determining the approach to solving a given task). Consequently, it is still unclear\nhow slow-thinking operates on multimodal inputs.\nOverall, we believe that developing multimodal slow-thinking reasoning systems presents a significant\nchallenge, as it involves addressing the complex data and modeling mechanisms inherent in multi-\nmodal tasks. Considering this challenge, we aim to explore a simple idea to implement multimodal\nreasoning systems: Can we directly adapt MLLMs by fine-tuning them with text-based long thought\ndata? The intuition is that, since reasoning is primarily handled by the LLM component within an\nMLLM, we might be able to elicit its slow-thinking capacities using text-only instructions. In fact,\nthere is growing evidence in enhancing the capabilities of MLLMs by leveraging more extensive\ntext-only instruction [16, 17].\nIn this paper, we design a straightforward approach to enhance MLLMs with slow-thinking capacities\nby leveraging text-based long thought data. Our primary focus is investigating two key questions:\n(1) Can the slow-thinking ability transfer across modalities through fine-tuning with text-based\nlong thought data? (2) Can the ability derived from text-based long thought data be comparable to\nthat distilled from multimodal slow-thinking systems? Specifically, we consider collecting textual\nreasoning instructions shared by previous studies [18], as well as generating visual reasoning instruc-\ntions by distilling from multimodal slow-thinking systems. Following this method, we implement a\nmultimodal slow-thinking systems, denoted as Virgo (Visual reaoning with long thought). We select\na capable MLLM, Qwen2-VL-72B-Instruct, as the backbone model and employ different instruction"}, {"title": "Method", "content": "In this section, we present our preliminary attempts to adapt MLLMs by equipping them with\nslow-thinking capacities for complex multimodal tasks. We explore two straightforward adaptation\nmethods: (1) transferring slow-thinking abilities using text-based long thought data, and (2) distilling\nmultimodal long thought data from existing slow-thinking MLLMs. Our aim is to investigate how\nslow-thinking capacities are elicited in MLLMs and to identify which approaches are more effective\nfor achieving this goal. Next, we describe the specific implementation details."}, {"title": "Capacity Transfer from Text-only Instructions", "content": "Previous studies [18] have shown that slow-thinking reasoning is likely a behavioral mode that can\nbe elicited by fine-tuning with a small amount of long thought data. Moreover, this capacity can\ngeneralize across different domains. Therefore, our idea is to investigate whether this ability can\nalso transfer to different modalities, given that existing MLLMs are developed with LLMs as their\nbackbone."}, {"title": "Collecting Textual Long Thought Data", "content": "We begin by collecting textual long thought data from our previous study [18]. Specifically, we\nobtain approximately 5K long thought instruction instances distilled from two open slow-thinking rea-\nsoning systems: DeepSeek-R1-Lite-Preview [2] (abbreviated as R1) and QwQ-32B-preview [3]\n(abbreviated as QwQ). The statistics of the collected instruction data are categorized by domain as\nfollows: math (3.7K), science (0.9K), code (0.2K) and puzzle (0.1K). We select the majority of the\ninstruction data from the math domain because it contains more challenging problems that require\nlonger reasoning processes.\nThese instructional data are formatted with two distinct parts: the thought process, indicated by\nspecial symbols \u201c<|begin_of_thought|>", "<|end_of_thought |>": "and the final solution,\nindicated by special symbols \u201c<|begin_of_solution|>", "<|end_of_solution|>\". More\ndetails about the data composition and instruction format can be found in our previous paper [18": "."}, {"title": "Textual Long Thought Instruction Tuning", "content": "After collecting instruction data for long-form reasoning, we fine-tune the base MLLM to emulate\nslow-thinking reasoning behavior. We choose Qwen2-VL-72B-Instruct [19] as the target model\ndue to its excellent multimodal capabilities. Additionally, our previous work [18] indicates that\nslow-thinking capacities are more readily achieved in stronger models.\nTo optimize the target MLLM, we train only the parameters from the LLM and cross-modal connector\nwhile keeping the parameters in the visual encoder frozen. We use the following optimization settings:\na learning rate of 7e-6, a batch size of 128, and training for 10 epochs. Based on the performance on\nthe development set, we select the model at the 5th epoch for evaluation.\nWe do not employ more advanced training algorithms, such as DPO [20] and RLHF [21], as our\nobjective is not to attain the maximum possible performance. Instead, we aim to explore the potential\nof transferring slow-thinking capacities through simple fine-tuning with textual long thought data.\nOur aim is to investigate the effect of textual long thought data using the straightforward imitation\nmethod."}, {"title": "Capacity Distillation from Slow-thinking MLLMs", "content": "The second approach we explore is the direct distillation of multimodal long thought data from\nslow-thinking MLLMs (e.g., QVQ). This approach aims to achieve two goals: first, to compare the\nfine-tuning performance of textual long thought data with that of multimodal long thought data, and\nsecond, to investigate the potential effects of combining both textual and multimodal instruction data."}, {"title": "Visual Long Thought Data Collection", "content": "To construct visual long thought data, a crucial step is to gather a set of high-quality visual problems,\nwhich include both task descriptions and images as input. Additionally, these problems should be\naccompanied by ground-truth answers for correctness verification. We consider selecting problems\nfrom visual question answering (VQA) datasets to cover diverse domains such as geometry, tables,\nfigures, and icons. We select these domains because they typically present more challenging problems\nfor MLLMs.\nSpecifically, we select four geometry datasets (Geos [22], GeoQA+ [23], Geometry3K [24], and\nUniGeo [25]), three table and figure datasets (TabMWP [26], FigureQA [27], and ChartQA [28]), and\nan object dataset (CLEVR [29]). These datasets can be accessed from the LLaVA-OneVision [30]\ndata collection, where each instance provides a question, image, and answer triple. Detailed statistics\nfor each dataset are presented in Table 1."}, {"title": "Visual Long Thought Instruction Tuning", "content": "When distilling the long thought data from QVQ (denoted by $D_{QVQ}$), the training process is straight-\nforward: we fine-tune only the parameters of the LLM and the modality connector, as we do with\nthe textual long thought data described in Section 2.1. Although the visual instruction data includes\nimage information, our experimental results indicate that updating the visual encoder does not result\nin substantial performance improvement.\nAs another alternative approach, we design a multi-stage tuning method for self-distillation. Specifi-\ncally, we first fine-tune the selected MLLM (i.e., Qwen2-VL-72B-Instruct) on the textual long thought\ninstruction set $D_T$, obtaining model $M_O$. Next, we use $M_O$ to generate the visual long thought\ninstruction set by self-distillation $D_{SD}$, which can be subsequently used for fine-tuning the original\nMLLM.\nIn our experiments, our aim is to investigate the effects of individual instruction datasets (i.\u0435., $D_T$,\n$D_{SD}$ and $D_{QVQ}$) and their combinations on the slow-thinking performance."}, {"title": "Experiments", "content": "To validate the effectiveness of our methods, we conduct experiments on four challenging benchmarks:\nMathVerse [31], MathVision [32], OlympiadBench [33], and MMMU [34]. MathVerse consists of\n2,612 multi-subject math problems from diverse sources. MathVision comprises 3,040 high-quality\nmathematical problems sourced from established mathematics competitions. OlympiadBench features\n8,476 bilingual multimodal problems for Olympic-level mathematics and physics competitions.\nMMMU encompasses 11,500 problems spanning 30 subjects and 183 subfields. To ensure a fair\ncomparison, we conduct evaluations on the validation set of MMMU and the testmini set of MathVerse.\nFollowing VLMEvalKit [35], we exclude the text-only split from MathVerse and the theorem-proof\nparts from OlympiadBench. Among all the benchmarks, OlympiadBench is the most challenging,\nwhile MMMU demonstrates relatively lower difficulty levels and focuses more on comprehensive\nsubject knowledge.\nWe select Qwen2-VL-72B-Instruct [19] as our base MLLM due to its strong multimodal capabilities.\nWe denote our model as Virgo-72B and compare it with a range of models that are capable of\nconducting 01-like slow-thinking (i.e., OpenAI 01 and QVQ-72B-preview). We also include advanced\ngeneral-purpose models (i.e., GPT-40, Gemini-Pro, and Claude-3.5-Sonnet) for comparison. We also\ntrain Virgo-7B based on Qwen2-VL-7B-Instruct to further study the influence of model size."}, {"title": "Main Results", "content": "In this section, we provide a comprehensive performance comparison of various methods on the\nselected evaluation benchmarks, as summarized in Table 2. The results include the performance of\n01-like MLLMs, general-purpose MLLMs, and our approaches that extend the backbone model with\ndifferent long thought instruction datasets.\nFirst, the slow-thinking reasoning ability can be effectively transferred through text-only reasoning\ndata. As demonstrated in the second group of Table 2, after fine-tuning with only 5K textual long\nthought instructions, our model yields highly competitive results, approaching and even surpassing\nthose of industry counterparts. For instance, our model achieves 38.4% accuracy on MathVision and\n29.3% accuracy on OlympiadBench. However, another observation is that our model does not show"}, {"title": "Further Analysis", "content": "After presenting the overall performance analysis, we further investigate the detailed effects of long\nthought instruction data on visual reasoning. We present the major findings below.\nHarder tasks benefit more from long thought reasoning. We first examine how our approach im-\npacts model performance across tasks of varying difficulty levels. Previous research[18] has indicated\na correlation between the average length of responses generated by models and the complexity of the"}, {"title": "Case Study", "content": "In this section, we present several examples to demonstrate the advantages of slow-thinking reasoning\nin addressing complex multimodal problems. Additionally, we provide examples highlighting some\nof the negative impacts introduced by our approach."}, {"title": "Conclusion", "content": "In this paper, we have proposed a simple yet effective approach to implementing slow-thinking\nreasoning systems. Our core finding is that by simply fine-tuning MLLMs with textual long-form\nthought data, a capable MLLM can exhibit substantially enhanced slow-thinking capacities. We have\nconducted extensive experiments on four challenging benchmarks, and the results demonstrate that\nour approach achieves highly competitive performance when compared to industry reasoning systems.\nWe have also examined the effect of different factors of textual instruction data, such as data amount\nand length distribution.\nWe are aware that our current attempt at building multimodal slow-thinking systems is preliminary.\nFor future work, we aim to expand the sources of challenging multimodal problems and design more\nprincipled methods to enhance this capacity."}]}