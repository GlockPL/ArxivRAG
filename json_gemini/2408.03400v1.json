[{"title": "Attacks and Defenses for Generative Diffusion Models: A Comprehensive Survey", "authors": ["Vu Tuan Truong", "Luan Ba Dang", "Long Bao Le"], "abstract": "Diffusion models (DMs) have achieved state-of-the-art performance on various generative tasks such as image synthesis, text-to-image, and text-guided image-to-image generation. However, the more powerful the DMs, the more harmful they potentially are. Recent studies have shown that DMs are prone to a wide range of attacks, including adversarial attacks, membership inference, backdoor injection, and various multi-modal threats. Since numerous pre-trained DMs are published widely on the Internet, potential threats from these attacks are especially detrimental to the society, making DM-related security a worth investigating topic. Therefore, in this paper, we conduct a comprehensive survey on the security aspect of DMs, focusing on various attack and defense methods for DMs. First, we present crucial knowledge of DMs with five main types of DMs, including denoising diffusion probabilistic models, denoising diffusion implicit models, noise conditioned score networks, stochastic differential equations, and multi-modal conditional DMs. We further survey a variety of recent studies investigating different types of attacks that exploit the vulnerabilities of DMs. Then, we thoroughly review potential countermeasures to mitigate each of the presented threats. Finally, we discuss open challenges of DM-related security and envision certain research directions for this crucial topic.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, diffusion models (DMs) [1]\u2013[11] have showcased remarkable capability in a wide range of generative tasks, setting the new state-of-the-art among other categories of deep generative models such as generative adversarial networks (GANs) [12], variational autoencoders (VAEs) [13], [14], and energy-based models (EBMs) [15]. In general, DMs consist of two main processes. The forward (diffusion) process progressively adds noise to the original data to gradually diffuse the data distribution into the standard Gaussian distribution. The reverse (generative) process employs a deep neural network, which is often a UNet [16], to reverse the diffusion, reconstructing the data from the Gaussian noise. With its impressive potentials, DMs have been investigated in various domains, including computer vision [17]\u2013[28], natural language processing (NLP) [29]\u2013[34], audio processing [35], [36], 3D generation [37]-[42], bioinformatics [43], [44], and time series tasks [45]-[47].\nDMs can be classified into different categories based on their diffusion and generative processes. The first category, inspired by the non-equilibrium thermodynamics theory [1], comprises of denoising diffusion probabilistic models (DDPMs) [1]\u2013[4]. DDPMs can be viewed as a Markovian Hierarchical VAE, in which the diffusion process is modeled as a Markov chain with multiple consecutive VAEs. Each diffusion step corresponds to the encoding process of a VAE, while each denoising step can be considered a decoding operation of the corresponding VAE. On the other hand, denoising diffusion implicit models (DDIMs), a variant of DDPMs, take a non-Markovian approach that allows the models to skip steps in the denoising process, thus increasing the generating speed with a certain trade-off for quality. Another category of DMs is noise conditioned score networks (NCSNs) [6], [7], [10], in which a neural network is trained based on score matching [48] to learn the score function (i.e., the gradient of the log likelihood) of the true data distribution. This score function points towards the data space that the training data inhabits. Therefore, by following the score, well-trained NCSNs can generate new samples accordingly to the true data distribution. This process can also be viewed as a denoising process [49]. The final main category, score-based stochastic differential equation (SDE), encapsulates both DDPMs and NCSNs into a generalized form. While the forward process maps data to a noise distribution via an SDE, the reverse process uses the reverse-time SDE [50] to generate samples from noise. Moreover, cross-attention technique [51] can be used to constraint the denoising neural network with multi-modal conditions such as text and images, forcing the denoising process to generate results that follow the given conditions. This sparked a wide variety of multi-modal generative tasks such as text-to-image and text-guided image-to-image generation [9].\nDespite these remarkable potentials, DMs are especially vulnerable to various security and privacy risks due to the following reasons: (i) robust DMs are often trained on large-scale data collected from diverse open sources, which might include poisoned or backdoored data; (ii) pre-trained DMs are published widely on open platforms like HuggingFace\u00b9, making it easier for hackers to spread their manipulated models. For instance, by manipulating the training data and modifying the training objective, attackers can embed a backdoor trigger into DMs to conduct a backdoor attack [58]\u2013[67]. Consequently, once the trigger is fed into the backdoored DM during inference, it will consistently produce a specific result that is designated by the attackers (e.g., a sensitive image or a violent text). Even in a more secure setting in which the attackers cannot modify the DMs' parameters, they can still craft the inputs of eligible DMs to make it generates sensitive contents; this is referred to as adversarial attack [68]\u2013[84]. In terms of privacy, membership inference can be launched to detect whether a particular example was included in the training dataset of DMs. This is especially dangerous when the training data is highly sensitive (e.g., medical images). Furthermore, as DMs are also used in various security applications such as adversarial purification and robustness certification, attacking the DMs integrated in such applications can disable the entire DM-based security systems [85], [86].\nSince DMs are gaining significant attention and various DM-based applications have been used widely by the public, it is undeniable that security of DMs is an important research direction. However, existing surveys on DMs mostly exploit its developments in terms of architecture improvements, performance, and applications, while the security aspect of DMs is totally neglected. For instance, the authors in [53] surveyed a wide range of algorithm improvements for DMs, including the improvements in sampling acceleration, diffusion process design, likelihood optimization, and bridging distributions. Besides, they also reviewed various applications of DMs such as image/video generation, medical analysis, text generation, and audio generation. Similarly, the survey [52] also discussed the applications and development of DMs, with a significant attention on efficient sampling methods and improved likelihood. Furthermore, the authors investigated thoroughly the connection between DMs and other categories of deep generative models like VAES, GANs, and EBMs. In terms of application-centric surveys, there are multiple surveys study DM-based applications, including computer vision [54], NLP [55], medical imaging [56], and time-series applications [57].\nAs none of existing surveys investigates the security aspect of DMs, this paper aims to fill the gap by providing a systematic and comprehensive overview of state-of-the-art research studies in this crucial topic. By categorizing different types of DM-targeted attacks and presenting countermeasures for tackling these attacks, we hope this survey provides a helpful guideline for researchers to explore and develop state-of-the-art security methods for DMs. The contribution of the paper can be summarized as follows:\n\u2022 We provide readers with necessary background knowledge of different types of DMs, including DDPM, DDIM, NCSN, SDE, and multi-modal conditional DMs. We demonstrate how different categories of DMs relate to each other under a consistent diffusion principle.\n\u2022 We investigate a wide range of attacks on DMs, categorized into three main groups, including backdoor attacks, membership inference attacks (MIAs), and adversarial attacks. Each attack is categorized further into sub-groups based on the corresponding methods/ applications.\n\u2022 We survey various countermeasures for DM-targeted attacks based on state-of-the-art research studies in that field.\n\u2022 We discuss multiple open challenges in this topic and envision some interesting research directions to improve the security aspect of DMs and DM-based applications.\nThe rest of this paper is presented as follows. Section II provides preliminaries of different types of DMs and background knowledge of DM security. Section III surveys state-of-the-art methods for attacking DMs and DM-based systems. Then, different countermeasures for the presented attacks are discussed in Section IV. Section V discusses various open challenges and future research directions on this field, while Section VI concludes our survey."}, {"title": "II. BACKGROUND KNOWLEDGE", "content": "DM is a type of deep generative models that learn to generate samples from random noise via two processes", "16": "is trained to predict and remove the noise gradually from step T back to step 0", "Process": "The forward process of DDPMs is modelled as a Markov chain", "as": "n$q(x_t|x_{t-1"}, "mathcal{N}(x_t; \\sqrt{\\alpha_t}x_{t-1}, (1 - \\alpha_t)I)$, (1)\nwhere $\\alpha_t \\in (0,1)$ is the noise schedule that determines the noise scale at each diffusion step t. While $\\alpha_t$ can be either a fixed or a learnable hyperparameter, it must be chosen such that the final distribution $X_T$ becomes a standard Gaussian, \u0456.\u0435., $p(x_T) = \\mathcal{N}(x_T; 0, I)$. In general, $\\alpha_t$ often decreases over time, which means that the noise increases gradually from step 0 to step T. Then, we can apply the reparameterization trick on (1) to sample $x_t$ from $X_{t-1}$ with a sampled noise $\\epsilon \\sim \\mathcal{N}(0, I)$:\n$x_t = \\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1 - \\alpha_t}\\epsilon$. (2)\nBy repeating the reparameterization trick recursively based on (2), the choice of (1) leads to an important property that allows the direct sampling of $x_t$ from the clear data $x_o$:\n$q(x_t|x_o) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t}x_o, (1 - \\bar{\\alpha}_t)I)$ (3)\n$x_t = \\sqrt{\\bar{\\alpha}_t}x_o + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_0$, (4)\nwhere $\\bar{\\alpha}_t = \\Pi_{i=1}^t \\alpha_i$ and $\\epsilon_0 \\sim \\mathcal{N}(0, I)$.\n2) Reverse Process: The reverse process uses deep neural networks parameterized by $\\theta$ with the following denoising transitions:\n$p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$, (5)\nwhere $\\mu_\\theta(x_t, t)$ and $\\Sigma_\\theta(x_t, t)$ are the mean and variance parameterized by a deep neural network. The neural network approximates the data distribution via the log likelihood:\n$\\log p_\\theta (x_o) = \\log \\int p_\\theta (x_{o:T}) dx_{1:T}$, (6)\nwith $p_\\theta(x_{o:T}) = p(x_T) \\Pi_{t=1}^T p_\\theta(x_{t-1}|x_t)$.\n3) Training and Sampling: Using Jensen's inequality, the problem of maximizing the log likelihood in (6) can be interpreted as maximizing its evidence lower bound (ELBO), resulting in the following training objective:\n$L_{ddpm} = E_q[-\\log p_\\theta(x_o|x_1) + D_{KL}(q(x_T|x_o)||p(x_T)) + \\sum_{t=2}^T D_{KL}(q(x_{t-1}|x_t, x_o)||p_\\theta(x_{t-1}|x_t))"], "obtain": "n$q(x_{t-1"}, {"with": "n$\\mu_q(x_t", "epsilon_\\theta$": "n$\\mu_\\theta(x_t", "becomes": "n$L_{ddpm"}, ["D_{KL}(q(x_{t-1}|x_t, x_o)||p_\\theta(x_{t-1}|x_t))"], {"quality": "n$L_{simple"}, {"samples": "n$x_{t-1"}, {"Process": "while DDPMs first choose the diffusion transition $q(x_t|x_{t-1"}, {"first": "n$q_\\sigma(x_{t-1"}, {"follows": "n$\\mu_\\sigma(x_t", "property": "n$q_\\sigma(x_t|x_o) = \\mathcal{N"}, {"rule": "n$q_o (x_{t-1"}, {"Process": "By recursively applying the reparameterization trick on (19)", "that": "n$x_t = \\sqrt{\\bar{\\alpha"}, {"21)": "n$\\epsilon_o^{(0)"}, {"t)$": "n$p_\\theta(x_{t-1"}, {"Sampling": "The neural network $\\theta$ is also optimized by maximizing the log likelihood presented in (6)", "objective": "n$L_{ddim"}, {"24)": "n$x_{t-1"}, {"follows": "n$x_t = x_{t-1"}, {"objective": "n$L_{sm"}, [6], {"objective": "n$L_{ncsm"}, {"samples": "n$x_t = x_{t-1"}, {"that": "both (i) denoising and (ii) following the score function of data enable moving in directions that maximize the log probability of the data", "follows": "n$dx = f(x", "form": "n$dx = [f(x", "objective": "n$C_{sde"}, {"DMs": "n$p(x_{o:T"}, {"Attacks": "In backdoor attacks [90], the attackers modify the training data and objective function to embed a backdoor trigger into machine learning (ML) models. During inference, if the trigger is activated as input, the models will produce abnormal results driven by the attackers. On the other hand, the models still behave normally on benign samples. This stealthy property of backdoor attacks makes it challenging to be recognized by"}]