{"title": "Attacks and Defenses for Generative Diffusion Models: A Comprehensive Survey", "authors": ["Vu Tuan Truong", "Luan Ba Dang", "Long Bao Le"], "abstract": "Diffusion models (DMs) have achieved state-of-the-art performance on various generative tasks such as image synthesis, text-to-image, and text-guided image-to-image generation. However, the more powerful the DMs, the more harmful they potentially are. Recent studies have shown that DMs are prone to a wide range of attacks, including adversarial attacks, membership inference, backdoor injection, and various multi-modal threats. Since numerous pre-trained DMs are published widely on the Internet, potential threats from these attacks are especially detrimental to the society, making DM-related security a worth investigating topic. Therefore, in this paper, we conduct a comprehensive survey on the security aspect of DMs, focusing on various attack and defense methods for DMs. First, we present crucial knowledge of DMs with five main types of DMs, including denoising diffusion probabilistic models, denoising diffusion implicit models, noise conditioned score networks, stochastic differential equations, and multi-modal conditional DMs. We further survey a variety of recent studies investigating different types of attacks that exploit the vulnerabilities of DMs. Then, we thoroughly review potential countermeasures to mitigate each of the presented threats. Finally, we discuss open challenges of DM-related security and envision certain research directions for this crucial topic.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, diffusion models (DMs) [1]\u2013[11] have showcased remarkable capability in a wide range of generative tasks, setting the new state-of-the-art among other categories of deep generative models such as generative adversarial networks (GANs) [12], variational autoencoders (VAEs) [13], [14], and energy-based models (EBMs) [15]. In general, DMs consist of two main processes. The forward (diffusion) process progressively adds noise to the original data to gradually diffuse the data distribution into the standard Gaussian distribution. The reverse (generative) process employs a deep neural network, which is often a UNet [16], to reverse the diffusion, reconstructing the data from the Gaussian noise. With its impressive potentials, DMs have been investigated in various domains, including computer vision [17]\u2013[28], natural language processing (NLP) [29]\u2013[34], audio processing [35], [36], 3D generation [37]\u2013[42], bioinformatics [43], [44], and time series tasks [45]\u2013[47].\nDMs can be classified into different categories based on their diffusion and generative processes. The first category, inspired by the non-equilibrium thermodynamics theory"}, {"title": "II. BACKGROUND KNOWLEDGE", "content": "DM is a type of deep generative models that learn to generate samples from random noise via two processes, which are the forward diffusion process and the reverse process. These processes are illustrated in Fig. 1a. Given a training data sample such as an image, the diffusion process progressively adds noise into the image. After T noising steps, the original training data sample at step 0 is completely destroyed, resulting in a standard Gaussian noise at step T. Then, in the denoising process, a deep neural network, which is often a UNet [16], is trained to predict and remove the noise gradually from step T back to step 0, thus reconstructing the original image. After finishing the training process on a large-scale dataset, the UNet can generate new samples from any arbitrary Gaussian noise, in which the generated samples lie on the space of the original training data."}, {"title": "A. Denoising Diffusion Probabilistic Models (DDPMs)", "content": "DDPMs model both the forward and reverse processes as Markov chains, illustrated in Fig. 1b. This means that the result of each step only depends on its previous step, and there is a stochastic factor during any step transition.\n1) Forward Process: The forward process of DDPMs is modelled as a Markov chain, which progressively adds noise to the clear data sample xo in a total of T steps to generate a sequence of noisy distributions X1, X2, ..., X_T. The transition between two consecutive diffusion steps is defined as:\n$q(x_t|x_{t-1}) = N(x_t; \\sqrt{\\alpha_t}x_{t-1}, (1 - \\alpha_t)I),$\nwhere $\\alpha_t \\in (0,1)$ is the noise schedule that determines the noise scale at each diffusion step t. While $\\alpha_t$ can be either a fixed or a learnable hyperparameter, it must be chosen such that the final distribution $x_T$ becomes a standard Gaussian, \u0456.\u0435., $p(x_T) = N(x_T; 0, I)$. In general, $\\alpha_t$ often decreases over time, which means that the noise increases gradually from step 0 to step T. Then, we can apply the reparameterization trick on (1) to sample $x_t$ from $x_{t-1}$ with a sampled noise $\\epsilon \\sim N(0, I)$:\n$x_t = \\sqrt{\\alpha_t}x_{t-1}+\\sqrt{1 - \\alpha_t}\\epsilon.$\nBy repeating the reparameterization trick recursively based on (2), the choice of (1) leads to an important property that allows the direct sampling of $x_t$ from the clear data xo:\n$q(x_t|x_0) = N(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1 - \\bar{\\alpha}_t)I)$\n$x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_0,$\nwhere $\\bar{\\alpha}_t = \\prod_{i=1}^t\\alpha_i$ and $\\epsilon_0 \\sim N(0, 1)$.\n2) Reverse Process: The reverse process uses deep neural networks parameterized by $\\theta$ with the following denoising transitions:\n$p_\\theta(x_{t-1}|x_t) = N(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)),$\nwhere $\\mu_\\theta(x_t, t)$ and $\\Sigma_\\theta(x_t, t)$ are the mean and variance parameterized by a deep neural network. The neural network approximates the data distribution via the log likelihood:\n$\\log p_\\theta (x_0) = \\log \\int p_\\theta(x_{0:T})dx_{1:T},$\nwith $p_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^T p_\\theta(x_{t-1}|x_t)$.\n3) Training and Sampling: Using Jensen's inequality, the problem of maximizing the log likelihood in (6) can be interpreted as maximizing its evidence lower bound (ELBO), resulting in the following training objective:\n$L_{ddpm} = E_q[-\\log p_\\theta(x_0|x_1) + D_{KL}(q(x_T|x_0)||p(x_T))$\n$+ \\sum_{t=2}^T D_{KL}(q(x_{t-1}|x_t, x_0)||p_\\theta(x_{t-1}|x_t))],$\nwhere $D_{KL}$ denotes the KL divergence between two distributions. It can be seen that the second term in (7) can be ignored since it has no trainable parameter. Although the first term can be optimized using Monte Carlo estimate, it is just a single denoising step, which is totally dominated by the final term which consists of T - 1 steps; thus the first term can be ignored in practice. Consequently, only the final summation"}, {"title": "B. Denoising Diffusion Implicit Models (DDIMs)", "content": "In DDIMs, the number of denoising steps T is often chosen to be large (e.g., 1000 steps in [2]) to make the reverse process close to a Gaussian distribution [1]. With a low value of T, the generation results degrade significantly since the reverse process modeled with Gaussian distributions is no more a good approximation. However, such a large T leads to a low sampling/generating speed since all the steps must be performed sequentially due to the Markov chain's property. In DDIMs, the inference process is designed to be non-Markovian so we can skip steps in the denoising process, resulting in a faster generating speed with a certain trade-off in terms of generating quality. It is described in Fig. 1c.\n1) Forward Process: while DDPMs first choose the diffusion transition $q(x_t|x_{t-1})$ (1), then use it to derive the posterior $q(x_{t-1}|x_t, x_0)$ (8) via the Bayes rule, DDIMs take an opposite approach by choosing this posterior first:\n$q_\\sigma(x_{t-1}|x_t, x_0) = N(x_{t-1}; \\mu_\\sigma(x_t, x_0), \\sigma_t^2I),$\nwhere $\\sigma = [\\sigma_1,..., \\sigma_T]$ is a vector of positive coefficients controlling the stochastic magnitude of the forward process, and the mean $\\mu_\\sigma(x_t, x_0)$ is chosen as follows:\n$\\mu_\\sigma(x_t, x_0) = \\sqrt{\\bar{\\alpha}_{t-1}}x_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2} \\frac{x_t - \\sqrt{\\bar{\\alpha}_t}x_0}{\\sqrt{1 - \\bar{\\alpha}_t}}.$\nThe intentional choice of this mean function is to offer the following desirable property:\n$q_\\sigma(x_t|x_0) = N(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1 - \\bar{\\alpha}_t)I),$\nwhich is proved in [5]. This property is similar to the one presented in DDPMs (3). With this property, the forward transition of DDIMs can be derived based on the Bayes rule:\n$q_\\sigma (x_{t-1}|x_t, x_0)q_\\sigma(x_t|x_0)$\n$q_\\sigma (x_t|x_{t-1}, x_0) = \n$\\bar{\\alpha}_{t-1}|x_0).\nIt can be seen that each diffusion step t of DDIMs depends on both $x_{t-1}$ and $x_0$, making it non-Markovian.\n2) Reverse Process: By recursively applying the reparameterization trick on (19), we have that:\n$x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_0.$\nWe can train a neural network to predict the source noise $\\epsilon_0$, thus predicting the corresponding $x_0$ from (21):\n$x_\\theta^{(0)}(x_t,t) = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_\\theta (x_t, t)}{\\sqrt{\\bar{\\alpha}_t}},$"}, {"title": "C. Noise Conditioned Score Networks (NCSNS)", "content": "Unlike DDPMs and DDIMs which generate samples by predicting and removing noise, NCSNs take another approach by following the score function of the training data to generate samples. The score function of a data density $p(x)$ is defined as the gradient of the log probability density $\\nabla_x \\log p(x)$. Essentially, the score function indicates the direction in the data space that one needs to move in order to maximize the likelihood of the data x. The key idea of NCSNs bases on this property, demonstrated in Fig. 1e. Starting from an arbitrary point in the data space (i.e., a random noise), NCSNs are trained to iteratively follow the direction of the score function to move towards the high-density space that the data x inhabits, thereby generating a new sample when reaching a mode of the data distribution. This is referred to as Langevin dynamics, a method originated from physics. Specifically, each step of Langevin dynamics is computed as follows:\n$x_t = x_{t-1} + \\frac{\\gamma}{2} \\nabla_x \\log p(x) + \\sqrt{\\gamma}\\epsilon,$\nwhere $t \\in \\{0,1,...,T\\}$, $x_0$ is randomly sampled from a prior distribution, $\\gamma$ controls the scale of the update in each step, $\\epsilon \\sim N(0,I)$ is a noise that is added to make the generated samples more diverse and stochastic instead of deterministically collapsing onto a local minimum.\nSampling from (27) only requires the score function. Hence, a neural network parameterized by $\\theta$ can be trained to approximate the score function such that $s_\\theta(x) \\approx \\nabla_x \\log p(x)$. There are different techniques that can be used to train the neural network, including score matching [48], sliced score matching [88], and denoising score matching [49]. For instance, using score matching leads to the following training objective:\n$L_{sm} = E_p ||s_\\theta(x) - \\nabla \\log p(x)||^2.$\nHowever, there are several problems when training with this objective [6]. By taking the expectation over all examples $p(x)$, rarely-seen examples in low-density space would be dominated by the data in high-density regions. Consequently, score estimation becomes unreliable in low density regions. Another problem is about the manifold hypothesis. Assuming that the data is RGB images. The ambient space that consists of all possible RGB images of size H \u00d7 W is obviously very large (with $2^{H\\times W \\times 3 \\times 8}$ possible images), while the training data often lies on only a low dimensional manifold (e.g., car images). As a result, the sampling results would always converge to the low dimensional manifold, while other points outside of this manifold would have probability zero, making the log in the score function ill-defined.\nSong et al. [6] proposed solving these problems by adding a multi-scale Gaussian noise into the data. This noise could push the sampled points to be outside of the low dimensional manifold, solving the manifold hypothesis problem. Moreover, it also increases the regions covered by the modes of the training data, thus covering better low-density regions, mitigating the issue of low data density. Using the noise distribution $q_\\sigma(x|x) = N(x, \\sigma I)$, the score function can be computed as $\\nabla \\log q_\\sigma (x|x) = -\\frac{x-x}{\\sigma^2}$, where x is a noise-perturbed version of x, and $\\{\\sigma_t\\}_{t=1}^T$ is a sequence of decreasing noise levels. This results in the following objective:\n$E_{nc\\sigma_t^2} ||s_\\theta(x, \\sigma_t) + \\frac{x-x}{\\sigma_t^2}||^2,$\nwhere $\\lambda(\\cdot)$ is a weighting function, chosen as $\\lambda(\\sigma_t) = \\sigma_t^2$ [6]. After training the neural network $s_\\theta$, it can be used by an annealed version of Langevin dynamics to generate samples:\n$\\gamma_t \\frac{s_\\theta(x_{t-1},\\sigma_t)}{\\sigma_t^2} + \\sqrt{\\gamma_t}\\epsilon,$\nwhere $\\gamma_t = \\mu(\\frac{t}{\\tau})^2$ anneals/scales down the update over time. In practice, $\\mu$ is empirically selected between $5\\cdot10^{-6}$ and $5\\cdot10^{-5}$, while $\\sigma$ starts from $\\sigma_1 = 1$ to $\\sigma_T = 0.01$."}, {"title": "D. Score Stochastic Differential Equation (SDE)", "content": "While DDPMs, DDIMs, and NCSNs operate in a discrete-time scheme with T iterative steps, score SDE generalizes these models to a continuous diffusion process. As illustrated in Fig. 1d, the forward process uses an SDE to diffuse a data example into random noise. The reverse process employs a deep neural network to approximate the reverse-time SDE [50], thus generating samples based on a numerical SDE solver such as predictor-corrector samplers and probability flow ordinary differential equation (ODE). In particular, the SDE for the diffusion process is defined as follows:\n$dx = f(x,t)dt + g(t)dw,$\nwhere $t \\in [0,T]$ is a continuous time variable, f(.,t) is a function computing the drift coefficient of x(t), g(.) computes the diffusion coefficient, and w is the Brownian motion. The drift term represents the deterministic part of the SDE, responsible for data destruction, while the diffusion coefficient controls the scale of noise from the stochastic part $\\theta w$. On the other hand, the reverse-time SDE has the following form:\n$dx = [f(x,t) - g(t)^2\\nabla_x \\log p_t(x)]dt + g(t)dw,$\nwhere w is the Brownian motion when the time is reversed back from T to 0. Essentially, the reverse process starts from a random noise $x_T$, then reverses the time to $x_0$ to generate samples. From a denoising perspective, the term $g(t)^2\\nabla_x \\log p_t (x)$ can be viewed as the noise that one needs to remove to reconstruct clear samples. Similar to NCSNs, the score function $\\nabla_x \\log p_t (x)$ here can also be approximated by a deep neural network $s_\\theta(x,t)$ trained with denoising score matching, resulting in the following objective:\n$C_{sde} = E_{t,p_0,p_t}[x(t)||s_\\theta (x_t, t) - \\nabla_x \\log P_t (x_t|x_0)||^2],$\nwhere $x(t)$ is a weighting function, while the time variable t is sampled over the distribution $U([0, T])$. Besides denoising score matching [49], other methods such as sliced score matching [88] and finite-difference score matching [89] are also applicable in this case."}, {"title": "E. Multi-Modal Conditional Diffusion Models", "content": "When a DM is trained on a particular dataset, it can generate high-quality samples that lies on the space of the training data. However, the generating results are uncontrollable as it depends on the random noise $x_T$ and other stochastic factors like $\\epsilon_t$ during the sampling process. One way to mitigate this issue is to add a condition c at each transition step of DMs:\n$p(x_{0:T}/c) = p(x_T) \\prod_{t=1}^T P_\\theta (x_{t-1}|x_t, c).$"}, {"title": "F. Preliminary of Diffusion Model Attacks", "content": "1) Backdoor Attacks: In backdoor attacks [90], the attackers modify the training data and objective function to embed a backdoor trigger into machine learning (ML) models. During inference, if the trigger is activated as input, the models will produce abnormal results driven by the attackers. On the other hand, the models still behave normally on benign samples. This stealthy property of backdoor attacks makes it challenging to be recognized by average users and even detection systems. In the context of DMs, backdoor attacks cause DMs to generate a particular image (designated by the attackers) when the trigger is used; this image is called backdoor target.\nThe purposes of backdoor attack on DMs are diverse. For instance, since many reputable organizations are offering DMs as a service, attackers might inject poisoned data to the training process to backdoor their DMs with sensitive/violent contents as backdoor targets. Then, when the backdoored DMs are used widely by the public, the attackers could publish the backdoor triggers so that everyone can use the triggers to generate the designated sensitive contents, posing a severe threat to both the community and the organizations in terms of reputation and legality. On the other hand, attackers can train their own backdoored DMs, then publish them on open-source platforms like HuggingFace to harm users who download, use, or fine-tune the backdoored models. This risk is even more harmful for downstream applications built upon the backdoored DMs. In this case, a fault caused by the backdoored DMs can make the entire system collapses. Backdoored DMs can be used for advertisement, generating fake news, copyrights. For instance, attackers could manipulate the language model of DMs, making the DMs always generates images related to Starbucks when there is a trigger word \"coffee\" in the text prompt."}, {"title": "III. ATTACKS ON DIFFUSION MODELS", "content": "Although the capability of DMs is undeniable, their danger is also inestimable when they are manipulated or intentionally used for malicious purposes. This section surveys a variety of attacks targeted on DMs with in-depth discussion. For the ease of understanding, the data of DMs in this section is considered images if no information is supplemented (although DMs can be used for many applications other than computer vision).\nIn general, backdoor triggers can be stealthily embedded into DMs via the following main components: (i) denoising model, (ii) conditional model (e.g., language model), and (iii) personalization method. They are surveyed as follows."}, {"title": "1) Backdoor via Denoising Model:", "content": "This is the main category of backdoor attacks which aims to modify the forward and reverse processes such that the neural network would learn undesirable correlation between the backdoor trigger and the backdoor target. TrojDiff [62] is among the first studies that investigated this type of backdoor attacks on DMs. Essentially, while a benign diffusion process gradually adds noise into an image xo until it becomes a Gaussian noise $X_T \\sim N(0,I)$, TrojDiff's backdoored diffusion process diffuses to into, which a noisy version of the backdoor trigger:\n$x \u2248 (1 \u2013 \u03b3)x + \u03b3\u03b5,$\nwhere $\\epsilon \\sim N(0, I)$ is a Gaussian noise, $\\delta$ is the backdoor trigger, and $\u03b3 \\in [0, 1]$ is a \u201cblending\u201d coefficient determining how noisy the noisy trigger is. For instance, as illustrated in Fig 3, the backdoor trigger $\\delta$ of TrojDiff is a hello-kitty image $x$, while $x$ is a noisy version of such the hello-kitty image. In TrojDiff, the backdoored diffusion process is applied on only a particular image in the dataset (i.e., the backdoor target), while the process on other training images remains unchanged to retain the model's performance. By doing so, during inference, the backdoored DMs would generate the backdoor target if the model's input is the noisy hello-kitty image (i.e., the noisy trigger), while producing benign results if the input is a standard Gaussian noise. To do so, TrojDiff modifies the diffusion transition of DDPMs from (1) to the following form for backdooring DMs:\n$q(x_{t+1}|x_t) = N(x_{t+1}; \\sqrt{\\alpha_t}x_t+k_t(1 \u2212 \u03b3)\u03b4, (1 \u2212 \u03b1_t)\u03b3^2I),$"}, {"title": "C. Membership Inference Attacks", "content": "In this study, based on the capabilities of attackers, MIAs are divided into three categories: i) black-box attack, ii) gray-box attack, and iii) white-box attack, as described in Table (VI)\n1) Black-box MIA: In this scenario, Matsumoto et al. [116] directly adopt the full black-box attack of GAN-Leaks [103] for DDIM [5] and evaluate how vulnerable DMs are compared to deep convolutional GAN (DCGAN) [127]. Specifically, given a candidate image, GAN-Leaks [103] generates 12,000 images to form a generator's output space and then infers the membership based on the smallest reconstruction error. However, since DMs are more complex, this method causes a huge bottleneck in terms of computational resources and time when sampling such a large number of samples. Additionally, due to the difference in training and inference procedures, the results demonstrate that existing approaches designed for GAN are significantly less effective when applied to DMs."}, {"title": "IV. DEFENSES FOR DIFFUSION MODELS", "content": "It has been shown in section III that there are a variety of attacks threatening DMs. Thus, investigating possible countermeasures for these attacks is obviously an important topic.\nA. Countermeasures of Backdoor Attacks\nIn practice, it is especially challenging to detect backdoor attacks targeting on DMs due to its stealthiness: (i) Backdoored DMs still perform normally on benign inputs, and (ii) the input data space (e.g., image, text, audio) is usually too large for searching-based methods that try to identify the trigger. Furthermore, the processes of DMs are totally different from standard ML models like regression or classification models, making existing backdoor detection methods relying on labeled data [123], [146], [147] inapplicable for DMs.\nFortunately, the diffusion and reverse processes of DMs do have certain special properties to be exploited for detecting the embedded backdoor patterns [148], [149]. The distribution shift caused by the backdoor trigger can be observed clearly from the presented TrojDiff and BadDiffusion methods. In the diffusion process of each work, the data distribution is shifted gradually from the target image x towards the noisy trigger. For instance, the forward transition of TrojDiff presented in (36) can be reparameterized into the following form:\nx = \u221a(\u221a\u03b1\u03c4)\u03bat(1 \u2212 \u03b3)d + \u221a(1\nwhere \u03f5\u2208N(0,I), and \u03b4 is the backdoor trigger (i.e., the trigger distribution). As shown in Fig. 6, the \u201ctrigger shift\u201d term represents a small amount of distribution shift that guides the data distribution towards the trigger distribution \u03b4 (i.e., the hello-kitty image in Fig. 6) in each diffusion step. The scale/amount of this trigger shift is kt(1\u2212\u03b3). Concurrently, the \u201cnoise shift\u201d makes the data gradually resembles a Gaussian noise. The combination of both these shift terms results in the noisy trigger at the final diffusion step (i.e., the noisy hello-kitty image in Fig. 6). This observation is also true for BadDiffusion when we apply reparameterization on BadDiffusion\u2019s forward transition (40):\nx = \u221a(\u221a\u03b1\u03c4 + (1 \u2212 \u221a\u03b1\u03c4)x + \u221a(1 \u2013 \u03b1\u03c4"}, {"title": "V. OPEN CHALLENGES AND RESEARCH DIRECTIONS", "content": "As discussed earlier, various studies regarding attacks and defenses for DMs have been proposed and cover three main branches of DM security. However, it can be seen that this field is still in its infancy, since many other issues of both attacks and countermeasures for DMs have not been investigated. Thus, this section presents a throughout discussion about open challenges and future research directions of DM security, inspiring future development towards more secure and privacy-preserving DM-based systems."}, {"title": "A. Backdoor Attack", "content": "In backdoor attacks, the attackers must modify the diffusion processes, loss function, and training data to embed the triggers into the target DMs. As a result, the backdoor can be activated when the backdoor trigger is fed into the model's input. However, the modifications applied on the backdoored DM eventually led to a significant downgrade on the model's performance when non-trigger inputs are used. This not only impacts user experience, but also decreases the stealthiness of the backdoor attack if the generative results of the backdoored DM are compared to those generated by a benign model."}]}