{"title": "Improving Uncertainty-Error Correspondence in Deep Bayesian Medical Image Segmentation", "authors": ["Prerak MODY", "Nicolas F. CHAVES-DE-PLAZA", "Chinmay RAO", "Eleftheria ASTRENIDOU", "Mischa DE RIDDER", "Nienke HOEKSTRA", "Klaus HILDEBRANDT", "Marius STARING"], "abstract": "Increased usage of automated tools like deep learning in medical image segmentation has alleviated the bottleneck of manual contouring. This has shifted manual labour to quality assessment (QA) of automated contours which involves detecting errors and correcting them. A potential solution to semi-automated QA is to use deep Bayesian uncertainty to recommend potentially erroneous regions, thus reducing time spent on error detection. Previous work has investigated the correspondence between uncertainty and error, however, no work has been done on improving the \"utility\" of Bayesian uncertainty maps such that it is only present in inaccurate regions and not in the accurate ones. Our work trains the FlipOut model with the Accuracy-vs-Uncertainty (AvU) loss which promotes uncertainty to be present only in inaccurate regions. We apply this method on datasets of two radiotherapy body sites, c.f. head-and-neck CT and prostate MR scans. Uncertainty heatmaps (i.e. predictive entropy) are evaluated against voxel inaccuracies using Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves. Numerical results show that when compared to the Bayesian baseline the proposed method successfully suppresses uncertainty for accurate voxels, with similar presence of uncertainty for inaccurate voxels. Code to reproduce experiments is available at https://github.com/prerakmody/bayesuncertainty-error-correspondence.", "sections": [{"title": "1. Introduction", "content": "In recent years, deep learning models are being widely used in radiotherapy for the task of medical image segmentation. Although these models have been shown to accelerate clinical workflows (Zabel et al., 2021; Van Dijk et al., 2020), they still commit contouring errors (Brouwer et al., 2020). Thus, a thorough quality assessment (QA) needs to be conducted, which places a higher time and manpower requirement on clinical resources. This creates a barrier to the adoption of such deep learning models (Petragallo et al., 2022). Moreover, it also creates an obstacle for adaptive radiotherapy (ART) workflows, which have been shown to improve a patient's post-radiation quality-of-life (Grepl et al., 2023). This obstacle arises due to ART's need of regular contour updates. Currently, commercial auto-contouring tools do not have the ability to assist with quick identification and rectification of potentially erroneous predictions (Brouwer et al., 2020; Petragallo et al., 2022).\nQuality assessment (QA) of incorrect contours would require two steps \u2013 1) error detection and 2) error correction (Chaves-de-Plaza et al., 2022). Currently, errors are searched for by manual inspection and then rectified using existing contour editing tools. Error detection could be semi-automated by recommending either potentially erroneous slices of a 3D scan (Wang et al., 2020), or by highlighting portions of the predicted contours (Sander et al., 2020) or blobs (Nair et al., 2020). Upon detection of the erroneous region, the contours could be rectified using point or scribble-based techniques (Lei et al., 2019; Sambaturu et al., 2023) in a manner that adjacent slices are also updated. For this work, we will focus on error detection.\nVarious approaches to error detection have suggested using Bayesian Deep Learning (BDL) and the uncertainty that it can produce as a method to capture potential errors in the predicted segmentation masks (Wang et al., 2020; Sander et al., 2020; Nair et al., 2020; Garifullin et al., 2021; Bragman et al., 2018; Ng et al., 2022; Camarasa et al., 2021). Although such works established the potential usage of uncertainty in the QA of predictions, it may not be sufficient in a clinical workflow that relies on pixel-wise uncertainty as a proxy for error detection. In our experiments with deep Bayesian models, we observed that the relationship between prediction errors and uncertainty is sub-optimal, and hence has low clinical \"utility\". Ideally, for semi-automated contour QA, the uncertainty should be present only in inaccurate regions and not in the accurate ones. At times, literature usually refers to this as uncertainty calibration (Kumar et al., 2019; Krishnan and Tickoo, 2020; Zhang et al., 2020; Camarasa et al., 2021; Gruber and Buettner, 2022), but we find this term incorrect as historically, calibration is referred to in context of probabilities of a particular event (Dawid, 1982). Thus, we believe it is semantically incorrect to say uncertainty calibration and instead propose to use the term uncertainty-error correspondence.\nTo create a Bayesian model that is incentivized to produce uncertainty only in inaccurate regions, we use the Accuracy-vs-Uncertainty (AvU) metric (Mukhoti and Gal, 2018) and its probabilistic loss version (Krishnan and Tickoo, 2020) during training of a UNet-based Bayesian model (Wen et al., 2018). This loss promotes the presence of both accurate-if-certain (nac) as well as inaccurate-if-uncertain (niu) voxels in the final prediction (Figure 1). With uncertainty present only around potentially inaccurate regions, one can achieve improved synergy between clinical experts and their deep learning tools during the QA stage. Our work is the first to use the AvU loss in a dense prediction task like medical image"}, {"title": "2. Related Works", "content": ""}, {"title": "2.1 Epistemic and aleatoric uncertainty", "content": "Recent years have seen an increase in work that utilizes probabilistic modeling in deep medical image segmentation. The goal has been to account for uncertainty due to noise in the dataset (aleatoric uncertainty) as well as in the limitations of the predictive models learning capabilities (epistemic uncertainty). Noise in medical image segmentation refers to factors like inter- and intra- annotator contour variation (Brouwer et al., 2012; van der Veen et al., 2019) due to factors such as poor contrast in medical scans. Works investigating aleatoric uncertainty model the contour diversity in a dataset by either placing Gaussian noise assumptions on their output (Monteiro et al., 2020) or by assuming a latent space in the hidden layers and training on datasets containing multiple annotations per scan (Hu et al., 2019). A popular and easy-to-implement approach to model for aleatoric uncertainty is called test-time augmentation (TTA) (Wang et al., 2019a). Here, different transformations of the image are passed through a model, and the resulting outputs are combined to produce both an output and its associated uncertainty.\nIn contrast to aleatoric uncertainty, epistemic uncertainty could be used to identify scans (or parts of the scan) that are very different from the training dataset. Here, the model is unable to make a proper interpolation from its existing knowledge. Methods such as ensembling (Mehrtash et al., 2020) and Bayesian posterior inference (e.g., Monte-Carlo DropOut, Stochastic Variational Inference) (Sander et al., 2020; Nair et al., 2020; Wang et al., 2020; Garifullin et al., 2021; Bragman et al., 2018; Gibson et al., 2022; Krishnan and Tickoo, 2020) are common methods to model epistemic uncertainty in neural nets. While Bayesian modeling is a more mathematically motivated and hence, principled approach to estimating uncertainty, ensembles have been motivated by the empirically-proven concept"}, {"title": "2.2 Uncertainty use during training", "content": "Other works also use the uncertainty from a base segmentation network to automatically refine its output using a follow-up network. This refinement network can be graphical (Soberanis-Mukul et al., 2020) or simply convolutional (Sander et al., 2020). Uncertainty can also be used in an active learning scenario, either with (Diaz-Pinto et al., 2022) or without (Iwamoto et al., 2021) interactive refinement. Shape-based features of uncertainty maps have also been shown to identify false positive predictions (Bhat et al., 2022). Similarly, we too use uncertainty in our training regime, but with the goal of promoting uncertainty only in those regions which are inaccurate, an objective not previously explored in medical image segmentation."}, {"title": "2.3 Model calibration", "content": "In context of segmentation, model calibration error is inversely proportional to the alignment of a models output probabilities with its pixel-wise accuracy. Currently there is no proof that reduction in model calibration error leads to improved uncertainty-error correspondence. However, a weak link can be assumed since both are derived from a models output probabilities. It is well known that the probabilities of deterministic models trained on the cross entropy (CE) loss are not well calibrated (Guo et al., 2017). This means that they are overconfident on incorrect predictions and hence fail silently, which is an undesirable trait in context of segmentation QA and needs to be resolved.\nTo abate this overconfidence issue, methods such as post-training model calibration (or temperature scaling) (Guo et al., 2017; Ding et al., 2021; Ouyang et al., 2022), ensembles (Mehrtash et al., 2020; Ovadia et al., 2019), calibration-focused training losses (Pereyra et al., 2017; Mukhoti et al., 2020; Murugesan et al., 2023b,a) and calibration-focused training targets (M\u00fcller et al., 2019; Islam and Glocker, 2021) have been shown to improve model calibration for deterministic models. Temperature scaling, a post-training model calibration technique, has been shown to perform poorly in out-of-domain (OOD) settings (Ovadia et al., 2019), relies wholly on an additional validation dataset and/or needs explicit shape priors (Ouyang et al., 2022).Local temperature scaling techniques have been proposed that calibrate on the image or pixel level (Ding et al., 2021), however they are still conceptually similar to the base method and are hence plagued by the same concerns."}, {"title": "3. Methods", "content": ""}, {"title": "3.1 Neural Architecture", "content": "We adopt the OrganNet2.5D neural net architecture (Chen et al., 2021) which is a standard encoder-decoder model connected by four middle layers. It contains both 2D and 3D convolutions in the encoder and decoder as well as hybrid dilated convolutions (HDC) in the middle. This network performs fewer pooling steps to avoid losing image resolution and instead uses HDC to expand the receptive field. To obtain uncertainty corresponding to the output, we add stochasticity to the deterministic convolutional operations by replacing them with Bayesian convolutions (Blundell et al., 2015; Wen et al., 2018). We experiment with replacing deterministic layers in both the HDC as well as the decoder layers to understand the effect of placement.\nIn a Bayesian model, a prior distribution is placed upon the weights and is then updated to a posterior distribution on the basis of the training data. During inference (Equation 1), we sample from this posterior distribution p(W|D) to estimate the output distribution p(y|x, D) with x, y and W being the input, output and neural weight respectively:\n$$p(y|x, D) = E_{W~p(W|D)} [p(y|x, W)].$$\nThis work uses a Bayesian posterior estimation technique called stochastic variational inference, where instead of finding the true, albeit intractable posterior, it finds a distribution close to it. We chose FlipOut-based (Wen et al., 2018) convolutions which assume the distribution over the neural weights to be a Gaussian and are factorizable over each hidden layer. Pure variational approaches would need to sample from this distribution for each element of the mini-batch (Blundell et al., 2015). However, the FlipOut technique only samples once and multiplies that random sample with a Rademacher matrix, making the forward pass less computationally expensive."}, {"title": "3.2 Training Objectives", "content": "In this section, we use a notation format, where capital letters denote arrays while non-capital letters denote scalar values."}, {"title": "3.2.1 SEGMENTATION OBJECTIVE", "content": "Upon being provided a 3D scan as input, our segmentation model predicts for each class c\u2208 C, a 3D probability map P of the same size. Each voxel i \u2208 \u03a9 has a predicted probability vector Pi containing values $p_i^c$ for each class that sum to 1 (due to softmax). To calculate the predicted class of each voxel $y^i$, we do:\n$$y^i = \\underset{c \\in C}{\\text{argmax}}\\, p_i^c.$$\nTo generate a training signal, the predicted probability vector $p_i$ is compared to the corresponding one-hot vector $Y^i$ in the gold standard 3D annotation mask. $Y^i$ is composed of $y_i^c$ \u2208 {0,1}. Inspired by (Taghanaki et al., 2019; Yeung et al., 2022), we re-frame the"}, {"title": "3.2.2 UNCERTAINTY OBJECTIVE", "content": "In a Bayesian model, multiple forward passes (m \u2208 M) are performed and the output 3D probability maps $(P^c)^m$ of each pass are averaged to output $P_i^c$ (Equation 1). Using $P_i^c$, we can calculate a host of statistical measures like entropy, mutual information and variance. We chose entropy as it has been shown to capture both epistemic uncertainty, which we explicitly model in FlipOut layers, as well as aleatoric uncertainty, which is implicitly modeled due to training data (Gal, 2016). We use the predicted class probability vector Pi for each voxel and calculate its (normalized) entropy $u^i$:\n$$u^i = -\\frac{1}{ln(C)} \\sum_{c \\in C} p_i^c ln(p_i^c).$$\nSince we have access to the gold standard annotation mask, each voxel has two properties: accuracy and uncertainty. Accuracy is determined by comparing the gold standard class $y^i$ to the predicted class $\\hat{y}^i$. We use this to classify them in four different categories represented by $n_{ac}$, $n_{au}$, $n_{ic}$ and $n_{iu}$, where n stands for the total voxel count and a, i, u, c represent the accurate, inaccurate, uncertain and certain voxels. A visual representation of these terms can be seen in Figure 1. Here, a voxel is determined to be certain or uncertain on the basis of a chosen uncertainty threshold t \u2208 T where the maximum value in T is the maximum theoretical uncertainty threshold (Mody et al., 2022a). The aforementioned four terms are the building blocks of the Accuracy-vs-Uncertainty (AvU) metric (Mukhoti and Gal, 2018) as shown in Equation 5 - Equation 7 and it has a range between [0,1]. A higher value indicates that uncertainty is present less in accurate regions and more in inaccurate regions, thus improving the \"utility\u201d of uncertainty as a proxy for error detection.\n$$AvU^t = \\frac{n_{act} + n_{iut}}{n_{act} + n_{aut} + n_{ict} + n_{iut}}$$\n$$n_{ac}^t = \\sum_{\\substack{i \\\\\\ y_i=\\hat{y_i} \\& \\\\ u_i < t}} 1,$$ $$n_{au}^t = \\sum_{\\substack{i \\\\\\ y_i=\\hat{y_i} \\& \\\\ u_i < t}} 1.$$\n$$n_{ic}^t = \\sum_{\\substack{i \\\\\\ y_i \\neq \\hat{y_i} \\& \\\\ u_i < t}} 1,$$ $$n_{iu}^t = \\sum_{\\substack{i \\\\\\ y_i \\neq \\hat{y_i} \\& \\\\ u_i < t}} 1.$$\nTo maximize AvU for a neural net, one can turn it into a loss metric to be minimized. As done in Krishnan and Tickoo (2020) for an image classification setting, we minimize"}, {"title": "3.3 Evaluation", "content": ""}, {"title": "3.3.1 DISCRIMINATIVE AND CALIBRATION EVALUATION", "content": "We evaluate all models on the DICE coefficient for discriminative performance. Calibration is evaluated using the Expected Calibration Error (ECE) (Guo et al., 2017). Numerical results are compared with the Wilcoxon signed-ranked test where a p-value < 0.05 is considered significant."}, {"title": "3.3.2 UNCERTAINTY EVALUATION", "content": "As the model is trained on the Accuracy-vs-Uncertainty (AvU) metric, we calculate the AvU scores up to the maximum normalized uncertainty of the validation dataset. A curve with the AvU score on the y-axis and the uncertainty threshold on the x-axis is made and the area-under-the-curve (AUC) for each scan is calculated. AUC scores provide us with a"}, {"title": "4. Experiments and Results", "content": ""}, {"title": "4.1 Datasets", "content": ""}, {"title": "4.1.1 HEAD-AND-NECK CT", "content": "Our first dataset contained Head and Neck CT scans of patients from the RTOG 0522 clinical trial (Ang et al., 2014). The annotated data, which had been collected from the MICCAI2015 Head and Neck Segmentation challenge, contained 33 CT scans for training, 5 for validation and 10 for testing (Raudaschl et al., 2017). We further expanded the test dataset with annotations of 8 patients belonging to the RTOG trial from the DeepMindT-CIA dataset (DTCIA) (Nikolov et al., 2021). This dataset included annotations for the mandible, parotid glands, submandibular glands and brainstem. Although there were annotations present for the optic organs, we ignored them for this analysis as they are smaller compared to other organs and require special architectural design choices. Since the train and test patients came from the same study, we considered this as an in-distribution dataset. We also tested our models on the STRUCTSeg (50 scans) dataset (Ye et al., 2022), hereby shortened as STRSeg. While the RTOG dataset contained American patients, the STRSeg dataset was made up of Chinese patients and hence considered out-of-distribution (OOD) in context of the training data. The uncertainties of this dataset were evaluated to a value of 0.4 since that is the maximum empirical normalized entropy."}, {"title": "4.1.2 PROSTATE MR", "content": "Our second dataset contained MR scans of the prostate for which we use the ProstateX repository (Meyer et al., 2021) containing 66 scans as the training dataset. The Medical Decathlon (Prostate) dataset with 34 scans (Antonelli et al., 2022) and the PROMISE12 repository with 50 scans (Litjens et al., 2014) served as our test dataset. The Medical Decathlon dataset (abbreviated as PrMedDec henceforth) contained scans from the same clinic as the ProstateX training dataset. We combined the Peripheral Zone (PZ) and Transition Zone (TZ) from the MedDec dataset into 1 segmentation mask. The PROMISE12 dataset (abbreviated as PR12) was chosen for testing since literature (Mehrtash et al., 2020) has shown lower performance on it and hence it serves as a good candidate to evaluate the utility of uncertainty. This dataset is different from ProstateX due to the usage of an endo-rectal coil in many of its scans as well as the presence of gas pockets in the rectum and dark shadows due to the usage of older MR machines. Thus, although these datasets contained scans of the prostate region, there exists a substantial difference in their visual textures. The maximum empirical normalized entropy of this 2-class dataset is 1.0 and hence the uncertainty-error correspondence metrics were calculated till this value."}, {"title": "4.2 Experimental Settings", "content": "We tested the Accuracy-vs-Uncertainty (AvU) loss on four datasets containing scans of different modalities and body sites. We trained 11 models: Det (deterministic), Det+AvU,\nEnsemble, Focal, LS (Label Smoothing), SVLS (Spatially Varying Label Smoothing), MbLS\n(Margin based Label Smoothing), ECP (Explicit Confidence Penalty), TTA (Test-Time\nAugmentation), Bayes and Bayes + AvU. As the names suggest, Bayes and Bayes + AvU\nare Bayesian versions of the deterministic OrganNet2.5D model (Chen et al., 2021). The\nbaseline Bayes model contained Bayesian convolutions in its middle layers and was trained\nusing only the cross-entropy (CE) loss. The Bayes + AvU was trained using both the CE\nand Accuracy-vs-Uncertainty (AvU) loss. Two additional Bayesian models were trained\nwhich tests if the placement of the Bayesian layers had any effect: BayesH and BayesH +\nAvU. Here, BayesH refers to the Bayesian model with Bayesian layers in the head of the\nmodel (i.e the decoder). Results for these models can be found in Appendix E.\nThe Ensemble was made of M = 5 deterministic models with different initializations\n(Ovadia et al., 2019). For TTA, we applied Gaussian noise and random pixel removals for\nM = 5 times each and then averaged their outputs. The hyperparameters of the other\nmodels were chosen on the basis of the best discriminative, calibrative and uncertainty-\nerror correspondence metrics on the validation datasets (C). For the calibration focused\nmethods we used the following range of hyperparameters: Focal (\u03b3 = 1,2,3), MbLS (m =\n8,10,20,30) for head-and-neck CT, MbLS (m = 3,5,8,10) for prostate MR, LS (\u03b1 =\n0.1, 0.05, 0.01), SVLS (\u03b3 = 1,2,3) and ECP (\u03bb = 0.1,1.0, 10.0, 100.0) for head-and-neck\nCT and ECP (\u03bb = 0.1,1.0, 10.0, 100.0, 1000.0) for prostate MR. For the AvU loss, we\nevaluated weighting factors in the range [10,100,1000,10000] for the head-and-neck dataset,\nand [100,1000,10000] for the Prostate dataset.\nWe trained our models for 1000 epochs using the Adam optimizer with a fixed learning\nrate of 10-3. The deterministic model contained \u2248 550K parameters and thus the Ensemble\ncontained \u2248 2.75M parameters. Since the Bayesian models double the parameter count in"}, {"title": "4.3 Results", "content": "In Section 4.3.1 and Section 4.3.2 we show discriminative (DICE), calibrative (ECE) and uncertainty-error correspondence metrics (ROC-AUC, PRC-AUC) for the two datasets."}, {"title": "4.3.1 HEAD-AND-NECK CT", "content": "Results in Table 1 showed that the AvU loss on the Bayes model significantly improved calibrative and uncertainty-error correspondence (unc-err) metrics for both in-distribution (ID) and out-of-distribution (OOD) datasets. The Bayes+AvU model also always per-"}, {"title": "4.3.2 PROSTATE MR", "content": "Similar to the head-and-neck CT dataset, the use of the AvU loss on the baseline Bayes model significantly improved its uncertainty-error correspondence (unc-err) while maintaining calibration performance (Table 2). Moreover, it improved the DICE values such that its one of the most competitive amongst all models. Also, the Bayes+AvU had better performance in both unc-err and calibrative metrics when compared to the Det, calibration-focused and TTA models. When comparing to the Ensemble, the Bayes+AvU had similar DICE. While Bayes+AvU had better calibrative and unc-err performance in the in-distribution (ID) dataset, the Ensemble performed better in the out-of-distribution (OOD) setting. The AvU loss had no positive effect on the DICE and unc-err performance of the Det model in both the ID and OOD setting, however there was an increase in ECE.\nVisual results show that the Bayes+AvU successfully suppresses uncertainty in the true negative (Case 1 in Figure 3a, Case 2 in Figure 3b) and true positive (Case 2 in Figure 3a) regions of the predicted contour. It also shows uncertainty in the false positive regions (Case 2 in Figure 3a, Case 1/3 in Figure 3b)"}, {"title": "5. Discussion", "content": "Although medical image segmentation using deep learning can now predict high quality contours which can be considered clinically acceptable, a manual quality assessment (QA) step is still required in a clinical setting. To truly make these models an integral part of clinical workflows, we need them to be able to express their uncertainty and for those uncertainties to be useful in a QA setting. To this end, we test 11 models which are either Bayesian, deterministic, calibration-focused or ensembled."}, {"title": "5.1 Discriminative and Calibrative Performance", "content": "In context of DICE and ECE, the use of the AvU loss on the baseline Bayes model always showed results which have never statistically deteriorated. Moreover, the DICE results for the in-distribution (ID) head-and-neck dataset (RTOG) were on-par with existing state-of-the-art models (83.6 vs 84.7 for Nikolov et al. (2021)). The same held for the ID Prostate dataset (PRMedDec) where results were better than advanced models (84.9 vs 83.0 for Antonelli et al. (2022)). These results validate the use of our neural architecture (Chen et al., 2021), and training strategy.\nSecondly, although the Ensemble model, in general, had better or equivalent DICE and ECE scores across all 4 datasets, it also required 3x more parameters than the Bayes+AvU model. Also, as expected, and due to 5x more parameters, the Ensemble model performed better than the Det model for DICE and ECE.\nFinally, in the regime of segmentation \"failures\" as the inaccuracy map, the calibrative methods did not generally have improved calibration performance when compared to the Det model. In theory, these models regularize the model's probabilities by making it more"}, {"title": "5.2 Uncertainty-Error Correspondence Performance", "content": "Although calibrative metrics are useful to compare the average truthfulness of a model's probabilities, they may not be relevant to real-world usage in a pixel-wise segmentation QA scenario. Considering a clinical workflow in which uncertainty can be used as a proxy for error-detection, we evaluate the correspondence between them. Results showed that across both in- and out-of-distribution datasets, the Bayes+AvU model has one of the highest uncertainty-error correspondence metrics. Similar trends were observed for the BayesH+AvU (Appendix E) model, however Bayes+AvU was better. We hypothesize"}, {"title": "5.3 Future Work", "content": "In a radiotherapy setting, the goal is to maximize radiation to tumorous regions and minimize it for healthy organs. This goal is often not optimally achieved due to imperfect contours caused by time constraints and amorphous region-of-interest boundaries on medi- cal scans. Thus, an extension of our work could evaluate the contouring corrections made by clinicians in response to uncertainty-proposed errors in context of the dose changes to the different regions of interest. Such an experiment can better evaluate the clinical utility of an uncertainty-driven error correction workflow."}, {"title": "6. Conclusion", "content": "This work investigates the usage of the Accuracy-vs-Uncertainty (AvU) metric to improve clinical \"utility\" of deep Bayesian uncertainty as a proxy for error detection in segmenta- tion settings. Experimental results indicate that using a differentiable AvU metric as an objective to train Bayesian segmentation models has a positive effect on uncertainty-error correspondence metrics. We show that our AvU-trained Bayesian models have equivalent or improved uncertainty-error correspondence metrics when compared to various calibrative and uncertainty-based methods. Given that our approach is a loss function, it can be used with other neural architectures capable of estimating uncertainty.\nGiven that deep learning models have shown the capability of reaching near expert-level performance in medical image segmentation, one of the next steps in their evolution is evaluating their clinical utility. Our work shows progress on this using a uncertainty-driven loss in a Bayesian setting. We do this for two radiotherapy body-sites and modalities as well in an out-of-distribution setting. Our hope is that the community is inspired by our positive results to further contribute to human-centric approaches to deep learning-based modeling."}, {"title": "Appendices", "content": ""}, {"title": "A. Segmentation \u201cFailures\u201d and \u201cErrors\u201d", "content": ""}, {"title": "B. Weightage of AvU loss", "content": "The table below show the weights used for the AvU loss which were finetuned on the validation datasets of the head-and-neck CT and prostate MR. The final weightage was chosen by identifying the inflection point at which the ROC-AUC and PRC-AUC drop precipitously. Given that the AvU loss is a log term, its values are inherently small (\u2264 1.0). This is then added to the cross-entropy term, which is a sum of logs (Eqn (3)) over all the voxels (=N) and all the classes (=C). Thus, we used a balancing term in the range of 10\u00b9 to 104."}, {"title": "C. Hyperparameter selection", "content": "In the tables shown below, we report results for different hyperparameters of different model classes. If the DICE of a hyperparameter is 10.0 points lower than the class maximum, we ignore it. We also ignore models with large drops in ECE or AvU-AUC when compared to models in its own class. To choose the best hyperparameter, it has to perform as the best in four out of the five metrics, else we chose the middlemost hyperparameter."}, {"title": "D. Visual Results", "content": "Visual results in Figure 2 and Figure 3 show pairs of consecutive CT/MR slices to better understand the 3D nature of the output uncertainty across all models. We show examples with both high and low DICE to investigate the presence and absence of uncertainty in different regions of the model prediction."}, {"title": "D.1 Head-And-Neck CT", "content": "The first two rows of Figure 2a and Figure 2b show the mandible (i.e. lower jaw bone) with only the Bayes+AvU model having overall low uncertainty in accurate regions and high uncertainty in (or close to) inaccurate regions.\nIn the next set of rows for head-and-necks CTs, we observe the parotid gland, a salivary organ, with (Figure 2a - Case 2) and without (Figure 2b - Case 2, Case 3) a dental scattering issue. In both cases, while the Det model shows low uncertainty, the baseline Bayes model shows high uncertainty in accurate regions. Usage of the AvU loss lowers uncertainty in these regions, while still exhibiting uncertainty in the erroneous regions, for e.g. the medial (i.e. internal) portion of the organ in Figure 2a (Case 2).\nMoving on to our last case, we see the submandibular gland, another salivary gland in Figure 2a (Case 3). The Ensemble, Focal, SVLS and MBLS models all display high uncertainty in the core of the organ, which are also accurately predicted. On the other hand, the AvU loss minimizes the uncertainty and shows uncertainty in the erroneous region on the second slice."}, {"title": "D.2 Prostate MR", "content": "For the prostate datasets, we see two cases with high DICE in Figure 3a (Case 1) and Figure 3b (Case 2) where the use of the AvU loss reduces uncertainty for the baseline Bayes model.\nWe also see cases with low DICE in Figure 3a (Case 2) and Figure 3b (Case 1). Due to their low DICE all models display high uncertainty, but the Bayes+AvU model shows high overlap between its uncertain and erroneous regions. The same is also observed in Figure 3b (Case 3).\nFinally, in Figure 3a (Case 3), we do not see any clear benefit of using the AvU loss on the Bayes model."}, {"title": "E. BayesH model", "content": ""}]}