{"title": "Federated Impression for Learning with Distributed Heterogeneous Data", "authors": ["Atrin Arya", "Sana Ayromlou", "Armin Saadat", "Purang Abolmaesumi", "Xiaoxiao Li"], "abstract": "Standard deep learning-based classification approaches may not always be practical in real-world clinical applications, as they require a centralized collection of all samples. Federated learning (FL) provides a paradigm that can learn from distributed datasets across clients without requiring them to share data, which can help mitigate privacy and data ownership issues. In FL, sub-optimal convergence caused by data heterogeneity is common among data from different health centers due to the variety in data collection protocols and patient demographics across centers. Through experimentation in this study, we show that data heterogeneity leads to the phenomenon of catastrophic forgetting during local training. We propose FedImpres which alleviates catastrophic forgetting by restoring synthetic data that represents the global information as federated impression. To achieve this, we distill the global model resulting from each communication round. Subsequently, we use the synthetic data alongside the local data to enhance the generalization of local training. Extensive experiments show that the proposed method achieves state-of-the-art performance on both the BloodMNIST and Retina datasets, which contain label imbalance and domain shift, with an improvement in classification accuracy of up to 20%. The code is available at https://github.com/Atrin78/FedImpress.", "sections": [{"title": "1 Introduction", "content": "Deep learning models are widely utilized in medical imaging owing to their promising outcomes. However, these models are typically designed for centralized environments where all data are stored in a single database. Despite its benefits, centralizing data can be impractical for training purposes, i.e., healthcare facilities are generally hesitant to disclose their patients' information due to issues of data privacy, transmission costs, and access rights [20]. Federated Learning(FL) presents a promising alternative, enabling multiple hospitals to leverage distributed data without sharing it. In each iteration, local models are initialized with the distributed server model. They are then trained on local data and send back their updates to the server for aggregation. However, conventional FL methods such as FedAvg [18] encounter performance degradation, when applied to non-IID (heterogeneous) data [10,14].\nHeterogeneity happens due to 1) label imbalance i.e., various disease populations in different medical centers, and 2) domain shift, i.e., various data acquisition settings in medical devices. Studies have been carried out to mitigate each of the mentioned heterogeneities independently. However, based on our experiments in Fig. 1, we show that both of these cases lead to a common issue called catastrophic forgetting, which has been usually overlooked in previous works. In FL, catastrophic forgetting [5] occurs when a model overwrites past aggregated knowledge with local data. As shown in Fig. 1, when observing a specific client during local training, the local model's accuracy on the other local datasets degrades since the server model's past aggregated knowledge is overwritten by the local heterogeneous data. In this work, we focus on solving the catastrophic forgetting issue in FL caused by label imbalance and domain shift.\nRecent efforts in FL literature have mainly concentrated on improving local training on client side [9,13,16,30]; and refining aggregation on the server side [15,25,29,17]. Notably, client side enhancements have been reported to achieve better outcomes [13]. To improve client side training, two main categories of methodologies have been investigated: 1) model-level approaches, which refine model optimization strategies through techniques such as setting a prior on model weights [13] or gradient update corrections [22,10]; and 2) data-level methods which aim to alleviate statistical heterogeneity among local data across clients by employing techniques like sharing statistical information [7,21] or synthetic data generation [24,30]. Among them, model-level studies such as [22] and data-level studies such as [26] have directly tackled the issue of catastrophic forgetting in FL. In terms of addressing catastrophic forgetting, data-level approaches exhibit superior model agnosticity, which is advantageous in deep learning [5]. However, the generation of synthetic images with high fidelity that preserves the server model's information remains a persistent challenge.\nIn this paper, we propose a data-level approach, FedImpres, to mitigate catastrophic forgetting, caused by heterogeneous data in FL setting. To achieve this, after server aggregation in each FL iteration, we generate high-quality prototypical synthetic images by back-propagating on the server model's aggregated weights as a federated impression of global data. Furthermore, we add a model gradient-based constraint to this optimization to ensure that the synthesized data globally fits the entire latent distribution of the server model. We share the synthesized data with clients and perform weighted training on both local and synthesized data on the client-side. We have chosen to use FedAvg as the base method for aggregating the local models on the server-side for the sake of simplicity. However, it is important to note that our approach is also compatible with other model aggregation strategies."}, {"title": "2 Method", "content": ""}, {"title": "2.1 Problem setting", "content": "The general FL setting aims to collaboratively train over a group of clients {C1, C2, ..., CN} and their respective local datasets, with N being the number of clients. The objective is to maintain high classification accuracy across all clients. Let $(x_n^i, y_n^i) \\in X_n$ represent an input image and its corresponding class label drawn from client n's dataset. We denote the weights of feature extractors as $\\theta$ and that of classifiers as $\\phi$. In this setting, our goal is to have a model on the server that performs well on all clients by minimizing the following objective:\n$J(\\theta_G, \\phi_G) = \\sum_{n=1}^N E_{(x_n^i,y_n^i)\\in x_n} l(g(f(x_n^i; \\theta_G); \\phi_G), y_n^i), \\quad(1)$\nwith loss function l which is cross-entropy (CE) loss, LCE, in our case, client number n, server model's feature extractor $f(; \\theta_G)$ and its classifier $g(; \\phi_G)$. Note that the local data cannot be shared due to privacy concerns. As a result, in each round r, we train models {f(; $\\theta_1^r$), ..., f (; $\\theta_N^r$)} initialized by f(;$\\theta_G^r$) using their respective client's local dataset, and share their weights {$\\theta_1^r$,...,$\\theta_N^r$} with the server model to aggregate them into $\\theta_G^{r+1}$. A common strategy for aggregation is [18] simply averaging the weights of clients, which we will follow in our study."}, {"title": "2.2 Overview", "content": "As described in the introduction (Sec. 1), catastrophic forgetting during local training is one of the primary problems in heterogeneous FL. To develop a robustFL algorithm suitable for heterogeneous data, we need to address two fundamental challenges: 1) How to alleviate catastrophic forgetting in local training? This can be achieved by utilizing a united synthetic data as a regularizer in local client training to penalize catastrophic forgetting; 2) How to generate this synthetic dataset? We can synthesize data using the server model to capture a genuine federated impression for local training. The overall paradigm of our method is shown in Fig. 2. In the following sections, we will provide a detailed description of our proposed paradigm."}, {"title": "2.3 Federated Impression", "content": "Past methods like VHL [24] have proposed to use global synthetic data to improve FL on heterogeneous data. However, VHL's synthetic data does not preserve the server model's information useful for the targeted classification task during local training. Inspired by [2], to empower the global synthetic data to assist FL, we introduce an adaptive global data generation paradigm, which synthesizes data based on the server model in each communication round. Next, we aim to have not only high-fidelity data but also the information-preserving property, i.e., training a model from scratch using synthesized data results in a model that performs similarly to the original server model. To obtain data withthis characteristic, we optimize pixel values on the image space $V_1, V_2, ..., v_s \\in \\nu$ CE loss of the server model. Additionally, to achieve the information-preserving property following [28], we add an equality constraint to the optimization process to ensure that the gradient of the server model's CE loss on V with respect to its weights $\\theta$ is close to 0. Specifically, we aim to solve\n$\\min_\\nu \\sum_{(v_i,\\hat{y}_i) \\in V} L_{CE}(g(f(v_i; \\theta_G); \\phi_G), \\hat{y}_i) s.t. \\nabla_{\\theta, \\phi} L_{CE} = 0, \\quad(2)$\nwhere $\\hat{y}$ is initialized with the prediction of the server model when given $v_i$. Since optimizing Eq. 2 is computationally expensive, according to [28], we solve the relaxed version of the optimization problem imposing the equality constraint on $\\phi$ only\n$\\min_\\nu \\sum_{(v_i,\\hat{y}_i) \\in V} L_{CE}(g(f(v_i; \\theta_G); \\phi_G), \\hat{y}_i) s.t. \\nabla_{\\phi} L_{CE} = 0, \\quad(3)$\nIt's worth noting that such a relaxation does not steer us away from our ultimate goal of information-preserving property. Instead of generating precise images with this property, we aim to produce images whose latent representation would capture the exact global distribution of the server in the latent space. Next, we solve it using the augmented lagrangian formulation:\n$\\max_\\lambda \\min_y L_{FedImpres}\n\\sum_{(v_i,\\hat{y}_i) \\in V} [L_{CE}(g(f(v_i; \\theta_G); \\phi_G), \\hat{y}_i) + tr(\\Lambda^T \\nabla_{\\phi} L_{CE}) + \\frac{\\rho}{2} ||\\nabla_{\\phi} L_{CE}||^2], \\quad(4)$\nwhere $\\Lambda$ is the lagrangian dual variable matrix for the equality constraint in Eq. (2) and $\\rho$ is the penalty hyperparameter. According to [28], we solve it approximately using an alternating direction method of multipliers (ADMM) [4]. After synthesizing this data as the federated impression, we pass it to all clients for local training. Note that we don't need any additional private data information to generate the synthetic dataset compared to general FL methods like [18]."}, {"title": "2.4 Forgetting-Penalized Local Training", "content": "To train the local model for client n, we receive an optimized synthetic dataset V from the server at the beginning of each local training round. To prevent catastrophic forgetting during local training, we train the model on synthetic data in addition to the local data using the following\n$\\min_{(\\theta_n, \\phi_n)} L_{local} (\\theta_n^r, \\phi_n^r) + \\beta L_{global} (\\theta_n^r, \\phi_n^r); \\quad(5)$\nwhere $L_{local}$ and $L_{global}$ are CE loss over each client's local data and shared global data, respectively. Here, $L_{global}$ basically used as a regularization term forimproving the generalizability of local training over captured federated impression in the previous step. This approach preserves information from the server model due to the information-preserving property of the synthetic data. Note that as opposed to [28], we use the CE loss directly on the synthesized data to enforce the information-preserving property. It is also worth noting that merely replacing the global loss with another regularization that instead aims to decrease the distance between the local model's and the server model's weights directly, as done in [22], may not be optimal since it would limit the ability to capture local information."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Datasets", "content": "We use two public medical image datasets to evaluate FedImpres on two typical heterogeneous settings for classification: label imbalance and domain shift:\nBloodMNIST [1] is one of the datasets in the standard medical imaging benchmark, MedMNIST [27]. We chose this dataset over other modalities as it contains adequate classes (eight in total), which can better demonstrate FedImpres on imbalanced labels settings. The images in this dataset are padded to size 32 \u00d7 32.\nRetina dataset [3,6,19,23] consists of retina images of size 256 \u00d7 256 gathered from four different sites, resulting in label imbalance and domain shift. We aim to solve the binary classification problem to detect Glaucomatous images from normal ones for this dataset. Samples and label distribution of both datasets for each client are provided in the supplementary material."}, {"title": "3.2 Experimental Settings", "content": "We conducted experiments to study label imbalance and domain shift among FL clients. For each experiment, we used three different alternatives of initialization for the synthesis step of FedImpres, i.e., random noise, public natural images (CIFAR-10 [12]), and a public unlabeled medical dataset in a similar domain of local private data, which will be explained for each dataset separately. Note that obtaining unlabeled data from the same modality used for synthesis initialization is not a problem in the real world.\nData Heterogeneity: To simulate class imbalance, we used BloodMNIST. To replicate unlabeled medical data for synthesis initialization, we randomly selected 10% of the data that were mutually exclusive from all of the training data. Afterwards, we utilized Latent Dirichlet Analysis (LDA) [8,25] to divide the remaining data into eight clients for an eight-way classification. We set the partition parameter of LDA ($\\alpha$) to 0.01 and 0.005 to create moderate and severe imbalanced datasets. Subsequently, in a more practical evaluation, we carried out experiments on the Retina dataset, which encompasses data from four distinct domains with different demographic distributions and are naturally class-imbalanced. We employed data from one of the four sites as publicly accessibleunlabeled data for synthesis initialization and performed binary classification on the remaining three datasets.\nImplementation Details: We used a simple Convolutional Neural Network (CNN) for classification in all settings. The architecture is detailed in the supplementary. All models were implemented with PyTorch and trained on one NVIDIA Tesla V100 GPU with 16 GB of memory. Our implementation contains two stages of optimization in each communication round. 1) We freeze model weights for the image synthesis stage and use the SGD optimizer and optimize the batch of [16,32] images for 5 ADMM epochs in BloodMNIST and Retina, respectively. 2) In local model training, we update local model weights again with the SGD optimizer. We fixed the total training epochs for 400 iterations and performed our experiments in two different settings. We reported our results for 80 and 40 communication rounds with local update epochs (E) of 5 and 10, warmed up with 15 and 10 rounds of FedAvg, respectively. Hyperparameters are detailed in the Supplementary."}, {"title": "3.3 Comparison with Baselines", "content": "We compared our results with common and state-of-the-art (SOTA) FL algorithms. Among common methods, we choose FedAvg [18] and FedProx [13] as two main baselines. FedProx solves performance degradation compared to FedAvg in the Non-IID setting by adding a regularization term for local training, which prevents divergence of local model weights from the server model. We also compare with SOTA FL methods that share similar ideas with ours by adding global synthetic data or editing local training. VHL [24], which generates global virtual data using untrained StyleGAN [11] and does not update global virtualdata during training. We also compare our results with FedVSS [30], which adversarially modifies local data using the server model to synthesize more general data for each client. Finally, we compare our results to SOTA methods FedCurv [22] and FedReg [26] that focus on tackling the issue of catastrophic forgetting in FL.\nThe results are illustrated in Table 1. Although medical initialization has the best results, we show that even with CIFAR-10 and noise initialization, we outperform SOTA in most experiments, and this proves the effectiveness of the synthesis step regardless of the initialization. In all of the experiments FedImpres improves FedAvg by a large margin. This can be particularly observed when the level of heterogeneity is higher with $\\alpha$ = 0.005 and the Retina dataset. Although FedProx was designed to have smoother local training by adding a penalty for divergence from the server model, this is harmful to severe heterogeneity due to a shortage in learning local data. Compared to VHL and FedVSS, we surpass them by virtue of our adaptive and unified synthesis data approach among clients, correspondingly. Although, FedCurv achieves close results to our method on Retina dataset, its performance degrades when facing label shift on the BloodMnist dataset. FedReg does not perform well on both datasets since it's not designed for architectures with batch normalization."}, {"title": "3.4 Ablation Studies", "content": "To assess the effect of our data synthesis algorithm, we consider another synthetic data generation variant adopted by our proposed method and study its performance on the Retina dataset, as it is a real-life dataset and has both label imbalance and domain shift. For this, we omit the constraint of globalizing data synthesized to distribution seized by the server model in Eq. (2) and optimize only with CE loss. For both methods, we use random noise to initialize data synthesis to omit any initialization bias. As shown in Table 2, the proposed FedImpres approach surpasses its other variant, showing the effectiveness of its data synthesis algorithm for data generation."}, {"title": "4 Conclusion", "content": "Previous FL approaches suffer from catastrophic forgetting in their local training due to the heterogeneity of the distributed data. This problem becomes morepronounced for clients dealing with medical data due to the heterogeneity caused by both domain shift and label imbalance across clients. To this end, we proposed a novel method called FedImpres, which uses the server model to generate synthetic data at each round to account for the server model's information in the local training and avoid forgetting. We demonstrated how this method could achieve superior performance for two benchmark medical datasets, particularly in highly heterogeneous cases. Moreover, the ablation section showed the data synthesis algorithm's effectiveness. It is worth noting the synthetic data-restoring method is efficient without training additional generative models. Furthermore, our proposed method shows the potential to be applied in many healthcare applications using data from multiple centers. We will explore integrating our research with other practical applications in the medical domain. This may involve testing our approach on various medical datasets and improving the pipeline to meet the preferred standards of clinical practice."}]}