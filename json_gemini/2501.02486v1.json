{"title": "LLMPC: Large Language Model Predictive Control", "authors": ["Gabriel Maher"], "abstract": "Recent advancements in prompting techniques for Large Language Models (LLMs) have improved their reasoning, planning, and action abilities. This paper examines these prompting techniques through the lens of model predictive control (MPC). We show that LLMs act as implicit planning cost function minimizers when planning prompts are used. Under our framework we demonstrate that LLM planning performance can be improved further by incorporating real planning cost functions and evaluators.", "sections": [{"title": "Introduction", "content": "A growing body of literature has shown that the performance of Large Language Models (LLMs) can be improved by guiding their output with structured prompts. Asking LLMs to reason about their actions and to plan is particularly effective as shown by chain-of-thought prompting [Wei et al., 2022], self-consistency [Wang et al., 2022], the ReAct framework [Yao et al., 2022] and agentic applications of LLMs Hong et al. [2023], Qin et al. [2023].\n\nThe structured prompting trend raises several questions however. Why does structured prompting improve the performance of LLMs? What are the limits of enhancement with structured prompting? How can the methods be extended to further improve performance? Answering these questions requires a framework to understand the consequences of different prompting methods.\n\nThe viewpoint of LLMs as planning and acting naturally suggests a control theoretic approach to modeling this problem. In particular model predictive control (MPC) is a framework in which action plans are generated and then executed by a controller. For instance, the Tree-of-Thoughts method [Yao et al., 2023] explores branching reasoning paths and then selects the most promising trajectory, closely mirroring the search and pruning steps in model-based planning. Similarly, the Reflexion technique [Shinn et al., 2023] and iterative prompting methods can be viewed as repeatedly updating model states and predictions over a receding horizon. Finally Monte Carlo Tree Search (MTCS) style algorithms have been used together with trained value and reward functions to improve the planning and reasoning abilities of LLMs Wang et al. [2024], Jiang et al. [2024]."}, {"title": "Model Predictive Control", "content": "In the model predictive control setting an agent must navigate a state space $s_t \\in S \\subset \\mathbb{R}^d$. The agent decides on an action $a_t \\in A \\subset \\mathbb{R}^m$. The state is then updated according to a state transition model $s_{t+1} = f (s_t, a_t, \\varepsilon_t)$, where $\\varepsilon_t$ is a noise or disturbance term.\n\nThe goal is to find a sequence of actions $a_t, a_{t+1},...,a_{t+H}$ over a planning horizon of $H$ steps that minimizes an objective function $C (S_t, a_t, a_{t+1}, . . ., a_{t+H})$. That is\n\n$\\hat{a_t} ... \\hat{a_{t+H}} g(s_t) := \\underset{a_t, a_{t+1},...,a_{t+H}}{arg \\min} C (s_t, ..., s_{t+H}, a_t, ...,a_{t+H})$\n\nThe objective function typically involves a task-specific cost and a regularization cost. The task-specific cost is often a measure of the distance between the current state and a desired state. The regularization cost is often a measure of the complexity of the sequence of actions."}, {"title": "LLM as Planner", "content": "An LLM is a neural network function that operates on a length L sequence of input tokens $q_t \\in Q^L := \\{1,2,..., M\\}^L$ to produce a probability vector over the possible M tokens for the next token in the sequence. The LLM input is typically referred to as a \u2018prompt' and 'prompting' is the process through which relevant state information from $s_t$ is embedded into the prompt. In the control scenario the previous prompt $q_{t-1}$ and the current state $s_t$ create the current prompt $q_t$, thus prompting can be represented as a function $P : S \\times Q^L \\rightarrow Q^L$. Calling the LLM produces a probability distribution for the next token. Various methods are used for sampling the actual next token value. For example, Beam Search is a common approach where several likely token sequences are sampled by selecting the top most likely tokens at each step and using these to sample subsequent tokens with repeated pruning to remove unlikely sequences. Repeatedly evaluating the LLM with the new output tokens pro- duces a sequence of output tokens $\\hat{Y} \\in \\{1, 2, ..., M\\}^T$. We can represent the process of output token sequence generation with LLMs as a function $F : \\{1, 2, . . ., M\\}^L \\rightarrow \\{1, 2, ..., M\\}^T$.\n\nIn the LLM-as-planner scenario, the token sequence is mapped to the sequence of actions $a_t,..., a_{t+H}$ for the controller. That is there is a mapping function $\\phi : \\{1, 2, . . ., M\\}^T \\rightarrow A^H$. We now see that the LLM-as-planner approximately minimizes the cost function through\n\n$\\hat{a_t} \\hat{a_{t+H}} = \\phi(\\hat{Y}) := \\phi(F(q_t)) := \\phi(F(P(S_t, q_{t-1}))) \\approx g (s_t)$"}, {"title": "Analysis of LLM Planning Methods", "content": "The different approaches to LLM planners such as ReAct or ToT, thus differ mostly in their modeling of the prompting and action mapping functions $P$ and $\\phi$.\n\nWith the ReAct framework a prompting function is used where the LLM is given the current state and then may perform a number of reasoning and action steps before proposing a final action. The action mapping function then maps the final action the LLM proposed to the action space. The reasoning and acting steps other than the final proposal are 'hidden' in the sense they do not update the state and can thus be considered part of the prompting function.\n\nIn ToT the LLM is instructed to sample multiple control trajectories and select the best trajectory. In the trajectory sampling there is no actual evaluation of the state update function, the LLM outputs the state updates as well as the actions. Finally the LLM also approximates the cost function in that it selects the best trajectory. Thus ToT is a particular form of a prompting function $P$ coupled with an action mapping function $\\phi$ that selects the trajectory the LLM proposed as optimal.\n\nAgentic applications of LLMs apply a form of prompting whereby the LLM is instructed to play different roles [Hong et al., 2023, Qin et al., 2023]. While agents may have the appearance of using multiple models or controllers, in reality a single LLM is used and it is the input prompt that changes. In Agents the prompting function can be seen as making use of an LLM to sample the agent prompts. The action mapping function selects the action proposed by the agent that is responsible for control of the state."}, {"title": "Improving LLM Control Performance with LLMPC", "content": "We see that most LLM planners differ primarily in their choice of prompting function. The connection we have made between LLM planning and MPC provides inspiration for other ways to improve LLM planners. In the MPC framework, the main limitation of LLM planning performance is due to the limits of LLM sampling as approximately minimizing the MPC objective. Thus by making better use of the objective function we can improve the performance of the LLM planners.\n\nWe propose to incorporate the cost function by using the LLM to sample a number of control sequences. The cost function and state update function are then used to evaluate the samples and the best sample is selected as the control sequence to apply.\n\nThat is we sample a number of plans $A := \\{a_t,..., a_{t+H}\\}$ from the LLM. For each plan we simulate the system to obtain the states $S_i := \\{s_t,..., s_{t+H}\\}$. We then pick the plan that minimizes the objective function among the sampled plans\n\n$A_t := \\underset{i=1,...,K}{arg \\min} C (S_i, A_i)$\n\nWe then execute up to H steps from the plan $\\hat{A}_t$ and then replan with the new state information. We note that while we use the LLM to sample plans directly, they could also be sampled one action at a time while using beam-search or other algorithms to construct the final plan samples."}, {"title": "Experiments", "content": "A key observation is that for our method we need access to the objective function $C$ and state update function $f$. In many applications these are readily available or approximate models can be substituted."}, {"title": "Control of Spring and Mass System", "content": "Here we compare LLMPC against MPC on the problem of applying force to a spring and mass system to arrive at a particular goal state. The equations of motion for the system are\n\n$\\frac{A_t}{m} (u - k(x \u2013 x_0 - l))$,\n\n$v_{t+1} = v_t + dt a_t$,\n\n$x_{t+1} = x_t + dt v_t$.\n\nHere u is a force applied by the controller to control the spring. The objective is to bring the spring to the goal state $x^*$ with zero velocity. The objective function is\n\n$C_t = Q_x (x_{t+H} - x^*)^2 + Q_v (v_{t+H} \u2013 v^*)^2 + \\sum_{k=t}^{t+H} Q_u u_k^2$\n\nFor both MPC and LLMPC we set H = 3 and execute 2 of every 3 steps from the returned action sequence. For MPC we use CVXPY to solve the planning problem. For LLMPC we used GPT-40-mini and asked the LLM to suggest 5 plans at every step using a templated prompt (listing 1). Each of the suggested action sequences is evaluated using (4), and the plan with the lowest objective value is selected.\n\nWe solved the control problem with both MPC and LLMPC with $x_0 = 1, x^* = 2, m = 1$, and $k = 5$ (Fig. 1). Both MPC and LLMPC produce control sequences that control the spring to the goal state. As expected, the objective values from the plans produced by LLMPC are higher than when solving the problem exactly with MPC, highlighting that LLMs are approximate planners."}, {"title": "Code Generation", "content": "Here we compare LLMPC to a one-step code generation model. Both models use GPT-40 as the LLM. We ask the models to generate code for the popular game \u2018Flappy Bird' using Javascript, HTML and CSS. For the one-step model, simply asking the LLM to produce the code without any other instructions results in incomplete code. Therefore we use a ReAct style prompt (Listing 2) that allows the LLM to perform multiple thinking, designing and prototyping steps before producing the final output.\n\nFor the LLMPC prompts we use a shared system prompt, with separate instructions for the planner and for the action mapper (Listing 3). We use tools to set up the action mapper prompt, the prompt includes instructions on creating files, and appending to and modifying files. Our code then interprets the tool calls to perform file system operations. We use H = 3 and ask the planner to plan the next 3 steps at every iteration.\n\nBoth models produce complete and functioning code. However we see that the LLMPC code contains more details such as including sprites, a game over screen and buttons to restart the game. Some flaws remain such as the orientation of the pipes on the top part of the screen and missing a background sprite, but overall the game is complete. This highlights how asking the LLM to plan over longer horizons can allow it to tackle more complex tasks."}, {"title": "Related Works", "content": "In the context of robotics and embodied agents, approaches such as SayCan [Ahn et al., 2022] and Voyager [Wang et al., 2023] use LLMs to generate action plans in complex environments. This line of work demonstrates that LLMs can serve as zero-shot planners, guiding agents"}, {"title": "Code Generation", "content": ""}]}