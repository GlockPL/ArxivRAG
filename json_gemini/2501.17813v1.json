{"title": "P-TAME: Explain Any Image Classifier with Trained Perturbations", "authors": ["Mariano V. Ntrougkas", "Vasileios Mezaris", "Ioannis Patras"], "abstract": "The adoption of Deep Neural Networks (DNNs) in critical\nfields where predictions need to be accompanied by jus-\ntifications is hindered by their inherent black-box nature.\nIn this paper, we introduce P-TAME (Perturbation-based\nTrainable Attention Mechanism for Explanations), a model-\nagnostic method for explaining DNN-based image classi-\nfiers. P-TAME employs an auxiliary image classifier to\nextract features from the input image, bypassing the need\nto tailor the explanation method to the internal architec-\nture of the backbone classifier being explained. Unlike\ntraditional perturbation-based methods, which have high\ncomputational requirements, P-TAME offers an efficient\nalternative by generating high-resolution explanations in a\nsingle forward pass during inference. We apply P-TAME to\nexplain the decisions of VGG-16, ResNet-50, and ViT-B-16,\nthree distinct and widely used image classifiers. Quantita-\ntive and qualitative results show that our method matches\nor outperforms previous explainability methods, including\nmodel-specific approaches. Code and trained models will\nbe released upon acceptance.", "sections": [{"title": "1. Introduction", "content": "Advances in deep neural networks (DNNs) over the past\ndecade have been tremendous. However, a persistent chal-\nlenge is the lack of DNN explainability [15]. DNNs are\noften referred to as \"black-box\" models because they do\nnot provide users with insights into their decision-making\nprocess, and this poses a significant barrier to their wider\nadoption in many important application domains such as\nhealthcare, journalism and law enforcement, where the\nability to justify decisions is a critical requirement [23, 27].\nConsequently, there is a growing interest in developing\nmethods to make the decisions of DNNs more understand-\nable to users, i.e., in developing eXplainable Artificial Intel-\nligence (XAI) methods [15]. Within this research domain,\na dominant direction to advancing the explainability of\nDNN image classifiers is to generate saliency maps [28],\nwhich highlight the regions of the input image that are most\nrelevant to the decision of the DNN. Saliency maps (a.k.a.\nexplanation maps; Fig. 1) can help users understand why\na DNN made a particular decision and can also be used\nto identify potential biases in the decision-making process\n[14].\nSeveral classes of methods have been proposed to gen-\nerate saliency maps for DNNs, including gradient-based\n[5, 31], perturbation-based [8, 24, 39] and response-based\n[20, 21, 29, 35, 41] methods. Gradient-based methods\ncompute the gradient of the output with respect to the input\nimage and use it to generate the saliency map. They suffer\nfrom the vanishing gradient problem and can be noisy and\nunreliable [1]. Additionally, the most widely used methods\nin this category, Grad-CAM, and Grad-CAM++ [5, 31]\nrequire the extraction of intermediate feature maps from the\nnetwork being explained, thus needing to be adapted to the\nDNN architecture of interest. Perturbation-based methods\ngenerate saliency maps by perturbing the input image and\nobserving the change in the output. They are more robust\nand reliable than gradient-based methods but are compu-\ntationally expensive at the inference stage. Furthermore,\nthey are not always model-agnostic, e.g. the widely used\nScore-CAM [39] relies on extracting intermediate feature\nmaps (similarly to Grad-CAM, Grad-CAM++). Response-\nbased methods, e.g. CAM [41], generate saliency maps\nby combining the intermediate feature maps of the DNN"}, {"title": "2. Related work", "content": "Humans have long explained and justified their actions, a\ncore aspect of how they relate, cooperate, and build trust"}, {"title": "3. P-TAME", "content": "The process of yielding explanations for the predictions of\nimage classifiers with P-TAME involves two main steps.\nThe first step is to train an attention mechanism that gen-\nerates explanation maps from feature maps. In contrast to\nT-TAME, feature maps are never directly extracted from\nthe backbone network (the DNN whose decisions should\nbe explained); instead, they are produced by an auxiliary\nclassifier (whose weights are also frozen). Thus, the P-\nTAME method is model-agnostic: only the input images\nand the backbone's output predictions are required. The\nsecond step involves using the trained attention mechanism\nto directly produce class-specific explanations for the back-\nbone's predictions."}, {"title": "3.2. Definitions", "content": "Consider an image classifier network (a.k.a. backbone)\n$f: \\mathcal{X} \\rightarrow \\mathbb{R}^C$ that maps an input image $x \\in \\mathcal{X}$ to a vector\nof logits $y = (y)_x = f(x) \\in \\mathbb{R}^C$, where $\\mathcal{X}$ is the space\nof images and $C$ is the number of classes. We denote the\n$c$-th element of $y$ as $y_c$. Let $c^* = \\arg \\max y$ be the model-\ntruth class, i.e., the prediction of the model, which can be\ncontrasted with a ground-truth class provided by a labeled\ndataset. Additionally, consider an auxiliary image classifier\nnetwork $f_{aux}: \\mathcal{X} \\rightarrow \\mathbb{R}^C$. The auxiliary classifier $f_{aux}$\nis constrained to only CNN-based architectures, because\nthey produce three-dimensional feature maps. We denote\nthe feature map extracted from layer $l$ of the auxiliary\nclassifier as $F_l \\in \\mathbb{R}^{d_l \\times w_l \\times h_l}$. Here, $d_l$, $w_l$, and $h_l$ are\nthe number of channels, height, and width of the feature\nmap, respectively. The attention mechanism of P-TAME\ntakes as input multiple feature maps from different layers\nof the auxiliary classifier, to improve the resolution of the\nproduced explanation maps, based on the findings of [21].\nLet $\\mathcal{A}(F_\\mathcal{L}) = E$ be the attention mechanism, where $F_\\mathcal{L}$ the\nset of feature maps extracted from $\\mathcal{L} = \\{l_1, l_2, ..., l_s\\}$ lay-\ners, and $E \\in [0,1]^{C \\times W_E \\times h_E}$ the class-specific explanation\nmaps. Finally, we denote by $R = W_E h_E$ the resolution of\nthe explanation maps."}, {"title": "3.3. Auxiliary classifier and attention mechanism", "content": "The auxiliary classifier, a CNN pretrained on the same\ndataset as the backbone (e.g. ResNet-18 [10], see Sec-\ntion 4.1 for experimentation details), extracts features that\nfollow a predictable pattern: deeper layers capture seman-\ntically rich features, while earlier layers detect simple pat-\nterns or edges [16]. These features are three-dimensional,\nspatially consistent with the input image, and straight-\nforward to process. The P-TAME attention mechanism\ncombines feature maps from various layers of the auxiliary\nclassifier, which differ in channel count and spatial resolu-\ntion. Using these feature maps, it generates explanations\nthat highlight the most salient input regions according to\nthe backbone. This adaptation involves processing each\nfeature map individually and combining them to produce\nclass-specific explanation maps, as illustrated in Fig. 3a.\nFeature maps $F_l$ from different layers of the auxil-\niary classifier are processed individually through a fea-\nture branch comprising a $1 \\times 1$ convolution layer, batch\nnormalization, a skip connection, an activation function,\nand bilinear interpolation (Fig. 3b). Bilinear interpolation\nupscales smaller feature maps to match the resolution of\nthe largest feature map. While feature maps extracted\nfrom deeper layers typically have lower resolutions, some\narchitectures produce feature maps of equal resolution (e.g.\narchitectures using inverted residual blocks), making bilin-"}, {"title": "3.4. Training regime", "content": "The attention mechanism we defined has to be trained to\ncorrectly combine the input feature maps into meaningful\nclass-specific explanation maps. The auxiliary classifier's\nweights are frozen, thus only the attention mechanism's\nweights need to be trained. This is done in a self-supervised\nmanner, similarly to T-TAME (Fig. 2). Specifically, images\nfrom the dataset used to train the backbone $f$ are input\nto both $f$ and the components of P-TAME: the auxiliary\nclassifier $f_{aux}$ and the attention mechanism $\\mathcal{A}$. During\ntraining, to measure how salient the explanations produced\nby P-TAME for the training image $x$ are, we first select the\nexplanation $E_{c^*}$ corresponding to the model truth class $c^*$,\nand use it to mask the image:\n$x_m \\coloneqq x \\odot up_{bilinear}(E_{c^*}),$"}, {"title": "3.5. Inference", "content": "During inference, only one forward pass is required to\ncompute explanation maps, as illustrated in Fig. 2. The\nimage is input to the backbone classifier to generate a\nprediction and to the auxiliary classifier to extract feature\nmaps. Then, the feature maps are processed by the trained\nattention mechanism to generate class-specific explanation\nmaps."}, {"title": "4. Experiments", "content": "We perform a comprehensive evaluation of P-TAME by\ncomparing it both quantitatively and qualitatively against\nSoA explainability methods across 3 backbone image clas-\nsifiers: VGG-16 [32], ResNet-50 [10], and ViT-B-16 [9].\nFor measuring explanation quality, we adopt evaluation\nmeasures that are widely used in the domain. We also report\nthe resolution of the produced explanation maps before\nrescaling, and measure the computational requirements of\ndifferent explainability methods by reporting the number of\nforward passes required to produce an explanation. Further-\nmore, we perform an ablation study examining the effects of\ndifferent choices of auxiliary classifiers both quantitatively\nand qualitatively. In the latter ablation, we compare be-\ntween three lightweight image classifiers: ResNet-18 [10],\nMobileNetV3 [11] and MnasNet [36]. Besides assessing\ndifferences in the explanation quality, we also compare\nthe computation requirements imposed by each auxiliary\nclassifier (measured in GFLOPs) and we quantify how the\nfeatures extracted from each different layer of the auxiliary\nclassifiers contribute to the final explanation maps."}, {"title": "4.2. Quantitative results and comparisons", "content": "In Table 1, our proposed P-TAME method is compared\nwith the following SoA methods: Grad-CAM [31], Grad-\nCAM++ [5], RISE [24], Score-CAM [39], Ablation-CAM\n[8] and T-TAME [21]. We selected these specific meth-\nods because they are among the most widely used and\nperformant methods of their respective class (gradient-,\nperturbation- and response-based approaches). From the re-\nsults, we observe that for the ViT-B-16 backbone, we obtain\ntop performance in the AD and IC measures, except for the\n$\\upsilon = 100\\%$ threshold, which is dominated by Opti-CAM\nacross different backbones. However, Opti-CAM exhibits\nthe worst performance in the more challenging AD(15%),\nIC(15%) and ROAD measures. For the CNN models VGG-\n16 and ResNet-50, we obtain near-top performance for\nthe AD and IC measures, competing in performance only\nwith T-TAME and the model-agnostic perturbation method\nRISE. In the MoRF and LeRF measures, which signal if\nthe ordering of pixels by importance is correct, P-TAME\nprovides mixed results. This is mostly caused by the fact\nthat the explanation maps produced by P-TAME have a\nmuch higher resolution, and providing a good ordering of\n$R$ pixels is much simpler for lower resolutions. This is\nfurther elucidated in Section 4.4. Still, the fact that P-\nTAME generates explanation maps in a single forward step\nand can be applied to any image classifier architecture is\na significant advantage compared to more computationally\nintense methods such as RISE, or more restrictive feature\nmap extraction methods such as Grad-CAM and T-TAME."}, {"title": "4.3. Ablations", "content": "In Table 2 we examine different auxiliary classifiers for\nexplaining the ResNet-50 backbone, comparing our choice\nof ResNet-18 with MobileNetV3 [11] and MnasNet [36]\n(again, models pretrained on ImageNet, retrieved from\n37]). We observe that smaller auxiliary classifiers of-\nfer computational advantages but produce coarser expla-"}, {"title": "4.4. Qualitative results", "content": "In Fig. 4, explanation maps produced for the ResNet-\n50 backbone using P-TAME and the SoA methods of\nTable 1 are shown, following the findings of [6] on the\nimportance of complementing quantitative evaluation with\nqualitative analysis. We select the ResNet-50 backbone for\nthis qualitative comparison because it is one of the most\nwidely used CNN architectures, and most of the compared\nexplainability methods were developed for CNNs. We ob-\nserve that P-TAME produces the most activated explanation\nmaps, followed by T-TAME and RISE. P-TAME correctly\nhighlights the entire class, when it can be localized (rows\n1, 4, 5). In cases where the class cannot be localized,\nP-TAME correctly highlights salient features, in line with\nmethods that directly make use of features extracted from\nthe backbone. Along with the good quantitative results in\nTable 1, this shows that P-TAME produces high-quality\nexplanation maps in a single forward pass without requir-\ning any backbone architecture-specific tailoring to extract\nand process feature maps. The only other model-agnostic\nmethod, RISE, besides requiring 8000 forward passes to\nproduce the shown explanation maps, produces much more\nnoisy results, especially in cases where the class is not\neasily localizable (rows 2, 3, 5).\nIn Fig. 5 we compare explanation maps produced for\nour three backbones (VGG-16, ResNet-50, and ViT-B-16)\nusing P-TAME. For the first image, illustrating a localizable\nclass, the explanation maps are similar across backbones.\nHowever, for the second image, whose class cannot be\neasily localized to a specific region of the image, the ViT-\nB-16 backbone, the most performant model out of the three\nin terms of classification performance (see 1st column of\nTable 1), shows the highest level of detail in its explanation.\nE.g., the number \"29\" in the second image is shown to\nhave low importance for the model-truth prediction. For\nless performant models, like VGG-16, the explanations\nshow much less detail, even though the resolution of the\nexplanation map is the same as for ViT-B-16. This indicates\na performance-explainability trade-off, i.e., that a higher-\nperforming classifier can support the generation of more\ndetailed explanations for it."}, {"title": "5. Conclusions", "content": "This paper presented P-TAME, a method for explaining\nDNN image classifiers by training an attention mechanism\nto combine feature maps produced by an auxiliary classifier\ninto explanation maps, highlighting the important regions\nfor the backbone model's prediction. P-TAME improves\nupon the paradigm established by T-TAME, extending it\nby decoupling the input of the attention mechanism re-\nsponsible for producing explanations from the intermediate\nfeature maps of the backbone being explained. This makes\nP-TAME a model-agnostic method, rendering it much more\nwidely applicable. P-TAME produces explanation maps\nin a single forward pass during inference, while producing\nexplanations that are on par with or better than those of the\nSoA explainability approaches. An exciting future direction\nis to investigate finetuning the auxiliary classifier used in P-\nTAME, to better tailor it to the backbone being explained."}]}