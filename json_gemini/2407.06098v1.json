{"title": "Epistemological Bias As a Means for the Automated Detection of Injustices in Text", "authors": ["Kenya Andrews", "Lamogha Chiazor"], "abstract": "Injustice occurs when someone experiences unfair treatment or their rights are violated and is often due to the presence of implicit biases and prejudice such as stereotypes. The automated identification of injustice in text has received little attention, due in part to the fact that underlying implicit biases or stereotypes are rarely explicitly stated and that instances often occur unconsciously due to the pervasive nature of prejudice in society. Here, we describe a novel framework that combines the use of a fine-tuned BERT-based bias detection model, two stereotype detection models, and a lexicon-based approach to show that epistemological biases (i.e., words, which presupposes, entails, asserts, hedges, or boosts text to erode or assert a person's capacity as a knower) can assist with the automatic detection of injustice in text. The news media has many instances of injustice (i.e. discriminatory narratives), thus it is our use case here. We conduct and discuss an empirical qualitative research study which shows how the framework can be applied to detect injustices, even at higher volumes of data.", "sections": [{"title": "Introduction", "content": "The most basic duty of the media is knowledge sharing. Yet, the tool necessary for wide-spread knowledge sharing is influence. With this influence, the media is able to shape how one will understand the intricacy of a story-line in a news story with little effort. This often results in the use of epistemological biases which involves propositions that are presupposed, entailed, asserted, hedged, or boosted in text (Recasens et al., 2013) to erode or assert a person's capacity as a knower, leading to framing issues and injustice within the text. Particularly, some of these word choices in text can lead to injustices occurring. These injustices can affirm or perpetuate stereotypes concerning the subject. Though it has always been a harsh reality with various ringing consequences, in recent years we have publicly witnessed how the affirmation of stereotypes can lead to physical violence, prejudice, and negative self-image (Harrison and Esqueda, 1999; Gover et al., 2020; Kuykendall, 1989). These experiences are harmful and dangerous, explicitly for the victims but for all members of our society.\nTherefore, we seek to detect instances of testimonial injustice, what we define as character injustice, and framing injustice. Testimonial injustice occurs when word choices cast modified believability on a statement due to prejudices (e.g. stereotypes) about the sayer (Fricker, 2007). Character assassination is \"the deliberate destruction of an individual's reputation or credibility\" (Icks et al., 2019). This often leads to character injustice, which is an unjustified attack on a person's character that results in an unfair criticism or inaccurate representation of them (e.g. exaggeration or defamation). According to (Entman, 2007) framing bias happens when the use of subjective, one-sided words that reveals the stance of an author occurs. Which means that an individual's choice from a set of options is influenced more by how the information is worded, rather than by the information itself. We recognise that news content can be positively or negatively framed to influence the narratives. In this paper, we aim to show how word choices which negatively frame a statement - opposed to speaking about it factually or neutrally, when it affects particular subjects or individuals, can lead to framing injustice (e.g. vilification).\nWith this work, we seek to make room for subjects of text, even in creative writing, to not have their credibility shot or character assumed due to well-known stereotypes which are harmful and unfounded. It can be noted here that toxic or hate speech is often blatant and highly recognisable, here we focus on implicit biases which are less obvious but are just as harmful. We pursue this, acknowledging it is achievable to speak about any happening in a factual way that does not seek to cause harm on the subject of the text.\nOur proposed framework will help with the detection of testimonial, character, and framing injustices. The framework includes a fine-tuned BERT model based on work from (Pryzant et al., 2020b) to automatically tag words associated with epistemological bias from an input text, use the (Kwon and Gopalan, 2021) CO-STAR model and (Sap et al., 2020) Social Bias Frames (SBF) to generate potential associated stereotypes and the concepts of those stereotypes to the input text, and show when the tagged words (associated with some epistemological bias) or less credibility of a person are correlated with a stereotype which causes injustice. Though we could use examples from various fields (e.g. politics, marketing, medicine, etc...), we will use news media as a use-case throughout this work. Thus, we present the following contributions: (1) We develop a novel framework that uses the results of 3 models to automatically detect character, testimonial, and framing injustices in News Media, (2) We produce a fine-tuned tagger model to automatically detect epistemological bias, (3) We develop an Interactive User-Interface for journalists and editors to submit text to and receive output and explanations surrounding the tagged word, and (4) We produce empirical evidence showing how epistemological bias can translate to injustices.\nAn outcome of this work is to give journalists and editors a tool which will help them easily and quickly notice and avoid testimonial, character, and framing injustices in their work. This will be accomplished by showing users which words they use that produce epistemological bias, the potential stereotype associated with the tagged words and text, offering the user explainability with the help of the stereotype concepts as defined by (Kwon and Gopalan, 2021), and resources to reference literature on the particular epistemological bias type(s) identified in their input text."}, {"title": "Background", "content": "Many works have established it is difficult for the common person to identify a biased word in a sentence and establish the need for computational agents to take on this charge. Section 2.1, 4.1, and 6.1 and Table 8 of (Pryzant et al., 2020b) shows humans have low ability to detect bias and show humans perform worse than their detection model. Recasens et al. (2013) shows in Table 4 of their paper - that the accuracy of Humans annotators on AMT (amazon mechanical turk) was not more than 37.39% for a single detected biased word. The difficulty arises due to us holding our own biases as facts and lack of education on sentence construction. It has also been observed that when human, expert annotators follow guides that help them detect bias, can more easily find instances of such biases. For example, the editors of the Wiki Neutrality Corpus had to follow the Netural Point of View (NPOV) Guidelines 1.\nGreat efforts have been put towards identifying potential words in text materials that could encourage epistemological bias (Recasens et al., 2013; Hube and Fetahu, 2019; Pryzant et al., 2020b). Following on from the work of Pryzant et al. (2020b), we fine-tuned their tagger model to automatically tag words associated with epistemological bias from an input text. Identifying words which cause epistemological bias is a step towards awareness of social harms. This begs the question, what do we do with our new found knowledge and awareness? What kinds of implications does our use of these words impact society? What communities are affected by these word choices? These are the questions we explore in this work.\nAuthors Kwon and Gopalan (2021) have trained a model to detect widely-known stereotypes and the concept of those stereotypes in text materials. We leverage the results of their CO-STAR model and the SBF model (Sap et al., 2020) to offer some explainability of the word choices by the model. Associating a particular text with a stereotype and the concept of that stereotype is a critical step towards awareness of social harms that might cause character or testimonial injustice to a particular individual or group. Lack of identifying the words in a sentence which imply and promote these stereotypes leaves us with the undirected burden and question of: how can we address these harms? This will be further discussed in the methods section of the paper.\nBeach et al. (2021) identify words in text that cause testimonial injustice in medical records of Black patients. Detecting such testimonial injustice is helpful in seeing the unjust realities of our society. They conclude the testimonial injustice that persists in these medical records has a high potential of causing disparity in the quality of health care for Black patients, which correlates with findings that"}, {"title": "Methodology", "content": "Hamborg et al. (2019) concluded that various forms of media bias is already analysed in the Social Sciences field and can be implemented in an automated fashion by primarily using Natural Language Processing (NLP) techniques. We leverage research methods using NLP and deep learning whilst also using analysis concepts by researchers from social sciences. Thus, we propose a novel, technical framework (Figure 1) which includes using NLP models for detecting potential epistemological biased words and potential stereotypes along with their concepts, which we semantically link to given sentence(s). From the literature, we found very few models which consider linguistic, epistemological features, or common sense reasoning to detect implicit biases or reasoning about potential stereotypes. This in turn influenced our decision to use three baseline models with promising results (i.e. Tagger Model, Co-Star, and SBF).\nFigure 5, shows the steps required to scale the framework to analyse multiple headlines."}, {"title": "Tagger model", "content": "We leverage the detection component of the modular model as discussed in Pryzant et al. (2020b) and refer to this as the tagger model for the rest of this paper.\nThe tagger model (see Figure 7 in Appendix A) takes as input sentence(s) (e.g. a headline text, a collection of headlines, etc...), and predicts the probability of each word in the sentence(s) been biased (see Equation 1 and Figure 1). Then it returns the word with the highest probability as the tagged biased word.\n$P_{i} = \\sigma(b_{i}W' + e_{i}W^{e} + b)$ (1)\nWhere $b_{i} \\in R^{b}$ is a word's $w_{i}$ semantic meaning and $e_{i}$ are the experts features as proposed by (Recasens et al., 2013) and based on Equation 2.\n$e_{i} = ReLU(f_{i}W^{in})$ (2)\nThe detection model itself is a BERT-based neural sequence tagging model that has been fine-tuned to include the expert linguistic and epistemological bias features from Recasens et al. (2013). An example of the expert feature as listed in Table 3 of Recasens et al. (2013) is the part of speech (POS) tag of each word in the sentence. Another example of an expert feature from vetted experts in linguistics is \u201cassertive verbs\u201d. The model leverages the semantic meaning of each word in the given sentence via the BERT (Kenton and Toutanova, 2019) contextualised word vector.\nWhen there are words in a sentence which do not help with understanding, but are excess information these are known to cloud the judgment of the reader while simultaneously giving the reader reason to be more confident in the conclusions they make (Lynch, 2023; Spira, 2011). We hypothesised here that - the too much information (TMI) can be considered from an English linguistic standpoint to be a sentence instance which has more than 2 descriptive words (i.e. adjectives and adverbs) in it that do not add to its understanding but seeks to cloud the judgement of the readers. We manually analysed a few biased and non-biased headlines and saw an excessive use of descriptors (i.e. adjectives, adverbs) in headlines that were biased. We extend the detection model further by including a feature on whether or not the sentence contained TMI. We created methods in python which makes use of the core NLP dependency tree and a tree traversal algorithm to go through sentences starting from the root node, then count the number of adjectives and adverbs in the sentence to determine if there is \"TMI\" or \"no TMI\" based on the hypothesis. TMI is not a known indicator of epistemological bias, but we acknowledge it might contribute to causing doubt which leads to injustice. This is why we introduce this new feature to see if it will be a contributing feature for the tagger model to detect epistemological biases. We considered an ablation study to see the effects of adding this feature, whilst training the tagger model in Section 4.1."}, {"title": "CO-STAR Model and Social Bias Frames", "content": "The CO-STAR (COnceptualisation of Stereotypes for Analysis and Reasoning) framework allows input from the user and generates outputs of stereotypes and stereotype concepts using a GPT based model. The SBF model generates and classifies stereotypes associated with an inputted text. These outputs are quite simple to understand but have a very complex history. The accuracy of the baseline SBF model was analysed by looking for the demographic group the statement targeted and the implied stereotype from the statement. The authors measured the accuracy with BLEU-2 (group 83.2%, stereotype - 68.2%) and Rouge-L (group 49.9%, stereotype - 43.5%). The authors of the CO-STAR model manually evaluated their model, but did not specify the results of their evaluation. However, we analysed how each of these models performed on our sentences and found them to generate stereotypes which are often well fitted to the sentences submitted.\nSince testimonial injustice occurs because of widely known stereotypes, the outputs of these models will help us inform our users of any potential stereotypes that are exacerbated by their inputted text. The potential stereotype can also be used to determine if a person's character has been unjustly targeted. This is because stereotypes are not based on the actual truth of the particular person they are attached to and stigmatises the individual. Their presence amidst character assassination is evidence of character injustice. Stereotype concepts will offer explainability as to why a certain word was tagged as being epistemologically biased.\nWe submit news article headlines to the CO-STAR and SBF models and receive an output of 6 potential stereotypes and 3 stereotype concepts the sentence is related to. Semantic similarity describes how closely related two items (e.g. 2 words or 2 sentences) are in terms of meaning. We use semantic similarity to determine how closely related each of the generated stereotype sentences in the list are to the original headline sentence. After obtaining the vector embedding for the sentences, we used a distance metric on the encoded vectors (cosine similarity metric from sentence transformers), to get the distance scores, then we ranked the outputs based on their semantic similarity to the headlines. The closest stereotype and stereotype concept to the sentence is output to the user as the potential stereotype and concept which casts doubt on the subject of the inputted text."}, {"title": "Lexicon Lookup", "content": "Once the tagger model has returned the top tagged word, we proceed to automatically look up and semantically search the tagged word in the epistemological lexicons from the social sciences - to discover the epistemological bias types it is associated with. These lexicons are from the collection of datasets we discuss in the dataset section 3.5. When a tagged word is not found in the lexicons, we lemmatize said word (considering its context) to find its base word and find the stem word (removing the prefixes and suffixes). We then search for the lemmatized and stemmed words in the lexicons."}, {"title": "Interactive Interface for Learning about Bias", "content": "We propose the use of an interactive interface (UI) for editors of text content as a mechanism to keep human-in-the-loop control over outcomes of the models to mitigate any potential bias types and associated stereotypes, and make final editorial decisions prior to publishing their content.\nDuring the analysis, we first leverage the nltk python library to remove any stop words (i.e. a, the, etc..). We also check the sentence length. We make an assumption here, that if the sentence is less than 3 words, then there is not enough context to analyse the text for potential stereotypes. However, if we have enough context, the content is sent to the epistemological tagger model to find the word with the highest probability of bias (see Figure 6). The conditions from the framework (Figure 1) are then implemented.\nIn Figure 6 in Appendix A and 2 show screenshots of the the UI after a user has submitted a text for analyzing. The word suspected of causing injustice is highlighted with the certainty score, an explanation of why the word was tagged. When the user clicks Show Details they will see more details as displayed in Figure 2.\nWe display the types of associated epistemological bias detected to the user and provide links to resources which explains more on the specific types of biases as shown in Figure 2. See examples of these outputs in table 5 and table 4 of the Appendix."}, {"title": "Dataset source used", "content": "We leverage and use the bias data corpus\u00b2 as created by Pryzant et al. (2020b). It contains a Wikipedia Neutrality corpus with Wikipedia articles that were annotated for neutrality by their editors, who adhere to NPOV guidelines 3.\nLexicons used in the lexicon lookup were from social science research and collated in Pryzant et al. (2020a). We compiled these lexicons into one large dictionary and included some metadata about the lexicon i.e. source, creators, resources about the epistemological bias type, etc. Solely using an epistemological lexicon look up directly to find the biased words will be limited in its performance, for various reasons e.g. distributional shift, lexicons cannot scale well enough without requiring regular manual auditing, updating of the lexicon databases when newer forms of subtle bias words arise, etc. There are various studies which show the limitations of using just a lexicon based approach alone, for example in Cryan et al. (2020), the authors discuss on page 8 and show in Table 5 how lexicon approaches perform less accurately than an end to end approach using BERT to detect gender based stereotypes. Therefore, it is beneficial to not solely lean on lexicons and we include it as a contributor in detecting injustices in our framework.\nWhen it comes to Meghan Markle and Kate Middleton, two people associated to the British royal family by marriage, the media depicts several aspects of their lives which they share differently and often can be reflective of framing or character injustices. We use actual headlines (examples as seen in Table 3 in the Appendix) from such depictions in our comparative test to further illustrate the detection and harms of these injustices. We scale the inputs to our framework from an initial 20 article headlines used in the comparative test by gathering 1645 articles from the Bing API (up to 100 articles for each topic from no more than 1 month back). We searched the topics listed in Appendix A. The results of these comparative tests are further discussed in our results (Section 4)."}, {"title": "Results and Discussions", "content": "We performed an ablation to analyse the fine-tuned tagger model performance when there is no expert feature as proposed by Recasens et al. (2013) and"}, {"title": "Tagger Model Ablation Study", "content": "When they are included. Part of the training experiment was also to determine if there are any benefits from including the TMI feature into the input data. We carry out the training on 23,000 training samples from the bias-data training set and 700 validation and 1,000 test samples. Training was done using 4 CPUs and 1 GPU. We used a learning rate of 3e-5 and initially trained for 10 epochs."}, {"title": "Comparative Test", "content": "We performed a comparative test to illustrate how our framework shows injustices e.g. character and framing injustice. Particularly, we capture the type of epistemological bias attached to the tagged word in each entry, observe if a relevant stereotype is associated with the inputs, and observe the depiction of different subjects. In this study, we look at Meghan Markle and Kate Middleton as our subjects of interest. Meghan is associated to the British royal family, by marriage, and resigned from her royal duties due to abuse from the media and royal family (e.g. the Firm). Kate is also a member of the British royal family, by marriage, and was promoted to Princess of Wales. Even before Meghan Markle had resigned from her royal duties, the media sought to minimize her experience and diminish her comments and character. We often see instances of Meghan as the subject of an article and the media using sarcasm and criticism towards her, thus character injustice. Such acts have led readers to having tainted images of Meghan and even not believing her statements or actions, thus testimonial injustices. Yet, for the same and similar topics of concern, media members speak charmingly about Kate, thus leading to framing injustices shown. It is important to note here, that Kate Middleton and Meghan Markle shared similar positions, interests, and abilities. With this, we see them as good subjects to observe in our comparative test.\nIn Figure 4, we plotted results of an experiment where we initially analysed 10 article headlines of Meghan Markle and 10 articles of Kate Middleton through our framework. Each article was chosen because it discusses a topic which is common between each of the two subjects and allows us to see the differences in how the two subjects are spoken about concerning that topic. It has been acknowledged by several outlets that these particular articles which we have mentioned are unjust4. Majority of these articles are seen in the aforementioned articles to show these comparisons, but also a few additional articles were chosen by our team. In this plot, we capture the subject of the headline (Meghan/Kate), sentiment of the entire headline sentence, and the associated epistemological bias types for each headline as shown in table 4 located in the Appendix. See the complete list of headlines in Table 3 in Appendix A. The light blue circles represent the overall sentiment of the headlines, we will refer to them as the sentiment headline level. We use the 3 categories of positive, neutral, and negative sentiments. Within each sentiment headline level, we capture the subject of the headline, we will call this the subject level. Note that in Figure 4, Kate takes up the entire positive sentiment space, there were no examples that were detected to have positive sentiments for Meghan. Within each subject level, we capture the epistemological bias types associated with the tagged words in the headlines for each subject; we will refer to this as the bias-type level. This illustration shows us how much space is filled by each subject.\nFrom this plot, we can see the headline sentiment level, the negative sentiment circle is mostly filled with articles about Meghan and the most common bias type is positively subjective. The intuition here is that many articles about Meghan contain sarcasm, though this is not the only indicator. There was one article about Kate which had negative sentiment and each of the bias types in her circle were identified in the headline, this is why they are all equal in size. In looking at the headline sentiment level, the positive sentiment circle is completely filled with articles about Kate and the most common bias type is implicatives. The intuition here is that, in articles which have common topics between Meghan and Kate, only Kate is talked about in a suggestively positive way. In looking at the headline sentiment level, the neutral sentiment circle is mostly filled with articles about Kate and the most common bias type is positively subjective and regular. Recall, regular is the epistemological bias type assigned to detected words which do not appear, nor do similar words appear in the lexicon lookup. We can also observe, the articles which appear for Meghan with neutral sentiment are negatively subjective in nature, informing us they likely are not sarcastic but have negative suggestions. For Kate, she is talked about in a suggestively positive way, even in the neutral sentiments. This analysis confirms that when there is a topic that Meghan and Kate share - there was a use of words for Meghan which are sarcastic and cause framing and character injustices to her as a subject. Both of which can contribute to someone taking Meghan less seriously, thus causing testimonial injustice.\nWe manually evaluated the relevance of the generated stereotypes to the headlines as shown in table 2 (see appendix table 5 for the full table). A lower semantic similarity scores implies the generated stereotypes are irrelevant to the headlines. Where the semantic similarity distance was greater than 0.3, we observe that 5 out of 7 of such entries related to Meghan as the subject and only 2 related to Kate. More interestingly, we observed that one of the potential stereotypes relating to Meghan (personal spending habits) aimed at a personal attribute. Which can be seen as an indication of character assassination, thus character injustice."}, {"title": "Scaling the comparative test", "content": "After comparing the few headlines for both Kate and Meghan in 4.2 above, we used the process of getting more data into the framework as shown in Figure 5 to extract about 1600 new headlines for both subjects into the framework. Our aim of doing this is to show that the framework can be scaled to help aide the automatic analysis for potential subtle biases and injustice in text.\nIn Figures 8a and 8b in Appendix A, we see the plots obtained from scaling this analysis. Supporting our previous discussion, the observation from the Negative Sentiment circle is that Meghan often has more implicit subjective biases in a negative sentimental context compared to Kate. Likewise, for Kate we observed that the implicit subjective biases detected in the text associated with her are mainly used in a positively sentimental context (she takes up more space in the overall Positive Sentiment circle compared to Meghan). Most of the sample headline sentences had a Neutral Sentiment, hence the reason why the outer circle in Figure 8b showing the neutral sentimental context is extremely large. We observed that both subjects had almost equal amount of the epistemological bias types in this neutral context."}, {"title": "Conclusions", "content": "We describe a novel framework which uses a unique combination of different NLP techniques to detect character, testimonial, and framing injustices in text. These forms of injustices are often subtle and hard to quickly detect. The framework includes a fine-tuned BERT based epistemological tagging model, a GPT based stereotype generative model and SBF model, semantic similarity searching and lexicon lookup of epistemological biased words from the social science field. We provide qualitative empirical evidence as a justification for using this framework in media settings to detect such injustices. Further, we show our proposed UI (aimed at helping editors and authors of text content), by automatically detecting and explaining potential bias types and injustices that might be present in their content. We anticipate this UI will encourage them to take necessary, preventative steps in avoiding unjust acts pre-publication."}, {"title": "Limitations", "content": "\u2022 Learning Better Patterns for injustices:- Identifying a single word which potentially cause bias in a given sentence is only a start. We acknowledge that multiple words or phrases in a given sentence might be the culprit and the ability to tag multiple words or phrases will take this work further.\n\u2022 Veridicality Assessment:- We intend to incorporate veridicality assessment in our framework to assess if a statement is actually factual, which will allow for a more accurate analysis.\n\u2022 Evaluating Generative Stereotype Models:- We needed to manually evaluate the relevance of the generated stereotypes to our headlines. In the future, we would like to have evaluation metrics to help with analyzing the stereotypes generated by the CO-Star and SBF models."}, {"title": "Ethics Statement", "content": "A critical area for bias in systems and models' design often stem from a given human's intrinsic biases. They are usually a reflection of ourselves. One possible solution to this problem is to ensure that a diverse group of individuals are involved from the inception of the solutions design to the testing phase of such technical solutions\u2014this is one reason why we gathered a diverse group of authors to be involved in the discussions presented in this paper.\nA second area for bias surrounds the definition of terms and assumptions biases. A way we attempt to resolve this is by considering the existing consensus definitions of epistemological bias from the social sciences, where media bias has been studied for decades.\nA third potential area for bias can occur in the training and validation datasets, as well as in the models used for implementing the proposed solution. For example, the sourced datasets do not contain enough semantic information that is very reflective of the injustices considered. One possible way to combat this is to investigate ways to build more robust training datasets or leverage models that do not require lots of training examples to generalise properly.\nBroader problems may arise in any situation where technology is naively applied to solve a societal issue. As envisaged, our framework should be applied as a means to help people working in the media improve their output with respect to bias and injustices. However, as warned by Goodhart's law (Manheim and Garrabrant, 2018), if the measures and metrics suggested here become targets, they will cease to be useful. For example, in situations where experts deliberately bias their content the tool can become beneficial to the readers instead, so that they are aware of potential biases when reading an article. However, a main purpose of our proposed concept and tool is to help journalists who are aware that they might use biased terms but do so unintentionally. On the other hand, we cannot control the adoption of our tool. It will however help the editors who are checking for bias using manual means to quickly detect such biases. For example, the authors or editors of Wikipedia contents, have to follow and adhere to a NPOV (neutrality point of view) encyclopedia when writing or editing article contents, to ensure a neutral view point, and such a tool will be beneficial to them."}]}