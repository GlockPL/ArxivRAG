{"title": "Multi-Level Feature Distillation of Joint Teachers\nTrained on Distinct Image Datasets", "authors": ["Adrian Iordache", "Bogdan Alexe", "Radu Tudor Ionescu"], "abstract": "We propose a novel teacher-student framework to dis-\ntill knowledge from multiple teachers trained on distinct\ndatasets. Each teacher is first trained from scratch on its\nown dataset. Then, the teachers are combined into a joint\narchitecture, which fuses the features of all teachers at mul-\ntiple representation levels. The joint teacher architecture\nis fine-tuned on samples from all datasets, thus gathering\nuseful generic information from all data samples. Finally,\nwe employ a multi-level feature distillation procedure to\ntransfer the knowledge to a student model for each of the\nconsidered datasets. We conduct image classification ex-\nperiments on seven benchmarks, and action recognition ex-\nperiments on three benchmarks. To illustrate the power\nof our feature distillation procedure, the student architec-\ntures are chosen to be identical to those of the individual\nteachers. To demonstrate the flexibility of our approach,\nwe combine teachers with distinct architectures. We show\nthat our novel Multi-Level Feature Distillation (MLFD) can\nsignificantly surpass equivalent architectures that are ei-\nther trained on individual datasets, or jointly trained on all\ndatasets at once. Furthermore, we confirm that each step of\nthe proposed training procedure is well motivated by a com-\nprehensive ablation study. We publicly release our code at\nhttps://github.com/AdrianIordache/MLFD.", "sections": [{"title": "1. Introduction", "content": "The usual paradigm in machine learning is to train\na model on a single dataset with the desired goal that the\ntrained model should be able to generalize to unseen ex-\namples for a specific task. The training algorithm and the\ndataset, the two key ingredients used in training the model,\nshould be specifically tailored to the task at hand in order\nto facilitate obtaining a robust model that generalizes to the\ntraining data distribution. Although the size of the datasets\nused in training various models for different visual tasks\nhas increased exponentially in the last twenty years, rang-"}, {"title": "2. Related Work", "content": "Knowledge distillation. The goal of Knowledge Distil-\nlation (KD) is to transfer knowledge from a large teacher\nmodel to a small student model, such that the obtained"}, {"title": "3. Method", "content": "We propose a new multi-level distillation method to im-\nprove the generalization capacity and performance of mul-\ntiple models trained on distinct datasets, at no additional\ncosts during inference time. We achieve this by combining\nmultiple teacher models, each trained on a distinct dataset,\ninto a joint teacher, which is trained on data samples from\nthe combined datasets. Then, the knowledge from the joint\nteacher is distilled into dataset-specific students. The dis-\ntillation step is carried out at multiple representation lev-\nels, resulting in a multi-level feature distillation (MLFD)\nframework. Not only do the students receive information\nfrom different teachers, but they also capture information\nfrom distinct datasets, which significantly boosts their gen-\neralization capacities. Our MLFD framework is formally\npresented in Algorithm 1.\nThe MLFD algorithm essentially takes as input a set of\ndatasets D, a set of teacher architectures T, and a set of stu-"}, {"title": "4. Experiments", "content": "We next present experiments on three datasets for image\nclassification: CIFAR-100 [20], ImageNet-Sketch [47] and\nTinyImageNet [8]. Results on additional datasets and tasks\nare discussed in the supplementary."}, {"title": "4.1. Datasets", "content": "CIFAR-100. The CIFAR-100 dataset consists of 60,000\ncolor images of 32 \u00d7 32 pixels, grouped into 100 classes.\nWe use the official split with 500 training images and 100\ntesting images per class, for a total of 50,000 training im-\nages and 10,000 test images.\nImageNet-Sketch. The ImageNet-Sketch dataset consists\nof black and white images from 1000 classes, correspond-\ning to the ImageNet validation classes. The dataset is in-\ntended to be used for evaluating the ability of models to\ngeneralize to out-of-domain data, represented by sketch-like\nimages. The dataset comprises 41, 088 training images and\n10, 752 test images.\nTinyImageNet. The TinyImageNet dataset is a subset of\nthe ImageNet [8] dataset, containing 100,000 training im-\nages from 200 object classes. All images have a resolution\nof 64 \u00d7 64 pixels. As the labels for official test images are\nnot publicly available, we use the 10,000 validations im-\nages as test data."}, {"title": "4.2. Baselines", "content": "In our experiments, we consider three types of baselines\nthat are described below.\nDataset-specific models. As primary baselines for our\nmethod, we consider models that have identical architec-\ntures to our student models. These baseline models are\ntrained from scratch on individual datasets, so they do not\nbenefit from information gathered from other datasets. An\naccuracy boost over these baselines will demonstrate the\nbenefit of our multi-dataset distillation approach.\nMulti-head multi-dataset models. As a second baseline,\nwe propose an architecture composed of a shared backbone\nand an independent prediction head for each dataset. During\nevery iteration, we train our model by sequentially forward-\ning a batch from every dataset, and accumulating the gradi-\nents from all batches to be updated at once. Each batch is\nequally weighted in the final loss."}, {"title": "4.3. Experimental Setup", "content": "We evaluate our method on two sets of individual teach-\ners, denoted as T\u2081 and T2, respectively. T\u2081 contains three\nmodels, namely a ResNet-18 [13] trained on CIFAR-100,\nan EfficientNet-B0 [41] trained on TinyImageNet, and a\nSEResNeXt-26D [16] trained on ImageNet-Sketch. T2 con-\ntains a different lineup of models, namely a ConvNeXt-V2\nTiny [50] trained on CIFAR-100, a SwinTransformer-V2\nTiny [30] trained on TinyImageNet, and a FastViT SA24\n[44] trained on ImageNet-Sketch. In summary, T\u2081 and\nT2 contain a variety of state-of-the-art image classification\nmodels, being based on both convolutional and transformer\narchitectures. This is to demonstrate that the individual\nteachers can have distinct architectures. To demonstrate\nthe full power of our multi-dataset distillation pipeline, we\ntrain the individual teachers from scratch and refrain from\nusing weights pre-trained on ImageNet. Moreover, the stu-\ndent models, as well as the dataset-specific baseline models,\nhave identical architectures to the individual teachers in T\u2081\nand T2, respectively. This allows us to attribute the reported\naccuracy improvements to our method, and not to the archi-\ntectural choices."}, {"title": "4.4. Hyperparameter Tuning", "content": "We train each model from T\u2081 and T2 in a different man-\nner, adapted to the model and the target dataset. For models\nin T1, we apply a training procedure based on a constant\nlearning rate, using the Adam optimizer [19] with weight\ndecay for regularization, and the AutoAugment [6] with Im-\nageNet policy for augmentation. We tried replacing the con-\nstant learning rate with one-cycle learning for all models in\nT1, without observing any significant improvements. All\nmodels are trained for 200 epochs on images with a resolu-\ntion of 224 x 224 pixels.\nFor models in T2, each architecture has a specific training\nrecipe described in the corresponding original papers. We\nstart with almost the same hyperparameters, and make min-\nimal changes in order to adapt the hyperparameters to each"}, {"title": "4.5. Joint Teacher Architecture", "content": "We explore several architectures for the joint teacher,\ndepending on the type of layers that need to be fused.\nWhen combining embeddings from dense layers of indi-\nvidual teachers, we initially explored variations of feed-\nforward layers, 1D convolutional layers, and multi-head at-\ntention layers. For each alternative, we considered versions"}, {"title": "4.6. Results", "content": "We evaluate the performance of our approach in terms\nof two metrics: top-1 accuracy (acc@1) and top-5 accu-\nracy (acc@5). We present quantitative results of our exper-"}]}