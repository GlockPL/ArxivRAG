{"title": "Falcon Mamba: The First Competitive Attention-free 7B Language Model", "authors": ["Jingwei Zuo", "Maksim Velikanov", "Dhia Eddine Rhaiem", "Ilyas Chahed", "Younes Belkada", "Guillaume Kunsch", "Hakim Hacid"], "abstract": "In this technical report, we present Falcon Mamba 7B, a new base large language model based on the novel Mamba architecture. Falcon Mamba 7B is trained on 5.8 trillion tokens with carefully selected data mixtures. As a pure Mamba-based model, Falcon Mamba 7B surpasses leading open-weight models based on Transformers, such as Mistral 7B, Llama3.1 8B, and Falcon2 11B. It is on par with Gemma 7B and outperforms models with different architecture designs, such as RecurrentGemma 9B, and RWKV-v6 Finch 7B/14B. Currently, Falcon Mamba 7B is the best-performing Mamba model in the literature at this scale, surpassing both existing Mamba and hybrid Mamba-Transformer models, according to Open LLM Leaderboard (Fourrier et al., 2024). Due to its architecture, Falcon Mamba 7B is significantly faster at inference and requires substantially less memory for long sequence generation. Despite recent studies suggesting that hybrid Mamba-Transformer models outperform pure architecture designs, we demonstrate that even the pure Mamba design can achieve similar, even superior results compared to the Transformer and hybrid designs. We make the weights of our implementation of Falcon Mamba 7B publicly available on https://huggingface.co/tiiuae/ falcon-mamba-7b, under a permissive license\u00b9.", "sections": [{"title": "Introduction", "content": "Modern foundation models are predominantly based on the Transformer and its core attention layer (Vaswani et al., 2017). Due to its quadratic complexity regarding the window length, recent research attempts to propose more efficient alternatives of vanilla attention, such as FlashAttention (Dao et al., 2022; Dao, 2024), sliding window attention (Beltagy et al., 2020). New architectures beyond Transformers such as Griffin (De et al., 2024), RWKV (Peng et al., 2023), and Mamba (Gu & Dao, 2023) have recently been proposed and have demonstrated performance comparable to Transformers. However, most of them either proved their performance at small scale, or still show a performance gap with recent Transformer-based performing LLMs.\nThere have been efforts from the community to scale up Mamba LLMs beyond the original test-purpose 2.8B Mamba model (Gu & Dao, 2023). Notable examples include Mamba-7B-rw (Mercat et al., 2024), Zamba 7B (Glorioso et al., 2024), Samba 3.8B (Ren et al., 2024), Mamba2 8B"}, {"title": "Model Architecture", "content": "The Falcon Mamba 7B model architecture is based on Mamba (Gu & Dao, 2023). The core parameters of the architectures are summarized in Table 1.\nWe have untied the input embeddings from the output weights throughout the entire training process to increase model flexibility. Based on our experimental results, this approach has led to improved model performance at the 7B scale.\nNote that, in contrast to transformers, the sequence length is not a part of Mamba architecture. Any sequence length can be used during inference, while the actual ability of the model to process long sequences is determined by the sequence length used for training.\nDesign decision: Recent work (Dao & Gu, 2024; Lieber et al., 2024) suggests that a hybrid architecture, with interleaved attention and SSM layers, can outperform pure Transformer or SSM models. This improvement is hypothesized to arise from the complementary features from both models: the general sequence-to-sequence mapping capabilities of SSMs and the fast retrieval properties of attention layers. Recent Mamba-based language models follows this intuition and scale up the hybrid design beyond 2.8B models, such as Samba 3.8B (Ren et al., 2024), Zamba 7B (Glorioso et al., 2024), Jamba 12B/52B (Lieber et al., 2024). However, introducing attention layers compromises the linear scalability of the Mamba architecture, prompting the question: can a purely Mamba-based design achieve competitive performance against state-of-the-art (SoTA) open LLMs at scale, while conserving its linear scalability? Recent attention-free models, such as RWKV-v6 (Peng et al., 2024), show their performance at small scale or/and on certain academic benchmarks. However, they are far behind popular LLMs when setting up more thorough comparisons on various benchmarks.\nModel stability During pre-training, we observed consistent loss spikes that occurred randomly and unpredictably. Notably, when we applied higher learning rates, the model exhibited more pronounced loss spikes and became more prone to divergence. This phenomenon was also observed in the training of Falcon2 (Malartic et al., 2024), and recent papers like Jamba (Lieber et al., 2024) and Mamba2 (Dao & Gu, 2024) have reported similar issues. In particular, we found that the Mamba architecture is more sensitive to learning rates than Transformers. Careful model initializations and reducing model's learning rate sensibility are crucial for addressing this issue. Aligned with (Dehghani et al., 2023), it's becoming a common practice to apply pre-norm and post-norm with"}, {"title": "Pre-training", "content": "Falcon-Mamba-7B was trained on 256 H100 80GB GPUs for the majority of the training, using only Data Parallelism (DP=256). This was combined with ZeRO optimization to efficiently manage memory and training processes.\nThe model was trained using the AdamW optimizer with \u03b2\u2081 = 0.9 and \u1e9e2 = 0.95, \u20ac = 10\u22128, and weight decay value 0.1. Although we didn't apply Z-loss on output logits during Falcon-Mamba-7B pre-training, in the follow-up experiments we observed that it helps to stabilize the training, in agreement with (Wortsman et al., 2024).\nWe applied warmup-stable-decay (WSD) learning rate schedule (Hu et al., 2024) with a fixed warmup duration of 1GT, and learning rate max = 6.4 \u00d7 10-4 during the stable stage. This way, our model was trained with a relatively high learning rate during most of the pertaining, leading to a quick adaptation to data distribution shifts introduced between different training stages and the beginning of the decay stage (see section 3.2.2). In the decay stage, we reduced learning rate to the minimal value \u03b7min = \u03b7max/25 using exponential schedule with profile \u03b7(t) = \u03b7max exp[-t/tdecay log \u03b7max/\u03b7min], where tdecay is the duration of the decay stage. Contrary to most technical reports, we found out that longer LR decay stage provided better results evaluation-wise. We kept around 10% of the total training tokens for the decay to have optimal performances, which is aligned with recent miniCPM's conclusions (Hu et al., 2024).\nIn the beginning of the training, we used batch size rampup. Specifically, we were linearly increasing the batch size initial value bmin = 128 to the maximum value bmax = 2048 over the first 50GT. In our experiments, we noticed that batch size rampup affects the loss curve and final model performance. This effect is most conveniently interpreted in terms of gradients noise temperature Tnoise, defined for Adam optimizer as (Malladi et al., 2022)\nTnoise = \u03b7 / sqrt(b) (1)\nDuring batch size rampup, noise temperature (1) is decreased. This leads to better loss during the stable LR phase but a smaller loss boost within LR decay phase. To counter this deficiency, we apply batch scaling: keeping the Adam noise \u03b7/b temperature constant by adjusting learning rate \u03b7 whenever batch size b is changed. We have found that batch scaling leads to a better final loss after the LR decay stage, even during long training durations much exceeding the length of rampup period."}, {"title": "Pre-training data", "content": "Falcon Mamba 7B was mostly trained on the data from Falcon2-11B (Malartic et al., 2024). Since a 7B model may not be sufficient to perform promising performances on multilingual tasks without harming the English ones, we exclude multilingual data from the pre-training corpus. Nevertheless, a continual pre-training stage can be adopted to empower the model with multilingual capabilities. We adopt the same tokenizer as the Falcon series model (Almazrouei et al., 2023) with no change."}, {"title": "Data sources", "content": "The model was trained on a diverse data mixture consisting primarily of web, curated, code, and math data.\nWeb data We mainly leveraged RefinedWeb (Penedo et al., 2023), which is a high-quality English pre-training dataset composed of five trillion tokens coming from web data only. Starting from raw Common Crawl data, samples were filtered out through language identification, filtering (line-wise and document-wise) as well as fuzzy and exact deduplication."}, {"title": "Data mixtures", "content": "The pre-training was conducted in four constant learning rate (LR) stages, followed by a final LR decay stage. The first four stages consisted in progressively increasing the sequence length, from 2048 up to 8192. Following the curriculum learning concept, we carefully selected data mixtures throughout the training stages as shown in Fig. 1, considering both data diversity and complexity. The main idea is to increase high quality and scientific data at late stages. Due to limited data from certain resources, we applied multiple epochs for less-represented data, e.g., math, code, curated data. Since the packing tokens were used in the pretraining, we carefully selected the proportions of short and long samples at each stage to prevent any distribution shifts.\nIn the decay stage, we introduced more diverse and higher-quality data to refine or shapen the knowledge learned during earlier stages. This included using parts of Fineweb-Edu (Penedo et al., 2024) as web data, along with synthetic data from Cosmopedia (Ben Allal et al., 2024). Additionally, a small portion of multitask instruction data (four epochs for 3.7%) was used, similar to other studies (Hu et al., 2024; Yang et al., 2024), to enhance the model's zero-shot and few-shot learning capabilities. The inclusion of instruction data during pretraining is a debated topic, as it may potentially reduce a model's fine-tuning flexibility. However, from our experimental results, we found that keeping a minimal amount of instruction data enhances Mamba's in-context retrieval ability (Wen et al., 2024) while not overfitting the multitask data with limited epochs of repetitions. Additionally, we observed that the training loss was still decreasing at the end of Stage 4, suggesting that the model's performance could be further improved with continued training on more high-quality data. To support the community in further research or continual training on the model, we decided to release as well the pre-decay checkpoint 2 of the model."}, {"title": "Evaluation and Results", "content": "We conducted a comparative evaluation of our model against state-of-the-art models across three distinct architectural categories: State Space Models (SSMs), Transformers, and Hybrid models. The Hybrid models integrate a combination of attention mechanisms with Recurrent/Mamba blocks."}, {"title": "Throughput and memory consumption", "content": "The attention mechanism is inherently limited in processing long sequences due to the increasing compute and memory costs as sequence length grows. Leveraging the theoretical efficiency of SSM models in handling large sequences (Gu & Dao, 2023), Falcon-Mamba-7B demonstrates that these scaling limitations can indeed be overcome without compromising performance.\nSetup To replicate real-world use cases, we compared the memory usage and generation throughput of Falcon-Mamba-7B with popular Transformer-based models of a similar scale, including Llama3.1-8B (Dubey et al., 2024), Mistral-7B (Jiang et al., 2023), and Qwen2-7B (Yang et al., 2024). All evaluations were conducted using the Hugging Face transformers library (Wolf et al., 2020). For a fair comparison, we rescaled the vocabulary size of all transformer models to match Falcon-Mamba-7B, since it has a big impact on the memory footprint of the model.\nParallel Prefill and Sequential Prefill Before diving into the results, it is important to clarify the difference between the prompt (prefill) and generated (decode) parts of a sequence. For state space models (SSMs), the prefill process is more critical than for transformer models. When a transformer generates the next token, it must attend to the keys and values of all previous tokens in the context, resulting in both memory and generation time scaling linearly with context length. In contrast, an SSM only stores and attends to its recurrent state, which avoids the need for additional memory or time when generating large sequences. While this demonstrates the efficiency of SSMs during the decoding phase, the prefill phase requires additional framework optimizations to fully leverage the SSM architecture.\nThe standard method for prefill is processing the entire prompt in parallel, maximizing GPU uti-lization, referred to here as Parallel Prefill. This is the approach used in most frameworks like Optimum-Benchmark 4. In this approach, the memory usage grows with prompt length due to the need to store hidden states for each token. For transformers, memory is dominated by stored key-value (KV) caches, whereas SSMs don't require KV caching. However, for SSMs, the memory required to store hidden states still scales with the prompt length, making it challenging to handle arbitrarily long sequences, similar to transformers. An alternative method, which we referred to as Sequential Prefill, processes the prompt token by token (or in larger chunks for better GPU usage), similar to sequence parallelism. While this method offers little benefit for transformers, it allows SSMs to process arbitrarily long prompts, mitigating the memory scaling issue seen with parallel prefill. This requires more community supports for optimizing existing inference frameworks for SSMs.\nWith these considerations in mind, we first evaluate the maximum sequence length that can fit on a single 24 GB A10 GPU, as shown in Fig. 2. The batch size is fixed at 1, and we employ float32 precision for all operations. Our results show that, even for parallel prefill, Falcon-Mamba-7B is capable of fitting larger sequences compared to a standard transformer architecture, while in sequential prefill, Falcon-Mamba-7B can unlock its full potential and process arbitrarily long prompts."}, {"title": "Model Integration and Availability", "content": "In real-world scenarios, input sequences of varying lengths are often batched together for efficiency, which introduces padding tokens to align the sequences. This can pose challenges for SSM-based models like Mamba, as right-side padding, while effective during training\u2014where padding tokens are masked out in the loss computation\u2014becomes problematic during inference. In inference, the Mamba model predicts the next token based on all previous hidden states, so including padding tokens from shorter sequences can lead to inaccurate predictions."}, {"title": "Model Availability", "content": "The Falcon-Mamba-7B models, including the pre-decay checkpoint, are made available under the Falcon Mamba 7B TII License 5, a permissive Apache 2.0-based software license which includes an acceptable use policy 6 that promotes the responsible use of AI.\nThe models are fully integrated within the Hugging Face ecosystem and can be accessed through the Transformers library (Wolf et al., 2020). This includes support for inference, quantization (using most supported quantization schemes), and fine-tuning via the TRL library (von Werra et al., 2020). All associated artifacts, including GGUF files, can be browsed through the Falcon Mamba 7B collection in Hugging Face.\nAdditionally, support for Falcon-Mamba-7B has been added to the 1lama.cpp package 7, enabling easy deployment of Falcon-Mamba-7B on local machines using CPU hardware. We are planning to expand the support for more platforms in the future."}, {"title": "Discussions and conclusion", "content": "We have introduced Falcon Mamba 7B, the first competitive 7B language model based purely on the Mamba architecture. Our results show that it matches or outperforms state-of-the-art transformer models such as Llama 3.1 and Mistral 7B in a variety of benchmarks. This way, Falcon Mamba 7B sets a new benchmark for attention-free models, proving that pure SSM-based designs can achieve state-of-the-art performance. We hope that our model will strengthen the belief in further innovation of efficient language model architectures, challenging the infamous \u201cattention is all you need\" saying.\nThe main advantage of mamba architecture lies in the long-context generation, where it maintains constant memory and throughput usage regardless of sequence length. We have confirmed this statement with throughput and memory analysis for Falcon Mamba 7B. However, as we focused on obtaining a strong general-purpose language model, the actual proficiency of the model in long sequence understanding and generation was not emphasized in Falcon Mamba 7B training strategy, featuring rather medium 8k context length. Tailoring the training procedure towards extra-large contexts and verifying mamba proficiency in this regime remains an important yet underexplored area for future research and development. If successful, it would make mamba-based models ideal for real-world applications requiring low-latency, large-scale generation, e.g., audio, video.\nWhile Falcon Mamba 7B performs well, particularly in reasoning tasks and long-context learning, it shows potentially some limitations in in-context learning compared to Transformers. Although high-quality data, especially Chain-of-Thought (CoT) instruction data or tailored prompting tech-niques (Arora et al., 2024), help mitigate these potential disadvantages, it may still not be sufficient to close the gap with Transformers (Wen et al., 2024), given the same data budget. However, data scaling and model scaling in the Mamba architecture have been less explored in the literature, leaving the potential limitations and optimizations of Mamba as an open area for further research. Moreover, the complementary features of sequence mixing performed by SSM and attention suggest that hybrid models might have the best of both worlds. Although many recent models (Lieber et al., 2024; Ren et al., 2024; Dao & Gu, 2024; De et al., 2024) have started to explore this direction, we believe that the question of how to optimally use SSM and attention in a single architecture remains open."}]}