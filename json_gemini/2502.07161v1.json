{"title": "A Survey on Mamba Architecture for Vision Applications", "authors": ["Fady Ibrahim", "Guangjun Liu", "Guanghui Wang"], "abstract": "Transformers have become foundational for visual tasks such as object detection, semantic segmentation, and video understanding, but their quadratic complexity in attention mechanisms presents scalability challenges. To address these limitations, the Mamba architecture utilizes state-space models (SSMs) for linear scalability, efficient processing, and improved contextual awareness. This paper investigates Mamba architecture for visual domain applications and its recent advancements, including Vision Mamba (ViM) and VideoMamba, which introduce bidirectional scanning, selective scanning mechanisms, and spatiotemporal processing to enhance image and video understanding. Architectural innovations like position embeddings, cross-scan modules, and hierarchical designs further optimize the Mamba framework for global and local feature extraction. These advancements position Mamba as a promising architecture in computer vision research and applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning has revolutionized artificial intelligence (AI), and has been instrumental in advancing computer vision, significantly enhancing machines' ability to interpret and understand visual information from the world. By harnessing the power of neural networks, particularly Convolutional Neural Networks (CNNs) [1], [2] and, more recently, Vision Transformers (ViTs) [3]-[5], deep learning has enabled the development of complex, highly accurate models for tasks such as image classification [6], object detection [7], semantic segmentation [8], [9], and spatio-temporal video understanding. [10], [11]. Transformers, originally popularized in natural language processing (NLP) [4], have shown significant potential in vision tasks. Architectures such as Vision Transformers (ViTs) [5] and hybrid models [12], [13] leverage their ability to capture complex dependencies, delivering state-of-the-art results.\nAt the core of these advances are models with massive parameter counts capable of capturing intricate patterns in data. Transformers despite their success, face challenges due to the quadratic complexity of attention calculations, resulting in time-consuming inference. Mamba, a novel architecture inspired by state-space models (SSMs), presents a promising alternative for building foundation models [14]. With near-linear scalability in sequence length and hardware-efficient processing, Mamba has been recently adapted to computer vision tasks, including image and video processing. Models like Vision Mamba (ViM) [15] and VideoMamba [16] demonstrate Mamba's ability to handle long-range dependencies while addressing the computational and memory bottlenecks faced by Transformer-based models in the visual domain.\nThis paper offers a comprehensive review of Mamba architectures applied to visual tasks, highlighting their similarities, differences, and effectiveness in addressing various challenges. Our survey exclusively focuses on Mamba's role in visual tasks, setting it apart from broader Mamba surveys that cover applications in text, recommendation systems, and medical imaging [17]-[20]. While previous works like [21], [22] examine Mamba in vision, this paper uniquely focuses on ViM and VideoMamba as core models. We assess architectural enhancements, including position embeddings, cross-scan modules, and hierarchical designs, to provide new insights into optimizing Mamba for both local and global feature extraction. Additionally, we offer a comparative performance analysis across image classification, semantic segmentation, and object detection, giving practical recommendations for selecting the best architectures for various vision tasks. This task-specific comparison sets our work apart from general Mamba surveys and provides valuable guidance for vision-based research.\nThe rest of this survey is organized as follows: Section II provides an introductory overview of the building blocks of ViM and the theoretical foundations of SSMs. Section III compares ViM and Video Mamba. Section IV reviews recent advancements in Mamba architectures. Section V presents a performance comparison and analysis of the novel architectures on common datasets. Section VI discusses the challenges associated with ViM and its applications. Finally, the paper is concluded in Section VII."}, {"title": "II. PRELIMINARY", "content": "Mamba enhances the context-aware capabilities of traditional SSMs by making its parameters functions of the input. Fig. 1 shows the original Mamba architecture block intended for 1D sequential data such as language tokens. SSMs map a 1D function or sequence $x(t) \\in R$ to an output $y(t) \\in R$ through a hidden state $h(t) \\in R^N$. Using parameters $A \\in R^{N\\times N}, B \\in R^{N\\times 1}, C \\in R^{1\\times N}$, the continuous system is defined by\n$\\frac{dh(t)}{dt} = Ah(t) + Bx(t), y(t) = Ch(t)$  (1)\nFor the discrete sequences defined by the input $x = (x_0,x_1,...) \\in R^L$, the continuous system parameters in Eq. (1) must be discretized using a step size $ \\Delta $. Zero Order Hold (ZOH) [23] is used as the simplest and most reliable discretization method in [15], [16], [24]. Using step size $ \\Delta $, the continuous system parameters A, B are converted to $\\overline{A}$, $\\overline{B}$ as\n$\\overline{A} = exp(\\Delta A)$,\n$\\overline{B} = (\\Delta A)^{-1}(exp(\\Delta A) - I) \\cdot \\Delta B$.\nand the discretized system equation can be represented as\n$h_t = \\overline{A}h_{t-1} + \\overline{B}x_t, y_t = Ch_t$\n(3)\nTraditional SSMs struggle with maintaining context awareness in dense text and data. Mamba introduces a selection mechanism that dynamically adapts to input data, filtering irrelevant information while retaining relevant data indefinitely. Mamba's innovation lies in the Selective Scan Mechanism (S6) [14], making SSM parameters B, C, A functions of the input as $S_B(x), S_C(x), S_\\Delta(x)$. This is achieved through a linear transformation\n$B, C, \\Delta = Linear(x)$\nwhere $B, C, \\Delta \\in R^{B \\times L \\times N}$, L denotes the input sequence length, B is the batch size, and D denotes the number of channels.\nThe original Mamba block [14] introduced a hardware-aware algorithm to efficiently compute the selective SSM parameters. These parameters are used in SSM equations Eq. (1) and Eq. (3) to compute the latent hidden state and output. This mechanism allows Mamba to selectively retain or discard information based on the relevance of the input sequence.\n$h_t = \\overline{A}h_{t-1} + \\overline{B}x_t, y_t = S_C h_t$\n(4)\n$\\overline{A} = exp(S_\\Delta A)$ (5)\n$\\overline{B} = (S_\\Delta A)^{-1}(exp(S_\\Delta A) - I) \\cdot S_\\Delta S_B$.\nVision Mamba extends the Mamba architecture to address the complexities of visual data. It tackles position sensitivity by incorporating position embeddings, similar to those used in ViT [5]. To capture global context, ViM employs bidirectional SSMs, as illustrated in Fig. 2, allowing each sequence element to access information from both preceding and succeeding elements [15].\nThe original Mamba block's unidirectional scanning suits causal sequences. However, visual data is non-causal. ViM employs bidirectional scanning, processing sequences forward and backward to capture dependencies effectively, as shown in Fig. 2. For video understanding, spatiotemporal scanning extends this concept. VideoMamba employs 3D bidirectional blocks to capture spatial and temporal dependencies, as shown in Fig. 3. In addition to these strategies, researchers have explored various spatiotemporal methods, such as spatial-first versus temporal-first scanning, as well as sweeping scan, continuous scan, and local scan techniques, to further enhance ViM's performance and efficiency. An overview of how these methods are utilized in key architectures is presented in Section IV."}, {"title": "III. VISION MAMBA VS. VIDEOMAMBA", "content": "This section compares the Vision Mamba block with the more complex VideoMamba model, focusing on block design, scanning mode, and memory management."}, {"title": "IV. KEY ARCHITECTURES", "content": "This section summarizes the recent advancements in Vision/Video Mamba-based studies from the perspectives of key architectural innovations. Fig. 6 shows additional selective scan mechanisms employed by the architecture variants described below. Each novel scanning strategy has been configured to supplement using Mamba for 2D and 3D applications, such as spatial relationship modeling, limited local representations, and varying directional scanning for better visual contextual features. Fig. 4 shows visual block representations of some of the key architectures used in image tasks and how these blocks compare to the original Mamba block in Fig. 1.\na) Selective State Space Models (SSMs): This is the foundation of the Mamba architecture. SSMs, especially the S6 block, offer a global receptive field and linear complexity in relation to sequence length, making them a computationally efficient alternative to traditional Transformers [14], [26], [29], [41]. Mamba introduces a selection mechanism, enabling the model to selectively retain or discard information based on the input, enhancing context-based reasoning [14], [41]. This architecture enables the model to perform content-based reasoning by making the SSM parameters functions of the input. This allows the models to selectively propagate or forget information along the sequence length, filtering irrelevant information and maintaining long-term memory of relevant information [14], [30], [41], [42].\nb) Multi-path Scanning Mechanisms: To overcome the limitations of Mamba's causal nature, which is not ideal for image processing, several models like VMamba in Fig.4(b), LocalMamba in Fig.4(c), and PlainMamba in Fig.4(d) have adopted multi-path scanning mechanisms. This allows the models to capture both local and global features more effectively [15], [27], [43].\nLocalMamba [27] improves image scanning by dividing images into distinct local windows, ensuring relevant tokens are closely arranged to better capture local dependencies while maintaining spatial coherence. To incorporate global context, the method integrates a selective scan mechanism across four directions: the original two and their flipped counterparts, allowing for both forward and backward token processing, as shown in Fig. 6(c).\nc) Structure-Aware SSM Layer: Introduced in Spatial-Mamba, this layer incorporates a Structure-Aware State Fusion (SASF) branch and a multiplicative gate branch as shown in Fig. 4(e). This allows the model to capture both local and global spatial dependencies [44]. Unlike previous methods that rely on multiple scanning directions, Spatial-Mamba fuses the state variables into a new structure-aware state variable [44]. Once an input image is flattened into 1D patches and state parameters are determined, the patches are reshaped into 2D, and SASF is applied to integrate local dependencies before generating the final output.\nd) Cross-Layer Token Fusion: Famba-V, a technique designed to enhance the training efficiency of Vision Mamba (ViM) models, incorporates three cross-layer token fusion strategies [45]-[48].\nToken fusion is applied within the ViM block as shown before the final linear projection operation in Fig. 4 (f), the tokens are grouped and paired based on cosine similarity, and fused by averaging. There are three token fusion strategies implemented in [45] with upper layer token fusion, producing the best results on vision tasks shown in Section V:\n$\\bullet$ Interleaved Token Fusion \u2013 Applies fusion to alternate layers, starting from the second layer, to maintain efficiency without overly aggressive fusion.\n$\\bullet$ Lower-layer Token Fusion \u2013 Applies fusion only to the lower layers, leveraging early-stage efficiency gains while preserving high-level reasoning.\n$\\bullet$ Upper-layer Token Fusion \u2013 Applies fusion only to upper layers, assuming high-level features can be fused without significantly affecting performance.\ne) Hierarchical Mamba Architecture (Hi-Mamba): This architecture in Fig. 4(g) designed for image super-resolution, introduces a hierarchical Mamba block (HMB) comprised of local and region SSMs with single-direction scanning. This setup aims to capture multi-scale visual context efficiently [49].\nf) Direction Alternation Hierarchical Mamba Group (DA-HMG): Within Hi-Mamba, this group further enhances spatial relationship modeling. It allocates single-direction scanning into cascaded HMBs, improving performance without increasing computational costs or parameters [49].\ng) Non-Causal State Space Duality (NC-SSD): Models like VSSD introduce non-causal SSD, eliminating the causal mask in the state space dual model (SSD) of Mamba2 [30]. This architectural innovation allows for a more flexible application of Mamba to inherently non-causal data like images [15], [26], [29], [50].\nThe VSSD block [26] enhances Mamba2 SSD for vision tasks by incorporating Depth-Wise Convolution (DWConv) [51], a Feed-Forward Network (FFN) for better channel communication, and a Local Perception Unit (LPU) for improved local feature extraction. The four-stage hierarchical VSSD model uses VSSD blocks in the first three stages and Multi-Head Self-Attention (MSA) in the last, balancing efficiency and performance in vision applications [4].\nh) Hidden State Mixer-based SSD (HSM-SSD): EfficientViM models feature this layer, which shifts the channel mixing operations from the image feature space to the hidden state space. This design aims to alleviate the computational bottleneck of the standard SSD layer without compromising the model's ability to generalize [29].\ni) Register Tokens in Vision Mamba: Mamba\u00ae introduces registers, input-independent tokens, into the Vision Mamba architecture. These tokens, inserted evenly throughout the input and recycled for final predictions, aim to mitigate artifacts and enhance the model's focus on semantically meaningful image regions [25].\nj) Masked Backward Computation: VideoMambaPro incorporates masked backward computation during the bidirectional Mamba process, mitigating the issue of historical decay in token processing and enabling the model to better utilize historical information [16], [32].\nk) Elemental Residual Connections: VideoMambaPro also introduces residual connections to the matrix elements of Mamba, addressing the challenge of element contradiction, where elements in the sequence can conflict during computation. This enhances the model's ability to extract and process complex spatio-temporal features in video data [16], [32].\nMany of these innovations focus on enhancing the efficiency and performance of Mamba-based models in vision tasks, while preserving the linear complexity advantage over traditional Transformers."}, {"title": "V. PERFORMANCE COMPARISON", "content": "To provide a systematic comparison of the performance of different models across various datasets and tasks, this section compares the results of [14]-[16], [24]-[27], [29], [30], [32], [43]-[45] in common dataset benchmarks. The models shown are only those of a similar size in the cases where researchers provided 'small', 'base', or 'large' model variations. The results are categorized by tasks; image classification against the Imagenet-1k dataset [35], [52], semantic segmentation against the ADE20K dataset [38], [53], object detection against the MS-COCO dataset [54], and human action recognition for video understanding benchmarks against both Kinetics-400 [55], [56] and Something-Something-V2 [57] datasets.\nImage Classification\nComparison\nBased on the above experimental evaluations, it is evident that Spatial Mamba-S and Vmamba-S are the best choices for image classification (84.6% and 83.6% Top-1 ACC) and semantic segmentation (50.6 mIoU), though they require high computational cost. For object detection, Spatial Mamba-S (54.2 APbb75) and VSSD-S (53.1 APbb75) perform best, leveraging structure-aware and non-causal mechanisms. VSSD-S and LocalMamba offer a balance between accuracy and efficiency across tasks."}, {"title": "VI. CHALLENGES AND FUTURE RESEARCH", "content": "Mamba models are capable of modeling long-range dependencies with linear complexity, making them well-suited for tasks involving high-resolution images, long sequences, and large video datasets. Techniques such as bidirectional scanning and multi-scale processing further enhance Mamba's ability to capture both local and global context. Mamba is currently being explored in specialized vision domains, such as:\n$\\bullet$ Scene flow estimation. FlowMamba [59] uses an iterative SSM-based update module and a feature-induced ordering strategy to capture long-range motion.\n$\\bullet$ Vein recognition. Global-local Vision Mamba (GLVM) combines local and global feature learning for enhanced vein recognition [40].\n$\\bullet$ Medical image analysis. PV-SSM is a pure visual state space model achieving strong results across medical image analysis [60].\n$\\bullet$ Skin lesion segmentation. MambaU-Lite [61] combines Mamba and CNN architectures for efficient skin lesion segmentation, using a novel P-Mamba block.\nFuture research aims to develop native 2D and 3D Mamba models, non-causal state-space dual models, and efficient token fusion techniques. Some potential directions include:\n$\\bullet$ Multi-modal image fusion. Shuffle Mamba employs a random shuffle-based scanning method to mitigate biases in multi-modal image fusion [62].\n$\\bullet$ RGB-D salient object detection. MambaSOD is a dual Mamba-driven network for RGB-D salient object detection, using cross-modal fusion to combine RGB and depth information [63].\n$\\bullet$ Point cloud processing. NIMBA reorders point cloud data to maintain 3D spatial structure, enhancing Mamba's sequential processing [64]. Serialized Point Mamba [65] uses a state-space model for dynamic compression of point cloud sequences for efficient segmentation.\nCombining Mamba with CNNs and Transformers is also a research direction to compensate for each other's weaknesses.\n$\\bullet$ Object detection with YOLO. Mamba-YOLO integrates Mamba into the YOLO architecture, achieving improved object detection performance [66].\n$\\bullet$ 3D object detection. MambaBEV [42] and PillarMamba [67] integrate Mamba into 3D object detection models for autonomous driving. MambaDETR is also used for 3D object detection [68].\n$\\bullet$ Point cloud enhancement. MambaTron uses Mamba for cross-modal point cloud completion, combining Mamba's efficiency with Transformer's analytical capabilities [69].\nVideoMamba [16] still has challenges in achieving performance parity with Transformers, addressing historical decay, and managing potential element contradictions [70]. Future research will focus on enhancing stability and closing the performance gap with other architectures, in order to fully unlock Mamba's potential for efficient, long-range context modeling in vision and video."}, {"title": "VII. CONCLUSION", "content": "This paper provides an overview of visual Mamba, detailing their architectures, applications, and challenges. Vision Mamba functions as a general vision backbone, processing images as sequences of patches with bidirectional scanning to effectively capture spatial context. Video Mamba extends this approach to 3D video sequences, operating across both spatial and temporal dimensions. Despite significant advancements and key architectural developments, challenges persist in fully realizing Mamba's potential for vision tasks. Future research should focus on refining these architectures, broadening Mamba's applicability to a wider range of visual modalities, developing efficient token fusion techniques, and improving its scalability for real-world deployment."}]}