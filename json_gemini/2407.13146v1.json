{"title": "PG-Rainbow: Using Distributional Reinforcement Learning in Policy Gradient Methods", "authors": ["WooJae Jeon", "KangJun Lee", "Jeewoo Lee"], "abstract": "This paper introduces PG-Rainbow, a novel algorithm that incorporates a distributional reinforcement learning framework with a policy gradient algorithm. Existing policy gradient methods are sample inefficient and rely on the mean of returns when calculating the state-action value function, neglecting the distributional nature of returns in reinforcement learning tasks. To address this issue, we use an Implicit Quantile Network that provides the quantile information of the distribution of rewards to the critic network of the Proximal Policy Optimization algorithm. We show empirical results that through the integration of reward distribution information into the policy network, the policy agent acquires enhanced capabilities to comprehensively evaluate the consequences of potential actions in a given state, facilitating more sophisticated and informed decision-making processes. We evaluate the performance of the proposed algorithm in the Atari-2600 game suite, simulated via the Arcade Learning Environment (ALE).", "sections": [{"title": "Introduction", "content": "In recent years, significant improvements have been made in deep reinforcement learning, with multiple algorithms showing state-of-the-art performance across diverse domains and environments (Berner et al. [2019], Mnih et al. [2013]). Amidst the various branches in deep reinforcement learning, the field of on-policy reinforcement learning has attracted great attention because it directly interacts with the environment during training, offering advantages in quicker training time and the ability to adapt to changing conditions in real-time, thereby holding promise for applications in dynamic and uncertain environments.\nThough on-policy algorithms show great performance, it has the critical problem of sample inefficiency. The policy can only be optimized using experiences collected by the latest policy network, which leads to longer training time and higher variance. Though several approaches, such as constraining the policy to learn in small steps (Schulman et al. [2015]) or using entropy regularization (Haarnoja et al. [2018]), were suggested to mitigate this issue, on-policy algorithms are still more likely to converge to a local optima than off-policy algorithms. Also, current on-policy reinforcement learning ignores the distribution of rewards in training, which potentially limits its ability to capture the full spectrum of uncertainty and variability inherent in complex environments.\nTo overcome these drawbacks, we propose a method that uses an off-policy distributional reinforcement learning algorithm that provides the policy network with information about the reward distribution. While distributional reinforcement learning has been integrated with various off-policy value function methods (Hessel et al. [2017]), to the best of our knowledge, the utilization of this approach in conjunction with on-policy algorithms remains relatively unexplored. By using distributional reinforcement learning, we alleviate the sample inefficiency inherent in on-policy algorithms by utilizing experiences that would have been discarded to train an off-policy algorithm, thereby enhancing data efficiency and potentially improving learning efficiency in reinforcement learning tasks.\nIn this paper we introduce a novel reinforcement learning algorithm called PG-Rainbow that distills information produced by distributional reinforcement learning [Bellemare et al., 2017] to an on-policy algorithm, PPO (Schulman et al. [2017]). By feeding the reward distribution information into the policy network, we anticipate that the policy agent will take into account of state-action value distribution when making action selection decisions in any given state.\nThe main contributions of our paper is as follows:\n1. We explore the field of integrating distributional reinforcement learning algorithm into on-policy reinforcement learning.\n2. We alleviate the sample inefficiency problem of on-policy algorithms by storing experiences collected to train an off-policy distributional reinforcement learning algorithm."}, {"title": "Related Work", "content": "Consider an environment modeled as a Markov Decision Process (MDP), denoted (S, A, R, P, \u03b3), where S represents the state space, A represents the action space, \\(R: S \\rightarrow \\mathbb{R}\\) represents the rewards function, \\(P : S \\times A \\times S \\rightarrow \\mathbb{R}\\) is the transition probability function and \u03b3 \u2208 [0, 1] a discount value. A policy \u03c0(s) is a function that maps an arbitrary state s to a distribution of available actions. Denote \\(v_\\pi(s)\\) as the expected return of an agent being positioned in state s and following \u03c0.\nThe goal of a reinforcement learning agent is to maximize its cumulative rewards attained over the course of an episode, where an episode is essentially a trajectory of states and actions that the agent went through until the agent has arrived at a terminal state. Hence, the agent is given the task of maximizing the return \\(\\Sigma_{\\tau_0} \\gamma^t R(s_t, a_t)\\), which is the discounted sum of all future rewards starting from the initial state s0. For an agent following policy \u03c0, the action-value function is defined as \\(Q^\\pi(s, a) = E[\\Sigma_{t=0}^{\\infty} \\gamma^t R(s_t, a_t)]\\) and the Bellman equation is defined as:\n\\(Q^\\pi(s, a) = E[R(s, a)] + \\gamma E[Q^\\pi(s', a')]\\)\nTo find an optimal policy \u03c0*, Bellman [1957] introduced the Bellman optimality operator, where the optimal policy can be obtained by iteratively improving the action-value function (Watkins [1989]). In essence, the policy is optimized by accurately approximating the state-action value function, and the policy selects the action that returns the highest action-value \\( \\pi^*(s) = arg \\max_a Q^{**}(s, a)\\)."}, {"title": "Policy Gradient Methods", "content": "In policy gradient methods, the agent attempts to maximize the return by learning a parameterized policy \u03c0\u03b8 that can select the optimal action given the current state. The parameter is updated using a gradient estimator \u011d, where a is a step size (Sutton and Barto [2018]).\n\\( \\theta \\leftarrow \\theta + \\alpha \\hat{g} \\)\nThe estimator is computed by differentiating the objective function L(\u03b8). The objective function takes the following form, where At is the advantage function at timestep t.\n\\( L(\\theta) = E[\\log \\pi_\\theta(a_t | s_t)A_t] \\)\nHowever, policy gradient methods are more prone to irreversible huge gradient steps, which may lead to a terrible new policy \u03c0new. Hence, Trust Region Policy Optimization (TRPO) suggests a constraint to the policy parameter update to prevent extensive changes to policy parameters (Schulman et al. [2015]). Nevertheless, TRPO deals with second order optimizations, which adds complications.\nProximal Policy Optimization (PPO) introduces the constraint by using a clip function that sets a upper and lower bound for the gradient step (Schulman et al. [2017]). In PPO, the loss function is"}, {"title": "Distributional Reinfocement Learning", "content": "Distributional reinforcement learning differs from traditional reinforcement learning by considering the return distribution of an environment. Formal reinforcement learning assumes that the return generated by taking an action a in state s is the expected return. However, the reward distribution is rarely a normal distribution, and taking a simple expected value is not sufficient to capture the possible mutli-modality of the reward distribution (Bellemare et al. [2023]).\nDistributional reinforcement learning incorporates the reward distribution by using a distributional Bellman equation defined as:\n\\( Z(s, a) \\overset{d}{=} R(s, a) + \\gamma Z(S', A') \\)\nwhere the state-action value Q(s, a) is replaced by value distribution Z(s, a). The distributional Bellman optimality operator is then defined as:\n\\( TZ(s, a) := R(s, a) + \\gamma Z(S', arg \\max_{a' \\in A} E[Z(S', a')]) \\)\nBellemare et al. [2017] proved that the distributional Bellman operator for policy evaluation and control to be a contraction in the p-Wasserstein distance and proposed the C51 algorithm, where the reward distribution is projected over a fixed set of equidistant values. However, the C51 algorithm does not strictly follow the contraction theory, as it minimizes the KL divergence between the current and target distributions, instead of using the Wasserstein distance.\nTo address the disparity between theory and practical applications, methodologies employing quantile regression have been developed. It is shown that using quantile regression in distributional reinforcement learning makes the projected distributional Bellman operator a contraction in the \u221e-Wasserstein metric (Dabney et al. [2017]).\nThe Implicit Quantile Network (IQN) follows the quantile regression approach, where it randomly samples the quantile values to learn an implicit representation of the return distribution. Let \\( F_Z^{-1}(\\tau) \\) be the quantile function at \u03c4\u2208 [0, 1], where 7 is a random sample from the base reward distribution. IQN models the state-action quantile function as a mapping from state-actions and samples from a base distribution to Z\u03c4(s, a). For two samples \u03c4, \u03c4' \u2208 U([0, 1]) and policy \u03c0\u03b2, the sampled TD error at step t is defined as:\n\\( \\delta_{t}^{\\tau, \\tau'} = r_t + Z_{\\tau'}(s_{t+1}, \\pi_\\beta(s_{t+1})) - Z_{\\tau}(s_t, a_t) \\)"}, {"title": "PG-Rainbow", "content": "Though PPG and DCPG improve the vanilla PPO algorithm by tuning the value function, they still fall short in fully leveraging the distributional nature of the environment and the inherent uncertainty in reinforcement learning tasks. Realizing the distributional nature of returns in an environment is crucial for a better understanding of the environment, as the underlying reward distribution is rarely in the form of a normal distribution. In other words, it is important for the policy agent to take into account of the multi-modality of the reward distribution per action while seeking the optimal policy. However, current policy gradient methods fall short in incorporating the multi-modal distribution of the environment, as the advantage function used in the objective function is calculated with a single scalar value V(s).\nTo show how current policy gradient methods lack in capturing the reward distribution while training, we conduct a simple experiment where we sample 100,000 trajectories from a PPO agent and plot the"}, {"title": "Analyzing Multi-Modality of Rewards", "content": "distribution of V(so) values. We also sample 25,000 trajectories where ao is fixed to NO-OP in order to plot the distribution of Q(s, ao) values. Figure 2 illustrates the results of this experiment."}, {"title": "Utilizing a distillation network", "content": "Knowledge distillation is the process of transferring knowledge of a large teacher model to a relatively smaller child model. The transfer of knowledge may take different forms, from directly tuning the child model's network parameters to match the teacher's network parameters, to training the child model such that the model output matches that of the teacher's (Hinton et al. [2015]). Though PG-Rainbow is not a conventional application of knowledge distillation, we use the intuition behind the method such that the distributional reinforcement model transfers the value distribution information to the policy gradient model.\nWe propose that by incorporating an additional neural network, termed the distillation network, relevant value distribution information can be integrated into the value network of the policy gradient model, thereby enhancing the model's ability to capture the distributional characteristics of the environment. Employing a separate neural network for the distillation process is crucial, as the objective of the distillation network is to formulate a new value function that leverages information from both the policy gradient method and a distributional reinforcement learning method. This approach differs from model ensemble methods where the outputs of distinct models are merged to produce the final output."}, {"title": "Algorithm", "content": "To address the issue of potential suboptimality arising from relying solely on the value function, we propose a novel method called PG-Rainbow that leverages a distillation network to incorporate quantile distribution information from the Implicit Quantile Network (IQN) into the Proximal Policy Optimization (PPO) value function head. This method enhances the value function by integrating the rich distributional information provided by IQN, capturing the variability and uncertainty in the expected returns for different actions. Consequently, the value function reflects not just the mean expected return, but also the spread and quantiles of the return distribution, offering a comprehensive understanding of potential outcomes and addressing issues highlighted in Section 2.2. By distilling quantile-based insights from IQN into the PPO framework, PG-Rainbow aims to improve the accuracy and robustness of policy evaluations, aligning learned policies more closely with true reward distributions. This approach mitigates the risk of suboptimal policy decisions arising from average-based evaluations and leverages the strengths of both distributional reinforcement learning and policy optimization techniques. We anticipate that PG-Rainbow will significantly enhance both the robustness and performance of agents across various tasks, demonstrating the efficacy of integrating distributional reinforcement learning with policy gradient methods to provide a nuanced and detailed representation of the value landscape."}, {"title": "Experiments and Results", "content": "We evaluate the performance of PG-Rainbow in the Atari-2600 game suite, simulated via the Arcade Learning Environment (ALE) (Bellemare et al. [2012]) and follow the practice introduced by Machado et al. [2017] for preprocessing the game environment. We train our agent for 1 million frames and measure the average and standard deviation of returns. The performance is compared with the vanilla PPO algorithm. To ensure faster training time, we use an A2C-style (Mnih et al. [2016]) approach to initialize 8 environments in parallel to collect experiences. We use a batch size of 32 to train the models. Note that hyperparameters and model architecture shared between PPO and PG-Rainbow, such as batch size or policy update epochs, are identical. The list of key hyperparameters used in this experiment is shown in Table 1."}, {"title": "Implementation Details", "content": "We first compare the performance of PG-Rainbow with the PPO algorithm, shown in Figure 4. PG-Rainbow outperforms the PPO algorithm in most environments in terms of episodic returns for Atari environments. To prove the efficacy of integrating quantile information into the PPO algorithm, we undertake a series of experiments to elucidate the influence of adjustments made to the distillation network or the distributional reinforcement learning aspect on the performance of our model. First, we compare our PG-Rainbow algorithm with a delayed version, in which the quantile information is distilled to the critic network only after the 500,000th timestep. Note that delaying the distillation process does not mean that the IQN model isn't trained, but that the distributional information is not fed forward into the distillation network 4. Figure 5 shows the results of this experiment, where it is shown that when the quantile information is distilled to the PPO agent in a delayed manner, there is a significant degradation in performance. Next, we conduct the same experiment as in Section 3.1, where we plot the reward distribution of PG-Rainbow. We sample trajectories from the DemonAttack environment with the trained PG-Rainbow agent. The distribution of returns between the V(s0) values and Q(s, ao), where ao is the action NO-OP, is shown in Figure 6. Notably, the value function now captures the distributions of returns across actions more accurately than in our previous analysis of the regular PPO. The skewness in the return distributions is more accurately reflected in the value function of PG-Rainbow and it even captures the spike in return values within the 200-300 range. This is a significant improvement over regular PPO, where the value function primarily captures average"}, {"title": "Results", "content": "expected returns without adequately reflecting the spread and variability of action-specific returns. The enhanced fidelity in capturing return distributions demonstrates the efficacy of incorporating quantile information through the distillation network. Consequently, we postulate that this integration leads to better-informed policy decisions and improved overall performance of the model. We also experiment how the change in number of epochs for the PPO update within PG-Rainbow affects training, with all other hyperparameters equal. The results are shown in Figure 7. It is noteworthy that diminishing the number of epochs allocat ed for PPO training yields an enhancement in the algorithm's performance. We postulate that training with more epochs and using more samples does not necessarily improve the performance of the policy gradient algorithm. Instead of solely focusing on increasing the number of epochs, our findings suggest that the pivotal factor for enhancing the performance of an actor-critic style policy gradient lies in training a more accurate value function. This is achieved through the utilization of a distillation network in the case of PG-Rainbow. This assertion is consistent with the observations made by Cobbe et al. [2020], wherein it was demonstrated that PPO derives advantages from increased sample reuse primarily due to the supplementary epochs facilitating additional training of the value function.\nTo further support this claim, we train PG-Rainbow by varying the number of quantiles the IQN model provides to the distillation network. The results are shown in Figure 8. It was shown that increasing the number of quantiles enables the IQN model to capture a more comprehensive representation of the distribution of rewards, thereby resulting in improved performance (Dabney et al. [2018]). Our"}, {"title": "Conclusion", "content": "In this work we proposed a novel method to incorporate distributional reinforcement learning with policy gradient methods. We have shown that by providing the value distribution information to the critic network of the policy gradient algorithm, the agent can make better informed actions, leading to higher episodic returns. By storing experiences gathered by the PPO network, we can utilize them to train the IQN network, alleviating the sample inefficiency problem of policy gradient methods. It is also worthwhile to note that our method PG-Rainbow incorporates both off-policy algorithms with on-policy reinforcement learning algorithms. To the best of our knowledge, few attempts have been made to formulate a reinforcement learning model where the strengths of both fields are combined. We hope that our work provides the foundation for further research in incorporating complementary methodologies from both off-policy and on-policy reinforcement learning paradigms, fostering the development of more robust and versatile approaches to solving complex decision-making problems.\nOur work can be further improved and pave way for further research in the following domains. First, while using the IQN network represents a significant advancement, its inherent limitation lies in its applicability only to discrete action spaces. However, many real-world scenarios in fields such as robotics, healthcare, and finance demand efficient algorithms capable of operating in continuous action spaces. Thus, future research efforts could focus on extending and adapting the principles of implicit quantile networks to address this crucial requirement, enhancing their applicability and effectiveness in a broader range of domains. Second, our model architecture can be further fine-tuned"}, {"title": "", "content": "As introduced in Section 3.3, the input to the distillation network has been implemented as a pointwise multiplication of the scalar value function output from PPO and the quantile values from IQN. However, it is important to note that this is not the only viable method for integrating these inputs. The distillation network's input can be adapted in various forms, provided it allows the network to learn the relationship between the value function output and the quantile values effectively. The critical requirement for any input form is its structural form such that the distillation network can uncover and leverage the intricate relationships between the PPO value function and the distributional outputs from IQN. We test the following different methods of computing inputs to the distillation network.\n1. Concatenate the critic output from the PPO model with the quantile values.\n\\(Concat(V_\\theta(s), q_\\phi(s))\\)\n2. Average the critic output from the PPO model with the quantile values.\n\\( \\frac{1}{2} (V_\\theta(s) + \\frac{1}{32} \\sum_{i=1}^{32} q_{\\phi, i}(s)) \\)\n3. Compute the difference between the critic output and the mean of quantile values weighted by each action probabilities.\n\\( \\pi_{\\theta}(\\cdot|s) \\times q_\\phi(s) - V_\\theta(s) \\)\n4. Feed the critic output and quatile values to a Bilinear layer before the distillation network.\n\\( Bilinear(V_\\theta(s), q_\\phi(s)) \\)\nNote that for method 3, we add the critic value function output, V\u03b8(s), with the distillation network output to derive the final value function output. The purpose of the distillation network in method 3 is to compute the difference between the quantile values scaled by the action-value probabilities provided the PPO model with the value function output of the PPO model. The difference is then added to the critic value function output, hence adjusting the value estimates based on the distributional information from IQN. The results are shown in Figure 9. All the methods exceed the performance of the PPO model, with some methods even surpassing the performance of the originally proposed PG-Rainbow. However, our experiments show that the originally proposed PG-Rainbow method ensures the best stability, as other methods tend to have varying performance across environments. By exploring and optimizing these various input forms in future research, the integration process can be made more robust and effective, potentially leading to improved performance and stability."}]}