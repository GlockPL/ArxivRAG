{"title": "Quantum automated learning with provable and explainable trainability", "authors": ["Qi Ye", "Shuangyue Geng", "Zizhao Han", "Weikang Li", "L.-M. Duan", "Dong-Ling Deng"], "abstract": "Machine learning is widely believed to be one of the most promising practical applications of quantum computing. Existing quantum machine learning schemes typically employ a quantum-classical hybrid approach that relies crucially on gradients of model parameters. Such an approach lacks provable convergence to global minima and will become infeasible as quantum learning models scale up. Here, we introduce quantum automated learning, where no variational parameter is involved and the training process is converted to quantum state preparation. In particular, we encode training data into unitary operations and iteratively evolve a random initial state under these unitaries and their inverses, with a target-oriented perturbation towards higher prediction accuracy sandwiched in between. Under reasonable assumptions, we rigorously prove that the evolution converges exponentially to the desired state corresponding to the global minimum of the loss function. We show that such a training process can be understood from the perspective of preparing quantum states by imaginary time evolution, where the data-encoded unitaries together with target-oriented perturbations would train the quantum learning model in an automated fashion. We further prove that the introduced quantum automated learning paradigm features good generalization ability with the generalization error upper bounded by the ratio between a logarithmic function of the Hilbert space dimension and the number of training samples. In addition, we carry out extensive numerical simulations on real-life images and quantum data to demonstrate the effectiveness of our approach and validate the assumptions. Our results establish an unconventional quantum learning strategy that is gradient-free with provable and explainable trainability, which would be crucial for large-scale practical applications of quantum computing in machine learning scenarios.", "sections": [{"title": "Introduction", "content": "Machine learning, the core of artificial intelligence (AI), has achieved dramatic success [1-3]. A number of long-standing challenging problems, such as playing the game of Go [4, 5], predicting protein structures [6], and automated theorem proving at the olympiad level [7], have been cracked in recent years, elevating AI to new scientific heights. Yet, as Moore's law is approaching the end and machine learning models become unprecedentedly large devouring a tremendous amount of resources, further development of AI would be subject to the limitations of computational power and energy consumption [8]. Quantum computing promises a potential way out of this dilemma.\nIndeed, parallel to machine learning, the field of quantum computing has also made remarkable progress in the past decades [9], with experimental demonstrations of quantum supremacy [10-12] and error correction [13, 14] marked as the latest milestones. A variety of quantum algorithms have been proposed to enhance, speed up or innovate machine learning [15, 16], giving rise to a new vibrant research frontier of quantum machine learning [17\u201319]. Proof-of-principle experiments have been reported with current noisy intermediate-scale quantum devices [20-26], including these on quantum convolutional networks [20], reinforcement learning [21], adversarial learning [22], federated learning [27], and continual learning [28]. Most of these quantum learning schemes rely on parametrized quantum circuits and utilize gradient-based approaches for training [29]. They face three major difficulties when scaling up. First, in classical deep learning the gradients can be calculated in an efficient and parallel way by the backpropagation algorithm [30]. Whereas, in quantum scenarios the implementation of the backpropagation algorithm is resource demanding [31, 32] and quantum gradients are usually obtained one-by-one for each variational parameter [29, 33]. This renders large-scale quantum learning impractical, especially for models as large as GPT-4 [34] and GLaM [35] with up to trillions of parameters. Second, quantum landscapes can have exponentially many local minima [36] and exhibit the notorious barren plateau phenomenon [37\u201339], where the gradients vanish exponentially with the system size. Consequently, the gradient-based training of variational quantum learning models becomes intrinsically hard as they scale up. Third, there lacks a clear understanding of the gradient-based training dynamics to guide further development of large-scale quantum learning models.\nIn this paper, we circumvent these difficulties by introducing quantum automated learning (QAL) with provable and explainable trainability. Our approach involves no variational parameter and thus is inherently gradient-free and scalable (see Fig. 1). Specifically, we focus our discussion on supervised learning with pre-labeled training data. We convert the training process into quantum state preparation for a target state, on which the outcome of a data-encoded measurement gives the predicted label of the data sample. To prepare the desired state, we design a dissipation process for each training sample that drives a random initial state towards the target state in an automated manner. We prove that such a dissipation process converges exponentially to the target state corresponding to the global minimum of the loss function. We show that this training process has a clear physical interpretation: it essentially implements an imaginary time evolution that cools down the system to the ground state of a"}, {"title": "Quantum automated learning", "content": "We first introduce the general framework for quantum automated learning. For convenience, we focus our discussion on classification tasks in the setting of supervised learning [42], where we assign a label $y(x) \\in Y$ to an input data sample $x \\in X$, with $Y$ being a finite label set of size $k$ and $X$ the set of all possible samples. We denote the training set as $S_N = {(x_1, y_1), \\dots, (x_N, y_N)}$, where $x_i \\in X$ is sampled from an unknown distribution $D$, $y_i \\in Y$ represents the label of $x_i$, and $N$ is the size of the training set. The goal of classification is to predict the labels of unseen samples drawn from $D$ with high probability.\nA widely studied strategy in solving classification problems relies on quantum neural networks [43, 44], where the input"}, {"title": "Provable convergence and physical explanation", "content": "We now show that the training process of the QAL scheme introduced above can be understood from the perspective of preparing quantum states through imaginary time evolution and it converges exponentially to the global minimum of a naturally defined loss function. To this end, we define a Hamiltonian $H_x = I - U(x)^\\dagger \\Pi_{y(x)} U(x)$, where $\\Pi_{y(x)}$ denotes the measurement projection corresponding to the state encoding the label $y(x)$. The probability of correct prediction reads $\\langle \\psi | U(x)^\\dagger \\Pi_{y(x)} U(x) | \\psi \\rangle = 1 - \\langle \\psi | H_x | \\psi \\rangle$. A direct calculation shows that the training step (iii) effectively updates the state according to (Supplementary Sec. II E):\n$\\psi \\leftarrow \\frac{(I - \\eta H_x) | \\psi \\rangle}{|| (I - \\eta H_x) | \\psi ||},$ (1)\nwhere $|| (I - \\eta H_x) | \\psi ||$ is a normalization factor whose square gives the success probability of post-selection. We define a loss function as the average failure probability to predict the label of a random training datum: $R_S(\\psi) = E_{x \\sim S} {\\langle \\psi | H_x | \\psi \\rangle} = \\langle \\psi | H_S | \\psi \\rangle$, where $E_{x \\sim S}$ denotes the expectation with $x \\sim S$ meaning $x$ is uniformly sampled from"}, {"title": "A subtle trade-off", "content": "In the QAL protocol, we exploit post-selection to implement the desired data-dependent dissipation during the training process. This gives rise to a concern that the overall success probability may decay exponentially with the number of steps. In this section, we show that there is a subtle trade-off between the prediction accuracy and the overall post-selection success probability: we prove that, as long as $\\rho_0$ has a constant overlap with the low energy eigenspace of $H_S$, by choosing an appropriate $\\beta$, we can achieve a near-optimal training loss with a constant success probability. We have the following theorem as proved in Supplementary Sec. III D:\nTheorem 2. Assume the spectrum of $H_S$ has a heavy tail, that is, it has a constant proportion of low-energy eigenstates. With a random initial state $\\rho_0$ in the computational basis, an appropriate learning rate $\\eta$, and an appropriate number of steps $T$, the final state $\\rho_\\beta$ achieves a near-optimal training loss with a constant success probability.\nWe remark that the heavy-tail assumption is reasonable in machine learning scenarios\u2014it stems from the fact that data samples with the same label should bear similar data structures and thus correspond to similar Hamiltonians. Consider a simple example of classifying images of dogs and cats, where all dogs look similar and all cats look similar. The Hamiltonian $H_S$ is approximately a mixture of two projectors of dimensions $2^{n-1}$, $H_{\\text{dogs}}$ and $H_{\\text{cats}}$. Regarding $H_{\\text{dogs}}$ and $H_{\\text{cats}}$ as random projectors, then $H_S$ has a constant proportion of near-zero eigenvalues, thus has a heavy tail. We note that heavy-tail assumption does not hold for typical Hamiltonians encountered in quantum physics, which in general exhibit exponential concentration of energy levels around the middle spectra."}, {"title": "Bounded generalization error", "content": "The above discussions have established a provable and explainable trainability for the QAL protocol. Yet, its generalization ability remains uncharted: it is unclear whether the good performance on the training dataset can be generalized to unseen data samples. We address this crucial issue in this section by proving an upper bound for the generalization error."}, {"title": "State reusability", "content": "In classical machine learning or conventional gradient-based quantum learning models, variational parameters are usually stored in a classical computer and they can be copied and reused on demand during the inference stage. In contrast, in the QAL approach what we obtain after training is a particular quantum state that carries information about the training samples. Due to the quantum no-cloning theorem [48], this state cannot be copied. This, together with the fact that the quantum measurements involved are destructive, results in a possible drawback of the QAL approach: it seems that one needs to carry out repeatedly the whole training process to prepare the desired state for each run of the inference. Fortunately, this is not the case.\nThere are two complementary ways to solve this problem. First, we can incorporate quantum shadow tomography techniques [51] into step (v) of the QAL protocol. This will substantially reduce the number of copies of the desired states during inference: polylog($N_p$) copies would be sufficient for classifying $N_p$ unseen data samples. Second, one may reuse the state after the measurements in step (v). Theoretically, the state after training $|\\psi^*\\rangle$ would output correct labels for unseen sample $x$ with high probability, as proved above that the QAL process converges to the global minimum with bounded generalization error. Hence, the measurement {$U(x)^\\dagger \\Pi_{y(x)} U(x)$}$_y$ is in fact a gentle measurement that would not disturb $|\\psi^*\\rangle$ too much. Although the post-measurement state is different from $|\\psi^*\\rangle$, a few more training steps would recover the system to a state with high prediction accuracy. This reusability of states is verified by extensive numerical simulations on different real-life datasets (Fig. 2f). One can combine shadow tomography with state reusability to further reduce the resources required during the inference step."}, {"title": "Numerical results", "content": "To further illustrate and benchmark the effectiveness of QAL, we carry out extensive numerical simulations on a range of datasets, including real-life images (e.g., hand-writing digit images) and quantum data (e.g., thermal and localized quantum many-body states). In Fig. 2, we plot our numerical results for classifying images in the Fashion MNIST dataset [41]. Fig. 2a shows some samples from the dataset and the encoding scheme used. Fig. 2b plots the training and testing accuracy versus training iterations, from which it is clear that the accuracy increases rapidly and then saturates at a high value of 0.99. In addition, we find that this approach is surprisingly efficient in the sense that one only need to run the experiment very few times to achieve a relatively high accuracy. From Fig. 2b, the accuracy for 29 trials is already very close to that for infinite trials. In Fig. 2c, we plot the training"}, {"title": "Discussion and outlook", "content": "The introduced QAL protocol is gradient-free and thus escapes the barren plateau problem inherently. It features a number of striking merits such as provable and explainable trainability with bounded generalization error. Yet, several questions of fundamental importance remain unsolved. First, in conventional gradient-based approaches, quantum neural networks are shown to possess universal representation power. They can approximate an arbitrary function to arbitrary accuracy as long as the number of variational parameters is large enough [53]. In contrast, the QAL protocol involves no variational parameter and its representation power depends on particular encoding schemes. Intuitively, one can always exploit different encoding schemes to approximate an arbitrary function. However, a rigorous proof of the universal representation power for the QAL protocol is technically challenging and remains unknown. Second, the demonstration of quantum advantages is a long-sought-after goal in the field of quantum machine learning. It would be interesting and important to prove in theory and demonstrate in experimental quantum"}, {"title": "Methods", "content": "Implementation of the target-oriented perturbation\nDuring the training process, the state $|\\psi\\rangle$ is updated in step (iii) by a non-unitary perturbation $M_y = |y\\rangle\\langle y|+ (1 - \\eta) (I - |y\\rangle\\langle y|)$. Such a perturbation has a clear physical meaning of suppressing (enhancing) the probability of incorrect (correct) prediction, hence evolving the state $|\\psi\\rangle$ towards the target state $|\\psi^*\\rangle$. This is similar to the Hamiltonian-echo-backprogagation based self-learning approach [54] for classical learning, where a small error signal is injected on top of the evaluation field to guide the training process. To implement $M_y$, we consider adding an ancillary qubit and embedding $M_y$ into a unitary $U_y$:\n$U_y = M_y \\otimes Z + \\sqrt{I - M_y^2} \\otimes X,$ (4)\nwhere $Z$ and $X$ are the Pauli matrices acting on the ancillary qubit. We initialize the ancillary qubit to state $|0\\rangle$ and then apply $U_y$ to $|\\psi\\rangle |0\\rangle$. Noting that $(I\\otimes \\langle 0|)U_y(I\\otimes |0\\rangle) = M_y$, the state $|\\psi\\rangle$ will be updated by $M_y$ after we measure the ancillary qubit in the computational basis and post-select the outcome 0. In this way, we effectively implement the non-unitary perturbation $M_y$ by adding an ancillary qubit combined with post-selection.\nIt is worthwhile to remark that $U_y$ can be implemented with elementary gates very efficiently. For a $k$-class classification task, $M_y$ acts on $[\\log k]$ qubits, where $[\\cdot]$ denotes the ceiling function that outputs the least integer greater than or equal to the input. $U_y$ acts on $[\\log k] + 1$ qubits and is a multi-controlled gate up to single-qubit gates. It can be efficiently compiled into elementary gates with gate complexity scaling linearly in $\\log k$ (See Supplementary Sec II C). We note that such a gate complexity scaling is much better than that for compiling a general unitary with the Solovay-Kitaev algorithm [46], where a $2^{O(\\log k)} = \\text{poly}(k)$ gate complexity is required for a given accuracy.\nData encoding\nFor classical classification, the data sample can be represented as a vector $x$ (for example, the pixel information of an image). We encode $x$ into a quantum circuit consisting of $[\\frac{l}{3n}]$ blocks with similar structure, where $l$ denotes the dimension"}]}