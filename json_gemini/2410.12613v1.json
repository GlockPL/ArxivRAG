{"title": "EXPLORING MODEL KINSHIP FOR MERGING LARGE\nLANGUAGE MODELS", "authors": ["Yedi Hu", "Yunzhi Yao", "Ningyu Zhang", "Shumin Deng", "Huajun Chen"], "abstract": "Model merging has become one of the key technologies for enhancing the capa-\nbilities and efficiency of Large Language Models (LLMs). However, our under-\nstanding of the expected performance gains and principles when merging any two\nmodels remains limited. In this work, we introduce model kinship, the degree of\nsimilarity or relatedness between LLMs, analogous to biological evolution. With\ncomprehensive empirical analysis, we find that there is a certain relationship be-\ntween model kinship and the performance gains after model merging, which can\nhelp guide our selection of candidate models. Inspired by this, we propose a new\nmodel merging strategy: Top-k Greedy Merging with Model Kinship, which can\nyield better performance on benchmark datasets. Specifically, we discover that\nusing model kinship as a criterion can assist us in continuously performing model\nmerging, alleviating the degradation (local optima) in model evolution, whereas\nmodel kinship can serve as a guide to escape these traps\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "Fine-tuning pre-trained models (PTMs) for downstream tasks has become a popular practice, partic-\nlarly demonstrating significant effectiveness in Large Language Models (LLMs) (Kolesnikov et al.,\n2020; Qiu et al., 2020; Askell et al., 2021; Ouyang et al., 2022; Zhao et al., 2023). However, deploy-\ning separate fine-tuned models for each task can be resource-intensive (Fifty et al., 2021), which\ndrives the increasing demand for multitask learning solutions (Zhang & Yang, 2022; Lu et al., 2024;\nLiu et al., 2024). Recent studies suggest that model merging (Singh & Jaggi, 2020; Sung et al.,\n2023; Goddard et al., 2024; Matena & Raffel, 2022; Yang et al., 2024a) offers a viable approach for\nachieving multitask objectives by integrating multiple expert models. Furthermore, advancements in\nmodel merging toolkits (Goddard et al., 2024; Tang et al., 2024) enable users with limited expertise\nto easily conduct merging experiments, leading to an evolution of LLMs for the community.\nTo date, through model merging techniques, resercheres have developed many more powerful LLMs\nthrough iterative model merging (Beeching et al., 2023), and to some extent, achieved model evo-\nlution (Figure 1(c)). Despite these successes, progress has predominantly relied on trial and error,\nalong with extensive human expertise, but lacks formalized guidance and standardized procedures.\nAs the merging iterations progress, achieving further generalization gains becomes increasingly\nchallenging (More details in Section 3). For example, as shown in Figure 1, model merging often\nresembles the process of hybrid evolution in biology, where the next generation may not show\nsignificant improvements or may even regress, highlighting the imperative for a deeper exploration\nof the underlying mechanisms driving these advancements.\nTo address this, we introduce model kinship, a metric inspired by the concept of kinship (Sahlins,\n2013) from evolutionary biology (Figure 1(a)). This metric is designed to estimate the degree of\nsimilarity or relatedness between LLMs during the iterative model merging process, offering in-\nsights intended to enhance the effectiveness of the merging strategy. We utilize the model kinship to\nconduct a comprehensive analysis of model merging experiments from two perspectives: the over-\nall merging process, including various independent merge experiments and the evolution path of\nspecific models, demostrating the complete merging trajectory."}, {"title": "2 BACKGROUND", "content": "Model merging aims to integrate two or more domain-specific models into a unified framework,\nthereby harnessing their compositive capabilities across multiple tasks (Sung et al., 2023). While\nthis approach shares conceptual similarities with ensemble methods (Dietterich et al., 2002; Dong\net al., 2020; Jiang et al., 2023b), model merging generates a single, generalized model, avoiding the\nincreased inference time associated with ensembles. Let $f_i$ represent the i-th model for merging,\neach with its unique parameters $\\theta_i$. If the merging process follows method $F$, the prediction $\\hat{y}$ of the\nmerged model $f_{merge}$ for input $x$ is:\n$\\hat{y} = f_{merge}(x) = F (f_1(x; \\theta_1), f_2(x; \\theta_2), ..., f_n(x; \\theta_n))$                                                                                              (1)\nWhen employing parameter averaging methods, the merged model preserves the same architecture\nand parameter size as the original models, enabling it to be reused in future merging processes\u00b2. This\nmakes it feasible to iteratively enhance the model through repeated applications of model merging.\nIn this paper, we refer to this process as \"Model Evolution\". Empirical evidence on open LLM\nleaderboard (Beeching et al., 2023) demonstrates that model evolution can successfully produce\nhighly generalized models, often outperforming their original ones (Maxime Labonne, 2024)."}, {"title": "2.2 MODEL KINSHIP", "content": "Intuitively, understanding the task capabilities of models is crucial for effective model merging.\nHowever, the black-box nature of LLMs presents significant challenges in comprehending their\nadvantages Shi et al. (2024) and task-specific capabilities. For fine-tuned models, task benchmark\nresults (Gao et al., 2024; Li et al., 2023c) and the characteristics of the fine-tuning datasets are often\nused as indicators of task-related knowledge. Yet, these metrics often fall short in comprehensive\ncomparisons. The internal relationships between tasks and datasets can arise confusion, particularly\nin complex merging scenarios that involve multiple tasks.\nMotivated by the parallel drawn between biological reproduction and the process of model evolu-\ntion (as seen in Figure 1), we propose that the concept of kinship, which is central to evolutionary\nbiology for understanding breeding relationships and human genealogies (Thompson, 1985), can be\nconceptually extended to the field of model merging, specifically to describe the kinship between\nmodels. To be noted, Ilharco et al. (2023) has proposed Task Arithmetic and offered new insights\ninto how the merging process preserves and blends task information from the original models. (II-\nharco et al., 2023) propose to use task vectors, defined as the differences in weight space between\nfine-tuned models and a pre-trained model, and demonstrate that arithmetic operations on these task\nvectors can serve as an effective method for model merging.\nInspired by this idea, we introduce the concept of Model Kinship, a metric designed to assess the\ndegree of similarity or relatedness between large language models (LLMs) based on their \"genetic\"\ninformation (a.k.a. the changes in weights during model evolution). Considering two target models\ninvolved in a single merge, the weights of each target model are denoted as $\\Theta_{target} \\in \\mathbb{R}^{d}$. In this\npaper, it is significant to claim that a target model may either be a fine-tuned model or a previously\nmerged model. Similarly, $\\Theta_{base} \\in \\mathbb{R}^{d}$ represents the weights of the base model from which the two\ntarget models are derived. The difference of weights $\\delta_{target}$ between the target model and the base\nmodel is denoted as:\n$\\delta_{target} = \\Theta_{target} - \\Theta_{base}$.\nFor a model merging process where models have been fine-tuned or merged from the same base\nmodel, the model kinship $r$ can be defined using the similarity metric function $sim(\\cdot, \\cdot)$, which\nmeasures the similarity between two model weight differences. For n models, we formally define\nthe model kinship $r$ as:"}, {"title": "3 PRELIMINARY ANALYSIS OF MODEL KINSHIP", "content": "We begin with a preliminary analysis of model kinship with open-sourced LLMs from the commu-\nnity to better understand model evolution."}, {"title": "3.1 EVALUATION METRICS", "content": "Let T be the set of tasks in the task group, where T = {$T_1, T_2, . . . , T_n$}. Each task $T_i$ in the set T is\nassociated with a performance measure $P_i$ for the LLM. For a multitask objective, the Average Task\nPerformance (Avg.) P is calculated using the equation:\n$\\overline{P} = \\frac{1}{n}\\sum_{i=1}^{n} P_i$                                                                                                                               (2)\nTo evaluate the effectiveness of a single merge, we propose the merge gain metric. Assume we\nhave two models $m_{pre-1}$ and $m_{pre-2}$ and their average task performance are $P_{pre-1}$ and $P_{pre-2}$,\nintuitively, we believe the $P_{merged}$ lie around the mean of $P_{pre-1}$ and $P_{pre-2}$. The merge gain is\ncalculated as the difference of $P_{merged}$ from the mean value of $P_{pre-1}$ and $P_{pre-2}$. For a merging\nrecipe with k models, the merge gain is:\n$Gain = P_{merged} - \\frac{1}{k}\\sum_{i=1}^{k} P_{pre-i}$                                                                                                       (3)\nIn the following analysis, we use the task group T = {ARC, HellaSwag, MMLU, TruthfulQA, Wino-\ngrande, GSM8K}. All models are either fine-tuned or merged from the Mistral-7B architecture."}, {"title": "3.2 CORRELATION ANALYSIS OF MODEL KINSHIP AND PERFORMANCE GAIN", "content": "In this analysis, we examine the distribution\nof merge gain and model kinship based on\nPearson Correlation Coefficient (PCC), Cosine\nSimilarity (CS) and Euclidean Distance (ED)\nin open-sourced LLMs, originating from the\nMistral-7B (Jiang et al., 2023a). Those mod-\nels are obtained from the HuggingFace, with as-\nsistance from the Open LLM Leaderboard (De-\ntails in Appendix A.)."}, {"title": "3.2.1 RESULTS", "content": "Figure 2 illustrates the distribution of model\nkinship based on three similarity metrics (PCC,\nCS, ED) in relation to merge gain. The scat-\nter plots reveal a moderate correlation between\nmodel kinship and merge gain, as indicated by the trend lines. To further quantify these relation-\nships, the correlation value (use Pearson Correlation Coefficient) between model kinship and merge\ngain are calculated, as detailed in the second column of Table 1. While moderate correlations are\nobserved for all three metrics, the corresponding p-values indicate a weak level of statistical sig-\nnificance, ranging from 0.05 to 0.1. In contrast, when examining absolute merge gain, we find\nstronger and statistically significant correlations, as shown in the third column of Table 1. These\nresults suggest that model kinship alone is insufficient for predicting whether a model can achieve\ngeneralization gains through merging. However, it may serve as a key factor in determining the\nupper limit of merge gains, thus indicating potential improvements. Since no significant differences\nare observed among the three metrics, we will focus solely on model kinship based on PCC in the\nfollowing sections to simplify the demonstration."}, {"title": "3.3 SEQUENCE ANALYSIS OF MODEL EVOLUTION PATHS", "content": "In this analysis, we examine changes in performance and model kinship across independent model\nevolution paths to identify the phased pattern of the merging process. We focus on the yamshadow\nexperiment 28-7B (Labonne, 2024), a Mistral 7B architecture model ranked as the top 7B merged\nmodel on the Open LLM Leaderboard. From its model family tree, we extract two primary merging\npaths: Path 1 and Path 2."}, {"title": "3.3.1 RESULTS", "content": "We first focus on the average task performance and merge gains throughout the model evolution path\n(Figure 3.) Detailed data and branch information are summarized in Appendix A). Our observations\nindicate that the performance improvements of the iterative merging process are not linear and can\nbe divided into two stages:\n\u2022 Learning Stage. In this stage, the average task performance generally experiences a rapid\nincrease. Noticeable merge gains suggest that the merged models are continually acquiring\nmultitask capabilities through the merging process.\n\u2022 Saturation Stage. As the process continues, improvements begin to plateau. During this\nstage, the merge gains approach zero, indicating that the model can no longer benefit from\nthe merging process and has ceased to improve.\nAdditionally, we compare the trend of model kinship with average task performance. Figure 4\nillustrates the changes in model kinship alongside average task performance (normalized to the same\nrange as the corresponding metric) throughout the model evolution paths. In addition to the notable\ncorrelation observed between average task performance and model kinship throughout the evolution\nprocess, model kinship exhibits a similar stage-specific pattern, particularly evident in the saturation\nstage, suggesting a potential relationship with the underlying cause of saturation."}, {"title": "3.4 ANALYSIS OF THE MODEL KINSHIP IN DIFFERENT MERGING STAGES", "content": "Findings in previous analysis raises the question of \u201chow model kinship affects performance im-\nprovements in model evolution\". To investigate the causality between model kinship and the stag-\nnation of improvements, we examine the variation of model kinship across different merging stages\nfrom a broader perspective.\nGiven the community's predominant use of the performance-prior strategy, we calculate model kin-\nship among models with similar performance, simulating the selection of top-performing models at\neach stage. For this analysis, we randomly select 5 models from each merging stage - Saturation\nStage (\u2265 0.75), Learning Stage (<0.75 and \u22650.73), and Initial Merges (fine-tuned models) to form\nthree foundation model groups, representing potential merges at different stages of model evolution.\nFigure 5 illustrates the model kinship between models within each group. Our analysis reveals that\nmodel kinship increases with the average task performance, even across models that follow different\nevolution paths yet share similar average performance levels. Additionally, during the saturation\nstage, all potential merges display a strong affinity, with model kinship values nearing 1. Since\nmodel kinship indicates the similarity of weights, we concluide the final findings as:\nFindings: Model merging experiences a saturation stage, where the model kinship among top-\nperforming models increases throughout the iterative merging process. This implies that the mod-\nels converge to similar forms, resulting in excessive relatedness that undermines the effectiveness\nof the model merging strategy.\""}, {"title": "4 USING MODEL KINSHIP TO IMPROVE MODEL MERGING", "content": "Inspired by the above findings, we further leverage model kinship to enhance the model merging pro-\ncess. We firstly conduct experiments employing a performance-prior greedy merging strategy. Note\nthat the greedy strategy may eventually lead to convergence, we further introduce Top-k Greedy\nMerging with Model Kinship (Algorithm 1). Our results indicate that while the greedy strategy\nfocuses on short-term gains, it can lead to parameter convergence and suboptimal outcomes. By\nintegrating model kinship, we can help the strategy avoid local optima. Furthermore, we find that\nmodel kinship holds potential for enhancing merging strategies as an early stopping criterion."}, {"title": "4.1 EXPERIMENT SETUP", "content": "$\\mathbf{Algorithm 1}$ Top k Greedy Merging with Model Kinship.\n$\\mathbf{Require}$: A set $M$ of n foundation models {$M_1, M_2,..., M_n$}, Evaluation function $f$, Similarity\nmetric function $sim(\\cdot, \\cdot)$ for model kinship.\n1: Generate the first generation of merged models by merging each pair in set $M$\n{$M_1, M_2,..., M_n$}.\n2: Evaluate each merged model using $f$ and select the top k models. Denote this set as $S$\n{$M_1, M_2,..., M_n$}.\n3: Initialize a variable $S_{prev} = \\emptyset$ to store the top m models from the previous iteration.\n4: $\\mathbf{while}$ $S \\neq S_{prev}$ $\\mathbf{do}$\n5:    Set $M_{prev} = M$.\n6:    Set $S_{prev} = S$.\n7:    Select k pairs of models from S with the highest performance according to f.\n8:    Identify the current best model $M_{best} \\in S$.\n9:    Identify the model $M_f \\in S$ with the highest model kinship to $M_{best}$ from the $M_{prev}$ according\nto the similarity metric $sim(\\cdot, \\cdot)$.\n10:   Merge $M_f$ with $M_{best}$ to generate a new model $M_{exp}$ and add $M_{exp}$ into set $M$.\n11:   Merge each selected pair to $M_{merged}$ (named as $\\text{Model-gen-id}$) for merged models and add\nmerged models into set $M$.\n12:   Evaluate each new model using $f$ and update S to be the new top k models.\n13: $\\mathbf{end while}$\n$\\text{Note}$: The blue-highlighted steps are only executed in modified experiments incorporating model\nkinship-based exploration.\nLLMs. We select three fine-tuned, open-source LLMs based on the Mistral-7B architecture\nfrom HuggingFace: mistral-7b-instruct-v0.2, metamath-mistral-7b, and open-chat-3.5-1210. Each"}, {"title": "6 CONCLUSION AND LIMITATIONS", "content": "In this paper, we introduce model kinship, the degree of similarity or relatedness between LLMs,\nfor merging LLMs, which can help guide our selection of candidate models. We conduct compre-\nhensive experiments to demonstrate its effectiveness in understanding the model evolution process.\nWe further propose a new model merging strategy: Top-k Greedy Merging with Model Kinship. We\nshow that model kinship plays a crucial role in model evolution by guiding the process to escape lo-\ncal optima traps (in saturation stage), enabling further improvements. Additionally, we demonstrate\nthat model kinship can detect the onset of convergence, allowing for early stopping and reducing the\nwaste of computational resources in the merging process.\nIn a broad sense, our work explores how models can achieve autonomous evolution through model\nmerging. Model merging can, to some extent, be likened to biological hybridization. Biological\norganisms have undergone billions of years of evolution to reach their current state. However, how\nsilicon-based intelligence, represented by LLMs, evolves remains an unresolved mystery. We aspire\nthat this work offer guidance and insights for the future merging and evolution of LLMs.\nHowever, there are several limitations to consider: a) The experiments in this study are conducted\nexclusively on models with the Mistral architecture, leaving uncertainty about the transferability of\nour metric and method to other architectures, such as Mamba (Gu & Dao, 2023). b) The analysis\nrelies on open-source data from the Open Leaderboard, which is community-generated and may\ncontain noise due to user bias. c) Correlation metrics for model kinship have not been fully explored.\nOther metrics may perform better than those discussed in this paper. d) The effectiveness of model\nkinship is demonstrated through empirical evidence. However, a theoretical framework (such as"}]}