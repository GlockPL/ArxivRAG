{"title": "Understanding Optimization in Deep Learning with Central Flows", "authors": ["Jeremy Cohen", "Alex Damian", "Ameet Talwalkar", "Zico Kolter", "Jason D. Lee"], "abstract": "Optimization in deep learning remains poorly understood, even in the simple setting of deterministic (i.e. full-batch) training. A key difficulty is that much of an optimizer's behavior is implicitly determined by complex oscillatory dynamics, referred to as the \"edge of stability.\u201d The main contribution of this paper is to show that an optimizer's implicit behavior can be explicitly captured by a central flow: a differential equation which models the time-averaged optimization trajectory. We show that these flows can empirically predict long-term optimization trajectories of generic neural networks with a high degree of numerical accuracy. By interpreting these flows, we reveal for the first time 1) the precise sense in which RMSProp adapts to the local loss landscape, and 2) an acceleration via regularization mechanism, wherein adaptive optimizers implicitly navigate towards low-curvature regions in which they can take larger steps. This mechanism is key to the efficacy of these adaptive optimizers. Overall, we believe that central flows constitute a promising tool for reasoning about optimization in deep learning.", "sections": [{"title": "1 Introduction", "content": "Optimization in deep learning remains poorly understood, even in the simple setting of deterministic (i.e. full-batch) training. A key difficulty is that much of an optimizer's behavior is determined implicitly by complex oscillatory dynamics (Xing et al., 2018; Wu et al., 2018; Jastrz\u0119bski et al., 2019, 2020; Cohen et al., 2021). As a result, an optimizer's update rule often sheds little light on its actual behavior.\nTo address this challenge, we develop a new methodology for analyzing optimization in deep learning. To analyze an optimizer, we derive a central flow: a differential equation which directly models the time-averaged (i.e. smoothed) trajectory of the oscillatory optimizer. We use informal mathematical reasoning to derive central flows for gradient descent (Section 3), a simple adaptive optimizer (Section 4), and RMSProp, i.e. Adam with no momentum (Section 5), and we empirically demonstrate that these flows can predict long-term optimization trajectories of neural networks with a high degree of numerical accuracy. We are unaware of any other theoretical analyses of deep learning optimization with a similar degree of predictive power."}, {"title": "2 Related Work", "content": "Edge of Stability The dynamics of optimization in deep learning remain poorly understood, even in the seemingly simple setting of deterministic (i.e. full-batch) training. Indeed, recent research showed that gradient descent on neural networks typically operates in a regime termed the \"edge of stability\u201d (EOS) in which (1) the largest Hessian eigenvalue equillibrates around the critical threshold 2/\u03b7, and (2) the algorithm oscillates along high-curvature directions without diverging (Xing et al., 2018; Wu et al., 2018; Jastrz\u0119bski et al., 2019, 2020; Cohen et al., 2021). These dynamics could not be explained by existing optimization theory, which led Cohen et al. (2021) to observe that there was no explanation for how or why gradient descent can function properly in deep learning.\nSubsequently, several studies sought to theoretically explain EOS dynamics. Some works rigorously analyzed EOS dynamics on specific objective functions (Agarwala et al., 2023; Ahn et al., 2024; Chen and Bruna, 2023; Even et al., 2024; Kreisler et al., 2023; Song and Yun, 2023; Li et al., 2022; Wu et al., 2024; Zhu et al., 2023), while other works (Arora et al., 2022; Lyu et al., 2022; Damian et al., 2023), gave generic analyses based on a local third-order Taylor expansion of the loss, which is one order higher than is normally used in the theoretical analysis of gradient descent. Similar arguments were first used by Blanc et al. (2019) to study implicit regularization in SGD.\nOur analysis is most directly inspired by Damian et al. (2023), which rigorously analyzed EOS in the special case where gradient descent oscillates along a single direction. Whereas they analyze the fine-grained oscillatory dynamics, we argue that analyzing the time-averaged dynamics is simpler, and is sufficient for many purposes. We first reproduce their main result using a simple, albeit non-rigorous, time-averaging argument. We then show that this time-averaging methodology easily extends to the more realistic and challenging setting where gradient descent oscillates along multiple directions simultaneously, as well as to the analysis of two adaptive optimizers.\nUnderstanding Adaptive Optimizers Ma et al. (2022) observed that RMSProp and Adam oscillate, and Cohen et al. (2022) showed that such dynamics can be viewed as an adaptive version of the edge of stability, a finding which we will leverage. Khaled et al. (2023) and Mishkin et al. (2024) observed that on quadratic functions, certain adaptive optimizers implicitly adapt their effective step size to the maximum stable step size; we show this holds more generally, beyond quadratics. Experiments in Roulet et al. (2024) and Wang et al. (2024d) are explained by the phenomenon we call \u201cacceleration via regularization.\u201d Many works have also conducted rigorous convergence analyses of adaptive optimizers, generally focused on deriving rates of convergence to a global minimizer or stationary point (Duchi et al., 2011; Reddi et al., 2018; Chen et al., 2019a,b; Zaheer et al., 2018; Zou et al., 2019; D\u00e9fossez et al., 2022; Li and Lin, 2024; Chen et al., 2022; Wang et al., 2024a; Yang et al., 2024; Guo et al., 2021; Shi et al., 2021; Zhang et al., 2022; Crawshaw et al., 2022; Li et al., 2024; Wang et al., 2024b; Hong and Lin, 2024; Zhang et al., 2024; Wang et al., 2024c; H\u00fcbler et al., 2024)."}, {"title": "3 Gradient Descent", "content": "We begin by studying the simplest first-order optimizer: gradient descent with a fixed step size n.\n$w_{t+1} = w_t - \\eta \\nabla L(w_t).$"}, {"title": "3.1 The Dynamics of Gradient Descent", "content": "The sharpness S(w) := \u03bb\u2081(H(w)), defined as the largest Hessian eigenvalue, plays a key role in the dynamics of gradient descent. In particular, gradient descent oscillates whenever the sharpness is \u201ctoo large\" relative to the learning rate \u03b7. For example, consider optimizing a one-dimensional quadratic objective L(x) = \u00bdSx\u00b2, which has global sharpness S. The gradient descent iterates {xt} evolve via xt+1 = (1 \u2212 nS)xt. If S exceeds the critical threshold 2/\u03b7, then (1 \u2013 \u03b7S) < \u22121, so the iterate xt flips signs and grows in magnitude at each step, i.e. gradient descent oscillates with exponentially growing magnitude. More generally, for a quadratic objective in multiple dimensions, gradient descent oscillates with exponentially growing magnitude along all Hessian eigenvectors with eigenvalues exceeding 2/\u03b7. While deep learning objectives are not globally quadratic, a local quadratic Taylor approximation suggests that if the sharpness exceeds 2/n, gradient descent will oscillate along the top Hessian eigenvector(s). This suggests that gradient descent cannot function properly when the sharpness exceeds 2/n."}, {"title": "3.2 Deriving the Gradient Descent Central Flow", "content": "In this section, we will derive a differential equation which models the trajectory of gradient descent. The standard continuous-time approximation to gradient descent is the gradient flow:\n$\\frac{dw}{dt} = -\\eta \\nabla L(w).$\nCohen et al. (2021) observed that trajectory of gradient descent is well-approximated by that of gradient flow so long as training is stable, i.e. so long as the sharpness S(w) remains below 2/n. However, once the sharpness reaches 2/n and the optimizer enters the EOS regime, gradient descent departs from the gradient flow trajectory and takes a different path. For example, the gray line in Figure 5(d) plots the weight-space distance between the gradient descent iterate at step t and the gradient flow solution at time t. This distance remains small so long as training is stable, but starts to grow large once training enters EOS, indicating that these trajectories diverge.\nWe now derive a more general differential equation, which we call a central flow, that approximates the trajectory of gradient descent even at the edge of stability. The central flow directly models the time-averaged (i.e. smoothed) trajectory of the oscillatory optimizer. In other words, the central flow averages out the oscillations while retaining their lasting effect on the optimization trajectory.\nWe will derive the central flow using informal mathematical reasoning, and we will empirically demonstrate that it can accurately predict long-term optimization trajectories of neural networks with a high degree of numerical accuracy. For example, the black line in Figure 5(d) plots the weight-space distance between gradient descent at step t and the central flow at time t. Observe that this distance stays small throughout training, indicating that while the gradient descent and gradient flow trajectories diverge, the gradient descent and central flow trajectories remain close."}, {"title": "3.2.1 The Special Case of One Unstable Eigenvalue", "content": "We will introduce our time-averaging methodology by analyzing the special case when only the largest Hessian eigenvalue has crossed the critical threshold 2/n, and gradient descent oscillates along a single direction the corresponding eigenvector. The general case of multiple unstable eigenvalues will then be tackled in Section 3.2.2.\nWe start our analysis at the instant when the sharpness S(w) reaches 2/\u03b7. From this point onward, we will model the GD trajectory by wt = Wt + xtut where wt is the gradient descent iterate, Wt is the time-averaged iterate, ut is the top Hessian eigenvector at Wt, and xt denotes the displacement between wt and Wt along the ut direction. Note that by definition, E[xt] = 0, i.e. the time-averaged displacement is zero. To track the evolution of wt, we begin by time-averaging both sides of the gradient descent update:\n$W_{t+1} = E[w_{t+1}] = E[w_t \u2013 \u03b7\u2207L(w_t)] = W_t - \u03b7 E[\u2207L(w_t)].$\nThus, the time-averaged iterates follow the (negative) time-averaged gradient. To approximate the time-averaged gradient, we time-average the Taylor expansion of the gradient given in eq. (2):\n$E[\u2207L(w_t)] \u2248 \u2207L(W_t) + S(W_t) E[x_t]u_t + \\frac{1}{2} E[x_t^2]\u2207S(W_t) = \u2207L(W_t) + \\frac{1}{2} E[x_t^2]\u2207S(W_t).$\nThus, the time-averaged gradient E[\u2207L(wt)] is equal to the gradient at the time-averaged iterate \u2207L(Wt), plus an implicit sharpness penalty whose strength is proportional to E[x], the variance of the oscillations at step t. Substituting eq. (5) into eq. (4) and switching to continuous time, we therefore model the time-averaged iterates Wt"}, {"title": "3.2.2 The General Case (Multiple Unstable Eigenvalues)", "content": "When multiple eigenvalues have reached the critical threshold 2/\u03b7, gradient descent oscillates in the span of the corresponding eigenvectors. We assume the displacement dt := wt Wt between the true process and the time-averaged process lies in the span of these eigenvectors. For example, when only one eigenvalue has reached 2/\u03b7, taking dt = xtut recovers the analysis in Section 3.2.1. We model the iterate wt as wt = Wt + dt where, by definition, E[dt] = 0. As above, the time-averaged iterates wt should follow the time-averaged gradient E[\u2207L(wt)]. To compute the time-averaged gradient, we first Taylor-expand the gradient around wt:\n$\u2207L(w_t) \u2248 \u2207L(W_t) + H(W_t)d_t + \\frac{1}{2} \u2207_t (H(W_t), d_t d_t) + O(||d_t||^3).$"}, {"title": "3.3 Interpreting Gradient Descent via its Central Flow", "content": "We now use the central flow to understand the behavior of gradient descent. As a smooth flow, the central flow is an simpler object to reason about than the oscillatory gradient descent trajectory. In particular, we can use the chain rule to quantify the evolution of any differentiable metric.\nFor example, the rate of loss decrease under the central flow is simply given by $\\frac{dL(w)}{dt} = (\u2207L(w), \\frac{dw}{dt})$. Combining this with the projection interpretation (Definition 3), we prove in Proposition 1 that $\\frac{dL(w)}{dt} < 0$, i.e. that the loss along the central flow L(w(t)) is monotonically decreasing (black line in Figure 6). This stands in stark contrast to the loss along the gradient descent trajectory, which behaves non-monotonically (blue line in Figure 6). Thus, the central flow loss is a hidden progress metric for the optimization process.\nAs is visible in Figure 6, the gradient descent train loss L(wt) is generally higher than the central flow train loss L(w(t)), due to the oscillations. However, since the central flow also models the oscillation covariance \u03a3(t), it can render predictions for the time-averaged train loss along the gradient descent trajectory:\n$E[L(w_t)] \u2248 L(w(t)) + \\frac{1}{2} (H(w(t)), \u03a3(t)) = L(w(t)) + \\frac{1}{2} S(w(t))tr(\u03a3(t)).$"}, {"title": "4 Scalar RMSProp", "content": "As a stepping stone to the analysis of RMSProp, we now study \"Scalar RMSProp,\" a simple adaptive optimizer which uses one global adaptive step size, rather than separate adaptive step sizes for each coordinate:\n$v_t = \u03b2_2v_{t-1} + (1 \u2212 \u03b2_2)||\u2207L(w_t)||^2, ~~~~w_{t+1} = w_t \u2212 \\frac{\u03b7}{\\sqrt{v_t}} \u2207L(w_t).$\nThe algorithm maintains an exponential moving average (EMA), v, of the squared gradient norm, and takes gradient steps of size \u03b7/\u221av, which we call the effective step size. The EMA hyperparameter \u03b22 is a knob that interpolates the algorithm between gradient descent when \u03b22 = 1 and normalized gradient descent (NGD) when \u03b22 = 0.\nIn this section, we will use the central flow framework to understand the behavior of this simple adaptive optimizer. We first describe the dynamics of Scalar RMSProp in Section 4.1. We then leverage this to derive a central flow in Section 4.2. Finally, in Section 4.3, we interpret this flow to understand the optimizer's behavior. In particular:"}, {"title": "4.1 The Dynamics of Scalar RMSProp", "content": "The dynamics of Scalar RMSProp revolve around the effective sharpness, defined as Seff := S(w)/\u221av. First, the effective sharpness controls the oscillations: when Seff > 2/\u03b7, Scalar RMSProp oscillates with growing magnitude along high curvature direction(s). Second, such oscillations in turn trigger a reduction of effective sharpness. This occurs via a combination of two distinct mechanisms. One mechanism, shared with gradient descent, is that oscillations implicitly reduce sharpness due to Equation (2), thereby decreasing the effective sharpness via its numerator. The other mechanism, new to Scalar RMSProp, is that oscillations increase the gradient norm and hence v, thereby decreasing effective sharpness via its denominator. These dynamics give rise to a negative feedback loop that keeps the effective sharpness automatically regulated around 2/n, as depicted in the bottom left plot in Figure 8. The fine-grained dynamics are complex and challenging to analyze, even in the case of a single oscillatory direction. Fortunately, we will see in the next section that analyzing the time-averaged dynamics is much simpler."}, {"title": "4.2 Deriving the Central Flow", "content": "Recall that while gradient descent trains stably, it is well-approximated by gradient flow. One can derive an analogous \"stable flow\" for Scalar RMSProp (Ma et al., 2022, cf.):\n$\\frac{dw}{dt} = -\\frac{\u03b7}{\\sqrt{v}} \u2207L(w), ~~~~\\frac{dv}{dt} = \\frac{1-\u03b2_2}{\u03b2_2} [||\u2207L(w)||^2 \u2212 v].$\nHowever, at the edge of stability, the trajectory of Scalar RMSProp deviates from eq. (14). We will now derive a more general central flow that characterizes the time-averaged trajectory even at EOS. In the main text, we will focus on the case where one eigenvalue is (and remains at) the edge of stability. See Appendix C.4 for our full derivation which accounts for multiple eigenvalues at EOS and for eigenvalues entering and leaving EOS.\nIn section 3.2.1, we derived an approximation for the time-averaged gradient, E[\u2207L(w)]. Using the first two terms of eq. (2), we can also derive a time-averaged approximation for the squared gradient norm E[||\u2207L(w)||2]:\n$E[||\u2207L(w)||^2] \u2248 ||\u2207L(w)||^2 + 2 (\u2207L(W), u) S(w) E[x_t] + S(w)^2 E[x_t^2]$\nwhere we again used E[xt] = 0 to ignore the middle term. Based on these time averages, we make the ansatz that the joint dynamics of (wt, vt) follow a central flow (w(t), v(t)) of the form:\n$\\frac{dw}{dt} = -\\frac{\u03b7}{\\sqrt{E[v_t]}} E[\u2207L(w_t)], ~~~~\\frac{dv}{dt} = \\frac{1-\u03b2_2}{\u03b2_2} [||\u2207L(w)||^2 + S(w)^2 \u03c3^2(t) \u2212 \u03bd].$"}, {"title": "4.3 Interpreting Scalar RMSProp via its Central Flow", "content": "We now interpret the Scalar RMSProp central flow to shed light on the behavior of the algorithm and the function of its hyperparameters \u03b7 and \u03b22. Because the dynamics usually transition from stable to EOS quite early in training, we focus on interpreting the central flow in the EOS regime. We discuss step size adaptation in Section 4.3.1,\nimplicit curvature reduction in Section 4.3.2, and finally in Section 4.3.3, we show that the interplay between these two mechanisms is key to the success of Scalar RMSProp."}, {"title": "4.3.1 Implicit step size selection", "content": "The central flow renders explicit the step size strategy that is implicit in the oscillatory dynamics of Scalar RMSProp. Recall that while the central flow is at EOS, the effective sharpness Seff := S(w)/\u221av is fixed at 2/\u03b7. This condition can be rearranged into a statement about the effective step size: \u03b7/\u221a\u03bd = 2/S(w). Notably, the value 2/S(w) is the largest stable step size for the current location w in weight space. In other words, while the algorithm is at EOS, the oscillatory dynamics continually adapt the effective step size to the current largest stable step size. This is the precise sense in which Scalar RMSProp \"adapts\" to the local loss landscape."}, {"title": "4.3.2 Implicit curvature reduction", "content": "Understanding the implicit step size strategy employed by Scalar RMSProp is not sufficient to fully characterize the behavior of the algorithm. To do so, we need to return to the central flow, which additionally accounts for the curvature regularization induced by oscillations. In general, the Scalar RMSProp central flow is a joint flow over (w, v). However, at EOS, because \u03b7/\u221a\u03bd = 2/S(w), we can eliminate v from the expression for dw,\ndv, and write the central flow in terms of w alone:\n$\\frac{dw}{dt} = \\frac{2}{S(w)} [\u2207L(w) + \u03c3^2 (\u03c9; \u03b7, \u03b2_2)\u2207S(w)].$\nwhere \u03c3\u00b2(\u03c9; \u03b7, \u03b22) is given by eq. (16). In other words, the time-averaged trajectory of Scalar RMSProp at EOS is essentially equivalent to that of the following simpler-to-understand algorithm: at each iteration, compute the sharpness S(w), and take a gradient step of size 2/S(w) on a sharpness-regularized objective, where the strength of the sharpness regularizer is given by eq. (16). This characterization, which directly exposes the optimizer's implicit behaviors, is far from obvious from the raw update rule (eq. 13).\nInterestingly, the hyperparameters \u03b7, \u03b22 are not used to determine the effective step size 2/S(w). Instead, their only role is to modulate 02, which controls the strength of the implicit sharpness penalty. The effect of the learning rate hyperparameter n is to monotonically increase o\u00b2 \u2014 indeed, the numerator of eq. (16) is increasing in \u03b7 while the denominator is decreasing in \u03b7, which implies the overall expression for o\u00b2 is increasing in \u03b7. The simplest case is that of NGD, i.e. when \u03b22 = 0, for which eq. (16) reduces to 02 \u2248 17. Meanwhile, the effect of the hyperparameter B2 is to monotonically interpolate 02 between that of NGD when \u03b22 = 0 and that of gradient descent when \u03b22 = 1.\nThe interpretations of \u03b7, \u03b22 generalize to the setting of multiple oscillating directions, as detailed in Lemma 3."}, {"title": "4.3.3 Acceleration via regularization", "content": "To fully grasp the modus operandi of Scalar RMSProp, it is necessary to consider the link between step size adaptation and curvature regularization. By regularizing sharpness S(w), Scalar RMSProp is able to steer itself towards regions where the maximal locally stable step size of 2/S(w) is larger. In such regions, Scalar RMSProp can and does take larger steps. Thus, by regularizing sharpness, Scalar RMSProp enables larger steps later in training. We call this mechanism acceleration via regularization. Our experiments suggest that this mechanism is a critical component"}, {"title": "5 RMSProp", "content": "We now study RMSProp (Tieleman and Hinton, 2012), which is equivalent to Adam (Kingma and Ba, 2015) without momentum. RMSProp maintains an EMA v of the elementwise squared gradients \u2207L(w)2, and uses per-coordinate effective step sizes of n/\u221av:\n$v_t = \u03b2_2v_{t-1} + (1 \u2212 \u03b2_2)\u2207L(w_t)\u00a92, ~~~~w_{t+1} = w_t - \\frac{\u03b7}{\\sqrt{v_t}} \u2207L(w_t),$\nwhere represents the entrywise product. RMSProp can also be viewed as preconditioned gradient descent wt+1 = wt  PL(wt) with the dynamic preconditioner Pt := diag(\u221a\u03bd\u03c4/\u03b7). While Adam employs the same dynamic preconditioner and has achieved widespread success in deep learning, it has remained unclear why this specific preconditioning strategy is so effective (Kunstner et al., 2019; Orabona, 2020; Martens, 2020).\nIn this section, we use the central flows framework to understand the behavior of RMSProp. We will show that the success of RMSProp is intrinsically tied to its oscillatory dynamics. Our analysis of these dynamics will allow us to understand basic aspects of RMSProp's behavior for the first time.\nWe start by describing the dynamics of RMSProp in Section 5.1. We then derive a central flow in Section 5.2. Finally, in Section 5.3, we interpret this flow to understand the optimizer's behavior. In particular:"}, {"title": "5.1 The Dynamics of RMSProp", "content": "The dynamics of RMSProp revolve around the effective sharpness Seff(w; v), defined as the largest eigenvalue of the preconditioned Hessian P\u22121H(w) where P = diag(\u221a\u03bd/\u03b7). When Seff > 2, the iterates oscillate along the top right eigenvector of the preconditioned Hessian. In turn, such oscillations reduce Seff, via a combination of two mechanisms: (1) they implicitly reduce curvature, and (2) they increase the gradient, growing v and therefore P. The net effect is that the effective sharpness Seff stays regulated around 2 throughout training (Cohen et al., 2022)."}, {"title": "5.2 Deriving the RMSProp Central Flow", "content": "In Appendix C.5 we derive a central flow (w(t), v(t)), in the same way as above, which models the time-averaged trajectory of RMSProp. We verify its accuracy in Figure 10 and Appendix D."}, {"title": "5.3 Interpreting RMSProp via its Central Flow", "content": "We now interpret the RMSProp central flow to understand the behavior of RMSProp. Because the dynamics usually transition from stable to EOS early in training, we focus on the EOS regime."}, {"title": "5.3.1 The stationary preconditioner", "content": "The central flow for RMSProp is harder to interpret than that for Scalar RMSProp, because even at EOS, v cannot be expressed as a closed-form function of w, and instead remains an independent variable. This reflects the fact that for any w, there are potentially many values for v that could stabilize optimization, and the actual value used by RMSProp depends on the historical trajectory. Nevertheless, it turns out that, in some circumstances, v implicitly converges under the dynamics of RMSProp to a value that depends on the current w alone. In particular, imagine"}, {"title": "6 Experimental Results", "content": "We have used informal mathematical reasoning to derive central flows for three optimizers. In this section, we empirically validate the accuracy of these flows. To assess how well a central flow approximates its corresponding optimizer, we run both the flow and the optimizer simultaneously, starting from the same initialization. For a baseline, we also run the corresponding stable flow (i.e. for gradient descent, the gradient flow), which does not account for the effect of the oscillations and hence should fail to properly model the trajectory in the EOS regime.\nOur experiments span a variety of neural network architectures: a CNN, a ResNet (He et al., 2016), a vision transformer (ViT) (Dosovitskiy et al., 2021), a GPT-2 (Radford et al., 2019) style transformer, and a recurrent neural network (RNN). We evaluate the CNN, ResNet, and ViT on an image classification task, and we evaluate the GPT-style transformer and RNN on a sequence prediction task, using both mean-squared-error (MSE) and cross entropy losses. Since discretizing the flows is computationally expensive, we are restricted to small scale datasets: our image dataset is a subset of CIFAR-10 with 1,000 examples and either 4 or 10 classes, while our sequence dataset is a sorting task"}, {"title": "7 Discussion", "content": "7.1 Modeling decisions\nDeterministic setting Our analysis is restricted to the simple setting of deterministic (i.e. full-batch) training, whereas practical deep learning generally involves minibatch training. We study the full-batch setting because, as the simplest special case, understanding full-batch training is a necessary prerequisite for understanding minibatch training. However, we also believe that understanding full-batch training might suffice for some practical purposes, such as designing optimizers. For example, Kunstner et al. (2024) showed that the advantage of adaptive methods over SGD grows larger with larger batch sizes, suggesting that the relevant algorithmic principles can be best understood in the deterministic setting.\nAn interesting direction for future research is to try to extend our central flows methodology to the stochastic setting. Like deterministic optimizers, stochastic optimizers are known to implicitly regularize the curvature along their trajectories, and in fact this effect is stronger in the stochastic setting (Keskar et al., 2017; Jastrz\u0119bski et al., 2020, 2021). However, extending the central flows methodology to the stochastic setting may be nontrivial; due to the randomness, it is not clear whether there exists a deterministic differential equation around which SGD oscillates. An interesting question is whether there exists a differential equation that can predict derived metrics such as network predictions or training loss curves, even if it cannot model the weight-space trajectory of SGD.\nFinally, while our analysis in this paper sheds light on adaptive optimizers in the deterministic setting, it remains to be seen if these optimizers exhibit similar preconditioning strategies in the stochastic setting.\nBlack-box model of the loss Our analysis treats the loss function as a black box, and never uses that the optimization problem at hand involves training a neural network. The advantage of this approach is its generality: we expect our analysis to apply to generic deep learning architectures and learning problems, including those that do not yet exist. The disadvantage, however, is that the predictions made by our theory are at the abstraction level of the loss landscape, and would need to be further translated in order to make concrete claims about the network architecture or learning problem. For example, our theory tells us that the learning rate hyperparameter modulates the strength of an implicit sharpness penalty, but does not tell us how this sharpness penalty affects learning. Nor does our theory shed light on how different layers of the neural network are mechanistically implicated in progressive sharpening or sharpness reduction.\nOn the one hand, the loss landscape level of abstraction is in some sense \u201cnatural\u201d \u2014 the overall path that optimizers follow really does intrinsically depend on the (effective) sharpness. But on the other hand, understanding many important aspects of optimization in deep learning will likely require cracking open the black box a bit more."}, {"title": "7.2 Takeaways from our analysis", "content": "The unreasonable effectiveness of time-averaging As demonstrated by prior works on EOS, it is challenging to analyze the oscillatory EOS dynamics in fine-grained detail. Our work shows that, perhaps surprisingly, simple heuristics allow us to analyze the time-averaged dynamics with excellent numerical accuracy. Interestingly, the success of this time-averaging approach seems to imply that the oscillations only affect the macroscopic trajectory in an ergodic sense, i.e. via their covariance rather than via their fine-grained details. An promising direction for future work is to identify realistic conditions under which our heuristic time-averaging arguments can be made rigorous.\nNecessity of third-order Taylor expansions While optimization theory generally relies on second-order Taylor expansions of the loss, Damian et al. (2023) showed that a third-order Taylor expansion is key for understanding the convergence of gradient descent; such a Taylor expansion reveals that oscillations implicitly trigger curvature reduction, a form of negative feedback which stabilizes optimization. In this work, we have shown that a third-order Taylor expansion is similarly necessary for understanding the acceleration via mechanism which underlies the success of adaptive optimizers. Thus, our work further drives home the necessity of a third-order Taylor expansion when analyzing optimization in deep learning.\nOscillatory first-order methods are implicitly second-order methods A key surprise over the last decade is that there are no second-order optimizers which consistently outperform first-order adaptive optimizers. Our work partially demystifies this observation. We show that when first-order optimizers oscillate, they implicitly pick up second order information. Thus, even though RMSProp is a first-order optimizer, it implicitly employs a sophisticated second-order preconditioning strategy, detailed in Section 5.3.1. Further, this preconditioning strategy is efficient, requiring no more gradient queries than gradient descent does. An exciting direction for future work is to intentionally design first-order adaptive methods with such implicit preconditioners in mind."}, {"title": "8 Conclusion", "content": "In this paper, we have developed a framework for analyzing deep learning optimization algorithms. To analyze an algorithm, we derive a central flow which directly models the time-averaged trajectory of the oscillatory optimizer. We have empirically demonstrated that these central flows can accurately predict long-term optimization trajectories of neural networks, and by interpreting these flows we have obtained new insights about optimizers' behavior.\nThese advances are made possible by the fact that we adopt different goals from most works in optimization. Rather than try to characterize global convergence rates, we set ourselves the more modest goal of characterizing the local optimization dynamics throughout training. The local dynamics are important, they are more interesting than may have been assumed (even vanilla gradient descent gives rise to rich dynamics), and they are empirically consistent across different deep learning settings, which suggests that general theory is feasible. We believe that similar analyses can be fruitfully conducted for other optimizers, and we hope to inspire work in that direction."}]}