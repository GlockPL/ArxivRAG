{"title": "Safety challenges of AI in medicine", "authors": ["Xiaoye Wang", "Nicole Xi Zhang", "Hongyu He", "Trang Nguyen", "Kun-Hsing Yu", "Hao Deng", "Cynthia Brandt", "Danielle S. Bitterman", "Ling Pan", "Ching-Yu Cheng", "James Zou", "Dianbo Liu"], "abstract": "Recent advancements in artificial intelligence (AI), particularly in deep learning and large language models (LLMs), have accelerated their integration into medicine. However, these developments have also raised public concerns about the safe application of AI. In health- care, these concerns are especially pertinent, as the ethical and secure deployment of AI is crucial for protecting patient health and privacy. This review examines potential risks in AI practices that may compromise safety in medicine, including reduced performance across diverse populations, inconsistent operational stability, the need for high-quality data for effective model tuning, and the risk of data breaches during model development and deployment. For medical practitioners, patients, and researchers, LLMs provide a conve- nient way to interact with AI and data through language. However, their emergence has also amplified safety concerns, particularly due to issues like hallucination. Second part of this article explores safety issues specific to LLMs in medical contexts, including limita- tions in processing complex logic, challenges in aligning AI objectives with human values, the illusion of understanding, and concerns about diversity. Thoughtful development of safe AI could accelerate its adoption in real-world medical settings.", "sections": [{"title": "1 Introduction", "content": "Recent advances in artificial intelligence (AI) have created significant opportunities to enhance the efficiency and quality of healthcare. In 2023 alone, nearly 700 AI-enabled devices were authorized by the U.S. Food and Drug Administration, spanning various medical fields including radiology, ophthalmology, and hematology [1]. Despite this rapid progress, suc- cessful AI deployment in medicine remains limited. A recent U.S. survey revealed that 60% of the population feels uncomfortable with healthcare providers relying on AI [2]. Build- ing confidence and trust among patients, clinicians, and the public is crucial, and the key to achieving this is ensuring safety.\nSafety challenges in translating AI systems into healthcare include scientific issues inher- ent to machine learning, logistical implementation hurdles, adoption barriers, and the need for socio-cultural changes. We identify two key areas where safety concerns arise: reliability and alignment. The complexity of AI reliability encompasses issues such as data harmonization, consistent performance, calibration, generalization, bias and fairness, domain adaptation, and AI-human interaction [3, 4]. In this article, AI alignment refers to ensuring AI adheres to human-defined objectives and principles, addressing concerns like objective mis-specification and hacking reward set by patients or clinicians. We aim to explore AI safety issues in medicine by first discussing inherent problems of AI related to risks in medicine, followed by specific medical AI risks, and concluding with safety concerns unique to large language models (LLMs) in healthcare."}, {"title": "2 Inherent problems of AI related to medicine", "content": "Despite the significant advances in AI recently, there are still many challenges and problems of the methods which limit their applications in medicine and may lead to potential safety issues. In this section, we focus our discussion on reliability and alignment challenges in AI which are strongly related to medicine."}, {"title": "2.1 Reliability problem of artificial intelligence in medical applications", "content": "In the practice of medicine, building AI models that can be relied by both patients, clinicians, and the general public is important, given the high stakes involved in healthcare decisions and outcomes. However, obtaining a reliable model, which means the established model should be invariant to potential disturbances and always provide the correct and accurate guid- ance towards medical requirements regarding different inputs, is challenging due to various issues, including data harmonization, model calibration, model generalization, and biases. We discuss each of the four challenges in medical settings."}, {"title": "2.1.1 Data harmonization between different healthcare providers", "content": "Data harmonization challenge originates from the heterogeneity of data used to train machine learning models. The exponential growth in data volume is accompanied by increased diver- sity, presenting significant challenges for data management and analytics. The task of sifting through such heterogeneous data is particularly complex, affecting the processes of data visu- alization and prediction and, consequently, the analytical outcomes. Data harmonization is a crucial process aimed at standardizing the disparate forms of data for uniform interpretation especially in medicine [5].\nIn the domain of machine learning for medicine, models trained on limited datasets will lack generalizability. The current trend gravitates towards the use of extensive datasets amassed from multiple healthcare institutions, making the harmonization of multi- institutional data indispensable. This harmonization is often the preliminary step in data science projects. Inadequately done, it can introduce errors that lead to grave medical reper- cussions. Regardless of scale, the errors invariably alter feature values and distributions, with some features being less tolerant of such perturbations. As a result, combining these fea- tures for statistical analysis and model building could yield unreliable outcomes, either by"}, {"title": "2.1.2 Model calibration on differnet paients", "content": "Model calibration is another major challenge in Al for medicine. Model calibration in machine learning is a critical process that ensures the predicted probabilities of an outcome align closely with the actual occurrence rates of that outcome. For instance, if a calibrated model predicts an event with a probability of 80%, then in a large patient population, that event should manifest approximately 80% of the time.\nConsider the following example to illustrate this concept: A model analyzes a group of patients' medical history, genetics, and lifestyle factors. For Patient A, the model predicts a 70% chance of developing a certain disease within the next year. For Patient B, the predic- tion is a 20% chance. For a well-calibrated model, the predicted risk probability for a large cohort of patients should be similar to the empirical observation, therefore approximately 70% of patients similar to patient A will develop the disease. The process of calibration ensures that the predictions made by Al models in medical contexts are reflective of real-world probabilities.\nThe primary challenge in model calibration in the medical field is the proper interpre- tation and application of confidence levels associated with AI predictions. First, there is a"}, {"title": "2.1.3 Generalization across different patient groups", "content": "In machine learning, \u201cgeneralization\" is a model's ability to perform well with unseen data after training on known data (figure 2). The concept of generalization is borrowed from psy- chology, where learned knowledge can be transferred to novel scenarios [18]. For instance, a child quickly knows a cheetah is a cat without seeing many cats. Machines lack human intuition but can learn from vast amounts of data. It's crucial to have comprehensive data about a subject to make model that can generalize. Strict regulations on patient health data prevent many ML-driven healthcare studies from being evaluated on external patient groups, resulting in discrepancies in the model's performance across different sites. Most data sources are confined to specific institutions or regions. A study discovered that only about 23% of healthcare-related ML articles utilized multiple datasets [19]. In some instances, the error rate of a deep learning model for retinal image analysis was 5.5% on the training dataset but increased to 46.6% when tested with images from another vendor. The problem of gener- alizability has become one of the major roadblocks to deploying deep learning models into clinical practices [20, 21].\nSeveral types of generalization issues commonly arise in the application of AI in medicine: (1) Overfitting. This occurs when a model memorizes the specifics of the train- ing data, including noise and outliers. Consequently, while it performs well on the training dataset, its effectiveness diminishes significantly on new, unseen data. (2) Underfitting. The converse of overfitting, underfitting happens when a model is too simplistic, failing to cap- ture the underlying complexities and patterns in the data. This leads to poor performance on both the training and test datasets. (3) Population Shift. If the training dataset lacks diversity and does not sufficiently represent various patient populations, the AI model's ability to gen- eralize effectively to underrepresented groups is compromised. (4) Temporal Changes. The dynamic nature of medical practices and patient demographics means that models trained on historical data might not perform effectively in contemporary or future scenarios."}, {"title": "2.1.4 Bias toward different patient groups", "content": "The problem of bias in machine learning model performance in medicine is a important concern as it impacts the reliability and fairness of medical decisions, potentially leading to suboptimal care and health disparities among different population subgroups [22-24]. The development and deployment of AI in medicine faces several bias-related challenges:"}, {"title": "2.1.5 Difficulty in adapting to a new patient population", "content": "Fine-tuning machine learning models to local data in medicine is essential to adapting models to the specific characteristics of new patient populations and healthcare settings. However, it poses several challenges that impact the effectiveness and generalizability of models. [25, 26] Apart from the Regulatory and Ethical Issues discussed before and the aforementioned data"}, {"title": "2.2 Alignment problems of artificial intelligence in medicine", "content": "Besides performing different tasks reliably and robustly, it is important to ensure the AI mod- els' behaviors reflect human users' goals and values. In this section, we discuss the challenges of alignment problems for AI in medicine, with a focus on technical and normative alignment as well as AI-clinician Interaction."}, {"title": "2.2.1 Alignment challenges when using AI in medical practice", "content": "The alignment problem in Al refers to the challenge of ensuring that artificial intelligence systems reliably behave in ways that are in accordance with human values, intentions, and expectations. This problem arises from the inherent difficulty in specifying complex human values and objectives precisely and completely in a manner that AI systems can understand and implement. The goal of AI value alignment is to ensure that AI is properly aligned with human values [27].\nThe challenge of alignment has two parts. The first is technical, focusing on the formal incorporation of values or principles into AI to ensure their reliable and appropriate actions. Real-world instances of AI misalignment have been observed, such as chatbots promot- ing abusive content when interacting freely online [28]. Moreover, this challenge escalates with more advanced AIs, raising issues like 'reward-hacking' where agents find unantici- pated methods to fulfill objectives, diverging from intended outcomes-and the difficulty of evaluating Als whose cognitive abilities might significantly surpass human capabilities [29].\nThe second aspect of the value alignment problem is normative, raising the question of which values or principles should be encoded into AIs. This can be viewed through two lenses: minimalist and maximalist conceptions of value alignment. The minimalist approach seeks to bind AI to a viable framework of human values to prevent unsafe outcomes, while the maximalist perspective aims for alignment with the most accurate or optimal human values on a broader societal or global scale. Although the minimalist perspective rightly notes that optimizing for nearly any single metric could yield adverse outcomes for humans, advancing beyond this approach might be necessary for achieving full alignment with AI. This is because Al systems could be safe and reliable but still fall short of optimal or truly desirable outcomes [27]. Here we conclude the alignment challenges as in Table 3."}, {"title": "2.2.2 AI-clinician interaction and alignment", "content": "AI-clinician interaction and alignment is a fundamental aspect of implementing AI solutions in medical practice. A symbiotic collaboration between AI and clinicians, achieved through consensus finding or disagreements cross-validation, would significantly improve the qual- ity of patient care and reduce clinical operation costs [30]. Several challenges need to be"}, {"title": "3 Risks of using AI in medicine", "content": "Recent years have seen a significant increase in the adoption of AI technologies in healthcare, affecting clinical practice, public health, and many other areas. While AI has brought promis- ing improvements in efficiency and quality, it also presents potential risks in clinical practice, healthcare operations, and social impacts."}, {"title": "3.1 Safety issues in clinical practice", "content": "In clinical practice, AI safety concerns exist in various forms, each carrying distinct risks. Although AI is usually designed and functions as assistive tools to support clinical decisions (e.g., diagnostic-AI, clinical monitoring, etc.[31-33]), unrobust, unsafe, and unexplainable"}, {"title": "3.2 Safety issues in healthcare system operation", "content": "The integration of AI in healthcare systems, such hospital operations, community clinic oper- ations, pharmacy operation and health insurance, brings various safety issues that could lead to risks and dangers. Data security and privacy is one of the largest issues when adopting AI in healthcare operations, as AI systems require access to vast amounts of sensitive patient data. There is a risk of data breaches, unauthorized access, or misuse of this data, which can lead to privacy violations and compromise patient confidentiality [41, 42]. Another issue is reliability, which refers to the possibility that AI systems might make errors in diagnosis or treatment recommendations due to limitations in their algorithms or training data. These errors can have serious consequences for patient health [43]. Ensuring that AI systems com- ply with existing healthcare regulations and standards is crucial. Non-compliance can lead to legal and ethical issues. Additionally, interoperability and system integration pose challenges in healthcare as integrating AI into existing systems can be complicated due to inter-operator discrepancies and differences in data standards, which can lead to inefficiencies and errors in patient care [12]. Resource allocation should be taken into consideration, as AI-driven decisions might influence the distribution of resources in healthcare, potentially prioritizing certain groups or treatments over others, which could exacerbate inequalities [44]. Apart from these technical problems, dependence and skill degradation among healthcare workers can occur due to over-reliance on AI, leading to a decrease in their decision-making skills [45]."}, {"title": "3.3 Social risks of applying AI in medicine", "content": "Al safety issues in medicine when presented on a societal scale, can lead to a range of broader risks and challenges. Here we outline general issues that may arise from policy level down to population level in table 2. These include public trust and perception, the ethical and societal norms, and the bias, fairness issues and more."}, {"title": "4 AI safety issues related to large language models in medicine", "content": "Large language models (LLMs) like GPT-4-omni and Llama 3 have a wide range of applica- tions in medicine. LLMs have been adapted for clinical documentation and reporting, medical information retrieval, disease prediction and diagnosis, and drug discovery and development and many other tasks [51-54]. In recent years, a combination of a shortage of well-trained physicians and the increased complexity in the medical field constitutes a significant chal- lenge for the timely and accurate delivery of healthcare. LLMs have been tested to diagnose complex clinical cases and show better diagnosis than human annotators [55]. Therefore, LLMs, together with machine learning models, could possibly be the key solutions to improve efficiency in healthcare. However, as a new interface between humans, data and computers, LLMs have many unique safety issues that are rarely found in other types of machine learning"}, {"title": "Limitations in Functionality", "content": "models. Over the past year leading up to the publication of this manuscript, we have wit- nessed a surge in research dedicated to enhancing the safety of LLMs from various angles. In this section, we aim to discuss different problems of LLMs that may lead to potential medical risks when applied in healthcare, as summarized in Figure 3.\nIt is pointed out that when using generative AI such as GPT-4 for medical purpose, two major safety problems are [56]: \"Do not trust it. This type of AI makes things up when it does not know an answer. Never ever act on what it tells you without checking\" and \"You should know, though, that if you paste medical information into an online AI program, it loses our privacy protections\u201d, The first point is a problem referred to as \u201challucination\" in the AI community. The second point is the privacy issue of sensitive medical data."}, {"title": "4.1 various limitations of inherent functionality of current LLMs", "content": "Various limitations of inherent functionality of current LLMs raise safety concerns in medical applications. From issues of data accuracy and privacy to the models' constraints in understanding complex medical data, we critically examine the facets that must be navigated carefully to harness the full potential of LLMs while ensuring patient safety and maintaining the integrity of medical practices. In this section, we attempt to identify these limitations and related safety issues in medicine.\nIn the field of AI, a hallucination or artificial hallucination [57] is a confident response by an AI that does not seem to be justified by its training data or real-world facts. In order"}, {"title": "Difficulties to handle complex logic", "content": "to answer the request of the user, LLMs may make up responses that may not be true. Hal- lucination about diagnoses, treatment and patients outcomes in texts generated by LLMs will confuse and mislead decisions by both clinicians and patients. Math calculation ability of LLMs has been recently reported to be an issues in medicine. According to Barker et al [58], one in five medication doses during hospital stages are given with errors, and more than 7 million patients per year in the US are affected by such errors [59]. There, AI and LLMs hold huge opportunities to improve these issues. Recent work shows that LLMs, such as GPT-4, is able to conduct calculations or code calculator app, indicating their potential to revolution- ize the way healthcare professionals approach medication dosing and reduce the likelihood of errors. However, the integration of these advanced technologies in clinical settings is not without its challenges. Prof. Kohane's observation about GPT-4's inaccuracies in calculating the Pearson correlation for a patient's salt intake and systolic blood pressure serves as a criti- cal reminder of the limitations and pitfalls associated with relying solely on AI for healthcare decisions [60]. Such discrepancies highlight the importance of incorporating an additional layer of verification, particularly for complex mathematical tasks that have direct implications for patient care.\nConcerns were also raised about current LLMs' difficulty to handle complex logic. According to Dziri et al [61], through multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem, they show empirically that LLMs cannot han- dle compositional tasks that require breaking down problems into compositional sub-steps and synthesizing these steps into a precise answer. In fact, transformer models solve com- positional tasks by reducing multi-step compositional reasoning into linearized subgraph matching without necessarily developing systematic problem-solving skills. This will lead to unreliable results for compositional tasks in medical settings such as treatment design that requires multiple step of logic reasoning based on observation from different observations. Everyday medical operation requires strong common-sense reasoning ability of the AI mod- els. As reported by Lee et al. [60], GPT-4 is able to have amazing performance in reasoning how to transfer different patients to different destinations and infer thoughts of the nurse who is arranging the transfer. However, some studies show limitations and instability of perfor- mance of LLMs in various common sense reasoning tasks such as in physical relation among household items[62]. If we do not have a way to estimate LLMs' limit, and when they can be trusted, it will be very difficult to develop robust LLM based tools to handle important medical use cases such as arrangement of medical devices."}, {"title": "Interaction with private patient's Information", "content": "Interaction with private patient's Information always requires the highest level of caution. As stated by ChatGPT itself, the models don't explicitly memorize input data. However, there are still concerns about potential data leakage or the possibility of extracting specifics about the training data. In the most recently deployed LLMs, local users do not have access to the model but need to access the model by providing queries and contextual data to an online portal. In healthcare, most data belongs to individual patients and is protected by law [63].\nAnother challenge in LLMs is physical world understanding. When inputting the question \"Can you describe how you perceive or experience the physical world?\" to ChatGPT, we got the following answer: \"I don't have sensory experiences. While I can provide information about the physical world based on my training data, I don't 'experience' or 'understand' it as humans do.\" Even visual-large language models such as ChatGPT-V recently became available [64], we still lack an efficient way to enable LLMs to interact with the physical"}, {"title": "Fact-checking", "content": "world, such as asking questions like \"Which of the three ventilators in the next room should I use for the patient seen by Jone yesterday morning in his clinic?\", which limits usage of LLMs in many healthcare related activities.\nFact-checking in real-time is another problem of using LLMs in medicine. The users can not verify the real-time accuracy of statements or determine the truthfulness of recent claims, such as generated clinical summaries. The user sometimes can rely on the training data for factual information but not all the time. As many LLMs are trained offline, the lack of up-to- date information prevents the use of LLMs in healthcare operations which require frequent updates of entirely new information. For instance, the information used to train models like ChatGPT is typically a few months old. In the healthcare sector, where current information is crucial, the slow pace at which models are updated by their developers poses a significant limitation. Therefore the hospital users often needs to tune their own version of LLMs either using service from large model providers or based on open source models [65]. When inter- acting with LLMs, we may not always receive the outcome we want due to the limitations in communication between human users and the models. These limitations include and are not limited to LLMs acting as a echo chambers and not following instruction (see more in Table 3)."}, {"title": "Context length", "content": "Context length is another challenge in some LLMs. Even though many LLMs and related transformer architectures have shown significant advancements in understanding and gen- erating complex language structures, there is a notable limitation in their context length capabilities, even with recent increases in newer version such as GPT-4-omini [66, 67]. This constraint becomes especially apparent when comparing the memory of LLMs to humans, particularly across diverse conversations and documents. The limitation of accessible data types poses a another challenge for clinicians, patients, and researchers looking to leverage LLMs in medical practice or research. Currently, these models predominantly excel with tex- tual data and images, which they can analyze and interpret with a high degree of accuracy. However, when it comes to other crucial types of data in the medical field, such as EEG waves, protein 3D structures, or genomic sequences, ordinary LLMs face limitations. These data types embody complex patterns that require not only advanced processing to convert them into a format understandable by LLMs but also necessitate the development of special- ized algorithms capable of capturing the intricacies inherent to such information. Healthcare data is inherently multimodal and longitudinal, with only a fraction of data in structured forms. Rather than textual and visual data, most large generative models, including GPT-4V and Gemini, are not able to process a large number of different data modalities. Even a model is adjusted for different data modality, researchers still lack of a reliable set of benchmark to systematically evaluate model performance in medical settings as evaluating generative AI models is challenging. The vast amount of routinely collected real-world clinical data, such as electronic medical records or remote sensing data, represents a significant opportunity for the growth of multimodal AI models. Yet, the AI for medicine community still searches for effective methods to harness these multimodal real-world datasets."}, {"title": "Effective methods", "content": "Unlike many other fields, it is extremely challenging to gather a huge amount of relevant data with high quality for specific practices in healthcare, such as dentistry. Despite rigorous efforts to sanitize and filter the vast amount of training data, it takes a significant amount of efforts, if not impossible, to eliminate all harmful and inappropriate content, which may inadvertently propagate through the responses generated by LLMs. Inherently, these LLMs"}, {"title": "5 Conclusion", "content": "In this review, we explored the potential safety issues of AI in healthcare, highlighting AI's rapid advancements alongside the significant safety challenges that impede its broader adop- tion. We identified key concerns related to AI reliability, including data harmonization, model calibration, generalization, and biases, as well as alignment issues, such as ensuring AI adheres to human-defined objectives. Furthermore, we delved into the specific risks posed by large language models (LLMs) in medical applications, such as hallucinations, privacy concerns, and difficulties in processing complex logic. Ultimately, while AI holds transforma- tive potential for healthcare, addressing these safety risks is crucial for ensuring the ethical, reliable, and secure deployment of AI technologies in real-world medical settings."}]}