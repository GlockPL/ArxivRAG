{"title": "Learning Generative Interactive Environments By Trained Agent Exploration", "authors": ["Naser Kazemi", "Nedko Savoy", "Danda Paudel", "Luc Van Gool"], "abstract": "World models are increasingly pivotal in interpreting and simulating the rules and actions of complex environments. Genie, a recent model, excels at learning from visually diverse environments but relies on costly human-collected data. We observe that their alternative method of using random agents is too limited to explore the environment. We propose to improve the model by employing reinforcement learning based agents for data generation. This approach produces diverse datasets that enhance the model's ability to adapt and perform well across various scenarios and realistic actions within the environment. In this paper, we first release the model GenieRedux- an implementation based on Genie. Additionally, we introduce GenieRedux-G, a variant that uses the agent's readily available actions to factor out action prediction uncertainty during validation. Our evaluation, including a replication of the Coinrun case study, shows that GenieRedux-G achieves superior visual fidelity and controllability using the trained agent exploration. The proposed approach is reproducable, scalable and adaptable to new types of environments. Our codebase is available at https://github.com/insait-institute/GenieRedux.", "sections": [{"title": "Introduction", "content": "Recently, world models have emerged as tools for understanding rules, meaning and consequences of actions in increasingly complex environments. World models have developed from rough imagination models assisting reinforcement learning agents Chiappa et al. (2017), Ha and Schmidhuber (2018), Hafner et al. (2019), Hafner et al. (2023), Sekar et al. (2020) to independent realistic video generation models conditioned on actions Micheli et al. (2022),Chen et al. (2022), Yang et al. (2024),Robine et al. (2023). For example, works like Menapace et al. (2021), Yang et al. (2023), Bruce et al. (2024), Hu et al. (2023), simulate real-world environments.\nNotably, Bruce et al. (2024) propose Genie - a model capable of learning from many visually different environments with the same behavior - particularly platformer games. This allows the model to apply the learned per-frame motion controls to new unseen images. Moreover, Genie incorporates a Latent Action Model predicting actions and enabling the model to be trained on action-free data. We recognize that using multiple environments is an important step towards generalizable world models.\nHowever, Genie's approach is to use human demonstrations of exploring environments - they obtain a large scale dataset by collecting and cleaning online playthrough videos of platformer games. Such datasets are difficult to build and switching to a different kind of environment requires another costly human action data collection or recording. As an alternative to human demonstrations, the authors"}, {"title": "Methodology", "content": "GenieRedux consists of three components, as shown in Fig. 1. A video tokenizer encodes input frame sequences into spatio-temporal tokens. A Latent Action Model encodes input frame sequences into spatio-temporal tokens. A dynamics model predicts the next frame based on frame tokens and actions. We adhere closely to Genie's specifications for implementing these components.\nST-ViViT. All components use the Spatiotemporal Transformer (STTN) architecture Xu et al. (2020), with ST-Blocks that capture spatial and temporal patterns using separate attention layers for efficiency. Causal temporal attention allows for multiple future predictions at once. ST-ViViT is an encoder-decoder model with a VQ-VAE objective Van Den Oord et al. (2017) for generating discrete tokens, inspired by C-ViViT Villegas et al. (2022) but with more efficient ST-Blocks. The encoder alternates spatial and temporal attention, mirrored by the decoder. Position Encoding Generator (PEG) Chu et al. (2021) is used for spatial and temporal attention, while Attention with Linear Biases (ALiBi) Press et al. (2021) is used for temporal attention.\nGenieRedux. The video tokenizer is an ST-ViViT autoencoder, while the Latent Action Model (LAM) is an ST-ViViT encoder-decoder predicting the next frame by generating a token for the action between the last two frames (with a linear layer at the encoder). We offer two dynamics model variants: GenieRedux, which follows Genie by summing LAM encoded actions with tokenized frames, and GenieRedux-G, which uses the concatenation of frame tokens with one-hot agent actions,"}, {"title": "Experiments", "content": "Baseline Evaluation. In this experiment we repeat the original case study with a random agent, as advised by Bruce et al. (2024) and evaluate our implementation of the GenieRedux-Base and GenieRedux-G-Base models and their components on the Basic Test Set. We show visual fidelity results on Tab. 1. FID is not reported for some models due to slight image processing differences, with other metrics unaffected. We note that in the original case study of Genie scores are not reported. However, we compare our tokenizer's 38.25 PSNR with the reported tokenizer's 35.7 PSNR in their Appendix C.2. We also observe high visual fidelity for our LAM. In spite of that, we observed LAM to struggle with capturing actions. This lead to insufficient action conditioning for the dynamics of GenieRedux-Base. However, GenieRedux-G-Base demonstrates high quality visual fidelity, able to consistently respond to actions and progressing motions over time (demonstrated in App. B). Note that the evaluation of dynamics consists of predicting 10 images in the future, given a single image and the actions to perform. The prediction a single step one with 25 MaskGIT iterations.\nTrained Agent Exploration Models Evaluation. In this experiment, we evaluate our models trained with the trained agent exploration, rather than the random agent - GenieRedux-TA and GenieRedux-G-TA. The evaluation set is the Basic Test Setto match the classic case study. Visual fidelity results are shown in Tab. 2. Tokenizer-TA and LAM-TA show significantly improved visual fidelity compared to the Base models. While LAM still struggles to represent most of the actions, we mark improvement as the jump action is better captured. GenieRedux-TA is negatively affected by LAM's uncertainty, but it is able to perform the jump action consistently (see App. C). Meanwhile, GenieRedux-G-TA shows good visual quality and is consistently able to enact all environment actions and progress motions, as seen on Fig. 4 (more in App. E). All actions are demonstrated on Fig. 2.\nComparison between Trained and Random Exploration. In this experiment we select our best models so far and compare trained exploration (GenieRedux-G-Base) with random agent exploration (GenieRedux-G-TA) on the various scenarios in the Diverse Test Set. Tab. 3 shows that trained agent exploration outperforms random exploration in terms of visual fidelity. Moreover, it offers a significant gain in controllability, when comparing with the $\u25b3_1$PSNR metric, defined in Bruce et al. (2024). This is also demonstrated on Fig. 2.\nComparison with Jafar. We compare with Jafar Willi et al. (2024) - a concurrent with ours imple- mentation of Genie (in JAX). We obtain and train their model as instructed. We train GenieRedux- Base with Jafar's model parameters. Despite GenieRedux-Base not representing actions well, it shows significantly better visual fidelity metrics, achieving 17.91 PSNR (46.12 FID), compared to Jafar's 12.66 PSNR (154.12 FID). GenieRedux-Base does not exhibit Jafar's artifacts or the reported problematic \u201chole digging\u201d behavior (more in App. D). Moreover, we observe that Jafar lacks causality, which poses issues when predicting sequence content from input.\nPrediction Horizon Evaluations. We evaluate our best model's controllability over varying predic- tion horizons on Fig. 3. As expected, predictions become more challenging further into the future. The first prediction is also difficult due to insufficient motion information - we obtain 0.4 \u25b3PSNR for t = 1. To address this issue, we provide the model with 4 frames and actions, and observe an improvement from 24.79 PSNR (12.75 FID) to 25.88 PSNR (12.41 FID) on Diverse Test Set."}, {"title": "Conclusion", "content": "In this work, we revisited Bruce et al. (2024)'s approach, noting that while Genie achieves strong results, it relies on costly human data and limited random agent exploration. We address these limitations by demonstrating that RL-based exploration provides a scalable, effective alternative, enhancing the generalizability and efficiency of world models in complex environments."}]}