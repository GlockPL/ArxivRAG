{"title": "Paving the way toward foundation models for irregular and unaligned Satellite Image Time Series", "authors": ["Iris Dumeur", "Silvia Valero", "Jordi Inglada"], "abstract": "Abstract-Although recently several foundation models for satellite remote sensing imagery have been proposed, they fail to address major challenges of real/operational applications. Indeed, embeddings that don't take into account the spectral, spatial and temporal dimensions of the data as well as the irregular or unaligned temporal sampling are of little use for most real world uses. As a consequence, we propose an ALIgned Sits Encoder (ALISE), a novel approach that leverages the spatial, spectral, and temporal dimensions of irregular and unaligned SITS while producing aligned latent representations. Unlike SSL models currently available for SITS, ALISE incorporates a flexible query mechanism to project the SITS into a common and learned temporal projection space. Additionally, thanks to a multi-view framework, we explore integration of instance discrimination along a masked autoencoding task to SITS. The quality of the produced representation is assessed through three downstream tasks: crop segmentation (PASTIS), land cover segmentation (MultiSenGE), and a novel crop change detection dataset. Furthermore, the change detection task is performed without supervision. The results suggest that the use of aligned representations is more effective than previous SSL methods for linear probing segmentation tasks. Additionally, the experiments show that ALISE representations are suitable for change detection. Lastly, the code and datasets are released at https://src.koda.cnrs.fr/iris.dumeur/alise.", "sections": [{"title": "I. INTRODUCTION", "content": "Over the past decade, a number of satellite missions have been launched with the objective of monitoring the changes induced by climate change. To detect these shifts, missions such as Sentinel-2 [1] provide multi-spectral land surface imagery with a high temporal revisit. These data, which can be exploited in the form of Satellite Image Time Series (SITS), provide crucial information for Earth monitoring tasks such as land use classification, agricultural management, climate change or disaster monitoring [2], [3], [4], [5]. However, these applications often lack labeled data, which hinders the development of scalable methods that cover a wide range of temporal and geographical configurations. Therefore, pre-trained foundation models are a promising solution to significantly reduce the need for labeled data in these applications. Thanks to their self-supervised pre-training, these models can learn from vast unlabeled datasets. However, despite their potential\nand the abundance of open-source satellite data, pre-trained remote sensing foundation models with SITS remain largely unexplored. In this paper, we address three key obstacles to constructing remote sensing foundation models designed to generating easy-to-use and meaningful SITS representations.\nFirst, due to the critical role of temporal signals in Earth monitoring, remote sensing foundation models must take into account the specificities of SITS. These time series often have varying acquisition dates and revisit frequencies, leading to unalignment and irregularity, respectively. We posit that existing methods [6], [7], [8], [9] do not produce SITS representations that are user-friendly for geoscientists. We propose that to ensure usability, the pre-trained model should require no further training for downstream tasks (remain frozen), and the latent representation should be aligned and of fixed dimension. In contrast, current methods generate SITS representations with temporal dimensions matching those of the input SITS, resulting in non-aligned representations of variable temporal size.\nSecond, the pre-training strategy used to train a foundation model should yield meaningful SITS representations. Masked auto-encoders have been frequently employed for SITS pre-training due to their ease of implementation [7], [6], [8], [9]. However, these strategies predict in a low-semantic space, which can limit the extraction of high-level semantic features in the representations [10]. In contrast, other self-supervised learning (SSL) techniques propose to perform the self-supervision at the latent space level. For example, instance discrimination strategies are multi-view SSL techniques designed to maximize the similarity between representations of two views from the same input data while avoiding representation collapse. Instance discrimination remains largely unexplored in SITS because it requires aligned representations, specific domain data augmentation, and often benefits from large batch sizes. Moreover, recent researches [11], [12] suggest combining various SSL strategies, such as instance discrimination with masked auto-encoders, to learn more meaningful representations.\nThird, while several foundation models [9], [13] in remote sensing are evaluated on classification tasks, most remote sensing applications necessitate high spatial resolution semantic maps. Despite some growth, there remains a scarcity of downstream labeled segmentation datasets for SITS, limiting the assessment of foundation models in producing meaningful spectro-spatio-temporal SITS representations.\nGiven the above challenges, we propose an Aligned SITS Encoder (ALISE) as a new step toward developing a foundation model for SITS. Our approach addresses the previously"}, {"title": "II. RELATED WORKS", "content": "A. Masked auto-encoder on SITS\nMasked auto-encoders (AE) with Transformer architecture [16] were popularized thanks to the great performance ob-tained by BERT [17] in NLP. The masked AE strategy involves corrupting several elements (tokens) of the input sequence and training the model to recover these corrupted tokens. For SITS, masked auto-encoders employ either a temporal mask-ing strategy or a spatio-temporal masking strategy, depending on whether a temporal transformer or Vision Transformer (ViT) is used, respectively. On one hand, in fully temporal masking strategies, the Transformer processes pixel-level time series and is trained to recover corrupted acquisitions. The models are either fully-temporal such as SITS-BERT [7] and Presto [9], or spatio-temporal such as SITS-Former [6] and U-BARN [8]. In these two latter configurations, the Transformer backbone is merged with a spectral spatial preprocessing. The Transformer processes pixel-level time series of pseudo spectral-spatial features. On the other hand, motivated by the success of masked auto-encoders with ViT, other works such as SatMAE [13] and Prithvi [18] propose fully-attentional spatio-temporal masking for SITS. In these methods, each input image of the SITS is divided into small patches, and the pre-training involves reconstructing these masked patches. However, this spatio-temporal attention limits the input size, leading SatMAE and Prithvi to process SITS with only three temporal acquisitions [8].\nThese two families of methods also differ in how they handle corrupted tokens. Inspired by the original BERT [17], methods with temporal masking provide the corrupted tokens directly to the SITS encoder. In contrast, spatio-temporal meth-ods, inspired by masked auto-encoders in vision [19], use an asymmetric encoder-decoder architecture. Here, the corrupted tokens are not fed to the encoder but are concatenated to the input representations and processed solely by the decoder using a self-attention mechanism. Additionally, recent masked auto-encoders for regular time series such as [20], [21] employ a lightweight decoder that performs cross-attention between corrupted tokens and the latent representation.\nAnother interesting idea from regular time series processing is the proposed masking pattern. While retrieving a masked word in NLP requires a holistic understanding of the sentence, neighboring data points in time series or image processing are highly correlated. Therefore, several studies [21], [20], [22] advocate splitting the time series into non-overlapping temporal sub-series before model processing and applying the masking strategy at the sub-series level to force the model to reconstruct local variations. However, this methodology is not directly applicable to irregular SITS, where each sub-series would represent different temporal scales.\nConsequently, unlike several previous studies on SITS [7], [6], [8], [13], our approach masks successive acquisitions, and the reconstruction task utilizes a lightweight decoder with cross-attention."}, {"title": "B. Instance discrimination self-supervised learning", "content": "As per [23], we consider instance discrimination as a subset of SSL, where a siamese network is trained to produce similar representations of two views of the same data. These multi-view SSL techniques can be divided into four categories: contrastive [24], clustering [25], [26], distillation [27], [28], [29] and redundancy reduction [30], [31], [32]. These ap-proaches differ in their strategies to prevent representation collapse. First, contrastive learning [24] and its variants for segmentation tasks [33] heavily rely on negative pair sam-pling. Efficient negative pair sampling is challenging for SITS because pixels from different SITS may still represent the same classes. Consequently, for pixel-level SITS classification,"}, {"title": "III. METHOD", "content": "Our method, depicted in Figure 1, consists in the pre-training of an ALIgned SITS Encoder, ALISE, which produces aligned representations for multi-year irregular and unaligned SITS. The details of ALISE architecture are presented in the next section, followed by the description of the multi-view self-supervised learning framework."}, {"title": "A. ALISE: Aligned SITS representation Encoder", "content": "ALISE harnesses the spectral, spatial, and temporal di-mensions of irregular and unaligned input time series $X \\in \\mathbb{R}^{(bs,t,c,h,w)}$, where bs, t, c are respectively the batch, temporal, and spectral dimensions and how the spatial dimensions. Although t may vary for each SITS, ALISE generates a latent representation $Y\\in \\mathbb{R}^{(bs,nq,dmodel,h,w)}$ of fixed dimension, where $d_{model}$ and ng are the channel and temporal sizes of the latent representation. As illustrated in Figure 2, ALISE is composed of two main blocks. First, a Spatial, Spectral and Temporal Encoder (SSTE), noted $\\Psi$ in Equation (1), which corresponds to the U-BARN architecture detailed in [8]. As the original U-BARN was initially designed to handle annual SITS, the positional encoding in ALISE has been modified to process multi-year SITS. Specifically, the temporal information provided to ALISE is not the Day of Year (DoY), but $d_t$, the difference in days between the image acquisition date and a given reference date (03/03/2014).\nSecond, to generate aligned SITS representations, a tem-poral projector processes the irregular and unaligned output of the SSTE, $\\Psi(X)$. Specifically, based on the Perceiver I/O mechanism proposed in [35], the temporal projector consists in a temporal cross-attention mechanism between learnable queries and $\\Psi(X)$ to project $\\Psi(X)$ into a common temporal projection. The scaled dot product of the cross-attention is detailed in Equation (1) with $Q_\\alpha$ the learnable queries, X the input time series, $d_{model}$ the number of features in $\\Psi(X)$ and $\\sigma$ the softmax function. The attention product is fully temporal, thus $Q_\\alpha\\in \\mathbb{R}^{(n_q,d_{model})}$, $W_1\\in \\mathbb{R}^{(d_{model},d_{model})}$ and $\\Psi(X) \\in \\mathbb{R}^{(t,d_{model})}$. The temporal dimension of the latent representation Y is determined by the number $n_q$ of learnable queries. Besides, the temporal projector does not shrink the spatial dimension of $\\Psi(X)$, meaning that each pixel of the SITS is represented by $d_{model}$ features along $n_q$ positions.\n$Y = \\sigma \\bigg(\\frac{Q_\\alpha W_1[\\Psi(X)]^T}{\\sqrt{d_{model}}}\\bigg)$"}, {"title": "B. Multi-view pre-training task", "content": "The multi-view SSL task, detailed in Figure 1, combines a cross-reconstruction loss with additional losses computed on the embedded latent representations. As detailed in Equa-tion (2), the total SSL loss, corresponds to the weighted sum of three terms $\\mathcal{L}_{inv}, \\mathcal{L}_{cov}$ and $\\mathcal{L}_{rec}$ respectively the invariance, covariance and reconstruction losses, described in the following sections.\n$\\mathcal{L} = w_{inv} \\mathcal{L}_{inv} + w_{cov} \\mathcal{L}_{cov} + w_{rec} \\mathcal{L}_{rec}$\n1) View generation: The view generation protocol is driven by the need to generate views that preserve semantic meaning. For SITS, we aim to create views that maintain the pixel information of the observed Earth's surface. Consequently, we construct two views, $X^A$ and $X^B$, representing the same location but with different acquisition times. Specifically, first, N adjacent acquisitions are selected among an irregular and multi-year SITS. As detailed in Equation (3), this latter time series is divided along $n_w$ non-overlapping temporal windows composed of $t_w$ dates. Given that SITS are irregular, each subseries may represent a different temporal scale.\n$X = \\bigcup_{i=0}^{n_w-1} \\{ X_i | i \\times t_w \\leq j < (i + 1) \\times t_w \\}$\nFinally, to ensure that the two views cover nearly identical periods, every other sub-series is used to construct respectively $X^A$ (Equation (4a)) and $X^B$ (Equation (4b)). Therefore, $t_w$ corresponds to the number of consecutive dates that the model is trained to reconstruct. We posit that increasing $t_w$ complex-ifies the cross-reconstruction task as more variations should be retrieved by the model. This generation approach ensures that the views are temporally intertwined: $X^A \\bigcup X^B = X$ and $X^A \\bigcap X^B = \\emptyset$ and provides a parameter $t_w$ which controls the difficulty of the pretraining task.\n$X^A = \\bigcup_{i=0}^{\\frac{n_w}{2} -1} \\{ X_i | 2 \\times i \\times t_w \\leq j < (2 \\times i + 1) \\times t_w \\}$\n$X^B = \\bigcup_{i=0}^{\\frac{n_w}{2} -1} \\{ X_i | (2 \\times i + 1) \\times t_w \\leq j < (2 \\times i + 2) \\times t_w \\}$\n2) Latent space losses: As illustrated in Figure 1, the augmented views $X^A, X^B$ are independently encoded by ALISE. The aligned latent representations $Y^A$ and $Y^B$ are then processed into embeddings $Z^A, Z^B$ by a projector in order to eliminate the information by which the two represen-tations differ. Specifically, the projector operates exclusively on the channel dimensions: $\\pi_\\omega : \\mathbb{R}^{(d_{model})} \\rightarrow \\mathbb{R}^{(d_{emb})}$. In other words, pixel-level latent vectors of each $n_q$ query are indepen-dently processed by the projector. We denote $z_{(b,n,i,j)}^k \\in \\mathbb{R}^{d_{emb}}$ the pixel-level embedded vector of $Z^k$ located at the spatial position (i,j) for the nth query and kth batch position. We propose to compute the invariance and covariance losses on the embeddings $Z^A$ and $Z^B$. First the invariance loss maximizes the similarity between the embedded vectors $z^A$ and $z^B$ (see Equation (5)). As $X^A$ and $X^B$ have distinct acquisition dates but cover the same time-period, $\\mathcal{L}_{inv}$ aims at learning representations which are invariant to the acquisition dates.\n$\\mathcal{L}_{inv}(Z^A, Z^B) = \\frac{1}{bs \\times n_q \\times h \\times w} \\sum_{(b,n,i,j)}||z_{(b,n,i,j)}^A - z_{(b,n,i,j)}^B ||_2$\nSecond, we also investigate whether the covariance loss allows learning better representations. The covariance loss decorre-lates the $d_{emb}$ different features. The total covariance loss, Equation (7), corresponds to the sum of the covariance loss computed for each embedding $Z^k$. For centered embeddings $Z \\in \\mathbb{R}^{(bs \\times n_q \\times h \\times w, d_{emb})}$ the covariance loss aims to minimize the off-diagonal values of the co-variance matrix C(Z) in Equation (6). In other words, the covariance matrix of the $d_{emb}$ variables, is estimated on a batch composed of $b_s \\times n_q \\times h \\times w samples. In subsection VII-D, we discuss how these latent losses are related to the VicRegL [32] losses.\n$l_{cov} (Z) = \\frac{1}{d_{emb}} \\sum_{i \\neq j} [C(Z)]^2$\n$\\mathcal{L}_{cov} = l_{cov}(Z^A) + l_{cov} (Z^B)$\n3) Cross reconstruction loss: As depicted in Figure 3, the latent representations $Y^A, Y^B$ are also employed in a cross-reconstruction task. A specific fully-temporal decoder using a\ncross-attention mechanism followed by a fully connected layer is trained to recover the latent representation of one view from the other. The fully-connected layer operates exclusively on the channel dimension of each pixel of the images, to recover the Sentinel-2 bands from the $d_{model}$ features. As proposed in [20] the cross-attention mechanism exploits $Q_\\beta \\in \\mathbb{R}^{(t_w \\times n_w, d_{model})}$ which specifies the dates to be reconstructed. As detailed in Equation (8), $Q_\\beta$ corresponds to the sum of a shared learnable masked token $M_\\beta \\in \\mathbb{R}^{(d_{model})}$ with the temporal positional encoding\u00b9 of the acquisition to reconstruct. Additionally, as described in Equation (9), the latent representation $Y^k$ with $k\\in \\{A,B\\}$, is used to construct the keys $Y^kW_1$ and the values $Y^k$.\n$Q_\\beta = [M_\\beta + PE(\\delta t_i)]_{1 < i < t_w \\times n_w}$\n$Cross Attention(Q_\\beta, Y^k) = \\sigma\\bigg(\\frac{Q_\\beta W_1 W_1^T (Y^k)^T}{\\sqrt{d_{model}}}\\bigg)Y^k$"}, {"title": "IV. EXPERIMENTAL SETUP", "content": "First, the four Sentinel 2 L2A data-sets used in our dif-ferent experiments are presented: the novel unlabeled large scale data-set used for pre-training U-BARN and the three downstream labeled data-sets (PASTIS, MultiSenGE and the novel RotCrop). Secondly, the implementation details of our two type of downstream tasks setup (segmentation and change detection) as well as the corresponding competitive works are described.\nA. Datasets\nALISE is pre-trained on a large scale multi-year European dataset. Besides, three labeled datasets are used to assess the quality of the pre-training. The geographical distribution of the different datasets used is presented in Figure 4. For these data-sets, only the four 10 m and the six 20 m resolution bands of S2 are used. The 20m resolution bands are resampled onto the 10 m resolution grid by bi-cubic interpolation. Similarly to [8], a robust data normalization is applied on S2 L2A reflectances. Due to GPU memory limitation, ALISE is trained to process SITS with a spatial dimension of 64 x 64. If the used dataset provides larger images, a random crop 2 (resp. center crop) is operated during training (resp. validation/testing) steps.\n1) European unlabeled pre-training dataset: We have built an unlabeled dataset composed of multi-year S2 SITS acquired from January 2017 to December 2020. It is divided into training and validation sets with respectively 1920 and 180 SITS of spatial dimension 64 \u00d7 64. The downloaded S2 SITS correspond to data processed by Sen2cor\u00b3. The validity mask employed in the cross-reconstruction task is built thanks to the information provided by SLC and CLM layers\u2074. Specifically, as shown in Figure 4, the pre-training dataset gathers data from 18 S2 tiles. To build the training dataset, 10 smaller regions of interest (ROIs) of size 512 \u00d7 512 are randomly selected from each tile. The disjoint validation dataset is composed of the remaining 6 S2 tiles, from which 30 ROIs of size 128 \u00d7 128 are randomly drawn. The pre-trained model with the lowest loss on the validation set is selected for downstream task assessment.\n2) PASTIS crop segmentation: The PASTIS dataset [14] provides labels for 18 crop classes from the French Land Parcel information System. The SITS considered in our ex-periments are collected from January to December 2019. The complete dataset contains 2433 SITS and it is divided into 5 stratified folds. In line with [8], the segmentation task is performed exclusively on known crop classes. Background and void class are ignored. The competitive method Presto requires cloud masks. As these data are not available in the original PASTIS dataset, the raw Sentinel-2 L2A and their\n3) MultiSenGE land cover segmentation: MultiSenGE [15] is a dense land cover labeled dataset for eastern France in 2020. It is composed of 5 urban classes and 9 natural classes. We selected 4145 SITS with a spatial dimension of 256 \u00d7 256. Only images with less than 10% cloud cover were selected [15] and no cloud masks are provided. SITS are composed of 3 to 14 acquisitions. In contrast to PASTIS, MultiSenGE pro-vides dense labels. A random split is performed to divide the dataset into training (60%), validation (16%) and test (24%). Lastly, in opposition to the two previous datasets MultiSenGE data are preprocessed with Theia and not Sen2cor.\n4) RotCrop Crop change detection: This paper introduces a novel dataset for change detection. The dataset was generated using labels provided by RPGExplorer [37]. For this dataset, the following classes were selected based on the RPG (Registre Parcellaire Graphique) labels: rapeseed, cereals, proteagi-nous, soybean, sunflower, maize, rice, tubers, and grassland. These classes categorize vegetation based on its physiological characteristics and can be identified using remote sensing data. Pixels that are not part of these crops for the two years 2019 and 2020 considered as background. Then, the label change is assigned to pixels that have a different label between 2019 and 2020. Each dataset sample includes Sentinel L2A SITS for 2019 and 2020, along with their corresponding labels. The label tensor has three channels containing crop labels for 2019, 2020, and change label. In our proposed downstream task, change detection is performed while ignoring background pixels. The SITS were built using the SITS spatial extent from PASTIS where sufficient labels from the RPGExplorer were available. Due to this specific selection, the crop classes proteaginous, soybean and tuber do not appear in our dataset. Nevertheless, once accepted, the code used to build this labeled data set will be published, enabling it to be extended to other regions of France and to other years. These missing classes might be integrated in an augmented version of the dataset. Lastly, the change matrix between 2019 and 2020 is detailed in subsection VII-C."}, {"title": "B. Evaluation Protocol", "content": "1) Downstream segmentation tasks: As detailed in Fig-ure 5, we classify the pixel-level latent vector thanks to a single linear layer in both segmentation tasks. Noting the pixel-level latent vector as $y_{(b,h,w)} \\in \\mathbb{R}^{(d_{model}xXnq)}$, the unnormalized logits for each class k at the pixel level can be written as: $c_{(b,h,w)} = y_{(b,h,w)}A + b$ where $A \\in \\mathbb{R}^{(d_{model} \\times nq,k)}$ and $b\\in \\mathbb{R}^k$. The classical cross-entropy loss function is used for training. The latent representations are generated by a pre-trained ALISE whose weights are frozen in linear probing and updated during fine-tuning. We denote the fine-tuning and linear probing configurations as ALISEFT and ALISELP respectively, while the fully supervised model is\n2) Change detection: As detailed in Equation (12), and illustrated in Figure 5, change detection between two SITS $X^1, X^2$ is performed by computing the mean square error between two representations which is averaged along the channel and pseudo-temporal dimensions.\n$d(Y^1,Y^2) = \\frac{1}{n_g \\times d_{model}} \\sum_{n, d, h, w} |Y_{n, d, h, w}^1 - Y_{n,d,h,w}^2 |$\nC. Competitive methods\n1) SITS segmentation concurrent works.: We compare the ALISE architecture with two fully supervised baselines, UTAE [14] and U-BARNFS[8]. The representation from the pre-trained ALISE is compared to two other masked AE SSL frameworks for the segmentation tasks.\n1) Presto. In Presto, to process irregular SITS from dif-ferent sensors, the time series are aligned on a common temporal grid corresponding to the least cloudy scene of each month. This sampling protocol does not ensure that each pixel of the image has a clear acquisition. Therefore, as usually operated in remote sensing, we train Presto with SITS composed of the median value of each band among the cloud-free acquisitions of each month. To exploit the latent representations provided by Presto a temporal mean is performed [9].\n2) U-BARN. U-BARN [8] is a spatio-spectro-temporal SITS encoder pre-trained as an MAE. As U-BARN does not encode SITS into a fixed size latent representation, the shallow classifier (SC) with a mean query attention mechanism proposed in [8] is considered here. Com-pared to the original implementation, we have modified the positional encoding so that U-BARN can process multi-year SITS. Besides, we have pre-trained U-BARN on our European unlabeled dataset with the same pre-training configuration as ALISE. We call these SSL models U-BARNFT and U-BARNFR to denote the fine-tuning and frozen configurations."}, {"title": "V. EXPERIMENTS", "content": "This section evaluates the representations provided by the pre-trained ALISE on three downstream tasks and compares them to competitive methods. First, we present a detailed analysis of ALISE's performance in both fine-tuned and frozen configurations for the two segmentation tasks (PASTIS and MultiSenGE). We also examine the effectiveness of the pre-training under a scenario of severe labeled data scarcity. Next, we provide an extensive discussion on the influence of various pre-training parameters (tw, Ng, Wrec, Winv, Wcov).\nA. Segmentation tasks results\nTable II presents the averaged F1 on the PASTIS and MultiSenGE segmentation tasks. Additional metrics are given in the supplementary materials\nFirst, although this paper does not focus on the construction of a novel fully supervised framework for SITS, FS architec-tures achieve performances consistent with current SOTA (U-TAE). Then, we observe that, in linear probing, ALISELP out-performs the previous frameworks PrestoLP and U-BARNFR by respectively 41,5% and 8,8% on PASTIS dataset. Differences between ALISE and the two competitive works is illustrated in Figure 6 and further detailed below.\n1) ALISE vs U-BARN: ALISE significantly outperforms U-BARN in linear probing while having a shallower classifier and a smaller latent representation. These results can be explained by the differences between ALISE and U-BARN. ALISE differs from U-BARN in two main aspects: (i) its encoder provides fixed-size, aligned representations, and (ii) the pre-training strategy is different. First, as detailed in subsection III-A, ALISE corresponds to the U-BARN archi-tecture on top of which we have placed a temporal projector. Experiments detailed in subsection V-D show that ALISE'S pre-training is primarily driven by its cross-reconstruction task, which is close to U-BARN's masked AE pre-training. Therefore, we believe the improvement in performance when freezing the pre-trained SITS encoder is largely due to the inclusion of the temporal projector in ALISE. This finding also aligns with the observation that fine-tuning results are similar between ALISE and U-BARN. Typically, pre-training is expected to have a significant impact on fine-tuning results. Consequently, we posit that the performance boost observed with ALISE is due to the pre-training of the temporal projector, which, unlike U-BARN furnish aligned representations. This aligned representations can then be used by a single fully connected layer, without performing a temporal compression as in the U-BARN's shallow classifier.\n2) ALISE vs Presto: We observe that ALISELP outperforms both frozen and fine-tuned configuration of ALISE, by 41.5% and 13,6%, respectively. This unexpected low performance of Presto may be due to several factors. Firstly, Presto is a lightweight fully-temporal architecture, which may not be rel-evant for segmentation tasks. Additionally, due to the required under-sampling protocol (Presto exploits monthly synthesis instead of all available acquisitions), it may miss important temporal variations in comparison to ALISE. Furthermore, the proposed temporal positional encoding in 8 raises ques-tions. In the Transformer model, the positional encoding is usually added or concatenated to the input along the channel dimension. However, from our understanding of the code, in the proposed implementation, the positional encoding is concatenated along the temporal dimension. We do not fully understand the relevance of this choice for the attention mechanism in the Transformer.\nB. Label scarcity scenario\nTo assess the model's behavior under a severe data scarcity scenario, a smaller version of the PASTIS dataset has been created. Following the approach in [8], five smaller datasets, each composed of 30 SITS, are created each PASTIS fold. Therefore, the results shown in Table III correspond to the averaged macro F1 score across 25 trials. Under severe data scarcity, the fine-tuned model outperforms the fully-supervised framework by 12.5%. Interestingly, the frozen ALISE also outperforms its fully-supervised configuration by 9.7%. Given"}, {"title": "C. Change detection task", "content": "To evaluate the relevance of the frozen ALISE represen-tations for change detection, we compare the Area Under the ROC Curve (AUC) score of the distance map computed between the representations of SITS from two different years. The AUC on the novel crop change detection dataset is presented in Table IV. As expected, ALISE provides repre-sentations that are relevant for change detection. Besides, in contrast to the Gap-filling method, U-BARN do not require cloud mask information. Furthermore, even though we do not provide information on the annual periodicity of the SITS in the temporal encoding, ALISE can still learn the invariance of SITS between different years. For a qualitative analysis of the change detection task, see subsection VII-A."}, {"title": "D. Influence of tw", "content": "Increasing $t_w$ is assumed to have a dual effect: increasing the difficulty of the reconstruction task while creating more discrepancy between views. Therefore, we aim to assess the co-influence of the view generation protocol (controlled by $t_w$) and the losses weights. Therefore, we detail here the result obtained by conducting four pre-trainings with different seed for each $(t_w, w_{inv}, w_{cov}, w_{rec})$ configuration and assessing each of them on five PASTIS fold. Between all these pre-trained models, only the losses weights and $t_w$ vary. All other hyper-parameters are fixed. We set the covariance weight"}, {"title": "E. Impact of ng", "content": "For practical purposes, it is relevant to reduce ng while preserving the downstream tasks performances. Figure 9 plots the segmentation performances on the PASTIS dataset as a function of ng. For each configuration, one pre-training was done, and the performance was assessed on one out of the five available PASTIS experiments. We observe that increasing the value of ng improves downstream task per-formance. This can be attributed to two factors. Firstly, a larger value of ng results in a larger classifier during linear probing. Secondly, it is assumed that a smaller value of ng makes the cross-reconstruction task more challenging due to temporal compression in the temporal projector. This effect could penalize the cross-reconstruction pre-training task. The second hypothesis is reinforced by a second experiment. We observe a strong drop of performances on experiments with $w_{inv} = 0, w_{cov} = 0$ between nq = 10 and nq = 1. Addition-ally, unlike when nq = 10, when operating strong temporal compression (nq = 1) there is a significant improvement in segmentation performances when the invariance loss is used. Furthermore, Table V studies the impact of additional latent losses in a high-temporal compression configuration (nq = 1) for two different values of tw. While as observed in Figure 7, at tw = 5 and ng = 10 latent losses degrade the linear probing segmentation task, we observe conversely with ng = 1 and tw = 5 a 9.6% gain in F1 score when using latent losses. Nevertheless, unexpectedly, for tw = 2 and nq = 1, we do not obtain a similar trend. Given that our findings are based on one pre-training, these results should consequently be interpreted with caution, and further experiments could be conducted to extract a more meaningful trend. Nevertheless, this result underlines that the correct balance between the losses weights and tw also heavily depends on nq. Each of these parameters (tw, nq, Wrec, Winv, Wcov) influences pre-training in its own way, and understanding the interaction between them remains a challenge. A discussion on the effect of the batch size and"}, {"title": "VI. CONCLUSION", "content": "This paper paves the way toward the construction of re-mote sensing foundation model. Our proposed method, named ALISE, leverages the spatial, spectral, and temporal dimen-sions and generates aligned and fixed dimensional representa-tions of irregular and unaligned multi-year SITS. In the novel devised a multi-view SSL pre-training task, we have explored the contribution of instance discrimination SSL approaches to MAE on SITS. First, it has been demonstrated that in linear probing on crop segmentation and land cover segmentation downstream tasks ALISE outperforms competitive methods Presto [9] and U-BARN [8]. Therefore, in contrast to existing works, we provide aligned representations which are mean-ingful and easy-to-use. Indeed, due to their fixed-dimension, these representations could be used by traditional machine learning algorithms, which are often employed by geoscientists to address numerous earth monitoring tasks. Additionally, we have proposed a novel crop change detection downstream task, named CropRot to assess foundation model on SITS. Our results demonstrate that the proposed aligned SITS rep-resentations can be used for downstream change detection"}, {"title": "VII. APPENDIX", "content": "A. Change detection qualitative analysis\nGiven two annual irregular and unaligned SITS from 2019 and 2020, Figure 10 illustrates the change map produced. In this example, the 2020 SITS is composed of more dates during spring than the 2019 SITS. When performing change detection on SITS, different agricultural practices, meteoro-logical events, and different acquisition dates cause important intra-class variablity. We observe in Figure 10 this intra-class variability when looking at the fields located at the center bottom of the SITS. Although the crop class of this field has not changed between 2019 and 2020"}]}