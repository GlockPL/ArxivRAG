{"title": "Paving the way toward foundation models for\nirregular and unaligned Satellite Image Time Series", "authors": ["Iris Dumeur", "Silvia Valero", "Jordi Inglada"], "abstract": "Abstract-Although recently several foundation models for\nsatellite remote sensing imagery have been proposed, they fail\nto address major challenges of real/operational applications.\nIndeed, embeddings that don't take into account the spectral,\nspatial and temporal dimensions of the data as well as the\nirregular or unaligned temporal sampling are of little use for\nmost real world uses. As a consequence, we propose an ALIgned\nSits Encoder (ALISE), a novel approach that leverages the\nspatial, spectral, and temporal dimensions of irregular and\nunaligned SITS while producing aligned latent representations.\nUnlike SSL models currently available for SITS, ALISE incor-\nporates a flexible query mechanism to project the SITS into a\ncommon and learned temporal projection space. Additionally,\nthanks to a multi-view framework, we explore integration of\ninstance discrimination along a masked autoencoding task to\nSITS. The quality of the produced representation is assessed\nthrough three downstream tasks: crop segmentation (PASTIS),\nland cover segmentation (MultiSenGE), and a novel crop change\ndetection dataset. Furthermore, the change detection task is\nperformed without supervision. The results suggest that the use\nof aligned representations is more effective than previous SSL\nmethods for linear probing segmentation tasks. Additionally, the\nexperiments show that ALISE representations are suitable for\nchange detection. Lastly, the code and datasets are released at\nhttps://src.koda.cnrs.fr/iris.dumeur/alise.", "sections": [{"title": "I. INTRODUCTION", "content": "Over the past decade, a number of satellite missions have\nbeen launched with the objective of monitoring the changes\ninduced by climate change. To detect these shifts, missions\nsuch as Sentinel-2 [1] provide multi-spectral land surface\nimagery with a high temporal revisit. These data, which can be\nexploited in the form of Satellite Image Time Series (SITS),\nprovide crucial information for Earth monitoring tasks such\nas land use classification, agricultural management, climate\nchange or disaster monitoring [2], [3], [4], [5]. However, these\napplications often lack labeled data, which hinders the devel-\nopment of scalable methods that cover a wide range of tem-\nporal and geographical configurations. Therefore, pre-trained\nfoundation models are a promising solution to significantly\nreduce the need for labeled data in these applications. Thanks\nto their self-supervised pre-training, these models can learn\nfrom vast unlabeled datasets. However, despite their potential\nand the abundance of open-source satellite data, pre-trained\nremote sensing foundation models with SITS remain largely\nunexplored. In this paper, we address three key obstacles to\nconstructing remote sensing foundation models designed to\ngenerating easy-to-use and meaningful SITS representations.\nFirst, due to the critical role of temporal signals in Earth\nmonitoring, remote sensing foundation models must take into\naccount the specificities of SITS. These time series often\nhave varying acquisition dates and revisit frequencies, leading\nto unalignment and irregularity, respectively. We posit that\nexisting methods [6], [7], [8], [9] do not produce SITS repre-\nsentations that are user-friendly for geoscientists. We propose\nthat to ensure usability, the pre-trained model should require no\nfurther training for downstream tasks (remain frozen), and the\nlatent representation should be aligned and of fixed dimension.\nIn contrast, current methods generate SITS representations\nwith temporal dimensions matching those of the input SITS,\nresulting in non-aligned representations of variable temporal\nsize.\nSecond, the pre-training strategy used to train a founda-\ntion model should yield meaningful SITS representations.\nMasked auto-encoders have been frequently employed for\nSITS pre-training due to their ease of implementation [7],\n[6], [8], [9]. However, these strategies predict in a low-\nsemantic space, which can limit the extraction of high-level\nsemantic features in the representations [10]. In contrast,\nother self-supervised learning (SSL) techniques propose to\nperform the self-supervision at the latent space level. For\nexample, instance discrimination strategies are multi-view\nSSL techniques designed to maximize the similarity between\nrepresentations of two views from the same input data while\navoiding representation collapse. Instance discrimination re-\nmains largely unexplored in SITS because it requires aligned\nrepresentations, specific domain data augmentation, and often\nbenefits from large batch sizes. Moreover, recent researches\n[11], [12] suggest combining various SSL strategies, such as\ninstance discrimination with masked auto-encoders, to learn\nmore meaningful representations.\nThird, while several foundation models [9], [13] in remote\nsensing are evaluated on classification tasks, most remote\nsensing applications necessitate high spatial resolution seman-\ntic maps. Despite some growth, there remains a scarcity of\ndownstream labeled segmentation datasets for SITS, limiting\nthe assessment of foundation models in producing meaningful\nspectro-spatio-temporal SITS representations.\nGiven the above challenges, we propose an Aligned SITS\nEncoder (ALISE) as a new step toward developing a founda-\ntion model for SITS. Our approach addresses the previously"}, {"title": "II. RELATED WORKS", "content": "Masked auto-encoders (AE) with Transformer architecture\n[16] were popularized thanks to the great performance ob-\ntained by BERT [17] in NLP. The masked AE strategy involves\ncorrupting several elements (tokens) of the input sequence\nand training the model to recover these corrupted tokens. For\nSITS, masked auto-encoders employ either a temporal mask-\ning strategy or a spatio-temporal masking strategy, depending\non whether a temporal transformer or Vision Transformer\n(ViT) is used, respectively. On one hand, in fully temporal\nmasking strategies, the Transformer processes pixel-level time\nseries and is trained to recover corrupted acquisitions. The\nmodels are either fully-temporal such as SITS-BERT [7] and\nPresto [9], or spatio-temporal such as SITS-Former [6] and U-\nBARN [8]. In these two latter configurations, the Transformer\nbackbone is merged with a spectral spatial preprocessing.\nThe Transformer processes pixel-level time series of pseudo\nspectral-spatial features. On the other hand, motivated by the\nsuccess of masked auto-encoders with ViT, other works such\nas SatMAE [13] and Prithvi [18] propose fully-attentional\nspatio-temporal masking for SITS. In these methods, each\ninput image of the SITS is divided into small patches, and\nthe pre-training involves reconstructing these masked patches.\nHowever, this spatio-temporal attention limits the input size,\nleading SatMAE and Prithvi to process SITS with only three\ntemporal acquisitions [8].\nThese two families of methods also differ in how they\nhandle corrupted tokens. Inspired by the original BERT [17],\nmethods with temporal masking provide the corrupted tokens\ndirectly to the SITS encoder. In contrast, spatio-temporal meth-\nods, inspired by masked auto-encoders in vision [19], use an\nasymmetric encoder-decoder architecture. Here, the corrupted\ntokens are not fed to the encoder but are concatenated to\nthe input representations and processed solely by the decoder\nusing a self-attention mechanism. Additionally, recent masked\nauto-encoders for regular time series such as [20], [21] employ\na lightweight decoder that performs cross-attention between\ncorrupted tokens and the latent representation.\nAnother interesting idea from regular time series processing\nis the proposed masking pattern. While retrieving a masked\nword in NLP requires a holistic understanding of the sentence,\nneighboring data points in time series or image processing\nare highly correlated. Therefore, several studies [21], [20],\n[22] advocate splitting the time series into non-overlapping\ntemporal sub-series before model processing and applying the\nmasking strategy at the sub-series level to force the model to\nreconstruct local variations. However, this methodology is not\ndirectly applicable to irregular SITS, where each sub-series\nwould represent different temporal scales.\nConsequently, unlike several previous studies on SITS [7],\n[6], [8], [13], our approach masks successive acquisitions,\nand the reconstruction task utilizes a lightweight decoder with\ncross-attention."}, {"title": "III. METHOD", "content": "Our method, depicted in Figure 1, consists in the pre-\ntraining of an ALIgned SITS Encoder, ALISE, which produces\naligned representations for multi-year irregular and unaligned\nSITS. The details of ALISE architecture are presented in the\nnext section, followed by the description of the multi-view\nself-supervised learning framework."}, {"title": "A. ALISE: Aligned SITS representation Encoder", "content": "ALISE harnesses the spectral, spatial, and temporal di-\nmensions of irregular and unaligned input time series $X \\in$\n$[(bs,t,c,h,w)$, where bs, t, c are respectively the batch, temporal,\nand spectral dimensions and how the spatial dimensions.\nAlthough t may vary for each SITS, ALISE generates a latent\nrepresentation $Y\\in \\mathbb{R}^{(bs,nq,dmodel,h,w)}$ of fixed dimension,\nwhere dmodel and ng are the channel and temporal sizes of\nthe latent representation. As illustrated in Figure 2, ALISE\nis composed of two main blocks. First, a Spatial, Spectral\nand Temporal Encoder (SSTE), noted in Equation (1),\nwhich corresponds to the U-BARN architecture detailed in\n[8]. As the original U-BARN was initially designed to handle\nannual SITS, the positional encoding in ALISE has been\nmodified to process multi-year SITS. Specifically, the temporal\ninformation provided to ALISE is not the Day of Year (DoY),\nbut dt, the difference in days between the image acquisition\ndate and a given reference date (03/03/2014).\nSecond, to generate aligned SITS representations, a tem-\nporal projector processes the irregular and unaligned output\nof the SSTE, \u03a8(\u03a7). Specifically, based on the Perceiver I/O\nmechanism proposed in [35], the temporal projector consists\nin a temporal cross-attention mechanism between learnable\nqueries and \u03a8(X) to project \u03a8(X) into a common temporal\nprojection. The scaled dot product of the cross-attention is\ndetailed in Equation (1) with $Q_&$ the learnable queries, X\nthe input time series, dmodel the number of features in (X)\nand o the softmax function. The attention product is fully\ntemporal, thus $Q_a\\in\\mathbb{R}^{(n_g,dmodel)}$, $W_1\\in\\mathbb{R}^{(dmodel,dmodel)}$ and\n$\\Psi(X) \\in \\mathbb{R}^{(t,dmodel)}$. The temporal dimension of the latent\nrepresentation Y is determined by the number ng of learnable\nqueries. Besides, the temporal projector does not shrink the\nspatial dimension of \u03a8(X), meaning that each pixel of the\nSITS is represented by dmodel features along ng positions.\n$Y = \\sigma(\\frac{Q_aW_1[\\Psi(X)]^T}{\\sqrt{dmodel}})$"}, {"title": "B. Multi-view pre-training task", "content": "The multi-view SSL task, detailed in Figure 1, combines\na cross-reconstruction loss with additional losses computed\non the embedded latent representations. As detailed in Equa-\ntion (2), the total SSL loss, corresponds to the weighted\nsum of three terms $L_{inv}$, $L_{cov}$ and $L_{rec}$ respectively the\ninvariance, covariance and reconstruction losses, described in\nthe following sections.\n$L = W_{inv} L_{inv} + W_{cov}L_{cov} + W_{rec}L_{rec}$\n$L_{inv}(Z^A, Z^B) = \\frac{1}{bs \\times n_q \\times h \\times w} \\sum_{(b,n,i,j)} \\|z^A_{b,n,i,j} - z^B_{b,n,i,j}\\|_2$\n$l_{cov}(Z) = \\frac{1}{d_{emb}}\\sum_{i \\neq j} [C(Z)]_{i,j}$\n$L_{cov} = l_{cov}(Z^A) + l_{cov}(Z^B)$\nand provides a parameter tw which controls\nthe difficulty of the pretraining task.\n$X = \\bigcup_{i=0}^{n_w-1} {X_j | i \\times t_w \\leq j < (i + 1) \\times t_w}$"}, {"title": "V. EXPERIMENTS", "content": "This section evaluates the representations provided by the\npre-trained ALISE on three downstream tasks and compares\nthem to competitive methods. First, we present a detailed\nanalysis of ALISE's performance in both fine-tuned and frozen\nconfigurations for the two segmentation tasks (PASTIS and\nMultiSenGE). We also examine the effectiveness of the pre-\ntraining under a scenario of severe labeled data scarcity. Next,\nwe provide an extensive discussion on the influence of various\npre-training parameters ($t_w$, $N_g$, $W_{rec}$, $W_{inv}$, $W_{cov}$). Experiments detailed in subsection V-D show that ALISE'S\npre-training is primarily driven by its cross-reconstruction\ntask, which is close to U-BARN's masked AE pre-training.\nTherefore, we believe the improvement in performance when\nfreezing the pre-trained SITS encoder is largely due to the\ninclusion of the temporal projector in ALISE. This finding\nalso aligns with the observation that fine-tuning results are\nsimilar between ALISE and U-BARN. Typically, pre-training\nis expected to have a significant impact on fine-tuning results.\nConsequently, we posit that the performance boost observed\nwith ALISE is due to the pre-training of the temporal projector,\nwhich, unlike U-BARN furnish aligned representations. This\naligned representations can then be used by a single fully\nconnected layer, without performing a temporal compression\nas in the U-BARN's shallow classifier."}, {"title": "VI. CONCLUSION", "content": "This paper paves the way toward the construction of re-\nmote sensing foundation model. Our proposed method, named\nALISE, leverages the spatial, spectral, and temporal dimen-\nsions and generates aligned and fixed dimensional representa-\ntions of irregular and unaligned multi-year SITS. In the novel\ndevised a multi-view SSL pre-training task, we have explored\nthe contribution of instance discrimination SSL approaches to\nMAE on SITS. First, it has been demonstrated that in linear\nbing on crop segmentation and land cover segmentation"}]}