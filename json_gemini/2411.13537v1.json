{"title": "Metacognition for Unknown Situations and Environments (MUSE)", "authors": ["Rodolfo Valiente", "Praveen K. Pilly"], "abstract": "Metacognition\u2014the awareness and regulation of one's cognitive processes\u2014is central to human adaptability in unknown situations. In contrast, current autonomous agents often struggle in novel environments due to their limited capacity for adaptation. We hypothesize that metacognition is a critical missing ingredient in adaptive autonomous systems, equipping them with the cognitive flexibility needed to tackle unfamiliar challenges. Given the broad scope of metacognitive abilities, we focus on two key aspects: competence awareness and strategy selection for novel tasks. To this end, we propose the Metacognition for Unknown Situations and Environments (MUSE) framework, which integrates metacognitive processes-specifically self-awareness and self-regulation-into autonomous agents. We present two initial implementations of MUSE: one based on world modeling and another leveraging large language models (LLMs), both instantiating the metacognitive cycle. Our system continuously learns to assess its competence on a given task and uses this self-awareness to guide iterative cycles of strategy selection. MUSE agents show significant improvements in self-awareness and self-regulation, enabling them to solve novel, out-of-distribution tasks more effectively compared to Dreamer-v3-based reinforcement learning and purely prompt-based LLM agent approaches. This work highlights the promise of approaches inspired by cognitive and neural systems in enabling autonomous systems to adapt to new environments, overcoming the limitations of current methods that rely heavily on extensive training data.", "sections": [{"title": "1 Introduction", "content": "The pursuit of fully autonomous agents in artificial intelligence (AI) remains a significant challenge. Current autonomous agents are primarily designed for operating environments, conditions, and uses that are known a priori. They rely on either automated scripted behaviors or policies trained via deep reinforcement learning (RL), both of which struggle to handle unknown situations effectively. As a result, when faced with novel environments, they are prone to fail with suboptimal or even detrimental outcomes. This limitation severely restricts their deployment in variable or safety-critical real-world environments, especially for long-duration missions or applications with little to no human oversight. Therefore, there is a critical need to reduce the failure rate, time to completion, and cost of autonomous missions, enabling resilient handling of unknowns during deployment.\nIn mainstream AI, large-scale multi-task pre-training has emerged as the leading approach for enhancing adaptability in autonomous agents [Team et al., 2021]. For example, Adaptive Agent (AdA) [Team et al., 2023] was trained with billions of frames and tasks to enable rapid adaptation to unseen, open-ended tasks. Similarly, RT-2 [Brohan et al., 2023] and RT-X [Collaboration et al., 2023] models leverage large-scale robotic trajectory datasets to train agents capable of solving novel manipulation tasks and generalizing to new robots and environments. However, internet-scale pre-training for every potential change and combination of changes in real-world applications is impractical, prohibitively resource-intensive, and expensive. Even with significantly limited data,\nAI agents must intelligently interpolate and extrapolate beyond their pre-trained scenarios while continually learning and adapting to novelty [Kudithipudi et al., 2023]. In other words, when faced with novel scenarios, pre-trained knowledge must be continuously updated to strike a dynamic balance between stability and plasticity [Grossberg, 1980].\nSimilar to humans, AI agents can leverage pre-deployment training to acquire a wide range of skills across diverse, known scenarios. Importantly, they can also be equipped with the ability to engage in efficient online learning, enabling rapid adaptation to novel situations without catastrophic forgetting of previously acquired knowledge. For example, a teenager attending driving school follows a structured curriculum that teaches foundational vehicle control skills, which are then progressively built upon to master more complex tasks, such as merging onto highways or navigating construction zones. This learning process is cumulative\u2014mastery of foundational skills simplifies the acquisition of more advanced ones. Moreover, the key principles of driving are consolidated in the student's brain, protecting them from catastrophic forgetting. Ultimately, the end of driving school marks the beginning of a lifelong learning process, where the student must draw on prior experiences to navigate novel driving challenges independently, without an instructor.\nMetacognition, defined as the awareness and control over one's cognitive processes, is a key human trait extensively studied in cognitive psychology [Flavell, 1979, Begg et al., 1989, Leonesio and Nelson, 1990, Metcalfe et al., 1993, Koriat, 1993, 1995, 1997, Dunlosky and Metcalfe, 2008]. For instance, students who perform poorly often overestimate their abilities, leading to under-preparation for exams. This is a common issue in education, where overconfident students allocate insufficient time to study, believing they have already mastered the material. Conversely, students who underestimate their knowledge may spend excessive time reviewing topics they already understand, hindering their progress. By leveraging metacognition, students can more accurately assess their knowledge and adjust their study habits accordingly [Cohen, 2012, Chen et al., 2017]. The use of metacognition among college students has been shown to correlate significantly with various measures of academic success [Young and Fry, 2008, Isaacson and Fujita, 2006]. Even children as young as three years old benefit from activities designed to develop metacognitive skills [Chatzipanteli et al., 2014]. Research indicates that self-awareness of competence is a capability that is teachable and improves as one becomes more skilled or knowledgeable, ranging from over-confidence in novices to slight under-confidence in experts [Kruger and Dunning, 1999, Dunning, 2011]. Neuroscience further reveals that the subregions of the prefrontal cortex responsible for metacognitive judgments are distinct from those involved in cognitive functions like visual memory recognition (e.g., Miyamoto et al. [2017]).\nWhile metacognition spans a wide range of capabilities and higher-order cognitive processes (e.g., Feeling of Knowing, Judgment of Learning, Source Monitoring), it can be conceptualized as an internal perception-action loop of self-awareness and self-regulation [Nelson and Narens, 1990, Dunlosky and Bjork, 2013]. Self-awareness in this context refers to an individual's ability to accurately assess their competence regarding a specific task. Self-regulation involves the strategic selection and control of actions based on this self-assessment. This metacognitive flexibility enables humans to learn online efficiently and solve problems iteratively, and thereby generalize and adapt rapidly to new tasks. So the overarching goal of this paper is to equip AI agents with such metacognitive capabilities so they can also achieve more efficient learning and improved generalization to unknown scenarios.\nWe introduce the Metacognition for Unknown Situations and Environments (MUSE) framework to integrate self-awareness and self-regulation into sequential decision-making agents. The self-awareness mechanism is designed to predict the agent's likelihood of successfully completing a given task for proposed action plans, leveraging a continually learning internal model informed by past experiences. The self-regulation mechanism leverages this self-awareness to enable iterative cycles of competence-aware strategy selection. To instantiate the metacognitive cycle for iterative problem-solving that goes beyond the traditional perception-action loop of existing AI agents (Figure 1), we present two initial implementations: one based on world modeling and the other utilizing large language models (LLMs). Our experiments in two distinct environments\u2014Meta-World and ALFWorld-demonstrate that MUSE agents achieve substantial improvements in handling novel scenarios compared to baseline approaches."}, {"title": "2 Related Work", "content": "2.1 Self-Awareness\nSelf-awareness broadly refers to an agent's ability to recognize and assess its internal states, capabilities, and performance in relation to tasks and goals. This ability enables humans to reflect on their skills and make adjustments to improve outcomes [Flavell, 1979, Nelson and Narens, 1990, Schraw, 1998].\nWorld Models: These are generative models of environmental dynamics [Ha and Schmidhuber, 2018, Robine et al., 2023, Micheli et al., 2023, Hansen et al., 2022, 2023], which can be used to estimate expected cumulative reward of agents. These models often employ sequence-based architectures, such as recurrent neural networks, to predict the next model state, reward, and terminal signals. Decoder-based World Models [Hafner et al., 2023, Robine et al., 2023, Micheli et al., 2023] can additionally generate the input state corresponding to the predicted next model state. In contrast, decoder-free World Models focus on predicting the outcomes of actions in the latent and recurrent space, bypassing the need to decode input states [Hansen et al., 2022, 2023].\nWorld Models act as proxy simulators during training, enabling agents to learn more efficiently by reducing the reliance on real-environment interactions. Typically, World-Model-based agents follow an iterative two-stage training procedure: first, they collect a dataset of environment interactions that is then used to update the World Model. Next, the updated World Model is used for offline agent training, often incorporating explicit planning during both training and evaluation [Koul et al., 2020].\nIn this work, we extend the capabilities of decoder-based World Models by training them to predict not only environmental dynamics but also the agent's competence to solve a given task.\nLLM critics: These models are designed to evaluate the performance of LLMs. Typically, the LLM itself is prompted to provide feedback on its outputs [Madaan et al., 2023], intermediate reasoning steps [Paul et al., 2023], or even the prompt itself [Hu et al., 2023]. Some approaches enhance LLM-based reasoning using stochastic beam search guided by self-evaluation [Xie et al., 2023]. Recognizing the limitations of self-evaluation [Huang et al., 2023], researchers have augmented LLM critics with external tools, such as search engines, calculators, and knowledge bases, to improve reliability [Gou et al., 2023]. Retrieval Augmented Generation (RAG) approaches have also been proposed to strengthen self-evaluation [Asai et al., 2023]. In contrast, MUSE does not rely solely on pre-trained knowledge or external tools. Instead, it continuously learns and grounds itself to evaluate its competence on given tasks and uses this real-time awareness to inform policy decisions. This approach enables more robust decision-making, particularly in unfamiliar environments.", "2.2 Self-Regulation": "Self-regulation is the process by which an agent dynamically adjusts its behavior based on self-awareness and external feedback to achieve specific goals [Flavell, 1979, Nelson and Narens, 1990]. This ability is essential for humans to function autonomously in unpredictable environments.\nModel-based Reinforcement Learning (MBRL): In AI, self-regulation has been primarily explored through MBRL. These systems utilize World Models to simulate potential outcomes of actions, enabling the agent to evaluate and select actions that maximize expected cumulative reward. Techniques such as Dyna [Sutton, 1991] and Monte Carlo Tree Search (MCTS) [Silver et al., 2016] are widely used to incorporate planning into decision-making. Dyna, in particular, is a foundational architecture that integrates learning and planning within a single agent. Its core idea is to use the agent's interactions with the environment to not only refine its policy but also update its internal model of the environment. In contrast to traditional MBRL approaches, which prioritize maximizing return, our perspective on self-regulation emphasizes competence as the primary evaluation metric. This shift aims to enhance the agent's ability to navigate and adapt effectively in unknown situations.\nPrompt-based LLM Agents: The capabilities of LLMs extend beyond language generation, making them increasingly popular for reasoning tasks to potentially deal with novelty. Chain-of-Thought (CoT) prompting [Wei et al., 2022], for example, decomposes complex inputs into sequential intermediate steps to arrive at a final answer. However, these methods may not yield accurate results due to error propagation as the number of steps increases [Chen et al., 2022]. Advancements such as self-consistency [Wang et al., 2022], least-to-most prompting [Zhou et al., 2022], and Tree-of-Thought (ToT) prompting [Yao et al., 2024] aim to mitigate this issue by improving sampling strategies and leveraging search algorithms. Nevertheless, these methods typically rely solely on the LLM's pre-trained knowledge, which limits their ability to adapt to external feedback.\nBeyond reasoning tasks, LLMs have also been applied to operate in an agentic loop of perception and action, unlocking the benefits of large-scale pre-training for agent-based tasks without relying on RL. ReAct [Yao et al., 2022] was among the first purely prompt-based LLM agents that integrated both reasoning and action planning to perform text-based task-solving. However, ReAct is inefficient and limited in its ability to transfer performance improvements to subsequent episodes. To address this issue, Reflexion [Shinn et al., 2023] built on ReAct by adding an LLM that reflects on task failures and provides persistent verbal feedback to the policy for improved performance in subsequent episodes. The performance gains from these LLM agent methods rely primarily on enhanced in-context prompting strategies, which limit their ability for continual learning from varied experiences. While MUSE also makes use of prompting for both reasoning and planning, it also learns online from its experiences to facilitate more effective exploration in unknown situations and environments."}, {"title": "3 Decoder-based World Model implementation", "content": "In this section, we describe our implementation of the MUSE framework using a decoder-based world model to equip MBRL agents with metacognitive abilities of self-awareness and self-regulation."}, {"title": "3.1 Methods", "content": "3.1.1 Self-Awareness through World Modeling\nWe leverage the decoder-based World Model from Dreamer-v3 [Hafner et al., 2023] to implement self-awareness for agents, but we note that our approach can be extended to decoder-free World Models as well. Dreamer-v3 uses a Recurrent State-Space Model (RSSM) to model the environment dynamics. See Equations 1-3 for the formulation from Hafner et al. [2023]. The RSSM maps the input state $x_t$ and recurrent state $h_t$ to a latent embedding $z_t$ and uses the concatenation of $h_t$ and $z_t$ to predict the reward $\\hat{r}_t$ and terminal signal $\\hat{d}_t$ as well as reconstruct the input state $\\hat{x}_t$.\n$h_t = f(h_{t-1}, z_{t-1}, a_{t-1})$\nht = f(ht\u22121, Zt\u22121, at-1)\nSequence model:\n$z_t \\sim q_{\\phi}(z_t | h_t, x_t)$\nzt~q\u03c6(zt|ht,xt)\nDynamics predictor:\n$\\hat{z}_t \\sim p_{\\theta}(z_t | h_t)$\n\u02c6zt~p\u03b8(zt|ht)\nReward predictor:\n$\\hat{r}_t \\sim p_{\\theta}(r_t | h_t, z_t)$\n\u02c6rt~p\u03b8(rt|ht,zt)\nTerminal signal predictor:\n$\\hat{d}_t \\sim p_{\\theta}(d_t | h_t, z_t)$\n\u02c6dt~p\u03b8(dt|ht,zt)\nDecoder:\n$\\hat{x}_t \\sim p_{\\theta}(x_t | h_t, z_t)$\n\u02c6xt~p\u03b8(xt|ht,zt)\n(1)\nThe RSSM employs a convolutional neural network (CNN) to process image-based inputs and a multi-layer perceptron (MLP) to process proprioceptive inputs, combining them into a joint latent embedding. Independent MLP heads are trained to map the RSSM state to distributions over the reward, terminal signal, and next latent embedding. Note that aside from CNN and MLP networks, alternative architectures such as GPT-style transformers can also be utilized [Brown et al., 2020, Radford et al., 2019].\nFollowing Hafner et al. [2023], given a sequential batch of inputs $x_{1:T}$, actions $a_{0:T}$, rewards $r_{1:T}$, and terminal signals $d_{1:T}$, the World Model parameters $\\theta$ are optimized to minimize the prediction loss $\\mathcal{L}_{pred}$, the dynamics loss $\\mathcal{L}_{dyn}$, and the representation loss $\\mathcal{L}_{rep}$. The prediction loss $\\mathcal{L}_{pred}$ is the joint negative log-likelihood of the multiple probabilistic predictors (Equation 3). Real-valued quantities like the reward and decoded state are trained with a symlog squared loss, whereas the terminal signal, which is a binary-valued quantity, is trained with logistic regression.\n$\\mathcal{L}(\\phi) = E_q \\Big[ \\sum_{t=1}^{T} (\\mathcal{L}_{pred}(\\phi) + \\mathcal{L}_{dyn}(\\phi) + 0.1\\mathcal{L}_{rep}(\\phi)) \\Big]$\nL(\u03c6)=Eq[\u2211T t=1(Lpred(\u03c6)+Ldyn(\u03c6)+0.1Lrep(\u03c6))]\n(2)\n$\\mathcal{L}_{pred} = -lnp_{\\theta}(r_t | h_t, z_t) - lnp_{\\theta}(d_t | h_t, z_t) - ln p_{\\theta}(x_t | h_t, z_t)$\nLpred=\u2212lnp\u03b8(rt|ht,zt)\u2212lnp\u03b8(dt|ht,zt)\u2212lnp\u03b8(xt|ht,zt)\n(3)\nWe augment the above World Model with an additional head for predicting task success conditioned on the RSSM state, referred to as the Self-Awareness Model. This model predicts the quantile (relative to the maximum episode length) in which the task will be solved. Specifically, in this implementation, the Self-Awareness Model is an MLP with $N$ outputs that map the RSSM state to time-to-success predictions. The MLP outputs parameterize $N = 5$ independent Bernoulli distributions {$\\psi_1,\\cdots\\psi_N$}.\nA self-awareness prediction involves sampling from each $\\psi_i$, such that $\\hat{c}_t \\sim \\psi_i(\\hat{c}_t|h_t, z_t)$, and combining these samples into a prediction vector. For example, a self-awareness prediction of success in the first quantile would yield the vector [1, 1, 1, 1, 1], whereas a prediction of failure would yield the vector [0, 0, 0, 0, 0]. This process is visualized in Figure 2. Note that each component of the self-awareness prediction $\\psi_i$ is trained independently using a binary cross-entropy loss.", "3.1.2 Self-Regulation": "Even with multi-task pre-training, including parametric variations, to learn basic skills, an MBRL agent exhibits limited generalization to novel tasks that require either new, orchestrated combinations of those skills or entirely new skills [Ketz and Pilly, 2022]. While Dreamer-v3 can handle novel parametric variations for a known task, it struggles to make progress when faced with an unknown reward function that differs semantically from those of the pre-training tasks. Central to our MUSE framework is the self-regulation algorithm, which performs competence-aware actions to solve novel tasks. Specifically, the decision-making process selects actions that maximize the likelihood of task success. There are three primary approaches for planning using the learned World Model, current policy, and the evaluation criterion of self-assessed competence:\n(a) Simulate multiple future scenarios (rollout trajectories) based on the current state and potential actions, then greedily select the path that maximizes the self-awareness criterion"}, {"title": "3.2 Experiments", "content": "For the decoder-based World Model implementation, we evaluated our approach in the Meta-World robotic manipulation simulator [Yu et al., 2020], using Dreamer-v3 [Hafner et al., 2023] as the MBRL baseline for comparison. During training and adaptation, Dreamer-v3 alternates between updating its World Model with real data and updating its actor and critic neural networks with imagined data. To ensure consistency, we used Dreamer-v3's network architectures, hyperparameters, and learning procedures across all shared components between the two agents (e.g., an imagination horizon of 15 time steps). Note the replay buffers from pre-deployment training were retained for deployment adaptation. Both methods were implemented in the same PyTorch codebase, with the self-awareness and self-regulation modules omitted to evaluate Dreamer-v3."}, {"title": "3.2.1 Meta-World", "content": "Meta-World provides a suitable testbed for learning a shared perceptual and dynamics model across multiple tasks using a 6 degrees-of-freedom (DOF) robotic arm. We utilized the MT10 benchmark, which consists of 10 manipulation tasks with distinct reward functions: [button-press, door-open, drawer-close, drawer-open, peg-insert-side, pick-place, push, reach, window-close, window-open] as our pre-deployment training set (Figure 3). By default, object and goal positions were randomly sampled to enable domain randomization. For pre-deployment training, We adopted a multi-task learning paradigm [Mandi et al., 2023], encompassing all 10 training tasks over 2M total environment steps, instead of the more computationally expensive meta-reinforcement learning approaches [Wang et al., 2021]. Each episode was limited to a maximum of 500 time steps. We leveraged the built-in success signal returned by the Meta-World environment to train the Self-Awareness Model. The agents received a 64 x 64 RGB observation alongside a 40-dimensional proprioceptive state. Additionally, we included a task embedding that was represented as a single integer-valued channel appended to the visual state. For novel tasks, this task embedding was a zero-valued channel. Following pre-deployment training, the agents were evaluated on a set of 10 novel tasks with distinct reward functions, semantically different from those in the pre-deployment training set (Figure 4). The agents adapted to one novel task at a time, starting with pre-deployment trained weights, for 20 episodes per task. Each of these episodes also had a maximum time limit of 500 steps. Performance on the novel tasks was assessed during these adaptation episodes."}, {"title": "3.2.2 Metrics", "content": "Self-Awareness We evaluate the agents' ability to predict their success on novel tasks using accuracy and the Area under the Receiver Operating Characteristic Curve (AUROC). For MUSE, these metrics indicate how effectively the self-awareness signal can support rapid online adaptation. Although Dreamer-v3 does not have an explicit self-awareness module, we use the value predictions of its critic neural network (with reward normalization) as a proxy for predicting episode outcomes (success or failure). Each agent is evaluated for each novel task separately across 20 adaptation episodes. During these episodes, we collect MUSE's step-wise self-awareness predictions and Dreamer-v3 critic's step-wise value predictions for evaluation. The predictions at each time step across all tasks are compared with the true labels to compute accuracy and AUROC metrics. Accuracy for Dreamer-v3 is determined using a threshold set to the median of its value predictions across all novel tasks and episodes. Additionally, MUSE's success quantile predictions, which are multi-class classification outputs, provide a finer-grained metric of competence awareness. The ROC curve for MUSE is computed by treating the sum of the self-awareness components $(\\sum_{i=1}^{N} \\psi_i)$ as the signal.\nSelf-Regulation We evaluate the agents' generalization to unknown situations using two metrics: the percentage of novel tasks solved and the average time to task completion. Each agent is assessed for each novel task separately across the 20 adaptation episodes. The percentage of episodes where the agent successfully completes the task within the maximum time limit is averaged across all novel tasks to calculate the success rate. Similarly, the number of time steps required to achieve success in each episode is averaged across all episodes and tasks to compute the time-to-completion metric."}, {"title": "3.3 Discussion", "content": "The pre-deployment training tasks were selected to encompass a broad range of skills, enabling the agents to develop a comprehensive understanding of various manipulation strategies. Following this training phase, the agents were exposed to out-of-distribution tasks selected to evaluate their ability to generalize learned skills to novel and unseen challenges. This two-phase approach-training on familiar tasks and adapting to novel ones\u2014was critical for rigorously assessing the effectiveness and adaptability of MUSE and Dreamer-v3 in unknown environments. Overall, the experiments with the Decoder-based World Model implementation demonstrated that MUSE significantly outperforms Dreamer-v3 by avoiding over-confidence in low-competence novel situations and under-confidence in high-competence ones, resulting in higher success rates across both scenarios."}, {"title": "4 LLM-based implementation", "content": "In this section, we describe our implementation of the MUSE framework to equip LLM agents with metacognitive abilities of self-awareness and self-regulation."}, {"title": "4.1 Methods", "content": "4.1.1 ReAct\nYao et al. [2022] was among the first to introduce an LLM agent that interacts with its environment to accomplish tasks by being prompted to reason and act (Figure 7). We describe their method within a more generic framework. At time step t, the ReAct agent ($M_a$) perceives an observation $O_t \\in \\mathcal{O}$, executes an action $a_t \\in \\mathcal{A}$ based on a policy $\\pi(a_t | I, C_{t-1}, O_t)$, and receives a reward $r_t$. Here, $I$ is the natural language description of the task, and $c_t = {O_1, a_1, r_1..., O_t, a_t, r_t}$ is the running context of the trajectory within the episode. The agent also receives the episode outcome (success or failure) when either the task is solved within the time budget or the episode terminates due to running out of time. The context is reset at the end of each episode. The ReAct agent leverages chain-of-thought (CoT) reasoning [Wei et al., 2022] to facilitate reasoning steps expressed in natural language for improved action selection. Notably, actions $a_t$ stored in $c_t$ include both standard actions and CoT reasoning traces. To facilitate CoT reasoning, ReAct employs two-shot domain-specific trajectory examples in the in-context prompt, which mitigates syntactic errors and improves adaptation efficiency."}, {"title": "4.1.2 Reflexion", "content": "Reflexion [Shinn et al., 2023] enhances in-context prompting for LLM agents by generating internal feedback, referred to as \u201creflection,\u201d expressed in natural language to transfer lessons learned across episodes for a given task (Figure 8). At the end of each episode e, Reflexion prompts an LLM to reason and generate verbal feedback $r_{x_e}$ about the agent's performance by analyzing the entire trajectory $c_t$ from the episode's start (t = 1) to finish (t = T) and the episode outcome (success or failure). The Reflection LLM ($M_{rx}$) also receives the reflections and outcomes from previous episodes {1 : e - 1}. The Reflexion agent ($M_a$) follows the policy $\\pi(a_t | I, C_{t-1}, O_t, rx{1:e-1})$, which incorporates cumulative reflections from earlier episodes. In contrast to ReAct, which relies solely on short-term memory comprising the agent's running trajectory within the current episode, Reflexion leverages both short- and long-term memory for improved strategic adaptation to novel situations. This iterative internal feedback mechanism allows the agent to refine its strategies progressively by integrating lessons learned from failures in previous episodes. Reflexion addresses the credit assignment problem through implicit reasoning about specific actions within the trajectory that led to failures, proposing alternative strategies for future episodes. Through this process, the agent develops an enhanced understanding of effective plans to continually improve its adaptability to novel tasks.\nWhile the original Reflexion study [Shinn et al., 2023] utilized a single LLM for both action generation and reflection, our implementation uses distinct LLMs fine-tuned for each respective task. Additionally, we extend the reflection process to include successful outcomes, enabling the model to reinforce effective strategies alongside learning from failures. We also allow the model to consolidate cumulative reflections into summarized forms as memory grows, rather than limiting memory capacity based on the context window size of the Actor LLM."}, {"title": "4.1.3 MUSE", "content": "The MUSE framework, illustrated in Figure 9, builds on mechanisms from ReAct and Reflexion by incorporating additional modules for self-awareness and self-regulation:\n\u2022 World Model ($M_w$): A LLM that can be prompted to predict the next observation, reward, and task completion signal given the current observation and action. This LLM works in conjunction with the Actor ($M_a$) to generate potential future states and actions (rollout trajectories), enabling look-ahead planning. In text-based domains, such as the experiments presented here, $M_w$ can use the same LLM as $M_a$. The LLM can be directly prompted to generate diverse trajectories without requiring explicit interaction between the two.\n\u2022 Self-Awareness Model ($M_{sa}$): A language-conditioned neural network that evaluates and scores trajectories generated by World Model/Actor to assess their alignment and effectiveness for the agent's goals. Specifically, it predicts task competence, or the probability of task success, for each trajectory.\n\u2022 Self-Regulation ($M_{sr}$): A module that decides a competence-aware course of action based on one of the options outlined in Subsection 3.1.2. In this implementation, $M_{sr}$ chooses the first action $a_t$ from the rollout trajectory that is most likely to achieve task success, as determined by competence evaluations from $M_{sa}$.\nWorld Model/Actor\nDuring deployment, the World Model/Actor generates several potential future state-action sequences with a horizon H at each time step t. The diversity of these rollout trajectories, denoted by $T_t = {a_t, r_t, O_{t+1},..., a_{t+H-1}, Y_{t+H-1}, O_{t+H}}$, is controlled by the temperature setting of the LLM. These trajectories represent hypothetical paths extending from the current observation $o_t$ and context $C_{t-1}$, guided by the task description I and task-specific reflections $rx{1:e-1}$ stored in memory. For this implementation, we explicitly did not specify a horizon H; instead, the LLM was allowed to generate trajectories up to the maximum length permitted by its context window. The temperature of the LLM was set to 0.5, and five rollout trajectories were generated at each time step.\nSelf-Awareness\nThe Self-Awareness Model ($M_{sa}$) utilizes a transformer encoder [SentenceTransformers, 2024] and an MLP to predict the probability of task success (Equation 5) for trajectories generated by the World Model/Actor before their actual execution in the environment. Specifically, $M_{sa}$ evaluates the alignment and effectiveness of potential trajectories for the task at hand. The algorithm for training $M_{sa}$ and using it for evaluation is detailed in Algorithm 2.\n$Y_{pred} = M_{sa}(I, P^e, \\tau)$\n(5)\nHere, $\\tau$ represents a trajectory, and $P^e$ denotes the initial plan generated by the World Model/Actor at the start of episode e. The output layer of $M_{sa}$ employs a sigmoid activation function to yield"}, {"title": "4.1.3 MUSE framework", "content": "The MUSE agent builds upon the Reflexion agent [Shinn et al., 2023] that is exposed to training tasks for a fixed number of episodes each. The offline and online training procedures are detailed in Algorithm 3. For the pre-deployment training, the MUSE agent utilizes all data collected by the Reflexion agent across the various training tasks. Specifically, the Reflection LLM ($M_{rx}$) in MUSE is trained using Direct Preference Optimization (DPO) with a preference dataset [Rafailov et al., 2024] created by comparing reflections of success and failure. A successful (positive) reflection occurs when the agent fails in episode $e_t$ but succeeds in episode $e_{t+1}$ following the reflection. Conversely, a failure (negative) reflection happens when the agent succeeds in episode $e_t$ but fails after reflection in episode $e_{t+1}$ despite the reflection. DPO directly optimizes $M_{rx}$ to generate reflections that maximize the likelihood of satisfying these preferences. This enables $M_{rx}$ to iteratively improve its ability to help the agent adapt and recover from failures and adapt to diverse tasks through more effective reflective processes.\nThe Actor LLM ($M_a$) is trained using Supervised Fine-Tuning (SFT) with only successful episodes across tasks. By imitating the behavior demonstrated in these episodes, $M_a$ learns to align its policy with the optimal actions seen during successful episodes across various tasks. This joint multi-task fine-tuning process equips MUSE with generalizable success-driven strategies, allowing it to outperform both ReAct and Reflexion in adapting to novel tasks. To efficiently align with the training data, both $M_a$ and $M_{rx}$ are fine-tuned using Low-Rank Adaptation (LoRA) [Hu et al., 2021]. Additionally, the Self-Awareness Model ($M_{sa}$) is trained using pertinent data from the Reflexion agent, which maps trajectory chunks to corresponding episode outcomes.\nDuring deployment, when MUSE encounters a novel task, it engages in sequential episodes until task success is achieved. MUSE employs competence-aware planning, as described above, to choose its actions. Furthermore, each constituent model of MUSE can be updated online as new data becomes available from the novel task. In contrast, alternate approaches like ReAct and Reflexion primarily rely on in-context prompting and reflective text to learn from prior experiences. These methods do not involve updating model parameters using gradient-based learning, leading to knowledge being stored only for the short term. This constraint hinders the transfer of knowledge across episodes and tasks. MUSE overcomes these limitations by actively updating the weights of its models, enabling continual learning and more effective knowledge transfer."}, {"title": "4.2 Experiments", "content": "For the LLM-based implementation, we evaluated our approach within the ALFWorld simulator [Shridhar et al., 2020] and compared it against ReAct [Yao et al., 2022] and Reflexion [Shinn et al., 2023] as the baselines. For these experiments, we selected the Mistral-7B-Instruct-v0.2 as the primary LLM [Jiang et al., 2023]. This LLM features a substantial context window size of 32,000 tokens, allowing it to process large chunks of data in a single pass for improved in-context learning. To further explore the robustness of our framework, we conducted additional experiments using less capable LLMs such as Open-Orca/Mistral-7B-OpenOrca [Mukherjee et al., 2023, Open-Orca, 2024] and Apple/OpenELM-3B-Instruct [Mehta et al., 2024]."}, {"title": "4.2.1 ALFWorld", "content": "ALFWorld [Shridhar et al., 2020] is a synthetic, text-based game simulator with diverse interactive environments that challenge agents to solve multi-step tasks (e.g., \u201cFind two plates and put them in a cabinet"}]}