{"title": "Imbalanced Medical Image Segmentation with Pixel-dependent Noisy Labels", "authors": ["Erjian Guo", "Zicheng Wang", "Zhen Zhao", "Luping Zhou"], "abstract": "Accurate medical image segmentation is often hindered by noisy labels in training data, due to the challenges of annotating medical images. Prior research works addressing noisy labels tend to make class-dependent assumptions, overlooking the pixel-dependent nature of most noisy labels. Furthermore, existing methods typically apply fixed thresholds to filter out noisy labels, risking the removal of minority classes and consequently degrading segmentation performance. To bridge these gaps, our proposed framework, Collaborative Learning with Curriculum Selection (CLCS), addresses pixel-dependent noisy labels with class imbalance. CLCS advances the existing works by i) treating noisy labels as pixel-dependent and addressing them through a collaborative learning framework, and ii) employing a curriculum dynamic thresholding approach adapting to model learning progress to select clean data samples to mitigate the class imbalance issue, and iii) applying a noise balance loss to noisy data samples to improve data utilization instead of discarding them outright. Specifically, our CLCS contains two modules: Curriculum Noisy Label Sample Selection (CNS) and Noise Balance Loss (NBL). In the CNS module, we designed a two-branch network with discrepancy loss for collaborative learning so that different feature representations of the same instance could be extracted from distinct views and used to vote the class probabilities of pixels. Besides, a curriculum dynamic threshold is adopted to select clean-label samples through probability voting. In the NBL module, instead of directly dropping the suspiciously noisy labels, we further adopt a robust loss to leverage such instances to boost the performance. We verify our CLCS on two benchmarks with different types of segmentation noise. Our method can obtain new state-of-the-art performance in different settings, yielding more than 3% Dice and mloU improvements.", "sections": [{"title": "I. INTRODUCTION", "content": "IMAGE segmentation is an important task in medical image analysis, with significant potential for various clinical applications. Deep learning algorithms based on convolutional neural networks (CNNs) have demonstrated remarkable advancements in medical image segmentation [1], [2]. However, such impressive success is closely dependent on large amounts of clean training data with precise pixel-level annotations. In practical medical applications, obtaining high-quality pixel-level annotations is a challenging and labor-intensive task, due to the lack of experienced annotators, visual ambiguity in object boundaries, and limited budgets [3], [4]. As a result, the medical training dataset inevitably contains noisy labels, and their presence may mislead the segmentation model to memorize wrong semantic correlations, reducing the generalizability of models. Hence, it is important to develop robust medical image segmentation techniques adept at handling noisily labeled data for model training.\nThe issue of noisy labels has been extensively studied in image classification tasks. However, the impacts of pixel-wise label noise on segmentation tasks, particularly in the realm of medical image analysis, have not been thoroughly investigated. In contrast to the extensively explored class-dependent noise, pixel-wise label noise defies prior distribution assumptions and lacks fixed noise patterns. This type of noise is not only practical but also prevalent in real-world medical scenarios. One apparent solution is to employ general regularization techniques to address noisy labels, which helps in preventing the model from overfitting noisy patterns. These techniques include early stopping, drop out, and label smoothing. Early stopping is commonly applied in noisy classification tasks, which relates to the commonly known \"memorization effect\", where deep neural networks tend to first memorize and fit majority (clean) patterns and then overfit minority (noisy) patterns [5]. Early stopping therefore can avoid over-fitting noisy labels. However, stopping the training prematurely can result in underfitting clean labels. Another technique to employ is spatial label smoothing regularization, often utilized for class-wise label refurbishments [6]. Label smoothing prevents the network from becoming overly confident, which, in turn, helps reduce the extent to which the model overfits noisy label data. While these general regularization methods are effective to some extent for classification tasks, they prove to be insufficient for segmentation tasks with pixel-level noise.\nRecent dominant approaches to address noisy labels for medical image segmentation can be broadly categorized into two groups. The first group denoises class-wise label noise by using the noise transition matrix (NTM) and robust losses. The works [7], [8] modeled class-wise noisy label distribution through either the confusion matrix or noise transition matrix (NTM) and then leveraged the modeled distribution for loss correction. However, estimating the noisy label distribution becomes increasingly challenging when the number of classes increases or the noise rate escalates [9] in the segmentation"}, {"title": "II. RELATED WORK", "content": "In this section, we briefly discuss learning with noisy labels on medical image segmentation tasks, the collaborative learning framework, curriculum learning strategy and noise type for segmentation tasks, which are related to our work.\nLearning with Noise Labels\nLearning with noise labels has been extensively studied in recent works [11], [12]. Two common groups of approaches are employed to address the problem of noisy labels in segmentation tasks: denoising class-wise labels and denoising pixel-wise labels. For the first group, researchers employ various techniques such as regularization, noise transition matrix, and robust losses to mitigate the impact of noisy labels without explicitly discarding the noisy labeled data. These types of approaches are similar to those handling noisy labels for classification tasks. The objective is to effectively reduce the influence of label noise during training while preserving as much useful information as possible. For example, a regularization technique was proposed by Wei et al. [13]. Some methods estimated noise transition matrices to correct label noise in a class-wise manner [14]. Other popular methods reduced the effects of noisy labels by using noise-tolerant loss functions [15], [16]. However, these methods tend to perform poorly when confronted with high noise rates and a large number of classes in segmentation tasks. Moreover, estimating the distribution relationship between noisy and clean data based on categories is impractical in segmentation tasks. Alternative"}, {"title": "B. Collaborative Learning", "content": "Collaborative learning aims to learn two or more distinct and divergent feature extractors for the same instances, which has also been employed to handle noisy label tasks. For example, Co-Teaching [18], a popular two-branch approach, involves two subnets providing each other with different and complementary information. It uses small loss tricks to select confident clean samples without considering the original labels. This two-branch framework has been used in segmentation tasks, but the two sub-nets are easy to collapse, i.e., converging to the same predictions. To prevent this issue, some methods [19] used feature-level perturbations to produce different inputs of the decoders, preventing the decoders from collapsing into each other. However, using artificial perturbations on encoders makes it easy to corrupt learning reasonable features from the encoders. The work [18] imposed consistency on two segmentation networks perturbed with different initializations for the same input image to generate predictions of different views. However, this method only guarantees divergence at the beginning of training, while the predictions of the two-branch network gradually converge as the training progresses. In contrast, in our two-branch framework, we propose a discrepancy loss to prevent the collapse of two sub-nets. It compels the two sub-nets to learn from distinct views to vote for the label prediction, which achieves better performance. Moreover, our collaborative learning with confident voting combines the predictions from both the two branches and the original labels, leading to a stable and accurate prediction and reducing the influence of confirmation bias."}, {"title": "C. Noise Type for Segmentation Tasks", "content": "Most existing methods to deal with artificially synthesized noisy labels were initially proposed for image classification tasks [20]. In these classification tasks, synthesized noisy labels typically focus on both symmetric and asymmetric label noise. Symmetric label noise involves flipping real labels to other labels with equal probability, while asymmetric label noise entails flipping real labels to other labels based on fixed rules. However, when it comes to segmentation tasks, symmetric and asymmetric noise models appear unreasonable. Unlike the traditionally synthesized label noise for classification, Source-Free Domain Adaptation (SFDA) noise follows different distributions [21]. SFDA noisy labels are produced by adapting a pre-trained model to label the data samples. The work in [21] showed that existing noisy label learning methods relying on noise distribution assumptions are unable to address the label noise in SFDA. On the other hand, to simulate manual noisy annotations in segmentation tasks, the"}, {"title": "III. METHODOLOGY", "content": "In this section, we introduce our Collaborative Learning with Curriculum Selection (CLCS) framework in detail. We first give a brief problem statement and an overview of our method in Sec. III-A, and then explain the two main modules of our method, i.e., the Curriculum Noisy Label Sample Selection (CNS) module and the Noise Balance Loss (NBL) module, in Sec. III-B and Sec. III-C, respectively.\nA. Overview\nIn the context of medical image segmentation with noisy labels, we are given a set of medical images $X = {x_i}_{i=1}^N$ and the corresponding annotations $Y = {y_i}_{i=1}^N$. The input image $X_i \\in \\mathbb{R}^{H \\times W \\times 3}$ has a size of H \u00d7 W with 3 channels, while $Y_i \\subset {0,1}^{H\\times W\\times C}$ is the one-hot ground truth segmentation mask, where C indicates the number of visual classes in total. Specifically, we let $x_{i,j}$ and $Y_{i,j}$ denote the value and the given label of the j-th pixel of the i-th image, respectively. Note that the annotations in the training stage are accompanied by noise, i.e., some of the annotations are incorrect, while the clean annotations are only available during the inference stage for validating the performance.\nThe overview of our proposed collaborative learning with curriculum selection (CLCS) framework is shown in Fig. 1. Our CLCS framework consists of two main modules, a curriculum noisy label sample selection (CNS) module and a noise balance loss (NBL) module. In particular, our CNS module includes a boosted collaborative learning (BCL) component with a discrepancy loss, a curriculum dynamic thresholding (CDT) component, and a collaborative confidence voting (CCV) component, aiming at separating clean and noisy annotations. Then, we perform our NBL module to enable the model to make full use of both clean and noisy annotations for learning."}, {"title": "B. Curriculum Noisy Label Sample Selection (CNS)", "content": "Boosted Collaborative Learning (BCL) with a Discrepancy Loss: As seen from Fig. 1, the core of our method is a collaborative training framework, which lays the foundation for the subsequent collaborative confidence voting component. It is based on a two-branch network while each branch independently generates predictions. Specifically, each branch consists of an encoder $E_k$ and a decoder $D_k$, where k is the branch index. We could readily obtain the feature map extracted by each branch of the network as $f_k = E_k(x_i)$ and the logits as $p_i^k = D_k(f_i^k)$. Note that $f_i^k$ is a D-dimensional down-scaled feature map with a shape of h \u00d7 w while $p_i^k$ is"}, {"title": "2) Curriculum Dynamic Thresholding (CDT):", "content": "Our BCL method is designed to generate precise predictions, emphasizing the model's capacity to learn from clean annotations. However, the inherent noise in the provided annotation set prompts us to identify confident predictions that can be utilized for further collaborative confidence voting. An ideal approach considers the predictions with a confidence score $\\hat{p}_{i,j}^k$ surpassing a predetermined threshold. The threshold could be established by calculating the evaluation accuracies for each class and using the accuracies to scale a base threshold, i.e., $T_c(t) = A_c(t) \\cdot \\tau$. Unfortunately, the noisy nature of the given annotation set makes it challenging to compute an accurate $A_c(t)$, leading to an unsatisfactory outcome for confident sample selection. On the other hand, dynamically adjusting thresholds during the training process poses a significant challenge, as it substantially hinders training speed.\nFacing the issues, in this part, we come up with a curriculum dynamic thresholding (CDT) strategy for confident sample selection (Fig. 2), which uses an alternative way to estimate the learning status, neither introducing additional inference processes nor requiring clean labels. It's crucial to note that when the threshold is set high, the learning effectiveness of a class can be inferred by examining the number of samples whose predictions falling into this class surpass the threshold. Simply put, a class with fewer samples exhibiting confidence above the threshold is perceived as having a higher noise rate, suggesting increasing learning difficulty or indicating a worse learning status [24]. Based on this, we define the learning status of the class c at time step t by $\\sigma_c(t)$ as:\n$\\sigma_c(t) = \\sum_{i=1}^N \\sum_{j=1}^{H \\times W} \\mathbb{1} (P_{i,j} > \\tau) \\cdot \\mathbb{1} (\\hat{y}_{i,j} = c), (2)$\nwhere $\\mathbb{1}(\\cdot)$ is an indicator function, returning 1 when the event occurs and 0 otherwise. In medical image segmentation tasks, lesions usually occupy a small portion of the image, which is against the substantial background component, leading to a notable class imbalance issue. The presence of noise labels"}, {"title": "3) Collaborative Confidence Voting (CCV):", "content": "Building upon our collaborative training framework, which generates predictions from diverse perspectives for each pixel, we further propose a confident voting method to identify clean label data based on the original label and the predictions of the two branches of the model. We assert that when the predictions of both branches align with the original label and exhibit high confidence, there is a strong likelihood that the label is accurate. Otherwise, we consider the corresponding annotations as potentially noisy. In particular, let \u0393 indicate whether the label"}, {"title": "C. Noise Balance Loss (NBL)", "content": "Following the distinction between clean and noisy annotations, it seems natural to focus the model's learning on the clean annotations. However, in this context, we posit that there is value in leveraging information from the noisy annotation set. The reason being, the set of noisy annotations may encompass some annotations that are correct but challenging to discern. Previous work [25] has shown that cross-entropy loss is prone to make the model overfit to noisy labels on some easier classes and underlearn on some harder classes. To avoid overfitting the noisy labels, we propose a Noise Balance Loss (NBL), which can be written as:\n$\\mathcal{L}_{NBL} = \\frac{1}{2 N H \\times W} \\sum_{k=1}^2 \\sum_{i=1}^N \\sum_{j=1}^{H \\times W} [(\\omega_{i,j}^k) l_{ce} (P_{i,j}^k, Y_{i,j}) + (1 - \\omega_{i,j}^k) l_{rce} (P_{i,j}^k, Y_{i,j})] \\cdot (1 - \\Gamma_{i,j}). (7)$\nNote that our noise balance loss is a combination of the CE loss $l_{ce}$ [16] and the RCE loss $l_{rce}$ [25], while $l_{ce}$ aims at achieving good convergence from the clean labels and $l_{rce}$ aims at mitigating the impact of noise. As demonstrated in [25], the risk minimization under $l_{rce}$ is noise-robust because it has a similar global minimizer under noise-free or noisy data. However, $l_{rce}$ exhibits weaker convergence than $l_{ce}$, so we combine both to benefit from their complementary nature. We design an adaptive weight to balance $l_{ce}$ and $l_{rce}$, with each weight determined by the confidence score of the corresponding prediction, denoted as $\\omega_{i,j}^k = \\hat{p}_{i,j}^k$. A higher confidence score signified a more likely correct prediction,"}, {"title": "IV. EXPERIMENTS", "content": "A. Implementation Details\nOur method is implemented by PyTorch and trained on a single Nvidia 2080Ti GPU. We employ DeepLabV2 [29] with the pre-trained encoder ResNet101 as the backbone network. The SGD optimizer is adopted. The initial learning rate is set as 1e-3. We adopt a batch size of 6 and a maximum epoch number of 200. The loss weights \u03b1 and \u03b2 are set to 1 and 0.01, respectively, for both of the two datasets. For a fair comparison, we keep the same backbone for all baselines. For our model, we only use one branch to evaluate the model so that the model weights are at the similar level as other methods in comparison. The whole segmentation framework is trained in an end-to-end fashion. The segmentation performance is assessed by Dice and IoU scores.\nB. Datasets\nWe validate the methods on two public datasets. The first one is the surgical instrument dataset Endovis18 [30]. It consists of 2384 images annotated with three types of instrument part labels including shaft, wrist, and clasper classes. The dataset is split into 1639 training images and 596 test images following [31]. We resize the images to 256\u00d7320 as the inputs following [3]. The second dataset, RIGA [32], is a benchmark for retinal cup and disc segmentation, which contains in total 750 color fundus images from three different sources, including 460 images from MESSIDOR, 195 images from BinRushed and 95 images from Magrabia. Six glaucoma experts from different organizations labeled the optic cup and disc contour masks manually for the RIGA benchmark [32]. During model training, we selected 195 samples from BinRushed and 460 samples from MESSIDOR as the training set. The Magrabia set with 95 samples is used as the test set, which is not homologous to the training dataset.\nC. Noise Patterns\nTo comprehensively verify the robustness of each method, we conduct experiments with SFDA noise, SFDA combined with morphological changes, and manual annotation noise. For the SFDA noise following [3], we train the source model solely on Endovis17 [33] containing 1800 annotated images with domain shift to Endovis18 and generate realistic noisy labels by applying the source model directly on Endovis18. For SFDA combined with morphological changes, we introduce noises generated by randomly eroding or dilating the contours of accurate annotations following [4] and add them to SFDA. For the manual annotation noise, in the RIGA dataset [32], the clean labels for the testing dataset are produced following [34]. The noisy labels for the training dataset are from the rater 6's annotations. The above annotation noises are similar to a real-life scenario.\nD. Methods in comparison\nThree groups of methods are involved in the experimental comparison: robust loss methods, loss correction methods, and pixel-wise denoising methods. The first group, robust loss methods, designed a noise-tolerant loss term to facilitate learning of hard classes and mitigate overfitting to noisy labels. A representative method SCE [25] is involved in our comparison. The second group, loss correction methods, modeled the noise transition matrix (NTM) that defines the probability of one class changing to another class. We compare our model with VolMin [8] and JACS [3] along this line. The third group, Pixel-wise Denoising Methods, separated the dataset into the clean set and noise set, and then only trained the model on the clean set. Two representatives Co-Teaching+ [23] and DCT [35] are compared. For a fair comparison, we re-run the codes released by the authors of the above works on the same training and test datasets as used by our method. The hyperparameters of these models are reasonably tuned towards a good performance.\nE. Performance Comparison\nIn this section, we evaluate the segmentation performance of different methods on two medical image datasets with different types of noise. We also perform ablation studies to evaluate the contribution of each module of our model, i.e., CDT, CCV, discrepancy loss, and NBL.\nResults on the Surgical Instrument Dataset: Table I and Table II present the quantitative comparison results on the Endovis18 [30] training dataset with two different noise types: SFDA noise (denoted as SFDA-Noise) and SFDA noise"}, {"title": "F. Impact of Curriculum Dynamic Threshold", "content": "If we use a fixed threshold based on the initial class distribution, i.e., with the threshold for each class being the proportion of that class's pixels to the total pixels, the Dice"}, {"title": "G. Clean Label Selection via CCV", "content": "To verify the clean label selection through our CCV module, we plot the clean ratio curve on the Endovis18 [30] dataset with noisy labels. The clean label ratio is calculated as $r_{clean}(c) = \\frac{\\sum \\mathbb{1} (y_{clean}=y | (y=c))}{\\sum \\mathbb{1} (y=c)}$, where $Y_{clean}$ represents clean labels and y represents original labels (possibly containing"}, {"title": "H. Impact of the Discrepancy Loss", "content": "We now verify the effect of our discrepancy loss in encouraging different perspectives of the two branches for the same instance. Fig. 7 shows the numbers of pixels predicted differently by the two branches across the training procedure"}, {"title": "I. Impact of Noise Balance Loss", "content": "From Table IX, we can find that the average Dice/mIoU increases from 66.51%/53.95% to 67.51%/55.21% by adding the NBL. As shown in our loss function (Eq. 9), a hyperparameter \u03b1 is utilized to control the trade-off between the NBL ($\\mathcal{L}_{NBL}$) and other losses ($\\mathcal{L}_{clean}$ and $\\mathcal{L}_{dis}$). We investigate the impact of different values of \u03b1 as shown in Table VII. As shown, the optimal performance is achieved when \u03b1 is set as 0.01. When \u03b1 changes within a reasonably large range, say 100 times from 0.01 to 1, our model could still outperform the methods in comparison.\nJ. Ablation Study on Model Components\nAblation studies are performed over the key components of the proposed CLCS, including $\\mathcal{L}_{dis}$, CCV, and NBL, as reported in Table IX. The ablation studies are conducted on the Endovis18 [30] dataset with SFDA + ED-Noise. As seen, when we sequentially add the proposed modules to the baseline, the model performance is gradually improved. Specifically, by integrating $\\mathcal{L}_{dis}$ with collaborative learning into the baseline, the two branches produce a reasonable"}, {"title": "K. Limitation", "content": "While our method is robust and performs exceptionally well in most medical image segmentation tasks, it may face challenges when the annotations are excessively noisy due to highly unskilled annotators. Additionally, in rare cases where there is an unusually high amount of noise, the segmentation performance of the model may be constrained. Despite these potential limitations, our method remains highly effective and reliable for the vast majority of medical image segmentation scenarios."}, {"title": "V. CONCLUSION", "content": "This paper introduces CLCS, a robust framework designed for noise-robust medical image segmentation. CLCS exhibits the ability to effectively learn from complex pixel-wise noisy labels while adeptly addressing the inherent class imbalance challenges associated with medical image segmentation while dealing with noisy labels. The efficacy of CLCS is validated across various types of noisy labels, including realistic annotation noise, consistently demonstrating superior performance when compared to existing methods."}]}