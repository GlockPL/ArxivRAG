{"title": "DRAGTEXT: Rethinking Text Embedding in Point-based Image Editing", "authors": ["Gayoon Choi", "Taejin Jeong", "Sujung Hong", "Jaehoon Joo", "Seong Jae Hwang"], "abstract": "Point-based image editing enables accurate and flexible control through content dragging. However, the role of text embedding in the editing process has not been thoroughly investigated. A significant aspect that remains unexplored is the interaction between text and image embeddings. In this study, we show that during the progressive editing of an input image in a diffusion model, the text embedding remains constant. As the image embedding increasingly diverges from its initial state, the discrepancy between the image and text embeddings presents a significant challenge. Moreover, we found that the text prompt significantly influences the dragging process, particularly in maintaining content integrity and achieving the desired manipulation. To utilize these insights, we propose DRAGTEXT, which optimizes text embedding in conjunction with the dragging process to pair with the modified image embedding. Simultaneously, we regularize the text optimization process to preserve the integrity of the original text prompt. Our approach can be seamlessly integrated with existing diffusion-based drag methods with only a few lines of code.", "sections": [{"title": "1. Introduction", "content": "Text-to-image (T2I) models have made significant advancements in image editing alongside the development of diffusion models [1, 23\u201326, 28]. These models are effective in detailed modifications, such as inpainting [34, 36], style transfer [3, 39], and content replacement [4, 9, 27]. For instance, when a user inputs the text prompt \u201ca dog looking up\u201d into a T2I model, the resulting image shows the dog lifting its head. However, if one wishes to provide more explicit instructions for more detailed structural edits (e.g., movement angle or distance), designing appropriate text prompts to inject such a level of intention becomes far from trivial. To deal with this, several studies have shifted towards non-text controls (e.g., points, edges, poses, sketches) to avoid ambiguity and achieve controllability [18, 21, 35, 37]. Among these, point-based image editing (Fig. 1) is particularly noteworthy in that it employs pairs of instruction points, allowing fine-grained control.\nPoint-based image editing has recently advanced with diffusion models, effectively manipulating both synthetic and real images across diverse domains [12, 13, 21, 29, 40]. To generalize image editing in various fields, large-scale pre-trained latent diffusion models have been utilized. These models are trained with cross-attention between text and image embeddings in denoising U-Net. Therefore, they also require text prompt as input. When point-based image editing drags content in the image embedding space, the text prompt provides conditions to transform the latent vector. However, to the best of our knowledge, there is no prior research which investigates the impact of text. Unlike T2I which actively analyzes text and is thus able to utilize text in innovative ways [3, 4, 15], the role of text and its potential in point-based image editing remains unknown.\nIn this study, we explore how the text prompt affects dragging and discover whether the drag successfully moves to the desired position is influenced by the text prompt. As the image is optimized through drag editing, it naturally deviates from the original image in the image embedding space [19]. However, text embedding remains stationary and thus fails to accurately describe the edited image. This static text embedding is still used during the drag editing process and the denoising process in point-based image editing. For example, as shown in Fig. 2, in the editing process where a woman\u2019s hair is made to move aside, the edited image is no longer coupled with the original text \u201cA woman with a leaf tattoo on her neck. \u201dTherefore, as observed in the second image, it fails to reach the desired drag points, which we call drag halting, or it reaches the points but loses semantics.\nWe propose DRAGTEXT, a method designed to rectify the static text embedding in point-based image editing. DRAGTEXT optimizes the original text embedding in a similar way as image optimization. \u201cDragging\u201d text embedding in parallel with the image ensures that the edited text embedding remains coupled with the edited image embedding. As a result, drag-halting is alleviated, and the image embedding is sufficiently optimized and helps handle points reach their target points. Additionally, we designed a text embedding regularization technique to ensure the edited text embedding does not diverge too significantly from the original text embedding. It preserves key styles of the image while allowing for natural changes during drag editing.\nContributions. Our contributions are as follows: 1) We are the first to analyze the impact of text on dragging, a previously neglected yet crucial component of point-based image editing; 2) We propose DRAGTEXT which optimizes text embedding that aids drag operations without forgetting content; 3) DRAGTEXT can be easily applied in a plug-and-play manner to various diffusion-based drag models, consistently improving performance."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Point-based Image Editing", "content": "Task Definition. Point-based image editing enables precise modifications to images [2, 16, 17, 21]. As shown in Fig. 1, a user inputs the image to be edited, draws a mask representing an editable region, and specifies handle points along with their corresponding target locations. During the editing process, handle points are moved toward their respective target points through an image optimization approach. The positions of these points are continuously tracked and updated after each optimization step.\nExisting Methods. DragGAN [21] is a representative method in point-based image editing that proposes latent code optimization and point tracking. However, it struggles with generalization, particularly with real images. DragDiffusion [29] employs a text-conditional diffusion model [26] to expand applicability and improve spatial control, following the method of DragGAN. Following these advancements, many point-based editing diffusion models have been developed. For instance, FreeDrag [12] uses original image features as a reference template and reduces the burden on point tracking with a line search. DragNoise [13] reduces computational load by optimizing the U-Net bottleneck feature instead of the latent vector. GoodDrag [40] alternates between dragging and denoising to prevent error accumulation and enhance image fidelity. As shown in these examples, previous works have primarily focused on image optimization to directly improve manipulation. Our approach emphasizes the underestimated role of the text prompt, offering a new perspective that enhances not only manipulation quality but also the dragging mechanism."}, {"title": "2.2. Text Optimization", "content": "Recent advancements in Vision-Language Models (VLMs) have significantly enhanced the ability to flexibly connect text and images [8, 22]. To improve these connections, various methods of text optimization are being explored. One approach is context optimization [41, 42], which refines the text used by pre-trained VLMs for a better understanding of handcrafted captions by models. Another method, developed by Gal et al. [3], optimizes a single word to effectively convey content information, thereby personalizing text-based image editing. Additionally, Mokady et al. [15] introduce a technique for optimizing null-text \u201c\u201d, addressing the issue of images deviating from their original trajectory due to accumulated errors from classifier-free guidance [6]. Our work is inspired by these prior studies which closely examine the strong coupling between text and image."}, {"title": "3. Motivations", "content": "Editing an image causes the original image embedding to move in the image embedding space. Therefore, the edited image embedding must follow a different denoising trajectory compared to the original image embedding [15, 19]. As text and image interact through cross-attention, we hypothesize that text embedding also plays a crucial role in determining this trajectory."}, {"title": "3.1. Text Prompt in Diffusion Model", "content": "Role in Inversion and Denoising Processes. The inversion process pairs the image embedding with text embedding, regardless of whether the text embedding semantically matches the image. Maintaining this pairing during the denoising process is important for the fidelity of image sampling.\nIn Fig. 3 (a), we examine differences in image sampling outcomes based on whether the text embedding is also paired in the denoising process. Our analysis reveals that using the paired text embedding during the denoising process enables the accurate sampling of the original image. However, if unpaired text embedding is employed, the image's style alters, and the model fails to sample the original image accurately. This observation highlights the necessity of text embedding-image embedding pairing to maintain the integrity of the image's attributes. Moreover, it raises concerns about the appropriateness of using the original input text embedding in the denoising process of dragging, as the image is modified during the drag editing process, it can no longer be accurately paired with the text embedding.\nRemark. To achieve the appropriate image sampling result, the text embedding used after inversion remains consistently paired with the image embedding."}, {"title": "3.2. Text Embedding in Point-based Image Editing", "content": "Challenges in Original Text. During dragging, the image embedding gradually changes to reflect the manipulation. However, the text embedding remains static, restricting the extent of image manipulation.\nAs shown in the top-left image of Fig. 3 (b), the original text embedding fails to reach the target point during the dragging process. Moreover, since the original text embedding is paired with the input image, the edited image embedding does not maintain a strong coupling with the text embedding. To avoid this weakly-coupling issue, one might hypothesize that this issue would be resolved with unconditional text embedding (i.e. null text). However, the top-right image of Fig. 3 (b) shows that the model fails to maintain the semantic information of the object without text conditions. Therefore, we search for an alternative text embedding that mitigates drag halting while preserving semantics.\nRemark. While text embedding plays a crucial role in maintaining semantics, it can also impede the handle points from reaching the target points."}, {"title": "4. Method", "content": "In this study, we propose DRAGTEXT, a novel framework to optimize text embedding along with point-based image editing. We briefly explain the diffusion process first (Sec. 4.1), and then describe the drag editing process in detail, including text embedding optimization (Sec. 4.2). Representative diffusion model-based dragging methods [12, 13, 29, 40] share the same approach, therfore, we explain DRAGTEXT based on DragDiffusion [29], and minor modifications for others are in the Supplement."}, {"title": "4.1. Preliminaries on Diffusion Models", "content": "Diffusion models generate images through a forward process and a reverse process. By using a denoising U-Net, these models predict noise at a specific time-step t and progressively denoise it to create an image. Unlike DDPM [5], DDIM [32] allows for deterministic image generation, which has made real image editing possible.\nRecent advancements in Latent Diffusion Models (LDM) [26] have enabled the diffusion processes to occur within the latent space. The input image xo is encoded with VAE encoder [10] to make latent vector zo. LDM converts zo into a noisy latent vector zt using a forward process, allowing for image editing by manipulating this latent vector. Additionally, cross-attention mechanisms allow for conditioning with different prompts, making text-based conditioning feasible with text embedding c. The zt is denoised using the predicted ee(zt, t, c) from the denoising U-Net and then converted into an image to through a VAE decoder. In our work, we use a pre-trained LDM Stable Diffusion [26], which is employed in previous studies."}, {"title": "4.2. Drag Editing", "content": "As shown in Fig. 4, we optimize the latent vector zt and the text embedding c according to n number of instruction points {hi = (Xi, Yi), gi = (Xi, \u1ef9i) : i = 1,2,..., n} via three stages: motion supervision, text embedding optimization, and point tracking. These stages are sequentially repeated until all handle points have reached corresponding target points or the maximum number of iteration steps k is achieved."}, {"title": "4.2.1 Motion Supervision", "content": "Motion supervision drags image content around handle points he toward target points gi on the image feature space by optimizing the latent vector 2, where k implies the number of drag editing iterations. The feature map F(\u00e2\u017a,\u0109k) is obtained from the third decoder block of the denoising U-Net. Its corresponding feature vector at the specific pixel location q is denoted as Fq(z, \u0109k).\nThe motion supervision loss at the k-th iteration is defined as:\n\nLms = \\sum_{i=1}^{n} \\sum_{q_1} ||F_{q_1+d_i} (z^k, \\hat{c}^k) - sg(F_{q_1} (z^k, \\hat{c}^k)) ||_1\n+ \\lambda_{image} || (z^{k-1} - sg(z^{k-1})) (1 - M_{image})||_1,\n\nwhere sg() is the stop-gradient operator. It prevents the gradient from being backpropagated for the term sg(.). q1 = \u03a9(h, r\u2081) = {(x, y) : |x \u2212 x\u00a5\u00a6| \u2264 r1, |y - y| < r1} describes the square region centered at his with radius r1. di = (gi-h)/ ||gi \u2013 h ||2 is the normalized vector from hk to gi, thus Fq1+d\u2081 is computed through the bilinear interpolation. This design allows his to be moved to h + di, but not the reverse with sg(\u00b7). Mimage is the binary mask drawn by the user to define an editable region. The second term of the equation adjusts the extent to which regions outside the mask remain unchanged during the latent optimization process with Aimage.\nFor each iteration k, 2 undergoes a gradient descent step to minimize Lms:\n\n2_{t}^{k+1} = z_{t}^{k} - \\eta_{ms} \\frac{\\partial L_{ms} (z_{t}^{k}, \\hat{c}_{t}^{k})}{\\partial z_{t}^{k}},\n\nwhere \u03b7ms is the learning rate for the latent optimization."}, {"title": "4.2.2 Text Optimization", "content": "The input text is encoded to the text embedding c \u2208 R77\u00d7d through the CLIP text encoder [22]. 77 represents the number of text tokens, out of which l tokens carry semantic meaning. Here, the l tokens represent the number of tokens obtained when the text passes through the tokenizer, typically calculated as the number of words plus one. To optimize the text embedding \u0109k, we use the same loss function as in the motion supervision. It allows \u0109k to follow the latent optimization towards the dragging direction. In addition, to preserve important content in the original text embedding c\u00ba, we use the mask Mtext \u2208 Rlxd and regularize the text optimization process.\nThe text embedding optimization loss at the k-th iteration is defined as:\n\nL_{text} = \\sum_{i=1}^{n} \\sum_{q_1} ||F_{q_1+d_i} (z^k, \\hat{c}^k) - sg(F_{q_1} (z^k, \\hat{c}^k)) ||_1\n+ \\lambda_{text} || (\\hat{c}^k - sg(c^0)) \\odot M_{text}||_1\n\nFor each iteration, \u00eak undergoes a gradient descent step to minimize Ltext:\n\n\\hat{c}^{k+1} = \\hat{c}^{k} - \\eta_{text} \\frac{\\partial L_{text} (z_{t}^{k}, \\hat{c}_{t}^{k})}{\\partial \\hat{c}_{t}^{k}},\n\nwhere \u03b7text is the learning rate for the text optimization."}, {"title": "4.2.3 Point Tracking", "content": "After the latent vector 2 and text embedding \u0109k are optimized at the k-th iteration step, the positions of the handle point he change according to the content dragging. Thus, it is necessary to track the new handle point h+1 in the updated feature map F(z+1, \u0109k+1). To find the new handle points h+\u00b9 corresponding to the previous handle points h, we find the region in the feature map F(2+1, \u0109k+1) that is most similar to the region around the initial handle points h in the feature map F(z\u0142, \u0109\u00ba):\n\nh^{k+1} = arg \\min_{q_2} || F_{q_2} (z^{k+1}, \\hat{c}^{k+1}) - F_{h_i^0} (z_{t}^{0}, c^{0}) ||_1\n\nwhere q2 denotes a point within the region \u03a9(h, r2). This ensures that handle points are consistently updated via nearest neighbor search for subsequent optimization iterations."}, {"title": "5. Experiments", "content": "To evaluate the effectiveness of the proposed DRAG-TEXT, we conduct a series of experiments on DragDiffusion [29], FreeDrag [12], DragNoise [13], and Good-Drag [40]. DRAGTEXT is applicable to these methods without hyperparameter tuning, ensuring robustness. Moreover, hyperparameters for text optimization are uniformly applied across all methods."}, {"title": "5.1. Qualitative Evaluation", "content": "In Fig. 5, we compared qualitative results with and without DRAGTEXT. Existing methods struggle to reach target points or suffer from the degradation of important image semantics, due to the use of image and text embeddings that are not strongly coupled. In contrast, by applying DRAGTEXT, images can be accurately dragged to the specified target points (e.g., (b) mountain peaks and (d) statues) while preserving essential semantics that should not disappear (e.g., (c) the mouth of a crocodile). Additionally, DRAGTEXT effectively manages content removal and creation caused by dragging, producing high-quality results."}, {"title": "5.2. Quantitative Evaluation", "content": "In Table 1, we present the quantitative results on the DragBench dataset [29] for the four baseline methods and methods with DRAGTEXT applied to these baselines. The evaluation metrics are LPIPS [38] and mean distance (MD) [21]. LPIPS measures the perceptual similarity to the original image as a perceptual loss. MD measures the distance between the final position of the handle point h and the target point g in the feature map of the DIFT model [33]. There is little difference in performance between the baseline models and the DRAGTEXT applied models in terms of LPIPS, but there is a significant improvement in MD. As shown in Fig. 7, this difference is more dramatic. It shows that applying DRAGTEXT results in substantial performance improvements across all models. This indicates that text embedding optimization allows for effective dragging while maintaining image fidelity."}, {"title": "5.3. Ablation Study", "content": "Effect of Text Regularization. In Fig. 6, we investigated the impact of text designed to preserve the semantics of the image. We edited images by varying text from 0 to 10 to regularize the text embedding. As text approaches 0, the force of dragging becomes more dominant than the preservation of semantics. For example, in Fig. 6 (d), the icecap has disappeared, indicating extensive dragging. Conversely, applying an excessively high text maintains the original content of the image and inhibits editing during text embedding optimization. In Fig. 6 (a), minimal dragging is observed, showing that the original image remains largely unchanged. We found that setting text to 0.1 achieves the optimal balance between dragging and content preservation, and it can be changed by users.\nEffect of the Number of U-Net Decoder Block. In Fig. 8, we perform DRAGTEXT using feature maps from four different U-Net decoder blocks. To assess the impact of U-Net decoder blocks on text embedding, the image embedding is fixed using the feature map from the 3rd block. The images optimized with the feature map from the 3rd block exhibited optimal semantic preservation and effective dragging. Feature maps from lower blocks (e.g., Block 1) make it difficult to maintain semantics, whereas feature maps from higher blocks (e.g., Block 4) preserve semantics well but result in poor dragging performance. This phenomenon is likely due to the lower blocks of the U-Net containing low-frequency information of the image [30]. Quantitative metrics supporting our qualitative evaluation are in the Supplement."}, {"title": "6. Manipulating Embeddings", "content": "DRAGTEXT enables the model to comprehend the transformation and degree of changes in the image while pre-"}, {"title": "7. Limitation", "content": "Diffusion-based editing models have the common problem of often losing the dragged feature in the latent space, leading to the vanishing of content in surrounding objects that are not the editing target (Fig. 10 (a)). For instance, grape grains within the red circle disappear when images are dragged, regardless of whether DRAGTEXT is applied. However, this situation changes dramatically when the user provides a more detailed text prompt. For instance, in Fig. 10 (b), although the user provides the exact word grapes, the baseline model still cannot drag the grape feature. On the other hand, DRAGTEXT is able to drag the grape feature because it optimizes the text embedding along with the image embedding, preserving the original content. The optimized text embedding plays a complementary role for the features lost in the image embedding space."}, {"title": "8. Conclusion", "content": "In this study, we introduced DRAGTEXT, a method emphasizing the critical role of text prompt in point-based image editing with diffusion models. We found that static text embeddings hinder the editing process by causing drag halting and loss of content. By optimizing text embeddings in parallel with image embeddings, DRAGTEXT ensures that text and image embeddings remain strongly coupled, ensuring better content preservation and drag accuracy. Our approach integrates seamlessly with existing diffusion-based drag models and includes a text embedding regularization technique to retain semantic integrity, hence significantly improving the performance and fidelity of image editing."}, {"title": "A. More Details on Prompt Engineering", "content": "In this section, we provide a detailed explanation of the analysis from Section 3.2., examining the effectiveness of prompt engineering in point-based image editing. First, we explain how the analysis was conducted. Next, we present more examples of the intention text we used and the corresponding results."}, {"title": "A.1. Implementation Details", "content": "Drag Editing with Intention Text. We estimated the editing intentions for each image using the handle points, the target points, and the image masks provided by the DragBench dataset [29]. Additionally, we referenced the edited results from four methods [12, 13, 29, 40]. The intention text prompts were crafted by injecting these editing intentions into the original text prompts. To ensure that secondary changes in the text prompts did not affect the editing results, we minimized alterations to the vocabulary and sentence structures of the original text prompts.\nFor example, consider an image with the original prompt \"a photo of a jug and a glass\" where the jug's neck needs to be shortened. We can craft an intention text prompt via [20] such as:\n\"Create an image of a jug with a shorter neck. Shorten the neck by the distance between a red dot and a blue dot. The jug should have a smooth, glossy finish. Place the jug against a simple, neutral background.\"\nHowever, this significantly altered the content of the original text prompt. This alteration makes it challenging to discern whether the changes in the edited result were due to these secondary modifications or the incorporation of the editing intention in the text. Consequently, we incorporated concise terms representing the editing intention while preserving the original vocabulary and sentence structure as much as possible, for example: \"a photo of a short-neck jug and a glass.\"\nLinear Interpolation. To reflect gradual changes in the image embeddings to the text embeddings, we linearly interpolate between the original text embeddings and the intention text embeddings during the dragging process. The weights of the original text embeddings and the intention text embeddings are determined based on the distance between the handle point h and the target point gi:\n\nw^k = \\frac{\\sum_{i=1}^{n} ||g_i - h^k||}{\\sum_{i=1}^{n} ||g_i - h_i||},\n\\hat{c}^k = w^k. \\hat{c}_{org} + (1 - w^k). \\hat{c}_{int}\nwhere corg is the original text embedding and \u0109t is the intention text embedding. At the beginning of the point-based editing, wk = 1 so \u0109k = \u0109org. Conversely, as the dragging progresses and handle points approach target points, the weight value wk increases, resulting in a higher proportion of \u0109nt. In this way, as the image is progressively edited, the proportion of the intention text embedding is gradually increased."}, {"title": "A.2. More Qualitative Results for Prompt Engineering", "content": "In Fig. 11, we present additional results for prompt engineering. Corresponding results for DRAGTEXT are also presented to validate the effectiveness of our approach in comparison to prompt engineering. Prompt engineering is found to have little impact on alleviating drag halting. In contrast, DRAGTEXT dragged handle points closer to target points, compared to the original text prompt, the intention text prompt, and their interpolation."}, {"title": "B. More Details on DRAGTEXT", "content": "In this section, we provide a comprehensive overview of our method to ensure clarity and ease of understanding. We describe the pipeline of DRAGTEXT using pseudo-code to aid in understanding our approach. Additionally, we detail the modifications necessary to apply DRAGTEXT to other point-based image editing methods."}, {"title": "B.1. Pseudo Code of DRAGTEXT", "content": "Algorithm 1 Pipeline of DRAGTEXT\nInput: Input image x0, text prompt c, image mask Mimage, handle points {hi}=1, target points {gi}=1, denoising U-Net Ue, diffusion time step t, maximum number of iterations steps K\nOutput: Output image xo\n1: Zt \u2190 apply DDIM inversion to xo conditioned on c\n2: z\u0142, c\u00ba, hzt, c, hi\n3: fork in 0: K-1 do\n4: F(,\u0109k)\u2190U(;\nUpdate \u0109k using text optimization\n5:\n\u0109k)\n6: Update 2 using motion supervision\n7: 2+1,\u0109k+1\u2190,\u0109k\n8: Update {hk+1}=1 using points tracking\n9: 2t, \u0109\u2190, \u0108K\n10: fort int: 1 do\n11:\n2t-1 \u2190 apply denoising to 2t conditioned on \u00ea\n12: 020\nIn DragText, another important element is the mask Mtext \u2208 Rlxd. This mask is used to regularize text embedding optimization but is not an input. Instead, it is automatically calculated by the CLIP tokenizer [22]. After the text prompt passes through the tokenizer, the tokens excluding [EOS] and [PAD] tokens contain significant semantic information. The length of these important tokens is l."}, {"title": "B.2. Modifications for Integrating with Other Methods", "content": "In the main paper, we explained DRAGTEXT based on DragDiffusion [29]. We chose this method because representative diffusion model-based dragging methods [12, 13, 40] all utilize approaches from DragGAN [21] and DragDiffusion. Therefore, they are constructed upon the foundation of DragDiffusion. However, they also developed techniques to overcome the limitations of DragDiffusion. Taking these improvements into account, we made minor modifications to DRAGTEXT to adapt our approach for each method."}, {"title": "B.2.1 FreeDrag", "content": "Point-based image editing has faced challenges, such as the disappearance of handle points during the dragging process. Additionally, point tracking often fails to reach the target point because it moves to the location with the smallest difference in the feature map. To address these issues, FreeDrag introduces Template Feature and restricts the path of point tracking to a straight line.\nFreeDrag generates the corresponding template features Tk for each of the n handle points. During the optimization step, the feature map is optimized to match the template features:\n\nL_{ms} = \\sum_{i=1}^{n} \\sum_{q_1} ||F_{q_1+d_i} (z^k, \\hat{c}^k) - T_i^k||_1\n+ \\lambda_{image} || (z^{k-1} - sg(z^{k-1})) \\odot (1 - M_{image}) ||_1.\n\nThis involves up to five iterations of motion supervision until the predefined conditions are met. Depending on the outcome, feature adaptation is categorized into (a) well-learned features, (b) features in the process of learning, and (c) poorly learned"}, {"title": "B.2.2 DragNoise", "content": "The bottleneck features st effectively capture richer noise semantics and efficiently capture most semantics at an early timestep t. Thus, DragNoise optimizes the bottleneck feature st of the U-Net instead of the latent vector zt, thereby shortening the back-propagation chain. Accordingly, DRAGTEXT optimizes st instead of zt during the image optimization processes. For each iteration k, \u015d undergoes a gradient descent step to minimize Lms:\n\n\\hat{s}_t^{k+1} = s_t^{k} - \\eta_{ms} \\frac{\\partial L_{ms} (\\hat{s}_t^{k},\\hat{c}_t^{k})}{\\partial \\hat{s}_t^{k}}\n\nIn DRAGTEXT, neither the latent vector z nor the bottleneck feature s undergoes gradient descent during the text optimization. Thus, the text optimization procedure is not modified in Drag Noise."}, {"title": "B.2.3 GoodDrag", "content": "GoodDrag alternates between dragging and denoising. This approach is different from traditional diffusion-based dragging methods. They generally execute all drag operations at once before denoising the optimized noisy latent vector 2t. During the denoising process, which involves sampling images 0 from a noisy latent vector 2t, perturbations from dragging are corrected. However, if the denoising process is performed only after all drag operations are completed, the errors accumulate too significantly to be corrected with high fidelity. To address this, GoodDrag applies one denoising operation after B image optimization and point tracking steps.\nFor example, the latent vector z has been denoised [\u300d times, the drag optimization is performed at the timestep t = T-. To ensure this process is consistent, the total number of drag steps K should be divisible by B. Since DRAGTEXT performs one text optimization step after one image optimization step, we sequentially repeat the image optimization, text optimization, and point tracking steps B times, and then apply one denoising operation.\nMoreover, when drag editing moves the handle points {hi}_1, the features around handle points tend to deviate from their original appearance. This deviation can lead to artifacts in the edited images and difficulties in accurately moving the handle points. To prevent this, GoodDrag keeps the handle point hi consistent with the original point h, throughout the entire editing process:\n\nL_{ms} = \\sum_{i=1}^{n} \\sum_{q_1} ||F_{q_1+d_i} (z^k, \\hat{c}^k) - sg(F_{q_1^0} (z^0, \\hat{c}^0)) ||_1\n+ \\lambda_{image} || (z^{k-1} - sg(z^{k-1})) \\odot (1 - M_{image}) ||_1,\n\nwhere q1 = (h, r1), and qf describes the square region centered at the original handle point h. Additionally, drag operations per denoising step B = 10. Similarly, DRAGTEXT ensures the handle point he remains consistent with the"}, {"title": "C. Implementation Details", "content": "Table 2. Hyperparameters for point-based image editing methods and DRAGTEXT.\nIn Table 2, we list the hyperparameters used for each point-based image editing method [12, 13, 29, 40]. These values are consistently used in both the Baseline and w/ DRAGTEXT experiments. For a fair comparison, we applied the same hyperparameter values from each respective paper to our experiments. Additionally, we maintained the same text optimization loss across all methods to demonstrate the robustness of our approach.\nIn FreeDrag, values related to point tracking are omitted since it replaces point tracking with line search."}, {"title": "D. More Qualitative Results", "content": "In Fig. 12, we additionally present the results of applying DRAGTEXT to each method [12, 13, 29,40]. In our experiments, we applied DRAGTEXT to various point-based image editing methods and evaluated their performance. The results show that DRAGTEXT can effectively drag the handle points to their corresponding target points while maintaining the semantic integrity of the original image. Moreover, the consistent success of DRAGTEXT across multiple methods underscores its robustness and adaptability."}, {"title": "E. Evaluation Metrics", "content": null}, {"title": "E.1. LPIPS", "content": "LPIPS [38] uses ImageNet classification models such as VGG [31], SqueezeNet [7], and AlexNet [11]. We measured LPIPS using AlexNet. LPIPS measures the similarity between two images by calculating the Euclidean distance of the activation maps obtained from several layers of a pre-trained network, scaling them by weights w, and then averaging the values channel-wise to compute the final LPIPS score.\nLPIPS is an appropriate metric for measuring the similarity between two images, emphasizing that image editing should maintain similarity to the original image. However, due to the nature of the drag editing task, the image will inevitably change. Consequently, when dragging is performed successfully, the LPIPS score might worsen. For instance, if an image does not change at all, it would yield an LPIPS score of 0, the best possible score. As shown in Fig. 13, even though we achieved a more desirable image editing outcome, the LPIPS score was lower. Therefore, we propose that LPIPS should not be overly emphasized if the score falls below a certain threshold. To address this issue, we suggest using the product of LPIPS and MD, which are complementary metrics, as a more robust evaluation metric."}, {"title": "E.2. Mean Distance", "content": "Mean distance (MD) is computed via DIFT [33]. First, DIFT identifies corresponding points in the edited image that correspond to the handle points in the original image. These identified points are regarded as the final handle points after"}, {"title": "F. Visual Ablation on the Hyperparameters of Regularization", "content": "In Fig. 15, we provide extra visual ablation results to demonstrate how the hyperparameter text impacts the regularization process in text optimization. We modified images by adjusting text within a range from 0 to 10, which allowed us to control the level of regularization applied during the text optimization phase. When text is close to 0, it results in some of the important semantic information being lost. On the other hand, applying an excessively high text prevents the optimization of the text embedding from effectively altering the image."}, {"title": "G. Visual Ablation on the U-Net Feature Maps", "content": "We utilize various U-Net decoder blocks for DRAGTEXT with the image embedding fixed from the 3rd block. In Fig. 16 and Table 3"}]}