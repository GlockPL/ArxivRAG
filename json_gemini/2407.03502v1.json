{"title": "AgentInstruct:\nToward Generative Teaching with Agentic\nFlows", "authors": ["Arindam Mitra", "Luciano Del Corro", "Guoqing Zheng", "Shweti Mahajan", "Dany Rouhana", "Andres Codas", "Yadong Lu", "Wei-ge Chen", "Olga Vrousgos", "Corby Rosset", "Fillipe Silva", "Hamed Khanpour", "Yash Lara", "Ahmed Awadallah"], "abstract": "Synthetic data is becoming increasingly important for accelerating the development of\nlanguage models, both large and small. Despite several successful use cases, researchers\nalso raised concerns around model collapse and drawbacks of imitating other models. This\ndiscrepancy can be attributed to the fact that synthetic data varies in quality and diversity.\nEffective use of synthetic data usually requires significant human effort in curating the data.\nWe focus on using synthetic data for post-training, specifically creating data by powerful\nmodels to teach a new skill or behavior to another model, we refer to this setting as Generative\nTeaching. We introduce AgentInstruct, an extensible agentic framework for automatically\ncreating large amounts of diverse and high-quality synthetic data. AgentInstruct can create\nboth the prompts and responses, using only raw data sources like text documents and code\nfiles as seeds. We demonstrate the utility of AgentInstruct by creating a post training dataset\nof 25M pairs to teach language models different skills, such as text editing, creative writing,\ntool usage, coding, reading comprehension, etc. The dataset can be used for instruction\ntuning of any base model. We post-train Mistral-7b with the data. When comparing\nthe resulting model (Orca-3) to Mistral-7b-Instruct (which uses the same base model), we\nobserve significant improvements across many benchmarks. For example, 40% improvement\non AGIEval, 19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement\non BBH and 45% improvement on AlpacaEval. Additionally, it consistently outperforms\nother models such as LLAMA-8B-instruct and GPT-3.5-turbo.", "sections": [{"title": "Introduction", "content": "Synthetic data accelerated the development of LLMS: The rise of synthetic data in\nthe training of Large Language Models (LLMs) has been a significant development of the\nlast year. Synthetic data was used to significantly accelerate the progress of model training\n(especially SLMs) in all stages of training from pre-training (e.g., [1]), to instruction-tuning\n(e.g., [21, 36]) and RLHF(e.g., [12, 28]).\nGenerating high quality synthetic data is hard: On the other hand, research has also\nshown that pre-training models on synthetic data generated by other models can lead to\nmodel collapse [29], leading to models gradually degenerating as a result. Similar arguments\nhave been made against using synthetic data for pos-training, which could amount to an\nimitation process that could teach the trained model to pick only stylistic characteristics\nand not real capabilities [8]. This discrepancy could be explained by the observation that\ncreating high-quality and diverse synthetic data is hard [17]. Successful use of synthetic data\ninvolved significant human effort in curating and filtering the data to ensure high quality. If\nwe focus on post-training synthetic data, we will see the most widely used approach includes\nstarting with a set of prompts and using a powerful model such as GPT-4 [22] to generate\nresponses to these prompts [24] or of an expanded set of the prompts [36]. This recipe\nwas further improved by eliciting explanations or step-by-step instructions from the teacher\nmodel [20] or using more complex prompting techniques to elicit higher quality answers [18].\nSynthetic data meets Agents: Another major development we witnessed last year is the\nrise of Agentic (especially multiagent) workflows [33, 13]. Agentic workflows can generate\nhigh quality data, that surpasses the capabilities of the underlying LLMs, by using flows\nwith reflection and iteration, where agents can look back at solutions, generate critiques and\nimprove solutions. They can also use tools (e.g. search apis, calculator, code interpreters)\naddressing limitations of LLMs. Multi-agent workflows bring in additional benefits such\nas simulating scenarios where we can generate both new prompts and the corresponding\nresponses. They also enable automation of the data generation workflows reducing or\neliminating need for human intervention on some tasks.\nGenerative Teaching & Orca AgentInstruct: Generating synthetic data for post-\ntraining often relies on an existing prompt set that is used as is or used as seeds for\ngenerating more instructions. In this work, we generalize the problem settings to a broader\nobjective of generating abundant amounts of diverse, challenging and high-quality data to\nteach a particular skill to an AI model, we refer to this setting as Generative Teaching.\nAgentInstruct is an agentic solution for Generative Teaching. AgentInstruct focuses on\ncreating demonstration and feedback data and requires only raw documents as input. When\ngeneric data is used as seeds, AgentInstruct can be used to teach an LLM a general capability\n(e.g. Math, Reasoning, RAG, etc.). Domain specific data (e.g. gaming, finance) can also be\nused as seeds to improve the model in a certain specialization. AgentInstruct can create:\n1. High-quality data: using powerful models like GPT-4, coupled with tools like search\nand code interpreters.\n2. Diverse data: AgentInstruct generates both prompts and responses. It uses a large\nnumber of agents (equipped with powerful LLMs, tools and reflection flows) and a\ntaxonomy (of over 100 subcategories) to create diverse and high quality prompts\nand responses,\n3. Large quantities of data: AgentInstruct can run autonomously and can apply flows\nfor verification and data filtering. It does not require seed prompts and uses raw\ndocuments for seeding.\nUsing raw data (unstructured text documents or source code) as seeds has two benefits.\nFirst, this data is available in abundance enabling the use of AgentInstruct to create large\namounts of diverse data. Additionally, using raw data as seeds, and hence, avoiding using\nexisting prompts, as is or after paraphrasing, can promote learning more general capabilities\nas opposed to benchmark-specific ones.\nWe demonstrate the utility of AgentInstruct by creating a comprehensive synthetic post-\ntraining dataset of 25 million prompt and response pairs. The dataset covers a wide array"}, {"title": "Generative Teaching: AgentInstruct", "content": "Creating synthetic datasets for supervised fine-tuning and instruction-tuning has seen\nsignificant progress over the last year. The quality of these datasets has been steadily\nimproving. High quality can be achieved by using powerful frontier models (or agenetic flows\nbased on these models) to generate responses. However, when creating synthetic data, in\naddition to quality, we also need to consider several other fundamental questions:\n1. How can we create a vast amount of data?\n2. How can we ensure that the generated data is diverse?\n3. How can we generate complex or nuanced data points?\nIn the AgentInstruct methodology, we outline a structured approach to tackle these challenges\nas follows:"}, {"title": "AgentInstruct Flow for Reading Comprehension", "content": "Reading comprehension is a critical skill involving processing and understanding text, which\nis necessary for learning and encompasses decoding, fluency, and vocabulary knowledge.\nReading comprehension questions range from asking for explicit information (literal com-\nprehension) to requiring inferences, understanding vocabulary in context, analyzing text\nstructure and argumentation, critically evaluating the content, and synthesizing informa-\ntion from different parts of or multiple texts. Reading comprehension is a very important\ncapability and can enable scenarios like question answering, search, grounded reasoning, etc.\nContent Transformation Flow Web crawls encompass an extensive collection of human-\ngenerated text, which holds potential for generating reading comprehension materials.\nHowever, these sources are not inherently structured to facilitate the teaching of read-\ning comprehension skills. Consequently, they do not support the consistent generation of\ndiverse question types required for comprehensive reading comprehension evaluation. For\ninstance, the LSAT Logical Reasoning test features specialized question categories, including\nassumption, strengthening/weakening, flaw, and inference questions. Crafting such questions\nnecessitates passages with a particular stylistic and logical framework. The objective of\nContent Transformation Flow is to transform arbitrary articles into well-crafted pieces that\nare conducive to the formulation of a wide array of reading comprehension question types.\nOur current flow for Reading Comprehension encompasses a suite of nine content transfor-\nmation agents for generating argument passages, debates and conversations, long passages,\nmeeting transcripts, poems, satarical content, etc. Detailed description is provided in Ap-\npendix A. Given a seed article, the flow will randomly pick one of the Content Transformation\nAgents to assess the seed article and attempt to generate the text passages. The following\nprovides an example for the Argument Passage Generator."}, {"title": "AgentInstruct Flow for Text Modification", "content": "Text modification is the process of editing and refining written content to enhance its quality\nand effectiveness or alter its attributes. This involves correcting spelling and grammar,\nclarifying ideas, reorganizing content for better flow, adjusting tone, ensuring style consistency,\nfact-checking, removing redundancies, formatting, developing content, and adapting to specific\naudiences. It is a useful skill for LLMs that help in content writing. While several content\ntransformation agents can be introduced to intentionally modify the text as described earlier,\nwe focus here on showing the how the instructions are created and refined.\nSeed Instruction Generation Flow We have currently compiled a collection of 18 types\nof text modification tasks including paraphrasing, expansion, simplification, redacting or\nremoving content, styling, code switching, etc. The full list is in Appendix A.\nWe define an Agent for each of the task type. Each agent takes as input a piece of text and\ncreates several text modification tasks of the associated type. Here we provide an example\ninput and a task created by the Paraphrasing Agent."}, {"title": "AgentInstruct Flow for Tool Use", "content": "The task of tool use or API use for LLMs involves enabling models to interact with external\ntools or services; via APIs. This capability allows AI systems to extend their functionality,\naccess external data, and perform actions beyond their native capabilities.\nContent Transformation Flow We use source code snippets or an API description [26] as\nthe random seed. If source code snippets has been used as the seed, a content transformation\nagent is used to synthesize an API description from the code snippet. The goal of the\nContent Transformation Flow is to synthesize list of APIs from the random seed. API lists\nare created by either: (1) using an API retrieval agent that iteratively searches for similar\ncode to expand the API list or (2) the agent uses the LLM to hypothesize other APIs present\nin the library.\nThe following figure provides an example of the library reconstruction scenario."}, {"title": "Orca-3", "content": "We implemented an AgentInstruct Flow for 17 different capabilities as described in Table 1.\nWe created a collection of approximately 22 million instructions aimed at teaching the\naforementioned skills. We have used unstructured text and code files sampled from\nKnowledge Pile [7], AutoMathText [38], a subset of openstax and a subset of apache-2.0\nlicensed source code files from [4]. The dataset covers variety of skills, as detailed in Table 1.\nUsing unstructured content as seeds for instruction data generation has several benefits.\nFirst, there is abundance of such data enabling the generation of large-scale and diverse\ninstruction data. Additionally, it enables us to avoid using any benchmark-specific data as\nseeds and hence focus on optimizing for a capability, not for a specific benchmark.\nIn addition to the 22 million instructions, we have incorporated approximately 3.8 million\npaired instructions sourced from Orca-1[21], Orca-2[18], Orca-Math[19] and samples from\nother publicly available sources such as [5, 37, 10, 30]. We refer to this data as Orca-2.5-\ndataset.\nThe culmination of these datasets results in approximately 25.8 million paired instructions,\nall of which are incorporated into the training of Orca-3. Furthermore, we have trained a\nseparate model, referred to as Orca-2.5, using the 3.8 million instructions (Orca-2.5-dataset).\nThe purpose of this is to compare and evaluate the impact of the 22 million instructions\ncurated through AgentInstruct."}, {"title": "Training Details", "content": "We use the 25.8 million paired instructions described earlier to finetune Mistral-7b-v0.1.\nWe choose this model because the it makes the weights publicly available for the base\n(no-instruction-tuned) version, with a permissive license allowing easy redistribution. We\nrefer to the finetuned model (Mistral-7b-v0.1 finetuned on AgentInstruct dataset) as Orca-3.\nEach pair in the dataset undergoes a tokenization process using the Mistral tokenizer,\nensuring a maximum sequence length of 8192 with packing. To guarantee that the training\nloss is calculated based only on the response conditioned on the prompt, label masking is\napplied. Weight decay was set at 0.1\nThe finetuneing used 19 NVIDIA A100 nodes, or 152 NVIDIA A100 GPUs, each with a\nbatch size of 10. We used AdamW optimizer with an initial learning rate of 8e-6 and a a\ncosine learning rate schedule. We also used a linear learning rate warm-up during the initial\n500 steps. The model was trained for three epochs and the training process concluded after\napproximately 200 hours."}, {"title": "Evaluation Results", "content": "The Orca-Bench dataset serves as a held-out test set, consisting of 100 samples from each of\nthe 17 skills for which data was curated using AgentInstruct, except for the Open Domain\nQuestion Answering (ODQA) category, where we created two test sets. The first subset,\nreferred to as ODQA, consists of 100 questions originated from the initial seed instruction\nphase. The second subset, termed Complex ODQA, includes more intricate questions\ndeveloped during the refinement phase.\nWe evaluated the performance of all baselines using the Orca-Bench dataset. These were\nscored relative to GPT-4 on a scale ranging from 0 to 10. It is worth noting that some of\nthe entries within Orca-Bench involve multiple exchanges. To illustrate, let a multi-turn\ninteraction in Orca-Bench be denoted by the sequence (system message, user1, assistant,\nuser2, assistant, ...), where each turn is crafted by GPT4 (teacher). For every user; input, we\ngenerate a corresponding student response, which is conditioned on the preceding conversation\nhistory as established by the teacher. We then evaluate the student's generated response"}, {"title": "Orca-Bench", "content": "The average (macro) scores across all assessed dimensions. On average,\nincluding orca-3 after each training epoch, the inclusion of AgentInstruct data has led to\na performance augmentation of 33.94% over the Orca 2.5 baseline and an enhancement of\n14.92% over Mistral-Instruct-7B."}, {"title": "Benchmark Results", "content": "We evaluate Orca-3 against 5 baseline models including Orca-2.5, Mistral-7B-Instruct-v0.3,\nLLAMA3-8B-Instruct, GPT-3.5-turbo and GPT-4 on the following benchmarks:\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 AGIEval: AGIEval [39] is a human-centric benchmark that evaluates a model's\nabilities in tasks pertinent to human-cognition and problem-solving. It evaluates\nhow well models perform in answering questions from human-centric standardized\nexams such as SAT, LSAT and math competitions.\nMMLU: Massive Multitask Language Understanding (MMLU) [9] benchmark\nmeasures a model's multitask understanding. The benchmark includes approximately\n16000 multiple choice questions covering a wide range of 57 academic subjects such\nas maths, philosphy, medicine, psychology, computer-science, law etc. testing both\ngeneral and specialized knowledge of the model being tested.\nARC: The AI2 Reasoning Challenge (ARC) [2] benchmark, developed by AllenAI,\nmeasures the reasoning, commonsense knowledge and deep comprehension abilities\nof language models. The test set contains 3548 multiple-choice questions that are\ndivided into 2 sets: Easy (2376) and Challenge(1172).\nBBH: Big Bench Hard [31] consists of a set of 23 tasks selected from the broader\nBig-Bench benchmark spanning a wide array of academic subjects requiring complex,\nmulti-step reasoning.\nGPQA: Graduate-level Google-Proof Q&A [27] is a challenging benchmark of 448\nhigh-quality and extremely difficult multiple-choice questions created by domain\nexperts (pursuing PhDs in their domains) in biology, chemistry and physics.\nDROP: Discrete Reasoning over Paragraphs [6] is a Reading Comprehension bench-\nmark requiring the models to resolve references in questions and perform discrete\noperations over them such as sorting, counting, addition etc."}, {"title": "Evaluation: Reading Comprehension", "content": "Reading comprehension is a crucial capability for LLMs. It is arguably even more important\nfor Small Language Models (SLMs), as they are better suited as reasoning engines than mere\nretrieval systems. Through targeted training with AgentInstruct, we observe substantial\nimprovement in Mistral's reading comprehension capabilities (Table ??)-showcasing an 18%\nimprovement over Orca 2.5 and a 21% gain relative to Mistral-Instruct-7b. Furthermore, by\nleveraging this data-driven approach, we have elevated the performance of a 7B model to\nmatch that of GPT-4 on the reading comprehension sections of the Law School Admission\nTests (LSATs), which are considered difficult for human test-takers."}, {"title": "Evaluation: Math", "content": "Assessing the reasoning capabilities of AI models can be effectively accomplished through\nmath problem solving. While SLMs have shown considerable improvement in elementary\nmath, their performance typically falters with more complex high school and college-level\nmathematics. Math problems are generated by the Open Domain Question Answering\nand Multiple-Choice Questions Flows. With AgentInstruct, we have managed to enhance\nMistral's proficiency across a spectrum of difficulties, ranging from elementary to college-level\nmath (Table 5. This has led to a signficant performance boost, with improvements ranging"}, {"title": "Evaluation: Format Following", "content": "Following formatting guidelines is essential for language models to be applicable in real-world\nsituations. In all AgentInstruct flows, we ensure that format-following is taught for each\nparticular scenario, by synthesizing, through agents, several formatting guidelines. By doing\nso, we are able to significantly improve (11.5%) Mistral's ability to follow formats, surpassing\neven the capabilities of Gemini Pro."}, {"title": "Evaluation: Abstractive Summarization", "content": "Summarization is an important capability for Language Models, with many models achieving\nhigh quality summarization performance, yet struggling with hallucination. We assessed\nsummarization ability using two key metrics: hallucinations and quality. For this purpose,\nwe utilized GPT4 as our evaluator. The prompts utilized in these evaluations can be found\nin Appendix B. We used the following benchmarks for evaluating summarization abilities:\n\u2022\n\u2022\n\u2022 ACI-Bench: The Ambient Clinical Intelligence Benchmark (ACI-Bench) [32] is a\ndataset designed for benchmarking automatic report generation from doctor-patient\nconversations. The test set comprises 120 data points.\nInstruSum: A dataset [15] for evaluating the generation capabilities LLMs for\ninstruction-controllable summarization. It consists of 100 datapoints.\nOrca-Sum: A newly created benchmark to evaluate LLMs' ability to follow summa-\nrization and grounded data transformation instructions. To construct this test set,\nwe sampled data from 45 summarization datasets collected from Hugging Face across\nmultiple domains such as news, conversations, science, health, social, e-mails, code,\netc. for a total of 458 datapoints. We randomly collected, up to 1000 datapoints\nwhich then we carefully deduplicated to avoid overlapping with the training set. We\nthen used GPT-4 to generate a set of 40 prompts for each dataset out of each we\nrandomly sampled one for each selected datapoint. The prompts are dataset-specific\nand focus on summarization, grounding, and data transformation. For instance, a\nprompt may ask the model to generate a TikTok video out of a scientific paper or\na legal contract from a Wikipedia page. This allows us to measure not only the\nquality of the response but also hallucination in a challenging scenario, as the model\nis forced to move between formats and domains."}, {"title": "Evaluation: RAG", "content": "The RAG (Retrieval Augmented Generation) skill significantly boosts the capacity of Lan-\nguage Models to generate informed, contextually precise responses, hence upgrading their\noverall performance and usefulness. It is arguably more effective to test the RAG proficiency\nof language models in areas where the models have limited knowledge. For this study, we\nselected MIRAGE[35], a benchmark that focuses on answering medical questions by referring\nto information retrieved from a medical corpus. Since the medical domain is not typically a\nprimary focus of the models evaluated in this study, MIRAGE provides an effective platform\nfor assessing their RAG capabilities. Additionally, AgentInstruct RAG data used generic,\nnon medical data seed, enabling us to test how well can the skill (RAG) be applied to new\ndomains."}, {"title": "Limitations", "content": "AgentInstruct reduces human expertise required for data generation significantly and enables\ncreating of high-quality synthetic data at scale. However, this is till an early step in this\ndirection and could suffer from many limitations associated with synthetic data generation,\nincluding but not limited to:\nExtensibility: Creating the agentic flows for different skills depends on human effort for\nthe construction of the flows. Future work should consider how to automate the construction\nof the agentic flow from the user specification.\nAccuracy: Synthetic data may not perfectly replicate the complexity and nuances of real-\nworld data, leading to potential inaccuracies. Additional work is needed to better assess the\nquality of the data.\nCost: Generating synthetic data with multiple agents using LLMs and tools can be resource-\nintensive.\nBias: If the original seed data used to generate synthetic data contains biases, these biases\ncan be reflected and even amplified in the synthetic data.\nValidation: It can be difficult to validate synthetic data to ensure it accurately represents\nthe desired scenarios.\nDependency on Seed Data: The quality of synthetic data is dependent on the quality of\nthe real data used as seeds. Poor quality input data could result in poor quality synthetic\ndata.\nOrca-3 is fine-tuned with the AgentInstruct data based on the Mistral model family, and\nretains many of its limitations, as well as the common limitations of other large language\nmodels and limitations originating from the training process, including:\nData Biases: Large language models, trained on extensive data, can inadvertently carry\nbiases present in the source data. Consequently, the models may generate outputs that could\nbe potentially biased or unfair.\nLack of Transparency: Due to the complexity and size, large language models can act\nas \"black boxes\", making it difficult to comprehend the rationale behind specific outputs or\ndecisions. We recommend reviewing transparency notes from Azure for more information\u00b2.\nContent Harms: There are various types of content harms that large language models\ncan cause. It is important to be aware of them when using these models, and to take\nactions to prevent them. It is recommended to leverage various content moderation services\nprovided by different companies and institutions. On an important note, we hope for better\nregulations and standards from government and technology leaders around content harms\nfor AI technologies in future. We value and acknowledge the important role that research\nand open source community can play in this direction.\nHallucination: It is important to be aware and cautious not to entirely rely on a given\nlanguage model for critical decisions or information that might have deep impact as it is\nnot obvious how to prevent these models from fabricating content. Moreover, it is not clear\nwhether small models may be more susceptible to hallucination in ungrounded generation\nuse cases due to their smaller sizes and hence reduced memorization capacities. This is an"}, {"title": "Conclusions", "content": "The AgentInstruct approach to Generative Teaching offers a promising solution to the\nchallenge of generating large amount of diverse and high-quality data for model post-training.\nThis method stands out by using agentic flows for synthetic data generation, thus addressing\nkey concerns associated with the use of synthetic data in model training, such as the lack of\ndiversity and the need for intensive human curation and intervention during the data creation\nprocess. By leveraging an agentic framework, AgentInstruct can generate tailored datasets\ncomprising both prompts and responses from unstructured data sources, facilitating the\npost-training of models and teaching them variety of skills. The efficacy of this approach is\nexemplified by the substantial improvement observed in the Orca-3 model, which, post-trained\nwith a 25M pair dataset generated by AgentInstruct, showcased a notable performance gain\nacross multiple benchmarks. We believe using agentic flows for creating synthetic data can\nshow significant value for all stages of model training, including pre-training, post-training\nand domain/task specialization. The ability to use unstructured content to generate diverse\nand high-quality instruction data given any specifications could pave the way for creating\n(semi) automated pipelines using synthetic data for model customization (using domain\nspecific content as seeds) and continual improvement (generating higher quality data than\nthe base model with agentic flows)."}, {"title": "Agentic Flows Details", "content": "Reading Comprehension transformation agents:\n1. Argument Passage Generator: This agent is adept at creating passages that\narticulate arguments, which may occasionally contain logical inconsistencies.\n2. Debate Passage Generator: It specializes in crafting passages that mimic the\nstructure and content of debate transcripts.\n3. Conversation Passage Generator: This agent generates passages that depict\ndialogues.\n4. Meeting Transcript Generator: It is designed to produce meeting transcripts.\n5. Poem Generator: This agent generates poems.\n6. Satirical Passage Generator: It creates texts infused with satirical wit.\n7. Instructional Passage Generator: This agent generates passages resembling\ninstructional manuals.\n8. Long Text Generator: It extends the original text by incorporating additional\ninformation, thereby increasing its length.\n9. Identity Agent: A straightforward agent that replicates the input text verbatim.\nInstruction Taxonomy for Seed Instruction Generation Flow\n1. Literal Comprehension Question (Short Answer (or list)): a question that asks for a\nspecific detail(s) or fact(s) clearly stated in the text.\n2. Numerical Discrete Reasoning (Reasoning): questions that require the reader to use\nnumerical reasoning over many facts from the text.\n3. Critical Comprehension Question (True/False): construct two statements about the\npurpose or point of view that the reader must assess as true or false, with one being\ntrue and the other false.\n4. Evaluative Comprehension Question (Essay): an open-ended question that prompts\nan in-depth analysis of the text's theme or the effectiveness of an argument.\n5. Vocabulary and Language Use (Fill-in-the-Blank): a fill-in-the-blank question that\ntests understanding of a particular word or phrase used in the text.\n6. Relationship Comprehension Question (Matching): a matching question where\nrespondents pair items based on a specific criterion.\n7. Sequencing Events (Ordering): a series of events from the text arranged in the\ncorrect chronological order.\n8. Strengthen: identify information that would make the argument's conclusion more\nlikely to be true.\n9. Weaken: find evidence or an argument that would make the conclusion less likely to\nbe true.\n10. Assumption (Necessary Assumption): determine what must be true for the argument\nto hold.\n11. Flaw: point out a mistake in the argument's reasoning.\n12. Inference (Must Be True): Choose an option that logically follows from the informa-\ntion provided.\n13. Principle (Identify the Principle): Recognize the general rule or principle that\nunderlies the argument.\n14. Method of Reasoning (Describe the Argument): Describe how the argument is\nconstructed logically.\n15. Resolve the Paradox: Offer an explanation that reconciles seemingly contradictory\ninformation."}, {"title": "Text Modification Flow", "content": "Instruction Taxonomy for Seed Instruction Generation Flow\n1. Paraphrasing: Rewriting text using different words and sentence structures while\nmaintaining the original meaning.\n2. Text Simplification: Making text easier to read and understand by using simpler\nwords and sentence structures, often for children or language learners.\n3. Text Expansion: Adding more information or detail to make text more comprehensive\nor to meet a certain word count.\n4. Text Translation: Converting text from one language to another while attempting\nto preserve the original meaning as closely as possible.\n5. Text Formatting: Altering the appearance of text to improve readability or for\nstylistic purposes.\n6. Sentiment Modification: Changing the tone of the text to alter its emotional impact,\nsuch as making a sentence sound more positive or negative.\n7. Text Annotation: Adding notes, comments, or explanations to a text, often for the\npurpose of analysis or to provide additional context.\n8. Keyword Replacement: Substituting specific words or phrases with synonyms or\nrelated terms.\n9. Text Removing: Redacting or removing content from text.\n10. Text Capitalization: Adjusting the case of letters in text, such as converting to\nuppercase, lowercase, title case, or sentence case, starting every sentence with a\nparticular letter, word.\n11. Text Styling: Applying styles like bold, italics, underline, etc., to emphasize certain\nparts of the text or for aesthetic purposes.\n12. Content Rewriting: Extensively modifying a text to produce a new version, which\ncould involve changing the perspective, style, or target audience.\n13. Data Normalization: Standardizing text to ensure consistency, such as converting\ndates and times to a standard format or unifying the spelling of words.\n14. Plagiarism Rewording: Altering text to avoid plagiarism, ensuring that the content\nis original.\n15. Code Switching: Alternating between languages or dialects within a text, often to\nreflect bilingual speakers' patterns or for creative writing.\n16. Text Obfuscation: Intentionally making text vague or harder to understand, some-\ntimes for security purposes (like masking personal data).\n17. Textual Entailment: Modifying a sentence or phrase to either entail or contradict\nanother sentence, often used in natural language processing tasks.\n18. Rewriting with vocabulary limitations: Rewriting the entire text or a piece of it\nwhile using a limited vocabulary. For example, all words should start with letter 'a',\nall n-th word should start with letter 'b', each sentence should start with a 'vowel',\netc."}, {"title": "Evaluation Details", "content": "The types of tasks/benchmarks and the corresponding method used to extract answer and\ngenerate metrics is specified below:\n\u2022 Multiple Choice Questions: All the models are evaluated in an open-ended\ngeneration setting with an empty system message We then use GPT-4 for extraction\nof the option selected by the model from model's response instead of regex based\nextraction done in [18]. The extracted prediction is matched with the ground truth\nto generate accuracy scores.\nThe system message used for the GPT-4 extractions is as follows:\n\u2022 Exact Match/Span Extraction Problems: For tasks with math based questions\nlike GSM8K and problems where a ground-truth answer value is given (like DROP),\nwe prompt the models being evaluated to generate the answer and use GPT-4\nto extract the exact answer and also match it with the ground-truth provided to\nproduce a final verdict of whether the model's answer was 'Correct' or 'Incorrect'.\nWe use a specific system message for maths based questions, and another for all the\nother exact match/span extraction problems, both of which are provided below.\n\u2022 EQBench: For EQBench, we prompt the models to generate the emotion scores\ngiven the conversation in the prompt and then use GPT-4 to extract the scores\ngenerated by the model for each emotion in the prompt. The metric scores are\ngenerated using both the version 1 and 2 implementations described in the EQBench\npaper and the creators' github repository. The scoring calculation is calibrated such\nthat a score of 0 corresponds to answering randomly, and a 100 would denote perfect\nalignment with the reference answer. The system message used for extraction of\nemotion scores from evaluated model's response using GPT-4 is given below:\n\u2022 Open-Ended Generation: These are the tasks where model is prompted to\ngenerate an answer to an open-ended question, but a ground-truth to match the\nanswer is not available. The metric calculation method for the benchmarks in this\ncategory are provided below:"}]}