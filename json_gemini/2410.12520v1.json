{"title": "QueensCAMP: an RGB-D dataset for robust Visual SLAM", "authors": ["Hudson M. S. Bruno", "Esther L. Colombini", "Sidney N. Givigi Jr."], "abstract": "Abstract-Visual Simultaneous Localization and Mapping (VSLAM) is a fundamental technology for robotics applications. While VSLAM research has achieved significant advancements, its robustness under challenging situations, such as poor lighting, dynamic environments, motion blur, and sensor failures, remains a challenging issue. To address these challenges, we introduce a novel RGB-D dataset designed for evaluating the robustness of VSLAM systems. The dataset comprises real-world indoor scenes with dynamic objects, motion blur, and varying illumination, as well as emulated camera failures, including lens dirt, condensation, underexposure, and overexposure. Additionally, we offer open-source scripts for injecting camera failures into any images, enabling further customization by the research community. Our experiments demonstrate that ORB-SLAM2, a traditional VSLAM algorithm, and TartanVO, a Deep Learning-based VO algorithm, can experience performance degradation under these challenging conditions. Therefore, this dataset and the camera failure open-source tools provide a valuable resource for developing more robust VSLAM systems capable of handling real-world challenges.", "sections": [{"title": "I. INTRODUCTION", "content": "The process of Visual Odometry (VO) involves estimating an agent's position and orientation (pose) based solely on visual information obtained from cameras attached to it [1]. VO is a crucial component of Visual Simultaneous Localization and Mapping (VSLAM). VSLAM has become a fundamental technology in robotics, enabling systems to navigate and understand their environment using visual data autonomously. It is widely used in robotics due to its low cost and lightweight nature [2].\nHowever, the deployment of these systems in real-world scenarios presents multiple challenges, especially because VO and VSLAM algorithms are prone to fail under challenging situations, such as textureless areas, inadequate illumination, motion blur, varying weather conditions, and camera lens failures [3]. Consequently, ensuring the robustness of these systems is considered one of the most challenging tasks in the field today [4].\nThe evaluation and improvement of VSLAM algorithms depend on the availability of datasets that simulate real-world challenges. While widely employed benchmarks such as KITTI [5], TUM RGB-D [6], and Euroc MAV [7] offer diverse environments and motion patterns, these datasets lack the complexity needed to fully assess VSLAM performance under challenging scenarios.\nTo address these gaps, this paper introduces an RGB-D VSLAM dataset that includes not only easy-to-capture"}, {"title": "II. RELATED WORK", "content": "The robustness of VSLAM systems under challenging conditions has been a longstanding concern for researchers in robotics [4]. To evaluate VSLAM systems, multiple datasets have been introduced. The most commonly used datasets for evaluating these systems are KITTI [5], TUM RGB-D [6], and Euroc MAV [7].\nThe KITTI dataset [5] consists of 22 stereo image sequences covering a total distance of 39.2 km, captured from a moving car. However, KITTI is limited in its motion patterns, as the vehicle predominantly moves forward and does not exhibit significant upward or backward motion. In contrast, the TUM RGB-D dataset [6] offers a benchmark for evaluating RGB-D SLAM systems, with 39 sequences recorded in two indoor environments using an RGB-D camera mounted on a Pioneer 3 robot and a handheld camera. Moreover, the Euroc MAV dataset [7] is designed for the assessment of visual-inertial SLAM and 3D reconstruction, providing 11 sequences that range from slow flights under favorable visual conditions to dynamic flights with motion blur and poor illumination. This dataset focuses on small-scale indoor scenes with six degrees of freedom (DoF).\nAlthough the KITTI, Euroc MAV, and TUM RGB-D datasets cover a variety of scenes and camera motions, they do not address some of the more challenging scenarios, such as adverse weather conditions, featureless areas, and poor illumination. To tackle these limitations, other specialized datasets have been proposed. For instance, a benchmark for outdoor visual localization evaluation, under changing illumination and weather, was proposed in [8]. Similarly, Carlevaris-Bianco et al. [9] proposed a dataset containing changing environments (indoor and outdoor), moving obstacles, changing lighting, varying viewpoints, and seasonal and weather changes. Moreover, in [10] a dataset with more than 20 million images was provided with varying weather conditions, including heavy rain, night, direct sunlight, and snow."}, {"title": "III. METHODOLOGY", "content": "In this section, we explain how the dataset was built, including the materials used to collect the data and the scripts applied to inject failures. We also present the algorithms used to evaluate the collected data and the metrics used to assess the estimated trajectories."}, {"title": "A. Data collection", "content": "We collected data in an indoor environment equipped with a Vicon motion capture system. This system provides precise six DoF position measurements of an aerial vehicle at 30 Hz. The capture area measures 5 meters in width, 7.5 meters in length, and 3 meters in height. shows the environment that includes various objects"}, {"title": "IV. RESULTS", "content": "In this section, we present the quantitative and qualitative results obtained with the experiments described in Section III-B. All metrics and plots presented in this section were computed with a Python package to evaluate Odometry and SLAM called EVO [21]."}, {"title": "A. Quantitative Results", "content": "Table II presents the ATE obtained using the proposed algorithms across all sequences in the dataset. For ORB-SLAM2, tracking can be lost when the number of extracted and matched keypoints is insufficient, with relocalization occurring only when the system recognizes a previously seen location [20]. Therefore, we report results only for sequences where at least 50% of the images were successfully tracked. In contrast, as a deep learning-based method, TartanVO does not extract keypoints from images and tracks all images regardless of the pose estimation quality.\nObserving Table II, we found that ORB-SLAM2 fails to track at least 50% of the trajectories in most scenarios involving lens dirt, and in half of the sequences with lens condensation. On the other hand, in sequences with wet lenses, although ORB-SLAM2 maintains trajectory tracking, the quality of the estimated trajectories is consistently lower compared to other scenarios. This degradation occurs because the algorithm continues to extract and match a sufficient number of keypoints, even when many of these are outliers, generally due to water droplets on the lens.\nAdditionally, we observed that ORB-SLAM2 failed to track the camera pose in sequences 08 and 10, which contain featureless areas viewed for extended periods. In these sequences, the algorithm did not lose track of the estimated trajectory with the breakage, wet, and condensation failures, however, the pose estimates present high errors. This is because, in the absence of sufficient distinctive features, the algorithm predominantly tracked and matched points generated by the lens failures.\nFurthermore, we found that the injection of underexposure and overexposure failures did not significantly impact the"}, {"title": "B. Qualitative Results", "content": "We selected sequences 01 and 14 for the qualitative evaluation due to their distinct characteristics. Sequence 01 features dynamic objects and favorable lighting conditions, as previously detailed in Table I. In contrast, sequence 14 is characterized by low light conditions and static objects. The plots comparing the estimated trajectories and ground-truth trajectory are shown in Fig. 5.\nIn the impact of breakage and wet lens failures on ORB-SLAM2 is evident. These failures significantly degrade the estimated trajectory, which aligns with the substantial errors observed in the quantitative results. Additionally, the pose estimation degradation caused by condensation is clearly visible in Fig. 5c. Moreover, in sequence 01 the dirt lens failure causes the algorithm to lose track of a significant portion of the trajectory. In sequence 14, the lens breakage causes the algorithm to estimate minimal camera movement, rendering the trajectory almost static and invisible in the plot.\nThe trajectories estimated by TartanVO are consistent with the original estimation when applying underexposure and overexposure. Interestingly, the algorithm was robust to underexposure even in sequence 14 (Fig. 5d) where the low light conditions are exacerbated by noise injection. However, the algorithm exhibits considerable degradation when the other failures are introduced. Therefore, although the algorithm appears more consistent based on the ATE, the plots reveal that the estimated trajectories can still be significantly compromised under failure conditions."}, {"title": "V. CONCLUSIONS", "content": "This paper introduces the QueensCAMP dataset, designed to evaluate the robustness of RGB-D VSLAM algorithms. The dataset comprises RGB images, depth images, and six DoF ground-truth trajectories. The captured data features real-world indoor scenes with dynamic objects, motion blur, varying illumination, and emulated camera failures, including lens dirt, condensation, underexposure, and overexposure."}]}