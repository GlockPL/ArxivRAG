{"title": "Is Training Data Quality or Quantity More Impactful to Small Language Model Performance?", "authors": ["Aryan Sajith", "Krishna Chaitanya Rao Kathala"], "abstract": "This study investigates the relative impact of training data quality versus quantity on the performance of small language models (SLMs), utilizing the TinyStories dataset for empirical analysis. Analysis of dataset variations with respect to size (25% and 50% of the original size) and duplication (controlled rates of 25%, 50%, 75%, and 100%) were performed. Model performance was evaluated based on the validation loss, accuracy, and perplexity metrics. Results indicate training data quality plays a more significant role in the overall performance of SLMs, especially given scale of this experiment. Minimal duplication positively impacted model accuracy (+0.87% increase in accuracy at 25% duplication) without significantly increasing perplexity (+0.52% increase going from 0% to 25% duplication) but excessive duplication led to pronounced performance degradation (-40% drop in accuracy at 100% duplication). The implications of this exploration extend beyond just model performance; training large-scale models imposes significant financial and computational burdens, which can be prohibitive for organizations, individuals, and the public at large, especially in developing countries. Additionally, the energy consumption associated with large-scale training raises environmental concerns. Understanding the relative importance of data quality versus quantity could democratize AI technology, making advanced models more accessible and sustainable.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have revolutionized the field of Natural Language Processing (NLP), demonstrating remarkable capabilities across a myriad of tasks such as problem-solving, logical reasoning, and summarization by developing a statistical distribution over a corpus of text and effectively predicting subsequent tokens [9]. However, despite their successes, the internal mechanisms of LLMs remain largely opaque, often functioning as a \"black box\" where their multi-equational and complex nature makes their decision-making process hard to interpret and understand [9]. This difficulty in interpretability and understandability, along with significant bottlenecks in the scale of training data available for training LLMs, has contributed to various experiments on the comparative importance of training data quantity versus training data quality for LLM performance [1, 2, 3, 4, 5, 6, 7, 8].\nRecent research highlights the importance of various aspects of data management, including the quantity and quality of training data [1]. The potential balance between these two critical elements can significantly impact the model's performance and accessibility, making it an essential area of further consideration. Some studies suggest that high-quality data can reduce the need for extensive data quantities, potentially lowering training costs and improving model performance [2, 7]. For instance, refined datasets with stringent filtering and deduplication outperform larger, less curated corpora [6]. Conversely, other research indicates that the sheer volume of training data remains crucial, particularly for ensuring comprehensive model coverage and minimizing overfitting [8]. Further exploration to clarify the relative importance of training data quality versus training data quantity will play a crucial role in the further progression of language models and our understanding of them.\nThe implications of this exploration extend beyond simply model performance. From a socio-economic perspective, training large-scale models imposes significant financial and computational burdens, which can be prohibitive for organizations, researchers, and individuals in developing countries. The cost of training and"}, {"title": "2. Methodology", "content": "This work taps into small language model training by comparing the relative impact of data quantity versus quality for SLMs using the TinyStories dataset, which consists of more than 2 million stories. Our novel approach introduces controlled, induced duplications across different dataset sizes at various rates, providing valuable insights into the subtle effects of inducing training data repetition on model performance. Additionally, this work highlights the potential value of induced duplications as a tool for refining the training of language models. Lastly, by conducting this study on relatively accessible hardware, this work hopes to pave the path for democratized language model training in resource-constrained environments that still exhibit valuable functional competency."}, {"title": "2.1. Data Source", "content": "Based on the work from Eldan and Li, showcasing that SLMs can exhibit emergent properties present in large language models with high quality training data, we utilized their TinyStories dataset released under the CDLA-Sharing-1.0 license for this experiment [5]. All dataset variations we created for this experiment are also open-sourced, free to access and released under the CDLA-Sharing-1.0 li-cense and links to them can be found below the abstract."}, {"title": "2.2. Proposed Approach", "content": "To measure the impact of dataset size we created two size-based variations from the original TinyStories dataset with 25% and 50% the number of stories from the baseline dataset. The specific stories chosen were done so stochastically as per our prepare.py scripts to avoid positional bias by choosing stories right at the beginning, the end, and so on.\nTo measure the impact of dataset quality we created several variations at each size level by duplicating an arbitrary story 25%, 50%, 75% and 100% of the time. The stories chosen to duplicate and those chosen to be left behind were, once again, stochastically chosen to minimize bias in the selection process. All stochastic dataset variations and python scripts for non-stochastic variations can be accessed via links below the abstract."}, {"title": "2.3. Model Source and Description", "content": "The SLM in this work was trained using the publicly released model nanoGPT, which can be accessed here:\nhttps://github.com/karpathy/nanoGPT. Said model was released under a MIT license, which was forked for this experiment. The model is a character-level recurrent neural network chosen for its simplicity and ease of setup. Given that the experiment was performed to analyze the impact of training data quality versus training data quantity, said consistent model architecture was utilized throughout this work.\nThis model leverages many well-established deep learning techniques: Forward propagation of running input sequences through a neural network to calculate loss, backward propagation computation of gradients to update model weights via gradient descent to minimize loss, incrementally changing learning rate due to learning rate decay, and a"}, {"title": "2.4. Computer Specs and Setup", "content": "An M1 MacBook Pro with 16 gigabytes of RAM, 1 terabyte of storage, 10 core CPU, and 16 core GPU was used for training these models. The laptop was fully charged and plugged into a power source while training these models."}, {"title": "2.5. Experimental Algorithm", "content": "Specified steps can be found at https://github.com/Aryan-Sajith/URV-Data_Quantity_VS_Data_Quality-\nResearch/tree/main/data/tinyStories and are summarized below:\n1. Open the following directory: https://github.com/Aryan-Sajith/URV-Data_Quantity_VS_Data_Quality-\nResearch/tree/main/data/tinyStories\n2. Run the main prepare.py script from the following directory: sizes/100-percent-size to prepare the base-line training and evaluation set of data in .txt and .bin formats. The .txt files are utilized for creating further variations of the dataset using other pre-pare.py scripts whereas the .bin files are utilized for training the model.\n3. Run whatever variational prepare.py script was necessary to obtain the specific duplication and size level necessary as found in the sizes/ relative directory.\n4. Run the main re-encode.py file with the newly created dataset from the previous step to encode the .txt file into a .bin file for training the model.\n5. Modify the relevant file paths to track evaluation measures and training checkpoints. Firstly, change write_output in ../../config/train_tinyStories.py for model checkpoint(.. means to jump out one level in the file tree). Secondly, change output_file in train.py for output file on eval stats.\n6. Run the training we ran the following command: py-thon train.py config/train_tinyStories.py\n7. To retrain and re-evaluate the model we repeated from the third step."}, {"title": "3. Analysis", "content": ""}, {"title": "3.1. Model Evaluation", "content": "The three main measures of model evaluation used were loss, accuracy, and perplexity relative to the evaluation set. Loss acts as a measure of the model prediction inaccuracy relative to the true output, so lower loss is better. Accuracy acts as a measure of correctly predicted next characters in the sequence, so higher accuracy is better. Perplexity acts as a measure of how \u201csurprised\u201d the model was based on the observed true output, so lower perplexity is better. These measures were calculated within the main train.py file during the evaluation step of the procedure by outputting to a file."}, {"title": "4. Limitations and Future Directions", "content": ""}, {"title": "4.1. Model and Dataset Constraints", "content": "Firstly, experiments conducted in this study were limited to small language models (SLMs), the TinyStories dataset and a relatively low quantity of training data. For example, the largest difference between dataset size levels was observed for valuation perplexity going from the 25% size level to the 100% size level with a 26.86% improvement in model performance (can be observed in figure 4). Aside from this, no significant changes were found across varying size levels. Thus, the results may not generalize to larger language models or datasets of larger scale with varying characteristics. This limitation restricts the broader applicability of the findings to other contexts within the field of Natural Language Processing (NLP). Scaling up this study to larger datasets with larger models still within the scope of public affordability could yield novel insights."}, {"title": "4.2. Experimental Setup", "content": "Firstly, the training was conducted on a laptop, specifically an M1 MacBook Pro, with 16 GB of RAM, a 1 TB SSD, a 10-core CPU, and a 16-core GPU. While this setup offers more computational power than traditional lower-end hardware, which opens even more interesting and excellent opportunities for alternative or lower-end hardware extensions to our work, it still does not match the specialized, high-end GPUs typically used in deep learning research. These hardware limitations likely impacted both the efficiency of the training process and the final model performance. However, the successful use of an M1 MacBook Pro highlights an important consideration: Meaningful progress in language model development can be made without access to prohibitively expensive, dedicated GPUs. This finding is particularly significant in the context of accessibility. By demonstrating that lower-end computers, possibly equipped with alternative CPUs and GPUs, can still be effective for training models, we broaden the potential for participation in AI research. Individuals or institutions with limited resources can contribute to the field, promoting a more inclusive and diverse research community. Expanding the scope and access of language model usage has even wider social implications. As language models become more integral to various applications, from education to healthcare, ensuring that these technologies are available to a wider population is crucial. The ability to develop and deploy models in local languages and cultural contexts can significantly enhance the relevance and effectiveness of AI in addressing community specific challenges. For instance, in regions where linguistic diversity is high, the ability to train models on localized datasets using affordable hardware can empower communities to create tools that reflect their unique linguistic and cultural needs.\nLastly, the study employed a character-level recurrent neural network model with the configurations detailed in the methodology section. The choice of model architecture and hyperparameters played a crucial role in the final performance. Exploring different architectures or settings could yield varying results, as evidenced by the primary work from which the TinyStories dataset was created [5]. This opens up exciting possibilities for future studies using other lower-spec hardware and more accessible setups, which could provide valuable insights into the trade-offs between computational power and model performance, further contributing to this line of research."}, {"title": "4.3. Performance Metrics", "content": "Firstly, the performance metrics used were based on validation loss, accuracy, and perplexity. While these metrics are standard, they may not capture all aspects of model performance, such as robustness, interpretability, or specific application-based effectiveness. Future studies could incorporate a broader range of evaluation metrics to provide a more comprehensive assessment of model performance.\nSecondly the study introduced controlled duplications at various rates to measure their impact on model performance. While this approach provides insights into the effects of duplication, it does not account for other potential quality issues in the dataset, such as noise, errors, or inconsistencies that might arise in real-world data scenarios. Additionally, whether artificially induced duplication diminishes model performance in the same fashion that deduplication increases model performance is an interesting research direction to consider."}, {"title": "4.4. Long-Term Implications", "content": "The study focuses on immediate performance metrics without considering the long-term implications of training data quality and quantity on model maintenance, updates, and real-world deployment. Future work could explore how"}, {"title": "5. Conclusion", "content": "In summary, while this study provides valuable insights into the relative importance of training data quality versus quantity for Small Language Models (SLMs), it is constrained by several factors, including model and dataset limitations, experimental setup, and evaluation metrics. Addressing these limitations in future research could enhance the understanding and application of these findings to a broader range of models and real-world applications in the accessible computing sphere.\nEmpirical analysis, using the TinyStories dataset, indicates that training data quality plays a more important role in the performance of SLMs than training data quantity, especially at the scale of this experiment. Minimal duplication positively impacted model accuracy(+0.87% increase in accuracy at 25% duplication) without significantly increasing perplexity(+0.52% increase going from 0% to 25% duplication) but excessive duplication led to pronounced performance degradation(-40% drop in accuracy at 100% duplication). These insights are pivotal, as they suggest that strategic improvements in data quality can offset the need for extensive data quantities, thus lowering the financial and computational burdens associated with large-scale model training. This reduction in cost and compute resources is crucial for democratizing access to language models, making them more sustainable and accessible, especially in resource constrained settings and lower-income, middle-income, and underserved communities. Further research should expand the exploration of these dynamics, considering different model architectures, broader metrics, beneficial yet accessible scale, and more to understand the full implications that training data form can have on model performance and accessibility."}]}