{"title": "SPECULATIVE KNOWLEDGE DISTILLATION: BRIDGING THE TEACHER-STUDENT GAP THROUGH INTERLEAVED SAMPLING", "authors": ["Wenda Xu", "Rujun Han", "Zifeng Wang", "Long T. Le", "Dhruv Madeka", "Lei Li", "William Yang Wang", "Rishabh Agarwal", "Chen-Yu Lee", "Tomas Pfister"], "abstract": "Recent advances in knowledge distillation (KD) have enabled smaller student models to approach the performance of larger teacher models. However, popular methods such as supervised KD and on-policy KD, are adversely impacted by the knowledge gaps between teacher-student in practical scenarios. Supervised KD suffers from a distribution mismatch between training with a static dataset and inference over final student-generated outputs. Conversely, on-policy KD, which uses student-generated samples for training, can suffer from low-quality training examples with which teacher models are not familiar, resulting in inaccurate teacher feedback. To address these limitations, we introduce Speculative Knowledge Distillation (SKD), a novel approach that leverages cooperation between student and teacher models to generate high-quality training data on-the-fly while aligning with the student's inference-time distribution. In SKD, the student proposes tokens, and the teacher replaces poorly ranked ones based on its own distribution, transferring high-quality knowledge adaptively. We evaluate SKD on various text generation tasks, including translation, summarization, math, and instruction following, and show that SKD consistently outperforms existing KD methods across different domains, data sizes, and model initialization strategies.", "sections": [{"title": "1 INTRODUCTION", "content": "Text generation capabilities of large language models (LLMs) have seen continuous improvement, largely driven by scaling up the number of parameters and the amount of training data (Kaplan"}, {"title": "2 PRELIMINARIES", "content": "Language Models. An auto-regressive language model, denoted as M, predicts the probability distribution of the next token in a sequence. Conditioned on an input sequence X and the generated prefix $y_{<i} = {Y_1, Y_2, \u2026, Y_{i-1}}$ consisting of (i \u2013 1) tokens from a vocabulary V, M calculates the probability of the i-th token, $P_{y_i} = M(y_i|X,y_{<i})$. The auto-regressive language model M computes a logit l at the i-th token position. To obtain the probability $p_{y_i}$, we apply the softmax function with temperature t: $p_{y_i} = \\frac{exp(l_i/t)}{\\sum exp(l_i/t)}$. The temperature t introduces randomness into the text generation process. A higher temperature leads to more diverse sample output, while a lower temperature results in more focused and deterministic generation. In practical implementations, the vocabulary V is often truncated using top-k sampling (Radford et al., 2019), which improves text generation compared to searching over the entire vocabulary distribution. We draw inspiration from top-k sampling to develop our acceptance criteria for student-generated tokens. We include background of divergence metrics in the Appendix A.\nSpeculative Decoding (Leviathan et al., 2022; Chen et al., 2023) accelerates the decoding process by employing a smaller draft model to generate token candidates, which are then verified by larger approximation models. Our interleaved sampling is inspired by speculative decoding but introduces key differences. While speculative decoding ensures that final sampled tokens adhere to the larger model's distribution, our method directly assesses the feasibility of student-proposed tokens within the teacher's top K tokens (See Sec 3.2)."}, {"title": "3 SPECULATIVE KNOWLEDGE DISTILLATION FOR LANGUAGE MODELS", "content": "Problem Setup. Given a teacher LM $M_t$ and a student LM $M_s$, with differing model capacities, our goal is to train $M_s$ to mimic the behavior of $M_t$ on a specific task T. We assume that $M_s$ has learnable parameters $\\theta_s$, while $M_t$'s parameters $\\theta_t$ are fixed. For task T, we have a set of prompts {x} and corresponding output sequences {y}, which can be generated by either model or provided as ground truth. To measure the divergence between the token-level distributions of $M_t$ and $M_s$ for a given input-output pair (x, y), we define the following metric:\n$D(M_t||M_s)(y|x) = \\frac{1}{L_y} \\sum_{y=1}^{L_y} D(M_t(. \\mid y_{<i}, x) || M_s(. \\mid y_{<i}, x))  \\qquad(1)$\nwhere $L_y$ is the length of the output sequence y, and i indicates decoding steps. Our training objective is to minimize $D(M_t||M_s)(y|x)$ such that $M_s$ can effectively imitate $M_t$ on task T."}, {"title": "3.1 BASELINE KNOWLEDGE DISTILLATION APPROACHES", "content": "Supervised FT is a training method where a student policy is trained on a fixed dataset of input-output pairs (x, y). The objective is to minimize the negative log-likelihood of the student's pre-dictions: $L_{SFT} = -logp_s(y|x)$. It's also a common initialization strategy for more advanced knowledge distillation techniques like Supervised KD, On-Policy KD, and SKD.\nSeqKD (Kim & Rush, 2016a) is a supervised fine-tuning method that trains on sequences gener-ated by a teacher model using maximum likelihood.\nSupervised KD (Sanh et al., 2020) trains a student model to mimic the token-level probability distribution of a teacher model over a fixed ground truth dataset. This is achieved by minimizing the distance between the student's and teacher's predictions, as defined in Equation 1. The ground truth labels, denoted by y, are obtained from the fixed dataset."}, {"title": "3.2 SPECULATIVE KNOWLEDGE DISTILLATION", "content": "Speculative knowledge distillation (SKD) draws two important inspirations from imitation learning (Ross et al., 2011; Ross & Bagnell, 2010): 1) Directly taking samples from a fixed dataset or sampling from teacher can cause the state distribution to be unrealistically good. As a result, student model never learns to correct previous mistakes, leading to poor performance at test time. 2) Directly drawing samples from student can result in low quality samples that can be out-of-distribution for the teacher, resulting in inaccurate feedback when assigning logits to the student. SKD addresses above limitations by using major components from speculative sampling (Leviathan et al., 2023): 1) student model proposes on-policy sample candidate, and 2) the teacher model evaluates these proposed samples concurrently and accepts only those that it deems likely to generate. After speculative sampling, SKD calculates teacher and student's token probabilities and employ equation 1 to train the student model (L9 in Algorithm 1).\nInterleaved On-Policy Sampling Given a prompt x, student model M, will generate a sequence of tokens, ($y_i, ..., y_{i+\\gamma}$), following its own distribution $M_s(.|y_{<i},x)$ (L4 in Algorithm 1). The teacher model $M_t$ will evaluate this sequence, accepting or rejecting each token based on pre-defined criteria, which resembles speculative decoding (Leviathan et al., 2023). If a token $y_i$ is rejected, the subsequent tokens ($y_{i+1}, ...)$ will be discarded, and $y_i$ will be resampled from $M_t$'s distribution $M_t(.|y_{<i}, x)$ (L5-7 in Algorithm 1). The student model will then continue to generate tokens on-policy based on $M_s(.|y_{<i}, x, y_i)$ for the remaining $\\gamma$ tokens. This sample generation process is particularly advantageous for auto-regressive models, where early errors can propagate and affect future predictions (Ross & Bagnell, 2010). By focusing on on-policy samples that are likely to be generated by the teacher model, we can effectively utilize limited student capacity to mimic teacher.\nToken Acceptance Criteria Using the default acceptance criteria in speculative decoding corre-sponds to sampling from teacher's distribution, which would degenerate to supervised KD. Instead, we are interested in generating student samples that are likely under the teacher. To do so, we draw inspiration from top-k sampling (Radford et al., 2019) that draws samples using only the k highest probability tokens from the entire vocabulary V at each generation step. Following this intuition, we define our acceptance criteria based whether the student token yi falls into the top K tokens of teacher's distribution $M_t(.|y_{<i}, x)$. Similar to speculative decoding, the teacher Mt can evaluate sequence ($y_i, ..., y_{i+\\gamma}$) proposed by student in parallel and return to student with the token position that discarded. We replace the discarded student token by re-sampling a token from the teacher's distribution. Note that in Algorithm 1, we simplify the SKD presentation by setting $\\gamma = 1$, but in practice use $\\gamma = 5$ for efficient implementation."}, {"title": "4 EXPERIMENTAL SETUP", "content": "Student and Teacher Models. We conducted experiments using two model families, GEMMA1 (Team et al., 2024) and QWEN2 (Yang et al., 2024a). We use supervised fine-tuned (SFT) GEMMA-7B-IT (IT indicates instruction-tuned) and QWEN-7B-IT as the teacher models, while GEMMA-2B-IT and QWEN-0.5B-IT were employed as student models. We further leveraged SFTed student models to compare KDs under different model initializations.\nHyperparameters. For all fine-tuning process, we use learning rate 1e-5, warmup ratio 0.1 and 0.1 drop out rate for all fine-tuning processes. All SFT checkpoint is trained for three epoches and we select the checkpoint with the lowest validation loss. Details about SFT training dataset can be found in Section 4.1. Without parameter search, we fixed acceptance criteria K to be 25 throughout main experiment and ablation (K=25 is a common choice for top-k sampling (Radford et al., 2019). See Appendix B for detailed study). We include training details and our hyper-parameter choice of baselines and SKD in the Appendix Section C.\nBaselines. We conduct SKD over widely accepted KD approaches listed in Section 3.1: Super-vised Fine-tuning, Supervised KD, on-policy KD and ImitKD. We compared SeqKD to supervised fine-tuning (SFT) using ground truth data and found that SFT consistently outperformed SeqKD. Therefore, we did not include SeqKD results."}, {"title": "4.1 DATASET AND EVALUATION.", "content": "We evaluated SKD and baseline methods across three generation tasks: low-resource translation, dialogue summarization, and arithmetic reasoning. Task-specific KD often operates with training data around 1K samples (Quteineh et al., 2022). Thus, we randomly draw 1K samples from our task data, and further test low-data regime with 100 samples. Additionally, we performed experiments using 1K and 10K on a task-agnostic instruction-following task in the math domain. It's important to note that different baseline KD methods have access to different training data. Supervised KD, SFT, and ImitKD have access to both (x, y) pairs from the ground truth dataset, while on-policy and SKD only see the prompt x and self-generate the target label during training. Since our assumption that the teacher model should be an expert in a specific task, we used full training data for each task to supervise the FT teacher model. For SFTed student initialization, we supervise the FT student model with our constructed training set x, y (around 1000 instances). For reproducibility, all baselines and SKD use greedy decoding for evaluation. We include all results of supervised FT teacher model, supervised FT student model and instruction tuned student models in the Appendix D.\nLow Resource Translation. We utilize the Flores-200 (Team, 2022) Assamese-to-English trans-lation dataset for low-resource translation. Due to the scarcity of Assamese-to-English training data, we employ the Flores-200 development set (997 instances) as our training set. Additionally, we split the Flores-200 testing set (1012 instances) into a development set (500 instances) and a testing set (512 instances) for evaluation. We use SOTA learned metric COMET (Rei et al., 2022) to evaluate the translation quality. We use 997 instances from our constructed training set to SFTed both teacher and student models.\nDialogue Summarization. For dialogue summarization, we utilize the DialogSum dataset (Chen et al., 2021). We randomly sample 1K instances from the DialogSum training set to create our input-output pairs (x, y). For evaluation, we employ the dataset's development set (500 instances)"}, {"title": "5 RESULTS", "content": "We validate SKD's effectiveness through comprehensive studies, including task-specific (Section 5.1) and task-agnostic distillation (Section 5.2), different model initializations (Section 5.3), low-data regimes (Section 5.4), and an ablation study on two-staged training (supervised KD followed by on-policy KD) (Section 5.5)."}, {"title": "5.1 TASK-SPECIFIC DISTILLATION", "content": "In this section, we investigated task-specific distillation, where the training and inference prompt-response sets share the same format, and the task is known during training. We study two model families, supervised FT GEMMA-7B to instruction tuned GEMMA-2B (GEMMA-2B-IT) and super-vised FT QWEN2-7B to instruction tuned QWEN2-0.5B (QWEN2-0.5B-IT). As shown in Table 1, the performance of supervised KD and on-policy KD is highly task-dependent, likely due to the model's initial familiarity with each task (see Sec 5.3 for details). SKD, trained on GEMMA-2B-IT, consistently outperformed all baseline KD approaches across three in-domain text generation tasks.\nWhen initialized with QWEN2-0.5B-IT, SKD outperformed all baselines on summarization and GSM8k. We found that on-policy KD consistently underperformed supervised KD and SKD for"}, {"title": "5.2 TASK-AGNOSTIC DISTILLATION", "content": "To explore task-agnostic instruction following, where the exact nature of the task is unknown during training and can vary during deployment, we trained and evaluated SKD and baselines on a math domain instruction following task. Our training set included diverse math instruction following data, such as MathQA (Amini et al., 2019), math reasoning (Mishra et al., 2022), and tabular processing (Lu et al., 2023). We tested each KD approach's performance on four held-out testing sets, ranging from simple math word problems (ASDiv and SVAMP) to grade-level math problems (GSMplus) and competitive math problems (Math).\nAs illustrated in Figure 3, SKD initialized with GEMMA-2B-IT consistently outperformed super-vised KD and on-policy KD across these four held-out sets when trained on 1k prompts. We also calculated the performance gain of SKD over SFT and reported baseline performance gains rela-tive to SKD. Results showed that under 1k prompts, supervised KD and on-policy could approach close performance on simple tasks like SVAMP and ASDiv but still had a 20-40% performance gap compared to SKD (Second row on the left of Figure 3). The middle figure demonstrates SKD's superiority over baselines on the Math task under 7 different concepts. Since the training set was task-agnostic instruction following data, it could be trained on a larger scale in practical scenarios. On the right side of Figure 3, we show that SKD can outperform or match all baseline methods with 10k prompts. Our results demonstrate SKD's generalization capability under task-agnostic distillation. Detailed table result can also be found at Appendix D."}, {"title": "5.3 DIFFERENT MODEL INITIALIZATION", "content": "In this section, we explore how different student model initialization impact the effectiveness of various KD approaches. As shown in Figure 4, on-policy KD initialized with an instruction-tuned student model can surprisingly degrade performance over time. This raises the question of whether a"}, {"title": "5.4 LOW DATA REGIME", "content": "We further investigated SKD under an extremely low data setting with 100 data points per task (10% of the training data). As shown in Table 3, SKD outperformed supervised KD and on-policy KD for all three tasks when initialized with IT and SFTed GEMMA-2B, demonstrating SKD's superior performance in this extreme low-data setting. Notably, both supervised KD and SKD exhibited lower or comparable performance when switching from instruction-tuned to SFTed initialization. This is attributed to unavoidable overfitting during SFT with only 100 data points. In Appendix Section H, we illustrate how overfitting at the SFT stage can lead to suboptimal performance in post-training KDs. This finding suggests that end-to-end SKD, capable of training from any base model and bypassing the SFT stage, offers a more optimal solution than two-stage on-policy KD (SFT+KD) in extreme low-data scenarios."}, {"title": "5.5 TWO-STAGED TRAINING", "content": "Given that SKD transitions from supervised KD-like behavior at the beginning of training to on-policy KD towards the end, we investigated whether a simple approach of training with supervised KD for the first half of iterations and self-generated samples for the second half could achieve SKD-like performance. We conducted an ablation study by training the student model with supervised KD first, followed by on-policy KD (see Appendix Section I for details). As shown in Figure 5, SKD significantly outperforms this mixed training strategy. Surprisingly, this heuristic baseline was often outperformed by either supervised KD or on-policy KD across three tasks and two model initializations. This is likely due to the data discrepancy encountered during the training process \u2013 naively mixing samples from the teacher and student in two different stages lacks natural confidence measurement of the tokens; therefore, simply combining supervised and on-policy KD can even degrade the performance of either approach. This underscores the importance of our adaptive SKD, which implicitly covers intermediate transitions between two model behaviors."}, {"title": "5.6 SUMMARY AND ADDITIONAL RESULTS", "content": "SKD consistently outperforms baselines in both task-specific and task-agnostic distillation. Unlike on-policy methods, which are sensitive to student-generated sample quality, SKD maintains its ad-"}, {"title": "6 RELATED WORK", "content": "Knowledge Distillation (KD) (Hinton et al., 2015) is a widely adopted technique to transfer knowl-edge from a large, complex teacher model to a smaller, more efficient student model. This approach has been successfully applied to autoregressive models (Sanh et al., 2020) and language models in particular (Yang et al., 2024b). There are two primary categories of KD methods: hard label and soft label. Hard label methods train the student model to mimic the teacher's input-output pairs, similar to supervised fine-tuning (Kim & Rush, 2016a; Taori et al., 2023; Chiang et al., 2023; Xu et al., 2023). Various data augmentation techniques, such as fine-tuning on rationales (Hsieh et al., 2023) or instruction-following formats (Wang et al., 2023), have been explored within this frame-work. Soft label methods, on the other hand, compute logits for both the student and teacher models and minimize a distance metric, such as KL-divergence, between their output distributions (Hinton et al., 2015; Sanh et al., 2020). Samples for computing logits can be generated by either model or both. Our proposed SKD falls under the category of soft label KD. To the best of our knowledge, this work is the first to introduce interleaved teacher-student sampling for soft-label KD.\nSoft-label Knowledge Distillation can be broadly categorized into supervised KD and on-policy KD based on the source of training data. In supervised KD (Sanh et al., 2020), the training data is fixed, leading to a discrepancy between training and inference time for the student model (Agarwal et al., 2024). MiniLLM (Gu et al., 2024), for instance, frames this as a reinforcement learning problem, optimizing reverse KL divergence over the fixed dataset. Lin et al. (2020). explored different distance metrics, including JSD and total variation distance. On the other hand, on-policy KD, pioneered by ImitKD (Lin et al., 2020), leverages samples generated by the student model itself. This approach addresses the training-inference mismatch by directly sampling target tokens from the student's own output. Inspired by on-policy imitating learning (Ross et al., 2011), Agarwal et al. (2024) introduced on-policy KD for language models, calculating both teacher and student token probabilities to train the student on its self-generated data. DistillM (Ko et al., 2024) further enhanced this by incorporating a mix of on-policy samples and ground truth. As discussed in \u00a73.2, both supervised KD and on-policy KD are special cases of SKD. SKD outperforms both baselines and surpasses the performance from mixing ground-truth and on-policy samples.\nSpeculative Decoding (Leviathan et al., 2022; Chen et al., 2023) inspired our interleaved sampling approach in SKD. However, instead of sampling from teacher's distribution, SKD assesses the feasi-bility of student-proposed tokens within the teacher's top K tokens. Interleaved sampling allows us to obtain high quality on-policy samples. Zhou et al. (2024); Liu et al. (2024) enhanced speculative decoding alignment with various KD methods. Building on these, we benchmarked SKD's speedup against KD baselines, finding it superior in both speed and token acceptance."}, {"title": "7 CONCLUSION", "content": "We propose Speculative Knowledge Distillation (SKD), a novel method that addresses the limita-tions of existing KD approaches. SKD leverages interleaved sampling to mitigate the discrepancy between training and inference samples and remove low-quality student-generated data. Our ex-periments consistently show SKD's superiority in both task-specific and task-agnostic distillations. SKD is robust to various model families, initialization, and dataset sizes. We also demonstrate its superiority in token-level mixing over sequence-level mixing. Furthermore, SKD-trained student models can accelerate speculative decoding, enabling new applications."}, {"title": "8 REPRODUCIBILITY STATEMENT", "content": "We discussed our model implementation, hyper-parameters, baseline details in Section 4. We in-clude information about our dataset and evaluation metrics in Section 4.1. We discuss training and hyper-parameter details in Appendix C. We include examples of our prompt and response for all tasks in Appendix E. We include additional information about our evaluation metrics in Appendix F. All data, model and evaluation metrics are open sourced and available for research community. Upon the camera ready of the paper, we will include a link to a publicly accessible code reposi-tory, including all implementations about baseline KDs and SKD, running instructions and hyper-parameter settings. All results reported in this paper can be fully reproduced by the community."}, {"title": "A DIVERGENCE METRICS.", "content": "As previously discussed, for each token position, the auto-regressive language model (LM) M can generate a probability distribution, p(.|X, $y_{<k}$). Assuming the teacher and student models share the same vocabulary V, we can directly employ divergence metrics, like KL divergence, to quantify the distance between their respective probability distributions. For token position k, let P(C) and Q(C) represent the vocabulary distributions of the teacher and student, respectively. The KL divergence at this position is calculated as $D_{KL}(P||Q) = \\sum_{c \\in C} P(c) (log\\frac{P(c)}{Q(c)})$. By using this divergence metric as our objective function, we can effectively conduct knowledge distillation between the teacher and student models."}, {"title": "B CAN SKD OUTPERFORM BASELINES UNDER DIFFERENT ACCEPTANCE CRITERIA?", "content": "In this section, we tested a wide range of K values and showed SKD's performance across three text generation tasks. In Figure 6, we show that while extremely low or high K values can not yield the most optimal performance, a broad range of K values between 5 to 50 can consistently outperform supervised KD and on-policy KD across all three text generation tasks. The overall performance trend confirms our hypothesis that as K increases, SKD degenerates to on-policy KD like behavior, while decreasing K degenerates SKD to supervised KD like behavior. Consequently, neither the highest nor lowest values of K are optimal.\nWe maintained a fixed K of 25 (a common choice for top-K sampling) in our primary experiments to assess SKD's general effectiveness without extensive parameter tuning. Our results in this section suggest that further task-specific optimization of K could potentially yield even better performance."}, {"title": "C TRAINING AND HYPERPARAMETER DETAILS FOR BASELINES AND SKD", "content": "For both GEMMA-2B-IT and QWEN2-0.5B-IT models, we set the learning rate to le-5. We disabled dropout during sampling, as in direct preference learning (Rafailov et al., 2023). Maximum input and output lengths were set task-specifically: (256, 256, 1024, 1024) and (256, 512, 128, 1024) for translation, arithmetic reasoning, summarization, and math instruction, respectively. All baseline models used a batch size of 8 and a gradient accumulation step of 1. Student model is trained over 375, 375, 375 and 1225 training steps for summarization, GSM, math and translation tasks respectively. We train 7500 steps for full data training at Math. For extreme low data setting (with prompt size 100), we trained student model over 125 steps for summarization, GSM and translation tasks. We report checkpoint results with the lowest validation loss.\nInitial experiments with top-p sampling (temperature=1, top-p=1) led to significantly degraded gen-eration quality compared to greedy decoding (temperature=0). This was particularly evident in math instruction task, where accuracy halved. To balance exploration and performance, we gradually re-duced temperature and top-p until top-p sampling matched or outperformed greedy decoding. We"}, {"title": "I TRAINING DETAILS OF MIXED TRAINING OF SUPERVISED KD AND ON-POLICY KD", "content": "The parameter setting for this baseline is identical for the setup in Appendix C. We start to train student model with supervised KD samples for 188 steps (roughly 1.5 epoches), then we perform on-policy KD for another 187 steps. Total training step is 375. Initial learning rate is 1e-5 and linear decay is applied during training process."}, {"title": "J SPEEDUP TO SPECULATIVE DECODING", "content": "Motivated by (Zhou et al., 2024), we further investigate the speedup achieved by each baseline model when used as a draft model to approximate a teacher model during speculative decoding (Leviathan et al., 2022)]. As depicted in Figure 8, SKD-trained student models consistently exhibit the highest speedup and token acceptance ratio (Approximation model will accept or reject tokens depending on whether draft token is sampled under its probability distribution) across translation and summarization tasks. These models accelerate instruction-tuned GEMMA-2B by 1.21x and 1.17x in translation and summarization, respectively, while also improving token acceptance ratio by 1.71x"}]}