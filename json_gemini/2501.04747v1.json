{"title": "Discovering new robust local search algorithms with neuro-evolution", "authors": ["Mohamed Salim Amri Sakhri", "Adrien Go\u00ebffon", "Olivier Goudet", "Fr\u00e9d\u00e9ric Saubion", "Cha\u00efma\u00e2 Touhami"], "abstract": "This paper explores a novel approach aimed at overcoming existing challenges in the realm of local search algorithms. Our aim is to improve the decision process that takes place within a local search algorithm so as to make the best possible transitions in the neighborhood at each iteration. To improve this process, we propose to use a neural network that has the same input information as conventional local search algorithms. In this paper, which is an extension of the work [1] presented at EvoCOP2024, we investigate different ways of representing this information so as to make the algorithm as efficient as possible but also robust to monotonic transformations of the problem objective function. To assess the efficiency of this approach, we develop an experimental setup centered around NK landscape problems, offering the flexibility to adjust problem size and ruggedness. This approach offers a promising avenue for the emergence of new local search algorithms and the improvement of their problem-solving capabilities for black-box problems.", "sections": [{"title": "1 Introduction", "content": "Local search (LS) algorithms are commonly used to heuristically solve discrete optimization problems [2]. LS algorithms are usually composed of several components: a search space, a neighborhood relation, an evaluation function, and a selection strategy. The optimization problem instance to be solved can be fully defined by its set of feasible solutions \u2014the decision space\u2014 and an objective function that must be optimized. A classic and direct use of local search, when applicable, is to consider the decision space as the search space, the objective function as the evaluation function, and a natural neighborhood relation, defined from an elementary transformation function (move) such as bitflip for pseudo-Boolean problems, or induced by specific operators for permutation problems [3].\nStarting from an initial random solution, various components collaborate to drive the search towards optimal solutions. The effectiveness of this search process depends on the complexity of the problem, including factors such as deception and other structural characteristics [4-7]. The strategy to advance in the search involves selecting neighboring solutions based on their evaluations, using a wide variety of criteria. These criteria can range from simple ones, such as choosing neighbors with better or the best evaluations, to more intricate approaches involving stochastic methods. The strategy is often derived from metaheuristic frameworks based on local search such as tabu search [8] or iterated local search [9]. The goal is to effectively leverage the available local and partial knowledge of the landscape to identify the most promising search paths that lead to optimal solutions.\nFitness landscape analysis [10] provides the optimization and evolutionary computation community with theoretical and practical tools to examine search landscapes. It allows for the assessment of problem characteristics and the evaluation of the performance of search algorithms. In the context of combinatorial fitness landscapes, these are represented as graphs defined by a discrete search space and a neighborhood relation. A fundamental challenge lies in developing a search algorithm capable of navigating a fitness landscape to reach the highest possible fitness value.\nIn general, achieving optimal solutions through a straightforward adaptive approach is quite challenging. This difficulty arises from the complex interplay among different parts of the solution, which can lead to locally optimal solutions. These local optima cannot be escaped by intensification or exploitative move strategies. Consequently, the optimization algorithm becomes stuck in a suboptimal state. The primary concern in such optimization processes is to strike a balance between exploiting promising search areas through greedy search methods and diversifying search trajectories by temporarily exploring less promising solutions.\nTo overcome this challenge, researchers have developed many metaheuristics [11] and even hyperheuristic schemes [12] to mix different strategies. These approaches typically incorporate parameters that allow for precise adjustment of the trade-off between exploration and exploitation. Still, they might not"}, {"title": "Objective of the paper", "content": "This study explores the potential for emerging search strategies to overcome existing challenges. The objective is to change how information is leveraged while retaining simple and generic search components. Considering a basic hill climber algorithm to achieve a baseline search process for solving black-box binary problem instances, our aim is to benefit from machine learning techniques to get new local search heuristics that will be built from basic search information instead of choosing a priori a predefined move heuristic (e.g., always select the best neighbor)."}, {"title": "2 General framework", "content": null}, {"title": "2.1 Pseudo-Boolean Optimization Problems and Local Search", "content": "Let us consider a finite set $X \\subseteq \\{0,1\\}^N$ of solutions to a specific problem instance. These solutions are tuples of values that must satisfy certain constraints, which may or may not be explicitly provided. We evaluate the quality of these solutions using a pseudo-Boolean objective function $fobj : X \u2192 R$. Therefore, a problem instance can be fully characterized by the pair $(X, fobj)$. In terms of solving this problem, X is referred to as the search space.\nWhen solving an optimization problem instance with an LS algorithm, the objective is to identify a solution $x \u2208 X$ that optimizes the value of $fobj(\u0445)$. Since we are primarily concerned with maximization problems, let us note that any minimization problem can be reformulated as a maximization problem. In this context, an optimal solution, denoted as $x^* \u2208 X$, must satisfy the condition that for every $x \u2208 X, fobj(x) \u2264 fobj(x^*)$. While exhaustive search methods, or branch and bound algorithms, guarantee the computation of an optimal solution, this is not the case with LS algorithms. However, computing optimal solutions within a reasonable time is often infeasible, leading to the use of local search algorithms within a limited budget of evaluations of $fobj$ to approximate near-optimal solutions.\nLS algorithms operate within a structured search space, thanks to a fixed-sized neighborhood function denoted as $N : X \u2192 2^X$. This function assigns a set of neighboring solutions $N(x) \\subseteq X$ for each solution $x \u2208 X$. To maintain a fundamentally generic approach to LS, we assume that N is defined using basic flip functions, $flipi : X \u2192 X$, where $i \u2208 [1,N]$, and $flipi(x)$ is equal to x except for the ith element, which is changed from 0 to 1 or vice versa. In this case, $N(x) = \\{flipi(x) | i \u2208 [1, N]\\}$. Starting from an initial solution, often selected randomly and denoted as xo, LS constructs a path through the search space based on neighborhood relationships. This path is typically represented as a sequence of solutions bounded by a limit (horizon) H, denoted"}, {"title": "2.2 Local Search as an Episodic Task Process", "content": "According to previous remarks, an LS process can be modeled by a sequence of actions performed on states. Let us describe all the components of this episodic task process in the following subsections."}, {"title": "2.2.1 States", "content": "In this paper, we consider only LS processes that do not involve memory that records past decisions. In this work, we assume that a state $s \u2208 X \u00d7 2^X$ is fully described by a current solution $x \u2208 X$ and its neighborhood N(x). S corresponds to the set of states that can be reached during the search. We are in the context of episodic (discrete state) tasks with a terminal state (the end of the search, e.g. fixed by a maximal number of moves H)."}, {"title": "2.2.2 Observations", "content": "We introduce the notion of observation of a state as a function corresponding to an abstraction of the real search states in order to gather only useful information for the considered local search strategy.\nIn this work, we propose to study different observation functions that will be used by local search strategies to make decisions. Given $s\u2208 S$, with $s = (x,N(x))$, with x the current solution encountered by the local search and $N(x) = \\{flip;(x) | i \u2208 [1,N]\\}$ its one-flip neighborhood, we define four observation functions from S to $R^{Nxd}$.\n1. The first observation function abstracts the sate as the variation of the fitness function for each possible flip of the values of x.\n$01(x) = \\begin{pmatrix}\n\u03941(x) \\\\\n\u03942(x) \\\\\n:\\\\\n\u0394\u039d(\u03a7)\n\\end{pmatrix}$,\nwhere $Ai(x) = fobj(x) - fobj(flipi(x))$.\n2. The second observation function describes the state with a matrix with N rows and d = 2 columns. It gives the information of the value of the fitness"}, {"title": "2.2.3 Deterministic policy and actions", "content": "Following a reinforcement learning-based description, a local search process can be encoded by a policy $\\pi :R^{N\u00d7d} \u2192 A$ where A is a set of actions. Here we consider deterministic policies, i.e. for each $o\u2208 R^{N\u00d7d}$, there exists one and only one action $a \u2208 A$, such that $\\pi(o) = a$. Note that the policy can be parameterized by a parameter vector \u03b8. The set of all policies is $\\Pi = \\{\\pi\u03b8 |\u03b8\u2208 \\Theta\\}$ where $\\Theta$ is the parameter space.\nIn our context, we consider only actions that are bitflips flip; defined in Section 2.1. Hence, $A = \\{flip; | i \u2208 [1, N]\\}$. Note that of course, other actions can be introduced to fit specific strategies."}, {"title": "2.2.4 Transitions and trajectory", "content": "We consider a transition function $\u03b4 : X \u00d7 A \u2192 X$ such that $\u03b4(x, a) = x \u2295 a$. This function specifies the change in state of the environment in response to the chosen action. Note that the transition is used to update the current state and compute the next state of the LS process. An LS run can be fully characterized by the search trajectory that has been produced from an initial starting solution.\nGiven an instance $I = (X, fobj)$, an initial solution $xo \u2208 X$, a policy \u03c0\u03b8, and a horizon H, a trajectory $T(x\u03bf, \u03c0\u03b8, H, I)$ is a sequence $(xo, A0, X1, A1, ..., XH\u22121, \u0430\u043d-1,XH)$ such that $(x0,...,xH) \u2208 P$ is an LS path (the multiset $\\{xo,...,xH\\}$ will be denoted P(T(x\u03bf, \u03c0\u03b8, \u0397, I))), $\u2200i \u2208 [0, H \u2212 1], ai = \u03c0\u03bf(\u03bf(xi))$ and $\u2200i e [1, H], xi = (xi\u22121, ai\u22121)$. Note that the trajectory is defined with regard to solutions belonging to X, while the policy operates on the space of observations obtained from these solutions."}, {"title": "2.2.5 Reward", "content": "Compared to classic reinforcement schemes, where a reward can be assigned after each action, in our context, the reward will be computed globally for a given trajectory. Note that if we considered only the best-improvement strategy, then the reward could be assigned after each move to assess that the best move has been selected. Unfortunately, such a strategy will only be suitable for simple smooth unimodal problems. Hence we translate this reward as a function $R(x\u03bf, \u03c0\u03b8, H, I) = maxx\u2208P(T(x\u03bf,\u03c0\u03bf,H,I)) fobj(x)$.\nOur objective is to maximize the maximum score encountered by the agent during its trajectory, and not the sum of local fitness improvements obtained during its trajectory. We are therefore not in the case of learning a Markov decision process. This is why classical reinforcement learning algorithms such as Q-learning or policy gradient are not applicable in this context.\nThis justifies our choice of neuro-evolution where the policy parameters will be abstracted by a neural network that will be used to select the suitable action. The parameters of this neural network will be searched by means of an evolutionary algorithm according to a learning process defined below."}, {"title": "2.3 Policy Learning for a Set of Instances", "content": "In this paper, we focus on NK landscapes as pseudo-boolean optimization problems. The NK landscape model was introduced to describe binary fitness landscapes [26]. The characteristics of these landscapes are determined by two key parameters: N, which represents the dimension (number of variables), and K (where K < N), which indicates the average number of dependencies per variable and, in turn, influences the ruggedness of the fitness landscape. An NK problem instance is an optimization problem represented by an NK function. We use random NK functions to create optimization problem instances with adjustable search landscape characteristics. This adjustment will be achieved by varying the parameters (N, K), thus generating diverse search landscapes. Hence, we consider NK(N, K) as a distribution of instances generated by a random NK function generator.\nIn order to achieve our policy learning process, we must assess the performance of a policy as $F(\u03c0\u03bf, NK(N, K), H) = EI~NK(N,K),xo~x[R(x\u03bf, \u03c0\u03bf, \u0397, \u0399)]$ where xo ~ X stands for a uniform selection of an element in X. However, since this expectancy cannot be practically evaluated, we rely on an empirical estimator computed as an average of the score obtained by the policy \u03c0\u03b8 for a finite number q of instances I1, ..., Iq sampled from the distribution NK(N, K) and a finite number of restarts $x1),...,xr)$ drawn uniformly in X:\n$F(\u03c0\u03bf, NK(\u039d, \u039a), H) = \\frac{1}{qr} \\sum_{i=1}^{q}\\sum_{j=1}^{r}R(x^{(j)}, \u03c0\u03bf, \u0397, I\u2082).$\n(1)\nFigure 1 highlights our general learning methodology and the connections between the LS process at the instance solving level and the policy learning task that will be achieved by a neural network presented in the next section."}, {"title": "3 Deterministic local search policies for pseudo-boolean optimization", "content": "In this paper, our objective is to compare an LS policy learned by neuro-evolution, with three different baseline LS algorithms. To ensure a fair comparison among all the algorithms, all the different strategies take as input the same type of information, i.e. the fitness of the current solution x and the fitness of the solutions in its one-flip neighborhood N(x). All of these strategies are deterministic and memoryless. The set of possible actions available to the different strategies always remains $A = \\{flip; | i \u2208 [1, N]\\}$."}, {"title": "3.1 Neural network local search policy", "content": "We introduce a deterministic LS policy \u03c0\u03b8 : $R^{N\u00d7d} \u2192 A$, called Neuro-LS, which uses a neural network ge, parametrized by a vector of real numbers 0. When in a state x, this neural network takes as input an observation matrices $o(x) \u2208 R^{Nxd}$ and gives as output a vector $ge(o)$ of size N whose component $go(0)i$ corresponds to a preference score associated to each observation oi. Then, the action a\u017c corresponding to the highest score $ge(0);$ is selected.\nIn the experimental section of this paper, we will compare the impact on the results of using the different types of observation matrices as input for the neural network (see Section 2.2.2).\nA desirable property of this neural network policy is to be permutation equivariant with respect to the input vector of observations, which is a property generally entailed by a local search algorithm, in order to make its behavior consistent for solving any type of instance. Formally, an LS algorithm is said to be invariant to permutations in the observations if for any permutation \u03c3 on [1, N], we have \u03b1\u03c3(i) = \u03c0\u03b8(\u039f\u03c3(1), 00(2),\u06f0\u06f0\u06f0,00(N)). As an example, the basic hill climber HC defined with Example 1 in Section 2.2 has this property.\nIn order to obtain this property for Neuro-LS, the same regression neural network ge : Rd \u2192 1 is applied to each row $o(x);$ of the observation matrix $o(x) \u2208 R^{N\u00d7d}$. Note that this is a simplified version of architecture presented at EvoCOP2024 in [1]. In this version, each variable is processed independently,"}, {"title": "4 Computational experiments", "content": "In this section, we address four questions experimentally:\n1. impact of the different observation functions used as inputs to the neural network on the performance of the Neuro-LS strategy,\n2. performance of Neuro-LS compared to the baseline LS strategies presented in the last subsection for NK landscape problems of different sizes and ruggedness,\n3. robustness of the learned Neuro-LS strategy for other types of instances coming from a different pseudo-Boolean problem (QUBO),\n4. study of the emergent strategies discovered by Neuro-LS at the end of its evolutionary process.\nFirst, we discuss the experimental setting. Then, we follow the classical steps of a machine learning workflow: subsection 4.2 describes the Neuro-LS training process; it will allow us to select different emerging strategies for each type of NK landscape on validation sets, which we will then compare with the different baseline LS strategies on test sets coming from the same distribution of instances (subsection 4.3) or from an other distribution of instances (subsection 4.4). Finally, an in-depth analysis of the best emerging strategies discovered by Neuro-LS will be performed in subsection 4.5."}, {"title": "4.1 Experimental settings", "content": "In these experiments, we consider independent instances of NK-landscape problems. 12 different scenarios with N \u2208 {32,64,128} and K \u2208 {1,2, 4, 8} are considered. For each scenario, three different sets of instances are sampled independently from the NK(N, K) distribution described in Section 2.3: a training set, a validation set and a test set.\nFor the resolution of each instance, given a random starting point xo \u2208 X, each LS algorithm performs a trajectory of size H = 2 \u00d7 N (iterations) and returns the best solution found during this trajectory. The experiments were performed on a computer equipped with a 12th generation Intel\u00ae CoreTM i7-1265U processor and 14.8 GB of RAM.\nNeuro-LS is implemented in Python 3.7 with Pytorch 1.4 library. For all experiments with different values N and K, we use the same architecture of the neural network composed of two hidden layers of size 10 and 5, with a total of | 0 |= 81 parameters to calibrate when d = 1 (when using observation functions 01 and 03) and | 0 |= 91 parameters to calibrate when d = 2 (when using observation functions 02 and 04).\nTo optimise the weights of the neural network, we used the CMA-ES algorithm of the pycma library [33]. The multivariate normal distribution of CMA-ES is initialized with mean parameter \u03bc (randomly sampled according to a unit normal distribution) and initial standard deviation ginit = 0.2."}, {"title": "4.2 Neuro-LS training phase", "content": "For each NK-landscape configuration and for each of the four observation functions used to compute the input of the neural network (see Section 2.2.2), we run 10 different training processes of Neuro-LS with CMA-ES, and a maximum number of 100 generations of CMA-ES, to optimize the empirical score F defined by (1), computed as an average of the best fitness scores obtained for 100 trajectories (r = 10 independent random restarts for each of the q = 10 training instances).\nAt each generation, the 10 training instances are regenerated to avoid overfitting. Then, CMA-ES samples a population of 17 individuals (17 vectors of weights @ of the neural network), and the best Neuro-LS strategy of the population on the training set is evaluated on the 10 instances of the validation set (with 10 independent restarts per instance).\nThe evolution of the average score obtained on the Neuro-LS validation sets, considering as input the observation matrices 01, 02, 03 and 04 are respectively indicated with green, red, blue and yellow lines in Figure 3.\nThe light blue line is a reference score. It corresponds to the average score Fobtained by the BHC+ local search strategy (see Section 3.2) on the same validation set."}, {"title": "4.3 Test phase", "content": "In this phase, we performed a series of evaluations to assess whether the best Neuro LS strategies, selected based on the validation set for each configuration of NK landscape, continue to perform well in new test instances that are independently sampled from the same NK(N, K) distribution.\nThe strategy (1, \u03bb)-ES has a hyperparameter A that we calibrated in the range [1, N] on the training instances, for each (N, K) configuration.\nTo perform a fair comparison between the different strategies, we compute an average estimated score F on the same 100 test instances. For each instance, we use the same starting point to compute the trajectory produced by each LS strategy. \nThis score improvement compared to the other baseline methods, obtained with the same budget of H = 2\u00d7 N iterations performed on each instance, can be attributed to a more efficient exploration of the search space as K increases (as seen in Section 4.5).\nAmong the Neuro-LS strategies, we observe that the strategy using the second observation function 02 gets the worst scores on the validation sets. This is because the neural network struggles to extract relevant information using the rawest possible information about the fitness of the current solution and the fitness of its neighbors.\nThe other strategies combined with the observation functions 01, 03, 04 obtain almost the same score, when tested on instances from the same distribution as used in the training phase.\nHowever, we will see in the next section that some of these strategies are more or less robust when tested on new instances of a different problem."}, {"title": "4.4 Out-of-distribution tests", "content": "Here, our aim is to check whether the best Neuro-LS strategies can also perform well on instances of different sizes and issued from distributions of instances on which they have not been trained.\nWe select the best Neuro-LS strategies trained on the distribution of NK landscape instances with size N = 64 and K = 8, and we test them"}, {"title": "4.5 Study of Neuro-LS emerging strategies", "content": "The objective here is to analyse in detail the best Neuro-LS strategies that have emerged with neuro-evolution and to understand their decision-making processes for the different types of NK landscapes on which they have been trained (smooth or rugged) and for the different types of information given as input to the neural network."}, {"title": "4.5.1 Emerging strategies for smooth landscape", "content": "For smooth NK landscapes (when K = 1 or K = 2) and for all the observation functions used as input, Neuro-LS almost always learns to perform a best"}, {"title": "Emerging strategy based on raw information about current and neighboring fitness", "content": "Figure 9 displays a representative example of the trajectory performed by Neuro-LS using observation 02 when N = 64 and K = 8. We observe on this plot that the emerging Neuro-LS strategy has two successive operating modes like the strategy based on the variations of fitness for each possible move (see Section 4.5.2): small steps hill climbing behavior and Jump with worst move. It is interesting to see that this same behavior can also emerge when using very raw information as input.\nHowever we see in this plot that the strategy is slightly less imprecise to find the local maxima, because we see in Figure 9 that the jump with worst move can be triggered even if there are still improving moves. As a result, this strategy can sometimes miss opportunities to find a good solution, which explain why this strategy obtain less good results as reported in Table 1."}, {"title": "A Other emerging strategies", "content": "In this section we analyze the other strategies that emerged when the 02 and 04 observation functions were used. Analyses of these strategies are a little less"}]}