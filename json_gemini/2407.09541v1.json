{"title": "MATE: Meet At The Embedding - Connecting Images with Long Texts", "authors": ["Young Kyun Jang", "Junmo Kang", "Yong Jae Lee", "Donghyun Kim"], "abstract": "While advancements in Vision Language Models (VLMs) have significantly improved the alignment of visual and textual data, these models primarily focus on aligning images with short descriptive captions. This focus limits their ability to handle complex text interactions, particularly with longer texts such as lengthy captions or documents, which have not been extensively explored yet. In this paper, we introduce Meet At The Embedding (MATE), a novel approach that combines the capabilities of VLMs with Large Language Models (LLMs) to overcome this challenge without the need for additional image-long text pairs. Specifically, we replace the text encoder of the VLM with a pretrained LLM-based encoder that excels in understanding long texts. To bridge the gap between VLM and LLM, MATE incorporates a projection module that is trained in a multi-stage manner. It starts by aligning the embeddings from the VLM text encoder with those from the LLM using extensive text pairs. This module is then employed to seamlessly align image embeddings closely with LLM embeddings. We propose two new cross-modal retrieval benchmarks to assess the task of connecting images with long texts (lengthy captions / documents). Extensive experimental results demonstrate that MATE effectively connects images with long texts, uncovering diverse semantic relationships.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Vision Language Models (VLMs) such as CLIP (Radford et al., 2021) and others (Schuhmann et al., 2022; Jia et al., 2021; Li and et al., 2022) have successfully connected visual and textual data by embedding them into a shared space. These models exhibit robust generalization across various visual domains, including medical imaging, art, and remote sensing (Lin et al., 2023; Liu et al., 2023; Conde and Turgutlu, 2021; Hentschel et al., 2022; Singha et al., 2023; Li et al., 2023). The core strength of VLMs stems from leveraging extensive image-caption pairs to obtain generalized and robust representations across diverse visual domains.\nDespite their success, most text encoders in current VLMs are primarily designed for direct alignment between short captions and corresponding images. For instance, the text encoder in CLIP has a maximum context length of 77, and this limitation also applies to its longer caption-based variants (Yang et al., 2023; Fan et al., 2024; Zheng et al., 2024). As a result, these encoders struggle to fully comprehend the rich textual context of longer texts, such as captions exceeding 77 tokens or entire documents, that are related to images. Moreover, the reliance on caption-only training samples limits the ability to connect images with texts from various domains. As shown in Figure 1, there are many practical applications in associating images with various long texts which remain largely unexplored, prompting us to investigate this area further.\nIn this work, we introduce a novel method named Meet At The Embedding (MATE), which aligns embeddings to connect images and long texts. MATE leverages a Large Language Model (LLM) and VLMs without requiring additional image-long text pairs. Specifically, MATE aligns image embeddings from a VLM with text embeddings from a pretrained LLM-based encoder (Wang et al., 2023), thereby enhancing image-long text interactions. The LLM-based encoder, trained on diverse text domains, develops a robust understanding of language and advanced reasoning capabilities for handling long texts. We leverage this capability to understand long texts and produce discriminative embeddings for retrieval.\nOur MATE model consists of the LLM encoder and the VLM's image encoder, with an additional projection module that converts image embeddings into LLM-aligned embeddings. MATE progressively aligns the VLM embeddings with the LLM embeddings through a multi-stage process: text-to-LLM alignment and image-to-LLM alignment. In the text-to-LLM alignment stage, we first pre-train the projection module with large-scale captions to align the VLM text encoder with the LLM encoder. Then, we fine-tune the module using query-document pairs (Nguyen et al., 2016) that contain rich textual information, inputting queries to the VLM text encoder and documents to the LLM. In the image-to-LLM alignment stage, we adapt this text-trained module to the VLM image encoder, aligning image embeddings with LLM embeddings using a minimal set of image-caption pairs. This approach effectively connects images with long texts without requiring direct image-long text pairs.\nFurthermore, we introduce two new image-long text retrieval evaluation benchmarks: one for images paired with detailed, human-annotated lengthy captions (Onoe et al., 2024) or generative model produced lengthy captions (Zheng et al., 2024), and another for images associated with documents, using pairs sourced from Wikipedia (Chen et al., 2023b; Hu et al., 2023). The results demonstrate that our MATE method effectively links images with long texts and uncovers diverse semantic relationships. This capability enhances intuitive retrieval outcomes and advances our understanding of integrating complex textual and visual information, paving the way for diverse applications, including multi-lingual cases.\nWe summarize our contributions as:\n\u2022 To the best of our knowledge, this is the first approach that addresses cross-modal interaction at the image-long text level including documents, establishing a new research topic in the field.\n\u2022 We introduce the Meet At The Embedding (MATE) method, which efficiently aligns VLM and LLM embeddings to facilitate connections between images and long texts.\n\u2022 With our newly introduced benchmarks, we demonstrate the superior performance of the MATE method in cross-modal retrieval."}, {"title": "2 Related Work", "content": "Embedding-based Representation Learning.\nBy mapping given input samples into an embedding space, embedding-based representation learning methods have been actively explored in the fields of language (Su et al., 2023; Wang et al., 2022), vision (Qian et al., 2021; Chen et al., 2020b; Zhang et al., 2022), audio (Jansen et al., 2018) and many others. Various models have achieved significant success by incorporating diverse intra-modality samples at scale across different domains. These models facilitate single-modality and multi-domain representation learning, resulting in enhanced interactions.\nOn the other hand, VLMs (Radford et al., 2021; Schuhmann et al., 2022; Jia et al., 2021; Li and et al., 2022) have emerged as powerful tools for bridging the modality gap between visual and textual data. These models utilize dual-encoder architectures to encode images and text separately, effectively aligning them within a common embedding space that provides robust representations. However, unlike the diverse images in the VLM training sets, the text component is often limited to short descriptive captions. This limitation may restrict the depth of textual understanding and contextual richness that the models can achieve. Efforts such as (Yang et al., 2023; Fan et al., 2024; Zheng et al., 2024) have been made to mitigate this issue by rewriting captions to be lengthy and informative. Nevertheless, these methods still face limitations because they require a costly captioning process, and the resulting captions are still short, at most 77 tokens. The longer caption-version CLIP (Zhang et al., 2024) was also developed, but it is still limited to 248 tokens, which is insufficient.\nAdditionally, these models rely solely on image-caption pairs, which lack the capability to incorporate complex reasoning that can be obtained from dense text. In this work, we propose a new efficient approach that connects a powerful LLM-based encoder (Wang et al., 2023) with the VLM image encoder, not only enhancing the textual understanding capability but also enabling robust connections between long texts and images.\nVision Language Cross-Modal Retrieval. The primary application of embedding-based representation learning models is information retrieval, which leverages embeddings to assess the similarity between query and gallery samples. Effective embedding models generate discriminative embeddings by grasping the underlying semantics of data samples, thereby enhancing the accuracy of retrieval results. Many existing methods in image and text retrieval focus on short captions related to images or vice versa, or on composing image queries with brief textual modifications to retrieve related images (Chen et al., 2020a; Li et al., 2019a; Long et al., 2024; Jang and Lim, 2024). We identify a gap in cross-modal retrieval between images and long texts (lengthy captions / documents), where significant potential remains unexplored. To this end, we propose new image and document retrieval experiments involving lengthy captions (Zheng et al., 2024; Onoe et al., 2024) and Wikipedia-style documents (Chen et al., 2023b; Hu et al., 2023). These necessitate a comprehensive understanding of the long texts to accurately match related images from a large-scale database, and our MATE approach achieves the best retrieval results, demonstrating superior performance in understanding complex cross-modal interactions."}, {"title": "3 Method", "content": "In this section, we present our MATE method, which aims to establish image-long text alignment by employing a VLM image encoder and a pre-trained LLM-based encoder. It should be noted that MATE does not require additional image-long text pairs for training. The pre-trained CLIP (Schuhmann et al., 2022) and LLM-based E5 (Wang et al., 2023) are utilized as our baseline models. First, we investigate how these models are trained to distribute embeddings (in Section 3.1) to assess the feasibility of connecting these models. Next, we outline the multi-stage training strategy (in Section 3.2) that efficiently achieves our goal."}, {"title": "3.1 Preliminary", "content": "Renowned by CLIP, VLM models are trained using a large dataset $D_v = \\{(x_n, t_n)\\}_{n=1}^{N_B}$ consisting of pairs of images $(x_n)$ and their corresponding captions $(t_n)$. These models utilize an image encoder $E_I$ and a text encoder $E_T$, which generate the image embedding $v \\in \\mathbb{R}^{k_a}: v = E_I(x)$ and the text embedding $w \\in \\mathbb{R}^{k_a}: w = E_T(t)$, both in the same dimension $k_a$. All embeddings are typically $l_2$-normalized to compute cosine similarity easily.\nThen, the InfoNCE loss (also known as a contrastive loss) (Oord et al., 2018) is utilized to update trainable parameters of both modality encoders as:\n$L_{VLM} = L_{nce}(V, W) + L_{nce}(W, v) \\qquad(1)$\nwhere $L_{nce}$ is computed with the given embedding vectors $x$ and $y$ as:\n$L_{nce} = \\sum_{i=1}^{N_B} \\log \\frac{\\exp{(x \\cdot y_i/\\tau)}}{\\sum_{j=1}^{N_B} \\exp{(x \\cdot y_j/\\tau)}} \\qquad(2)$\nfor $N_B$ number of image-text pairs with temperature $\\tau$. This training objective results in an image and its corresponding caption being aligned, while those that are not paired are distanced.\nSimilarly, the LLM-based encoder $E_5$ is also updated using a contrastive approach. Unlike VLM, it utilizes a query $(q_n)$-document $(d_n)$ paired text-only dataset $D_l = \\{(q_n, d_n)\\}_{n=1}^{N_B}$, where the query represents relatively shorter text compared to the document. The query embedding $q \\in \\mathbb{R}^{k_b}: q = E_5(q)$ and the document embedding $d \\in \\mathbb{R}^{k_b}: d = E_5(d)$ are obtained with $E_5$ as $k_b$-dimensional, $l_2$-normalized vectors.\nThe training loss for the LLM encoder is applied as:\n$L_{LLM} = L_{nce}(q, d) \\qquad(3)$\nwhich leads to embeddings of the query and its corresponding document to be closely aligned, while non-paired instances become distant. Note that both VLM and LLM embedding spaces are developed in a contrastive manner, and are presumed to share some common representations."}, {"title": "3.2 Multi-stage Alignment", "content": "When building a connection between the VLM image encoder and the LLM encoder, we could consider utilizing image-long text pairs for training. However, these pairs are scarce due to the complexity of labeling, as defining what constitutes relevant pairs is challenging. Thus, our idea is to train indirectly using existing datasets of image-caption pairs and query-document pairs in a multi-stage manner. This multi-stage approach is beneficial as it allows for incremental learning, where each stage builds upon the knowledge acquired in the previous one, transitioning from query-document (short text-long text) to image-caption. As a result, MATE can perform image-long text retrieval without directly relying on image-long text pairs. We achieve this by first aligning the text encoder of the VLM with the LLM (Section 3.2.1), and then connecting the image encoder of the VLM with the LLM (Section 3.2.2), as shown in Figure 2.\nHere, we employ an additional projection module $\\phi$, due to the differences in dimensionality and representation between VLM and LLM embeddings. This module consists of a few linear layers that project VLM embeddings into the LLM embedding space. Specifically, $\\phi$ takes VLM embeddings as inputs and produces either $u$ or $\\bar{u}$, where $u = \\phi(v)$ and $\\bar{u} = \\phi(w)$. Both $u$ and $\\bar{u}$ are embedding vectors with the same $k_a$-dimensionality as the LLM embeddings $d$."}, {"title": "3.2.1 Text-to-LLM Alignment", "content": "First, we pre-train the module $\\phi$ by utilizing the VLM text encoder $E_T$ and the LLM encoder $E_5$ with a large-scale text-only dataset of captions (t), to reduce the gap between embeddings of VLM and LLM. We train $\\phi$ to align $\\bar{u}$, where $\\bar{u} = \\phi(w)$ and $w = E_T(t)$, with $d$, where $d = E_5(t)$, in a contrastive manner using Equation 3.\nThen, we fine-tune $\\phi$ with a text dataset configured with query-document pairs to provide further context of long texts. This process helps $\\phi$ to better understand and align the nuances between related texts, enhancing its ability to accurately match VLM embeddings with the most relevant documents. Similar to the pre-training stage, we utilize $E_T$ and $E_5$ with the query-document pairs (q, d) to train $\\phi$ to align $\\bar{u}$ and $d$ with Equation 3. We utilize the same number of caption pairs as query-document pairs in a training batch to ensure that $\\phi$ remains robust across diverse captions.\nThroughout these processes, we freeze the parameters of $E_5$ and $E_T$ to preserve the original generalized representation of LLM embeddings and ensure smooth integration with the corresponding VLM image encoder $E_I$ in the subsequent stage."}, {"title": "3.2.2 Image-to-LLM Alignment", "content": "With $\\phi$ trained on text-only data in the previous stage, we initialize the parameters of the same architecture $\\phi$ in this stage to transfer dense textual knowledge. Additionally, we apply LoRA (Hu et al., 2021) parameters to both $\\phi$ and $E_I$ to keep the original parameters and train the entire model efficiently. LoRA facilitates fine-tuning by introducing trainable low-rank matrices that adapt the original weights of the model without directly modifying them. This approach helps preserve the original model's capabilities, allowing $\\phi$ to retain its understanding of query-document relationships.\nGiven a minimal set of image-caption pairs (x, t), we aim to robustly connect image embeddings to LLM embeddings. Specifically, we seek to align $u$, where $u = \\phi(v)$ and $v = E_I(x)$, with $d$,"}, {"title": "4 Experiments", "content": "4.1 Setup\nDatasets. For MATE model training, we utilize the datasets as: text-only datasets for Section 3.2.1 include a standard subset of image-caption pairs from the BLIP (Li and et al., 2022) pre-training stage, specifically 16M out of a total of 115M, where only the captions are used for pre-training. We use the 532K query-document pairs from MSMARCO (Nguyen et al., 2016) passage retrieval dataset for fine-tuning. For Section 3.2.2, we use the 585K image-caption pairs from LLaVA-alignment (Liu et al., 2024), which is collected from the CC3M dataset.\nTo evaluate MATE and other models for the new image-long text cross-modal retrieval tasks, we re-configure existing image-lengthy caption paired datasets: DOCCI (Onoe et al., 2024) and CC3M-long (Zheng et al., 2024), and Wikipedia-based image-document paired datasets: Infoseek (Chen et al., 2023b) and Oven (Hu et al., 2023).\nSpecifically, DOCCI contains about 1.5K high-resolution images accompanied by human-annotated, detailed descriptive captions. DOCCI is divided into a training set of 9.6K pairs and a test set of 5.1K pairs. We use the test set for image-lengthy caption retrieval experiments. CC3M-long features images and model-generated lengthy captions from three different large multi-modal models (Liu et al., 2024; Chen et al., 2023a; Dai et al., 2024). We use 5K pairs of the Share-GPT4V-generated version for evaluation, ensuring no images overlap with the LLaVA-alignment dataset.\nFor image-document retrieval tests, we adopt Infoseek (Chen et al., 2023b) and Oven (Hu et al., 2023) datasets provided by (Wei et al., 2023). Both datasets include triplets of images, query text, and document passages. We merge the passages to reconstruct the original lengthy documents. As a result, the Infoseek dataset comprises 1.8K documents with 9.6K related images, averaging 5.3 paired images per document. The Oven dataset includes 3.5K documents with 37.6K related images, averaging 10.7 paired images per document.\nTo further investigate whether the length of text in each dataset is sufficient to be defined as long texts, we report token count statistics using the tokenizers from CLIP (Radford et al., 2021) and Mistral (Jiang et al., 2023) in Table 1. The average token counts across all datasets exceed the CLIP text encoder's maximum capacity of 77 tokens.\nEvaluation Metrics. Following standards in retrieval evaluation (Radford et al., 2021; Li et al., 2019a; Jang and Lim, 2024), we report image-lengthy caption retrieval results using recall scores at top K (R@K) and employ mean Average Precision (mAP@K) for image-document retrieval to better assess multi-positive connections.\nImplementation Details. In this paper, we employ the baseline VLM with CLIP-ViT-G/14 (Cherti et al., 2023), which utilizes Transformer-based image and text encoders. For the LLM-based encoder, we use the instruction-tuned Mistral 7B (Jiang et al., 2023) and the fine-tuned E5 (Wang et al., 2023) model as a baseline with the final embedding dimension of $k_b = 4,096$. The projection module $\\phi$ comprises three linear layers, each followed by layer normalization and GELU (Hendrycks and Gimpel, 2016) activation. The intermediate hidden dimension of the linear layers is set to four times the dimensionality of the output embedding. We employ additional LoRA (Hu et al., 2021) parameters for the image encoder and $\\phi$ in Section 3.2.2, configured as follows: LoRA = 16, rank = 16, and dropout = 0.1.\nFor training, we use 8 A100-80GB GPUs for training and evaluation. The AdamW optimizer (Loshchilov and Hutter, 2017) is employed with"}, {"title": "4.2 Results on Image-Lengthy Caption", "content": "DOCCI-test. The image-lengthy caption retrieval results on the DOCCI test set are reported in Table 2. We categorize the methods into two groups: zero-shot, which includes the original VLM models and our MATE model, and the fine-tuned version, which is trained on the DOCCI training set images and captions. In the zero-shot scenario, CLIP shows the lowest performance due to its training on shorter captions of less than 77 tokens, while the average token count in the DOCCI dataset is significantly higher. ALIGN achieves better scores than Long-CLIP and BLIP primarily due to its ability to process larger images of width and height of 289 compared to 224 of others, and the fact that the images in the DOCCI dataset are mostly of much higher resolution. Despite using the same CLIP image encoder, our MATE model achieves significantly better retrieval results by successfully leveraging the LLM encoder.\nIn terms of the fine-tuned case, we train the models using the fine-tuning setup for retrieval proposed in BLIP (Li and et al., 2022). We fine-tune ALIGN with images of width and height of 289 due to its architectural constraints, and utilize larger scale images, 336 or 448, to fine-tune BLIP and MATE to determine whether the models can be improved with more visual information. We observe that all models show improved retrieval scores, with BLIP outperforming ALIGN by processing larger images. Notably, MATE demonstrates a significant performance gain and achieves the best results when the largest images are used. This demonstrates that MATE is effective at leveraging increased visual details for enhanced performance.\nCC3M-long. The experimental results on CC3M-long test set with model-generated captions are presented in Table 2. Similar to the observations in human-annotated captions, our MATE achieves the best retrieval performance. Compared to CLIP, MATE shows an impressive average improvement of approximately 60.8 pp across all recall metrics. When compared to the second-best performing model, ALIGN, MATE still exhibits a notable average improvement of around 3.11 pp although MATE uses smaller scale images. These results highlight MATE's robustness and accuracy in capturing exact matches from cross-modal samples, which is crucial as the reliance on generative models grows and the need for effective evaluation mechanisms becomes more pronounced."}, {"title": "4.3 Results on Image-Document", "content": "Infoseek. The image-document retrieval results on the Infoseek dataset, as detailed in Table 3, highlight the outstanding performance of the MATE model in both retrieval scenarios. MATE significantly outperforms other models, achieving an average improvement of approximately 17 pp and 23.6 pp over CLIP, and 6.36 pp and 7.47 pp over Long-CLIP, across all evaluated metrics, respectively. This is particularly notable in the challenging environment of matching documents to images and vice versa, where MATE leads with the highest mAP scores across all evaluated metrics. This underscores MATE's advanced effectiveness in navigating and extracting relevant information across different media types, setting a new benchmark for accuracy in cross-modal retrieval tasks.\nOven. More challenging experiments conducted on the Oven dataset, which contains a far more extensive collection of images and documents, are shown in Table 3. The results demonstrate the superior performance of MATE across all metrics compared to other methods. Specifically, MATE significantly outperforms other models, achieving an average improvement of approximately 12.49 pp and 21.97 pp over CLIP, and 5.57 pp and 8.08 pp over ALIGN, across all evaluated metrics, respectively. This highlights MATE's robustness and effectiveness in handling complex cross-modal image-to-document retrieval tasks involving diverse and large-scale gallery samples."}, {"title": "4.4 Further Analysis", "content": "Investigation on Choice of Image Encoder. We measure the alignment between three CLIP variants, as detailed in Table 4, and the LLM using the metrics proposed in (Huh et al., 2024), to determine which one is the most feasible for connection. The scores are reported in Figure 3 using the image-short caption pairs from the COCO test set (Lin et al., 2014) and the image-lengthy caption pairs from the DOCCI test set. Three key observations emerge from the results. First, larger encoder sizes yield higher alignment scores. Second, lengthy captions result in higher scores. Lastly, and most interestingly, the alignment score of the VLM image to LLM generally exceeds that of the VLM image to VLM text and it is dominant for lengthy captions (DOCCI). Based on these findings, we hypothesize that the LLM encoder shares more common representations with the larger VLM image encoder. Consequently, we select the ViT-G image encoder as our baseline for image-long text connection.\nAblation Study. To validate the proposed schemes of MATE, we perform an ablation study as shown in Table 5. We experiment with configurations (a, b, c) to evaluate the impact of the multi-stage training strategy. For (a), we directly connect the VLM image encoder with the LLM encoder without utilizing $\\phi$. For (b) and (c), we either remove the pretraining with large-scale captions or omit the fine-tuning with query-document pairs, respectively. The results confirm that combining all training procedures significantly contributes to performance gains. In experiments (d, e), we test different image encoders and find that the choice of ViT-G achieves the best performance. In (f), we increase the number of image-caption pairs utilized in Section 3.2.2 from 0.58M to 3M and observe that the performance is either saturated or slightly degraded, indicating that MATE does not require an excessive number of image-caption pairs to achieve optimal performance. Overall, the optimal performance is achieved when all proposed components are integrated.\nMultilingual Capability. We test MATE's cross-modal retrieval with Chinese captions and images from the CN-COCO dataset (Li et al., 2019b), which includes 4.5K pairs. Despite not being trained on image-Chinese caption pairs, MATE shows decent performance and closely matches to Chinese caption-based CN-CLIP (Yang et al., 2022), while other image-English caption-based methods do not perform as well, as shown in Table 6. This success can be attributed to the multilingual capabilities of the LLM encoder, enabling MATE to effectively retrieve relevant content across different languages without specific training, thus highlighting its broad applicability."}, {"title": "5 Conclusion", "content": "In this paper, we introduce MATE, a novel method that effectively bridges the gap between images and extensive texts without paired data. MATE integrates a pretrained LLM-based text encoder with a VLM-based image encoder to efficiently align image embeddings with text embeddings. The process begins by aligning VLM text embeddings with LLM embeddings using extensive text pairs, followed by aligning image embeddings with these LLM embeddings. We also introduce new benchmarks to test image-long text retrieval tasks, demonstrating that MATE effectively connects images with extensive texts. This work pioneers a new direction for research in cross-modal interactions."}, {"title": "Limitations", "content": "The proposed MATE approach, while innovative in bridging VLMs with LLMs to handle complex text-image interactions, presents certain limitations that warrant further exploration. Primarily, the reliance on a projection module to align embeddings from different models introduces potential challenges in maintaining semantic consistency across modalities, especially when scaling to diverse and extensive datasets. Additionally, the effectiveness of MATE in real-world scenarios where data may not be as cleanly labeled or structured as the datasets used in training remains to be thoroughly evaluated. On the broader impact front, MATE has the potential to significantly enhance the accessibility and interpretability of visual content across various domains, by enabling more nuanced and context-aware image-text associations."}]}