{"title": "Boosting Code-Switching ASR with Mixture of Experts Enhanced Speech-Conditioned LLM", "authors": ["Fengrun Zhang", "Wang Geng", "Hukai Huang", "Yahui Shan", "Cheng Yi", "He Qu"], "abstract": "In this paper, we introduce a speech-conditioned Large Language Model (LLM) integrated with a Mixture of Experts (MoE) based connector to address the challenge of Code-Switching (CS) in Automatic Speech Recognition (ASR). Specifically, we propose an Insertion and Deletion of Interruption Token (IDIT) mechanism for better transfer text generation ability of LLM to speech recognition task. We also present a connecter with MoE architecture that manages multiple languages efficiently. To further enhance the collaboration of multiple experts and leverage the understanding capabilities of LLM, we propose a two-stage progressive training strategy: 1) The connector is unfrozen and trained with language-specialized experts to map speech representations to the text space. 2) The connector and LLM LORA adaptor are trained with the proposed IDIT mechanism and all experts are activated to learn general representations. Experimental results demonstrate that our method significantly outperforms state-of-the-art models, including end-to-end and large-scale audio-language models.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently advancements in Speech Conditioned Large Language Models (SC-LLM) have marked a significant milestone for Automatic Speech Recognition (ASR) [1]\u2013[4]. Conditioned on speech representation and instruction, Typical SC-LLMS autoregressively generate speech transcriptions as the answer. Previous end-to-end models [5]-[7] are limited by the scarcity of labeled speech data due to the from-scratch training manner. Pre-trained on massive text data, LLMs process powerful capabilities for natural language processing (NLP). Benefiting from LLMs' ability to understand and generate human-like text, SC-LLM emerges as a promising framework for next-generation ASR, which may be a potential breakthrough for some challenges, such as code-switching (CS), multilingual and contextual biasing [8] ASR.\nCS scenarios refer to situations where a speaker switches between two or more languages within a single sentence. Although recent years have seen more and more effort dedicated to addressing this issue, nowadays CS scenarios remain challenging due to the following difficulties:\n(1) Phonemic confusion. A speaker might switch between languages that share acoustic similities, which makes it challenging to correctly detect the language and recognize the words. Mixture of Experts (MoE) based models, as referenced in [9]\u2013[11], have been proposed to alleviate this issue.\n(2) Insufficient data. Related studies were limited by insufficient data, particularly in scenarios where specific languages are interspersed, such as when Mandarin is mixed with English.\n(3) Model design omits CS scenarios. Nowadays, large-scale speech models, such as Qwen-Audio [12] and Whisper [13], due to the label design of language identification, also perform poorly in CS scenarios.\nSince LLMs are pre-trained on multilingual text, including a substantial amount of code-switched text, they inherently possess the capability to understand and generate text that covers multiple languages. We consider this characteristic beneficial for further mitigating the aforementioned issues.\nWhile previous works have demonstrated the powerful potential of SC-LLM in general ASR tasks [3], [4], their performance on CS scenarios remains largely uncharted. In this paper, we pioneer the exploration of their potential in Madarin-English CS scenarios. Inspired by the success of the MoE architecture in E2E models, we propose an SC-LLM integrated with a MoE-based connecter to enhance the model's capabilities. We also propose the Insertion and Deletion of Interruption Token (IDIT) mechanism, which better transfers the LLMs' text generation capability to ASR via constraining on predicted tokens. Furthermore, we introduce a two-stage progressive training strategy: an alignment stage to map the speech representation to the text space with language-specialized experts (LSE); and a finetuning stage to enhance the collaboration among experts and achieve higher accuracy with IDIT. Our contributions are summarized as follows:\n\u2022\tWe introduce an MoE-integrated SC-LLM to achieve high recognition accuracy in code-switching scenarios.\n\u2022\tWe propose an IDIT mechanism to effectively transfer the LLMs' text understanding ability to speech recognition.\n\u2022\tWe propose a two-stage progressive training strategy to facilitate better modules collaboration. Our ablation study has demonstrated that this method enhances the model's capabilities in code-switching scenarios.\n\u2022\tOur method significantly surpasses other models with a relative improvement of over 10% on ASRU-2019 Mandarin-English code-switching dataset, which demonstrates the potential of SC-LLM."}, {"title": "II. PROPOSED METHOD", "content": "An overview of our proposed model is shown in Fig. 1. In our framework, speech encoders [13]\u2013[15] are utilized to extract robust speech representations. Subsequently, the connector is designed to map downsampled speech representations into the text space, thereby aligning the different modalities and facilitating integration. Then the speech representation and instruction are concatenated to form a prompt and fed into a transformer-based LLM [16]. The corresponding transcription is first converted into token IDs using the proposed IDIT mechanism and the tokenizer of LLM. Finally, these token IDs are fed into LLM as labels and generated via the next-token prediction process. With the learnable parameters from the connector and the Low-Rank Adaptation (LoRA) [17] adaptor, the framework is trained to predict tokens autoregressively."}, {"title": "B. Insertion and Deletion of Interruption Token", "content": "Subword tokenization has been widely adopted in contemporary LLMs, including techniques such as WordPiece, Unigram and Byte-Pair Encoding (BPE) tokenization [18]. These tokenizers aim to achieve better text compression, thereby improving the efficiency of both training and inference processes. Moreover, tokenization ensures the preservation of semantic integrity. The vocabulary size of these tokenizers is typically substantial, for example, Qwen2-7B [19] has a vocabulary with 151,643 tokens.\nBy experience, we have identified several key characteristics of these tokenizers: 1) due to their large vocabulary size, frequently used words are encoded into a single tokens exemplified by the Chinese word 'nihao'. 2) they differentiate in encoding English words based on case sensitivity. Since words with uppercase letters occur less frequently in the English corpus, they are typically split into word pieces and encoded as multiple tokens. For instance, while Qwen2-7B [19] tokenizer encodes 'speech' as a single token, it encodes 'SPEECH' into three separate tokens: 'S', 'PE', and 'ECH\u2019.\nWe consider the aforementioned characteristics to be inconsistent with speech recognition. In human speech perception, it is typical to segment Chinese at the character level and English at the word level. This distinction is also incorporated with the CS-ASR evaluation metrics, where Character Error Rate (CER) is used for Chinese and Word Error Rate (WER) for English. Therefore we believe that improving the tokenization process will better align the tokenizer with the speech recognition task.\nHence, we introduce the IDIT mechanism, which tokenizes Chinese at the character level and English at the word level. The details of this approach are elaborated in the algorithm 1."}, {"title": "C. MoE-based Connector", "content": "An MoE-based connector is proposed to map the speech representations to multilingual embeddings, specifically tailored for code-switching scenarios. The connector is based on [20], which comprises a weight-shared router and multiple experts. The input is fed into the router, which predicts the probability of assigning each frame to a specific expert. Subsequently, the outputs from the activated experts are combined using the weighted probabilities. This process can be formulated as follows:\n\\(P(x_i) = softmax(Wx_i + b)\\)\n\\(h_i = \\sum_{j=1}^{n} P(x_i)_j e_j(x_i)\\)\nwhere \\(x_i \\in R^{d \\times 1}\\) is ith frame from speech representation \\(X \\in R^{d \\times t}\\). The parameters \\(W \\in R^{n \\times d}\\) and \\(b \\in R^{n \\times 1}\\) project the information from speech representation into the probability of selected experts 61:n.\nMoreover, We design a two-step method to enhance the code-switching representations. The two steps are illustrated in Fig. 2 and detailed as follows. Specifically, We categorize parallel experts as language-specific experts to make better use of the monolingual and CS data in the training set. Experts are categorized into language-specialized \\(e_{ch}\\) for Mandarin and \\(e_{en}\\) for English.\nIn step 1, the process for the monolingual input can be formulated as follows:\n\\(h_i = \\begin{cases} e_{ch}(x_i), x_i \\in Chinese \\\\ e_{en}(x_i), x_i \\in English \\end{cases}\\)\nMeanwhile, frames from CS samples are routed into a selected expert with the maximum probability and concatenated along the original temporal dimension as the output \\(H_{cs}\\).\nIn step 2, all experts are activated and the output is obtained via Equation (1) and (2). While strengthened in their respective languages, the experts then focus on learning generalized representations across multilingual inputs, thereby enhancing collaboration and generalization."}, {"title": "D. Two-stage Progressive Training Strategy", "content": "To harness the potential of multiple experts in CS scenarios, we propose a two-stage progressive training strategy to enhance collaboration and generalization among the experts. The first stage maps speech representation to text representation with language-specialized experts. The second stage integrates the IDIT mechanism and LoRA adapter to better align the output to speech recognition.\nIn the first stage, we aim to refine each expert's proficiency by LSE with step 1 in Fig. 2. Additionally, we do not use the IDIT mechanism to encode transcriptions at this stage. This means that the tokenizer encodes transcriptions consistent with the massive pre-trained data. This process aims to ensure that the connector is better trained with the semantic understanding provided by LLM.\nIn the second stage, the connector is trained with step 2 in Fig. 2 to learn generalized representations. Besides, we apply the IDIT mechanism to constrain the predicted tokens to word level for English or character level for Chinese. To bridge the gap between the LLM's original text generation capabilities and the predicted granularity better suited for speech recognition, we employ LoRA fine-tuning further to adapt the LLM to predict speech transcriptions. Finally, the predicted granularity is clarified via the IDIT mechanism to further enhance the capabilities."}, {"title": "III. EXPERIMENTAL SETUPS", "content": "We conduct experiments on the ASRU-2019 Mandarin-English code-switching dataset [21], which comprises approximately 200 hours of code-switching training data and an additional 500 hours of Mandarin monolingual training data. Consistent with previous work [10], [11], we further add 460 hours of English monolingual data from Librispeech. The test set comprises 20 hours of code-switching data, containing approximately 16,000 utterances."}, {"title": "B. Configuration", "content": "All experiments in our work adhere to the following configurations unless otherwise specified. We adopt Whisper-large V3\u00b9 as the speech encoder and Qwen2-7B2 as the LLM. The connector consists of two experts, and the output dimension of the router is equal to the number of experts. Each expert is a Feedforward Neural Network (FFN) with an intermediate dimension configured to 2048. The downsampling module operates through a factor of 5 via frame splicing. For the LORA configuration, we set a = 32 and r = 8. We set the batch size of 6 with a gradient accumulation of 3, resulting in a final batch size of 18. We use AdamW optimizer with the following settings: \u03b2 = (0.9, 0.999), lr = 5e \u2013 5, and zero weight decay. Additionally, We employ the warmup schedule for the first 1000 training steps. Our models are trained on eight A800 GPUs. For the proposed two stages, each stage is trained for 1 epoch, encompassing approximately 25,000 steps. Our experiments are primarily conducted on SLAM-LLM toolkit\u00b3."}, {"title": "Ablation Study on Training Strategies", "content": "We evaluate the effectiveness of our proposed method by showcasing various strategies in Table III. As strategy F shows the proposed two-stage progressive training strategy achieves the best performance, we believe that employing LSE to initialize the connector and employing IDIT for the final predicted granularity is more effective. As shown in Table III, enabling only IDIT (strategy B) or LSE (strategy C) throughout the entire process is not optimal, which results in performance degradation compared with strategy a. Instead, enabling only LSE in stagel (strategy D) or IDIT in stage2 (strategy E) obtains higher performance compared with strategy A. Moreover, strategy F leads to the best performance, suggesting that the initial application of LSE to enhance the connector's ability, followed by the subsequent use of the IDIT mechanism to predict speech-level tokens, is highly effective within the training process."}, {"title": "D. Generalizability", "content": "To assess the generalizability of our proposed method, we conduct experiments by replacing the speech encoder with Hubert or LLM with Baichuan2-7B5. As shown in Table. IV, our method delivers competitive results when integrated with various encoders or LLMs."}, {"title": "V. CONCLUSION", "content": "In this paper, we introduce a speech-conditioned LLM enhanced by MoE to address the challenge of Code-Switching Speech Recognition. The innovation of our proposed method lies in the design of the interruption token and the two-stage progressive training with a MoE-based connector. The interruption token forces the LLMs' tokenizer to encode the transcriptions with word level for English and character level for Chinese, thereby boosting the recognition accuracy. Additionally, the code-switching ability is further developed by the MoE-based connector with language-specified experts. Experimental results on Mandarin-English code-switching datasets demonstrate superior performance over existing models."}]}