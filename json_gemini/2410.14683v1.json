{"title": "Brain-Aware Readout Layers in GNNs: Advancing Alzheimer's early Detection and Neuroimaging", "authors": ["Jiwon Youn", "Dong Woo Kang", "Hyun Kook Lim", "Mansu Kim"], "abstract": "Alzheimer's disease (AD) is a neurodegenerative disorder characterized by progressive memory and cognitive decline, affecting millions worldwide. Diagnosing AD is challenging due to its heterogeneous nature and variable progression. This study introduces a novel brain-aware readout layer (BA readout layer) for Graph Neural Networks (GNNs), designed to improve interpretability and predictive accuracy in neuroimaging for early AD diagnosis. By clustering brain regions based on functional connectivity and node embedding, this layer improves the GNN's capability to capture complex brain network characteristics. We analyzed neuroimaging data from 383 participants, including both cognitively normal and preclinical AD individuals, using T1-weighted MRI, resting-state fMRI, and FBB-PET to construct brain graphs. Our results show that GNNs with the BA readout layer significantly outperform traditional models in predicting the Preclinical Alzheimer's Cognitive Composite (PACC) score, demonstrating higher robustness and stability. The adaptive BA readout layer also offers enhanced interpretability by highlighting task-specific brain regions critical to cognitive functions impacted by AD. These findings suggest that our approach provides a valuable tool for the early diagnosis and analysis of Alzheimer's disease.", "sections": [{"title": "1 Introduction", "content": "Alzheimer's disease (AD) is characterized by progressive impairment of memory and cognitive functions, affecting approximately 47 million people worldwide. A major challenge in diagnosing AD is its heterogeneity among individuals, as it typically manifests as a decline in cognitive abilities. Although the onset of AD correlates with the accumulation of amyloid proteins beyond a certain threshold,"}, {"title": "2 Related Works", "content": "GNNs are a powerful framework for learning from graph-structured data. Unlike traditional neural networks that excel at processing grid-like data such as images or sequences, GNNs are designed to handle data represented in the form of graphs. In a graph G = (V, E), the set V represents nodes (or vertices) and E represents edges (or relationships) between the nodes.\nThe fundamental concept of GNN is the iterative updating of node representations by aggregating information from neighboring nodes in the graph using an aggregation layer. This process enables GNN to capture complex relational dependencies and structural properties inherent in the data. Finally, the readout layer has been used to aggregate node representations to produce a single representation for the entire graph."}, {"title": "2.1 Aggregation Layer", "content": "Graph Convolutional Networks(GCN). GCN is an aggregation layer based on convolution operation for graphs by using a localized first-order approximation of spectral graph convolutions [18]. The fundamental concept of the GCN is a layer-wise propagation rule, defined as:\nH(1+1) = 0 (\u010e\u2212\u00bd\u00c3\u010e\u2212H(1) W())\nwhere A = A + In is the adjacency matrix with added self-loops, D is the degree matrix of \u00c3, H(1) is the matrix of activation in the l-th layer, W(1) is a layer-specific weight matrix, and o is an activation function like ReLU. The final layer uses a softmax activation for classification:\nZ = softmax (\u00c4ReLU (\u0102XW(0)) W(1))\nThis method efficiently integrates both node features and graph structure, and the adjacency matrix is normalized to prevent numerical instabilities and ensure effective gradient flow during training.\nGraphSAGE. GraphSAGE is also an aggregation layer based on an inductive framework for generating node embeddings in large graphs [19]. GraphSAGE learns embeddings by sampling and aggregating features from a node's local neighborhood, allowing it to generalize to previously unseen nodes. The three steps of GraphSAGE are described as follows: 1) sampling a fixed-size set of neighbors, 2) aggregating feature information using various functions (mean, LSTM, pooling), and 3) updating the node's representation. Here are the typical aggregation functions used in GraphSAGE:\n1. Mean Aggregator: This function computes the mean of the feature vectors of the sampled neighbors.\nhk\nN(v)\n1\n\u03a3\n(0) = N(0) (0)\nu"}, {"title": "2. LSTM Aggregator", "content": "An LSTM is employed to aggregate the features from the neighborhood. This approach is order-sensitive.\nh(v) = LSTM({h\u22121 : u \u2208 N(v)})\nNote: The LSTM aggregator processes the embeddings of the neighbors sequentially, and the output is the final hidden state of the LSTM."}, {"title": "3. Pooling Aggregator", "content": "Applies a neural network followed by a max-pooling operation to the features of each neighbor.\nhk\nh(v) = max({ReLU(W \u2022 h\u22121 + b) : u \u2208 N (v)})"}, {"title": "Graph Attention Networks(GAT)", "content": "Graph Attention Networks (GATs) is an aggregation layer with masked self-attention [20]. These self-attentional layers enable the GNN model to selectively focus on specific parts of a node's neighborhood, assigning different weights to neighboring nodes. This approach eliminates the need for computationally expensive matrix operations and does not require prior knowledge of the entire graph structure. The key components and equations of GATs are as follows:\n1. Attention Coefficients: This equation computes the attention coefficients that indicate the importance of each node's features to the central node. The attention coefficients are normalized using the softmax function to ensure they sum to one and can be interpreted as probabilities.\nAij\nexp (LeakyReLU (a[Whi||Whj]))\n\u03a3\u03ba\u2208N(i) exp (LeakyReLU (aT [Whi||Whk]))\n2. Aggregation: The node features are aggregated using the weighted sum of the features of its neighbors, with weights given by the attention coefficients.\nh = \u03c3\n(Wh\nJEN(i)\naij Whj"}, {"title": "2.2 Readout Layer", "content": "The readout layer, also known as the graph-level pooling layer, plays a crucial role in the GNN model by transforming node-level embeddings into a graph-level representation. This representation is essential for tasks such as graph classification, graph regression, and entire graph embeddings. Conventionally, GNNs utilize simple pooling operations, such as averaging or summing, as described as follows:\nMean Pooling Readout Layer Mean pooling aggregates node features by computing the average. This process effectively captures the central tendency"}, {"title": "3 Methods", "content": "We collected a total of 383 Alzheimer's Disease patients, including 243 CN and 140 preclinical AD participants, from the Catholic Aging Brain Imaging(CABI) database. All participants, matched for age, gender, and education level, underwent T1-weighted MRI, resting-state functional MRI(rs-fMRI), and 18F-Florbetaben PET(FBB-PET). This database contains brain scans of patients who visited the outpatient clinic at the Catholic Brain Health Center, Yeouido St. Mary's Hospital, The Catholic University of Korea, between 2017 and 2022. The study was conducted under ethical and safety guidelines set forth by the Institutional Review Board of Yeouido St. Mary's Hospital, College of Medicine, The Catholic University of Korea (IRB number: SC22RIDI0153). The informed consent was waived by the IRB because we only used retrospective data. The detailed demographic information is described in Table 1."}, {"title": "3.2 Data Preprocessing", "content": "T1w MRI preprocessing was performed based on Micapipe to align normalized T1w MRI and parcellated brain into 374 regions, consisting of 360 cortical regions from Glasser atlas and 14 subcortical regions from aseg subcortical atlas [21]. The rs-fMRI had been pre-processed using Micapipe [22], including the following steps: slice timing and head motion correction, skull stripping, intensity normalization, and band-pass filtering. The pre-processed data were registered onto"}, {"title": "3.3 Brain-aware Graph Neural Network", "content": "Motivation Unlike social networks or chemical molecular networks, brain networks have the unique characteristic that nodes(i.e., brain regions) are organized into networks based on similar roles or functions. Therefore, simple readout layers that average or sum node features are insufficient for capturing the complex and heterogeneous characteristics of brain networks. To overcome limitations, we propose a novel readout layer clustering brain regions regarding similar tasks and allowing the GNN model to represent details brain characteristics. By leveraging these clusters, we aim the proposed GNN model to learn the intricate and heterogeneous features of brain networks, leading to more accurate and meaningful graph-level representations.\nBrain-aware Readout Layer (BA readout layer) The BA readout layer is designed to effectively capture the complex and heterogeneous characteristics of brain networks by clustering brain regions based on functional connectivity and node embedding. As shown in Figure 1 (D), the BA readout layer is expressed as:\nhBA = [f(hc\u2081), f(hc\u2082), ..., f (hc\u2081)]T, where hc\u2081 =\n1\nVei\n\u03a3\u03c5\nVEVC\nwhere hBA represents group-level representation, f(.) denotes a linear transformation function used to aggregate embeddings, Ve\u2081 represents a set of nodes in ci cluster, and he\u2081 denotes a cluster-level embedding in the ci cluster.\nBriefly, the brain-aware clusters and their members have been selected by using a simple auxiliary neural network. An auxiliary network consisted of a single-layer artificial neural network with softmax function feed position embedding of each node, C = softmax(f(P)), and compute its probability of belonging"}, {"title": "4 Experiments and results", "content": "We have applied the proposed BA readout layer on various GNN models and assessed their predictive performance compared with the model with conventional readout layers. Many researchers have successfully adopted the GNN model for brain connectivity analysis. We have carefully chosen three GNN models: 1) GCN, 2) GraphSAGE, and 3) GAT, and two readout layers: 1) mean-pooling layer, and 2) add pooling layer. For all benchmark algorithms, we have constructed a graph where a cortico-cortical functional connectivity is based on Pearson's correlation with a predefined atlas as the edges of the graph and the pre-calculated SUVR for each brain region as node attributes of the graph.\nIn this study, we focus on the early detection of AD at the preclinical stage, which predicting the PACC score. We have applied a five-fold cross-validation strategy to examine the performance of the model, in terms of R2 score. The model hyperparameters, such as hidden dimensions and learning rates, optimizing the number of clusters for each model, are tuned by Bayesian search on the training set implemented in Sweep methodology from Weights and Biases (WandB) [24]. Bayesian search strategy efficiently searches the most effective combination of hyperparameters using Bayesian optimization. Once optimal hyper-parameters are determined, the trained model is applied to the test set to generate the final performance."}, {"title": "4.2 Performance Comparison Across Readout Layer on Various\nGNN Model", "content": "To assess the effectiveness of the proposed method(i.e., BA readout layer), we have compared the predictive power of various GNN models with conventional readout layer, such as mean readout and add readout layer, against those using BA readout layer. The predictive power of the model is evaluated by predicting the PACC score of CN and preclinical AD subjects."}, {"title": "4.3 Effectiveness of Prior Knowledge in BA Readout Layer", "content": "Additionally, we examine the effectiveness of incorporating prior knowledge into the BA readout layer. In detail, we apply the well-known Yeo 7 functional network as a predefined cluster of BA readout layer, called as Yeo BA readout layer, and compared with the proposed BA readout layer, called adaptive BA readout layer [25]. Overall, the integration of prior knowledge with the proposed layer (i.e., Yeo BA readout layer) demonstrates enhanced performance compared to layers without such knowledge (i.e., adaptive BA readout layer). However, we noted that the adaptive BA readout layer exhibited a lower standard deviation than the Yeo BA readout layer. This suggests that the adaptive BA readout layer offers greater robustness and stability in its performance."}, {"title": "4.4 Interpretability of the GNN Model with BA Readout Layer", "content": "The advantage of the adaptive BA readout layer is that be able to select a set of task-specific brain regions of the clusters, regardless of the GNN model. Particularly, the members of clusters are AD-specific brain regions, which play roles in memory, cognitive, or sensory function, that improve the model's interpretability. We utilized Neurosynth [26], a platform that synthesizes large amounts of human brain imaging data to decode patterns of neural activity associated with psychological terms and cognitive states to interpret selected brain regions of our model. Top 5 Neurosynth topics were investigated and the topics related to anatomical terminology were excluded.\nFigure 2 illustrates five distinct clusters and their top five Neurosynth topics identified by the GAT with the BA readout layer. The brain regions within Cluster 1 are associated with language-related tasks, those within Cluster 2 are linked to memory functions, those within Clusters 3 and 4 are associated with dementia and social interactions, and those within Cluster 5 are related to sensory-motor functions. We found that the model identified the anterior cingulate, temporal pole, anterior insula, orbitofrontal cortex, and prefrontal cortex as members of clusters 3 and 4. These regions play important roles in cognitive functions and emotional responses or memory decline which is associated with AD [27,28,29].\nFigure 3 illustrates three distinct clusters and their top five Neurosynth topics identified by the GraphSAGE with the BA readout layer. The brain regions within cluster 1 are associated with language and memory processing, which is crucial for detecting early signs of cognitive decline related to linguistic abilities and memory retention [30,31]. The brain regions within cluster 2 are linked to social cognition, essential for understanding and interacting with others, which can be impaired in Alzheimer's disease [32,33]. Those within Cluster 3 are associated with sensory processing and pain.\nFigure 4 illustrates five distinct clusters and their top five Neurosynth topics identified by the GCN with the BA readout layer. The brain regions within each"}, {"title": "5 Conclusion", "content": "AD presents significant challenges in both diagnosis and treatment, particularly due to its heterogeneous manifestation and the varying progression among individuals. This study introduces a novel approach to improve the interpretability and predictive power of GNNs in analyzing brain networks for early diagnosis of AD. We propose a brain-aware readout layer (BA readout layer) that clusters brain regions based on functional connectivity and node embedding, allowing the GNN model to capture intricate and heterogeneous characteristics of brain networks more effectively.\nOur experiments demonstrate that GNN models incorporating the BA readout layer outperform those using conventional readout layers in predicting the PACC score. Specifically, the GCN and GAT models with the Yeo BA readout layer, as well as the GraphSAGE model with the adaptive BA readout layer, achieve the highest R\u00b2 scores, indicating superior predictive performance.\nMoreover, the adaptive BA readout layer provides robustness and stability, exhibiting lower standard deviation compared to the Yeo BA readout layer. Furthermore, unlike the Yeo BA readout, which focuses solely on functional connectivity, the adaptive BA readout considers functional connectivity as well as node embedding, allowing for a richer and more meaningful graph-level representation. This suggests that the adaptive approach is more reliable across different datasets and scenarios. The interpretability of our model is further enhanced by the ability to identify task-specific brain regions within the clusters, which are associated with key cognitive functions and areas known to be affected by AD.\nIn summary, our proposed brain-aware readout layer for GNNs offers a promising tool for improving the diagnosis of Alzheimer's disease by leveraging the complex patterns inherent in brain data. By effectively clustering brain regions based on functional connectivity and node embedding, our approach enhances both the interpretability and performance of GNN models in neuroimaging analysis. Future research can further explore the potential of this method in other neurological disorders and refine the clustering techniques to capture even more nuanced brain network characteristics."}]}