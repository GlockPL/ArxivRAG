{"title": "Safe Semi-Supervised Contrastive Learning Using In-Distribution Data as Positive Examples", "authors": ["Min Gu Kwak", "Hyungu Kahng", "Seoung Bum Kim"], "abstract": "Semi-supervised learning methods have shown promising results in solving many practical problems when only a few labels are available. The existing methods assume that the class distributions of labeled and unlabeled data are equal; however, their performances are significantly degraded in class distribution mismatch scenarios where out-of-distribution (OOD) data exist in the unlabeled data. Previous safe semi-supervised learning studies have addressed this problem by making OOD data less likely to affect training based on labeled data. However, even if the studies effectively filter out the unnecessary OOD data, they can lose the basic information that all data share regardless of class. To this end, we propose to apply a self-supervised contrastive learning approach to fully exploit a large amount of unlabeled data. We also propose a contrastive loss function with coefficient schedule to aggregate as an anchor the labeled negative examples of the same class into positive examples. To evaluate the performance of the proposed method, we conduct experiments on image classification datasets\u2014CIFAR-10, CIFAR-100, Tiny ImageNet, and CIFAR-100+Tiny ImageNet-under various mismatch ratios. The results show that self-supervised contrastive learning significantly improves classification accuracy. Moreover, aggregating the in-distribution examples produces better representation and consequently further improves classification accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep neural networks have shown promising results in various supervised learning problems, including image classification [1], object detection [2], natural language processing [3], and signal data analysis [4]. It is well known that a large-scale training dataset with well-annotated labels is required for dependable performance in supervised learning tasks [5]. However, creating such extensive collections of labeled data is typically time-consuming and incurs high costs for many real-world problems. Consequently, it limits the broad adoption of deep neural networks for many practical issues and applications.\nSemi-supervised learning (SSL) algorithms have been proposed to reduce the labeling overload and improve model performance by leveraging unlabeled data when only a limited number of data samples have corresponding labels [6], [7]."}, {"title": "II. RELATED WORKS", "content": "Traditional deep SSL methods have raised the model capability to the point where it can achieve similar performance to supervised learning. They tackle the scenario when the class categories of labeled data and unlabeled data are identical. Pseudo-label was proposed as a method to use predicted class probabilities as targets for a standard supervised loss function [33]. Making the network produce confident predictions by minimizing the entropy for all unlabeled data has also been proposed [13]. The \u03a0-Model introduced a simple framework to apply consistency regularization in SSL [14]. Consistency regularization means that even when subtle perturbations such as data augmentation are given to an input, a model should"}, {"title": "III. PROPOSED METHOD", "content": "We propose a safe semi-supervised contrastive learning method to mitigate the performance degradation caused by class distribution mismatch. The core principle of our method is the specific reassignment of labeled negative examples that share the same class label as the anchor, treating them as additional positive examples. The reassigned examples are indeed ID. Our method enhances the model's ability to identify and aggregate similar patterns within a class, thereby increasing the precision and distinctiveness of class representations. Additionally, by not discarding OOD data and instead utilizing it in our training process, we can more effectively learn the information commonly shared across the dataset.\nThe proposed method is based on MoCo, an SSCL approach renowned for its enhanced representation learning capability and computational efficiency. It is worth noting that the selection of MoCo as our foundation is attributed to its dynamic dictionary mechanism of a memory queue. MoCo maintains a memory queue of encoded representations, which are employed as negative examples. In the context of SSL, where labeled data is scarce, methods like SimCLR that do not utilize a memory queue require significantly larger batch sizes. This is because, in smaller batches, the number of negative examples with labels can be extremely low or even non-existent. However, large batch sizes often lead to memory shortage issues. To effectively implement our method, we designed our model based on the MoCo framework, which reduces the need for large batch sizes while ensuring a sufficient supply of labeled negative examples for contrastive learning in class distribution mismatch scenarios."}, {"title": "A. Problem Statement", "content": "Let D = DL \u222a Du be a training dataset, where DL = {(xi, Yi)}^n_{i=1} is the labeled data of n instances and Du = {xj}^m_{j=1} is the unlabeled data of m instances. In a typical SSL classification problem, x \u2208 X \u2208 Rd, y \u2208 {1,2,\u2026\u2026,C}, and m > n where d is the number of input dimension and C is the number of classes. The goal of deep SSL is to train a parameterized network h that minimizes the following loss function:\n\nLSSL = \\sum_{(x_i, y_i) \\in D_L} l_1(h(x_i, y_i)) + \\sum_{x_j \\in D_U} l_2(h(x)), (1)\n\nwhere l1 and l2 are loss functions. In general, cross-entropy is used for l1, and a regularization function is used for l2. The network h converges to capture the data representations from D and classifies new instances with a clue from the limited number of DL. However, if the class distribution of DL and Du is different, the data representations are poorly learned, resulting in the classification performance degradation [22]."}, {"title": "B. SSCL: Instance Discrimination", "content": "SSCL methods conduct instance discrimination, enabling a model to utilize all information in unlabeled data without being adversely influenced by the distribution discrepancies between labeled and unlabeled data. The instance discrimination aims to learn the representations by making similar instances pull each other together and dissimilar instances push each other apart. To achieve this goal, stochastic data augmentation strategies are applied. Given an instance xi \u2208 D, two different views are generated by applying data augmentation twice: xi and x+i denote the anchor example and positive example, respectively. Then, data augmentation is applied to K instances sampled from D \u2013 {x1} to achieve negative examples {xi,k}^K_{k=1}. The representations of a positive pair (xi, xi+) must be close because they originate from an identical instance. On the"}, {"title": "C. Proposed Contrastive Loss and Schedule Method", "content": "It is known that the classes are linearly separable on an L2-normalized unit hypersphere if the instances are successfully"}, {"title": "IV. EXPERIMENTS", "content": "For a thorough and fair comparison, we followed the experimental settings and evaluation protocol for SSL with"}, {"title": "V. CONCLUSIONS", "content": "In this study, we proposed a safe semi-supervised con-trastive learning method built upon MoCo. It is the first study to apply a SSCL approach to SSL with OOD data. The proposed method contributes to considerable gains in classification performance in various SSL scenarios involving class distribution mismatches. Because the MoCo learns data representations in terms of instance discrimination, it has the"}]}