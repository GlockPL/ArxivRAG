{"title": "Generalizable Facial Expression Recognition", "authors": ["Yuhang Zhang", "Xiuqi Zheng", "Chenyi Liang", "Jiani Hu", "Weihong Deng"], "abstract": "SOTA facial expression recognition (FER) methods fail on test sets that have domain gaps with the train set. Recent domain adaptation FER methods need to acquire labeled or unlabeled samples of target domains to fine-tune the FER model, which might be infeasible in real-world deployment. In this paper, we aim to improve the zero-shot generalization ability of FER methods on different unseen test sets using only one train set. Inspired by how humans first detect faces and then select expression features, we propose a novel FER pipeline to extract expression-related features from any given face images. Our method is based on the generalizable face features extracted by large models like CLIP. However, it is non-trivial to adapt the general features of CLIP for specific tasks like FER. To preserve the generalization ability of CLIP and the high precision of the FER model, we design a novel approach that learns sigmoid masks based on the fixed CLIP face features to extract expression features. To further improve the generalization ability on unseen test sets, we separate the channels of the learned masked features according to the expression classes to directly generate logits and avoid using the FC layer to reduce overfitting. We also introduce a channel-diverse loss to make the learned masks separated. Extensive experiments on five different FER datasets verify that our method outperforms SOTA FER methods by large margins.", "sections": [{"title": "1 Introduction", "content": "Facial expression recognition (FER) aims to understand human feelings and is vital to human-computer interaction. With the development of deep learning, FER methods achieve great success on both laboratory-collected and in-the-wild FER datasets. However, we find that things are different when the test sets have domain gaps with the train set. For example, if we train the SOTA FER model EAC [49] on one given FER dataset like RAF-DB [25], then it can only achieve high performance on the test set of RAF-DB, while very low performance on other different FER test sets like AffectNet [29], shown in Fig. 1. Though some works try to deal with the domain generalization problem in FER, they all assume accessing labeled or unlabeled target set samples for FER model fine-tuning. However, in the real-world FER, as we do not previously know the distribution of the test samples, we might not be able to get access to even the unlabeled target samples. In such cases, the domain adaptation FER methods cannot work. In this paper, we aim to improve the zero-shot generalization ability of FER methods on different unseen test sets using only one train set. The difference between our work and existing domain adaptation FER works is that we only utilize one given train set and we do not utilized labeled or unlabeled target samples to fine-tune our model. This is more similar to the real-world deployment where we usually only have one train set with a similar style of annotating (without considering annotation inconsistency) and we need to recognize unseen test samples from various domains. As the generalization ability of the FER model is our main focus, we name the paper as Generalizable Facial Expression Recognition (GFER).\nWe find in our experiments that SOTA FER methods achieve low test accuracy on different unseen FER test sets. The reason might lie in that these methods fit the given FER train set too well (including domain-specific information) to predict on test sets with domain gaps. However, we humans take a very generalizable way to recognize facial expressions. Given images with domain gaps, we first locate the face features, then we disentangle expression features from the face features and make a recognition only based on the expression features. Inspired by that, we design a novel pipeline to improve the generalization ability of FER methods. We first extract generalizable face features from pre-trained large models like CLIP. As the large models have been trained with numerous images including face images, we can assume the extracted face features from them could generalize seamlessly to different FER domains. However, it is non-trivial to adapt the general features extracted by CLIP for specific tasks like FER. We further propose a novel method to learn sigmoid masks to select expression-related features for FER. Specifically, we fix the general face features during the whole training process to maintain the generalization ability. We then"}, {"title": "2 Related Work", "content": "2.1 Facial Expression Recognition\nFacial Expression Recognition (FER) plays a vital role in human-computer inter-action, and extensive research has been conducted to enhance the performance of FER [1, 7, 14, 15, 17, 21\u201325, 34, 36, 47, 50, 51]. For instance, Li et al. [25] use crowd-sourcing to simulate human expression recognition, while [1, 15] employ model ensembling to leverage more information. Farzaneh et al. [7] propose a center loss variant to maximize intra-class similarity and inter-class separation for FER, and Ruan et al. [34] acquire expression-relevant information during the decomposition of an expression feature. However, we find that these FER methods are effective when the test set has no domain gap with the train set, while their performance drops drastically when facing domain-different test sets. In this paper, we aim to improve the generalization ability of the FER model and make it suitable for real-world deployment. We train the FER model on one given FER train set and evaluate it on all unseen different FER test sets.\n2.2 Domain Generalization\nDomain Generalization (DG) aims to help models trained on a set of source do-mains generalize better on unseen target domains [5, 8, 12, 16, 20, 27, 28, 33, 39, 40, 44\u201346]. A common practice is to reduce the feature discrepancy among multiple source domains. [26, 28, 41] all adopt maximum mean discrepancy on multiple layers to enforce the distribution similarity between source and target features. Deep CORAL [38] uses feature covariance to measure the domain discrepancy. Another stream of works tries to enlarge the available train data space with augmentation of source domains [3, 6, 30, 37, 52, 53]. Several approaches leverage regularization through domain adversarial learning [13, 32] to address DG. How-ever, domain generalization (DG) methods in the FER field usually assume the availability of labeled or unlabeled target expression samples to aid in fine-tuning the FER models. However, our approach differs significantly. We strive to en-hance the generalization ability of facial expression recognition (FER) methods by exclusively training the FER model on a single FER dataset and evaluating it on various unseen FER test sets. We refrain from accessing any samples from the target domain for fine-tuning our method, rendering existing FER domain adaptation methods infeasible for our task."}, {"title": "3 Problem Definition", "content": "In this paper, we aim to improve the generalization ability of the FER model and guide it to recognize unseen test samples with domain gaps of the train samples. Learning is conducted on one given FER train set, and then test samples from different FER test sets with domain gaps of the train set should be recognized on the fly, which is similar to the real-world deployment of FER models.\nFER models are trained with $D_{train} = \\{(x_i, y_i)\\}_{i=1}^N$, where $x_i$ is the $i$-th training image and $y_i \\in Y = \\{1, ..., L\\}$ is the corresponding label, $N$ is the number of training samples and $L$ is the number of expression classes, we consider the seven basic expressions across different FER datasets in this paper, thus $L$ equals 7. Existing FER models are evaluated on the test set $D_{test} = \\{(x_i, y_i)\\}_{i=1}^M$ that has no domain gap between the train set, $M$ is the number of test samples. However, the real-world test set $D_{real} = \\{(x_i, y_i)\\}_{i=1}^M$ is different from $D_{test}$, as $D_{real}$ might contain samples with domain gaps of the training samples. In this paper, we aim to train the FER model on $D_{train}$ that can generalize well on the real-world test set $D_{real}$. The difference between our setting and the traditional FER is that we test the FER models on $D_{real}$ instead of $D_{test}$. The difference between our setting and domain adaptation FER is that we only use the training set from one source domain and do not have access to the unlabeled samples from $D_{real}$ to fine-tune the FER model. Our setting is more similar to the real-world deployment as we cannot previously know the distribution of the target domain in the real world."}, {"title": "4 Method", "content": "To solve the aforementioned problem, we propose a novel method to mimic the Cognition of human for Facial Expression and name it as CAFE. As humans, when we encounter test samples with domain gaps from the training samples, we first extract their face features to exclude the domain features. Afterward, we focus solely on the features related to the expressions in order to determine the expression contained within the test sample. Following a similar approach, the proposed method initially extracts the face features of the test samples. Then, a trained FER model generates masks for selecting expression-related features from the facial features and making decisions solely based on the selected features. The framework of our proposed method is shown in Fig. 2.\n4.1 Mask on Fixed Face Features\nWe design a novel method to guide the model to learn masks on fixed face fea-tures in order to selectively choose useful expression features. Since face features can be extracted using pre-trained large models like CLIP [31], we can assume that these features are sufficiently generalizable for different domains.\nGiven images x from $D_{train}$, we first extract the face features using CLIP, denoted as $F \\in \\mathbb{R}^{N\\times C}$, where $N$ is the number of images and $C$ the number of feature dimensions, we fix $F$ during the training process to prevent the FER model to optimize face features to overfit the given train set. This operation improves the generalization ability of our proposed method. The FER model, such as ResNet-18, is trained to learn masks for the given face features. We first extract features $f \\in \\mathbb{R}^{N\\times C\\times 1\\times 1}$ from x after the global average pooling (GAP) layer and resize them to generate the masks $M \\in \\mathbb{R}^{N\\times C}$ for face features. Further, in order to regularize the learned masks to generalize to unseen test samples, we apply a sigmoid function on M and get $M_s$ as\n$M_s = Sigmoid(M)$.\nThe sigmoid function is vital to the success of our method as it introduces non-linearity into the model, which is crucial for capturing and learning non-linear patterns to generate the masks. The sigmoid function also normalizes M, ensur-ing that the values of M fall within [0, 1], which reduces the overfitting ability of the learned masks. Furthermore, the sigmoid function provides a probability-like output, where the output value represents the probability of selecting the feature of the corresponding channel, which is semantically similar to humans choosing expression-related features from the face features.\nas\nWe further utilize $M_s$ to select the face features for expression recognition\n$\\widetilde{F} = M_sF$."}, {"title": "4.2 Mask Generalization", "content": "To make the learned mask generalizable to other unseen FER test sets, we further design a separation module to regularize the learned mask. Specifically, we set apart the masked features into seven pieces according to the channel dimension to make the masked features correspond to seven basic expressions and then max pool each piece to directly transform them into logits. In such a design, we could avoid the use of the FC layer and directly connect the masked features to the FER labels. The motivation of our design is three folds: Firstly, the learning ability of the FC layer might be too strong to overfit the train set. Thus, we directly transform the masked features into logits to prevent the FC layer from overfitting the labels with the learned features. Secondly, the channel size of 512 when using ResNet-18 might be too large to learn generalizable masks and could also lead to overfitting on the training set. If we set apart the mask into seven pieces and make each piece of the mask correspond to one basic expression, the mask piece with a small channel size will be more likely to only focus on the useful information. Thirdly, setting the masked features into seven pieces is similar to label distribution learning, we guide the seven pieces to focus on features related to the seven basic expressions on one image to solve the FER from a label distribution learning perspective. As FER is similar to a label distribution learning task, one image might contain features of several different expressions. For example, there are compound expressions and one compound expression image contains features from several different basic expression classes. Specifically, given the masked features $\\widetilde{F}$, we divide them according to the channel number $C$ to $L$ pieces corresponding to the class number $L$ on the second dimension, $\\widetilde{F} = \\{\\widetilde{F}_1, \\widetilde{F}_2, ..., \\widetilde{F}_L\\}$. If $C$ can not be divided by $L$, we could divide the selected features non-uniformly. For example, when using ResNet-18, the channel size is 512, we could assign 73 channels for each of the 6 expressions and leave the rest 74 channels for the neural expression. We utilize channel-dropping on the selected features to mitigate the overfitting problem of the selected features. The drop mask is denoted as $M_{drop} = \\{M_1, M_2, ..., M_L\\}$ with the same size of the selected features. Each mask contains 0 or 1 for feature selection. For example, in the $M_1$, which is a vector of size (N, 73), N is the batch size, we keep the mask same in each batch, thus, for simplicity, we neglect N in the following. On the channel dimension of 73, if we set the drop rate as 10/73, there are 10 channels randomly selected as 0 and all the rest are 1. The channel drop module guides the model to focus on all channels, which increases the generalization ability of the masked features. After channel dropping, the selected features are denoted as\n$\\overline{F} = \\widetilde{F}M_{drop} = \\{\\widetilde{F}_1M_1, \\widetilde{F}_2M_2, ..., \\widetilde{F}_LM_L\\}$.\nThe selected features $\\overline{F}$ is downsized to logits $F^d$ of size (N, L) for classification through a maxpooling operation on the second dimension.\n$F^d = \\{max(\\widetilde{F}_1M_1), max(\\widetilde{F}_2M_2), ..., max(\\widetilde{F}_LM_L)\\}$.\nThen, we compute a classification loss $l_{sep}$ with the labels and the logits $F^d$ that are obtained by the separation module without the FC layer.\n$l_{sep} = -\\frac{1}{N}\\sum_{i=1}^N (log(\\frac{e^{F_y^d}}{\\sum_{j=1}^L e^{F_j^d}})).$\nTo increase the generalization ability of the learned masks, we want to make the channels corresponding to each class as diverse as possible. Thus, we further introduce a channel-diverse loss. Specifically, we input the masked features $\\widetilde{F}\\in \\mathbb{R}^{N\\times C}$ into the max pooling operation to get\n$F_{max} = \\{max(\\widetilde{F}_1), max(\\widetilde{F}_2), ..., max(\\widetilde{F}_L)\\}$,\nwhere $F_{max} \\in \\mathbb{R}^{N\\times L}$. The selected max feature channel of the second dimension of $F_{max}$ is regularized to be diverse with other feature channels by the channel-diverse loss $l_{div}$,\n$l_{div} = 1- \\frac{1}{N L} \\sum_{i=1}^N\\sum_{j=1}^L F^{max}_{i,j}$."}, {"title": "5 Experiments", "content": "5.1 Datasets\nRAF-DB [25] is annotated with seven basic expressions by 40 trained human coders, including 12,271 images for training and 3,068 images for testing.\nFERPlus [2] is extended from FER2013 [9] with cleaner labels, which consists of 28,709 training images and 3,589 test images collected by the Google search engine, we utilize the same seven basic expressions with the RAF-DB.\nAffectNet [29] is a large-scale FER dataset, which contains eight expressions (seven basic expressions and contempt). There are a total of 286,564 training images and 4,000 test images. We utilize the seven basic expressions in our experiments.\nSFEW2.0 is the most commonly used version of SFEW [4]. SFEW2.0 contains 958 train samples, 436 validation samples, and 372 test samples. Each image is assigned to one of the seven basic expressions.\nMMA is a large-scale FER dataset with the majority of expressions from individuals of European and American descent. The dataset contains 92,968 training samples, 17,356 validation samples, and 17,356 test samples. Each image is assigned to one of the seven basic expressions.\n5.2 Implementation Details\nWe utilize the most widely employed ResNet-18 [10] as the backbone following other FER works [43, 49]. We use the ViT-B/32 CLIP model, only employing its visual component to extract generalizable face features. Any large models trained on extensive face datasets could replace the CLIP model. As studying different models for face feature extraction is not the focus of our paper, we directly utilize the ViT-B/32 CLIP model across all our experiments. The learning rate \u03b7 is set to 0.0002 and we use Adam [18] optimizer with weight decay of 0.0001.\n5.3 Main Experiments of GFER\nTo evaluate the generalization ability of existing FER methods, we utilize one of the five FER datasets as the training set. Instead of testing only on the test set of the corresponding train set, we test on all five FER test sets. Note that we do not have access to any labeled or unlabeled images of the target domain. The results are shown in Tab. 1. Our comparison methods are FER methods published in the top conferences in recent years, such as SCN [42], RUL [48], EAC [49], and OFER [19].\nGeneralization Ability The results reveal that SOTA methods in facial ex-pression recognition (FER) do not exhibit strong performance when applied to FER test datasets characterized by domain gaps relative to the training set. For instance, while an advanced FER method like EAC significantly enhances per-formance over the SCN method on the test set that corresponds to the training set, its results on other FER test datasets are comparable to, or even worse than, those of the SCN. We speculate the reason might lie in that EAC fits the train set too well, which improves the test accuracy on the corresponding test set and degrades the test accuracy on other different FER test sets. We underline the best result of other FER methods and compare it with our method. Our method outperforms other FER methods by large margins under almost all settings. Un-der different FER train sets, our method always achieves the best mean accuracy on five different FER test sets. We also show the test accuracy of each expression class and the mean accuracy in the Supp. material and find that our method also achieves the best mean test accuracy of different expression classes.\nCross-Domain Analysis We observe that test accuracy on RAF-DB and FER-Plus are the highest across other datasets. Using SFEW2.0 as the train set achieves the lowest test accuracy on other test sets, as SFEW2.0 is the smallest dataset used in our paper and some images have low quality. Test accuracy on AffectNet is always low given different train sets because the labels of AffectNet are very noisy. Some error examples are shown in the Supp. material to help understanding.\n5.4 Discussion\nOur paper focuses on the generalization ability of FER methods in-stead of the test accuracy on the source domain. Actually, we find that strong FER methods like EAC that perform well on the test set of source domain only fits the train set well and cannot generalize to other different unseen test sets. Thus, in this paper, we aim to propose a method that can perform well on the source domain test set and generalize to other unseen test sets at the same time. We do not claim that our method always achieves the best performance on the test set of the source domain, which is not our main focus. Based on the experimental results in Tab. 1, we argue that our method performs on par with SOTA FER methods on different source domain test sets and achieves the best performance on different unseen test sets, which completely fulfills our goal.\n5.5 Ablation Study\nTo study the effectiveness of each of the proposed modules in our method, we carry out thorough ablation studies. The results in Tab. 2 show that without our method, the FER model cannot generalize to other datasets that have domain gaps with the train set, which is unsatisfactory for the real-world deployment of FER models. With our proposed sigmoid mask learning, the performance on other unseen test sets outperforms the baseline by a large margin. However, the fitting probability of the mask is too strong as the dimension of 512 might be too much to learn generalizable expression features. Thus, we further introduce the separation module which separates the masked features into pieces correspond-ing to the number of expression classes. It shows that the results improve from only using the sigmoid mask module. We also introduce a channel-diverse loss to make the channels in each piece as diverse as possible, which further improves the accuracy based on using the sigmoid mask and the channel-separation mod-ule. From the results in Tab. 2, each module contributes to the success of our proposed method, and combining them together achieves the best result.\n5.6 Different Backbones\nWe combine our method with different backbones to show its generalization ability. The results in Tab. 3 illustrate that our method improves the performance of baseline under different backbones by remarkable margins. Specifically, the improvement is largest when using ResNet-18 as the backbone, the reason might lie in that the dimension of the output feature of ResNet-18 is 512, which is the same as the dimension of the output feature of CLIP. When using MobileNet [11] or ResNet-50, we reduce the size of the output feature through mean operation to suit the feature dimension of CLIP, which might slightly limit the performance improvement. For example, when the backbone is ResNet-50, we simply reduce the output feature dimension of 2048 to 512 through mean operation using sliding windows. We also observe that stronger backbones have better generalization ability in our experiment. The ResNet-50 backbone achieves the overall best performance across experiments.\n5.7 Comparison with CLIP+Finetune\nTo demonstrate the significant generalization ability inherent in our proposed method, we construct a comparison group comprising a fixed CLIP model along-side a trainable FER model, which has similar trainable parameters with our method for fair comparison. We then fine-tune a fully connected layer on the concatenated features. The results of this comparison are depicted in Tab. 4, confirming that it is the specific design of our method that effectively unleashes CLIP's generalization potential for FER.\n5.8 The Effect of the Sigmoid Function\nTo illustrate the effectiveness of the sigmoid function applied to the learned mask, we design a comparison group of our method without the sigmoid function, while we keep the others exactly the same. The experiment results on different FER test sets are shown in Tab. 5. The results demonstrate that the sigmoid function for mask learning is very important, without it, our method barely works. The reason lies in that the sigmoid function normalizes the learned masks, ensuring\n5.9 Hyperparameter Study\nWe study the influence of the weight of separation loss A and the weight of diverse loss B on our method. The results shown in Fig. 3 illustrate that both the A and B can be chosen from a wide range and the performance is only slightly different across an order of magnitude, e.g., A from [0.5,5] and B from [1,10]. For simplicity, we choose A as 1.5 and B as 5 across all our experiments. The cases when A is 0 or when B is 0 are studied in Tab. 2.\n5.10 Visualization Results\nWe use GradCAM [35] to visualize the features of FERPlus test samples by the models trained on RAF-DB. Results are shown in Fig. 4. As both EAC and our method utilize ResNet-18 instead of Transformers as the backbone, the learned features do not lie in some local areas. According to our visualization results and analyses in paper [49], EAC focuses on wrong partial features, while our method focuses on the whole face, learns better expression-related features"}, {"title": "6 Conclusion", "content": "In this paper, we aim to improve the zero-shot generalization ability of FER methods on different unseen test sets with only one train set. Enlightened by how humans first detect faces and then recognize expressions, we propose a novel method to learn sigmoid masks on fixed general face features to extract expression-related features, combining the generalization ability of large models extracted features and the high precision of FER models. To further improve the generalization ability on unseen test sets, we propose channel-separation and channel-diverse modules to regularize the learned sigmoid masks. Extensive experiments on five different FER datasets illustrate that our method achieves the best generalization ability across SOTA FER methods."}]}