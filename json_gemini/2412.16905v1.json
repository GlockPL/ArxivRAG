{"title": "A Backdoor Attack Scheme with Invisible Triggers Based on Model Architecture Modification", "authors": ["Yuan Mat", "Xu Ma", "Jiankang Wei", "Jinmeng Tang", "Xiaoyu Zhang", "Yilun Lyu", "Kehao Chen", "Jingtong Huang"], "abstract": "Machine learning systems are vulnerable to backdoor attacks, where attackers manipulate model behavior through data tampering or architectural modifications. Traditional backdoor attacks involve injecting malicious samples with specific triggers into the training data, causing the model to produce targeted incorrect outputs in the presence of the corresponding triggers. More sophisticated attacks modify the model's architecture directly, embedding backdoors that are harder to detect as they evade traditional data-based detection methods. However, the drawback of the architectural modification based backdoor attacks is that the trigger must be visible in order to activate the backdoor. To further strengthen the invisibility of the backdoor attacks, a novel backdoor attack method is presented in the paper. To be more specific, this method embeds the backdoor within the model's architecture and has the capability to generate inconspicuous and stealthy triggers. The attack is implemented by modifying pre-trained models, which are then redistributed, thereby posing a potential threat to unsuspecting users. Comprehensive experiments conducted on standard computer vision benchmarks validate the effectiveness of this attack and highlight the stealthiness of its triggers, which remain undetectable through both manual visual inspection and advanced detection tools.", "sections": [{"title": "I. INTRODUCTION", "content": "Machine Learning (ML) is facing a new type of threat, where attackers deliberately implant hidden behaviors as back-doors into neural networks [1]. These backdoors cause the network to exhibit pre-defined changes in behavior when specific triggers are present in the model inputs; otherwise, the network operates normally and maintains high evaluation performance without the triggers. Most existing backdoor attacks are implemented by altering the training weights of the model [1] that is, embedding backdoors into the parameters during the training phase of the neural network. This can be accomplished through direct approaches, such as explicitly modifying the weight values [2]. However, such backdoor attacks have limitations. When a third party later modifies the model weights, for example, through fine-tuning to adapt to new tasks or environments, the original backdoor may be removed or weakened [3]. This is mainly because fine-tuning alters the parameter distribution within the model, thereby affecting the effectiveness of the backdoor.\nSubsequently, methodologies have surfaced that introduce backdoors through alterations in the architecture [4]. Diverging from conventional techniques reliant on weight adjustments, this approach accomplishes backdoor functionality by manipulating the model's structure. Despite eschewing direct modifications to model weights, these techniques resist eradication through subsequent retraining efforts. However, the triggers within these methodologies are often not sufficiently covert. Many architecture-based backdoor attacks rely on specific layers or functions to activate triggers, which usually require distinct features or patterns to exist in the input data. This means that, to ensure the triggers can be successfully activated by these particular layers or functions, attackers tend to design the triggers in the input data to be quite conspicuous. These features or patterns can sometimes be captured by human eyes or detection tools, thus exposing the existence of the backdoor.\nTo overcome this limitation, our research explores how to design more covert triggers, ensuring that even if the triggers are finely tuned to maintain an almost imperceptible presence, they can still maintain effective activation by the modified model architecture. We find that by ingeniously leveraging the characteristics of the modified model architecture, it is possible to inject triggers into the input data that are almost undetectable by conventional detection methods and the naked eye, and successfully activate them through these modified architectures. This provides new possibilities for backdoor attacks.\nTo achieve the aforementioned requirements, we need to ensure: (1) Directly connect the input data to the output through some kind of backdoor activation function; (2) The operation of this mechanism should not depend on or affect any existing parameters in the model; (3) In the process of adding triggers to the input data, it is crucial to minimize the numerical perturbation to the original input data as much as possible; (4) Be able to handle various input preprocessing functions. Our work makes the following key contributions:\n\u2022\n\u2022 Revealing a new type of backdoor attack targeting neural network architectures, where the backdoor is embedded within the model's architecture,\n\u2022 Proposing an absolutely covert method for injecting trig-gers, ensuring that these triggers remain undetectable by both conventional detection tools and advanced analytical techniques,\nUnlike methods that rely on model weights [1], archi-tectural modification backdoors can be added directly to already trained model architectures without requiring any additional training."}, {"title": "II. RELATED WORK", "content": "A. Backdoor Attack\nIn the field of machine learning and deep learning, backdoor attacks refer to scenarios where attackers deliberately introduce a hidden backdoor trigger condition during the model training process, causing the model to produce specific, usually incorrect, outputs when encountering certain inputs, while behaving normally with regular inputs. Such attack methods can be achieved through the following approaches:\nPoisoning-based Attack. Gu et al. [1] describe an attack method that manipulates model behavior by embedding specific triggers in the training data, allowing these backdoors to be activated after the model is deployed, leading to abnormal behavior when the model receives specific inputs. Liu et al. [5] achieve this by injecting hidden triggers into the model during training. The trigger is designed to activate under specific conditions, causing the model to behave abnormally when it encounters inputs containing the backdoor pattern. Turner et al. [6] maintain high accuracy on normal data while injecting backdoors, making detection more difficult. A key feature of these attacks is that, even with backdoors injected, the model's predictions on normal data remain consistent with labels, not significantly affecting overall performance. Shafahi et al. [7] add carefully crafted poisoning samples to the training dataset, causing the model to exhibit erroneous behavior for specific inputs after deployment. Unlike traditional backdoor attacks, this method does not require embedding obvious triggers in input data but achieves its goal by altering data labels. Salem et al. [8] stand out because their backdoor triggers and target classes can dynamically change according to different scenarios, increasing the stealth and effectiveness of the attack. Compared to static backdoor attacks, dynamic backdoor attacks are more flexible and harder to detect.\nNon-poisoning-based Attack. Guo et al. [9] implement attacks by embedding hidden Trojan models within neural networks. This method ensures the model behaves normally on regular data while activating the hidden Trojan model under specific conditions, leading to incorrect outputs. Hong et al. [2] distinguish between automatically generated and handcrafted backdoors, noting that the latter are manually designed and embedded by attackers, often featuring higher stealth and customization. Bober-Irizar et al. [4] propose an attack method that embeds backdoors in neural networks by modifying the model architecture. Unlike traditional backdoor attacks that primarily rely on data poisoning, architectural backdoors directly implant malicious components at the design level of the model, causing abnormal behavior under specific conditions. Qi et al. [10] describe an attack carried out after the model has been trained and deployed in a production environment. This method aims to modify the model or data during the deployment phase to cause abnormal behavior under specific conditions. Salem et al. [11] uniquely enable abnormal model behavior under specific conditions without explicit triggers.\nB. Backdoor Defense\nIn the field of machine learning and deep learning, de-fense measures against backdoor attacks refer to a series of techniques and strategies designed to detect and mitigate the impact of backdoors. The goal of these defense methods is to ensure that models maintain their accuracy and reliability in predictions when confronted with maliciously crafted inputs, while not affecting the model's ability to process normal inputs.Here are several common defense methods against backdoors:\nSample Filtering based Empirical Defense. Chen et al. [12] proposes an activation clustering method to detect back-door attacks by analyzing the internal activations of neural networks during inference. Gao et al. [13] proposes a statistical Test for Identifying Poisoning this is a lightweight method that uses statistical tests to detect the presence of trojan triggers without requiring labeled data. Chou et al. [14] introduces a new method for detecting localized universal attacks against deep learning systems. These attacks are characterized by adding a fixed perturbation pattern to the input data, which can cause the model to produce incorrect predictions. Such perturbations are typically very subtle and difficult for the human eye to detect. Guo et al. [15] proposes a method aimed at efficiently detecting black-box input-level backdoor attacks by analyzing scaled prediction consistency.\nTrigger Synthesis based Empirical Defense. Wang et al. [3] introduces Neural Cleanse, a method to detect and remove backdoors by identifying the minimum perturbation required to trigger the backdoor behavior. Qiao et al. [16] proposes a method for defending against neural network backdoor attacks through generative distribution modeling. This method uses generative models to model the distribution of normal input data and detects potential backdoor triggers through anomaly detection. Huang et al. [17] introduces a method for defending against backdoor attacks by extracting cognitive backdoor patterns from images. This method uses deep learning tech-niques to identify and extract subtle features from images, effectively detecting and preventing backdoor attacks. Tao et al. [18] introduces an improved trigger inversion optimization method aimed at enhancing the efficiency and accuracy of backdoor scanning. This method improves detection precision and computational efficiency through optimized algorithms, demonstrating good detection performance against various types of backdoor attacks. Dong et al. [19] introduces a method for detecting black-box backdoor attacks under conditions of limited information and data. This method can effectively detect backdoor attacks even when only a small amount of clean data and limited information are available.\nModel Reconstruction based Empirical Defense. Liu et al. [20] presents Fine-Pruning, a technique that leverages model pruning to eliminate the parts of the network that have been influenced by backdoor poisoning.Li et al. [21] intro-duces a method for erasing backdoor triggers from deep neural networks using attention distillation. This method captures and amplifies key features using attention mechanisms, effectively identifying and removing backdoor triggers, thereby ensuring the model maintains normal prediction performance even when faced with malicious inputs. Wu et al. [22] introduces a method for purifying backdoored deep models through adversarial neuron pruning. This method effectively eliminates the impact of backdoor attacks by identifying and removing key neurons, thereby restoring the model's normal prediction performance. Pang et al. [23] introduces a method for detecting and pu-rifying backdoored deep models using unlabeled data. This method identifies and removes backdoor triggers by analyzing the behavior of unlabeled data in the model, thereby restoring the model's normal prediction performance.\nModel Diagnosis based Empirical Defense. Liu et al. [24] introduces a method for detecting complex backdoor attacks by computing symmetric feature differences. This method analyzes the feature differences between normal inputs and potential backdoor inputs, effectively identifying complex backdoor attacks. Xiang et al. [25] introduces a method for post-training detection of backdoor attacks, particularly suitable for two-class and multi-attack scenarios. By analyzing the model's behavior on different inputs, this method can effectively identify potential backdoor triggers."}, {"title": "III. METHODOLOGY", "content": "A. Threat model\nWe assume that an attacker aims to influence the training process of a neural network, where the user receives a model from the attacker. This scenario could arise when the user downloads a pre-trained model from the internet or outsources model training to a third party, both of which are very common in reality. The attacker's goal is to generate a model with a backdoor so that it produces results significantly different from normal predictions when specific triggers are present in the input images, while concealing the existence of the backdoor and triggers. This paper describes a backdoor attack initiated solely through the model architecture, proposing a new method for covertly implanting backdoors. This method does not rely on the original model's weights and can be directly added to a well-trained model to take effect.In this section, we use a simple example based on ResNet-18 [26] to explain the design. Note that, in practice, this scheme can be injected into any architecture. The process can be divided into three stages: architecture modification, trigger generation, and backdoor activation.\nB. architecture modification\nAs shown on the left side of Figure 1, in traditional CNN models like ResNet-18 [26], data passes through an average pooling (Avgpool) layer before reaching the final classification layer, which pools the output shape to (1,1). Additionally, many models also use an adaptive average pooling (AAP) layer at this stage. We choose to launch our attack at this layer. Specifically, we add an extra connection in the network from the input data to the output of the avgpool layer, which goes through a processing function and an activation function. This setup allows the system to process the values of the original image and detect and activate the backdoor trigger, because once the image has passed through several convolutional layers, its size and pixel values are altered, making it impossible to determine whether the backdoor is present.\nIn an ideal scenario, the modified activation function adds a value that is infinitesimally close to zero to the output of the pooling layer when the trigger is not present. Then, when the original image contains the trigger, the output of the activation function changes, adding significant values to the output of the pooling layer. This error subsequently propagates through the rest of the network, ultimately altering the model's predictions.\nC. trigger generation\nTo ensure the high stealthiness of the trigger, making it completely undetectable by the naked eye or detection tools, we limit the modifications to the original pixel values to within 1. For example, if the original pixel value is 128, we will only change it to 127 or 129. This ensures that the presence of the trigger does not affect the visual appearance of the image, thereby achieving the goal of stealthiness. So, how do we determine whether to increase, decrease, or leave the original pixel value unchanged?\nIt is well known that the original range of values for RGB images is [0, 255]. When using deep learning frame-works like PyTorch [27], the input data is preprocessed, and transforms.ToTensor() is always used, which normalizes the original pixel values to a range of [0, 1] as floating-point numbers. Our plan is to perform simple addition or subtraction by 1 on the pixel values of the original image. The goal is that, when the user receives the model and dataset, the pixel values of the images will be normalized through preprocessing. After normalization, we multiply the pixel values by a certain factor and round them, ensuring that almost all pixel values in the image become either odd or even. As shown in Algorithm 1. We use this overall odd or even pattern as the trigger, ensuring its high stealthiness and making it completely undetectable by the naked eye or detection tools. As shown in Figure 2, the two images are indistinguishable from each other.\nD. backdoor activation\nAs shown on the right side of Figure 1, the entire backdoor activation process consists of three steps: data processing, backdoor activation, and output summation.\nData Processing. A copy of the input image is made, and all its values are multiplied by 10000 and rounded. Why choose 10000? Let the initial input data be x Through extensive experimentation, we have found that when the original pixel value x is normalized to 255, even if rounding (255 \u00d7 10000) does not result in an even number, rounding (255 \u00d7 10000) or (\u00d710000) will inevitably result in an even number. Multiplying by a factor less than 10000 (such as 1000 or 100) would not guarantee that adding or subtracting 1 from the original pixel value x would result in an even number. On the other hand, multiplying by a factor greater than 10000 would"}, {"title": "IV. EXPERIMENT", "content": "In this section, we conducted extensive experiments to evaluate the effectiveness and stealthiness of the trigger in our scheme. All the following experiments were tested using the ResNet-18 [26] model and the CIFAR-10 [28], SVHN [29] and GTSRB [30] datasets.\nA. Effectiveness Test\nFirst, we will evaluate the effectiveness of the scheme by measuring the accuracy on the CIFAR-10 [28], SVHN [29] and GTSRB [30] test sets. Specifically, we will apply the trained model with the added backdoor activation layer to the CIFAR-10 [28], SVHN [29] and GTSRB [30] test sets. We will gradually add triggers to images of different classes through multiple experiments to observe the specific impact of the backdoor activation layer and triggers on the test results. The accuracy of the model will be used as the sole evaluation metric to observe the effects of adding triggers to images of different classes on a pre-trained model.\nAs shown in Table 1, the model's accuracy gradually decreases as the number of classes with injected triggers increases, indicating that our backdoor attack is highly suc-cessful.\nB. BadNets\nHere, we will verify that the trigger is only effective for the backdoor activation function and not for other types of backdoor attacks. To do this, we will add triggers to images in the CIFAR-10 [28], SVHN [29] and GTSRB [30] training set at a certain ratio and change the labels of the corresponding images. Then, we will train a model without the backdoor"}, {"title": "D. GradCAM", "content": "GradCAM [31] is a powerful tool designed to help un-derstand the internal workings of deep learning models. By calculating the gradient of specific class outputs relative to the feature maps of each convolutional layer, GradCAM [31] can generate heatmaps that visually highlight which regions of the features contribute most to the model's final prediction. For benign input samples, the heatmaps generated by GradCAM [31] typically focus on parts of the image closely related to the target category. For poisoned samples, the heatmaps generated by GradCAM tend to concentrate around the trigger.\nGradCAM has been applied to both backdoor images with triggers and clean images without triggers. Figure 5, Figure 6 and Figure 7 shows the heatmaps generated after applying GradCAM [31]. It can be observed that, whether for normal samples or samples containing triggers, the heatmaps gener-"}, {"title": "E. SCALE-UP", "content": "SCALE-UP [15] is an efficient black-box input-level back-door detection method designed to identify backdoor attacks in deep neural networks (DNNs). This method recognizes potential malicious inputs by analyzing the consistency of amplified predictions, and SCALE-UP [15] does not require access to the internal structure of the model, providing reli-able backdoor detection and defense capabilities in practical applications. Specifically, under the data-free setting, SCALE-UP [15] examines each suspicious sample by measuring its Scale Prediction Consistency (SPC) value. The SPC value represents the proportion of times the label of the scaled-up image matches the label of the input image. The higher the SPC value, the more likely the input is malicious. Figure 8 shows the SPC values after applying SCALE-UP [15]. We randomly selected 20 images from a certain class and tested their SPC values, and then injected triggers into the same 20 images and tested them again. It can be observed that the SPC value distributions for both normal samples and samples containing triggers are very similar. This indicates that our triggers are designed to be highly covert, making it difficult for this method to significantly differentiate between samples containing triggers and normal samples."}, {"title": "F. Neural Cleanse", "content": "Neural Cleanse [3] is a technique designed for identifying and mitigating backdoor attacks in neural networks. Neural Cleanse [3] starts by attempting to reverse-engineer the trig-gers that might have been used to poison the model. This is achieved by optimizing patterns that cause abnormal behavior in the model, such as misclassifying inputs into attacker-chosen target classes. Once potential triggers are identified, Neural Cleanse [3] evaluates the extent of their impact on model predictions. This involves measuring changes in output probabilities with and without the presence of the triggers. After evaluating the triggers, mitigation measures can be taken. These may include retraining the model with uncontaminated datasets, fine-tuning the model to diminish the influence of the triggers, or applying defensive distillation techniques to make the model more resilient to adversarial examples.\nNeural Cleanse [3] posits that triggers corresponding to at-tacked target labels are significantly smaller than other triggers and uses an anomaly index to ascertain their authenticity. Any trigger with an anomaly index greater than 2 is considered genuine. Conversely, if no trigger has an anomaly index greater than 2, the model is deemed benign, free from backdoors. Figure 9 illustrates the obtained anomaly indices, showing that despite injecting backdoors into different label categories, Neural Cleanse [3] failed to detect actual malicious triggers in any of the label categories."}, {"title": "V. EXTENSIONS", "content": "In this section, we primarily focus on addressing the chal-lenges of standardization in image preprocessing. It is well known that images undergo various preprocessing steps before being used for model training, including normalization, stan-dardization, rotation, cropping, MAB [4] operates within an RGB range of [-1, 1], essentially performing standardization with a mean and standard deviation both set to 0.5. Most people apply multiple preprocessing methods during training, but only perform normalization and standardization when testing or deploying the model.\nThe solution provided can perfectly handle scenarios where only normalization is applied. However, adding standardiza-tion after normalization may cause the solution to fail, as the calculation method for standardization is formula $\\frac{x-mean}{std}$"}, {"title": "VI. CONCLUSION", "content": "In this work, we propose a novel backdoor attack method that entirely relies on model architecture and features ex-tremely hidden triggers. We demonstrate how this backdoor operates: unlike other backdoor attacks, our architecture-targeted attack can be directly applied to a pre-trained model without the need for retraining, and the triggers are highly concealed, undetectable by both human eyes and testing tools."}]}