{"title": "Efficient Deep Learning Infrastructures for Embedded Computing Systems: A Comprehensive Survey and Future Envision", "authors": ["XIANGZHONG LUO", "DI LIU", "HAO KONG", "SHUO HUAI", "HUI CHEN", "GUOCHU XIONG", "WEICHEN LIU"], "abstract": "Deep neural networks (DNNs) have recently achieved impressive success across a wide range of real-world vision and language processing tasks, spanning from image classification to many other downstream vision tasks, such as object detection, tracking, and segmentation. However, previous well-established DNNs, despite being able to maintain superior accuracy, have also been evolving to be deeper and wider and thus inevitably necessitate prohibitive computational resources for both training and inference. This trend further enlarges the computational gap between computation-intensive DNNs and resource-constrained embedded computing systems, making it challenging to deploy powerful DNNs upon real-world embedded computing systems towards ubiquitous embedded intelligence. To alleviate the above computational gap and enable ubiquitous embedded intelligence, we, in this survey, focus on discussing recent efficient deep learning infrastructures for embedded computing systems, spanning from training to inference, from manual to automated, from convolutional neural networks to transformers, from transformers to vision transformers, from vision models to large language models, from software to hardware, and from algorithms to applications. Specifically, we discuss recent efficient deep learning infrastructures for embedded computing systems from the lens of (1) efficient manual network design for embedded computing systems, (2) efficient automated network design for embedded computing systems, (3) efficient network compression for embedded computing systems, (4) efficient on-device learning for embedded computing systems, (5) efficient large language models for embedded computing systems, (6) efficient deep learning software and hardware for embedded computing systems, and (7) efficient intelligent applications for embedded computing systems. Furthermore, we also envision promising future directions and trends, which have the potential to deliver more ubiquitous embedded intelligence. We believe this survey has its merits and can shed light on future research, which can largely benefit researchers to quickly and smoothly get started in this emerging field.", "sections": [{"title": "1 INTRODUCTION", "content": "With the increasing availability of large-scale datasets and advanced computing paradigms, deep neural networks (DNNs)\u00b9 have empowered a wide range of intelligent applications and have demonstrated strong performance [1-3]. These intelligent applications may span from image classification [2] to downstream vision tasks, such as object detection [4], tracking [5], and segmentation [6], to natural language processing (NLP) tasks, such as automatic speech recognition [7], machine translation [8], and question answering [9]. In the subsequent years, deep neural networks have been evolving deeper and deeper with more and more layers in order to maintain state-of-the-art accuracy on target task [1-3]. In the meantime, novel network structures and advanced training techniques have also emerged, which further push forward the attainable accuracy [10-12]. These powerful deep learning (DL) networks and advanced training techniques, starting from VGGNet [1] and ResNet [2], mark the emergence of the deep learning era.\nThe tremendous breakthroughs of DNNs have subsequently attracted a huge amount of attention from both academia and industry to deploy powerful DNNs upon real-world embedded computing systems, including mobile phones [13, 14], autonomous vehicles [15, 16], and healthcare [17, 18], to enable intelligent embedded applications towards embedded intelligence [19]. In practice, this may bring significant benefits. For example, embedded computing systems explicitly allow real-time on-device data processing, which significantly improves the processing efficiency and thus delivers enhanced user experience. This also protects data security and privacy since everything can be locally processed without being uploaded to the remote server [19]. Despite the above promising benefits, deploying powerful DNNs upon real-world embedded computing systems still suffers from several critical limitations. On the one hand, in order to maintain competitive accuracy, recent representative networks have been evolving deeper and deeper with hundreds of layers [2, 3], and as a result, lead to prohibitive computational complexity [19, 20]. For example, ResNet50 [2], as one of the most representative deep networks, consists of over 4 billion floating-point operations (FLOPs) and 25 million parameters, which requires over 87 MB on-device storage to deal with one single input image. On the other hand, real-world embedded computing systems like mobile phones and autonomous vehicles typically feature limited available computational resources in order to optimize the on-device power and energy consumption. In sight of the above, the evolving network complexity continues to enlarge the computational gap between computation-intensive deep neural networks and resource-constrained embedded computing systems [20], inevitably making it increasingly challenging to embrace ubiquitous embedded intelligence.\nTo bridge the aforementioned computational gap towards ubiquitous embedded intelligence, a plethora of model compression techniques have been recently proposed, including network pruning [21-23], network quantization [24-26], and network distillation [11, 27, 28], which strive for better"}, {"title": "2 MANUAL NETWORK DESIGN FOR EMBEDDED COMPUTING SYSTEMS", "content": "The tremendous success of DNNs highly relies on the prohibitive network complexity, leading to the computational gap between computation-intensive DNNs and resource-constrained embedded computing systems [20]. To bridge the above computational gap, one of the most representative solutions is to design computation-efficient DNNs to accommodate the limited computational resources on embedded computing systems. To this end, we, in this section, systematically discuss recent state-of-the-art efficient manual networks. For better understanding, we divide these efficient networks into two main categories and sub-sections, including efficient convolutional networks"}, {"title": "2.1 Manual Convolutional Neural Network Design", "content": "As shown in previous state-of-the-art deep convolutional networks, such as AlexNet [76], VGGNet [1], GoogleNet [77], ResNet [2], DenseNet [3], and EfficientNets [78, 79], despite being able to push forward the attainable accuracy on ImageNet [80] from 57.2% [81] to 87.3% [79], the network complexity has increased over time. We note that the convolutional network consists of convolutional layers, pooling layers, and fully-connected layers, where most of the network complexity comes from convolutional layers [82]. For example, in ResNet50 [2], more than 99% floating-point operations (FLOPs) are from convolutional layers. In sight of this, designing efficient convolutional layers is critical to innovating computation-efficient convolutional networks. In practice, there are five typical efficient convolutional layers, including pointwise convolution, groupwise convolution, depthwise convolution, dilated convolution, and Ghost convolution:\n\u2022 Pointwise Convolution. Pointwise convolution is a type of convolutional layer with the fixed kernel size of 1 \u00d7 1, which performs an element-wise multiplication and addition along the depth dimension. On the one hand, compared with the standard K\u00d7K convolutional layer, the pointwise convolutional layer is able to reduce the number of FLOPs and parameters by $K^2$ times, which therefore significantly improves the efficiency. On the other hand, we note that the output from the pointwise convolutional layer typically has the same spatial dimensions as the input but may have a different number of channels. As such, the pointwise convolutional layer can be used to adjust the intermediate feature maps in terms of the number of channels. Specifically, it can reduce or increase the number of channels, making it a practical technique for compressing or expanding convolutional networks.\n\u2022 Groupwise Convolution. Groupwise convolution is a type of convolutional layer that (1) divides the input feature map into G groups along the depth dimension, (2) performs convolution in terms of each group, respectively, and (3) concatenates the outputs along the depth dimension to derive the final output. For example, given an input feature map with the size of B\u00d7C\u00d7H\u00d7W, each kernel in the K\u00d7K groupwise convolutional layer is with the size of (C/G)\u00d7K\u00d7K, which convolves the above G groups of feature maps, respectively. Therefore, compared with the standard K \u00d7 K convolutional layer, the groupwise convolutional layer is able to reduce the number of FLOPs and parameters by G times.\n\u2022 Depthwise Convolution. Depthwise convolution is a type of convolutional layer that has gained popularity due to its ability to significantly reduce the number of FLOPs and"}, {"title": "2.2 Manual Transformer Design", "content": "2.2.1 Transformer for NLP. In parallel to convolutional networks, transformer [90] is another well-established branch of DNNs, which exploits multi-head self-attention mechanisms. In practice, transformer is first designed and applied to natural language processing (NLP) tasks, where it has achieved tremendous success. For example, BERT [91], as one of the most representative transformers in the field of NLP, is able to achieve state-of-the-art performance across 11 downstream NLP tasks, such as language translation, question answering, language generation, etc., at the moment of BERT being proposed. Furthermore, GPT-3 [48], also known as Generative Pre-trained Transformer 3, pioneers to scale up and pre-train a massive transformer that consists of 175 billion parameters on 45 TB compressed plaintext data, which unlocks even stronger performance across almost all downstream NLP tasks, and more importantly, without requiring fine-tuning on specific NLP tasks. More recently, GPT-4 [49] has been proposed by OpenAI, which can significantly outperform GPT-3 across a wide range of language processing tasks and has also been widely integrated into various real-world language processing tasks, such as ChatGPT [92], to provide intelligent language processing services. These early transformer-based deep networks, thanks to their prohibitive computational complexity, have been pushing forward the boundaries of various language processing tasks and dominating recent advances in the field of NLP (see Fig. 4).\nNonetheless, it is quite challenging to deploy powerful transformers on embedded computing systems due to the computational gap between computation-intensive transformers and computation-limited embedded computing systems. For example, as pointed out in [93], to translate a short sentence with only 30 words, a typical transformer model needs to execute 13 G FLOPs, which takes 20 seconds on a Raspberry Pi device. This significantly hinders the user experience in real-world embedded scenarios. To tackle this issue, a series of computation-efficient transformers have emerged, among which TinyBERT [94], MobileBERT [95], DistilBERT [96], Linformer [97], and Reformer [98] are some of the most representative ones. The main intuition behind these efficient transformers is to resolve the memory bottleneck and increase the parallelism, making it possible to deploy NLP workloads on resource-constrained embedded computing systems. Note that, compared with computer vision tasks like image classification and object detection, running NLP workloads on embedded computing systems is less common due to the high inference latency. For example, as demonstrated in [93], running language translation workloads with hardware-tailored transformers on a Raspberry Pi device takes even seconds, whereas running image classification workloads typically takes milliseconds per image. More recently, inspired by the remarkable success of GPTs [48, 49], transformer-based large language models have become increasingly popular in the NLP community. To optimize the efficiency of transformer-based large language models, a plethora of efficient transformer-based large language models have been proposed, which typically focus on improving the training efficiency [99\u2013101], the inference efficiency [102, 103], and the fine-tuning efficiency [104, 105] of transformers in the context of large language models. For example, to optimize the inference efficiency of transformer-based large language models, [102] partitions large language models over different hardware chips in order to fit weights and activation tensors into memory and run computation and memory workloads within the given latency constraint,"}, {"title": "2.2.2 Transformer for Vision", "content": "Inspired by the tremendous success of transformer in the field of NLP, researchers have recently applied transformer to vision tasks, which achieves surprisingly strong performance (see Fig. 4). This opens up a new direction and further challenges the dominant role of convolutional networks in vision tasks. Specifically, DETR [106] and Vision Transformer (ViT) [107] are the very early transformers in vision tasks, among which ViT is the most representative one. These early pioneers have motivated a myriad of subsequent transformers in various vision tasks, such as image classification [107-109], object detection [110\u2013112], semantic segmentation [113-116], and video analysis [117-119]. For example, ViT is first proposed in June 2020, which has since gained over 20,000 citations as shown in Google Scholar. In particular, the main intuition behind ViT is surprisingly simple and straightforward, which (1) splits the input image into a series of fixed-size patches, (2) linearly embeds each of them, and (3) feeds the resulting sequence of vectors into the standard transformer encoder as illustrated in Fig. 5. However, there is no free lunch. The surprisingly strong performance of ViT and its variants comes at the cost of prohibitive computational complexity, which significantly hinders the practical deployments of ViT and its variants on embedded computing systems with limited computational resources.\nTo resolve the complexity bottleneck, some recent works have pioneered to design computation-efficient transformers for vision tasks, with the aim of reducing the computational complexity while maintaining competitive accuracy. The representative computation-efficient transformers in vision tasks include LeViT [120], MobileFormer [121], MobileViTs [122-124], EfficientViT [125], EdgeViT [126], EdgeNeXt [127], CastlingViT [128], and FastViT [129]. The above computation-efficient vision transformers are summarized and compared in Fig. 6.\nLeViT [120] is a hybrid vision transformer built on top of convolutional networks, which aims to improve the trade-off between accuracy and efficiency. To this end, LeViT introduces several enhancements to shrink down the network size, including (1) a multi-stage transformer architecture that uses attention mechanisms as down-sampling, (2) a computation-efficient patch descriptor that shrinks down the number of features in the early layers, (3) a per-head translation-invariant attention bias that replaces ViT's positional embeddings, and (4) an efficient MLP-based attention"}, {"title": "2.3 Future Envision", "content": "In this section, we envision the future trends and possible directions of manual network design, including convolutional networks and transformers, which are summarized as follows:\n(1) Hardware-Aware Optimization. The trend in the field of network design is to reduce the number of FLOPs. However, the number of FLOPs only represents the theoretical complexity and the reduction in the number of FLOPs does not necessarily lead to the inference speedup on target hardware [122, 123, 126, 130, 131]. For example, PiT [132] has \u00d73 fewer FLOPs than DeiT [133], but both have similar inference latency on iPhone 12 (i.e., DeiT vs. PiT on iPhone 12: 10.99 ms vs. 10.56 ms) [122]. In parallel, the attention mechanisms are powerful"}, {"title": "3 AUTOMATED NETWORK DESIGN FOR EMBEDDED COMPUTING SYSTEMS", "content": "In contrast to manual network design, automated network design, also known as neural architecture search (NAS) [137], has recently flourished, which strives to automate the design of efficient neural networks. In the past decade, NAS has achieved impressive performance in the field of network design, which delivers more advanced networks with both higher accuracy and efficiency than the conventional manual network design (see Section 2). To this end, we, in this section, further discuss recent advances in the field of NAS, especially from the perspective of hardware-aware NAS that searches for hardware-efficient network solutions, including modular search space in Section 3.1, search strategy in Section 3.2, and speedup techniques and extensions in Section 3.3."}, {"title": "3.1 Modular Search Space", "content": "The search space A plays a prominent role in the success of NAS since the search engine of NAS strives to search for top-performing architecture candidates within the pre-defined search space. This also indicates that the search space determines the upper-performance limit of modern NAS algorithms. However, designing efficient and effective search spaces is quite difficult and challenging since there are a myriad of possible operator candidates (e.g., 1 \u00d7 1, 3 \u00d7 3, 5 \u00d7 5, and 7 \u00d7 7 convolutional layers) and different network configurations (e.g., the combination strategies of different operator candidates and the network channel layouts) [137, 138, 144]. Therefore, to reduce the search space size and trim down the search complexity, previous state-of-the-art NAS methods [137, 138, 144] often restrict the search space to allow efficient search and leverage modular search spaces, which are coarse-grained in contrast to layer-wise fine-grained search spaces. In practice, previous state-of-the-art NAS methods are based on the following two representative types of modular search spaces, including cell-based search space and block-based search space."}, {"title": "3.2 Search Strategy", "content": "In this section, we discuss recent state-of-the-art NAS algorithms and divide them into three main categories, including reinforcement learning-based search [137], evolutionary algorithm-based search [157], and gradient-based search (also known as differentiable search) [138].\nReinforcement Learning-Based Search. In the field of NAS, [137] is the first NAS work\u00b3 that opens up the possibility to automate the design of top-performing DNNs, which features reinforcement learning (RL) [159] as the search engine. Specifically, [137] leverages a simple yet effective recurrent neural network (RNN) as the RL controller to generate possible architecture candidates from the search space as shown in Fig. 9. The generated architecture candidate is then trained from scratch on target task to evaluate the accuracy. And next, the accuracy of the generated architecture candidate is fed back into the aforementioned RNN controller, which optimizes the RNN controller to generate better architecture candidates in the next iteration. Once the search process terminates, the well-optimized RNN controller is able to provide DNNs with superior accuracy on target task. For example, the network generated by the RNN controller achieves 96.35% top-1 accuracy on CIFAR-10, which is comparable to or even better than the family of manually designed DNNs, such as ResNet [2]. The promising performance of [137] marks an important milestone in the field of NAS, pioneering an effective alternative to automate the design of competitive DNNs. Subsequently, based on [137], NASNet [144] introduces the flexible cell-based search space as shown in Fig. 7, which further boosts the attainable accuracy on target task. For example, NASNet achieves 97.6% top-1 accuracy on CIFAR-10, which is +1.25% higher than [137] while involving fewer parameters (i.e., 37.4 M in [137] vs. 27.6 M in NASNet). Despite the promising performance, [137] and NASNet have to train a large number of possible architecture candidates from scratch, thus inevitably necessitating prohibitive computational resources. For example, to optimize the RNN controller, [137] needs to train 12,800 stand-alone architecture candidates. To overcome such"}, {"title": "3.3 Speedup Techniques and Extensions", "content": "In this section, we further discuss recent state-of-the-art advances in general speedup techniques and extensions for NAS algorithms, including one-shot NAS enhancements, efficient latency prediction, efficient accuracy prediction, low-cost proxies, zero-cost proxies, efficient transformer search, efficient domain-specific search, and mainstream NAS benchmarks, which have the potential to significantly benefit NAS algorithms and largely facilitate the search process.\nBeyond One-Shot NAS. Despite the high search efficiency, one-shot NAS often suffers from poor ranking correlation between one-shot search and stand-alone training. As pointed out in [205], one-shot search results do not necessarily correlate with stand-alone training results across various search experiments. To overcome such limitations, a plethora of one-shot NAS enhancements have been recently proposed [206\u2013211]. Specifically, [206-210] turn back to few-shot NAS. In contrast to one-shot NAS [160] that only features one supernet, few-shot NAS further introduces multiple supernets to explore different regions of the pre-defined search space, which slightly increases the search cost over one-shot NAS but can deliver much more reliable search results. For example, as shown in [206], with only up to 7 supernets, few-shot NAS can establish new"}, {"title": "3.4 Future Envision", "content": "In this section, we further envision several promising future trends and possible directions in the field of automated network design, which are summarized as follows:\n(1) General Search Spaces. The success of NAS highly relies on the well-engineered search space, such as the cell-based search space [137, 144, 160] and the block-based search space [38-40]. In the past, researchers manually design the search spaces using heuristic-based strategies, which are typically based on existing state-of-the-art networks, such as MobileNets [123, 268] and ShuffleNets [34, 35]. This effectively restricts the search space to improve the search efficiency and delivers competitive architecture candidates with promising accuracy and efficiency. In the meantime, this, however, may significantly limit the search performance, which may reject more competitive architecture candidates outside the well-engineered search space. To overcome such limitations, [283, 284] pioneer to design more general search spaces than the cell-based and block-based search spaces, which, unfortunately, are under-explored since [283, 284] still suffer from human biases. Therefore, one promising future direction in the field of NAS is to innovate and explore more general search spaces to unleash the power of automated network design.\n(2) Fully Automated Architecture Search. The early NAS practices either focus on searching for the optimal architecture candidate [137, 144, 160], the optimal data augmentation [280, 285], the optimal activation function [286, 287], or the optimal training recipe [288, 289]. As demonstrated in FBNetV3 [288] and AutoHAS [289], different architecture candidates may prefer different training recipes, in which jointly searching for the optimal architecture candidate and its tailored training recipe has the potential to push forward the attainable accuracy. This observation can be easily generalized. For example, different architecture candidates may prefer different data augmentations. Therefore, one promising future direction in the field of NAS is fully automated search, which jointly searches for the optimal architecture candidate and its tailored data augmentation, activation function, and training recipe in one single search experiment to maximize the attainable accuracy.\n(3) Multi-Task Architecture Search. Previous NAS works typically focus on searching for task-specific architecture candidates that can achieve promising performance in the specified task, such as image classification [138, 146], object detection [268\u2013270], and semantic segmentation [271-273]. This search paradigm, however, significantly increases the total search cost when the number of tasks exponentially evolves since we have to conduct search experiments for each task, respectively. To alleviate this issue, FBNetV5 [290] takes the first step to search for multi-task architecture candidates which can achieve competitive performance across multiple tasks, including image classification on ImageNet [80], object detection on COCO [291], and semantic segmentation on ADE20K [292]. Nonetheless, this is far from enough since we have a large number of tasks in real-world scenarios. Therefore, one promising future direction in the field of NAS is multi-task search, which automates"}, {"title": "4 NETWORK COMPRESSION FOR EMBEDDED COMPUTING SYSTEMS", "content": "In addition to designing novel networks, another alternative is to compress existing networks at hand, either manually designed networks or automatically searched networks, to reduce the network redundancy, which therefore leads to network variants with better accuracy-efficiency trade-offs. As illustrated in previous relevant literature [82, 302], there are three popular branches of network compression techniques, including network pruning, network quantization, and network distillation. Note that these three branches are parallel with each other as shown in [303], which indicates that they can be combined to further enable better accuracy-efficiency trade-offs. Among them, network pruning and network quantization focus on improving the accuracy-efficiency trade-off from the efficiency perspective, whereas network distillation enhances the accuracy-efficiency trade-off from the accuracy perspective. To this end, we, in this section, systematically discuss recent state-of-the-art network compression techniques. For better understanding, we divide these network compression techniques into three main categories and sub-sections, including network pruning in Section 4.1, network quantization in Section 4.2, and network distillation in Section 4.3, since these network compression techniques feature different algorithms to improve the accuracy-efficiency trade-off from different perspectives. Note that these network compression techniques can typically generalize across different networks (e.g., convolutional networks and transformers). For example, we can leverage knowledge distillation to enhance the training process of both convolutional networks and transformers towards better training accuracy."}, {"title": "4.1 Network Pruning", "content": "The rationale behind network pruning is that DNNs are usually over-parameterized and redundant in terms of network weights and channels [304, 305]. As such, eliminating redundant network weights and channels can largely benefit the network efficiency at the cost of minimal accuracy loss, thus being able to accommodate limited available computational resources and rigorous storage requirements in real-world embedded scenarios. Following previous well-established pruning conventions, we divide recent state-of-the-art pruning methods into two main categories according to their pruning granularity, in which non-structured pruning (i.e., weight pruning) is fine-grained whereas structured pruning (i.e., channel pruning and layer pruning) is coarse-grained. As illustrated in Fig. 17, weight pruning focuses on removing the redundant weight connections, whereas channel"}, {"title": "4.1.1 Non-Structured Pruning", "content": "Non-structured pruning, also referred to as weight pruning, removes the less important network weights, which is typically more fine-grained than structured pruning as illustrated in Fig. 17. In particular, applying weight pruning for network compression can be traced back to the early 1990s. For example, Optimal Brain Damage [307] and Optimal Brain Surgeon [308], as the very early weight pruning approaches, pioneer to investigate the efficacy of weight pruning on vanilla fully-connected networks, in which the less important network weights are removed based on Hessian of the loss function. More recently, [29] proposes a simple yet effective weight pruning technique to compress deep convolutional networks, such as AlexNet [76] and VGGNet [1], instead of vanilla fully-connected networks. Specifically, [29] observes that the network weights with smaller magnitudes typically contribute less to the network accuracy, based on which [29] removes the less important network weights with smaller magnitudes. Subsequently, this weight pruning technique is further integrated into Deep Compression [89] to obtain highly compressed networks, making it possible to aggressively reduce the network size without sacrificing the network accuracy. For example, Deep Compression is able to significantly reduce the network size of VGGNet by \u00d749 times, from 552 MB to 11.3 MB, while maintaining comparable accuracy on ImageNet. Nonetheless, the reduction in terms of the network size cannot directly translate into the speedup on target hardware since the resulting compressed networks are of high irregular network sparsity. To overcome such limitations, EIE [306] designs an efficient specialized inference engine to maximize the inference efficiency of compressed networks. In parallel, [309] proposes an efficient data-free weight pruning approach to iteratively remove redundant network weights. Besides, [310] and [311] leverage Variational Dropout and $L_0$-norm regularization-based stochastic gates, respectively, to remove the less important network weights."}, {"title": "4.2 Network Quantization", "content": "Different from network pruning that aims to reduce the network complexity at the structure level, network quantization instead focuses on representing the network weights and activations with lower bits, which is able to significantly reduce the network complexity at the precision level. Therefore, the resulting quantized network maintains the same network structure (i.e., the same number of layers and channels), but with lower-bit network weights and activations. In practice, network quantization can be traced back to the 1990s, when early quantization works pioneer to quantize the network weights for Boltzmann machines [447], optical networks [448], and multi-layer perceptrons (MLPs) [449]. Note that quantization has the potential to significantly trim down the network size to accommodate the limited storage in real-world embedded scenarios. For example, Deep Compression [89] is able to reduce the network size of VGGNet by \u00d749 times, from 552 MB to 11.3 MB, while delivering comparable accuracy on ImageNet [80]. Thanks to its surprisingly strong performance in reducing the computational complexity and alleviating the storage requirements, renewed research interest in network quantization has emerged since the 2010s [409], which demonstrates that, compared with full-precision weights (i.e., 32 bits), 8-bit quantized weights can effectively accelerate the network inference on mainstream CPUs without significant accuracy degradation. To this end, we, in this section, discuss recent advances in the field of network quantization, including representative quantized networks and popular quantization-related extensions/implementations, which are also summarized in Fig. 21."}, {"title": "4.2.1 Quantized Networks", "content": "Below we introduce several representative quantized networks, including binarized networks, ternarized networks, INT8 networks, and mixed-precision networks.\nBinarized Networks. Binarized networks are built upon only 1-bit weights, which are constrained to be either +1 or \u22121 during forward and backward propagations [24, 25]. This can effectively eliminate computation-intensive multiply-accumulate operations and allows to replace multiply-accumulate operations with cheap additions and subtractions, and as a result, can lead to significant performance improvement in terms of latency and energy consumption as demonstrated in [24]. In the relevant literature, BinaryConnect [24] and BinaryNet [25] are the very early seminal binarized networks, which pioneer to investigate the efficacy of 1-bit weights in order to reduce the computational complexity and alleviate the storage bottleneck. Specifically, BinaryConnect [24] introduces the first binarized network, which explores both deterministic binarization:\n$w_b = sign(w) = \\begin{cases} +1, & \\text{if } w > T\\\\ -1, & \\text{otherwise} \\end{cases}$       (14)\nand stochastic binarization to stochastically binarize the network weights:\n$w_o = \\begin{cases} +1, & \\text{with probability } p = \\sigma(w)\\\\ -1, & \\text{with probability } p = 1 - \\sigma(w) \\end{cases}$               (15)\nwhere $\u03c3(\u00b7)$ is the hard sigmoid function and can be mathematically formulated as follows:\n$\\sigma(x) = clip(\\frac{x+1}{2}, 0, 1) = max(x, min(1, \\frac{x+1}{2}))$         (16)\nNote that BinaryConnect exploits the above hard sigmoid function rather than the soft version because it is far less computationally expensive and can still yield competitive results. As shown in BinaryConnect, the stochastic binarization is more advanced and can achieve much better quantization accuracy than the deterministic counterpart. So far, BinaryConnect only enables weight-level binarization, whereas the network inputs are still required to be full-precision. In sight of this, BinaryNet [25] extends BinaryConnect to support both binarized weights and binarized inputs in order to maximize the inference efficiency of binarized networks. Furthermore, XNOR-Net [26] demonstrates that [24, 25] cannot be generalized to large-scale datasets like ImageNet. To address this, XNOR-Net introduces an effective approach to estimate binarized weights to maintain $w \u2248 \u03b1 \u00b7 w_b$, after which the estimated \u03b1 can be detached from the binarized weight to rescale the input. To further enhance the binarization accuracy, a plethora of binarized networks [394\u2013400] have been subsequently proposed. For example, [399, 400] propose learnable activation binarizer [399] and adaptive binary sets [400] to explore more accurate binarized networks.\nTernarized Networks. In addition to binarized networks, ternarized networks [401, 402] are another representative branch of quantized networks and have gained increasing popularity, thanks to their superior accuracy. Specifically, ternarized networks quantize the network weights from 32 bits to 2 bits, in which the 2-bit weights are constrained to \u22121, 0, and +1 in contrast to \u00b11 in binarized networks. As such, ternarized networks can achieve much better accuracy than binarized networks at the cost of slightly increased computational complexity. To achieve this, [401], as the first ternarized network, proposes to quantize the full-precision weights as follows:\n$w_t = \\begin{cases} +1, & \\text{if } w > \\Delta\\\\ 0, & \\text{if } |w| \\le \\Delta\\\\ -1, & \\text{if } w < -\\Delta \\end{cases}$           (17)\nwhere \u0394 is a positive constant to control the ternarization threshold. To derive the optimal ternar-ization threshold \u2206*, [401] turns back to XNOR-Net [26] and borrows the binarization estimating"}, {"title": "4.2.2 Quantization Extensions and Implementations", "content": "Below we introduce several quantization extensions and implementations, including quantization-aware training, automated mixed-precision quantization, and quantization-aware hardware accelerators.\nQuantization-Aware Training. Quantization-aware training refers to the technique that trains quantized networks, which is fundamentally different from post-training quantization as shown in Fig. 22. Note that post-training quantization can achieve satisfactory performance on early networks like AlexNet [76] and VGGNet [1], which, however, suffers from significant accuracy loss when applied to more advanced lightweight networks like MobileNets [32, 85] and ShuffleNets [34, 35]. In general, quantization-aware training incorporates the quantization loss into the training loss, which then allows the optimizer to minimize the quantization loss during the training process in order to unlock better quantization accuracy than post-training quantization. In practice, [414], as the seminal quantization-aware training work, proposes to quantize both weights and activations with 8-bit integers. To maximize the accuracy of INT8 quantized networks, [414] also introduces an effective tailored quantization-aware training approach to train the resulting INT8 quantized networks. Similar to [414], [422, 423] unify and improve quantization-aware training of INT8 quantized networks to minimize the accuracy degradation. To generalize quantization-aware training to train other types of quantized networks (e.g., 1-bit and 2-bit networks), a plethora of quantization-aware training works [424-427] have been subsequently proposed, which further push forward the attainable quantization accuracy."}, {"title": "4.3 Network Distillation", "content": "Network distillation, also referred to as knowledge distillation"}, {"title": "4.3.1 Data-Dependent Knowledge Distillation", "content": "In this section, we introduce several representative data-dependent knowledge distillation techniques, including logits-based knowledge distillation, intermediate layers-based knowledge distillation, multi-teacher knowledge distillation, teacher-free knowledge distillation, and privileged knowledge distillation"}]}