{"title": "DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG", "authors": ["Jinyoung Kim", "Dayoon Ko", "Gunhee Kim"], "abstract": "In the rapidly evolving landscape of language, resolving new linguistic expressions in continuously updating knowledge bases remains a formidable challenge. This challenge becomes critical in retrieval-augmented generation (RAG) with knowledge bases, as emerging expressions hinder the retrieval of relevant documents, leading to generator hallucinations. To address this issue, we introduce a novel task aimed at resolving emerging mentions to dynamic entities and present DYNAMICER benchmark. Our benchmark includes dynamic entity mention resolution and entity-centric knowledge-intensive QA task, evaluating entity linking and RAG model's adaptability to new expressions, respectively. We discovered that current entity linking models struggle to link these new expressions to entities. Therefore, we propose a temporal segmented clustering method with continual adaptation, effectively managing the temporal dynamics of evolving entities and emerging mentions. Extensive experiments demonstrate that our method outperforms existing baselines, enhancing RAG model performance on QA task with resolved mentions.", "sections": [{"title": "Introduction", "content": "In the real world, large amounts of textual information are constantly being generated at an incredible rate. The dynamic nature of human language, characterized by the continuous emergence of new expressions, presents multiple significant challenges (Hirschberg and Manning, 2015). The way we refer to named entities changes over time, influenced by the shifts that include the use of metaphors, adoption of slang, creation of euphemisms, and other linguistic evolutions (Li et al., 2020). For example, consider the named entity \"Elon Musk\u201d. Over time, various mentions are used to refer to him, such as \"the Tesla CEO\u201d, or \u201cthe tech billionaire\u201d. Emerging slang or metaphoric expressions like \u201creal-life Iron Man\" or \"Mars man\u201d also appear. As a dynamic entity, his attributes change over time as he was initially known as \"PayPal co-founder\", later as \"Hyperloop visionary\u201d, and more recently as \"Twitter owner\u201d. All of these expressions refer to the same entity at different time points, yet a system must be able to recognize them despite the dynamic linguistic landscape. Therefore, it is crucial for a system to resolve these new expressions, accurately linking them to evolving entities in a continuously updating knowledge base (KB)."}, {"title": "Related Work", "content": "Entity Linking. Entity linking (Hoffart et al., 2011; Guo and Barbosa, 2018) aims to match an entity mention to a unique named entity in a KB such as Wikipedia pages. There has been much research on how to correctly link varied mentions of the same entity. For instance, Andy et al. (2017) design an algorithm to identify entities from social media during the 3-4 hour Grammy Awards, constructing an alias list for short-term use. Botzer et al. (2021) collect a Reddit entity linking dataset, demonstrating that models trained on conventional text encounter difficulties with the unique formats and lexical variations prevalent in social media. In the biomedical domain, Mohan and Li (2018) propose the MedMentions dataset, which compiles a comprehensive biomedical corpus with entity mention annotations. However, the static nature of this domain fails to accommodate the time-evolving linguistic evolution. The zero-shot setting of entity linking (Lin et al., 2017; Logeswaran et al., 2019) targets linking entities unseen in training time. This task focuses on domain adaptation to resolve new entities, rather than on handling the mention variation of specific entities over time.\nFor temporal entity linking, the TempEL dataset offers detailed tracking and annotation of changes in existing entities as well as the emergence of new entities across multiple temporal snapshots. Our annotation is similar to TempEL in that we handle temporal development of existing entities. However, our dataset also considers the continuous emergence of new mentions of the same entity, highlighting that the way it is referenced varies across multiple points in time.\nCoreference Resolution. Coreference resolution (CR) (Pradhan et al., 2012; Webster et al., 2018) evaluates a model's ability to match entities with their antecedents. The task is to group all spans that point to the same objects in a context by detecting mentions. Lee et al. (2017) integrates these two processes in an end-to-end manner by considering all spans as potential coreference candidates and learning a conditional probability distribution for clustering. Joshi et al. (2020) extends BERT by training via masked contiguous random spans and predicting the spans using boundary representation.\nCross-document CR (CDCR) (Cybulska and Vossen, 2014; Webster et al., 2018) resolves coreference across multiple documents. Caciularu et al. (2021) utilizes long-range transformers to encode multiple related documents. Allaway et al. (2021) sequentially adds each mention to cluster candidates while incrementally updating the coreference candidate cluster representation. Our dynamic entity linking seems similar to CDCR in that both link the mentions referring to the same entity over documents. However, our task is different since it aims to link varied mentions to the evolving entities of a continuously updating KB.\nRAG with Dynamic Corpus. Since continuously retraining LLMs with up-to-date data is demanding, RAG has been employed to handle temporal adaptability. For instance, Liska et al. (2022) and Kasai et al. (2024) respectively propose dynamic QA tasks with time-stamped and newly published news articles, demonstrating that retrieving up-to-date documents can improve generation results. Moreover, Dhingra et al. (2022) and Margatina et al. (2023) introduce a cloze query to evaluate the acquisition of temporal knowledge. Neelam et al. (2022) proposes Knowledge Base QA (KBQA) tasks to evaluate the ability of temporal reasoning. Recently, Ko et al. (2024) introduces dynamically evolving open-domain QA and dialogue benchmarks along with a novel training-free retrieval-interactive LLM framework. While existing works focus on the temporal adaptability of models for retrieval and reasoning, they do not specifically address the dynamic nature of entity expressions, which is crucial for applications requiring precise entity linking over time."}, {"title": "The DYNAMICER Dataset", "content": "DYNAMICER consists of two tasks: an entity linking task and an entity-centric QA task. The entity linking task focuses on resolving emerging expressions that appear over time to entities in a KB, while the entity-centric QA task evaluates RAG models in answering entity-specific questions. We construct DYNAMICER through a pipeline, whose key idea at each stage is to first generate automatically using LLMs, and then thoroughly verify the quality with human review: (1) selecting and filtering textual corpora (\u00a7 3.1), (2) identifying mentions of target entities (\u00a7 3.2), (3) annotating the appropriate entity for each mention (\u00a7 3.3), and (4) generating QA pairs using resolved entities (\u00a7 3.4). The prompt templates for dataset generation are provided in Appendix F."}, {"title": "Corpora Collection", "content": "Post Selection. We choose the sports domain to capture the mention variations of famous athletes, teams, and coaches, given its inherently dynamic nature. This domain is particularly suitable since it features diverse naming conventions, such as nicknames and abbreviations for entities. Moreover, frequent updates and news about events like matches and player transfers contribute to its dynamic nature. The sports domain is also event-driven with clear temporal markers like seasons and tournaments.\nWe target soccer and baseball for corpora selection. Specifically, for soccer, we select the top 15 teams according to Forbes World's Most Valuable Soccer Teams\u00b9 and leagues to which these teams belong. For baseball, we select 30 teams from Major League Baseball. Using the /tagged method in Tumblr API, we download all posts tagged with our selected hashtags, focusing on posts from 2023-05-01 to 2024-04-30. The full list of hashtags can be found in Appendix C.\nInitial Filtering. We filter the posts with fewer than 50 or more than 3000 characters to exclude content that is either too brief to provide sufficient contextual meaning or too lengthy to potentially divert attention with extraneous information. Additionally, we use the FastText module (Joulin et al., 2016a,b) to ensure text is written in English, filtering out the posts that are not confidently identified as English."}, {"title": "Mention Identification", "content": "We first identify expressions referring to named entities using the GPT-4 turbo (Achiam et al., 2023). The prompt we use is as follows: \u2018Please identify all expressions in the given text that explicitly name or describe a player, coach, or team. This includes direct names, nicknames, and any role-specific references (like positions or accolades) that refer to a particular individual or team.' We refrain from using typical named entity recognition (NER) models since they struggle to identify long expressions,"}, {"title": "Entity Annotation", "content": "Once expressions are mined by GPT-4, we use a substring matcher to highlight each expression in order, followed by human verification. We employ a dedicated team of workers to annotate the data. Please refer to Ethics Statement for the details. We instruct the annotators to adjust the offset of the expression if the highlighting is incorrect and to additionally highlight any missing expressions that refer to players, coaches, or teams. Next, the annotators link each expression to the corresponding Wikipedia entity. They search Wikipedia for a suitable entity and submit a valid URL of the Wikipedia page to the system. If a valid Wikipedia entity cannot be found, or if the context from the post is too ambiguous to resolve the expressions, annotators label it as NOT VALID, which is then further filtered out."}, {"title": "Entity-Centric QA Pairs", "content": "Based on the previous annotation, we create entity-centric QA pairs, which require entity resolution to provide accurate answers. Our basic idea is to replace each entity name in the questions with its various mentions. However, this can introduce ambiguity, as some mentions may not clearly identify the entity without additional context. For example, mentions like The Bronx Bombers are unambiguous and can be identified as NEW YORK YANKEES without context, whereas mentions like the winning team are not explicit without context. Hence, before creating QA pairs, we filter out ambiguous cases that are checked by the prompt to GPT-4: 'Select the mentions from the list that unambiguously refer to {entity} without context.'. We further perform human verification for the remaining mentions.\nFinally, we use the Wikipedia description of each entity to generate the knowledge-intensive QA pairs. To evaluate temporal challenges, we use Wikipedia articles from the revision that corresponds to the time when the mentions first appeared. With the description, we prompt GPT-4: \u2018Below is the description of {entity}. Please generate a question-answer pair regarding {entity}. The entity name itself should be included.'. We instruct GPT-4 to enclose the entity within the bracket in the question text. Once QA pairs are generated, we replace the bracketed entity name in each question with its varied mention. The generated QA pairs are then subjected to human validation to ensure the accuracy of the question-answer pair. The details of generating QA pairs are provided in Appendix D."}, {"title": "Approach", "content": "Resolving new expressions based solely on the document where they first appear can be challenging due to their low lexical similarity to the entity name and the ambiguity of context. Thus, it can be advantageous to consider multiple documents that share similar contexts and expressions to resolve these mentions jointly. Previous works have studied this joint clustering approach (Ganea and Hofmann, 2017; Le and Titov, 2018; Angell et al., 2021; Agarwal et al., 2022), but they assume a static scenario and resolve the mentions without considering the time dimension. In our problem, on the other hand, entities evolve over time, with changes in definitions or attributes such as status, role, affiliation, or characteristics. As exemplified in Figure 1, it is hard to find the coreference between \"The Angels' superstar\u201d and \u201cThe Dodgers' number 17\" for Shohei Ohtani. Instead, clustering mentions that appear at similar time steps could be feasible since they share events or contexts at similar time steps. Therefore, we propose a temporal segmented clustering approach with continuous adaptation (TempCCA), as shown in Figure 2. Our approach follows the joint clustering methods as prior works but continuously clusters the emerging mentions at each time step and further utilizes the cluster representation to resolve mentions in the next time step."}, {"title": "Dual Encoder Clustering", "content": "We follow the dual encoder clustering approach from Agarwal et al. (2022). We construct a weighted graph G where the nodes represent the combined set of entities & and mentions M. We then cluster these nodes based on the affinity between each pair of nodes. The weight of each edge is defined by affinity functions, & and ; the former measures affinity between an entity and a mention and the latter is between mentions. For $e \\in E$ and $m_i, m_j \\in M$, we define the weight $W_{e,m_i} = -\\phi(e, m_i)$ and $W_{m_i,m_j} = -\\psi(m_i, m_j)$. Each affinity function is formulated by the inner product of corresponding node embeddings:\n$\\phi(e, m_i) = u_C(e)^\\mathsf{T} u_M(m_i),$\n$\\psi(m_i, m_j) = u_M(m_i)^\\mathsf{T} u_M(m_j)$\nwhere $u_C(e)$ denotes the embedding of entity cluster formulated in the last previous time step, and $u_M(m_i)$ denotes the mention representation. The difference between Agarwal et al. (2022) and our work is that we formulate an entity cluster embedding, rather than using the pure output from the entity encoder.\nFor the entity encoder, the input tokens are structured as follows: [CLS] en [NAME] ed [SEP], where en is the name of entity, and ed is the description of entity. We adopt a special token [NAME] to separate the name and description of the entity. For the mention encoder, the input tokens are structured as follows: [CLS] $c_l$ [START] mi [END] $c_r$ [SEP], where $c_l$, $c_r$ refer to the context to the left and right of the mention mi within the document. We adopt special tokens [START], and [END] to indicate the mention span. The mention representation is defined by the output of the mention encoder, regardless of the time step."}, {"title": "Continuous Training", "content": "At the initial time step, each entity forms a single cluster. The representation of a single cluster is simply defined by the output of the entity encoder. Using the obtained representations at the initial time step, we train the affinity function and their affiliated encoders. We adopt an arborescence-based clustering approach (Agarwal et al., 2022). Training objectives, including positive and negative sampling, are shown in Appendix E.\nAt each subsequent time step, we utilize the most recent previously resolved mentions to form an entity cluster representation:\n$u_C(e) = \\alpha \\text{Enc}_E(e) + (1-\\alpha)\\frac{1}{|C(e)|} \\sum_{m_i \\in C(e)} \\text{Enc}_M(m_i)$,\nwhere $\\text{Enc}_E(e)$ ($\\text{Enc}_M(m_i)$) denotes the output of the entity (mention) encoder for the input token from entity e (mention mi). $C(e)$ represents mentions linked to entity e in the previous time step. If the previous time step is the training phase, we use gold linking. If it is the test phase, we formulate $C(e)$ with predicted linking. The hyperparameter $\\alpha$ is set to optimize the affinity models."}, {"title": "Experiments", "content": "We investigate the following research questions.\n1. How well does our method resolve emerging mentions compared to existing entity linking and coreference methods?\n2. Can resolving new mentions assist the RAG model in a knowledge-intensive task?\n3. In which cases does this resolution contribute to its generative capabilities?"}, {"title": "Experimental Setup", "content": "Entity Linking Task\nBaselines. We use the following models as baselines: (i) SpEL (Shavarani and Sarkar, 2023): the structured prediction entity linking approach, achieving the state-of-the-art performance on the AIDA-CoNLL Dataset (Hoffart et al., 2011), (ii) c-SpEL: continuously trained SpEL over each time segment, (iii) ArboEL (Agarwal et al., 2022): the state-of-the-art model on MedMentions (Mohan and Li, 2018), and (iv) TempCCA: our temporal segment clustering approach with continuous adaptation. We use the dual-encoder setting from ArboEL for fair evaluation."}, {"title": "Entity-Centric QA", "content": "Baselines. We use four types of baselines for the entity-centric QA: (i) LLM (e.g., Llama3-8B-Instruct), (ii) LLM-ER: LLM with the top-1 entity linking prediction, (iii) RaLM (Ram et al., 2023): LLM with concatenated top-k retrievals, (iv) RaLM-CoT: RaLM with a prompt similar to zero-shot Chain-of-Thought (Kojima et al., 2022), where we first ask the LLM to resolve the mention in the question to an entity and then answer the question, and (v) RaLM-ER: RaLM with the top-1 entity linking prediction. We use the E5 (Wang et al., 2022) as the retriever and Llama3-8B-Instruct as the generator (see Llama3 Documentation), which are state-of-the-art models.\nFor LLM-ER and RaLM-ER, we utilize TempCCA to perform entity linking to resolve target mentions in question and then provide the top-1 entity prediction in the LLM's prompt. To provide TempCCA's top-1 entity linking prediction in the prompt for LLM-ER and RaLM-ER, we insert a sentence 'The {mention} may also be referred to as {top-1 entity prediction}.' right before the question. Additionally, we provide the LLM with top-3 retrievals for all RAG baselines using the format 'Context: {concatenated retrievals}' in the beginning. The exact format for each baseline can be found in the Appendix A.\nRAG. Embedding all Wikipedia documents in the database requires significant computation, so we randomly select 100K articles, including the articles used for dataset collection. We create separate databases for each genre. For soccer, we select articles linked to Category:Association football, while for baseball, we select from Baseball, Basketball, and American football to ensure enough articles. We parse each article using the LangChain document loader (see LangChain Documentation), and index the documents using FAISS (Johnson et al., 2019) following Shi et al. (2023). We chunk the documents with a maximum of 1500 characters, ensuring a 10-character overlap between chunks.\nMetrics. To evaluate generated answers, we use the F1 score following Petroni et al. (2021). Since most answers in our QA dataset are either a noun phrase or a short sentence, we only consider the first sentence of each answer. We parse this first sentence using the nltk sentence tokenizer.2"}, {"title": "Experimental Results", "content": "We present the performance of our entity linking and entity-centric QA tasks in the soccer genre. Additionally, the performance in the baseball genre is reported in Appendix B."}, {"title": "Results of Entity Linking", "content": "Table 3 presents the accuracy of our entity linking task in the soccer genre. To rigorously evaluate the performance of each method in resolving mentions across different levels of lexical similarity, we present the results for each bin of lexical similarity separately. For each mention, we calculate the Jaccard similarity (Zhang et al., 2021) with the entity name using a character-level token as a straightforward measure of lexical overlap. The number of mentions for each bin is presented in Table 4.\nThe results reveal a clear trend: as Jaccard similarity increases, the accuracy of all baselines improves. This underscores the importance of lexical similarity in entity linking tasks, where emerging mentions with lower lexical similarity typically lead to less accurate linking of mentions.\nc-SpEL surpasses SpEL in 1112, but falls below in 0102 and 0304 in total. This implies that the performance of c-SpEL deteriorates as it moves further from the last time point of learning. Besides, TempCCA consistently outperforms other baselines in most cases, except for Set 4 (0.6 - 0.8), when ArboEL exceeds TempCCA from 0.11 to 0.4. Nevertheless, TempCCA shows a robust performance across all months and sets. Notably, TempCCA surpasses the baselines the most in Set 1, from 1.42 in 0102 to 8.27 in 1112. This implies that utilizing the recently predicted mentions can help jointly resolve mentions with low lexical overlap, as there tend to be similar mentions within a similar time step."}, {"title": "Results of Entity-Centric QA", "content": "Table 5 presents the performance of our entity-centric QA task in the soccer genre. The accuracy of TempCCA's top-1 entity linking prediction on QA questions attains 66.62, 67.62, and 65.86 for 1112, 0102, and 0304, respectively. Compared to the base LLM, LLM-ER improves performance by an average of 3, indicating that resolving new mentions helps the LLM generate more accurate responses. Still, both LLM and LLM-ER underperform the RAG baselines. Specifically, RaLM improves the LLM's performance by an average of 15 using the retrieved documents. Interestingly, RaLM-CoT performs significantly worse than RaLM by an average of 6.8, suggesting that entity prediction by the LLM itself does not effectively contribute to QA accuracy. On the other hand, RaLM-ER improves RaLM by an average of 1.4 and outperforms all other baselines, indicating that resolving mentions can be beneficial to knowledge-intensive QA tasks.\nTo analyze the results comprehensively, we report the results separately for cases of retrieval success (retrieval hit) and failure (retrieval miss). As shown in the middle section of Table 5, RAG baselines significantly enhance LLM performance; RaLM improves the base LLM by 30.73, 28.37, and 27.98 in 1112, 0102, and 0304, respectively. Additionally, RaLM-ER performs on par with or slightly better than RaLM when the retrieval succeeds, despite potential errors in entity resolution. Conversely, when retrieval fails, RaLM performs worse than the base LLM, with a decrease of 3.16 in 1112. This highlights the critical impact of retrieval failures. On the other hand, LLM-ER enhances the base LLM in all cases. Furthermore, RaLM-ER mitigates hallucinations, improving RaLM by approximately 2.06, 3.58, and 2.02 in 1112, 0102, and 0304, respectively. Remarkably, RaLM-ER even outperforms all baselines despite incorrect retrievals in 0304.\nTo further pinpoint the improvement, we analyze the results of RaLM and RaLM-ER in four scenarios: when entity resolution is correct or incorrect, within the context of both retrieval hits and misses. Table 6 shows the averaged results across time segments. When retrieval is successful, the performance of RaLM-ER improves by 1.2 when the entity resolution is correct; however, it drops approximately 1.0 when the entity resolution is wrong. Conversely, when retrieval fails, performance is enhanced in both correct and incorrect resolution cases. RaLM-ER improves upon RaLM by 2.3 when the resolution is correct and by 1.8 when the resolution is incorrect. Although it seems crucial to avoid introducing incorrect resolutions, providing entity resolution results to the LLM generally improves end-to-end performance."}, {"title": "Conclusion", "content": "In this work, we addressed the challenge of resolving new linguistic expressions in the dynamic and ever-evolving landscape of human language. We introduced DYNAMICER to evaluate the ability of models to resolve emerging mentions. Our benchmark proposes entity linking tasks for resolving emerging mentions and entity-centric QA tasks for RAG evaluation. To address the temporal dynamics of emerging mentions, we proposed a temporal segmented clustering method with continual adaptation. Our exhaustive experiments demonstrated that our method surpassed existing baselines in resolving new expressions, particularly when there is less lexical overlap.\nFuture work may extend to updating KB using entity resolution, which can directly handle the retrieval failures caused by mention dynamics. Through DYNAMICER, we provide a resource for the research community to further explore and improve dynamic entity resolution. We hope this fosters further research towards developing more robust models capable of handling the continuous emergence of new expressions."}, {"title": "Limitations", "content": "We acknowledge several limitations in our work. Firstly, our dataset and method are primarily designed to handle variations in single entity mentions and may not effectively address cases where multiple entities are combined into a single mention, such as \"Kimye\" referring to Kanye West and Kim Kardashian. Future research could explore developing benchmarks and models capable of resolving such combined mentions into multiple entities. Additionally, our dataset may reflect biases introduced by the GPT-4 model and the specific prompts used during its creation. Although we perform thorough human validation and revisions, future studies could benefit from employing a diverse set of language models to mitigate these potential biases."}, {"title": "Ethics Statement", "content": "Safety. All data were sourced from Tumblr, which is publicly available. To ensure the safety of our dataset, we conduct a two-stage filtering process. Initially, annotators were instructed to report any potentially harmful or privacy-invading content. Following this, the authors reviewed the remaining content to further filter out inappropriate materials. Despite these efforts, biases such as stereotyping may still be present due to the nature of real communication on Tumblr, where a significant portion of the user base consists of teenagers and young adults.\nIntended Use. The DYNAMICER dataset is intended to be used for research purposes only, and the use is subject to Tumblr Terms of Service and Community Guidelines.\nAnnotator Compensation. We hired university students as annotators. To uphold ethical standards, we compensated our annotators with a fair hourly wage of approximately USD $15. The estimated completion time for each task was determined through multiple preliminary trials conducted by our research team. Consequently, the average expense per datapoint amounted to approximately $0.30. Data points requiring additional time were compensated at a proportionately higher rate to ensure fairness. The full text of instructions is provided in Figure 16."}]}