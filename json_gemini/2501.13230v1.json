{"title": "LET SSMS BE CONVNets: StaTE-SPACE MODELING WITH OPTIMAL TENSOR CONTRACTIONS", "authors": ["Yan Ru Pei"], "abstract": "We introduce Centaurus, a class of networks composed of generalized state-space model (SSM) blocks, where the SSM operations can be treated as tensor contractions during training. The optimal order of tensor contractions can then be systematically determined for every SSM block to maximize training efficiency. This allows more flexibility in designing SSM blocks beyond the depthwise-separable configuration commonly implemented. The new design choices will take inspiration from classical convolutional blocks including group convolutions, full convolutions, and bottleneck blocks. We architect the Centaurus network with a mixture of these blocks, to balance between network size and performance, as well as memory and computational efficiency during both training and inference. We show that this heterogeneous network design outperforms its homogeneous counterparts in raw audio processing tasks including keyword spotting, speech denoising, and automatic speech recognition (ASR). For ASR, Centaurus is the first network with competitive performance that can be made fully state-space based, without using any nonlinear recurrence (LSTMs), explicit convolutions (CNNs), or (surrogate) attention mechanism.", "sections": [{"title": "INTRODUCTION", "content": "Sequence or temporal modeling encompasses a wide range of tasks from audio processing to language modeling. Traditionally, there have been many (related) statistical methods employed (Box et al., 2015). In the age of deep learning, neural networks have been predominantly used (LeCun et al., 2015), including recurrent neural networks (RNNs), convolutional neural networks (CNNs), transformers (Vaswani, 2017), and neural ODEs (Chen et al., 2018). In many cases, the model will inevitably suffer from one of two drawbacks: 1) cannot be efficiently trained (or fitted) in parallel due to the sequential nature of the model, 2) cannot be efficiently configured for online inference due to its large memory and computational requirement. To address this, deep state-space models (SSMs) were adapted for sequence modeling, and have shown incredible potential across a wide range of tasks (Gu et al., 2021; Goel et al., 2022; Gu & Dao, 2023). Due to the linearity of the SSM layers, they can not only be configured for efficient online inference with small memory and computational resources, but also configured for efficient training using parallel hardware with unrolling strategies (Gu et al., 2022; Smith et al., 2022; Dao & Gu, 2024; Heinsen, 2023).\nCurrently, most deep SSM networks (along with most neural networks in general) follow the architectural recipe of transformers, where they are composed of uniform \"SSM blocks\" throughout the network, containing little to no variations in the shapes of the intermediate features or weights. This simplifies the designs of deep SSM networks, but may sacrifice performance and efficiency in practice. To explore the opposite direction, we go \"back to the future\" to classical CNN designs instead, where a much more heterogeneous design principle is followed. More specifically, as we go deeper in the network, we will gradually downsample the temporal dimension, increase the channel dimension, and increase connective sparsity (Tan & Le, 2021), to balance between size and efficiency (Pei & Coenen, 2024). In order to allow for the possibility of such a heterogeneous design, we make two novel contributions in this work. First, we express SSM blocks as tensor networks (or with einsum expressions), where we can easily observe the connective structure of existing SSM blocks as being mostly depthwise-separable. This motivates us to design new SSM blocks using this"}, {"title": "RELATED WORK", "content": "tensor network formalism, giving us connective structures such as full and bottleneck SSM blocks, as inspired by classical CNN blocks. Then, we optimize the contraction order of each SSM block dynamically based on the type of the SSM block, the shape of the input features, and the shapes of the SSM system matrices. This enables significant speedups during training for all SSM blocks (both old and new ones). We name this class of efficient CNN-like networks Centaurus\u00b9.\n2.1 DEEP STATE-SPACE MODELING\nThe seminal work proposing a memory encoding using orthogonal Legendre polynomials in a recurrent state-space model is the Legendre Memory Unit (LMU) (Voelker et al., 2019), where Legendre polynomials (a special case of Jacobi polynomials) are used. The HiPPO formalism (Gu et al., 2020) then generalized this to other common orthogonal functions. Later, this sparked a cornucopia of works interfacing with deep state-space models including S4 (Gu et al., 2021), H3 (Fu et al., 2022), and Mamba (Gu & Dao, 2023), achieving impressive results on a wide range of tasks from audio generation Goel et al. (2022) to language modeling. Besides a few recent exceptions (Smith et al., 2022; Dao & Gu, 2024), These networks mostly use an underlying depthwise structure, which may limit the network capacity, albeit reducing the compute requirement of the network. An important focal point of this work is to generalize such SSM models to enable more connective flexibility, such that design choices of classical convolutional blocks can be carried over to the Centaurus model.\n2.2 CLASSICAL CONVOLUTIONAL BLOCKS\nHere, we look at the variants besides the standard full convolutional layer (where every pair of input and output channels is connected). First, the simplest variant is the depthwise convolutional layer, made popular by the MobileNetV1 architecture (Howard et al., 2017). This variant connects the input and output channels one-to-one, which drastically reduces the connectivity of the architecture, hence also its parameters and computational complexity. This architecture is prevalent for both computer vision and speech domains (Kriman et al., 2020; Hannun et al., 2019). Next, a variant with moderate connectivity is the grouped convolutional layer, appearing as early as the AlexNet work (Krizhevsky et al., 2012). In this structure, the input/output channels are divided into groups, and each input-channel group is associated with an output-channel group one-to-one. The input and output channels are then only intra-connected within each group, but there are no inter-connections between groups. Finally, there is a class of convolutional blocks known as bottlenecks, typically containing a sequence of three convolutional layers. This structure is used prominently in the ResNet model (He et al., 2016), and also in MobileNetV2 (Sandler et al., 2018) onwards. Typically, the order of convolutional layers can be pointwise-depthwise-pointwise, resulting in an output feature of the same tensor shape as the input feature.\n2.3 TENSOR NETWORKS\nTensor networks were originally developed as a tool for approximating the wavefunctions of many-body quantum systems (Or\u00fas, 2019). It can be considered as an incredibly generalized form of low-rank decomposition, where high-dimensional features can be compressed as the contraction of a few low-dimensional tensors (Kolda & Bader, 2009). It has a direct relationship to the einsum expression, where each einsum operand is considered a node of the tensor network, and each contraction index is considered a (hyper)edge. The memory- and compute-optimal contraction or evaluation of a tensor network can be formulated as a congestion optimization problem on the base graph, allowing tensor networks to contest with \"quantum supremacy\" (Gray & Kourtis, 2021). The application of tensor networks to machine learning began with the seminal work of Novikov et al. (2015), where it is shown that the fully connected layers of a neural network can be exponentially"}, {"title": "BACKGROUND", "content": "compressed with minimal degradation in performance. This spawned a series of works applying tensor networks for compressing convolutional and transformer blocks. Our work extends this line of \"tensorization\" technique to deep state-space models.\n3.1 STATE SPACE MODELS\nState-space models (SSMs) are general representations of linear time-invariant (LTI) systems (Hamilton, 1994), and they can be uniquely specified by four matrices: $A \\in \\mathbb{R}^{N\\times N}, B\\in\\mathbb{R}^{N\\times H}, C\\in \\mathbb{R}^{H'\\times N}$, and $D \\in \\mathbb{R}^{H\u00b4\\times H}$. The first-order ODE describing the LTI system is given as\n$\\dot{x} = Ax + Bu$,  $y = Cx + Du$,\nwhere $u \\in \\mathbb{R}^{H}$ is the input signal, $x \\in \\mathbb{R}^{N}$ is the internal state, and $y \\in \\mathbb{R}^{H'}$ is the output. The parameters {H, H', N} denote the number of features for the input, output, and internal state respectively. Setting H = H' = 1 yields a single-input, single-output (SISO) SSMs (Gu et al., 2021; 2022), and letting H > 1, H' > 1 yields a multiple-input, multiple-output (MIMO) SSMs (Smith et al., 2022). We discretize the system using zero-order hold (ZOH), which gives us the discrete-time SSM matrices A and B as follows (Gu et al., 2022):\n$A = \\exp(\\Delta A)$,  $B = (\\Delta A)^{-1} \\cdot (\\exp(\\Delta A) \u2013 1) \\cdot \\Delta B$.\nThe discrete SSM is then given by\n$x[t+1] = Ax[t] + Bu[t]$, $y[t] = C x[t]$\nIt is then straightforward to check that the discrete-time impulse response is given as $k[\\tau] = CA^{\\tau} B$, where $\\tau$ denotes the kernel timestep. During training, k can be considered the \"full\" long 1D convolutional kernel with shape (output channels, input channels, length), in the sense that the output y can be computed via the long convolution $y_j[t] = \\sum_i(U_i *k_{ji})[t]$.\nSimilar to previous works (Gu et al., 2022), we assume A to be complex diagonal, but restrict B and C to be real projection matrices to reduce memory and computational loads. In addition, we ignore the term Du as it can be absorbed into the SSM system. Justification of these restrictions is given in Appendix A. Like previous works also, we allow the parameters {$A, B, C, \\Delta$} to be directly learnable, which indirectly trains the kernel k. Unlike previous works in deep SSMs, we do not try to keep the sizes H, H', N consistent, to allow for more flexibility in feature extraction at each layer, mirroring the flexibility in selecting channel and kernel sizes in CNNs\u00b2. The flexibility of the tensor shapes requires a careful choice of the optimal order of operations during training to minimize memory and computation, which will be the focal point of this work.\n3.2 EINSTEIN SUMMATION NOTATION\nThe Einstein summation notation (Einstein, 1922), or einsum, is a concise representation of general tensor contractions. We do not give a formal description of this notation here, but instead introduce it in the context of describing a MIMO SSM. Recall that the output of a MIMO SSM is computed by convolving the input with the impulse response, which we normally would write as\n$Y_j [t] = \\sum_i (U_i * k_{ji})[t] \\approx \\sum_i U_i * (\\sum_n B_{ni}K_{n}C_{jn} )[t]$,\nwhere we defined the basis kernels $K[\\tau] = R(A^\\tau)$. We get a rather messy expression involving three summation indices. i denotes the input channel, n denotes the internal state index, and j denotes the output channel.\nTo lessen the notation burden, we observe that the summation expression is redundant and can be inferred from the tensor indices. In particular, if an index appears on the RHS of the equation but"}, {"title": "A GENERALIZATION", "content": "not the LHS, then it must have been summed over (or contracted). This allows us to reduce the expression down to\n$Y_j [t] = (u_i * (B_{ni}K_{n}C_{jn})) [t]$,\nwhich is much better than before, and we can always assume the indices i and n to be summed over without the explicit hinting of a summation symbol. We can further simplify equation 5 by discarding convolution operator *. Naturally, we can leverage the convolution theorem, which states that the convolution operator is mapped to pointwise multiplication in the frequency domain. If we index the Fourier modes using f, then equation 5 can be expressed in the frequency domain as\n$\\hat{y}_j[f] = \\hat{u}_i[f]B_{ni}K_{n}[f]C_{jn}$ or $\\hat{y}_{jf} = \\hat{u}_{if}B_{ni}K_{nf}C_{jn}$\nwhere the hat symbol denotes the Fourier transformed features (see Appendix A for details).\n3.3 A GENERALIZATION\nLooking at $K_n$, we realize that there is only one oscillation mode per internal coefficient n, which in certain cases may limit the expressiveness of the network. A natural generalization is to expand the basis kernels as $K_{nm}$, and arrive at the following system that is more expressive:\n$\\hat{Y}_{jf} = \\hat{u}_{if}B_{ni}K_{nmf}E_{nm}C_{jn}$.\nHere, $E_{nm}$ serves as additional weighting factors for the basis kernels (which again we restrict to be real), representing the importance of each oscillation mode. The recurrent form of this system is then\n$u_n[t] = B_{ni} U_i[t]$, $X_{nm}[t+1] = A_{nm} X_{nm}[t] + U_n$, $Y_j[t] = C_{jn} E_{nm} R(x_{nm}[t])$,,\nwhich can be considered a parallel cascade of n SISO state-space systems, intra-connected by the $E_{nm}$ factors, and inter-connected by the $B_{ni}$ and $C_{jn}$ projection matrices. Alternatively, we can consider n to index a \"state block\", and m to index \"sub-states\u201d within the state block, which is an extension beyond the pure SISO and MIMO configurations. As a sidenote, this configuration is similar to the Mamba block architecture (Gu & Dao, 2023), but without the data-gating mechanism and the gated MLP structure.\nLike Mamba, it is possible to configure the SSM matrices to be data-dependent: A(u), B(u), C(u), D(u), in which case the system becomes time-variant (Katsch, 2023; Gu & Dao, 2023; Dao & Gu, 2024). We will not place a major focus on this configuration in our work for two (temporary) reasons. Under the present algorithmic understanding, such dynamic systems require materialization of the internal states for scan operations (in Mamba 1) or require the introduction of a new sequence dimension into the tensor operands (in Mamba 2), meaning that it can restrict the flexibility of tensor contraction orders. On the more practical end, such models generally require custom kernels (in Triton or CUDA) for specialized support currently, making it difficult to build heterogeneous networks with different connective configurations. We believe however that there are no fundamental restrictions to combining our framework with the data-gating mechanism in theory, and leave it as a future direction of study to achieve this efficiently in practice."}, {"title": "SSMS AND CNNS ARE TENSOR NETWORKS", "content": "Within an SSM layer, the temporal kernels are constructed via the basis kernels, which are further generated (or parameterized) by the recurrent coefficients in A. In other words, a temporal kernel can be generated by a weighted sum of selected Fourier modes, where each mode n is associated with a complex frequency of $A_n$ and a (real) weighting factor $E_n$. A natural viewpoint is that the parameters of A and E are analogous to the parameters of convolutional filters, but not directly parameterized. For instance, if we consider A having 3 complex parameters and E having 3 real parameters, then they together may contain the same \u201cexpressivity\" as a 3 \u00d7 3 spatial filter, both representable with 9 real numbers. One may note that standard CNN spatial filters are local in space, while our temporal IIR filters are global in time. Interestingly however, by themselves, the difference between local convolutions and global convolutions is not too fundamental. This is because, by the convolution theorem, a temporal convolution is just simply a pointwise product in the frequency domain, and vice versa. Therefore, a dual viewpoint is that in the frequency domain, deep SSM models are simply pointwise operations with \u201cnon-local\" activation functions in between playing the \"frequency mixing\" role. This idea is also explored in the Hyena work (Poli et al., 2023)."}, {"title": "GENERAL SSM OPERATIONS WITH EINSUM EXPRESSIONS", "content": "In standard CNN layers, besides the canonical convolution operation, there is usually an additional interaction between the input and output channels. For instance, we have configurations such as \"depthwise convolution\u201d, \u201cgroup convolution\u201d, and \u201cfull convolution\", in increasing degree of channel connectivity. In this section, we will detail how to use the einsum expression to endow the A tensor with these channel-interaction structures. Naturally, matrices that are purely channel-mixing like B and C are akin to \"pointwise convolution\u201d layers or simply projection operations, which we will place less emphasis on.\nFrom here on, we will use {i, j, n, g, f} to index the input channel, output channel, internal state, group/head, and Fourier mode respectively. The simplest example is a \"depthwise\u201d SSM block, or $y_{if} = \\hat{u}_{if} E_{in} K_{in}$, noting the appearance of the \u201cinput channel\u201d index i in every operand and the total absence of the \"output channel\" index j. Therefore, the layer by itself will not see any interactions among the \"channel\" dimension, hence why an additional mixing matrix M is needed at the end $\\hat{y}'_{jf} = \\hat{y}_{if} M_{ji}$. In short, we have a pure sequence-mixing layer followed by a pure channel-mixing layer, forming a depthwise-separable structure, as also discussed in Gu et al. (2022).\nOn the other extreme, we have a \u201cfull\u201d SSM block, or $\\hat{y}_{jf} = \\hat{u}_{if}E_{jin}K_{jin}$, where both indices i and j appear for the basis kernels, as each input-output pairing needs to be separately parameterized. In this case, both the sequence-mixing and channel-mixing structures are \"baked\" into the full SSM block, so not additional mixing layers are necessary."}, {"title": "OPTIMAL CONTRACTION ORDERS FOR TRAINING: THE DEPTHWISE EXAMPLE", "content": "In online inference mode, we have no choice but to explicitly materialize and evolve the internal states, leaving little room for optimization of the order of operations. This is also true for other parallelization strategies where the internal states are materialized in some form (Smith et al., 2022; Heinsen, 2023). Fortunately, we have the flexibility of choosing the order of contraction if we perform the SSM operations in the frequency domain (e.g. via FFTs), and considerable speedups (at times, orders of magnitudes) can be achieved by choosing the optimal contraction order. A slight drawback of using FFT is that the time complexity is $O(L \\log L)$ instead of the desired $O(L)$.\nHowever, this is typically not a practical issue, as FFT is rarely a compute-bound operation unless the sequence length is exceedingly large or the operation is already kernel-fused (Fu et al., 2022). At this point of discussion, we will start accounting for the batch dimension as b, omitted previously for clarity of exposition. In practice, the batch size will factor into the determination of the optimal contraction order.\nTaking again the depthwise SSM layer, or $\\hat{y}_{bif} = \\hat{u}_{bif} E_{ni} K_{ni}$, intuitively we would opt to contract the E and K tensors first to \u201ccollapse\u201d down the basis kernels along the state dimension (indexed n) as such, $k_{if} = E_{ni}K_{nif}$. It then becomes much more manageable to compute the output by simply taking the product $\\hat{y}_{bif} = \\hat{u}_{bif}k_{if}$. Note that\n$F(k_{it}) = F(E_{ni}K_{nit}) = E_{ni}F(K_{nit})$\ndue to the linearity of the Fourier operator F, hence justifying the freedom of the contraction order. To optimize further, we would ideally compute $F(E_{nir}K_{nir}) = F(k_{ir})$ instead of $E_{ni}F(K_{ni\\tau})$, as the former requires performing the Fourier transform on a much smaller tensor (vector). Even in this simple example, we observe two important points: 1) the contraction order matters, 2) the placement of the Fourier operators matters.\nTo make things clearer, it is useful to think of the Fourier operator $F_{ft}$ itself as an operand to be contracted. This will allow us to write the SSM operations as\n$Y_{bit'} = F(F_{ft}u_{bit}) (F_{fr}E_{ni} K_{nit})$.\nHere, we represented the Fourier operator as a matrix (i.e. the DFT matrix), but unlike standard matrix multiplication, we know that multiplying/contracting with a DFT matrix can be done via FFT, so that it will not incur the same computational complexity as the standard tensor contraction operation. Correctly accounting for the FFT complexity is important in determining the optimal order of contractions, which in this picture also includes the placement of the FFT operators.\nThis is a simple example where the optimal contraction is somewhat obvious and static, but there are cases where we have more terms to contract and the differences between the contraction paths are more subtle. And in these cases, the optimal contraction order is dynamically linked with the shapes of the tensor operands. Fortunately, there is a systematic way to evaluate the memory and compute requirement of an einsum contraction path (used prominently in packages such as opt-einsum) (Daniel et al., 2018), and we make a simple augmentation to this prescription to handle \u201ccontractions\" involving FFT operators, while being somewhat mindful of the software and hardware back-ends (PyTorch and CUDA)."}, {"title": "A PRACTICAL WALKTHROUGH: THE BOTTLENECK BLOCK", "content": "The main example that we will walk through here is the bottleneck layer example, or $\\hat{y}_{bif} = \\hat{u}_{bi f} B_{ni} E_{nmi} K_{nmf}C_{jn}$, where 5 tensor operands are involved. If we include the Fourier operators as 3 additional operands, then we have a total of 8 operands, which yields a sufficiently complex design where the systematic optimization of contraction orders will show its power. \nIn theory, it is possible to have opt einsum handle optimizing all the contraction orders, and the pseudocode would appear much simpler (i.e. we only need one einsum expression in line 43 of Listing 1). However, there are certain SW and HW idiosyncrasies that make it more efficient to explicitly \"force\" certain contraction patterns. For instance: 1) complex tensors are not yet \u201cnatively\u201d supported by CUDA, meaning that it is more efficient to operate on real tensors as much as possible. In other words, we perform computations in the complex frequency domain only if necessary; 2) torch.compile may not yet be able to identify kernel fusion opportunities within a sufficiently complex torch.einsum expression. Therefore, it may be more efficient to explicitly modularize and kernel-fuse certain contraction steps, particularly those that are memory-bound.\nWe try to achieve a happy medium between full automation with einsum expressions and full specialization with custom CUDA kernels. In other words, we \"semi-manually\" inspect all possible contraction paths and discard the clearly non-optimal ones. Then, we perform some amount of practical optimization for the \u201cfeasible\u201d contraction paths, as we will explore in the following sections. Initially, for ease of exposition, we will continue our discussion treating the temporal domain"}, {"title": "GENERAL SSM OPERATIONS WITH EINSUM EXPRESSIONS", "content": "and frequency domain as almost equivalent, as the two can be easily traversed via FFTs (which is fairly light in compute). Only at the very end will we make the effort to separate the two domains as we begin to consider the optimal \u201cinsertion points\" of the FFT operations, as the second-order optimization.\nFirst, based on visual inspection of the bottleneck tensor network representation, we can clearly see the first step should be to always \"contract away\" the inner edge m, or equivalently compute the kernel $k_{\\eta\\tau} = E_{nm}K_{nm\\tau}$ first. However, we note that the basis kernels $K_{nm\\tau}$ themselves are generated by $A_n$ and $A_{nm}$. Therefore, under kernel fusion, it is possible to not even materialize the basis kernels $K_{nm\\tau}$ in the GPU VRAM at all, which shaves off memory and computational requirements considerably during training. This is encapsulated in the get_kernel function in Listing 1, which is meant to be (jit-)compiled during training, for example with torch.compile or custom triton kernels. In the frequency domain, we are now left with the expression $\\hat{y}_{bif} = \\hat{u}_{bif}B_{ni}k_{nf}C_{jn}$, which still allows for many possible contraction paths in theory. However, heuristically speaking, we should try not to produce any intermediate tensor of dimension greater than 3, as it will be unfavorable to materialize and operate on it. Lemma 1 will heavily restrict the \"feasible\" contraction paths based on this criteria, a full proof of which is given in Appendix B.\nLemma 1. Given the einsum expression $\\hat{y}_{bif} = \\hat{u}_{bif}B_{ni}k_{nf}C_{jn}$ arranged in this order, all intermediate tensors will have at most 3 dimensions, if and only if $\\hat{u}$ (and intermediate tensors resulting from it) is contracted with only its neighboring operands.\nRemark. An interpretation of Lemma 1 is that the contraction order should roughly follow the \"natural order of operations\" of the underlying state-space system, in some form of \u201cassociative\u201d fashion. The proof sketch is to try contracting $\\hat{u}$ with its non-adjacent operands such as k or C, and observe the appearance of high dimensional intermediate tensors, for instance, $\\hat{u}_{bif}k_{nf} = (\\hat{u}k)_{binf}$"}, {"title": "BOTTLENECK SSM CONTRACTION ORDERS", "content": "Recall that Lemma. 1 states that: Given the einsum expression $\\hat{y}_{bif} = \\hat{u}_{bif} B_{ni}k_{nf}C_{jn}$ arranged in this order, all intermediate tensors will have at most 3 dimensions, if and only if $\\hat{u}$ (and intermediate tensors resulting from it) is contracted with only its neighboring operands.\nProof. If we contract $\\hat{u}_{bif}$ with $k_{nf}$, this immediately results in a 4D tensor $(\\hat{u}k)_{binf}$. Similarly, if we contract $\\hat{u}_{bif}$ with $C_{jn}$, this immediately results in a 5D tensor $(\\hat{u}C)_{bjinf}$. This means that we can only contract $\\hat{u}_{bif}$ with $B_{ni}$ first (its only neighboring operand), resulting in the 3D tensor $(\\hat{u}B)_{bnf}$.\nAt the second stage, if we contract $(\\hat{u}B)_{bnf}$ with $C_{jn}$, this will result in a 4D tensor $(\\hat{u}BC)_{bjnf}$. This means that we can only contract $(\\hat{u}B)$ with $k_{nf}$ (again its only neighboring operand), resulting in the 3D tensor $(\\hat{u}Bk)_{bnf}$.\nFor the remaining contraction paths, we have to verify that any contraction paths within $B_{ni}k_{nf}C_{jn}$ (not including $\\hat{u}$) will only result in intermediate tensors of dimension at most 3: $B_{ni}k_{nf} = (Bk)_{nif}, k_{nf}C_{jn} = (kC)_{jnf}$, and $B_{ni}C_{jn} = (BC)_{jni}$.\nAn interpretation of Lemma 1 is that the contraction order should roughly follow the \u201cnatural order of operations\" of the underlying state-space system, in some form of \u201cassociative\u201d fashion. For example, it clearly makes little sense to contract the input u directly with the output project matrix C before even passing the input through the internal states first, and this is formally reflected as the production of a 5D intermediate tensor. Note that this is not to say that the contractions of an einsum expression should be restricted to neighboring operands. In fact, einsum expressions are agnostic to the ordering of operands, meaning that contractions can be performed on any two operands at any stage. The discussion here is only specific to the state-space system $\\hat{y}_{if} = \\hat{u}_{bif}B_{ni}k_{nf}C_{jn}$, for"}, {"title": "ESTIMATION OF PARAMETERS AND FLOPS", "content": "It is very tempting to train network variants that are feasible by optimal contractions, but eventually turn out to be incredibly memory or computationally expensive during inference. An example would be to aggressively use \u201cfull\u201d SSM blocks with large channels and internal states, which will result in the internal states of size (out channels, input channels, states) needing to be maintained and updated during inference. Even though the memory and computational bottlenecks can be mitigated during training via optimal contractions, this is not possible during online inference time if none of the internal states can be \u201ccontracted away\u201d11. Therefore, following classical designs of lightweight convolutional networks (Howard et al., 2017; Sandler et al., 2018), we use full SSM blocks sparingly only in the first few layers where the channel dimensions are still small, and we will mostly be using (pointwise) bottleneck blocks in the deeper layers where the channel dimensions become larger.\nNote that the number of parameters and FLOPs differ between online inference and training. For inference parameters, certain learnable parameters can be absorbed into the system matrices, such as the A parameter. For inference computations, the number of FLOPs is always linear with respect to the sequence length; however, this is often outweighed by the suboptimal order of operations imposed by the explicit materialization of internal states. In Table. 3, we provide the parameter count and the FLOPs per recurrent step when various SSM block variants are configured for online inference. There are a couple of points to note:\n\u2022 During online inference, the internal states need to be explicitly maintained and updated, meaning that the order of operation is always first the input projection, internal states update, output projection, and an optional mixer layer (for S4D).\n\u2022 The internal states are maintained as complex tensors, but we only take the real parts for the output projection. Note that a complex weight is doubled the parameter count of the real counterpart, and a complex multiplication is 6 times the FLOPs of the real counterpart (4 multiplications + 2 additions). In addition, updating the internal state with the projected real input requires 1 FLOP, and 2 FLOPs if the project input is also complex.\n\u2022 In the case where the projection matrices (e.g. B and C) are also complex, a complex dot product (used in matrix multiplications) is performed and will incur additionally 2 extra FLOPs per accumulation, resulting in a total of 8 FLOPs per matrix element. However, if we only need the real part of the projected outputs (or analogously only having real inputs), the projection operation will incur half the number of FLOPs compared to the full complex projection, or 4 FLOPs per matrix element.\n\u2022 We do not count peripheral parameters and FLOPs for biases or affine transformations in normalization layers, as they are negligible compared to the SSM operations.\nIn Centaurus, we restrict all model parameters except for the state transition matrix A to be real. If we make the projection matrices complex, we can then recover the original S4D and S5 implementations as DWS and PW bottleneck blocks respectively. Following the original implementations, we still restrict the inputs and outputs of the SSM layers to be real. Besides using complex projection matrices, there are some additional idiosyncratic differences:\n\u2022 The original S4D layer uses a standard where a complex internal state is considered to be two states, whereas S5 and Centaurus do not and simply consider it as one full state.\n\u2022 The original S4D layer uses the GLU activation, meaning that the pre-activations hence the C matrix will have double the size compared to Centaurus.\nNote that if a direct residual connection is added to the SSM block, there will only be a trivial amount of FLOPs added equaling $H'$. In the case where the residual connection is a projection operation (necessary if $H \\neq H'$), then there will be $HH'$ additional parameters added and roughly $2HH'$ additional FLOPs. Residual projections will be used in our Centaurus networks for keyword spotting and automatic speech recognition."}, {"title": "EXPERIMENT DETAILS", "content": "Unless otherwise mentioned, all of our network variants are trained with:\n\u2022 AdamW optimizer with the PyTorch default configs"}]}