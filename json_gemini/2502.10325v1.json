{"title": "Process Reward Models for LLM Agents: Practical Framework and Directions", "authors": ["Sanjiban Choudhury"], "abstract": "We introduce Agent Process Reward Models (AgentPRM), a simple and scalable framework for training LLM agents to continually improve through interactions. AgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo rollouts to compute reward targets and optimize policies. It requires minimal modifications to existing RLHF pipelines, making it easy to integrate at scale. Beyond AgentPRM, we propose InversePRM, which learns process rewards directly from demonstrations without explicit outcome supervision. We also explore key challenges and opportunities, including exploration, process reward shaping, and model-predictive reasoning. We evaluate on ALFWorld benchmark, show that small 3B models trained with AgentPRM and InversePRM outperform strong GPT-40 baselines, and analyze test-time scaling, reward hacking, and more. Our code is available at: https://github.com/sanjibanc/agent_prm.", "sections": [{"title": "1 Introduction", "content": "Large language model (LLM) agents excel in decision-making tasks such as web navigation [1], robotics [2], and interactive code generation [3]. However, they rely heavily on prompting [4, 5] or supervised fine-tuning (SFT [6]. Prompting demands extensive manual effort [1, 7] and does not enable autonomous improvement. SFT, while effective, is constrained by demonstration quality and lacks mechanisms for self-correction at test time.\nThis raises a fundamental question: How can LLM agents improve through interaction without extensive human supervision? Reinforcement learning (RL) naturally enables policy refinement through experience, but applying RL to LLM agents presents key challenges: (1) Long-horizon decision-making: LLM agents must reason over multiple steps, producing structured multi-token outputs that blend reasoning and actions. (2) Sparse rewards: Feedback is often delayed until the end of long interactions, complicating credit assignment. While large-scale RL approaches have been explored [8], they remain impractical due to high sample complexity.\nInstead of large-scale RL, we propose a more tractable alternative: Agent Process Reward Models (AgentPRM). PRMs provide fine-grained supervision at each step, akin to critic [9] or value functions in RL. By evaluating intermediate actions rather than relying on sparse outcome rewards, PRMs improve sample efficiency. While PRMs have been explored in multi-step reasoning tasks [10-12], they are underexplored in agentic settings where actions impact an external environment. Our work addresses this gap.\nWe propose a simple and scalable framework for training AgentPRMs. It has two key aspects:\n1. Automatic PRM annotation: PRM targets are computed using asynchronous Monte Carlo rollouts, enabling agents to learn without manually labeled rewards.\n2. Iterative training: PRMs and policies are jointly trained in an iterative process, where each refines the other to improve overall performance."}, {"title": "2 Agent Process Reward Models: A Simple Framework", "content": "2.1 Formulation\nConsider an agent interacting with an environment over multiple turns to solve a task. We model this interaction as a turn-level Markov Decision Process (MDP). At turn t, the state $s_t$ is the history of observations and actions, $s_t = {o_0, a_0,..., o_{t-1}}$. The agent selects an action $a_t$ and transitions to a new state $s_{t+1}$ according to the environment dynamics. The agent receives a reward $r(s_t, a_t) \\in [0, 1]$, typically provided at terminal states and referred to as the outcome reward, which evaluates the overall success of the task. The agent's behavior is determined by a policy $\\pi(a_t | s_t)$, which maps states to a distribution over actions. The objective of the policy is to maximize the expected return, defined as the sum of discounted rewards $E_\\pi[\\sum_{t=0}^{T-1} \\gamma^t r(s_t, a_t)]$, where $\\gamma$ is the discount factor.\nFor LLM agents, each action $a_t$ consists of a sequence of tokens, encoding both reasoning and an environment action. This induces a two-level decision hierarchy:\n1. Turn-level MDP: Models the sequence of agent-environment interactions over multiple turns.\n2. Token-level MDP: Models the sequence of tokens within each turn, each token is an action.\nTypically, RLHF frameworks are single-turn and hence perform RL only at token-level MDP. We next look at how to lift these frameworks to solve turn-level MDPs.\nAgent Process Reward Models. A process reward model (PRM) [10] assigns turn-wise scores in a multi-turn response, providing structured feedback to guide policy learning. In turn-level MDPs, a PRM functions as a state-action value function, analogous to a Q-function in RL. Formally, the PRM is $Q_T^*(s_t, a_t) = E_\\pi[\\sum_{k=t}^T \\gamma^{k-t} r(s_k, a_k) | s_t, a_t]$. Maximizing PRM $Q^*(s_t, a_t)$ enables the policy to improve task performance through intermediate feedback rather than relying on outcome rewards.\nDistinction from Reasoning Tasks. PRMs have primarily been studied in multi-step math reasoning tasks [10, 16] where transitions are deterministic and known. In these settings, test-time search methods like beam search [17] can be used to optimize reasoning sequences. In contrast, LLM agents operate in external environments with unknown, stochastic transitions, where actions have uncertain effects. This makes beam search impractical, as future states cannot be enumerated in advance. We focus on training PRMs and policies under these complex settings.\n2.2 Approach\nWe adopt a policy iteration framework to jointly train the process reward model $Q^\\pi(s, a)$ and the agent policy $\\pi(a|s)$. Algorithm 1 describes the three-stage process:\n1. Rolling out the current policy $\\pi_\\theta$ to collect data and compute Q targets\n2. Train the PRM $Q_\\phi(s, a)$ given Q targets (standard RLHF)\n3. Train the policy $\\pi_\\theta$ using reinforcement learning on the trained RM (standard RLHF)\nThis follows standard RLHF pipelines, with the key difference being Stage 1, where PRM targets are computed from rollouts rather than preference labels. We describe each stage below.\nStage 1: Rollout and Compute Target. At iteration i, we roll out the policy $\\pi_{i-1}$ in the environment to generate trajectories of states, actions, and rewards $D_{rollout} = {(s_0, a_0, r_0,...,s_{T-1},a_{T-1},r_{T-1})}$. To scale up data collection, we run environments in parallel and step through them in batched mode. Each batch of states is sent to the model, which returns a corresponding batch of actions. We leverage fast inference libraries such as SG-Lang [18] and VLLM [19]. To improve state coverage, we roll out $\\pi_{i-1}$ multiple times on the same task, ensuring repeated state visits. Rollouts are stored in a dictionary G(s, a), which maps each hashed state-action pair to the set of trajectories passing through (s, a). We compute PRM targets as\n$Q(s, a) = \\frac{1}{|G(s,a)|} \\sum_{(s_t,a_t) \\in D(s,a)} \\sum_{k=t}^{T-1} \\gamma^{k-t} r_k \\qquad (1)$\nFinally we normalize the targets Q(s, a) to be between [0, 1]. The final dataset is then D = {(s, a, Q)} which is used to train the PRM. Note that we found this approach to be significantly simpler than"}, {"title": "3 Inverse Process Reward Models", "content": "The agent PRM framework in Sec. 2 assumes access to outcome rewards, which may not always be available. Designing rewards manually is labor-intensive and susceptible to misspecification [28, 31], as it requires explicitly capturing every success and failure condition. Instead, consider a setting where the agent has access only to expert demonstrations\u2014sequences of successful actions performed by a human, rule-based agent, or promoted LLM agent. The key challenge is: How can we learn process reward models solely from demonstrations, without access to explicit outcome rewards?\n3.1 Formulation\nGiven a set of expert demonstrations $D^* = {(s^*, a^*)}$, the goal is to infer a reward function r(s, a) that explains expert behavior. We formulate this as inverse reinforcement learning (IRL), which learns a reward that maximizes the expert's expected return relative to any other policy. Formally, IRL can be posed as a min-max adversarial game between a reward player r(s, a) (discriminator) and a policy player \u03c0 (generator):\n$\\min_\\pi \\max_r E_{\\pi^*} [r(s^*, a^*)] \u2013 E_\\pi[r(s, a)]. \\qquad (6)$\nThis game is solved iteratively. At each iteration i, the reward function $r_i(s, a)$ is updated to distinguish expert demonstrations from all past learner policies (no-regret update). The policy player $\\pi_i(a|s)$ then optimizes against the updated reward function (best response update):\n$r_i = \\underset{r}{\\text{arg max}} E_{\\pi^*} [r(s^*, a^*)] - E_{\\pi_{0:i-1}} [r(s, a)] \\qquad \\pi_i = \\underset{\\pi}{\\text{arg max}} E_{\\pi} [r_i(s, a)] \\qquad (7)$\nwhere sampling from $\\pi_{0:i-1}$ amounts to aggregating (s, a) data from all past policies and sampling uniformly from that.\nIRL via PRMs. A naive IRL implementation would require an outer optimization loop around the agent PRM framework, making it computationally impractical. Instead, we use a telescoping"}, {"title": "3.2 Approach", "content": "Algorithm 2 describes InversePRM: a simple three-stage iterative process to learn and refine PRMs and policies given expert demonstration.\n1. Create positive $D^+$ and negative $D^-$ transitions using expert demos and rollouts from $\\pi_{i-1}$.\n2. Train the PRM $Q_i(s, a)$ to discriminate between $D^+$ and $D^-$ (similar to RLHF)\n3. Train the policy $\\pi_i$ using reinforcement learning on the trained RM (similar to RLHF)\nThe framework is very similar to the three stage process in AgentPRM (Algorithm 1) with the difference being no outcome reward and instead expert demonstrations. Stage 1 and 2 differ to accommodate this, while Stage 3 remains the same. Just like AgentPRM, the algorithm for InversePRM builds on existing RLHF frameworks making it easy to implement and use. We describe each stage in detail below:\nStage 1: Create Positive / Negative Transitions. We initialize with an positive dataset $D^+ = {(s^*, a^*, s'^*)}$ containing state, action, next-state transitions from expert demonstrations. At iteration i, we rollout policy $\\pi_{i-1}$ in the environment to collect $D_i = {(s, a, s', a')}$ to get state, action, next-state, next-action transitions. These rollouts are then aggregated with an existing negative dataset $D^- \\leftarrow D^- \\cup D_i$. Finally, the next-action in both $D^+$ and $D^-$ are relabeled by calling $a' \\sim \\pi_{i-1}(s')$. We end up with a positive dataset $D^+ = {(s^*,a^*,s'^*, a')}$ where the transitions are from expert demonstrations, and negative dataset $D^- = {(s, a, s', a')}$ where the transitions are from all previous learner policies.\nStage 2: Training Process Reward Model. At iteration i, the PRM $Q_i(s, a)$ is trained to distinguish expert transitions $D^+$ from learner transitions $D^-$. We frame this as a binary classification problem, where expert transitions are labeled as positive (1) and learner transitions as negative (0).\nA key distinction from standard reward modeling is that the classifier operates on the difference of PRM values, $Q_\\phi(s, a) \u2013 \\gamma Q_\\phi(s', a')$, capturing the relative advantage of one transition over another. The loss function is:\n$L(\\phi) = E_{(s^*,a^*,s'^*,a')\\sim D^+} [log\\sigma(Q_\\phi(s^*, a^*) \u2013 \\gamma Q_\\phi(s'^*, a'))] + E_{(s,a,s',a')\\sim D^-} [log(1 \u2013 \\sigma(Q_\\phi(s, a) \u2013 \\gamma Q_\\phi(s', a')))]$"}, {"title": "4 Challenges and Opportunities", "content": "Reinforcement learning presents several challenges, some well-known in RL (e.g., exploration) and others specific to LLM agents (e.g., model-predictive reasoning). Addressing these challenges requires both established RL/IL techniques\u2014such as reset distributions and reward shaping\u2014and novel strategies leveraging LLM-specific capabilities, such as steered exploration.\n4.1 Exploration\nExploration remains a fundamental challenge in RL, requiring agents to explore effectively at both the turn level (solving multi-step tasks) and the token level (generating improved reasoning and actions). Fig. 6 shows that the first iteration of AgentPRM progresses slowly, requiring over 500 training steps before ramping up and plateauing at 73.9% success rate.\nTraditional exploration strategies include stochastic action selection methods such as e-greedy, entropy bonuses, or adjusting sampling temperature. However, these approaches do not scale well to high-"}, {"title": "Strategy 1: Reset Distribution", "content": "A simple yet effective exploration strategy is to reset the agent to a good distribution of states p(s) that an optimal policy is likely to visit. A good distribution is one that covers optimal state distribution. Practitioners often use a 50% - 50% reset distribution [35], where 50% of initial states are sampled from successful expert demonstrations such as human demonstrations or rule-based policies\u2014while the remaining 50% come from the agent's on-policy rollouts. Intuitively, this approach helps bootstrap learning by exposing the agent to good states early, making it easier to recover from errors. We call this strategy Reset-50-50."}, {"title": "Strategy 2: Steered Exploration", "content": "Unlike conventional RL policies, LLMs can be explicitly prompted to explore, rather than relying on stochastic action selection. We call this strategy Steered Exploration. Concretely, during RL (stage 3), we inject a small addition to the agent prompt:"}, {"title": "Strategy 3: Post-hoc Rationalization", "content": "The connection to posterior sampling yields another interesting way to do exploration. Suppose the agent had access to some privileged information, e.g., the future trajectory or hidden information about the MDP (hidden location of objects). Conditioned on that information, the agent can generate post-hoc rationalization for good actions to take. We explore training agents in this fashion in our prior work LEAP [34]. However, one challenge we faced is that not all post-hoc rationalizations are good, some are better than others."}, {"title": "4.2 Process Reward Shaping", "content": "Reinforcement learning from scratch is slow and sample inefficient. Practitioners often try to bootstrap RL using existing policies that have reasonable performance. We study a setting where only 10K rollout trajectories can be collected, but a reference policy with moderate performance (65.0%) is available. We look at two such strategies: (1) initializing the agent via imitation learning and then doing RL, and (2) using process reward shaping, where the reference policy provides structured guidance during RL training."}, {"title": "Strategy 1: Initialize with IL, then do RL", "content": "The simplest approach is to initialize the agent via SFT on trajectories generated by the reference agent. This ensures the initial policy is not random."}, {"title": "Strategy 2: Process Reward Shaping", "content": "We next look at involving the reference policy in the RL process itself. We look at process reward shaping, where instead of relying solely on sparse rewards, we shape the process reward using the advantage function of the reference policy.\nGiven a reference policy \u00b5, we add a shaping term to the PRM target:\n$Q(s, a) \\leftarrow (1 \u2013 \\alpha)Q^\\pi (s, a) + \\alpha A^\\mu (s, a) \\qquad (13)$\nwhere $A^\\mu(s, a)$ is the advantage w.r.t the reference policy \u00b5, i.e., $A^\\mu(s, a) = r(s, a) + V^\\mu(s') \u2013 V^\\mu(s)$.\n\u03b1 controls the power of the reference policy. Setting \u03b1 = 0 recovers the original PRM. Setting \u03b1 = 1 amounts to doing imitation learning, notable the AGGREVATE [41, 42] algorithm. Our procedure is:\n1. Fit a value V (s) using trajectories from the reference policy.\n2. In Stage 1, modify the PRM target to be $(1 \u2013 \\alpha)Q^\\pi (s, a) + \\alpha A(s, a)$\n3. Stage 2 and 3 remain unchanged."}, {"title": "4.3 Model-Predictive Reasoning", "content": "Recent large-scale RL advances have demonstrated promising results in multi-step reasoning tasks [8]. However, applying RL to agentic settings remains challenging because each interaction requires querying the environment, significantly slowing down learning. This raises a key question: How can we reduce costly interactions while enabling agents to reason and plan effectively?\nOne approach is to leverage learned world models. Instead of relying solely on trial-and-error, an LLM agent can simulate future trajectories using an internal model of the environment. This paradigm has been central in robotics, where real-world interactions are expensive and risky [43]. Model-based RL strategies, such as training policies in simulation before real-world deployment [44], have proven effective. Theoretically, generative models can provide mini-max optimal policies in model-based RL [45]. We extend this perspective to LLM agents: Can we train them to plan (or deliberatively reason) with their internal models to improve decision-making?\nStrategy: Deliberative Reasoning with a Learned World Model. Instead of treating reasoning as a single-step process that immediately outputs an action, we propose a structured multi-stage approach where the agent explicitly predicts future consequences before committing to an action. This decomposes the learning problem into two components:\n1. Learning a world model: Train an internal reasoning model to predict future states given an action, using rollouts from the current agent.\n2. Multi-turn planning and RL: Optimize the agent's reasoning process via reinforcement learning to maximize outcome rewards.\n3. Plan-and-execute policy: Structure the agent's reasoning to first generate a complete plan, select the initial action, execute it, and then replan iteratively.\nThis approach naturally connects to model-predictive control (MPC), where agents reason over predicted trajectories before taking actions, rather than relying purely on reactive decision-making."}, {"title": "5 Related Work", "content": "Fine-tuning agents. Most of the work on LLM agents rely on prompting LLMs, e.g. ReAct [4], Reflexion [5], AdaPlanner [27]. However, prompting alone is insufficient to correct errors encountered at test-time [1, 46]. A simple way to improve LLMs is to fine-tune on successful trajectories generated manually or via a prompted LLM [6, 47, 48]. However, manually collecting demonstrations of reason and actions is challenging and hard to scale.\nRecent work LEAP has looked at leveraging privileged AI feedback [34] to design critics that distill the information into student agents, showing strong performance in text-based games, web navigation and interactive coding. However, the privileged correction in LEAP can be unrealizable for the agent, leading to poor success rates. Hence, we look at training agents directly using RL to maximize the outcome reward.\nFinally, ARCHER [49] proposes a very similar framework to train LLM agents using hierarchical RL. The Q-value is trained using temporal difference, while the policy is trained using REINFORCE. However, the results are limited to small models (GPT2). We simplify the framework so it connects with existing RLHF pipelines, do RL with Llama 3B models, propose novel algorithms like InversePRM, and provide practical recipes like using reset distribution and reward shaping to improve efficiency.\nProcess Reward Models. PRMs have mostly been looked at in the context of multi-stage math reasoning problems [50], where they were trained in human annotation data to provide fine-grained supervision [10, 11]. Recent works look at automatically computing PRMs as Q value estimates [16, 51]. PRMs have been used to train generators [52] and used for test-time scaling with beam search [17], heuristic search [53] or tree search [54].\nThere are interesting similarities and differences between PRMs used for math reasoning and the agent setting we look at here. Many works [11, 52, 55] report small gains from optimizing PRMs rather than the outcome reward. In contrast, we see pretty strong gains with PRMs, where outcome reward is infeasible given long-horizons and limited access to the external environment. Some works have noted the reward-hacking / value-estimation issues with PRMs that we also analyze in Sec. 2.3. To counter such issues, recent works [12] propose reward shaping PRMs using reference policies, which we also explore in Sec. 4.2."}, {"title": "6 Conclusion", "content": "We introduced AgentPRM, a simple and scalable framework for training LLM agents using process reward models, and InversePRM, which learns PRMs directly from demonstrations without explicit outcome rewards. Our results on ALFWorld show that small models trained with AgentPRM outperform strong GPT-40 baselines, and InversePRM achieves near-expert performance with significantly fewer rollouts. We outlined key challenges\u2014exploration, process reward shaping, and model-predictive reasoning\u2014and proposed methods that leverage both RL techniques and LLM-specific capabilities. Future work includes extending PRMs to richer agentic environments and exploring large-scale RL via model-predictive reasoning."}]}