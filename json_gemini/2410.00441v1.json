{"title": "ReXplain: Translating Radiology into Patient-Friendly Video Reports", "authors": ["Luyang Luo", "Jenanan Vairavamurthy", "Xiaoman Zhang", "Abhinav Kumar", "Ramon R. Ter-Oganesyan", "Stuart T. Schroff", "Dan Shilo", "Rydhwana Hossain", "Mike Moritz", "Pranav Rajpurkar"], "abstract": "Radiology reports often remain incomprehensible to patients, undermining patient-centered care. We present ReXplain (Radiology eXplanation), an innovative AI-driven system that generates patient-friendly video reports for radiology findings. ReXplain uniquely integrates a large language model for text simplification, an image segmentation model for anatomical region identification, and an avatar generation tool, producing comprehensive explanations with plain language, highlighted imagery, and 3D organ renderings. Our proof-of-concept study with five board-certified radiologists indicates that ReXplain could accurately deliver radiological information and effectively simulate one-on-one consultations. This work demonstrates a new paradigm in AI-assisted medical communication, potentially improving patient engagement and satisfaction in radiology care, and opens new avenues for research in multi-modal medical communication.", "sections": [{"title": "1. Introduction", "content": "Background and Motivation. Radiology reports play a crucial role in patient care, and studies show a consistent, strong desire among patients to access these reports. However, these documents often remain inaccessible to the very individuals they concern most: the patients themselves. The complex medical terminology and abbreviations, designed for efficient communication among healthcare professionals, create a significant barrier to patient understanding. This can lead to misunderstandings, anxiety, and potentially impact treatment decisions, ultimately increasing the burden on doctors.\nPatient-centered Radiology. Patient-centered Radiology, which involves radiologists actively engaging with patients by introducing imaging procedures, communicating results directly, and addressing concerns, has gained increasing attention. Recent efforts have focused on implementing patient-centered practices in real clinical settings, aiming to align radiological services with patients' needs and preferences. Studies have shown that both healthcare providers and patients"}, {"title": "Current Approaches and Limitations.", "content": "Prior work has demonstrated that providing patients access to their radiology images can improve their understanding of reports. However, connecting text descriptions to findings on radiology images remains challenging for patients without medical training, limiting their comprehension of highly specialized medical information. Various approaches have been attempted to enhance patient understanding, including structured reports, additional explanations, and increased radiologist-patient interactions. Studies have also shown that radiology video reports highlighting important findings can improve patients' understanding of reports and corresponding images. Specifically, Recht et al. (2022) found that video reports made by radiologists, featuring explanations and visual guidance, were preferred by patients over written reports alone. Nevertheless, these approaches require additional effort from radiologists, which may be impractical given their already heavy workload, especially for volumetric imaging like CT or MRI."}, {"title": "AI in Radiology Communication.", "content": "Recent advances in artificial intelligence (AI) have shown promise in bridging this communication divide. Progress in language processing, medical image computing, and cross-modal generation demonstrates great potential in healthcare and medicine. The advent of Large Language Models (LLMs) has opened new possibilities in this field, with potential applications in generating impressions, simplifying radiology reports for patients, and improving patient engagement. Recent research has explored these possibilities. Typical applications have been developed to translate complex medical terminology into easily understandable language. These developments offer new possibilities for tackling the limitations of current approaches and could allow patients to better process and understand their own medical data."}, {"title": "Challenges in AI-driven Radiology Reporting.", "content": "However, designing and generating effective radiology video reports remains challenging, both technically and practically. To date, efforts have mainly focused on medical language processing, with few studies attempting to expose patients to their radiology images, which are more challenging to understand. The technical obstacle lies in connecting text descriptions to the findings on radiology images, which is a non-trivial task."}, {"title": "Our Approach and Contributions.", "content": "We propose ReXplain, an AI-driven radiology video report generation pipeline addressing this challenge. The core innovation of our approach lies not in the development of new AI models, but in the novel integration of existing state-of-the-art modules to create a functional, end-to-end system. By combining advanced language models, image segmentation techniques, and avatar generation technology, we've created a proof-of-concept system that produces comprehensive, visually-enhanced explanations of radiology findings.\nThe key components of ReXplain include:"}, {"title": "2. Designing ReXplain", "content": "Sketching Video Reports Concepts. In a common clinical workflow, radiologists or clinicians will walk the patient through the findings based on the radiology report and corresponding radiology image. To understand this process, we conducted a simulation where one radiologist explained the radiology report to an imaginary patient. Noticeably, there were several noticeable steps that help improve the understanding, which involved typically (1) explaining the important findings to the patient with plain language; meanwhile (2) pointing at the abnormalities on the image; and (3) supplementing necessary explanations on how the image should look if it was from a healthy individual.\nWe thus sketched the design concept of a video report to mimic the above behavior, which led to a pipeline integrating different AI models. Specifically, we identified the following elements that are important to generate a patient-friendly explanation: (1) lay-language interpretation that translates the original brief, expertise-demanding reports into easy-to-understand descriptions; (2) localization of the corresponding finding on the image for an intuitive illustration; (3) comparison with the scan from a healthy individual that emphasizes the difference between abnormal and normal conditions; and (4) a properly-designed interface to display the generated visual and textual information. The remaining of this section will introduce our approach to achieving each of the above goals."}, {"title": "Interpreting Radiology Reports with Lay-language.", "content": "The first goal is to interpret the radiology reports to be easy-to-understand by non-experts. Here, we leveraged the large language model, GPT-40 which has been reported capable of lowering the reading level of professional medical documents. First, we required GPT-40 to extract phrases that describe positive findings, which are supposed to be the most essential information for the patients. Then, we prompted GPT-40 to imitate radiologists's explanation and generate three types of messages in lay language: (1) an explanation of what findings are detected; (2) a description of how the findings appear on a CT scan; (3) a description of how the CT scan should appear without the abnormal findings. This procedure mimics the radiologist-patient communication of explaining the radiological findings. In this way, we managed to rephrase the specialist radiology reports to have higher readability with structured presentation order, which is prepared to be simultaneously presented with images displayed later. The prompts we used can be found in Appendix A."}, {"title": "Connecting Image Regions with Reports.", "content": "To imitate radiologists' explanation for CT images, we would need to localize the regions of interest (ROIs) for an intuitive visualization of the report findings. As there are currently few AI models that can localize every types of lesions mentioned in the reports, we relaxed the requirement from localizing the findings to localizing the anatomic structures that contain the findings. To achieve this, we first prompted GPT-40 to match the extracted findings to one of the organs from a finite set of 201 human organs. Then, to achieve comprehensive localization across a wide range of anatomical structures,"}, {"title": "Generating Avatar Explainer.", "content": "To facilitate more effective dissemination and reception of complex medical information, we enhanced our instructional videos with an AI-generated virtual explainer. This avatar would simulate a medical practitioner, expected to improve patient comprehension and promote greater acceptance of the content. Building upon the translated report from previous steps, this step leveraged a text-to-video avatar generation model. Particularly, we took advantage of the commercially ready avatar generation pipeline, Tavus, which consists of a series of AI techniques, including text to speech generation, 3D head and shoulders reconstruction, audio to facial animation generation, and avatar video rendering utilizing 3D Gaussian Splatting. These features allow the pipeline to generate a lifelike, expressive, and engaging avatar to effectively convey medical information in an accessible manner. The resulting video demonstrated a high-fidelity, realistic avatar with smooth transitions between expressions and can adapt to various facial structures and expressions. This integration of an AI avatar bridged the gap between technical medical information and user-friendly interface, thus enhancing the comprehension and understanding for the video reports."}, {"title": "3. Patient-friendly Video Reports", "content": "Integrating Video Reports The former pipeline composes ReXplain , based on which we designed a comprehensive video logic to effectively convey radiology image findings to patients, in a way mimics the radiologists. As mentioned, the finding is presented using easily understandable lay language, ensuring that the information is accessible to those without medical backgrounds. To synchronize the CT and the avatar displays, we simply extended the clip with a shorter duration by its last frame. Meanwhile, we also added the display of the organ rendering to illustrate the global view of the region of interest. The windows used to show the CT images were set to be consistent with the findings, e.g., lung window would be used to display findings on the lungs. The final video represents an integrated presentation of visual and auditory information, which will help reinforce the explanation of the radiology reports."}, {"title": "4. Eliciting User Feedback", "content": "Material. The CT images used for this study were sampled from CT-RATE, a publicly available dataset of non-contrast 3D chest CT volumes paired with radiology text reports. We standardized all CT volumes to a uniform voxel spacing of 1 \u00d7 1 \u00d7 3 mm to meet the input requirement by SAT. As the data from CT-RATE could partially cover the abdomen region, we amended the prompt to match the report findings to 201 specific anatomical regions covering both the thorax and abdomen. The regions are listed in the appendix.\nUser Study Design We conducted a pilot user study to evaluate the effectiveness of the video reports. Specifically, we produced ten video reports, each lasting up to 3 minutes and explaining one or two radiological findings. Then, both the video reports and the original text reports were provided to the radiologist team (four radiologists with 6-10 years of practice, and one with 11-15 years of practice). Importantly, four of the radiologists team members were not involved in the design of the video reports. A survey will be completed after each pair of reports being reviewed and compared.\nWe prepared 12 questions for each video, which can be found in Table 1. Specifically, questions #1-6 evaluated the correctness of the information delivered and its suitability for patient communication, while questions #7-12 investigated the helpfulness of each element of video. For each question, we rated our level of agreement from 5 to 1, representing \"strongly agree,\" \"agree,\" \"neither agree nor disagree,\" \"disagree,\" and \"strongly disagree,\" respectively. In addition to direct scoring, we provided opportunities for more flexible feedback, inviting radiologist team members to offer written comments on \"which parts of the video were most useful or valuable\" and \"how would you improve this video?\" This approach allowed us to gain a deeper understanding of the collective expert opinions."}, {"title": "Ethics Statement", "content": "This work was based on publicly available data with no identifiable personal information. The user study was conducted exclusively with the participation of the authors, all of whom are investigators on this project. No external participants were involved, and no personally identifiable information was collected or used. As the current research involved only the investigators themselves, it did not constitute human subjects research requiring Institutional Review Board (IRB) approval. Therefore, IRB approval was not required for this study."}, {"title": "ReXplain is Useful in Enhancing Understanding.", "content": "Based on ten videos and feedback from five radiologists, we collected 50 data points for each of the questions. We plotted the distribution of the scores in Fig. 4 and sorted the questions in descending order of the percentage of the combination of \u201cstrongly agree\u201d and \u201cagree\u201d.\nSpecifically, there were at least 64% positive feedback (\u201cagree\u201d or \u201cstrongly agree\u201d) on questions #1-4, indicating that the information delivered by the videos are largely correct and easy-to-understand. These results reveal that the video reports can be potentially used to convey the radiology findings to patients without medical knowledge.\nMoreover, questions #8-10 and #12 received at least 74% positive feedback, indicating the useful components in the videos and answered how the video reports improved patient-friendly radiology. The written comments showed that \"showing comparison with normal image\" and \"the layperson explanation of the report\" were the two most frequent positive feedback. These results showed that most elements in the video reports, i.e., the translated reports with lay language, the connection between image regions and reported findings, the comparison to the normal images, and the use of the avatar explainer, could effectively assist in improving the understanding and engagement of audience."}, {"title": "ReXplain Shows Potential in Clinical Practice.", "content": "In clinical practice, limited time and heavy workload often prevents radiologists from directly communicating with patients. The study by Recht et al. (2022) showed that hand-crafted video reports by radiologists were helpful in improving patients' understanding. However, this approach offered limited information with increased workload (each video had 55 \u00b1 30 seconds of duration and was generated in 238 \u00b1 141 seconds). In contrary, ReXplain functions end-to-end without requiring extra time or effort from radiologists. Furthermore, the explanation could be tailored to provide flexibility for customization through appropriate prompting. We observed that 66% of responses were optimistic about walking patients through the videos (Question #5), which indicates that the generated video reports effectively conveyed the information we intended for patients to understand. This result further demonstrated that ReXplain holds promise in improving patients understanding and experience with lighter workload on radiologists."}, {"title": "5. Discussion", "content": "Current Limitations and Outlook. This proof-of-concept study aimed to to figure out whether and how the AI-driven pipeline, ReXplain, could help translate the conventional radiology reports into patient-friendly video reports. Nevertheless, our user study was conducted based on radiologists, of whom the feedback may not precisely reflect the experience of real patients. The current work shows that the video reports overall were considered useful for enhancing patients' understanding, and this finding motivates us to conduct future works involving patients without medical knowledge to further investigate the usefulness of the AI-generated video reports.\nWe acknowledge another noteworthy limitation of this study: merely the organs containing the abnormal findings were highlighted, which is limited by the current state of medical image segmentation. A significant portion of written feedback from the radiology team suggested that highlighting entire organs may have limited effectiveness in improving understanding, particularly for small findings such as lung nodules. This probably led to predominantly negative feedback regarding the usefulness of organ rendering for patient comprehension (question #11) and that only 32% of responses (question #6) indicated acceptance of allowing patients to view the videos without supervision. We believe this challenge can be addressed with advancements in text-prompted lesion segmentation, such as BiomedParse. We are also interested in exploring whether and how more accurate identification and localization of findings might alter users' opinions."}, {"title": "Conclusion", "content": "In this paper, we present ReXplain, an innovative integration of existing state-of-the-art modules to create a functional, end-to-end system that produces novel, patient-friendly radiology video reports. We showcase an integrative AI pipeline with a practical interface design to solve a complex healthcare communication challenge, bridging a crucial gap in patient understanding of their diagnostic results. The user study by five board-certified radiologists yielded encouraging responses regarding the usefulness of the AI generated video reports. By developing and evaluating ReXplain, this paper lays the groundwork for AI-assisted patient-centered radiology."}, {"title": "Appendix A. Prompting GPT-4 for report explanation", "content": "A.1. Matching report phrases to organs.\nAct as an experienced clinician with radiology knowledge, please extract a list of the entities (\norgans) with positive findings out of the given text report. If the detected organ belongs to\nthe following list, then add this exact organ into the output list; otherwise look into the\nrelationships between the detected organ and those in the list, find the closest one (in\nterms of belonging relationship) in the list and put that one into the output list. The\noutput should strictly formatted a list of organs, no other text should be provided.\nA.2. Report phrase explanation.\nYou are an experienced clinician with radiology knowledge. Given the description of a patient's\nCT imaging report, please help explain it to the patient in a friendly, easy-to-understand,\nand brief way. Your explanation should contain 1. A brief concise description in layman\nlanguage of what this abnormality means; including what it indicates clinically; 2. How it\nshould looks like in the CT image; and 3. How a normal person's CT would be like without the\nabnormality. Generate it in a smooth way as if you are guiding the patient to look at the two\ndifferent CT images. Your output should be in the format of a json file. Here is an example:"}, {"title": "A.3. List of Organs.", "content": ""}]}