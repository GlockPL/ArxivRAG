{"title": "LLMS VS. HUMANS IN LATENT CONTENT ANALYSIS", "authors": ["Ljubi\u0161a Boji\u0107", "Olga Zagovora", "Asta Zelenkauskaite", "Vuk Vukovi\u0107", "Milan \u010cabarkapa", "Selma Veseljevi\u0107 Jerkovi\u0107", "Ana Jovan\u010devi\u0107"], "abstract": "In the era of rapid digital communication, vast amounts of textual data are generated daily, demanding efficient methods for latent content analysis to extract meaningful insights. Large Language Models (LLMs) offer potential for automating this process, yet comprehensive assessments comparing their performance to human annotators across multiple dimensions are lacking. This study evaluates the reliability, consistency, and quality of seven state-of-the-art LLMs, including variants of OpenAI's GPT-4, Gemini, Llama, and Mixtral, relative to human annotators in analyzing sentiment, political leaning, emotional intensity, and sarcasm detection. A total of 33 human annotators and eight LLM variants assessed 100 curated textual items, generating 3,300 human and 19,200 LLM annotations, with LLMs evaluated across three time points to examine temporal consistency. Inter-rater reliability was measured using Krippendorff's alpha, and intra-class correlation coefficients assessed consistency over time. The results reveal that both humans and LLMs exhibit high reliability in sentiment analysis and political leaning assessments, with LLMs demonstrating higher internal consistency than humans. In emotional intensity, LLMs displayed higher agreement compared to humans, though humans rated emotional intensity significantly higher. Both groups struggled with sarcasm detection, evidenced by low agreement. LLMs showed excellent temporal consistency across all dimensions, indicating stable performance over time. This research concludes that LLMs, especially GPT-4, can effectively replicate human analysis in sentiment and political leaning, although human expertise remains essential for emotional intensity interpretation. The findings demonstrate the potential of LLMs for consistent and high-quality performance in certain areas of latent content analysis.", "sections": [{"title": "Introduction", "content": "In an era characterized by rapid digitization and the proliferation of online communication platforms, vast amounts of textual data are generated daily. This surge presents both an opportunity and a challenge: while there is unprecedented access to public opinion and discourse, analyzing these data to extract meaningful insights requires substantial effort and resources. Latent content analysis, which involves decoding the underlying meanings, sentiments, and nuances in text, is crucial for understanding social dynamics, informing policy decisions, and guiding business strategies (Neuendorf, 2017). Automating this process could significantly enhance our ability to respond to societal needs promptly and effectively.\nThe societal implications of effectively analyzing textual content are profound. Sentiment analysis can reveal public opinion on policies or products, influencing governmental decisions and corporate strategies (Liu, 2012). Understanding political leanings aids in assessing electoral landscapes and fostering democratic engagement (DiMaggio, Evans, & Bryson, 1996). Detecting emotional intensity and sarcasm in communication is vital for mental health monitoring, customer service, and even national security (Pang & Lee, 2008; Ghosh et al., 2018). Large Language Models (LLMs) offer the potential to perform these analyses at scale, reducing reliance on extensive human labor and accelerating the time to insight (Bojic et al., 2023)."}, {"title": "Evolution of Automated Content Analysis", "content": "The field of automated content analysis has evolved significantly over the past few decades. Early computational approaches relied on manual coding schemes applied to small datasets (Krippendorff, 2019). The advent of machine learning introduced algorithms capable of handling larger datasets with increased efficiency (Sebastiani, 2002). Traditional models, such as Na\u00efve Bayes and Support Vector Machines, were used for tasks like sentiment classification but often struggled with contextual understanding (Pang et al., 2002).\nThe introduction of deep learning architectures marked a transformative period in natural language processing (NLP). Models utilizing word embeddings captured semantic relationships between words (Mikolov et al., 2013). Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks improved the modeling of sequential data (Hochreiter & Schmidhuber, 1997). These advancements enhanced performance in sentiment analysis and emotion detection tasks (Socher et al., 2013).\nThe integration of multi-modal data\u2014combining text with images, audio, or video-has emerged as a promising approach to enhance sentiment analysis and affect detection (Cambria et al., 2017). Thareja (2024) addressed the challenges posed by extreme emotional sentiments on social media platforms like Twitter, which can impact users' mental well-being. Introducing Tweet-SentiNet, a multi-modal framework utilizing both image and text embeddings, the study demonstrated improved sentiment analysis by effectively filtering content with extreme sentiments. Similarly, Li et al. (2024) proposed a multi-modal sentiment analysis model based on image and text fusion using a cross-attention mechanism. By extracting features using advanced techniques like ALBert for text and DenseNet121 for images, and then fusing them with cross-attention, their model outperformed baseline models on public datasets, achieving accuracy and F1 scores of over 85%. Akhtar et al. (2020) explored a deep multi-task contextual attention framework for multi-modal affect analysis. Recognizing that emotions and sentiments are interdependent, they leveraged the associations among neighboring utterances and their multi-modal information.\nDespite these improvements, models have been found to still face challenges in interpreting complex linguistic features such as sarcasm and nuanced emotions (Poria et al., 2017). Sarcasm detection, for instance, requires an understanding of contextual cues and sometimes external knowledge beyond the text itself (Joshi et al., 2018; Bojic et al., 2023), leading researchers to explore context-aware and multi-modal approaches to enhance detection accuracy. Baruah et al. (2020) investigated the impact of conversational context on sarcasm detection using deep-learning (BERT, BiLSTM) NLP models and ML classifier (SVM). They found that incorporating the last utterance in a dialogue significantly improved classifier performance on Twitter datasets, achieving an F-score of 0.743 with BERT. Exploring the distinction between intended and perceived sarcasm, Oprea and Magdy (2020) introduced the iSarcasm dataset, which consists of tweets labeled for sarcasm directly by their authors emphasizing the need for datasets that reflect the intended use of sarcasm to improve detection systems."}, {"title": "The Rise of Transformer Models and LLMS", "content": "The introduction of the Transformer architecture (Vaswani et al., 2017) and pre-trained language models such as BERT (Devlin, Chang, Lee, & Toutanova, 2019) and RoBERTa (Liu et al., 2019) significantly advanced NLP capabilities. These models utilized attention mechanisms to capture long-range dependencies in text, leading to state-of-the-art results in various tasks.\nLarge Language Models (LLMs) like GPT-2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020) expanded these capabilities by increasing model size and training data. GPT-3, with 175 billion parameters, demonstrated remarkable proficiency in zero-shot and few-shot learning scenarios, performing well on tasks it was not explicitly trained for (Brown et al., 2020).\nRecent studies have explored LLMs in sentiment analysis and related tasks. Chang & Bergen (2024) investigated the use of GPT-3 for sentiment classification and found that it performed competitively with fine-tuned models on specific datasets. Similarly, Floridi and Chiriatti (2020) discussed the potential of GPT-3 in understanding and generating human-like text, highlighting its applicability in content analysis.\nThe incorporation of context-aware mechanisms (Baruah et al., 2020), consideration of intended versus perceived meanings (Oprea & Magdy, 2020), and the use of multi-modal data (Thareja, 2024; Li et al., 2024; Akhtar et al., 2020) represent critical steps toward improving model performance in complex NLP tasks. The development of domain-specific models like PoliBERTweet (Kawintiranon & Singh, 2022) highlights the potential benefits of customizing language models to better capture specific content areas, such as political discourse. The integration of symbolic reasoning with deep learning in SenticNet 6 further highlights the importance of combining different AI approaches to enhance understanding and interpretation of subtle linguistic features (Cambria et al., 2020).\nHowever, challenges remain regarding the ethical and practical implications of relying on LLMs. Concerns include model bias, the interpretability of results, and the tendency of LLMs to produce plausible but incorrect or biased outputs (Bender et al., 2021; Bodro\u017ea et al., 2024). Additionally, studies have shown that while LLMs excel in language tasks, their performance in detecting sarcasm and nuanced emotions is inconsistent (Zhang, et al., 2023).\nThe consistency of LLMs over time is another area of interest. Although not updated by service providers, models that are prompted on different instances, may produce different outputs on the same input, raising questions about reliability in longitudinal studies (Imamguluyev, 2023). LLMs can be sensitive to input phrasing, leading to different interpretations based on slight changes in wording (Gao et al., 2021)."}, {"title": "The Current Study", "content": "Human annotators have long been the gold standard in content analysis due to their ability to understand context, cultural references, and subtle language cues (Krippendorff, 2019). Inter-annotator agreement metrics such as Krippendorff's alpha are used to assess consistency among human coders (Hayes & Krippendorff, 2007). Comparing LLM performance against human benchmarks is essential to evaluate their viability as substitutes or supplements in content analysis tasks.\nWhile Large Language Models (LLMs) have demonstrated impressive capabilities, there is a notable lack of comprehensive evaluations comparing their performance to human annotators across multiple dimensions of latent content analysis. Existing studies often focus on single tasks or lack extensive statistical analysis of agreement and quality (Zhang et al., 2023). Additionally, the consistency of LLMs over time and their reliability in capturing complex linguistic features remain underexplored. To address these gaps this study formulates the following research questions:\nRQ1: How reliably do LLMs and humans' rate latent content across dimensions such as sentiment, political leaning, emotional intensity, and sarcasm? Although the field of content analysis has advanced from manual coding schemes (Krippendorff, 2019), through machine learning introduced algorithms capable of handling larger datasets with increased efficiency (Sebastiani, 2002), and finally peaked with deep learning architectures (e.g., Mikolov et al., 2013), research is still needed especially in interpreting complex linguistic features such as sarcasm and nuanced emotions (Poria et al., 2017). On the other hand, other studies show that machines underperform in comparison to humans in certain tasks (Lottridge et al., 2023). One big gap in these studies is the lack of comparison with human annotations to compare how LLMs and humans annotate complex linguistic features and whether humans are better at these tasks.\nPrevious studies have focused on specific aspects of content analysis, such as sentiment classification (Chang & Bergen, 2024) and sarcasm detection (Bojic et al., 2023), but there is limited research on how the level of agreement between LLMs and humans varies across different dimensions. This is another gap to be addressed in this study.\nRQ2: Are LLMs consistent over time when analyzing textual content? Consistency over time is another underdressed issue. Models used in different instances can create different outputs in different instances (Imamguluyev, 2023). This issue is especially complex considering that just small changes in prompts can lead to different outcomes (Gao, Fisch, & Chen, 2021). The issue of consistency and reliability over time is another one to be tackled in this study.\nRQ3: To what extent do LLMs provide analysis that is comparable to human analysis in terms of quality? Human annotators have long been the gold standard in content analysis due to their ability to understand context, cultural references, and subtle language cues (Krippendorff, 2019). However, the ability of LLMs to learn from the context is being examined (Brown et al., 2020), as well as their ability to produce human-like texts (e.g., GPT; Floridi & Chiriatti, 2020) is rising, with an aim to replace human annotators with LLMs. Even though these studies are rising in numbers, quality check studies comparing humans and LLMs are still lacking, and this is another gap to be addressed in this study.\nRQ4: Does LLM reliability, consistency, agreement level, and comparability vary across different LLM models? Previous studies examined some LLM models' success in annotation, but usually in one or two tasks (e.g., Zhang, et al., 2023), and usually using one or two models (Lottridge et al., 2023) in this study we aim to close this gap by including multiple models and multiple tasks, and examine all their reliability and consistency. Thus, there is a scarcity of research examining how different LLMs compare with each other in terms of reliability and performance across multiple dimensions of latent content analysis. Given the rapid development and diversity of LLM architectures\u2014each trained on varying datasets and employing different model sizes-it's crucial to understand whether these differences translate into variations in content analysis outcomes. This question addresses the gap in literature concerning the comparative effectiveness of multiple LLMs, aiming to inform practitioners about the optimal models to employ for specific analytical tasks and to determine if certain models consistently outperform others or exhibit unique strengths and weaknesses."}, {"title": "Method", "content": "To evaluate the reliability and quality of large language models (LLMs) in latent content analysis, we conducted a comparative study involving both humans and eight types of LLMs that each responded to presented queries to evaluate content by assigning values to statements. Our objective was to benchmark the performance of LLMs and humans across four key dimensions: sentiment, political leaning, emotional intensity, and sarcasm detection by performing a) within (internal consistency) and b) between analyses (comparison of performance).\nThe ethical approval was acquired from the Ethics Committee prior to this research. All methods were carried out in accordance with relevant guidelines and regulations."}, {"title": "The Sample", "content": "The study involved 33 human annotators who were proficient in English. The group brought substantial academic and professional expertise to the study. The sample included 81.8% of annotators holding PhDs and various academic titles ranging from PostDocs to Full and Associate Professors. The annotators included experts from disciplines such as Social Psychology, Communication Science, Linguistics, and Computing and Information Technology. Their affiliations spanned 18 European countries, including Poland, Albania, Czech Republic, Serbia, Portugal, Turkey, Bosnia and Herzegovina, Spain, Austria, Norway, Cyprus, Belgium, Germany, Netherlands, Romania, United Kingdom, France and Ireland. Annotators were integral members of the COST Action Network CA21129, which focuses on integrating theoretical and methodological approaches to analyzing opinionated communication (Opinion, 2024). This diverse expertise facilitated a robust analysis of opinions, enriching the study with interdisciplinary insights.\nIn addition to human participants, seven state-of-the-art LLMs were selected for evaluation: GPT-3.5, GPT-4, GPT-4o, GPT-4o-mini, Gemini, Llama-3.1 and Mixtral. Additionally, GPT-4o was prompted in a different way through interplay of various agents which we called a hard prompt. Thus, we had eight variations of LLM. These models were chosen to represent a range of architectures and training data, providing a comprehensive overview of current LLM capabilities."}, {"title": "Annotation Sentences", "content": "The textual items used for annotation (OSF, 2024) were curated and, where appropriate, adapted from existing literature and established datasets commonly used in the study of sentiment analysis, political leaning, emotional intensity, and sarcasm detection. This approach ensured that the sentences were representative of the types of content typically encountered in real-world situations and aligned our study with prior research methodologies.\nTo enhance the validity and comparability of our study, we referred to several well-established datasets and research studies in the field of natural language processing (NLP) and sentiment analysis. These included the Stanford Sentiment Treebank (Socher et al., 2013) and the Sentiment140 dataset (Go et al., 2009) for sentiment analysis; studies by Iyyer et al. (2014) for political ideology in text; the EmoBank corpus (Buechel & Hahn, 2017) for emotional intensity; and the Sarcasm Corpus V2 (Khodak et al., 2018) for sarcasm detection. By using these sources, we aimed to align our sentences with the standards in the field and to ensure that our results are comparable to previous research.\nFor each dimension, we developed 25 unique sentences, resulting in a total of 100 sentences for analysis. The sentences were designed to span the entirety of the 5-point Likert scales used for annotation, from one extreme of the dimension to the other. This allowed us to assess both human annotators and LLMs across the full spectrum of possible ratings, evaluating their ability to correctly identify clear cases as well as their proficiency in handling more nuanced or ambiguous instances.\nSentiment. In the sentiment analysis dimension, sentences were crafted to represent a full range of sentiments from strongly negative to strongly positive. For example, a sentence representing strong negative sentiment was adapted from examples in Pang and Lee (2005): \"I hate everything about this product. It's a complete waste of money and time.\" A neutral or mixed sentiment sentence was: \"The report had its ups and downs. Some sections were really informative, while others were lacking depth.\" An example of strong positive sentiment, adapted from Socher et al. (2013), was: \"I absolutely love this place! The service is fantastic and the food is incredible.\" By including sentences with varying emotional tones, we aimed to test the annotators' and LLMs' abilities to accurately perceive and rate the sentiment expressed.\nPolitical leaning. For the political leaning dimension, sentences were designed to reflect a spectrum of political opinions from strongly left-leaning to strongly right-leaning. Drawing from themes in Iyyer et al. (2014), a strongly left-leaning sentence was: \"Universal healthcare is essential for a just and equitable society.\" A neutral or centrist sentence was: \"Economic policies should balance the needs of business growth and social welfare.\" A strongly right-leaning sentence was: \"Lowering taxes is the best way to stimulate economic growth and individual freedom.\" These sentences incorporated references to common political themes and policies, allowing us to evaluate how well annotators and LLMs could detect and interpret ideological cues.\nEmotional intensity. In the emotional intensity dimension, sentences were constructed to exhibit varying levels of emotional expression, guided by the EmoBank corpus (Buechel & Hahn, 2017). A sentence with very low emotional intensity, similar to neutral sentences in EmoBank, was: \"The data from the recent survey shows a slight increase in customer satisfaction.\" A sentence with moderate emotional intensity was: \"The announcement was met with mixed feelings; some viewers expressed joy while others felt disappointment.\" For very high emotional intensity, we used: \"In an outburst of euphoria, he shouted and danced around, his joy uncontainable.\" By varying the language from factual and straightforward to vividly expressive, we challenged the annotators and LLMs to discern subtle differences in emotional intensity.\nSarcasm. For sarcasm detection, sentences ranged from literal statements to overtly sarcastic remarks. We examined the Sarcasm Corpus V2 (Khodak et al., 2018) and examples from studies like Riloff et al. (2013) to incorporate sentences exhibiting different levels of sarcasm. A non-sarcastic sentence was: \"I'm delighted with the new features in the app; it's exactly what we needed.\" A moderately sarcastic sentence, adapted from Riloff et al. (2013), was: \"Oh great, another meeting. Just what I needed.\" An overly sarcastic sentence, common in sarcasm datasets, was: \"You've really outdone yourself this time.\" These sentences were designed to test the ability of annotators and LLMs to detect sarcasm, which often relies on contextual cues and can be challenging to interpret in written form.\nWhile crafting and selecting these sentences, we adhered to several considerations. First, we wanted to make sure our materials were consistent with prior research, facilitating comparability and enhancing the validity of our findings (OSF, 2024). Second, we ensured variability across the scales by choosing sentences to represent the full spectrum of each dimension's Likert scale, allowing for a thorough evaluation of annotators' and LLMs' abilities to distinguish between different levels. Third, we included a diversity of content, covering various topics and contexts such as products, services, policies, and everyday situations, to mimic the diversity found in real-world text data. Fourth, sentences were designed to be self-explanatory, providing sufficient context for accurate annotation without requiring external information. Fifth, we avoided including sentences that could be biased, culturally insensitive, or offensive, ensuring ethical considerations were met. Finally, although some sentences were adapted from existing sources, they were paraphrased or modified where necessary to fit the specific grading criteria and to avoid direct replication of copyrighted material."}, {"title": "Procedure for human annotation", "content": "Annotators were provided with detailed instructions and training materials to ensure a consistent understanding of the annotation tasks. The instructions guided them in evaluating content by assigning values on a Likert scale ranging from 1 to 5. Thus, each human annotator evaluated a total of 100 textual items, comprising 25 items for each of the four dimensions (sentiment, political leaning, emotional intensity, and sarcasm), resulting in 3,300 human annotations (OSF, 2024). For each of the four dimensions, we assembled a set of 25 unique textual items, resulting in 100 items overall. All annotations employed a 5-point Likert scale tailored to each dimension, including Sentiment: 1 (Strongly Negative Sentiment) to 5 (Strongly Positive Sentiment); Political Leaning: 1 (Strongly Left-Leaning) to 5 (Strongly Right-Leaning), Emotional Intensity: 1 (Very Low Emotional Intensity) to 5 (Very High Emotional Intensity) and Level of Sarcasm: 1 (Not Sarcastic) to 5 (Overly Sarcastic).\nHuman grading was administered during the COST Opinion meeting in Salamanca, Spain on 12/06/2024, utilizing Google Forms (OSF, 2024). Human annotators individually assessed all 100 textual items to ensure evaluations were independent and uninfluenced by others. They were instructed to rely solely on the text provided, without consulting external resources. The instructions emphasized the importance of consistency and attention to distinctions in the text.\nAll human participants provided informed consent before participating in the study. They were briefed on the study's purpose, procedures, and their right to withdraw at any time without penalty. To protect participants' privacy, all responses were anonymized, and data were stored securely. The study design was reviewed and approved by the institutional review board to ensure adherence to ethical standards.\nEfforts were made to minimize potential biases in the study. The selection of textual items aimed at diversity in content to avoid cultural or contextual biases that could affect annotations. Instructions for both human annotators and LLMs emphasized neutrality and objectivity in evaluations."}, {"title": "Procedure for LLMs annotation", "content": "For the LLMs, we crafted standardized prompts that mirrored the instructions given to human annotators. Prompt design was critical to ensure comparability between human and LLM annotations. Each prompt included clear instructions and specified the annotation scale (OSF, 2024).\nEight variations of LLM testing included seven LLMs with addition to slightly changed Hard Prompt GPT-4o. This meant giving additional instructions to the language model, that initiated role play of 3 agents. LLM was asked to describe two agents most suitable to do the grading and then the third agent to reach the final decision. This is how the additional instruction was formulated only in the Hard Prompt:\n\"Evaluate the qualifications and attributes of Agent 1 and Agent 2, detailing why each agent is best suited to grade the provided text. Additionally, describe the role and qualifications of Agent 3, who will serve as the judge to make the final grading decision. After assessing the text, provide the grades given by Agent 1 and Agent 2, followed by the final decision rendered by Agent 3.\" (OSF, 2024)\nRegular Prompts for GPT-3.5, GPT-4, GPT-4o, GPT-4o-mini, Gemini, Lamma-3.1, Mixtral, and Hard Prompt for GPT-4o were administered_on_ 12/08/2024, 13/08/2024 and 14/08/2024 in the same time-frame of the day from approximately 18:31 until 21:08 (OSF, 2024)."}, {"title": "Data Analysis", "content": "Data analysis was performed using statistical software packages SPSS (Version 26), R (Version 4.0.2), and Python 3. These tools facilitated the computation of descriptive statistics, reliability coefficients, t-tests, ANOVAs, and effect sizes. The LLMs were accessed through their respective APIs or interfaces, ensuring consistency in how prompts were delivered and responses were recorded.\nRQ1. To address RQ1we performed within-group analysis first, by assessing inter-rater reliability among human annotators and among LLMs. To do so, we calculated Krippendorff's alpha for each dimension. Krippendorff's alpha is suitable for evaluating agreement among multiple raters using ordinal data and accounts for the possibility of agreement occurring by chance (Hayes & Krippendorff, 2007). High values of Krippendorff's alpha indicate strong agreement among annotators (Lombard et al., 2002). For each dimension, we conducted 1000 simulations using random subsets of one-third of our total pool of 33 annotators, and we visualized the results in a boxplot to illustrate the distribution and variability of Krippendorff's alpha within these subsets. The same methodology was applied to the LLMs. Additionally, the mean value from these simulations is presented on the boxplot for each dimension, estimating the inter-rater reliability that might be expected across all annotators, or LLMs thereby facilitating comparative analysis.\nRQ2. To address RQ2, which investigated whether Large Language Models (LLMs) are consistent over time when analyzing textual content, we conducted a repeated measures analysis focusing on the intra-model consistency of each LLM across the three time points. Each LLM evaluated the same set of 100 textual items on three separate occasions, allowing us to assess the stability of their ratings over time. For each LLM and dimension, we calculated the Intra-Class Correlation Coefficient (ICC) to quantify the degree of consistency in the ratings across the three time points. The ICC is a reliability index that measures the proportion of variance in the ratings due to the items being rated, relative to the total variance including measurement error (Shrout & Fleiss, 1979). ICC values range from 0 to 1, with higher values indicating greater consistency.\nRQ3. To address RQ4, which examines the extent to which Large Language Models (LLMs) provide analysis comparable to human analysis in terms of quality, we conducted direct statistical comparisons between human annotators and LLMs across all four dimensions of latent content analysis. We began by performing independent samples t-tests for each dimension\u2014sentiment, political leaning, emotional intensity, and sarcasm detection\u2014to compare the mean ratings provided by the human annotators with those generated by the LLMs. Prior to the t-tests, we used Levene's Test to assess the equality of variances, ensuring the appropriate version of the t-test was applied (standard t-test or Welch's t-test). This allowed us to determine whether there were statistically significant differences in the average ratings between humans and LLMs.\nRQ4. To address RQ4, which examines the extent to which reliability, consistency, agreement level and comparability vary across different LLM models? we calculated effect sizes using Cohen's d, providing insight into the magnitude of differences between the groups. Additionally, we conducted a one-way Analysis of Variance (ANOVA) for each dimension to compare mean ratings among all groups, including each individual LLM and the human annotators. When the ANOVA identified significant differences, we performed post hoc tests with Bonferroni correction to pinpoint specific group differences while controlling for Type I errors. This comprehensive statistical approach enabled us to evaluate both the statistical and practical significance of differences between LLMs and human analyses, thereby determining the extent to which LLM-generated annotations are comparable to human annotations in terms of quality across multiple dimensions."}, {"title": "Results", "content": "The comparative analysis between human annotators and large language models (LLMs) across the four dimensions of latent content analysis\u2014sentiment, political leaning, emotional intensity, and sarcasm detection-revealed insightful findings about the reliability and quality of LLMs in replicating human judgments."}, {"title": "RQ1", "content": "Inter-Rater Reliability. To address RQ1, which focused on assessing the consistency among LLMs and human annotators, Krippendorff's alpha was calculated for both human annotators and LLMs in each dimension (Hayes & Krippendorff, 2007). In terms of Sentiment Analysis, an alpha coefficient of 0.95 indicated a very high level of agreement among the 33 human annotators (Figure 1). The narrow interquartile range (IQR) suggested minimal variability, highlighting consensus in evaluating sentiment. Concerning the Political Leaning the alpha value was 0.55, reflecting moderate agreement with a broader IQR. This variability suggests differences in perceiving political leanings, possibly due to subjective interpretations or nuanced content. In Emotional Intensity, an alpha of 0.65 signified fair to good agreement among annotators. While agreement was better than for political leaning, the presence of variability indicated challenges in consistently assessing emotional intensity. Regarding Sarcasm Detection, an alpha of 0.25 pointed to the low agreement among annotators, with a wide IQR and outliers. This low consistency means the inherent difficulty in detecting sarcasm, even among human judges."}, {"title": "RQ2", "content": "ICCs. All LLMs exhibited excellent temporal consistency across all dimensions, with ICCs ranging from 0.981 to 0.998. The highest consistency was observed in the Sentiment Analysis dimension, where ICCs were consistently above 0.995 for all models. This indicates that the LLMs' sentiment evaluations were highly stable over the three time points.\nIn the Political Leaning dimension, ICCs were slightly lower but still indicated high consistency, ranging from 0.989 to 0.993. This suggests that LLMs provided stable assessments of political leaning over time, despite the potential complexity involved in interpreting political content.\nFor Emotional Intensity, ICCs ranged from 0.983 to 0.988, demonstrating strong consistency among the LLMs. Although this dimension involves subjective interpretation of emotional expression, the LLMs maintained a high level of agreement in their ratings across time.\nIn the Sarcasm Detection dimension, ICCs were the lowest among the four dimensions but still indicated high consistency, ranging from 0.981 to 0.986. This mirrors the findings from RQ1, where both humans and LLMs showed low inter-rater reliability in sarcasm detection, suggesting inherent challenges in interpreting sarcasm. Nevertheless, individual LLMs were consistent with themselves over time.\nThe ICCs for each LLM across the four dimensions are presented in Table 1. To further examine the temporal stability of the LLMs, we calculated the mean standard deviation of ratings across the three time points for each LLM and dimension. The mean standard deviations were low across all LLMs and dimensions, further confirming the high temporal consistency of the models."}, {"title": "RQ3", "content": "T-tests. To evaluate the extent to which LLMs provide analysis comparable to human annotators (RQ3), independent sample t-tests were conducted for each dimension.\nIn sentiment analysis, the mean rating from human annotators was 3.19 (SD = 0.11), while LLMs had a mean rating of 3.22 (SD = 0.08). The t-test indicated no significant difference between the two groups, t(55) = -1.097, p = .277, with a small effect size (Cohen's d = -0.29). This suggests that LLMs perform on par with humans in evaluating sentiment, providing ratings that are statistically and practically similar.\nFor political leaning, human annotators had a mean rating of 2.89 (SD = 0.25), and LLMs had a mean of 2.82 (SD = 0.14). The t-test, adjusted for unequal variances due to a significant Levene's test, showed no significant difference, t(51.22) = 1.366, p = .178, with a small effect size (Cohen's d = 0.33). This indicates that LLMs' assessments of political leaning are comparable to those of human annotators, despite some variability.\nIn emotional intensity, human annotators' mean rating was 3.44 (SD = 0.34), whereas LLMs had a mean of 3.19 (SD = 0.17). The t-test revealed a significant difference, t(49.42) = 3.615, p < .001, with a large effect size (Cohen's d = 0.88). This significant disparity indicates that humans perceived and rated emotional intensity higher than LLMs, suggesting that LLMs may underrepresent the emotional nuances apparent to human annotators.\nRegarding sarcasm detection, humans had a mean rating of 3.75 (SD = 0.50), and LLMs had a mean of 3.89 (SD = 0.51). The t-test showed no significant difference between the groups, t(55) = -1.002, p = .323, with a small effect size (Cohen's d = -0.27). This result indicates that both humans and LLMs struggled with sarcasm detection, providing statistically similar but variable ratings.\nANOVAs. One-way ANOVAs were conducted for each dimension to assess differences among the nine groups (eight LLMs and human annotators).\nIn sentiment analysis, the ANOVA revealed no significant differences among groups, (F (8, 48) = 1.514, p = .177, $\\eta\u00b2$ = 0.20), indicating small effect size, and that mean sentiment ratings were consistent across humans and LLMs.\nFor political leaning, the ANOVA showed no significant differences, (F (8, 48) = 0.688, p = .700, $\\eta\u00b2$ = 0.10), suggesting that LLMs' political leaning assessments did not significantly differ from human annotators or among themselves.\nIn emotional intensity, significant differences were found among groups, (F (8, 48) = 2.256, p = .039, $\\eta\u00b2$ = 0.27). Post hoc tests (Bonferroni correction) revealed that human ratings significantly differed from those of certain LLMs. Humans rated emotional intensity higher than GPT-3.5 (p < .001), Mixtral (p = .004), and Gemini (p = .006). These findings indicate that some LLMs may consistently underreport emotional intensity compared to human annotators.\nIn sarcasm detection, the ANOVA showed marginal differences, (F (8, 48) = 2.126, p = .051, $\\eta\u00b2$ = 0.26), hinting at potential differences among groups. While specific significant differences were not conclusively identified, this suggests variability in how LLMs and humans detect sarcasm."}, {"title": "RQ4", "content": "Mean ratings. For RQ4, to identify which LLMs most closely replicate human performance, we compared the mean ratings of each model to human means across all dimensions (Table 2).\nIn sentiment analysis, the LLM GPT-4o-mini had a mean rating of 3.19, identical to the human mean. This exact match suggests that GPT-40-mini provided sentiment evaluations"}]}