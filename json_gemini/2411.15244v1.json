{"title": "Adversarial Prompt Distillation for Vision-Language Models", "authors": ["Lin Luo", "Xin Wang", "Bojia Zi", "Shihao Zhao", "Xingjun Ma"], "abstract": "Large pre-trained Vision-Language Models (VLMs) such as Contrastive Language-Image Pre-Training (CLIP) have been shown to be susceptible to adversarial attacks, raising concerns about their deployment in safety-critical scenarios like autonomous driving and medical diagnosis. One promising approach for improving the robustness of pre-trained VLMs is Adversarial Prompt Tuning (APT), which combines adversarial training with prompt tuning. However, existing APT methods are mostly single-modal methods that design prompt(s) for only the visual or textual modality, limiting their effectiveness in either robustness or clean accuracy. In this work, we propose a novel method called Adversarial Prompt Distillation (APD) that combines APT with knowledge distillation to boost the adversarial robustness of CLIP. Specifically, APD is a bimodal method that adds prompts for both the visual and textual modalities while leveraging a cleanly pre-trained teacher CLIP model to distill and boost the performance of the student CLIP model on downstream tasks. Extensive experiments on multiple benchmark datasets demonstrate the superiority of our APD over the current state-of-the-art APT methods in terms of both natural and adversarial performances. The effectiveness of our APD method validates the possibility of using a non-robust teacher to improve the generalization and robustness of VLMs.", "sections": [{"title": "1. Introduction", "content": "Large pre-trained Vision-Language Models (VLMs) have made significant progress in bridging the visual and language modalities [17, 22, 32]. By aligning visual and textual data within a joint embedding space, these models effectively capture semantic relationships across modalities, enhancing performance on a variety of cross-modal tasks. One notable pre-trained VLM is CLIP [32], which employs contrastive learning to align visual and textual representations by encouraging the embeddings of matched image-text pairs to be close while pushing those of non-matched pairs apart. Given the superior representation capability of these large-scale models, pre-trained VLMs have been widely adopted in different applications to solve challenging real-world problems. However, these foundation models are vulnerable to adversarial attacks [10\u201312, 26, 27, 42] which are one type of test-time attacks that adversarially perturbed the inputs to disrupt the image-text representation alignment. This raises significant concerns on the deployment of pre-trained VLMs in safety-critical scenarios.\nVarious defense strategies [16, 25, 28, 34, 37, 43, 49] have been proposed to address the adversarial vulnerability of pre-trained VLMs (e.g., CLIP), with most of them focusing on the image modality robustness against adversarial image attacks. Among these methods, Adversarial Prompt Tuning (APT) has been shown to be a promising approach [25, 43, 49]. APT methods apply adversarial training [27, 38, 41] during prompt tuning to allow efficient robustification of the CLIP image encoder for downstream tasks. Two representative adversarial prompt tuning methods are AdvPT [43] and APT [25]. AdvPT pre-generates a set of fixed adversarial images based on the image encoder and then tunes textual prompts to defend against those fixed adversarial images. While showing certain robustness on pre-generated adversarial images, AdvPT is still vulnerable to adversarial images generated on the fly during inference. The APT method, on the other hand, exploits an adversarially finetuned CLIP image encoder as a robust backbone [28] to further improve its robustness through text prompt tuning. However, obtaining a robust backbone for CLIP via adversarial full finetune is computationally expensive. Moreover, both methods are single-modal defense methods that utilize textual prompts to address the visual vulnerability, limiting their effectiveness in either robustness or clean accuracy.\nIn this work, we focus on enhancing the robustness of the raw (non-robust) CLIP image encoder to adversarial image attacks and propose a novel defense method named Adversarial Prompt Distillation (APD). APD is a bimodal APT"}, {"title": "2. Related Work", "content": "Adversarial Attacks on VLMs Adversarial attacks on VLMs can be categorized into three types: image-based, text-based, and bimodal attacks. For image-based attacks, classical methods such as FGSM [10], PGD [27], C&W [2] and AutoAttack [5] are commonly used to generate adversarial examples by adding perturbations to the input images. In text-based attacks, adversaries typically modify or replace tokens in the input text [8, 19, 24, 33], disrupting the model\u2019s ability to understand and process the language inputs correctly. Bimodal attacks target both image and text inputs simultaneously. Co-Attack [42] is a white-box bimodal attack that perturbs both input texts and images to distort the multimodal embedding or make the image embedding away from the perturbed text embedding. Other bimodal attacks such as SGA [26], SA-Attack [12], and Ot-Attack [11] focus on generating transferable attacks across different VLMs. In this paper, we focus on the white-box robustness against image modality attacks and test our method against PGD and AA attacks.\nAdversarial Prompt Tuning Prompt tuning has been widely adopted for fine-tuning VLMs [18, 21, 36, 40, 44, 45, 47, 48]. Recently, there has been growing interest in enhancing the adversarial robustness of VLMs using prompt tuning, collectively known as adversarial prompt tuning [3, 25, 43, 49]. Adversarial prompt tuning applies adversarial training [27, 38, 41] during the prompt tuning process to visual, textual, or bimodal prompts. Adversarial Visual Prompting (AVP) [3] employed visual prompting to improve the adversarial robustness of a pre-trained model. TeCoA [28] and PMG-AFT [37] utilized visual prompt tuning to enhance the zero-shot robustness of CLIP. On the other hand, AdvPT [43] and APT [25] applied textual prompt tuning to defend CLIP against image modality attacks. Both methods are single-modal methods that tune the textual prompts only. AdvPT pre-generates static adversarial images before tuning, which makes it vulnerable to dynamic attacks generated on the fly during inference. APT leverages a robust CLIP backbone to ensure robustness to dynamic attacks. However, training a robust CLIP backbone can be computationally expensive. Built upon MaPle [21], Few-shot Adversarial Prompt learning (FAP) [49] is a bimodal defense method that introduces a multimodal feature consistency loss to tune both visual and textual prompts. In this work, we explore bimodal APT methods that can defend CLIP against image modality attacks without relying on robustly trained backbones.\nAdversarial Distillation Our work is closely related to adversarial distillation, which has demonstrated promising defense results compared to standalone adversarial training [9, 15, 20, 29, 31, 46, 50, 51]. Existing research on adversarial distillation primarily focuses on distilling adversarial robustness from a robust teacher model into a non-robust student model. Adversarial Robust Distillation (ARD) [9] aligns the adversarial outputs of student models with the clean outputs of robust teacher models. Adversarial Knowledge Distillation (AKD) [29] mixes the teacher\u2019s outputs on adversarial examples with clean labels and employs early stopping strategies during the training of"}, {"title": "3. Proposed Method", "content": "3.1. Preliminaries\nCLIP A typical CLIP model consists of an image encoder \\(f_v: \\mathcal{I} \\rightarrow \\mathbb{R}^d\\), parameterized by \u03b8v, and a text encoder \\(f_t: \\mathcal{T} \\rightarrow \\mathbb{R}^d\\), parameterized by \u03b8t. These two encoders respectively extract features from images and texts, mapping inputs from different modalities into unified representations within the joint d-dimensional space. For image classification on a dataset \\( \\mathcal{D} = \\{x_i, Y_i\\}_{i=1}^N \\) with C classes, CLIP generates textual descriptions tj using the template \u201c a photo of a CLASS; \u201d for each class name by default. For an input image xi and text tj, their corresponding representations \\(z_v^{(i)}\\) and \\(z_t^{(j)}\\) can be computed as follows:\n\\(z_v^{(i)} = f_v (x_i; \\theta_v), \\quad z_t^{(j)} = f_t (t_j; \\theta_t).\\) (1)\nCLIP calculates the similarity between the image representation \\(z_v^{(i)}\\) and the text representation \\(z_t^{(j)}\\) as the logits:\n\\(L_{i,j} = \\cos (z_v^{(i)}, z_t^{(j)}),\\) (2)\nwhere cos(\u00b7, \u00b7) denotes the cosine similarity. The probability that image xi belongs to the j-th class pij can be obtained as follows:\n\\(p_{i,j} = \\frac{\\exp{(q_{i,j})}}{\\sum_{k=1}^{C} \\exp{(q_{i,k})}}\\) (3)\nBimodal Prompt Tuning for CLIP\nA classic bimodal prompt tuning method is vision-language prompt tuning (VLP) [21] which tunes both the textual prompts Pt and the visual prompts Pv following the same training schedule. Here, we denote the image and text representation outputs of VLP as:\n\\(z_v^{(i)} = f_v (x_i, P_v; \\theta_v), \\quad z_t^{(j)} = f_t (t_j, P_t; \\theta_t).\\) (4)\n3.2. Adversarial Prompt Distillation\nOverview An overview of our proposed method APD is illustrated in Figure 1. APD involves two pre-trained CLIP"}, {"title": "Inner Maximization", "content": "models, designated as the teacher and the student, respectively. The student model takes adversarial images, generated on the fly, as input and learns to align its logits with those of the teacher via the Kullback-Leibler (KL) divergence loss. The teacher model processes only natural images. During the prompt tuning process, it is tuned to minimize the cross-entropy between its outputs and the ground truth to generate high-quality soft labels. At the same time, it receives feedback from the student to assist the student model in aligning its logits with that of the teacher on adversarial samples. Overall, APD defines a bi-level optimization process, involving inner maximization to generate adversarial examples and outer minimization to tune both the teacher and student models via bimodal prompt tuning. Next, we will introduce the inner and outer optimizations in detail.\nInner Maximization During the inner maximization process of APD, adversarial samples are generated on the fly for the student model. Since CLIP processes both text and image data, adversarial examples can be crafted in different modalities: visual, textual, or bimodal. In APD, we focus on visual vulnerability and only generate adversarial examples for the images.\nGiven an input image x, APD generates its adversarial version x' by perturbing the image to maximize the dissimilarity between the image representation and its ground truth text representation (i.e., the representation of the ground truth class prompt). Formally, the maximization process can be defined as:\n\\(\\mathop{\\text{argmax}}\\limits_{x'} \\mathcal{L}_{CE} (S(x'), y), \\text{ s.t. } ||x' - x|| \\le \\epsilon,\\) (5)\nwhere S(x') denotes the logits output of the student model for adversarial example x', \\( \\mathcal{L}_{CE} \\) is the cross-entropy loss, and \u03f5 constrains the magnitude of the perturbation."}, {"title": "Outer Minimization", "content": "Outer Minimization During the outer minimization process, APD employs an online distillation strategy that fine-tunes both the teacher and student models simultaneously. The teacher model processes only the natural (clean) examples, with its optimization involving two terms: one for natural training and the other for receiving feedback from the student. The corresponding minimization process for the teacher model is formulated as follows:\n\\(\\mathop{\\text{argmin}}\\limits_{P^{(T)}} \\mathcal{L}_{CE} (T(x), y) + \\beta \\cdot \\mathcal{L}_{KL} (T(x), S(x')),\\) (6)\nwhere T(x) denotes the logits output of the teacher model for natural input x. The term \\(P^{(T)} = \\{P_v^{(T)}, P_t^{(T)}\\}\\) represents the updated visual and textual prompts for the teacher. The cross-entropy loss \\( \\mathcal{L}_{CE} \\) between the teacher\u2019s logits T(x) and the ground truth y aids the teacher in achieving high natural accuracy, ensuring reliable output soft labels. Meanwhile, the KL divergence \\( \\mathcal{L}_{KL} \\) quantifies the difference between the outputs of the student model S(x') and the teacher model T(x), enabling the teacher to adjust its outputs based on feedback from the student. This feedback assists the student model in more effective training. The hyperparameter \u03b2 balances these two loss terms.\nThe student model, on the other hand, takes adversarial examples generated by Eq. (5) as input, using the soft labels produced by the teacher for supervision. In its optimization, the student model learns robust prompts by minimizing the KL divergence between its probability outputs and the teacher\u2019s soft labels. Formally, the distillation process is formulated as:\n\\(\\mathop{\\text{argmin}}\\limits_{P^{(S)}} \\mathcal{L}_{KL} (S(x'), T(x)),\\) (7)\nwhere \\(P^{(S)} = \\{P_v^{(S)}, P_t^{(S)}\\}\\) represents the updated prompts for the student model. It is important to note that the student input is an adversarial example x' while the teacher input is a natural example x. Since the teacher, trained on clean data, provides soft labels with strong generalization properties, this alignment helps the student become more robust to adversarial perturbations while inheriting the teacher\u2019s generalization capabilities, thereby achieving a better balance between natural accuracy and adversarial robustness."}, {"title": "4. Experiments", "content": "4.1. Experimental Setup\nDatasets and CLIP Models Following previous work [48], we evaluate our defense method on 8 commonly used"}, {"title": "5. Ablation Studies", "content": "Unimodal APD vs. Bimodal APD We compare the effectiveness of unimodal and bimodal APD in defending against adversarial attacks. In unimodal APD, the defense relies on a single modality prompt. When only textual prompts are used, we refer to this approach as APD-T, and when only visual prompts are used, it is termed APD-V. Our proposed method, APD, by default, utilizes both textual and visual prompts for defense. Table 4 shows the average results across eight datasets for APD-T, APD-V, and APD. Consistent with Table 1, using only textual prompts (APD-T) is highly ineffective against adversarial image perturbations, resulting in an average adversarial robustness of 0.\nFurthermore, compared to APD-V (which uses only visual prompts), APD, which leverages both textual and visual prompts, demonstrates superior defense performance with an improvement in adversarial robustness by 3.72%. These results highlight the advantages of using bimodal prompts in adversarial defense.\nPrompt Depth and Length In Figure 2 (left), we examine the impact of prompt depth on the performance of our APD method, compared to the counterpart baseline method, APT-VL. Across varying prompt depths, APD consistently surpasses APT-VL in both natural accuracy and adversarial robustness. The solid line representing APD remains consistently above the dashed line for APT-VL, indicating a clear performance advantage. Furthermore, as prompt depth increases, APD\u2019s adversarial robustness steadily improves, suggesting that deeper prompts enhance the model\u2019s ability to resist adversarial attacks by providing richer, more nuanced representations.\nFigure 2 (right) illustrates the effect of prompt length on APD\u2019s performance. Across different prompt lengths, APD"}, {"title": "Outer Minimization", "content": "Outer Minimization During the outer minimization process, APD employs an online distillation strategy that fine-tunes both the teacher and student models simultaneously. The teacher model processes only the natural (clean) examples, with its optimization involving two terms: one for natural training and the other for receiving feedback from the student. The corresponding minimization process for the teacher model is formulated as follows:\n\\(\\mathop{\\text{argmin}}\\limits_{P^{(T)}} \\mathcal{L}_{CE} (T(x), y) + \\beta \\cdot \\mathcal{L}_{KL} (T(x), S(x')),\\) (6)\nwhere T(x) denotes the logits output of the teacher model for natural input x. The term \\(P^{(T)} = \\{P_v^{(T)}, P_t^{(T)}\\}\\) represents the updated visual and textual prompts for the teacher. The cross-entropy loss \\( \\mathcal{L}_{CE} \\) between the teacher\u2019s logits T(x) and the ground truth y aids the teacher in achieving high natural accuracy, ensuring reliable output soft labels. Meanwhile, the KL divergence \\( \\mathcal{L}_{KL} \\) quantifies the difference between the outputs of the student model S(x') and the teacher model T(x), enabling the teacher to adjust its outputs based on feedback from the student. This feedback assists the student model in more effective training. The hyperparameter \u03b2 balances these two loss terms.\nThe student model, on the other hand, takes adversarial examples generated by Eq. (5) as input, using the soft labels produced by the teacher for supervision. In its optimization, the student model learns robust prompts by minimizing the KL divergence between its probability outputs and the teacher\u2019s soft labels. Formally, the distillation process is formulated as:\n\\(\\mathop{\\text{argmin}}\\limits_{P^{(S)}} \\mathcal{L}_{KL} (S(x'), T(x)),\\) (7)\nwhere \\(P^{(S)} = \\{P_v^{(S)}, P_t^{(S)}\\}\\) represents the updated prompts for the student model. It is important to note that the student input is an adversarial example x' while the teacher input is a natural example x. Since the teacher, trained on clean data, provides soft labels with strong generalization properties, this alignment helps the student become more robust to adversarial perturbations while inheriting the teacher\u2019s generalization capabilities, thereby achieving a better balance between natural accuracy and adversarial robustness."}, {"title": "6. Limitation", "content": "While APD shows significant improvements over adversarial prompt tuning (APT) in both natural accuracy and adversarial robustness, it still has some limitations. The distillation process in APD requires more computational resources than standard tuning, making it more resource-intensive than APT. However, since APD is built on prompt tuning, its overall training time and computational requirements remain manageable. Additionally, a remaining challenge for future work is determining how to select the most effective teacher to maximize the benefits of the distillation process."}, {"title": "7. Conclusion", "content": "In this work, we studied the problem of efficiently fine-tuning large Vision-Language Models (VLMs), such as CLIP, to improve their adversarial robustness on downstream tasks. To tackle this, we introduced a novel method Adversarial Prompt Distillation (APD), which leverages both textual and visual prompts to strengthen defense against image-modality adversarial attacks. APD combines adversarial prompt tuning (APT) with knowledge distillation, using a cleanly pre-trained teacher CLIP model to distill soft-label guidance into the student model. Our extensive experiments on multiple benchmark datasets demon-"}]}