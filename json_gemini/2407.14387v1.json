{"title": "GLAudio Listens to the Sound of the Graph", "authors": ["Aurelio Sulser", "Johann Wenckstern", "Clara Kuempel"], "abstract": "We propose GLAudio: Graph Learning on Au- dio representation of the node features and the connectivity structure. This novel architecture propagates the node features through the graph network according to the discrete wave equation and then employs a sequence learning architec- ture to learn the target node function from the au- dio wave signal. This leads to a new paradigm of learning on graph-structured data, in which infor- mation propagation and information processing are separated into two distinct steps. We theoret- ically characterize the expressivity of our model, introducing the notion of the receptive field of a vertex, and investigate our model's susceptibility to over-smoothing and over-squashing both the- oretically as well as experimentally on various graph datasets.", "sections": [{"title": "1. Introduction", "content": "With the advent of the popular architectures GCN (Kipf & Welling, 2017), GAT (Velickovic et al., 2017), and GIN (Xu et al., 2018), Graph Neural Networks (GNNs) have emerged as a powerful tool for learning on rela- tional data. Despite the theoretical importance of depth for the expressivity of neural networks, most GNNs en- countered in applications are relatively shallow. This is related to two fundamental issues impairing the ex- pressivity of deep GNNs: over-squashing (Alon & Yahav, 2020), (Topping et al., 2021) and over-smoothing (Li et al., 2018), (Oono & Suzuki, 2019). A comprehensive theo- retical framework to assess the representational capabili- ties of GNNs was introduced in (Xu et al., 2018). It es- tablished that GNNs can possess a representational power at most equivalent to the Weisfeiler-Leman graph isomor- phism test when applied to featureless graphs. This reve- lation prompts an intriguing inquiry: what types of func- tions are learnable by GNNs when node features are in- cluded? A detailed characterization of these functions for most GNNs remains elusive, instead, the two primary limi- tations to their expressive power have been extensively ex- plored: over-smoothing and over-squashing. The concept of over-squashing was first introduced by (Alon & Yahav, 2020). They observed that for a prediction task involv- ing long-range interactions between nodes separated by a distance k, a GNN requires an equivalent number of k layers to capture these interactions effectively. As the k- th neighborhood of a vertex expands exponentially with k, the exploding volume of data must be compressed into a fixed-size vector at each vertex v, leading to over- squashing. In (Giovanni et al., 2023), it was explored how over-squashing reduces the class of functions learnable by GNNs. In (Di Giovanni et al., 2023), it was noted that stan- dard GNNs are guided by the heat equation, a smooth- ing process. For such heat-equation guided GNNs, over- smoothing and over-squashing are intrinsically related to the spectral gap of the graph Laplacian, resulting in an in- evitable trade-off between these two issues (Giraldo et al., 2023). In recent years, there has been a significant effort to introduce new architectures addressing these performance impairments, moving beyond heat-equation-guided GNNs. A promising approach are continuous-time GNNs that are often physically inspired (Chamberlain et al., 2021), (Bodnar et al., 2022). One such architecture is GraphCON (Rusch et al., 2022) that models the node features as con- trolled and damped oscillators, coupled via the adjacency structure of the underlying graph. Inspired by the great suc- cess of the Transformer Model in natural language process- ing, a completely new approach called Graph Transformers was proposed (Kreuzer et al., 2021), (Dwivedi & Bresson, 2020), (Ramp\u00e1\u0161ek et al., 2022). Unlike GNNs, graph trans- formers do not propagate the node features over the connec- tivity structure of the graph instead the node features are augmented by a laplacian encoding of the node's position in the graph. In that sense, the encoding of the connectivity structure is separated from the learning task, mitigating the phenomena over-smoothing and over-squashing.\nContribution. Continuing this idea of separating the en- coding of the connectivity structure from the learning task,"}, {"title": "2. GLAudio", "content": "we propose GLAudio. GLAudio propagates the node fea- tures through the graph network according to the discrete wave equation (I) and then uses sequence learning architec- tures like LSTM or the Transformer model to learn the node function $Y_v : \\mathbb{R}^{|V(G)| \\times d_0} \\rightarrow \\mathbb{R}^{d_1}$ at each vertex $v \\in V(G)$ from the wave signal received at v. In that sense, GLAudio separates the feature propagation and signal processing into two distinct steps. Unlike the heat equation, the wave equa- tion preserves the Dirichlet energy. Thus, the node features can be propagated over long distances without smoothing out. Moreover, the compression of information takes place during the encoding of node features and graph structure into a wave signal. Consequently, with increasing reso- lution the signal received at a vertex in GLAudio should be significantly less compressed compared to the fixed-size vector in standard GNNs. These two facts suggest that over-smoothing and over-squashing are mitigated. We are able to characterize the function class learnable by GLAudio and address these two phenomena."}, {"title": "2.1. Wave Signal Encodes Features and Graph Structure", "content": "The discrete wave equation on a graph G with Laplacian matrix L and initial resting configuration $x \\in \\mathbb{R}^{|V(G)| \\times d_0}$ reads\n$\\begin{cases} \\dot{X}(t) = -L \\cdot X(t) \\\\  X(0) = x \\  \\dot{X}(0) = 0. \\end{cases}$\n(I)\nIt is well known that the unique solution is given by the continuous-time signal $X(t) = cos(L^{1/2} \\cdot t) \\cdot x$. The fol- lowing theorem proves that this signal encodes much of the information about the features and the graph structure.\nTheorem 2.1. Given two graphs G, H on the same vertex set with initial features $X_G, X_H$, then we have for the two corresponding wave signals $X_G(t), X_H(t), \\sqrt{t} > 0$\n$X_G|_{[0,t]} = X_H|_{[0,t]} \\iff \\forall n \\in \\mathbb{N}_0: L_H^n X_H = L_G^n X_G$\nMotivated by this fact, we propose to use the signal X(t) as an encoding of the graph's features and structure."}, {"title": "2.2. Model Architecture", "content": "Encoder. To implement the encoder, we solve the differen- tial equation (I) in discrete time using an implicit-explicit ODE scheme (Norsett & Wanner, 1987). Let N denote the number of discrete time steps and T the stopping time. Let $h = \\frac{T}{N}$ be the step size. We denote the approximation of X(ih) by $X^i$ for i = 0, ..., N. The encoder architecture then reads\n$\\begin{cases} X^{i+1} = X^i + hV^{i+1} \\\\ V^{i+1} = V^i - hLX^i \\\\ X^0 = x \\  V^0 = 0. \\end{cases}$\n(II)\nwhere $V^i$ is an auxiliary \"velocity\" variable. We denote the step function provided by the numerical scheme by $X(x,t) = X^0 1_{[0,h]}(t) + \\sum_{i=2}^N X^i 1_{((i-1)h,ih]}(t)$. It is well known that as N increases $X(x, t)$ converges in the function space $L^{\\infty}([0,T]; [\\mathbb{R}^{|V(G)| \\times d_0})$ to the true solution X(x, t) uniformly over all possible initial features of some fixed compact set $C \\subseteq \\mathbb{R}^{|V(G)| \\times d_0}$.\nDecoder. For the decoder model, we can use any se- quence learning architecture. We have tested RNN de- coders, in particular, LSTM (Gers et al., 2000) and CORNN (Rusch & Mishra, 2020). But it might also be interesting to investigate how the Transformer model (Vaswani et al., 2017) or State Space Models (Smith et al., 2022), (Gu & Dao, 2023) perform as a decoder. For sequence learning architectures, universal approximation theorems have been established (Sch\u00e4fer & Zimmermann, 2006), (Lanthaler et al., 2023), (Yun et al., 2019), (Wang & Xue, 2024). In the following, we will make use of the univer- sality of RNNs (Sch\u00e4fer & Zimmermann, 2006) to charac- terize the expressivity of GLAudio. A simple RNN on an input sequence $x_1,...,x_n \\in \\mathbb{R}^{d_0}$ is given by for all $1<i<n$\n$\\begin{cases} s_i = \\sigma(W \\cdot s_{i-1} + U \\cdot x_i) \\\\  y_i = V \\cdot s_i, \\end{cases}$\n(III)\nwhere $W \\in \\mathbb{R}^{s \\times s}, V \\in \\mathbb{R}^{s \\times d_1}, U \\in \\mathbb{R}^{d_0 \\times s}$ and $o$ is a non- linearity, $s_i$ are the hidden states and $y_i$ are the outputs.\nTheorem 2.2 ((Sch\u00e4fer & Zimmermann, 2006)). For any dynamical system $S_{i+1} = g(S_i, X_{i+1}), Y_{i+1} = h(S_{i+1})$, where $g : \\mathbb{R}^s \\times \\mathbb{R}^{d_0} \\rightarrow \\mathbb{R}^s$ measurable and $g : \\mathbb{R}^s \\rightarrow \\mathbb{R}^{d_1}$ continuous, there exists an arbitrary good approximation of $Y_n$ by some $y_n$ of the form (III) uniformly for any input se- quence $X_1,..., X_n$ of some fixed compact set $C \\subseteq \\mathbb{R}^{n \\times d_0}$.\nFor a detailed list of all hyper-parameters and further con- figuration options, we refer to Appendix C."}, {"title": "2.3. Expressivity of GLAudio", "content": "In this section, we provide a thorough characterization of the expressive power of GLAudio. This analysis enables us to articulate how GLAudio potentially alleviates the is- sues of over-squashing and over-smoothing. To ease the notation, we restrict the discussion to the case $d_0 = 1$. The presented results easily generalize to the case of arbitrary $d_0$. Let {$\\phi_i$}$_i$ be an eigenbasis of L. We define the recep- tive field $R_v$ of a vertex v to be the set {$\\phi_i | \\forall i \\in [n]:$"}, {"title": "3. Experiments", "content": "For details on training methods and hyper-parameters, we refer to Appendix C. The code for all experiments is avail- able on https://github.com/AurelioSulser/GLAudio."}, {"title": "3.1. Node Classification on Network Graphs", "content": "For a comparison with other model architectures, we eval- uated the performance of GLAudio on six widely popular semi-supervised node classification single-graph datasets: Cora, CiteSeer, PubMed, Texas, Wisconsin and Cornell. Derived from citation networks, the first three are ho- mophilic, i.e., adjacent vertices tend to have the same class label. The latter three are significantly more heterophilic, i.e. adjacent vertices tend to have different labels. Due to their smoothing bias, this property poses a significant challenge for traditional MPNNs like GCN. Average test accuracies for all six datasets are reported in Table 1.\nDiscussion and Results. We observe that on all six datasets GLAudio is able to successfully learn a classifi- cation strategy. On the heterophilic graph datasets, GLAu- dio outperforms the GCN and GAT architecture, whereas on homophilic graphs of Cora and CiteSeer GCN and GAT achieve higher accuracies. These results match our theo- retic understanding: Both GCN and GAT can be seen as time discretizations of the discrete first-order heat equation equipped with learnable parameters causing them to natu- rally smooth the nodes' features. This bias is useful on homophilic datasets, yet disadvantageous on heterophilic graphs. Derived from the second-order wave equation, GLAudio does not exhibit this flaw, achieving consistently high accuracies on all datasets.\nTo verify that GLAudio is not prone to over-smoothing, we measured its accuracy on homophilic datasets by varying the time steps N, keeping other hyper-parameters constant, notably the stopping time T. Contrary to the effect of over- smoothing, GLAudio's performance improved with more time steps and reached optimal levels within the range of 50 to 200, as shown in Figure 1. This contrasts with the findings in (Chamberlain et al., 2021) where GCNs' accu- racies diminish significantly below 50% with more than 16 layers."}, {"title": "3.2. Graph Regression on ZINC Dataset", "content": "In (Rusch et al., 2022), it was found that the Mean Absolute Error (MAE) of standard GCNs on the ZINC dataset in- creased with the model's depth, while for GraphCon-GCN, the MAE showed a decreasing trend. This indicates the significance of long-range dependencies within the ZINC dataset. Given the capability of GLAudio to support excep- tionally deep models, as highlighted in Figure 1, its poten- tial is explored on ZINC.\nThe ZINC dataset comprises approximately 12,000 molec- ular graphs, curated for the regression of a molecular prop- erty known as constrained solubility. Each graph represents a molecule, where the node features correspond to the types of heavy atoms present, and the edge features denote the types of bonds connecting these atoms.\nDiscussion and Results. GLAudio's initial performance on the ZINC dataset appears modest, performing on par with GIN, and GatedGCN, see Table 2. The reason could be that GLAudio might disproportionately emphasize long- range over short-range interactions as models successful on ZINC typically emphasize the latter (Dwivedi et al., 2022). To test this, we combined a 4-layer GCN (~ 10,000 parameters) and a GLAudio-CoRNN (~ 90,000 parame- ters), allowing GCN to focus on short-range and GLAu- dio on long-range dependencies. The result achieved a test MAE of 0.3157, markedly surpassing standard GNN mod- els and nearing state-of-the-art performance, suggesting GLAudio's proficiency in capturing dependencies missed by shallow GCN."}, {"title": "3.3. Long Range Graph Benchmark: Peptides-struct", "content": "Long Range Graph Benchmark is a set of 5 datasets to measure the ability of GNNs and Graph Transformers to solve problems depending on long-range interactions (Dwivedi et al., 2023). We evaluated GLAudio on one of these tasks, namely Peptides-struct, a multi-dimensional graph regression task on molecular graphs.\nDiscussion and Results. The results in Table 3 show GLAudio achieving on par MAEs with traditional MPNNs but underperforming graph transformers. This suggests that GLAudio might have difficulties capturing the long- range dependencies of Peptides-struct. On the contrary, we observed significant performance enhancements with an increasing number of time steps. Moreover, recent find- ings from (T\u00f6nshoff et al., 2023) reveal that a simple GCN with extensive hyper-parameter tuning can achieve state-of- the-art results (0.2460 MAE) on Peptides-struct. It casts doubt on the suitability of Peptides-struct as a benchmark for long-range graph interactions."}, {"title": "4. Conclusion", "content": "We proposed a novel graph learning architecture based on wave propagation. A central distinction from other mod- els lies in the separation of information propagation and information processing into two different steps. This sep- aration allows for deep feature propagation without over- smoothing. Moreover, our theoretical study of expressivity provides a new approach to understanding over-squashing as a miss-alignment between graph structure and task. In our empirical studies, GLAudio was benchmarked against"}, {"title": "A. Proof of Theorem 2.1", "content": "We remark that the power series representation of the signal XH (t) is given by\n$X_H(t) = cos \\left(L_H^{1/2} t\\right) X_H = \\sum_{n=0}^{\\infty} \\frac{(-1)^n L_H^n t^{2n}}{(2n)!} X_H$\nIt is thus immediate that $\u2200n \\in \\mathbb{N}_0: L_H^n X_H = L_xG$ implies that\n$X_H(t) = X_G(t)$.\nFor the other implication, we note that $X_G|_{[0,t]} = X_H|_{[0,t]}$ implies that\n$\\forall n \\in \\mathbb{N}_0: \\frac{d^{(2n)}}{dx^{(2n)}} X_G(0) = \\frac{d^{(2n)}}{dx^{(2n)}} X_H (0)$.\nAccording to the power series representation of $X_G(t)$ and $X_H (t)$, this is equivalent to\n$L_H^n X_H = L_G^n X_G$"}, {"title": "B. Proof of Theorem Theorem 2.3", "content": "Before we present the actual proof, we introduce the following definition.\nWe call an operator $\\Phi : (L^{\\infty} ([0, 1]; \\mathbb{R}), || \\cdot ||_{L^{\\infty}}) \\rightarrow (L^{\\infty} ([0, 1]; \\mathbb{R}), || \\cdot ||_{L^{\\infty}})$ causal if for any two input signals u, v $\\in$ $L^{\\infty} ([0, 1]; \\mathbb{R})$\n$u|_{[0,t]} = v|_{[0,t]} \\rightarrow \\Phi(u)|_{[0,t]} = \\Phi(v)|_{[0,t]}.$We begin the proof with the claim that if there exists a continuous (with respect to the $L^{\\infty}$-norm on the input-/output-signals), causal operator $\\Phi$ such that $\\Phi(X)(T) = Y(x)$, then we can conclude.\nProof of the claim. As a preliminary step, we define K = {X(x)|\u22001 < i < N, for all initial configuration x \u2208 C}. It follows from compactness of C that K is compact. In the following, we will give a description of \u03a6(X)(T) as a dynamical system and apply Theorem 2.2 to this description together with the compact set K.\nFor an arbitrary input sequence $X_1, . . ., X_N$, set $S_i = (i, X_i, V_i)$, where $V_i = V_{i-1} - h \\cdot L \\cdot X_{i-1}$ and $V_0 = 0$. It is clear that $S_{i+1}$ is computable from $S_i, X_{i+1}$, hence there exists a measurable function g such that $S_{i+1} = g(S_i, X_{i+1})$. We note that for the special input sequence $X_1 = X_v, ..., X_N = X_v$, we have that $S_\u2081 = (i, X_v, V^i_v)$.\nTo define the function $h(S_i)$, let us introduce vectors $\\tilde{X}_1, . . ., \\tilde{X}_i, \\tilde{V}_1, . . ., \\tilde{V}_i$ (that are in some sense approximations of $X_1,..., X_i, V_1, ..., V_i$) given by\n$\\begin{aligned} \\tilde{X}_i &= X_{j+1} \u2013 h\\tilde{V}_{j+1} \\\\ \\tilde{V}_j &= \\tilde{V}_{j+1} + h \\cdot L \\cdot \\tilde{X}_i \\\\ \\tilde{X}_i &= X \\  \\tilde{V}_i = V^i. \\end{aligned}$\nIn the special case where $X_1 = X^v, ..., X_N = X^v$, we have that $\\tilde{X}_1 = X_1, ..., \\tilde{X}_N = X^N_v$. We define the continuous function f : $\\mathbb{R}^{|V(G)|} \\times \\mathbb{R}^{|V(G)|} \\rightarrow L^{\\infty}([0,1]; \\mathbb{R})$ given by $f(\\tilde{X}_i, \\tilde{V}_i)(t) = \\tilde{X}_i 1_{[0,1]}(t) + \\sum_{i=2}^N \\tilde{X}_i 1_{((i-1)h,ih]}(t)$. This allows us to define $h(S_i) = (f(\\tilde{X}_i, \\tilde{V}_i))(ih)$. We note that in the special case that $X_1 = X_v, ..., X_N = X^N_v$ that $f(\\tilde{X}_i, \\tilde{V}_i)|_{[0,ih]} = X_v|_{[0,ih]}$ and hence by causality of \u0424, we have that h(Si) = \u03a6(Xv)(ih). Applying Theorem 2.2 to this dynamic system with compact set K, we get the output of some RNN $Y_N$ satisfies that \u2200x \u2208 C : ||\u03a6(X(x))(Nh) \u2013 $Y_N(\\tilde{X}^1_v(x), ..., \\tilde{X}^N_v(x))$|| \u2264 \u20ac. Provided we picked N large enough such that \u2200x \u2208 C : ||Y(x) \u2013 \u03a6(X(x))(Nh)|| = ||\u03a6(X(x))(Nh) \u2013 \u03a6(X(x))(Nh)|| \u2264 \u20ac, we can combine the two bounds to obtain the statement of the theorem."}]}