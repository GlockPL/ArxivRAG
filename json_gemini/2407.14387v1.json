{"title": "GLAudio Listens to the Sound of the Graph", "authors": ["Aurelio Sulser", "Johann Wenckstern", "Clara Kuempel"], "abstract": "We propose GLAudio: Graph Learning on Audio representation of the node features and the connectivity structure. This novel architecture propagates the node features through the graph network according to the discrete wave equation and then employs a sequence learning architecture to learn the target node function from the audio wave signal. This leads to a new paradigm of learning on graph-structured data, in which information propagation and information processing are separated into two distinct steps. We theoretically characterize the expressivity of our model, introducing the notion of the receptive field of a vertex, and investigate our model's susceptibility to over-smoothing and over-squashing both theoretically as well as experimentally on various graph datasets.", "sections": [{"title": "1. Introduction", "content": "With the advent of the popular architectures GCN (Kipf & Welling, 2017), GAT (Velickovic et al., 2017), and GIN (Xu et al., 2018), Graph Neural Networks (GNNs) have emerged as a powerful tool for learning on relational data. Despite the theoretical importance of depth for the expressivity of neural networks, most GNNs encountered in applications are relatively shallow. This is related to two fundamental issues impairing the expressivity of deep GNNs: over-squashing (Alon & Yahav, 2020), (Topping et al., 2021) and over-smoothing (Li et al., 2018), (Oono & Suzuki, 2019). A comprehensive theoretical framework to assess the representational capabilities of GNNs was introduced in (Xu et al., 2018). It established that GNNs can possess a representational power at most equivalent to the Weisfeiler-Leman graph isomorphism test when applied to featureless graphs. This revelation prompts an intriguing inquiry: what types of functions are learnable by GNNs when node features are included? A detailed characterization of these functions for most GNNs remains elusive, instead, the two primary limitations to their expressive power have been extensively explored: over-smoothing and over-squashing. The concept of over-squashing was first introduced by (Alon & Yahav, 2020). They observed that for a prediction task involving long-range interactions between nodes separated by a distance k, a GNN requires an equivalent number of k layers to capture these interactions effectively. As the k-th neighborhood of a vertex expands exponentially with k, the exploding volume of data must be compressed into a fixed-size vector at each vertex v, leading to over-squashing. In (Giovanni et al., 2023), it was explored how over-squashing reduces the class of functions learnable by GNNs. In (Di Giovanni et al., 2023), it was noted that standard GNNs are guided by the heat equation, a smoothing process. For such heat-equation guided GNNs, over-smoothing and over-squashing are intrinsically related to the spectral gap of the graph Laplacian, resulting in an inevitable trade-off between these two issues (Giraldo et al., 2023). In recent years, there has been a significant effort to introduce new architectures addressing these performance impairments, moving beyond heat-equation-guided GNNs. A promising approach are continuous-time GNNs that are often physically inspired (Chamberlain et al., 2021), (Bodnar et al., 2022). One such architecture is GraphCON (Rusch et al., 2022) that models the node features as controlled and damped oscillators, coupled via the adjacency structure of the underlying graph. Inspired by the great success of the Transformer Model in natural language processing, a completely new approach called Graph Transformers was proposed (Kreuzer et al., 2021), (Dwivedi & Bresson, 2020), (Ramp\u00e1\u0161ek et al., 2022). Unlike GNNs, graph transformers do not propagate the node features over the connectivity structure of the graph instead the node features are augmented by a laplacian encoding of the node's position in the graph. In that sense, the encoding of the connectivity structure is separated from the learning task, mitigating the phenomena over-smoothing and over-squashing.\nContribution. Continuing this idea of separating the encoding of the connectivity structure from the learning task,"}, {"title": "2. GLAudio", "content": "we propose GLAudio. GLAudio propagates the node features through the graph network according to the discrete wave equation (I) and then uses sequence learning architectures like LSTM or the Transformer model to learn the node function Y_v : R^{|V(G)|\\times d_0} \\rightarrow R^{d_1} at each vertex v \\in V(G) from the wave signal received at v. In that sense, GLAudio separates the feature propagation and signal processing into two distinct steps. Unlike the heat equation, the wave equation preserves the Dirichlet energy. Thus, the node features can be propagated over long distances without smoothing out. Moreover, the compression of information takes place during the encoding of node features and graph structure into a wave signal. Consequently, with increasing resolution the signal received at a vertex in GLAudio should be significantly less compressed compared to the fixed-size vector in standard GNNs. These two facts suggest that over-smoothing and over-squashing are mitigated. We are able to characterize the function class learnable by GLAudio and address these two phenomena."}, {"title": "2.1. Wave Signal Encodes Features and Graph\nStructure", "content": "The discrete wave equation on a graph G with Laplacian matrix L and initial resting configuration $x \\in R^{|V(G)|\\times d_0}$ reads\n\n$\\begin{cases}\nX(t) = -L \\cdot X(t)\\\\\nX(0) = x \\\\\n\\dot{X}(0) = 0\n\\end{cases}$\n(I)\n\nIt is well known that the unique solution is given by the continuous-time signal $X(t) = cos(L^{1/2} \\cdot t) \\cdot x$. The following theorem proves that this signal encodes much of the information about the features and the graph structure.\nTheorem 2.1. Given two graphs G, H on the same vertex set with initial features $X_G, X_H$, then we have for the two corresponding wave signals $X_G(t), X_H(t), \\sqrt{t} > 0$\n\n$X_G|_{[0, t]} = X_H|_{[0, t]} \\qquad \\forall n \\in \\mathbb{N}_0 : L_H^n X_H = L_G^n X_G$\n\nMotivated by this fact, we propose to use the signal X(t) as an encoding of the graph's features and structure."}, {"title": "2.2. Model Architecture", "content": "Encoder. To implement the encoder, we solve the differential equation (I) in discrete time using an implicit-explicit ODE scheme (Norsett & Wanner, 1987). Let N denote the number of discrete time steps and T the stopping time. Let $h = \\frac{T}{N}$ be the step size. We denote the approximation of X(ih) by $X^i$ for i = 0, ..., N. The encoder architecture then reads\n\n$\\begin{cases}\nX^{i+1} = X^i + hV^{i+1}\\\\\nV^{i+1} = V^i - hLX^i\\\\\nX^0 = x \\qquad V^0 = 0\n\\end{cases}$\n(II)\n\nwhere $V^i$ is an auxiliary \"velocity\" variable. We denote the step function provided by the numerical scheme by $X(x, t) = X^0 1_{[0, h]}(t) + \\sum_{i=2}^N X^i 1_{[(i-1)h, ih]}(t)$. It is well known that as N increases X(x, t) converges in the function space $L^{\\infty}([0, T]; [R^{|V(G)|\\times d_0})$ to the true solution X(x, t) uniformly over all possible initial features of some fixed compact set $C \\subseteq R^{|V(G)|\\times d_0}$.\nDecoder. For the decoder model, we can use any sequence learning architecture. We have tested RNN decoders, in particular, LSTM (Gers et al., 2000) and CORNN (Rusch & Mishra, 2020). But it might also be interesting to investigate how the Transformer model (Vaswani et al., 2017) or State Space Models (Smith et al., 2022), (Gu & Dao, 2023) perform as a decoder. For sequence learning architectures, universal approximation theorems have been established (Sch\u00e4fer & Zimmermann, 2006), (Lanthaler et al., 2023), (Yun et al., 2019), (Wang & Xue, 2024). In the following, we will make use of the universality of RNNs (Sch\u00e4fer & Zimmermann, 2006) to characterize the expressivity of GLAudio. A simple RNN on an input sequence $x_1, ..., x_n \\in R^{d_0}$ is given by for all $1 < i < n$\n\n$\\begin{cases}\ns_i = \\sigma(W \\cdot s_{i-1} + U \\cdot x_i)\\\\\ny_i = V \\cdot s_i\n\\end{cases}$\n(III)\nwhere $W \\in R^{s\\times s}, V \\in R^{s \\times d_1}, U \\in R^{d_0\\times s}$ and $o$ is a non-linearity, $s_i$ are the hidden states and $y_i$ are the outputs.\nTheorem 2.2 ((Sch\u00e4fer & Zimmermann, 2006)). For any dynamical system $S_{i+1} = g(S_i, X_{i+1}), Y_{i+1} = h(S_{i+1})$, where $g: R^s \\times R^{d_0} \\rightarrow R^s$ measurable and $g : R^s \\rightarrow R^{d_1}$ continuous, there exists an arbitrary good approximation of $Y_n$ by some $y_n$ of the form (III) uniformly for any input sequence $X_1, ..., X_n$ of some fixed compact set $C \\subseteq R^{n\\times d_0}$.\nFor a detailed list of all hyper-parameters and further configuration options, we refer to Appendix C."}, {"title": "2.3. Expressivity of GLAudio", "content": "In this section, we provide a thorough characterization of the expressive power of GLAudio. This analysis enables us to articulate how GLAudio potentially alleviates the issues of over-squashing and over-smoothing. To ease the notation, we restrict the discussion to the case $d_0 = 1$. The presented results easily generalize to the case of arbitrary $d_0$. Let ${\\phi_i}_i$ be an eigenbasis of L. We define the receptive field $R_v$ of a vertex v to be the set ${\\phi_i | \\forall i \\in [n] : (v, \\phi_i) \\neq 0}$"}, {"title": "3. Experiments", "content": "For details on training methods and hyper-parameters, we refer to Appendix C. The code for all experiments is available on https://github.com/AurelioSulser/GLAudio."}, {"title": "3.1. Node Classification on Network Graphs", "content": "For a comparison with other model architectures, we evaluated the performance of GLAudio on six widely popular semi-supervised node classification single-graph datasets: Cora, CiteSeer, PubMed, Texas, Wisconsin and Cornell. Derived from citation networks, the first three are homophilic, i.e., adjacent vertices tend to have the same class label. The latter three are significantly more heterophilic, i.e. adjacent vertices tend to have different labels. Due to their smoothing bias, this property poses a significant challenge for traditional MPNNs like GCN. Average test accuracies for all six datasets are reported in Table 1.\nDiscussion and Results. We observe that on all six datasets GLAudio is able to successfully learn a classification strategy. On the heterophilic graph datasets, GLAudio outperforms the GCN and GAT architecture, whereas on homophilic graphs of Cora and CiteSeer GCN and GAT achieve higher accuracies. These results match our theoretic understanding: Both GCN and GAT can be seen as time discretizations of the discrete first-order heat equation equipped with learnable parameters causing them to naturally smooth the nodes' features. This bias is useful on homophilic datasets, yet disadvantageous on heterophilic graphs. Derived from the second-order wave equation, GLAudio does not exhibit this flaw, achieving consistently high accuracies on all datasets.\nTo verify that GLAudio is not prone to over-smoothing, we measured its accuracy on homophilic datasets by varying the time steps N, keeping other hyper-parameters constant, notably the stopping time T. Contrary to the effect of over-smoothing, GLAudio's performance improved with more time steps and reached optimal levels within the range of 50 to 200, as shown in Figure 1. This contrasts with the findings in (Chamberlain et al., 2021) where GCNs' accuracies diminish significantly below 50% with more than 16 layers."}, {"title": "Over-smoothing", "content": "There exist a number of measures in the literature on over-smoothing in deep GNNs, e.g. measures based on the Dirichlet energy (Cai & Wang, 2020), (Zhao & Akoglu, 2019) or on the mean-average distance (MAD) (Chen et al., 2020), (Zhou et al., 2020). The survey paper (Rusch et al., 2023) gives a unified, rigorous, and tractable definition.\nDefinition 2.4. (Rusch et al., 2023) Given a sequence of GNNs on a graph G, where the (N + 1)-th GNN differs from the N-th GNN by exactly one additional layer. We say that the sequence over-smooths w.r.t. some node similarity $\\mu$ if $\\mu(y^N, y^{N+1}) \\leq C_1 e^{-c_2 N}$, where $y^N$ is the output of the N-th GNN and $C_1, C_2 > 0$.\nNote that in our model, as we increase N for fixed stopping time T, the input signal X(x, t) of the RNN converges towards the true signal X(x, t) and thus the output $y_n$ can only become more accurate. This implies that provided there are two vertices $u, v$ such that $Y_u(T) \\neq Y_v(T)$ and using Theorem 2.3, $\\mu(y_n)$ cannot converge to 0."}, {"title": "Over-squashing", "content": "To formalize the concept of over-squashing, (Topping et al., 2021) described it in terms of the impact of a distant node's feature $x_u$ on the prediction $y_v$ at node v. They noted an exponential decay in $\\frac{\\partial y_v}{\\partial x_u}$ with increasing distance between nodes u and v in standard GNNs. Utilizing Theorem 2.3, we find that the output $y_v$ reacts similar to $Y_v$ to changes along $R_v$ while being insensitive to perturbations perpendicular to $R_v$. This nuanced understanding enables a more targeted approach to addressing the limitations of GLAudio in handling long-range interactions."}, {"title": "4. Conclusion", "content": "We proposed a novel graph learning architecture based on wave propagation. A central distinction from other models lies in the separation of information propagation and information processing into two different steps. This separation allows for deep feature propagation without over-smoothing. Moreover, our theoretical study of expressivity provides a new approach to understanding over-squashing as a miss-alignment between graph structure and task. In our empirical studies, GLAudio was benchmarked against"}]}