{"title": "Graphusion: A RAG Framework for Scientific Knowledge Graph Construction with a Global Perspective", "authors": ["Rui Yang", "Boming Yang", "Aosong Feng", "Sixun Ouyang", "Moritz Blum", "Tianwei She", "Yuang Jiang", "Freddy Lecue", "Jinghui Lu", "Irene Li"], "abstract": "Knowledge Graphs (KGs) are crucial in the field of artificial intelligence and are widely used in downstream tasks, such as question-answering (QA). The construction of KGs typically requires significant effort from domain experts. Large Language Models (LLMs) have recently been used for Knowledge Graph Construction (KGC). However, most existing approaches focus on a local perspective, extracting knowledge triplets from individual sentences or documents, missing a fusion process to combine the knowledge in a global KG. This work introduces Graphusion, a zero-shot KGC framework from free text. It contains three steps: in Step 1, we extract a list of seed entities using topic modeling to guide the final KG includes the most relevant entities; in Step 2, we conduct candidate triplet extraction using LLMs; in Step 3, we design the novel fusion module that provides a global view of the extracted knowledge, incorporating entity merging, conflict resolution, and novel triplet discovery. Results show that Graphusion achieves scores of 2.92 and 2.37 out of 3 for entity extraction and relation recognition, respectively. Moreover, we showcase how Graphusion could be applied to the Natural Language Processing (NLP) domain and validate it in an educational scenario. Specifically, we introduce TutorQA, a new expert-verified benchmark for QA, comprising six tasks and a total of 1,200 QA pairs. Using the Graphusion-constructed KG, we achieve a significant improvement on the benchmark, for example, a 9.2% accuracy improvement on sub-graph completion.", "sections": [{"title": "CCS Concepts", "content": "\u2022 Information systems \u2192 Language models; Extraction, transformation and loading; Database utilities and tools."}, {"title": "Keywords", "content": "Retrieval Augmented Generation, Knowledge Graphs, Question-Answering"}, {"title": "Related Work", "content": "Knowledge Graph Construction KGC aims to create a structured representation of knowledge in the form of a KG. A KG can be created from various sources, such as databases or texts. In our work, we focus on KGC from natural language resources. Research on KGs spans various domains, including medical, legal, and more. Typically, KGC from text involves several methods such as entity extraction and link prediction, with a significant focus on supervised learning. Recently, LLMs have been utilized in KGC relying on their powerful zero-shot capabilities. Although relevant works propose pipelines for extracting knowledge, they often remain limited to localized views, such as extracting triplets from the sentence or paragraph level. In contrast, our work focuses on shifting from a local perspective to a global one, aiming to generate a more comprehensive KG. Approaches such as GraphRAG which uses graph indexing and community detection to generate query-focused summaries, effectively answer global questions. However, GraphRAG focuses less on the detailed steps of graph construction, such as entity resolution and relation inference. In contrast, this work places greater emphasis on the process of global KGC, including entity merging, conflict"}, {"title": "Graphusion: Zero-shot Knowledge Graph Construction", "content": "We now introduce our Graphusion framework for constructing scientific KGs, shown in Fig. 2. Our approach addresses three key challenges in zero-shot KGC: 1) the input consists of free text rather than a predefined list of entities; 2) the relations encompass multiple types, and conflicts may exist among them; and 3) the output is not a single binary label but a list of triplets, making evaluation more challenging."}, {"title": "Problem Definition", "content": "A KG is defined as a set of triplets KG = {(hi, ri, ti) | hi, ti \u2208 E, ri \u2208 R, i = 1, 2, ..., n}, where E is the set of entities, R is the set of possible relations, and n is the total number of triplets in the KG. The task of zero-shot KGC involves taking a set of free text T and generating a list of triplets (h, r, t) spanning a KG. Optionally, there is an expert-annotated KG, GE, as input, in order to provide existing knowledge. In our setting, the number of triplets of KG is much larger than GE. We select the domain to be NLP, so the entities are limited to NLP entities, with other entity types such as people, and organizations"}, {"title": "Step 1: Seed Entity Generation", "content": "Extracting domain-specific entities using LLMs under a zero-shot setting is highly challenging due to the absence of predefined entity lists. This process is not only resource-intensive but also tends to generate a large number of irrelevant entities, or entities with a bad granularity, thereby compromising the quality of extraction. To address these issues, we adopt a seed entity generation approach for efficiently extracting in-domain entities from free text . Specifically, we utilize BERTopic for topic modeling to identify representative entities for each topic. These representative entities serve as seed entities, denoted as Q. The initialized seed entities ensure high relevance in entity extraction and provide certain precision for subsequent triplet extraction."}, {"title": "Step 2: Candidate Triplet Extraction", "content": "These seed entities obtained from Step 1 would guide us to conduct entity extraction. In Step 2, we begin extracting candidate triplets from the free text. Each time, we input an entity q \u2208 Q ({query}) as the query entity and retrieve documents containing this entity ({context}) through information retrieval. Our goal is to extract any potential triplet that includes this query entity. To achieve this, we design a Chain-of-Thought (CoT) prompt. We first instruct the LLMs to extract in-domain entities, and then identify the possible relations between those entities and q. Then, we ask LLMs to discover novel triplets, even if q is not initially included. This approach ensures that the seed entities play a leading role in guiding the extraction of in-domain entities. Meanwhile, the candidate triplets will encompass novel entities. We design the Extraction Prompt to be the following:\nGiven a context {context} and a query entity {query}, do the following:"}, {"title": "Step 3: Knowledge Graph Fusion", "content": "The triplets extracted in the previous step provide a local view rather than a global perspective of each query entity. Due to the limitations of context length, achieving a global view is challenging. Additionally, the relations extracted between two entities can be conflicting, diverse, or incorrect, such as (neural summarization methods, Used-for, abstractive summarization) and (neural summarization methods, Hyponym-of, abstractive summarization). To address the aforementioned challenge, we propose the fusion step. This approach helps reconcile conflicting relations, integrate diverse or incorrect relations effectively, and ultimately provides a global understanding of an entity pair. Specifically, for each query entity q, we first query from ZS \u2013 KG, and obtain a sub-graph that contains q:\nLLM-KG = {(h, r, t) \u2208 ZS-KG | h = q or t = q}.\nOptionally, if there is an expert-annotated KG GE available, we will also query a sub-graph, marked as GE-G. Moreover, we conduct a dynamic retrieval of q again from the free text ({background}), to help LLMs to have a better understanding on how to resolve the conflicted triplets. This key fusion step focuses on three parts: 1) entity merging: merge semantically similar entities, i.e., NMT vs neural machine translation; 2) conflict resolution: for each entity pair, resolve any conflicts and choose the best one; and 3) novel triplet inference: propose new triplet from the background text. We utilize the following Fusion Prompt:\nPlease fuse two sub-knowledge graphs about the entity: {entity}.\nGraph 1:\n{LLM-KG}\nGraph 2: {E-G}\nRules for Fusing the Graphs:\n1. Union the entities and edges.\n2. If two entities are similar, or refer to the same entity, merge them into one entity, keeping the one that is meaningful or specific.\n3. Only one relation is allowed between two entities. If there is a conflict, read the ### Background to help you keep the correct relation...\n4. Once step 3 is done, consider every possible entity pair not covered in step 2. Then, please refer to ### Background to summarize new triplets.\n### Background:\n{background}"}, {"title": "Experiments on Knowledge Graph Construction", "content": "In these experiments, we investigated the general capabilities of Graphusion for KGC.\nDataset To conduct scientific KGC, we need a large-scale free-text corpus to serve as the knowledge source. We collect the proceedings papers from the ACL conference spanning 2017-2023, which includes a total of 4,605 valid papers. Considering that abstracts provide high-density, noise-free information and save computational resources, we perform topic modeling and KGC on the paper abstracts.\nImplementation We implement Graphusion on top of four settings with different LLMs: LLaMa3-70b, GPT-3.5, GPT-4 and GPT-4o. Additionally, we compare with multiple baselines, including zero-shot (GPT-4o zs) and RAG (GPT-4o RAG). We query what is the relation of a node pair as the prompt, and the zero-shot setting answers directly, while the RAG one answers with retrieved results from the same data with Graphusion.\nBaseline We compare with a local graph model (GPT-4o Local), which equals to the Graphusion model without the fusion step (Step 3). Note that it is challenging to evaluate the entity quality, as a simple prompt will generate out-of-domain nodes, so we focus on comparing the relation quality. We also compare with the GraphRAG framework. Like Graphusion, we first tune the prompt with zero-shot and CoT settings for GraphRAG' entity/relation extraction. We define the relation types within the prompt. We also utilize GraphRAG's prompt auto-tuning ability to create community reports to adapt the generated knowledge graph to the NLP domain."}, {"title": "Experiments on Link Prediction", "content": "While the task of KGC is to generate a list of triplets, including entities and their corresponding relations, we also evaluate a sub-task: focusing solely on Link Prediction (LP) for pre-defined entity pairs and a single relation type,r =Prerequisite_of. Specifically, given an entity pair (A, B), the task is to determine if a relation r exists between two given entities. For instance, to learn the entity of POS Tagging, one must first understand the Viterbi Algorithm. Initially, a predefined set of entities C is given.\nWe then design an LP Prompt to solve the task. The core part is to provide the domain name, the definition and description of the dependency relation to be predicted, and the query entities. Meanwhile, we explore whether additional information, such as entity definitions from Wikipedia and neighboring entities from training data (when available), would be beneficial."}, {"title": "TutorQA: A Scientific Knowledge Graph QA Benchmark", "content": "We aim to evaluate the practical usefulness of the Graphusion-constructed KG from an educational perspective. In NLP classes, students often have specific questions that require answers grounded in NLP domain knowledge, rather than general or logistical queries related to the course. To address this need, we introduce the TutorQA benchmark, a QA dataset designed for scientific KG QA.\nTutorQA consists of six categories, encompassing a total of 1,200 QA pairs that have been validated by human experts, simulating questions typically encountered in classes. These questions extend beyond simple syllabus inquiries, covering more complex and challenging topics that require KG reasoning, along with proficiency in text comprehension and question answering. We list some similar benchmarks in Tab 4. While numerous open-domain QA benchmarks exist, our focus has been primarily on those within the scientific domain and tailored for college-level education, aligning with our objective to compare with benchmarks that can emulate a learning"}, {"title": "TutorQA Tasks", "content": "We design different difficulty levels of the questions and divide them into 6 tasks. We summarize the tasks and provide example data in Fig 4. More data statistics and information can be found in the supplementary materials.\nTask 1: Relation Judgment The task is to assess whether a given triplet, which connects two entities with a relation, is accurate.\nTask 2: Prerequisite Prediction The task helps students by mapping out the key entities they need to learn first to understand a complex target topic.\nTask 3: Path Searching This task helps students identify a sequence of intermediary entities needed to understand a new target entity by charting a path from the graph.\nTask 4: Sub-graph Completion The task involves expanding the KG by identifying hidden associations between entities in a subgraph.\nTask 5: Similar Entities The task requires identifying entities linked to a central idea to deepen understanding and enhance learning, aiding in the creation of interconnected curriculums.\nTask 6: Idea Hamster The task prompts participants to develop project proposals by applying learned entities to real-world contexts, providing examples and outcomes to fuel creativity."}, {"title": "Scientific Knowledge Graph Question Answering", "content": "To address TutorQA tasks, we first utilize the Graphusion framework to construct an NLP KG. Then we design a framework for the interaction between the LLM and the graph, which includes two steps: command query and answer generation. In the command query stage, an LLM independently generates commands to query the graph upon receiving the query, thereby retrieving relevant paths. During the answer generation phase, these paths are provided to the LLM as contextual prompts, enabling it to perform QA."}, {"title": "Evaluation Metrics", "content": "Accuracy We report the accuracy score for Task 1 and Task 4, as they are binary classification tasks.\nSimilarity score For Tasks 2 and 3, the references consist of a list of entities. Generally, LLMs demonstrate creativity by answering with novel entities, which are often composed of more contemporary and fresh words, even though they might not exactly match the words in the graph. Consequently, conventional evaluation metrics like keyword matching are unsuitable for these tasks. To address this, we propose the similarity score. This metric calculates the semantic similarity between the entities in the predicted list Cpred and the ground truth list Cgold. Specifically, as shown in Eq 1, for an entity m from the predicted list, and an entity n from the ground truth list, we calculate the cosine similarity between their embeddings achieved from pre-trained BERT model . We then average these similarity scores to obtain the similarity score.\nScore = \\frac{\\sum_{m\\in C_{pred}} \\sum_{n \\in C_{gold}} sim(m, n)}{|C_{pred} x C_{gold}|}       \\qquad (1)\nBy averaging the similarity scores, the final score provides a comprehensive measure of the overall semantic alignment between the predicted and ground truth entities.\nHit Rate For Task 5, we employ the classical Hit Rate metric, expressed as a percentage. This measure exemplifies the efficiency of LLM at retrieving and presenting relevant entities in its output as compared to a provided list of target entities."}, {"title": "Experimental Setup", "content": "In our experimental setup, we employed Hugging Face's  and  for LLaMA2 and LLaMA3 on a cluster equipped with 8 NVIDIA A100 GPUs. For GPT-3.5 and GPT-4, we used OpenAI's gpt-3.5-turbo, gpt-4-1106-preview, and gpt-4o APIs, respectively, each configured with a temperature setting of zero. The RAG models are implemented using Embedchain . To solve TutorQA tasks, we implemented our pipeline using LangChain. The total budget spent on this project, including the cost of the GPT API service, is approximately 500 USD."}, {"title": "Additional Corpora Description", "content": "TutorialBank We obtained the most recent version of TutorialBank from the authors, which consists of 15,583 manually curated resources. This collection includes papers, blog posts, textbook chapters, and other online resources. Each resource is accompanied by metadata and a publicly accessible URL. We downloaded the resources from these URLs and performed free text extraction. Given the varied data formats such as PDF, PPTX, and HTML, we encountered some challenges during text extraction. To ensure text quality, we filtered out sentences shorter than 25 words. Ultimately, this process yielded 559,217 sentences suitable for RAG and finetuning experiments.\nNLP-Papers We downloaded conference papers from EMNLP, ACL, and NAACL spanning the years 2021 to 2023. Following this, we utilized Grobid (https://github.com/kermitt2/grobid) for text extraction, resulting in a collection of 4,787 documents with clean text."}, {"title": "Ablation Study", "content": "Prompting Strategies In Tab. 7, we explore the impact of different prompting strategies for entity graph recovery, comparing CoT and zero-shot prompts across both NLP and CV domains. The results indicate the introduction of CoT is not improving. We further find that CoT Prompting more frequently results in negative predictions. This finding serves as a drawback for our study, as it somewhat suppresses the performance of our system. This observation highlights the need to balance the impact of CoT on the rigor and complexity of predictions, especially in the context of graph recovery."}, {"title": "Finetuning", "content": "We further explore the impact of finetuning on additional datasets, with results detailed in Table 8. Specifically, we utilize LLaMA2-70b , finetuning it on two previously mentioned datasets: TutorialBank and NLP-Papers. Both the zero-shot LLAMA and the finetuned models are employed to generate answers. As these answers are binary (YES or NO), we can calculate both the accuracy and F1 score for evaluation. However, the results indicate that finetuning does not yield positive outcomes. This can be attributed to two potential factors: 1) the poor quality of data, and 2) limited effectiveness in aiding the graph recovery task. We leave this part as the future work."}, {"title": "Ablation Study: RAG Data for Link Prediction", "content": "We explore the potential of external data in enhancing entity graph recovery. This is achieved by expanding the  part in the LP Prompt. We utilize LLaMa as the Base model, focusing on the NLP domain. We introduce three distinct settings: Doc.: In-domain lecture slides data as free-text; Con.: Adding one-hop neighboring entities from the training set as additional information related to the query entities. Wiki.: Incorporating the introductory paragraph of the Wikipedia page of each query entity. As illustrated in Fig 6, our findings indicate that incorporating LectureBankCD documents (Doc.) significantly diminishes performance. This decline can be attributed"}, {"title": "TutorQA", "content": "Benchmark Details\nWe show the data analysis in Tab. 9."}, {"title": "GraphRAG Results", "content": "We extend the results in Tab. 6 by adding GraphRAG as a baseline, the full version of the evaluation is shown in Tab. 10. Based on the established indexing pipelines in knowledge graph construction, we utilize GraphRAG's query engine with the local search method to directly ask the questions in TutorQA. Notably, the performance of GraphRAG appears less satisfactory, which may be due to an evaluation approach that is not well-suited for GraphRAG's results. For example, in Task 5, GraphRAG produces concepts with very broad or specific terms with a bad granularity, such as predict sentiment, emotion cause pair extraction, emotional support conversation. This observation holds across other tasks, where achieving higher scores requires a more granular concept list. This indicates the critical importance of Step 1, which involves generating a well-defined seed concept, in the Graphusion pipeline."}, {"title": "Task 2 and Task 3: case study", "content": "Knowledge Graph Construction Analysis\nAverage Rating We compare expert ratings on the Graphusion KGC results produced by four models: LLaMA, GPT-3.5, GPT-4, and GPT-4o. Fig. 7 and 8 display the average ratings for entity quality and relation quality, respectively, grouped by relation type. Most types achieve an average rating of around 3 (full score) in entity quality, indicating that the extracted triplets contain good in-domain entities. In contrast, the ratings for relation quality are slightly lower. GPT-4 and GPT-4o perform better in relation prediction.\nRelation Type Distribution We then compare the Graphusion results for each relation type across the four selected base LLMs, as shown in Fig. 9. All models tend to predict Prerequisite_of and Used_For relations. The results from LLaMA show relatively even distributions across relation types, whereas the results from the GPT family do not.\nWord cloud Visualization Finally, in Fig. 10, we present a word cloud visualization of the entities extracted by Graphusion, comparing the four base LLMs. High-frequency entities include word embedding, model, neural network, language model, and others."}, {"title": "Conclusion", "content": "In this work, we proposed the Graphusion to construct scientific KGs from free text using LLMs. Through three key steps: seed entity generation, candidate triplet extraction, and KG Fusion, Graphusion builds KGs from a global perspective, addressing the limitations of traditional KGC methods. Additionally, we introduced the new benchmark dataset TutorQA, which encompasses 1,200 expert-verified QA pairs across six tasks. TutorQA is specifically designed for KG-based QA in the NLP educational scenario. We developed an automated pipeline that leveraged the Graphusion-constructed KG, significantly enhancing the performance on TutorQA compared to pure LLM baselines."}, {"title": "Ethical Statement", "content": "We encourage human verification to ensure accuracy in automated knowledge extraction, especially in critical fields such as education. Unlike pure LLM applications and LLM RAG applications, the knowledge stored in KGs can be manually verified, which can enhance the trustworthiness of AI applications that use KGs to retrieve background knowledge. To address biases in LLMs and KGs, we used trusted data sources, but LLM biases may persist. No personal or sensitive data were used, and all experiments followed ethical AI standards."}]}