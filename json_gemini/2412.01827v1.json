{"title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders", "authors": ["Ziqi Pang", "Tianyuan Zhang", "Fujun Luan", "Yunze Man", "Hao Tan", "Kai Zhang", "William T. Freeman", "Yu-Xiong Wang"], "abstract": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable of generating images in arbitrary token orders. Unlike previous decoder-only AR models that rely on a predefined generation order, RandAR removes this inductive bias, unlocking new capabilities in decoder-only generation. Our essential design enables random order by inserting a \u201cposition instruction token\" before each image token to be predicted, representing the spatial location of the next image token. Trained on randomly permuted token sequences a more challenging task than fixed-order generation, RandAR achieves comparable performance to its conventional raster-order counterpart. More importantly, decoder-only transformers trained from random orders acquire new capabilities. For the efficiency bottleneck of AR models, RandAR adopts parallel decoding with KV-Cache at inference time, enjoying 2.5\u00d7 acceleration without sacrificing generation quality. Additionally, RandAR supports inpainting, outpainting and resolution extrapolation in a zero-shot manner. We hope RandAR inspires new directions for decoder-only visual generation models and broadens their applications across diverse scenarios. Our project page is at https://rand-ar.github.io/.", "sections": [{"title": "1. Introduction", "content": "Inspired by the success of \"next-token prediction\" in language modeling, computer vision researchers have explored using GPT-style uni-directional decoder-only transformers for image generation. The typical approach tokenizes an image into discrete 2D tokens, arranges them into 1D sequences in a row-major (raster) order from top-left to bottom-right, and applies a decoder-only transformer for sequential next visual token prediction. This design has shown promising results in uni-modal and multi-modal image generation [44, 46, 51, 57, 61]. However, enforcing a uni-directional raster order limits the decoder-only trans-"}, {"title": "2. Related Work", "content": "AR Language Generation. Current large language models (LLMs) generate text as 1D sequence autoregressively. Since the initial efforts of scaling up, two distinct architectures have emerged: bi-directional BERT-like models [19, 28, 36, 54] and unidirectional GPT-like models [3, 14, 35, 48, 49]. The BERT-like architecture follows an encoder-only or encoder-decoder design and usually uses mask tokens as placeholders for language generation. In comparison, the GPT-like architectures are plain decoder-only transformers in causal attention, which learn to conduct \"next-token prediction\". Decoder-only architectures have recently become the dominant choice for language generation due to simplicity, scalability, and zero-shot generalization across various tasks. Inspired by the versatility of GPT models, we aim to build on the current decoder-only image model, reducing its inductive bias and expanding its zero-shot capabilities by adopting random 2D generation orders.\nDecoder-only AR Image Generation.  Models [37, 46, 51, 53, 57, 61] represented by VQGAN [10], RQ-Tran [23], and LLaMAGen [44] directly transfer the GPT-style decoder-only language models for visual generation. These models turn 2D images into 1D sequences following a pre-defined factorization, typically raster order or coarse-to-fine resolutions modeled by bi-directional attention [47]. Instead, our RandAR provides a simple strategy empowering decoder-only transformers for arbitrary generation orders, which greatly extends their capabilities.\nMasked AR Image Generation.  Masked AR methods [5, 6, 11, 13, 24, 25, 27, 30, 52, 55, 58] employ bi-directional attention commonly implemented with an encoder-decoder design learning to decode place holding mask tokens. While these architectures lack KV-Cache support and direct compatibility with large language models (LLMs), e.g., MaskGIT [5] and MAR [25], they offer greater flexibility and versatility than raster-order decoder-only models, such as parallel decoding and image inpainting. Therefore, the major objective of our paper is to introduce such random order and bi-directional ability into"}, {"title": "3. Method", "content": ""}, {"title": "3.1. Preliminaries", "content": "Decoder-only Autoregressive Models generate sequences by predicting each token sequentially, using only past information. Formally, given a 1D sequence of N variables, denoted as x = [x1, x2, ..., XN], an autoregressive model is trained to model the probability distribution of each variable In based on its precedents [x1,.., Xn-1]:\n$P_{\\theta}(x) = \\prod_{n=1}^{N} P_{\\theta}(x_n | x_1, ..., x_{n-1}),$                                                                    (1)\nwhere $p_{\\theta}(x)$ may be implemented using a multinomial distribution for discrete tokens or a diffusion model for continuous tokens [25]. Currently, one of the most scalable implementations of autoregressive models employs a stack of unidirectional transformer layers with causal attention, i.e., decoder-only transformer [3].\nTo apply this unidirectional approach to image generation, 2D images must be converted to 1D sequences. Existing works [10, 25, 44, 50] enforce a predefined generation order, such as the raster-line order. This design introduces an inductive bias, focusing the network on predicting adjacent patches, only using context from one direction. Models trained in this way are limited to fixed generation orders and lack flexibility for tasks such as inpainting and outpainting.\nIn contrast, RandAR removes this inductive bias entirely by generating image token sequences in arbitrary orders. Building upon prior decoder-only visual autoregressive models [44], we introduce minimal modifications (only one additional trainable parameter) to support random-order next-token prediction. Furthermore, we show that our model achieves generation quality comparable to raster-"}, {"title": "3.2. RandAR Framework", "content": "Our goal is to introduce minimal modifications to the original GPT-style visual autoregressive framework [44] to enable random order generation. The key insight is that the model needs to be informed about the position of each next token. Our solution is straightforward: we insert a special token, called position instruction token, before each image token to be predicted, to represent its position. Specifically, we arrange image tokens in raster order, then randomly shuffle the sequence and drop the last:\n$[x_{\\pi(1)},x_{\\pi(2)},...,x_{\\pi(N-1)}],$                                              (2)\nwhere $x_{\\pi(i)}$ is the i-th token in this randomly shuffled sequence of length N, and \u03c0(\u03af) denotes its original position in raster order. We then insert a positional instruction token $P_{T(i)}$ before each image token $x_{\\pi(i)}$ as Fig. 2:\n$[P_{T(1)}, x_{\\pi(1)}, P_{T(2)}, x_{\\pi(2)},...,x_{\\pi(N-1)}, x_{\\pi(N)}].$          (3)\nRandAR then applies a standard decoder-only transformer with causal attention to this sequence and supervises the prediction of each position instruction token with its subsequent image token. This random-order autoregressive modeling can be formalized as:\n$P_{\\theta}(x|P) = \\prod_{n=1}^{N} P_{\\theta}(x_{\\pi(n)}|P_{T(1)}, x_{\\pi(1)}, ..., P_{T(n-1)}, x_{\\pi(n)}).$     (4)\nFor simplicity, we omit the subscript \u03c0(i) in later equations. Adding position instruction tokens before each image token resembles the concept of target-aware representation, as discussed in XLNet [54].\nPosition Instruction Tokens. We insert position instruction tokens, representing the spatial location of each next token, to enable random-order generation, shown in Fig. 2(b)."}, {"title": "3.3. RandAR Enables Parallel Decoding", "content": "Decoder-only AR image models, by default, generate one token at a time during inference. However, this sequential decoding is bottlenecked by hardware's memory bandwidth [4, 20, 41] (also well-known as \u201cmemory wall\"), as each new token generation step requires a forward pass through the model, and the model needs to load all parameters into GPU registers, which is a process considerably slower than computation. Therefore, the number of steps is a crucial factor for the latency of AR models.\nFortunately, RandAR can predict tokens at any location based on previously generated tokens. This enables parallel decoding, where RandAR simultaneously predicts tokens at multiple locations in one iteration. By reducing the number of forward steps, parallel decoding significantly decreases generation latency.\nWe illustrate two-token parallel decoding as an example. Suppose the generated token and position instruction token sequence up to now is x1:n\u22121 = [P1, X1, ..., Pn\u22121, Xn-1], at the new iteration, we append two position instruction tokens [Pn, Pn+1] at the end of the sequence:\n$[P_1, X_1, ..., P_{n-1}, X_{n-1}, P_n, P_{n+1}],$          (6)\nand pass it through the network. RandAR then predicts and samples the next two tokens [xn, Xn+1]. After sampling, we rearrange the newly added sequence to the training-time interleaved format as follows:\n$[P_1, X_1, ..., P_{n-1}, X_{n-1}, P_n, X_n, P_{n+1}, X_{n+1}].$            (7)\nIn subsequent iterations, we append new position instruction tokens at the sequence's end and repeat this process. Such rearrangement ensures the sequence maintains the same interleaved format used during training, with each image token preceded by a position instruction token.\nOur parallel decoding requires no training modification or fine-tuning. It preserves causal masking and remains compatible with the KV cache. Such parallel decoding is already explored in masked AR methods like MaskGIT [5, 25], but lacks the support of KV-cache acceleration. We show our effective acceleration ratio with parallel decoding in Table 4.\""}, {"title": "3.4. Zero-shot Applications for RandAR", "content": ""}, {"title": "3.4.1. Inpainting and Class-conditional Image Editing", "content": "A predefined generation order limits AR image generators in image manipulation tasks, as they cannot aggregate contextual information from different parts of the image. Consequently, decoder-only AR models lack zero-shot capability for tasks like inpainting and class-conditioned image editing, which are achievable with masked image modeling methods such as MaskGIT [5].\nRandAR overcomes this limitation by enabling unidirectional transformers to incorporate contextual information from any part and any direction of the image. For inpainting and class-conditional image editing, we simply position visible image tokens and their corresponding position instruction tokens before the instruction tokens for the areas to be edited. RandAR then completes the remaining tokens autoregressively, as in Fig. 3(b). The capability to use spatially randomized context and support arbitrary sampling orders is necessary for these tasks and is also central to RandAR's functionality. Results are in Sec. 4.4.1."}, {"title": "3.4.2. Outpainting", "content": "Outpainting requires extending the content of an existing image beyond its boundary in a visually coherent and contextually relevant way. Raster-order models can only extract contextual information from top-left image patches to predict the next token; thus, they have to employ strategies like"}, {"title": "3.4.3. Resolution Extrapolation", "content": "Unlike outpainting, which extends an image's boundaries, resolution extrapolation requires generating finer details within the existing image boundaries essentially out-painting in the frequency domain. We explore this task by generating 512\u00d7512 images with RandAR trained solely on 256x256 ImageNet images, without additional fine-tuning.\nOur resolution extrapolation involves two steps as illustrated in Fig. 3(c): (1) Generating tokens at even coordinates to establish the overall layout, using interpolated ROPE positional embeddings; (2) Generating the tokens at the remaining coordinates. In this step, we use top-k high-frequency components in the extrapolated RoPE to replace the interpolated RoPE, which better captures the finer details, motivated by NTK-ROPE [12] for extending context lengths in language models.\nThis hierarchical decoding schedule relies on RandAR's ability to process tokens in random orders. For comparison, we also show generated results without this decoding approach, which exhibit visual inconsistencies. Inspired by classifier-free guidance [17], we introduce a new type of"}, {"title": "3.4.4. Bi-directional Encoding", "content": "We find that RandAR can effectively extract features using bi-directional context without additional training, outperforming the representations encoded by raster-order models. Formally, an image is first tokenized and arranged into a sequence of image tokens x = [X1,X2, ..., XN], following raster order for raster-order models and random order for random-order models. Position instruction tokens are inserted before each image token, creating an interleaved sequence, which is then passed to the model pe as: $P_{\\theta}(P_1, X_1, P_2, x_2,..., P_N, X_N)$. The output hidden embeddings corresponding to the position instruction tokens are treated as the extracted features for the image. In a unidirectional model, earlier tokens are restricted from receiving information from later parts of the image. To enable bi-directional information aggregation with a causal transformer, we pass the sequence through the model twice:\n$P_{\\theta}(P_1, X_1, ..., P_N, X_N, P_1, X_1, \u2026\u2026\u2026, P_N,X_N),$           (8)\nand use the features from the second round as shown in Fig. 3(d). Notably, only RandAR trained with random orders benefits from this second pass, whereas raster-order"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Implementation Details", "content": "We evaluate RandAR on class-conditional image generation using the ImageNet benchmark. RandAR is implemented based on LLaMAGen [44] for standardized comparison, with only one additional trainable embedding added for position instruction tokens."}, {"title": "5. Conclusions", "content": "We introduce RandAR, a GPT-style causal decoder-only transformer that generates image tokens autoregressively in random orders. Our RandAR achieves this with specially designed position instruction tokens representing the location of next-token prediction. Despite the challenges of learning random order generation, RandAR achieves comparable performance with raster-order counterparts. Moreover, RandAR shows several new zero-shot applications for decoder-only models, including parallel decoding for 2.5\u00d7 acceleration, inpainting, outpainting, resolution extrapolation, and feature extraction with bi-directional contexts. We hope RandAR inspires further exploration of unidirectional decoder-only models for visual tasks.\nLimitations and Future Works Our RandAR investigates enabling decoder-only transformers to generate image tokens in random orders. Although it illustrates the advantages of combining bi-directional contexts from images, random-order generation so far achieves comparable performance compared with the raster-order counterparts, as learning from a much larger number of orders is significantly more challenging. Therefore, a meaningful future investigation would be improving the data efficiency of training a random-order model.\nIn addition, we notice the trend of joint visual language generation with decoder-only transformers [46, 51, 61], which uniformly follows a raster-order design. From this aspect, RandAR can be further scaled up from ImageNet pre-training to the image-text and image-video datasets."}, {"title": "A. Pseudo-Code", "content": "We have provided the Pytorch-style pseudo-code for our random-order training (Algorithm A) and parallel decoding inference (Algorithm B). We will release the code upon acceptance."}, {"title": "B. Additional Implementation Details", "content": "For image preprocessing, we follow the approach in Llama-Gen [44]. Specifically, for 256\u00d7256 experiments, we resize the image's shorter edge to 256 \u00d7 1.1 and apply a center square crop with the same size, and then perform ten-crop augmentation.\nAll the experiments in Table 1, including raster-order counterparts, use classifier-free guidance (CFG) with a linear schedule for sampling. The optimal CFG weight is determined through a sweep with a step size of 0.1 across all methods. We use a temperature of 1.0 without applying top-k filtering.\nModel Configurations. We have provided several sizes of the RandAR model following LLaMAGen [44], which are plain decoder-only transformers. The detailed architectures are in Table A."}, {"title": "C. Performance at Middle Training Steps", "content": "We provide the additional results of our RandAR models at intermediate training steps in Table B. All the results are evaluated with 88 steps of generation with parallel decoding described in Sec. 3.3."}, {"title": "D. RandAR on ImageNet at 384 Resolution", "content": "We report results on ImageNet at 384x384 resolution using the same tokenizer, which produces 24\u00d724 tokens per image, corresponding to 576! random permutations. Our XL-sized model, with 775M parameters, is trained using the same setup. With 180 sampling steps, the model achieves an FID of 2.32 and an Inception Score of 323. Using 144 sampling steps, it achieves an FID of 2.35 and an Inception Score of 322. The FID is slightly higher than that of the 256x256 model, consistent with observations in Llam-aGen [44] that models smaller than 1B parameters perform slightly worse at 384\u00d7384 resolution."}, {"title": "E. Additional Results on Feature Encoding", "content": "We have conducted feature encoding experiments in Sec. 4.4.4 and Table 5, suggesting that decoder-only transformers learned in random generation orders can generalize to extracting features from bi-directional contexts, while raster-order models cannot. In this section, we provide additional ablation studies and discussions.\nComparison with VQ Tokenizer. Here we demonstrate that our autoregressive transformer learns better representation than its VQ tokenizer, which provides the input token indices to our RandAR transformers. Specifically, we conduct the feature correspondence experiment on SPari71k [32] with the DIFT [45] framework, only re-"}, {"title": "F. Spatial Contextual Guidance", "content": "In Sec. 3.4.3, we introduce a new type of guidance called \"Spatial Contextual Guidance\" (SCG) inspired by the classifier-free guidance (CFG). This section describes SCG in detail and analyzes its benefits."}, {"title": "F.1. Formulation of Spatial Contextual Guidance", "content": "The motivation of SCG is to enable better consistency in high-frequency details, as shown in Fig. 6. Inspired by CFG, SCG guides the generation by calculating the difference between the two sampling results with all the previous tokens as context and part of the previous tokens as context. Denoting the RanAR network as ee(\u00b7), the spatial contextual guidance is:\n$\\tilde{e}_{\\theta}(X_{1:n}, c) = e_{\\theta}(x_{1:n}, c)+W_{scg} (e_{\\theta} (x_{1:n}, c) \u2013 (e_{\\theta} (x_{1:n}^{\\phi}, c)),$          (A)\nwhere c is the class conditioning, x1:n is the set of tokens generated in previous steps, and xin is the set of tokens with a random dropout. With such guidance, the final generated result \u1ebdo (x1:n, c) has better consistency with the tokens dropped out from X1:n.\nWhen combining SCG with the conventional CFG, we follow InstructPix2Pix [2] and Liu et al. [26] to compose two guidances together:\n$\\tilde{e}_{\\theta}(X_{1:n}, c) =e_{\\theta} (x_{1:n}, c)+\nW_{scg} (C_{\\theta} (X_{1:n}, c) \u2013 C_{\\theta}(x_{1:n}^{\\phi}, c))+(B)\nW_{cfg} (e_{\\theta} (X_{1:n}, C) \u2013 e_{0}(X_{1:n}, c^{\\phi})),$          \nwhere Wscg = 1 will make the above guidance equivalent to conventional CFG."}, {"title": "F.2. Evaluation of Spatial Contextual Guidance", "content": "SCG can improve the visual quality of regular resolution generation and resolution extrapolation.\nResolution Extrapolation. As shown in Fig. B, using SCG enhances the high-frequency details of the images when generating 512\u00d7512 images directly from our 775M RandAR trained from 256\u00d7256. Numerous extraneous parts of the objects and uneven patterns are removed.\nRegular 256\u00d7256 Generation. Although SCG is primarily proposed for the challenging zero-shot resolution extrapolation, its effects are also reflected in regular 256\u00d7256 generation. Our observation is also validated by quantitative evaluation. As in Table D evaluating the 775M RandAR model, SCG of Wscg = 1.2 can improve sFID, which emphasizes more low-level details, at a marginal drop of FID."}, {"title": "G. Additional Generation Results", "content": ""}, {"title": "G.1. Outpainting", "content": "We provide additional visualizations of the outpainting results in Fig. C."}, {"title": "G.2. Regular Image Generation", "content": "We demonstrate the uncurated 256\u00d7256 images generated from our 775M RandAR-XL. They are displayed from Fig. D to Fig. I."}, {"title": "G.3. Resolution Extrapolation", "content": "We provide uncurated resolution extrapolation results from Fig. J to Fig. O with 775M RandAR-XL. Our zero-shot extrapolation produces high-quality images with unified layouts and detailed patterns like furs of dogs (Fig. J), coral reefs (Fig. K), and scenery (Fig. M). However, we also notice that zero-shot resolution extrapolation is a challenging task. As the model has never been trained on high-frequency details, it will struggle with the small patterns, e.g., eyes and noses of dogs (Fig. J, Fig. L) and straight shapes of man-made objects (Fig. N)."}]}