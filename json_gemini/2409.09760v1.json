{"title": "ELMI: Interactive and Intelligent Sign Language Translation of Lyrics for Song Signing", "authors": ["Suhyeon Yoo", "Khai N. Truong", "Young-Ho Kim"], "abstract": "d/Deaf and hearing song-signers have become prevalent across video-sharing platforms, but translating songs into sign language remains cumbersome and inaccessible. Our formative study revealed the challenges song-signers face, including semantic, syntactic, expressive, and rhythmic considerations in translations. We present ELMI, an accessible song-signing tool that assists in translating lyrics into sign language. ELMI enables users to edit glosses line-by-line, with real-time synced lyric and music video snippets. Users can also chat with a large language model-driven AI to discuss meaning, glossing, emoting, and timing. Through an exploratory study with 13 song-signers, we examined how ELMI facilitates their workflows and how song-signers leverage and receive an LLM-driven chat for translation. Participants successfully adopted ELMI to song-signing, with active discussions throughout. They also reported improved confidence and independence in their translations, finding ELMI encouraging, constructive, and informative. We discuss research and design implications for accessible and culturally sensitive song-signing translation tools.", "sections": [{"title": "1 Introduction", "content": "Song-signing\u00b9 is a performing art, encompassing not only translating spoken lyrics to sign language but also conveying musical elements through facial expressions and bodily movements [7, 35]. d/Deaf and hearing song-signers perform in visually dynamic forms, blending the rhythm and emotion of songs with the physical expressiveness of sign language [23]. Translation quality is crucial, as it enhances the message of the song by incorporating emotional tone, timing, and cultural nuances, going beyond the literal meaning of the words [7, 16, 35]. This requires a deep understanding of both spoken and sign languages, ensuring that the final signed performance is both accurate and resonant [61, 71].\nHowever, translating song lyrics into signs presents challenges. Often, sign language may not have the equivalent vocabulary for some terms from the spoken language, necessitating the use of fingerspelling or creative alternatives [71]. Furthermore, since the syntax and modality of two languages are distinct, the glosses\u00b3 should be adjusted to fit the rhythm of the music and the signing space performing sign language [61, 71]. While past research has explored ways to make music more accessible for d/Deaf individuals, much of this work has focused on music perception [30, 38] and creation [46, 50] rather than performance. Research on the artistic interpretation of sign language remains sparse, and prior work predominantly focused on communication accessibility [56]. Only a few projects have recently begun to understand song-signing [71]. For example, Yoo et al. identified tensions between d/Deaf and hearing song-signers and the different barriers they encounter in the process of song-signing. Still, little effort has been put into designing accessible systems for song-signing.\nIn this work, we aim to design a system that supports song-signing in a more accessible manner. To better understand the current song-signing translation process and challenges, we conducted formative interviews with eight song-signers (five d/Deaf\nAlso known as sign singing, signed song, sign singing, or karaoke signing [16].\nIn research, the acronym DHH (Deaf and Hard of Hearing) is often used to refer to individuals with varying degrees of hearing loss [27]. However, in this study, we use \"d/Deaf\" to emphasize cultural identity. \"deaf\" (with a lowercase 'd') refers to the physical condition of hearing loss. These individuals may use hearing aids, cochlear implants, or rely on lip reading. On the other hand, \"Deaf\" (with an uppercase 'D') refers to individuals who identify as part of the Deaf community, share a common language (such as ASL), and embrace Deaf culture and identity [60].\nGloss refers to a written or typed representation of a signed language, using words from a spoken language (like English) to indicate the meaning of individual signs. Glossing helps translate the structure and meaning of a signed language but does not fully capture nuances such as facial expressions, which are crucial elements of ASL communication [65]."}, {"title": "2 Related Work", "content": "In this section, we cover related work in three parts: (1) song-signing, (2) technologies for automated sign language translation, and (2) lyric translation technologies."}, {"title": "2.1 Song-signing", "content": "Song-signing is an art form rooted in Deaf culture [35]. Song-signers convey not only the lyrics but also capture musical elements such as rhythm, emotion, and instruments through facial expressions and bodily gestures [71]. Song-singing performance is crucial in making music accessible to d/Deaf individuals by providing an augmented visual representation [71]. It has also proved to be more meaningful and authentic to those fluent in sign language, offering a genuine and enjoyable experience [17].\nThe core activity of song-signing - trans-languaging from spoken language to sign language - requires a high degree of creativity, encompassing the translation of lyrics or the composition of original songs in sign language, where performers manipulate signs and signing space to convey musical features [35]. This practice involves complex interactions between music, lyrics, and sign language, requiring artists to navigate across languages and modalities [61]. Yoo et al. proposed three layers in song-signing translation: technical translation, understanding the lyrics and matching gloss with the music; artistic translation, editing the gloss to convey non-lyrical elements like pitch, rhythm, and volume; and cultural translation, appreciating of Deaf culture and language [71]\nRecent studies on song-signing have identified several challenges, including the difficulty of translating lyrics into sign language, conveying musical features, and addressing diverse linguistic repertoires [35, 71]. Song-signing involves navigating between visual and auditory channels and integrating music, lyrics, and sign language [61]. One significant challenge is that many song lyrics carry symbolic meanings rather than straightforward messages. When translated into sign language, the interpretation is shaped by the translator, which can lead to a disconnect between the original intent of the writer and the signed interpretation [17]. Additionally, song-signing can appear weak if the translator struggles with timing, particularly when trying to produce musical American Sign Language (ASL) within the time constraints of English songs [17]. These challenges underscore the need for culturally responsive approaches to music accessibility [55, 71]. However, there is still limited research on how technology can support song-signing translation."}, {"title": "2.2 Automated Sign Language Translation", "content": "Sign language is the primary mode of communication for d/Deaf individuals [57], and research in this area is crucial for facilitating communication between Deaf communities and others [49]. As one dominant line of efforts that bridge the gap between spoken and sign languages, ample research in machine learning and accessibility has presented automated models that convert signed sentences into spoken or written language and vice versa [19, 33, 44, 49]. As sign language is carried over bodily movements, video recognition or generation is required for ideal end-to-end translation [33]. To make the task straightforward, gloss text has often been used as a representation medium of sign language, and a majority of machine learning approaches tackled gloss-spoken language translation [19, 44]. For example, researchers proposed gloss-based neural machine translation models (e.g., [1, 13, 14, 42]), Recognizing the loss of partial linguistic cues when using gloss as a representation of sign language [19, 43], recent work leveraged large language models (LLMs) [47] to implement gloss-free sign language translation in combination with vision models [25, 70].\nDespite the growing body of automated sign language translation technologies, existing models focus on communication and mapping the meanings, trained on datasets in general contexts such as word-level sign lexicons (e.g., [6, 32]), news and weather forecast (e.g., [12]), and daily conversations (e.g., [67]). In other words, they are not designed with lyric translation in mind, which critically involves considerations of timing and poetic interpretation [71].\nHence, lyric-to-sign-language translation should incorporate users in the loop, rather than being carried over a fully-automated approach. In this work, we support lyric translation driven by song-signers, while facilitating the translation process with AI-driven guidance and discussions."}, {"title": "2.3 Lyric Translation and Tools", "content": "Emotion and timing are crucial in translating lyrics due to their significant impact on the overall musical experience. Translators must navigate the complex interplay between language, music, and cultural nuances to maintain the emotional resonance of the original song [66]. This translation process requires careful consideration of both implicit and explicit information, as well as the emotional content of the source text [29]. Franzon describes the strategic choices made by translators and lyricists in achieving 'singability', the musico-verbal fit of text to music-by balancing prosodic, poetic, and semantic layers [24]. The emotional intelligence of translators also plays a pivotal role in their decision-making and problem-solving during translation: understanding of linguistic, cultural, and emotional subtleties to effectively bridge linguistic and cultural boundaries in music [29, 66].\nNew tools for lyric translation and interpretation have been introduced. Statistical Machine Translation was used for an automated lyric annotation system to clarify complex terminology and abstract concepts using the Genius dataset [62]. Semi-automatic lyric generation using context-free grammar has shown promise, with generated lyrics sometimes indistinguishable from those written by humans [52]. Recent research explores the potential of large language models (LLMs) in supporting lyric interpretation and song understanding. For instance, BART-fusion combines a pre-trained language model with an audio encoder to generate lyric interpretations, showing improved performance with the integration of audio information [73]. These studies demonstrate the versatility of LLMs in understanding and generating music-related content, highlighting their potential to enhance interpretability across various domains.\nDespite these technological advancements, the translation of lyrics continues to pose significant challenges. Translators must capture stylistic elements, cultural references, and semantic nuances while maintaining fluency and creativity [2]. The ambiguous nature of lyrics, along with the use of slang, jargon, and the need to preserve rhyme and repetition, adds to the complexity of this task [3]. Moreover, translators face difficulties related to rhythm, syllable count, vocal burden, and the need to adapt cultural nuances across languages [3]. Cultural differences often necessitate adaptations and retranslations to accurately convey meaning. Translating idioms within lyrics, in particular, requires a deep understanding of both"}, {"title": "3 Formative Study", "content": "To understand the current translation process and challenges in song-signing, as well as to explore the applicability of LLM on song-signing translation, we conducted semi-structured interviews with song-signers. The study protocol was approved by the university research ethics board."}, {"title": "3.1 Methods", "content": "Participants. We recruited eight song-signers (2 deaf, 3 hard of hearing, and 3 hearing) comprising five males and three females, with ages ranging from 25 to 67 years old. Our inclusion criteria were song-signers who are 18 years or older, proficient in speaking and writing English, and have experience in song-signing within the past year. Participants were recruited through emails, social media platforms (e.g., Facebook, YouTube), and the snowball sampling method. The majority of participants (N = 5) used American Sign Language (ASL) at expert and native levels, while others also utilized Pidgin Signed English (PSE). On average, participants have been involved in song-signing for 9.3 years, spanning from 4 to 17 years. While some participants used tools such as dictionaries for translation, the use of LLM-driven tools like ChatGPT was minimal, with most participants relying on personal knowledge and community resources.\nProcedure. We invited each participant to Zoom sessions which lasted between 50 to 70 minutes. We utilized Zoom chats, closed captioning, and sign language interpreters for d/Deaf participants who preferred those options rather than spoken language. The interviews covered: (1) the current work process and practice method for translating English lyrics to sign language, (2) factors and considerations influencing translation, and (3) challenges and workarounds in translation. To collect participants' preliminary feedback on using LLMs in song-signing, we asked them to try ChatGPT [48] while translating a snippet of the example song \"Dynamite\" by BTS (See Figure 2). During that exploration, participants were encouraged to query ChatGPT regarding various aspects of translation, such as understanding the context of the song, interpreting specific lyrics, seeking confirmation on glossing and interpretation, finding videos of performances or song-signings, and providing images or videos of example signs.\nAnalysis. We recorded the video call sessions and transcribed them for analysis. Two researchers independently performed open coding and grouping, followed by thematic analysis [9, 10]. This process involved two rounds of discussion to ensure accuracy and consistency."}, {"title": "3.2 Lyric Translation Practice", "content": "3.2.1 Familiarization. Participants begin by listening to the music and watching music videos to understand the context of the song's"}, {"title": "3.3 Challenges of Song-signing Translation", "content": "Semantic Translation [Meaning]. Participants pointed out the difficulty of grasping the meaning of the lyrics. This challenge arises from the inherent complexity of the lyrics, including poetic or ambiguous expressions that can have multiple meanings (N = 6). Accurately capturing meaning becomes even more challenging when translating concepts that are culturally or linguistically specific. S2 noted how accents or unfamiliar terms can lead to misinterpretations: \"[...] Another challenge is the accent. I might miss something because I don't understand a particular term.\"\nSyntactic Translation [Glossing). Since ASL relies heavily on visual elements to convey meaning, participants noted that it is particularly difficult to translate concepts that are abstract or primarily auditory in nature. For example, three participants (S3, S4, S7) mentioned challenges in translating repetitive or simplistic messages in a way that feels both meaningful and visually appealing. Another challenge arises when English phrases do not have direct equivalents in ASL (S2, S3, S4, S8), requiring signers to find alternative signs or creative ways to convey the meaning. Also, acoustic rhymes in spoken language, which depend on sound, do not translate well into sign language.\nExpressive Translation [Emoting]. Participants pointed out that beginners or those new to Deaf culture would often struggle with emotional demands in sign language due to their lack of experience and the mismatch between their own personality and the song's mood. S1 remarked, \u201cIt's a challenge, especially if you're a person who may be outwardly calm or just don't like to express yourself. Then you really have to work on that.\" Certain songs, particularly those with shifting moods, demand expressive control that can be hard to master. S3 shared,", "lyrics": "It was tough, I had to repeat and repeat until I could be in sync with the music. I would repeat one sentence over 50 times to feel confident to sign as they sing.\u201d This meticulous process highlights the challenge of aligning signs with the music, a task that demands significant time and effort."}, {"title": "3.4 Common Strategies to Overcome Challenges", "content": "Participants commonly looked up external resources (N = 5). Many turned to online ASL dictionaries to find the appropriate signs and expressions, appreciating that the short video clips and accompanying descriptions helped clarify hand positions and sign execution. Participants also relied on search engines like Google, particularly for searching the origins and meanings of songs. Various types of videos-including original music videos, performance/dance videos,"}, {"title": "3.5 Feedback on Using LLMs for Translation", "content": "The majority of participants shared positive views on using GPT in the translation process, recognizing its capabilities extending beyond mere literal translation, as it offers interpretations that capture the deeper meaning and emotional nuances of the original content. Additionally, GPT's recommendations for incorporating non-manual signs were particularly impressive to participants. For example, S3 noted:", "that.": "espite its potential, participants also expressed concerns about relying on it for accurate and contextually appropriate translations because ChatGPT sometimes yielded wrong glosses or interpreted the lyrics literally, producing very long glosses."}, {"title": "4 ELMI: Explore Lyrics and Music Interactively", "content": "Informed by the formative study, we designed and developed ELMI, a novel web application that aids d/Deaf and hearing individuals in song-signing. We refined the interface and core functionalities through pilot sessions with experienced song-signers. This section outlines the design rationales for ELMI and provides details on its implementation."}, {"title": "4.1 Design Rationale", "content": "DR1: Use sufficient visual feedback to convey emotion and timing. Music can be appreciated not only by auditory but also by visual channels [20, 63]. d/Deaf individuals often lip-read and observe expressions of singers to capture musical elements [11]. Captioning also plays a vital role in enhancing the comprehension and enjoyment of music, as it is one of the most widely used visual aids for d/Deaf [40]. Visual feedback is thus essential for conveying musical elements like emotion and timing. Consistent with prior work [71], formative study participants reported using visual aids, including online videos. To address this, we designed ELMI to provide music videos with captions synchronized to the lyrics (e.g., in Figure 3), allowing users to fully grasp the nuances and accurately translate them.\nDR2: Support discussion about nuances, performance, and translation of lyrics. Individuals often engage in online forums to"}, {"title": "4.2 User Interface and Interaction Components", "content": "ELMI operates in two modes: By default, the system is in the Global Play mode, where the user can play the entire video using a conventional video player ( in Figure 1). In this mode, the Information Panel (Figure 1-A) serves as the user's entry point for exploring the story and meaning of the song; it displays essential song details, such as genre, runtime, and behind-the-scenes insights, all sourced from the song description of Genius music database [39]. While the song is being played, the Translation Panel (Figure 1-B) automatically scrolls to display the current line.\nThe user can enter the Line Loop mode by selecting a lyric line on the Translation Panel. In this mode, the selected line is highlighted (Figure 1-B, top), and the user can create and refine glosses with real-time feedback from the chatbot. In this mode,"}, {"title": "4.3 Conversational Design: 4 Main Discussion Topics", "content": "The dialogue interaction is structured around key discussion topics designed to support users throughout the translation process. If a user provides their own gloss, the ELMI recognizes this input, updating its suggestions to align with the user's input. This capability is managed through specialized chatbot modules that handle different discussion topics such as meaning extraction, gloss refinement, emoting, and timing:"}, {"title": "4.4 Interacting with ELMI", "content": "In this section, we illustrate how ELMI can be used through a scenario featuring Cindy, a Deaf song-signer with five years of experience. Cindy is preparing to interpret BTS's \"Dynamite\" in real-time at an upcoming concert and decides to use ELMI to create accurate glosses for the song.\nAfter creating a new song project, Cindy enters the translation page. To familiarize herself with the song, she plays the music video, finding ELMI's feature of highlighting sung words in real-time particularly helpful for understanding the song's timing. Cindy begins her translation process by focusing on the first line of the song ( in Figure 1). As she clicks on the line, the Information Panel updates to provide a Mood and Performance guide (A in Figure 1). She confirms that her feelings about the song align with ELMI's interpretation, which suggests a joyful and uplifting mood. ELMI also offers guidance on facial expressions, advising Cindy"}, {"title": "4.4.1 Conversation System with LLMs", "content": "The conversation system in ELMI leverages OpenAI's GPT-4.0 for preprocessing, intent classification, and chatbot interactions. The system uses tailored prompts specifically designed for each function to ensure accurate and contextually relevant responses. An example of the prompt used for the discussion with the chatbot is provided in the Appendix.\nLyric Analysis. When a user creates a new project, ELMI preprocesses the song by running the reference lyrics and metadata through four inference modules, as shown in the pipeline diagram (Fig. 6). The Line Inspector annotates each line for challenges like poetic, cultural, or mismatched meanings. Based on these notes, the Base Gloss Generator creates default glosses for all lines. Using these glosses, the Performance Guide Generator provides performance recommendations for gestures and expressions"}, {"title": "4.5 Implementation", "content": "We developed ELMI's interface as a web application using TypeScript with React.js. The backend is powered by Python and built with the FastAPI framework, which serves as a REST API to manage both data and computational pipelines. These pipelines were implemented using LangChain to streamline multiple steps of LLM inferences, with data validation handled by Pydantic. A SQLite"}, {"title": "5 User study", "content": "We conducted an exploratory user study with ELMI, employing observation methods to examine how a Large Language Model (LLM) chat agent can support song-signing gloss creation. To ensure the effectiveness of the system and the study protocol, we refined both through two pilot sessions with hard of hearing participants."}, {"title": "5.1 Participants", "content": "We recruited 13 song-signers (P1-13) through various channels, including social media platforms such as Facebook groups and Reddit, email lists, word of mouth, and snowball sampling. Our inclusion criteria required participants to (1) have completed at least three song-signing projects before the study, (2) be a minimum of 18 years old, (3) be comfortable reading and writing in English, and (4) use American Sign Language (ASL) or Pidgin Signed English (PSE) in their song-signing work. Table 1 summarizes the demographic of our study participants. Participants were aged between 25 and 67 years (M = 38.3) and included 9 males, 4 females, and 1 non-binary individual. Ten out of 13 participants communicated primarily in ASL, with others using Pidgin Signed English (PSE). On average, participants had 12.2 years of experience in song-signing, with their involvement ranging from 4 to 30 years, driven by professional and personal motivations, ranging from ASL interpreters and artists to hobbyists and Deaf school teachers."}, {"title": "5.2 Songs for Translation", "content": "Our study involved two songs for participants to translate. First, participants translated the same song (referred to as the SongAssigned hereinafter) so that we could observe common interaction patterns and diversity of translation. Second, each participant chose one song (referred to as the SongSelected hereinafter) they wanted to translate.\nFor the SongAssigned, we selected \"Butter\u201d by \"BTS\" 4, considering its moderate complexity, clear emotional tone, and being free"}, {"title": "5.3 Study Procedure", "content": "5.3.1 Pre-study Preparation. Before the main study session, participants completed a pre-study survey, which collected demographic information, hearing description, song-signing experience, ASL expertise, and details about the tools and methods they used for song-signing and gloss creation. Participants also submitted three candidates for the SongSelected.\n5.3.2 Main Study Session. Each participant engaged in a 2-hour main study session remotely on Zoom. Depending on their hearing capability, the session was carried on via spoken language with live captioning, Zoom chats, or with an ASL interpreter. The Zoom session was audio- and video-recorded for analysis.\nBriefing and Tutorial. We first described the goal of our study and the study protocol, covering the overall process of creating a project, glossing, and chatting. As a tutorial for the ELMI interface, participants shared their screen and tried ELMI with the example song, \"Viva La Vida\" by Coldplay. Participants were allowed to practice with the tool until they felt confident about the interface. This phase took about 20 minutes.\nSong Translation. Participants created glosses using ELMI for two projects, translating SongAssigned and SongSelected. Due to time constraints, we asked participants to translate only the first verse of each song. We allowed up to 30 minutes for each song but also asked not to rush to finish within the time limit and to prioritize the translation quality as usual.\nDebriefing. We conducted a semi-structured interview around 30 minutes at the end of the session. We asked participants about"}, {"title": "5.4 Analysis", "content": "Glossing. We analyzed the translation results of the SongAssigned from 11 participants who completed the first verse. Key metrics such as the number of lines completed and the average word count were analyzed using the Pandas Python.\nChats. We examined chat threads of both SongAssigned and SongSelected from 11 participants who completed post-study assignments. We defined a turn as a single message exchange, with user turns representing participant messages and system turns representing those by ELMI. Using the Pandas Python package, we computed various descriptive statistics, such as the total number of turns per session. Additionally, we categorized conversation turns using a top-down approach based on four discussion topics, and researchers independently coded the dialogues of three participants.\nInterviews and Surveys. We also analyzed debriefing transcripts, post-study surveys, and video recordings and screen captures, to understand participants' perspectives and expectations regarding ELMI. Thematic analysis [9] was employed to code and examine the transcripts. Through a series of group discussions, recurring themes were identified, compared, discussed, and refined until consensus. Three key themes emerged: how ELMI streamlined participants'"}, {"title": "6 Results", "content": "This section presents the study's findings across five key areas: (1) Translation Outputs, (2) Conversations with ELMI, (3) Perceptions of ELMI and (4) Strengths and Limitations of Using ELMI."}, {"title": "6.1 Translation Outcome", "content": "We present the glossing results for SongAssigned, BTS's \"Butter\" to examine how ELMI affects how different users may translate the same lyrics. We analyzed the first three parts (Verse 1, Pre-chorus, Chorus), comprising 19 lines with a total of 105 words. Among the participants, six (3 d/Deaf, 3 hearing) used ASL, while five (2 d/Deaf, 3 hearing) used PSE. Participants spent varied time translating lyrics during the main session, completing between 3 and 19 lines, with an average completion of 9.82 lines (SD = 5.25, min = 2 , max = 19 ). Table 2 summarizes the complete translation result of SongAssigned by 11 participants who completed the post-study assignments. Participants produced a wide variety of gloss translations, incorporating non-manual signals (NMS), such as classifiers, visual vernacular, facial expressions, and body language. On average, participants used 3.72 signs per line (SD = 1.38, min = 1.9 [line 9], max = 5.72 [line 15]), with 0.44 NMS per line (SD = 0.91, min = 0.09 [line 3, 13], max = 1.09 [line 14]). Specifically, d/Deaf participants used an average of 3.61 signs (SD = 1.42, min = 2.2 [line 9,13], max = 5.8 [line 15]) and 0.75 NMS (SD = 1.24, min = 0.2 [line 3, 13], max = 1.6 [line 14, 17, 18]), while hearing participants used more signs (3.82 on average) (SD = 1.3, min = 1.6 [line 9], max = 5.6 [line 15]) but fewer NMS (0.18 on average) (SD = 0.33, min = 0 [line1, 3, 6, 9, 13, 15, 6], max = 0.66 [line 14]).\nTo assess the diversity of translation among participants, we calculated the average overlap coefficient of pairwise combinations of manual signs for each lyric line (see Table 2, 'Average Overlap'). The average overlap coefficient of ASL signs of 19 lines was 36.36% (SD = 20.53%, min = 0% [line 2], max = 72.22% [line 10]), and that of PSE was 38.39% (SD = 21.62%, min = 0% [line 9], max = 80.38% [line 10]), suggesting that participants shared around one-third of signs for the same line. For instance, when translating line 2,", "undercover,": "SE participants seemed to assign straightforward signs such as", "THIEF": "In contrast, ASL participants produced more diverse and distinct signs such as", "OVERLOOK": "r \u201cPERSON DANGEROUS DISGUISE", "},\n    {": "itle", "6.2 Conversation with ELMI": "content\": \"Overall, participants opened a total of 222 threads, with 114 threads from SongAssigned and 108 from SongSelected. These threads comprised 1,346 messages-730 from SongAssigned and 616 from SongSelected, featuring 563 user messages (308 from Song-Assigned, 255 from SongSelected) and 775 system messages (414 from SongAssigned, 361 from SongSelected).\nFor SongAssigned, participants opened an average of 10.36 threads from 19 lines (SD = 5.32, min = 3 , max = 17 ), with each thread containing 5.91 messages (SD = 1.97, min = 3.5 , max = 10.53 ). As for SongSelected, participants opened an average of 9.82 threads from 47.36 lines (SD = 7.07, min = 1 , max = 23 ), with each thread containing 5.14 messages (SD = 1.61, min = 3 , max = 7.73 ) (See Table 3). To interact with ELMI, participants either clicked on one of four shortcut buttons (seed in Figure 1) or typed manual inputs. Participants used both shortcut message and manual message in a similar manner (See Table 3).\nMeaning was frequently discussed at the beginning of their threads to explore how to interpret specific words, 7.9 times (SD = 6.25, min = 0, max = 17) for SongAssigned and 4.55 times (SD = 4.1, min = 0, max = 12 for SongSelected (See Table 4). Meaning was rated 3.81 on average (SD = 1.25, min = 2, max = 5), where d/Deaf participants gave 4 (SD = 1.22), and hearing gave 3.66 (SD = 1.37) (See Figure 7). P6HH engaged with ELMI to interpret \"Cool shade stunner\". ELMI broke down the metaphor and encouraged P6HH to consider the symbolic meaning and how it could be visually represented (see Dialogue 1)."}, {"title": "6.3 Perceived Attitude and Ownership", "content": "Reflecting on the conversation with the ELMI chatbot, participants reported various attitudes and stances of the chatbot. Participants generally found ELMI to be encouraging and supportive in the translation process, helping users feel more confident in their work, while it sometimes provided critical and constructive suggestions helping participants fine-tune their translations.  and  noted that ELMI supported balancing between their initial ideas and refined ones: \u201cThe suggestions helped me align my approach, providing a healthy middle ground if I was overthinking or missing something ( P4HH )\u201d. ELMI was also informative and guiding by providing context that deepened participants' understanding of the songs.  and  highlighted how ELMI helped them grasp the essence of the songs, which enhanced their interpretations: \u201cThere are times when I'm not aware of the context, but ELMI already"}, {"title": "6.4 Strength and Drawbacks of ELMI", "content": "In the post-study survey, participants highly evaluated their satisfaction with the quality of translation they produced using ELMI, rating an average of 4.27 out of 5 on a Likert scale (SD = 0.65, min = 3 , max = 5 ). Participants found ELMI easy to use, giving it an average rating of 4.36 out of 5 (SD = 0.5, min = 4 , max = 5 ). d/Deaf participants rated it 4.2 (SD = 0.45), while hearing participants rated it slightly higher at 4.5 (SD = 0.55) (See Figure 8-C). In terms of reducing participants' effort, ELMI received an average rating of 3.63 (SD = 0.8, min = 2 , max = 5 ), with d/Deaf participants giving it a 3.4 (SD = 0.89) and hearing participants rating it 3.83 (SD = 0.75) (See Figure 8-D).\nHowever, participants were more critical of ELMI's accuracy, with an average rating of 3.45 out of 5 (SD = 0.93, min = 2 , max = 5 ). d/Deaf participants rated the accuracy"}, {"title": "6.4.1 How ELMI Supported the Translation Workflow", "content": "Participants expressed a strong interest in integrating ELMI into their workflow, with an average willingness rating of 4.36 out of 5 on the Likert scale (SD = 0.67, min = 3 , max = 5 ). d/Deaf participants rated their willingness slightly higher, at 4.6 (SD = 0.55), while hearing participants rated it 4.17 (SD = 0.75). Figure 9 summarizes the main features of ELMI that participants mentioned as the top three favorites in the post-study surveys. Here we cover noteworthy features.\nLine-by-line Focus. Most of the participants (N = 8, 4 d/Deaf and 4 hearing) liked ELMI's ability to break down song lyrics into \"lines\", making the translation process more precise (See Figure 9). This was particularly useful for complex lyrics, where understanding the meaning is crucial. Additionally, looping specific lines with the video was received as useful for practice and refinement.  mentioned ELMI's \u201cline-by-line setup made it easy to see how the person in the video is conveying emotions and actions, which is incredibly helpful for aligning translation with the intended expression.\u201d\nInteractive Discussion and Chat. As  described,"}]}