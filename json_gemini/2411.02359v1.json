{"title": "DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution", "authors": ["Yang Yue", "Yulin Wang", "Bingyi Kang", "Yizeng Han", "Shenzhi Wang", "Shiji Song", "Jiashi Feng", "Gao Huang"], "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable comprehension and reasoning capabilities with complex language and visual data. These advances have spurred the vision of establishing a generalist robotic MLLM proficient in understanding complex human instructions and accomplishing various embodied tasks. However, developing MLLMs for real-world robots is challenging due to the typically limited computation and memory capacities available on robotic platforms. In contrast, the inference of MLLMs involves storing billions of parameters and performing tremendous computation, imposing significant hardware demands. In our paper, we seek to address this challenge by leveraging an intriguing observation: relatively easier situations make up the bulk of the procedure of controlling robots to fulfill diverse tasks, and they generally require far smaller models to obtain the correct robotic actions. Motivated by this observation, we propose a Dynamic Early-Exit Framework for Robotic Vision-Language-Action Model (DeeR-VLA, or simply DeeR) that automatically adjusts the size of the activated MLLM based on each situation at hand. The approach leverages a multi-exit architecture in MLLMs, which allows the model to terminate processing once a proper size of the model has been activated for a specific situation, thus avoiding further redundant computation. Additionally, we develop novel algorithms that establish early-termination criteria for DeeR, conditioned on predefined demands such as average computational cost (i.e., power consumption), as well as peak computational consumption (i.e., latency) and GPU memory usage. These enhancements ensure that DeeR operates efficiently under varying resource constraints while maintaining competitive performance. Moreover, we design a tailored training method for integrating temporal information on top of such multi-exit architectures to predict actions reasonably. On the CALVIN robot manipulation benchmark, DeeR demonstrates significant reductions in computational costs of LLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance. Code and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.", "sections": [{"title": "1 Introduction", "content": "The recent astonishing progress in multimodal large language models (MLLMs) has unveiled their remarkable potential of extracting, aligning, and integrating the representations from complicated language and visual data [1, 2, 3, 4]. These advances have spurred the vision of a generalist robot, i.e., an embodied agent equipped with vision-language comprehension and problem-solving capabilities, proficient in interacting with humans and the physical world to flexibly execute complex manipulation tasks [5, 6]. An encouraging preliminary work, RT-2 [7, 8], has demonstrated the feasibility of adopting MLLMs to control robots in end-to-end. This not only yields performant robotic policies, but also exhibits some emergent abilities obtained from large models, such as understanding novel commands, generalizing to objects never seen before, and reasoning.\nDespite these favorable findings, the high demands of MLLMs on hardware are usually an important bottleneck that inhibits the establishment of generalist robots with advanced MLLMs. Typically, robotic applications are based on resource-hungry platforms with limited computational capability, memory space, and battery capacity, yet usually necessitate acting in real-time and performing low-latency interactions with humans or the physical environments. However, every time MLLMS are activated to obtain a robotic action involves utilizing billions of parameters to accomplish a computationally intensive inference process. Such inefficiencies may lead to considerable GPU memory requirements, tremendous power consumption, as well as nontrivial time delays in controlling robots. These weaknesses make it challenging to deploy MLLMs on real embodied robotic systems.\nTo alleviate this problem, we propose an approach based on dynamic neural networks. Our work is inspired by an intriguing observation: in the procedure of controlling a robot to fulfill various tasks, relatively 'easier' circumstances make up the bulk of all the situations confronted by the robot. When encountered with these 'easier' situations, an embodied agent can actually acquire proper robotic actions with a much smaller model compared to the full MLLMs. Or more precisely, only the remaining small number of 'more difficult' circumstances necessitate the full capacity of large MLLMs. This phenomenon can be demonstrated using the representative example in Table 1, where we train RoboFlamingo [9] with varying model sizes, and report the FLOPs and task successful rate in the Calvin Long-Horizon Multi-Task Language Control (LH-MTLC) challenge [10]. Adopting the officially recommended 24-layer Flamingo only correctly finishes 3.2% (78.9% v.s. 75.7%) more tasks compared to using 6 layers, but it increases the computational cost by 4x. In other words, computational resources are wasted on activating larger models in many easy circumstances for which smaller models are sufficient.\nMotivated by this observation, we propose a Dynamic Early-Exit for Robotic MLLM (DeeR) framework, seeking to automatically configure the size of MLLMs conditioned on each situation confronted by an embodied agent. Specifically, we introduce a MLLM architecture featuring multiple intermediate exits, with which a correct robotic action can be immediately obtained once a proper size of the model has been activated, eliminating further redundant computation. Additionally, we develop novel algorithms that are able to establish early-termination criteria for DeeR conditioned on arbitrarily specified demands of average computational cost (i.e., power consumption), and peak computational cost (i.e., latency) or GPU memory overhead. At inference, DeeR can adaptively activate smaller models for less complex situations and larger models for more challenging cases. Consequently, computation is unevenly allocated among situations, yielding a considerable improvement in efficiency. Besides, the computational cost of DeeR can be adjusted online by simply modifying the termination criterion on top of a fixed main model, making it appealing in flexibility. Moreover, we design a tailored training method for DeeR, enabling integrating temporal information on top of such multi-exit architectures to control robots reasonably.\nThe performance of DeeR is evaluated on 3 CALVIN LH-MTLC challenges with RoboFlamingo [9]. Extensive robot experiments show that DeeR reduces the LLM computational cost by 5.2-6.5x without sacrificing performance. Surprisingly, even when considering GPU memory limitations in the termination criterion, DeeR remains competitive with other SOTA methods while only utilizing 2GB memory for the activated LLM. Consequently, DeeR demonstrates the potential to enable a wider range of users to operate their own robots equipped with MLLMs on resource-limited platforms."}, {"title": "2 Related Works", "content": "LLM/MLLM for language-conditioned robot control. A range of studies have explored the use of natural language to instruct robots in performing tasks [11, 12, 13, 14, 15, 16]. Methods such as SayCan and PaLM-E [17, 18, 19] utilize LLMs as high-level planners to translate commands into individual primitives that are then executed by low-level controllers. However, these controllers are usually domain-specific small models and lack the semantic understanding and reasoning capabilities that LLMs/MLLMs possess. To fully leverage LLM's astonishing capabilities, RT-2 [7, 9] proposes an end-to-end MLLM that directly generates low-level robotic actions via co-finetuning on robotic data and image-language data. It exhibits some emergent abilities obtained from large MLLMs, such as generalizing to instructions and objects never seen before, and reasoning. Further, RoboFlamingo [9] proposes to adapt existing MLLMs to a low-level robotic policy through straightforward fine-tuning on robotics data. While representative projects like RT-2 and RoboFlamingo have showcased the promising potential in enabling generalist robots, the use of MLLMs for such low-level control is computationally intensive. This is because each robotic action requires processing through all layers of an MLLM, whose inefficiencies often yield significant bottlenecks in practical robotic applications.\nEfficient LLM. Considerable strides have been made to improve the inference efficiency of LLMs [20, 21, 22]. Research in this domain typically falls into three categories: efficient structural design[23, 24, 25, 26, 27, 28, 29], model compression [30, 31, 32, 33, 34, 35, 36], and dynamic networks [37, 38, 39, 40]. Our research focuses on the third category, dynamic networks, which optimize computational resources based on input data to reduce unnecessary computation. A key strategy within this category is early exiting, discussed further below.\nEarly exiting is an innovative method for dynamically halting forward propagation at a certain layer based on intermediate predictions. This technique has been well explored in both computer vision [41, 42, 43, 44, 45, 46], language processing [47, 48, 49, 50, 51, 52, 53], and multimodality [54, 55]. A challenge in implementing early-exiting models is devising an appropriate metric to determine when to issue an intermediate prediction. Traditionally, in tasks such as image classification, metrics such as Softmax confidence [43] or entropy [49] are utilized. Alternative approaches include training learning-based exit strategies with pseudo labels [56, 48, 57, 58]. Recent advancements [38, 39] have expanded early exiting to encompass the next-token prediction of LLMs focused on, treating it as a classification task. Diverging from these methods, our work adapts an MLLM to generate action outputs for sequential decision-making. We introduce a novel dynamic paradigm that integrates temporal information to predict action. Further, we devise a novel early-exiting metric based on action consistency, necessary because typical metrics like confidence and entropy are infeasible without direct Softmax outputs. Lastly, we develop an algorithm to derive termination criteria through online environmental interaction\u2014a strategy not explored in prior early-exiting research in vision or NLP."}, {"title": "3 Dynamic Early-Exit for Robotic MLLM", "content": "The strong task instruction understanding and visual grounding capabilities of MLLMs [5, 4] have exhibited great promise for language-instructed multitask robotic manipulation [7, 8, 9]. However, existing works tend to be computationally intensive since the actions of the robot are obtained by inferring all layers of an MLLM. At each timestep, this process may activate billions of parameters, necessitating substantial computation and memory, and yielding a significant latency and power consumption. These inefficiencies are usually important bottlenecks for practical robotic applications.\nOverview. We seek to address this challenge by leveraging an intriguing observation: relatively 'easier' situations make up the bulk of the procedure of controlling robots to fulfill various tasks, and they generally require far smaller models to obtain the correct robotic actions (as shown in Table 1). Inspired by this phenomenon, we propose Dynamic Early-Exit for Robotic MLLM (DeeR), aiming to improve the computational efficiency of the robotic MLLM systems by dynamically adopting a proper size of MLLM for each situation. In specific, we first develop a novel MLLM architecture with multiple intermediate exits (Section 3.1). Consequently, given an input, one can immediately acquire a proper robotic action once a sufficient number of model parameters have been activated, avoiding further redundant computation. Then, Section 3.2 establishes early-termination criteria for DeeR conditioned on arbitrarily specified demands of average computational cost, and peak computational cost or GPU memory overhead. Finally, Section 3.3 proposes a tailored training algorithm for our model, demonstrating how to integrate temporal information on top of this dynamic network and reasonably predict robotic actions."}, {"title": "3.1 Multi-exit Architecture for Robot", "content": "We first introduce an MLLM architecture featuring multiple intermediate exits, enabling the dynamic adaptation of the MLLM's size to suit the varying situations encountered by robots.\nBasic architecture. Tasked with a language instruction $l$, a robot receives an observation $o_t$ from sensors (e.g., RGB image from the camera in our paper) at timestep $t$ and predicts an action $a_t$ to execute. To correctly predict the action, the robot should not only sufficiently understand the language instructions, but also extract task-relevant information from the images [59]. Built upon an existing work [9], we achieve this by employing a pretrained MLLM, i.e., Flamingo [60, 61], to process and integrate both vision and language inputs, thus obtaining fused multimodal features for decision-making.\nOur basic MLLM mainly consists of a vision encoder $E_V$ and a LLM. The vision encoder $E_V$ comprises a Vision Transformer (ViT) [62] paired with a Perceiver Resampler [60], which encodes an input image $o_t$ into a sequence of informative tokens. For multimodal fusion, an LLM is established on top of the visual representations generated by the vision encoder $E_V$. More specifically, we interleave the self-attention blocks of a pretrained, frozen text-only LLM with newly introduced, learnable cross-attention blocks that cross-attend to the visual tokens. This configuration allows the original MLLM to function as an effective multimodal feature extractor $F_\\theta$, formalized as follows:\n$x_t = F_\\theta(l, E_V(o_t))$,\nwhere $l$ denotes the input language instruction tokens with a length $L$, and the output $X_t = (x_{t,1}, x_{t,2},..., x_{t,L})$ represents the hidden state sequence from the last layer of our MLLM at timestep $t$. Notably, despite the effectiveness of LLMs in multimodal feature integration, their reliance on billions of parameters results in substantial computational costs and memory usage.\nVisual language model with intermediate exits. We dynamically adapt the size of the LLM to the specific requirements of each situation encountered by a robot by introducing a model with intermediate exits. Specifically, we divide the LLM layers into $N$ consecutive groups, noted as $F_1, F_2, ..., F_N$. Each group $F^i$ outputs an intermediate hidden state sequence $x^i_t = (x^i_{t,1}, x^i_{t,2},..., x^i_{t,L})$. When computation terminates at an intermediate exit $i$, we apply a max-pooling operator $P$ to aggregate the information across the token dimension, resulting in a compact representation $x^i_t = P(x^i_{t,1}, x^i_{t,2},..., x^i_{t,L})$"}, {"title": "3.2 Adaptive Inference", "content": "This section demonstrates how DeeR efficiently executes robot tasks by adaptively activating a proper size of the MLLM under predefined computation and GPU memory budgets. Specifically, we first discuss the termination criterion utilized by DeeR, designed to activate smaller models for less complex scenarios and larger models for more challenging conditions. Next, we explore our approach to devising an effective resource allocation strategy that addresses limitations in computation and GPU memory. The inference process of DeeR is illustrated in Figure 1.\nTermination criterion. As mentioned in related works, many previous works utilize confidence-based criteria for determining when to terminate, typically involving metrics such as the maximum element or entropy of the SoftMax output [37, 43, 65, 66, 49]. In our case, where the goal is action prediction and SoftMax output is not readily available, we adopt a different approach by leveraging the consistency of action predictions from adjacent intermediate features as our criterion. The underlying intuition is that if the action predictions from two differently sized MLLMs remain consistent, it suggests that the computational model may have reached saturation, and further processing is unlikely to yield any further improvements. For a given timestep $t$, we identify the smallest $i$ within the range $[1, 2, ..., N]$ that satisfies the following action consistency condition as termination exit:\n$||\\pi_\\theta(x^i_t, h_{t-1}) - \\pi_\\theta(x^{i-1}_t, h_{t-1})||_2 < \\eta_i,$\nwhere we disregard the hidden state outputs of $\\pi_\\theta$ and focus solely on comparing the $L_2$ norm of the difference in predicted actions against a predefined threshold $\\eta_i$. We always adopt infinity as $\\eta_N$ to ensure all samples can exit. For $i = 1$, we use the input features to the LLM layer as $x^{i-1}_t$.\nBudgeted task execution. Given the predefined constraints of computation and memory budgets, it can be challenging to manually set optimal threshold values ${\\eta_1, \\eta_2, . . . }$ to ensure that the robotic MLLM policy achieves peak performance while adhering to budget limitations. In contrast, we propose to determine these values by formulating an optimization problem. We operate under a Budgeted Task Execution Setting, where DeeR is required to perform a set of tasks $T$ within a specified total computational budget $B > 0$. To ensure that each action is delivered within an acceptable waiting time, we impose constraints on peak computation where $G > 0$. Additionally, we limit GPU memory usage to $M > 0$ to accommodate scenarios where users may not have access to high-memory GPUs. Let $S_{cc}(T, {\\eta_1, \\eta_2, . . . })$ represent the success rate of tasks in $T$, and let"}, {"title": "3.3 Training Algorithm", "content": "Notably, it is nontrivial to train our dynamic robotic MLLM properly. Specifically, dynamic adjustment of the network architecture leads to a discrepancy between training and inference. During inference, we use a deterministic criterion to select a proper intermediate feature at each timestep. Nevertheless, during training, we lack a well-defined termination criterion and remain unaware of the distribution of features across the exits. To enable our model to learn to integrate temporal information effectively, we propose a tailored training algorithm, as introduced in the following.\nLearning with an arbitrary size of models. To reduce the aforementioned discrepancy, we propose a simple yet effective random sampling strategy during training. As depicted by the \u201cwinding\u201d curves on the right side of Figure 1, our approach involves sampling an exit index from 1 to $N$ at each timestep. We implement two types of sampling strategies. The first strategy, denoted as $s_1$, is to uniformly sample an exit index from 1 to $N$ at each step. This ensures that features from all possible exits are effectively captured in the action head during training. It simulates scenarios where the action head might encounter features from all exits within a given time window, thus accommodating an"}, {"title": "4 Experiments", "content": "Setup. In this section, we conduct experiments to validate the effectiveness of DeeR as an efficient robot policy. Specifically, we build DeeR upon the RoboFlamingo++ codebase. We preserve hyper-parameters from RoboFlamingo++ for fair comparison, except for the number of LLM layers and our proposed dynamic early-exit paradigm. we compare DeeR in terms of budget v.s. performance with similarly sized RoboFlamingo++ models [9] and other SOTA baselines. We provide implementation details in Appendix A.\nMeasures of efficiency. In modern foundation models, the LLM typically plays a pivotal role within an MLLM in terms of reasoning and problem-solving tasks, and it usually contains the majority of the"}, {"title": "4.1 Main Results", "content": "Results on Flamingo 3B are presented in Figure 3. We train just a single model in each CALVIN setting. Given the predefined total computational budget $B$, the maximum FLOPs $G$, and the GPU memory $M$, we adhere to these budgets by adjusting the termination thresholds, which are determined by solving Equation (4) with the CALVIN dataset. Then we assess the average successful length of DeeR under different thresholds to plot the curves. It can be observed that DeeR consistently reduces the computational cost of the LLMs across all settings. For instance, in the setting D\u2192D, DeeR achieves an average successful length 2.71 with 5.9x fewer average FLOPs, 2x fewer maximum FLOPs and 2x fewer GPU memory. Surprisingly, DeeR-S achieves relatively high performance with only 2GB memory consumed by LLM, which is affordable to most users. Thus, DeeR demonstrates the potential to enable a broader range of users to operate their own robots effectively with LLMs.\nComparison with SOTA baselines. In Table 2, we benchmark the DeeR model against recently SOTA methods in the CALVIN benchmark. Our analysis reveals that DeeR achieves competitive performance compared to the latest SOTA model GR-1 which uses additional proprioceptive information. When compared with traditional imitation learning methods without foundation model, DeeR demonstrates superior performance, particularly in generalization scenarios (ABC\u2192D). Moreover, DeeR slightly outperforms RoboFlamingo while requiring less computation."}, {"title": "4.2 Ablation Study", "content": "Auxiliary losses. In this study, we explore the effect of auxiliary losses using the ABCD\u2192D setting as a representative scenario. As shown in Table 3, the model trained without auxiliary losses demonstrates much lower performance. The drop may stem from insufficient optimization for features across exits: The training paradigm without auxiliary losses optimize only one feature from a single exit at each timestep (chosen by sampling strategy) for action prediction. We also observed that in smaller datasets, such as in the D\u2192D setting, omitting auxiliary losses has little to no impact on performance. This may be because smaller datasets are easier to fit, reducing the necessity for auxiliary losses.\nEarly-termination criterion. Based on a fixed DeeR model, we explore various criteria for adaptive inference. We consider using cosine similarity between exit points to determine termination [55]. Specifically, if the similarity value exceeds a threshold, the process is terminated. We introduce another metric that progressively increases the size of the activated LLM as a task progresses, based on the observation that the initial stage of a task generally present simpler scenarios. Our results, detailed in Table 4, demonstrate that our straightforward yet effective action consistency criterion outperforms other criteria across several average computational budgets.\nReal inference efficiency. We conducted evaluations of real-world operational efficiency. Both RoboFlamingo++ and DeeR were tested on the same Nvidia V100 GPU. As shown in Table 5, DeeR achieved a 68.1% reduction in LLM inference time compared to RoboFlamingo++ (abbreviated as Robo++ in Table 5) when both models achieved the same performance, which aligns with the theoretical 80.7% reduction in FLOPs. This evaluation was performed without code optimizations for early-exit implementation. We expect that with further optimizations, DeeR's real-world operational efficiency will improve, potentially aligning even more closely with the predicted FLOPs reduction."}, {"title": "4.3 Visualization", "content": "Figure 5 displays rollouts of DeeR with the termination points. Situations with a higher exit index are considered \"harder\" by DeeR and thus are allocated more computational resources. One can observe that \"hard\" situations often involve relatively complex and delicate operations, while \"easy\" situations typically involve straightforward movements toward target objects. For example, in the task of stacking blocks (1st row), lifting the blue block from the table (1st image) and placing it down on the pink block (images 4 and 5) are allocated more computation, whereas simply moving towards the pink block (images 2 and 3) requires only the smallest LLM to handle. Similar observations occur in the 2nd and 3rd rows, where the stage of moving toward the target object require minimal computation, while pushing the lightbulb switch or moving the sliding door are sophisticated operations that necessitate more LLM processing."}, {"title": "5 Conclusion and Limitations", "content": "In this paper, we introduced the Dynamic Early-Exit for Robotic MLLM (DeeR) framework, aiming to dynamically configure the size of MLLMs based on the specific requirements of each situation encountered by a robotic agent. In specific, we proposed a novel MLLM architecture with multiple intermediate exits. Further, we establish early-termination criteria for DeeR based on action consistency and solve thresholds via a dataset or online interaction. Additionally, we crafted a tailored training method that integrates temporal information within this multi-exit framework to enhance robotic control. Extensive robotic experiments demonstrated that DeeR significantly reduces LLM computational costs and GPU memory usage, highlighting its potential to empower a broader range of users to manage their robots on resource-constrained platforms. While our study shows promising results, it has some limitations. We focused on improving LLM efficiency for robotic execution since LLMs account for most of the parameters and GFLOPs. However, the computational cost of the visual encoder is also significant. We expect this limitation to be alleviated as more efficient, lightweight visual encoders are developed. Besides, our experiments were limited to a simulated benchmark. Future work will aim to improve the inference efficiency of the entire MLLM-based robotic system in realistic environments."}, {"title": "6 Acknowledgement", "content": "The Tsinghua University team is supported in part by the National Key R&D Program of China (2022ZD0114903)."}, {"title": "A Implementation Details", "content": "We developed DeeR based on our reproduced RoboFlamingo++ codebase, maintaining all hyper-parameters for a fair comparison, except for the number of LLM layers and our proposed dynamic early-exit paradigm. Below, we detail the implementations of RoboFlamingo++ and DeeR."}, {"title": "A.1 Network Architecture", "content": "For the MLLM, we utilize the pretrained model OpenFlamingo, which includes a frozen LLM and vision encoder. The vision-language fusion modules, specifically a perceiver sampler and cross-attention, are trained using the web-scraped image-text datasets LAION-2B and Multimodal C4. The architecture specifics are outlined in Table 7. For the action head, which integrates temporal information for action prediction, we employ a 4-layer LSTM to process temporal information and a 3-layer MLP for predicting actions. To mitigate overfitting, we implement dropout for the LSTM and MLP. Additionally, LayerNorm [74, 75] is applied prior to activation functions.\nWe configured exit points after every two self-attention layers in all MLLM models. To conserve training resources, we employed a subset of the OpenFlamingo model as our backbone. Specifically, for OpenFlamingo3B (which has 24 LLM layers) and OpenFlamingo9B (which has 32 LLM layers), we used only the first 12 layers for DeeR multiexit architecture, resulting in 6 exit points. For RoboFlamingo++, we utilize 6/12/24 LLM layers from OpenFlamingo3B and 8/16/32 LLM layers from OpenFlamingo9B to create a range of model sizes for comparing budget versus performance curve with DeeR."}, {"title": "A.2 Inference Details", "content": "After training, we obtain a multiexit model. During inference, the model remains fixed, but the computational cost of DeeR can be adjusted dynamically based on computational constraints, without modifying the multiexit model. For the 12-layer multiexit model built on top of OpenFlamingo 3B, each LLM layer consumes approximately 0.5GB of memory and requires 1.3G FLOPs during inference (with a batch size of 1, excluding the vision encoder). Given a GPU memory limit of 6GB and a maximum of 15.6G FLOPs, DeeR-B loads all 12 LLM layers, offering 6 exit points. For more resource-constrained scenarios, DeeR-S loads only the first 4 LLM layers, with 2 exit points. For the OpenFlamingo 9B model, each LLM layer consumes approximately 1.0GB of memory and requires 2.85G FLOPs. DeeR-B loads 12 LLM layers with 6 exit points, requiring about 12GB of GPU memory, while DeeR-S loads 8 LLM layers with 4 exit points, using around 8GB of memory. Crucially, users have the flexibility to define custom inference-time models beyond DeeR-S and DeeR-B by selecting the number of LLM layers to load, based on memory or FLOPs constraints.\nAfter determining the number of LLM layers and exit points, the average FLOPs per action can be further dynamically adjusted by adjusting the exit thresholds based on the criteria in Equation (5) or Equation (6). We compute the thresholds using the validation set from environment D. However, we found that thresholds computed using just 1% of the training set achieve similar performance, demonstrating that these thresholds effectively generalize and are robust enough."}, {"title": "A.3 Training Details", "content": "For both RoboFlamingo++ and the multiexit model for DeeR, we employ a two-phase training schedule. Initially, we jointly train the trainable components of the MLLM (the perceiver sampler and cross-attention layers) alongside the action head. Since the backbone MLLM is pretrained and converges more rapidly, we later freeze the MLLM and finetune only the action head, which we refer to as post-training for the action head. Our experiments indicate that this additional finetuning step for the action head results in slightly better performance. The hyperparameters used during training are detailed in Table 8. Note that the dropout rates for LSTM and MLP are 0.3 and 0.4, respectively,"}, {"title": "A.4 Training Cost", "content": "We leverage PyTorch's Automatic Mixed Precision (AMP) acceleration to optimize training efficiency. For DeeR using the OpenFlamingo 3B model, training is conducted on 8 NVIDIA V100 32G GPUs, taking approximately 14 hours for 4+4 epochs on Dataset D, 24 hours for 4+1 epochs on Dataset ABC, and 25 hours for 3+1 epochs on Dataset ABCD. When scaling DeeR to the OpenFlamingo 9B, the model is trained on 8 NVIDIA A100 80G GPUs. The training for 4+4 epochs on Dataset D takes around 24 hours. Multi-node parallel training support is included in the code, offering faster training."}, {"title": "B More Experimental Results", "content": null}, {"title": "B.1 Quantative Results", "content": "The successful rate for the 1st to 5th tasks in a task chain is shown in Table 9, Table 10, and Table 11."}, {"title": "B.2 Rollout Visualization", "content": "In Figure 6, we present a rollout visualization of DeeR successfully completing all five consecutive subtasks in a task chain."}, {"title": "C Broader Impacts", "content": "The potential negative societal impacts of our method align with those typically associated with general robotic technologies. We emphasize the importance of adhering to fair and safe deployment principles in robotic systems."}, {"title": "NeurIPS Paper Checklist", "content": "1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\nAnswer: [Yes]\nJustification: the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope.\nGuidelines:\n\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.\n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.\n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.\n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: We discuss the limitations of the work in the section of conclusion and limitation.\nGuidelines:\n\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.\n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper."}]}