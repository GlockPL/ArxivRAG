{"title": "LLMS FOR DRUG-DRUG INTERACTION PREDICTION: A COMPREHENSIVE COMPARISON", "authors": ["De Vito Gabriele", "Ferrucci Filomena", "Athanasios Angelakis"], "abstract": "The increasing volume of drug combinations in modern therapeutic regimens needs reliable methods\nfor predicting drug-drug interactions (DDIs). While Large Language Models (LLMs) have revolu-\ntioned various domains, their potential in pharmaceutical research, particularly in DDI prediction,\nremains largely unexplored. This study thoroughly investigates LLMs' capabilities in predicting\nDDIs by uniquely processing molecular structures (SMILES), target organisms, and gene interaction\ndata as raw text input from the latest DrugBank dataset. We evaluated 18 different LLMs, including\nproprietary models (GPT-4, Claude, Gemini) and open-source variants (from 1.5B to 72B param-\neters), first assessing their zero-shot capabilities in DDI prediction. We then fine-tuned selected\nmodels (GPT-4, Phi-3.5 2.7B, Qwen-2.5 3B, Gemma-2 9B, and Deepseek R1 distilled Qwen 1.5B) to\noptimize their performance. Our comprehensive evaluation framework included validation across\n13 external DDI datasets, comparing against traditional approaches such as 12-regularized logistic\nregression. Fine-tuned LLMs demonstrated superior performance, with Phi-3.5 2.7B achieving a\nsensitivity of 0.978 in DDI prediction, with an accuracy of 0.919 on balanced datasets (50% positive,\n50% negative cases). This result represents an improvement over both zero-shot predictions and\nstate-of-the-art machine-learning methods used for DDI prediction. Our analysis reveals that LLMs\ncan effectively capture complex molecular interaction patterns and cases where drug pairs target\ncommon genes, making them valuable tools for practical applications in pharmaceutical research and\nclinical settings.", "sections": [{"title": "1 Introduction", "content": "Drug-drug interactions (DDIs) represent a significant challenge in clinical practice, as they can alter the intended\nresponses when patients take multiple drugs simultaneously, resulting in unexpected side effects or decreased clinical\nefficacy [1]. These interactions can lead to adverse drug reactions (ADRs), reduced therapeutic efficacy, or, in severe\ncases, life-threatening conditions [1, 2]. The risk of DDIs is particularly critical given the increasing prevalence of\npolypharmacy [3], where recent studies show alarming rates among older adults, ranging from 40-50% in Western\ncountries (United States, Ireland, Sweden) to over 80% in some Asian countries (South Korea, Taiwan) [4]. With ADRs\nestimated to cost only the U.S. healthcare system $30.1 billion annually, and approximately 18% of these attributed to\nDDIs [2], predicting potential DDIs before clinical use is crucial for patient safety and successful drug development.\nTraditional approaches to identifying DDIs rely on experimental methods, including in vitro and in vivo studies [5].\nHowever, given the vast number of possible drug combinations, these approaches are time-consuming, expensive, and\noften impractical. Moreover, experimental testing of potentially harmful interactions raises significant ethical concerns\ndue to the risk of adverse effects on human subjects [6].\nRecent advances in computational biology and the increasing availability of molecular data have spurred the development\nof in silico methods for DDI prediction, aiming to complement traditional experimental approaches while addressing\nchallenges of interpretability and clinical validation [7]. These computational approaches can be broadly categorized\ninto three main types: literature-based extraction methods [8, 9, 10], which use natural language processes techniques to\nextract DDI information from biomedical literature, but are limited to documented interactions; machine learning-based\nprediction methods [11, 12, 13, 14], which leverage structured data from databases like DrugBank [15], but often require\ncomplex feature engineering, careful architecture design and extensive training data; and pharmacovigilance-based\nmethods [16, 17], which can only identify DDIs after their occurrence in clinical practice [1]. Despite their success,\nthese approaches often require careful integration of heterogeneous data sources, making them challenging to scale and\nadapt to new drug combinations [1].\nLLMs have recently emerged as powerful tools for various biomedical tasks [18], demonstrating remarkable ability\nto identify hidden patterns in textual data. While LLMs have recently shown promise in various pharmaceutical\napplications, such as drug-target interaction prediction [19], molecule-indication translation [20], and DDI gene\nsignature identification through knowledge graph augmentation [21], their potential for direct drug-drug interaction\nprediction remains largely unexplored.\nThis study presents the first comprehensive investigation of LLMs for DDI prediction, leveraging their ability to process\nmultiple drug information simultaneously (SMILES notation, target organisms, and gene interactions) as text. We\nevaluate both zero-shot capabilities and fine-tuning approaches across 18 different LLMs, ranging from state-of-the-art\nmodels to smaller, more efficient variants. Our extensive experiments reveal several key findings. First, while LLMs\ndemonstrate limited effectiveness in zero-shot DDI prediction (average sensitivity of 0.5463), fine-tuning significantly\nimproves their performance. Surprisingly, smaller models like Phi-3.5 (2.7B parameters) achieve the best results, with\na sensitivity of 0.978 and accuracy of 0.919, significantly outperforming both the 12-regularized logistic regression\nbaseline [13] and larger LLMs. This performance advantage of smaller models is consistent across 13 external validation\ndatasets, suggesting that model size is not the determining factor for DDI prediction tasks.\nThe main contributions of the paper are the following:\n\u2022 We present the first comprehensive study of LLMs for DDI prediction, evaluating 18 models ranging from\n1.5B to over 250B parameters.\n\u2022 We introduce a text input representation that combines SMILES notation, target organisms, and gene interac-\ntions, enabling LLMs to leverage multiple aspects of drug information simultaneously.\n\u2022 We demonstrate that while zero-shot approaches show limited effectiveness, fine-tuned smaller models can\nachieve superior performance compared to larger models and the 12-regularized logistic regression baseline,\nestablishing a new state-of-the-art for DDI prediction.\n\u2022 We validate our findings through extensive experimentation across 13 external datasets, confirming the\nrobustness and generalizability of our approach."}, {"title": "2 Background", "content": "In this section, we provide essential background information to contextualize our work. We first describe drug-drug\ninteractions, their mechanisms, and their clinical implications. We then introduce LLMs, focusing on their architecture,\ncapabilities, and the key concepts of zero-shot learning and fine-tuning that are relevant to our study."}, {"title": "2.1 Drug-Drug Interactions", "content": "Drug-drug interactions (DDIs) occur when two or more drugs, taken simultaneously or sequentially, interact in ways\nthat alter their individual effects. These interactions can be classified into two main categories: pharmacokinetic\nand pharmacodynamic interactions [22]. Pharmacokinetic interactions affect how drugs are absorbed, distributed,\nmetabolized, or eliminated from the body, while pharmacodynamic interactions involve changes in a drug's effects at\nits target site. The mechanisms underlying DDIs are complex and can involve various molecular pathways. Common\nmechanisms include:\n\u2022 Competition for drug-metabolizing enzymes (e.g., cytochrome P450).\n\u2022 Alterations in drug transport proteins.\n\u2022 Changes in drug absorption due to pH modifications.\n\u2022 Interference with receptor binding.\nImportantly, DDI prediction is a complicated problem. The order of drug administration can significantly affect the\ninteraction outcomes, making it an inherently directional (asymmetric) problem [23, 24, 25, 26]. For instance, drug A\naffecting drug B's metabolism might have different implications than drug B affecting drug A's metabolism. Moreover,\nthe same drug combination might lead to different interactions, ranging from beneficial (enhanced therapeutic effects)\nto adverse (increased toxicity or treatment failure).\nThe clinical implications of DDIs range from mild to severe. Predicting these interactions presents multiple challenges\n[27]. The vast number of possible drug combinations makes experimental testing impractical, while the complexity of\nbiological pathways and the directionality of interactions add further layers of complexity. Moreover, individual patient\nvariability in drug response and the influence of genetic factors on drug metabolism make the prediction task even more\nchallenging. Understanding and predicting DDIs thus requires approaches capable of capturing both the molecular\nmechanisms of drug interactions and their directional nature. This complexity has led to the development of various\ncomputational methods, each attempting to address different aspects of the DDI prediction problem."}, {"title": "2.2 Large Language Models", "content": "LLMs are neural networks trained on vast amounts of text data to understand and generate human-like text. Through\ntheir transformer-based architecture and attention mechanisms, these models have demonstrated remarkable capabilities\nin various language-related tasks, from translation to summarization [28, 29, 30]. The field has seen rapid advancement\nwith proprietary models like GPT-4 [31] and Claude 3.5 [32], alongside open-source alternatives like LLama [33],\nmaking these technologies increasingly accessible to researchers and practitioners. The development of an LLM\ntypically involves two phases: pre-training and adaptation. Pre-training is a resource-intensive process where the model\nlearns general language understanding from massive datasets. This foundation can then be adapted to specific tasks\nthrough different approaches. The most straightforward is zero-shot learning, where the model makes predictions\nwithout task-specific training, relying solely on its pre-trained knowledge. A more sophisticated approach is fine-tuning,\nwhere the model's parameters are adjusted using task-specific data to optimize performance for a particular application.\nIn the context of DDI prediction, LLMs offer several compelling advantages. Their ability to process multiple drug\ninformation simultaneously allows them to handle diverse inputs, from SMILES notation (a string representation of\nmolecular structure) to target organisms and gene interactions. Furthermore, their attention mechanisms can potentially\ncapture complex relationships between different aspects of drug information. At the same time, their pre-training\non vast amounts of biomedical literature may enable them to leverage implicit knowledge about drug interactions.\nHowever, applying LLMs to DDI prediction also presents unique challenges. The models must be carefully prompted\nto understand the task requirements, and their predictions must be validated against established knowledge. Moreover,\nthe computational resources required for larger models can be substantial, making the efficiency of smaller models\nparticularly relevant for practical applications in healthcare settings."}, {"title": "3 Related Work", "content": "In this section, we review existing approaches for DDI prediction, from literature-based extraction methods to machine\nlearning and pharmacovigilance-based approaches. We conclude with recent applications of LLMs in drug discovery\nand the research gap our work addresses."}, {"title": "3.1 Literature-based extraction methods", "content": "Literature-based extraction methods aim to automatically identify and extract DDI information from biomedical texts,\nincluding medical reports, scientific journals, and clinical documents. These approaches typically frame DDI extraction\nas a relation extraction task, modeled as a multiclass classification problem. Early methods relied on conventional\nclassifier-based approaches, particularly Support Vector Machines (SVMs), using both non-linear [34] linear kernels\n[8], achieving F-scores of 0.6510 in the DDIExtraction 2013 challenge [35].\nMore recent approaches leverage deep learning techniques, including Convolutional Neural Networks, Recurrent Neural\nNetworks and NLP, which can automatically learn representations from text [9, 10, 36]. These models have shown\nsuperior performance to traditional approaches, with state-of-the-art methods achieving F-scores above 0.86 on standard\nbenchmarks like the DDIExtraction 2013 corpus [35, 37].\nWhile these methods have demonstrated success in extracting known DDIs from literature, they are inherently limited\nby their reliance on existing documented interactions, making them unable to predict novel, previously unreported DDIs\n[38]."}, {"title": "3.2 Machine learning-based prediction methods", "content": "Machine learning approaches for DDI prediction can be broadly categorized into several types. Traditional approaches\nare based on similarity measures, where the fundamental concept is that if an interaction exists between drug A and\ndrug B, and drug C is similar to drug A, then an interaction between drug B and drug C may occur [39]. Early works\n[11, 12, 40, 41] employed various similarity measures with classical algorithms such as logistic regression and SVM.\nDeep learning methods (DNN) have demonstrated significant advances in this field. DNN-based approaches, such\nas DeepDDI [42], process structural similarity profiles through dimensionality reduction techniques before feeding\nthem into neural networks for predicting DDI. Other approaches combine multiple drug similarities with Gaussian\ninteraction profiles as input features [43]. Graph-based methods have emerged as powerful tools, modeling DDI\nprediction as a multi-relational link prediction problem on multimodal graphs incorporating drugs, proteins, and side\neffect relationships [44], [45]. These approaches can capture complex patterns in drug interaction networks, though\nthey often require extensive computational resources and careful feature engineering.\nMatrix factorization techniques have emerged as another effective approach for DDI prediction. For instance, ISCMF\nemploys similarity-constrained matrix factorization on the DDI matrix, integrating eight types of similarities (including\nsubstructure, targets, and side effects) [46]. Another method, namely AMF (Adjacency Matrix Factorization) uniquely\nuses only known DDIs as input, sharing latent factors between rows and columns of the interaction matrix [47]. Network\ndiffusion-based methods have also shown promise, developing an integrative label propagation framework that considers\nhigh-order similarities and feature integration [48]. In [49] a random walk-with-restart algorithm has been employed\non protein-protein interaction networks to simulate signaling propagation, demonstrating how network topology can\ninform DDI prediction.\nMore recently, a more straightforward yet effective approach using drug target profiles with 12-regularized logistic\nregression has been proposed [13], demonstrating that gene-level information alone can achieve state-of-the-art\nperformance. While these methods have shown promising results, they typically focus on specific types of drug\ninformation, suggesting the potential benefit of approaches capable of processing multiple drug representations\nsimultaneously."}, {"title": "3.3 Pharmacovigilance-based methods", "content": "Pharmacovigilance-based methods focus on detecting adverse drug events induced by DDIs through the analysis of post-\nmarketing data, playing a crucial role in public health and patient safety. These methods primarily utilize two primary\ndata sources: Spontaneous Reporting Systems, which collect reports of suspected adverse events from healthcare\nprofessionals and patients, and Electronic Health Records, which contain both structured (e.g., laboratory results) and\nunstructured (e.g., clinical notes) data [50, 51]. Three main approaches characterize this field. Disproportionality\nanalysis methods, such as those proposed in [52, 53], detect drug-adverse event combinations occurring at higher\nthan expected frequencies. Multivariate regression approaches [54] employ logistic regression models to analyze the\neffects of concomitant drugs while adjusting for various factors. Association rule mining methods [55], [17], discover\nrelationships between sets of drugs and adverse events using algorithms like Apriori. While these methods have proven\nvaluable for post-marketing surveillance, they face several limitations. They rely heavily on reported adverse events,\nwhich may be incomplete or biased and often have significant detection latency. Moreover, they can only identify DDIs\nafter they have occurred in clinical practice, making them less suitable for preventive screening of potential interactions."}, {"title": "3.4 LLMs in Drug Discovery", "content": "LLMs have recently emerged as promising tools in drug discovery applications. The DTI-LM, a framework leveraging\nLLMs for drug-target interaction prediction that processes protein amino acid sequences and drug SMILES representa-\ntions has been introduced in [19]. Their approach demonstrated that while LLMs show promise in capturing protein\nsimilarities and interactions, current chemical language models still face challenges in effectively representing drug\nsimilarities. A framework leveraging protein language models (ESM-2) and chemical language models (ChemBERTa)\nfor drug-target interaction prediction, has been introduced in [20]. Their approach demonstrated that while LLMs\nshow promise in capturing protein similarities and interactions, current chemical language models still face challenges\nin effectively representing drug similarities. While showing promise, their work highlighted current limitations in\nchemical language models and the need for larger datasets. The DDI-GPT [21] combines LLMs with knowledge graphs\nfor DDI prediction, achieving superior performance (AUROC 0.964) compared to existing methods and demonstrating\neffective zero-shot prediction capabilities. This work also provided interpretable predictions through gene importance\nscoring and network analysis. While these studies show the growing potential of LLMs in drug discovery, the direct\napplication of LLMs for DDI prediction remains an emerging field with opportunities for novel research approaches."}, {"title": "3.5 Research Gap", "content": "Reviewing existing DDI prediction methodologies highlights several limitations that warrant further investigation.\nLiterature-based extraction methods are constrained by their reliance on previously documented DDIs, inherently\nprecluding the prediction of novel interactions [38]. While promising, machine-learning approaches often necessitate\nintricate feature engineering and the seamless integration of diverse, heterogeneous data sources [1]. While the\nefficacy of utilizing gene target information alone for prediction, it has been shown [13], more complex machine-\nlearning methods still face these challenges. Furthermore, deep learning approaches require careful architecture design,\nextensive hyperparameter tuning, and large-scale training data to achieve optimal performance [1]. By their nature,\npharmacovigilance-based methods are reactive, only identifying DDIs following their manifestation in clinical practice\n[50]. This study addresses these critical research gaps by introducing an input representation that combines multiple\ndrug characteristics. We present a systematic evaluation of LLMs for DDI prediction, comparing their performance\nagainst a well-established baseline utilizing gene target information. This work aims to provide insights into the\npotential of LLMs for DDI prediction and establish a foundation for future research in this area."}, {"title": "4 Data and Method", "content": "In this section, we present our research goals, describe the datasets and data processing methods, detail our approach with\nLLMs (both zero-shot and fine-tuned), and outline our evaluation framework for assessing DDI prediction performance."}, {"title": "4.1 Research Goals", "content": "This study aims to investigate the effectiveness of LLMs for DDI prediction using only textual drug information\n(SMILES notation, target genes and organisms). We focus on three main aspects: (i) LLMs zero-shot capabilities,\n(ii) the impact of fine-tuning, and (iii) their performance compared to traditional approaches across multiple external\ndatasets. More specifically, our empirical assessment is driven by three main research questions:\nRQ1 How effectively are LLMs predicting DDIs in a zero-shot setting?"}, {"title": "4.2 Datasets", "content": "Our study utilized two main data sources: DrugBank [15] and a comprehensive collection of external DDI datasets\n[56]. DrugBank is a comprehensive database containing detailed drug information. The dataset includes 16,581\ndrugs with chemical formulas (i.e., SMILES notation), target organisms, and gene interactions. DrugBank contains\nabout 3,921 unique target genes and documents 1,420,072 known drug-drug interactions. This rich dataset is our\nprimary source for zero-shot evaluation and fine-tuning experiments, and we also used it to generate negative examples\n(non-interacting drug pairs) for all datasets. For external validation, we leverage the comprehensive DDI repository\n[56], which aggregates drug interaction information from 14 distinct sources:\n\u2022 Clinical knowledge bases: CredibleMeds, HEP, and HIV.\n\u2022 Annotated corpora: DDI Corpus 2011, DDI Corpus 2013, NLM Corpus, and PK DDI Corpus.\n\u2022 Healthcare systems: OSCAR EMR and WorldVista.\n\u2022 Reference resources: French DDI Referrals, KEGG, and NDF-RT.\n\u2022 Clinical guidelines: ONC High Priority DDI List and ONC Non-Interuptive DDI List.\nThese diverse datasets enable a comprehensive evaluation of our approach across different contexts and data sources."}, {"title": "4.3 Data Processing", "content": "Our data processing (Figure 1) consists of four sequential steps. In the first step, we processed the DrugBank database\nby filtering drugs to include only those that are approved or experimental while excluding withdrawn or illicit drugs.\nFurthermore, we selected only drug pairs where both drugs target at least one gene, as drug target profiles are essential\nfor our representation. For each drug pair, we extracted DrugBank IDs, SMILES notation, target organisms, and two\nbinary vectors (length 3,921) representing gene targets, where each position corresponds to a gene in DrugBank's\nlexicographically ordered gene list. This processing resulted in 1,035,150 positive drug-drug interactions from\nDrugBank.\nIn the second step, we processed the 14 datasets from [56]. We filtered out drugs not present in our processed DrugBank\ndataset and removed any interactions already present in our DrugBank dataset. We then enriched the remaining drug\npairs with features from DrugBank (SMILES, target organisms, gene vectors). We obtained 13 usable datasets, due\nto the fact that the ONC High Priority DDI dataset contained no valid interactions after filtering, totaling 21,947\nadditional unique DDIs. In the third step, after combining all known interactions from DrugBank and external datasets\n(all_known_interactions), we generated 1,057,097 negative examples, ensuring no overlap with any known interaction.\nWe added 1,035,150 negative examples to the processed DrugBank dataset, creating a new balanced dataset. The\nremaining 21,947 negative examples were used to create balanced versions of the external datasets for final validation.\nFinally, from the DrugBank balanced dataset, we randomly extracted 1,000 examples for training and 1,090 for\nvalidation using stratified sampling. The data preprocessing pipeline and the processed datasets (with the exception of\nthe DrugBank dataset, which cannot be publicly distributed) are available in our online repository [57] for reproducibility."}, {"title": "4.4 Methods", "content": "In this section, we describe our experimental methodology. We first present the LLMs selected for our study. Then, we\ndetail our zero-shot and fine-tuning approaches, including prompt design and training strategies."}, {"title": "4.4.1 Selected LLMs", "content": "Our study evaluates 18 different LLMs, ranging from 1.5B to over 250B parameters, to investigate the relationships\nbetween model size, DDI prediction performance with zero-shot and fine-tuned LLMs. These models can be categorized\ninto four groups:\n\u2022 Proprietary models: GPT-4 [31], Claude 2.1 [32], and Gemini 1.5 Pro [58], representing state-of-the-art\ncommercial LLMs accessed through their respective APIs.\n\u2022 Open-source large models: LLaMA 3.3-70B [59] and Qwen2 72B [60], which are publicly available models\nwith architectures and parameter count comparable to commercial solutions.\n\u2022 Open-source middle-range models: Granite 3.1 8B [61], LLaMa 3.1 8B [59], Gemma 2 9B [62], Falcon 3 10B\n[63], Mistral-Nemo 12B [64], Qwen 2.5 14B [60], Gemma2 27B [62], Aya-expanse-32b [65], and Qwen 2.5\n32B [60], representing a balance between computational efficiency and model capacity.\n\u2022 Open-source efficient models: Phi-3.5 (2.7B) [66], Qwen2.5 3B [60], LLaMa3.2 3B [59], and DeepSeek R1\ndistilled Qwen 1.5B [67], representing recent advances in efficient model architectures.\nFor local deployment of open-weight models, we utilized LM Studio [68], a comprehensive platform for experimenting\nwith open-weight LLMs. LM Studio provides a unified interface for model deployment and inference, supporting\nvarious model architectures and configurations. This platform enabled us to maintain consistent experimental conditions\nacross all open-source models while proprietary models were accessed through their respective APIs with standardized\nparameters. Several considerations drove the selection of these models. First, we aimed for model diversity, including\nlarge-scale and efficient architectures, to investigate the relationship between model size and performance. Second, we\nconsidered accessibility by incorporating proprietary and open-source models to assess the feasibility of DDI prediction\nacross different deployment scenarios. Third, we included the latest models, such as Phi-3, Gemma, and DeepSeek R1,\nto evaluate cutting-edge architectures. Finally, we focused on smaller models (1.5B-7B parameters) to explore practical\ndeployment options.\nThis comprehensive selection allows us to evaluate LLMs' general capability in DDI prediction and the specific\ntrade-offs between model size, computational requirements, and prediction performance."}, {"title": "4.4.2 Zero-shot Approach", "content": "To evaluate LLMs' inherent ability to predict drug-drug interactions without any task-specific training, we designed a\nstructured prompt that incorporates all relevant drug information. Using the validation set described in Section 4.3, we\nformatted each drug pair into the following prompt structure:\nSystem prompt\nYou are an expert in drug-drug interaction.\nGiven two drugs, where the order of administration counts, the genes and organisms targeted by the two drugs and\nthe SMILES formulas of the two drugs, classify whether their administration causes 'interaction' or 'no interaction.'\nAnswer only with the classification ('interaction' or 'no interaction'), nothing else.\nUser prompt\nDrug1: drug1\nSMILES for drug1: smiles1\nOrganism targeted by drug 1: org1\nGenes targeted by drug1: genes1\nDrug2: drug2\nSMILES for drug2: smiles2\nOrganism targeted by drug2: org2\nGenes targeted by drug2: genes2\nCLASSIFICATION:\nThis prompt design explicitly includes all available drug characteristics: drug names, molecular structures (SMILES\nnotation), target organisms, and gene interactions. The system prompt emphasizes the importance of drug administration\norder and constrains the model's output to a binary classification.\nFor proprietary models, we submitted these prompts through their respective APIs. We leveraged LM Studio's REST\nAPI feature for open-source models, which provides an OpenAI-compatible interface for local model deployment and\ninference. We collected and stored the ground truth labels and the models' predictions in pickle files for subsequent\nanalysis. This standardized approach ensures consistent evaluation across all models while maintaining the directional\nnature of drug-drug interactions. All evaluation scripts and corresponding results are available in our online repository\n[57]."}, {"title": "4.4.3 Fine-tuning Strategy", "content": "Using the LLM training and validation sets described in Section 4.3, we created JSONL files containing conversational\nsequences structured with system prompts, user prompts (as presented in Section 4.4.2), and assistant responses. Each\nline in these files contains a dictionary containing each role's textual content (system, user, and assistant). For Gemma2,\nwhich does not support the system role, we concatenated the system prompt to the user prompt, resulting in JSONL\nfiles with only user and assistant interactions.\nFor proprietary models, we selected GPT-4 as our state-of-the-art representative for fine-tuning experiments. While\nGemini also offers fine-tuning capabilities, we opted to limit our investigation to one proprietary model due to cost\nconsiderations. Claude was not included in the fine-tuning experiments as Anthropic currently does not provide fine-\ntuning capabilities for Claude Sonnet. For GPT-4, we utilized OpenAI's API to fine-tune the following hyperparameters:\n3 epochs, batch size 3, and learning rate multiplier 0.3, which are suggested by the OpenAI documentation [69].\nWe selected four representatives for open-source models: Phi-3.5 2.7B, Qwen2.5 3B, Deepseek R1 Distilled Qwen\n1.5B, and Gemma2 9B. This selection was motivated by investigating how smaller models perform compared to state-\nof-the-art models like GPT-4 and practical considerations regarding computational resources. We employed Low-Rank\nAdaptation (LoRA) [70] for fine-tuning these models. To optimize the fine-tuning parameters, we implemented a\nhyperparameter search using Optuna [71], aiming to minimize validation loss while avoiding overfitting. The search\nspace included learning rate (log-uniform distribution in the range [1.8e-4, 2.8e-4]), number of model layers to fine-tune\n(16, 18, 20, 22, 24, 26, 28 for Deepseek R1 with 30 linear layers, extended to 32 for models with more layers), LORA\nrank (16 or 32), LoRA alpha scaling (1 or 2, where 1 indicates alpha equals rank and 2 indicates alpha doubles rank),\nLORA dropout (uniform distribution in [0.0, 0.02] with 0.001 steps), and LoRA scale (uniform distribution in [3.8, 4.4]\nwith 0.1 steps).\nWe used the Adam optimizer with a cosine decay learning scheduler across 1000 trials. The optimal parameters for\neach model were:\n\u2022 Phi-3.5 2.7B (1 and 3 epochs): layers=16, learning_rate=2e-4, rank=16, alpha=16, scale=4.0, dropout=0.0.\n\u2022 Qwen2.5 3B (3 epochs): layers=16, learning_rate=2e-4, rank=16, alpha=16, scale=4.0, dropout=0.0.\n\u2022 Deepseek R1 (3, 4, and 5 epochs): layers=20, learning_rate=2.2e-4, rank=32, alpha=64, scale=4.0,\ndropout=0.009.\n\u2022 Gemma2 (5 epochs): layers=16, learning_rate=1e-5, rank=16, alpha=16, scale=4.0, dropout=0.1.\nDuring fine-tuning, model adapters were saved every 100 iterations and merged with the base model layers upon\ncompletion. Given the non-deterministic nature of LLMs, we performed five repeated classifications on both the\nvalidation set and the external datasets described in Section 4.3 to evaluate our fine-tuned models and ensure the\nresults' reliability. These repetitions showed no prediction variability, confirming the stability of our fine-tuned models'\nperformance. We collected and stored the ground truth labels and the models' predictions for subsequent analysis,\nfollowing the same approach used for zero-shot evaluation. All optimization scripts, fine-tuning code, and corresponding\nresults are available in our online repository [57]."}, {"title": "4.5 Evaluation Framework", "content": "To assess the performance of our approaches, we employed a comprehensive set of evaluation metrics and compared\nour results against an established baseline. All the experiments have been performed using a MacBook Pro M3 Max,\nwith 96GB of RAM, 14 cores and a Metal GPU. This section details our evaluation methodology."}, {"title": "4.5.1 Metrics", "content": "We evaluated the performance of both zero-shot and fine-tuned models using several complementary metrics: 'sensitivity'\nmeasures the model's ability to correctly identify positive interactions, which is particularly crucial in drug safety\napplications. 'Precision' quantifies the 'accuracy' of positive predictions, while the F1-score provides their harmonic\nmean. We also calculated the 'accuracy' to measure general performance across both classes. Since the dataset is\nfully balanced regarding the 'target' (same number of positive and negative data instances), 'accuracy' is a reliable\nperformance metric. We didn't use ROC-AUC since there are no probabilities as output from the LLMs. These metrics\nprovide a comprehensive view of model performance, considering the critical importance of identifying dangerous\ninteractions: 'sensitivity' and the need for reliable predictions: 'accuracy'."}, {"title": "4.5.2 Baseline Comparison", "content": "We selected the 12-regularized logistic regression model [13] as our baseline. In their original work, the authors reported\nimpressive performance metrics, achieving an 'accuracy' of 0.9479, 'sensitivity' of 0.9556, 'specificity' of 0.948 and\nROC-AUC of 0.9884. Using the balanced DrugBank dataset described in Section 4.3, we created training and validation\nsets (95% and 5%, respectively). Following the original methodology, we used only the drug target gene profiles as\ninput features. The model was tuned using a stratified 10-fold cross validation exploring the regularization parameter C\nin the range $]12^{-16}, 2^{16}]$ specified in the original paper. We evaluated this trained model on the same LLM validation set\nand external balanced datasets described in Section 4.3, computing the metrics detailed in Section 4.5.1.\nOur baseline comparison served two purposes: first, to try to reproduce the results of [13], and second, to provide a\ndirect performance comparison between our LLM-based approaches and the established state-of-the-art method on the\nsame datasets."}, {"title": "5 Results and Discussion", "content": "This section presents our experimental findings around our three research questions: zero-shot capabilities of LLMs, the\nimpact of fine-tuning, and comparative analysis with the 12-regularized logistic regression baseline."}, {"title": "5.1 RQ1: Zero-shot Learning Analysis", "content": "Our zero-shot evaluation (see Table 3) reveals several interesting patterns across different model sizes and architectures.\nProprietary models (GPT-4, Claude Sonnet, and Gemini Pro) generally demonstrated superior performance, with\nClaude Sonnet achieving the highest 'accuracy' (0.7358) and 'precision' (0.8859) among all models. However, even\nthese state-of-the-art models showed relatively modest 'sensitivity' scores (0.5413-0.5927), indicating limitations in\nidentifying positive interactions without task-specific training.\nAmong"}]}