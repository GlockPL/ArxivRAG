{"title": "Learning Graph Node Embeddings by Smooth Pair Sampling", "authors": ["Konstantin Kutzkov"], "abstract": "Random walk-based node embedding algorithms have attracted a lot of attention due to their scalability and ease of implementation. Previous research has focused on different walk strategies, optimization objectives, and embedding learning models. Inspired by observations on real data, we take a different approach and propose a new regularization technique. More precisely, the frequencies of node pairs generated by the skip-gram model on random walk node sequences follow a highly skewed distribution which causes learning to be dominated by a fraction of the pairs. We address the issue by designing an efficient sampling procedure that generates node pairs according to their smoothed frequency. Theoretical and experimental results demonstrate the advantages of our approach.", "sections": [{"title": "1 Introduction", "content": "Representation learning from graphs has been an active research area over the past decade. DeepWalk [24], one of the pioneering approaches in this field, learns node embeddings by generating random walks on the graph. A standard and highly efficient method for optimizing the embedding objective is to train a binary classification model that distinguishes between positive and negative node pairs, known as the negative sampling approach. Positive pairs are generated by applying the skip-gram model [21] to the node sequences and represent nodes whose embeddings should be similar, in contrast to negative pairs.\nMany works have since extended the original DeepWalk algorithm in three main directions: i) presenting different (random) walk strategies for traversing the graph and generating node sequences [1, 9, 10, 25, 29, 31, 39, 41], ii) designing new embedding learning models [1, 2, 11, 12, 26], and iii) designing new techniques for negative pair sampling [3, 16, 19, 35]. Inspired by observations on real graphs, we take a different approach and propose a general regularization technique that adjusts the frequency distribution of positive node pairs.\nIn the standard negative sampling setting, when a positive pair u, v is generated, we also sample k \u2265 1 negative pairs u, x, where the node x is selected at random from some distribution on the graph nodes. Usually, this is the \u03b1-smoothed node degree distribution $\\frac{d(u)}{\\sum_{v \\in V}d(u)^{\\alpha}}$, where d(u) is the degree of node u and \u03b1 \u2208 (0, 1] is a hyperparameter.\nThe pairs are then provided as input to a binary classification model that learns the node embeddings.\nWe propose to apply frequency smoothing to positive pairs. If a pair u, v occurs #(u,v) times in the random walk corpus D, after smoothing it will be provided as a positive example to the classification model approximately $T_{\\beta}#(u, v)^{\\beta}$ times, where $T_{\\beta}$ increases with decreasing \u03b2 \u2208 (0,1] and $\\sum_{u,v \\in D} #(u, v) = \\sum_{u,v \\in D}T_{\\beta}#(u,v)^{\\beta}$.\nMotivation and paper contribution. Before introducing yet another hyperparameter like the smoothing exponent \u03b2 we must consider the following questions:\n\u2022 Are there any insights suggesting that frequency smoothing might be beneficial? We present theoretical results in Section 4, but at a high level, we argue that smoothing enhances the robustness of the learning process. Random walk-based embeddings inherently assume that the most frequent positive pairs are the most important, and that embeddings should preserve their similarity. However, in real graphs, the frequency distribution is highly skewed.\n\u2022 Can frequency smoothing be efficiently implemented? Applying the approach used for negative sampling would be to compute and explicitly store the frequencies of all pairs that appear in the random walk sequences and then sample according to the smoothed frequencies. But as we show in Section 7, this would be unfeasible for larger graphs because the number of pairs grows superlinearly with the graph size. We present a simple and highly efficient technique for smooth pair sampling which can be of independent interest. Our algorithm leverages data sketching techniques which provably ensures its scalability.\nOrganization of the paper In the next section we define the problem setting. In Section 3 we present SmoothDeep-Walk, an algorithm that achieves smooth positive pair sampling by only slightly modifying DeepWalk. We then provide a theoretical analysis of the benefits of smoothing for node embeddings in Section 4 and, to address scalability, present an efficient data sketching approach for large graphs in Section 5. Related work is discussed in Section 6. Experimental evaluation is presented in Section 7 and the paper is concluded in Section 8."}, {"title": "2 Preliminaries", "content": "We consider undirected connected graphs G = (V, E) but all presented algorithms also work for directed graphs. Let n = |V|. The degree of a node u \u2208 V is the number of edges (u, v) \u2208 E and is denoted by d(u). The set of neighbors of node u is N(u) = {v \u2208 V : (u, v) \u2208 E}. A walk w of length l on G is a sequence of l nodes in G such that $(w_i, w_{i+1}) \\in E$ for i\u2208 {1,2, . . ., l \u2212 1}. Given a set of walks W, we define the pair corpus to be the multiset of positive node pairs D = {(wi, wj) : \u2203w \u2208 W such that |i \u2013 j| \u2264 t, i \u2260 j} for a user-defined window size t \u2265 1.\nThe total number of pairs is M = |D|, and we denote the number of unique pairs in D by P. Further, $M_{\\beta} = \\sum_{u,v \\in D} #(u, v)^{\\beta}$ for \u03b2 \u2265 0, where #(u, v) is the frequency of u, v in D. We set $\\#u^{(\\beta)} = \\sum_{v \\in V} #(u, v)^{\\beta}$. Let the pairs be sorted in descending order by their frequency and denote the i-th pair frequency in D as fi(D). We say that the i-th pair has rank i. It holds $M = \\sum_{u,v \\in D} #(u, v) = \\sum_{i=1}^{P} f_i(D)$.\nThe d-dimensional vector embedding of node u is denoted by \u0169 \u2208 Rd.\nAt a high level, we propose to replace the original corpus D by a \u03b2-smoothed corpus D\u03b2 in which the original frequencies #(u, v) are smoothed to $T_{\\beta}#(u, v)^{\\beta}$ with $T_{\\beta} = [M/M_{\\beta}]$. Observe that $T_{\\beta} \\in [1, M/P)$ for \u03b2 \u03b5 (0,1] and monotonically increases with decreasing \u03b2. Also, the cardinality of D\u03b2 remains M.\nFrequency distribution. A common statistical formalization of the skewness in real-life datasets is the assumption of Zipfian distribution with parameter z > 0. Let $S_z = \\sum_{i=1}^{P} \\frac{1}{i^z}$ and $X_z = M/S_z$. In our setting, we assume the pair frequency for the i-th pair is $f_i = \\frac{X_z}{i^z}$ and it thus holds $\\frac{M}{S_z} \\sum_{i=1}^{P} \\frac{1}{i^z} = M$."}, {"title": "Training objective.", "content": "The embedding algorithm maximizes\n$J = \\sum_{(u,v) \\in D} log \\frac{exp(\\tilde{u}^T \\tilde{v})}{\\sum_{s \\in V} exp(\\tilde{u}^T \\tilde{s})}.$\nLet \u03bc : V \u2192 (0, 1] be a distribution on the graph nodes. The above objective is efficiently approximated by the following alternative objective which uses negative sampling, i.e., for a positive pair u, v \u2208 D we generate k \u2265 1 negative pairs u, x from \u03bc:\n$J = \\sum_{(u,v) \\in D} log \\sigma(\\tilde{u}^T \\tilde{v}) + k \\cdot E_{x \\sim \\mu} log \\sigma(-\\tilde{u}^T \\tilde{x})$\nwhere \u03c3(x) = $(1 + exp(-x))^{-1}$ is the sigmoid function. A popular choice for \u03bc is the smoothed degree distribution $\\mu(v) = \\frac{d(v)}{\\sum_{v \\in V} d(v)^{\\alpha}}$ for \u03b1 \u2208 [0, 1]."}, {"title": "3 Smooth Pair Sampling", "content": "We present an algorithm that builds upon the positive pair generation approach used in the original DeepWalk algorithm and smooths the pair frequencies on the fly. Algorithm 1 outlines how the proposed SmoothDeepWalk works. The total number of positive pairs M is a function of the number of nodes, the number of walks per node, the walk length and the window size. We iterate over the graph nodes, from each node we start s random walks, each of length l, and from each node sequence generate positive node pairs using the skip-gram approach [21]. For each positive pair we sample k \u2265 1 negative node pairs that are fed into a binary classification model with an embedding matrix \u03a6, until we have sampled M positive pairs. The only difference to DeepWalks is in lines 13-14 where we sample a candidate pair u, v with probability $\\#(u, v)^{\\beta-1}$ for \u03b2 \u2208 (0,1]. Note that for \u03b2 = 1 we have the standard DeepWalk algorithm. In a single pass over the random walk corpus we observe the pair u, v exactly #(u, v) times, thus we expect to sample it $\\#(u, v)^{\\beta}$ times. We also expect $T_{\\beta} = [M/M_{\\beta}]$ passes. Using that individual samples are independent, we show the following result (proof in Appendix A):\nTheorem 1 Let G = (V, E) and D be the corresponding random walk corpus. Let \u03b2 \u2208 (0,1]. The following hold for SmoothDeepWalk:\n\u2022 Let $M_{\\beta} > cT_\u03b2(T_\u03b2 + 1)$ for some c > 1. With probability at least $1 \u2212 e^{\u2212c}$, SmoothDeepWalk needs between $T_\u03b2 \u2212 1$ and $T_\u03b2 + 1$ passes over the corpus D.\n\u2022 Let $S_{u,v}$ be the number of positive samples of pair u, v returned by SmoothDeepWalk. It holds $E(S_{u,v}) = T_\u03b2\\#(u,v)^\u03b2$.\nIf $\\#(u, v) \u2265 1/\u03b5^2 log 1/\u03b4$ for \u03b5, \u03b4 \u2208 (0,1), then with probability 1 \u2013 \u03b4 it holds $|S_{u,v} \u2013 T_\u03b2(\\#(u,v)^\u03b2)| \u2264 \u03b5\\#(u, v)$.\nThe theorem shows that SmoothDeepWalk accurately smooths the pair frequencies and despite the random sampling step, the running time of the algorithm is predictable and almost certain. For any reasonable choice of the hyperparameters l, t, s and \u03b2, $T_\u03b2 = M/M_\u03b2$ is a constant and the condition $M_\u03b2 > cT_\u03b2(T_\u03b2 + 1)$ is satisfied for a large value of c."}, {"title": "4 How does smoothing help?", "content": "As already discussed, for a smoothing parameter \u03b2\u2208 (0,1] the pair frequencies are transformed to $T_{\\beta}#(u,v)^{\\beta}$. The cardinality of the most frequent pairs thus decreases, and of less frequent pairs increases. The vertical gray line in Figure 1 shows the transition point. Under the assumption that pair frequencies follow Zipfian distribution with parameter z, we first analyze what is the pair rank when this transition occurs depending on z and \u03b2, i.e., how the data skew and the smoothing level affect the location of the gray line.\nTheorem 2 Let D be a corpus of cardinality M of node pairs. Let the frequencies in D follow a Zipfian distribution with parameter z \u2265 0, and D\u03b2 be the \u03b2-smoothed corpus for \u03b2\u2208 (0,1]. Let j be the minimum pair rank such that $f(D_{\\beta})_j > f(D)_j$.\n\u2022 If z > 1 and \u03b2z > 1, then j = c(z, \u03b2) for some constant c.\n\u2022 If z > 1 and \u03b2z < 1, then $j = P^{\\frac{1-\\beta z}{1-z}} = o(P)$.\n\u2022 If z < 1, then j = O(P).\nThe above result has a very intuitive interpretation. For highly skewed distributions, a slight decrease in the skew such that \u03b2z > 1 would reduce the frequency only for a few of the most frequent pairs. If the decrease is more significant, i.e., \u03b2z < 1 (one can visually interpret this as the \"angle\" between the blue and orange lines in Figure 1 becoming larger), then we move j, i.e., the gray vertical line, to the right. Still, the frequency only for a sublinear number of the pairs decreases. And if the distribution is not very skewed, then we will increase the frequency only for a constant fraction of the pairs.\nWe next analyze how smoothing impacts the embeddings of individual nodes. The proof of the next theorem follows from [18] and can be found in Appendix A.\nTheorem 3 Let D\u03b2 be \u03b2-smoothed corpus of cardinality M\u03b2 for \u03b2\u2208 (0,1]. Let \u03bc : V \u2192 [0,1] be the negative sampling probability distribution. For all node pairs u,\u03c5 \u2208 V, SmoothDeepWalk optimizes the objective $\\tilde{u} \\tilde{v} = log \\frac{\\#(u,v)^\u03b2}{\\#_u^{(\\beta)} \\mu(\\nu)} - log k$.\nRecall that $\\#_u^{(\\beta)} = \\sum_{w \\in V} #(u, w)^\u03b2$. The negative sampling distribution \u03bc(v) is independent from u. We analyze the expression $\\frac{\\#(u,v)^\u03b2}{\\#_u^{(\\beta)} \\mu(\\nu)}$ under the assumption that for the n pairs u, w with w \u2208 V, the frequencies follow a Zipfian distribution with parameter z > 1. Let the rank of the pair u, v be i, and its frequency be O(1/i^z). We have $\\#_u^{(\\beta)} = \\sum_{i=1}^{n} (\\frac{k}{i})^\u03b2$.\nWe distinguish the following cases:\n\u2022 If i is one of the top ranks among the u, w pairs, then we have $\\frac{\\#(u,v)^\u03b2}{\\#_u^{(\\beta)}} = O(1)$ for z > 1. The objective then optimizes $O(\\frac{1}{\\mu(v)})$. By smoothing, we can bound $\\frac{\\#(u,v)^\u03b2}{\\#_u^{(\\beta)}} = O(n^{1-\\beta z})$ for \u03b2z < 1 which regularizes the optimization objective for high-frequency pairs from O(1/\u03bc(v)) to $O(\\frac{n^{1-\\beta z}}{\\mu(v)})$.\n\u2022 For i > k, we have $\\frac{\\#(u,v)^\u03b2}{\\#_u^{(\\beta)}} > \\sum_{j=1}^{n}(\\frac{k}{j})^\u03b2 = O(k^{\\tilde{z}})$. In particular, for \u03bc(v) > $1/k^{\\tilde{z}}$ the number of negative samples of u, v will exceed the number of positive samples. (Note that this might explain the observation that hierarchical softmax in word2vec is better for infrequent words than negative sampling [21].) By smoothing the frequencies such that \u03b2z < 1, we get $\\sum_{j=1}^{n}(\\frac{k}{j})^{\\beta z} = O(k^{\\beta z} n^{1-\\beta z})$. Thus, if the rank of u, v is $k > \\frac{n^{1-\\beta z}}{z-\\beta z}$ it holds $\\frac{\\#(u,v)^\u03b2}{\\#_u^{(\\beta)}} > \\frac{\\#(u,v)}{\\#_u}$. Thus, the representation for low frequency pairs improves.\nIn summary, Theorem 2 shows how smoothing affects the overall distribution of positive samples, and Theorem 3 presents a generalized version of DeepWalk's optimization objective that distributes the contribution of individual pairs more evenly. This highlights the motivation behind smoothing as a tool that allows to extend the set of node pairs that are considered important."}, {"title": "5 Efficient Pair Frequency Estimation", "content": "The obvious drawback of Algorithm 1 is that we need to know the frequencies #(u,v) of all P pairs in the corpus, and thus it needs O(P) memory. It holds $P = O(n \\cdot l \\cdot t \\cdot s)$ and when using the default values for the hyperparameters l, t, s, for the 8 real graphs considered in our experimental evaluation it holds $P = O(n^c)$ for c > 1.64. Thus, for larger graphs the amount of available memory on commodity machines will likely be insufficient. Solutions that use external memory like Glove [23] can be impractical for various reasons. We address the issue by leveraging algorithms for frequent item mining in data streams, a widely studied data mining problem [8]. We use a compact summary of the pair frequency distribution that allows us to approximate the frequency of each node pair (u, v).\nWe describe how the FREQUENT method [13] detects heavy hitters in two passes over a data stream, pseudocode can be found in Appendix B. FREQUENT maintains a dictionary S, the so-called sketch, that stores up to b distinct pairs together with a counter lower bounding the frequency of each pair. When a new pair (u, v) is generated, we check whether it is already in S. If so, we increment the corresponding counter. Otherwise, we insert (u, v) with a counter set to 1. If there are b + 1 pairs in S, we decrease by 1 the counter of all those pairs and remove pairs whose counter has become 0, so we guarantee at most b pairs remain in S. The last step corresponds to removing b + 1 distinct pairs from the multiset of positive pairs. The total number of pairs is M and the weight of a pair can be underestimated by at most M/b. Thus, all pairs that appear more than M/b times are guaranteed to be in S after processing all pairs. We implement S as a hash table and charge the cost of each counter decrement to the cost incurred at its arrival. Thus, the amortized cost per update is constant, and the total running time is O(M).\nIn a second pass over the corpus (we assume that the random walks are created by setting a random seed and the corpus is generated on the fly), we compute the exact frequency of all pairs recorded in the sketch. In this way, in SmoothDeepWalk we work with the exact frequencies of the heavy pairs. For pairs not recorded in the sketch, we return an overestimation of the pair frequency as (M \u2013 total weight of pairs in sketch)/b. (In Appendix C we discuss alternative sketching methods.)\nThe following result shows that for a skewed frequency distribution we need a compact sketch in order to provably detect the most frequent pairs. The proof is based on a result by [5] and is provided in Appendix A.\nTheorem 4 Let the pair frequencies follow Zipfian distribution with parameter z. For z > 1, FREQUENT returns the exact frequency of the k most frequent pairs using a sketch of size b = O(k). For z < 1, the required sketch size is b = O(k* P^{1-z}).\nComputational complexity. The space complexity of Algorithm 1 is O(n \u00b7 d + b) where b is the number of pairs in the sketch and can be chosen according to the amount of available memory. The time complexity depends on the sampling probability $\\#(u, v)^{\\beta\u22121}$. In a single pass over the random walk sequences we expect to sample $M\u03b2 = \\sum_{(u,v) \\in S} #(u, v)$ pairs. Thus, the expected number of passes is $T_{\\beta} = [M/M\u03b2]$. However, the number of pairs provided as input to the embedding learning algorithm is independent of \u03b2 and training the embeddings is the computationally intensive part."}, {"title": "6 Related work", "content": "We give an overview of the main node embeddings approaches. The reader is referred to the survey by [14] for more details and a deeper discussion.\nFactorization of the graph adjacency matrix is an intuitive and efficient approach as most graphs are sparse and there exist efficient approaches to sparse matrix factorization [22, 40]. Inspired by word2vec [21], DeepWalk [24] was the first algorithm that employs random walks and which in turn inspired other approaches to exploit different (random) walk strategies [1, 9, 10, 25, 29, 31, 39, 41]. DeepWalk uses hierarchical softmax [21] for optimizing the embedding objective. Due to its efficiency, negative sampling has become the standard optimization method and many negative sampling variants have been proposed [3, 16, 19, 35]. Building upon [18], [26] show that several of the above algorithms can be unified in a common matrix factorization framework and provide a precise definition of the corresponding matrix. A main advantage of word2veec and DeepWalk is that the matrix is never explicitly generated.\nAs an alternative, discrete node embeddings represent nodes by vectors consisting of discrete features such as node attributes [17, 33, 34]. Discrete embeddings are compared using their Hamming distance and are interpretable if the entries are human readable features. The embedding models are efficient as there is no need to train a classification model. But they achieve lower accuracy and limit the choice of machine learning algorithms they can be combined with for downstream tasks.\nGraph neural network models [6, 11, 15] often yield state-of-the-art embedding methods. The models are inductive and can compute embeddings for previously unseen nodes by aggregating the embeddings of their neighbors. In contrast, the above discussed approaches are transductive and need to retrain the model to learn embeddings for new nodes. Also, GNNs naturally incorporate into the learning process node attributes and can be trained in end-to-end classification tasks. This comes at the price of the typical deep learning disadvantages such as scalability, tedious hyperparameter tuning and the difficulty to obtain rigorous theoretical results like the ones in [26].\nSmooth negative sampling. Probably most relevant to our approach is the work by [35]. The authors argue for smooth sampling of negative pairs from the positive distribution in order to guarantee that we don't oversample or undersample some negative pairs. Using only limited training data, it is shown that learning needs to trade off the objective of positive sampling and the expected risk that the embeddings deviate from the objective due to a limited number of samples. It is concluded that \"negative sampling distribution should be positively but sublinearly correlated to their positive sampling distribution\u201d. So instead of generating $\\#(u, v)^\u03b2$ positive samples, we generate $\\#(u, v)$ positive and $\\#(u, v)^{1-\u03b2}$ negative samples. The big advantage of our approach compared to smooth negative sampling is its efficiency. In order to sample a negative node pair, [35] uses a complex MCMC sampling algorithm that requires the evaluation of the inner product for several candidate node pairs which needs time O(d2). In contrast, SmoothDeepWalk needs a single hash table lookup per candidate pair and time O(1)."}, {"title": "7 Experimental evaluation", "content": "In this section, we introduce the graph datasets used for evaluation and discuss their properties. We then present the experimental setup and analyze how the performance of SmoothDeepWalk in link prediction is influenced by the sketch budget b and the smoothing exponent \u03b2. We derive default values for b and \u03b2 depending on the computational complexity and the graph structure. Using these default values, we evaluate the performance of smoothing for node classification, and present a smooth version of node2vec. The performance of the proposed algorithms is then compared to other node embedding methods. Note that since the evaluation is extensive, we only present our main findings. We refer to Appendix D for details and additional results. A prototype Python implementation \u00b9 is publicly available, see Appendix D for details about the implementation."}, {"title": "8 Conclusions and future work", "content": "We presented pair frequency smoothing, a novel regularization technique for (random) walk-based node embedding learning algorithms. Theoretical and experimental results highlight the potential of the proposed approach. A natural direction for future research is to apply the method to other node embedding algorithms like those discussed in the previous section. Additionally, various approaches to negative sampling have been developed; we refer to [19] for an overview of recent results. Future research could explore optimal ways to combine smooth sampling of positive pairs with these negative sampling approaches.\nHowever, we have identified two main open questions:\n\u2022 Develop a deeper understanding of how the graph structure affects the performance of smoothing. Our observations on the clustering coefficient are only a first step in this direction. For instance, we have yet to explain why smoothing enhances node classification on the Cora and Citeseer graphs but provides no advantage for link prediction on these graphs.\n\u2022 Explore the use of smoothing in graph neural networks. In Section D in the appendix we present results for the unsupervised version of GraphSage [11] where the input are positive and negative node pairs generated in the same way as in DeepWalk. The gains are relatively modest. However, it should be noted that this setting does not allow GNNs to show their full potential and DeepWalk and node2vec yield somewhat better results. Nonetheless, we anticipate that smooth sampling can be used as a tool for sampling from the local neighborhood of each node. For example, PinSage [36] uses importance sampling from the local neighborhood of each node by employing short random walks."}, {"title": "A Theoretical analysis", "content": "Proofs\nTheorem 1 Let G = (V, E) and D be the corresponding random walk corpus. Let \u03b2 \u2208 (0, 1]. The following hold for SmoothDeepWalk:\n\u2022 Let $M_\u03b2 > cT_\u03b2(T_\u03b2 + 1)$ for some c > 1. With probability at least $1 \u2212 e^{\u2212c}$, SmoothDeepWalk needs between $T_\u03b2 \u2212 1$ and $T_\u03b2 + 1$ passes over the corpus D.\n\u2022 Let $S_{u,v}$ be the number of positive samples of pair u, v returned by SmoothDeepWalk. It holds $E(S_{u,v}) = T_\u03b2\\#(u,v)^\u03b2$.\nIf $\\#(u, v) \u2265 1/\u03b5^2 log 1/\u03b4$ for \u03b5, \u03b4 \u2208 (0,1), then with probability 1 \u2013 \u03b4 it holds $|S_{u,v} \u2013 T_\u03b2(\\#(u,v)^\u03b2)| \u2264 \u03b5\\#(u, v)$.\nProof: In a single pass over the pair corpus D we expect to sample MB pairs. We thus expect TB = [M/MB] passes over D will be necessary in order to sample M pairs. We first bound the probability to sample less than M pairs in TB + 1 passes over D. Let X\u2081 be indicator random variables such that X\u2081 = 1 if the i-pair is sampled, for 1 \u2264 i \u2264 M (Ts + 1) and let L = $\\sum_{i=1}^{M(T_3+1)} X_i$. It holds E(L) = M + MB. The X\u2081 are independent but not identically distributed Bernoulli random variables. By Hoeffding's inequality we obtain\n$Pr(L < M) = Pr(L < E(L) \u2013 M_\u03b2) \u2264$\n$exp \\Bigg(\\frac{-2M_\u03b2^2}{M(T_\u03b2 + 1)}\\Bigg) = exp \\Bigg(\\frac{-2M_\u03b2}{T_\u03b2(T_\u03b2 + 1)}\\Bigg) \u2264 e^{-2c}$.\nwhere the last inequality follows from the assumption $M_\u03b2 > cT_\u03b2(T_\u03b2 + 1)$.\nSimilarly, we obtain that\n$Pr(\\sum_{i=1}^{M(T_\u03b2-1)} X_i \u2265 M) \u2264 e^{-2c}$\nBy the union bound we obtain that the number of passes over D is between \u03a4\u03b2 1 and TB + 1 with probability 1-2e-2c > 1-e-.\nWe next analyze the number of samples for individual pairs. In the following we assume that the number of passes is concentrated around T\u03b2 with high probability. For a pair u, v, there are T\u03b2#(u, v) sampling trials with probability $\\#(u, v)^{\u03b2\u22121}$, therefore the expected value of Su,v follows from linearity of expectation. The random numbers r \u2208 U[0, 1) are independent, and we have identically distributed variables. We can thus consider #(u,v) Bernoulli i.i.d. random variables Yi with E(Yi) = $\\#(u, v)^{\u03b2\u22121}$. By Chernoff inequality for $\\#(u, v) \u2265 1/\u03b5^2 log 1/\u03b4$ we have\n$Pr\\Bigg(\\sum_{i=1}^{\\#(u,v)} Y_i \u2265 #(u, v)^\u03b2 + \u03b5\\#(u,v)\\Bigg) < exp(-2#(u, v)\u03b5^2) \u2264 \u03b4/2$\nSimilarly we have\n$Pr\\Bigg(\\sum_{i=1}^{\\#(u,v)} Y_i \u2264 #(u, v)^\u03b2 \u2212 \u03b5\\#(u,v)\\Bigg) < \u03b4/2$\nand by the union bound the concentration bound follows.  \nIn the following proofs we use the following bounds on the Zipfian distribution over P unique pairs. For z \u2260 1 it holds\n$S_z = \\sum_{i=k}^{P} \\frac{1}{i^z}; \\\\ X_z = \\frac{M}{S_z} ; \\\\ \\sum_{i=1}^{k} f_i = \\sum_{i=k}^{P} \\frac{X_z}{i^z} = \\sum_{x=k}^{P} (\\frac{P^{-1}}{x^z}) \\\\ \\sum_{i=k}^{P} (\\frac{k^{-1}i^z X_z}{i^z}) = O(\\frac{k^{1-z} X_z}{X})$"}, {"title": "Theorem 2", "content": "Let D be a corpus of cardinality M of node pairs. Let the frequencies in D follow a Zipfian distribution with parameter z \u2265 0, and D\u03b2 be the \u03b2-smoothed corpus for \u03b2\u2208 (0,1]. Let j be the minimum pair rank such that\n$f(D_{\\beta})_j > f(D)_j$.\n\u2022 If z > 1 and \u03b2z > 1, then j = c(z, \u03b2) for some constant c.\n\u2022 If z > 1 and \u03b2z < 1, then $j = P^{\\frac{1-\\beta z}{1-z}} = o(P)$ where P is the number of unique pairs in D.\n\u2022 If z < 1, then j = O(P).\nProof: We analyze for which node pairs the probability to be sampled increases after applying smoothing when the pair frequencies follow a Zipfian distribution. We need $T_{\\beta}\\frac{f_i^{\\beta}}{M}$ decreases with increasing z. for k > 1 and z > 1 it holds $\\sum_{i=k}^{P} f_i = O(\\frac{1-z}{P})$ For z < 1 1 it holds Sz = $O(p^{-z})$. passes over the random walk corpus D. The j-th pair that is originally sampled fj times, after smoothing will be sampled in expectation $T_aff^i$ times, and as we have shown in Theorem 1 the number of samples is concentrated around the expected value. Thus, the j-th pair will be sampled more times when\n$\\frac{T_\u03b2f_j^{\\beta}}{M} > \\frac{f_j}{M} \\\\T_\u03b2 > f_j^{(1-\\beta)}$ \\\\$\\frac{\\sum_{i=1}^P f_i}{\\sum_{i=1}^P f_i^\u03b2} > f_j^{(1-\\beta)}$  \\tag{1}\nIndependent of the smoothing exponent, we sample exactly M positive pairs in total. Under the power law assumption we have $f_j = \\frac{Xz}{jz}$ for some z \u2265 0 for the frequency of the j-th pair, and it holds $\\sum_{j=1}^{p} \\frac{Xz}{jz} = M$. By replacing the power law frequencies fj by $X_z/j^z = \\frac{3M}{X}$ in the above we obtain\n$\\frac{\\sum_{i=1}^P f_i}{\\sum_{i=1}^P f_i^\u03b2} = \\frac{\\sum_{i=1}^P \\frac{Xz}{i^z}}{\\sum_{i=1}^P (\\frac{Xz}{i^z})^\u03b2}= \\frac{\\sum_{i=1}^P Xz}{i^z}\\/{\\sum_{i=1}^P \\frac{X^\u03b2}{i^\u03b2z}} = M^M^{-\u03b2} \\frac{S^\u03b2jz}{S_j}$.\nPlugging in the above into (1) we obtain\n$\\frac{\\sum_{i=1}^P f_i}{\\sum_{i=1}^P f_i^\u03b2} > f_j^{(1-\\beta)}$$\\\\\\M^{1-\\beta} \\frac{S^{\u03b2}jz}{S_j} > (\\frac{X}{j^z})^{(1-\\beta)}$\\\\\\M^{1-\\beta} > (\\frac{X}{j^z})^{(1-\\beta)} > \\frac{S^{\u03b2}jz}{S_j} > {S^{\u03b2}jz}{S_j}$.\nWe can now observe how smoothing affects the number of generated positive samples for less frequent pairs using the bounds for Sz. We distinguish following cases:\n\u2022 z > 1, \u03b2z > 1. For all pairs of rank at least j where $jz(1\u2212\u03b2) > C\u03b2z/Cz$, we will sample more positive pairs when smoothing with parameter B. It holds C\u03b2z > Cz. However, only for a constant fraction of the most frequent pairs we will generate less positive pairs, as even for \u03b2z close to 1 the term C\u03b2z is a small constant.\n\u2022 z > 1, \u03b2z < 1. The minimum necessary rank j now depends on the total number of pairs $P: j^{z(1-3)} > \\frac{Cz}{C_{\\beta z}^{\beta}}$. It holds \u03b2\u2208 [0, 1/z). Assuming 1 \u2212 \u03b2z is constant, we need $j > P^{\\frac{1}{z-\\beta z}}$. Thus, for larger z or for a small enough \u03b2 only o(P) pairs will have a larger number of positive samples before smoothing.\n\u2022 z < 1. Here we obtain that the minimum pair rank j needs to satisfy $jz(1\u2212\u03b2) > \\frac{j^{z(1-B)Pz}}{P-Bjz}$. This is only possible for a constant fraction of the positive pairs. This confirms the intuition that smoothing would significantly affect only embedding learning for highly skewed distributions. (In the extreme case when z \u2192 0 and the frequencies are almost uniformly distributed, smoothing will not change anything.)"}, {"title": "Theorem 3", "content": "Let D\u03b2 be \u03b2-smoothed corpus of cardinality M\u03b2 for \u03b2 \u03b5 (0", "1": ".", "\u03bc": "V \u2192 [0"}, {"1": "be the negative sampling probability distribution. For nodes u", "k$.\nProof": "For the training objective it holds\n$J = \\sum_{u \\in V} \\sum_{v \\in V}T\u03b2#(u,v) log \u03c3(\\tilde{u}^T\\tilde{v})+\\\\ \\sum_{u \\in V} \\sum_{v \\in V"}]}