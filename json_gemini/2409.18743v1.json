{"title": "OpenObject-NAV: Open-Vocabulary Object-Oriented Navigation Based on Dynamic Carrier-Relationship Scene Graph", "authors": ["Yujie Tang", "Meiling Wang", "Yinan Deng", "Zibo Zheng", "Jiagui Zhong", "Yufeng Yue"], "abstract": "In everyday life, frequently used objects like cups often have unfixed positions and multiple instances within the same category, and their carriers frequently change as well. As a result, it becomes challenging for a robot to efficiently navigate to a specific instance. To tackle this challenge, the robot must capture and update scene changes and plans continuously. However, current object navigation approaches primarily focus on semantic-level and lack the ability to dynamically update scene representation. This paper captures the relationships between frequently used objects and their static carriers. It constructs an open-vocabulary Carrier-Relationship Scene Graph (CRSG) and updates the carrying status during robot navigation to reflect the dynamic changes of the scene. Based on the CRSG, we further propose an instance navigation strategy that models the navigation process as a Markov Decision Process. At each step, decisions are informed by Large Language Model's commonsense knowledge and visual-language feature similarity. We designed a series of long-sequence navigation tasks for frequently used everyday items in the Habitat simulator. The results demonstrate that by updating the CRSG, the robot can efficiently navigate to moved targets. Additionally, we deployed our algorithm on a real robot and validated its practical effectiveness.", "sections": [{"title": "I. INTRODUCTION", "content": "With the advancement of visual language models (VLM) [1] and large language models (LLM) [2], the realization of cognitive navigation [3] has attracted increasing attention. Imagine a daily environment, a robot is expected to navigate efficiently to any object, whether it is static furniture or a frequently used object with changing positions (such as a cup). This necessitates the robot to represent and update the current state of objects in the scene and navigate to the target.\nCurrent object navigation methods [4]-[13] can effectively navigate to static objects (like a sofa). However, they are often limited to searching for semantic-level objects and lack the capability to update scenes. Hence, when it comes to fre- quently used daily items such as a \u201ccup on the black table,\u201d these objects typically come in various colors and styles. They may appear in places such as the kitchen, bedroom, and others, and their positions are not fixed. Additionally, they are usually carried by other objects, meaning that the carriers are also not fixed. Such navigation targets are highly dynamic and subject to interference, making it challenging to efficiently and effectively navigate to them.\nTo appropriately represent highly dynamic and interference-prone daily environments and achieve more efficient navigation to everyday objects, an appropriate hierarchical scene representation is essential. Current scene representation methods [14]-[20] have already constructed hierarchical scene graphs. However, they struggle to represent everyday dynamic environments due to two key challenges. First, their graph structures fail to capture the relationships between static objects and frequently moved everyday items. Second, frequently used daily objects often have changing positions and repetitive semantics, making efficient updates more difficult. Considering that frequently used objects are typically carried by static ones, this article distinguishes between static carriers and the objects they carry. We leverage LLM and VLM to identify these static carriers and construct an open-vocabulary carrier-relationship scene graph (CRSG), which effectively represents the carrying relationships between objects. In the process of robot navigation, new observations are matched with carrier objects in the carrier layer. For the observed carrier objects, dynamic updates are performed to reflect any changes in the everyday objects they carry.\nBased on the CRSG, we designed an object-oriented navigation strategy, modeling the object search process as a Markov Decision Process (MDP) [21]. At each navigation step, the robot decides to navigate toward candidate target ob- jects or unexplored carrier objects based on visual-language feature similarities and the commonsense knowledge from the LLM, until the target is found.\nIn summary, our contributions are as follows:\n\u2022 We present an adaptable carrier relationship scene graph (CRSG) that primarily describes the dynamic carrier and carried relationships between objects.\n\u2022 We design a navigation strategy based on the CRSG, utilizing visual-language features and commonsense knowledge from the LLM to inform decision-making.\n\u2022 Extensive qualitative and quantitative experiments demonstrate that our method effectively navigates to long sequences of moved objects, and the effectiveness of updating CRSG has been validated. Additionally, we deployed and tested the algorithm on a real robot, confirming its practicality."}, {"title": "II. RELATED WORK", "content": "A. Open Vocabulary Mapping\nWith the advent of vision-language models like CLIP [22] and its variants, scene mapping with different representations has moved beyond the limitations of fixed classes [23], [24], and expanded to open-vocabulary [19], [25]\u2013[29]. Clip-fields [30] integrates CLIP and SBERT features into the neural implicit map, enabling open-vocabulary map queries and navigation for robots. VLMap [31] and IVLMap [32] project CLIP features top-down onto a 2D grid to enable zero-shot (instance) vision-language navigation. ConceptGraph [19] and Hovsg [20] constructed instance-level point cloud maps with CLIP features embedded and scene graph representing certain relationships between objects, which facilitates more detailed and precise object retrieval.\nThese maps provide crucial support for applications such as open-vocabulary object queries, scene understanding, and robot navigation. However, they do not have dynamic update capabilities. We construct a carrier-relationship scene graph (CRSG) that describes the dynamic carrier and carried re- lationships between objects, and continuously updating the CRSG during navigation. This helps achieve more efficient navigation of everyday objects.\nB. Object Navigation\nObject navigation [4]-[13], [33], as one of the key tasks in the field of embodied intelligence, primarily involves navigating to a specified semantic or instance location within a scene. [4]-[9] mainly perform object navigation within the close-classes. [19], [20], [32] constructed an offline instance map of the scene, enabling zero-shot instance navigation. [10]-[13] perform open-vocabulary object navigation using the frontier exploration method. However, they [10]-[13], [19], [20], [32] cannot capture changes in the positions of fre- quently used objects, or the addition and removal of instances in the scene. [33] involves object navigation within close- classes and similarly does not involve scene updates. We have built a dynamic open vocabulary CRSG that not only supports semantic object navigation but also enables efficient navigation to everyday instances (such as a red cup) that are spatially variable and subject to semantic interference. The approach most similar to ours is GOAT [34], which also implements memory capabilities for the latest scene and supports multi-type and multi-modal navigation command inputs. However, for navigating to a displaced everyday object, we have designed a navigation strategy based on CRSG, while GOAT selects the closest unexplored region to navigate to the object."}, {"title": "III. METHOD", "content": "A. Problem Definition\nIn a daily environment, when given a navigation command, the robot queries the CRSG to determine the navigation end- point and proceeds to the specified destination. If the target is a daily item (e.g., a cup) that is being carried, the robot evaluates whether the item remains in its original location based on the current observations. If not, the robot initiates a strategic exploration process. We define this challenge as a displaced object exploration and navigation task within an everyday setting. An overview of the system framework is provided in Fig. 2. The cost function is defined as follows:\n$\\P_L = \\sum_{t=1}^{T} Length(L_t, L_{t+1})$\nLet $L_t$ represent the position of the exploration target at step t, and $Length(L_t, L_{t+1})$ denote the shortest path between $L_t$ and $L_{t+1}$, calculated by using path planning algorithms. Additionally, T represents the number of ex- ploration attempts the robot makes to navigate to the target object. We aim to minimize $P_L$ in Eq. (1).\nB. Carrier-Relationship Scene Graph (CRSG)\nWe first construct an open-vocabulary instance map M using the pre-collected RGB-D data of the scene. Unlike ConceptGraph [19], each instance object $O_i \\in O$ ($O$ is the set of all objects) not only contains a CLIP feature $V_F$; but also stores a caption description list cap\u00bf generated from the Tokenize Anything model [35] and text feature $T_{Fi}$ encoded with SBERT model [36]. A Carrier-Relationship Scene Graph (CRSG) S-G is then constructed below.\nBuilding and Room layer: Existing works, such as [16], [20], propose various methods for room segmentation. We select specific point clouds from M and project them onto the x y plane: wall point clouds within a certain height range from the ground, and \u201cdoor\u201d point clouds filtered using captions and text features. Next, we apply the line-search method, inspired by [37], to identify closed contours for each room, assigning objects within these contours to their corresponding rooms. The combined room layers constitute the building layer."}, {"title": "Carrier layer", "content": "We calculate the similarity between the text features TF of each object $O_i$ and the SBERT-encoded text feature T for \u201cfurniture for holding objects\u201d. The mathemat- ical expression for this is as follows.\n$\\operatorname{sim}(T\\_F\\_i,T) = \\frac{T F\\_i T}{\\|T F\\_i\\| \\|T\\|}$    \nNext, we select the set of objects $O \\subset O$ with a similarity score exceeding a specified threshold \u03c3, as shown below.\n$\\tilde{O} = \\{(O_i, cap_i, T\\_F\\_i) | sim(T\\_F\\_i, \\tilde{T}) > \\sigma\\}$    \nNext, we extract the three most frequent captions for each cap in O, input them into a LLM (GPT-40 for test), and use a specific prompt to identify potential carrier-type objects, denoted as $O_1 \\subset O$.\nFinally, we select the final set of carrier-layer objects, denoted as $\\tilde{O} \\subset O_1$, based on criteria such as the objects' geometric dimensions exceeding a certain size and their contact with the ground, as shown below.\n$\\overline{O} = f(O_1)$   \nObject layer: For any non-carrier-layer object $O_i \\in (O-O)$, we determine whether $O_i$ is carried by a carrier- layer object $O_j \\in O$ based on $O_i$'s dimensions, the closest distance, and the spatial overlap relationship in the x-y-z directions between $O_i$ and $O_j$ (exceeding a certain overlap rate). h($O_j$, $O_i$) is defined to encapsulate the consideration of the aforementioned factors, where h($O_j$,$O_i$) = 1 if all the conditions are satisfied. For any $O_j \\in O$, we define the set of objects C($O_j$) carried by $O_j$ as follows:\n$\\mathbb{C}(O\\_j) = \\{O | h(O\\_j,O\\_i) = 1, O\\_i \\in (O - \\overline{O})\\}$"}, {"title": "C. Navigation Strategy for a Displaced Object", "content": "Let the input navigation command for the target object be either a text, or an image. text or image is encoded using the SBERT or CLIP model, respectively. The resulting feature is then compared with the SBERT or CLIP features of each object in the CRSG S_G using cosine similarity, similar to Eq. (2). The object with the highest similarity score is selected as the target object, $O_{target}$.\nWe model the exploration of a displaced object as a fixed- policy Markov decision process (MDP) below.\nstate space S: In the current step t, we define:\n1. the robot's pose $L_t \\in L$,\n2. the set of unexplored carrier-layer objects $CR_t \\in CR$,\n3. the set of candidate target objects on the unexplored carrier-layer objects $CT_t \\in CT$,\n4. the flag of finding the target or not $F_t \\in \\{0,1\\}$. ( L, CR and CT denote the value set of $L_t$, $CR_t$ and $C\u0422_t$ respectively.)\nThe state variable $S_t$ is defined in (6).\n$S_t = (L_t, CR_t, CT_t, F_t) \\in S$   \nIn the initial state $S_0 = (L_0,CR_0, CT_0, F_0)$, $L_0$ is the initial position of the robot, $CR_0 = \\emptyset$, and $CT_0 = O_{target}$.\naction space A:\n$A = \\{Stop, Explore(cr), Goto(ct) | cr \\in CR\\_t, ct \\in CT\\_t\\}$    \nStop indicates that the task is completed or all carrier- layer objects have been explored. Explore(cr) and Goto(ct) represent exploring the carrier-layer object $cr \\in CR_t$ and navigating to the location of $ct \\in CT_t$, respectively.\nThe robot selects the next action $a_t \\in A$ based on the current state $S_t$ according to a specific policy \u03c0(\u00b7) in (8).\n$a_t = \u03c0(S_t)$   \npolicy \u03c0(\u00b7): Given current state $S_t = (L_t,CR_t, CT_t, F_t)$,\n1. if $F_t = 1$ or $CR_t = \\emptyset$, then $a_t = Stop$.\n2. If $F_t = 0$ and $CT_t \\neq \\emptyset$, we prioritize and select a candidate object to proceed with. Specifically, let $CT_t$ = {$O_{t1}$,..., $O_{ti}$}. Some additional variables are stored: the SBERT similarities $SS_t$ = {$sst_1$, ..., $sst_i$} between $CT_t$ and $O_{target}$, the distances $D_t$ = {$d_{t1}$,\u2026\u2026,$d_{ti}$} between $L_t$ and"}, {"title": null, "content": "$CT_t$, and the average depth values $D_t$ = {$d_{t1}$,\u2026,$d_{ti}$} when $CT_t$ are observed by the robot's camera. The priority rating of any $O_{tj} \\in CT_t$ corresponding to $sst_j$, $d_{tj}$ and $d_{tj}$, is evaluated as follows. The parameters in (9) are set as $w_1 = 3$, \u03b1 = 0.1 and $w_2 = 1$ in the experiments.\n$P\\_R(O\\_{tj}) = \\frac{w\\_1 \\cdot sst\\_j \\cdot \\exp(-\\alpha d\\_{tj})}{1+w\\_2 \\cdot d\\_{tj}}$   \nWhere $sst_j$ is positively correlated with $P\\_R(O_{tj})$, as we assume that a larger $sst_j$ indicates a higher likelihood that the candidate is the target. Moreover, $d_{tj}$ is negatively correlated with $P\\_R(O_{tj})$, based on the assumption that the accuracy of the front-end detection model decreases as $d_{tj}$ increases. Therefore, $\\exp(-\\alpha d\\_{tj})$ is considered the confidence level of $sst_j$. The robot will navigate to the location of the object with the maximum $P_R$ and explore for $O_{target}$.\n3. If $F_t = 0$, $CT_t = \\emptyset$ and $CR_t \\neq \\emptyset$, the LLM selects one of the carrier objects $cr_k \\in CR_t$ and the robot executes the action $a_t$ = Explore($cr_k$). Specifically, the captions for each carrier object in $CR_t$ are extracted and provided as input to the LLM, along with the image or caption of the target object. Leveraging the LLM's commonsense understanding of object-carrier relationships (e.g., \"a cup is unlikely to be placed on a toilet\"), the LLM identifies the carrier object where the target object is most likely to be found.\nstate transition process: If $a_t$ = Explore(cr) (where $cr \\in CR_t$) or $a_t$ = Goto(ct) (where $ct \\in CT_t$), then during the robot's movement, let $CR_{observed}$ represent the set of carrier objects observed within a small radius r that have no candidate targets on them (based on the latest environmental observations), and let $CT_{new}$ represent the set of new target candidates found on unexplored carrier objects. Since some candidates in $CT_t$ may be carried by objects in $CR_{observed}$, $CT_t$ is updated to $CT_t^\u2217$ after these candidates are removed. Specifically, the candidates in $CT_{new}$ are those for which the SBERT feature similarities with the target exceed a threshold $\u03b8_1$. Additionally, the similarities between the target $O_{target}$ and the objects carried in $CR_{observed}$ don't exceed $\u03b8_1$.\n1. if $a_t$ = Explore(cr), $CR_{t+1}$ and $CT_{t+1}$ are updated as:\n$CR\\_{t+1} = CR\\_t \\setminus (\\{cr\\} \\cup CR\\_{observed})$  \n$CT\\_{t+1} = CT\\_t^\u2217 \\cup CT\\_{new}$  \n2. if $a_t$ = Goto(ct), $CR_{t+1}$ and $CT_{t+1}$ are updated as:\n$CR\\_{t+1} = CR\\_t \\setminus (\\{cr\\_1\\} \\cup CR\\_{observed}), ct \\in C(cr\\_1)$   \n$CT\\_{t+1} = CT\\_t^\u2217 \\cup CT\\_{new} \\setminus \\{ct\\}$  \nIn either case, the SBERT feature similarities between $O_{target}$ and the objects carried by cr or $cr_1$ are calculated. If the input command is an image, an LLM-based image comparison is also performed. If the combined score from the LLM's image comparison and the SBERT text similarity for a carried object exceeds $\u03b8_2$, then $F_{t+1}$ = 1, and the task is marked as complete."}, {"title": "D. CRSG Adaptation", "content": "Matching carrier objects. As the robot navigates, it periodically captures RGB and depth images from the en- vironment. The RGB images are processed through Crop- Former [38], Tokenize Anything model [35], CLIP [22] and SBERT [36] to obtain instance masks, captions, encoded CLIP features and encoded SBERT features, respectively. For newly observed objects, the robot compares them with the carrier objects in SG to identify the observed carrier objects Omatch. The primary aspects of comparison include the object's size, the distance between their center positions, and the similarity scores based on CLIP and SBERT features.\nThe Addition or Removal of Carried Objects. For currently observed instances, h(\u00b7,\u00b7) in Eq. (5) is used to determine whether they are being carried by Omatch. Let the set of carried objects in the new observations be defined as Ocr_d. The previously carried objects on Omatch are then compared with Ocr_d. The criteria also include the object's size, the distance between center positions, and the SBERT feature similarity score. After the comparison, the carried objects on Omatch are updated accordingly: they are either added, removed, or left unchanged."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "We aim to answer the following research questions:\n1. Does the carrier-relationship scene graph (CRSG) im- prove the accuracy of instance object queries (Sec. IV-A)?\n2. Does the dynamic update of the CRSG contribute to more efficient instance navigation (Sec. IV-B, IV-C)?\nMetrics. We report Success Rate(SR) and Success weighted by inverse Path Length (SPL) [39]. SPL measures the efficiency of an robot's path by comparing it to the shortest route from the starting point to the target object instance. If the robot fails to reach the target, the SPL score is zero. Otherwise, the score is calculated as the ratio of the shortest path length to the robot's actual path length, with higher values indicating better performance.\nA. Object Query on the Outdated Offline Map\nWe compare the object query accuracy of scene repre- sentations with those of VLMap [31] and ConceptGraph [19]. A total of 50 queries with different types of nav- igation instructions (semantic, instance and requirement- driven) were conducted across 3 scenes in Gibson [40]. The experimental results are presented in Tab. I, where our object query success rate averages 86% and is the highest in all three scenarios. Because in instance queries like \u201ca cup on the table\", the CRSG we constructed records the carrying relationship between cup and table, thus allowing for precisely locating the instance. In contrast, VLMap [31] and ConceptGraph [19] may find the table instead of the c\u0438\u0440. Besides, we additionally incorporates text features of caption descriptions for each object in CRSG, demonstrates superior performance in differentiating between similar objects, such as black cup and white cup. Meanwhile, VLMap projects the CLIP features from 3D space to a 2D grid, and its queryable semantic types are limited, resulting in inferior performance. We illustrate partial query results of different methods in Fig. 3, and the results demonstrate that our method performs better in distinguishing between objects of the same category and in querying carried instances.\nB. Long-sequence Navigation Task for Frequently Used Ev- eryday Items\nWe conducted a series of long-sequence frequently used daily items navigation experiments (4 or 5 objects as a sequence) in three everyday scenarios in Gibson [40] using the Habitat navigation simulator [41]. In each scene, we placed a variety of frequently used items from different instances (like black cup, blue alarm clock, plastic bottle, game controller and so on), while offline constructing the CRSG for each scene. We set up multiple distractor objects of the same category (like black / white cup) to validate our ability to correctly navigate to a specific instance. We then randomly altered the positions of these items to simulate the variability in the locations of commonly used everyday objects. Next, the robot is instructed to sequentially navigate to these objects in each scene.\nWe present the navigation results for each scene in Tab. II for SR and Tasks_SR(i), and Fig. 5 for SPL. Tasks_SR(i) represents the success rate of correctly navigating to all i objects. We also presented SR in Tab. II when using only SBERT feature similarity (ours-Text) and GPT-40 (ours- LLM) for image matching to determine targets. Tab. II shows that ours achieves generally the highest SR and Tasks_SR(i). This indicates that considering both SBERT feature similarity and the image matching results from GPT-40 contributes to navigating to the true-positive target. Fig. 4 illustrates an example of long-sequence navigation, where the efficiency of navigating to the target significantly improves as the number of navigated objects increases. Additionally, as shown in Fig. 5, the SPL for the first object is noticeably lower, while the SPL for the remaining objects shows significant improvement. Since the process of navigating to the first object involves multiple explorations, it often results in a navigation path length that greatly exceeds the shortest path. During this period, the observed CRSG is updated, enabling the positions of other objects to be refreshed. As a result, when navigating to the remaining objects (including new instances), point-to-point navigation is primarily achieved, leading to a increase of SPL.\nC. Ablation Study\nTo further investigate the role of CRSG updates in effi- cient navigation to everyday objects, we conducted ablation experiments on one long-sequence navigation tasks in each of Scene 2 and Scene 3, comparing cases with and without CRSG updates. The results (in Fig. 5, second and third fig- ures) indicate that when CRSG is updated, the SPL gradually increases, while there is no improvement in SPL without CRSG updates. Thus, the updates to CRSG contribute to more efficient navigation to displaced everyday objects.\nWe also conducted single daily object navigation exper- iments in three different scenes to evaluate the impact of various modules in our navigation strategy on navigation efficiency. only-carriers_Random represents navigating to a randomly selected carrier object for exploration, without considering candidate target objects. only-carriers_LLM selects the next carrier object for exploration based on LLM's recommendations, building upon only-carriers_Random. As shown in Tab. III, our method achieves the highest SPL, followed by only-carriers_LLM. This indicates that the strategy of navigating to candidate target objects and select- ing the carrier objects to explore based on the commonsense knowledge of LLM contributes to more efficient navigation.\nD. Real-World Validation\nWe validated our algorithm using an Autolabor robot in a real scene, equipped with an industrial computer featuring an NVIDIA GeForce RTX 3080. We equipped the robot with a Livox Mid 360 LiDAR and utilized the Cartographer SLAM algorithm [42] to obtain its global pose. Additionally, an Azure Kinect DK was mounted to capture RGB-D infor- mation. The robot successfully navigates to a displaced red book shown in Fig. 6."}, {"title": "V. CONCLUSIONS", "content": "This paper has proposed an open-vocabulary navigation method for frequently used everyday items, leveraging a dynamic carrier-relationship scene graph (CRSG). Specifi- cally, we first construct the CRSG to capture the dynamic relationships between carrier objects and the objects they carry. Next, a navigation strategy based on the CRSG is developed to navigate to frequently used items, modeling the object search process as a Markov Decision Process (MDP). At each navigation step, the CRSG is dynamically updated based on the robot's observations of the environment. The robot then decides whether to navigate toward candidate target objects or unexplored carrier objects, guided by visual-language feature similarities and commonsense knowledge from the LLM. Both simulations and physical experiments demonstrate that our method efficiently navigates to objects that are subject to positional changes, even in the presence of distractors from the same category. In the future, we plan to incorporate an online mapping module and parallel processing to improve exploration efficiency."}]}