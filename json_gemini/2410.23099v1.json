{"title": "Comparative Analysis of Demonstration Selection Algorithms for LLM In-Context Learning", "authors": ["Dong Shu", "Mengnan Du"], "abstract": "In-context learning can help Large Language Models (LLMs) to adapt new tasks without additional training. However, this performance heavily depends on the quality of the demonstrations, driving research into effective demonstration selection algorithms to optimize this process. These algorithms assist users in selecting the best k input-label pairs (demonstration examples) based on a given test input, enabling LLMs to in-context learn the relationship between the provided examples and the test inputs. Despite all the proposed demonstration selection algorithms, their efficiency and effectiveness remain unclear. This lack of clarity make it difficult to apply these algorithms in real-world scenarios and poses challenges for future research aimed at developing improved methods. This paper revisits six proposed algorithms, evaluating them on five datasets from both efficiency and effectiveness perspectives. Our experiments reveal significant variations in algorithm performance across different tasks, with some methods struggling to outperform random selection in certain scenarios. We also find that increasing the number of demonstrations does not always lead to better performance, and that there are often trade-offs between accuracy and computational efficiency. Our code is available at https://github.com/Tizzzzy/Demonstration_Selection_Overview.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have achieved state-of-the-art performance across a wide range of natural language processing tasks (Achiam et al. 2023; Dubey et al. 2024; AnthropicAI 2023). One of the key factors contributing to this success is their capability for in-context learning, which allows these models to adapt to new tasks without additional training (Xie et al. 2021). However, their performance is highly sensitive to the quality of the provided demonstrations. Recently, various demonstration selection algorithms have been developed to enhance this quality by selecting the most informative and relevant examples from the data pool. These algorithms have significantly reduced the time required for LLMs to address unseen tasks and have greatly improved their overall performance.\nDespite the success of these approaches, the effectiveness of selected examples and the efficiency of the selection and inference processes are not well understood. This lack of understanding makes it challenging for future research to identify areas for improvement and makes it difficult to use these algorithms in real-life situations. In this paper, we present a comparative analysis of six prominent demonstration selection algorithms, evaluating them on five diverse datasets from both efficiency and effectiveness perspectives. Our study aims to provide valuable insights into the strengths and limitations of current approaches, serving as a benchmark for future research and guiding practitioners in selecting appropriate demonstration selection methods for their specific use cases. Our comparative experiments yield the following key findings:\n\u2022 Contrary to expectations, not all demonstration selection algorithms consistently outperform random selection. Some algorithms struggle to surpass the random selection in certain scenarios.\n\u2022 We observe substantial accuracy gaps between different algorithms on the same dataset with the same number of demonstrations. For instance, in the MRPC dataset, this accuracy difference can be as high as 45% when comparing CBDS with RD-direct.\n\u2022 Our analysis reveals that increasing the number of demonstrations does not always lead to better performance. The relationship between the number of demonstrations and accuracy is not monotonic and varies significantly across different tasks and algorithms.\n\u2022 We find that while some algorithms like CBDS and UP-RISE achieve high accuracy, they do so at the cost of poor efficiency, requiring over 5 and 3 seconds respectively per sample for demonstration selection. This trade-off poses challenges for real-world applications where quick response times are crucial."}, {"title": "Demonstration Selection Algorithms", "content": "In this study, we compare six demonstration selection algorithms for in-context learning: Concept Based Demonstration Selection (CBDS), Rethinking Demonstrations direct (RD-direct) and channel (RD-channel), LLM Retriever, UP-RISE, and OpenICL TopK. These algorithms employ various strategies, from leveraging latent concepts to using retrieval models, to select the best examples for LLMs. Additionally, we include OpenICL Random as a baseline, which randomly selects examples from the data pool. Here are more details of the comparing algorithms:\n\u2022 Concept Based Demonstration Selection (CBDS) (Wang et al. 2024b): This algorithm proposes a Bayesian approach to select demonstration examples for in-context learning in LLMs. The approach involves two main steps: (1) The optimal value of the latent concept variable @ is learned as a set of new token embeddings using a small LLM. This step aims to align the latent concept variable with the token embedding space of the LLM. (2) After learning the optimal latent concept, the algorithm selects demonstrations that maximize the likelihood of inferring the optimal latent variable for the task at hand. These selected demonstrations can then be used with larger LLMs to improve performance. The selection process is mathematically grounded in the following equation:\n$P(Y | X, Y_1, ..., X_k, Y_k, X) = \\int_{\\Theta} P(Y |\\Theta,X)P(\\Theta | X_1, Y_1,..., X_k, Y_k, X) d\\Theta.$\nIt represents the probability of predicting the correct output Y given the selected demonstrations and the test input X, integrating over the latent variable space \u04e8.\n\u2022 Rethinking Demonstrations (Min et al. 2022): This approach examines the factors that contribute to the success of in-context learning in LLMs. It explores the impact of different aspects of demonstrations, particularly focusing on the distribution of the input text, the label space, and the overall format. The key findings are encapsulated in the following:\n$P(y | x_1, y_1,...,x_k, y_k, x) \\approx format + label space + input distribution$\nThis formula reflects the idea that in-context learning performance is driven by the structure and content of the demonstrations, rather than the accuracy of the label pairings.\n\u2022 LLM Retriever (Wang, Yang, and Wei 2023): This framework focuses on selecting high-quality in-context examples to enhance the performance of LLMs. The key approach involves an iterative process where an initial set of example candidates is retrieved using an unsupervised method like BM25 (Robertson, Zaragoza et al. 2009). These candidates are then ranked based on the conditional log probabilities of the ground truth outputs pro-\nvided by the LLM. The ranking is formalized in the following equation:\n$logp(y | x, x_i, Y_i), \\forall i \\in \\{1, 2, ..., n\\},$\nwhere $p(y | x, x_i, Y_i)$ represents the conditional probability of the output y given the input x and the i-th candidate example $(x_i, Y_i)$. A reward model, based on a cross-encoder architecture, is then trained to distill these ranking preferences into a dense retriever. This retriever is further refined iteratively by leveraging the feedback from the LLM, ultimately improving the selection of in-context examples.\n\u2022 UPRISE (Cheng et al. 2023): UPRISE enhances the performance of LLMs in zero-shot settings by retrieving and utilizing effective prompts. The approach involves two main steps: first, the retriever retrieves a set of positive prompts $P^+$ from a pre-constructed pool $P$ based on a given task input x, as formulated in the equation:\n$P^+ = R(x, P)$,\nwhere R(x, P) is the retrieval function. Then, these retrieved prompts are concatenated with the input and fed into a frozen LLM to generate the output y:\n$Y_{p^+} = LM(y_{p^+} | P^+ + x)$.\nThis approach allows for cross-task and cross-model generalization, meaning the retriever is trained on diverse tasks with a smaller LLM but can be applied to larger LLMs and unseen tasks during inference.\n\u2022 OpenICL (Wu et al. 2023): It is designed to facilitate ICL research and improve the evaluation of LLMs. The core approach for selecting demonstration examples involves retrieving examples based on various methods like TopK, VoteK, and BM25, which are then used to construct the context for the LLM's inference. The retrieval of examples can be formalized as:\n$(x, y) \\in R(X, Y)$,\nwhere R(X, Y) represents the retrieval function applied to the training data X and Y. The selected examples are then concatenated with the test input to form a single sequence, which is fed into the LLM to generate predictions. In this paper, we adopt the TopK and Random retrieval strategies.\nNotice that the RD algorithm supports both direct and channel approaches for demonstrating in-context examples, denoted as E. As the Figure 2 shows, for each example $e_i$ in E, it is paired with a input x and a label y. In the direct approach, $e_i$ is structured with x presented first, followed by y. Conversely, in the channel approach, $e_i$ is structured with y presented first, followed by x. It is also important to note that we used \"demonstrations with gold labels\" for both the direct and channel approaches. As for other algorithms, we use the direct approach.\nBy evaluating these diverse approaches, we aim to provide a comprehensive analysis of current demonstration selection methods and their impact on LLM performance."}, {"title": "Experiments", "content": "In this section, we present a comprehensive evaluation of the six demonstration selection algorithms and a baseline random selection method.\nDatasets: The demonstration selection algorithms will be evaluated on 5 datasets, categorized into two groups: classification and multi-choice. The classification datasets include GLUE-MRPC, GLUE-QNLI, and GLUE-SST2. The multi-choice datasets are CMSQA and SWAG. Their statistic is shown in Table 1, and below are more details of the datasets:\n\u2022 GLUE-MRPC (Wang et al. 2018): MRPC is a dataset consisting of sentence pairs, each annotated with a binary label indicating whether the sentences in the pair are semantically equivalent.\n\u2022 GLUE-QNLI: QNLI dataset is derived from the Stanford Question Answering Dataset (SQUAD). It is a binary classification task where the goal is to determine whether the context sentence contains the answer to the question.\n\u2022 GLUE-SST2: SST-2 is a binary sentiment classification dataset that includes movie review excerpts, where the task is to predict whether the sentiment of the review is positive or negative.\n\u2022 CMSQA (Talmor et al. 2018): The CommonsenseQA dataset consists of multiple-choice questions that require commonsense knowledge. Each question is paired with five answer choices, where only one is correct.\n\u2022 SWAG (Zellers et al. 2018): SWAG dataset is a large-scale benchmark for grounded commonsense inference."}, {"title": "Related Work", "content": "In-context learning has emerged as a powerful capability of LLMs, enabling them to adapt to new tasks without fine-tuning, but the effectiveness of this approach heavily depends on the quality and relevance of the provided demonstrations. With numerous demonstration selection algorithms in the field, the optimal method for selecting these examples remains an open question. For instance, Wang et al. (2024b) emphasize aligning demonstrations with the model's latent structure, improving performance by uncovering latent variable explanations. Zhang, Feng, and Tan (2022); Qin et al. (2023) employ an active and iterative selection approach to identify the most informative examples. Retrieval-based methods are utilized by Wang, Yang, and Wei (2023); Wang et al. (2024a); Li et al. (2023); Xu et al. (2024) to select demonstrations. Liu, Zhu, and Dou (2024) adopt a ranking mechanism that considers relevance, similarity, and diversity for optimal demonstration selection. Ye et al. (2023) suggest constructing demonstrations through the combination of simpler examples to better capture complex task dynamics. Van, Wu et al. (2024) introduce InfICL, which uses influence functions to identify the most impactful examples for selection. Liu et al. (2024) uncover the importance of task-agnostic multi-level similarity and task-specific label similarities, proposing methods that integrate these factors for improved performance across diverse tasks. Additionally, Kim et al. (2022); Zhou et al. (2024) introduce the concept of self-generated demonstrations, which involves generating examples. Together, these works underscore the critical role of demonstration selection in enhancing the effectiveness of in-context learning across varied applications.\nOur work differs from previous studies by providing a comparative analysis of multiple demonstration selection algorithms across various tasks and datasets. While existing literature has primarily focused on developing individual algorithms or comparing a limited set of methods, we offer a holistic evaluation of both the effectiveness and efficiency of six demonstration selection algorithms."}, {"title": "Conclusions and Future Work", "content": "In this paper, our comprehensive evaluation of demonstration selection algorithms reveals significant variations in both effectiveness and efficiency across different tasks and datasets. While some algorithms show promising results, the trade-offs between accuracy and computational cost present challenges for real-world applications. Our findings highlight the need for task-specific optimization and suggest potential avenues for future research. These include developing adaptive algorithms that can dynamically adjust the number of demonstrations based on task complexity, integrating more advanced retrieval techniques, and exploring methods to balance effectiveness with computational efficiency."}]}