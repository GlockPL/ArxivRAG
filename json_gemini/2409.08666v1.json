{"title": "Towards certifiable AI in aviation: landscape, challenges, and opportunities", "authors": ["HYMALAI BELLO", "DANIEL GEISSLER", "LALA RAY", "STEFAN M\u00dcLLER-DIV\u00c9KY", "PETER M\u00dcLLER", "SHANNON KITTRELL", "MENGXI LIU", "BO ZHOU", "PAUL LUKOWICZ"], "abstract": "Artificial Intelligence (AI) methods are powerful tools for various domains, including critical fields such as avionics, where certification is required to achieve and maintain an acceptable level of safety. General solutions for safety-critical systems must address three main questions: Is it suitable? What drives the system's decisions? Is it robust to errors/attacks? This is more complex in AI than in traditional methods. In this context, this paper presents a comprehensive mind map of formal AI certification in avionics. It highlights the challenges of certifying AI development with an example to emphasize the need for qualification beyond performance metrics.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence (AI) is revolutionizing the avionics field (AI in aviation), offering many advan- tages and challenges. This fusion can increase efficiency, enhance safety, and improve passenger experience. Al in aviation currently focuses on AI-for-Cabin and non-critical tasks. On the other hand, AI-for-non-Cabin tasks encompass artificial intelligence techniques for the operation of the aircraft, for example, vehicle management or flight control/guidance/management system functions. AI-for-non-Cabin tasks are therefore subject to stringent certification requirements and a thorough and explainable understanding of the target tasks and AI methods to ensure the safety of passengers, flight crew, and aircraft. Moreover, the scope of AI-for-non-Cabin tasks ranges from communication, radar, digital electronics, integrated avionics systems, and navigation, to advanced traffic detection systems, all being considered critical tasks.\nTo develop any application in the safety-critical aviation sector, certain standards must be followed to meet the industry's safety and security restrictions. The authorities recognize several industry standards as acceptable means of compliance. For example, for system-related aspects, the Guidelines for Development of Civil Aircraft and Systems (ARP4754B) are available [129]. For software aspects, the Software Considerations in Airborne Systems and Equipment Certification (DO-178C) exists [133]. In the case of data certification, there are also the Standards for Processing Aeronautical Data (DO-200B) [134]. The main limitation of these guidelines is that they do not entirely cover the challenges of AI-enabled systems. This led to the European Union Aviation Safety Agency (EASA) to work on defining equivalent methods for the safe use of machine learning (ML) approaches. In 2024, the EASA published the Artificial Intelligence Concept Paper: Guidance for Level 1 & 2 machine learning applications [29] in response to the EU AI Act Chapter III [41]. It defines four AI certification building blocks, following the Ethics Guidelines for Trustworthy AI [6]:\n\u2022 AI Trustworthiness Analysis\n\u2022 AI Assurance\n\u2022 Human Factors for AI\n\u2022 AI safety risk mitigation\nFurthermore, the paper focuses on Level 1 AI (assistance to humans) and Level 2 AI (human-AI teaming), covering the scope of the Rule Making Task RMT.0742 to be executed at the end of 2027. The guideline for Level 3 AI (advanced automation) is estimated to be ready at the end of 2025. Additionally, EASA, in cooperation with industry partners, has published its final report of \"Machine Learning Application Approval\" (MLEAP)[30]. These documents present basic guidance standards for the aviation industry's certification of AI methods in Europe. This is accompanied by the recently released \"Roadmap for Artificial Intelligence Safety Assurance\" by the Federal Aviation Administration (FAA) of the United States[16], in compliance with Executive Order 14110: Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence[114]. The guideline uses common methods such as configuration management and validation. These classical methods are complemented by new techniques that address the specific characteristics of deep learning (DL) systems, including data collection through the training phase to DL deployment. This should be seen as a complement to established development methods and standards.\nAs stated previously, the EASA provides a basic certification guideline for Level 1 & Level 2 AI. It raises questions about how to translate aviation requirements to specific areas of AI research. Al research is a broad and multidisciplinary area, and the same question often has many answers. Moreover, the complexity of avionics combined with recent massive AI methods (at parameter and complexity levels) leads to a very beneficial and risky fusion. This makes concise and correct cooperation between industry and academia crucial. This is a bidirectional communication channel."}, {"title": "2 Background", "content": "In contrast to conventional control systems, neural networks (NN) elevate computer capabilities by facilitating learning from experience, also called data-driven methods. This transformative approach empowers computers to make decisions and predictions without explicit programming. These networks emulate the adaptability of the human brain, introducing a dynamic and intuitive dimension to computing where machines evolve and respond intelligently to diverse scenarios [131]. Deep Neural Network (DNN) is a type of NN with multiple layers between the input and output layers. These additional layers allow the network to learn complex patterns and non-linear relationships in the data to adapt to today's complex use-case scenarios and rising amounts of data. This makes them incredibly versatile and powerful for a wide range of tasks[81].\nFig. 1, depicts the general pipeline for developing DNN. It is an iterative process that finishes when the required performance is reached. The pipeline applies to classification/regression tasks for various learning paradigms. The generation or search for the dataset is the first step. The data source's main requirement is to represent the modality and purpose of the specific use case. Typically, the dataset will require preprocessing. For example, filtering noisy data, handling missing values, and normalization. This helps the model to converge faster and generalize better. Dataset preparation includes splitting it into three partitions: 1. train; 2. validation; 3. test datasets. The second step is the NN architecture design, for data scientists is a crucial phase, where the selection of an appropriate structure significantly influences the model's performance. The selection depends on the nature of the task, the data type involved, and the problem to solve. Usually, each architecture has specifications, which need to be considered to solve the envisioned task. For example, it is typical to select the number and type of layers, patterns, and activation functions to ensure that the neural network learns and generalizes effectively from the data provided. Next, is the training, where the model is trained by iteratively presenting batches of data through the network for a specified number of epochs. Each epoch represents a complete pass through the entire training dataset. During each epoch, the DNN optimizer adjusts the model's weights to minimize the chosen loss function through backpropagation. The optimizer calculates the gradients of the loss concerning the model's parameters, indicating how much each parameter contributed to the error. The optimizer then updates the model's weights in the opposite direction of the gradients, gradually improving the model's ability to make accurate predictions throughout training. After each epoch, the validation set is passed through the network to monitor the loss for passing \u201cunseen\u201d data through the model to prevent overfitting (the model does not generalize but memorizes the data). This information can further be used to adjust the model's hyperparameters and detect convergence. After the training is completed, the final model is evaluated on the test dataset to ensure the required model's performance on previously \u201cunseen\u201d data is fulfilled. This signals the breaking point of the loop and the proposed solution is completed. The decision is based on performance metrics such as accuracy, recall, precision, and F1 score[74]. This excludes trustworthy analysis, AI assurance, human factors, and safety and risk management of the solution.\nOptimization of the model is an optional step but imperative for the efficient deployment of the solution on hardware-constrained devices[107]. It can be integrated into the training process or applied after training as a fine-tuning step. It includes methods such as pruning and quantization. Pruning removes superfluous or unnecessary connections within the neural network with reduced impact on the performance. Identifying and eliminating less meaningful connections makes the model lightweight while maintaining the best predictive capabilities possible. On the other hand, DNNs are trained in floating-point 32-bit arithmetic to take advantage of the wider dynamic range. Quantization is a technique that reduces the bit precision of the model's parameters. The model's memory footprint is reduced by representing weights and activations with fewer bits, leading to faster inference times and reduced resource requirements during deployment. There is also the option of using quantized parameters to train the model, which is called quantization-aware training. The idea is to model the effect of quantization, which allows for increased accuracy at the time of inference compared to post-quantization methods[86]. The selection of the optimization strategy is part of the qualification process of the DNN model. This impacts the operational performance of the final model. It is important to note that at this stage the NN is frozen, and any changes will"}, {"title": "2.2 Avionics", "content": "Avionics are the electronic systems used on an aircraft. It is derived from \"aviation electronics\", which includes communication, navigation, flight control, monitoring, display, and aircraft man- agement systems. These systems continually evolve to improve efficiency, cost, safety, and risk management[110]. The aviation field is currently undergoing an AI revolution [60]. The AI can assist in predictive maintenance, for example, automatic visual inspection (AVI)[165]. This helps operators with faster damage detection, holistically reducing the time expended in maintenance by detecting damages in the early stage. Additionally, Air Traffic Control Speech-to-Text Technology (ATC-STT) aims to translate spoken instructions into text, thereby increasing safety[13, 88, 89]. Moreover, the Airborne Collision Avoidance System for Unmanned Aircraft (ACAS) can benefit from faster object detection and warning response times to avoid intruder aircraft in time[26, 118]. The above are examples of aviation use cases that can have a highly beneficial/risky impact when using an Al model.\nThe avionic use cases involve complex systems with high dimensionality. To overcome the complexity, the first step in the task-solving process is to define the operational domain (OD). OD captures the operation conditions under which a solution/product is specifically designed to function as intended. The OD is defined as a set of constraints and requirements for a specific"}, {"title": "2.3 Al certification in Avionics", "content": "Certification of any system intended to be used in avionics is required to achieve and maintain an acceptable level of safety. One of the prominent means of compliance includes the Software Consideration in Airborne Systems and Equipment Certification (DO-178C). This is the primary document used by the most famous certification authorities such as EASA for Europe, the Federal Aviation Administration (FAA) for the United States, and Transports Canada Civil Aviation (TCCA) to demonstrate design assurance for software items in avionics systems [128]. For hardware certification, the Design Assurance Guidance for Airborne Electronic Hardware (DO-254) exists[45, 63], in addition to the Environmental and Test Procedures for Airborne Equipment (DO-160)[141], among others\u00b9.\nHigh certification standards are also to be expected when Al meets avionics. As shown in Fig. 1, the DNN pipeline suffers from a lack of qualification. In this context, the EASA Concept Paper [29] intends to guide Level 1 & Level 2 AI development in aviation. Level 1 relates to human assistance. The requirements for this level include learning assurance, AI explainability, and continuous safety and security risk assessment. Level 2 requires additional measures such as an ethics-based assessment and human-AI teaming. Furthermore, the EASA defines Level 3 AI as advanced automation and beyond. This upper level is the scope of the EASA's future work and the guidance for Level 3 is expected in 2025. It considers the extension to reinforcement and symbolic learning, statistical and hybrid AI combined with human-AI supervision, and unsupervised automation safety risk mitigation. It should be noted that the guidance of EASA for Level 1 & Level 2 is still under discussion and is expected to be finalized by 2026. And, the first expected AI approval for Level 2/3A will be in 2035, so AI certification is still in its infancy. For Level 1 & Level 2, the Fig. 2 depicts the iterative certification flow of AI for aviation purposes. It presents four main blocks: Trustworthiness Analysis (TA), AI Assurance (AIA), Human Factor for AI (HFAI), and AI Safety Risk Mitigation (AIS), and based on these four blocks this work is divided. The following sections summarize each of the blocks individually to describe their main purposes, along with a review of the SOTA approaches related to each block is presented."}, {"title": "3 Trustworthiness Analysis (TA)", "content": "Trustworthiness Analysis (TA) for AI is independent of the type of learning algorithm; supervised, unsupervised, or reinforcement learning (RL). For this analysis, the system is considered as a whole,"}, {"title": "4 Al Assurance (AIA)", "content": "AI assurance (AIA) defines the objectives of the AI subsystem, employing a system and user-centric approach. Two main blocks are identified, Learning Assurance and AI explainability. Fig. 4 shows the W-shape model for AI assurance of EASA together with an overview of takeaways from each step. Below the dotted line are the steps that need to be adapted for AI systems and above it is the traditional assurance cycle. As shown in the Fig. 4 there is a clear separation between offline (in blue color) and online (in green color) AI assurance process. Each step in the cycle is co-dependent and the next step verifies that the previous certification steps are still valid.\nAnd lastly, it is the Data and Learning Verification of Verification. Here, comes the answer to the question \"Did we build the item right?\". This is achieved by confirming compliance with ODD requirements and verifying the completeness and representativeness of the data. In the case of updating the system, such as reusing the model from another domain (transfer learning), a new certification procedure and a configuration management system are required to record different versions and logs (errors/failures). This completes the Learning Assurance block of AIA.\nThe second block is the development and post-ops AI explainability. This is related to trans- parency, traceability, safety, security, and accountability of the AI constituent. It should be noted that the Al system must be interpretable by a wide range of users and personnel from official institutions, such as engineers, certification authorities, and flight crews. The wide range of users/stakeholders implies different levels of detail of explainability for each target audience (e.g., specialized EASA personnel or pilots as end users). A relevant requirement for the stakeholder is to be able to build trust in the system. This requires quantifying the confidence level (uncertainty) in the AI system's output. Uncertainty level and performance need to be continuously monitored during the system's lifetime.\nOverall, AI assurance emphasizes the following objectives: completeness, representativeness, generalization bounds, stability and robustness of the model, explainability, and continuous moni- toring of performance and confidence levels. Therefore, research methods with these objectives are presented in the following subsections."}, {"title": "4.1 Methods for data completeness and representatives", "content": "A trade-off between completeness and representativeness is needed to assure generalization bounds, thus the state-of-the-art approaches of these objectives are merged in Table 2. This is a requirement from the data management step. Real data is high dimensional and faces challenges such as missing values, outliers, noise, and labeling errors. The question is how to quantify the trade-off between completeness and representatives in real conditions. One way is to reduce the dimensions to visualize the data and discover hidden patterns in the distribution. A complete and representative dataset has a homogeneous scatter plot. There are multiple methods for this and they depend on the data types (text, signal, pixels) and whether linear and nonlinear reduction techniques are necessary [10, 124].\nPrincipal component analysis (PCA) is one of the most used techniques in the literature to find uncorrelated features. It is a simple visualization technique that removes multicollinearity and reduces parameters and training time. The vanilla version of PCA applies to linear datasets and is sensitive to outliers [100]. At the same time, it is computationally expensive, the new dimensions are not interpretable and there is information loss.\nOn the other hand, it is common to assume that the dataset contains independent and identically distributed data points. However, this assumption is often violated. Sensors monitoring a common phenomenon are interrelated with each other. Moreover, a phenomenon in nature involves many interactions between subsystems, e.g., in chemistry molecules will interact with each other in biochemical events. The graph-based analysis captures these dependencies. It can be used to check the desired coverage of ODD data while filtering out redundant data and enforcing evenly distributed data points. In machine learning, exists an entire area dedicated to graph neural networks[167, 177]. Table 2 mentioned some of the newest techniques in this area.\nA relevant technique is entropy analysis. This can identify patterns within data by measuring the level of randomness in the dataset. It can detect anomalies and group similar data points together, and can then be used to enforce independence between data points. The idea will be to homogeneously increase the entropy of the data (e.g., label-wise). For example, with augmentation techniques, special care must be taken to avoid the addition of outliers, which would result in heterogeneous addition. Hence, Table 2 includes information-aware augmentation techniques. These techniques depend highly on the dataset type and require domain expertise[101].\nAnother method for high-dimensional data is quantifying the data points' similarity. The idea is to reduce and extract meaningful information from the input using latent space (embedding). A metric is then used to measure the similarity in the embedding space. This method depends on the technique applied to create the latent space and the similarity metric selected[106, 115]. In [48] an interactive latent space an inspector tool is introduced. This tool allows AI developers to inspect neural network models' output behavior. The user can manipulate values in any latent layer and analyze the response. This is a particularly relevant technique to test the robustness of the model against adversarial attacks. Exploring the feature space while entering out-of-distribution data can provide information about system behavior at the boundaries, also aiding in fault identification. Moreover, neural coverage is an attempt to find an intuitive test criterion for a neural network[57]. It is based on measuring the proportion of activated neurons (nodes) activated in a forward pass."}, {"title": "4.2 Methods for Al generalization, explainability, and uncertainty assurance", "content": "The next objective is the generalization of the AI method. Generalization refers to the ability of a model to maintain average performance on unseen data consistently. A model with generalization properties can handle real-world data variability and will adjust to different operation conditions. To evaluate this, the model's training can be explored using learning curves and convergence stability, and then, at test time, the empirical measure of the gap between the training and test data sets can be obtained. Some methods exist to increase generalizability. These include regularization [77, 132], stability[176], deep metric learning [75], model architecture, and hyperparameter tuning. They aim to learn richer network representations to boost performance on unseen data. Table 3 shows an overview of recent methods toward the generalizability of AI models. Still, effective methods need to be developed to quantify levels of generalization assurance throughout the learning assurance cycle.\nFurthermore, AI requires explainability and uncertainty assurance in critical domains such as avionics. There are two main types of uncertainty: random and epistemic. Random uncertainty is"}, {"title": "5 Human Factors for Al (HFAI)", "content": "Human-centered AI focuses on cooperation and collaboration that builds teams of human AI-based systems. This team encompasses a wide range of end users with diverse skill sets that ally with AI to achieve a goal. In the case of cooperation, the AI-based system works as a tool that helps the user(s) fulfill the user's goal. In collaboration, the AI and the user(s) work together and jointly to accomplish a shared goal. Collaboration implies real-time communication and situational awareness between the AI and the human. For the certification cycle to take into account the human factors of AI, five main requirements must be meet: 1. AI operational explainability; 2. Human-AI teaming (collaboration); 3. Modality of interaction and style of interface; 4. Error management; 5. Failure management. Fig. 5, highlights the description of the five main requirements.\nThe AI system must be equipped with an unambiguous and time-aware explanation of its output to the user, simultaneously with requests for cross-validation by the end user. This operational explainability depends on the user's level of expertise and the task to achieve. The aim is to progressively build trust in the AI system together with confidence level monitoring. Moreover, a balanced level between the information given to the user and the user's cognitive load is necessary. In human-Al teaming, the interaction style of the interface varies in terms of modality. The modalities include natural language, procedural language, gesture language, and multimodality. The selection of one or multiple of those modalities needs to consider the context of the situation to guarantee the performance level under a hostile environment, for example, in the case of a noisy environment and involuntary gestures. The AI system has to be able to automatically select the modality(ies) based on the user's state (workload, stress, cognitive resources), situation, and perceived context and adapt to the user's preference. In addition, human-AI factors can lead to errors that, undetected, become defects that, in turn, can become failures. Consequently is highly relevant to detect, minimize, and provide solution support for errors and failures. Human-AI teaming is a system with a huge variety of resources, thus it is a must to employ crew resources management (CRM). CRM is the effective use of resources, including people, for a safe and efficient operation. This is defined in the SKYbrary website\u00b2. SKYbrary contains articles related to aviation safety and certification on the topics of operational issues, human performance, enhancing safety, and safety regulations, among others. SKYbrary focuses on the usual avionics system, the challenge is how to establish the connection between AI-based avionics and current regulations.\nThe human factor in AI has been the subject of numerous studies on how it should be applied, but it remains in question[43, 58, 173]. This is because making the same decision without AI is different from making it with AI, and there are still many open issues on how AI works. HACO [38] introduces a framework for developing Human-AI teaming using a graphical user interface. The authors in [61] present, a commercial version of an AI platform that provides a solution for human-AI collaboration in manufacturing, called \u201cTeaming.AI\". They employed knowledge graphs to integrate semantic information of diverse processes executing during runtime. In general, human-centric AI will relieve the human decision-maker of pressure. However, this could lead to over-reliance on AI predictions, worsening the performance compared to working unassisted. In [19], the authors aim to improve the decision-making of humans working with AI with the use of \"behavior descriptions\". These descriptions come from the AI developer's mental model, which are details of how the AI performs on subsets of instances [9]. A trending question is how to use large language models (LLM) to support the Human-AI teaming [146]. In [111] the authors use a large language model (LLM) to describe the data regions. These LLM descriptions are then used to teach the human user through an onboarding stage to improve the human-AI association. Moreover, the authors in [99] noticed that humans rarely trigger analytical thinking when a disagreement with AI occurs, thus they proposed \"Human-AI deliberation\" to promote human reflection and discussion related to the AI decision-making process. This is aligned with the definition of contestability. This means there must be a timely process to allow individuals to challenge the use or outcomes of the Al system. Contestable AI is necessary when the AI system significantly affects a person, community, group, or environment [84]. Hence, identifying and addressing users' transparency needs becomes a challenge and a critical element of the Human-AI teaming [144]. The goal of"}, {"title": "6 Al Safety Risk Mitigation (AIS)", "content": "This section addresses the reality of the impracticality that could arise at the moment of certification. AI safety risk mitigation is required to counter the fact that exhaustive testing is impossible for complex systems and residual risks remain. Partially complying with the certification requirements means the entire system has inherent AI risks. This is to be expected in black box models such as Al systems. Safety risk mitigation is not aimed at compensating partial coverage of objectives belonging to the trustworthiness analysis (TA) certification block i.e., the TA block is critical. The purpose is to minimize unexpected/inexplicable behavior of the AI/ML constituent. Hence, real-time monitoring and safety net backups (traditional backups) are means to achieve this. Still, it is difficult to determine the safety precautions of AI systems due to the newness of AI in the aviation domain and the lack of field experience [30]."}, {"title": "7 Certification Challenges of Classical Al Research", "content": "This section presents an example use case to show the certification needs in the classic AI devel- opment cycle. One relevant use case in avionics is collision avoidance systems. This includes two steps: 1. detect the object; 2. perform an avoidance maneuver and/or suggest a maneuver to the pilot to satisfy the remain-well clear requirement. Detecting an object is a task that a human does every day without thinking, making it a comprehensible objective. Cameras are the most widely used and accurate modality in the literature for solving object detection tasks.\nYou Only Look Once (YOLO) [125] is a vision-based model widely used in object detection, which has multiple versions and has been adapted to embedded implementations [42, 65, 67]. YOLO was first published by Joseph Redmon et.al. in 2016, at the time of writing this article, YOLO has already reached version 10. Its objective is to predict bounding boxes around objects and class probabilities of the identified object at the same time. YOLOv9 [152] and YOLOv10 [149] are currently under review, with a reduced number of related publications compared to its predecessor YOLOv8 [73]. Thus the YOLOv8 is the selected algorithm to analyse. It focuses on a series of improvements and"}, {"title": "8 Discussion and insights", "content": "Avionic is the leading safety-critical domain in AI[29]. In addition, aviation is one of the most regulated areas for development, with multiple public agencies and users involved in the process. Despite the above, the status of AI in avionics is in its infancy. The structure of the minimum requirements for certification is currently being outlined. The complexity of critical sectors and the lack of AI certification make AI-avionics teamwork extremely delicate. Therefore, collaboration between industry, government, and researchers is crucial to identify effective and feasible means of meeting the defined certification objectives. This section presents a summary of the limitations of the certification of AI in aviation. Due to the sheer size and complexity of avionics and AI systems, this list of insights is far from complete, but it offers a glimpse of what to expect on the road toward certifiable AI.\n\u2022 Generalization of methods: The certification process and sub-processes are not generaliz- able. It is a high-dimensional problem that needs tailored assessment methods for application and domain, demanding intensive time-consuming efforts. This disrupts the classic cycle of research advances, in which the most cited projects are general-purpose models. The general purpose modeling style requires a huge effort for certification due to the common practice of bypassing certification in the development cycle and mistakenly assuming that the design only has to meet the output performance metrics. This leaves elements such as ethics, and safety and risk assessment unattended. Certification should be considered from the beginning of AI development. Novel algorithms are constantly being released without being accountable to any of the certification blocks.\n\u2022 Operational design domain description (ODD): The lack of OD and ODD description in the DNN development process greatly affects the completeness and representativeness of the dataset selection. Furthermore, the type of data also influences the model structure and parameter settings. Consequently, the whole process risks becoming worthless or meaningless, because in the end it does not solve a practical application in a certifiable way. Moreover, without a correct OD and ODD, it is impossible to identify singular point, edge, and corner cases to test the robustness and stability of the system.\n\u2022 New learning paradigms: A variety of deep learning methods are proposed at an incred- ibly fast pace. Particularly, there is growing interest in new ways to improve the learning capabilities of the model. Due to the large DNN community, it is challenging to list all new methods. Therefore, to reduce complexity the focus will be on four areas of interest: 1. guid- ance/teaching models; 2. contrastive models; 3. expert knowledge models; 4. autonomous learning.\nAmong teacher models, transfer learning (TL), and knowledge distillation (KD) exist. DNN models require a large amount of data to converge, hence multiple methods are proposed to mitigate the requirement of large datasets for each specific task. The TL process requires two steps: the first step consists of selecting or training a network in a domain where a large dataset is available. The second step consists of fine-tuning the last layers (re-training) of a pre- trained neural network (old domain) using data from the new domain/task[179]. This method requires a certification procedure for the new domain/task despite being certified in the old/mother domain/task. KD offers the perks of transferring knowledge from a cumbersome model (teacher) to a smaller and more manageable neural network model (student). In this way, the student can learn faster with the teacher's regularization, and the computational complexity and size are reduced, which at the same time can increase the interpretability of the solution. This property is important at the time of model deployment on constrained hardware devices[55].\nContrastive learning(CL) is a deep learning methodology where the network learns by com- parison among different input samples. The comparison can be between similar/dissimilar pairs of data points. With this method, the NN learns to push together similar samples and pull away the dissimilar points. For an efficient learning process the selection of the positive/negative samples is crucial. This depends on designing the similarity distribution so that positive pairs are different in the input space but are still semantically related, and on a dissimilarity distribution that ensures that negative pairs are similar in the input space but are semantically unrelated[82]. In [158, 159] the authors use CL for out-of-distribution data detection, and in [59] uncertainty estimation is assisted by contrastive learning. Therefore, CL can be used in the analysis of the completeness and representativeness of the dataset.\nDespite their advantages, the above methods are of great complexity and are mostly conceived without expert knowledge to add explanatory power. On the other hand, researchers are joining efforts to build models with some explanatory meaning based on expert knowledge from other disciplines. Spiking neural networks (SNNs) are an example of extending the power of NNs by replicating brain behavior as an organic network. This coincides with the main goal of NNs, which are supposed to mimic neural connections in the brain, including interaction and reaction between them. SNNs exist since spikes of biological neurons are sparse in time and space, and event-driven, which is closer to how the human brain computes at the neural description level. SNNs employ bio-plausible local learning rules, making them suitable to build low-power neuromorphic hardware for SNNs[142]. Biologically plausible local learning rules can increase the robustness of NN to noise without sacrificing the performance of the task, as synaptic balancing[140]. Evolutionary algorithms (EA) are also an example of methods based on the principle of biological evolution. EAs can be used as a computational optimization to improve the population of potential solutions iteratively, making them suitable for improving hyperparameters with an objective function [121, 139]. Physic-informed neural networks (PINNs) encode physics laws in the form of partial differential equations, which are then used as an additional loss term in the loss function when training the neural network. The learning capability of deep neural networks depends on the size of the dataset. PINNS help to converge the model with a small number of samples without violating known physical laws (added as terms in the loss function)[35, 85]. Expert knowledge can be represented as rule-based systems, which is the case of symbolic artificial intelligence (SAI). It offers a set of methods based on high-level symbolic representations of problems, logic, and search. SAI copes with the unsustainable computational resources of DNN development while adding properties of robustness and explainability to the AI cycle. The combination of NN and symbolic approaches can impact human-AI collaboration with reasoning and cognitive capabilities within AI development[21, 34, 148, 161].\nThe fourth area is autonomous learning. These are methods that enable AI to learn tasks autonomously. Reinforcement learning (RL) is a powerful method to fully automate AI models. An interesting sub-field of RL is explainable reinforcement learning (XRL). This area aims to understand the decision-making process of RL agents, adding interpretability to these methods helps the use of them in critical domains. In [53, 108] the authors present a survey of the techniques, challenges, and opportunities of XRL. In [118] a team of researchers present an RL application for an autonomous Airborne Collision Avoidance System. They use expert knowledge for their model by defining airspace characteristics and aircraft models. They employ a summary of basic concepts of relative geometry and kinematics, adding reliability to the system. In addition, continuous reinforcement learning offers the idea of never stopping learning new tasks, in contrast to typical RL, which consists of finding/improving solutions on predefined tasks[2]. In general, despite the advantages of autonomous AI, it also involves additional unknown certification steps. This area is within the next round of discussion by aviation regulators.\n\u2022 Explainability: Deep neural networks are astonishingly increasing in size and complexity while understanding why the new method performs best remains a mystery. This is connected to the need for contestable AI systems. Contestable AI becomes more important when an AI system significantly affects an individual, community, group, or environment. In this context, a timely process must allow individuals to challenge the use or results of the AI system. This requires a dynamic relationship between human and AI methods to explain/revise their decision-making process[84] progressively.\n\u2022 AI system definition: The definition of the AI system and subsystems varies according to the specific avionics domain. It could include the AI-human interaction, requiring human-AI teaming accountability. Moreover, AI development needs to quantify the emotional intelli- gence requirement to understand and manage the human-AI interaction.\n\u2022 Automated Machine Learning (AutoML): AutoML is used to generate and optimize AI models. It includes parameter selection/optimization, and an automatic neural architecture search (NAS). A successful AutoML tool should reinforce the researcher's trust, making clear the need for transparency in the development process[37, 181]. This leads to inquiries such as: Can AutoML be relied upon to speed up the certification process of requirements definition and compliance? Is it possible to include the description of the users and the operational domain in the cycle? Is it possible to automatically select the AI classification? Can Fairness be automated with the use of AutoML?[8, 156]\n\u2022 Environmental and well-being: The research community focuses primarily on perfor- mance. Currently, performance improvement translates into the use of massive models, which also require enormous use of resources. This urgently claims for techniques that advance in Al in an environmentally responsible manner[145, 160]. Network training is oblivious to the resources and energy consumption requirements. Training includes designing huge models by trial and error and tuning hyperparameters, which consumes a large amount of energy[47, 62].\n\u2022 Failure/error detection and management: To ensure safe operations, the DNN and the system must undergo rigorous verification and validation, including advanced statistical analysis. The performance and safety of the DNN and the system's behavior must be analyzed for the nominal case and in numerous outlier and failure cases. This is part of the safety assurance of the AI system. The definition of safety by researchers mainly refers to the use of the DNN model for safety tasks, without assessing the compliance of the DNN method with safety standards.\n\u2022 Unbalanced attention on certification blocks: The AI assurance block receives the most attention from the research community. This is mainly due to the close relationship between the AI assurance block and performance improvement. The performance improvement of a model compared to related work is currently the main metric to be accepted by the research community. Meanwhile, ethical and human factors, such as emotional intelligence and training requirements, and managing the security risks of AI solutions are underrepresented and urgently need attention by the community."}, {"title": "9 Conclusion", "content": ""}]}