{"title": "The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK Employment Tribunal", "authors": ["Huiyuan Xie", "Felix Steffek", "Joana Ribeiro de Faria", "Christine Carter", "Jonathan Rutherford"], "abstract": "This paper explores the intersection of tech- nological innovation and access to justice by developing a benchmark for predicting case outcomes in the UK Employment Tribunal (UKET). To address the challenge of extensive manual annotation, the study employs a large language model (LLM) for automatic annota- tion, resulting in the creation of the CLC-UKET dataset. The dataset consists of approximately 19,000 UKET cases and their metadata. Com- prehensive legal annotations cover facts, claims, precedent references, statutory references, case outcomes, reasons and jurisdiction codes. Fa- cilitated by the CLC-UKET data, we examine a multi-class case outcome prediction task in the UKET. Human predictions are collected to establish a performance reference for model comparison. Empirical results from baseline models indicate that finetuned transformer mod- els outperform zero-shot and few-shot LLMs on the UKET prediction task. The performance of zero-shot LLMs can be enhanced by integrat- ing task-related information into few-shot ex- amples. We hope that the CLC-UKET dataset, along with human annotations and empirical findings, can serve as a valuable benchmark for employment-related dispute resolution.", "sections": [{"title": "1 Introduction", "content": "In recent years, there has been great interest in adapting natural language processing techniques in the legal domain. One notable application is the prediction of outcomes for legal disputes in various jurisdictions. However, the AI-based prediction of UK court decisions is still under-explored.\nThis paper investigates the prediction of dispute outcomes in the UK Employment Tribunal (UKET). The UKET serves a crucial function in the UK jus- tice system, specifically dealing with employment- related disputes. Cases heard at the UKET cover a wide range of issues, such as unfair dismissal, discrimination and breach of contract. The possi- bility to apply to the UKET for a decision ensures that employment rights can be enforced. Knowing the likely outcome of a court procedure improves access to justice and facilitates amicable dispute resolution.\nThe contributions of this paper are as follows:\n1. We constructed a large-scale CLC-UKET dataset based on the Cambridge Law Corpus (CLC). CLC-UKET in- cludes two components: CLC-UKETanno and CLC-UKETpred\u00b7 CLC-UKETanno consists of a selection of 19,090 UKET case judg- ments heard between 2011 and 2023 (inclu- sive). All cases come with metadata includ- ing a unique case identifier, the hearing date and jurisdiction codes. We further provided detailed legal annotations for all cases, includ- ing (a) facts, (b) claims, (c) references to le- gal statutes, acts, regulations, provisions and rules, (d) references to precedents and other court decisions, (e) general case outcome and (f) detailed order and remedies. We further cu- rated CLC-UKETpred, specifically designed to facilitate a multi-class case outcome predic- tion task. CLC-UKETpred consists of 14,582 cases, each supplemented with statements de- tailing the facts, claims and the general out- comes of the cases.\n2. We presented human performance on the UKET outcome prediction task on CLC- UKETpred with the aim of setting a human performance reference to calibrate prediction models.\n3. We experimented with a range of baseline models to predict the general case outcomes based on information about facts and claims of UKET cases."}, {"title": "2 UK Legal System and UKET", "content": "The UK has a special category of judicial body, the employment tribunals, which deal exclusively with employment disputes. The UKET is one of the three largest tribunals in the greater tribunals system (Judicial Office, 2016). The UKET aims to provide a procedure which is easily accessible, informal, speedy and inexpensive (BEIS, 2020, p. 23). Claimants must comply with procedural and substantive requirements to be successful. For in- stance, claimants must submit their claims on time, comply with the orders of the tribunal, present re- quired evidence or information in a timely man- ner, and avoid scandalous, unreasonable or vexa- tious conduct (which makes a fair trial impossi- ble). These are usually considered as procedural requirements. Claimants must also comply with the substantial requirements of the rules supporting the claims. For example, in order to be success- ful with a discrimination claim on grounds of dis- ability, the claimant must prove their status as an employee, demonstrate their disability and show that they faced discrimination, which are consid- ered as substantial requirements of the case. These procedural and substantial requirements are not necessarily determined at one final hearing or in- cluded in one final judgment. Instead, they may be iteratively decided at different stages, which can result in multiple decisions.\nThe employee (claimant) and the employer (re- spondent) submit their claims and responses, re- spectively, through a standardised form (Rules 8 and 16 of the Employment Tribunals Rules of Pro- cedure 2013, hereinafter referred to as Rules). The tribunal considers these forms and may dismiss a claim for procedural or substantial reasons, e.g., for lack of jurisdiction or for lack of any reasonable prospect of success (Rules, r. 27). At any stage of the proceedings, the tribunal can determine a preliminary issue, make a procedural order (e.g., a deposit order or require the presentation of ad- ditional documents) or make a final decision (e.g., strike out the claim, Rules, r. 37). There may be multiple final hearings for different issues, for ex- ample, one hearing to determine whether or not a party is liable, another hearing to determine the remedy and another to determine who pays what costs (Rules, r. 57). Each of these hearings results in a separate judgment, written out in a separate document. Finally, a party may request a reconsid- eration of a previous judgment, which will lead to another judgment (Rules, r. 70). As a consequence, the resolution of a dispute may not be covered by one judgment only, but may be determined by it- erative multiple decisions resulting in various case documents.\nEach decision is linked to one or multiple juris- diction codes. In the case of UKET, there are 54 jurisdiction codes in total, which are used to iden- tify the matter of disputes. By way of example, the jurisdiction code \"unfair dismissal\u201d is used when claimants argue that they have been unfairly dis- missed. This jurisdiction code is often employed in addition to other jurisdiction codes, such as unlaw- ful deduction from wages, redundancy, protective award, breach of contract, and working time regu- lations.\nIn stark contrast with typical UK judgments, UKET decisions are relatively clearly structured, not only because there are no dissenting opinions, but also because there are specific rulings that set out which elements a judgment must contain (Rules, r. 62(5)). Nevertheless, UKET judgments are not always consistent since there are no formal rules on the style to be used in drafting a decision. Most English judgments summarise their decisions in a paragraph, although this summary does not need to respect any particular form (d'\u00c9tat, 2012, p. 136). In the case of UKET, the summary is often found at the beginning of the judgment. How- ever, judgments on multiple claims are sometimes divided into chapters, each analysing one claim containing the relevant decision. Also, while a judgment may contain an initial statement that the claimant is successful, it may not be clear which claim(s) this relates to in cases where there are multiple claims."}, {"title": "3 Related Work", "content": "Quantitative methods for analysing legal judgments have long been explored. In relation to employ- ment law, Grunbaum and Newhouse (1965) anal- ysed 20 US Supreme Court judgments to identify the variables which impacted outcomes. Similarly,"}, {"title": "3.1 Analysis of Employment Judgments", "content": "Field and Holley (1982) identified factors which influenced outcomes of performance appraisal judg- ments. Brudney et al. (1999) analysed the extent to which extradoctrinal factors such as political party, gender and professional experience influenced out- comes.\nMoreover, several studies explored correlations between specific demographic groups and the abil- ity to pursue their employment rights in tribunal. In the US, Schuster and Miller (1984) analysed 153 federal court cases, focusing on age discrimination, whilst Schultz and Petterson (1992) investigated race and sex discrimination. In the UK, Barnard and Ludlow (2016) investigated whether EU-8 mi- grant workers were able to enforce their rights by bringing claims before the UKET.\nMany of these studies occurred before judgments were published online, and therefore not only en- tailed costly journeys to the registers, but also re- quired manual extraction and tagging of specific elements of court decisions. More recently, Black- ham (2021) conducted quantitative analyses of em- ployment decisions, but despite having access to online judgments, some of their tasks still required manual labour."}, {"title": "3.2 Legal Judgment Prediction", "content": "The advance of deep learning models alongside the development of large-scale legal datasets has greatly advanced the research on legal judgment prediction (LJP). A large number of datasets have been created for both civil law sys- tems and common law systems . Facilitated by large-scale datasets, recent years have witnessed a surge in the application of deep learning models to LJP. Zhong et al. (2018) intro- duced TopJudge to address LJP using multi-task learning that combines three aspects: law articles, charges and terms of penalty. Another notable con- tribution is the work of Ma et al. (2021) where an end-to-end framework was built to predict dispute outcomes using multi-task supervision and multi- stage representation learning. To the best of our knowledge, the only notable LJP paper on UK law is Strickson and De La Iglesia (2020), which dates before the emergence of LLMs and is limited to the binary task of UK Supreme Court judges allowing or rejecting an appeal."}, {"title": "4 The CLC-UKET Dataset", "content": "We curated a large-scale dataset focusing on UK employment-related dispute resolution. The result- ing CLC-UKET dataset consists of two compo- nents: CLC-UKETanno consisting of 19,090 cases with detailed legal annotations and CLC-UKETpred with 14,582 cases curated for case outcome pre- diction for the UKET. The CLC-UKET dataset is constructed based on the UKET subset of the CLC by adding annotations for selected UKET cases. A common practice for collecting legal annotations is to ask legal experts to manually annotate texts. However, this can be costly and time-consuming. To alleviate the burden of manual annotation, we explored utilising large language models (LLMs) for automatic annotation.\nThe dataset curation pipeline of CLC-UKETanno consists of two steps: a case preparation module and an LLM-aided case annotation module."}, {"title": "4.1 Case Preparation", "content": "The raw UKET subset of the CLC contains 52,339 cases in total, covering employment-related cases heard at the UKET from January 2011 to August 2023 (inclusive). After analysing these cases, we noticed that many cases only consist of one page as regards the tribunal decision. These cases are mostly the result of straightforward procedural de- cisions, for example when claimants withdraw their cases or respondents do not respond at all such that a default judgment is made. As these cases do not contain substantial information on the facts and the substantive reasons, we excluded them at the case preparation step.\nAfter this filtering step, we obtained a collec- tion of 19,090 cases containing more than one page in their court decision files. For each case, we collected a list of metadata, including a unique case identifier used in the UKET records, date of filing, date of decision, place of hearing, judges, claimant(s), respondent(s) and appearances at the hearing. We also obtained jurisdiction codes for all cases from the UKET website. In legal contexts, a jurisdiction code typically refers to a numerical or alphanumeric code assigned to a specific legal ju- risdiction, a certain subject matter or a geographic"}, {"title": "4.2 LLM-aided Case Annotation", "content": "The CLC provides raw texts of the decisions of UKET cases. These documents usually contain en- tangled statements about facts provided by parties and their lawyers, reasoning towards a decision, legal statutes and precedents applied to justify the reasoning and final decisions regarding the case outcome. In this step, we followed similar lines to de Faria et al. (2024) and utilised the GPT-4- turbo model (Achiam et al., 2023) to automatically extract legal information from UKET decisions.\nWe applied an iterative development process to find the optimal prompt for the purpose of legal information extraction. The final prompt that we opted for yielded the best results in terms of the accuracy of information extracted, the adequacy of necessary information contained and the level of detail. The final prompt that we used for LLM- aided case annotation is presented in Appendix A.2. After automatic annotation, we obtained de- tailed annotations on important legal factors for 19,090 CLC-UKET cases, covering (1) facts, (2) claims, (3) references to legal statutes, (4) refer- ences to precedents, (5) general case outcomes, (6) general case outcomes labelled as \"claimant wins\", \"claimant loses\u201d, \u201cclaimant partly wins\u201d and \"other\", (7) detailed orders and remedies and (8) reasons."}, {"title": "5 Case Outcome Prediction", "content": "The annotated CLC-UKET data (i.e., CLC- UKETanno) provides a large collection of court decisions augmented with rich legal annotations, which can readily be used for downstream legal AI tasks. In this paper, we showcase a use case of the CLC-UKET data by investigating a classic task in legal AI, i.e., case outcome prediction."}, {"title": "5.1 Task Definition", "content": "Given a set of facts and claims of a UKET case, the task of case outcome prediction aims to automati- cally generate an outcome label falling into one of the four categories of \u201cclaimant wins\u201d, \u201cclaimant loses\", \"claimant partly wins\u201d and \u201cother\u201d. The facts and the claims are the judges' summarisation of the statements provided by the claimant(s) and respondent(s) prior to or during a hearing.\nMore formally, given a set of facts $F = {f_1, f_2,\\dots, f_m}$ and a set of claims $C = {c_1, c_2,..., c_n}$ for a UKET case, a prediction model CLS outputs a label $g$ for the general case outcome:\n$g = CLS(F, C)$\nwhere $g\\in$ [\"claimant wins\u201d, \u201cclaimant loses\u201d, \"claimant partly wins\u201d and \u201cother\u201d].\nNote that there is a debate concerning the differ- ence between the legal judgment prediction (LJP) task and the case outcome classification (COC) task. In this paper, we opted for the terminol- ogy \"prediction\u201d over \u201cclassification\u201d as we delib- erately excluded explicit information about case outcomes from the input of the prediction task, and only kept descriptions of facts and claims in the input. As such, this task focuses on predicting case outcomes based solely on information about case facts and applicants' claims. Similarly, the legal ex- perts predicting outcomes had only access to facts and claims.\""}, {"title": "5.2 Data Preparation for the Prediction Task", "content": "We tailored the CLC-UKETanno data to construct a case outcome prediction task for UKET. Three types of legal factors are needed for the prediction task, namely facts, claims and general case out- comes. The input to the prediction models is a se- quence of fact statements concatenated with claim statements, in the form of \"fact1, fact2, \u00b7\u00b7\u00b7, factn [SEP] claim\u2081, claim2, \u2026, claimm\". The target out- put of the prediction task is a general outcome label, which is a categorical variable labelling potential case outcomes as claimant wins, claimant partly wins, claimant loses and other."}, {"title": "5.3 Data Statistics", "content": "From the 19,090 cases in the CLC-UKETanno dataset, we filtered out cases where no substantial information about facts and claims was extracted by GPT-4 at the LLM-aided case annotation step. After the filtering, we obtained a set of 14,582 UKET cases, supplemented with fact and claim statements extracted by GPT-4. We denote this prediction dataset as CLC-UKETpred. Following general practice in machine learning research, we divided the 14,582 CLC-UKETpred cases into three splits: training, validation and testing. The details on data statistics of the train/val/test sets for CLC- UKETpred are summarised in Table 1.\nNote that for the training and validation sets, all three legal factors - facts, claims and outcomes - were sourced from information automatically ex- tracted by GPT-4, as detailed in Section 4.2. For the testing set, facts and claims were automatically extracted by GPT-4, whilst the case outcome labels were manually annotated by a legal expert. The expert annotator carefully analysed the full court judgments and summarised the judges' decisions into general case outcome labels. These manu- ally annotated outcome labels for the test cases represent the actual judicial decisions, serving as gold-standard references for prediction evaluation."}, {"title": "6 Experiments and Results", "content": "We experimented with two classes of baseline mod- els:\n1. Transformer-based models, including BERT and T5 ;\n2. LLM-based models, including GPT-3.5 and GPT-4 .\nThe two Transformer-based models were fine- tuned on our CLC-UKETpred data, whilst GPT-3.5 and GPT-4 were tested using zero-shot and few- shot settings without dedicated fine-tuning. Im- plementation details of the baseline models are presented in Appendix A.1.\nBERT. We fine-tuned BERT on the training set of CLC-UKETpred with the Adam optimiser with a learning rate of 1e-4 and a batch size of 32. The final checkpoint was obtained after training the model for 5 epochs.\nT5. The T5 model is also fine-tuned on the training set of CLC-UKETpred\u00b7 The model is optimised with a learning rate of 1e-4 for 5 epochs.\nGPT-3.5-turbo and GPT-4-turbo. We tested GPT-based models with diverse settings, including (1) zero-shot prediction, (2) few-shot prediction with randomly selected examples and (3) few-shot prediction with examples selected according to ju- risdiction codes. The prompts that we used for LLM experiments are presented in Appendix A.2.\n\u2022 Zero-shot prediction. In this setting, the GPT- based models are directly asked to predict an outcome based on information about facts and claims of a case. No examples are provided for the models in the prompts.\n\u2022 Few-shot prediction with randomly selected examples. We randomly selected a few exam- ples from the training set and included them in the prompt to GPT-based models. We also investigated the effects of the number of ex- amples on prediction performance by experi- menting with two numbers (i.e., 2 and 5) for examples included in the prompts.\n\u2022 Few-shot prediction with examples selected using jurisdiction codes. This setting differs from the above few-shot setting in that we deliberately sampled case examples according to jurisdiction code similarity. In other words, given a target case for which a case outcome is to be predicted, we first identified the set of jurisdiction codes associated with it. Next, we gathered a collection of cases that share at least one jurisdiction code with the target case. From this collection, we sampled a specified"}, {"title": "6.1 Baseline Models", "content": "We experimented with two classes of baseline mod- els:\n1. Transformer-based models, including BERT and T5 ;"}, {"title": "6.2 Human Prediction", "content": "We further investigated how well legal experts can predict UKET case outcomes given facts and claims. This investigation is of paramount impor- tance, as human performance can establish a refer- ence to calibrate model performance.\nTwo legal experts conducted the human predic- tion exercise. They are PhD candidates in Law with a focus on UK employment law. They were supervised by a professor of law based in the UK. Each test case in CLC-UKETpred was separately annotated by the two legal experts. We asked anno- tators to indicate what they think is the most likely case outcome after reading facts and claims of a case. They were also asked to indicate whether a prediction is of low confidence. Cases labelled with low confidence are usually cases that are hard to predict due to insufficient information contained in the given facts or claims or due to the intrinsic complexity of a case.\nAt the beginning of the annotation process, both annotators were provided with annotation guide- lines (see Appendix D for details). The annotation guidelines are consistent with our overarching ex- perimentation design for the prediction task. An- notators were asked to make their judgments sep- arately, avoiding discussions amongst themselves. We emphasised that human predictions should be made based on the same facts and claims that pre- diction models were evaluated on. Annotators were required to not search for the cases they were an- notating on the internet. Whenever questions re- garding the implementation of the annotation arose during the annotation process, the annotators were provided with clarification by the supervisor.\nAfter annotating, we obtained two independent sets of predicted case outcome labels for the 1,371 test cases. The Cohen's Kappa score for all annota- tions is 0.421."}, {"title": "6.3 Results", "content": "Overall results. Table 2 presents the overall evalua- tion results for the CLC-UKETpred prediction task. The experiment findings reveal several key insights regarding the performance of different models. All models tested significantly outperform the random"}, {"title": "7 Further Discussions", "content": ""}, {"title": "7.1 Relevance of Scores", "content": "We would like to emphasise that the evaluation scores reported for the CLC-UKETpred prediction task are baseline results. Both the transformer- based and the LLM-based models could be im- proved further for the task at hand. For example, the latter could be further enhanced by incorporat- ing retrieval-augmented generation or chain-of-thought ."}, {"title": "for legal practice.", "content": "Against this background, it is worth discussing a few patterns in the scores. First, both models and legal experts achieve higher recall than precision scores for \u201cwins\" and higher precision scores than recall scores for \"loses\". Precision is a useful mea- sure when the costs of a wrongly predicted positive are high. In a litigation context, this is the case when the costs of initiating litigation (e.g., fees for legal and other advisers, court fees, time and nerves invested etc) are high. Likewise, recall is a useful measure if the costs of missing a true positive are high. In the context of court proceedings, this is the case when the opportunity cost of not initiating likely successful litigation is high, for example, if the expected remedy has a high monetary value or otherwise has a high relevance for the poten- tial claimant (e.g., for emotional reasons). Hence, it depends on the specific situation of a potential claimant whether precision or recall provides bet- ter guidance. Since the UKET currently does not charge fees and claimants can represent themselves (thereby saving costs), recall may be the preferable score if the claim matters to the potential claimant. Second, it is worth noting that the F-score of GPT- 4 juris2 for \u201cpartly wins\u201d outperforms the human predictors. This may indicate the LLM's ability to navigate more complex litigation, which involves multiple claims or multiple parties on either side."}, {"title": "7.2 Possible Reasons for Errors", "content": "Models and annotators, based on the extracted facts and claims, cannot always determine whether a tri- bunal's decision will finally resolve the claim or only address a preliminary issue. For example, in a disability discrimination case, the tribunal might first issue a judgment confirming the claimant's disability (preliminary issue), followed by a sec- ond judgment addressing the actual discrimination claim. The first judgment (which the claimant may win) is a necessary step but does not resolve the final claim, while the second judgment might con- clude that there was no discrimination (such that the claimant ultimately loses). Preliminary issues are often contested, and some applications may solely seek a tribunal declaration on such issues (e.g., confirming the claimant is an employee or disabled). The possibility of such multi-step pro- ceedings increases the complexity of predictions and has likely had a negative effect on the results for both the models and the human predictors.\nFurther difficulties arise in cases where UKET renders a procedural decision instead of deciding on the substance of the claim. Such cases are clas- sified as \"other\". However, both models and hu- man annotators may predict a substantive instead of a procedural decision and, therefore, suggest \"claimant wins\u201d or \u201cclaimant loses\". According to our annotation guidelines, this affects, in particu- lar, the categories of \u201cclaimant partly wins\" and \"other\". This complexity may have contributed to low evaluation scores for \u201cclaimant partly wins\" and \"other\".\nMore generally, the extracted facts, which are the basis for both the models' and the humans' pre- dictions, may not include all the elements needed to form a prediction. This may be the result of GPT-4 not including all details in the facts sec- tion when extracting the facts from the underlying UKET judgments. For example, when there is an application for costs, which is highly dependent on the parties' behaviour, the models and legal experts may be limited in their prediction due to factual details missing. Additionally, certain outcomes may hinge on factors like the respondent's failure to challenge the claim or produce evidence, which might not be reflected in the extracted facts, lead- ing to incorrect predictions. Although extracted facts may include procedural aspects, they do not always capture procedural facts that determine the outcome, such as the timing of a claim that is dis- missed due to late submissions."}, {"title": "8 Conclusion", "content": "This paper explores the prediction of dispute out- comes for the UK Employment Tribunal (UKET). The paper demonstrates efforts on employing LLMs for automatic annotation practice to allevi- ate the burden of massive manual annotation. With LLM-aided annotation, we curated the CLC-UKET dataset with comprehensive, high-quality legal an- notations. We showcased how the CLC-UKET data can be used to construct a prediction task to categorise case outcomes based on sequences of facts and claims. We fine-tuned and evaluated two widely used Transformer-based models on this pre- diction task. We also evaluated LLMs on the pre- diction task with a range of settings, and reported human performance on the task to facilitate model calibration. These empirical efforts serve as a use- ful benchmark for the UKET prediction task. We will make the CLC-UKET dataset publicly avail- able with the publication of this paper."}, {"title": "Ethics Statement", "content": "The curated dataset is developed on the basis of the Cambridge Law Corpus (CLC), which aggregates publicly available UK legal judgments. Both the decisions in the CLC and the jurisdiction codes of UKET are licensed for use under the Open Gov- ernment Licence. This licence grants a worldwide, royalty-free, perpetual and non-exclusive licence. Access to the CLC is restricted to researchers with confirmed ethical clearance and requires compli- ance with the DPA and UK GDPR. While UK legal judgments are not anonymised, Rule 50 of the Em- ployment Tribunal Rules ensures that sensitive per- sonal information is anonymised when necessary. Additionally, Schedule 2, Part 5 of the DPA pro- vides derogations for academic research, alleviat- ing the burden of notifying all individuals involved in judgments.\nOur dataset does not go beyond publicly avail- able information and includes established proce- dures for data removal if requested. Like the origi- nal CLC, access to the dataset created for this paper is limited to qualified researchers who adhere to the relevant ethical and legal standards. Given the public availability of the data and our efforts to democratise access to legal information, we be- lieve that we meet the ethical requirements for this research.\nFor more details on the legal and ethical consid- erations concerning the underlying CLC dataset, see \u00d6stling et al. (2023)."}, {"title": "Limitations", "content": "While our study provides valuable insights into the prediction of dispute outcomes for the UK Em- ployment Tribunal, it is important to acknowledge certain limitations of our findings.\nAccess to the actual facts and claims of the cases. The facts and claims used in this paper were extracted from tribunal decisions. This was nec- essary given the impossibility of obtaining actual facts and claims in the number necessary for this paper. Consequently, we employed the extracted facts and claims from the court judgments as a prac- tical substitute, providing a tangible foundation for our judgment prediction models.\nThis approach could potentially introduce infor- mation biases at the input stage of the prediction task. The facts and claims that we used in the CLC- UKET dataset were derived from the judges' writ- ten decisions at the end of the proceedings. Since the judges know the result of the case at this stage of the process, the texts they write may inherently contain biased information. For example, sentiment words in the judges' statements might implicitly reveal their in- clinations towards certain decisions. The models might incorporate such factors when making pre- dictions related to case outcomes. Similarly, the legal experts may have picked up such sentiments. In subsequent research, we will explore alter- native methods of identifying facts and claims to better approximate the original submissions to the court, thus fostering a more realistic modelling of judgment prediction.\nAutomatic information extraction. Manual annotation of legal texts requires extensive expert knowledge and can be costly. To alleviate these challenges, this research utilised GPT-4 for au- tomatic information extraction. While the use of GPT-4 offers notable advantages in terms of time and cost efficiency, and the extraction results are generally satisfactory according to the quality check conducted by legal experts in a related study, this annotation practice is not without flaws. The quality of legal annotations could be further improved in future explorations. There is also room to explore the effect of extract- ing and providing more detailed facts compared to the relatively concise fact statements present in the current CLC-UKET dataset.\nDataset and evolution of law over time. We do not know whether the datasets employed are repre- sentative or include all decisions by the UKET in the relevant period. The dataset providing the cases to be predicted by the models and human experts covers the years 2011 to 2023. During this time, both employment and procedural law has evolved. Predicting a case outcome without knowing the precise decision date may lead to mistakes. Models and human predictors did not have direct access to the date at which the underlying case was decided. However, they may have inferred the decision date from the case identifier, which contains the year of the decision."}, {"title": "A Implementation Details", "content": ""}, {"title": "A.1 Experiment Settings for Transformer Models", "content": "The implementation of the two Transformer-based models is based on the HuggingFace transformer li- brary . We used the base versions for both models, initialised from their pre-trained weights. The BERT-base checkpoint has 110 mil- lion parameters. The T5-base checkpoint has 220 million parameters. The maximum input sequence length was set to 512 tokens. We tried different settings for other hyperparameters such as weight decay and the number of warm-up steps, and found that the values of those hyperparameters have an impact on how fast the model is trained, especially at the beginning steps, but do not have a strong impact on the final learning performance. For this reason, we set both weight decay and warm-up steps to 0 for ease of model implementation and future replication. All training processes were per- formed on an Nvidia RTX 8000 GPU."}, {"title": "A.2 Final Prompts Used in the GPT-based Experiments", "content": "We experimented with a number of prompts whilst exploring automatic legal annotation using GPT-4 and the prediction of case outcomes with GPT-3.5 and GPT-4. The final prompts that we used were selected based on the quality of the responses from GPT models for the task at hand.\nThe information extraction prompt that we used to extract data from UKET court decisions reads:\nYou are a legal assistant. Your task is to read through the court decisions that I will send you, and extract the follow- ing information for each input: 1. facts of the case; 2. claims made; 3. any references to legal statutes, acts, regu- lations, provisions and rules, including the specific number(s), section(s) and ar- ticle(s) of each of them, and including procedural tribunal rules; 4. references to precedents and other court decisions; 5. general case outcome; 6. general case outcome summarised using one of the four labels - 'claimant wins', 'claimant loses', 'claimant partly wins' and 'other'; 7. detailed order and remedies; 8. essen- tial reasons for the decision (procedural and substantial). If there are multiple claimants or respondents, extract the case outcome for each and all of the claimants or respondents separately. Please stick strictly to the text contents that I will send.\nThe zero-shot prompt that we used for the GPT- 3.5 and GPT-4 prediction experiments is:\nYou are a legal assistant. Your task is to predict the most likely outcome for a case based on the facts and claims that I will send you. Please summarise the case outcome using one of the four la- bels - 'claimant wins', 'claimant loses', 'claimant partly wins' and 'other'. Note that the label 'other' is to be reserved for cases for which the result cannot be pre- dicted or where the outcome cannot be described in terms of winning or losing (e.g., a merely procedural decision such as a stay or an evidence collection). The output should be one of the four labels.\nThe few-shot prompt that we used for GPT-3.5 and GPT-4 prediction experiments is:\nYou are a legal assistant. Your task is to read through a few examples of legal case outcome prediction that I will send you and predict the most likely outcome for a case based on the facts and claims that I will send you. Please summarise the case outcome using one of the four labels - 'claimant wins', 'claimant loses', 'claimant partly wins' and 'other'. Note that the label 'other' is to be reserved for cases for which the result cannot be pre- dicted or where the outcome cannot be described in terms of winning or losing (e.g., a merely procedural decision such as a stay or an evidence collection). The output should be one of the four labels. To give you a few examples:\nCase example #1\nFacts: \nClaims: \nThe case outcome label is:"}, {"title": "B Further Analysis of the CLC-UKET Dataset", "content": ""}, {"title": "B.1 Examples From the CLC-UKET Dataset", "content": "Table 5 presents facts, claims and general case out- comes for two cases in the CLC-UKETpred dataset. Facts and claims are extracted annotations from GPT-4. Facts and claims are concatenated to form the input to the prediction task. Outcome labels are manually extracted by a legal expert from court judgments and are used as the target output of the prediction task."}, {"title": "B.2 Page Count Distribution", "content": "We calculated the page counts for the 52,339 court decisions in the original UKET subset in the CLC, which"}]}