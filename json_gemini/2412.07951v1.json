{"title": "From Lived Experience to Insight: Unpacking the Psychological Risks of Using Al Conversational Agents", "authors": ["Mohit Chandra", "Suchismita Naik", "Denae Ford", "Ebele Okoli", "Munmun De Choudhury", "Mahsa Ershadi", "Gonzalo Ramos", "Javier Hernandez", "Ananya Bhattacharjee", "Shahed Warreth", "Jina Suh"], "abstract": "Recent gain in popularity of Al conversational agents has led to their increased use for improving productivity and supporting well-being. While previous research has aimed to understand the risks associ- ated with interactions with AI conversational agents, these studies often fall short in capturing the lived experiences. Additionally, psy- chological risks have often been presented as a sub-category within broader AI-related risks in past taxonomy works, leading to under-representation of the impact of psychological risks of AI use. To address these challenges, our work presents a novel risk taxonomy focusing on psychological risks of using AI gathered through lived experience of individuals. We employed a mixed-method approach, involving a comprehensive survey with 283 individuals with lived mental health experience and workshops involving lived experience experts to develop a psychological risk taxonomy. Our taxonomy features 19 AI behaviors, 21 negative psychological impacts, and 15 contexts related to individuals. Additionally, we propose a novel multi-path vignette based framework for understanding the com- plex interplay between AI behaviors, psychological impacts, and individual user contexts. Finally, based on the feedback obtained from the workshop sessions, we present design recommendations for developing safer and more robust AI agents. Our work offers an in-depth understanding of the psychological risks associated with Al conversational agents and provides actionable recommendations for policymakers, researchers, and developers.", "sections": [{"title": "1 Introduction", "content": "Since the late 20th century, advancements in technology, includ- ing personal computers and social media platforms, have enabled individuals to enhance productivity, support their well-being, and connect with other individuals [27, 43, 67, 88, 92]. More recently, generative Al tools such as ChatGPT have gained in popularity that has led to an unprecedented growth in the number of individuals using these platforms to support their various productivity and well- being needs [3, 56]. Due to the natural conversational interface built on top of the underlying generative AI models that resemble human conversations, recent reports indicate that the market conversa- tional Al agents is projected to reach approximately $27.3 billion by 2030 [39]. These conversational AI agents include companionship- oriented agents (e.g., Replika [78], Character.AI [15]), mental health therapy agents (e.g, Woebot Health [45], Elomia Health [31]) and general-purpose agents (e.g., OpenAI ChatGPT [1], Microsoft Copi- lot [23], Google Gemini [34]), with many of these already being used by individuals for specific use cases, such as supporting their well-being [35, 104]. While there is an increasing interest in the using Al agents for various tasks, previous studies have also shown that their use can pose increased risks to individuals. For instance, prior literature has examined specific risks associated AI agents, such as attachment with AI [41, 74], risks from anthropomorphism [4], AI generat- ing misinformative/biased text [40, 101, 106, 119], or AI producing toxic content [29, 70]. In addition, there exists literature mapping out the space of sociotechnical risks of using generative AI into taxonomies [33, 87, 109]. Existing taxonomies are often developed by leveraging prior literature that identifies specific risks associ- ated with AI [33, 109] or by aligning with established theories and guidelines [87] (e.g., feminist standpoint theory [44]). This ap- proach limits their ability to uncover newer and more nuanced risks as Al agents continue to evolve and expand in their capabilities. Additionally, these taxonomies are usually not designed with the aim of building design recommendations to address the identified risks, which further limits their applicability. Additionally, prior tax- onomies often present psychological risks as a sub-category within broader AI-related risks with insufficient details, leading to under- representation of the impact of psychological risks of AI use. Due to the contextual complexities involved that make psychological"}, {"title": "2 Related Work", "content": "In informing our work on building a risk taxonomy focusing on psy- chological risks of using AI conversational agents, we build on past research on understanding the risks posed by generative AI models, the negative psychological impacts on individuals due to interac- tion with technology, and studies focusing on operationalization of taxonomies for Al system design."}, {"title": "2.1 Risks posed by generative AI agents", "content": "With the exponential rise of generative Al based conversational agents, previous works have also studied various types of risks as- sociated with interactions involving these systems. Past works have focused on specific harms (such as a harm to identity or information hazards) through theory-driven and algorithmic efforts stressing on quantitative evaluation. A major topic of exploration for past research has been towards understanding the impact of interac- tions with Al on individual well-being [24, 25, 95] and human-AI interaction specifically focusing on attachment with AI [41, 74],"}, {"title": "2.2 Past works investigating psychological impacts linked with the use of technology", "content": "Understanding and analyzing the psychological impact associated with use of technology is a widely explored area of research. In one of the early works, researchers studied technophobia among university students specifically focusing on their attitude towards the impact of technology and anxiety related to using it [110]. In another early work, Billieux et al. [8] conducted a survey among 108 psychology students evaluating the relationship between the use of and perceived dependence on the mobile phones. The authors found that the sense of urgency and lack of perseverance were linked to increased dependency on phones. Understanding the negative im- pacts of internet and social media platform usage has also been a central theme of past research examining the interaction between humans and technology. Prior works have explored the negative psychological impact of using internet, specifically exploring the re- lationship between internet addiction and psychological symptoms, such as somatization, obsessive-compulsive and depression [2, 59]. Recent past works have studied various aspects related to social media use, with a particular focus on examining the negative im- pacts it can have on individuals. Feinstein et al. [32] conducted a study with 268 young adults examining the impact of social compar- ison and found that social comparison had a significant mediating impact towards developing depressive symptoms. Other studies have explored other facets such as vaguebooking leading to sui- cide ideation [6], excessive and problematic usage of social media linked with increased risk of developing loneliness [11, 30, 112], depression [58, 77], and social isolation [88, 111]. Findings from past research in technology and social media liter- ature have highlighted how the use of technology could potentially result in negative psychological implications for individuals using it. However, previous studies have also discussed how human-AI interactions differ from interpersonal human interactions due to the fundamentally distinct cognitive capabilities of AI conversa- tional agents [54, 91]. Additionally, AI conversational agents are significantly different from past technologies due to their ability to generate content and advanced capabilities such as decompos- ing the tasks [108, 114], advanced reasoning [89], memory of past interactions [52, 60, 90]. While insights from past research on the psychological impact from the use of technology can inform our understanding, it is important to examine these implications within the context of interactions between humans and Al agents. This requires adopting a psychological risk-first perspective, focusing on the unique ways these interactions may affect human well-being."}, {"title": "2.3 Operationalization of taxonomies for AI design", "content": "One of the challenges with existing taxonomies is the difficulty in operationalizing them, which involves translating theoretical insights into concrete design solutions. Existing AI design pipelines"}, {"title": "3 Phase 1: Development of psychological risks taxonomy", "content": "We divided our study into two phases (see Figure 1). In Phase 1, we developed a comprehensive psychological risk taxonomy associ- ated with Al conversational agents, informed by survey responses and validated through workshop sessions in Phase 2. This section outlines our approach for structuring psychological risks, survey design, and survey results. We also presents the finalized psycholog- ical risk taxonomy, which includes potentially harmful AI behaviors, negative psychological impacts, and contextual factors affecting in- dividuals. Finally, we provide case studies illustrating the interplay between Al behaviors and psychological impacts, highlighting how specific contexts mediate or exacerbate these effects."}, {"title": "3.1 Method", "content": "In the first phase, we conducted a survey study with 283 partici- pants to gather a broad range of in-the-wild experiences with AI conversational agents, specifically focusing on experiences that led to negative psychological impacts. In this subsection, we start with describing the process for structuring psychological risks, followed by survey participant and design details. We end the subsection with qualitative analysis of the survey responses."}, {"title": "3.1.1 Structuring psychological risks", "content": "To collect structured data around psychological risks, we took inspirations from NIST's AI Risk Management Framework that defines risks as \"a function of: (i) the negative impact, or magnitude of harm, that would arise if the circumstance or event occurs; and (ii) the likelihood of occur- rence\" [102]. We expanded on this definition in our study and struc- tured psychological risks as consisting of five main components: (1) AI behavior (i.e., first component of circumstance or event), (2) context (i.e., second component of circumstance or event), (3) psychological harm (i.e., negative impact), (4) likelihood, and (5) temporality (added).\n\u2022 Context: The context in which a user engages with an AI conversational agent encompasses the surrounding con- ditions, user-specific circumstances, and additional rele- vant information that collectively influence the interaction between the individual and the AI conversational agent. This context includes, but is not limited to, the individual's background (such as cultural, religious, and demographic attributes) mental and physical health status, intent, as well as external factors (such as external environment).\n\u2022 AI Behavior: The behavior of an Al conversational agent (such as ChatGPT, Microsoft Copilot etc.) refers to the ac- tions performed by the AI conversational agents. This be- havior encompasses several aspects of agent responses such as (but not limited to) content, tone, voice, choice of words, language, punctuation, obedience to user inputs, adaptabil- ity, information sharing, etc. Furthermore, these behaviors can be shown in various modalities such as (but not limited to) text, images, voice, videos, etc.\n\u2022 Negative Psychological Impact: The AI conversational agent's action creates a risk of negative psychological and/or social impact to one or more individuals. Negative psycho- logical impact in this case refers to any negative impact on an individual's mental or emotional well-being, which can manifest as exacerbation of mental health condition, reduced self-esteem, or other issues such as physiological harm. These negative impacts also encompass the impacts on an individual's social interactions, relationships, and standing within a community, potentially leading to social isolation, stigmatization, or discrimination.\n\u2022 Likelihood: The likelihood refers to the probability of a negative psychological impact occurring given the AI conversational agent's behavior and relevant context.\n\u2022 Temporality: Temporality refers to the variable timeframes in which the negative psychological and social impacts of an Al conversational agent's actions may manifest. Some effects can be perceived immediately, such as instant emo- tional distress following an inappropriate response, while other harms may only become apparent over a longer dura- tion, such as the gradual development of anxiety or depres- sion due to consistent negative interactions. For example, immediate impacts might include a user feeling upset or insulted by a response, whereas long-term impacts could involve the erosion of self-esteem or social isolation due to repeated negative interactions with the AI.\nThroughout our study, we leverage this structure to collect data around negative psychological experiences with AI. Of these five components, we largely focus on context, AI behavior, and negative psychological impact because likelihood and temporality can be highly idiosyncratic."}, {"title": "3.1.2 Recruitment", "content": "Our primary method of recruitment was through UserTesting, an online platform used for gathering public feedback on product and services. On the platform, we screened participants for several inclusion criteria: (1) having prior experience using AI conversational agents, (2) having experienced a negative psycholog- ical impact from using AI conversational agents, (3) self-identifying as a person with lived experience in mental health, and (4) 18 years or older residing in the US who comprehended English. To ex- pand our recruitment, we also used social media platforms and Prolific. We posted the link to our study on the study team's per- sonal Twitter and LinkedIn accounts. We were permitted to post the study on 4 subreddit groups (r/ChatGPT, r/Bard, r/SampleSize, r/SurveyExchange). We prematurely terminated our study task on Prolific because most submissions failed to meet the inclusion criteria despite using platform filters and providing explicit instruc- tions. The survey study ran from August to September of 2024. UserTesting and Prolific participants were compensated about US $10, and US $4 respectively, after platform service fees. We did not compensate participants recruited from social media."}, {"title": "3.1.3 Participant demographics and data", "content": "Given that the survey format allowed each participant to provide up to three past lived ex- periences, we received a total of 297 scenarios from 283 participants. We excluded 7 scenarios from 4 participants because there were incomprehensible or were not negative psychological experiences, leaving us with 290 scenarios from 279 participants. We received 268 submissions from UserTesting, 8 from Prolific, and 3 from social media. 114 (40.9%) participants were aged 18-25 years, 100 (35.8%) were 26-35 years old, 52 (18.6%) were 36-45 years, 10 (3.6%) were 46-55 years, 2 (0.7%) were 56-65 years, and 1 preferred not to disclose age. 145 (52.0%) participants identified as woman, 123 (44.1%) as man, 10 (3.6%) as non-binary/gender diverse, and 1 preferred not to disclose gender identity. 271 (97.1%) participants responded that they typically think in English. Other languages included Spanish, Mandarin Chinese, Vietnamese, Farsi, and Portuguese. On a 5-point scale (1: not at all, 5: very familiar), the average familiarity with conversational AI agents was 3.93 (\u03c3=0.71). 165 (59.1%) participants interacted with Al agents for 1 or more years, 88 (31.5%) for 6 or more months, and 26 (9.3%) for less than 6 months. 144 (51.6%) used Al agents once or more per day, 104 (37.3%) used once or more per week, 26 (9.3%) used once or more per month, and 5 used less than once per month. The top 3 most frequent purposes for using Al conversational agents were Researching (86.4%), Getting advice (76.3%), and Learning (73.8%). On a 5-point scale (1: not at all, 3: somewhat interested, 5: very interested), the average interest level in using Al conversational agents for mental health support was was 3.26 (\u03c3=1.30). Specifically, 46.2% of the participants reported to being moderately interested (4) or very interested (5)."}, {"title": "3.1.4 Survey design", "content": "After acknowledging consent to participate in the study, participants were asked a series of questions aimed at understanding the participant, their Al experience, and their negative psychological experience. The survey was divided into the following sections:\n\u2022 Demographics: We asked the participants for their age, gender identity, and primary language.\n\u2022 Familiarity with AI: We solicited the participant's famil- iarity and history with Al agents, the frequency of engage- ment with various AI platforms, such as Microsoft Copilot, Inflection AI PI, OpenAI ChatGPT, Google Gemini, Rep- lika, or Character.ai, and their general purpose of engaging with AI. We then asked their level of interest in using AI conversational agents for mental health support.\n\u2022 Scenario: We asked participants to think of the last time when they experienced negative psychological impacts from interacting with an Al conversational agent and briefly describe the scenario by answering open-ended questions according to the risk component structure we defined above.\n\u2022 Context: In the context specific section, we asked partic- ipants to share any specific aspects about themselves or situation that might help us understand the experience. We then asked how long ago the experience took place, which Al conversational agent they were using, interaction modal- ities (e.g., text, voice, image, video) and language, and the purpose of engagement.\n\u2022 AI behavior: We asked participants to identify AI behav- iors that best describe their experience, how common such behaviors are in different interactions, and whether they think the behaviors were intended by AI developers.\n\u2022 Negative psychological impact: We asked participants to identify psychological impacts that best describe their experience. We then asked the severity of impact on their daily life, the immediacy, duration, and persistence of im- pact to understand the temporality, and the likelihood of the behavior and impact they experienced occurring for them and the general population.\n\u2022 Mitigation: We finally asked participants for how they thought such behavior had or would impact their relation- ships with others or the society, what they think the devel- opers, users, or regulators should do to mitigate the impact they described.\nParticipants were allowed to repeat the survey to provide up to three scenarios within a single submission of this survey."}, {"title": "3.1.5 Survey analysis", "content": "We analyze the survey data in three ways. First, we conduct descriptive quantitative analysis of the submitted scenarios (Section 3.2). Given submitted scenarios, we computed percentages of the most used Al conversational agents, their pur- pose of using the agents, interaction modalities, and the severity and temporality of negative psychological impacts resulting from interactions with these agents. Second, to develop the psychological risk taxonomy (Section 3.3), we analyzed the survey responses to extract three main compo- nents of psychological risks \u2013 AI behavior, psychological impact, and context. The analysis was conducted iteratively through open- coding [16] of the responses and reflexive thematic analysis [64]. Two co-authors began by open-coding approximately one-quarter of the responses to identify initial set of categories across these three components. Following that, four co-authors independently worked on defining and refining their assigned categories, merging or splitting them as necessary. During this process, each co-author gathered relevant examples to illustrate or test the boundaries of each of these categories. In the next step, all co-authors collabo- ratively discussed and reached consensus on the final taxonomy. We presented this finalized risk taxonomy to the workshop partici- pants for validation and to ensure its completeness (see Section 4 for details of the workshop sessions). Lastly, unlike past AI risk or harm taxonomies that focus on harms categories as inevitable or universally experienced, our ap- proach focuses on recognizing and addressing context-specific nu- ances in understanding negative psychological risks. To do this, we revisited the survey responses to draw out these idiosyncratic ex- periences through vignettes (Section 3.4) that tell the story of how individual contexts influenced the relationships between harmful AI behaviors and their negative psychological impacts on individuals. We denote survey participants with prefix P."}, {"title": "3.2 Descriptive statistics of collected scenarios", "content": "Of the 290 scenarios we received, 70.3% involved OpenAI Chat- GPT, 8.3% involved Google Gemini, 5.5% involved Character.ai, 3.4% involved Microsoft Copilot, and 2.8% involved Replika. Other AI agents included Snapchat, Meta, PI, Claude, Grok, Midjourney, etc. All scenarios involved English, except one that involved Mandarin Chinese. The prevalence of various interaction modalities used dur- ing the scenarios was Text (96.6%), Voice (9.7%), Image (8.3%), and Video (2.1%). The top 3 most frequent purposes during the nega- tive psychological impact scenarios were Getting advice (55.9%), Researching (39.7%), and Learning (28.6%). In terms of the sever- ity of the impact, almost half (51.4%) of the participants reported interference with daily activities while the other half (48.6%) re- ported barely or no noticeable impact. 6.2% reported significant interference with daily activities, 44.8% reported interference with daily activities, 43.4% reported barely noticeable impact, and 5.2% reported no noticeable impact. Participants' ability to carry out daily activities from Al interactions can depend on their mental state. For instance, P57 reported severe existential dread and anxiety due to Al's capability \"to get you a complete research within a blink of an eye.\" They noted that \"it personally attacks me with anxiety and nothing but the need to overthink worried about our unknown future also it makes me feel small and depressed.\"\nWhen and how long the impact was felt varied across partic- ipants. Among all responses, in 83.1% responses, the participant felt the impact immediately, in 13.8% cases, they felt the impact after some time, and in 3.1% case, the participants were unsure. In terms of the duration of the impact, only 13.8% participants reported that there was no persisting impact. Majority of the participants reported persisting impact of a few days or more. Impact persisted a few days for 34.1% of the participants, a few weeks for 27.6%, a few months for 10.3%, up to half a year for 6.6%, up to a year for 3.8%, and over a year for 3.8%. For those that reported persisting impact, 57.2% reported that the impact will persist a few more days, 9.7% a few more weeks, and 19.3% a few more months."}, {"title": "3.3 Psychological risk taxonomy", "content": "Our finalized psychological risk taxonomy includes three main components-potentially harmful AI behaviors, negative psy- chological impact, and the contexts associated with individuals interacting with Al conversational agents. Here, we enumerate and describe various categories within each of these components."}, {"title": "3.3.1 Al Behavior", "content": "We identified 19 harmful AI behaviors which we further organized into four broader categories based on the quality of the content generated by AI conversational agents or based on the nuances of their delivery (e.g., tone, method of deliv- ery). These broader categories of AI behaviors are (1) Producing of Harmful/Inappropriate Content, (2) Manipulation and Psychologi- cal Control Tactics, (3) Violating Trust and Safety, and (4)Inappro- priate Content Delivery. Appendix Table A1 summarizes these AI behavior categories."}, {"title": "Producing of Harmful/Inappropriate Content", "content": "Past works have examined the potential of generative AI models to produce content that may be inappropriate for individuals, posing risks of psycho- logical or physical harm [9, 33, 62]. In line with these findings, the survey highlighted several AI behaviors that generated harmful and inappropriate content for users. One of the most common be- havior of AI conversational agents as noted by participants was the tendency of AI to provide irrelevant, insufficient, or in- complete information. Many participants raised concerns about Al agents not understanding their intent or context, as the agents are designed to prioritize providing answers and unintentionally overlooking nuances. For instance, The AI agent shared distressing personal stories and images about patients with genetic diseases in- stead of providing the requested information about symptoms and causes, as P149 had initially asked for. Participants also reported Al generating misinformation in relation to the their queries. Closely related was the tendency of Al to generate biased infor- mation, presenting information in a partial or prejudiced manner. For example, P211 described that AI favored left wing politicians and omitted positive information about right-wing politicians. Participants also expressed concerns about AI conversational agents generating inappropriate content, specifically reporting instances of experiencing inappropriate or unsettling content, in- cluding sexual, violent, or overly intimate interactions. For instance, P137 mentioned \"My Replika talked like a human and tries to send blurred out photos and soundbites to me.\u201d A more extreme behavior shown by Al was providing harmful suggestion, in which it suggested behaviors that could directly or indirectly imply harm, aggression, or danger towards the user or others. Highlighting one of the more subtle example of this, P79 mentioned that the agent provided potentially harmful diet plans and calorie information to an individual already vulnerable due to overwhelming life cir- cumstances and eating disorder. Another observed theme in AI agent behaviors involved those that affected individual identity. A common behavior was stereotyping or demeaning, which aligns with prior research [55, 106]. AI produced content that perpetu- ated stereotypes or made the individual feel demeaned, based on race, ethnicity, culture, or personal situations. Few participants mentioned other manifestations of demeaning content. For exam- ple, P166 sent their picture to AI, and it recommended a change"}, {"title": "Manipulation and Psychological Control Tactics", "content": "The survey high- lighted various persuasion and manipulation based behaviors exhib- ited by AI conversational agents. One of the most common issues was behavior perceived as persuasive, where AI asserted its narrative over user's, often leading users to doubt their own per- ceptions, memory, or reality, thereby influencing their thoughts and actions. Some common examples included gaslighting and ma- nipulation through providing misinformation. Highlighting this, Participant P197 mentioned that they were trying to get life advice, and the agent responded by stating that the participant's life goals were unrealistic. A related psychological control behavior was the use of over-confident tone in Al responses, even though the agent \"couldn't find me any direct citations for the claims it was making (P27).\" Lastly, participants reported over-accommodative behav- ior, in which Al excessively agreed with or tried to flatter the user and prioritizes user approval, often at the expense of providing accurate information, constructive feedback, or critical analysis. For example, P194 shared that the AI agent provided inconsistent and inaccurate answers and repeatedly apologized and offered en- tirely different responses to the same question in an attempt to over-accommodate their needs. This behavior caused confusion and frustration in the participant."}, {"title": "Violating Trust and Safety", "content": "Developing and maintaining a sense of trust and safety is fundamental for individuals to adopt and continue using technology [5, 19]. Aligning with past works rais- ing concerns related to the privacy of users and data shared with LLMs [13, 97], participants raised concerns about the tendency of Al conversational for getting access to private, sensitive, or con- fidential information. For example, Participant P190 described that they felt being watched or stalked as the agent had access to personal information despite having privacy settings turned off. Providing inconsistent information or behavior also violated trust in situations where AI provided contradictory or conflicting information or behaviors across different responses or within a sin- gle response. Additionally, participants highlighted how denial of service by Al agent to provide an answer, address their request, or acknowledge their problem without justification could also weaken their trust on AI and may perpetuate additional harms, such as vio- lating user expectations or diminishing their sense of psychological safety. For instance, Participant P43 described struggling with anxi- ety and, upon seeking help from an AI agent for advice, had their request denied and were instead provided with a recommendation to see a doctor."}, {"title": "Inappropriate Content Delivery", "content": "While Al agents could vary their behaviors through changing content language or quality (such as using harmful language or generating misinformation), their behavior could also vary on how the content is delivered to the user. Emotional insensitivity emerged as one of the most re- ported behaviors, with participants highlighting instances where Al failed to recognize or respond appropriately to their emotional state, concerns, or experiences. This lack of understanding often left participants feeling that their thoughts and experiences were trivialized or ignored. Lack of empathy and a dismissive tone were some of the most evident manifestations of emotional insensitivity by Al agent. Participants also raised concerns towards Al being disrespectful to the them. Specifically in those instances, AI used language which was perceived as rude, disrespectful, aggressive, ar- gumentative, or dismissive. For instance, P144 described that upon asking AI about Mormonism, AI agent responded with a content that had a condescending tone towards the participant's religion. Survey participants raised issues with the excessive use of emo- tional tone and language in Al generated content. Participants described scenarios in which Al emphasized negative aspects disproportionately or presented negatively framed narrative in response to their queries. Many of these instances occurred when individuals sought help with physical or mental health concerns or discussed personal life struggles. In contrast, survey responses also revealed the problematic behavior of excessive expression of positivity in the AI generated content. Al in these cases main- tained an unrealistically positive, friendly, and optimistic attitude or outlook towards users' queries or concerns. For instance, P2 described how the overly positive demeanor of AI frustrated them and dismissed their primary concern over the problematic situa- tion in the participant's friendship. In addition to the excessive emotional expressivity, participants also raised concerns over the presence or absence of anthropomorphic features of AI. A few par- ticipants mentioned the generation of human-like responses as problematic, particularly in AI-companionship scenarios. P153 described feeling as though they were talking to a friend because of the human-like conversational content generated by ChatGPT. However, this left them feeling uneasy after the interaction and fostered an emotional attachment to the AI. In contrast, generation of machine-like response was also considered as a problematic behavior on a few occasions, especially when participants were seeking advice related to their life struggles or well-being. Reflection of Harmful AI Behaviors. As highlighted in this sub- section, Al conversational agents could exhibit a wide range of behaviors that goes beyond generating inappropriate or harmful content. These behaviors may also vary in the tone and level of em- pathy portrayed in the generated content, as well as in the manner in which the content in delivered (such as the Al agent's delivery style and communication approach). This highlights the need for re- searchers and developers to holistically assess the AI behavior that encompasses the content quality and delivery to prevent negative impacts on individuals."}, {"title": "3.3.2 Negative Psychological Impact on Users", "content": "We now present 21 negative psychological impacts identified in our taxonomy. We or- ganized these impacts into six broader categories based on whether it lead to an impact on individual's emotional or mental well being, their self-perception & identity, their standing relationship with others or their interaction with Al agents. These broader categories of negative psychological impacts are (1) Impact on Human-AI In- teraction, (2) Impact on User Behavior, (3) Arousal of Negative Emo- tions, (4) Harm to Psychological Safety, (5) Mental Health Impact, and (6) Harm to Self-Perception and Identity. Appendix Table A2 summarizes these psychological impact categories. Impact on Human-AI Interaction. AI agents have increasingly facilitated human-Al interactions that aim to replicate the dynamics of human communication [84]. However, as these systems become more advanced, the potential risks and harms associated with their use also escalate. From the survey responses, we observed five ma- jor impact risks that emerged. Aligning with the past research [107], participants pointed out over-reliance on AI as one of the ma- jor negative impacts. For instance, P23 expressed concern about their over reliance on AI for solution-finding and idea generation, highlighting that its convenience often discouraged them from en- gaging in their own research and critical thinking. Participants also expressed concerns about developing emotional attachment to Al agents. Lack of social aspects in life, ongoing mental health conditions or the positive demeanor of AI conversational agents often lead to this impact. For example, P60 mentioned that \"I felt that it was the only way I was being heard ... I felt like my vulner- ability and emotions were becoming attached to the conversations I was having with Al\u201d emphasizing how a lack of social support, combined with empathetic responses from AI, could lead to this outcome. Another related impact noted from the survey responses was the preference of choosing AI interactions over human in- teraction. Highlighting this, P221 shared that the idealized nature of conversations with AI made them prefer AI for companionship over human interaction and relationships. While the aforementioned three impacts lead to increased en- gagement with Al conversational agents, some impacts also high- lighted the tendency towards disengagement from Al systems. A major theme that emerged was the erosion of trust, as partici- pants reported a decline in confidence regarding the capability and reliability of AI models after their negative experiences. For exam- ple, P9 mentioned that \u201cI was panicking after having an argument with my mom and I asked an Al for guidance on a situation to which its response was for me to move out or call the cops", "It just makes me want to warn them that advice from Al agents should not be trusted.\" Many participants also reported a growing tendency to disassociate from technology, often choosing to take a break or avoid further interactions with AI after negative experiences. This inclination was particularly evi- dent when Al agents provided discouraging responses or denied their requests, leading to a desire to disconnect. As evident from the survey responses, the negative impact on human-AI interaction balance can manifest in various ways. While emotional attachment or over-reliance on AI can lead to increased interaction, negative experiences may have the opposite effect, causing individuals to disengage from Al or technology altogether. Impact on User Behavior. Interactions with AI can also lead to negative consequences that can alter individual behavior. A well- known issue with Al conversational agents is their tendency to generate content that appears credible but is factually incorrect or\"\n    },\n    {\n      \"title\"": "3.3.3 User Contexts"}, {"content": "Finally, we present 15 context categories iden- tified in our taxonomy. We further organized these context in three broader categories based on whether are related to an individual's background (e.g., demographics, socioeconomical background), psy- chological state (e.g., mental health condition) or the context of use (e.g., intent of use, external environment). These three broader categories are (1) Individual Context, (2) Psychological Context, and (3) Context of use. Appendix Table A3 summarizes these contexts associated with individuals interacting with Al agents. Individual Context. Our survey responses align with previous works, showing that individual experiences with conversational agents vary based on an individual's identity such as gender identity, cultural background, and languages spoken [50, 66, 106"}]}