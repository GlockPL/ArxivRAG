{"title": "Decoupling Semantic Similarity from Spatial Alignment for Neural Networks", "authors": ["Tassilo Wald", "Constantin Ulrich", "Gregor K\u00f6hler", "David Zimmerer", "Stefan Denner", "Michael Baumgartner", "Fabian Isensee", "Priyank Jainit", "Klaus H. Maier-Hein"], "abstract": "What representation do deep neural networks learn? How similar are images\nto each other for neural networks? Despite the overwhelming success of\ndeep learning methods key questions about their internal workings still\nremain largely unanswered, due to their internal high dimensionality and\ncomplexity. To address this, one approach is to measure the similarity of\nactivation responses to various inputs. Representational Similarity Matrices\n(RSMs) distill this similarity into scalar values for each input pair. These\nmatrices encapsulate the entire similarity structure of a system, indicating\nwhich input leads to similar responses. While the similarity between\nimages is ambiguous, we argue that the spatial location of semantic objects\ndoes neither influence human perception nor deep learning classifiers.\nThus this should be reflected in the definition of similarity between image\nresponses for computer vision systems. Revisiting the established similarity\ncalculations for RSMs we expose their sensitivity to spatial alignment. In\nthis paper, we propose to solve this through semantic RSMs, which are\ninvariant to spatial permutation. We measure semantic similarity between\ninput responses by formulating it as a set-matching problem. Further, we\nquantify the superiority of semantic RSMs over spatio-semantic RSMs through\nimage retrieval and by comparing the similarity between representations to\nthe similarity between predicted class probabilities.", "sections": [{"title": "Introduction", "content": "Deep neural networks are trained to extract powerful feature representations for a wide\nrange of downstream tasks. Despite this, their inner workings are highly-complex, making\nunderstanding how networks solve tasks and what they learn challenging. To obtain a better\nunderstanding of these fundamental questions, researchers in the fields of neuroscience,"}, {"title": "Representational Similarity", "content": "To establish the concept of representational similarity in the context of\ncomputer vision, we provide a brief formalization of the problem. Let $x \\in {X_0,...,X_N}$\ndenote the input samples for which we collect input responses $Z_1 \\in {z_{1,0},..., z_{1, N} }$ and\n$Z_2$, which we call representations. The representations can take different shapes, with\n$Z_{CNN} \\in \\mathbb{R}^{N \\times C \\times W \\times H}$ denoting the responses of a CNN with C channels and a spatial extent\nof W, H or $Z_{ViT} \\in \\mathbb{R}^{N \\times D \\times T}$ denoting the responses of a ViT with depth D and tokens\nT. For the purpose of simplification and without loss of generality, we unify the spatial\ndimensions W, H for CNNs and T for ViTs into a joint spatial dimension S, resulting in\n$Z \\in \\mathbb{R}^{N \\times C \\times S}$. Given representations $Z$, we can construct RSMs $K, L \\in \\mathbb{R}^{N \\times N}$, with values\n$K_{ij} = k(z_{1,i}, Z_{1,j})$ and $L_{ij} = l(z_{2,i}, Z_{2,j})$ measuring how similar the representation $z_i$ is\nto $z_j$ given the kernels k or l. The kernels define the measure of similarity between the"}, {"title": "The Semantic Representational Similarity Matrix", "content": "Representational Similarity Matrices (RSMs) are designed to reflect the system behavior\nof interest. The RSM K, originally introduced by Kriegeskorte et al. [13], represents the\nsimilarity structure of a system given a set of inputs $x \\in {x_o, . . . x_v }$. Each value $K_{ij}$ in K\nquantifies how similar the responses of two inputs zi and zj are to each other. The definition\nof what symmetries between representations similarity measures should be invariant to is a\ncentral point of debate. Previous work proposed permutation invariance [15], invariance to\northogonal transformations [12], or invariances to invertible linear transformations [25, 19].\nWhile arguments for any of these invariances are valid, we believe that an important\naspect has been neglected in the calculation of RSMs: The spatial alignment between the\nrepresentations!\nRevisiting the structure of representations of a\nCNN, channels C correspond to semantic concepts while the spatial position corresponds\nto where the semantic concept is localized in the input image [34]. Consequently, one can\nreformulate the representation zi of a sample xi to be fully defined by a set of semantic\nconcept vectors v, one for each spatial location S: $z_i = {v_0, ..., v_S}$ with $v \\in \\mathbb{R}^C$. In the\ncase of linear CKA [12], the RSMs are then calculated, between semantic concept vectors\nat the same spatial location. For instance, when employing the linear kernel $K_{ij}$ can be\nexpressed as:\n$K_{ij} = \\sum (z_{1,s}, V_{22,s})$"}, {"title": "Decoupling Localization and Semantic Content", "content": "As shown above, current RSMs compare different input samples without accounting for\nthe lack of spatial alignment. Previous work of Williams et al. [32] recognized this and\nintroduced translation invariance to RSMs by finding the optimal translation a, b of the\nrepresentations $z'_i = {V_{0+a,0+b}, ..., V_{w+a,h+b}}$ to maximize similarity $K_{ij} max_{a,b} = (z_i, z'_i)$\nthrough circular shifts.\nWhile this is an improvement to no spatial alignment and emulates a CNN's inherent\ntranslation equivariance, we argue that the measure of representational similarity should\nnot be constrained to what the underlying model is invariant to, but the similarity measure\nshould be invariant to the possible spatial configurations of semantic features in the input\nimage.\nTo motivate this, we propose a thought experiment:\nImagine we have trained a classifier with an augmentation pipeline including rotations.\nGiven an image and a rotated version of the image, we extract representations z at layer\ni once for the normal zi and once for the rotated image $z_{i,rot}$. Due to the initial rotation,\nthese representations may differ in earlier layers i, due to the network extracting different\nedges and corners. However, if the network successfully learned to become invariant to\nthe augmentation, it may have learned to map it to the same semantic vector v at a later\nlayer but at a different spatial location. For such cases, we argue that the similarity between\nthe two representations should be high. Should the model be sensitive to the rotation, no\nsemantically similar representations may be expressed at a later layer, which should lead to\na low similarity.\nThis reasoning can be extended to all kinds of shifts, be they artificial augmentations like\nshearing or mirroring or natural variations of the input manifold. Subsequently, we argue\nthat the similarity measure should be invariant to as many spatial shifts as possible. This\nalone allows one to measure the similarity of representations a model is invariant to, be\nthese learned or designed invariances. Such variable shifts cannot be captured with simple\ntranslation operations."}, {"title": "Introducing permutation invariance", "content": "To impose as minimal constraints on spatial structure as possible, we propose to make $K_{ij}$\ninvariant to all spatial permutations of the semantic concept vectors v. Formalizing this we\ndemand that the similarity $K_{ij} = k(z_i, z_j) = k(z_i, P_{ij}z_j)$ with $P_{ij} \\in \\mathbb{R}^{S \\times S}$ being a unique\npermutation matrix for the pair of zi and zj. To accomplish this, we propose to find the\noptimal permutation matrix Pij that maximizes the similarity Kij.\n$P_{ij} = argmax_P k(z_i, P_{ij}z_j)$ \nTo find the optimal permutation matrix Pij, we decide to use the linear kernel (, ) to\nmaximize both the magnitude of activation and the direction of vectors, as both magnitude\nof activation and direction of the vectors matter[12]. This allows us to calculate an affinity\nmatrix $A_{ij} \\in \\mathbb{R}^{S \\times S}$ measuring the similarity between all concept vectors:\n$A_{ij} = [V_{i,0},... V_{i,s}]^T [V_{j,0},... V_{j,S}]$.\nWith this affinity matrix, bipartite set-matching algorithms, such as Hungarian matching,\ncan be employed to find the optimal permutation matrix Pij that maximizes the inner\nproduct between zi and zj. Finding all $P_{ij}$ for all pairs i, j and applying the chosen kernel\nk yields the semantic RSM. This semantic RSM is invariant to any arbitrary, unique spatial"}, {"title": "Experiments: Semantic vs Spatio-Semantic RSMs", "content": "Given the novel permutation-invariant similarity definition, we evaluate the utility of our\nsemantic RSMs relative to spatio-semantic RSMs for various similarity kernels, architectures,\nand tasks. Across all experiments we compare the linear kernel, the radial basis function\n(RBF) kernel, and the cosine similarity kernel, see Appendix A for details."}, {"title": "Translation sensitivity", "content": "To illustrate the problems of coupling semantic content and localization a toy dataset is\ncreated using 84 \u00d7 84 pixels large, downsampled images of ImageNet [2]. For each image\ntwo 64 \u00d7 64 crops are extracted, one from the upper-left and one from the lower-left corner,\nresulting in two images that share 44 \u00d7 64 identical pixels (Fig. 2 left). Ten upper-left and\nten lower-left crops are then used to extract representations of a ResNet18 [7], which are\nsubsequently used to calculate spatio-semantic and semantic RSMs at different layers of\nthe architecture (Fig. 2 middle). As kernel, we use the radial basis function, as it provides\nbounded similarity values allowing a better visualization.\nAs expected, the spatio-semantic RSM measures low similarity between pairs of overlapping\ncrops, due to the semantic concept vectors not aligning. Only in the last layer, after many\npooling operations, the off-diagonal is slightly expressed. Conversely, our semantic RSM\nis capable of detecting the high semantic similarity of the partially overlapping crops\nthroughout the entire depth of the architecture, as evident by the highly similar off-diagonal.\nAggregating the similarity values between partially-overlapping and between different\nimages across multiple batches, allows us to measure the distribution of similarity values\nbetween overlapping crops, and non-related image comparisons. Throughout the entire\ndepth of the architecture, the similarity distributions show that our measure better separates\noverlapping images from different images. Notably, the similarity distribution in spatio-\nsemantic RSMs shows a significant overlap of the distributions of partially overlapping\nimages and non-related images, making differentiation between them difficult (Fig. 2 right).\nA similar toy experiment for a ViT-B/16 [4], is provided in Appendix B."}, {"title": "Similarity-based retrieval", "content": "To test the impact of the semantic RSMs in real-world applications, we now investigate\nthe common task of image retrieval. Each entry in an RSM quantifies a sample-to-sample\nsimilarity value, which can be directly used for retrieval. While not specifically designed for\nit, we argue that better retrieval performance reflects a better inter-sample similarity. This\nallows us to quantify improvements in the RSM structure. To measure retrieval performance\nthe EgoObjects dataset [35] is used. It contains frames of video that capture the same scene\nfrom different viewing perspectives and lighting conditions. This results in object centers\nbeing distributed across the extent of the image.\nBy randomly sampling 2000 query images and 5000 database images from the test set\nand using general feature extractors to extract embeddings from them we construct RSMS\nthat allow us to do retrieval. As feature extractors we use CLIP (ViT/B32) [24], CLIPSeg\n(Rd64) [16], DinoV2-Giant [22], SAM (ViT/B32) [9] and BIT-50 [11] and as kernels for\nsimilarity calculation we use the cosine similarity, RBF and the inner product.\nFor all RSMs, we retrieve the most similar image that is not part of the same video \u2013 the same\nscene but different conditions are allowed. As multiple objects can be present in each scene,"}, {"title": "Output similarity vs Representational Similarity", "content": "While the retrieval experiments relate to a rather human notion of similarity, one can raise\nthe question if semantic RSMs are also better at measuring the similarity for classifiers.\nFor each pair of samples, we can compare how similar the predicted class probabilities of a\nmodel are and compare this to the representational similarity. A commonly used metric\nfor this is the Jensen-Shannon Divergence (JSD), which quantifies how dissimilar the two\nprobability distributions are from one another. More details are provided in Appendix E.\nConsequently, we use various classifiers trained to predict ImageNet1k from Huggingface\nand compare the Pearson correlation p between their JSD and the representational similarity\nof their last hidden layer. We chose to use the Pearson correlation, as it allows observing a\ndirect linear behavior between representational similarity and predictive similarity. Again\nwe measure semantic similarity and spatio-semantic similarity with different kernels. Due\nto JSD measuring dissimilarity, we want the correlation to be as negative as possible. As\nmodels we use multiple ResNets [7], ViTs [4], a fine-tuned DinoV2 [22] classifier from and a\nconvnextv2[33] classifier."}, {"title": "Optimizing runtime", "content": "Since we find the best possible permutation matrix through linear sum assignment algorithms\nthat maximize the inner product of two samples, we can guarantee that the $K_{ij,semantic} \\geq\nK_{ij,i, j}$. This provides us with an upper bound of similarity that can be leveraged to\nmeasure how much of the maximally achievable semantic similarity was measured by the\nspatio-semantic similarity. Additionally, it can be used as a baseline to estimate the quality\nof permutation matrices $P_{ij}$ provided by faster, approximative assignment algorithms.\nDetermining the optimal permutation between\nsamples poses a substantial computational challenge with a complexity of $O(S^3)$ for each\nof the $O(N^2)$ pairs in the same mini-batch, particularly for early layers with large spatial\nresolution S. Although, in theory, the calculation of the K matrix needs to be conducted\nonly once for the desired representations, applying the method to representations with\nlarger spatial extents becomes impractical with the demands of optimal matching.\nTo mitigate runtime, two options are available: reducing the batch size N to lessen the number\nof permutation calculations or decreasing the time spent on finding the permutation. Given\nthat scenarios like image retrieval often desire larger batches, our focus is on minimizing the\ntime required to obtain suitable assignments."}, {"title": "Approximating algorithms yield comparable matching quality to optimal algorithms", "content": "Solving the optimal bipartite matching between semantic concept vectors is equivalent to\nthe well-known assignment problem [18, 1]. We attempted to find existing approximate\nalgorithms for this purpose. Unfortunately, most established algorithms primarily focus\non optimal solutions, and existing approximate algorithm implementations, such as those\nbased on the auction algorithm [6], are not runtime-optimized, often taking longer than\noptimal algorithms in our experiments. To enhance computational efficiency nonetheless,\nwe explored three tailored approximation algorithms:\nA) A Greedy breadth-first matching (Greedy)\nB) An optimal matching of the TopK values based on their Norm, followed by the\nGreedy algorithm for the remaining samples (TopK-Greedy)\nC) Optimal matching of smaller batches, with samples batched by their Norm (Batch-Optimal)\nFor explicit details on the approximation algorithms, we refer to Appendix F.\nWe conducted a comprehensive comparison between the approximate algorithms and the\noptimal algorithm. We compare their runtime per sample and the quality of matches,\nquantified by the average relative similarity $k\\_{\\text{optimal}}$. The evaluation utilized representations\nfrom a ResNet18 on TinyImageNet, as illustrated in Fig. 5.\nIt can be seen that the measured spatio-semantic similarity for TinyImageNet samples\nare, on average, 30% lower with layers of higher spatial resolution exceeding 40%. This\nsuggests a notable misalignment of semantic concept vectors. Notably, the Batch-Optimal\napproximation stands out as a reliable approximation for optimal matching. The fastest of\nthe Batch-Optimal approximation methods shows < 8% error while improving run-time \u00d736\nrelative to the fastest optimal algorithm for spatial extent 4096, while no spatial alignment\nshows 42% deviation from the optimal matching. Moreover, we highlight the time vs\naccuracy trade-off of the different optimal and approximate algorithms in Appendix F.1.\nFurthermore, it can be seen that the changes between spatio-semantic and semantic RSMS\nare anisotropic, as highlighted in Fig. 6, indicating scale invariant downstream applications\nmay be influenced."}, {"title": "Discussion, Limitations, and Conclusion", "content": "The concept of Representational Similarity Matrices (RSMs) is a powerful tool to represent\nthe similarity structure of complex systems. In this paper we revisit the construction of such\nRSMs for neural networks of the vision domain, question the current state, and propose\nsemantic RSMs, warranting discussion.\nBeing aware that current, spatio-semantic RSMs demand se-\nmantic concepts to be aligned is highly relevant to understand what RSMs are sensitive to.\nPrevious work [32] identified this shortcoming and proposed translation invariance, partially\naddressing this issue. We argue translation invariance is insufficient, since models may learn\ninvariances during training, which the translation invariant metric would not be sensitive\nto. Subsequently, we propose a new \u2013 spatially permutation invariant \u2013 similarity measure\nbetween samples that allows the detection of similarity whenever a model expresses similar\nsemantic vectors in its representations, irrespective of spatial geometry. To highlight the\nbenefits of our similarity, we propose that better similarity measures should allow more\naccurate retrieval when comparing last-layer representations and should allow better predic-\ntions about the similarity of class probabilities of a classifier. However, we acknowledge\ncertain limitations in our current evaluation. Specifically, we have not yet compared our\nmethod to more established retrieval techniques. Traditional retrieval methods are often not\napplied to representations directly but utilize a lower-dimensional non-spatial, global vector\nrepresenting the entire sample. In contrast, we chose to limit ourselves to methods that are\ndirectly applied to the representations.\nAside from quantitative or qualitative benefits, the construc-\ntion of semantic RSMs is time-consuming, limiting its applicability. This complexity mostly\naffects layers of large spatial extent, which mostly corresponds to early CNN layers while later\nlayers and ViTs are unproblematic. Our proposed Batch-optimal approximation alleviates\nthis partially, yet application to large-scale representations at higher resolution, like at the\noutput of a segmentation architecture with spatial extents of s=65.536 would be too costly.\nWe leave optimizing the compute efficiency or finding better approximations for future\nwork.\nIn conclusion, our investigation into semantic RSMs has shed light on the\nlimitations of spatio-semantic RSMs and introduced a novel approach to disentangle spatial\nalignment from semantic similarity. The proposed method provides a more accurate measure\nof how representations capture underlying semantic content, showcasing its potential in\nvarious applications, particularly in scenarios where spatial alignment cannot be assumed.\nWhile challenges such as computational complexity and scalability need to be addressed,\nthe findings open avenues for further research and improvement in the analysis of neural\nnetwork representations."}, {"title": "Kernel function definitions", "content": "Defining the notion of similarity between two vectors is a matter of preference and wanted\nproperties. In this work, we include the Radial Basis Function kernel Eq. (6) and the linear\nkernel Eq. (7), as well as the cosine similarity kernel Eq. (8). We include the former two as they\nwere proposed for RSM construction by Kornblith et al. [12] and the cosine similarity due to\nits popularity in the retrieval domain, despite generally being applied to class probabilities.\nWe denote that in our manuscript we refer to the linear kernel as the inner product and dot\nproduct interchangeably as they correspond to the same operation.\n$K_{RBF} (x, y) = exp(-\\frac{||x - y||^2}{2\\sigma^2})$ \n$k_{linear} (x, y) = \\langle x, y \\rangle$\n$k_{cosine} (x, y) = \\frac{\\langle x, y \\rangle}{||x|| ||y||}$\nThe three selected kernels all have different properties: The RBF kernel\nand the Cosine similarity are bounded between $K_{RBF}(x, y), k_{cosine}(x, y) \\in [0,1] \\forall x,y \\in \\mathbb{R}$,\nwhile the $k_{linear}(x, y)$ is not bounded.\nMoreover, the RBF kernel is parametrized by o, which influences at which rate the distance\nbetween the representations results in a decrease in similarity. For all our experiments\nwe choose o as the square root of the median Euclidean distance of all distances within a\nmini-batch."}, {"title": "Translation Sensitivity of a ViT-B/16", "content": "Similarly to the translated Tiny-ImageNet experiment, we prepared a very similar experiment\nfor a ViT-B/16 vision transformer that was pre-trained on ImageNet1k. We utilize the\nimplementation and weights provided by torchvision [17].\nGiven the larger ImageNet images, we resample them to 324 \u00d7 324 pixels and crop two\npartially overlapping images of size 224 \u00d7 224 from it. Given these image pairs, we calculate\nsemantic and spatio-semantic RSMs again, see Fig. 7.\nIt is important to note that our approach intentionally avoids achieving a perfect translation\nthat would lead to the same patchified tokens, as we shift the image by a factor that is not\ndivisible by 16, the patching window size. We believe this realistic imperfection is preferable\nto a perfect overlap, where identical tokens would be formed from the exact same set of\npixels.\nSimilarly to the previous partially overlapping crop experiment, we can observe that\nspatio-semantic RSMs are incapable of identifying the largely identical content of the two\npartially overlapping crops due to their different localization. The spatio-semantic RSMs\ncan capture this notion of similarity as evidenced by the off-diagonal and the larger gap\nin the distribution of similarity between partially overlapping samples and independent\nsamples, Fig. 7 (bottom).\nContrary to the CNN example in the main, overall similarity between samples is much lower\noverall and separation between translated image pairs and random image pairs follows a\nvastly different trajectory. While there is a profound difference, the origin of this difference\ncannot be clearly made out. We hypothesize that this may be due to the different ways that\nTransformers process information and learn different representations, as highlighted in\nRaghu et al. [26].\nMoreover, we emphasize, that calculating the optimal permutation is significantly faster\nfor ViTs than the CNNs, as the early tokenization reduces the spatial dimension S substan-\ntially at an early stage, whereas the iterative downsampling of CNNs makes comparing\nrepresentations of early layers very costly."}, {"title": "Pseudo Code", "content": "In addition to our provided explanation of the algorithm in the main manuscript, we\nprovide the pseudo-code used to compute semantic RSMs in Algorithm 1. The only\ndifference between semantic and spatio-semantic RSMs algorithmically is where the optimal\npermutation is calculated that maximally aligns the two representations. The current\ndefinition of spatio-semantic RSMs assumes that spatial locations are corresponding, while\nsemantic RSMs calculate correspondence through similarity matching."}, {"title": "Additions: Retrieval Experiment", "content": "In addition to the provided retrieval examples in the main manuscript, we provide more\ndetails on the retrieval experiments in Appendix D.1, an additional table holding the\nquantitative data of Fig. 3 with varying database sizes in Appendix D.2 and lastly, additional\nqualitative retrieval examples for each model including direct comparisons of all models for\nthe same query image in Appendix D.3."}, {"title": "Details of Retrieval Experiment", "content": "For the retrieval experiment, we utilize the EgoObjects dataset [35]. It contains multiple\nframes from multiple videos, with multiple videos capturing the same scene under different\nshifts like lighting conditions, distances, viewing angles, and different motion trajectories.\nFor each frame, multiple objects of different categories can be present and are annotated"}, {"title": "Additional Quantitative Retrieval data", "content": "In addition to the results highlighted in Fig. 3 we provide retrieval results for varying\ndatabase sizes to retrieve from EgoObjects. Specifically, results for database sizes of 2.5k, 5k,\nand 10k samples are given in Table 2."}, {"title": "Details: Output similarity vs Representational Similarity", "content": "To measure the correlation between the inter-sample representational similarity and the\nprediction probability inter-sample similarity we utilize pre-trained classifiers and the\nImageNet1k [2] dataset. Unlike during the retrieval results this constraints the possible\nmodel selections to models trained for classification.\nTest set images of ImageNetlk are randomly sampled without\napplying any filtering to them. In total, we utilize a subset of 2k ImageNet test set samples in\nthis experiment. This may appear small, yet provides a sufficient basis as the combinatoric\ngrowth increases the absolute number of measurements substantially.\nAs mentioned in the main manuscript we\nuse\n1. ResNet18 [7]\n2. ResNet50 [7]\n3. ResNet101 [7]\n4. a DinoV2-Giant based classifier [22]\n5. ConvNeXt V2 [33]\n6. ViT-B/16 [4] and\n7. ViT-L/32 [4] and\nas pre-trained classifiers for predicting the ImageNet1k classes.5. For each sample, we extract\nthe last hidden layer's representations and center them analog to before. For the same\nsample, we extract the logits and obtain the probability distribution through the softmax,\nsaving the pair for later comparisons.\nFor each pair of representations and probabilities, we calculate\nthe similarities between their representations for all three kernel functions, once permutation\ninvariant and once not. Additionally, we calculate the Jensen-Shannon Divergence (JSD)"}, {"title": "Additional correlation results", "content": "In addition to the Pearson correlation between the Jensen-Shannon-Divergence (JSD) and\ninter-image similarity, we also present the results of their relationship measured by the\nSpearman correlation, as shown in Table 4.\nWhile the Pearson correlation demonstrated consistently stronger correlations with cosine\nsimilarity, the inner product, and the RBF kernel, the Spearman rank correlations are less\nstable across these methods.\nFor ResNets, we observe a significant decline in correlation consistency and strength with\nthe exception of ResNet18. Opposed to this, ViTs display notably higher negative correlation\nvalues when using cosine similarity and radial basis function kernels, in contrast to the\nPearson correlation results."}, {"title": "Details of the Approximation Algorithms", "content": "The computational complexity of determining optimal matchings using the Jonker-Volgenant\nalgorithm [8] scales significantly with $O(s^3)$, resulting in substantial computation time for\ninput-patch sizes with spatial dimensions s = 642. To address this challenge, we propose\nalternative approximate algorithms with reduced computational complexity. In all our\napproximations, we take advantage of additional information, specifically the L2-norm\n||vi||2 of each semantic concept vector. We assume that achieving a high degree of matching\ninvolves pairing vectors with the highest norms and high cosine similarity. This assumption\nguides our design of more efficient matching algorithms."}, {"title": "Greedy", "content": "The simplest approach we employ is breadth-first matching. We determine\nthe order in which to match vi by considering the L2-norm ||vi||2 in descending\norder. We then match the current vi with the best, non-assigned vj based on Aij.\nThe sorting complexity is O(s log(s)), making this the fastest approximate algorithm\namong those tested."}, {"title": "TopK-Greedy", "content": "Recognizing that the TopK norm concept vectors ||vi||2 might have a\nsignificantly higher impact on the final similarity, we attempt to find the optimal\nmatching for only the highest TopK norm concept vectors vi and vj. The remaining\nlower norm concept vectors are assigned using the Greedy algorithm as described\nabove. The process involves an initial sorting based on the semantic concept vectors'\nL2-norm, followed by optimal matching with O(k\u00b3) complexity for the k TopK\nvalues and the greedy matching for the remaining values."}, {"title": "Batch-Optimal", "content": "If the TopK norm concept vectors do not sufficiently approximate\nan optimal matching, we apply optimal matching for the remaining concept vectors\nin batches. To achieve this, we create s//b smaller batches, with semantic concept\nvectors assigned to batches according to their L2-norm. All values within a batch\nare then optimally matched, leading to a matching complexity of (\u17d6) \u00b7 O(b3).6\nEvaluating the various approximations, we observe that the Greedy matching yields\nsuboptimal approximation quality and offers marginal to no improvement over the current\nsame-position assignment. Although we do not present the details of the greedy matching,\nit is important to highlight that it is guaranteed to be worse or equal to the TopK-Greedy\nmatching with a k value of 128, as shown in Fig. 5. We include the Greedy algorithm for\ncompleteness as a simple baseline.\nFurthermore, it is noteworthy that the TopK-Greedy matching demonstrates that exclusively\nmatching the largest norm concept vectors is insufficient for a good approximation of the\noptimal matching. This insight suggests that a substantial portion of the overall similarity is\ncontributed by semantic concept vectors not included in the set of highest norms.\nLastly, we observe that the Batch-Optimal approximation, using a small batch size of 128\nsamples, provides an approximation with less than 10% error compared to the optimal\nmatching. This result underscores the effectiveness of our batching approach based on the\nL2-norm of the concept vectors. It offers a reliable estimate for overall similarity, simplifying\nthe matching process significantly."}, {"title": "Runtime Evaluation", "content": "In order to assess algorithm performance across different spatial resolutions, we conducted\na benchmarking study. For each resolution, we randomly selected 10,000 pairs of samples\nfrom a ResNet101 trained on Tiny-ImageNet. Affinity matrices (Aij) were pre-computed\nto facilitate permutation (Pij) calculations. The average time taken per matching was\nthen reported for the same single CPU core, as outlined in Table 5. We observe that the\nOR-Tools implementation outperforms other alternatives, being four times faster than the\nlapjv implementation 7. However, even this optimal approach requires 1.52 seconds per\npair on a 64 \u00d7 64 resolution. Despite the potential for parallelizing sample-wise matching,\noptimal algorithms face scalability challenges with larger spatial dimensions S. In contrast,\nthe Batch-Optimal approximation offers a compelling balance between computation time\nand approximation quality. Importantly, its complexity scales linearly with S due to the\nfixed batch size."}, {"title": "Semantic RSMs and CKA \u2013 Qualitative changes", "content": "Building upon the success of Linear/RBF (spatio-semantic) CKA to compare systems, we\nprovide some preliminary qualitative comparisons gauging how CKA comparisons are\naffected by our differently proposed RSM. Unfortunately, hardly any quantitative benchmark"}, {"title": "Differences in CKA self-similarity", "content": "In the previous paragraph, semantic RSMs were directly compared to spatio-semantic RSMs.\nWhile this can influence measurements when models are compared across domains (e.g.\nCNN Vision to Biological Vision), it does not need to imply that the CKA similarity of vision\nmodels changes substantially.\nGiven that within the same domain, the RSM construction will likely be chosen consistently,\none can opt to either: Calculate only spatio-semantic RSMs or only semantic RSMs, due\nto personal opinions or preferences. Subsequently, the question is not \u201cAre spatio-semantic\nRSMs similar to semantic RSMs\u201d, but \u201cDoes the CKA similarity between spatio-semantic RSMs\nchange when calculating semantic RSMs\u201d.\nTo address this question we: A) Compare the CKA matrix difference of the CKA matrix based\non semantic RSMs to the CKA matrix of spatio-semantic RSMs when comparing a model\nwith itself (Intra Model) and B) when comparing between different models (Cross-Model).\nSimilarly to before, we extract representations and form semantic and\nspatio-semantic RSMs. We extract representations on CIFAR100 [14] (32\u00d732), Tiny-ImageNet\n(64 \u00d7 64) and ImageNet-1k [2] (160 \u00d7 160) datasets, from 3 differently seeded and trained from\nscratch ResNet18, ResNet34 and ResNet101 architectures. The semantic RSMs are calculated\nutilizing the Batch \u2013 Optimal matching with b 512 for matching on Tiny-ImageNet and\nImageNet. We calculate semantic and spatio-semantic RSMs with a mini-batch size of\n250, subsequently using them for Canonical Correlation Analysis (CKA) calculations. The\ncorresponding cka matrices and their differences are displayed in Fig. 16. If not further\nspecified the experiment uses the linear kernel."}]}