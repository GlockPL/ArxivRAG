{"title": "R-LoRA: Random Initialization of Multi-Head LORA\nfor Multi-Task Learning", "authors": ["Jinda Liu", "Yi Chang", "Yuan Wu"], "abstract": "Fine-tuning large language models (LLMs) is\nprohibitively expensive in terms of computa-\ntional and memory costs. Low-rank Adap-\ntation (LoRA), as one of the most popular\nparameter-efficient fine-tuning (PEFT) meth-\nods, offers a cost-effective alternative by ap-\nproximating the model changes \u2206W \u2208 Rm\u00d7n\nthrough the product of down-projection ma-\ntrix A \u2208 Rmxr and head matrix B \u2208 Rr\u00d7n,\nwhere r < min(m, n). In real-world scenar-\nios, LLMs are fine-tuned on data from multiple\ndomains to perform tasks across various fields,\nembodying multi-task learning (MTL). LoRA\noften underperforms in such complex scenar-\nios. To enhance LoRA's capability in multi-task\nlearning, we propose R-LORA, which incorpo-\nrates Multi-Head Randomization. Multi-Head\nRandomization diversifies the head matrices\nthrough Multi-Head Random Initialization and\nMulti-Head Dropout, enabling more efficient\nlearning of task-specific features while main-\ntaining shared knowledge representation. Ex-\ntensive experiments demonstrate that R-LoRA\nis better at capturing task-specific knowledge,\nthereby improving performance in multi-task\nscenarios. The code is available at https:\n//github.com/jinda-liu/R-LoRA.", "sections": [{"title": "1 Introduction", "content": "In recent years, large language models (LLMs)\nhave manifested unprecedentedly superior per-\nformance in various natural language processing\n(NLP) tasks (Brown, 2020; Zhao et al., 2023;\nChang et al., 2024). Due to its impressive capa-\nbilities in language understanding and generation,\nLLMs have gained extensive interest from both\nacademia and industry. Despite their high general-\nizability, LLMs still require fine-tuning for specific\ndomains or updating the knowledge base (Agiza\net al., 2024; Xin et al., 2024).\nSupervised fine-tuning (SFT) is crucial for align-\ning large language models (LLMs) with human\ninstructions, which trains the model with a small\nyet high-quality set of labeled data (Hu et al., 2021;\nXia et al., 2024). The vast number of parameters\nin LLMs poses significant challenges regarding\ncomputational efficiency and memory consump-\ntion during full fine-tuning (FT), which updates all\nparameters.\nTo address the issue of hardware requirements\nfor LLM adaptation, a solution called parameter ef-\nficient fine-tuning (PEFT) has been proposed (Han\net al., 2024). PEFT methods reduce VRAM us-\nage of cached optimizer states by only optimiz-\ning a fraction of model parameters while keeping\nthe rest frozen. Various PEFT methods, such as\nprefix-tuning(Li and Liang, 2021), p-tuning(Liu\net al., 2024c), IA\u00b3(Liu et al., 2022) and Low-rank\nadaption(LoRA)(Hu et al., 2021), have been widely\nstudied. Among these methods, LoRA has emerged\nas the mainstream alternative to full parameter fine-\ntuning. Instead of updating the original parameter\nmatrix directly, LoRA approximates the updated\nparameters using the product of two smaller matri-\nces. During inference, the output obtained from the\noriginal parameter matrix is combined with the out-\nput from the updated parameter matrices. However,\nLORA does not perform well in multi-task scenar-\nios, particularly in dealing with complex datasets.\nRecent LoRA variants have improved multi-task\nlearning by employing multiple LoRA adapters,\nincluding Multi-LoRA (Wang et al., 2023), LoRA-\nMoE (Dou et al., 2023), and MoeLoRA (Liu et al.,\n2024a). We refer to this extended framework as\nthe Multi-Adapter LoRA architecture, which con-\nsists of multiple down-projection matrices (A) and\ntheir corresponding head matrices (B), enabling\ntask-specific adaptation through diverse parameter\nsets. Notably, LoRA-MoE and MoeLoRA further\nenhance this architecture by introducing a Mix-\nture of Experts (MoE) mechanism to aggregate"}, {"title": "", "content": "adapter outputs. Tian et al. (2024) observes that\nin the Multi-Adapter LoRA architecture, the pa-\nrameters of the down-projection matrices A are\nrelatively consistent, while the differences between\nthe head matrices B are more pronounced, which\naids in capturing task-specific knowledge. To lever-\nage this property, HydraLoRA (Tian et al., 2024)\nis proposed to feature an asymmetric architecture\nwith one shared down-projection matrix A and mul-\ntiple task-specific head matrices B. Additionally,\nHydraLoRA also employs an MoE mechanism to\naggregate the outputs of the head matrices. This\ndesign achieves a good balance between training\nperformance and parameter efficiency. The math-\nematical formalization of HydraLoRA is detailed\nin Section 2.2. In this work, we propose R-LoRA,\nwhich adopts HydraLoRA's asymmetric architec-\nture, explicitly defining it as a Multi-Head struc-\nture, and introduce Multi-Head randomization to\nimprove LLMs' performance on multi-task learn-\ning. Figure 1 illustrates the differences among the\naforementioned structures.\nHowever, in the Multi-Head architecture, the pa-\nrameter similarity among head matrices remains\nhigh, hindering task-specific knowledge learning\nand slowing convergence speed. This is due to\nzero initialization of head matrices B, leading to\nsimilar update directions. To address this, we use\nmulti-head randomization in R-LoRA, combining"}, {"title": "2 Related Works", "content": "2.1 LORA\nCurrent LLMs generally follow a decoder-only\nstructure, characterized by a series of blocks, each\ncomprising two key components with residual con-\nnections: a multi-head self-attention (MHA) layer\nand a feed-forward network (FFN) (Vaswani, 2017).\nThese layers involve using dense learnable matri-\nces.\nThere is a need to adapt LLMs for specific tasks\nor domains with limited resources. To achieve this,\nlow-rank adaptation (LoRA) (Hu et al., 2021), in-\nspired by the concept of low intrinsic dimension-\nality in LLMs, decomposes the weight gradient\nAW into low-rank matrices, thereby reducing the\nnumber of trainable parameters. Specifically, for a"}, {"title": "", "content": "dense weight matrix W \u2208 Rm\u00d7n, LoRA employs\ntwo low-rank matrices, B \u2208 Rmxr and A \u2208 Rr\u00d7n,\nto approximate the accumulated gradient updates\nAW. The rank r is chosen to be much smaller than\nthe minimum of d and k, effectively decreasing the\nnumber of trainable parameters. Consequently, the\nresulting weight matrix is expressed as W + \u0412\u0410,\nand the output h for an input x through this updated\nweight matrix is formulated as:\nh = (W + \u2206W)x = Wx + BAx\nNormally matrix B is initialized with zeroes and\nmatrix A is initialized with Kaiming Uniform (He\net al., 2015) to ensure that the initial outputs are\nconsistent with the pre-trained model, thereby\navoiding the introduction of random disturbances.\nFollowing LoRA, AdaLoRA (Zhang et al., 2023)\ndynamically learns the rank size needed for LORA"}, {"title": "2.2 Multi-Head architecture", "content": "MTL-LORA (Yang et al., 2024) and Hy-\ndraLoRA (Tian et al., 2024) are pioneering meth-"}, {"title": "", "content": "ods that introduce the multi-head architecture into\nLORA. This architecture is characterized by a cen-\ntral shared down-projection matrix A and multi-\nple distinct head matrices B, enabling efficient\nand flexible adaptation across diverse tasks. As\nshown in Figure 1, this architecture differentiates\ntask-specific information while effectively captur-\ning shared knowledge across various tasks. The\nMulti-Head architecture can be formulated as:\nW+AW = W +\nN\n\u03a3\ni=1\nWi BiA\nIn HydraLoRA (Tian et al., 2024), the weights\nwi are computed through the routing matrix Wr\nand the softmax function. It can be formulated as:\nw = Softmax(Wrx)\nNormal routing matrix is initialized with Kaiming\nUniform (He et al., 2015). R-LoRA retains the\nsame architecture as HydraLoRA, ensuring consis-\ntency in the routing mechanism and weight compu-\ntation."}, {"title": "2.3 Dropout", "content": "Dropout is a widely used technique to prevent over-\nfitting in deep networks by randomly deactivating\nunits during training (Srivastava et al., 2014). This\nprocess samples from an exponential number of\nthinned networks, reducing unit co-adaptation and\nenhancing noise robustness. At test time, the full\nnetwork is utilized, benefiting from the ensemble\neffect of the thinned networks. In our work, we\nadapt dropout to a novel context within the multi-\nhead structure of R-LoRA. Specifically, we employ\ndropout to differentiate the inputs of the head ma-\ntrices, ensuring that each head learns distinct and\ncomplementary representations."}, {"title": "3 Motivation", "content": "In this section, we analyze the parameter similarity\nbetween different head matrices in the Multi-Head\nLORA architecture. To achieve our objectives, we\nfocus on HydraLoRA (Tian et al., 2024) and use\ncosine similarity and the T-SNE method to observe\nthe parameters of the head matrices. We fine-tune\nQwen2.5-3B-Base (Qwen Team, 2024) with Hy-\ndraLoRA (Tian et al., 2024) on five different tasks:\nParaphrase Detection (QQP), Natural Language In-\nference (QNLI) (Wang, 2018), Commonsense Rea-\nsoning (SIQA) (Sap et al., 2019), Physical Com-\nmonsense Reasoning (PIQA) (Bisk et al., 2020),"}, {"title": "4 Method", "content": "In this work, we propose R-LoRA, which leverages\nmulti-head randomization to assist the model in\nlearning distinct knowledge. Multi-head random-\nization consists of two components: multi-head\ndropout and random initialization. An overview of\nR-LORA is illustrated in Figure 4\nReserach Objective: To exploit randomization to\ndifferentiate the head matrices, thereby facilitating"}, {"title": "", "content": "the convergence of their parameters to distinct re-\ngions and enhancing the diversity among the head\nmatrices."}, {"title": "4.1 Multi-Head Dropout", "content": "Multi-Head LoRA architecture is characterized by\na shared down-projection matrix A and several\ndistinct head matrices B. In HydraLoRA (Tian\net al., 2024), the head matrices receive the same\noutput from the shared matrix A. According to\n(Hayou et al., 2024) and (Tian et al., 2024), the\ndown-projection matrix A and the head matrix B\nin LoRA play distinct roles. We hypothesize that\nthe down-projection matrix A is more inclined to\nlearn task-agnostic knowledge, capturing general\nfeatures applicable across tasks, while the head ma-\ntrices tend to specialize in task-specific knowledge,\nenabling the model to differentiate and adapt to\nthe unique requirements of individual tasks. This\ndivision of roles enhances the model's ability to\nbalance generalization and specialization in multi-\ntask learning scenarios. We propose employing\nmulti-head dropout to differentiate the outputs of\ndown-projection matrix A, thereby ensuring that\nthe head matrices produce distinct outputs. The\nframework of Multi-Head dropout and R-LORA is\nshown in Figure 4. Our architecture is similar to\nHydraLoRA (Tian et al., 2024), but it introduces\nmulti-head dropout. The input, after being pro-\ncessed by the down-projection matrix A, obtains a\ntask-agnostic representation. Multi-head dropout\ndiversifies this representation, enabling the model\nto learn task-specific knowledge from multiple per-\nspectives, enhancing both generalization and task\nadaptability."}, {"title": "4.2 Multi-Head Random Initialization", "content": "The zero initialization of the head matrices results\nin identical starting points for the different head\nmatrices during training, causing them to converge\nto similar positions. As shown in Table 1, we uti-\nlize non-zero initialization for the head matrices to\nprovide them with distinct starting points during"}, {"title": "5 Experiment", "content": "In this section, we validate the superiority of R-\nLORA across various models and settings. First, we\nfollowed the settings of (Tian et al., 2024) and con-\nducted experiments on the LLaMA-2 model (Tou-\nvron et al., 2023), evaluating both single-task and\nmulti-task scenarios. Subsequently, we tested the\nperformance of R-LoRA under different multi-task\nsettings on the new Qwen2.5 (Qwen Team, 2024).\nThe model sizes range from 0.5B to 13B. Through\nan extensive ablation study, we demonstrate the\neffectiveness of the multi-head randomization in\nR-LORA."}, {"title": "5.1 Experiment Setting", "content": "Model: In the single-task setting, we use LLaMA2-\n7B, while in the multi-task setting, we additionally\nincorporated LLaMA2-13B. In the ablation study,\nwe use Qwen2.5-0.5B and Qwen2.5-3B models.\nDataset & Benchmarks:\nSingle-task:"}, {"title": "5.2 Performance", "content": "5.2.1 Performance of R-LoRA on Single Task\nAs shown in Table 2, in the single-task setting,\nwhere the knowledge and text format of the data\nare relatively homogeneous, multi-head random-\nization does not yield significant performance im-\nprovements. Nevertheless, R-LORA achieves per-\nformance on par with HydraLoRA, demonstrat-\ning that the multi-head randomization mechanism\npreserves learning effectiveness while maintaining\nstability for single-task scenarios. This highlights\nR-LORA's robustness and adaptability, even in set-\ntings where its full potential may not be fully uti-\nlized."}, {"title": "5.2.2 Performance of R-LoRA on Multi-Tasks", "content": "The evaluation across diverse tasks, as shown in\nTable 3, demonstrates that R-LoRA, building upon\nthe foundation of HydraLoRA, consistently out-\nperforms all other schemes. By introducing multi-\nhead dropout and random initialization for the head\nmatrices, R-LORA further enhances the model's\nstability and adaptability. The performance gains\nof R-LORA, rooted in these multi-head randomiza-\ntion techniques, surpass those of both conventional\nPEFT methodologies and HydraLoRA."}, {"title": "5.3 Parameter Analysis", "content": "Reserach Question2: Does multi-head randomiza-\ntion effectively enhance the acquisition of diverse\nknowledge across the head matrices?\nIn this section, we analyze the parameter differ-\nences among the head matrices in R-LoRA. The\nmethodology and experimental setup align with\nthose described in Section 3. As shown in Figure 5,\nthe parameter similarity between head matrices in\nR-LoRA is reduced to below 70%. This signifi-\ncant decrease indicates that multi-head randomiza-\ntion effectively enhances the model's capacity to\nlearn task-specific knowledge, thereby mitigating\nredundant learning and increasing the diversity of\nacquired knowledge across tasks. T-SNE analysis\nwill be shown in appendix C"}, {"title": "5.4 Training Process", "content": "Reserach Question3: Does multi-head randomiza-\ntion impact the stability of the training process?\nAs illustrated in Figure 6, R-LORA benefits from\nmulti-head randomization, exhibiting significantly\nlarger gradient norms in the early stages of train-\ning compared to HydraLoRA. This drives the head\nmatrices to converge to distinct regions, enhancing\nthe model's ability to capture diverse representa-\ntions and improving overall performance. Further-\nmore, R-LORA demonstrates greater training sta-\nbility than HydraLoRA, as evidenced by its more\nstable gradient norms throughout the training pro-\ncess. This stability enables the model to effectively\nacquire diverse knowledge without compromising\ntraining efficiency."}, {"title": "5.5 Ablation Study", "content": "In this section, we empirically validate the effective-\nness of R-LoRA's multi-head randomization com-\nponents through extensive experiments. Ablation\nstudies were conducted on two models, Qwen2.5-\n0.5B and Qwen2.5-3B, under two task settings:\n5-task and 8-task configurations. For the 5-task set-"}, {"title": "6 Conclusion", "content": "In this work, we first investigated the multi-head\nstructure of LoRA and analyzed the parameters\nof the head matrices, revealing that they remain"}, {"title": "", "content": "highly similar. To address this, we proposed R-\nLoRA, which introduces multi-head randomiza-\ntion-a simple yet effective approach to enable\nthe model to learn knowledge from different tasks,\nthereby enhancing its performance in multi-task\nscenarios. This method not only improves the\nmodel's generalization capabilities but also sup-\nports its adaptability across diverse tasks. Ex-\ntensive experiments have validated the superior-\nity of R-LORA. Parameter analysis demonstrates\nthat multi-head randomization effectively differen-\ntiates the head matrices, enabling them to learn\nknowledge from distinct tasks. This capability sig-\nnificantly enhances the model's performance in\nmulti-task scenarios, confirming the effectiveness\nof the proposed approach."}, {"title": "7 Limitation", "content": "Despite the promising results of R-LoRA, sev-\neral limitations should be acknowledged. While\nempirical evidence supports the effectiveness of\nmulti-head randomization, a rigorous theoretical\nanalysis of its underlying mechanisms remains\nabsent. Additionally, multi-head random initial-\nization does not ensure consistency with the pre-\ntrained model's outputs, potentially introducing\nrandom disturbances. Future work could explore\ndata-driven initialization as a promising approach\nto enhance the learning of task-specific knowledge\nby the head matrices, a direction we intend to pur-\nsue further."}, {"title": "A Datasets", "content": "A.1 Single-task\n1. General:\nWe fine-tune with the\ngeneral instruction tuning dataset\ndatabricks-dolly-15k for generic\nlanguage capability and evaluate with\nMMLU.\n2. Medical: We fine-tune with GenMedGPT and\nclinic-10k from ChatDoctor for medicine\napplications and evaluate medical tasks in\nMMLU including three related tasks: \"clini-\ncal knowledge\", \"professional medicine\", and\n\"college medicine\".\n3. Law: We fine-tune with two legal instruc-\ntion tuning datasets Lawyer-Instruct and\nUS-Terms then evaluate with law tasks in\nMMLU including two related tasks: \"profes-\nsional law\" and \"international law\".\n4. Math: We fine-tune with the training split of\nGSM8K for mathematical reasoning and evalu-\nate with the test set of GSM8K.\n5. Code: We fine-tune with CodeAlpaca\nfor code generation and evaluate with\nHumanEval."}, {"title": "A.2 Multi-task", "content": "For complex mixed multi-task/domain, we select\na portion of the Flanv2 datasets covering Natural\nLanguage Understanding (NLU) and Natural Lan-\nguage Generation (NLG), which can be grouped\ninto 10 distinct task clusters. Then we evaluate it\nwith the Big-Bench Hard (BBH) benchmark.\nWe summarize the details of the used datasets as\nfollows:\n1. Struct-to-Text Conversion: This task eval-\nuates the capability to generate natural lan-\nguage descriptions from structured data inputs.\nWe use the following datasets: (1) Common-\nGen; (2) DART; (3) E2ENLG; (4) WebNLG\n2. Translation: Translation involves convert-\ning text from one language to another, main-\ntaining the original meaning and nuances.\nWe use the following datasets: (1) En-Fr\nfrom WMT'14; (2) En-De, En-Tr, En-Ru, En-\nFi, En-Ro from WMT'16; (3) En-Es from\nParacrawl."}, {"title": "", "content": "3. Commonsense Reasoning: This involves as-\nsessing the ability to apply physical or scien-\ntific principles alongside common sense in rea-\nsoning tasks. We use the following datasets:\n(1) COPA; (2) HellaSwag; (3) PiQA; (4) Sto-\nryCloze.\n4. Sentiment Analysis: A fundamental task in\nnatural language processing (NLP) that de-\ntermines the sentiment polarity (positive or\nnegative) of a given text. We use the follow-\ning datasets: (1) IMDB; (2) Sentiment140; (3)\nSST-2; (4) Yelp.\n5. Paraphrase Detection: This task requires\nmodels to ascertain whether two sentences\nconvey the same meaning, indicating seman-\ntic equivalence. We use the following datasets:\n(1) MRPC; (2) QQP; (3) Paws Wiki.\n6. Coreference Resolution: Involves identify-\ning instances within a text that refer to the\nsame entity, demonstrating an understanding\nof textual context. We use the following\ndatasets: (1) DPR; (2) WSC273.\n7. Reading Comprehension: Assesses the ca-\npability to derive answers to questions from\na provided text containing relevant informa-\ntion. We use the following datasets: (1)\nBoolQ; (2) DROP; (3) MultiRC; (4) OBQA;\n(5) SQUADv1; (6) SQuADv2.\n8. Reading Comprehension with Common-\nsense: Merges traditional reading compre-\nhension skills with commonsense reasoning,\nrequiring understanding beyond the explicit\ntext. We use the following datasets: (1) Cos-\nmosQA; (2) ReCoRD.\n9. Natural Language Inference: Focuses on\ndeducing the relationship between two sen-\ntences, determining if the second sentence\nlogically follows from, contradicts, or is unre-\nlated to the first sentence. We use the follow-\ning datasets: (1) ANLI; (2) CB; (3) MNLI; (4)\nQNLI; (5) SNLI; (6) WNLI; (7) RTE.\n10. Closed-Book Question Answering: This\ntask challenges models to answer questions\nabout general knowledge without direct ac-\ncess to external information sources. We use\nthe following datasets: (1) ARC; (2) NQ; (3)\nTriviaQA."}, {"title": "A.3 Ablation Study", "content": "Due to limited computational resources, we se-\nlected a subset of the dataset for training and testing.\nFive tasks:\n\u2022 Task 1: Sentiment Analysis (SST2)\n\u2022 Task 2: Paraphrase Detection (QQP)\n\u2022 Task 3: Natural Language Inference (QNLI)\n\u2022 Task 4: Physical Commonsense Reasoning\n(PIQA)\n\u2022 Task 5: Commonsense Reasoning (SiQA)\nEight tasks:\n\u2022 Task 1: Sentiment Analysis (SST2)\n\u2022 Task 2: Paraphrase Detection (QQP)\n\u2022 Task 3: Natural Language Inference (MNLI +\nQNLI)\n\u2022 Task 4: Reading Comprehension (BoolQ +\nOBQA)\n\u2022 Task 5: Commonsense Reasoning (PiQA +\nSiQA)\n\u2022 Task 6: Reading Comprehension with Com-\nmonsense (CosmosQA)\n\u2022 Task 7: Coreference Resolution (SiQA)\n\u2022 Task 8: Closed-Book Question Answering\n(ARC)"}, {"title": "B Baselines", "content": "1. Prompt Tuning: This method adds task-\nspecific prompts to the input. These prompt\nparameters are updated independently while\nthe pretrained model parameters remain\nfrozen.\n2. P-Tuning: This method incorporates trainable\nprompt embeddings into the input, optimized\nby a prompt encoder to automatically discover\neffective prompts, removing the need for man-\nual design. Prompt tokens can be placed any-\nwhere in the input sequence, and anchor to-\nkens are introduced to enhance performance."}, {"title": "", "content": "3. Prefix Tuning: This method prefixes a series\nof task-specific vectors to the input sequence.\nThese prefix parameters can be learned while\nkeeping the pretrained model frozen. The pre-\nfix parameters are inserted into all layers of\nthe model.\n4. IA\u00b3: This method enhances efficiency by in-\nfusing learned vectors into transformer archi-\ntectures, drastically reducing the number of\ntrainable parameters.\n5. AdaLoRA: Unlike LoRA, which distributes\nparameters evenly across all modules,\nAdaLoRA optimizes the number of trainable\nparameters assigned to weight matrices and\nlayers. More parameters are allocated to\nimportant weight matrices and layers, while\nless important ones receive fewer parameters.\n6. LoraHub randomly aggregates 20 LoRAs for\nnew downstream tasks. It employs a black-\nbox optimization technique to determine the\nweight of each LoRA, eliminating the need for\ngradient calculations of the large model. This\ninvolves parameter-level weighted averaging.\n7. LoRA MoE. A collection of n parameterized experts, denoted as E1,..., En, is or-\nchestrated by a router network R. Ei = BiAi.\nRouter network features a dense layer with\nadjustable weights WR from Rdm\u00d7n. A softmax function then processes an intermediate\ntoken representation x, yielding gating scores\ns1,..., Sn that determine the weighted contri-\nbution of each expert's output:\nsi = R(x)i = softmax(Top(Wx, K))\nSubsequently, the overall output y is synthe-\nsized by aggregating the Top-K experts' out-\nputs, each modulated by its respective gating\nscore:\ny = \u2211 \u03a3 Si Ei(x)\ni=1\nThis results in a dynamic allocation of the\nmodel's capacity, enabling specialized pro-\ncessing by experts as directed by the router's\ngating mechanism.\n8. HydraLoRA uses a shared matrix A and mul-\ntiple matrices B\u2081, . . ., Bn. The shared matrix"}, {"title": "C More Results", "content": "The T-SNE analysis of R-LORA has been shown in\nFigure 7, 8, 9"}]}