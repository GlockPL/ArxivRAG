{"title": "LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction", "authors": ["Aishik Nagar", "Viktor Schlegel", "Thanh-Tung Nguyen", "Hao Li", "Yuping Wu", "Kuluhan Binici", "Stefan Winkler"], "abstract": "Large Language Models (LLMs) are increasingly adopted for\napplications in healthcare, reaching the performance of do-\nmain experts on tasks such as question answering and doc-\nment summarisation. Despite their success on these tasks,\nit is unclear how well LLMs perform on tasks that are tra-\nditionally pursued in the biomedical domain, such as struc-\ntured information extration. To breach this gap, in this paper,\nwe systematically benchmark LLM performance in Medical\nClassification and Named Entity Recognition (NER) tasks.\nWe aim to disentangle the contribution of different factors\nto the performance, particularly the impact of LLMs' task\nknowledge and reasoning capabilities, their (parametric) do-\nmain knowledge, and addition of external knowledge. To this\nend we evaluate various open LLMs-including BioMistral\nand Llama-2 models-on a diverse set of biomedical datasets,\nusing standard prompting, Chain-of-Thought (CoT) and Self-\nConsistency based reasoning as well as Retrieval-Augmented\nGeneration (RAG) with PubMed and Wikipedia corpora.\nCounter-intuitively, our results reveal that standard prompt-\ning consistently outperforms more complex techniques across\nboth tasks, laying bare the limitations in the current applica-\ntion of CoT, self-consistency and RAG in the biomedical do-\nmain. Our findings suggest that advanced prompting methods\ndeveloped for knowledge- or reasoning-intensive tasks, such\nas CoT or RAG, are not easily portable to biomedical tasks\nwhere precise structured outputs are required. This highlights\nthe need for more effective integration of external knowledge\nand reasoning mechanisms in LLMs to enhance their perfor-\nmance in real-world biomedical applications.", "sections": [{"title": "Introduction", "content": "The success of Large Language Models (LLMs) promises\nto reshape the landscape of AI healthcare applications,\nespecially for scenarios relying on Question Answering\n(Budler, Gosak, and Stiglic 2023; Subramanian et al. 2024),\nsummarisation (Schlegel et al. 2023) and extracting in-\nsights from unstructured patient-generated health data (Li\net al. 2023). While considerable progress has been made\nin leveraging LLMs for tasks requiring free-text outputs,\nmuch of the focus has been on optimizing the parametric\nknowledge-the information stored in the model's weights\nand learned during training. Recent works explore meth-\nods such as fine-tuning on task-specific data and in-context\nlearning (ICL) and reporting significant improvements in\nmodel performance.\nHowever, these approaches primarily enhance the mod-\nels' internal knowledge representation. As such, they rely\non readily available data for the structured tasks at hand, be\nit in form of training sets for task-specific fine-tuning (Ab-\nburi et al. 2023), or for selecting good-quality representa-\ntive few-shot examples for ICL (Zhang et al. 2024; Gutier-\nrez et al. 2022). In the biomedical domain, such resources\nfor structured prediction tasks are typically not available,\nas requirements might arise ad-hoc-for example when re-\nsearchers need to process a set of medical records to find pa-\ntients satisfying inclusion criteria for a clinical trial (Jullien\net al. 2023) (e.g., whether they're a smoker). But even for\nwell established tasks, such as medication name extraction,\nfor which resources exist (Wei et al. 2020), these resources\noften prove to be insufficient in a practical context, due to the\ndomain shift between public resources and internal hospital\ndata (Hadi et al. 2023). Therefore, solely training-set reliant\nimprovements parametric knowledge of LLMs as driver of\nperformance for structured prediction tasks is often infeasi-\nble and approaches need to be able to perform well in zero-\nshot scenarios. Despite this, the literature currently lacks a\nsystematic investigation of other crucial aspects of knowl-\nedge utilization.\nIn order to address this research gap, we first postulate\nthat the performance of LLMs in medical reasoning and in-\nformation extraction tasks in \u201ctrue\u201d zero-shot setting\u00b9 hinges\non three distinct categories of knowledge:\n\u2022 Parametric Knowledge: The inherent knowledge embed-\nded within the model's parameters.\n\u2022 Task Knowledge: The model's ability to reason about the\nspecific task, including understanding relevant labels and\nthe context of the task.\n\u2022 External Knowledge: Additional information and context\nretrieved to supplement the model's understanding and\ndecision-making process."}, {"title": "Related Work", "content": "We briefly survey the existing benchmarking literature in\nthe medical domain, outlining the lack of studies focusing\non structured prediction tasks. Furthermore, we cover recent\nprompting techniques that were proposed to elicit reasoning\nin LLMs, and augment their domain knowledge, either by\nbetter tapping into their parametric knowledge or by explic-\nitly providing them with relevant external context. Notably,\nwe omit approaches that rely on existence of training sets,\nsuch as few-shot prompting (Wang et al. 2023) or model\nfine-tuning, as one of the key challenges in the medical do-\nmain is the lack of annotated task data, due to privacy con-\ncerns over sharing medical records. Instead, as outlines in\nthe introduction, we focus on \u201ctrue\u201d zero-shot capabilities\nof LLMs.\nExisting LLMs Benchmarks: With the rising popular-\nity of LLMs, many works evaluated their performance in\nthe biomedical and clinical domains. These works typically\nfocus on evaluating domain-knowledge by means of Ques-\ntion Answering (Singhal et al. 2023; Harris 2023; Subra-\nmanian et al. 2024), or focus directly on possible applica-\ntion scenarios, such as summarisation (Li et al. 2023; Yim\net al. 2023) or clinical coding (Kaur, Ginige, and Obst 2023).\nMany works combine these two directions in an effort to\nprovide more comprehensive benchmarks (Srivastava et al.\n2024; Xiong et al. 2024; Feng et al. 2024; Chen et al. 2020;\nManes et al. 2024). However, many of these works over-\nlook the wealth of existing literature and plethora of avail-\nable resources for traditional structured prediction tasks in\nthe biomedical domain, such as document classification, en-\ntity recognition and linking and event and relation extraction\n(e.g., Pyysalo et al. (2007; 2012) to name a few). Fries et al.\n(2022) have provided a comprehensive and unified collec-\ntion of these resources, however their work prioritises re-\nportage of the resource collection over benchmarking re-\nsults. Their preliminary evaluations suggest that their evalu-\nated pre-LLM era models barely surpass the random guess\nbaseline in the zero-shot setting. We build upon their work\nby providing a detailed analysis to what extent approaches\nto enhance reasoning and knowledge in LLMs help to chal-\nlenge this status quo.\nReasoning- and Knowledge-enhancing approaches:\nCurrent work attempts to improve the performance of LLMs\nfrom different knowledge utilization perspectives. One of\nthe obvious methods is full parameter domain-specific pre-\ntraining (Xie et al. 2024). For example, Chen et al. (2023)"}, {"title": "Methodology", "content": "Our methodology is designed to answer the following two\nresearch questions: \"How well do LLMs perform on struc-\ntured prediction tasks?\" and \"To what extent can ap-\nproaches that enhance task and external knowledge im-\nprove their performance?\" To answer the first research ques-\ntion, we benchmark a representative sample of LLMs on a\nlarge collection of biomedical text classification and NER\ndatasets. More specifically, we choose the task of Medi-\ncal Text Classification and NER as representative structured\npredictions tasks. We focus on the \"true\" zero shot setting,\nsince, as discussed before, this allows us to establish the\nlevel of models' original parametric knowledge, which is\ndesirable as it more closely reflects real-world application\nscenarios, because annotated training data for such tasks in\nthe biomedical domain is usually not available due to the ad-\nhoc nature of task requirements and privacy constraints of\nmedical records. Thus improving parametric knowledge is\noften infeasible in practice. To answer the second question,\nwe compare their zero-shot performance to various methods\nthat aim to enhance task knowledge and external knowledge,\nwhile keeping the parametric knowledge static.\nDatasets\nSince we evaluate different prompting techniques, we re-\nstrict the choice of tasks to those where the number of pos-\nsible labels is small enough to fit in the evaluated LLMs'\ncontext window. We restrict the number of labels to ten and\nthe mean length of the input documents to at most 2048 to-\nkens. This leaves us with 14 different classification datasets\nfrom the BigBio collection\u00b2. For the NER task, we sam-\nple 12 datasets from the pool of those that satisfy the crite-"}, {"title": "Techniques", "content": "Standard prompting was used as a baseline for both the Clas-\nsification as well as the NER tasks. Chain-of-thought rea-\nsoning (Wei et al. 2022) has been shown to improve per-\nformance, particularly in QA and logical reasoning tasks.\nThus, we also ran experiments with chain-of-thought rea-\nsoning to measure its impact on model performance. For\nthe NER task, we adapted a more guided, two-stage ap-\nproach (Shen et al. 2021) to implement a novel chain-of-\nthought reasoning approach. Here, The first stage involves\ninducing a generic entity name from a datasets' known en-\ntity labels-e.g., \"Bodypart\" for the NER labels describing\ndifferent bodyparts\u2014and then labelling the input document\nwith that generic entity type. In the second stage all enti-\nties labelled in this way are further disambiguated with their\nrespective fine-grained dataset NER labels. Retrieval Aug-\nmented Generation (Lewis et al. 2020) has been established\nas an effective technique to improve model performance by\nintroducing relevant non-parameteric knowledge to models\nand thus grounding the generated outputs to factual infor-\nmation. Xiong et al. (2024) conducted a systematic study of\nRAG on medical QA, and we incorporate their findings into\nour study. We used PubMed abstracts (Sanyal, Bhowmick,\nand Das 2021) and Wikipedia articles as knowledge corpora,\nbecause Xiong et al.'s (2024) experiments found that using\nPubMed improved performance over non RAG techniques,\nwhile using Wikipedia reduced performance in medical QA\ntasks. Our goal was to evaluate whether the same holds true\nfor structured prediction tasks as well. For the RAG mod-\nule, we made use of FAISS (Douze et al. 2024; Johnson,"}, {"title": "Evaluation Results", "content": "Overview of results\nReasoning and knowledge enhancing techniques seem to\nnot improve performance. Figure 1 and Figure 2 compare\nthe results of the best performing techniques for each model\nfor classification and NER, respectively. As seen in Ta-\nble 1, perhaps counter-intuitively, Standard Prompting con-\nsistently achieves the highest average F1 scores across all\nmodels for classification task, with BioMistral-7B obtaining\n36.48%, Llama-2-70B-Chat-AWQ achieving 40.34%, and\nLlama-2-7b-chat-hf scoring 34.92%. This result indicates"}, {"title": "Detailed Comparison of Prompting Techniques", "content": "The use of CoT and Self Consistency are not helpful if\nthere is a lack of parametric knowledge about the task.\nFor BioMistral-7B, using Self-Consistency CoT prompting\nleads to the biggest reduction of about 16% for classifi-\ncation tasks. One possible reason is the domain-specific\npre-training equips the model to better follow the instruc-\ntions directly without needing additional reasoning struc-\ntures, which seem detrimental. Similar to the RAG case,\nself-consistency seems to not consistently improve perfor-\nmance for NER. While Self Consistency aims to improve\nthe reliability of Chain of Thought prompting by generat-\ning multiple reasoning paths and selecting the most consis-\ntent one, it might introduce additional complexity leading\nto errors or inconsistencies. This is especially true, if the\nmodel's answers have low confidence scores due to insuffi-\ncient parametric knowledge which prevents them to reliably\nsolve these problems and would explain the observed per-\nformance drop. For NER tasks, the combination of Chain of\nThought (CoT) and Self-Consistency prompting with RAG\n(Wikipedia) shows the most substantial performance differ-\nence between the 70B and 7B models. This suggests that\nlarger models are more adept at leveraging external knowl-\nedge and complex reasoning strategies for entity recognition\ntasks if there is lack of parametric knowledge.\nRAG does not help information extraction. The quality\nand relevance of the retrieved information can significantly\nimpact performance, as seen from the fact that there is an av-\nerage drop of 16.91% when using RAG with PubMed Cor-\npora and 16.47% when using RAG with Wikipedia corpora\nas compared to the best performing technique for classifica-\ntion. While incorporating external knowledge through RAG\ncan be generally beneficial for QA based tasks (Xiong et al.\n2024) where incorporating relevant facts to the given ques-\ntion can append relevant knowledge into the model, it is not\nas straightforward in classification and information extrac-\ntion tasks. This has to especially be considered in the given\ntask setting, where the model could be confused by the pres-"}, {"title": "Detailed Per-dataset analysis", "content": "Models Perform Significantly better on public datasets.\nModels perform significantly better on public datasets (aver-\nage accuracy of 30%) compared to private datasets (average\naccuracy of 12%). This might hint at possible data leakage\nduring pre-training or instruction-tuning, as publicly avail-\nable datasets are more likely to be included in a web-crawl\nor a dedicated instruction tuning dataset. This might suggest\nthat model performance on 'unseen' (yet publicly available)\ntasks could be a result of unintentional data leakage rather\nthan a by product of reasoning or generalisation.\nMultilingual Performance is not Scale Dependent. As\nshown in Figure 1, smaller models can match or even out-\nperform larger models on Chinese and Japanese datasets but\nnot on English datasets. This may be due to the heavy re-\nliance on large English corpora during training, with lim-\nited exposure to medical contexts in other languages. This\nforces models to generalize compressed language represen-\ntations to specialized domains, where overfitting on sparse\nlanguages may hinder larger models' performance.\nLLMs struggle on tasks high complexity tasks As seen\nin Figure 5, LLMs seem to struggle to outperform random\nbaselines for both single and multi class classification tasks.\nHowever, Figure 3 paints a more nuanced picture: guessing\nbaseine remains unbeaten only on two of 14 datasets, which\ndrags down the average performance significantly.\nFigures 3 and 4 show that Llama2 70B demonstrates\ngood performance in low-complexity tasks such as dis-\nease and symptom classification (CZIBASE, NTCIR13-\nEN) and medium-complexity tasks like Gene Expression\nclassification (GEO). However, the model is challenged by"}, {"title": "Conclusion", "content": "We provide a comprehensive benchmark and analysis of\nLLMs in Medical Classification and Named Entity Recogni-\ntion tasks, revealing several key insights that have significant\nimplications for the field. We carry out a critical investiga-\ntion of broad claims regarding LLM capabilities by replicat-\ning them in various contexts, domains and datasets. We find\nthat models suffer from fundamental drawbacks in general-\nizability, which hinder their performance in structured infor-\nmation extraction tasks on domain specific problems. This\nleads to Standard prompting outperforming more advanced"}]}