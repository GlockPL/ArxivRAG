{"title": "LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction", "authors": ["Aishik Nagar", "Viktor Schlegel", "Thanh-Tung Nguyen", "Hao Li", "Yuping Wu", "Kuluhan Binici", "Stefan Winkler"], "abstract": "Large Language Models (LLMs) are increasingly adopted for\napplications in healthcare, reaching the performance of do-\nmain experts on tasks such as question answering and doc-\nment summarisation. Despite their success on these tasks,\nit is unclear how well LLMs perform on tasks that are tra-\nditionally pursued in the biomedical domain, such as struc-\ntured information extration. To breach this gap, in this paper,\nwe systematically benchmark LLM performance in Medical\nClassification and Named Entity Recognition (NER) tasks.\nWe aim to disentangle the contribution of different factors\nto the performance, particularly the impact of LLMs' task\nknowledge and reasoning capabilities, their (parametric) do-\nmain knowledge, and addition of external knowledge. To this\nend we evaluate various open LLMs-including BioMistral\nand Llama-2 models-on a diverse set of biomedical datasets,\nusing standard prompting, Chain-of-Thought (CoT) and Self-\nConsistency based reasoning as well as Retrieval-Augmented\nGeneration (RAG) with PubMed and Wikipedia corpora.\nCounter-intuitively, our results reveal that standard prompt-\ning consistently outperforms more complex techniques across\nboth tasks, laying bare the limitations in the current applica-\ntion of CoT, self-consistency and RAG in the biomedical do-\nmain. Our findings suggest that advanced prompting methods\ndeveloped for knowledge- or reasoning-intensive tasks, such\nas CoT or RAG, are not easily portable to biomedical tasks\nwhere precise structured outputs are required. This highlights\nthe need for more effective integration of external knowledge\nand reasoning mechanisms in LLMs to enhance their perfor-\nmance in real-world biomedical applications.", "sections": [{"title": "Introduction", "content": "The success of Large Language Models (LLMs) promises\nto reshape the landscape of AI healthcare applications,\nespecially for scenarios relying on Question Answering\n(Budler, Gosak, and Stiglic 2023; Subramanian et al. 2024),\nsummarisation (Schlegel et al. 2023) and extracting in-\nsights from unstructured patient-generated health data (Li\net al. 2023). While considerable progress has been made\nin leveraging LLMs for tasks requiring free-text outputs,\nmuch of the focus has been on optimizing the parametric\nknowledge-the information stored in the model's weights\nand learned during training. Recent works explore meth-\nods such as fine-tuning on task-specific data and in-context\nlearning (ICL) and reporting significant improvements in\nmodel performance.\nHowever, these approaches primarily enhance the mod-\nels' internal knowledge representation. As such, they rely\non readily available data for the structured tasks at hand, be\nit in form of training sets for task-specific fine-tuning (Ab-\nburi et al. 2023), or for selecting good-quality representa-\ntive few-shot examples for ICL (Zhang et al. 2024; Gutier-\nrez et al. 2022). In the biomedical domain, such resources\nfor structured prediction tasks are typically not available,\nas requirements might arise ad-hoc-for example when re-\nsearchers need to process a set of medical records to find pa-\ntients satisfying inclusion criteria for a clinical trial (Jullien\net al. 2023) (e.g., whether they're a smoker). But even for\nwell established tasks, such as medication name extraction,\nfor which resources exist (Wei et al. 2020), these resources\noften prove to be insufficient in a practical context, due to the\ndomain shift between public resources and internal hospital\ndata (Hadi et al. 2023). Therefore, solely training-set reliant\nimprovements parametric knowledge of LLMs as driver of\nperformance for structured prediction tasks is often infeasi-\nble and approaches need to be able to perform well in zero-\nshot scenarios. Despite this, the literature currently lacks a\nsystematic investigation of other crucial aspects of knowl-\nedge utilization.\nIn order to address this research gap, we first postulate\nthat the performance of LLMs in medical reasoning and in-\nformation extraction tasks in \u201ctrue\u201d zero-shot setting\u00b9 hinges\non three distinct categories of knowledge:\n\u2022 Parametric Knowledge: The inherent knowledge embed-\nded within the model's parameters.\n\u2022 Task Knowledge: The model's ability to reason about the\nspecific task, including understanding relevant labels and\nthe context of the task.\n\u2022 External Knowledge: Additional information and context\nretrieved to supplement the model's understanding and\ndecision-making process."}, {"title": "Related Work", "content": "We briefly survey the existing benchmarking literature in\nthe medical domain, outlining the lack of studies focusing\non structured prediction tasks. Furthermore, we cover recent\nprompting techniques that were proposed to elicit reasoning\nin LLMs, and augment their domain knowledge, either by\nbetter tapping into their parametric knowledge or by explic-\nitly providing them with relevant external context. Notably,\nwe omit approaches that rely on existence of training sets,\nsuch as few-shot prompting (Wang et al. 2023) or model\nfine-tuning, as one of the key challenges in the medical do-\nmain is the lack of annotated task data, due to privacy con-\ncerns over sharing medical records. Instead, as outlines in\nthe introduction, we focus on \u201ctrue\u201d zero-shot capabilities\nof LLMs.\nExisting LLMs Benchmarks: With the rising popular-\nity of LLMs, many works evaluated their performance in\nthe biomedical and clinical domains. These works typically\nfocus on evaluating domain-knowledge by means of Ques-\ntion Answering (Singhal et al. 2023; Harris 2023; Subra-\nmanian et al. 2024), or focus directly on possible applica-\ntion scenarios, such as summarisation (Li et al. 2023; Yim\net al. 2023) or clinical coding (Kaur, Ginige, and Obst 2023).\nMany works combine these two directions in an effort to\nprovide more comprehensive benchmarks (Srivastava et al.\n2024; Xiong et al. 2024; Feng et al. 2024; Chen et al. 2020;\nManes et al. 2024). However, many of these works over-\nlook the wealth of existing literature and plethora of avail-\nable resources for traditional structured prediction tasks in\nthe biomedical domain, such as document classification, en-\ntity recognition and linking and event and relation extraction\n(e.g., Pyysalo et al. (2007; 2012) to name a few). Fries et al.\n(2022) have provided a comprehensive and unified collec-\ntion of these resources, however their work prioritises re-\nportage of the resource collection over benchmarking re-\nsults. Their preliminary evaluations suggest that their evalu-\nated pre-LLM era models barely surpass the random guess\nbaseline in the zero-shot setting. We build upon their work\nby providing a detailed analysis to what extent approaches\nto enhance reasoning and knowledge in LLMs help to chal-\nlenge this status quo.\nReasoning- and Knowledge-enhancing approaches:\nCurrent work attempts to improve the performance of LLMs\nfrom different knowledge utilization perspectives. One of\nthe obvious methods is full parameter domain-specific pre-\ntraining (Xie et al. 2024). For example, Chen et al. (2023)"}, {"title": "Methodology", "content": "Our methodology is designed to answer the following two\nresearch questions: \"How well do LLMs perform on struc-\ntured prediction tasks?\" and \"To what extent can ap-\nproaches that enhance task and external knowledge im-\nprove their performance?\" To answer the first research ques-\ntion, we benchmark a representative sample of LLMs on a\nlarge collection of biomedical text classification and NER\ndatasets. More specifically, we choose the task of Medi-\ncal Text Classification and NER as representative structured\npredictions tasks. We focus on the \"true\" zero shot setting,\nsince, as discussed before, this allows us to establish the\nlevel of models' original parametric knowledge, which is\ndesirable as it more closely reflects real-world application\nscenarios, because annotated training data for such tasks in\nthe biomedical domain is usually not available due to the ad-\nhoc nature of task requirements and privacy constraints of\nmedical records. Thus improving parametric knowledge is\noften infeasible in practice. To answer the second question,\nwe compare their zero-shot performance to various methods\nthat aim to enhance task knowledge and external knowledge,\nwhile keeping the parametric knowledge static.\nDatasets\nSince we evaluate different prompting techniques, we re-\nstrict the choice of tasks to those where the number of pos-\nsible labels is small enough to fit in the evaluated LLMs'\ncontext window. We restrict the number of labels to ten and\nthe mean length of the input documents to at most 2048 to-\nkens. This leaves us with 14 different classification datasets\nfrom the BigBio collection\u00b2. For the NER task, we sam-\nple 12 datasets from the pool of those that satisfy the crite-\nria. The resulting dataset sample features four non-English\ndatasets and six non-public classification datasets, which al-\nlows us to investigate whether LLMs perform better on mi-\nnority languages or on data that is less likely to be found\nin public pre-training corpora. We run the evaluation on the\nofficial test-set split where available, otherwise we consider\nthe full dataset. For datasets with more than 500 instances,\nwe sample 500 random but fixed instances to speed up the\nexperiments. Overall, our selection spans English and non-\nenglish source data, publicly available and private datasets,\nand various domains such as scientific papers, medical notes\nand social media. The overview of the datasets follows be-\nlow, with full details to be found in the technical appendix.\nClassification. The datasets used for classification tasks\ninclude both single-label and multi-label datasets, covering\na wide range of biomedical and clinical domains. For single-\nlabel classification, the GAD dataset focuses on identifying\nassociations between genes and diseases (Bravo et al. 2015),\nwhile the GEO dataset is concerned with classifying mi-\ncroarray, transcriptomics, and single-cell experiments from\nthe Gene Expression Omnibus (GEO) database (Elucidata\n2022). The MEDDIALOG dataset aims to classify dialogue\nsnippets as either being said by a doctor or a patient (Chen\net al. 2020). Furthermore, the CZIDRSM dataset has several\nsubsets, including one for classifying research articles based\non aspects of disease research (CZIBASE), and others for\nidentifying whether a paper describes substantive research\ninto Quality of Life (CZIQOL) or is a natural history study\n(CZINATHIST).\nIn multi-label classification, the LITCOVID dataset is used\nfor the classification of COVID-19-related articles (Chen\net al. 2021). The CAS and ESSAI datasets are utilized for\nidentify negation and uncertainty clinical cases from French-\nspeaking countries (Grabar, Claveau, and Dalloux 2018).\nThe NTCIR13 datasets include subsets for disease classi-\nfication of tweets in Japanese (*-JA), English (*-EN), and\nChinese (*-ZH) (Iso et al. 2017). Additionally, the PSYTAR\ndataset is used for sentence classification of various drug-\nrelated effects, such as Adverse Drug Reactions (ADR) and\nWithdrawal Symptoms (WDs) (Zolnoori et al. 2019), while\nthe SCICITE dataset is used for citation intent classification\nbased on the context within computer science and biomedi-\ncal domains (Cohan et al. 2019).\nNER. The datasets for Named Entity Recognition (NER)\ntasks are similarly divided into entity recognition (single\nentity type) and classification (multiple entity types). In\nthe single-type category, the GENETAG dataset is used for\ngene/protein NER, with two annotation versions: the orig-\ninal GENETAG-G and the corrected GENETAG-C (Tanabe\net al. 2005). Additionally, the GENIA-PPI dataset focuses\non protein-protein interactions or gene regulatory relations\nwithin the GENIA corpus, capturing primarily static rela-\ntions (Pyysalo et al. 2009; Hoehndorf et al. 2010; Ohta et al.\n2010).\nThe multiple-type NER datasets encompass various com-\nplex biomedical tasks. The ANEM dataset targets anatomi-\ncal entity recognition (Ohta et al. 2012), while the BIOIN-\nFER dataset focuses on recognizing proteins, genes, and"}, {"title": "Techniques", "content": "Standard prompting was used as a baseline for both the Clas-\nsification as well as the NER tasks. Chain-of-thought rea-\nsoning (Wei et al. 2022) has been shown to improve per-\nformance, particularly in QA and logical reasoning tasks.\nThus, we also ran experiments with chain-of-thought rea-\nsoning to measure its impact on model performance. For\nthe NER task, we adapted a more guided, two-stage ap-\nproach (Shen et al. 2021) to implement a novel chain-of-\nthought reasoning approach. Here, The first stage involves\ninducing a generic entity name from a datasets' known en-\ntity labels-e.g., \"Bodypart\" for the NER labels describing\ndifferent bodyparts\u2014and then labelling the input document\nwith that generic entity type. In the second stage all enti-\nties labelled in this way are further disambiguated with their\nrespective fine-grained dataset NER labels. Retrieval Aug-\nmented Generation (Lewis et al. 2020) has been established\nas an effective technique to improve model performance by\nintroducing relevant non-parameteric knowledge to models\nand thus grounding the generated outputs to factual infor-\nmation. Xiong et al. (2024) conducted a systematic study of\nRAG on medical QA, and we incorporate their findings into\nour study. We used PubMed abstracts (Sanyal, Bhowmick,\nand Das 2021) and Wikipedia articles as knowledge corpora,\nbecause Xiong et al.'s (2024) experiments found that using\nPubMed improved performance over non RAG techniques,\nwhile using Wikipedia reduced performance in medical QA\ntasks. Our goal was to evaluate whether the same holds true\nfor structured prediction tasks as well. For the RAG mod-\nule, we made use of FAISS (Douze et al. 2024; Johnson,\nDouze, and J\u00e9gou 2019), which allows retrieval of most sim-\nilar documents based on semantic similarity, where we used\nthe all-MiniLM-L6-v2 sentence transformers (Reimers\nand Gurevych 2019) model for embedding input documents\nand corpora. For each experiment, the number of retrieved\ndocuments was computed based on the maximum possible\ndocuments which could be used without exceeding the to-\nken limit of the model.\nSelf-consistency, proposed by Wang et al. (2022), improves\nchain-of-thought reasoning of LLMs by sampling reason-\ning paths for a given problem, followed by a majority vote\nfor the final answer. We also conduct a set of experiments\nemploying self-consistency to investigate whether such im-\nprovements can be observed on structured prediction tasks\nin the medical domain as well. For classification tasks, self\nconsistency was employed to generate multiple reasoning\nchains for the given problem, followed by answer extrac-\ntion from each reasoning chain and majority voting to se-\nlect the final answer. For NER tasks, since we follow the\ntwo-stage approach, self-consistency was employed in both\nstages. Multiple general entity labels were generated in the\nfirst stage, and entities were extracted for each such label.\nIn the second stage, self consistency was again used for the\nentity selection phase as well as the entity label determina-\ntion step. Majority voting was utilised in final label or class\nselection in each case (Xie et al. 2023).\nConstrained decoding in LLMs (Willard and Louf 2023)\nwas used to ensure structured information extraction and text\ngeneration. This allowed us to evaluate the LLMs for the\ntask at hand without the added variability due to the aleatoric\nuncertainties brought about by the probabilistic language\ngeneration fundamental to the architectures of the models.\nMore specifically, for classification tasks, we ensured the\npresense of at least one label in the generated outputs. For\nNER we restricted the generation of spans occurring in text\nin the first step, and in the second step, for each of the spans\nwe restricted the generation to any of the possible labels.\nThis is also one of the reasons why we opted against evaluat-\ning API-based closed-source LLMs\u00b3, as in our initial exper-\niments the hallucinations in generated outputs created prob-\nlems with reliably parsing the structured outputs.\nWe refer to chain of thought as COT, Self-consistency as\nSC, RAG as RAG-{P|W} for PubMed and Wikipedia cor-\npora, respectively, and to standard prompting as VANILLA."}, {"title": "Evaluation Results", "content": "Overview of results\nReasoning and knowledge enhancing techniques seem to\nnot improve performance. Figure 1 and Figure 2 compare\nthe results of the best performing techniques for each model\nfor classification and NER, respectively. As seen in Ta-\nble 1, perhaps counter-intuitively, Standard Prompting con-\nsistently achieves the highest average F1 scores across all\nmodels for classification task, with BioMistral-7B obtaining\n36.48%, Llama-2-70B-Chat-AWQ achieving 40.34%, and\nLlama-2-7b-chat-hf scoring 34.92%. This result indicates\nthat for structured prediction tasks, more complex reason-\ning techniques such as Chain of Thought (CoT) Prompt-\ning or Retrieval-Augmented Generation (RAG), do not out-\nperform simpler approaches like Standard Prompting. For\nNER tasks, the results present a more nuanced picture com-\npared to the classification tasks. While Standard Prompting\nremains effective, there is a noticeable shift in performance\nacross different models and datasets. Notably, the scores are\nsignificantly lower than typical F1 scores in biomedical NER\nbenchmarks. For instance, the NCBI disease corpus (Do\u011fan,\nLeaman, and Lu 2014; Krallinger et al. 2015) and CHEMD-\nNER dataset usually yield higher performances with special-\nized models or extensive pre-training. State-of-the-art mod-\nels on these benchmarks can achieve Span F1 scores up\nto 0.90 for the NCBI disease corpus (Kocaman and Talby\n2021; Zhou et al. 2023). However, similar to our findings, in\ntrue zero-shot setting, NER scores have been reported to be\nmarkedly low, even for the general domain (Shen et al. 2021)\nand when supplying label descriptions (Picco et al. 2024).\nA possible reason for poor performance might be that\nthese approaches have been tailored towards and shown to\nwork well on-knowledge- and reasoning-intensive tasks,\nsuch as Question Answering (Nori et al. 2023) or Math-\nematical Reasoning (Wang and Zhou 2024; Wang et al.\n2022; Li et al. 2024). Meanwhile more narrowly defined\ntasks like information extraction or classification require the\nunderstanding of specific task semantics over generic rea-\nsoning capabilities. They seem to not require broad knowl-\nedge, as it could be found in biomedical paper abstracts or\nWikipedia articles, but rather require application of domain\nknowledge in a specific and highly contextualized tasks,\ncontained within the input document and task description.\nModels need to be able to handle highly specialized vocab-\nulary, including jargon, acronyms, and synonyms that can\nvary widely between subfields (Kim et al. 2007; Zheng, Yu\net al. 2018; Jiang and Xu 2024). There is a fundamental re-\nquirement for context dependent disambiguation of ambigu-\nity and polysemy as well as nuances and variablity in syntax\nand expressions of biomedical concepts. This is often de-\nveloped through specialized pre-training or domain-specific\nenhancements, which the LLMs have not been able to cap-\nture. These challenges necessitate models that not only have\nrobust general NER capabilities but also an intricate under-\nstanding of biomedical context which can very for different\nsubtasks within the domain.\nScale drives improvements. In line with previous obser-\nvations, we find that the 70B model also shows a consid-\nerable improvement (5.4% for classification, 2.2% for NER\nSpan F1) over the 7B model. The most significant differ-\nence in performance between the Llama 7B and 70B Mod-\nels is observed when using Self-Consistency with Chain of\nThought and RAG (Wikipedia), where the 70B model out-\nperforms the 7B model by 15.45% on classification and on"}, {"title": "Detailed Comparison of Prompting Techniques", "content": "The use of CoT and Self Consistency are not helpful if\nthere is a lack of parametric knowledge about the task.\nFor BioMistral-7B, using Self-Consistency CoT prompting\nleads to the biggest reduction of about 16% for classifi-\ncation tasks. One possible reason is the domain-specific\npre-training equips the model to better follow the instruc-\ntions directly without needing additional reasoning struc-\ntures, which seem detrimental. Similar to the RAG case,\nself-consistency seems to not consistently improve perfor-\nmance for NER. While Self Consistency aims to improve\nthe reliability of Chain of Thought prompting by generat-\ning multiple reasoning paths and selecting the most consis-\ntent one, it might introduce additional complexity leading\nto errors or inconsistencies. This is especially true, if the\nmodel's answers have low confidence scores due to insuffi-\ncient parametric knowledge which prevents them to reliably\nsolve these problems and would explain the observed per-\nformance drop. For NER tasks, the combination of Chain of\nThought (CoT) and Self-Consistency prompting with RAG\n(Wikipedia) shows the most substantial performance differ-\nence between the 70B and 7B models. This suggests that\nlarger models are more adept at leveraging external knowl-\nedge and complex reasoning strategies for entity recognition\ntasks if there is lack of parametric knowledge.\nRAG does not help information extraction. The quality\nand relevance of the retrieved information can significantly\nimpact performance, as seen from the fact that there is an av-\nerage drop of 16.91% when using RAG with PubMed Cor-\npora and 16.47% when using RAG with Wikipedia corpora\nas compared to the best performing technique for classifica-\ntion. While incorporating external knowledge through RAG\ncan be generally beneficial for QA based tasks (Xiong et al.\n2024) where incorporating relevant facts to the given ques-\ntion can append relevant knowledge into the model, it is not\nas straightforward in classification and information extrac-\ntion tasks. This has to especially be considered in the given\ntask setting, where the model could be confused by the pres-\nence of irrelevant knowledge information which adds an ad-\nditional layer of complexity in extracting the relevant infor-\nmation for answering the relevant questions.\nSC helps models filter out irrelevant noise in case of\nRAG, but does not help CoT While Self Consistency aims\nto improve the reliability of Chain of Thought prompting\nby generating multiple reasoning paths and selecting the\nmost consistent one, is fundamentally dependent on the\nmodels epistemic certainty (Yadkori et al. 2024; Liu et al.\n2024). This hinders performance if the model's answers\nhave low confidence scores due to insufficient parametric\nknowledge which prevents them to reliably solve these prob-\nlems and would explain the observed performance drop.\nFor BioMistral-7B, using Self-Consistency CoT prompting\nleads to the biggest reduction of about 16% for classifica-\ntion tasks. One possible reason is the domain-specific pre-\ntraining equips the model to better follow the instructions\ndirectly without needing additional reasoning structures,\nwhich seem detrimental. Similar to the RAG case, self-\nconsistency seems to not consistently improve performance\nfor NER. The combination of Chain of Thought (CoT) and\nSelf-Consistency prompting with RAG (Wikipedia) shows\nthe most substantial performance difference between the\n70B and 7B models. This suggests that larger models are\nmore adept at leveraging external knowledge and complex\nreasoning strategies for entity recognition tasks to augment\nthe lack of epistemic uncertainty."}, {"title": "Detailed Per-dataset analysis", "content": "Models Perform Significantly better on public datasets.\nModels perform significantly better on public datasets (aver-\nage accuracy of 30%) compared to private datasets (average\naccuracy of 12%). This might hint at possible data leakage\nduring pre-training or instruction-tuning, as publicly avail-\nable datasets are more likely to be included in a web-crawl\nor a dedicated instruction tuning dataset. This might suggest\nthat model performance on 'unseen' (yet publicly available)\ntasks could be a result of unintentional data leakage rather\nthan a by product of reasoning or generalisation.\nMultilingual Performance is not Scale Dependent. As\nshown in Figure 1, smaller models can match or even out-\nperform larger models on Chinese and Japanese datasets but\nnot on English datasets. This may be due to the heavy re-\nliance on large English corpora during training, with lim-\nited exposure to medical contexts in other languages. This\nforces models to generalize compressed language represen-\ntations to specialized domains, where overfitting on sparse\nlanguages may hinder larger models' performance.\nLLMs struggle on tasks high complexity tasks As seen\nin Figure 5, LLMs seem to struggle to outperform random\nbaselines for both single and multi class classification tasks.\nHowever, Figure 3 paints a more nuanced picture: guessing\nbaseine remains unbeaten only on two of 14 datasets, which\ndrags down the average performance significantly.\nFigures 3 and 4 show that Llama2 70B demonstrates\ngood performance in low-complexity tasks such as dis-\nease and symptom classification (CZIBASE, NTCIR13-\nEN) and medium-complexity tasks like Gene Expression\nclassification (GEO). However, the model is challenged by"}, {"title": "Conclusion", "content": "We provide a comprehensive benchmark and analysis of\nLLMs in Medical Classification and Named Entity Recogni-\ntion tasks, revealing several key insights that have significant\nimplications for the field. We carry out a critical investiga-\ntion of broad claims regarding LLM capabilities by replicat-\ning them in various contexts, domains and datasets. We find\nthat models suffer from fundamental drawbacks in general-\nizability, which hinder their performance in structured infor-\nmation extraction tasks on domain specific problems. This\nleads to Standard prompting outperforming more advanced"}]}