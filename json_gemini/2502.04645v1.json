{"title": "Cross-Encoder Rediscovers a Semantic Variant of BM25", "authors": ["Meng Lu", "Catherine Chen", "Carsten Eickhoff"], "abstract": "Neural Ranking Models (NRMs) have rapidly advanced state-of-the-art performance on information retrieval tasks. In this work, we investigate a Cross-Encoder variant of MiniLM to determine which relevance features it computes and where they are stored. We find that it employs a semantic variant of the traditional BM25 in an interpretable manner, featuring localized components: (1) Transformer attention heads that compute soft term frequency while controlling for term saturation and document length effects, and (2) a low-rank component of its embedding matrix that encodes inverse document frequency information for the vocabulary. This suggests that the Cross-Encoder uses the same fundamental mechanisms as BM25, but further leverages their capacity to capture semantics for improved retrieval performance. The granular understanding lays the groundwork for model editing to enhance model transparency, addressing safety concerns, and improving scalability in training and real-world applications.", "sections": [{"title": "1 Introduction", "content": "Information retrieval (IR) has long relied on foundational models like BM25, grounded in intuitive heuristics for relevance estimation. BM25 leverages term frequency (TF) and inverse document frequency (IDF) to rank documents effectively, achieving strong performance across various tasks. Its simplicity and interpretability have made it a cornerstone of traditional IR systems. Inspired by BM25's success, earlier neural IR models (e.g., MatchPyramid [22], DRMM [13]) were purposefully designed to emulate BM25's principles. These models incorporated explicit components for semantic TF and IDF computations, blending neural architectures with established IR heuristics to improve relevance estimation.\nHowever, the advent of transformer-based models has revolutionized the field of IR. These models, trained end-to-end on raw data, excel at extracting context-dependent semantic signals for ranking tasks. By leveraging the multi-attention mechanism and vast parameter spaces, transformers [31] capture nuanced relationships between query and document terms that go beyond traditional heuristic-based approaches. Despite their superior performance, these models come with significant trade-offs: their complexity and lack of interpretability make it challenging to understand how they assess relevance. This raises fundamental questions: how do these models assess relevance? Do they rely on established IR principles such as TF and IDF, or do they operate as vast, unstructured statistical approximators?"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Interpreting Neural IR Models", "content": "In contrast to early neural IR models (e.g., MatchPyramid [22], DRMM [13]) that were explicitly designed to be neural term matching models, recent pre-trained transformer-based ranking models achieve significant performance gains by capturing nuanced query-document interactions and forming rich contextual representations [14, 30]. Although not explicitly designed to encode relevance concepts, these models have been found to learn relevance concepts such as TF [2], IDF [3], and even combinations of them such as BM25 [17, 25, 35, 36]. To better understand how these relevance signals are represented and utilized, researchers have turned to IR axioms [1, 26, 33] and constructed axiom-specific diagnostic datasets to test how well models rank document pairs in accordance with axiomatic constraints [27]. Many studies also rely on correlational methods such as probing [3, 9, 10, 17, 37] to diagnose model behavior and assess their adherence to desired relevance properties. However, the underlying mechanisms of how these concepts are encoded remain unclear. This work aims to identify the specific components involved in this process and explain how they interact to determine relevance."}, {"title": "2.2 Mechanistic Interpretability", "content": "To better understand how information is encoded in models, mechanistic interpretability [5, 18, 19, 21, 23, 34] aims to reverse-engineer model behavior by analyzing the causal effects of components, providing stronger causal evidence than probing [18, 32]. Gaining traction in NLP, these methods have been used to uncover how LLMs perform tasks, such as indirect object identification [18] and greater-than computation [15], and have recently been applied to IR to identify attention heads encoding term frequency signals [2].\nMechanistic interpretability methods rely on causal interventions, such as activation patching (or causal mediation analysis) where activations are swapped between input pairs to observe their effects on model behavior and localize specific mechanisms. Activation patching provides a solid foundation for initial analyses, allowing researchers to identify and isolate specific mechanisms within a model [16, 38]. For example, Chen et al. [2] use activation patching with IR axioms to identify attention heads that encode a term frequency signal in a bi-encoder. However, activation patching has limitations as it only evaluates the indirect effect components have on the output. In contrast, path patching allows us to trace the computational path (circuit) from input to output, providing insights into how components sequentially interact [12, 34]. In this work, we leverage path patching to reverse-engineer the computational path through which a NRM uses to compute relevance."}, {"title": "3 Methodology", "content": "In this section, we explain how we construct diagnostic datasets and use path patching to track how the model implements a semantic version of BM25."}, {"title": "3.1 Diagnostic Dataset Construction", "content": "Mapping BM25 components to IR axioms. IR axioms are derived from the same retrieval heuristics BM25 is composed of: TF, IDF, term saturation, and document length [7].  Note that for TF, we extend this analysis of term matching to include semantic matches, where semantically similar terms contribute to the relevance score, based on the intuition about the semantic understanding capabilities of Transformer-based models [30]. We refer to this extension as soft-TF and utilize both TFC1 and STMC1 as the corresponding axioms.\nConstructing minimal-pair-based diagnostic datasets. We construct a diagnostic dataset for each axiomatic component by perturbing documents to create input pairs. Our starting point is the base dataset from Chen et al. [2], who create a diagnostic dataset for analyzing TFC1 using MS-MARCO [20], consisting of 10k query-document pairs. To create the diagnostic dataset for each axiom, we perturb each query-document pair in the base dataset in accordance with the formal axiomatic definition. Each perturbed sample differs from the original only slightly, with the perturbation introducing an additional signal corresponding to the relevance concept we aim to analyze. Our perturbation strategies and examples of input pairs are shown in Table 1."}, {"title": "3.2 Path Patching", "content": "The intuition of path patching relies on the Transformer [31] architecture, where stacks of multi-headed attention and multi-layer perceptron (MLP) layers are connected by residual connections. These residual connections are additive functions, which essentially allow components to \"read\" and \"write\" information to a common residual stream. [28]. The goal of path patching is to isolate which components \"send\" and \"receive\" information through the residual stream, and what information is being transmitted.\nPath patching [12, 34] involves using minimal input pairs (we denote as baseline $b$ and perturbed $p$), that differ only by a target behavioral signal to be evaluated (e.g., axiomatic property). Activations (e.g., residual stream output, attention head or layer output, MLP output) are iteratively copied from the perturbed input into the corresponding location in the baseline input to isolate the impact of that component on downstream computations. In this work, we focus on analyzing the roles of attention heads in relevance computation and leave analysis of MLPs for future investigation.\nThe path patching algorithm is an iterative backward process that starts with identifying which upstream components send important information to the logits (Figure 1). Specifically, it involves four forward passes through the model to identify which upstream (sender) components send information to the target (receiver) component (i.e., logits): (1) Run on the baseline input $x_1$ and cache activations. (2) Run on the perturbed input $x_p$ and cache activations. (3) Select the sender set $s$, the components whose activations are patched in, and the receiver set $r$, the components where the effect of patching $s$ is analyzed. Run the forward pass on $x_p$, patch in $s$, freeze all other activations, and recompute $r'$, which is the same as $r$ from the $x_1$ run except for the direct influence from $s$ to $r$. Cache $r'$. (4) Run the model on $x_p$, and patch in $r'$ values. Measure the difference in logits between $x_1$ and $x_p$ to quantify the effect of $s$ on $r$ in terms of passing the additional signal.\nThe effect of a patch is measured by the difference in logits (which in the case of cross-encoders, is equivalent to the difference in relevance scores). This algorithm is then iteratively repeated for each important upstream component. For more information, we refer the reader to Wang et al. [34] and Goldowsky-Dill et al. [12].\nWe begin by applying path patching to track the path of term-matching, the most fundamental component of BM25. To carry this out, we use the TFC1 and STMC1 diagnostic datasets to identify the components responsible for encoding the relevance signal of an additional query term in a document, providing the foundation for our analysis of model behavior."}, {"title": "3.3 Model", "content": "We analyze ms-marco-MiniLM-L-12-v2 [6] for our BERT-based cross-encoder, a model trained on the MS MARCO dataset, and chosen for its high evaluation performance the MS MARCO Passage Re-Ranking [20] and TREC Deep Learning 2019 datasets [4]. The cross-encoder processes input $x$, consisting a query $q$ and a document $d$ in the format: $<CLS> q_1, q_2, ..., <SEP> d_1, d_2, ..., <SEP>$. After all transformer layers, the $<CLS>$ token vector is passed to a classifier to produce the logits."}, {"title": "4 Semantic Scoring Circuit", "content": "Previous work in mechanistic interpretability has identified groups of attention heads whose behaviors generalize across tasks. For example, \"induction heads,\" \"previous token heads,\" and \"duplicate token heads\" [21] have been shown to play a role in various NLP tasks by completing sub-tasks within larger task-specific circuits. Identifying these heads is therefore crucial. Following the convention of naming head groups based on their mechanisms, we assign names to key components according to their specialized roles in document relevance computation."}, {"title": "4.1 Relevance Scoring Heads", "content": "Information Flow. To identify components directly transmitting soft-TF signals to the relevance scores, we path patch to the logits using the TFC1 and STMC1 diagnostic datasets.\nFigure 3 highlights the most significant heads (i.e., those causing a > 30% increase in ranking score) for the soft-TF-related axioms: 10.1 (Layer 10, Head 1), 10.4, 10.7, and 10.10. The high correlation in patching importance values $(corr = 0.9996, p < 0.001)$ and the fact that the four critical heads are shared between TFC1 and STMC1 suggests that the model treats exact and semantic term matches similarly.\nComponent Behavior. Upon closer inspection of the attention patterns of these heads, we observe that the [CLS] token selectively attends to query tokens and retrieves soft-TF based on each query token's IDF value. Specifically, head 10.1 focuses predominantly on the highest-idf query token: the more uncommon a query token (i.e., the higher its IDF), the more the [CLS] token attends to it to capture its soft-TF information. Conversely, head 10.10 primarily attends to low-IDF tokens, while heads 10.7 and 10.4 focus on middle-to-high-IDF tokens.\nTo quantify this observation, we first compute the Pearson correlation between each head's attention pattern and the query tokens' IDF values. The results show strong correlations for heads 10.1 $(r = 0.878)$, 10.4 $(r = 0.780)$, and 10.7 $(r = 0.798)$, with $p < 0.001$ for all, establishing a statistically significant relationship (threshold: $p < 0.05$) between the attention values from [CLS] and the query tokens' IDF values for 10.1, 10.4 and 10.7. However, Head 10.10 exhibits a weaker correlation $(r = 0.206)$. Then, we calculate the average IDF value weighted by each head's attention distribution across query tokens to further validate this trend. Head 10.1 has the highest weighted average IDF (2.704), followed by heads 10.7 (1.983), 10.4 (2.171), and finally head 10.10 (1.457). This ranking quantitatively supports our observation and explains the weaker correlation between 10.10's attention value with IDF values due to its negative association with them. Thus, each head covers different parts of the query based on IDF ranges, and together they send soft-TF information for all the query tokens to [CLS] together using a computation consisting of both soft-TF and IDF.\nBM25-based Scoring Hypothesis. Considering that the mentioned computation resembles BM25, we hypothesize that the Relevance Scoring Heads perform a BM25-like summation of soft-TF values weighted by IDF of the query tokens. The model might attribute importance to the output of 10.1, 10.4 & 10.7, 10.10 from highest to lowest (e.g. controlled by different output projection matrix values) such that if a query token's soft-TF is processed by 10.1, its soft-TF contributes the most to the relevance score. In this way, the query token's soft-TF contribution to the relevance score is proportional to its IDF value. We verify this hypothesis in \u00a75."}, {"title": "4.2 Contextual Query Representation Heads", "content": "The first group of heads that send information to Relevance Scoring Heads are termed contextual query representation heads: 8.10 and 9.11. These heads aggregate soft-TF information of the higher-IDF query tokens and distribute this information among all query tokens, strengthening their representations for the Relevance Scoring Heads' final computation.\nAt the Relevance Scoring Heads, the [CLS] token retrieves soft-TF information from all query tokens. Thus, we analyze the attention patterns among query tokens in these two intermediary heads to understand how they modify the query token representation in the residual stream. The attention patterns reveal a distinct pattern: all query tokens in these heads consistently focus on one or two higher-IDF query tokens, with strong correlations to IDF values (9.11: $r = 0.829$, 8.10: $r = 0.781$) with both $p < 0.001$. The evidence suggests that 8.10 and 9.11 act as Contextual Query Representation Heads: they distribute the soft-TF information of higher-IDF tokens across all query tokens. This ensures that when [CLS] token retrieves soft-TF information at Relevance Scoring Heads, each query token retains a stronger representation of the higher-IDF tokens' soft-TF values.\nImportant Upstream Components. Path patching to 8.10 and 9.11 confirms that these heads also receive soft-TF information from the Matching Heads."}, {"title": "4.3 Matching Heads", "content": "Matching Heads (0.8, 1.7, 2.1, 3.1, 4.9, 5.7, 5.9, 6.3, 6.5, 7.9, 8.0, 8.1, 8.8) compute and transmit a Matching Score, combining soft-TF, term saturation, and document length signals.\nComponent Behavior. Patching results reveal that Matching Heads transmit soft-TF information to the Relevance Scoring Heads and Contextual Query Representation Heads. A key question, however, is whether these heads merely propagate soft-TF signals from earlier components or compute the soft-TF themselves. Analyzing their attention patterns indicates that they actively attend to semantic matches and compute the soft-TF. All query and documents tokens are strongly attending duplicates and mildly attending to semantically-similar terms.\nQualitative examination of the 13 heads with positive patching importance reveals attention patterns similar to those in  We quantitatively check whether the query tokens are in fact matching semantically by measuring the correlation between the attention values (from any token to any document token) and the tokens' semantic similarity, approximated using cross-encoder's embeddings. The results show that Matching Heads exhibit a significantly higher average Pearson correlation $(corr = 0.500, p<0.001)$ compared to other heads $(corr = 0.132)$, confirming that these heads function as Matching Heads: at these heads, query tokens actively attend to semantically similar terms, with the attention value serving as an indicator of semantic similarity. Following this intuition, the sum of attention values from a query token to all document tokens can approximate the soft-TF of a query term.\nTo validate the proposed function of the Matching Heads, we mean-ablate these Matching Heads and observe that the TF perturbed samples' average value drops from 5.146 to -4.394, and the STMC perturbed samples' average value drops from -1.998 to -4.611. The drastic drop occurs because ablation deactivates all semantic-matching functions for all query terms."}, {"title": "Matching Score Contains Term Saturation and Document Length Signals", "content": "The average Soft-TF Correlation Score of Matching Heads (0.500) indicates that their attention values from query tokens to document tokens encode not only semantic similarity but also additional signals. To examine whether these attention values capture term saturation or document length effects, we analyze their behavior under using two diagnostic datasets: (1) TFC2 (increasing occurrences of a query term), and (2) LNC1 dataset (increasing number of irrelevant sentences to simulate the isolated effect of longer document length).\nWe define $x_{selected}$ as the selected query term, which is the duplicate term in TFC2 samples shown in Table 1. Similarly, $x_{others}$ refers to document tokens that are not duplicates of any query token. For each case, we calculate the normalized sum of attention values from all query tokens to $x_{selected}$ and $x_{others}$, representing the \"total semantic match\" of the matched token or the average across unmatched tokens, respectively.\nAs the occurrences of $x_{selected}$ increase, its average attention value grows (Figure 6). On the other hand, the attention for $x_{others}$ remains relatively constant, which aligns with our soft-TF understanding: only document tokens that are semantically similar to a query token get nonzero attention values, proportional to the extent and frequency of semantic similarity. Furthermore, for most heads, the TFC2 trend is preserved in the attention values. For LNC1, injecting more irrelevant sentences leads to an increase in the attention value of all query tokens, regardless of whether they match specific query terms (Figure 7). Notably, this increase in attention occurs even as the overall relevance score decreases (as expected from LNC1), suggesting that attention values encode mixed and composite signals of soft-TF, term saturation, and document length, which the model has learned to disentangle to produce appropriate relevance score changes. The varying degrees of influence on the heads' attention values shown in Figure 6 and 7 suggest that some Matching Heads play a more significant role in regulating these effects, and they may collectively approximate the ideal term effects. Thus, we define this complex attention value as the Matching Score to reflect that it encapsulates soft-TF, term saturation, and document length signals, three important signals of BM25.\nImportant Upstream Components. Due to time constraint, we further path patch to the three most important Matching Heads (7.9, 4.9, 2.1) to track how soft-TF flows into them. We find that the embedding exhibits the most direct influence, while previous attention and mlp layers exhibit very little influence. This completes our tracing of soft-TF path, confirming that the Matching Heads are the original \"generator\u201d of soft-TF as query token attends to soft matches to compute the Matching Score."}, {"title": "4.4 IDF in the Embedding Matrix", "content": "In addition to soft-TF, IDF is a critical signal for Relevance Scoring Heads. Here, we use SVD and low-rank intervention to track where IDF is stored or computed.\nIDF exists in the first low-rank component of the embedding matrix. Previously, Choi et al. [3] provide correlational evidence of IDF information existing in the embedding matrix. We decompose the embedding space into orthogonal components using Singular Value Decomposition (SVD). Recall that the embedding matrix $W_E$ is structured such that each row corresponds to a word's representation in the vocabulary. The columns of $W_e$ represent different dimensions of this vector space, capturing various latent features of the words.\nSVD allows us to decompose a complex matrix into a sum of rank-1 matrices, ordered by their contribution to the overall structure of the matrix. A rank-1 matrix is the outer product of a column and row vector, capturing a single direction in the data. This decomposition enables us to represent $W_E$ in orthogonal directions, allowing us to analyze the dominant directions in an easily interpretable manner due to their rank-1 structure. Mathematically, the SVD of $W_E$ is expressed as:\n$W_E = USV^T = \\sum_{i=1}^r \\sigma_i u_i v_i^T$\nHere, $\\sigma_i$ are the singular values, which quantify the importance or strength of each rank-1 component; $u_i$ and $v_i$ left and right singular vectors, respectively, forming orthonormal bases that capture patterns in the row (token embeddings) and column spaces (feature dimensions); and $r$ is the rank of $W_E$, representing the number of non-zero singular values and independent directions in the matrix. By focusing on the largest singular values ($\\sigma_i$) and their corresponding singular vectors ($u_i$, $v_i$), we can study whether IDF is stored in dominant components of the Embedding.\nNotably, we find that the rank-1 vector with the highest singular value, we call $U_0$, has a large negative pearson correlation (-71.36%) with the IDF value from the MS-MARCO train set [20]: higher values in this low-rank component for a given vocabulary token correspond to lower IDF values for that token. This suggests that IDF information is encoded prominently within the most significant low-rank component of the embedding matrix.\nTo validate this correlation, we perform an intervention in the next section to demonstrate that the IDF values from $U_0$ have a causal effect on downstream components (i.e., relevance scoring heads) and thus, the overall relevance computation.\nIDF intervention shows causal evidence that the model uses IDF in $U_0$ for relevance computation. Given previous understanding on $U_0$, we can interpret $U_0$ as a 1-D IDF dictionary, where idx($q_i$) corresponds to the vocabulary index of $q_i$, mapping to its respective IDF value. Modifying the value at idx($q_i$) in $U_0$ allows us to adjust the importance of the Matching Score for $q_i$ (Figure 8).\nIf the model uses the IDF values encoded in $U_0$, then modifying these values should result in corresponding changes to the ranking score. Specifically, according to the BM25-based Scoring Hypothesis in \u00a74.1, there is a linear relationship between the IDF of a query token $q_i$ and the relevance score: increasing the IDF of $q_i$ should increase the relevance score. We have shown that $U_0$ is rank-1 and negatively correlated with IDF. If the model indeed uses the IDF values encoded in $U_0$, we can increase IDF ($q_i$) by decreasing $q_i$'s value in $U_0$, which would directly increase the relevance score. The converse should hold true for decreasing IDF.\nOne way to observe whether this is happening is to look at very controlled, minimal examples that we curate specifically to fully observe the effect. Using our base dataset, we create two documents, doc1 and doc2, containing distinct repeated tokens: first token (tok1) and second token (tok2), respectively (e.g., query: \"computer science department number\", doc1: \"computer computer computer\", doc2: \"science science science\"). Then, we can edit the IDF of tok1 and evaluate the IDF editing effect on the doc1 and doc2 relevance scores.\nFigure 9 offers support for the direct causal relationship: across most scaling factors, larger increase (or decrease) in tok1's IDF result in larger score increase (or decrease, respectively) for doc1. The largely monotonic trend not only offers causal evidence for the model's use of $U_0$ to acquire IDF values but also shows preliminary evidence supporting that the model computes a sum of soft-TF weighted by IDF values (\u00a74.1).\nFurthermore, the discovery of the low-rank nature of the IDF signals enables targeted interventions. Our intervention experiment demonstrates that modifying IDF values allows for precise control over the importance of specific terms within the cross-encoder."}, {"title": "5 Validation of BM25-like Computation", "content": "After we established where all the BM25-like components were initially computed or stored and how they were passed to Relevance Scoring Heads, we now assess whether the Relevance Scoring Heads perform a BM25-style computation as hypothesized in \u00a74.1. To do this, we first formalize the hypothesized function of the heads and the information flowing into them as a BM25-style linear function. Next, we evaluate the linear model's ability to reconstruct the cross-encoder scores by examining how well the hypothesized linear model fits the data."}, {"title": "5.1 Formalizing the Hypothesized Function", "content": "Specifically, as proposed in the BM-based Scoring Hypothesis, the relevance scoring heads appear to compute a summation of Matching Scores weighted by IDF. The Matching Score incorporates soft-TF, along with term saturation and document length signals, all of which are components of the BM25 function. If this hypothesis holds-that these components interact in a BM25-like manner-then the semantic scoring circuit can be expressed as a linear function:\n$linear\\_combo(U_0(q_i), MS_{total}(q_i, d_j), [U_0(q_i) MS_{total}(q_i, d_j)])$\nwhere the components of the linear combination are defined as follows:\n(1) $U_0 (q_i)$: The value of $q_i$ in the $U_0$ vector, representing the model's interpretation of $q_i$'s IDF.\n(2) $MS_{total} (q_i, d_j)$: The total Matching Score, computed as the sum of Matching Scores from individual heads ($MSH_k$) weighted by learned weights $\\alpha_k$. Each Matching Score represents the sum of attention values from $q_i$ to all document tokens:\n$MS_{total}(q_i, d_j) = \\sum_{k=1}^{13} \\alpha_k \\cdot MSH_k$\n(3) $U_0 (q_i) MS_{total} (q_i, d_j)$: The interaction term, analogous to the product of IDF and TF in BM25.\nSince the formalization contains not only the hypothesized computation function but also $U_0$ and MS from earlier subparts of the circuit, the linear model is an effective representation of the circuit."}, {"title": "5.2 Assessing the Linear Model Fit", "content": "Given the linear representation, we can determine whether our hypothesis holds by comparing the linear model's performance against the cross-encoder's actual relevance scores.\nWe first train a linear regression model using our base dataset. As the number of coefficients in our linear regression scales quickly with each additional included query term, we apply a query length cutoff to five tokens in this initial experiment to first establish feasibility of representing the circuit in this manner. When performing forward passes on the samples, we extract the following for each query token: (1) the MS (Matching Score), calculated as the sum of the query token's attention over document tokens from the 13 Matching Heads, and (2) the token's corresponding value in $U_0$, obtained by applying SVD to the embedding matrix. These serve as the input features (\"x\") for the linear regression model, while the cross-encoder's ranking scores are used as the target values (\"y\"). We then split the data into an 80/20 train-test set to evaluate how well the linear representation of the Semantic Scoring Circuit predicts the cross-encoder's relevance scores.\nThe linear regression model achieves a high pearson correlation $(corr = 0.8157, p < 0.001)$ with ground-truth relevance scores, showing that it effectively captures the core components of the cross-encoder's scoring mechanism in a simplified and interpretable form. This correlation surpasses that of the traditional BM25 scoring function under optimized parameters ($k = 0.5, b = 0.9$; $corr = 0.4200, p < 0.001$), which demonstrates the discovered $U_0$ and MS components more effectively capture the signals that the cross-encoder utilizes for ranking."}, {"title": "5.3 Generalization across IR datasets", "content": "We previously test our linear model on an MSMARCO-based dataset with a fixed number of query tokens as an initial proof of concept. Now", "using": 1, "Correlation": "quantifies the correlation between predicted and actual relevance scores; (2) Spearman Rank Correlation: assesses the consistency of the ranked lists between predictions and cross-encoder outputs; (3) NDCG@10: measures ranking performance and ensures no significant performance discrepancies. We include BM25 and a randomized set of linear regression features as baselines.\nThe experimental results confirm the hypothesized function of the Relevance Scoring Heads (\u00a74.1). Since this linear model summarizes the whole circuit as it is structured to incorporate both the computation and the necessary components, the high correlation with the cross-encoder's performance shows that our circuit"}]}