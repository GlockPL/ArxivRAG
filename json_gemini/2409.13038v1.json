{"title": "HeadCT-ONE: Enabling Granular and Controllable Automated Evaluation of Head CT Radiology Report Generation", "authors": ["Juli\u00e1n N. Acosta, MD", "Xiaoman Zhang, PhD", "Siddhant Dogra, MD", "Hong-Yu Zhou, PhD", "Sam Payabvash, MD", "Guido J. Falcone, MD, ScD, MPH", "Eric K. Oermann, MD", "Pranav Rajpurkar, PhD"], "abstract": "We present Head CT Ontology Normalized Evaluation (HeadCT-ONE), a metric for evaluating head CT report generation through ontology-normalized entity and relation extraction. HeadCT-ONE enhances current information extraction derived metrics (such as RadGraph F1) by implementing entity normalization through domain-specific ontologies, addressing radiological language variability. HeadCT-ONE compares normalized entities and relations, allowing for controllable weighting of different entity types or specific entities. Through experiments on head CT reports from three health systems, we show that HeadCT-ONE's normalization and weighting approach improves the capture of semantically equivalent reports, better distinguishes between normal and abnormal reports, and aligns with radiologists' assessment of clinically significant errors, while offering flexibility to prioritize specific aspects of report content. Our results demonstrate how HeadCT-ONE enables more flexible, controllable, and granular automated evaluation of head CT reports.", "sections": [{"title": "1. Introduction", "content": "Automated radiology report generation, leveraging artificial intelligence (AI) to produce descriptive text from medical images, has gained significant attention due to its potential to streamline clinical workflows and improve patient care (Moor et al., 2023; Rajpurkar and Lungren, 2023; Sloan et al., 2024). As AI systems approach human-like performance in generating radiology reports from images, the development of robust, automated evaluation metrics becomes paramount. However, evaluating AI-generated radiology reports presents unique challenges due to the specialized nature of medical language and the semantic complexity of these reports.\nTraditional natural language generation metrics, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), often fall short when applied to radiology, primarily focusing on lexical overlap and struggling to capture nuanced, semantic similarities crucial in medical reporting. One approach to address this limitation is the use of classification labels, such as the CheXpert F1 score (Irvin et al., 2019), which evaluates the presence or absence of specific clinical findings. However, while this method sidesteps lexical variation issues, it fails to capture the level of detailed crucial in radiology reports. More sophisticated approaches include RadGraph F1 (Yu et al., 2023), an evaluation metric based on RadGraph (Jain et al., 2021) that extracts entities and relations in reference and candidate reports and measure their overlap, providing a more meaningful evaluation but lacking robustness to variations in medical language (Figure 1), and embedding-based techniques (Zhang et al., 2020; Endo et al., 2021), which aim to capture semantic similarities by comparing vector representations of text, but output a single number, lacking explainability.\nLarge Language Model (LLM)-based evaluation methods have recently emerged as a promising approach (Huang et al., 2024; Chaves et al., 2024; Xie et al., 2024; Ostmeier et al., 2024), being more robust to stylistic differences (Banerjee et al., 2024) and aligning well with radiologists. However, current LLM-based metrics only ascertain error type and their clinical significance, limiting the potential for more granular evaluation, for example, by weighting on different pathologies, anatomical locations or descriptors, which may be more relevant for different clinical scenarios or distinct models being evaluated.\nWe argue that automated information extraction, coupled with accurate normalization to domain-specific ontologies can address these limitations by"}, {"title": "2. Related Work", "content": "There have been multiple efforts to develop reliable metrics for radiology report generation. Traditional natural language generation metrics, such as BLEU, ROUGE, and METEOR primarily focus on lexical overlap and struggle to capture the nuanced, semantic similarities that are crucial in medical reporting. (Yu et al., 2023) introduced RadGraph F1, a more clinically meaningful evaluation metric based on extracting entities and relations from reports using RadGraph (Jain et al., 2021). However, the inherent variability in radiological language where multiple phrases can express the same clinical finding\u2014poses a challenge for this type of metric. This variability can lead to the underestimation of similarity between clinically equivalent but lexically different reports.\nEmbedding-based evaluation metrics, such as BERTScore (Zhang et al., 2020) and SemB score (Endo et al., 2021), have attempted to address these limitations by leveraging pre-trained language models to capture semantic similarities. While these approaches offer improvements over traditional lexical overlap metrics, they still face challenges in the medical domain. These metrics often lack explainability, making it difficult for clinicians to interpret and trust the scores. Additionally, when applied at the report level, they struggle to provide granular insights or allow for fine-grained control over the evaluation process, which is crucial in the nuanced field of radiology.\nLarge Language Model (LLM)-based evaluation approaches have emerged as a promising alternative for assessing radiology reports, with multiple methods recently introduced, including FineRadScore (Huang et al., 2024), CheXprompt (Chaves et al., 2024), DocLens (Xie et al., 2024) and GREEN (Ostmeier et al., 2024), offering several advantages over traditional metrics. These methods demonstrate improved alignment with human judgment and exhibit robustness to lexical variations, capturing semantic similarities that might elude conventional metrics. However, LLM-based evaluations come with their own set of challenges, being limited to the categorization of error types and their clinical significance, which they struggle to capture accurately. Further, running these metrics at scale can be prohibitively expensive, especially when dealing with large datasets common in medical imaging. Moreover, the use of LLMs raises significant privacy concerns, particularly when handling sensitive medical data or proprietary datasets."}, {"title": "3. Method", "content": "Primary dataset. We leverage a large proprietary dataset of 101,319 head CT and their corresponding radiology reports from three health systems in the United States and multiple sites within them, containing studies from 35,380 patients. Mean age at the time of study is 65.7 (SD 18.7), and 57% of the studies are from female patients. This dataset is private and will not be available to other researchers.\nNER models training data. We randomly sampled 2,000 radiology reports from this dataset to train our NER models.\nExperiments data. Original reports. We manually selected 1 normal and 1 abnormal report per site for the 20 sites with most studies in the dataset. Additionally, we randomly sampled a non-overlapping group of 400 reports from these same 20 distinct sites.\nModified reports. We prompt GPT-4-0 through the Azure OpenAI secure API to obtain 5 different versions of these reports: rephrased, any error, observation errors, anatomical errors, and descriptor errors. Prompts are shown in the Appendix."}, {"title": "3.1. Dataset"}, {"title": "3.2. Ontology", "content": "Our team developed a specialized ontology for the three main entity types of the expanded information extraction schema.\nObservation ontology: We built on the ontology tree published by Buchlak et al. (2023). Specifically, we simplified the ontology to remove terms related to anatomy (e.g., basal cistern effacement \u2192 effacement) or descriptors (e.g., acute hemorrhage \u2192 hemorrhage), which will be already captured as other entity types in our schema, and introduced terms not covered by this ontology tree. This observation ontology covers elemental findings present in head CT reports, including pathological findings, devices or surgical changes. The resulting ontology tree can be seen in the Appendix.\nDescriptor ontology: We developed a comprehensive and extendable ontology of features associated with radiological findings or anatomical structures in medical imaging. These features include common descriptors such as quantity, size, shape, severity, temporality (e.g., acute, chronic), among many other categories. The full ontology, which can be extended, is presented on the Appendix.\nAnatomy ontology: We utilize the Foundational Model of Anatomy (FMA) (Rosse and Mejino, 2003), an well-validated ontology of anatomical knowledge. We limit our pipeline to use entities reachable by starting with anatomical structures related to head CTs (i.e., Head, and related anatomical structures, such as Epidural Space), with a maximum depth of 5 to avoid entities too granular too appear in reports."}, {"title": "3.3. Information Extraction Framework", "content": "We present a new information extraction framework that extends RadGraph (Jain et al., 2021) by introducing a distinct \"descriptor\" entity type. This separates descriptors from observations, which were previously combined in RadGraph. Our framework retains RadGraph's anatomy and observation entities while adding this new category, enhancing the precision of information extraction from radiology reports. Our framework utilizes four relations: suggestive of, associated with, located at, and modify. To develop models capable of extracting information from head CT reports according to our schema, We use GPT-4 (Achiam et al., 2023) to generate labeled entities and relations for a subset of our data. The prompts used for annotation are provided in the Appendix. Based on the annotated data, we train a Named Entity Recognition (NER) model using the Princeton University Relation Extraction (PURE) architecture (Zhong and Chen, 2021). This architecture employs a pipeline approach, decomposing the tasks of entity recognition and relation extraction into separate subtasks. We apply the trained model as the first step of HeadCT-ONE to extract all entities and relations from the reports."}, {"title": "3.4. Ontology Normalization", "content": "Following information extraction, we map all extracted entities to our predefined ontologies to ensure consistency and enable standardized analysis. For observation and anatomy entities, we leverage BioLORD-2023 (Remy et al., 2024), a state-of-the-art multilingual model, to generate high-fidelity embeddings of all ontology terms. We then match each extracted entity to the ontology term with the highest embedding similarity within its entity type. We also apply some minor rule-based postprocessing pipelines to refine the extracted entities. For example, \"frontoparietal\" would be separated into \"frontal\" and \"parietal\" to align with our ontology structure. For descriptors, given their limited categorical nature, we employ GPT-4 to classify extracted entities into our predefined descriptor ontology categories using a prompt provided in the Appendix. This normalization process facilitates consistent and controllable analyses across diverse reports."}, {"title": "3.5. HeadCT-ONE Metric", "content": "HeadCT-ONE metric assesses the model's ability to correctly identify entities (including the new descriptor type) and their relations in radiological reports. The HeadCT-ONE metric is calculated as the average of two weighted F1 scores: one for entity recognition and one for relation extraction. Weights are assigned to individual items (entities or relations) based on criteria such as entity type, relation type, and clinical relevance. This weighting mechanism allows the metric to be tailored to specific evaluation needs.\nDefine $e_{gt}$ and $e_{pred}$ as the sets of ground truth and predicted entities, and $r_{gt}$ and $r_{pred}$ be the sets of ground truth and predicted relations, respectively. The metric is calculated as the average of two weighted F1 scores:\nHeadCT-ONE = $\\frac{F1(e_{gt}, e_{pred}) + F1(r_{gt}, r_{pred})}{2}$ ,\nwhere F1 is a weighted F1 score calculated based on precision (P) and recall (R)."}, {"title": "3.6. Institutional Review Board (IRB)", "content": "This research utilizes anonymized data and does not require IRB approval."}, {"title": "4. Results", "content": "Ontology normalization enables HeadCT-ONE to consistently identify similar concepts across diverse radiology reports. To evaluate this, we conducted a comparative analysis using reports from 20 different sites. For each site, one normal report and one abnormal report were selected. Our experiment tested how well various metrics could recognize similarities between normal reports and distinguish between normal and abnormal reports, regardless of differences in writing styles."}, {"title": "4.1. Analysis on Original Reports", "content": "Our results demonstrate that HeadCT-ONE consistently outperformed RadGraph, achieving higher scores across all metrics. This superiority suggests that normalizing the data using ontologies significantly improves the overall analysis performance.\nWeighting on OBS-P enables HeadCT-ONE to easily distinguish between normal and abnormal reports. This indicates that the weighted approach in HeadCT-ONE is more sensitive to the differences between normal and abnormal reports.\nThe effectiveness of this weighting strategy is further demonstrated by HeadCT-ONE's performance on normal pairs, with a perfect score of 1 for normal pairs in 19 out of 20 sites. The one site where HeadCT-ONE did not achieve a perfect score was due to an error assignment of \"aeration\" as \"observation_present\" in the sentence \"The bone windows demonstrate normal aeration of the paranasal sinuses and mastoid air cells\", leading to this discrepancy. This example suggests that there is room for further refinement in its entity recognition accuracy, particularly for terms that may have nuanced meanings depending on their context."}, {"title": "4.2. Analysis on Modified Reports", "content": "The weighting mechanism for entity types allows HeadCT-ONE to focus on specific aspects of AI-generated reports. HeadCT-ONE's weighting mechanism for entity types provides a powerful tool for customizing the analysis of AI-generated radiology reports. This is evident in the results where the weighting scheme (1, 0, 0, 0) for (OBS-P, OBS-A, ANAT, DESC) yields the highest F1 score differences for observation errors (Reph-Obs: 0.251) and general errors (Reph-Any: 0.242). Similarly, if the focus is on detecting anatomy-related errors, assigning a higher weight to the anatomy (ANAT) category improves performance in this area."}, {"title": "4.3. Alignment with Radiologists", "content": "We investigate the correlation between automated metrics and radiologist assessments of radiology reports to validate the effectiveness of the HeadCT-ONE metric. A senior radiology resident classified modified reports as with and without significant errors compared to candidate reports."}, {"title": "5. Discussion", "content": "Our study introduces HeadCT-ONE, an ontology-enhanced information extraction-derived metric for head CT radiology report generation. Our results showcase that HeadCT-ONE offers significant improvements over existing metrics in terms of robustness, granularity, and controllability.\nEntity normalization enhances information extraction-derived metrics. Medical ontologies have a widespread use in healthcare, but their integration into radiology report generation and evaluation is challenging. Widely used ontologies such as SNOMED-CT (Gaudet-Blavignac et al., 2021), International Classification of Diseases (ICD) (Who, 2005) and Unified Medical Language System (UMLS) (Bodenreider, 2004) do not generalize well to the language utilized in radiology reports. While RadLex (Chepelev et al., 2023) aims to address radiology-specific terminology, its scope may not fully encompass all subspecialty terms (Datta et al., 2020), and its granularity can be inconsistent, sometimes lacking detail for certain findings while being overly specific for others, potentially affecting standardized usage across different radiologists and institutions. In our study, we modified an ontology tree specifically designed for head CT findings, and developed an ontology for radiology descriptors based on head CT but easily extendable to other CT study types. In addition, we use the FMA for anatomical ontology as we found it to match radiological terms better than other ontologies. In our experiments, we show that the normalization step improves the capability of HeadCT-ONE to identify similar reports regardless of variations of radiological language.\nEntity weighting allows for controllable evaluation and better alignment with experts. Radiology reports encompass a complex hierarchy of information, where the clinical significance of elements varies across different study types and analytical objectives. For instance, when focusing on diagnostic accuracy, the presence or absence of critical observations may be paramount, while the comprehensive use of descriptors would be relevant when evaluating report completeness. This variability extends to specific clinical scenarios: in a head trauma context, the evaluation might prioritize acute findings such as intracranial hemorrhage, midline shift, or skull fractures. Conversely, in the longitudinal follow-up of hydrocephalus, the focus may shift to changes in ventricular size and shunt positioning. This multifaceted variability in the importance of report elements poses a significant challenge for traditional evaluation metrics, which often treat all terms with equal weight. Our pipeline introduces a weighting mechanism that allows for focused evaluation on broad or specific entity types. This controllability enables more nuanced and clinically relevant assessments, as evidenced by the improved correlation with radiologist evaluations. Moreover, in recognizing the current limitations of generative AI in radiology, our approach allows for strategic leniency in certain areas. For instance, one could modulate the impact of discrepancies in quantitative descriptors like precise measurements, which AI systems may struggle to consistently reproduce.\nLimitations. Our study is not without limitations. The normalization process, while effective, may occasionally misclassify terms. Further, our analyses were done on synthetic reports, which may not completely represent outputs from real head CT report generation models. Additionally, the creation of comprehensive ontologies for different radiological domains remains a challenging and time-consuming task requiring domain knowledge. Looking ahead, promising directions for future research include combining our information extraction and normalization approach with the increasing capabilities of LLMs. LLM have shown to perform well at information extraction (Liu et al., 2023). For normalization, LLMs could provide more sophisticated and accurate term classification, enhancing the overall reliability of HeadCT-ONE. Additionally, leveraging LLMs for data-driven ontology creation could significantly improve scalability, reducing the need for extensive domain knowledge and expert involvement. Furthermore, as accurate head CT report generation models become available, validating HeadCT-ONE on their outputs will be crucial to ensure its effectiveness in evaluating real-world generated reports.\nIn conclusion, HeadCT-ONE enhances AI-generated head CT report evaluation, offering improved robustness, granularity, and controllability, being easily extendable to other imaging modalities."}]}