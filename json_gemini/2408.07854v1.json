{"title": "CON-FOLD Explainable Machine Learning with Confidence", "authors": ["MCGINNESS LACHLAN", "BAUMGARTNER PETER"], "abstract": "FOLD-RM is an explainable machine learning classification algorithm that uses training data to create a set of classification rules. In this paper we introduce CON-FOLD which extends FOLD-RM in several ways. CON-FOLD assigns probability-based confidence scores to rules learned for a classification task. This allows users to know how confident they should be in a prediction made by the model. We present a confidence-based pruning algorithm that uses the unique structure of FOLD-RM rules to efficiently prune rules and prevent overfitting. Furthermore, CON-FOLD enables the user to provide pre-existing knowledge in the form of logic program rules that are either (fixed) background knowledge or (modifiable) initial rule candidates. The paper describes our method in detail and reports on practical experiments. We demonstrate the performance of the algorithm on benchmark datasets from the UCI Machine Learning Repository. For that, we introduce a new metric, Inverse Brier Score, to evaluate the accuracy of the produced confidence scores. Finally we apply this extension to a real world example that requires explainability: marking of student responses to a short answer question from the Australian Physics Olympiad.", "sections": [{"title": "Introduction", "content": "Machine Learning (ML) has been shown to be incredibly successful at learning patterns from data to solve problems and automate tasks. However it is often difficult to interpret and explain the results obtained from ML models. Decision trees are one of few ML methods that offer transparency with regards to how decisions are made. They allow users to follow a set of rules to determine what the outcome of a task (say classification) should be. The difficulty with this approach is finding an algorithm that is able to construct a reliable set of decision trees.\nOne approach of generating a set of rules equivalent to a decision tree is the First Or-der Learner of Default (FOLD) approach introduced by Shakerin et. al. in 2017 [21]. To improve a model's ability to handle exceptions in rule sets, Shakerin, Wang, Gupta and others introduced and refined an explainable ML algorithm called First Order Learner of Default (FOLD) [23, 21, 24, 26, 25, 16] which learns non-monotonic stratified logic pro-grams [20]. The FOLD algorithm is capable of handling numerical data (FOLD-R) [21], multi-class classification (FOLD-RM) [26] and image inputs (NeSyFOLD) [16]. FOLD-SE uses Gini Impurity instead of information gain in order to obtain a more concise sets of rules [25]. Thanks to these improvements, variants of the FOLD algorithm are now competitive with state of the art ML techniques such as XGBoost and RIPPER in some domains [24, 25]."}, {"title": "Formal Framework and Background", "content": "We work with usual logic programming terminology. A (logic program) rule is of the form\n$h : - l_1, . . ., l_k, not e_1, . . ., not e_n$\n(1)\nwhere the $h, l_1, . . ., l_k$ and $e_1, . . ., e_n$ all are atoms, for some k, n \u2265 0. A program is a finite set of rules. We adopt the formal learning framework described for the FOLD-RM algo-rithm. In this, two-ary predicate symbols are used for representing feature values, which can be categorial or numeric. Example feature atoms are name(il,sam) and age(i1,30) of an individual i1. Auxiliary predicates and Prolog-like built-in predicates can be used as well. Rules always pertain to one single individual and its features as in this example:\nfemale (X) :- rule1(X).\n(1)"}, {"title": "Related Work", "content": "Confidence scores have been introduced in decision tree learning for assessing the admis-sibility of a pruning step [19]. In essence, decision tree pruning removes a sub-tree if the"}, {"title": "The CON-FOLD Algorithm and Confidence Scores", "content": "The CON-FOLD algorithm assigns each rule a confidence score as it is created. For easy interpretability, confidence scores should be equal to probability values (p-values from a binomial distribution) in the case of large amounts of data. However, p-values would be a very poor approximation in the case of small amounts of training data; if a rule covered only one training example it would receive a confidence value of 1 (100%).\nThere are many techniques for estimating p-values from a sample, we chose the centre of the Wilson Score Interval [27] given by Equation 2 below. The Wilson Score Interval adjusts for the asymmetry in the binomial distribution which is particularly pronounced in the case of extreme probabilities and small sample sizes. It is less prone to producing misleading results in these situations compared to the normal approximation method, thus making it more trustworthy for users [2].\n$p=\\frac{n_p+Z^2}{n + Z^2}$,\n(2)\nwhere p is the confidence score, $n_p$ is the number of training examples corresponding to the target class covered by the rule, n is the number of training examples covered by the"}, {"title": "Improvement Threshold Pruning", "content": "Improvement Threshold Pruning is designed to stop rules from overfitting data by becom-ing unnecessarily \u2018deep'. The rule structure in FOLD allows for exceptions, for exceptions"}, {"title": "Confidence Threshold Pruning", "content": "In addition to decreasing the depth of rules it is also desirable to decrease the number of rules. A confidence threshold is used to determine whether a rule is worth keeping in the model or whether it is too uncertain to be useful. If the rule has a confidence value below the confidence threshold then it is removed. This effectively means that rules could be removed on two grounds:\n\u2022 There are insufficient examples in the training data to lead to a high confidence value.\n\u2022 There may be many examples in the training data which match the rule, however a large fraction of the examples are actually counterexamples which go against the rule.\nThe two pruning methods introduced above can greatly simplify a model making it more human interpretable. However, the pruning of rules reduces the model's ability to fit noise and can lead to an increase in accuracy. However if rules are pruned to harshly then the model will be underfit and performance will decrease. In order to assess these effects we applied CON-FOLD to a sample dataset, the E.coli dataset. In our experiments, we varied the values for the two threshold parameters corresponding to the two pruning methods. This allowed us to assess the performance with respect to accuracy and to derive recommendations for parameter settings, see Figure 2. For this dataset, accuracy is highest for pruning with a low confidence threshold and a moderate improvement threshold. If the pruning is too harsh this leads to a significant decrease in performance."}, {"title": "Inverse Brier Score", "content": "A standard measure of performance in ML is accuracy which for multi-class tasks is defined by:\nAccuracy = $\\frac{1}{N} *\\sum_{i=1}^{N} A(y^*_i, y_i)$\nWhere: $A(y^*, y) = 1$ if $y^* = y$ and 0 otherwise.\n(4)\nWhen predictions are made with confidence scores it is possible to use more sophisti-cated measures of the model's performance. In particular the model should only be given a small reward if it makes a correct prediction with a low confidence, and the model"}, {"title": "Manual Addition of Rules and Physics Marking", "content": "A key advantage of FOLD over other ML methods is that users are able to incorpo-rate background knowledge about the domain in the form of rules. In addition to fixed background knowledge we include modifiable initial knowledge. Initial knowledge can be provided with or without confidence values. During training CON-FOLD can add con-fidence values to provided rules, prune exceptions and even prune these rules if they do not match the dataset.\nIn order for rules to be added to a FOLD model they must be admissible rules as defined below, with optional confidence values. Admissible rules obey the following conditions:\n\u2022 Every head must have a predicate of the value to be decided."}, {"title": "Results", "content": "We compare the CON-FOLD algorithm to XGBoost [7], a standard ML method, to FOLD-RM, and FOLD-SE, which is currently the state of the art FOLD algorithm. The results can be found in Table 1. The experiments use datasets from the UCI Machine"}, {"title": "Discussion", "content": "The runtime for XGBoost on the weight lifting data in Table 1 appears anomalously long. This has been observed previously [25] and can be attributed to the large number of features (m = 155) in the dataset which is an order of magnitude greater than the others. This result confirms that both CON-FOLD and the pruning algorithm scales well with the number of input features.\nTable 1 shows that the pruning algorithm reduces the number of rules compared to the FOLD-RM algorithm with a small decrease in accuracy. The main advantage of decreasing the number of rules is that it makes the results more interpretable to humans; as having hundreds of rules becomes quite difficult for a human to follow. Furthermore a smaller set of rules reduces the inference time of the model. The FOLD-SE algorithm produces a smaller number of rules and higher accuracy for most datasets. We note that the CON-FOLD pruning technique and the use of Gini-Impurity from FOLD-SE are not mutually exclusive and applying both may result in even even more concise results while maintaining performance.\nOur experiment in Figure 3 shows that for small amounts of stratified training data, the ability to put confidence values on predictions gives CON-FOLD a significant ad-vantage over XGBoost. We also note that pruning gives a very slight advantage in cases of very small training data sets. We attribute this to the prevention of the model from overestimating confidence from only a small number of examples.\nFor the physics marking dataset, the rules created from the marking scheme align almost perfectly with the scores that students were actually awarded, as expected. This results in very strong performance even when there is very little training data. We note that the unpruned CON-FOLD algorithm gradually increases its IBS as the amount of training data increases. We attribute this to a combination of increasing confidence in rules that were learned and being able to learn more complex rules from larger training data sets.\nFor the XGBoost experiments with features generated by regular expressions in Figure 4f, we note that the algorithm is able to score approximately 0.92 consistently until"}, {"title": "Conclusion and Future Work", "content": "We have introduced confidence values that allow users to know the probability that a rule from a FOLD model will be correct when applied to a dataset. This removes the illusion of certainty when using rules created by the FOLD algorithm. We introduce a pruning algorithm that can use these confidence values to decrease the number and depth of rules. The pruning algorithm allows for the number of rules to be significantly decreased with a small impact on performance, however it is not as effective as the use of the Gini Impurity methods in FOLD-SE.\nInverse Brier score is a metric that can be used to reward accurate forecasting of probabilities of rules while maintaining compatibility with non-probabilistic models by reducing to accuracy in the case of non probabilistic predictions.\nCON-FOLD allows for inclusion of background and initial knowledge into FOLD mod-els. We use the marking of short answer physics exams as a potential use case to demon-strate the effectiveness of incorporating readily-available domain knowledge in the form of a marking scheme. With this background knowledge, the CON-FOLD model's per-formance is significantly improved and out performs XGBoost, especially in presence of small amounts of training data.\nBesides improvements to the FOLD algorithm, an area for future work is feature extraction from short snippets of free-form text. NLP feature extraction tools were not able to capture the required features to implement a marking scheme, but otherwise were able to extract enough features to allow for accuracy and Inverse Brier Score of over 99% in the regime of large amounts of training data. Optical Character Recognition could allow for the grading of hand-written responses to be explored. As a final suggestion for future research, more advanced NLP tools such as Large Language Models (LLMs) could be used to allow for automated extraction of numbers and units from text."}]}