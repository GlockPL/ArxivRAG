{"title": "CON-FOLD Explainable Machine Learning with Confidence", "authors": ["MCGINNESS LACHLAN", "BAUMGARTNER PETER"], "abstract": "FOLD-RM is an explainable machine learning classification algorithm that uses training data to create a set of classification rules. In this paper we introduce CON-FOLD which extends FOLD-RM in several ways. CON-FOLD assigns probability-based confidence scores to rules learned for a classification task. This allows users to know how confident they should be in a prediction made by the model. We present a confidence-based pruning algorithm that uses the unique structure of FOLD-RM rules to efficiently prune rules and prevent overfitting. Furthermore, CON-FOLD enables the user to provide pre-existing knowledge in the form of logic program rules that are either (fixed) background knowledge or (modifiable) initial rule candidates. The paper describes our method in detail and reports on practical experiments. We demonstrate the performance of the algorithm on benchmark datasets from the UCI Machine Learning Repository. For that, we introduce a new metric, Inverse Brier Score, to evaluate the accuracy of the produced confidence scores. Finally we apply this extension to a real world example that requires explainability: marking of student responses to a short answer question from the Australian Physics Olympiad.", "sections": [{"title": "Introduction", "content": "Machine Learning (ML) has been shown to be incredibly successful at learning patterns from data to solve problems and automate tasks. However it is often difficult to interpret and explain the results obtained from ML models. Decision trees are one of few ML methods that offer transparency with regards to how decisions are made. They allow users to follow a set of rules to determine what the outcome of a task (say classification) should be. The difficulty with this approach is finding an algorithm that is able to construct a reliable set of decision trees.\nOne approach of generating a set of rules equivalent to a decision tree is the First Or-der Learner of Default (FOLD) approach introduced by Shakerin et. al. in 2017 [21]. To improve a model's ability to handle exceptions in rule sets, Shakerin, Wang, Gupta and others introduced and refined an explainable ML algorithm called First Order Learner of Default (FOLD) [23, 21, 24, 26, 25, 16] which learns non-monotonic stratified logic pro-grams [20]. The FOLD algorithm is capable of handling numerical data (FOLD-R) [21], multi-class classification (FOLD-RM) [26] and image inputs (NeSyFOLD) [16]. FOLD-SE uses Gini Impurity instead of information gain in order to obtain a more concise sets of rules [25]. Thanks to these improvements, variants of the FOLD algorithm are now competitive with state of the art ML techniques such as XGBoost and RIPPER in some domains [24, 25]."}, {"title": "Formal Framework and Background", "content": "We work with usual logic programming terminology. A (logic program) rule is of the form\n$h :- l_1, ..., l_k, not e_1, ..., not e_n$\nwhere the $h, l_1, ..., l_k$ and $e_1, ..., e_n$ all are atoms, for some $k, n \\geq 0$. A program is a finite set of rules. We adopt the formal learning framework described for the FOLD-RM algo-rithm. In this, two-ary predicate symbols are used for representing feature values, which can be categorial or numeric. Example feature atoms are name(i1,sam) and age(i1,30) of an individual i1. Auxiliary predicates and Prolog-like built-in predicates can be used as well. Rules always pertain to one single individual and its features as in this example:\nfemale (X) :- rule1(X)."}, {"title": "Related Work", "content": "Confidence scores have been introduced in decision tree learning for assessing the admis-sibility of a pruning step [19]. In essence, decision tree pruning removes a sub-tree if the classification accuracy of the resulting tree does not fall below a certain threshold, e.g., in terms of standard errors, and possibly corrected for small domain sizes. Decision trees can be expressed as sets of production rules [18], one production rule for each branch in a decision tree. The production rules can again be simplified using scoring functions [18].\nThe FOLD family of algorithms learns rules with exceptions by means of default nega-tion (negation-as-failure). The rules defining the exceptions can have exceptions them-selves. This sets FOLD apart from the production systems learned by decision tree clas-sifiers, which do not take advantage of default negation. This may lead to more complex rule sets. Indeed, [26] observe that \u201cFor most datasets we experimented with, the number of leaf nodes in the trained C4.5 decision tree is much more than the number of rules that FOLD-R++/FOLD-RM generate. The FOLD-RM algorithm outperforms the above methods in efficiency and scalability due to (i) its use of learning defaults, exceptions to defaults, exceptions to exceptions, and so on (i) its top-down nature, and (iii) its use of improved method (prefix sum) for heuristic calculation.\u201d We do feel, however, that these experimental results could be completed from a more conceptual point of view. This is beyond the scope of this paper and left as future work.\nScoring functions have been used in many rule-learning systems. The common idea is to allow accurracy to degrade within given thresholds for the benefit of simpler rules. See [12] for a discussion of the more recent ILASP3 system and the references therein. Hence, we do not claim originality of using scoring systems for rule learning. We see our main contribution differently and not in competition with other systems. Indeed, one of the main goals of this paper is to equip an existing technique that has been shown to work well \u2013 FOLD-RM \u2013 with confidence scores. With our algorithm design and experimental evaluation we show that this goal can be achieved in an \u201calmost modular\u201d way. Moreover, as our extension requires only minimal changes to the base algorithm, we expect that our method is transferrable to other rule learning algorithms."}, {"title": "The CON-FOLD Algorithm and Confidence Scores", "content": "The CON-FOLD algorithm assigns each rule a confidence score as it is created. For easy interpretability, confidence scores should be equal to probability values (p-values from a binomial distribution) in the case of large amounts of data. However, p-values would be a very poor approximation in the case of small amounts of training data; if a rule covered only one training example it would receive a confidence value of 1 (100%). There are many techniques for estimating p-values from a sample, we chose the centre of the Wilson Score Interval [27] given by Equation 2 below. The Wilson Score Interval adjusts for the asymmetry in the binomial distribution which is particularly pronounced in the case of extreme probabilities and small sample sizes. It is less prone to producing misleading results in these situations compared to the normal approximation method, thus making it more trustworthy for users [2].\n$p = \\frac{n_p + \\frac{Z^2}{2}}{n + Z^2},$\nwhere p is the confidence score, $n_p$ is the number of training examples corresponding to the target class covered by the rule, n is the number of training examples covered by the rule corresponding to all classes, and Z is the standard normal interval half width, by default we use $Z = 3$.\nTheorem 4.1 In the limit where there is a large amount of data classified by a rule (n \u2192 \u221e), the confidence score approaches the true probability of the sample being from the target class.\nProof. The true probability that a randomly selected example that follows the provided rule is from the target class is $p_r = \\frac{n_p}{n}$. As n increases, the law of large numbers states that the portion of examples which belong to the target class becomes $lim_{n\u2192\u221e} \\frac{N_p}{n} = p_rn$. Also, as n \u2192 \u221e, the relative contribution of $Z^2$ terms becomes negligible. Therefore:\n$lim_{n\u2192\u221e} p = lim_{n\u2192\u221e} \\frac{n_p + \\frac{Z^2}{2}}{n + Z^2} = lim_{n\u2192\u221e} \\frac{\\frac{n_p}{n}}{1} = p_r$\nOnce rules have the associated confidence scores they are expressed as follows:\np :: h : - l1, ..., lk, not e1, ..., not en\nwhere p is the confidence score. The format of confidence score annotations is directly supported by probabilistic logic programming systems such as Problog [8] and Fuse-mate [4]. In this paper, we do not explore this possibility and just let the confidence scores allow the user to know the reliability of a prediction made by a rule in the logic program."}, {"title": "Improvement Threshold Pruning", "content": "Improvement Threshold Pruning is designed to stop rules from overfitting data by becom-ing unnecessarily \u2018deep'. The rule structure in FOLD allows for exceptions, for exceptions to exceptions, and then for exceptions to these exceptions and so on. This may overfit the model to noise in the data very easily. We will refer to any exception to an exception at any depth as a sub-exception.\nIn order to avoid this overfitting, each time a rule is added to the model each exception to the rule is temporarily removed and a new confidence score is calculated. If this changes the confidence by less than the improvement threshold then this exception is removed. If an exception is kept then this process is applied to each sub-exception. This process is repeated until each exception and sub-exception has been checked."}, {"title": "Confidence Threshold Pruning", "content": "In addition to decreasing the depth of rules it is also desirable to decrease the number of rules. A confidence threshold is used to determine whether a rule is worth keeping in the model or whether it is too uncertain to be useful. If the rule has a confidence value below the confidence threshold then it is removed. This effectively means that rules could be removed on two grounds:\n\u2022 There are insufficient examples in the training data to lead to a high confidence value.\n\u2022 There may be many examples in the training data which match the rule, however a large fraction of the examples are actually counterexamples which go against the rule.\nThe two pruning methods introduced above can greatly simplify a model making it more human interpretable. However, the pruning of rules reduces the model's ability to fit noise and can lead to an increase in accuracy. However if rules are pruned to harshly then the model will be underfit and performance will decrease. In order to assess these effects we applied CON-FOLD to a sample dataset, the E.coli dataset. In our experiments, we varied the values for the two threshold parameters corresponding to the two pruning methods. This allowed us to assess the performance with respect to accuracy and to derive recommendations for parameter settings"}, {"title": "Inverse Brier Score", "content": "A standard measure of performance in ML is accuracy which for multi-class tasks is defined by:\n$Accuracy = \\frac{1}{N} * \\sum_{i=1}^{N} A(y^*_i, y_i)$\nWhere: $A(y^*, y) = 1$ if $y^* = y$ and 0 otherwise.\nWhen predictions are made with confidence scores it is possible to use more sophisti-cated measures of the model's performance. In particular the model should only be given a small reward if it makes a correct prediction with a low confidence, and the model should be punished when making high confidence predictions that are incorrect. We have three desiderata that for such a scoring system:\n1. It is a proper scoring system: the maximum reward can be gained when the prob-ability value given matches the true probability value.\n2. The scoring system reduces to accuracy when non-probabilistic predictions are made. This allows for the comparison between probabilistic and non-probabilistic models in terms of performance.\n3. The scoring system has an inbuilt mechanism for dealing with no given prediction.\nIn order to meet all three of these desiderata we propose a variant of the Brier scoring system and call it Inverse Brier Score (IBS). Brier score (or quadrature score) is often used to evaluate the quality of weather forecasts [3, 13] and can be obtained with the following formula [5]:\n$Brier Score = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} (P_{ki} - Y_{ik})^2,$\nwhere $i$ is an index over each data point in a test dataset, $k$ corresponds to a class in the dataset. $N$ is the total number of test examples and $K$ the total number of classes. This is commonly reduced to the following when only making predictions for one class [15]:\n$One Class Brier Score = \\frac{1}{N} \\sum_{i=1}^{N} (P_{i} - Y_{i})^2$\nWe define the IBS as:\n$IBS = 1 - \\frac{1}{N} \\sum_{i=1}^{N} (P_{i} - Y_{i})^2$\nTheorem 5.1 Inverse Brier Score is a proper scoring system.\nProof. It known that the Brier Score is a proper scoring system [15]. Since the propriety of a scoring system is preserved under linear transformations, the IBS is a proper scoring system.\nTheorem 5.2 When definite predictions are made (p = 1 or p = 0), IBS is equivalent to accuracy.\nProof. For non-probabilistic decisions, $p_i$ is replaced by the prediction $y^*_i$. Therefore $(y^*_i - Y_i)^2 = 1 - A(y^*_i, y_i)$ and the IBS reduces to the definition of accuracy as follws:\n$1 - \\frac{1}{N} \\sum_{i=1}^{N} 1 - A(y^*_i, y_i) = 1 - \\frac{N}{N} + \\frac{1}{N} \\sum_{i=1}^{N} A(y^*_i, y_i) = \\frac{1}{N} \\sum_{i=1}^{N} A(y^*_i, y_i)$\nFinally, IBS has a natural way of dealing with no predictions being made. If any class was given a prediction of $p = 0.5$, then IBS would be 0.75 regardless of the class that is chosen. This provides a natural default value if a model refuses to make a prediction due to low confidence, satisfying the third desideratum. Note that this is also important for the FOLD algorithm because it is possible that it will find a test sample which is significantly different to the training examples and may not match any rules.\nTherefore, the IBS satisfies all three desiderata. Both IBS and accuracy are used to evaluate CON-FOLD against other models in Section 7. We note that confidence values are regularly used in weather forecasting where Brier score is used as a metric to evaluate their quality. We use this metric to show that FOLD models with confidence scores obtain higher scores. Although Brier Score does not always align with user's expectations [10], it is a metric which indicates that models with confidence are more interpretable."}, {"title": "Manual Addition of Rules and Physics Marking", "content": "A key advantage of FOLD over other ML methods is that users are able to incorpo-rate background knowledge about the domain in the form of rules. In addition to fixed background knowledge we include modifiable initial knowledge. Initial knowledge can be provided with or without confidence values. During training CON-FOLD can add con-fidence values to provided rules, prune exceptions and even prune these rules if they do not match the dataset.\nIn order for rules to be added to a FOLD model they must be admissible rules as defined below, with optional confidence values. Admissible rules obey the following conditions:\n\u2022 Every head must have a predicate of the value to be decided.\n\u2022 Each body must consist only of predicates of values corresponding to features.\n\u2022 Bodies can be Boolean combinations literals.\n\u2022 The <, >, <, >, = and \u2260 operators are allowed for comparison of numeric variables.\n\u2022 For categorical variables only = and \u2260 are allowed.\nGiven formulas of this form are translated into logic programs. Stratified default negation is in place to ensure that rule evaluation is done sequentially, mirroring a decision list structure. Therefore the rules are in a hierarchical structure and the order in which the initial/background knowledge is added can influence model output in the case of overlapping rule bodies.\nInclusion of background and initial knowledge can be very helpful in cases where only very limited training data is available. An example of such a problem domain is grading a students' responses to physics problems. Usually, background domain knowledge is easily available in the form of well defined rules for how responses should be scored.\nWe evaluated this idea with data from the 2023 Australian Physics Olympiad provided by Australian Science Innovations. The data included 1525 student responses to 38 Aus-tralian Physics Olympiad questions and the grades awarded for each of these responses. We chose to mark the first question of the exam because most students attempted it and the answer is simply a number with units and direction. This problem was also favourable because the marking scheme was simple with only three possible marks, 0, 0.5 and 1. Responses to this question were typed so there was no need to for Optical Character Recognition (OCR) to interpret hand written work. An example of a rule used for marking is:\ngrade(1,X) :- rule1(X).\nrule1 (X) :- correct_number(X), correct_unit(X).\nIn order to apply the marking scheme, relevant features would need to be extracted from the student responses. Feature extraction is an active areas of research in Natural Language Processing [17]. Many approaches focus on extracting information and rela-tionships about entities from large quantities of text and can extract large numbers of features [22, 6]. The tools which use term frequency can struggle with short answers [14] especially if they contain large numbers of symbols and numbers, making them inappro-priate for feature extraction for grading physics papers.\nIn our work we use SpaCy [9] for Part of Speech (POS) tagging to extract noun phrases and numbers to be used as keywords. The presence or absence of these keywords is then one hot encoded for each piece of text to create a set of features. This method alone was not sufficient to extract sufficiently sophisticated features that would allow the marking scheme to be implemented. Therefore we also used regular expressions to extract the required features for the marking scheme. This required significant customisation to match the wide variety of notations used by students."}, {"title": "Results", "content": "We compare the CON-FOLD algorithm to XGBoost [7], a standard ML method, to FOLD-RM, and FOLD-SE, which is currently the state of the art FOLD algorithm. The results can be found in Table 1. The experiments use datasets from the UCI Machine Learning Repository [11]. The comparison uses 30 repeat trails where a random 80% of the data is selected for training and the remaining 20% is used for testing.\nThe hyperparameters for all XGBoost experiments are the defaults in the existing Python implementation\u00b2. In all FOLD-RM and CON-FOLD experiments, the ratio is set to 0.5 (default in FOLD-RM).\nThe hyperparameters for the pruned CON-FOLD algorithm are included with the results. FOLD-SE was not in-cluded in the experiments as the im-plementation is not publicly available, but values from Wang et. al. 2023 [25] are included. We note that the accu-racy and number of rules for XGBoost and FOLD-RM are very similar to the results given in the 2023 study and are confident that the experiments are equivalent. The times measured are wall time measured when the result is run on a PC with an Intel Core i9-13900K CPU with 64GB of RAM. Note that when only small amounts of training data were used we ensured that at least one example of each class was included in the training data; we will refer to this as stratified training data."}, {"title": "Discussion", "content": "The runtime for XGBoost on the weight lifting data in Table 1 appears anomalously long. This has been observed previously [25] and can be attributed to the large number of features (m = 155) in the dataset which is an order of magnitude greater than the others. This result confirms that both CON-FOLD and the pruning algorithm scales well with the number of input features.\nTable 1 shows that the pruning algorithm reduces the number of rules compared to the FOLD-RM algorithm with a small decrease in accuracy. The main advantage of decreasing the number of rules is that it makes the results more interpretable to humans; as having hundreds of rules becomes quite difficult for a human to follow. Furthermore a smaller set of rules reduces the inference time of the model. The FOLD-SE algorithm produces a smaller number of rules and higher accuracy for most datasets. We note that the CON-FOLD pruning technique and the use of Gini-Impurity from FOLD-SE are not mutually exclusive and applying both may result in even even more concise results while maintaining performance.\nOur experiment in Figure 3 shows that for small amounts of stratified training data, the ability to put confidence values on predictions gives CON-FOLD a significant ad-vantage over XGBoost. We also note that pruning gives a very slight advantage in cases of very small training data sets. We attribute this to the prevention of the model from overestimating confidence from only a small number of examples.\nFor the physics marking dataset, the rules created from the marking scheme align almost perfectly with the scores that students were actually awarded, as expected. This results in very strong performance even when there is very little training data. We note that the unpruned CON-FOLD algorithm gradually increases its IBS as the amount of training data increases. We attribute this to a combination of increasing confidence in rules that were learned and being able to learn more complex rules from larger training data sets.\nFor the XGBoost experiments with features generated by regular expressions in Figure 4f, we note that the algorithm is able to score approximately 0.92 consistently until 1.8% of the training data is reached. Then the model's performance seems to vary wildly between trials before jumping to an accuracy of 0.99 at 2.8%. We attribute the instability as sensitivity to specific training examples being included in its training data. Once 2.8% of the data is included in training this seems to settle as this is a sufficient amount that the required examples reliably fall into the training data set. A similar pattern is also present for Figure 4c.\nFor small amounts of training data, IBS of the CON-FOLD algorithm is mostly in-dependent of whether regular expression features were included or not. However in the regime of large amounts of training data shown in Figure 4a and Figure 4d, manually extracted features make a small but significant improvement in the performance of both XGBoost and CON-FOLD. This has implications for the use of automated feature POS tagging tools when being used for feature extraction for automated marking. Regular expressions for feature extraction allow for more accurate results and for the implemen-tation of background rules, but this comes at significant development costs."}, {"title": "Conclusion and Future Work", "content": "We have introduced confidence values that allow users to know the probability that a rule from a FOLD model will be correct when applied to a dataset. This removes the illusion of certainty when using rules created by the FOLD algorithm. We introduce a pruning algorithm that can use these confidence values to decrease the number and depth of rules. The pruning algorithm allows for the number of rules to be significantly decreased with a small impact on performance, however it is not as effective as the use of the Gini Impurity methods in FOLD-SE.\nInverse Brier score is a metric that can be used to reward accurate forecasting of probabilities of rules while maintaining compatibility with non-probabilistic models by reducing to accuracy in the case of non probabilistic predictions.\nCON-FOLD allows for inclusion of background and initial knowledge into FOLD mod-els. We use the marking of short answer physics exams as a potential use case to demon-strate the effectiveness of incorporating readily-available domain knowledge in the form of a marking scheme. With this background knowledge, the CON-FOLD model's per-formance is significantly improved and out performs XGBoost, especially in presence of small amounts of training data.\nBesides improvements to the FOLD algorithm, an area for future work is feature extraction from short snippets of free-form text. NLP feature extraction tools were not able to capture the required features to implement a marking scheme, but otherwise were able to extract enough features to allow for accuracy and Inverse Brier Score of over 99% in the regime of large amounts of training data. Optical Character Recognition could allow for the grading of hand-written responses to be explored. As a final suggestion for future research, more advanced NLP tools such as Large Language Models (LLMs) could be used to allow for automated extraction of numbers and units from text."}]}