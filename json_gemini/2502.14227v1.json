{"title": "SleepGMUformer: A gated multimodal temporal neural network for sleep staging", "authors": ["Chenjun Zhao", "Xuesen Niu", "Xinglin Yu", "Long Chen", "Na Lv", "Huiyu Zhou", "Aite Zhao"], "abstract": "Sleep staging is a key method for assessing sleep quality and diagnosing sleep disorders. However, current deep learning methods face challenges: 1) post-fusion techniques ignore the varying contributions of different modalities; 2) unprocessed sleep data can interfere with frequency-domain information. To tackle these issues, this paper proposes a gated multimodal temporal neural network for multidomain sleep data, including heart rate, motion, steps, EEG (Fpz-Cz, Pz-Oz), and EOG from WristHR-Motion-Sleep and SleepEDF-78. The model integrates: 1) a pre-processing module for feature alignment, missing value handling, and EEG de-trending; 2) a feature extraction module for complex sleep features in the time dimension; and 3) a dynamic fusion module for real-time modality weighting.Experiments show classification accuracies of 85.03% on SleepEDF-78 and 94.54% on Wrist HR-Motion-Sleep datasets. The model handles heterogeneous datasets and outperforms state-of-the-art models by 1.00%-4.00%.", "sections": [{"title": "1. Introduction", "content": "Sleep is a pivotal procedure for human health [1] and is associated with mood, behavior, and physiological functions such as memory and cognition [2]. Sleep staging is a central aspect of sleep assessment and research the accuracy of sleep staging is not only relevant to the assessment of sleep quality [3] but also key to achieving early intervention for sleep disorders and related psychiatric disorders [4].\nPolysomnography is a multi-parameter study of sleep [5], a test to diag-nose sleep disorders through different types of physiological signals recorded during sleep, such as electroencephalography (EEG), cardiography (CG), electrooculography (EOG), electromyography (EMG), oro-nasal airflow and oxygen saturation [6]. According to the Rechtschaffen and Kales (R&K) rule, PSG signals are usually divided into 30-second segments and classified into six sleep stages, namely wakefulness (Wake), four non-rapid eye movement stages (i.e., S1, S2, S3, and S4), and rapid eye movement (REM). In 2007, the American Academy of Sleep Medicine (AASM) adopted the Rechtschaffen & Kales (R&K) sleep staging system for Non-Rapid Eye Movement (NREM) sleep. Sleep specialists typically utilize these criteria for the manual classi-fication of sleep stages, a process that is not only labor-intensive but also prone to subjective bias [7]. Therefore, automated sleep staging is a more efficient alternative to manual methods and has more clinical value [8]. Mean-while, due to the shortcomings of polysomnography itself, on the one hand, the cost and resource consumption will make it prohibitive to implement polysomnography in areas with backward medical conditions, on the other hand, the need to use multiple electrodes and sensors during the test will cause physical discomfort and affect the patient's sleep quality. Therefore, people have begun to study the use of wearable devices for sleep detection, through the Apple Watch to collect relevant sleep data.\nSimple deep learning networks are unable to capture time-varying features and time-series information in sleep signals.To overcome these limitations, Supratak et al. proposed DeepSleepNet, an innovative deep-learning model that combines the strengths of CNNs and Bi-LSTMs [9]. DeepSleepNet is ca-pable of simultaneously extracting both time-invariant features and learning time-series information for a more comprehensive understanding and analysis of sleep data. Although DeepSleepNet has made breakthroughs in the field of sleep staging, it mainly adopts a one-to-one input-output model, which is direct and transparent but ignores the transition rules that exist between dif-"}, {"title": "2. METHOD", "content": "devices, heart rate, breathing, and step count feature information are aligned in the time dimension according to AASM rules, and missing values are processed using interpolation methods."}, {"title": "2.1. Model overview", "content": "Firstly, given a dataset {Sn}_1 of size N, Sn = ({(X(n), ..., X(n)}, y(n))) is the nth in a 30-second period containing the C channel data sets. In the SleepEDF-78 data set, $X_{i}^{(n)} \\in \\mathbb{R}^{T \\times F}$, $1 \\leq i \\leq C$, denotes a time-frequency image extracted from a 30-second period EEG or EOG that can effectively represent specific wave and frequency components, where T is a time frame value of 29 and F is a frequency interval value of 128. In the WristHR-Motion-Sleep dataset, $X_{i}^{(n)} \\in \\mathbb{R}^{T \\times F}$, $1 \\leq i \\leq C$, represents the sleep data in terms of inner heart rate or respiration data or number of steps per 30s, $X_{i} = {x_{j} \\in \\mathbb{R}^{N} | j = 1, 2, ...,T}$, with T denoting the subsequence length, $x_{j} = {A_{1}, A_{2}, ..., A_{F}}$, and F is the number of signals collected per epoch. In addition, $y^{(n)} \\in {0,1}^{K}$, y is the label, where K = 5 represents the five stages of sleep staging.\nFigure 2. shows the model architecture of SleepGMUformer, which con-sists of 1) single-channel temporal feature extraction; 2) multi-channel dy-namic feature fusion; and 3) classification. For each modal channel, a tempo-ral feature extraction module based on a self-attention mechanism extracts feature mappings from time-frequency images or timing data. These are independent so that they can adaptively capture different features from mul-timodal physiological signals. In the Multi-Channel Dynamic Feature Fusion module, post-fusion is performed by adding a GMU module, which effec-tively allows the model to weight modalities at the instance level, improving its performance while providing a better interpretability mechanism. Two fully connected layers and a softmax activation function at the end of the network complete the classification."}, {"title": "2.2. Preprocessing", "content": "For the SleepEDF-78 dataset, firstly, sampled records labeled as \u201cmotion\u201d and \"unknown\" in the original dataset were removed prior to the experiment. Then, according to the convention established in previous studies, 30-minute sleep phases (60 minutes in total) were removed from the recordings when subjects got out of bed, as these sleep phases were labeled as wakefulness. Subsequently, N3 and N4 were uniformly labeled as N3 to obtain the cate-gorical label y \u2208 {W, N1, N2, N3, REM}, following the guidelines provided in the AASM manual. Finally, the EEG data were de-trended.\nThe principle of Detrended Fluctuation Analysis (DFA) is to characterize the data by subtracting a fitted straight line from the dot plot image of the original data to eliminate the trend changes presented by the data. The polynomial curve is fitted using the least squares method to obtain the fitted curve of the trend of the image, and the principle of polynomial fitting based on the least squares method is deduced as follows:\nFirstly, P = {(X1,Y1), (X2,Y2), ..., (xn, Yn)}, P is a set of points that we assume, where the functional equation for x, y satisfies: f(xi) = Yi, i = 1,2,...,n. Suppose the m-order polynomial function is:\n$f(x_{i}, w_{j}) = w_{0} + w_{1}x_{i} + w_{2}x_{i}^{2} + ... + w_{m}x_{i}^{m}$\n$= \\sum_{j=0}^{m} w_{j}x_{i}^{j}$\nwhere w is the polynomial coefficient. If this m-order polynomial is to be used to represent the relationship between x and y, then the error between the polynomial value and the true value is\n$e_{i} = f (x_{i}) \u2212 f (x_{i}, w_{j}) = Y_{i} \u2013 \\sum_{j=0}^{m} w_{j}x_{i}^{j}$\nPolynomial fitting is performed using the least squares method to find the optimal set of polynomial coefficients to minimize the total error of the entire set of points after fitting. The problem of minimizing the total error can be transformed into minimizing the sum of squares of the errors. To obtain the sum of squares of the error for the entire set of points, the error sum and minimum are obtained by taking the partial derivative of Wk and making it zero\n$E(w) = \\sum_{i=1}^{n} (Yi \u2013 \\sum_{j=0}^{m} w_{j}x_{i}^{j}))^{2}$\n$\\frac{\\partial E(w)}{\\partial W_{k}} = 2\\sum_{i=1}^{n} \\sum_{j=0}^{m}((w_{j}x_{i}^{j} - Yi)x_{i}^{k}) = 0$\nThe derivation of the formula gives:\n$\\sum_{i=1}^{n} \\sum_{j=0}^{m} w_{j}x_{i}^{j+k} = \\sum_{i=1}^{n} x_{i}^{k}Yi  k = 0, 1, 2, ..., m$\nWriting it in matrix form gives:\n$ \\begin{bmatrix} \\sum_{i=1}^{n} 1 & \\sum_{i=1}^{n} x_{i} & \\sum_{i=1}^{n} x_{i}^{2} & ... & \\sum_{i=1}^{n} x_{i}^{m} \\\\ \\sum_{i=1}^{n} x_{i} & \\sum_{i=1}^{n} x_{i}^{2} & \\sum_{i=1}^{n} x_{i}^{3} & ... & \\sum_{i=1}^{n} x_{i}^{m+1} \\\\ \\sum_{i=1}^{n} x_{i}^{2} & \\sum_{i=1}^{n} x_{i}^{3} & \\sum_{i=1}^{n} x_{i}^{4} & ... & \\sum_{i=1}^{n} x_{i}^{m+2} \\\\ : & : & : & & : \\\\ \\sum_{i=1}^{n} x_{i}^{m} & \\sum_{i=1}^{n} x_{i}^{m+1} & \\sum_{i=1}^{n} x_{i}^{m+2} & ... & \\sum_{i=1}^{n} x_{i}^{2m} \\end{bmatrix} \\begin{bmatrix} w_{0} \\\\ w_{1} \\\\ w_{2} \\\\ : \\\\ Wm \\end{bmatrix} = \\begin{bmatrix} \\sum_{i=1}^{n} Yi \\\\ \\sum_{i=1}^{n} x_{i}Yi \\\\ \\sum_{i=1}^{n} x_{i}^{2}Yi \\\\ : \\\\ \\sum_{i=1}^{n} x_{i}^{m}Yi \\end{bmatrix}$\nCalculated through math $\\sum_{i=1}^{n} x_{i}^{j}$, j = 0, 1, 2, . . ., 2m and $\\sum_{i=1}^{n} x_{i}^{j}Yi$ = 0, j = 0, 1, 2, . . ., m, substituting into the above equation, the polynomial coefficients w are obtained and the fitted curve is obtained.\nThe image after detrending is obtained by subtracting the value of the ver-tical coordinate under the corresponding horizontal coordinate of the result-ing fitted curve from the value of the vertical coordinate of the corresponding point on the dot plot of the original data. Then, after the original data were detrended, according to the AASM scoring manual, EEG, EOG, EMG, and major body movements are the basis for sleep staging, and specific wave and frequency components are important features. Such as low-frequency com-ponents in the 4-7 Hz range are frequently observed in stage N1, and sleep spindle (SS) or K-complex (KC) waves are hallmarks of stage N2. Finally, by applying STFT and logarithmic scaling, we convert the raw signals from each channel into a time-frequency image as an input to the model, which can effectively represent those specific wave and frequency components, and the whole process is shown in Figure 3(a).\nFor the Wrist HR-Motion-Sleep dataset: heart rate data is acquired every 5s, respiration data is acquired every 0.02s, and labeling follows the AASM sleep stage division rule every 30s. Since the data acquisition frequency of each modality of sleep is different, and the start time of each modality ac-quisition is also different, it is necessary to process the raw data with feature alignment and missing values. Firstly, We filtered out 1500 respiratory data and 6 heart rate data contained within the same 30s in which each label was located according to the label division time. Then, missing values are filled for the label at the corresponding moment of each respiratory data within the 30s while missing values are filled at the corresponding moment for the heart rate data within the same 5s in which the corresponding moment of each res-piratory data is located. Finally, a few movements are recorded during sleep, and the corresponding moments of the supplemental tracts are recorded as 0 if there are none, as shown in Figure 3(b).\nConsidering the uncertainty of the measured magnitude and the pos-sibility of outliers, the raw data need to be preprocessed to fit a normal distribution by normalizing the values of its elements into the [0,1] parti-tion to increase the stability of the prediction. The preprocessing process is expressed as\n$\\chi = S(N(X))$\n$N(A) = \\frac{A - A_{min}}{A_{max} - A_{min}}$\n$S(A) = \\frac{A - \\mu}{\\sigma} ,   A = [a_{i,j}]_{nxm}$\nin Eq.(7) NS describes the computation process of the normalization and standardization methods, respectively, A represents a two-dimensional matrix, and \u03bcand \u03c3are the mean and variance of all the elements in matrix A. The data $X = [X_{1},X_{2},...,X_{T}]^{T}$ is obtained, and the shape of T \u00d7 Fis consistent with that of the original data.\nFinally, before the data from the two datasets are sent to the next module for single-channel temporal feature extraction, they need to pass through a feature mapping layer consisting of Multi-Layer Perception (MLP), which maps the low-dimensional raw data into a uniform high-dimensional space of dimension P. This is the first step in the process. F = \u03c7\u00b7w, $F\\in \\mathbb{R}^{T\\times P}$ high-dimensional feature passing matrix with weighting matrix W \u2208 $\\mathbb{R}^{F\\times P}$ matrix multiplication is performed to realize the dimensional transformation, in which the parameters of each moment are shared with each other and do not change in the time direction."}, {"title": "2.3. Single-channel temporal feature extraction", "content": "Based on the computational principle of signal frequency attention based mechanism, the single-channel temporal feature extraction module, which is centered on the attention mechanism, has the ability to mine temporal global dependency information from the input data in parallel. With the single-channel temporal feature extraction module, features relevant to sleep stage classification are extracted from a two-dimensional feature tensor $F\\in \\mathbb{R}^{T\\times P}$. The extractor consists of multiple Transformer blocks stacked together, and each Transformer block as shown in Figure 2(a), assumes the function of extracting features layer by layer.\nAs shown in Figure 2, to satisfy the input specifications of the signal frequency attention based mechanism module, firstly, we adjust the input to divide the tokens along the time dimension and add positional informa-tion describing the temporal relationships between the tokens. Attention mechanism to discover the dependencies between tokens by similarity calcu-lation. As shown in equation (8), $F = [f_{1}, f_{2}, ..., f_{T}]^{T} \\in \\mathbb{R}^{T\\times P}$ is divided into T tokens along the principal dimensions, each of which represents a time-step feature vector. There is an additional learnable token fo as a CLASS Token spliced with F for the final classification task. Since the transformer encoder eschews recursion and adds position encoding to the original input to intro-duce temporal information, the position encoding tensor PE \u2208 $\\mathbb{R}^{(T+1)\\times P}$ is computed, and PE is added to the T+1 feature tokens by an element-by-element addition operation to obtain a token F = {fo, f1, f2, ..., fr}, and the resulting F is input into a Transformer encoder consisting of multiple Transformer blocks.\n$F = PE + [f_{0} | F]^{T}$\n$PE_{(pos,2i)} = sin(\\frac{pos}{10000^{P}})$\n$PE_{(pos,2i+1)} = cos(\\frac{pos}{10000^{P}})$\nPE denotes the position encoding matrix. We use sine and cosine functions to encode the position information where pos is the position, 0 \u2264 pos < T + 1, 2i or 2i + 1 is the number of dimensions of the input and 0 \u2264 2i < 2i +\n1 < p. Each dimension of the position encoding corresponds to a sinusoidal waveform, which allows the model to easily learn the relative positions based on the features of the sine and cosine functions.\nWhen a feature consisting of T+1 tokens ${f_{0}^{i-1}, f_{1}^{i-1},..., f_{T}^{i-1}}$ into the attention module of the i-th head, according to the formula (9), the calcu-lation is carried out to get ${f_{0}^{i}, f_{1}^{i}, ..., f_{T}^{i}}$ and then input into the attention module of the next head, the structure of the attention mechanism module Figure 2(b) is shown.\n$f_{j}^{i} = \\sum_{k=0}^{T} S_{j,k}^{i} f_{k}^{i-1}$\n$S_{j,k}^{i} = softmax(\\frac{Q_{j}^{i-1} K_{j}^{i-1}}{\\sqrt{d}})$\n$V_{j}^{i-1} = \\xi(f_{j}^{i-1}W_{v})$\n$Q_{j}^{i-1} = \\xi(f_{j}^{i-1}W_{Q})$\n$K_{j}^{i-1} = \\xi(f_{j}^{i-1}W_{K})$\nwhere $W_{v}$, $W_{Q}$, $W_{K}$ \u2208 $\\mathbb{R}^{P}$ are the weight metrics corresponding to $f_{j}^{i-1}$, \u03be is the activation function, and d is the dimension of the feature for which the scaling operation is performed. As shown in the overall model Figure 2, after the last transformer block is processed, the entire temporal feature extractor output of CLASS Token $F_{1} \u2208 \\mathbb{R}^{P}$ indexed as 0 serves as the final discriminative feature for each channel."}, {"title": "2.4. Multi-channel dynamic feature fusion", "content": "In most of the previous sleep staging models, a fully connected layer is usually used to directly concatenate the features extracted from each channel, which we believe is not optimal because different modal channels correlate differently with different sleep stages. For sleep polysomnography data, EEG is the main indicator in non rapid eye movement sleep (NREM) EEG is dominated by theta waves without spindle or K complex waves during N1, EEG is dominated by spindle and K complex waves with less than 20% delta waves during N2, and delta waves are dominated during N3. EOG, on the other hand, is the main indicator to distinguish between rapid eye movements and non-rapid eye movements. For the WristHR-Motion-Sleep dataset, non-rapid eye movement sleep (NREM) results in decreased heart rate, frequent postural adjustments, and uniform breathing. In the rapid eye movement sleep (REM) stage, breathing is slightly faster and irregular, and body temperature and heart rate are elevated, with relatively sedentary physical activity. and the fully connected layer cannot correctly weight the relevance of each modality. In this work, we propose to adjust by changing the connectivity fusion with the GMU module.\nThe structure of the GMU module is shown in Figure 2(c), the GMU module receives the feature vector xi \u2208 $\\mathbb{R}^{P}$ of modality i. Firstly, it com-putes the hidden features hi, and then it computes the gates zi that control the contribution of each modality to the overall output of the GMU module. Lastly, it compares the gates $z_{1}, z_{2}, \u2026, z_{N}$ with hidden features $h_{1}, h_{2}, ..., h_{N}$ are fused, and since with $W_{i} \u2208 \\mathbb{R}^{shared\\times p}$ all modalities have the same di-mension, they can be directly weighted by the gates zi. The resulting GMU module has a global view of all modalities and can realize dynamic fusion due to the different correlations of different modal channels with different sleep stages, with the following formula.\n$h_{i} = tanh(W_{i}x) \u2208 \\mathbb{R}^{shared}$\n$z_{i} = \\sigma(W_{zi} [x_{i}]_{i=1}^{N}) \u2208 \\mathbb{R}^{shared}$\n$h = \\sum_{i=1}^{N} z_{i}h_{i}$\nwhere N is the number of modes, [x_{i}]_{i=1}^{N} denotes the result of splicing from vector x1 to Xxn, and stands for matrix multiplication."}, {"title": "2.5. Classifier", "content": "The classifier consists of a block of two fully connected layers that feed the results of the multichannel dynamic feature fusion [25] into the classifier, in the first fully connected layer the ReLU activation function is used, followed by the dropout layer. The two fully connected layers select and combine the features learned in the previous structure, and finally the softmax function is used to generate the output probabilities of mutually exclusive classes for sleep stage classification."}, {"title": "3. EXPERIMENTS", "content": ""}, {"title": "3.1. Datasets", "content": "Two publicly available datasets i.e. SleepEDF-78 [26] and WristHR-Motion-Sleep were used in this study.We used the Fpz-Cz EEG, Pz-Oz EEG, and ROC-LOC EOG (horizontal) channels in the PSG of the SleepEDF-78 and heart rate, respiration, and activity patterns recorded by a wearable de-vice in the WristHR-Motion-Sleep to complete the sleep stage classification task.\nSleepEDF-78: This is a subset of the Sleep - EDF extended dataset (2018 version) which has been extended to include 78 subjects between the ages of 25 and 101 with 153 overnight PSG sleep recordings. Similar to the SleepEDF-20, the researchers performed two nights of PSG recordings for each subject. Due to an equipment error, one recording was lost for each of the 13-, 36-, and 52-year-old subjects. Trained technicians manually per-formed sleep staging of the corresponding PSG signals according to R&K criteria and categorized each 30-s sleep period into labels W, N1, N2, N3, N4, REM, MOVEMENT, UNKNOWN.\nWristHR-Motion-Sleep: subjects spent the night in the laboratory with an eight-hour sleep opportunity, during which the subjects' respiration and heart rate were recorded using a WristHR-Motion-Sleep while polysomnog-raphy was performed. The dataset contained 39 subjects, excluding four patients with data transmission errors, three patients with sleep apnea, and one patient with REM sleep behavior disorder, divided into sleep stages (0-5, WAKE = 0, N1 = 1, N2 = 2, N3 = 3, REM = 5)."}, {"title": "3.2. Parameters", "content": "To extract the time-frequency image, the 30-second calendar element of the PSG signal is transformed into a time-frequency image by a 256-point STFT. A 2s Hamming window with 50% overlap was chosen as the win-dow function. We used logarithmic scaling on the spectrum to convert to a log-power spectrum. This produces an image $X \u2208 \\mathbb{R}^{T\\times F}$ where F = 128 fre-quency bins and T = 29 time points. The time-frequency images computed from each channel were normalized to the zero mean and unit variance of all time-frequency pairs and input to SleepGMUformer.\nIn the transformer encoder of our network, each sublayer of the trans-former encoder uses a dropout rate of 0.4, and we use H = 8 attention heads and 128 hidden units in the feedforward layer. For each channel, features are extracted using 3 transformer encoders. In the last two fully connected layers, 64 hidden units are used with a dropout rate of 0.5.\nThe experiments were conducted on two databases, WristHR-Motion-Sleep and SleepEDF-78, respectively. The dataset is divided into three parts based on the experimental ratio:60% as a training set, 20% as a validation set, and 20% for final testing. The division process is performed based on truth labels to ensure a balanced data distribution. An Adam optimizer with a learning rate of 5 \u00d7 10-3 is used to train the network. The optimizer performs degree descent based on this loss function. Finally, the batch size used for training is 256."}, {"title": "3.3. Performance assessment metrics", "content": "We used accuracy, Kappa coefficient, Macro F1 score (MF1), average sensitivity, and average specificity as overall performance metrics, whereas category-specific performance was evaluated using the class-wise MF1s for values used in the evaluation of this experiment."}, {"title": "3.4. Experimental results", "content": "As shown in Figure 4, the confusion matrix is used to show the per-formance of SleeoGMUformer on the datasets SleepEDF-78 and WristHR-Motion-Sleep, respectively, and we can see that the model SleeoGMUformer has excellent performance on both datasets and especially on the WristHR-Motion-Sleep dataset has the outstanding performance. We observe that most of the stages are accurately classified except for N1, possibly due to the fact that N1 is the first transition stage between wakefulness and sleep,"}, {"title": "3.4.1. Sleep staging performance", "content": "Table.1 shows the performance of SleeoGMUformer on the experimen-tal dataset and compares it with previous works. Accuracy, Cohen's kappa (k), macro f1 score (MF1), average sensitivity and average specificity were used as overall performance metrics, while category-specific performance was assessed using the class-wise MF1s.\nOn the SleepEDF-78 database, our model has an overall accuracy of 85.03% and a k value of 0.83. On the one hand, our model has an abso-lute improvement of 2.10% in accuracy and 0.07 in k compared to SeqSleep-Net.This result indicates that the methodology used by our model is more than SeqSleepNet has an advantage. On the other hand, the performance of our model is comparable to the existing state-of-the-art model XSleep-Net2. In terms of accuracy, our model is 1.00% higher than XSleepNet2, and in terms of k-value, it is 0.05 higher, showing that our model is com-petitive with XSleepNet2 in general. The class-wise MF1s further reveal the differences between them. Our model seems to favor sleep stages that are non-rapid eye movement stages (N1, N2, N3). In contrast, XSleepNet2 is superior in wakefulness stages (WAKE) and is comparable to and slightly superior to us in REM. Specifically, our model has an MF1 score of 57.45% in the N1 phase, 87.04% in the N2 phase, and 80.83% in the N3 phase. and 91.84% in the Wake phase. On the contrary, in the case of XSleepNet2, its MF1 score is 92.42% in the Wake phase, which indicates that XSleepNet2 performs better in this phase."}, {"title": "3.4.2. Hypnogram", "content": ""}, {"title": "3.4.3. Confidence estimation", "content": "By analyzing the model's confidence in its predictions, it can help us better understand the model's performance [37]. If the model has high con-fidence in most of the correct predictions and low confidence in incorrect predictions, this may indicate good model performance. In sleep staging, if the model has high confidence in the staging, then the physician or auto-mated system can rely on this result with more confidence. Conversely, if the confidence level is low, then additional manual review or further testing is required [38]. In previous studies, it has been found that times of misclas-sification are often associated with low confidence. For sleep in particular, classification of transitional epochs (whose sleep stages are different from their preceding and/or subsequent neighboring sleep stages) is often difficult [39].\nWe further demonstrate the above findings in Figure 6. In the figure, we perform a confidence quantification depicted along with the multi-class prob-ability output for the above figure by performing a hypnogram to analyze a full night's sleep of subject A. The confidence quantification is depicted in the figure. With a confidence threshold of 0.5, we can see that the vast ma-jority of the confidence levels are higher than the threshold, indicating that our model performs very well. Typically, on the other hand, low confidence tends to occur in the transition period between sleep stages, where the model has increased predictive uncertainty and also, by extension, the potential to be misclassified."}, {"title": "3.5. Ablation experiment", "content": "results that SleepGMUformer performs better in sleep stage classification on the WristHR-Motion-Sleep dataset than on the SleepEDF-78 dataset.\nIn order to prove the validity of our SleepGMUformer model, the module, we performed ablation experiments. We compared the original model in SleepEDF-78 as well as some variants on WristHR-Motion-Sleep, where other parameters are the same as the original model, respectively. These variants in SleepEDF-78 are described as follows.\n1) EEG Fpz-Cz+EEG Pz-Oz: two-channel feature extraction block for processing only the Fpz-Cz EEG and Pz-Oz EEG channels.\n2) EEG Fpz-Cz+EOG: two-channel feature extraction block, only for processing Fpz-Cz EEG and EOG channels.\n3) EEG Pz-Oz+EOG: two-channel feature extraction block, only used to process Fpz-Cz EEG and EOG channels.\n4) Concatenation: three channels are concatenated before input, no multi-channel feature fusion block.\n5) Original Model: SleepGMUformer.\nAs Figure 8. shows, the original SleepGMUformer outperforms all these variants on the SleepEDF-78 dataset. Based on 1), 2), and 3), comparing our original model with those variants using dual channels, we can see from the analysis that the scheme of EEG Fpz-Cz + EEG Pz-Oz, which only takes into account the changes in EEG activity, performs poorly. In contrast, the addition of EOG resulted in a significant improvement in performance, based on studies showing that the contribution of EOG to improving the performance of the N1 phase and the REM phase is outstanding. This is in line with our results, as rapid eye movements are dependent on EOG monitoring. It also illustrates the importance of EOG in providing another dimension of information [40]. The overall performance of the EEG Fpz-Cz + EOG scheme was slightly better than that of the EEG Pz-Oz + EOG in the conditions where EOG in considered. The overall performance of the EEG Fpz-Cz + EOG scheme was slightly better than that of the EEG Pz-Oz + EOG in the conditions where both EOGs were considered. According to the F1 scores for each level in Table 2, the Fpz-Cz showed a significant increase in the classification performance of the N2 and N3 phases, which may be due to the fact that the EEG Fpz-Cz channel typically covers the prefrontal region, a region that is associated with a wide range of cognitive functions, whereas the EEG Pz-Oz channel covers the central region and occipital lobe. The EEG Fpz-Cz channel, due to its location advantage, may be able to provide more useful information about sleep stage changes during feature extraction. For example, characteristic waveforms such as sleep spindle wave and K-complex wave may be more obvious in the Fpz-Cz channel, which in turn is one of the distinctive features of stages N2 and N3, which would be more helpful for the model to more accurately identify sleep stages. Another possibility is that the Fpz-Cz channel is potentially more complementary to the EEG Pz-Oz compared to the EOG channel, which requires further study in the future.\nThe above results show that the model with the multichannel feature fusion block outperforms the module without any single-channel variant, so we can conclude that fusing multichannel features by appropriate methods is effective for sleep staging. Another conclusion is that according to 4) the multichannel dynamic fusion block is able to effectively fuse features from different channels and is a key component of SleepGMUformer, resulting in better performance.\nIn Wrist HR-Motion-Sleep these variants are described as follows.\n1) Heart + Steps: two-channel feature extraction block for processing only the Heart and Steps channels.\n2) Heart + Motion: two-channel feature extraction block, only for pro-cessing only the Heart and Motion channels.\n3) Motion + Steps: two-channel feature extraction block, only for pro-cessing only the Motion and Steps channels.\n4) Concatenation: three channels are concatenated before input, no multi-channel feature fusion block.\n5) Original Model: SleepGMUformer.\nAs the figure shows, the original SleepGMUformer outperforms all of these variants on the WristHR-Motion-Sleep dataset. Comparing our origi-nal model with those variants using dual channels according to 1), 2) and 3), the results show that one of the three physiological signals is indispensable, and the lack of any of the single-channel models results in a significant loss of performance, where the addition of the Motion channel, which is used to record respiration, significantly improves the results. Research has shown that breathing patterns are closely related to sleep stages. The frequency, depth and pattern of breathing change during different sleep stages. For example, breathing typically becomes slower and more regular during deep sleep, while breathing becomes more rapid and irregular during REM (rapid eye movement) sleep. Therefore, incorporating respiratory information into the model can provide more physiological signals that are directly related to sleep stages, thus improving the accuracy of sleep staging. Based on the F1 scores shown in Table 3, Heart+Motion outperforms Motion+Steps in con-ditions where both breathing is considered, and heart rate has significantly improved classification performance for W and REM stages. It was shown that heart rate variability (HRV) correlates well with the sleep stage. The characteristics of HRV vary in different sleep stages, especially in the W and REM stages. This is in line with our experimental results and makes our experimental results more interpretable. Finally, according to 4) at the same time the increase of multichannel dynamic fusion block can help the model to get better sleep staging performance."}, {}]}