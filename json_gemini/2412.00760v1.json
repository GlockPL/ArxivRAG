{"title": "Automating Feedback Analysis in Surgical Training: Detection, Categorization, and Assessment", "authors": ["Firdavs Nasriddinov", "Rafal Kocielnik", "Arushi Gupta", "Cherine Yang", "Elyssa Wong", "Anima Anandkumar", "Andrew J. Hung"], "abstract": "This work introduces the first framework for reconstructing surgical dialogue from unstructured real-world recordings, which is crucial for characterizing teaching tasks. In surgical training, the formative verbal feedback that trainers provide to trainees during live surgeries is crucial for ensuring safety, correcting behavior immediately, and facilitating long-term skill acquisition. However, analyzing and quantifying this feedback is challenging due to its unstructured and specialized nature. Automated systems are essential to manage these complexities at scale, allowing for the creation of structured datasets that enhance feedback analysis and improve surgical education. Our framework integrates voice activity detection, speaker diarization, and automated speech recognition, with a novel enhancement that 1) removes hallucinations (non-existent utterances generated during speech recognition fueled by noise in the operating room) and 2) separates speech from trainers and trainees using few-shot voice samples. These aspects are vital for reconstructing accurate surgical dialogues and understanding the roles of operating room participants. Using data from 33 real-world surgeries, we demonstrated the system's capability to reconstruct surgical teaching dialogues and detect feedback instances effectively (F1 score of 0.79\u00b10.07). Moreover, our hallucination removal step improves feedback detection performance by \u224814%. Evaluation on downstream clinically relevant tasks of predicting Behavioral Adjustment of trainees and classifying Technical feedback, showed performances comparable to manual annotations with F1 scores of 0.82\u00b10.03 and 0.81\u00b10.03 respectively. These results highlight the effectiveness of our framework in supporting clinically relevant tasks and improving over manual methods.", "sections": [{"title": "1. Introduction", "content": "Importance: Formative verbal feedback delivered by a trainer to a trainee during surgical procedures is crucial for immediate correction and guidance (Wong et al., 2023) as well as for fostering long-term skill acquisition (Agha et al., 2015). High-quality feedback has been shown to significantly enhance intra-operative performance (Bonrath et al., 2015), accelerate surgical skill development (Ma et al., 2022), and bolster trainees' sense of autonomy (Haglund et al., 2021). The quality of communication has also been directly linked to operative outcomes (D'Angelo et al., 2020). This underscores the importance of understanding current practices of feedback delivery on the quality of surgical education (El Boghdady and Alijani, 2017). Given the critical role of feedback in surgical training and patient outcomes, automating feedback analysis is essential. It can help streamline the understanding of current practices, enhance trainer-trainee communication, and reduce inconsistencies in manual analysis. By improving training methods and creating structured datasets, automation enables the development of standardized, cost-efficient guidance systems (Ma et al., 2024), ultimately elevating the quality of surgical education and patient care."}, {"title": "Challenges:", "content": "Effectively quantifying and analyzing real-world surgical feedback at scale presents significant challenges. The contextual understanding of surgery, including its phases, specific procedures, and instructional tasks, necessitates deep domain expertise (Haque et al., 2024). Differentiating genuine feedback from mere comments or unrelated remarks requires a thorough familiarity with operating room interactions (Wong et al., 2023). Moreover, discerning which feedback is actionable and merits detailed annotation demands surgical teaching expertise. Additionally, the often extensive durations of surgeries coupled with the complex social dynamics among medical staff lead to many exchanges that do not qualify as clinically relevant feedback. Due to these challenges, previous studies have depended on labor-intensive manual identification and annotation of feedback by trained human raters (Wong et al., 2023; Hauge et al., 2001; Blom et al., 2007). Such manual annotation requires considerable time and effort from skilled annotators and is impractical to perform at scale, thereby hindering both the systematic quantification of feedback and the creation of structured, clinically-aligned datasets essential for developing automated feedback delivery systems (Laca et al., 2022)."}, {"title": "Approach:", "content": "We analyze full-length recordings of live surgical cases to automatically detect surgical feedback delivery moments based on the clinically validated definition of feedback from Wong et al. (2023). We further automatically categorize the content of this feedback and assess its effectiveness. We develop a framework for reconstructing teaching interactions in the operating room consisting of 3 consecutive steps as shown in Figure 1:\n1.  Dialogue Reconstruction: Combines Voice Activity Detection (VAD) to identify speech timespans in continuous audio, Speaker Diarization (DIA) to provide timespans for speech from different speakers, and Automated Speech Recognition (ASR) to transcribe audio within each timespan.\n2.  Dialogue Refinement: Removes ASR and DIA hallucinations and identifies trainer/trainee speaking turns by leveraging cosine similarity between embeddings of candidate speech segments and few-shot embedded voice samples from the trainer and trainee present in the operating room.\n3.  Clinical Task Evaluation: We demonstrate the crucial importance of each step on downstream clinically relevant tasks form Wong et al. (2023): Feedback Detection, Feedback Effectiveness Assessment, and Feedback Components Identification."}, {"title": "Findings:", "content": "\u2022\tWe show an ability to identify feedback utterances effectively with F1 of 0.79\u00b10.07 (Table 2).\n\u2022\tEvaluation on the downstream clinically relevant tasks shows performance comparable to manual annotations: F1 of 0.82\u00b10.03 for predicting Behavioral Adjustment of a trainee, F1 of 0.81\u00b10.03 for classifying Technical feedback (Table 3).\n\u2022\tWe show that our Hallucination Removal step improves performance on all tasks with the most performance gains in feedback detection with an increase of \u224814%."}, {"title": "Contributions:", "content": "\u2022\tTo the best of our knowledge, we are the first to attempt highly automated detection and assessment of surgical feedback from real-world teaching interactions in the operating room.\n\u2022\tWe provide a robust evaluation of our framework on 33 real-world surgical cases involving unseen surgeries and clinically relevant downstream tasks.\n\u2022\tAside from effectively combining existing voice tasks, we also introduce a novel step of Hallucination Removal & Speaker Identification (Figure 1), which leverages audio representations to identify trainers and filter out hallucinated text transcription fragments based on audio latent space."}, {"title": "2. Background and Related Work", "content": "Analysis of Feedback in Operating Room. Historical efforts in annotating surgical teaching interactions have primarily depended on manual observation. Hauge et al. (2001) and Blom et al. (2007) developed categorization schemes through manual annotation of recorded surgeries, involving hundreds of teaching behaviors. Ramprasad et al. (2024) manually transcribed 615 minutes of operating room interactions. Wong et al. (2023) analyzed 29 surgical videos and 3,711 interactions manually, establishing a validated feedback classification and differentiating effective from ineffective feedback across various surgical stages and trainee experience levels. However, the scalability of these manual methods is limited, impacting the automation of teaching feedback systems and the generation of comprehensive, clinically relevant datasets needed for automation.\nRecent efforts targeted the partial automation of feedback analysis in surgical training. Kocielnik et al. (2023) automated the feedback categorization scheme validated by Wong et al. (2023), using multimodal information. However, this automation was limited to the categorization phase, with the initial detection of feedback moments still dependent on manual annotations. Our work overcomes this limitation by introducing an automated system that detects feedback directly from raw recordings of surgical cases using validated definitions from Wong et al. (2023). We enhance the system's credibility by rigorously comparing its output to human annotations in clinically relevant tasks such as feedback categorization and trainee behavior change prediction."}, {"title": "Speech Reconstruction in Surgery.", "content": "Research on speech recognition in healthcare has primarily targeted non-surgical settings (Schaaf et al., 2021; Corbisiero et al., 2023), leaving significant gaps in the surgical context. The complexity of the operating room (OR) poses unique challenges for automated speech analysis systems due to non-standardized communication, background noise (Hasfeldt et al., 2010), and the dynamic interplay of multiple roles including surgeons, nurses, and anesthesiologists (Blom et al., 2007; Gardezi et al., 2009).\nIn this setting, the hallucinations in transcribed speech, which can misrepresent critical verbal exchanges is a key issue (Koenecke et al., 2024; Kuhn et al., 2024; Hasfeldt et al., 2010). Magesh et al. (2024) reports that ASR hallucinations can affect 17% to 33% of content in specialized settings. Prior work tackled this through diverse methods: using multi-step verification processes (Taki et al., 2024), audio-visual alignment (Zhang et al., 2024), majority voting from multiple ASR runs (Koenecke et al., 2024), or directly predicting hallucinated outputs (Serai et al., 2022). In surgical settings, hallucinations manifest differently, best described as \u201cnon-existent utterances generated during speech recognition fueled by noise in the operating room\u201d in line with Koenecke et al. (2024). These can occur across multiple steps of the dialogue reconstruction pipeline-voice activity detection, speaker diarization, and ASR transcription often producing repetitive affirmations like \"Yeah\", \"good\", \"thank you\" rather than factual inaccuracies. Such errors are particularly problematic in the OR, where hallucinated critical verbal exchanges can be misrepresented as approvals from the trainer or trainee acknowledgments. Our approach enhances dialogue reconstruction steps to address these challenges by filtering hallucinated responses using voice samples from the surgical team, improving both the accuracy and relevance of dialogue reconstruction in the OR. At the same time, it provides critical identification of speaking turns from trainer and trainee, which is crucial for detecting clinically relevant feedback. Our method represents a targeted solution to overcome the specific difficulties of dialogue reconstruction in surgery."}, {"title": "3. Methods", "content": ""}, {"title": "3.1. Data Acquisition", "content": "Our study utilized a dataset of genuine feedback from trainers to trainees captured during real-world robot-assisted surgical procedures obtained by Wong et al. (2023) and detailed in Table 1. The dataset covers multi-organ surgical contexts across 7 procedures, involving 4 trainers and 11 trainees. It has been rigorously annotated by 3 trained human raters. Feedback was recorded using wireless microphones on the surgeons and a video recorder capturing the surgeon's point-of-view through an endoscope camera, with all data synchronously captured using an external device and the da Vinci Xi surgical robot system (DiMaio et al., 2011). Each instance of feedback, totaling 4210, was timestamped and manually transcribed from the audio data."}, {"title": "3.2. Surgical Feedback Definition", "content": "Following the clinically validated definition by Wong et al. (2023), surgical feedback is any dialogue intended to modify trainee thinking or behavior during a live surgery. This feedback must be delivered in real-time by a trainer to a trainee who is actively operating the robotic console, allowing for immediate application and adjustment. The communication aims to influence the trainee's actions, decision-making, or understanding of the surgical task at hand and must be contextually relevant to the ongoing procedure. Social conversations and unrelated discussions within the operating room are not considered feedback."}, {"title": "3.3. Surgical Dialogue Reconstruction", "content": "The process of reconstructing surgical dialogues is outlined in Figure 1. Initially, the entire audio stream is divided into 3-minute chunks for individual processing. Each chunk is first processed using Voice Activity Detection (VAD) through python py-webrtcvad module (Wiseman, 2021), which assigns a value from 0 to 1 for every 10ms frame, where 0 indicates no activity and 1 maximum activity.\nIn addition to VAD, the raw audio undergoes Speaker Diarization (DIA) utilizing the speaker-diarization-3.1 model from Pyannote (Bredin, 2023; Plaquet and Bredin, 2023). This method identifies segments of audio containing speech and assigns random speaker IDs such as \"SPEAKER 0.\" Each segment detected by DIA is cross-verified with the VAD output, and segments without significant VAD activity (below a threshold of 0.3) are discarded. This threshold was determined through empirical testing of various values (see Appendix B).\nThe remaining segments are then transcribed using Automated Speech Recognition (ASR) with the Whisper-1 model (Radford et al., 2023), pre-trained on 680k hours of labeled English speech data for accurate transcription. This model was specifically fine-tuned for speech recognition tasks. Speech data was annotated using large-scale weak supervision."}, {"title": "3.4. Hallucination Removal & Trainer / Trainee Identification", "content": "The output of prior steps produces significant hallucinations due to background noise in the OR. Furthermore, the lack of knowledge about the role of the speaker hinders the detection of clinically meaningful guidance. To address both issues we introduced a custom dialogue refinement step.\nWe manually select and annotate a set of anchor audio segments for each trainer and trainee. These anchors are chosen to represent clear instances of each individual's voice. We identified at least 5 anchors per person, balancing the need for sufficient representation with practical constraints. The selection process involves visualizing speaking (or voice activity) times and choosing diverse segments across the surgery duration. We prioritize segments with minimal background noise and clear speech. The number of anchors (5+) was determined through empirical testing (Appendix L shows sufficient dissimilarity between separate speaker voices with the use of 5 anchors), to capture voice variations while remaining manageable in the applied context. These anchor segments are then embedded using Pyannote's \u201cembedding\u201d model (Bredin et al., 2020; Coria et al., 2020) that is based on the x-vector TDNN-based architecture (Snyder et al., 2018) with 4.2M parameters.\nOur method for hallucination removal and speaker role identification (Figure 1) processes each audio segment from the diarization step. We embed these segments using the same model as the anchors. For each segment, we compare its embedding to all anchor embeddings of the corresponding trainer and trainee using cosine similarity. We then calculate the average similarity for both trainer and trainee.\nTo identify hallucinations, we apply a threshold of 0.2 to both average similarities (see Appendix J for determining threshold). Segments falling below this threshold for both trainer and trainee are classified as hallucinations or other speakers and excluded. Otherwise, we assign the segment to either trainer or trainee based on the higher similarity score. This approach not only removes hallucinations, but also leverages domain knowledge to identify the critical roles of trainer and trainee, which are essential for understanding the surgical teaching context."}, {"title": "4. Experiments", "content": "We evaluate our method on several clinically relevant downstream tasks: Feedback Detection (Task 1), Feedback Effectiveness Assessment (Task 2), and Feedback Component Classification (Task 3). We compare the performance of our approach to human annotator baselines as well as to other solutions.\nWe evaluate models on a test set including five unseen surgery cases covering feedback from different trainer-trainee pairs under different procedures. This test set is also representative of the whole dataset in terms of class distributions under all the tasks (see Table 5 in Appendix C). We compute binary recall, precision, and F1 score (Scikit, 2024) between true and predicted labels for each task."}, {"title": "4.1. Task 1: Feedback Detection", "content": "This task involves identifying instances where the trainer provides feedback to the trainee operating the surgical console. Performance is evaluated as correctly labeling audio segments as feedback or not, compared to human annotations. Detecting feedback occurrences is crucial for assessing feedback quality. The annotation counts are in Table 1 under the \"Feedback Detection\" task. We introduce several baselines."}, {"title": "4.1.1. BASELINES", "content": "Voice Activity Detection: This approach simply detects any speech and separates it from the non-speech sounds in the OR. We then use these speech times as predicted feedback instances and align them with the human expert annotations. The performance of this baseline indicates how much of the verbal interaction in the OR is related to clinically valid teaching feedback.\nFixed-Window Temporal Event Detection: We use a 10-sec moving window with 5-sec overlap, chosen based on average feedback length. VAD is applied as a first preprocessing step, which identifies speech with sub-second precision. For detected speech fragments, we apply Automated Speech Recognition (ASR) with Whisper-1 (Radford et al., 2023) for text transcription. We then classify each window for feedback using Audio, Text, and Audio+Text late fusion models. We fine-tune Wav2Vec base (Baevski et al., 2020) for audio classification, and BERT base (Devlin et al., 2018) for text classification. For multimodal classification, we use late fusion, extracting 256-dimension vectors from audio and text, concatenating them, and passing through fully connected layers with ReLu activation and dropout.\nFor model training, we preprocess data using 10-second audio fragments (3 seconds before to account for any imperfections in annotating and 7 seconds after to capture feedback delivery). Audio is down-sampled to 16kHz mono. We fine-tune each classifier (audio, text, audio+text) with 5 different IID dataset splits using a balanced set of feedback/no-feedback instances from each surgery case for training (due to class imbalance), which was further split into 80%/20% train/val sets, separate from the test set. All hyperparameters were selected based on performance on the validation split. Models are trained for 20 epochs with an initial learning rate of 5e-5, Adam optimizer, and linear LR reduction. The best model is determined by the highest binary F1 score. We use standard fine-tuning with all weights being trainable.\nHallucination Removal - Multiple ASR Runs: Several existing methods for ASR hallucination removal described in \u00a72 address different types of hallucinations or rely on the presence of additional information not available in our context (e.g., aligned video modality, annotated hallucinations for direct prediction). One method we could compare to was proposed in Koenecke et al. (2024) and relies on running the ASR more than once and removing the transcriptions that differ as hallucinations."}, {"title": "4.1.2. EVALUATION SETUP", "content": "To assess the effectiveness of our dialogue reconstruction approach for feedback detection, we run our framework to obtain audio segments with transcriptions and speaker roles. GPT-40 classifies these for feedback presence (see prompt used in Appendix D), aligned with human annotations. We then apply a 5-second tolerance where we prompt GPT-40 to check if the phrase from the extracted dialogue and the human annotations have overlaps in transcriptions. This step is only needed for the alignment of ASR with human-transcribed instances for the purpose of evaluation. It is specifically relevant for situations where the beginning of the human-annotated transcription precedes the ASR transcription segment, but part of the content of the human transcription is still within the ASR transcribed content."}, {"title": "4.2. Task 2: Feedback Effectiveness", "content": "This task evaluates how effectively the delivered feedback impacts subsequent trainee behavior. This is measured by human expert annotated observed behaviors of the trainee post feedback delivery in two categories of Behavioral Adjustment - \u201cBehavioral adjustment made by the trainee that corresponds directly with the preceding feedback\" and Verbal Acknowledgment - \u201cVerbal or audible confirmation from the trainee confirming that they heard the feedback\"."}, {"title": "4.2.1. BASELINES", "content": "Human Selective Transcription: We leverage text transcription from human annotators available in our dataset with the same GPT-40 classifier using the same prompt but adapted for single phrases instead of a dialogue (see Appendix G). Human annotators provided transcriptions of only the trainer's feedback, without any conversational context. The details of the annotation and transcription process can be found in Wong et al. (2023)."}, {"title": "4.2.2. EVALUATION SETUP", "content": "We prompt GPT-40 to predict Verbal Acknowledgement and Behavioral Change (see Appendix F). This evaluation is done on true positive feedback phrases obtained from Task 1. True labels come from aligned human annotations."}, {"title": "4.3. Task 3: Feedback Components", "content": "This task requires categorizing feedback into 3 clinically validated components based on Wong et al. (2023). These components represent feedback categorized as Anatomic - \"familiarity with anatomic structures and landmarks\", Procedural - \"timing and sequence of surgical steps\", and Technical - \"performance of a discrete task with appropriate knowledge of factors including exposure, instruments, and traction\". This categorization is not mutually exclusive, as feedback instances can pertain to anatomic, procedural, and technical aspects simultaneously. The annotation counts for this task can be found in Table 1 under the component classification task."}, {"title": "4.3.1. BASELINES", "content": "Human Selective Transcription: We leverage the same selective transcriptions as for Task 2 (See Appendix I for the GPT-40 prompt used in this task)."}, {"title": "4.3.2. EVALUATION SETUP", "content": "We prompt GPT-40 to categorize feedback into Anatomic, Procedural, and/or Technical components in a multi-label fashion using the prompt in Appendix H. True labels come from aligned human annotations."}, {"title": "5. Results", "content": "We present results for Feedback Detection in Table 2 and for other downstream tasks of Feedback Effectiveness and Feedback Components in Table 3."}, {"title": "5.1. Task 1: Feedback Detection", "content": "Our results in Table 2 show the Precision, Recall, and F1 (Scikit, 2024) scores for detecting feedback using two techniques: Fixed-Window (baseline), and Dialogue Reconstruction. For the fixed-window, we see that the naive VAD-only classifier performed the worst with an F1 of 0.42\u00b10.20, indicating that not all speech in the operating room corresponds to feedback. Further, we see that the text and multimodal classifiers perform very similarly with F1's of 0.59\u00b10.13 and 0.58\u00b10.13, respectively, and outperform the audio classifier that achieves an F1 of 0.52\u00b10.11.\nThe results for the Dialogue row are obtained using initial dialogue reconstruction, and we see performance on par with best fixed-window classifiers. Our Hallucination Removal approach offers a big boost in performance, with an F1 of 0.66\u00b10.18, which is more effective than the baseline Hallucination Removal approach from Koenecke et al. (2024) with an F1 of 0.59\u00b10.07. Finally, identifying between Trainer and Trainee for each dialogue phrase achieves the highest F1 at 0.79\u00b10.07. Appendix K provides confusion matrices for various ablations of feedback detection.\nHigh recall is more important than high precision, as it's more important to correctly detect all the feedback instances than to have the model be correct each time it determines a segment to be feedback. This can be corrected in a subsequent human verification step. Our method performed the best with a recall of 0.71\u00b10.11 after removing hallucinations and 0.85\u00b10.09 after identifying between Trainer and Trainee."}, {"title": "5.2. Task 2: Feedback Effectiveness", "content": "Table 3 shows results for predicting Behavioral Adjustment and Verbal Acknowledgement under Feedback Effectiveness. For this task, the baseline is the classifications done on the manual annotations that yield an F1 of 0.78\u00b10.03 and 0.63\u00b10.04 for Beh. Adj. and Verb. Ack, respectively. We see that the results for the initial dialogue reconstruction slightly improve for Beh. Adj. and slightly worsen for Verb. Ack.. Further, Hallucination Removal improves both metrics with F1 of 0.82\u00b10.03 for Beh. Adj. and 0.66\u00b10.06 for Verb. Ack. Finally, identifying Trainer/Trainee does not help further."}, {"title": "5.3. Task 3: Feedback Components", "content": "Table 3 also shows results for classifying feedback as Anatomic, Procedural, and/or Technical under Feedback Components. Similar to Feedback Effectiveness, the baseline leverages manual annotations in classification. For classifying Anatomic feedback, the baseline Dialogue performs best at an F1 of 0.69\u00b10.09 while Hallucination Removal and Trainer/Trainee ID still outperform the manual annotations. For classifying Procedural feedback, Hallucination Removal yields the best results at 0.490.17 with Trainer/Trainee ID performing on par with manual annotations. For classifying Technical feedback, we see that Hallucination Removal and Trainer/Trainee ID are equal and the highest with an F1 of 0.81\u00b10.03 for both."}, {"title": "6. Discussion", "content": "Our framework achieved F1 scores of 0.79\u00b10.07 in feedback detection and up to 0.82\u00b10.08 in analyzing effectiveness, demonstrating the high feasibility of automated surgical training analysis. We enable highly automated quantification and analysis of feedback in real surgeries, with implications for improving training practices and patient care. Given the wide coverage of procedures, tasks, anatomic settings, as well as trainer-trainee interactions in our dataset as in \u00a73.1, our approach is likely to generalize to various educational clinical settings where guidance is performed via verbal feedback and dialogue. Our approach relies on data organization and preprocessing, rather than on a specific fine-tuned model, which should further reduce the reliance on a particular dataset.\nSurpassing Human Annotation in Downstream Tasks Our automation surpassed human annotations in several downstream tasks, primarily because it captured more comprehensive contextual information. Unlike humans, who transcribed only feedback itself, our method transcribed the entire dialogue involving rounds of discussion leading to feedback, better reflecting the true nature of interactions in the operating room (Wong et al., 2023). This full transcription allowed for a more effective grouping of feedback instances, combining several human-labeled segments into a single, more informative one. Secondly, some of the human transcriptions used abbreviations and selectively transcribed parts of what actually has been said. This is due to the manual and cognitive effort associated with literal transcription, which further motivates the benefits of automation. Finally, the automated system applied consistent classification criteria across all samples, avoiding the biases and inconsistencies often introduced by human annotators (Kiyasseh et al., 2023).\nUse of Few-shot Speech Samples Our dialogue refinement step using speech samples from known speakers enhances feedback detection but requires collecting clean speech before surgery and assumes consistent vocal traits during the actual procedure. This approach is effective for a fixed group of speakers. For unknown speakers or uncollectable samples, options like unsupervised speaker diarization (Xylogiannis et al., 2024) or role-based speaker recognition (Bellagha and Zrigui, 2020) offer alternatives, dependent on the surgical setting's constraints. Although our method improves accuracy, it demands thorough consideration of these implementation factors.\nLimitations and Future Directions Our research identifies key areas for future exploration. First, enhancing feedback analysis through the integration of visual data from surgeries could link verbal feedback to specific surgical actions, enriching the context significantly (Kocielnik et al., 2023). Second, adapting our methods for real-time feedback during surgeries would not only improve teaching assessments but also help in documenting educational elements in a live setting (Akbari et al., 2023). Lastly, investigating how feedback patterns evolve over time and developing methodologies to track and improve feedback delivery could provide deeper insights into the effectiveness of surgical training and pedagogical evolution (Ma et al., 2021)."}, {"title": "7. Conclusion", "content": "This work introduced a novel automated method for feedback detection in surgical education, utilizing dialogue reconstruction, hallucination removal, and speaker identification. Our method has shown robust performance, even surpassing human annotation. This scalable system promises to improve educational strategies and patient outcomes, marking a significant advancement in the automation of surgical education analysis. It sets the stage for future developments in real-time analysis and automated feedback delivery systems, with broad implications for healthcare training and patient care."}]}