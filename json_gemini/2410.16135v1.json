{"title": "BEYOND 2:4: EXPLORING V:N:M SPARSITY FOR EFFICIENT TRANSFORMER INFERENCE ON GPUS", "authors": ["Kang Zhao", "Tao Yuan", "Han Bao", "Zhenfeng Su", "Chang Gao", "Zhaofeng Sun", "Zichen Liang", "Liping Jing", "Jianfei Chen"], "abstract": "To date, 2:4 sparsity has stood as the only sparse pattern that can be accelerated using sparse tensor cores on GPUs. In practice, 2:4 sparsity often possesses low actual speedups (\u2264 1.3) and requires fixed sparse ratios, meaning that other ratios, such as 4:8, 8:16, or those exceeding 50% sparsity, do not incur any speedups on GPUs. Recent studies suggest that V:N:M sparsity is promising in addressing these limitations of 2:4 sparsity. This sparsity divides a weight matrix into multiple V\u00d7M blocks, pruning (M-4) columns within each block and applying 2:4 sparsity to the remaining columns. V:N:M sparsity inherently encompasses 2:4 sparsity but allows for higher and more flexible pruning ratios, typically resulting in greater practical speedups. However, regarding accuracy, the effects of V:N:M sparsity on broader Transformer models, such as vision Transformers and large language models (LLMs), are largely unexamined. Moreover, Some specific issues related to V:N:M sparsity, such as how to select appropriate V and M values, remain unresolved. In this study, we thoroughly investigate the application of V:N:M sparsity in vision models and LLMs across multiple tasks, from pretaining to downstream tasks. We propose three key approaches to enhance the applicability and accuracy of V:N:M-sparse Transformers, including heuristic V and M selection, V:N:M-specific channel permutation and three-staged LoRA training techniques. Experimental results show that, with our methods, the DeiT-small achieves lossless accuracy at 64:2:5 sparsity, while the DeiT-base maintains accuracy even at 64:2:8 sparsity. In addition, the fine-tuned LLama2-7B at 64:2:5 sparsity performs comparably or better than training-free 2:4 sparse alternatives on downstream tasks. More importantly, V:N:M-sparse Transformers offer a wider range of speedup-accuracy trade-offs compared to 2:4 sparsity. Overall, our exploration largely facilitates the V:N:M sparsity to act as a truly effective acceleration solution for Transformers in cost-sensitive inference scenarios.", "sections": [{"title": "INTRODUCTION", "content": "Transformer has gained significant popularity as backbones across various domains due to its remarkable performance in data modeling and scalability. However, Transformers are often characterized by a large number of parameters and high computational demands, resulting in prolonged inference latency. It is essential to compress Transformer models for efficient inference, especially in resource-constrained or latency-sensitive applications.\nOne possible way to accelerate Transformers is 2:4 sparsity, where only two out of every consecutive four parameters are retained in weight tensors. 2:4 sparsity is widely supported by Nvidia Ampere or newer GPUs. However, the current ecosystem for 2:4 sparsity exhibits three weaknesses that are rarely addressed. 1) Low practical speedups. Unlike the theoretical claims of a twofold speedup,\nin most cases, neural networks with 2:4 sparsity achieve only a speedup in the range of 1.1 to 1.3x"}, {"title": "RELATED WORK", "content": "Weight sparsity in Transformers can be categorized into three types: unstructured sparsity, like S\u00b2ViTE (Chen et al., 2021b), SparseGPT (Frantar & Alistarh, 2023), and\nWanda (Sun et al., 2023); structured sparsity, represented by ViT-Slim (Chavan et al., 2022), VTP\n(Zhu et al., 2021), UVC (Yu et al., 2022), and SAVIT (Zheng et al., 2022); and semi-structured\nsparsity. In particular, semi-structured sparsity generally offers a preferable trade-off between ac-\ncuracy and speed. Yu et al. (2023) introduce 2:4 sparsity to vision Transformers, demonstrating\nthat the DeiT series with 2:4 sparsity can maintain a nearly lossless performance. Xu et al. (2024)\nimplement block-wise sparsity in Transformers, achieving notable speed improvements on neural\nprocessing units. Nevertheless, this approach results in unavoidable accuracy declines. In contrast,\nour V:N:M sparse DeiT-base can preserve nearly lossless accuracy even at 75% sparsity. Beyond\nweight sparsity, other components, such as tokens and attention heads, can also be pruned in Trans-\nformers. Some works in this aspect include T2T-ViT-24 (Yuan et al., 2021), PVT (Wang et al.,\n2021), Evo-ViT (Xu et al., 2022), EViT (Liang et al., 2022), DynamicViT (Rao et al., 2021), PS-ViT\n(Tang et al., 2022), and AdaViT (Yin et al., 2021). However, in this study, we primarily focus on the\neffects of V:N:M sparsity on Transformers, rather than extreme compressing a Transformer."}, {"title": "PRELIMINARY", "content": "Pruning a weight matrix W to achieve the V:N:M-sparse pattern in-\nvolves three steps: 1) Calculate importance scores. First, compute the importance score for each\nweight in W. 2) Column Pruning. Next, prune the columns within each V\u00d7M block. Within each\nblock, the L1 norms of the importance scores for each column are compared, and the weights cor-\nresponding to minimal M-4 columns are pruned. 3) Conduct 2:4 Sparsity. After the column-wise\npruning, each block retains exactly four columns. Subsequently, for each row, the weights corre-\nsponding to the last two importance scores are further pruned to establish the final V:N:M-sparse\npattern. For descriptive convenience, we signify this V:N:N-sparse pruning process as $S_{V:N:M}$.\nIn this work, there are two commonly used criteria to form the importance score of a weight: the\nnaive absolute values (ABS) and relative importance and activation (RIA) (Zhang et al., 2024)."}, {"title": null, "content": "Specifically, RIA defines the importance score of a weight $W_{ij}$ as:\n$RIA_{ij} = (\\frac{|W_{ij}|}{\\sum_{i} |W_{ij}| + \\sum_{j} |W_{ij}|}) \\times (||X_{i}||_{2})^{\\alpha}$,\nwhere $\\sum W_{*j}$ and $\\sum W_{i*}$ denote the summation of the absolute values of the input channel j\nand output channel i, respectively. $||X_{i}||_{2}$ is L2 norms of activations and $\\alpha$ is a factor to control\nthe impact of activations on importance scores. Notably, both ABS and RIA are computationally\nefficient pruning criteria."}, {"title": null, "content": "To restore the accuracy of V:N:M-sparse\nTransformers, sparse training is essential as V:N:M sparsity lies in high sparsity levels f at least 60%\n(V:2:5). At these high levels, merely applying post-training pruning is insufficient to reduce the\nsignificant accuracy loss. Specifically, after pruning a weight matrix, its 0-1 mask M that follows\nthe V:N:M-sparse pattern can be easily derived. Denote the sparse weight matrix $W' = W \\odot M$,\nwhere $\\odot$ is the element-wise multiplication operator. The weight update mechanism for fixed mask\ntraining is represented as:\n$W'_{t} = W_{t-1} - \\gamma \\nabla_{W'} \\mathcal{L}(W'_{t-1})$\nMeanwhile, the weight update using dynamic mask training in the SR-STE framework is expressed\nas:\n$W_{t} = W_{t-1} - \\gamma (\\nabla_{W} \\mathcal{L}(W'_{t-1}) + \\lambda M_{t-1} W_{t-1})$\nIn Eq. 2 and 3, $\\nabla \\mathcal{L}$ denotes the gradient, while $\\gamma$ and $\\lambda$ are the learning rate and regularization\ncoefficient, respectively. Eq. 2 indicates that only the retained weights are updated, with the V:N:M-\nsparse mask M remaining unchanged after one-shot pruning. In contrast, Eq. 3 gradient-updates\nthe dense W and M is time-variant."}, {"title": null, "content": "As illustrated in Figure 2(b), the acceleration of V:N:M-sparse\nTransformers involves three steps: weight padding, conversion to compressed formats, and the ap-\nplication of V:N:M accelerated kernels. First, the weights of all linear layers are zero-padded to\nensure that the input and output channels of weight matrices in linear layers are divisible by M and\nV, respectively. Next, the padded weights are converted into sparse storage formats that include\nonly the compact non-zero values and their indices. The inference kernels then directly take these\nsparsely stored weights as input and leverage sparse tensor cores to accelerate V:N:M-sparse matrix\nmultiplications (MMs) (Castro et al., 2023), as detailed in Appendix A. Due to the effective uti-\nlization of higher sparsity greater than 50%, V:N:M-sparse MMs possess fewer computations than\n2:4-sparse MMs. Thus, for a dense Transformer, the V:N:M-sparse version typically achieves higher\nspeedups than its 2:4 counterpart, with maximal end-to-end speedups reaching over 2x."}, {"title": "METHODS", "content": "We show the process of generating a V:N:M-sparse Transformer with high ac-\ncuracy in Figure 2(a). Given a pretrained dense Transformer and a specified speedup threshold, a\nheuristic approach is employed to select appropriate V and M values for pruning the dense Trans-\nformer. After that, we consider two distinct scenarios. In the first scenario, where a limited training\nbudget is available, V:N:M-specific CP, RIA-based pruning, and fixed mask training are sequen-\ntially employed. CP and RIA can significantly improve the accuracy of V:N:M-sparse Transformers\nupon pruning, while for low training budget constraints, fixed mask training, no matter with full-\nparameters or LoRA, incurs significantly lower overhead compared to dynamic mask training as the\nmask update costs are canceled. Note the necessity of RIA is further explained in Appendix B.\nIn the second scenario, when the training budget is not\nconstrained, ABS-based pruning and dynamic mask train-\ning are conducted in order. In particular, we use ABS-\nbased pruning for dynamic mask training as the criterion\nperforms well provided long training duration (Huang\net al., 2024). As for dynamic mask training, two spe-\ncific cases involving full-parameter and LoRA training\nare considered. For full-parameter training, the SR-STE\nframework formulated using Eq. 3 is employed with one"}, {"title": null, "content": "modification. That is, the sparse masks are updated less frequently, specifically every five epochs in-\nstead of the one iteration per update reported in the original approach. As suggested in Table 1, this\nreduced update frequency enhances training stability, resulting in improved final accuracy for V:N:M\nsparse Transformers. Besides, we propose a three-staged LoRA training technique to train V:N:M-\nsparse Transformers under memory constraints, such as during the fine-tuning of LLMs. Overall,\nour scheme encompasses three training settings, each corresponding to one branch shown in Figure\n2(a). For clarity, these three training settings are designated as TS1, TS2, and TS3, respectively. By\naddressing all the possible conditions, our scheme significantly expands the applicability of V:N:M-\nsparse Transformers. Afterward, three key techniques marked with the blue color in Figure 2 in the\nscheme are detailed."}, {"title": "V AND M SELECTION", "content": "For a dense Transformer, different V and M value combinations result in different final accuracy and\nspeedups for its V:N:M-sparse counterparts. Among these combinations, we aim to select the proper\nV and M combinations with which a V:N:M-sparse Transformer consistently lies in the Pareto front\nin terms of both accuracy and speedups. However, it is often time-consuming to generate a V:N:M-\nsparse Transformer via sparse training, before its accuracy can be evaluated. To address this issue,\nwe propose a heuristic V and M selection strategy including two key factors:"}, {"title": null, "content": "We define the process of solving for optimal combinations of V and M on the\nPareto front as Eq. 4:\n$\\arg \\max_{V,M} \\text{Accu}.\\{f(w(V, N, M)), d_{v}\\},$\n$\\text{subject to } Speedup\\{f(w), f(w(V, N, M)\\} \\ge s$\nThat is, given a specified speedup threshold s, training data $d_{v}$, and a dense Transformer f(w),\nour goal is to identify a proper V and M to maximize the accuracy of the Transformer's sparse\nversion f(w(V, N, M)), on validation data $d_{v}$. This optimization is subject to the constraint that\nthe speedup of f(w(V, N, M)) relative to f(w) is at least s. In practice, considering the GPU\nacceleration affinity, ${V\\in 2^{k}|k \\in N^{+}, k \\ge 4}$ and ${M\\in N^{+}, M\\ge 5}$. Besides, N=2 in V:N:M\nsparsity if practical speedup is required."}, {"title": null, "content": "A two-phase sifting is conducted to select the optimal (V, M) combination from all the\n(V, M) combinations that meet the given speedup constraints. First, for a group of (V, M) combina-\ntions with the same V, it is evident the smallest M results in the highest accuracy of V:N:M-sparse"}, {"title": "V:N: M-SPECIFIC CHANNEL PERMUTATION FOR LOW TRAINING BUDGET", "content": "To enhance the accuracy of V:N:M-sparse Transformer in sce-\nnarios with limited training budgets, i.e., only a small number\nof training epochs, a V:N:M-specific channel permutation (CP)\napproach is proposed and should be conducted before RIA-\nbased pruning, i.e., as shown in Figure 2(a). Notably, CP for\n2:4 and V:N:M sparsity is different. In 2:4 sparsity, only the\ninput CP of a weight matrix influences the norm of importance\nscores of retained weights after pruning. In contrast, V:N:M\nsparsity allows both input and output CP to affect the retained\nnorm. Specifically, both input and output CP for a weight matrix W are:\n$Y = WX = P_{o}P_{i}WP_{i}P_{i}X = P_{o}WP_{p}X,$"}, {"title": null, "content": "where $P_{o}$ and $P_{i}$ are output CP and input CP matrices, respectively. $W_{p}$ is the weight matrix\nafter CP. After conducting V:N:M-sparse pruning to the permuted $W_{p}$, we aim for the norm of the\nimportance scores of the retained weights to be maximized, thus the optimization objective is:\n$\\arg \\max_{P_{o}, P_{i}} \\sum_{i,j} RIA_{ij}(S_{V:N:M}(W_{p}))$\nWe employ alternative optimization to iteratively solve for $P_{o}$ and $P_{i}$. Specifically, both $P_{o}$ and $P_{i}$\nare initialized as identity matrices. In the kth iteration,\n$P^{k+1}_{i} = \\arg \\max_{P} \\sum_{i,j} RIA_{ij} (S_{V:N:M}(P_{o}^{k}WP_{i}))$\n$P^{k+1}_{o} = \\arg \\max_{P} \\sum_{i,j} RIA_{ij} (S_{V:N:M} (P_{o}WP_{i}^{k+1}))$\nLike (Zhang et al., 2024), Eq. 8 or 9 can be approximately modeled as the traditional linear sum\nassignment problem, efficiently solvable using Hungarian algorithm(Kuhn, 1955). The total number\nof iterations is 2, based on the ablation study presented in Table 2. Note that during inference,\n$P^{T}_{i}$ and $P_{o}$ can be fused with post-Layernorm or preceding linear layers in standard Transformers,\ngenerally resulting in negligible time overheads (Zhang et al., 2024)."}, {"title": "THREE-STAGED LORA TRAINING", "content": "For LoRA training (Hu et al., 2021), the V:N:M sparse version W' of a dense weight matrix W is\nderived by:\n$W' = (W + BA) \\odot M,$\nwhere B and A are two low rank matrices. Normally, M is a function of $W_{u}$, $B_{u}$, and $A_{u}$.\nDuring training, $W_{u}$ remains fixed while $B_{u}$ and $A_{u}$ are updated. We propose a three-stage\nLoRA training technique to enable dynamic mask training with LoRA and enhance the accuracy\nof V:N:M-sparse Transformers. 1) Dense LoRA. At the beginning of training, standard LoRA fine-\ntuning is applied to the Transformer, where the masks M are consistently all-one matrices. 2)"}, {"title": "EXPERIMENTS", "content": "To evaluate the proposed V:N:M-sparse Transformer generation\nmethod-which incorporates V and M selection, V:N:M-specific channel permutations, and three\nstaged LoRA training techniques, three benchmarks have been established: 1) DeiT (Touvron et al.,\n2021) for image classification. This benchmark is widely recognized for assessing the efficacy\nof model compression techniques in vision Transformers. Given that V:N:M sparsity operates at\nhigh sparsity levels (greater than 50%), the DeiT-tiny is excluded due to insufficient redundancy.\nThe datasets used for the tasks include ImageNet-1K (Deng et al., 2009), Cifar-10 and Cifar-100\nKrizhevsky et al. (2009), Bird and Vehicle from the subset of ImageNet-1K. Note that the latter\nfour datasets are used to form downstream tasks. 2) Swin Transformers. This category of vision\nTransformers, known for its hierarchical architecture and shifted window mechanisms, demonstrates\nincreased sensitivity to model compression (Liu et al., 2021). In this work, the V:N:M-sparse Swin\nTransformers are assessed across two tasks: image classification on the ImageNet-1K dataset and\nobject detection on the COCO 2017 dataset (Lin et al., 2014). 3) Llama2-7B on downstream tasks\nincluding predicting the next token on wikitext2 (Merity et al., 2016) and eight well-established\n5-shot tasks (Gao et al., 2021). Note that Llama2-7B undergoes LoRA training on both Wikitext2\nand Alpaca (Taori et al., 2023) before testing on these tasks, which is a standard practice in the fine-\ntuning of LLMs. In addition, the speedups of these V:N:M-sparse Transformers compared to their\ndense counterparts are measured on RTX 3090 GPUs, which were also the speed-testing platform\nin the initial study."}, {"title": "RESULTS FOR V AND M SELECTION", "content": "Figure 4 compares the accuracy and speedup for V:N:M-sparse Transformers, both with and without\nthe proposed V and M selection technique. In this experiment, for practical speedups, V is confined\nto the values [16, 32, 64, 128]. Sparse Transformers with varying M values and unified V=64 are\nselected to establish the speedup threshold s, as specified in Eq. 4, with 64 representing a central\nvalue in the V distribution. Using the thresholds, the V and M selection technique is applied to\nderive new (V, M) and further generate the V:N:M-sparse Transformers accordingly, as indicated by\nthe yellow lines in Figure 4."}, {"title": "RESULTS FOR TS2", "content": "As the training budget is not limited in terms of TS2, the experiments\nare to investigate the extent to which the DeiT can be compressed while still achieving nearly lossless\nperformance, i.e., gap< 0.3%. As shown in Table 5, our TS2 allows DeiT-base to achieve lossless\naccuracy at a sparsity level of 75%, represented as 64:2:8. This level of sparsity results in a 73.8%\nreduction in parameters and a 71.6% reduction in FLOPs. Similarly, DeiT-small maintains lossless\naccuracy at 64:2:5, achieving a 57.9% reduction in parameters and a 54.3% reduction in FLOPs.\nDue to computational power limitations, larger Transformers, such as ViT-huge and ViT-giant, were\nnot included in this investigation. However, it is generally acknowledged that larger models tend to\nexhibit greater redundancy. For the ImageNet-1K classification task, larger vision Transformers than\nDeiT-base are anticipated to achieve higher sparsity than 64:2:8 while maintaining lossless accuracy."}, {"title": null, "content": "The Swin Transformer is generally considered challenging to compress. How-\never, the V:N:M-sparse Swin Transformer, utilizing our TS2, achieves results comparable to the\nstate of the art. As shown in Table 6, under identical training epochs, the Swin Transformer at a\nsparsity of 32:2:5 achieves the same Top-1 accuracy as LPViT (Xu et al., 2024). Notably, it achieves\na 59.1% reduction in parameters and a 60.4% reduction in FLOPs, which is significantly greater than\nLPViT's 27% reduction in FLOPs relative to its dense counterpart. For object detection, the dense\nH-DETR, using Swin-Tiny as the backbone and trained for 24 epochs, is employed to generate the\nV:N:M-sparse H-DETR. With TS2 and 12 training epochs, the H-DETR at 32:2:5 achieves a mean\nAverage Precision (mAP) of 47.8%, outperforming the dense equivalent trained for 12 epochs, by\n2.5%. Furthermore, at a sparsity of 32:2:6, the sparse H-DETR with TS2 retains an mAP of 45.3%,\nsurpassing that with the fixed mask training (FMT) setting by 1.2%. These results demonstrate that\nour TS2 is more favorable to the accuracy restoration of V:N:M-sparse Transformers."}, {"title": "RESULTS FOR THREE-STAGED LORA TRAINING AND TS3", "content": "The experiments aim to demonstrate that the V:N:M-\nsparse Llama2, with our TS3 involving three-staged\nLORA training, can achieve performance levels com-\nparable to its 2:4-sparse version formed by post-\npruning approaches, e.g., RIA (Zhang et al., 2024).\nThe LoRA training was conducted over 50,000 sam-\nples, each consisting of 1,024 tokens. Each training\niteration utilizes one sample as input, resulting in a\ntotal of 50,000 iterations. The results show that uti-\nlizing our approach, Llama2-7B achieved a perplex-\nity (PPL) of 9.97 on the Wikitext2 dataset, as shown"}, {"title": "CONCLUSION", "content": "This study focuses on enhancing the accuracy and accuracy-speedup trade-offs of V:N:M-sparse\nTransformers in multiple scenarios. We address the crucial yet unexplored questions specific to\nV:N:M sparsity, including selecting appropriate values for V and M, and CP tailored for V:N:M\nsparsity. Additionally, we propose a three-staged LoRA training technique, which for the first ex-\ntends V:N:M sparsity to LLMs. Extensive experiments demonstrate that, with our methodology,\nV:N:M-sparse Transformers can attain nearly lossless accuracy or perform comparably to those with\npost-pruning 2:4 sparsity. Given its superior speed performance, we conclude that V:N:M sparsity is\nmore effective than 2:4 for compressing highly redundant Transformers in inference-cost-sensitive\nscenarios. We hope our work will promote the widespread use of V:N:M sparsity as a truly effective\nsolution for compressing Transformers."}, {"title": "Reproducibility Statement", "content": "We are committed to ensuring the reproducibility of our work. The the-\noretical foundations and assumptions underlying our framework are thoroughly discussed in Section\n4, and some proofs of our claims are provided in the appendix. Detailed descriptions of our exper-\niments, including the architecture configurations and hyperparameters used for training the V:N:M-\nsparse Transformers, can be found in Section 5 of the main text. In addition, We will publicly release\nour source code anonymously at the appropriate time. We believe these resources collectively facil-\nitate the reproducibility of our findings and ensure that our methodologies can be adopted in future\nresearch without difficulty."}, {"title": null, "content": "Suppose we have two different V:N:M sparsity patterns for the same Transformer, adopting different\nvalues of V1, M1 and V2, M2, while N remains constant at N = 2. In this case, for a linear layer,\nwhere the shape of the linear weight is [m, n],\n$\\frac{MD_{V_{1}:N:M_{1}}}{MD_{V_{2}:N:M_{2}}}} = \\frac{[C_{M_{1}}(C_{N}^{V_{1}})]^{V_{1}M_{1}}}{[C_{M_{2}}(C_{N}^{V_{2}})]^{V_{2}M_{2}}} \\\\\n= \\frac{[C_{M_{1}}(C_{2}^{V_{1}})]^{V_{1}M_{1}}}{[C_{M_{2}}(C_{2}^{V_{2}})]^{V_{2}M_{2}}} \\\\\n= \\frac{K_{1}^{m n}}{K_{2}^{m n}}$\nThus, for a complete network, suppose the linear layer l has a linear weight shape of [mi, ni]. We\ncalculate the ratio of the MD under two different selections of the V and M parameters for the same\nmodel. The ratio $\\frac{MD_{1}}{MD_{2}}$ can be expressed as follows:\n$\\frac{MD_{1}}{MD_{2}} = \\frac{\\Pi[MD_{V_{1}:N:M_{1}}]}{\\Pi MD_{V_{2}:N:M_{2}}} \\\\\n= \\Pi \\frac{K_{1}^{m_{i}n_{i}}}{K_{2}^{m_{i}n_{i}}} \\\\\n= (\\frac{K_{1}}{K_{2}})^{\\sum m_{i}n_{i}}$\nThis indicates that for the same Transformer, as long as $K_{1} > K_{2}$, it implies that the overall net-\nwork's masking diversity satisfies $MD_{1} > MD_{2}$, regardless of the specific shapes of the linear\nweights."}, {"title": "SPEEDUP-ACCURACY CURVES OF DEIT-BASE ON DOWNSTREAM TASKS", "content": "Our V and M selection technique is applicable across all three training settings: TS1, TS2, and TS3.\nTherefore, it is essential to also evaluate our method using TS1, as shown in Figure 7. We present"}]}