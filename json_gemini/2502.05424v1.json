{"title": "SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and Cross-domain Adaptation", "authors": ["Xingtong Yu", "Zechuan Gong", "Chang Zhou", "Yuan Fang", "Hui Zhang"], "abstract": "Graphs are able to model interconnected entities in many online services, supporting a wide range of applications on the Web. This raises an important question: How can we train a graph foundational model on multiple source domains and adapt to an unseen target domain? A major obstacle is that graphs from different domains often exhibit divergent characteristics. Some studies leverage large language models to align multiple domains based on textual descriptions associated with the graphs, limiting their applicability to text-attributed graphs. For text-free graphs, a few recent works attempt to align different feature distributions across domains, while generally neglecting structural differences. In this work, we propose a novel Structure Alignment framework for text-free Multi-domain Graph Pre-Training and cross-domain adaptation (SAMGPT). It is designed to learn multi-domain knowledge from graphs originating in multiple source domains, which can then be adapted to address applications in an unseen target domain. Specifically, we introduce a set of structure tokens to harmonize structure-based aggregation across source domains during the pre-training phase. Next, for cross-domain adaptation, we design dual prompts, namely, holistic prompts and specific prompts, which adapt unified multi-domain structural knowledge and fine-grained, domain-specific information, respectively, to a target domain. Finally, we conduct comprehensive experiments on seven public datasets to evaluate and analyze the effectiveness of SAMGPT.", "sections": [{"title": "1 Introduction", "content": "How to build foundation models has emerged as an important question, paving a plausible path toward artificial general intelligence. In natural language processing, recent works [1, 42] have demonstrated the capabilities of universal foundation models. They are trained on a wide variety of data from multiple domains, and can be further adapted to solve a diverse range of tasks. Other than natural languages, the World Wide Web has become a vast knowledge repository, connecting an enormous amount of entities to form extensive and complex graphs. These graphs enable diverse Web applications, including social network analysis [13, 27], Web mining [2, 5], and recommendation systems [23, 28]. Given the rich graph data on the Web, can we build a universal graph model based on multi-domain graphs, to address various downstream graph-centric applications [19]?\nTraditional supervised graph learning struggles to build universal models. These approaches require retraining a new graph neural network (GNN) [6, 15, 60] or graph transformer [32, 56, 66] for each new task, relying on abundant task-specific labeled data. In contrast, more recent graph pre-training methods [11, 30, 44] attempt to learn universal properties from unlabeled graphs in a self-supervised manner, which can be subsequently adapted to a downstream task with some task-specific labels through fine-tuning [14, 30, 44] or prompt learning [21, 40, 62]. However, in most existing graph pre-training approaches, the pre-training and downstream graphs originate from the same dataset [21, 40, 44, 57], a practice we refer to as single-domain methods, which fall short of building a universal, multi-domain graph model from multiple graph datasets.\nResearch problem. Thus, it is crucial to pre-train a graph model on a wide range of multi-domain (i.e., multi-dataset) graphs and achieve cross-domain adaptation. However, graph structures from different datasets often exhibit markedly distinct characteristics. For instance, the structural patterns in a social network might not be directly applicable to a citation or e-commerce graph. Such diversity poses significant challenges in integrating graphs from multiple domains and adapting prior knowledge to different domains. Although some studies have explored cross-domain adaptation from a single source domain [4, 9, 45, 47, 55], they do not exploit multiple source domains. Another line of work [18, 41, 53] employs large language models to extract and utilize multi-domain knowledge based on textual descriptions associated with graphs, using text as a universal medium to bridge different domains. However, this limits their applicability to text-attributed graphs [49, 70] and cannot be extended to general graphs without textual descriptions. Few recent studies [65, 69] have explored multi-domain pre-training on text-free graphs, but they focus on aligning the divergent feature spaces and homophily patterns across multi-domain graphs, while overlooking the structural differences across domains.\nChallenges and insights. In this paper, we propose SAMGPT, a graph foundation model with Structural Alignment for text-free Multi-domain Graph Pre-Training, to facilitate cross-domain adaptation. This is non-trivial due to two key challenges.\nFirst, how do we harmonize structural variance across multiple domains during pre-training? Graphs from different domains often exhibit distinct structural and topological characteristics, such as average node degree, shortest path length and clustering coefficient, as depicted in Table 1. Thus, merging multi-domain graphs without structure alignment during pre-training may cause interference rather than synergy, leading to suboptimal performance. In SAMGPT, we propose structure tokens to align structural distributions across multiple domains, as shown in Fig. 1(a). Each domain is equipped with a series of structure tokens, which modify the structure-based aggregation in each layer of the graph encoder. These tokens are learnable vectors that capture domain-specific structural patterns, enabling the model to accommodate the unique structural characteristics of each domain during pre-training.\nSecond, how do we adapt multi-domain prior structural knowledge to cross-domain downstream tasks? Multi-domain prior knowledge includes not only holistic knowledge across source domains, but also domain-specific knowledge from each domain. Therefore, in SAMGPT, we propose dual structural prompts, comprising a set of holistic prompts and a set of specific prompts, thus facilitating the adaptation of both holistic and domain-specific knowledge to downstream tasks, as illustrated in Fig. 1(b). On one hand, the holistic prompts consist of learnable vectors that holistically align the target domain's structural characteristics with the unified pre-trained knowledge from all source domains. On the other hand, specific prompts integrate multi-domain structure tokens in a learnable mixture to align the target domain with knowledge from each source domain, capturing domain-specific structural information for finer-grained adaptation.\nContributions. In summary, we make the following contributions in this work. (1) We propose SAMGPT, a text-free graph foundation model with structure alignment for multi-domain graph pre-training and cross-domain adaptation. (2) For pre-training, we propose structure tokens to align structural distributions across domains, training a universal foundation model with multi-domain graphs. (3) For downstream adaptation, we propose a dual-prompt strategy, using holistic prompts to leverage holistic prior structural knowledge and specific prompts to facilitate finer-grained, domain-specific structural adaptation. (4) We conduct extensive experiments on seven benchmark datasets, demonstrating the superior performance of SAMGPT compared to state-of-the-art methods."}, {"title": "2 Related Work", "content": "We review related literature on pre-training, cross-domain transfer learning, and multi-domain pre-training for graph data.\nGraph pre-training. Graph pre-training methods aim to extract inherent properties of graphs, often utilizing self-supervised learning approaches, which can be either generative [10\u201312, 16] or contrastive [17, 44, 52, 54]. The pre-trained model is then employed to address downstream tasks through fine-tuning [30, 44, 57] or parameter-efficient adaptation methods, notably prompt-based learning [7, 21, 39, 59]. However, these methods typically assume that the pre-training and downstream graphs originate from the same domain, such as different subgraphs of a large graph [57, 61] or collections of similar graphs within the same dataset [11, 30], failing to account for multiple domains in either pre-training or downstream graphs.\nGraph cross-domain transfer. This line of work aims to transfer single-source domain knowledge to a different target domain by leveraging domain-invariant properties across domains [4, 9, 45, 47]. However, they rely exclusively on a single source domain, failing to harness the extensive knowledge available across multiple domains. Additionally, these approaches are often tailored to specific tasks or domains [4, 9, 45, 47], limiting their generalization.\nMulti-domain graph pre-training. In the context of graphs from multiple domains, recent works [18, 41, 53] utilize large language models to align node features from different domains through textual descriptions, thereby limiting their applicability to text-attributed graphs [50, 67, 70]. For graphs without textual attributes, GraphControl [72] applies ControlNet [68] to incorporate target"}, {"title": "3 Preliminaries", "content": "In this section, we provide technical background, and outline the scope of our work.\nGraph encoder. A graph is defined as $G = (V, E, X)$, where $V$ is the set of nodes, $E$ is the set of edges, and $X \\in R^{|V|\\times d}$ is the node feature matrix with each row $x_i$ representing the feature vector of node $v_i \\in V$. A collection of graphs is denoted as $G$.\nMessage-passing GNNs are a common choice for encoding graph representations [51]. Specifically, each node updates its embedding by receiving and aggregating features or embeddings from its neighbors. By stacking such message-passing layers, information can propagate recursively throughout the graph. Therefore, the node embeddings are encoded based on both input features and graph structure. Let us denote the embedding of node $v$ at the $l$-th layer as $h_v^l$, which is derived from the features or embeddings in the preceding layer as follows.\n$h_v^l = \\text{Aggr}\\left(h_v^{l-1}, \\left\\{h_u^{l-1} : u \\in N_v\\right\\}; \\Theta^l\\right),$ (1)\nwhere $N_v$ denotes the set of neighboring nodes of $v$, $\\Theta^l$ represents the learnable parameters in layer $l$, and $\\text{Aggr}()$ stands for the neighborhood aggregation function. In the first layer, the node embedding $h_v^0$ is initialized as the input feature vector $x_v$. We denote the output node embedding after the last layer as $H_O$, which is a row in the node embedding matrix $H$. Overall, the multi-layer message-passing process can be abstracted as a graph encoder, as follows.\n$H = \\text{GE}(G, X; \\Theta),$ (2)\nwhere $\\text{GE}$ denotes a graph encoder, $\\Theta = \\{\\Theta^1, \\Theta^2, ...\\}$ is the full set of trainable parameters for the graph encoder.\nMulti-domain pre-training with feature alignment. Consider a set of unlabeled graphs $G_S = \\{G_1, G_2, ..., G_K\\}$ for pre-training, where each graph $G_i$ belongs to a specific source domain $D_{S_i} \\in D_S$. Thus, we have graph-domain pairs $\\{(G_i, D_{S_i}) : i \\in \\{1, 2, ..., K\\}\\}$.\nAs different domains exhibit distinct feature distributions, previous works [58, 69] have proposed solutions to align feature dimensions and semantics, which can be directly employed in our work. Given a graph $G_i = (V_i, E_i, X_i)$ from the source domain $D_{S_i}$, we first align the dimensions of its feature matrix:\n$X_i = \\text{DAL}_{S_i}(X_i),$ (3)"}, {"title": "4 Proposed Approach: SAMGPT", "content": "In this section, we present SAMGPT, beginning with an overview and then delving into the details of multi-domain pre-training and cross-domain adaptation."}, {"title": "4.1 Overall Framework", "content": "SAMGPT consists of two phases: multi-domain pre-training, and cross-domain adaptation, as shown in Fig. 2.\nIn the pre-training phase, as depicted in Fig. 2(a), we first align the feature distributions from multiple source domains following previous work [65, 69]. Next, we introduce a set of structure tokens designed to align the structural distributions across diverse domains. These tokens are domain-specific and are integrated into each layer of the graph encoder, modifying the structure-based aggregation at each layer. Finally, the structure token-enhanced graph encoder is pre-trained using a self-supervised loss [21].\nIn the adaptation phase, as shown in Fig. 2(b), we first align the feature dimension of the target domain with that of the source domains. Then, we introduce dual prompts. The first type, holistic prompts, are learnable vectors that integrate the target domain with the holistic structural knowledge from all source domains. The second type, specific prompts, comprise a learnable mixtures of pre-trained structure tokens that incorporate domain-specific topological information tailored to the target domain. These prompts are applied to each layer of the graph encoder to adjust the structure-based aggregation, while keeping the pre-trained weights of the graph encoder frozen."}, {"title": "4.2 Multi-domain Graph Pre-training with Structure Alignment", "content": "As defined in Sect. 3, we are given a set of pre-training graphs from multiple source domains, $G_S$. As both the features and structures of these domains can exhibit divergent distributions, effective integration of these multi-domain graphs requires aligning both. As our work focuses on structure alignment, we follow previous feature alignment methods [58, 69], as outlined in the preliminaries.\nStructure alignment. Recall that in the graph encoder, node representations are updated layer-wise through a structure-based aggregation. Each layer captures different levels of structural information. For example, the first layer aggregates one-hop neighborhood information, while the second layer incorporates a broader two-hop neighborhoods. These layer-wise structural patterns may vary significantly across domains.\nTherefore, to unify the structural characteristics in multiple source domains, we introduce learnable structure tokens. For each source domain $D_{S_i}$, we inject a series of structure tokens $T_{S_i} = \\{t_{S_i}^l : l \\in \\{1, ..., L\\}\\}$, into the graph encoder, where $L$ denotes the number of layers. Specifically, when encoding the graph $G_i = (V_i, E_i, X_i)$ in $D_{S_i}$, we assign structure token $t_{S_i}^l$ to the $l$-th layer, guiding structure-based aggregation:\n$h_v^l = \\text{Aggr}\\left(h_v^{l-1}, t_{S_i}^l \\odot \\left\\{h_u^{l-1} : u \\in N_v\\right\\}; \\Theta^l\\right), \\forall v \\in V_i,$ (6)\nwhere $\\odot$ represents element-wise multiplication. Note that the graph encoders for feature alignment and structure alignment on all graphs share the same parameters $\\Theta$. Let $H_i^{SAL}$ denote the structure-aligned output node embedding matrix for $G_i$ in $D_{S_i}$, following the aggregation in Eq. (6). In general, each source domain is attached with its own set of structure tokens, which are applied to modify the aggregation on the graph in the corresponding domain. By stacking the structure-aligned output matrix across graphs in all domains, we obtain the overall structure-aligned embedding matrix, $H^{SAL} = \\text{Stack}(H_1^{SAL},..., H_K^{SAL})$.\nFinally, we fuse $H^{SAL}$ with $H^{FAL}$ in Eq. (4) to obtain the multi-domain node embedding matrix $H$, incorporating both feature and structure alignment, as shown below.\n$H^{AL} = H^{FAL} + \\alpha H^{SAL},$ (7)\nwhere $\\alpha > 0$ is a hyperparameter.\nPre-training loss. We leverage a universal task template based on subgraph similarity calculation [21, 59], which ensures compatibility across different tasks such as node classification and graph classification. As demonstrated in GraphPrompt+ [59], prevailing contrastive pre-training objectives can be unified under this template, making them suitable choices for the pre-training loss in"}, {"title": "4.3 Cross-domain Structure Adaptation", "content": "Beyond multi-domain pre-training, another challenge lies in cross-domain adaptation. Given a model pre-trained on graphs $G_S$ from source domains $D_S$, we aim to adapt it to a downstream task on graphs $G_T$ from a target domain $D_T \\notin D_S$. As this work focuses on structure adaptation, we directly apply previous work [58] for feature adaptation, as outlined in Sect. 3.\nFor structure adaptation, we propose dual prompts, consisting of holistic prompts and specific prompts. On one hand, the holistic prompts are designed to holistically utilize the pre-trained structural knowledge from all source domains. On the other hand, the specific prompts combine multi-domain structure tokens through a learnable mixture, adapting fine-grained, domain-specific structural knowledge to the target domain.\nHolistic prompts. To transfer the holistic multi-domain structural knowledge to a downstream task, we propose a set of holistic prompts designed to align the target domain $D_T$ with the model pre-trained on the source domains $D_S$. Like any pre-training framework, we encode a downstream graph $G = (V, E, X)$ using the pre-trained graph encoder with frozen layer-wise weights $\\Theta^{pre} = {\\Theta_1^{pre},..., \\Theta_L^{pre}\\}$. However, the key difference is that we inject a series of learnable vectors $P^{hol} = \\{P_{hol}^1, ..., P_{hol}^L\\}$ as holistic prompts into the downstream structure-based aggregation:\n$h_v^l = \\text{Aggr}\\left(h_v^{l-1}, \\left\\{P_{hol}^l \\odot h_u^{l-1} : u \\in N_v\\right\\}; \\Theta^{pre}\\right), \\forall v \\in V.$ (9)\nThe final layer outputs a holistic node embedding matrix for the downstream graph G, denoted as $H^{hol}$.\nSpecific prompts. In contrast to the holistic prompts, specific prompts are designed to adapt structural knowledge specific to each source domain. Since knowledge from related source domains is likely to be more applicable, it is essential to align the target domain with different source domains to varying extents, prioritizing the most relevant ones. Consequently, we define specific prompts as $P^{spe} = \\{P_{spe}^1, ..., P_{spe}^L\\}$, which will also be injected into different layers of the pre-trained graph encoder. Specifically, in the $l$-th layer, $p_{spe}^l$ is a combination of $\\{t_{S_1}^l,...., t_{S_K}^l\\}$, the pre-trained structure"}, {"title": "5 Experiments", "content": "In this section, we conduct experiments to assess the performance of SAMGPT and analyze its empirical results."}, {"title": "5.1 Experimental Setup", "content": "Datasets. We conduct experiments on seven benchmark datasets: (1) Cora [26], (2) Citeseer [35] and (3) Pubmed [35] are scientific paper citation networks from different fields, including computer science and biomedical research. Nodes represent academic publications and edges denote citation relationships. (4) Photo [36] and (5) Computers [25] are both e-commerce networks from Amazon in different categories, namely, photography and computer related products. Nodes represent products and edges signify frequent co-purchases between products. (6) Facebook [33] is a Web graph, where nodes represent official Facebook pages while the links are"}]}