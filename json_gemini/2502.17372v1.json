{"title": "Experimental validation of UAV search and detection system in real\nwilderness environment", "authors": ["Stella Dumen\u010di\u0107", "Luka Lan\u010da", "Karlo Jakac", "Stefan Ivi\u0107a"], "abstract": "Search and rescue (SAR) missions require reliable search methods to locate survivors,\nespecially in challenging or inaccessible environments. This is why introducing unmanned\naerial vehicles (UAVs) can be of great help to enhance the efficiency of SAR missions while\nsimultaneously increasing the safety of everyone involved in the mission. Motivated by this, we\ndesign and experiment with autonomous UAV search for humans in a Mediterranean karst\nenvironment. The UAVs are directed using Heat equation-driven area coverage (HEDAC)\nergodic control method according to known probability density and detection function. The\nimplemented sensing framework consists of a probabilistic search model, motion control\nsystem, and computer vision object detection. It enables calculation of the probability of the\ntarget being detected in the SAR mission, and this paper focuses on experimental validation of\nproposed probabilistic framework and UAV control. The uniform probability density to ensure\nthe even probability of finding the targets in the desired search area is achieved by assigning\nsuitably thought-out tasks to 78 volunteers. The detection model is based on YOLO and trained\nwith a previously collected ortho-photo image database. The experimental search is carefully\nplanned and conducted, while as many parameters as possible are recorded. The thorough\nanalysis consists of the motion control system, object detection, and the search validation. The\nassessment of the detection and search performance provides strong indication that the designed\ndetection model in the UAV control algorithm is aligned with real-world results.", "sections": [{"title": "1. Introduction", "content": "Unmanned Aerial Vehicles (UAVs) have emerged as efficient tools in Search and\nRescue (SAR) missions due to their ability to rapidly access remote, often challenging\nor inaccessible areas enhancing the speed of locating individuals in distress, especially\nin situations involving natural hazards and risks. A critical aspect of this capability is\nperson detection from aerial imagery. However, obtaining images for research purposes\nfor this application is a challenging task since it requires access to real-world SAR\nscenarios, which have the focus set on the mission instead of collecting data for research\npurposes and often require complete anonymity. Experiments conducted in controlled\nand monitored conditions can therefore serve as a valuable alternative for generating\ndatasets and advancing research of SAR missions. Additionally, existing datasets and\nalgorithms often fall short in addressing the unique challenges posed by SAR scenarios,\nespecially when detecting small-sized objects, such as individuals captured from a top-\ndown perspective (Hong et al., 2021). Unlike typical object detection datasets, the visual\nrepresentation of people in such images deviates from conventional forms, emphasizing\nthe need for specialized datasets tailored to this task (Akshatha et al., 2023).\nBeyond computer vision detection, effective motion control is essential for\nUAVs to systematically and efficiently survey target areas. Ergodic search algorithms,\nsuch as the Heat equation-driven area coverage (HEDAC) method (Ivic, 2020) used in\nthis study, enhance the search performance by distributing the search efforts\nproportional to the likelihood of locating a target showing significant efficiency in SAR\nmissions. However, implementing such strategies in SAR mission environments\nintroduces challenges, including obstacle avoidance, real-time communication, and\ncoordinating multiple UAVs. This is why the robustness of the motion control system is\nof utmost importance. The UAV search framework (Lan\u010da et al., 2024) used in this\nstudy, utilizes the ergodic HEDAC motion control system in combination with Model\nPredictive Control (MPC) to efficiently search a large area while performing aerial\nimagery. The underlying sensing model is based on the performance of the used\nYOLOv8 object detection model, meaning the UAV's motion is influenced by both the\ntarget probability density function and the detection model. As UAVs adjust their flight\nheight while searching complex terrains, the performance of YOLOv8 varies based on\nthe flight height impacting the ground sampling distance (GSD) (Petso et al., 2021),\n(Qingqing et al., 2020). Since existing person detection models often do not provide\nperformance metrics across extensive flight height and GSD ranges, the existing\npretrained model was additionally trained on our initial experiment data to fill this gap.\nDespite recent advancements, SAR applications still face challenges in both,\ncomputer vision object detection and UAV motion control. Detection algorithms must\ncontend with varying environmental conditions, occlusions, and the inherently low\nresolution of humans in aerial imagery. Meanwhile, motion control demands adaptive\nstrategies capable of balancing efficiency and reliability in high-stakes operations."}, {"title": "2. Literature overview", "content": "In the following section the utilization of UAVs in SAR missions is explored focusing\non the ergodic motion control and other strategies for efficient search area coverage, as\nwell as the usage of object detection models to help detect individuals in distress."}, {"title": "2.1 UAV in SAR missions", "content": "A detailed survey on the usage of UAVs in SAR missions is presented in (Lyu et al.,\n2023) giving an overview of different types of UAVs that can be used in SAR missions,\nas well as different operational scenarios of the UAVs in times of disasters. The\nadvantages that UAVs offer, such as accessing inaccessible and often dangerous areas\ninclude improved safety for human resources, cost-effective operations, and faster data\ncollection including the ability to gather high-resolution imagery or data used for\nresearch and monitoring. Equipped with advanced sensors, such as cameras including\nthermal cameras, multispectral cameras, and light detection and ranging (LiDAR),\nUAVs can be used to detect human body heat, identify structural damages, and map\ncomplex terrains. This is extremely important in situations of natural hazards and risk\nsuch as avalanches (Silvagni et al., 2017), (Bejiga et al., 2017),\nalbrigtsen2016application} or earthquakes (Qi et al., 2016), (Calamoneri et al., 2022),\n(Nedjati et al., 2016). Additionally, UAVs are increasingly being integrated with\ncommunication systems and payload delivery mechanisms to expand their functional\nroles in SAR missions (Doherty & Rudol, 2007). This can include delivering critical\nsupplies, such as medical kits, food, and water, to individuals in inaccessible locations.\nUAVs can also act as airborne relay stations as detailed in (WU et al., 2019). This\nmethod can be used to establish communication links in areas where conventional\nnetworks are disrupted, ensuring coordination among rescue teams.\nThe effectiveness of UAVs in SAR missions is further enhanced by\nadvancements in motion control and object detection technologies, which play a crucial\nrole in enabling efficient search missions while navigating complex environments and\nidentifying targets. Motion control systems enable UAVs to maintain stability and\nmanoeuvrability in challenging conditions, such as strong winds or obstructed terrains,\nensuring reliable performance during missions, as well as effective path planning to\nsearch the target area effectively. Similarly, object detection algorithms allow UAVs to\nidentify search targets, monitor hazards, and detect critical infrastructure, facilitating\ndecision-making processes. This can be done either on-board the UAV or offline on a\nground-based workstation. On-board processing enables real-time detection, providing\nimmediate results but demanding significant computational resources, which reduces\nthe battery life and limits the UAV's operational duration. On the other hand, offline\nprocessing involves analyzing captured images on a dedicated workstation with mostly\nbetter computing power, allowing for faster and more efficient processing while\nconserving UAV battery life, thereby extending the overall search duration."}, {"title": "2.2 UAV search and ergodic motion control", "content": "The ability to effectively control the motion of UAVs is crucial in a variety of\napplications, especially in situations that depend on the control efficiency such as SAR\nmissions. In these operations, UAVs can be deployed either independently or in\ncoordination with ground search teams to increase the search efforts as discussed in\n(Goodrich et al., 2008). Additionally, various search strategies have been explored to\noptimize UAV motion control such as the methods presented in (Lin & Goodrich, 2009)\nthat use straight paths in combination with 90\u00b0 turns to enhance the coverage in SAR\nscenarios.\nErgodic motion control has emerged as a promising solution due to its capability\nto efficiently guide the UAVs over a defined area. By leveraging the principles of\nergodicity, this approach ensures that the spatial distribution of the UAV's trajectory\naligns with the probability distribution of the target's presence, in particular, areas\nwithin the search domain. The benefits of using ergodic search are presented in (Miller\net al., 2015) suggesting the robustness of the method in different conditions and\nuncertainties. This has led to multiple ergodic motion control systems being developed.\nThe three widely recognized approaches for controlling single or multi-agent systems in\nergodic exploration are HEDAC, MPC, and Spectral Multiscale Coverage (SMC).\nThe HEDAC method (Ivic et al., 2016) is based on the heat equation used to\ncreate a potential field enabling efficient directing of either one or multiple agents. The\nHEDAC method was later improved by incorporating agent sensing and detection (Ivic,"}, {"title": "2.3 UAV images object detection", "content": "Even though recent advancements in computer vision algorithms have proven\nhighly beneficial in many fields, especially when large datasets are available for training\nand testing, the availability of large, annotated datasets for SAR-specific applications\nremains limited, hindering the development of more robust automated detection\nsystems. Some examples of existing datasets include (Akshatha et al., 2023), (Zhu et al.,\n2021), (Barekatain et al., 2017). However, the person detection from aerial images has\nsome specific challenges such as the top-down perspective of person objects resulting in\ndifferent characteristics that the object detection model should recognize. The image\nquality can depend on the UAV velocity, especially in SAR missions where the trade-\noff between the mission speed and image quality needs to be considered. Additionally,\nthe person objects in the image are already small-scaled, but the convolutional neural\nnetwork (CNN) downsampling is reducing the feature representations even more\nresulting in a lack of context information. These challenges could be tackled by\nextending the existing number of publicly available UAV image datasets enabling the\nmodels to learn from more images containing even more different image contexts.\nTo effectively utilize these datasets, efficient computer vision techniques are\nrequired to detect and localize objects in UAV imagery despite their small size and\ncomplex backgrounds. Object detection plays a crucial role in this process, as it"}, {"title": "3. UAV Motion control and machine vision detection", "content": "The successful usage of autonomous UAVs in SAR missions depends on several factors\nsuch as the implemented motion control and detection. In this section, the used\nmethodology in terms of the probabilistic model of the search, the UAV motion control\nusing HEDAC and MPC, and the YOLO object detection model are described."}, {"title": "3.1 Probabilistic model of the search", "content": "The main objective of the conducted search is to validate the search success. To achieve\nthis, the first step is to define the UAV's field of view (FOV) as well as the terrain\nmodel needed to determine the UAV's sensing. Since the experiment search is\nconducted in the mountain area, the terrain is uneven, hence the sensing may not capture\nthe whole FOV that would be visible on even terrain. This is why the terrain data as part\nof the geographic information system (GIS) needs to be introduced. The terrain data was\nobtained by digital elevation model (DEM) files from the Copernicus database\n(European Union space programme, 2024). The DEM data was integrated to provide the\ninformation needed for calculating relative heights in the motion control system, as well\nas the possible obstacles impacting the sensing. The relative flight height, defined as the\nheight of the UAV above the ground, is calculated using the starting point of all flights,\nnamely 45.2368\u00b0 latitude and 14.2031\u00b0 longitude, and the DEM data. This calculated\nheight is used to enable the flight height optimization and defining the no-fly safety\nzone. In Figure 1 it can be seen how the terrain can impact the UAV's FOV.\nTo check if the defined point p can be sensed by the camera, the point\ncoordinates need to be transformed to local coordinates in relation to the UAV's\ncoordinate system. By transforming the coordinates, the original x and y coordinates are\nused, while the z coordinate first needs to be calculated based on the UAV flight height\nand the terrain at the point Zr(x, y).. Based on if the point is in the FOV, the detection\nprobability 4 is defined as:\n$\\psi(R) = {\\begin{cases}\u0393(v R v),\u028c \\text{if} R \u2208 \u03a9_{FOV}\\\\\n0, \\text{otherwise}\\end{cases}$,\nwhere R is a 3D defined point relative to the UAV camera, I is used to define the\ndetection probability. For each point that is in the FOV, the detection probability is\ncalculated by \u0393, while points outside of the FOV have a 0 probability of detecting the\ntargets.\nDuring the whole duration of the flight, the coverage c is calculated as the\naccumulated detection probability for all points in the domain visible from the camera's\nposition X. This is accumulated to calculate the search coverage in space and time:\nc(p,t) = \u222b\u03c8(R(X(t),p))dt,\nThe probability of undetected target presence m is initially described by the\nprobability distribution mo at t = 0. Over time, m decreases as the agents apply their\nsensing effects, which are characterized by the coverage c. It is calculated as follows:\nm(p, t) = mo (p) \u2022 e^{-c(p,t)},\nTo calculate the overall detection probability \u03b7, the undetected targerget is integrated\nover the domain:\nn(t) = 1 \u2013 \u222b_{\u03a9} m(p,t)dp.\nThe detection probability is the key factor analyzed in this study since it is a measure of\nthe search effectiveness."}, {"title": "3.2 UAV motion control", "content": "The motion control system implementation used in the main experiment, was taken\nfrom (Lan\u010da et al., 2024) and consists of the HEDAC algorithm for defining the motion\ncontrol in 2D space and MPC for optimizing the flight regime in 3D space adding the\nheight as an additional control variable, as well as the UAV velocity. Although the\nproposed motion control framework is designed to handle multiple UAVs, all search\nmissions were conducted as single-agent searches.\nThe motion control consists of three control variables set by the motion control\nalgorithm, namely the velocity intensity p(t), the incline angle p(t), and the yaw\nangular velocity w(t). Using the velocity intensity and the incline angle, the horizontal\nand vertical velocities are calculated. In addition, w regulates the UAV direction in\nwhich the horizontal velocity acts. By this, the UAV state is defined using three\ncoordinates, namely the x, y, and z coordinates, as well as one orientation state.\nThe horizontal search control is defined by the potential field u(p,t) of the\nsearch area. The potential field is guiding the UAV towards the areas that have the\nhighest probability of containing undetected targets. It is calculated by solving the\ndifferential equation as follows:\n\u03b1\u00b7 \u0394u(p, t) = \u03b2 \u00b7 u(p, t) \u2013 m(p, t),\nwhere a and \u1e9e are HEDAC parameters used to modify the search behavior by adjusting\nthe smoothness and stability, and 4 is the laplace operator. Additionally, the following\ncondition has to be met:\n$\\frac{\\partial u}{\\partial n} = 0$..\nwhere n represents the normal outward to the search domain boundary defined by\n2D. Based on the gradient of the potential field, the direction of the UAV needs to be\nadjusted. This is calculated for each control step steering on the current direction\ntowards the wanted direction defined by the gradient. Additionally, the UAV \u2018s\nmaximum angular velocity is defined by the maximal turning velocity or equivalently\nthe minimum turning radius.\nTo control the UAV's flight height and velocity, MPC is introduced to optimize\ntwo objectives, namely maximizing the UAV velocity, while keeping the flight height\nas close to the height goal defined for each flight. The first constraint that needs to be\nsatisfied for the optimization to be feasible, is the need to fly above the minimum height\nrepresenting the no-fly zone set at 35 meters above the terrain obtained by the terrain\nmodel. The no-fly zone height takes into account the tree height, possible uncertainties\ncontained in the DEM data, as well as an additional safety factor to minimize the risk of\nany collision with possible obstacles. Additional constraints that need to be met are the\nminimum and maximum velocities defined by the UAV specifications, as well as\nminimum and maximum accelerations."}, {"title": "3.3 Computer vision system for human detection", "content": "To collect all the necessary data and to test out the motion control and vision detection\nsystems needed for the success of the main experiment, an initial experiment with 28\nparticipants was conducted on the mountain U\u010dka on 07.07.2024.\nThe initial experiment dataset was obtained by manually operated flights using\nDJI Matrice 210 and DJI M30T UAVs, where the obtained images have a resolution of\n2970 \u00d7 5280 pixels for the DJI Matrice 210 and a resolution of 3000 \u00d7 4000 pixels for\nthe DJI M30T. This is the initial dataset used to train the YOLO object detection model\nused later in the main experiment. All captured persons are manually labeled in all\nimages in the initial dataset.\nThe initial dataset consists of images in combination with the corresponding\nlabels in the YOLOv8 format representing detected individuals. The images are stored\nin JPG format and include metadata such as Global Positioning System (GPS)\ncoordinates, providing valuable context for analysis. The image preprocessing includes\ntiling the original image into smaller parts to ensure easier YOLO training and\nmodifying the existing labels to fit the new small-sized images."}, {"title": "Image labelling", "content": "The Computer Vision Annotation Tool (CVAT) in a local environment was used for\nlabeling. Three independent annotators manually labeled the original-sized images\nidentifying individuals. After the initial labeling, two independent reviewers, who were\nnot involved in the labeling process, reviewed the annotations for accuracy and\nconsistency. The images were labeled in an iterative process where labels were\ncorrected to increase the accuracy. The used label format is YOLO, specifically, it was\ndownloaded as the YOLOv8 Detection label format available in CVAT."}, {"title": "Image tiling", "content": "The process of dividing the original UAV images into smaller tiles was performed using\na custom Python script. Each high-resolution image was split into 512 \u00d7 512 pixel\nsections, ensuring an overlap between the tiles to maintain comprehensive coverage and\nprovide additional context for better analysis. The minimal overlap is experimentally\ndefined as 100 px. The tiling method is shown in Figure 2."}, {"title": "Ground sampling distance", "content": "Since different cameras were used, the images were divided into GSD groups to\nenable the comparison between different flight height conditions. Essentially, GSD is\nthe actual distance in the UAV image represented by 1 px defining how much detail is\ncaptured in the image. A lower GSD means that each pixel in the UAV image is\nrepresenting a smaller ground area enabling the image to show more detail, while in\ncontrast, a higher GSD is representing a bigger area, thus having less details. Using the\nhorizontal hGsD and vertical VGSD distance of the UAV's camera FOV, the GSD in the\nhorizontal and vertical directions can be calculated using the relative UAV height h of\neach image as follows:\nhGSD = 100$\\cdot\\frac{2\\cdot h\\cdot tanh\\frac{FOV}{2}}{Ximage}$,\nVGSD = 100$\\cdot\\frac{2\\cdot h\\cdot tan\\frac{FOV}{2}}{Yimage}$,\nBecause the horizontal and vertical distance of the FOV are calculated from the\naspect ratio and diagonal FOV, the vertical and horizontal GSD are the same. The GSD\nimage groups enabled us to compare the model at different GSD intervals. The recall\nmetric of the initial model for each GSD group is used for the motion control system.\nThe distribution of images in height and GSD groups is shown in Figure 3."}, {"title": "Object detection", "content": "The used model is YOLOv8 released in 2023 by Ultralytics and is the result of\nincremental improvements implemented on previous versions (YOLOv5, YOLOv6,\nYOLOv7, ...). The name YOLO comes from the simultaneous estimation of localization\nand classification that is done in one look of the images. The simplified scheme of the\nYOLOv8 architecture is shown in Figure 4 and consists of four main blocks: the input\ndata, the backbone, the neck, and the head.\nThe input data is the data provided to the model for training. The used images\nhave a resolution of 512 \u00d7 512 pixels. Different augmentation methods were used to\nintroduce new context improving the generalizability of the model. The augmentation\nmethods included in the training process were horizontal flip, vertical flip, rotation, hue,\nhsv, translate, scale, mosaic, erasing, and crop fraction. Most of these methods are set as\ndefault augmentation methods. The used model is the pre-trained YOLOv8 trained on\nthe COCO dataset.\nThe backbone network of the neural network is used to extract the features from\nthe images. Based on the extracted features, object classification and localization are\nperformed. The feature extraction is done in several layers. In our research, the\noriginally proposed custom CSPDarknet53 is used.\nIn the neck block, the extracted features are aggregated to form new features\nfrom different layers of the backbone network. To aggregate features, the original\nPANet was used.\nThe head is used for suggesting anchors bounding boxes of the detected objects,\nin our case persons. Additionally, the head is used to estimate the percentage of\ncertainty for detected objects. The used model head is the one presented in the original\nmodel, namely the YOLOv8 head."}, {"title": "Initial detection model performance", "content": "The sensing function is based on the YOLOv8 metrics, specifically the recall metric\nrepresenting the percentage of correctly identified objects in relation to the total number\nof actual objects in the dataset reflecting the model's effectiveness in detecting all\ninstances of a specific class. The recall used in the main experiment was obtained from\nthe initial experiment.\nSince the initial experiment flights were operated manually, there was no\noptimization of the flight height resulting in more GSD groups than in the optimized\nautonomous flight regime in the main experiment. The recall metric obtained by the\nvalidation on the initial dataset resulted in the recall metrics for each GSD shown in\nTable 1. It can be seen that the recall is generally getting lower for higher GSD\nintervals."}, {"title": "4. Experiment setup", "content": "The motivation of the experiment can be divided into two main goals, namely (1)\nadditional experimental validation of the autonomous motion control system using\nHEDAC and MPC and (2) creating a dataset containing people in different natural\nenvironments used in future SAR research. The experiment was conducted on the U\u010dka\nmountain, Croatia on 27.10.2024. with 84 volunteers including the organizers and\nconsists of a treasure hunt enabling the wanted motion behavior of the participants."}, {"title": "4.1 Location, environment and equipment", "content": "U\u010dka mountain Nature Park in Croatia presents a complex and challenging environment\nwell-suited for the evaluation of simulated UAV-based SAR operations. The area is\ncharacterized by uneven terrain, different low vegetation, and elevation variations,\nmaking it an ideal setting for assessing the capabilities of autonomous UAV systems in\nlocating missing persons in real-world conditions.\nIn this experiment, two UAVs were used: DJI Matrice 210 v2 and DJI Mavic 2\nEnterprise Dual. The UAVs characteristics are shown in Table 2. The specifications\nshow the o parameter of optimization used for defining the UAV movement in relation\nto the horizontal plane, minimum and maximum horizontal and vertical velocity,\nminimum and maximum horizontal acceleration, maximum angular velocity, and the set\nMPC time steps. In Table 3 the camera specifications of three used cameras are shown."}, {"title": "4.2 Design and preparation of the experiment", "content": "Ensuring an even probability of human targets within the defined search zone involved\nthe strategic placement of markers that needed to be found. The experiment consisted of\n150 markers where 50 of them were placed in each of the three zones as shown in\nFigure 5 to ensure the dispersion of individuals in the search area, thereby simulating a\nuniform distribution of targets in the search domain. The specification of each zone is\nshown in Table 5. The search domain is defined using the zones and consists of either\none or more zones with an offset between 50 and 100 m allowing UAVs to avoid\ntouching the boundary of the domain. The target probability distribution function was\nuniform for each zone. Additionally, the sum of undetected target probability\nthroughout the entire search domain is 1. The uniform probability inside each zone is\ndetermined by the number of people searching in that zone divided by the area of that\nzone."}, {"title": "4.3 Conducting the experiment", "content": "Conducting field experiments requires detailed planning, coordination, and adaptability\nespecially since real-world experiments introduce numerous challenges, including\nlogistical constraints, regulatory requirements, and unpredictable environmental factors.\nOne of these challenges is the unpredictable weather making long-term planning\nfor UAV-based SAR experiments challenging. While weather forecasts are monitored,\nsudden changes such as fog, wind, or rain can still occur, affecting UAV stability and\nvisibility. Despite this uncertainty, extensive logistical work must be completed in\nadvance, including inviting participants, coordinating with the nature park, obtaining\nflight and imaging permissions, and securing signed consent from all participants\ninvolved in the experiment. These preparations ensure regulatory compliance,\noperational feasibility, and safety. Even though the weather forecast seemed promising,\nthe weather on the experiment day was cloudy and foggy.\nManaging a relatively large number of participants also resulted in challenges.\nSome participants forgot to enter the required log data. Additionally, depending on the\nlocation inside the zones, internet connectivity issues prevented some of the participants\nfrom verifying their locations based on the GPS coordinates and maps provided in the\nflier, disrupting real-time decision-making resulting in some individuals straying\noutside of their assigned zones.\nSome of the images taken on 27.10.2024. during the experiment are shown in\nFigure 7. Subfigures (a) and (b) show the used drones, namely Matrice 210 v2 and\nMavic 2 Enterprise Dual. In (c) the home point of all flights is shown. Subfigure (d)\nshows the participants while giving them the introduction and explaining the\nexperiment. In Subfigures (e) and (f) an example of a UAV image from the first mission\nis shown, as well as a tile of the image containing a person."}, {"title": "5. Results", "content": "The following section presents the results of the conducted experiment consisting of the\nanalysis of the UAV motion control, the performance of the computer vision-based\nhuman detection, and the validation of the search and detection process."}, {"title": "5.1 Analysis of UAV motion control", "content": "As mentioned in the experiment setup section, the area containing markers where\npeople were expected to stay was divided into three zones. However, to capture the\nwhole zone, the UAV flight zone was larger than the defined search zones. The flight\ntrajectories of all flights during the three search missions are shown in Figure 8."}, {"title": "5.2 Computer vision human detection", "content": "The images resulted in most of them having no people. The number of images and\nnumber of labels in each flight is shown in Figure 10. The flight 3 which has been used\nto search all three zones has resulted in having the most images containing people\nmeaning it has also the highest number of labels, averaging on two persons per image\ncontaining people. Following flight 3, flight 1 has the most images containing people,\nbut flight 2 has more detected people."}, {"title": "5.3 Validation of the search and detection", "content": "To assess the motion control sensing predicted search accomplishment based on the\ninitial experiment recall, we have compared it to the YOLO recall obtained based on the\nmain experiment images and the first detection of the detected individuals. In image 14\nthe comparison of the search accomplishment and the YOLO detection rate for the first\nflight of the first search mission is shown. The predicted search accomplishment and the\nYOLO detection rate follow the same expected trend of detecting more people through\ntime having a similar increase in detection.\nUnfortunately, due to the increasing goal height and the camera resolution, it\nwas not possible to identify each individual in the second, third, and fourth flight to\nassess the success in the same way. The fifth flight doesn't have enough labels to\nconsider the result reliable so it has been discarded for the same analysis. However,\neven though the individuals were not identified, the YOLO success can be evaluated\nbased on the number of detected persons shown in Figure 14."}, {"title": "6. Discussion and drawbacks", "content": "Conducting UAV experiments presents significant challenges due to the\nunpredictability of environmental conditions. This variability makes it difficult to\ninform participants well in advance about the confirmed experiment date. Despite these\nuncertainties, the experiment was successfully carried out with 78 participants in the\nsearch and 6 staff members, even though the final date was set only five days before.\nWeather forecasts have consistently predicted sunny conditions with minimal chance of\nrain. However, on the experiment day, unexpected fog developed, followed by light rain\nafter the experiment concluded. Additionally, windy conditions can cause additional\ndifficulties, especially in UAV motion control. This has caused unpredictable low-\nquality of the images taken in Search missions 2 and 3 where individuals cannot be\nidentified. In addition to that, the Z30 camera used in the third flight of the Search\nmission 1 has shown unexpected low quality making it almost impossible to even\nmanually detect individuals.\nManaging a considerable large number of participants presents an additional\nsource of logistical challenges. In this study, one of the primary encountered difficulties\nwas data collection, as it relied on participants completing the logs accurately. Despite\nclearly outlining the required information and providing instructions on how to fill out\nthe forms, analysis of the submitted logs revealed some missing details, such as the\nnames of two participants and the shirt color of multiple individuals. In this case, the\nmissing names did not pose a significant issue, as they could be verified using the\nparticipant registration list. However, the shirt color has been shown as a bigger\nproblem since even though multiple individuals have not written any shirt color, there\nwere individuals who have changed the shirt or taken off the jacket during the\nexperiment, but have written only one color. These issues prevent the identification (not\ndetection) of individuals needed to obtain a first detection of each person which is\ncomparable to the search effectiveness \u03b7 calculated in the control framework.\nAdditionally, multiple participants have reported bad signal impacting the real-time map\ncausing issues in tracking the position inside their assigned zone."}, {"title": "7. Conclusion", "content": "This study presents the simulated SAR mission experiment conducted on U\u010dka\nmountain, Croatia with the goal of validating the search model, motion control system,\nas well as gathering additional data for future SAR research. The used motion control\nconsists of the HEDAC algorithm creating a potential field guiding the UAV's direction\nand MPC for optimizing the flight regime.\nThe experiment consisted of 150 markers being placed inside three zones\nensuring a uniform distribution of participants taking part in the treasure hunt for\nmarkers. During the search, three autonomously operated single-UAV search missions\nwere conducted, the first one consisted of three connected flights, while the other two\nconsisted of one flight each.\nThe results suggest that the probabilistic model of the search has a predicted\nsearch accomplishment similar to the manually detected individuals, as well as the\nYOLO detection rate. By this, the search and motion control systems are validated and\nshow promising results for this method to be used in SAR missions. This study can\nfurther be expanded by conducting a search exploration of a simulated SAR mission\nusing multiple UAVs to enhance the efficiency of the search."}]}