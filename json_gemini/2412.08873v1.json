{"title": "TOWARDS MODELING EVOLVING LONGITUDINAL HEALTH\nTRAJECTORIES WITH A TRANSFORMER-BASED DEEP LEARNING\nMODEL", "authors": ["Hans Moen", "Vishnu Raj", "Andrius Vabalas", "Markus Perola", "Samuel Kaski", "Andrea Ganna", "Pekka Marttinen"], "abstract": "Health registers contain rich information about individuals' health histories. Here our interest lies in\nunderstanding how individuals' health trajectories evolve in a nationwide longitudinal dataset with\ncoded features, such as clinical codes, procedures, and drug purchases. We introduce a straightforward\napproach for training a Transformer-based deep learning model in a way that lets us analyze how\nindividuals' trajectories change over time. This is achieved by modifying the training objective and\nby applying a causal attention mask. We focus here on a general task of predicting the onset of\na range of common diseases in a given future forecast interval. However, instead of providing a\nsingle prediction about diagnoses that could occur in this forecast interval, our approach enable the\nmodel to provide continuous predictions at every time point up until, and conditioned on, the time\nof the forecast period. We find that this model performs comparably to other models, including a\nbi-directional transformer model, in terms of basic prediction performance while at the same time\noffering promising trajectory modeling properties. We explore a couple of ways to use this model\nfor analyzing health trajectories and aiding in early detection of events that forecast possible later\ndisease onsets. We hypothesize that this method may be helpful in continuous monitoring of peoples'\nhealth trajectories and enabling interventions in ongoing health trajectories, as well as being useful in\nretrospective analyses.", "sections": [{"title": "1 Introduction", "content": "Large electronic health registers, such as nationwide health registers, contain rich information about individuals'\nhealth histories. Through the use of machine learning, we may be able to explore and better understand the health\nstatus and risk factors of individuals and groups in the population. This includes predicting future health outcomes.\nHowever, the nature of such data introduces various challenges from a machine learning perspective [1]. In our case,\nthe dataset includes many different age groups, it has large feature and label imbalances, and has missing information\nand ambiguity \u2013 resulting from treatments, procedures, diagnoses, medications, and disease prevalence change as a\nresult of the advancement of medicine."}, {"title": "2 Related Work", "content": "Predicting future health risks using Electronic Health Records (EHRs) data is a well-established area of research.\nModels like Logistic Regression and Support Vector Machines (SVMs) have been applied [8, 9]. The same goes for\ntree-based methods like eXtreme Gradient Boosting (XGBoost) [10, 11, 12, 13, 14]. However, these may not optimally\ncapture complex temporal relationships in longitudinal EHR data. Recurrent Neural Networks (RNNs), such as Long\nShort-Term Memory (LSTM) and Gated Recursive Unit (GRU) networks, can model sequential data [15, 16], but may\nstruggle with long-range dependencies [17].\nAttention-based transformer models represent the latest generation of sequential models, and have also been used\nin healthcare data analysis. Transformers are popular for their scalable parallel training capabilities and ability to\ncapture complex dependencies within sequences, making them well-suited for EHR data. Studies have shown that\ntransformer-based models perform well in tasks like predicting hospital readmission [18, 19] and identifying patient\nphenotypes [20]. Notably, the BEHRT model leverages transformers to effectively model sequential EHR data for\ntasks like disease pattern retrieval and postoperative mortality prediction [21]. Li et al. [22] proposed Hi-BEHRT\nmodel which improves disease risk prediction by using hierarchical transformers to handle long EHR sequences. Still,\ntransformer-based models do not necessarily outperform simpler models in basic health outcome prediction tasks\n[5, 6, 7]. Other works have explored variants of the transformer architecture [23, 24].\nSeveral works focus on generating and analyzing static (global) code embeddings from EHR data [25, 26]. Hettige et al.\n[27] introduced MedGraph, a graph-based model that captures both the relationships between codes within a visit and\nthe sequencing of visits over time. Lee and Van Der Schaar [28] introduced an RNN-based approach for clustering\ntime-series data that encourages latent representations (embeddings) of patients with similar future outcomes of interest\nto have similar representations at each time step. Munoz-Farre et al. [4] explores ways of interpreting the embedding\nspace of a bidirectional transformer-based model trained on EHR data. By projecting the embeddings down to two\ndimensions, they show that the model generates similar representations for similar comorbid-diseases or medications.\nThey also explore temporality by segmenting the data into fixed time windows (snapshots of 5 or 10 years) around\nwhen a target disease was assigned, before analyzing changes between the embeddings from the different time windows.\nThey show how this can be used to identify subgroups within a disease who have distinct progression patterns. Chen"}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Unidirectional Transformer Model with Evolving Predictions", "content": "The base machine learning architecture used is the transformer with multiple stacked multi-headed self-attention layers\n[2]. Each element in an input sequence is computed in parallel, which differs from recursive models. The originally\nproposed transformer model/architecture combined both an encoder and decoder component into an encoder-decoder\nmodel. Afterwards, we have seen models utilizing only encoders, such as BERT [32], with bidirectional attention,\nwhich are typically used for classification tasks. Decoder-only models have become popular as generative models,\nsuch as GPT [33, 34]. These incorporates causal attention masking to make the model unidirectional with generative\ncapabilities.\nInspired by previous work on transformer models for EHR data, the inputs to the models are the codes (code), the\nassociated ages of the person when these codes occur (age), and their position in the sequence (pos, see e.g. Li et al.\n[21]). We also introduce an additional parameter which represents the number of years until the forecast interval (t2f).\nCodes, ages, positional information, and years to the forecast period go through an embedding look-up layer before\nthese are position-wise summed and given as input to the model, as illustrated in Figure 2. The input to the model for\none individual n, with $T_n$ codes in their historical interval, is thus\n$x_n(1 : T_n) = (code_{1:T_n}, age_{1:T_n}, pos_{1:T_n}, t2f_{1:T_m})$. Each individual has one to multiple labels $(y_n)$ indicating possible\ndiseases, death, or none, occurring in the forecast interval. Since we have a fixed set of classes $C$, the labels for one\nindividual are represented as a binary vector $y_n \\in [0, 1]^C$.\nThe Evolve model The motivation for this approach is to enable the model to identify and predict, as early as\npossible in a person's life, upcoming health problems. This ultimately allows us to study how these predictions change\n(evolve) over time. To achieve the desired properties, we apply an attention mask as in decoder-based transformer\nmodels, however we do not train it in a next-token prediction manner. Instead, during training, the model is tasked\nwith always predicting, for every input $x_n(1 : t)$ with $t \\in 1 ... T_n$, the same set of diagnosis labels $y_n$ that occur in\nthis person's forecast interval. This means that we also have $T_n$ predictions, $\\hat{y}_n(1 : T_n)$. Attention masking makes the\nprediction $\\hat{y}_n(t)$ conditioned only on $x_n(1 : t)$. During training, $y_n(t)$ is the same for all $t \\in 1 . . . T_n$. For individual $n$,\nthe model $M$ will thus make $T_n$ multi-label predictions:\n$\\mathcal{M}(x_n(1:T_n)) = \\hat{y}_n(1:T_n)$"}, {"title": "3.2 Reference Models", "content": "Primarily to verify that the masking and training approach used in the Evolve model does not have a large negative\nimpact on its overall prediction performance, we also include a couple of reference models. One model is a bidirectional\ntransformer-based BERT-style model [32, 21] that we simply call CLS. This does not have attention masking and has a\nspecial \"CLS\" token as the first code position in the sequence. A decision layer is (only) applied to the output from this\nCLS token when predicting labels and calculating loss. We also include a Logistic Regression model [35] (LogReg)\nand a XGBoost model [36] (XGBoost), which are trained on count-based feature (code) vectors concatenated with age."}, {"title": "3.3 Extracting Age Embeddings", "content": "Transformer models are known for their contextualized embedding representations resulting from the attention mecha-\nnism. One way of analyzing health trajectories involves looking at the latent embedding space of the Evolve model. To\ndo so, we first aggregated the output embeddings at each position into age embeddings $(z_n)$. This is done by\napplying Position-Weighted Mean Pooling (PWMPooling) to the set of embeddings at each age $(z_n)$. This is similar to\nwhat Muennighoff [37] used to extract semantic sentence embeddings from a GPT model.\n$z_n(a) = PWMPooling({z_n(t) : age_t = a})$\nWeights are 1, 2, 3, ... m, where m is the number of inputs at age a. Most recent embedding gets the largest weight.\nThe embeddings are finally normalized. As individuals may not have recorded data at every age, we simply fill in the\ngaps with the embeddings from the preceding ages for the subsequent analysis."}, {"title": "3.4 Computing Nearness and Neighborhood Changes in the Embedding Space", "content": "To compute the nearness between pairs of age embeddings we used the cosine similarity metric $(cos)$, which quantifies\nthe similarity between two vectors (embeddings) to a value in the range 0-1, low-high. One application of this is to\ncalculate the similarity between two individuals, for example $n$ and $m$, at ages $a$ and $b$; $cos(z_n(a), z_m(b))$.\nWe also use this to compute the rate of change in the neighborhood of a target individual as they age. More specifically,\nwe iteratively calculate the fraction of changes among reference individuals that are most similar to the target individual,\nfrom one age ($a$ \u2013 1) to the next ($a$). $N_n(a, k)$ which is the $k$ nearest neighbors to $n$ (individuals with the highest cosine\nsimilarity) when all are at age $a$. The rate of change at age $a$ with neighborhood size $k$ is thus:\n$r_n(a, k) = 1 - \\frac{|N_n(a - 1, k) \\cap N_n(a, k)|}{k}$\nAs an example, the orange dots in Figure 1 shows the rate of change from one year to the next in these persons' health\nhistories."}, {"title": "4 Data", "content": "The dataset used is a collection of Finnish nationwide registers \u2013 FinRegistry [38]. FinRegistry is a comprehensive\nregister-based data resource that provides access to a diverse range of health and sociodemographic data for the entire\nFinnish population. The initial dataset included 7,166,416 individuals, with 74.51% alive on January 1, 2010. We\nexcluded individuals who died before the forecast interval, emigrated, or had no healthcare records leaving a sample of\n5,173,795: for training (70% - 3,620,947) validation (10% - 517,160) and testing (20% - 1,035,688). At the beginning\nof 2016 (the start of the forecast interval), the mean age of our study population was 41.9 years (SD=24.3). There were\nslightly more females (50.8%) than males (49.2%). For training we have used the data up to 2015.09.01 (historical"}, {"title": "5 Experiments and Results", "content": "Here we present a set of experiments and their results."}, {"title": "5.1 Model Training and Performance", "content": "For implementation of the transformer-based models (Evolve and CLS), our starting point was the nanoGPT imple-\nmentation by Karpathy [39]. Other Python libraries and packages used for implementation and evaluation include\nPytorch [40], NumPy [41], Scikit-learn [42]. The XGBoost model was trained using the xgboost Python library in\ncombination with Scikit-learn. For the LinReg model we used the implementation available through Scikit-learn. The\nMultiOutputClassifier wrapper was used to enable multi-label classification for the latter two models. See Appendix A\nfor more information on model training."}, {"title": "5.2 Detecting Trajectory Changing Events", "content": "Here we propose two ways to use the Evolve model to detect important events and codes in individual's health histories\nthat are associated with changes in their health status.\nLooking at the changes in the model's prediction probabilities (sigmoid values) This implies analyzing how the\nsigmoid values for each label evolve over time, as the model iteratively sees more of an individual's health history. This\nprovides insight into important codes and code sequences that trigger any changes to these sigmoid values. In Figure 1\nthe plots show examples of how the predicted probability for various diseases changes over time for this person relative\nto the forecast interval. Important events (input codes) associated with a label may be found by sampling codes causing\nincreases in the sigmoid value, or when the sigmoid value passes a predefined decision threshold. In this experiment we\nexplore the former approach.\nFirst, we presented the health history $x_n(1 : T_n)$ of each individual $n$ in the validation set to the model. From these we\nextracted the maximum jump in sigmoid value (from one code to the next) for each class where $y_c = 1$. From this set,\nwe accumulated the mean jump per code $J_{1:C}$. Next, for each individual in the test set, we sampled the sigmoid jumps\nper class $c$ that were equal to or greater than $j_c$. We also include the average age of when the codes were given\nand the average number of years to the forecast interval (t2f)."}, {"title": "5.3 Nearest Neighbors Analysis", "content": "Here we propose another way of exploring and interpreting individual's health trajectories with the Evolve model. This\ntoo involves looking at their nearest neighbors in the latent embedding space as they age. However, the aim here is to\nanalyze how a person's health profile becomes more or less similar to representatives from each disease class over\ntime.\nWe observed that all individuals, even with the same outcome labels, tend to have quite unique health trajectories,\nespecially as they get older. This increasing uniqueness of individuals' health trajectories is also observed in Jylh\u00e4v\u00e4\net al. [43]. Thus, instead of pre-calculating global diagnosis centroids for each diagnosis, we found that it made more\nsense to only compare the target person to a limited set of the most similar representatives from each diagnosis. For a\nsingle target person, at every age, we calculate how similar his or her embedding profile is to the mean embedding\nof the top $k$ most similar reference individuals from each class ($\\mathcal{{y^1}}$, $\\mathcal{{y^2}}$ ... $\\mathcal{{y^C}}$, i.e., we know they will get the\ndiagnosis $y$) when they were the same ages$^3$."}, {"title": "6 Discussion", "content": "From the performance evaluation in Section 5.1, we can conclude that the Evolve model performs comparable to the\nCLS model/approach in terms of predicting disease onsets in the forecast interval given the full historical interval as"}, {"title": "Ethics approval", "content": "FinRegistry is a collaboration project of the Finnish Institute for Health and Welfare (THL) and the Data Science Genetic\nEpidemiology research group at the Institute for Molecular Medicine Finland (FIMM), University of Helsinki. The\nFinRegistry project has received approvals for data access from THL (THL/1776/6.02.00/2019 and subsequent amend-\nments), Digital and Population Data Services Agency (VRK/5722/2019\u20132), Finnish Center for Pension (ETK/SUTI\n22003) and Statistics Finland (TK-53\u20131451-19). The FinRegistry project has received IRB approval from THL (Kokous\n7/2019)."}]}