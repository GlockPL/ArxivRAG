{"title": "Is Human-Like Text Liked by Humans?\nMultilingual Human Detection and Preference Against AI", "authors": ["Yuxia Wang", "Rui Xing", "Jonibek Mansurov", "Giovanni Puccetti", "Zhuohan Xie", "Minh Ngoc Ta", "Jiahui Geng", "Jinyan Su", "Mervat Abassy", "Saad El Dine Ahmed", "Kareem Elozeiri", "Nurkhan Laiyk", "Maiya Goloburda", "Tarek Mahmoud", "Raj Vardhan Tomar", "Alexander Aziz", "Ryuto Koike", "Masahiro Kaneko", "Artem Shelmanov", "Ekaterina Artemova", "Vladislav Mikhailov", "Akim Tsvigun", "Alham Fikri Aji", "Nizar Habash", "Iryna Gurevych", "Preslav Nakov"], "abstract": "Prior studies have shown that distinguishing text generated by large language models (LLMs) from human-written one is highly challenging, and often no better than random guessing. To verify the generalizability of this finding across languages and domains, we perform an extensive case study to identify the upper bound of human detection accuracy. Across 16 datasets covering 9 languages and 9 domains, 19 annotators achieved an average detection accuracy of 87.6%, thus challenging previous conclusions. We find that major gaps between human and machine text lie in concreteness, cultural nuances, and diversity. Prompting by explicitly explaining the distinctions in the prompts can partially bridge the gaps in over 50% of the cases. However, we also find that humans do not always prefer human-written text, particularly when they cannot clearly identify its source.", "sections": [{"title": "1 Introduction", "content": "Recent technological developments have advanced the generative artificial intelligence (AI) models, such as GPT-*, Gemini, Claude, and Llama (OpenAI, 2023; Team et al., 2023; Anthropic, 2024; Dubey et al., 2024), thus aggressively blurring the lines between human and AI capabilities. How often can state-of-the-art (SOTA) LLMs fool human evaluators, i.e., passing the Turing test? Can human evaluators correctly predict the origins of encountered content (human vs. machine), thus safeguarding against the potential misuse of LLMs?\nSeveral studies have explored whether humans can distinguish between content generated by a human vs. a machine. Unsurprisingly, the findings depend in part on the quality of the machine generator. Early studies probing the outputs of less advanced AI models found that human evaluators could indeed detect the difference (van der Lee et al., 2019). However, in studies using LLMs such as GPT-3.5-turbo, human evaluators are frequently close to random chance (Guo et al., 2023; Hitsuwari et al., 2023; Dugan et al., 2023; Chein et al., 2024; Wang et al., 2024a), particularly for laymen who are not frequent users of LLMs.\nWhile covering different domains, e.g., academic paper abstracts, Wikipedia paragraphs, question-answering responses (Wang et al., 2024b,a), news and review comments (Chein et al., 2024), most studies focused on GPT-3.5-turbo and English (see Table 1). Generally, fewer than 300 examples were evaluated by experts (LLM researchers or frequent users) in such studies.\nWith the advancement of newer LLMs such as GPT-40, Claude-3.5-Sonnet, and Llama3.1, we ask whether these findings generalize to them, to other languages, and to native expert annotators? What are the upper bounds of human detection performance across languages, domains and new LLMs?\nTo answer these questions, we perform a comprehensive case study over sixteen datasets spanning nine languages, nine domains, and eleven SOTA LLMs. We specifically focus on the following four research questions: (i) How well can human annotators distinguish human-written text from SOTA LLMs generations? (ii) What are the notably detectable linguistic qualities in human- and AI-authored texts that shape decisions about text origin? (iii) Can a prompting strategy fill the gap between human and machine? and (iv) Do humans always prefer human-authored text?"}, {"title": "2 Case Study", "content": "Previous studies presented the difficulty humans face in distinguishing SOTA LLM-generated content from human-written text, often resulting in a random guess (see more in Appendix A). However, most evaluations focused on English and generations by GPT-3.5-turbo, leaving the detectability of MGT in other languages and LLMs uncertain.\nTo verify whether this observation can be generalized to other languages and more advanced LLMs, we collected LLM generations based on 16 datasets across nine domains and nine languages. 19 native speakers who are either LLM researchers or practitioners performed human evaluations, investigating (i) whether humans can correctly discern human vs. AI outputs, and (ii) whether humans prefer fellow human answers or LLM responses."}, {"title": "2.1 Task and Dataset", "content": "MGT detection The goal is to identify whether the text was written by a human or generated by models given a single text, or to recognize which text is written by a human given a pair of texts: one human-written and one machine-generated.\nIn data collection, we focused on datasets from common domains such as community QA, news, tweets, and government reports, alongside domains requiring high-integrity LLM applications, including educational and academic contexts, such as accurate knowledge verification in Wikipedia-style texts, and identifying the authorship of student essays and peer reviews. For a given language and given a dataset, we sampled 300-600 human texts and then generated corresponding machine text using two SOTA LLMs: a multilingual model (e.g., from the GPT or the Claude series) and a language-specific model (e.g., ChatGLM or Qwen for Chinese and AceGPT for Arabic), to analyze the impact of different LLMs on detection performance, particularly for non-English languages. As shown in Table 4, we collected data based on 16 datasets across nine languages. The generation prompts and collection details are shown in Appendix B."}, {"title": "2.2 Human Detection Setups", "content": "Annotation Settings To mimic real-world machine-generated text detection scenarios, we set up four human evaluation settings. Given human-written text and machine-generated text, representing by hwt and mgt respectively, human annotators are asked to identify which text was written by a human. Note that mgt can be generated by multiple different LLMs, referring to as mgti, where i \u2208 [1, 2, \u2026\u2026\u2026, n].\nAs shown in Table 2, according to the input and the output options, we categorize detection settings as I. pair-binary, II. pair-four-class, III. single-binary, and IV. triplet-three-class. For single text input, either hwt or mgt, the goal is to recognize whether the text was written by a human, by answering just Yes or No. This is suitable for the scenario where for the human text there is no necessarily a corresponding machine-generated text, and thus they can be collected from different sources and for different topics, such as Arabic tweets. Given a pair of texts (text1, text2), a binary output is easier than the four-class detection. The pair-binary setting asks that either text1 or text2 is hwt, and the other one is mgt, while the pair-four-class setting has no restrictions: each of text1 and text2 can be hwt or mgt, regardless of the label of the other text. Sometimes, we want to compare human text to generations from different LLMs, in which case, we apply IV, which we limit to a three-class detection: human vs. LLM\u2081 vs. LLM2.\nOverall, I and IV are suitable for scenarios where there is a human text and its corresponding machine-generated text. II and III can be used if there are non-corresponding human-written and machine-generated texts. If the annotators have seen some human-written and some machine-generated text before detection, we refer to this as a few-shot setting; otherwise, we have zero-shot.\nAnnotation Tool To mitigate potential labeling biases arising from raw spreadsheet annotation and to enhance efficiency, we implemented two methods with optimized interfaces and workflows for our annotation: (1) a custom pipeline using the Google Workspace suite, including Apps Script, Google Sheets, and Google Forms. The core idea was to store all data in Google Sheets, use Apps Script to extract data and generate a survey in Google Forms; and (2) Label Studio, an open-source multi-type data labeling and annotation tool with a standardized output format. We designed a custom template for our annotation task and collected results using this platform. The annotators were given the choice to use their preferred tool.\nAnnotator Background In order to explore the upper bound of human detection capability, instead of using crowd-sourcing annotators, we conducted in-house labeling. The annotators were BSc, MSc, and PhD students, as well as postdocs, who were familiar with NLP tasks and LLM generations. All annotators independently completed their individual annotations. For each language, the annotators were all native speakers of that language. See more detail in Appendix A.3."}, {"title": "3 Human Detection", "content": "We performed an extensive case study on 9K instances across nine languages to verify how difficult it is for native speakers to detect AI outputs in everyday domains. Table 3 demonstrates that the average human detection accuracy is 87.6%. This reveals that this is not particularly difficult for native human experts, contrary to what previous studies have reported. Below, we zoom into the impact of various factors."}, {"title": "3.1 Language", "content": "Human detection accuracy exceeds 87.6% for Chinese, English, Arabic, Italian and Russian, while it falls below this level for Vietnamese, Kazakh, Hindi, and Japanese. This discrepancy is largely due to the challenge of Wikipedia text."}, {"title": "3.2 Domain", "content": "Wikipedia is widely used as training data for LLMs, particularly for low-resource languages, due to the scarcity of alternative datasets. Consequently, models often memorize Wikipedia content, leading to generated text that closely resembles human-written Wikipedia. Arabic tweets also present challenges for detection due to their short length and limited context, along with summaries, e.g., for Arabic and Russian summaries, the expert detection accuracy is about 80%. This conversely highlights the ability of language models to generate high-quality human-like text in the domains of Wikipedia, tweets, and summaries. In contrast, substantial differences between machine-generated and human-written text persist in news articles, QA, student essays, and peer reviews, making them much easier to recognize for human experts."}, {"title": "3.3 Generator", "content": "It is hard to detect MGT across generators and languages. While there are minimal differences for Chinese (accuracy is 100% vs. 98% for GPT-40 vs. Qwen-turbo), there are sizable differences for Italian and Arabic. Based on Italian DICE News, the same annotator detected generations by Anita (an Italian fine-tuned Llama3-8B), Llama3-405B, and GPT-40, achieving accuracy of 88%, 99%, and 100%, respectively. Similarly, for Arabic tweets, GPT-40's outputs are more similar to human text and thus more difficult to detect compared to those by Qwen2, as shown in Table 6."}, {"title": "3.4 Annotation Setting", "content": "We conducted the majority of our annotations under setting I. pair-binary: given a pair (hwt, mgt), it asks to identify which of the two texts is human-written. We assumed that more complex settings would be more challenging. For instance, II. pair-four-class should be harder, as each of the texts could be human- or machine-generated, independently of the other. Yet, for Chinese student essays, the performance for II does not degrade compared to I. Similarly, for government reports in IV. triplet-three-class, where the annotators have to select the human-written text among three candidates, there was no degradation compared to I.\nHowever, III. single-binary proves to be more difficult than I. pair-binary for both Arabic and Russian. While domain differences do exist, e.g., tweets vs. summary vs. news in Arabic and news vs. summary for Russian, the substantial performance gap (>20%) can still be partially attributed to the annotation settings. Overall, comparing the results for I. vs. III., it is easier to distinguish machine-generated content when given a comparison pair, rather than for single answer. Yet, introducing text triplets or increasing the number of machine-generated or human-written texts had minimal impact on detection performance. Moreover, using few shots before detection boosted the confidence of the annotators, resulting in higher accuracy compared to zero-shot. For datasets with high accuracy, before seeing labeled samples, the annotators found the distinction to be obvious. After seeing a few examples, the annotator was extremely confident in distinguishing human vs. machine text based on indicative features of MGT."}, {"title": "3.5 Expert Annotators", "content": "For the same language and dataset, individual annotator ability influences detection accuracy but not significantly. For instance, in Chinese Zhihu-QA (GPT-40 vs. human), five annotators achieved accuracies of 99%, 99%, 100%, 100%, and 100%. Similarly, for Zhihu-QA (Qwen-turbo vs. human), two annotators obtained 99% and 97%. In student essays (II. pair-four-class), three annotators recorded accuracies of 96%, 96%, and 99%. This may also result from the bias that all annotators are native speakers and expert-level LLM practitioners or researchers. Differences between individuals are minor in their cognitive abilities, language proficiency and domain knowledge."}, {"title": "3.6 Distinguishable Factors", "content": "We summarize five remarkable distinguishable signals between machine-generated vs. human-written text across the 16 datasets and the 9 languages; see more details in Appendix C.\n\u2022 Human text is more informative and concrete. Human-written text contains concrete numbers, specific names of people or institutions, exact places or dates, URLs, and other references, while machine-generated text tends to provide generic information, with little detail to support its statements.\n\u2022 Machine-generated text lacks regional, cultural, and religious nuances. For languages such as Arabic, Japanese, Hindi, Kazakh, and Chinese, human texts reflect the cultural and the religious nuances of the language, which is not true for machine-generated text.\n\u2022 Human-written text varies substantially in terms of length, structure, style, and sentiment. Human texts show diversity and inconsistency with large deviations in length, structure, style and emotions, while machine-generated texts tend to use a formulaic structure and neutral/positive emotion. This can be partially attributed to LLMs rigorously following instructions, and thus losing on flexibility.\n\u2022 Machine-generated text has formatting. MGTs are generally well-segmented with bullet points for better readability, while human-written texts are typically just large block of plain text, which may be due to human text collection and conversion. Moreover, machine-generated texts often use Markdown style, e.g., ** and ###, while human-written texts have typos, grammatical errors, hashtags, and other social media elements.\n\u2022 Machine-generated text shows a mixture of other languages. Non-English language responses often contain some English parts, which is very rare in human text."}, {"title": "4 Can Prompting Fill in the Gap?", "content": "Given that LLMs can strictly follow instructions and their outputs are heavily influenced by the system and the user prompts, we investigated whether explicitly instructing LLMs to mimic human style can help narrow the gap. Responding to the distinguishable factors summarized in Appendix C for each dataset, we asked the human annotators to craft new prompts, aiming to improve the generations and to reduce the gap between human-written and LLM-generated texts. This involved trying instructions that (1) incorporate specific details and references, (2) avoid formulaic structures and formats, e.g., bullet points and Markdown, and (3) generate texts of varying length, structure, and sentiment. Table 8 presents the results for both the original and the improved prompts for all datasets.\nMeasurements: We re-generated the machine-generated parts of the text pairs, using the same models with improved prompts, and then sampled 200-600 examples from each dataset to assess whether and to what extent, the prompting strategy narrowed the gap between human-written and machine-generated texts. We used two approaches: (1) fill-the-gap survey, where the original annotators evaluated whether the newly-generated text bridged the gap (Yes, No, or Partially), and (2) downstream detection, where we compared the detection accuracy before and after applying the improved prompts. A decline in detection accuracy indicated a reduced distinction between human and machine text, making the differentiation more difficult, and further revealing that prompting was effective. Our experiments involved both human annotator evaluation and automated detection."}, {"title": "5 Human-Like or Liked-by-Human?", "content": "We used the prompting strategy to bridge the gap between human-written and machine-generated text, aiming to make machine outputs more human-like. However, do humans favor human-like text? Below, we examine human preferences among four options: (1) human-written text, (2) machine-generated text using the original prompt, (3) machine-generated text using the improved prompt, or (4) none of the above."}, {"title": "6 Conclusion and Future Work", "content": "We conducted a comprehensive study to investigate the capability of humans to detect text generated by SOTA LLMs. Based on 16 datasets spanning 9 languages, 9 domains, and 11 LLMs, 19 expert annotators explored the upper bounds of human detection potential. Accuracy ranged in 50-100%, with an average of 87.6%, revealing that it is not that challenging for experts to identify MGTs.\nWe found that the major gaps between human-written and machine-generated text lie in concreteness, cultural nuances, diversity in length, structure, style, and sentiment, formatting and mixture of languages (with English) in the generations. Prompting by explicitly indicating the distinctions could partially bridge this gap, but cultural nuances and diversity remained a challenge. Yet, humans favored machine-generated text in over half of the cases, particularly when they could not recognize which text was written by a human.\nIn future work, we plan to further explore the subtle relationship between human detection accuracy, preference, and individual characteristics: (i) Are humans more likely to favor human-written text when they can identify it? and (ii) How do an individual's philosophy, personality, and experience influence their ability to discern between human-written and machine-generated text?"}, {"title": "Limitations", "content": "Annotator Bias All annotators involved in the detection process are advanced NLP researchers, including Msc and PhD students, as well as postdocs, specializing in LLMs. No laymen participated in the annotation process, which ensures that the findings and the conclusions were relevant within the expert domain. It is possible that different results could be obtained if laypersons were involved in the detection tasks. However, given our aim to explore the upper bound of human capability in detecting machine-generated text, the current setting is appropriate. Future work could consider involving a broader range of participants to assess the average detection capability across different population groups.\nStatistical Significance In our study, each dataset includes at least 300 examples for annotation. For some datasets, 3-5 annotators labeled the same set of data, and the average values were calculated. However, a larger sample size would be beneficial to ensure more reliable results. In particular, in the survey exploring whether improved prompts bridge the gap between human-written and machine-generated text, substantial variability between individuals was observed. This subjectivity should be carefully considered in future work. Fortunately, three evaluation methods were used to assess whether prompting could mitigate this gap in our work, partially addressing this limitation by comparing human and automatic detection accuracy before and after the application of the improved prompts.\nLinguistic Analysis Due to page limitations, we were unable to present a detailed linguistic analysis of the text from human-written, original machine-generated, and newly generated outputs. We will perform analysis of vocabulary features, part-of-speech tagging, dependency structures, sentiment analysis, language model perplexity, and other relevant metrics in future work."}, {"title": "Ethical Statement", "content": "This section outlines potential ethical considerations related to our work.\nData Collection and Licenses A primary ethical consideration is the data license. We reused existing dataset for our research, such as HC3 M4GT-Bench, MAGE, RAID, OUTFOX and LLM-DetectAIve, which have been publicly released under clear licensing agreements. We adhere to the intended usage of these dataset licenses.\nSecurity Implications The shared task datasets support the development of robust MGT detection systems, essential for addressing security and ethical concerns. These systems help prevent automated misinformation campaigns, protect against financial fraud, and ensure content integrity in critical domains like journalism, academia, and law. Beyond security, effective detection promotes digital literacy by fostering public awareness of LLM limitations and encouraging critical engagement with online content.\nIn multilingual contexts, MGT detection is particularly challenging due to linguistic and cultural nuances. Advanced systems must address these complexities in order to prevent disinformation, especially in less-resourced languages. Strengthening multilingual detection enhances the global trust in AI technologies and mitigates the security risks associated with their misuse.\nHuman Subject Considerations Our study engaged human evaluators in distinguishing between human-written and machine-generated texts, while expressing their preference. All annotators gave informed consent, were fully aware of the study's objectives, and had the right to withdraw at any time. Since this study was designed to be evaluated by experts, all annotators participating in it were advanced NLP researchers (MSc and PhD studnets, as well as postdocs, specializing in LLMs). While this ensures a rigorous assessment of the detection capabilities within an expert domain, we acknowledge that findings may not generalize to laypeople. Future research should include a more diverse range of participants to evaluate human detection performance across various demographic and professional backgrounds."}]}