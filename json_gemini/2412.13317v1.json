{"title": "Predictive Probability Density Mapping for Search and Rescue Using An Agent-Based Approach with Sparse Data", "authors": ["Jan-Hendrik Ewers", "David Anderson", "Douglas Thomson"], "abstract": "Predicting the location where a lost person could be found is crucial for search and rescue operations with limited resources. To improve the precision and efficiency of these predictions, simulated agents can be created to emulate the behavior of the lost person. Within this study, we introduce an innovative agent-based model designed to replicate diverse psychological profiles of lost persons, allowing these agents to navigate real-world landscapes while making decisions autonomously without the need for location-specific training. The probability distribution map depicting the potential location of the lost person emerges through a combination of Monte Carlo simulations and mobility-time-based sampling. Validation of the model is achieved using real-world Search and Rescue data to train a Gaussian Process model. This allows generalization of the data to sample initial starting points for the agents during validation. Comparative analysis with historical data showcases promising outcomes relative to alternative methods. This work introduces a flexible agent that can be employed in search and rescue operations, offering adaptability across various geographical locations.", "sections": [{"title": "I. INTRODUCTION", "content": "Search and Rescue (SAR) of vulnerable people is unfortunately a common task for the Police and other emergency services. Organizations like the Centre for Search and Rescue[1] and the Grampian Police [2] carry out research and training in areas related to SAR and while their training and published papers offer a valuable resource to the people responsible for finding a Lost Person (LP), they focus only on land-based search i.e. directing teams of individuals. This is a slow and methodical process that would undoubtedly benefit from the assistance of an airborne surveillance platform. As a result of rapid advances in the drone sector over the last decade, multirotors capable of carrying high-powered sensor payloads have become cheaper and more accessible than ever before. Consequently, several concept evaluation trials for measuring the efficacy of incorporating drones into the search process have recently been undertaken in Scotland[3].\nAir-based SAR is a core operational requirement of the Police Scotland Air Support Unit (PSASU)[3] and an up-and-coming tool for Scottish Mountain Rescue (SMR)[4]. Both PSASU and SMR traditionally use large helicopters which have the same limitations: cost, transit time, availability, and weather. The latter is a common issue during SMR rescues as near-perfect conditions are required for safe helicopter evacuation. Similarly, PSASU has their only Eurocopter EC135 based in Glasgow, Scotland. If this helicopter was required in remote locations, such as the Isle of Orkney, this life-saving resource would be multiple hours away from full deployment. Hence, PSASU and SMR are placing a small fleet of drones in key locations around Scotland for rapid deployment to assist in LP search operations where seconds matter. These will not be replacing the helicopter, but are rather intended to complement its operations.\nWhilst rapid deployment is a key requirement for the usage of Unmanned Aerial Vehicle (UAV) in an LP search, the cost of their usage must also be explored. During a rescue operation, cost can be defined in terms of time, personnel, and money. The latter is calculated pre- or post-mission, but the time and personnel cost are what the search leader balances. Having more personnel available means that more searchers can actively cover the search area and this could directly lead to faster find time. However, personnel are not an infinite resource and people need to be effectively assigned. Therefore, computational assistance to the UAV-based search segment is a key element in freeing up resources.\nThe mission profile for UAVs in a SAR setting is solely for search. This requires finding the LP as quickly as possible and relaying this information back to the search leader so that they can organize the rescue operations. The search further breaks down into prediction, flying, and sensing. Flying and sensing are done in real-time[5], but the prediction of where the LP could be can be done en route to the scene. By creating a Probability Distribution Map (PDM), the search leader and their available resources can be informed on where the most likely locations are to find the LP. This, in theory, results in a decrease in the time to find an LP that could save lives.\nOne key piece of information in the prediction process is using data to inform and validate the models. Historical data collected from SAR cases agrees that significant behavioral profiles exist[6], [1]. This means that a solo hiker behaves differently from an elderly person with dementia when moving across a landscape, which ultimately affects the location found. By using this a priori information along with other location data, these PDMs can be highly customized on a per-location and per-person basis resulting in better PDMs[7].\nA crucial constraint is the lack of available data. Even the largest SAR database ISRID[8] has only 50,000 data points, with many not including geospatial information. PDMs need large amounts of data that cannot be gathered effectively through simple data logging by various bodies. This drives the need for models to generate the data so that the likes of"}, {"title": "II. CURRENT PDM GENERATION ALGORITHMS", "content": "The most basic form of PDM generation is the Euclidean Distance Circle (EDC) as described by [6] and [10]. This method works on bounding a search area within a circle based on the straight line distance from statistics about how far away from the Place Last Seen (PLS) an LP might be. Whilst statistically sound, this method fails to incorporate the terrain of the search area at hand. Approximately 75% of wilderness SAR incidents happen in mountainous regions[11] where there are many barriers to travel around. [10] outlines a modified EDC algorithm that allows incorporating impassable features into the resultant PDM, however this is only possible through manual intervention by the searcher. This makes it unsuitable for any automatic PDM generation. As well as the inability to incorporate geographical features, the search area size radically changes depending on the LP profile. From data seen in [6], the 95% radius for Child 12-15 is 5.65 times larger than the Child 10-12 category. This makes it hard to focus resources in a SAR scenario based purely on the EDC.\nLike the EDC model, the Watershed model from [11] and [12] relies on historical distance data to be constructed. [12] found that in Yosemite National Park, 48% of LPs were found within the original (0th) watershed (an area of land that separates water flowing into different areas), and 38% were found in the adjacent watersheds with [11] finding similar results. However, whilst it incorporates information about the terrain, it is limited to a single source of data. Using more models, or data, is beneficial in creating more nuanced models, as discussed in [13]. This also gives the benefit of generating more customized PDMs for a single search area. A benefit of using simpler models is that less computational, or human effort, is required to generate a PDM. A searcher in the hills may not have access to a powerful computer to calculate specific PDMs or the time to wait for results.\nMobility models, such as by [14], try to estimate how far the LP may have traveled in all directions from the PLS. This is done by combining simulations of walking speed with environmental effects that may affect their motion. For example, walking through a dense forest will be slower than over a grassy field. Another approach is considering what the path of least resistance is to a given coordinate from the PLS. This can be done by calculating the cost of passing through a subset of the area and applying a path-planning algorithm like Randomly Exploring Random Tree [15]. Other models may use Accumulated Cost Surface [16] algorithms as they tend to be included in Geographic Information System (GIS) software.\nAn advancement on the mobility model is the Travel Time Cost Surface Model (TTCSM)[17]. This model uses the notion of percent of maximum travel speed to evaluate the time taken to travel through a cell. This value is assigned by a location expert and can vary for different search areas. Limits are assigned to the area through this concept by giving it a value of 0, such as for in a lake or a cliff (defined as any slope steeper than 31\u00b0). A major factor affecting the walking speed of the LP is the slope angle, which is extrapolated from the digital elevation map. TTCSM uses the Tobler model[18] for walking speed which provides a continuous estimation based on gradient $m = tan(s)$, where s is the slope angle.\n[19] developed the Travel Time Network Model which builds on TTCSM and focuses on trail networks. In this model, network theory is used rather than the rasterized method from before. Every edge (a path) is given a cost associated with the time taken to traverse. The network is then used to generate the PDM using a service area analysis at defined threshold values. These threshold values are the mobility times from [6]. An evident drawback of this approach is the assumption that the PLS is on a trail, and that there are sufficient trails for the model to work effectively. The latter is particularly important as this model would not work as well in the rural Scottish Highlands as it would in the English Lake District. However, the approach is viable when these assumptions have been met.\n[20] and [21] developed a six-behavior algorithm to simulate an agent navigating a 2D grid. Each time step involves selecting a behavior through a learned weighting vector. This chosen behavior assigns probabilities to the agent's current cell and its eight surrounding cells. For example, staying put would set the surrounding cells to 0 and the current cell to 1, whereas random walk sets all cells to $\\frac{1}{3}$. The agent subsequently picks one of the nine possible cells based on these probabilities and moves to it. This iterative process is then repeated. The weighting vector is constructed by evaluating all permutations (in steps of $\\frac{1}{10}$) and comparing the model data against the statistics from [6]. This was then further developed in [22] using a leave-one-out analysis to further improve the accuracy of the resultant behavior weighting vector compared to the real-world data from [6]. However, this vector has to be learned for every new location as results differed substantially between trials.\nThus, J2 (like J1) combines the combine the behavior-based approach from [22] and a modification of the network-based approach from [19] to meet its assumptions, to further use the physical information about the landscape. Unlike [22], however, behaviors are segmented by final find location rather than by movement types, which aims to prevent the per-location training."}, {"title": "III. METHODS", "content": "The method outlined in this section, named J2, has two parts. In Sect. III-A, the Monte Carlo path generation using the LP surrogate is introduced. The sampling of the paths to generate locations found is then developed in Sect. III-B.\nThe data relating to lost person behavior is primarily sourced from [1] with missing data being taken from [6]. Both sources consider multiple categories of LP, such as Child 10-12 or Climber. However, one of the largest datasets reported by both is Hiker (solo). All parameters used in this research are based on this category from here on in.\nAt its core, every simulation run is a behavior traversing the landscape from a starting point until the path length goes above $D_{max}$. For this stage, the agent does not tire. Whilst this is an unrealistic assumption by itself, the LP movement times are introduced during the sampling stage defined in Sect. III-B.\nThe LP is modeled as an agent navigating a 2D grid with square cells of shape 5m \u00d7 5m. Its viewpoint sits at a constant 1.6m above the surface, allowing it to see over smaller obstacles like rocks or long grass. At each time step, the agent moves to the next cell and the total distance increases accordingly until it reaches the termination distance $D_{max}$. The simulation will then terminate at the next distance check. The starting points are then sampled from the bivariate Gaussian probability distribution function $PDF_{start}(x)$ which is defined as\n$PDF_{start}(x) = \\frac{1}{\\sqrt{4\\pi^2 det \\sigma}} exp(-\\frac{1}{2}(x-\\mu)^T\\sigma^{-1}(x-\\mu))$ (1)\n$\\sigma = \\begin{bmatrix} \\sigma_{xx} & \\sigma_{xy} \\\\ \\sigma_{yx} & \\sigma_{yy} \\end{bmatrix}$ (2)\n$\\mu = \\begin{bmatrix} \\mu_x \\\\ \\mu_y \\end{bmatrix}$ (3)\ncentered around a given start location \u00b5. In a real SAR mission, this would be the PLS. The purpose of $PDF_{start}$ is to model the uncertainty, through \u03c3, of the reported PLS. SMR reports a PLS using the 6-digit Ordnance Survey National Grid reference, which reduces the British Isles into 100 \u00d7 100m cells. A PLS described by this system has a \u00b1100m error in x and y, with a total magnitude of the error being \u00b1141.42m. This would result in a variance of $\\sigma_{xx} = \\sigma_{yy} = 10,000$. To validate this model, Sect. IV outlines how \u03bc is generated based on real-world data.\nTo simulate the behavior of an LP, four behaviors were created. Each behavior corresponds to a land cover category that the agent will attempt to travel to. Throughout the simulation, the agent keeps the same behavior. The intent of this is to emulate an LP over a large number of data points. building, trees and water were given individual behaviors, but linear feature and travel aid were merged under Head2Paths. This was done because linear feature is defined as being a stream/ditch or wall/fenced line, and these features often follow paths and roads. As well as this, a stream is already being handled by the water behavior (as further outlined below). Finally, Hyp. 1 (with later empirical evaluation) was employed to generalize the open-ground find data.\nHypothesis 1: The open-ground land cover category location found data naturally results from an LP trying to navigate to the other possible locations.\nThe GIS maps utilized in this study are presented in table II. It is crucial to clarify that, although land cover ID and land cover category may seem similar, they differ. The former is exclusively employed within the viewshed-based behaviors, as outlined in Sect. III-A1, and is sourced from [24]. On the other hand, the latter influences the meta-behavior of the LP and originates from [6] and [1].\nHead2Water: The methodology for the Head2Water behavior was left unchanged from [9] with the usage of a vector field following model. shows that three of the six maps (cumulative catchment area, water surface type, and water outflow direction) contain hydrological information, with the water outflow direction map being used as a direct input to the vector field follower. To increase the fidelity of the model, the cumulative catchment area and the water surface type can further be used to force the agent to navigate around large enough bodies of water. This was done by evaluating the position of the agent in the next time step, and if the water surface type at the future position was either lake, sea, or river then a scaled probability is calculated of carrying out this step. This is done through\n$p(a) = 1 - \\begin{cases} 1, & a \\geq b \\\\ \\frac{v}{b} & else \\end{cases}$ (4)\nwhere v \u2208 [0,\u221e) is the cumulative catchment area value, which describes the total number of cells that drain into that cell, in the next time step. The upper bounds of the input a is b = 8000 for this research and was manually tuned. Thus p(a) is the scaled percentage chance for the agent to step into the body of water. If the check failed, then the agent would walk around the obstacle by turning left or right with equal probability. In a real-world scenario, this equates to a hiker stepping over a small stream or encountering a lake and walking along its edges.\nUsing the land cover ID and digital elevation maps, the agent could see the landscape and act accordingly. Using a viewshed algorithm means that expensive ray-casting can be mitigated."}, {"title": "IV. DESIGN OF EXPERIMENT", "content": "The data supplied by [1], [6], and [2] combines generalized data from hundreds of LP cases to draw broad conclusions about the various LP profiles. Therefore, to draw similar conclusions from the methods outlined in Sect. III, an adequate distinct number of PLS locations must be used. SMR provided a sample dataset for the Isle of Arran of historical rescue data, with \u2248 300 PLS locations. This position data is in the form of a 6-digit Ordnance Survey National Grid reference, which reduces the British Isles into 100 x 100m cells. A PLS described by this system has a \u00b1100m error in x and y, with a total magnitude of the error being \u00b1141.42m. This means a heatmap generated from this data has a cell size of 100m, as seen in Fig. 4. This would result in a large cluster of sampled points around (0.6,0.7), canceling out the attempts at increasing the number of distinct locations.\nTo up-sample the resolution of the heatmap seen, fitting a continuous function to represent the probability is a suitable solution. Using other methods, such as bicubic or Lanczos interpolation[31], results in the same disconnected modes as the original heatmap. To address the issues with these methods we used supervised machine learning in the form of a Gaussian Process (GP) model. The results of this method are discussed in Sect. V-A. This approach is appropriate for noisy and uncertain systems[32]. A GP learns a prior over functions, which can be sampled after observing data.\nFor the GP to work efficiently, the data must be formatted correctly. The original heatmap $\\bar{z}$ is normalized through\n$z^+ = \\frac{\\bar{z} - z_{min}}{z_{max} - z_{min}}$ (13)\nto give $z^+ \u2208 [0,1]$. The x and y values are normalized in the same manner. The normalized 2D heatmap is then unraveled in the row-major form such that every unique (x, y) pair maps to a single $z^+$ value in a 1D array.\nThe GP is then based primarily on the Matern kernel [32] which computes a covariance matrix between inputs a and b:\n$k_{Matern}(a, b) = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} (\\frac{\\sqrt{2\\nu} d}{\\Theta})^{\\nu} K_{\\nu} (\\frac{\\sqrt{2\\nu} d}{\\Theta})$ (14)\n$d = (a - b)^T \\Sigma^{-2} (a - b)$ (15)\nwhere d is the distance between a and b, scaled by the length scale parameter \u0398, \u03bd is a smoothness parameter[33], and $K_{\\nu}$ is the modified Bessel function[34]. $k$ can be simplified at half-integer steps of \u03bd (\u03bd = p+ $\\frac{1}{2}$ \u2200 p\u2208N) with the most commonly used values being v = 1.5 and v = 2.5. This is due to v = 0.5 generally giving noisy outputs, and \u03bd \u2265 3.5 being hard to distinguish between[32]. As such, a value of v = 2.5[35] was select which simplifies the modified Bessel function to\n$\u039a_{\\nu=2.5} = exp( -\\frac{\\sqrt{5}r}{\\Theta}) \\big(1 + \\frac{\\sqrt{5}r}{\\Theta} + \\frac{5r^2}{3\u0398^2}\\big) exp( -\\frac{\\sqrt{5}r}{\\Theta})$\nThe Matern kernel is then scaled using a scale kernel such that\n$K_{scaled} = \u0398_{scale} K_{orig}$ (16)\nwhere, $\u0398_{scale}$ is the output scale. As the dataset is image-based, and has a large amount of data points as a result, the KISS-GP [36] approximation for a given kernel is then applied. Given a base kernel K, the covariance $k(a, b)$ is approximated by using a grid of regularly spaced inducing points:\n$k(a, b) = W_a^T K_{uu} W_b$ (17)\nwhere U is the set of gridded inducing points, $K_{uu}$ is the kernel matrix between the inducing points, $W_a$ and $W_b$ are sparse vectors based on a and b that apply cubic interpolation. This covariance is then combined with a zero mean, in a multivariate normal distribution, and quantified by an exact marginal log-likelihood. This allows the measurement of the probability of generating the observed sample from a prior[33]. Finally, this is optimized using the Adam optimization algorithm[37]."}, {"title": "V. RESULTS AND DISCUSSION", "content": "A key element in deciding which location to analyze was the availability of the historical PLS data outlined in Sect. IV. The availability of that data, coupled with the availability of a high-quality LiDAR digital elevation map[23], meant that the Isle of Arran, Scotland, was therefore chosen.\nThe purpose of the process outlined in Sect. IV was to estimate the raw data provided by SMR, and sample PLS locations from this model to further drive the experiments such that generalized statistics could be drawn from it.\nThe start locations were then sampled from the trained GP model, like in , and then further sampled from Eqn. 1 with $\\sigma_{xx} = \\sigma_{yy} = 10,000$ to model the uncertainty in the reported PLS. In total 270 starting points were evaluated resulting in 50,000 generated paths. The final number of locations found was 41,000,000 after the sampling stage. This is a sufficiently large number of data points for statistical analysis since [6] and [1] reported n = 3,800 and n = 130 respectively.\nFig. 8 shows the paths created by various behaviors. As expected, there are substantially fewer paths for Head2Water than there are for Head2Paths. From Fig. 8b, it can be seen that due to the lack of buildings in the north of the Isle of Arran, the paths there converge on a couple of points whereas they tend to converge on different locations in the south. Furthermore, this behavior shows the effect of the sampled points from Sect. IV.\nAs the simulation termination distance was 10,000m, as described in Sect. III-B, it is no surprise that over 60% of paths terminated at this criterion. However, as seen in Fig. 9b, it is evident that the two viewshed behaviors (Head2Buildings and Head2Trees) are terminating early. Sect. III-A1 explains this behavior, and future work should explore this further.\nFig. 10 gives the first glimpse at the generated PDM. However, these are the aggregated results for multiple PLS locations and as such would look different for an individual PDM. Nonetheless, it reflects the sampled PLS by having more locations found in the north of the island with a slanted band of low probability. Insight like this would influence the decision-making during a search mission by not looking in that area for example.\nThe shortened paths from early termination of behaviors result in samples not being taken at distances longer than the path in question. Fig. 11 shows the times at which a point was sampled is skewed heavily to the left, with the original distribution more spread out along the time axis. The sampled points have a mean time of 0.75h and a standard deviation of 0.66, conversely, the original distribution has a mean of 1.06h with a standard deviation of 1.01.\nThis difference in means shows that the upper limit of time traveled correlates to the total time an LP might be missing, with the distribution becoming closer to matching the historical data as $D_{max} \\rightarrow \\infty$. Therefore, $D_{max}$ can be further used to inform the generated PDM for the scenario at hand.\nThe resultant sampled locations of the agents are the most important metric to gauge the effectiveness of this model. This will be compared to the original data[1], and J1, a previous version of the algorithm.\nAs can be seen from Fig. 12, an LP was found in the open ground land cover category 38.36% of the time. This is only a 2.44% difference to the original data (compared to a 27.22% difference for J1), empirically proving Hyp. 1. Therefore, LPs end up naturally in the open-ground land cover category without having a dedicated behavior.\nThe next land cover category is road, at which 38.95% of LPs were found. Similarly to open ground, this is only a difference of 6.65% to the original data whilst J1 has a 30.20% difference. Such a large improvement in the road land cover category is a result of an improved Head2Paths behavior algorithm from Sect. III-A1. However, the viewshed-based algorithms, Head2Trees, and Head2Buildings have improved but are still substantially off the mark compared to the source data. This result may occur by the way that the sampling is handled. Fig. 12 shows that even though 46.20% of paths are due to the Head2Buildings behavior, only 33.07% of sampled points were a result of the aforementioned paths. This is due to Head2Buildings having a low mean length of 5.62km compared to the largest of 11.04km as seen in Fig. 9b.\nThe last land cover category, water, remains close to the source data, as was the result for J1, showing that the strategy of following the water vector map remains valid.\nOverall, when using Eqn. 11, the algorithm proposed in this paper had a score of 61.56 whilst J1 has a score of 306.02. This is a substantial improvement when compared to a randomly generated distribution with a score of 159.16 (with n = 1 x 107). This shows a large improvement toward matching the source data, as is the target."}, {"title": "VI. CONCLUSION", "content": "This study explored the psychological profile-based PDM generation algorithm J2 which emulates the movement of an LP over a landscape. Through GP-driven analysis, it is clear that the sparse data problem experienced in SAR can be overcome leading to the tangible result of saving lives in future operations.\nBy characterizing the profile into four distinct behaviors (Head2Water, Head2Trees, Head2Paths, and Head2Buildings) the model can be adjusted to match the land cover category location found description from local datasets. These distinct, simpler, behaviors are their own model and traverse the landscape as if they were an LP with a single goal in mind. The data shows that running each behavior a percentage amount of times would produce a distribution of land cover category location found descriptions that match the original dataset. However, there were discrepancies with the tree and building categories. As both of these rely on viewshed-based behaviors, it follows that this is the root cause of the discrepancy as further explained in Sect. V-C. The introduction of the GP-based design of experiment also benefited the analysis by generalizing the PLS locations. This is something that was not done in J1 which further decreased the effectiveness of the analysis with a $\\nu$ = 2.\nOverall, the method of using the land cover category location found statistics directly from the source data in J2 is valid with a symmetric Kullback-Leibler (Eqn. 11) score of 61.56 whilst a true random distribution has a score of 159.16. This means that per-location training is not required and that a generalized model has been created. However, further effort needs to be put into improving the viewshed-based algorithms.\nFuture work will further explore extending the capabilities of the various behaviors. Following this, more GIS data will be incorporated to further solidify the agent's roots in the real world. Replacing behaviors with machine learning models is also something that will need to be explored. Furthermore, the usage of PDMs to create UAV search trajectories will be explored[7]."}]}