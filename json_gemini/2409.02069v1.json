{"title": "A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial", "authors": ["Anna L. Trella", "Kelly W. Zhang", "Hinal Jajal", "Inbal Nahum-Shani", "Vivek Shetty", "Finale Doshi-Velez", "Susan A. Murphy"], "abstract": "Dental disease is a prevalent chronic condition associated with substantial financial burden, personal suffering, and increased risk of systemic diseases. Despite widespread recommendations for twice-daily tooth brushing, adherence to recommended oral self-care behaviors remains sub-optimal due to factors such as forgetfulness and disengagement. To address this, we developed Oralytics, a mHealth intervention system designed to complement clinician-delivered preventative care for marginalized individuals at risk for dental disease. Oralytics incorporates an online reinforcement learning algorithm to determine optimal times to deliver intervention prompts that encourage oral self-care behaviors. We have deployed Oralytics in a registered clinical trial. The deployment required careful design to manage challenges specific to the clinical trials setting in the U.S. In this paper, we (1) highlight key design decisions of the RL algorithm that address these challenges and (2) conduct a re-sampling analysis to evaluate algorithm design decisions. A second phase (randomized control trial) of Oralytics is planned to start in spring 2025.", "sections": [{"title": "1 Introduction", "content": "Dental disease is a prevalent chronic condition in the United States with significant preventable morbidity and economic impact (Benjamin 2010). Beyond its associated pain and substantial treatment costs, dental disease is linked to systemic health complications such as diabetes, cardiovascular disease, respiratory illness, stroke, and adverse birth outcomes. To prevent dental disease, the American Dental Association recommends systematic, twice-a-day tooth brushing for two minutes (American Dental Association 2024). However, patient adherence to this simple regimen is often compromised by factors such as forgetfulness and lack of motivation (Chadwick, White, and Lader 2011; Yaacob et al. 2014).\nmHealth interventions and tools can be leveraged to prompt individuals to engage in high-quality oral self-care behaviors (OSCB) between clinic visits. This work focuses on Oralytics, a mHealth intervention designed to improve OSCB for individuals at risk for dental disease. The intervention involves (i) a Bluetooth-enabled toothbrush to collect sensor data on an individual's brushing quality, and (ii) a smartphone application (app) to deliver treatments, one of which is engagement prompts to encourage individuals to remain engaged in improving their OSCB. Oralytics includes multiple intervention components one of which is an online reinforcement learning (RL) algorithm which is used to learn, online, a policy specifying when it is most useful to deliver engagement prompts. The algorithm should avoid excessive burden and habituation by only sending prompts at times they are likely to be effective. Before integrating a mHealth intervention into broader healthcare programs, the effectiveness of the intervention is deployed and tested in a clinical trial. However, the clinical trial setting introduces unique challenges for the design and deployment of online RL algorithms as part of the intervention."}, {"title": "1.1 Design & Deployment Challenges in Clinical Trials", "content": "First, clinical trials, conducted with US National Institutes of Health (NIH) funding, must adhere to the NIH policy on"}, {"title": "1.2 Contributions", "content": "In this paper, we discuss how we addressed these deployment challenges in the design of an online RL algorithm a generalization of a Thompson-sampling contextual bandit (Section 3.3) as part of the Oralytics intervention to improve OSCB for individuals at risk for dental disease. The RL algorithm (1) learns online from incoming data and (2) makes decisions for individuals in real time as part of the intervention. Recently, the Oralytics intervention was deployed in a registered clinical trial (Shetty 2022). Key contributions of our paper are:\n1. We highlight key design decisions made for the Oralytics algorithm that deals with deploying an online RL algorithm as part of an intervention in a clinical trial (Section 4).\n2. We conduct a re-sampling analysis\u00b9 using data collected during the trial to (1) re-evaluate design decisions made and (2) investigate algorithm behavior (Section 5).\nFurther details about the clinical trial and algorithm design decisions can be found in Nahum-Shani et al. (2024); Trella et al. (2024a)."}, {"title": "2 Related Work", "content": "AI in Clinical Trials A large body of work exists that incorporates AI algorithms to conduct clinical trials. AI can improve trial execution by automating cohort selection (Glicksberg et al. 2018) and participant eligibility screening (Alexander et al. 2020; Haddad et al. 2021). Prediction algorithms can be used to assist in maintaining retention by identifying participants who are at high risk of dropping out of the trial (Pedersen et al. 2019; Teixeira et al. 2022).\nOnline RL Algorithms in mHealth Many online RL algorithms have been included in mHealth interventions deployed in a clinical trial. For example, online RL was used to optimize the delivery of prompts to encourage physical activity (Yom-Tov et al. 2017; Liao et al. 2019; Figueroa et al. 2021), manage weight loss (Forman et al. 2023), improve medical adherence (Lauffenburger et al. 2024), assist with pain management (Piette et al. 2022), reduce cannabis use amongst emerging adults (Ghosh et al. 2024a), and help people quit smoking (Albers, Neerincx, and Brinkman 2022). There are also deployments of online RL in mHealth settings that are not formally registered clinical trials (Zhou et al. 2018; Kumar et al. 2024). Many of these papers focus on algorithm design before deployment. Some authors (Kumar et al. 2024), compare outcomes between groups of individuals where each group is assigned a different algorithm or policy. Here we use a different analysis to inform further design decisions. Our analysis focuses on learning across time by a single online RL algorithm."}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Oralytics Clinical Trial", "content": "The Oralytics clinical trial (Table 1) enrolled participants recruited from UCLA dental clinics in Los Angeles. Participants were recruited incrementally at about 5 participants every 2 weeks. All participants received an electric toothbrush with WiFi and Bluetooth connectivity and integrated sensors. Additionally, they were instructed to download the Oralytics app on their smartphones. The RL algorithm dynamically decided whether to deliver an engagement prompt for each participant twice daily, with delivery within an hour preceding self-reported morning and evening brushing"}, {"title": "3.2 Online Reinforcement Learning", "content": "Here we consider a setting involving sequential decision-making for N participants, each with T decision times. Let subscript $i \\in [1 : N]$ denote the participant and subscript $t \\in [1 : T]$ denote the decision time. $S_{i,t}$ denotes the current state of the participant. At each decision time t, the algorithm selects action $A_{i,t}$ after observing $S_{i,t}$, based on its policy $\\pi_{\\theta}(s)$ which is a function, parameterized by $\\theta$, that takes in input state s. After executing action $A_{i,t}$, the algorithm receives a reward $R_{i,t}$. In contrast to batch RL, where policy parameters are learned using previous batch data and fixed for all $t \\in [1 : T]$, online RL learns the policy parameters with incoming data. At each update time T, the algorithm updates parameters $\\theta$ using the entire history of state, action, and reward tuples observed thus far H. The goal of the algorithm is to maximize the average reward across all participants and decision times, $E\\left[\\frac{1}{NT}\\sum_{i=1}^{N} \\sum_{t=1}^{T} R_{i,t}\\right]$."}, {"title": "3.3 Oralytics RL Algorithm", "content": "The Oralytics RL algorithm is a generalization of a Thompson-Sampling contextual bandit algorithm (Russo et al. 2018). The algorithm makes decisions at each of the T = 140 total decision times (2 every day over 70 days) on each participant. The algorithm state includes current context information about the participant collected via the toothbrush and app (e.g., participant OSCB over the past week and prior day app engagement). The RL algorithm makes decisions regarding whether or not to deliver an engagement prompt to each participant twice daily, one hour before a participant's self-reported usual morning and evening brushing times. Thus the action space is binary, with $A_{i,t} = 1$ denoting delivery of the prompt and $A_{i,t} = 0$, otherwise.\nThe reward, $R_{i,t}$, is constructed based on the proximal health outcome OSCB, $Q_{i,t}$, and a tuned approximation to the effects of actions on future states and rewards. This reward design allows a contextual bandit algorithm to approximate an RL algorithm that models the environment as a Markov decision process. See Trella et al. (2023) for more details on the reward designed for Oralytics.\nAs part of the policy, contextual bandit algorithms use a model of the mean reward given state s and action a, parameterized by $\\theta$: $r_{\\theta}(s, a)$. We refer to this as the reward model. While one could learn and use a reward model per participant i, in Oralytics, we ran a full-pooling algorithm (Section 4.3) that learns and uses a single reward model shared between all participants in the trial instead. In Oralytics, the reward model $r_{\\theta}(s, a)$ is a linear regression model as in Liao et al. (2019) (See Appendix A.2). The Thompson-Sampling algorithm is Bayesian and thus the algorithm has a prior distribution $\\theta \\sim N(\\mu_{prior}, \\Sigma_{prior})$ assigned to parameter $\\theta$. See Appendix A.3 for the prior designed for Oralytics.\nThe RL algorithm updates the posterior distribution for parameter $\\theta$ once a week on Sunday morning using all participants' data observed up to that time; denote these weekly update times by $\\Tau$. Let $n_{\\tau}$ be the number of participants that have started the trial before update time $\\tau$, and $t(i, \\tau)$ be a function that takes in participant i and current update time and outputs the last decision time for that participant. Then to update posterior parameters $\\mu_{\\tau}^{post}, \\Sigma_{\\tau}^{post}$, we use the history $H_{\\tau} := \\{(S_{i,t'}, A_{i,t'}, R_{i,t'})\\}_{i=1}^{n_{\\tau}, t'=1}^{t(i, \\tau)}$. Thus the RL algorithm is a full-pooling algorithm that pools observed data, $H_{\\tau}$ from all participants to update posterior parameters $\\mu_{\\tau}^{post}, \\Sigma_{\\tau}^{post}$ of $\\theta$. Notice that due to incremental recruitment of trial participants, at a particular update time $\\tau$, not every participant will be on the same decision time index t and the history will not necessarily involve all N participants' data.\nTo select actions, the RL algorithm uses the latest reward model to model the advantage, or the difference in expected rewards, of action 1 over action 0 for a given state s. Since the reward model for Oralytics is linear, the model of the advantage is also linear:\n$r_{\\theta}(s, a = 1) - r_{\\theta}(s, a = 0) = f(s)^T\\beta$ (1)\nf(s) denotes the features used in the algorithm's model for the advantage, and $\\beta$ is the subset of parameters of $\\theta$ corresponding to the advantage. For convenience, let $\\tau = \\tau(i, t)$ be the last update time corresponding to the current reward model used for participant i at decision time t. The RL algorithm micro-randomizes actions using $P(f(s)^T\\beta > 0 | s = S_{i,t}, H_{\\tau})$ and therefore forms action-selection probability $\\pi_{i,t}$:\n$\\pi_{i,t} := E_{\\beta \\sim N(\\mu_{\\beta}^{post}, \\Sigma_{\\beta}^{post})} [p(f(s)^T\\beta) | s = S_{i,t}, H_{\\tau}]$ (2)\nwhere $\\mu_{\\beta}$ and $\\Sigma_{\\beta}$ are the sub-vector and sub-matrix of $\\mu_{\\tau}^{post}$ and $\\Sigma_{\\tau}^{post}$ corresponding to advantage parameter $\\beta$. Notice that while classical posterior sampling uses an indicator function for p, the Oralytics RL algorithm instead uses a generalized logistic function for p to ensure that policies formed by the algorithm concentrate and enhance the replicability of the algorithm (Zhang et al. 2024).\nFinally, the RL algorithm samples $A_{i,t}$ from a Bernoulli distribution with success probability $\\pi_{i,\\tau}$:\n$A_{i,t} | \\pi_{i,\\tau} \\sim Bern(\\pi_{i,\\tau})$ (3)"}, {"title": "4 Deploying Oralytics", "content": ""}, {"title": "4.1 Oralytics Pipeline", "content": "Software Components Multiple software components form the Oralytics software service. These components are (1) the main controller, (2) the Oralytics app, and (3) the RL service. The main controller is the central coordinator of the Oralytics software system that handles the logic for (a) enrolling participants, (b) pulling and formatting sensor"}, {"title": "4.2 Design Decisions To Enhance Autonomy and Thus Replicability", "content": "A primary challenge in our setting is the high standard for replicability and as a result the algorithm, and its components, should be autonomous. However, unintended engineering or networking issues could arise during the trial. These issues could cause the intended RL system to function incorrectly compromising: (1) participant experience and (2) the quality of data for post-trial analyses.\nOne way Oralytics dealt with this constraint is by implementing fallback methods. Fallback methods are pre-specified backup procedures, for action selection or updating, which are executed when an issue occurs. Fallback methods are part of a larger automated monitoring system (Trella et al. 2024b) that detects and addresses issues impacting or caused by the RL algorithm in real-time. Oralytics employed the following fallback methods:\n(i) if any issues arose with a participant not obtaining the most recent schedule of actions, then the action for the current decision time will default to the action for that time from the last schedule pushed to the participant's app.\n(ii) if any issues arose with constructing the schedule of actions, then the RL service forms a schedule of actions where each action is selected with probability 0.5 (i.e., does not use the policy nor state to select action).\n(iii) for updating, if issues arise (e.g., data is malformed or unavailable), then the algorithm stores the data point, but does not add that data point to the batch data used to update parameters."}, {"title": "4.3 Design Decisions Dealing with Limited Decision Times Per Individual", "content": "Each participant is in the Oralytics trial for a total of 140 decision times, which results in a small amount of data collected per participant. Nonetheless, the RL algorithm needs to learn and select quality actions based on data from a limited number of decision times per participant. A design decision to deal with limited data is full-pooling. Pooling refers to clustering participants and pooling all data within a cluster to update the cluster's shared policy parameters. Full pooling refers to pooling all N participants' data together to learn a single shared policy. Although participants are likely to be heterogeneous (reward functions are likely different), we chose a full-pooling algorithm like in Yom-Tov et al. (2017); Figueroa et al. (2021); Piette et al. (2022) to trade off bias and variance in the high-noise environment of Oralytics. These pooling algorithms can reduce noise and speed up learning.\nWe finalized the full-pooling decision after conducting experiments comparing no pooling (i.e., one policy per participant that only uses that participant's data to update) and full pooling. We expected the no-pooling algorithm to learn a more personalized policy for each participant later in the trial if there were enough decision times, but the algorithm is unlikely to perform well when there is little data for that participant. Full pooling may learn well for a participant's earlier decision times because it can take advantage of other participants' data, but may not personalize as well as a no-pooling algorithm for later decision times, especially if participants are heterogeneous. In extensive experiments, using simulation environments based on data from prior studies, we found that full-pooling algorithms achieved higher average OSCB than no-pooling algorithms across all variants of the simulation environment (See Table 5 in Trella et al. (2024a))."}, {"title": "5 Application Payoff", "content": "We conduct simulation and re-sampling analyses using data collected during the trial to evaluate design decisions made for our deployed algorithm. We focus on the following questions:\n1. Was it worth it to invest in fallback methods? (Section 5.2)\n2. Was it worth it to run a full-pooling algorithm? (Section 5.3)\n3. Despite all these challenges, did the algorithm learn? (Section 5.4)"}, {"title": "5.1 Simulation Environment", "content": "One way to answer questions 2 and 3 is through a simulation environment built using data collected during the Oralytics trial. The purpose of the simulation environment is to re-simulate the trial by generating participant states and outcomes close to the distribution of the data observed in the real trial. This way, we can (1) consider counterfactual decisions (to answer Q2) and (2) have a mechanism for resampling to assess if evidence of learning by the RL algorithm is due to random chance and thus spurious (to answer Q3).\nFor each of the N = 72 participants with viable data from the trial, we fit a model which is used to simulate OSCB outcomes, $Q_{i,t}$ given current state $S_{i,t}$ and an action $A_{i,t}$. We also modeled participant app opening behavior and simulated participants starting the trial using the exact date the participant was recruited in the real trial."}, {"title": "5.2 Was it worth it to invest in fallback methods?", "content": "During the Oralytics trial, various engineering or networking issues occurred that impacted the RL service's intended functionality. These issues were automatically caught"}, {"title": "5.3 Was it worth it to pool?", "content": "Due to the small number of decision points (T = 140) per participant, the RL algorithm was a full-pooling algorithm (i.e., used a single reward model for all participants and updated using all participants' data). Even though be-"}, {"title": "5.4 Did We Learn?", "content": "Lastly, we consider if the algorithm was able to learn despite the challenges of the clinical trial setting. We define learning as the RL algorithm successfully learning the advantage of action a = 1 over a = 0 (i.e., sending an engagement prompt over not sending one) in a particular state s. Recall that the Oralytics RL algorithm maintains a model of this advantage (Equation 1) to select actions via posterior sampling and updates the posterior distribution of the advantage model parameters throughout the trial. One way to determine learning is to visualize the standardized predicted advantage in state s throughout the trial (i.e., using learned posterior parameters at different update times $\\tau$). The standardized predicted advantage in state s using the policy updated at time $\\tau$ is:\n$predicted\\_adv(\\tau, s) := \\frac{\\mu_{\\beta}^{post} f(s)}{\\sqrt{f(s)^T \\Sigma_{\\beta}^{post} f(s)}}$ (4)\n$\\mu_{\\beta}^{post}$ and $\\Sigma_{\\beta}^{post}$ are the posterior parameters of advantage parameter $\\beta$ from Equation 1, and f(s) denotes the features used in the algorithm's model of the advantage.\nFor example, consider Figure 4. Using posterior parameters $\\mu_{\\beta}^{post}, \\Sigma_{\\beta}^{post}$ learned during the Oralytics trial, we plot the standardized predicted advantage over updates times $\\Tau$ in a state where it is (1) morning, (2) the participant's exponential average OSCB in the past week is about 28 seconds (poor brushing), (3) the participant received prompts 45% of the times in the past week, and (4) the participant did not"}, {"title": "6 Discussion", "content": "We have deployed Oralytics, an online RL algorithm optimizing prompts to improve oral self-care behaviors. As illustrated here, much is learned from the end-to-end development, deployment, and data analysis phases. We share these insights by highlighting design decisions for the algorithm and software service and conducting a simulation and resampling analysis to re-evaluate these design decisions using data collected during the trial. Most interestingly, the resampling analysis provides evidence that the RL algorithm learned the advantage of one action over the other in certain states. We hope these key lessons can be shared with other research teams interested in real-world design and deployment of online RL algorithms. From a health science perspective, pre-specified, primary analyses will occur, which is out of scope for this paper. The re-sampling analyses presented in this paper will inform design decisions for phase 2. The re-design of the RL algorithm for phase 2 of the Oralytics clinical trial is currently under development and phase 2 is anticipated to start in spring 2025."}, {"title": "A Additional Oralytics RL Algorithm Facts", "content": ""}, {"title": "A.1 Algorithm State Space", "content": "$S_{i,t} \\in R^d$ represents the ith participant's state at decision point t, where d is the number of variables describing the participant's state.\nLet $f (S_{i,t}) \\in R^5$ denote the features used in the algorithm's model for both the baseline reward function and the advantage.\nThese features are:\nrespectively, where $\\gamma = 13/14$ and $c_1 = \\frac{1-\\gamma}{\\gamma} = \\frac{1}{13} \\approx .0769$. Recall that $Q_{i,t}$ is the proximal outcome of OSCB and $A_{i,t}$ is the treatment indicator. Feature 4 is 1 if the participant has opened the app in focus (i.e., not in the background) the prior day and 0 otherwise."}, {"title": "A.2 Reward Model", "content": "The reward model (i.e., model of the mean reward given state s and action a) used in the Oralytics trial is a Bayesian linear regression model with action centering (Liao et al. 2019):\n$r(s,a) = f(s)^T \\alpha_0 + n \\cdot f(s)^T \\alpha_1 + (a - \\pi) f(s)^T \\beta + \\epsilon$ (5)\nwhere $\\theta = [\\alpha_0, \\alpha_1, \\beta]$ are model parameters, $\\pi$ is the probability that the RL algorithm selects action a = 1 in state s and $\\epsilon \\sim N(0, \\sigma^2)$. We call the term $f(S_{i,t})^T \\beta$ the advantage (i.e., advantage of selecting action 1 over action 0) and $f(S_{i,t})^T \\alpha_0 + n_{i,t} f(S_{i,t})^T \\alpha_1$ the baseline."}, {"title": "B Simulation Environment", "content": "We created a simulation environment using the Oralytics trial data in order to replicate the trial under different true environments. Although the trial ran with 79 participants, due to an engineering issue, data for 7 out of the 79 participants was incorrectly saved and thus their data is unviable. Therefore, the simulation environment is built off of data from the 72 unaffected participants. Replications of the trial are useful to (1) re-evaluate design decisions that were made and (2) have a mechanism for resampling to assess if evidence of learning by the RL algorithm is due to random chance. For each of the 72 participants with viable data from the Oralytics clinical trial, we use that participant's data to create a participant-environment model. We then re-simulate the Oralytics trial by generating participant states, the RL algorithm selecting actions for these 72 participants given their states, the participant-environment model generating health outcomes / rewards in response, and the RL"}, {"title": "B.1 Participant-Environment Model", "content": "In this section, we describe how we constructed the participant-environment models for each of the N = 72 participants in the Oralytics trial using that participant's data. Each participant-environment model has the following components:\nEnvironment State Features The features used in the state space for each environment are a superset of the algorithm state features f(Si,t) (Appendix A.1). g(Si,t) \\in R7 denotes the super-set of features used in the environment model.\nOutcome Generating Function The outcome generating function is a function that generates OSCB Qi,t in seconds given current state Si,t and action Ai,t. We use a zero-inflated Poisson to model each participant's outcome generating process because of the zero-inflated nature of OSCB found in previous data sets and data collected in the Oralytics trial. Each participant's outcome generating function is:\n$Z \\sim Bernoulli(1 - sigmoid(g(S_{i,t})^T W_{i,b}) - A_{i,t} \\cdot max[\\Delta_i g(S_{i,t}), 0]))$\n$S \\sim Poisson \\left( exp \\left( g(S_{i,t})^T W_{i,p} + A_{i,t} \\cdot max[\\Delta_i g(S_{i,t}), 0] \\right) \\right)$ (6)\n$Q_{i,t} = ZS$\nWeights $W_{i,b}, W_{i,p}, \\Delta_i$ for each participant's outcome generating function are fit that participant's state, action, and OSCB data from the Oralytics trial. We fit the function using MAP with priors $W_{i,b}, W_{i,p}, \\Delta_i \\sim N(0, I)$ as a form of regularization because we have sparse data for each participant. Finalized weight values were chosen by running random restarts and selecting the weights with the highest log posterior density.\nApp Engagement Behavior We simulate participant app engagement behavior using that participant's app opening data from the Oralytics trial. Recall that app engagement behavior is used in the state for both the environment and the algorithm. Using this app opening data, we calculate $p_{app}$, the proportion of days that the participant opened the app during the Oralytics trial."}, {"title": "B.2 Assessing the Quality of the Outcome Generating Functions", "content": "Our goal is to have the simulation environment replicate outcomes (i.e., OSCB) as close to the real Oralytics trial data as possible. To verify this, we compute various metrics (defined in the following section) comparing how close the outcome data generated by the simulation environment is to the data observed in the real trial. I{\u00b7} denotes the indicator function. Let Var({Xk}K1) represent the empirical variance of X1, ..., XK.\nRecall that N = 72 is the number of participants and T = 140 is the total number of decision times that the participant produces data for in the trial. We consider the following metrics and compare the metric on the real data with data generated by the simulation environment.\n1. Proportion of Decision Times with OCSB = 0:\n$\\frac{\\sum_{i=1}^{N} \\sum_{t=1}^{T} I\\{Q_{i,t} = 0\\}}{N \\times T}$ (7)\n2. Average of Average Non-zero Participant OSCB:\n$\\frac{1}{N} \\sum_{i=1}^{N} Q_{i}^{non-zero}$ (8)\nwhere\n$Q_{i}^{non-zero} = \\frac{\\sum_{t=1}^{T} Q_{i,t} \\cdot I\\{Q_{i,t} > 0\\}}{\\sum_{t=1}^{T} I\\{Q_{i,t} > 0\\}}$\n3. Average Non-zero OSCB in Trial:\n$\\frac{\\sum_{i=1}^{N} \\sum_{t=1}^{T} Q_{i,t} \\cdot I\\{Q_{i,t} > 0\\}}{\\sum_{i=1}^{N} \\sum_{t=1}^{T} I\\{Q_{i,t} > 0\\}}$ (9)\n4. Variance of Average Non-zero Participant OSCB:\n$Var(\\frac{1}{N}Q_{i}^{non-zero})$ (10)\nwhere\n$Q_{i}^{non-zero} = \\frac{\\sum_{t=1}^{T} Q_{i,t} \\cdot I\\{Q_{i,t} > 0\\}}{\\sum_{t=1}^{T} I\\{Q_{i,t} > 0\\}}$\n5. Variance of Non-zero OSCB in Trial:\n$Var(Q_{i,t}: Q_{i,t} > 0)_{i=1,t=1}^{N,T}$ (11)\n6. Variance of Average Participant OSCB:\n$Var(\\frac{1}{T}Q_{i,t})_{i=1}^{N}$ (12)\nwhere $Q_i = \\sum_{t=1}^{T} Q_{i,t}$ is the average OSCB for participant i\n7. Average of Variances of Participant OSCB:\n$\\frac{1}{N}\\sum_{i=1}^{N} Var(Q_{i,t})_{t=1}^{T}$ (13)\nWe also compute the following error metrics. We use $\\hat{Q}_{i,t}$ to denote the simulated OSCB and $Q_{i,t}$ to denote the corresponding OSCB value from the Oralytics trial data.\n1. Mean Squared Error:\n$\\frac{1}{N \\times T} \\sum_{i=1}^{N} \\sum_{t=1}^{T} (\\hat{Q}_{i,t} - Q_{i,t})^2$ (14)"}, {"title": "2. Root Mean Squared Error:", "content": "$\\sqrt{\\frac{1}{N \\times T} \\sum_{i=1}^{N} \\sum_{t=1}^{T} (\\hat{Q}_{i,t} - Q_{i,t})^2}$ (15)"}, {"title": "3. Mean Absolute Error:", "content": "$\\frac{1}{N \\times T} \\sum_{i=1}^{N} \\sum_{t=1}^{T} |\\hat{Q}_{i,t} - Q_{i,t}|$ (16)"}, {"title": "B.3 Environment Variants for Re-sampling Method", "content": "In this section, we discuss how we formed variants of the simulation environment used in the re-sampling method from Section 5.4. We create a variant for every state s of interest corresponding to algorithm advantage features f(s) and environment advantage features g(s). In each variant, outcomes (i.e., OSCB Qi,t) and therefore rewards, are generated so that there is no advantage of action 1 over action 0 in the particular state s.\nTo do this, recall that we fit an outcome generating function (Equation 6) for each of the N = 72 participants in the trial. Each participant i's outcome generating function has advantage weight vectors $\\Delta_{i,B}, \\Delta_{i,N}$ that interact with the environment advantage state features g(s). Instead of using $\\Delta_{i,B}, \\Delta_{i,N}$ fit using that participant's trial data, we instead use projections $proj \\Delta_{i,B}, proj \\Delta_{i,N}$ of $\\Delta_{i,B}, \\Delta_{i,N}$ that have two key properties:\n2. for other states s' \u2260 s, they generate treatment effect values $g(s')^T proj \\Delta_{i,B}, g(s')^T proj \\Delta_{i,N}$ close to the treatment effect values using the original advantage weight vectors $g(s')^T \\Delta_{i,B}, g(s')^T \\Delta_{i,N}$\nTo find $proj \\Delta_{i,B}, proj \\Delta_{i,N}$ that achieve both properties, we use the SciPy optimize API\u2074 to minimize the following constrained optimization problem:"}, {"content": "$\\min_{\\Delta} \\frac{1}{K} \\sum_{k=1}^{K} [g(s')_k^T proj \\Delta - g(s')_k^T \\Delta]^2$\nsubject to: $\\bar{g}(s)^T proj \\Delta = 0$\n$\\{g(s')_k\\}_{k=1}^{K}$ denotes a set of states we constructed that represents a grid of values that g(s') could take. $\\bar{g}(s)$ has the same state feature values as g(s) except the \u201cDay of Week\u201d and \u201cDays Since Participant Started the Trial (Normalized)\" features are replaced with fixed mean values 2/7 and 0. The objective function is to achieve property 2 and the constraint is to achieve property 1.\nWe ran the constrained optimization with $\\Delta = \\Delta_{i,B}$ and $\\Delta_{i,N}$ to get proj $\\Delta_{i,B}, proj \\Delta_{i,N}$, for all participants i. All participants in this variant of the simulation environment produce OSCB $Q_{i,t}$ given state $S_{i,t}$ and $A_{i,t}$ using Equation 6 with $\\Delta_{i,B}, \\Delta_{i,N}$ replaced by proj $\\Delta_{i,B}, proj \\Delta_{i,N}$."}]}