{"title": "MagicMan: Generative Novel View Synthesis of Humans with 3D-Aware Diffusion and Iterative Refinement", "authors": ["Xu He", "Xiaoyu Li", "Di Kang", "Jiangnan Ye", "Chaopeng Zhang", "Liyang Chen", "Xiangjun Gao", "Han Zhang", "Zhiyong Wu", "Haolin Zhuang"], "abstract": "Existing works in single-image human reconstruction suffer from weak generalizability due to insufficient training data or 3D inconsistencies for a lack of comprehensive multi-view knowledge. In this paper, we introduce MagicMan, a human-specific multi-view diffusion model designed to generate high-quality novel view images from a single reference image. As its core, we leverage a pre-trained 2D diffusion model as the generative prior for generalizability, with the parametric SMPL-X model as the 3D body prior to promote 3D awareness. To tackle the critical challenge of maintaining consistency while achieving dense multi-view generation for improved 3D human reconstruction, we first introduce hybrid multi-view attention to facilitate both efficient and thorough information interchange across different views. Additionally, we present a geometry-aware dual branch to perform concurrent generation in both RGB and normal domains, further enhancing consistency via geometry cues. Last but not least, to address ill-shaped issues arising from inaccurate SMPL-X estimation that conflicts with the reference image, we propose a novel iterative refinement strategy, which progressively optimizes SMPL-X accuracy while enhancing the quality and consistency of the generated multi-views. Extensive experimental results demonstrate that our method significantly outperforms existing approaches in both novel view synthesis and subsequent 3D human reconstruction tasks.", "sections": [{"title": "1 Introduction", "content": "3D digital human creation is an important technique in computer vision and graphics, with a wide range of applications in games, movies, virtual and augmented reality. Traditional methods usually utilize a dense camera array to capture synchronized posed multi-view images for human reconstruction. However, these methods typically require a tedious and time-consuming process which is not practical for general users. Therefore, creating 3D human models from a single reference image is a significant task to be investigated.\nTo this end, early works like PIFu (2019), PaMIR (2021), ICON (2022), and ECON (2023) have been introduced to train feed-forward networks on scanned 3D human datasets, capable of reconstructing 3D humans from a single image. Nevertheless, data for 3D human scans are extremely scarce with limited diversity, and thus these methods exhibit poor generalizability to challenging poses and outfits. Besides, the weak generative capability from insufficient data also leads to overly-smoothed geometry and blurred textures.\nThanks to the abundant priors in text-to-image diffusion models trained on large-scale data, another category of works leverage pre-trained diffusion models to optimize 3D representations through a Score Distillation Sampling (SDS) loss to create human models from a single image. Although these approaches yield impressive results in terms of generalization and detailed textures, the absence of 3D awareness during SDS frequently leads to 3D inconsistencies like multi-face Janus problem (2022). Moreover, text descriptions or CLIP embeddings of reference images, which are typically used to guide SDS, contain only global semantic information and thereby hinder the generation of novel views with fine-grained textures consistent with the reference image.\nIn the field of 3D object generation, as exemplified by Zero123 (2023a), efforts have been made to use image diffusion models trained on multi-view data to directly generate novel views from a single image, which are subsequently used to reconstruct 3D models, achieving promising results. \nFollowing this paradigm, we present a multi-view diffusion model to directly generate human novel views, encompassing both generalizability and multi-view knowledge for 3D consistency. There are three main challenges in this task: Primarily, ensuring consistency for human multi-view images presents greater complexity than for 3D objects, due to the intricate geometry and detailed textures of human models. Secondly, existing works enhance multi-view consistency either by extending 2D self-attention in image diffusion models to 3D attention across all views , or by integrating 3D representations Both approaches are memory-consuming and thus can only generate sparse consistent views, which are insufficient for reliable reconstruction of 3D human models where self-occlusion typically occurs. Finally, recent researches in 3D human reconstruction and generation demonstrate the importance of utilizing parametric human models, e.g., SMPL (2015) or SMPL-X (2019), as 3D body priors, which is also confirmed to promote multi-view consistency in our task. However, SMPL-X meshes estimated from monocular images often display inaccurate poses, featuring depth ambiguities and misalignment with the reference image. This pose-image mismatch typically leads to ill-shaped geometry, manifesting as abnormal overall poses or distorted body parts.\nTo address these challenges, we propose MagicMan, which efficiently produces high-quality, dense, and consistent multi-view images for humans from a single reference image. Our core insight lies in utilizing Stable Diffusion (SD) as the 2D prior for generation and SMPL-X mesh as the 3D body prior for better consistency. Therefore, we present a view-conditioned diffusion model with the reference image and SMPL-X guidance as the network backbone. To obtain dense and consistent novel views, we first introduce a hybrid multi-view attention mechanism to establish connections between different views. Specifically, it includes an efficient 1D attention across all views, and a 3D attention that spans both spatial and view dimensions operating on a sparse subset of selected views to enhance information interchange with minimal memory overhead. This hybrid mechanism facilitates thorough information fusion between views with reduced memory consumption, enabling us to generate significantly denser (i.e., 20 views at 512\u00d7512 resolution) novel views while maintaining consistency. Next, to deal with detailed geometry, we propose a geometry-aware dual branch with shared blocks to simultaneously generate aligned novel view RGB images and normal maps. The normal map prediction supplements structure information, further improving the consistency of geometric details. Last but not least, to address the ill-shaped issues arising from inaccurate SMPL-X estimates, we propose an iterative refinement approach,"}, {"title": "2 Related Work", "content": "Our work focuses on the generative novel view synthesis from a single human image using diffusion models, which is related to the research topics in diffusion models, generative view synthesis, and human image synthesis.\nDiffusion Models. Diffusion models have demonstrated promising results in recent image synthesis works. Classifier guidance is proposed to boost sample quality with a trained classifier for conditional generation, while Ho and Salimans performs classifier-free guidance by combining the score estimates from conditional and unconditional generation. Based on diffusion models and large-scale data, various image synthesis tasks have achieved impressive results such as text-to-image synthesis and super-resolution (2022b). In this paper, we explore using diffusion models for human novel view synthesis.\nGenerative Novel View Synthesis. Generative novel view synthesis requires synthesizing views far beyond the input. Compared with traditional regression-based methods , it only has sparse or a single view as input to hallucinate unseen parts. With the development of generative models, this task has been studied by utilizing convolutional networks, generative adversarial networks and transformer. Recently, diffusion models have also been applied to this task. Zero-1-to-3 (2023a) proposes a viewpoint-conditioned diffusion model trained on a large-scale dataset that shows a strong zero-shot generalizability. To incorporate 3D information into the 2D diffusion models, GeNVS (2023) and SyncDreamer (2023b) use a 3D feature as condition. Tseng et al. uses an epipolar attention for consistent view synthesis. Wonder3D (2024) extend diffusion models with multi-view and cross-domain attention and SV3D (2024) utilizes a video diffusion model to improve consistency. In this work, we use diffusion models combined with the parametric body model SMPL-X (2019) as priors to ensure consistent view synthesis of humans.\nHuman Image Synthesis. Human image synthesis aims to synthesize novel views or poses giving a source image as inference. This problem is explored by using generative adversarial networks to synthesize the novel pose results in a single forward pass while Liquid Warping GAN (2019) also demonstrates the human novel view synthesis results. However, GAN-based methods require transferring the style from the source to the target image in a single forward pass and are hard to capture complex structures (2023). On the other hand, diffusion models simplify the generation process into several denoising steps and therefore have been used for human image synthesis recently. (Bhunia et al. 2023) introduces the first diffusion-based approach for pose-guided person synthesis. In addition, DreamPose (2023), Animate Anyone (2024), MagicAnimate (2024), and Champ (2024) generates animated videos from a source image using pre-trained SD. These methods could synthesize human images in novel views by giving the corresponding poses. However, with-"}, {"title": "3 Method", "content": "Our proposed MagicMan, as illustrated in Fig. 3, takes a single reference human image as input and generates high-quality consistent dense (i.e., 20 views) multi-view images. To utilize human priors from abundant in-the-wild images, MagicMan adopts a pre-trained diffusion model (2022) as the backbone, and accepts one reference image along with SMPL-X pose and viewpoint as generation conditions (Sec. 3.1). We modify the original diffusion blocks with an efficient hybrid-attention mechanism connecting different views, which contains a 1D attention across all views and a 3D attention across spatial locations and sparse selected views (Sec. 3.2). To achieve better geometric consistency, we propose a geometry-aware dual branch complementary to novel view image synthesis (Sec.3.3). Last but not least, we propose a novel iterative refinement strategy by updating both the accuracy of SMPL-X poses and the quality of generated multi-views across iterations, reducing the ill-shaped issues resulting from inaccurate pose estimation (Sec. 3.4)."}, {"title": "3.1 Conditional Diffusion Model", "content": "Our backbone is a denoising UNet (2015) that inherits the structure and pre-trained weights from SD 1.5 (2022). The vanilla SD UNet consists of downsampling, middle, and upsampling blocks. Each block contains several interleaved convolution layers, self-attention layers that perform feature aggregation across spatial locations, and cross-attention layers that interact with CLIP text embeddings. In our work, the denoising UNet is required to take multiple noise latents as input and produce human images of generated views consistent with the reference. Therefore, we incorporate the reference image and viewpoint information into the network, and provide SMPL-X meshes as geometry guidance to promote 3D awareness and consistency.\nReference UNet. Inspired by recent advances in character animation (Hu 2024; Xu et al. 2024; Zhu et al. 2024), we utilize a copy of the denoising UNet to extract features from reference images, which we refer to as the reference UNet, ensuring consistency between the generated images and the reference image at both semantic and low levels. Different from previous works that integrate reference information into self-attention and retain CLIP embeddings for cross-attention, we find CLIP to be redundant and simply discard it. Instead, we replace cross-attention with reference attention, while leaving self-attention unaltered.\nLet $x \\in \\mathbb{R}^{B \\times N \\times H \\times W \\times C}$ and $y \\in \\mathbb{R}^{B \\times H \\times W \\times C}$ denote"}, {"title": "3.2 Hybrid Multi-View Attention", "content": "With the reference UNet, we can generate independent images consistent with the reference. Next, we want to establish connections across different views to produce consistent multi-views. To generate as many views as possible to capture comprehensive human information while maintaining consistency, the key problem lies in how to ensure thorough information exchange across a wide range of views in a memory-efficient manner. Therefore, we propose a novel hybrid attention mechanism that combines the strengths of two types of multi-view attention, i.e. efficiency of 1D attention and thoroughness of 3D attention in multi-view interaction.\n1D multi-view attention. First, we insert an additional 1D attention layer behind the reference attention to establish connections between different views. This module enhances the multi-view similarity in a highly memory-efficient manner, as it is calculated along view dimension only between identical pixel locations, allowing coherent generation of up to 20 views in a single forward pass. Specifically, the feature map is reshaped to $\\mathbb{R}^{(BHW) \\times N \\times C}$ to calculate self-attention along $N$, and we employ relative positional encoding (2022) instead of the commonly used sinusoidal encoding to account for relative viewpoint differences.\n3D multi-view attention. Relying solely on 1D attention leads to content drift issues (2023) between views after large viewpoint changes (Fig. 6) since 1D attention lacks interaction between pixels at different locations and cannot find the corresponding pixel from other views. Therefore, we further integrate 3D multi-view attention, facilitating more thorough information sharing across both spatial and view dimensions. Owing to the initial interactions established by 1D attention, 3D attention can be confined to a sparse subset of views without incurring excessive memory overhead.\nTo be specific, we extend the origin self-attention of denoising UNet to 3D attention. For a reshaped feature map $X_i \\in \\mathbb{R}^{B \\times (HW) \\times C}$ of each view, 3D attention is efficiently conducted with a small number of feature maps $X_{j_{1:M}} \\in$"}, {"title": "3.3 Geometry-Aware Dual Branch", "content": "Since the detailed geometry is difficult to capture within the RGB domain, we introduce the dual branch to perform geometry-aware denoising, which generates the spatially aligned normal maps along with RGB images. To be specific, We replicate just one input and output block of the UNet from the RGB branch to function as the normal branch, while the remaining blocks serve as shared components, as illustrated in Fig. 3(b). For the output of the normal branch, we use normal maps rendered from the ground-truth human scans as training supervision. For the input to the reference normal branch, considering the unavailability of the ground-truth during inference, we employ an off-the-shelf monocular normal estimator to estimate the reference normal map from the input RGB image, unifying the training and inference setting. With these designs, the shared blocks facilitate feature fusion across domains. The normal branch enriches RGB by incorporating geometry awareness, improving structural stability and geometric consistency, while RGB assists in generating more accurate and detailed normal maps. Notably, despite the input normal map inferred from the estimator typically being smooth and lacking details, our method could still produce the normal maps of output images with fine-grained details benefiting from the feature fusion across different views and domains."}, {"title": "3.4 Iterative Refinement", "content": "SMPL-X (2019) is a template human mesh $M (\\theta, \\beta, \\psi)$ parameterized by pose parameters $\\theta$, shape parameters $\\beta$, and expression parameters $\\psi$.\nThe accuracy of the SMPL-X pose matters a lot since we employ its rendered normal and segmentation images as geometry guidance to improve 3D consistency. However, monocular estimation could give inaccurate SMPL-X poses that conflict with the reference images, leading to the generation of distorted novel view images and thus ill-shaped 3D reconstruction. On the other hand, generating novel view images without flawed SMPL-X guidance usually keeps its pose well matching the reference image, but inferior in terms of 3D consistency as shown in Fig. 7(a). Therefore, we propose that multi-view human images generated without flawed SMPL-X guidance can be used to optimize the accuracy of SMPL-X poses, while the refined SMPL-X meshes can then guide the generation of human muti-views with improved 3D consistency.\nBased on these observations, we randomly drop SMPL-X guidance with a certain ratio during training, enabling guidance-free generation in line with classifier-free guidance (CFG) (2022). In the inference stage, we introduce an iterative refinement process, as detailed in Algorithm 1. Initially, we set the CFG scale to 0, effectively disabling SMPL-X guidance to preserve more accurate poses in the generated novel view images that match the reference image. These images are then used to update the SMPL-X parameters. In subsequent iterations, we gradually increase the CFG scale to enhance pose guidance of the refined SMPL-X estimation to further enhance 3D consistency.\nSpecifically, the iterative refinement process starts with estimating the initial SMPL-X parameters using PyMAF-X . In each iteration, we use the current SMPL-X mesh as guidance, applying the corresponding CFG scale to generate human images. It\u2019s important to note that in the early iterations, the scale is kept small, resulting in weaker guidance that allows the generated images to better match the reference poses. Next, we use a differentiable renderer to produce SMPL-X mesh\u2019s normal maps $N^{SMPL-X}$ and silhouettes $S^{SMPL-X}$, as well as project 3D joints into 2D keypoints $J^{SMPL-X}$ according to the camera poses. SMPL-X parameters are then optimized under the supervision of all the generated novel view images with the generated normal maps $\\hat{N}_{1:N}$, silhouettes $\\hat{S}_{1:N}$, and detected 2D joint keypoints $\\hat{J}_{1:N}$ from the generated novel views. The SMPL-X optimization is performed by minimizing the following loss:\n$\\mathcal{L}_{refine} = \\lambda_{normal} \\mathcal{L}_{normal} + \\lambda_{silhouette} \\mathcal{L}_{silhouette} + \\lambda_{joint} \\mathcal{L}_{joint}$,\n$\\mathcal{L}_{normal} = |N^{SMPL-X} - \\hat{N}_{1:N}|, \\mathcal{L}_{silhouette} = |S^{SMPL-X} - \\hat{S}_{1:N}|$,\n$\\mathcal{L}_{joint} = |J^{SMPL-X} - \\hat{J}_{1:N}|$.\nAfter the optimization, SMPL-X parameters are refined to be more accurate and aligned with the reference image, and will be fed back into the generation process with an increased CFG scale in the next iteration.\nIn summary, during each iterative process, the SMPL-X parameters undergo refinement supervised by all generated multi-view images, and the multi-view generation is enhanced with the improved SMPL-X as guidance."}, {"title": "4 Experiments", "content": "Training data. We train MagicMan on 2347 human scans from THuman2.1 dataset (2021b). RGB and normal images are rendered using a weak perspective camera on 20 fixed viewpoints looking at the scan with evenly distributed azimuths from 0\u00b0 to 360\u00b0, at 512\u00d7512 resolution.\nEvaluation data. We test on 95 scans from THuman2.1 dataset and 30 scans from CustomHumans dataset (2023) and also evaluate on in-the-wild images, comprising 100 from SHHQ dataset and 120 from the Internet featuring varied poses, outfits, and styles."}, {"title": "5 Conclusion", "content": "In this paper, we introduce MagicMan, a method for generating human novel views from a single reference image by leveraging an image diffusion model as the 2D generative"}]}