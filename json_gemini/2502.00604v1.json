{"title": "GRADIENT ALIGNMENT IN PHYSICS-INFORMED NEURAL\nNETWORKS: A SECOND-ORDER OPTIMIZATION PERSPECTIVE", "authors": ["Sifan Wang", "Ananyae Kumar Bhartari", "Bowen Li", "Paris Perdikaris"], "abstract": "Multi-task learning through composite loss functions is fundamental to modern deep learning, yet\noptimizing competing objectives remains challenging. We present new theoretical and practical\napproaches for addressing directional conflicts between loss terms, demonstrating their effectiveness\nin physics-informed neural networks (PINNs) where such conflicts are particularly challenging to\nresolve. Through theoretical analysis, we demonstrate how these conflicts limit first-order methods\nand show that second-order optimization naturally resolves them through implicit gradient align-\nment. We prove that SOAP, a recently proposed quasi-Newton method, efficiently approximates the\nHessian preconditioner, enabling breakthrough performance in PINNs: state-of-the-art results on\n10 challenging PDE benchmarks, including the first successful application to turbulent flows with\nReynolds numbers up to 10,000, with 2-10x accuracy improvements over existing methods. We also\nintroduce a novel gradient alignment score that generalizes cosine similarity to multiple gradients,\nproviding a practical tool for analyzing optimization dynamics. Our findings establish frameworks\nfor understanding and resolving gradient conflicts, with broad implications for optimization beyond\nscientific computing.", "sections": [{"title": "Introduction", "content": "Multi-task learning through composite loss functions has become a cornerstone of modern deep learning, from computer\nvision to scientific computing. However, when different loss terms compete for model capacity, they can generate\nconflicting gradients that impede optimization and degrade performance. While this fundamental challenge is known to\nthe multi-task learning literature [1-3], several challenges remain open, particularly in settings where objectives are\ntightly coupled through complex physical constraints.\nIn this work, we examine gradient conflicts through the lens of physics-informed neural networks (PINNs), where the\nchallenge manifests acutely due to the inherent coupling between physical constraints and data-fitting objectives. Our key\ninsight is that while first-order optimization methods struggle with competing objectives, appropriate preconditioning\ncan naturally align gradients to enable efficient optimization. While our findings on gradient alignment and second-order\npreconditioning have broad implications for multi-task learning, here we focus on PINNs as they provide an ideal\ntestbed: their physically-constrained objectives are mathematically precise, their solutions can be rigorously verified,\nand their performance bottlenecks are well-documented. Through theoretical analysis and extensive experiments on\nchallenging partial differential equations (PDEs), we demonstrate breakthrough results in problems ranging from basic\nwave propagation to turbulent flows.\nTo better motivate our approach, consider training a PINN to solve the Navier-Stokes equations. The model must\nsimultaneously satisfy boundary conditions, conservation laws, and empirical measurements \u2013 objectives that often\npush a neural network's parameters in opposing directions. Traditional methods like Adam or gradient descent struggle"}, {"title": "Problem Formulation", "content": "Overview. Multi-task learning in deep neural networks requires simultaneously minimizing multiple competing\nobjectives \u2013 a challenge that manifests acutely in physics-informed neural networks (PINNs). Building upon the work\nof [5], PINNs approximate solutions to partial differential equations by minimizing a composite loss function that\nenforces both physical constraints and data-fitting objectives. Consider a general PDE system:\n$u_t +N[u] = 0, t\\in [0,T], x\\in\\Omega,$\nwith inital and boundary conditions\n$u(0, x) = g(x), x\\in\\Omega,$\n$B[u] = 0, t \\in [0, 1], x \\in \\Theta\\Omega,$\nwhere $N[\\cdot]$ represents a differential operator and $B[\\cdot]$ denotes boundary conditions. The core idea of PINNs is to\napproximate the solution $u(t, x)$ using a neural network $u_\\theta(t, x)$. Through automatic differentiation [6], we can\ncompute the PDE residual:\n$R[u_\\theta](t, x) = \\frac{\\partial u_\\theta}{\\partial t} (t, x) +N[u_\\theta](t, x).$\nThis leads to a composite loss function that encapsulates multiple competing objectives:\n$\\mathcal{L}(\\theta) = \\frac{1}{N_{ic}} \\sum_{i=1}^{N_{ic}} |u_\\theta (0,x_{ic}) - g(x_{ic})|^2 + \\frac{1}{N_{bc}} \\sum_{i=1}^{N_{bc}} |B [u_\\theta] (x_{bc})|^2 + \\frac{1}{N_{r}} \\sum_{i=1}^{N_{r}} |R [u_\\theta] (x_{r})|^2,$\nwhere $N_{ic}$These loss functions aim to fit the initial, boundary conditions and the PDE residuals, respectively. And $\\{x_{ic}\\}_{i=1}^{N_{ic}}$,\n$\\{t_{bc}, x_{bc}\\}_{i=1}^{N_{bc}}$ and $\\{t_{r}, x_{r}\\}_{i=1}^{N_{r}}$ may be selected either as fixed mesh vertices or through random sampling during each\ntraining iteration.\nHere lies the fundamental challenge: these different loss terms frequently work against each other during optimization.\nConsider the Navier-Stokes equations \u2013 enforcing no-slip boundary conditions requires precise control of velocity\ngradients near walls, which can conflict with maintaining conservation of mass and momentum in the bulk flow.\nWhen such conflicts occur, first-order methods like gradient descent or Adam can only follow the average gradient\ndirection, leading to inefficient optimization trajectories that zigzag between competing objectives. The severity of\nthese conflicts increases with problem complexity, becoming particularly acute for turbulent flows where maintaining\nphysical constraints across multiple scales is crucial."}, {"title": "Methods", "content": "Gradient Alignment in PINNS. PINNs face a funda-\nmental challenge of competing gradients during training,\nwhich manifests in two distinct modes, see Figure 1. The\nfirst mode, identified by [7, 8], involves back-propagated\ngradients of significantly different magnitudes. When\nthese magnitude imbalances occur, certain loss terms\ndominate the optimization process, leading to model fail-\nure. While this challenge has been partially addressed\nthrough self-adaptive weighting schemes [9\u201313], a sec-\nond, more fundamental mode of gradient conflict remains\nless explored.\nThis second mode occurs when gradients from different\nloss terms point in opposing directions, forcing the op-\ntimization to follow inefficient trajectories. Traditional\nscaling-based approaches cannot resolve these directional\nconflicts, which become particularly severe in complex\nPDE systems where multiple physical constraints must\nbe simultaneously satisfied. To systematically study and\naddress this challenge, here we introduce a new metric\ncalled the alignment score, defined as follows.\nDefinition 1. Suppose that $v_1, v_2,..., v_n$ are vectors,\nthen the alignment score is defined as\n$\\mathcal{A}(v_1, v_2, ..., v_n) = \\frac{2}{\\binom{n}{2}} \\sum_{i=1}^{n} \\frac{v_i}{||v_i||} -1.$\nThis score ranges from [-1,1] and naturally extends the concept of cosine similarity to multiple vectors. As illustrated\nin Proposition 1, for the special case of n = 2, our score exactly recovers the standard cosine similarity $cos(v_1, v_2) = \\frac{v_1 \\cdot v_2}{||v_1||||v_2||}$, where 1 indicates perfect alignment, 0 suggests orthogonal directions, and -1 represents complete opposition.\nProposition 1. For n=2, the alignment score $A(v_1, v_2)$ equals the cosine similarity between $v_1$ and $v_2$:\n$\\mathcal{A}(v_1, v_2) = cos(v_1, v_2) = \\frac{v_1 \\cdot v_2}{||v_1||||v_2||}$\nThe proof is provided in Appendix E.1. The alignment score enables us to quantify both the local conflicts between\nindividual loss terms within each gradient descent step and the global conflicts across consecutive steps. Formally:\nDefinition 2. Let $L = \\sum_{i=1}^{n} L_i$ be a composite loss function. At the k-th step of gradient descent, let $g^k$ denote the full\ngradient and $g_1^k, g_2^k, ..., g_n^k$ denote the gradients of individual loss terms. We define:\n(a) The intra-step gradient alignment score:\n$A_{intra}^k = \\mathcal{A}(g_1^k, g_2^k, ..., g_n^k).$\n(b) The inter-step gradient alignment score:\n$A_{inter}^k = \\mathcal{A}(g^{k-1},g^k).$\nIn the following, we will demonstrate that gradient direction conflicts widely exist in training PINNs, especially in\nthe early stages of training. To this end, we conduct experiments on five representative PDEs spanning from linear"}, {"title": "Experiments", "content": "To rigorously evaluate SOAP's performance, we examine a diverse set of 10 representative and challenging PDEs that\ngovern fundamental physical phenomena. These equations span wave propagation, shock formulation, chaotic systems,\nreaction-diffusion processes, fluid dynamics, and heat transfer. The detailed description of the problem setup, including\nthe PDE parameters, initial and boundary conditions, numerical implementations, and supplementary visualizations, are\npresented in Appendix F.\nBaselines. We focus our comparisons on PINN approaches that have the potential to scale to training large neural\nnetworks, as these are promising for solving realistic large-scale physical problems in the future. That said, we do\nnot compare against PINNs trained with natural gradients [24, 25] and L-BFGS variants [26], since these methods\nare limited to small networks and full-batch gradient descent, making them impractical for the challenging PDEs we\nconsider.\nOur baselines follow the current state-of-the-art training pipeline established by [27]. Unless otherwise specified, we use\nPirateNet [28] as the backbone architecture, with three residual blocks, a hidden dimension of 256, and Tanh activation\nfunctions. All weight matrices are enhanced using random weight factorization (RWF) [29], with \u03bc = 1.0, \u03c3 = 0.1.\nExact periodic boundary conditions are strictly enforced when applicable [30].\nFor model training, we use mini-batch gradient descent with the Adam optimizer [31], which has become the de\nfacto standard for training PINNs due to its robust performance and computational efficiency. We randomly sample\n8,192 collocation points at each iteration. The learning rate schedule begins with a linear warm-up phase over the first\n5,000 steps, increasing from 0 to 0.001, followed by an exponential decay with a factor of 0.9. To improve training"}, {"title": "Conclusion and Discussion", "content": "This work advances our understanding of gradient conflicts in multi-task learning through the lens of physics-informed\nneural networks. Our theoretical analysis reveals fundamental challenges that arise when different loss terms push\nparameters in opposing directions \u2013 a situation also common across a broad spectrum of deep learning applications \u2013\nfrom computer vision to natural language processing. We show that appropriate preconditioning through second-order\ninformation naturally aligns these conflicting gradients, providing a general principle for multi-task optimization. Our\nimplementation through SOAP offers both theoretical guarantees and practical efficiency, leading to breakthrough\nresults including the first successful application of PINNs to high Reynolds number turbulent flows with 2-10x accuracy\nimprovements over existing methods.\nAn intriguing observation emerged from our investigation of complex PDE systems: while training losses continue\nto decrease, prediction accuracy against high-resolution numerical solutions tends to plateau. Our spectral analysis\nof turbulent flows provides compelling evidence that PINNs can capture fine-scale dynamics more accurately than\ntraditional numerical solvers, even at very high grid resolutions. This suggests that PINNs, by learning continuous\nrepresentations unconstrained by discretization, might be approaching more accurate solutions to the underlying\nphysical equations than previously possible. These findings could have broad implications beyond scientific computing\nto areas like continuous representation learning.\nBuilding on these insights, several promising research directions emerge. A rigorous theoretical framework characteriz-\ning how PINNs approximate solutions in the continuous domain could fundamentally change our understanding of\nneural networks in scientific computing. While SOAP demonstrates the power of gradient alignment in handling coupled\nphysical constraints, opportunities exist for more efficient preconditioned algorithms that maintain their effectiveness\nwith reduced computational cost. More broadly, our work suggests that the principles of gradient alignment and\nsecond-order preconditioning could benefit many deep learning applications involving competing objectives, though\nchallenges remain in scaling these approaches to larger systems. Success in these directions could transform both\nscientific computing and multi-task optimization."}, {"title": "Connection between SOAP and Newton's method", "content": "For our purpose, we begin by noting that there exists a one-to-one correspondence between the original parameter space\nand the rotated space that preserves the matrix-vector multiplication.\nLemma 2. Let $Q_L \\in \\mathbb{R}^{m\\times m}$ and $Q_R \\in \\mathbb{R}^{n\\times n}$ be two orthogonal matrices. For any matrix $A \\in \\mathbb{R}^{mn \\times mn}$ and vector\n$v \\in \\mathbb{R}^{mn}$, define $\\tilde{v} := (Q_L \\otimes Q_R)v$ and $\\tilde{A} := (Q_L \\otimes Q_R)A(Q_L^T \\otimes Q_R^T)$. Then there holds\n$A v = (Q_L \\otimes Q_R)\\tilde{A} \\tilde{v} = \\tilde{A} \\tilde{v}.$\nThe proof follows directly by applying the transformation $Q_L \\otimes Q_R$ to $Av$ and the definitions of $\\tilde{A}$ and $\\tilde{v}$. Building on\nthe above lemma, one can easily transform the preconditioned gradient descent in the original space to the rotated one\nand vice versa.\nCorollary 1. Let $W_t, G_t \\in \\mathbb{R}^{m \\times n}$ be the weight matrix and gradient matrix for a layer at iteration $t$, respectively, with\nvectorizations $w_t = vec(W_t)$ and $g_t = vec(G_t)$. The preconditioned gradient descent update:\n$W_{t+1} = W_t - \\eta H^{-1}g_t,$\nis equivalent to performing preconditioning in the rotated space:\n$\\tilde{W}_{t+1} = \\tilde{W}_t - \\eta \\tilde{H}^{-1}\\tilde{g}_t,$\nwhere $H \\in \\mathbb{R}^{mn \\times mn}$ is the preconditioner, and $\\tilde{w}$, $\\tilde{g}$, and $\\tilde{H}$ are the rotated weight, gradient, and preconditioner\ndefined by the transformations in Lemma 2.\nProposition 3. Let $L_t = \\mathbb{E} [G_t^T G_t]$ and $R_t = \\mathbb{E} [G_t G_t^T]$ have eigendecompositions $L_t = Q_L \\Lambda_L Q_L^T$ and $R_t =$\n$Q_R \\Lambda_R Q_R^T$. Under the assumption of Lemma 5, the equivalent preconditioner in the rotated space is diagonal, i.e.,\n$\\tilde{H}_{GN} = diag(\\tilde{H}_{GN})$.\nProof. The proof follows from the combination of Lemma 2 and Lemma 5. First, we express $H_{GN}$ using the Kronecker\napproximation:\n$H_{GN} = L_t^{1/2} \\otimes R_t^{1/2}/ \\text{Tr} (\\mathbb{E} [G_t G_t^T]) .$\nThen, we derive the rotated preconditioner:\n$\\tilde{H}_{GN} = (Q_L \\otimes Q_R) H_{GN} (Q_L \\otimes Q_R)^T$\n$= (Q_L \\otimes Q_R) (L_t^{1/2} \\otimes R_t^{1/2}) (Q_L^T \\otimes Q_R^T) / \\text{Tr} (\\mathbb{E} [G_t G_t^T])$\n$= (Q_L L_t^{1/2} Q_L^T) \\otimes (Q_R R_t^{1/2} Q_R^T) / \\text{Tr} (\\mathbb{E} [\\Lambda_L])$\n$= \\Lambda_L^{1/2} \\otimes \\Lambda_R^{1/2} / \\text{Tr} (\\mathbb{E} [\\Lambda_L]) .$\nThe final expression shows that $H_{GN}$ is diagonal, as it is the Kronecker product of diagonal matrices scaled by a scalar\nfactor.\nFinally, we connect our analysis to Adam's update rule by adapting the following result from Molybog et al [75]:\nProposition 4 (Adapt from [75]). Suppose that $\\theta^*$ is a local minimum and assume that $\\theta - \\theta^* \\sim \\mathcal{N}(0, \\sigma^2 I)$. For Adam\nupdate rule denoted by $\\theta_{t+1} = \\theta_t - \\eta \\text{Adam}(g_t)$, we have\n$\\text{Adam}(g_t) \\approx \\text{diag} (H)^{-1} g_t.$\nProof. The Adam optimizer follows the update rule:\n$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t,$\n$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t \\odot g_t,$\n$\\hat{m}_t = m_t/(1 - {\\beta_1}),$\n$\\hat{v}_t = v_t/(1 - {\\beta_2}),$\n$\\theta_t = \\theta_{t-1} - \\eta \\hat{m}_t/(\\sqrt{\\hat{v}_t} + \\epsilon).$"}, {"title": "Analysis of Intra-step Gradient Alignment", "content": "We present some preliminary analysis to understand intra-step gradient conflicts in training PINNs via standard gradient\ndescent, Adam [31], and Shampoo algorithms [18], and how SOAP can effectively resolve them during training. For\nsimplicity, we consider the simplest case of using PINNs with the two-layer NN to solve the one-dimensional Laplace\nequation and focus on the analysis of the intra-step gradient alignment (3.3) with small initialization. The analysis can\nbe easily extended to other types of PDEs. Following the general setup in Section 2, without loss of generality, we\nconsider 1D Laplace equation as follows\n$\\begin{cases}\n\\Delta u = u'' = 0 \\text{ on } [-1,1],\\\\\nu(\\pm 1) = g_{\\pm 1}.\n\\end{cases} $\nWe approximate the solution u(x) by a two-layer network with width N:\n$u(x, \\theta) = \\sum_{i=1}^{N} a_i \\sigma(w_i x) = a \\cdot \\sigma(wx),$\nwhere a = $(a_1,...,a_N), w = (w_1,...,w_N) \\in \\mathbb{R}^{N}$, and $\\theta = (a, w) \\in \\mathbb{R}^{2N}$. Moreover, we limit ourselves to the\nactivation function $\\sigma(x) = tanh(x)$. In this case, the loss (2.5) reduces to\n$\\min_{\\theta=(a,w)} \\mathcal{L}(\\theta) = \\frac{1}{N_r} \\sum_{p=1}^{N_r} |u_{xx} (x_p, \\theta)|^2+\\sum_{s=\\pm 1} |u(s, \\theta) -g_s|^2.$\nTo analyze the gradient conflict phenomenon in training PINNs, we consider the small initialization regime."}, {"title": "Additional Lemmas and Proofs", "content": "Lemma 4 ( [19], Lemma 1). Let $G_1, ..., G_t \\in \\mathbb{R}^{m \\times n}$ be matrices of rank at most r. Let $g_s = vec (G_s)$ and define\n$H_t = I_{mn} + \\sum_{s=1}^t g_s g_s^T$. Define Lt, Rt as above: $L_t = \\mathbb{E}_{Im} + \\sum_{s=1}^t G_s G_s^T, R_t = \\mathbb{E}_{In} + \\sum_{s=1}^t G_s G_s$. Then for\nany p, q > 0 such that 1/p + 1/q = 1, we have $H_t \\le r L_t^{1/p} \\otimes R_t^{1/q}$.\nIt follows from the above lemma that for any p, q > 0 with 1/p + 1/q = 1, the full AdaGrad preconditioned gradient\n$H_t^{-1/2} g_t$ can be approximated by $(L_t^{1/p} \\otimes R_t^{1/q})^{-1/2} g_t = vec(L_t^{1/2p} G_t R_t^{1/2q})$. In particular, the case of $p = q = 2$\nyields the standard Shampoo update. Moreover, [20] explores the Hessian approximation perspective of Shampoo,\nshowing that the preconditioner in Shampoo is a Kronecker product approximation of the Gauss-Newton component of\nlayerwise Hessian (see Remark 1). Similar arguments will be used below to understand the second-order nature of\nSOAP.\nLemma 5 ([20], Corollary 2). Under the assumption that the reshaping of the Hessian tensor $H_{GN}$ is rank-1,\n$H_{GN} = (\\mathbb{E} [GG^T] \\otimes \\mathbb{E} [GG^T]) / \\text{Tr} (\\mathbb{E} [GG^T]) .$\nE.2 Proof of Lemma 1\nProof. By Taylor expansion, the gradient at step t + 1 can be expressed as:\n$g_{t+1} = g_t + H(\\theta_t)(\\theta_{t+1} - \\theta_t) + O(||\\theta_{t+1} - \\theta_t||^2)$\n$= g_t - \\eta H(\\theta_t) H^{-1}(\\theta_t) g_t + O(\\eta^2 ||g_t||^2)$\n$= (1 - \\eta)g_t + O(\\eta^2 ||g_t||^2).$"}]}