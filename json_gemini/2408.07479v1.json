{"title": "A Study on Bias Detection and Classification in Natural Language Processing", "authors": ["Ana Sofia Evans", "Helena Moniz", "Luisa Coheur"], "abstract": "Human biases have been shown to influence the performance of models and algorithms in various fields, including Natural Language Processing. While the study of this phenomenon is garnering focus in recent years, the available resources are still relatively scarce, often focusing on different forms or manifestations of biases. The aim of our work is twofold: 1) gather publicly-available datasets and determine how to better combine them to effectively train models in the task of hate speech detection and classification; 2) analyse the main issues with these datasets, such as scarcity, skewed resources, and reliance on non-persistent data. We discuss these issues in tandem with the development of our experiments, in which we show that the combinations of different datasets greatly impact the models' performance.", "sections": [{"title": "1 Introduction", "content": "There is a growing awareness of the extent to which human biases can influence our models and algorithms. This realization has led to a fast growth in fields dedicated to studying bias, such as the study of bias in Natural Language Processing (NLP), which has focused not only on bias mitigation but also on its detection and classification. However, bias detection is a relatively new field of study, lacking many publicly available benchmark datasets or state-of-the-art models that are able to complete this task. Existing datasets are relatively small, often do not focus on the same types of bias, and are not even aimed at the same downstream tasks. So, a question arises: can we learn how to detect and classify bias using these (publicly available) resources? And, if so, how?\nIn order to answer this question, we have outlined the following objectives:\nFind and collect publicly available datasets aimed at bias classification to serve as training data;\nTrain and analyse the performance of several classifiers, trained with different parameters and training data combinations;"}, {"title": "2 Related Work", "content": "In this section, we present an overview of work done in the fields of bias and hate speech detection, followed by a critical analysis on the limitations of the current state-of-the-art. Lastly, we examine datasets developed in the scope of bias and hate speech detection."}, {"title": "2.1 Overview", "content": "When it comes to the study of bias in NLP, [4] is an almost obligatory mention, having conducted one of the earliest studies we could find on the topic, focusing on Gender Bias in Word Embeddings. While more studies on Bias in Word Embeddings have been released since this initial study [5, 6, 7, 8, 9], we have also seen researchers further widening the scope of Bias in NLP, pouring over models or tools frequently used in various NLP tasks and study them under the lens of bias - sometimes as tools for detection and mitigation, other times as sources or propagators of bias. There is work focused on Neural Networks [10], on state-of-the-art models such as BERT [11, 12], techniques such as Adversarial Learning [13, 14], and various NLP tasks, such as Coreference Resolution [15], Sentiment Analysis [16], Dialogue Generation [17], and even POS tagging and Dependency Parsing [18].\nAnother way in which models developed in the scope of NLP can perpetuate bias is through their training data. A significant number of datasets are composed of non-curated content from the Web, due to the sheer amount of information that can easily be collected from online forums and platforms. While there are advantages to this approach (like the aforementioned ease in collecting large amounts of data, or the usage of casual, every day language instead of synthetic syntax), the fact remains that there is plenty of unsafe and offensive content on the Internet, which is uncritically collected to build these datasets."}, {"title": "2.2 Critiques and Limitations", "content": "While Bias Detection and Hate Speech detection are not the same field, they intersect substantially and share common pitfalls. For those reasons, the commentary of this section refers to both fields interchangeably.\nThe first issue in the current state-of-the-art is the lack of established taxonomies or centralized resources, whether in terms of terminology or benchmark datasets. While plenty of works use terms such as \"Bias\", \"Hate Speech\", or \"Abusive language\", the definitions associated with these terms are rarely in agreement. The absence of concise and concrete criteria leads to a \"sparsity of heterogeneous resources\" [36]. Countering this is the argument that there is no such thing as a set of pre-established criteria that could be applied, since there are no objectively correct definitions to be constructed, and we should instead strive for more clarity in the terminology used, as well as in the subtasks being studied [37].\nThe second limitation refers to the disproportionate focus given to certain target categories in these fields. We can find many examples of work done regarding sexism or gender bias, and, to a lesser extent, racism or racial bias. However, we will be hard-pressed to find significant data regarding ableism, transphobia, anti-Semitism, and many, many other categories worthy of a similar focus [33, 2, 37]. Additionally, works with gender as a target category often fail to conduct their research under an intersectional lens, thus reducing the nuance and depth of the phenomenon they propose to research [2].\nFurthermore, regarding uneven distribution of resources, there is the sheer amount of resources devoted to the English language in comparison to any other language. While this is, to a degree, understandable, due to how widely used English is in international contexts such as online spaces, it is not sustainable. The choice to center English-speaking internet users in this research, implicit or unintentional as it may be, creates its own form of data bias [2, 37]. While some works done in other languages do exist, these are few and far in between [38, 39].\nLastly, we would like to speak about dataset annotation.\nThe first issue we would like to expand upon is bias induced by dataset annotation. As humans, we are all prone to inherent biases. This is why datasets will usually be annotated by more than one person, and why measures such as inter-annotator agreement exist. In theory, these measures should allow labels to be chosen with as little bias as possible, especially if researchers resort to a diverse pool of annotators.\nHowever, we can still find instances of annotation bias. In [40], the authors find that entries of Hate Speech datasets which are written in AAE (African American"}, {"title": "2.3 Datasets", "content": "In this section, we present some of the publicly available datasets related to bias and hate speech detection. As mentioned in the previous section, not only are there few standard benchmark datasets available, but the datasets that do exist often do not follow specific, pre-existing taxonomies or definitions, and often focus on different manifestations of bias. As such, we chose to group our findings in accordance with the denominations we defined in Section 3.1.2, namely: Binary Classification, Single Target Classification, and Multi-Target Classification.\nBinary Classification As previoulsy said, we define \"Binary Classification\" as classification which focuses on identifying a certain phenomenon (whether that is bias, hate speech, abusive or toxic language, etc) without specifying a target category, like gender or race. Therefore, the datasets in this subsection focus only on the presence of a given phenomenon, and not on identifying if it refers to a particular group or not.\nDavidson [26] is a crowdsourced dataset with around 24,000 tweets intended for Hate Speech detection. This dataset is publicly available. In this dataset, entries"}, {"title": "3 Data Gathering", "content": "As previously mentioned, our objective was to gather and combine pre-existing resources, namely datasets developed in the scope of Bias and/or Hate Speech Detection, and evaluate if these could be used to successfully train a model in Bias Detection and Classification. After conducting our initial research, we settled on using the datasets depicted in Table 4."}, {"title": "3.1 Tweet Retrieval", "content": "Some of these datasets, namely Benevolent-Hostile Sexism [30] and Waseem-Hovy [35], are Twitter-based datasets which, due to privacy concerns, did not"}, {"title": "Interlude: Dataset Degradation, or the Problem of Non-Persistent Data", "content": "There is a notable problem with the strategy of using Tweet IDs to resolve privacy concerns; namely, the fact that we can only retrieve a tweet if that tweet still exists. Unavailable tweets cannot be recovered.\nIn order to better investigate this issue, we turned to the Founta dataset [1]. The creators of this dataset responded to privacy concerns by separating tweet identifiers and tweet text into separate files and then sharing both files, rather than withholding the text altogether. Ergo, while we had no need to retrieve tweets of this dataset, since the relevant information was freely provided, we still possess the identifiers and are free to use them.\nThe results of our analysis regarding unavailable tweets, across all three datasets, can be found in 5. The table contains the total number of tweets in the dataset, the number of available tweets, and the number of unavailable tweets, as well as why they were unavailable. Since Benevolent-Hostile Sexism separated the Benevolent and Hostile components into two files and their yielded results differed significantly, we chose to showcase them separately."}, {"title": "Consequences", "content": "This dataset degradation influences the usefulness of our resources, most notably the Waseem-Hovy dataset and, in particular, the entries annotated for racism. While the original dataset boasted 1,970 entries with the aforementioned label, this amount was reduced to a grand total of 12 entries. Regarding the unavailable entries, 38 entries related to deleted tweets, while 1,920 referred to suspended users.\nThe Benevolent Sexism portion of the Benevolent-Hostile Sexism dataset, however, yielded another problem entirely. Out of the original 7,210 tweets in total, only 2,411 remained after processing. While this may seem incredibly problematic, our main issue is actually related to the available entries. After briefly perusing the results, we realized that there seemed to be an unusual number of repeated textual content. We concluded that, out of these 2,411 available entries, only 631 were unique tweets. The remaining 1,780 entries consisted of retweets of the same original tweet, which resulted in different tweet IDs for what basically amounted to plenty of repeated content.\nBoth of these results had an immediate effect on our plans moving forward.\nFirstly, having been reduced to a mere 631 entries, we decided to remove the Benevolent Sexism portion from our dataset collection, being left with the Hostile Sexism portion. Secondly, while we had previously considered Waseem-Hovy as a multi-target classification dataset - as a dataset which annotated entries for both the \"gender\" and \"race\" categories \u2013 the fact that only 12 entries remained for \"racism\" meant that this was no longer viable. Thus, we removed these entries, instead integrating the dataset into our collection as a single-target classification dataset with the target category \"gender\"."}, {"title": "3.2 Label Mapping", "content": "After retrieving the missing Twitter data, we proceeded to uniformise our dataset collections. We replaced Twitter-specific markers, such as usernames or hashtags, by specific text markers which would later be saved as special tokens; we selected only the relevant content from each dataset and saved it to identically structured CSV files; and, finally, we established label coherency through label mapping.\nThe first mapping dimension we tackled was Binary Classification, i.e. simply identifying whether an entry was biased or non-biased in accordance to our proposed definition. The label correspondences are detailed in Table 7."}, {"title": "4 Model Training", "content": "For this work, we used the Emotion-Transformer, developed in the scope of Emotion Detection but adaptable to our Bias Classification task. The Emotion-Transformer is built on top of a pretrained Transformer model. In this work, we chose the DistilBERT pretrained model from HuggingFace, which served as a necessary compromise between temporal efficiency and overall performance.\nTo establish the Emotion-Transformer's level of performance, we trained it with individual datasets of our collection and compared the obtained results against results reported in the publication of those same datasets. Any comparison of results for Benevolent-Hostile Sexism and Waseem-Hovy would be invalid, due to the alterations these datasets suffered, described in the previous section. Additionally, DynGen was evaluated in a multi-labeling task, which would make our evaluation of it as a single-labeling task irrelevant."}, {"title": "4.1 Experimental Setup", "content": "Out of the remaining datasets, only Davidson and MLMA reported performance results. Davidson originally reported an F1-score of 0.9, using a Support Vector Machine with L2 regularization [26]. MLMA does not specify what type of methods were used in training and testing, but reports an F1-score 0.43 as its best result for the relevant classification task [45].\nWe obtained an F1-score of 0.8 for Davidson, training the Emotion-Transformer during 5 epochs, with Binary Cross-Entropy with Logits Loss and max pooling function; and an F1-score of 0,42 for MLMA, training the Emotion-Transformer during 4 epochs, with the same Loss and Pooling functions described for the previous experiment. While the F1-score obtained for Davidson is lower than originally reported, the values are still similar. Thus, we conclude that the Emotion-Transformer is able to perform at a similar level to those models used to test the original datasets.\nWe divided our datasets into four non-exclusive groups, named Group A, Group B, Group C, and Group D. Group A, as the smallest and most coherent of the groups, serves as our baseline for performance comparison. Groups B, C, and D each answer a research question, described in Table 9.\nWe performed a non-deterministic split of each group's data, splitting it into training, testing, and validation sets (80% train and 10% for testing and validation each). In total, we conducted over 100 experiments, in which we trained the model with different parameters and training data combinations.\nThe tested parameters were: Number of Training Epochs, Loss Function, and Pooling Function. The remaining parameters remained unchanged throughout experiments, such as Seed Value (12), Patience (1), Gradient Accumulation Steps (1), Batch Size (8), Number of Frozen Epochs (1), Encoder Learning Rate (1.0e-5), Classification Head Learning Rate (5.0e-5), and Layerwise Decay (0.95). These were the default values set for the Emotion Transformer."}, {"title": "4.2 Interlude: Class Imbalance, Undersampling, and Data Augmentation", "content": "As we mentioned in Section 2.2, one of the most blatant limitations of this field of study, at the moment, is the way certain target categories (most notably, \"Gender\" and \"Race\") receive a lot more attention and, as such, a lot more dedicated resources than any other category. This skewed distribution has had an obvious"}, {"title": "4.3 Results", "content": "As previously mentioned, Group A is our baseline. It is also the only group that can only be used to train models for the Binary Classification task. Groups B, C, and D can be used in both Binary Classification and in Multi-Target Classification.\nWe conducted three types of tests. The first was in Multi-Target Classification, using Groups B, C, and D, in which both the training and testing data were from the same group. The second type was in Binary Classification, using all groups, in which both the training and testing data were also from the same group. The third type was also in Binary Classification \u2013 but we used a Model trained with data from Group A to classify test data from Groups B, C, and D.\nThe best F1-scores obtained in the first testing round, on Multi-Target Classification, are depicted in Table 12. We refer to these experiments as \"Multi-B\", \"Multi-C\", and \"Multi-D\". Additionally, we will refer to the Binary Cross-Entropy with Logits Loss Function as simply \"BCE\".\nFurther examination of these results, particularly of Multi-C and Multi-D, show that the lower F1-scores result from lower values for precision and recall across classes. The \"Age\" class, in particular, yields an F1-score of 0 across all tests. This is unsurprising due to the extremely low number of entries for this category,"}, {"title": "5 Discussion", "content": ""}, {"title": "5.1 \u201cHow do Single-Target datasets influence performance?\u201d Or: Group-A vs Multi-B, Binary-B, and Inter-B", "content": "This is the question that led us to create Group B as a distinct control group, with its sole Target Category. Furthermore, since all the individual datasets in this group are Twitter-based, we also remove other variables from this experiment, such as the linguistic variation of Internet and synthetic data.\nAs can be seen in Tables 12 and 14, the difference in overall performance between Group A and Multi-B is slight. From this, we can conclude that the model is able to correctly predict when a sentence is biased, and also when that bias is aimed at target category gender.\nObserving the Binary-B results, shown in Table 14, we can see a 0.01 decrease in F1-score in the biased category when compared to Group A's results. While the model's ability to differentiate between biased and non-biased content is maintained, we can presume that the entries from the Single-Target datasets differ enough from the unspecified biased entries to result in a slight, decrease in performance. This addition does not seem to impact the non-biased category in any significant way.\nLastly, we can compare the Inter-B results with Group-A and Binary-B. Inter-B's F1-score of 0.8780 compared to A-El's 0.8974 shows us that the model solely trained on Group A data, while clearly able to identify some of the gender-biased entries and perform adequately, does not perform as well as the baseline. Most importantly, it also does not perform as well as a model trained with Group-B data, as evidenced by Multi-B's F1-score of 0.8909.\nWe can conclude that adding entries labeled for a specific target category to a general Bias/Hate Speech dataset results in a model which can accurately identify and classify biased content revolving around that very same target category, with little to no decrease in overall performance. These results are, therefore, highly promising."}, {"title": "5.2 \u201cHow do synthetic and Multi-Target datasets influence performance?\u201d Or: A Lukewarm Overview of Group C", "content": "This is the question that motivated the existence of Group C as a control group, by adding to our baseline those datasets that were Multi-Target and/or synthetic. This was an almost by default choice, since most of our Multi-Target datasets were also synthetic.\nAs depicted in Tables 12 and 14, the difference in performance between Group A and Multi-C is significant, even with the increase observe by removing the \"Age\" category, as depicted in Table 13. This result is caused by the lower scores obtained in the several target categories."}, {"title": "5.3 \u201cCan we obtain a better performance by using all of our resources together?\u201d Or: The Epic of Group D", "content": "Lastly, we arrived at our last control group, which is composed by the unification of all our resources. We are, therefore, analysing how well (or how badly) the general, Twitter-based Bias/Hate Speech Detection datasets, Single-Target datasets, and synthetic and/or Multi-Target datasets perform together.\nWe would like to remind that Group D is the only one to include the CONAN dataset, introduced in Section 2.3, which is a synthetic, Single-Target dataset for the target category \"Religion\". This dataset did not fit neatly into the previous control groups, but we decided to nevertheless include it in this Group; \"all of our resources\", after all, means all of our resources.\nAs can be seen in Tables 12 and 14, we once more find a significant difference in performance between Group-A and Multi-D, partially bridged by NoAge-D, in Table 13. Group A's overall F1-score consistently hits the 0.89 range, while Multi-D's rests in the 0.61 range and NoAge-D falls, on average, in the 0.67 range. Multi-D sees a decrease in performance for both the b_none and non-biased categories, even when compared to Multi-C.\nFigure 3 shows the comparison between F1-scores obtained across all categories for both Multi-C and Multi-D. As previously mentioned, there is a slight decrease in performance for classes b_none and non-biased, which is interesting not due to the severity of the decrease \u2013 which, as mentioned, is slight - but due to the fact that it happens at all.\nThere is, however, a severe decrease in performance worthy of note in gen-der_identity. We believe this might either be due to the split between train, validation, and test sets seeing as this class makes up a mere 0.56% of Group D, and, as such, is easily affected by the random data split or due to some type of"}, {"title": "6 Conclusion", "content": "Bias in NLP is a recent field of study, with plenty of works being published in recent years. We are discovering that there are many ways in which human biases can, and do, infiltrate our programs and algorithms. One of these ways is through biased training data, which teaches models how to replicate those very same biases.\nIn our work, we sought to use publicly available resources to train a classifier in the task of Bias Detection and Classification. The aim of our work was to discover if (or how) pre-existing resources could be used together to train a classifier in this task.\nWe find that while models can learn to identify Bias for a certain target category when trained when unspecified Bias/Hate Speech Detection datasets and a smaller dataset for that very same target category (Single-Target Classification), they do not perform well if one follows this system with many target categories and smaller datasets of varying sizes. However, models trained in this way still appear to be better at identifying Bias in synthetic text, or in more nuanced forms, than datasets trained only on generalized datasets and Twitter-based data, which implies that the model learns additional information that allows it to perform better in select contexts.\nThese conclusions emphasize the disproportionate attention given to certain targets of bias, which means that there are not enough resources available to train models to identify other types of biases. This is made worse by the reliance on non-persistent data, which leads to dataset degradation and further sabotages whatever available resources exist."}]}