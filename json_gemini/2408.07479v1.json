{"title": "A Study on Bias Detection and Classification in Natural Language Processing", "authors": ["Ana Sofia Evans", "Helena Moniz", "Luisa Coheur"], "abstract": "Human biases have been shown to influence the performance of models and algorithms in various fields, including Natural Language Processing. While the study of this phenomenon is garnering focus in recent years, the available resources are still relatively scarce, often focusing on different forms or manifestations of biases. The aim of our work is twofold: 1) gather publicly-available datasets and determine how to better combine them to effectively train models in the task of hate speech detection and classification; 2) analyse the main issues with these datasets, such as scarcity, skewed resources, and reliance on non-persistent data. We discuss these issues in tandem with the development of our experiments, in which we show that the combinations of different datasets greatly impact the models' performance.", "sections": [{"title": "Introduction", "content": "There is a growing awareness of the extent to which human biases can influence our models and algorithms. This realization has led to a fast growth in fields dedicated to studying bias, such as the study of bias in Natural Language Processing (NLP), which has focused not only on bias mitigation but also on its detection and classification. However, bias detection is a relatively new field of study, lacking many publicly available benchmark datasets or state-of-the-art models that are able to complete this task. Existing datasets are relatively small, often do not focus on the same types of bias, and are not even aimed at the same downstream tasks. So, a question arises: can we learn how to detect and classify bias using these (publicly available) resources? And, if so, how?\nIn order to answer this question, we have outlined the following objectives:\n- Find and collect publicly available datasets aimed at bias classification to serve as training data;\n- Train and analyse the performance of several classifiers, trained with different parameters and training data combinations;\n- Delve into issues such as reliance on non-persistent data, providing both consequence analysis and introducing possible solutions.\nThis work contributes, thus, with a thorough review of the current state-of-the-art in bias and hate speech detection in NLP. We provide an overview of various types of work developed in the scope of these fields, complemented by a detailed exposition of some pre-existing datasets which differ in terms of style, collection method, annotation style, and focus. The obtained results allow us to further analyse which aspects of this field require further work, which ones are in desperate need of attention, and possible avenues of future work.\nBefore we start, we need to define the concept of bias: \"bias\" refers to unequal treatment of a given subject due to preconceived notions regarding that very same subject, which necessarily influence our judgement. \"Social bias\", therefore, translates to unequal treatment of certain individuals or groups based on specific shared characteristics \u2013 namely, social constructs such as race, gender, gender identity, etc. Hence, there are two things that should be defined in order to construct our working definition of bias, namely: what is considered \"unequal treatment\"? And what shared characteristics will we consider?\nIn this paper, we chose to define \"unequal treatment\" as:\n- The use of derogatory terms which specifically target an individual or a group based on the defined social characteristics (for example \"bitch\", \"dyke\", \"tranny\");\n- The prevalence of stereotypes, which can also manifest through harmful beliefs (i.e. \"All Muslims are terrorists.\"), stereotypical societal roles (i.e. \"Women belong in the kitchen.\"), caricatures (i.e. \"The Angry Black Woman\"), or even apparently benevolent beliefs (i.e. \"Asians are good at math.\");\n- Otherwise abusive language which specifically targets a group or an individual based on the defined social characteristics (i.e. \"Gay people make me sick!\", \"I'd never date a black guy.\").\nAdditionally, we define that we will be considering the following social characteristics, henceforth referred to as \"Target Categories\": Gender, Race, Profession, Religion, Disability, Sexual Orientation, Gender Identity, Nationality, and Age.\nIn works similar to ours, we find that a term which often approximates our definition of bias is \"hate speech\". This is described in [1], as \"Language used to express hatred towards a targeted individual or group, or is intended to be derogatory, to humiliate, or to insult the members of the group, on the basis of attributes such as race, religion, ethnic origin, sexual orientation, disability, or gender.\" (2018:495) [1]. Although bias and hate speech share some similarities, they are not quite the same; while instances of Hate Speech will always be instances of bias, the same cannot be said in reverse. However, due to the aforementioned similarities, we will be utilizing resources from both fields.\nConsidering that the study of bias and hate speech is inherently a sensitive subject, which must be conducted with a degree of awareness and responsibility, we provide an Ethical Statement: due to our reliance in pre-existing resources, we have made a number of concessions regarding the complexities of the phenomenons being studied, such as the reduction of \"Gender\" to the two binary genders (and further exclusion of non-binary identities) or the uncritical approach to \"Race\", which, as a construct, is highly dependent of the sociocultural or national context it is discussed in [2]. Additionally, we were unable to use an Intersectional approach in our work. Intersectionality is a term coined by Kimberl\u00e9 Crenshaw in 1989 [3]. It refers to an analytical framework through which we can understand"}, {"title": "Related Work", "content": "In this section, we present an overview of work done in the fields of bias and hate speech detection, followed by a critical analysis on the limitations of the current state-of-the-art. Lastly, we examine datasets developed in the scope of bias and hate speech detection."}, {"title": "Overview", "content": "When it comes to the study of bias in NLP, [4] is an almost obligatory mention, having conducted one of the earliest studies we could find on the topic, focusing on Gender Bias in Word Embeddings. While more studies on Bias in Word Embeddings have been released since this initial study [5, 6, 7, 8, 9], we have also seen researchers further widening the scope of Bias in NLP, pouring over models or tools frequently used in various NLP tasks and study them under the lens of bias - sometimes as tools for detection and mitigation, other times as sources or propagators of bias. There is work focused on Neural Networks [10], on state-of-the-art models such as BERT [11, 12], techniques such as Adversarial Learning [13, 14], and various NLP tasks, such as Coreference Resolution [15], Sentiment Analysis [16], Dialogue Generation [17], and even POS tagging and Dependency Parsing [18].\nAnother way in which models developed in the scope of NLP can perpetuate bias is through their training data. A significant number of datasets are composed of non-curated content from the Web, due to the sheer amount of information that can easily be collected from online forums and platforms. While there are advantages to this approach (like the aforementioned ease in collecting large amounts of data, or the usage of casual, every day language instead of synthetic syntax), the fact remains that there is plenty of unsafe and offensive content on the Internet, which is uncritically collected to build these datasets.\nAn example of this is the work described in [19] on the Common Crawl Corpus, with a focus on finding instances of Hate Speech and sexually explicit content. The Common Crawl is a multilingual corpus, composed of 200 to 300 TB of text obtained from automatic web crawling, and with new versions being released monthly. After resorting to a series of different detection approaches, they found that 4.02% to 6.38% of their sample contained instances of Hate Speech, while 2.36% contained material deemed as sexually explicit. These percentages quickly become alarming when one considers the total size of the corpus in question, and thus that these percentages translate to a staggering number of Hate Speech instances.\nWhen we take into account these values, it becomes clear how models can easily learn biased content, even if we do not notice it right away. Examples such as Microsoft's Tay [20] or Meta's Galactica can serve as simpler cautionary tales, but even juggernauts such as ChatGPT face these issues [21]. In the case of ChatGPT, the solution found by the developing team is a mix of reliance on human annotators (which we will delve further on in Section 3.2) and overall avoidance of harmful language. Although this strategy has shown a measure of success, it is not infallible, reminiscent of the strategies employed by the team behind Philosopher AI, built with a predecessor of ChatGPT's current language model, when the software began exhibiting biased behaviour.\nThe presence of language models in our daily lives is, by now, unavoidable; the creation and maintenance of large training datasets (with their inherent biases) comes as an equally unavoidable consequence. Therefore, beyond studying how models can perpetuate bias (and how to mitigate it), it becomes relevant to learn how to leverage these very same models to detect and classify bias, or hate speech, in bodies of data.\nWhile some works have already focused on using NLP to detect and classify bias in real-life applications, such as analysing the Case Law Access Project (CAP) dataset regarding Gender Bias [22], analysing how Wikipedia pages portray LGBTQ people across different languages [23], or even determining whether there are noticeable differences in the way book critics review the works of male and female authors [24], the field that has truly embraced this method is Hate Speech Detection.\nHate Speech Detection, as a field of study, utilizes state-of-the-art models to detect and classify instances of Hate Speech. The detection of instances themselves might be simple, \"yes-or-no\" binary classification without specifying whom that phenomenon targets, simply whether or not it is present [25, 26, 27, 1, 28]. We refer to these as \"Binary Classification\" datasets. Other works also focus on a particular category or demographic, like sexism [29, 30, 31] or Islamophobia [32]. They might also focus on a simple \"yes-or-no\" classification (is the phenomenon present or not), or they might create their own subcategories for specific manifestations of the phenomenon in question. We refer to these as \"Single-Target Classification\" datasets. Lastly, some works consider several targets categories at"}, {"title": "Critiques and Limitations", "content": "While Bias Detection and Hate Speech detection are not the same field, they intersect substantially and share common pitfalls. For those reasons, the commentary of this section refers to both fields interchangeably.\nThe first issue in the current state-of-the-art is the lack of established taxonomies or centralized resources, whether in terms of terminology or benchmark datasets. While plenty of works use terms such as \"Bias\", \"Hate Speech\", or \"Abusive language\", the definitions associated with these terms are rarely in agreement. The absence of concise and concrete criteria leads to a \"sparsity of heterogeneous resources\" [36]. Countering this is the argument that there is no such thing as a set of pre-established criteria that could be applied, since there are no objectively correct definitions to be constructed, and we should instead strive for more clarity in the terminology used, as well as in the subtasks being studied [37].\nThe second limitation refers to the disproportionate focus given to certain target categories in these fields. We can find many examples of work done regarding sexism or gender bias, and, to a lesser extent, racism or racial bias. However, we will be hard-pressed to find significant data regarding ableism, transphobia, anti-Semitism, and many, many other categories worthy of a similar focus [33, 2, 37]. Additionally, works with gender as a target category often fail to conduct their research under an intersectional lens, thus reducing the nuance and depth of the phenomenon they propose to research [2].\nFurthermore, regarding uneven distribution of resources, there is the sheer amount of resources devoted to the English language in comparison to any other language. While this is, to a degree, understandable, due to how widely used English is in international contexts such as online spaces, it is not sustainable. The choice to center English-speaking internet users in this research, implicit or unintentional as it may be, creates its own form of data bias [2, 37]. While some works done in other languages do exist, these are few and far in between [38, 39].\nLastly, we would like to speak about dataset annotation.\nThe first issue we would like to expand upon is bias induced by dataset annotation. As humans, we are all prone to inherent biases. This is why datasets will usually be annotated by more than one person, and why measures such as inter-annotator agreement exist. In theory, these measures should allow labels to be chosen with as little bias as possible, especially if researchers resort to a diverse pool of annotators.\nHowever, we can still find instances of annotation bias. In [40], the authors find that entries of Hate Speech datasets which are written in AAE (African American"}, {"title": "Datasets", "content": "In this section, we present some of the publicly available datasets related to bias and hate speech detection. As mentioned in the previous section, not only are there few standard benchmark datasets available, but the datasets that do exist often do not follow specific, pre-existing taxonomies or definitions, and often focus on different manifestations of bias. As such, we chose to group our findings in accordance with the denominations we defined in Section 3.1.2, namely: Binary Classification, Single Target Classification, and Multi-Target Classification.\nBinary Classification As previoulsy said, we define \"Binary Classification\" as classification which focuses on identifying a certain phenomenon (whether that is bias, hate speech, abusive or toxic language, etc) without specifying a target category, like gender or race. Therefore, the datasets in this subsection focus only on the presence of a given phenomenon, and not on identifying if it refers to a particular group or not.\nDavidson [26] is a crowdsourced dataset with around 24,000 tweets intended for Hate Speech detection. This dataset is publicly available. In this dataset, entries"}, {"title": "Data Gathering", "content": "As previously mentioned, our objective was to gather and combine pre-existing resources, namely datasets developed in the scope of Bias and/or Hate Speech Detection, and evaluate if these could be used to successfully train a model in Bias Detection and Classification. After conducting our initial research, we settled on using the datasets depicted in Table 4."}, {"title": "Tweet Retrieval", "content": "Some of these datasets, namely Benevolent-Hostile Sexism [30] and Waseem-Hovy [35], are Twitter-based datasets which, due to privacy concerns, did not directly share the textual content of their Tweet entries. Instead, they share the Tweet IDs of each tweet. This is an alphanumerical identifier which, through the functionalities offered by Twitter API , can be used to Look-Up Tweets and retrieve the correspondent text. Thus, the initial phase of our work consisted of retrieving the content of these datasets so that we could then use them for our end goal."}, {"title": "Interlude: Dataset Degradation, or the Problem of Non-Persistent Data", "content": "There is a notable problem with the strategy of using Tweet IDs to resolve privacy concerns; namely, the fact that we can only retrieve a tweet if that tweet still exists. Unavailable tweets cannot be recovered.\nIn order to better investigate this issue, we turned to the Founta dataset [1]. The creators of this dataset responded to privacy concerns by separating tweet identifiers and tweet text into separate files and then sharing both files, rather than withholding the text altogether. Ergo, while we had no need to retrieve tweets of this dataset, since the relevant information was freely provided, we still possess the identifiers and are free to use them.\nThe results of our analysis regarding unavailable tweets, across all three datasets, can be found in 5. The table contains the total number of tweets in the dataset, the number of available tweets, and the number of unavailable tweets, as well as why they were unavailable. Since Benevolent-Hostile Sexism separated the Benevolent and Hostile components into two files and their yielded results differed significantly, we chose to showcase them separately."}, {"title": "Consequences", "content": "This dataset degradation influences the usefulness of our resources, most notably the Waseem-Hovy dataset and, in particular, the entries annotated for racism. While the original dataset boasted 1,970 entries with the aforementioned label, this amount was reduced to a grand total of 12 entries. Regarding the unavailable entries, 38 entries related to deleted tweets, while 1,920 referred to suspended users.\nThe Benevolent Sexism portion of the Benevolent-Hostile Sexism dataset, however, yielded another problem entirely. Out of the original 7,210 tweets in total, only 2,411 remained after processing. While this may seem incredibly problematic, our main issue is actually related to the available entries. After briefly perusing the results, we realized that there seemed to be an unusual number of repeated textual content. We concluded that, out of these 2,411 available entries, only 631 were unique tweets. The remaining 1,780 entries consisted of retweets of the same original tweet, which resulted in different tweet IDs for what basically amounted to plenty of repeated content.\nBoth of these results had an immediate effect on our plans moving forward.\nFirstly, having been reduced to a mere 631 entries, we decided to remove the Benevolent Sexism portion from our dataset collection, being left with the Hostile Sexism portion. Secondly, while we had previously considered Waseem-Hovy as a multi-target classification dataset - as a dataset which annotated entries for both the \"gender\" and \"race\" categories \u2013 the fact that only 12 entries remained for \"racism\" meant that this was no longer viable. Thus, we removed these entries, instead integrating the dataset into our collection as a single-target classification dataset with the target category \"gender\"."}, {"title": "Label Mapping", "content": "After retrieving the missing Twitter data, we proceeded to uniformise our dataset collections. We replaced Twitter-specific markers, such as usernames or hashtags, by specific text markers which would later be saved as special tokens; we selected only the relevant content from each dataset and saved it to identically structured CSV files; and, finally, we established label coherency through label mapping.\nThe first mapping dimension we tackled was Binary Classification, i.e. simply identifying whether an entry was biased or non-biased in accordance to our proposed definition. The label correspondences are detailed in Table 7.\nThe second mapping dimension dealt with the target categories each dataset tackled. Due to this, Davidson, Founta, and Golbeck are not included in this section, since these datasets solely deal with the Binary Classification task, as described in Section 2.2. The correspondences described below are summarized in Table 8. Many of our multi-target datasets used sub-labels to specify the target category of each entry. We chose to apply this principle to our work. After examining our collection and thus settling on our proposed definition of \"bias\", we similarly"}, {"title": "Model Training", "content": "For this work, we used the Emotion-Transformer, developed in the scope of Emotion Detection but adaptable to our Bias Classification task. The Emotion-Transformer is built on top of a pretrained Transformer model. In this work, we chose the DistilBERT pretrained model from HuggingFace, which served as a necessary compromise between temporal efficiency and overall performance.\nTo establish the Emotion-Transformer's level of performance, we trained it with individual datasets of our collection and compared the obtained results against results reported in the publication of those same datasets. Any comparison of results for Benevolent-Hostile Sexism and Waseem-Hovy would be invalid, due to the alterations these datasets suffered, described in the previous section. Additionally, DynGen was evaluated in a multi-labeling task, which would make our evaluation of it as a single-labeling task irrelevant."}, {"title": "Experimental Setup", "content": "We divided our datasets into four non-exclusive groups, named Group A, Group B, Group C, and Group D. Group A, as the smallest and most coherent of the groups, serves as our baseline for performance comparison. Groups B, C, and D each answer a research question, described in Table 9.\nWe performed a non-deterministic split of each group's data, splitting it into training, testing, and validation sets (80% train and 10% for testing and validation each). In total, we conducted over 100 experiments, in which we trained the model with different parameters and training data combinations.\nThe tested parameters were: Number of Training Epochs, Loss Function, and Pooling Function. The remaining parameters remained unchanged throughout experiments, such as Seed Value (12), Patience (1), Gradient Accumulation Steps (1), Batch Size (8), Number of Frozen Epochs (1), Encoder Learning Rate (1.0e-5), Classification Head Learning Rate (5.0e-5), and Layerwise Decay (0.95). These were the default values set for the Emotion Transformer."}, {"title": "Interlude: Class Imbalance, Undersampling, and Data Augmentation", "content": "As we mentioned in Section 2.2, one of the most blatant limitations of this field of study, at the moment, is the way certain target categories (most notably, \"Gender\" and \"Race\") receive a lot more attention - and, as such, a lot more dedicated resources - than any other category. This skewed distribution has had an obvious impact in our work; not only is our single-target control group focused on the target category \"Gender\", but also the distribution of available resources across our chosen target categories is glaringly skewed, as can be seen in Tables 10 and 11. Table 10 details the split between biased and non-biased entries in each dataset, while Table 11 splits into target categories.\nThe split between the \"biased\" and \"non-biased\" categories is relatively balanced. The distribution of target categories across other groups, however, is blatantly skewed.\nOne way of balancing a previously imbalanced dataset is through Undersampling; namely, removing entries from majority classes until we are close to an even split across classes. This is a solution that we cannot implement in our work. \"Age\", for example, features 23 entries in total. Undersampling would sabotage our performance, heavily reducing the amount of available data to a meager portion, which would not be enough for our model to learn from.\nThe other way of balancing a dataset is by turning to the opposite direction: if we cannot remove entries, then we shall add new ones. Since we cannot simply create new entries, due to it being a rather costly process, we could opt to augment our datasets through Data Augmentation, which is the process of creating new data by altering copies of pre-existing data. This can result in a stale and/or repetitive dataset, if used in excess, but it seems like a possible solution to our problem.\nHowever, one of the most complicated aspects of this field of study is the fact that \"bias\" is not a fixed category, with unanimously agreed-upon manifestations. There are some instances in which simply grabbing a biased sentence and replacing a word related to a target category by a word related to a different category would successfully result in a brand new biased sentence. For example, if one were to look at the sentence \"I hate Muslims!\", and simply swap \"Muslims\" for, say, \"Nurses\", we would obtain a brand new sentence which exhibited hate regarding the target category \"Profession\".\nHowever, the types of biased entries in our datasets - and the way bias often manifests in real life are often not this straightforward. We often consider certain sentiments or sentences to be biased not because of their inherent nature, but because they refer to a target category in a way that, in our sociocultural framework, is considered biased. \"All girls are terrorists.\" and \"All Muslims are terrorists.\" are both sentences which contain a generalization; however, only the second sentence represents a stereotype \u2013 or, in other words, \u201ca preconceived notion\" of a group of people which, quite often, results in unequal treatment of individuals perceived to be part of that very same group\". This is our definition of bias; not just any type of generalization.\nBias and Hate Speech are not concepts which exist in a vacuum, and can be carelessly replicated by simply swapping word pairs. We cannot divorce these concepts from the realities they represent without robbing them of their inherent meaning and fundamentally changing the aim of our work.\nHence, we decided to continue working with imbalanced datasets, shifting our exploratory focus to also analyse how this imbalance would impact model performance. We invite future work to further explore the possibility of balancing these types of datasets, and how to achieve that goal without compromising the complexity of the phenomenon being studied."}, {"title": "Results", "content": "As previously mentioned, Group A is our baseline. It is also the only group that can only be used to train models for the Binary Classification task. Groups B, C, and D can be used in both Binary Classification and in Multi-Target Classification.\nWe conducted three types of tests. The first was in Multi-Target Classification, using Groups B, C, and D, in which both the training and testing data were from the same group. The second type was in Binary Classification, using all groups, in which both the training and testing data were also from the same group. The third type was also in Binary Classification \u2013 but we used a Model trained with data from Group A to classify test data from Groups B, C, and D.\nThe best F1-scores obtained in the first testing round, on Multi-Target Classification, are depicted in Table 12. We refer to these experiments as \"Multi-B\", \"Multi-C\", and \"Multi-D\". Additionally, we will refer to the Binary Cross-Entropy with Logits Loss Function as simply \"BCE\".\nFurther examination of these results, particularly of Multi-C and Multi-D, shows that the lower F1-scores result from lower values for precision and recall across classes. The \"Age\" class, in particular, yields an F1-score of 0 across all tests. This is unsurprising due to the extremely low number of entries for this category,"}, {"title": "Discussion", "content": "\"How do Single-Target datasets influence performance?\u201d Or: Group-A vs Multi-B, Binary-B, and Inter-B\nThis is the question that led us to create Group B as a distinct control group, with its sole Target Category. Furthermore, since all the individual datasets in this group are Twitter-based, we also remove other variables from this experiment, such as the linguistic variation of Internet and synthetic data.\nAs can be seen in Tables 12 and 14, the difference in overall performance between Group A and Multi-B is slight. From this, we can conclude that the model is able to correctly predict when a sentence is biased, and also when that bias is aimed at target category gender.\nObserving the Binary-B results, shown in Table 14, we can see a 0.01 decrease in F1-score in the biased category when compared to Group A's results. While the model's ability to differentiate between biased and non-biased content is maintained, we can presume that the entries from the Single-Target datasets differ enough from the unspecified biased entries to result in a slight, decrease in performance. This addition does not seem to impact the non-biased category in any significant way.\nLastly, we can compare the Inter-B results with Group-A and Binary-B. Inter-B's F1-score of 0.8780 compared to A-El's 0.8974 shows us that the model solely trained on Group A data, while clearly able to identify some of the gender-biased entries and perform adequately, does not perform as well as the baseline. Most importantly, it also does not perform as well as a model trained with Group-B data, as evidenced by Multi-B's F1-score of 0.8909.\nWe can conclude that adding entries labeled for a specific target category to a general Bias/Hate Speech dataset results in a model which can accurately identify and classify biased content revolving around that very same target category, with little to no decrease in overall performance. These results are, therefore, highly promising."}, {"title": "\"How do synthetic and Multi-Target datasets influence performance?\u201d Or: A Lukewarm Overview of Group C", "content": "This is the question that motivated the existence of Group C as a control group, by adding to our baseline those datasets that were Multi-Target and/or synthetic. This was an almost by default choice, since most of our Multi-Target datasets were also synthetic.\nAs depicted in Tables 12 and 14, the difference in performance between Group A and Multi-C is significant, even with the increase observe by removing the \"Age\" category, as depicted in Table 13. This result is caused by the lower scores obtained in the several target categories."}, {"title": "\"Can we obtain a better performance by using all of our resources together?\u201d Or: The Epic of Group D", "content": "Lastly, we arrived at our last control group, which is composed by the unification of all our resources. We are, therefore, analysing how well (or how badly) the general, Twitter-based Bias/Hate Speech Detection datasets, Single-Target datasets, and synthetic and/or Multi-Target datasets perform together.\nWe would like to remind that Group D is the only one to include the CONAN dataset, introduced in Section 2.3, which is a synthetic, Single-Target dataset for the target category \"Religion\". This dataset did not fit neatly into the previous control groups, but we decided to nevertheless include it in this Group; \"all of our resources\", after all, means all of our resources.\nAs can be seen in Tables 12 and 14, we once more find a significant difference in performance between Group-A and Multi-D, partially bridged by NoAge-D, in Table 13. Group A's overall F1-score consistently hits the 0.89 range, while Multi-D's rests in the 0.61 range and NoAge-D falls, on average, in the 0.67 range. Multi-D sees a decrease in performance for both the b none and non-biased categories, even when compared to Multi-C.\nThere is, however, a severe decrease in performance worthy of note in gender_identity. We believe this might either be due to the split between train, validation, and test sets seeing as this class makes up a mere 0.56% of Group D, and, as such, is easily affected by the random data split or due to some type of"}, {"title": "Conclusion", "content": "Bias in NLP is a recent field of study, with plenty of works being published in recent years. We are discovering that there are many ways in which human biases can, and do, infiltrate our programs and algorithms. One of these ways is through biased training data, which teaches models how to replicate those very same biases.\nIn our work, we sought to use publicly available resources to train a classifier in the task of Bias Detection and Classification. The aim of our work was to discover if (or how) pre-existing resources could be used together to train a classifier in this task.\nWe find that while models can learn to identify Bias for a certain target category when trained when unspecified Bias/Hate Speech Detection datasets and a smaller dataset for that very same target category (Single-Target Classification), they do not perform well if one follows this system with many target categories and smaller datasets of varying sizes. However, models trained in this way still appear to be better at identifying Bias in synthetic text, or in more nuanced forms, than datasets trained only on generalized datasets and Twitter-based data, which implies that the model learns additional information that allows it to perform better in select contexts.\nThese conclusions emphasize the disproportionate attention given to certain targets of bias, which means that there are not enough resources available to train models to identify other types of biases. This is made worse by the reliance on non-persistent data, which leads to dataset degradation and further sabotages whatever available resources exist.\nThese conclusions also emphasize the need for clarity and diversity in further research in this field. It is paramount to diversify the focus of research, especially in an age in which social biases continue to grow in social importance. Technological advances must keep pace with societal ones, and that goal cannot be achieved if we remain stagnant and do not pay heed to recurring mistakes."}]}