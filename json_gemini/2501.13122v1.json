{"title": "Zero-Shot Verification-guided Chain of Thoughts", "authors": ["Jishnu Ray Chowdhury", "Cornelia Caragea"], "abstract": "Previous works have demonstrated the effectiveness of Chain-of-Thought (COT) prompts and verifiers in guiding Large Language Models (LLMs) through the space of reasoning. However, most such studies either use a fine-tuned verifier or rely on manually handcrafted few-shot examples. In contrast, in this paper, we focus on LLM-based self-verification of self-generated reasoning steps via COT prompts in a completely zero-shot regime. To explore this setting, we design a new zero-shot prompt, which we call COT STEP, to aid zero-shot decomposition of reasoning steps and design two new zero-shot prompts for LLM-based verifiers. We evaluate the verifiers' ability to classify the correctness of reasoning chains and explore different ways to use verifier scores in guiding reasoning for various mathematical and commonsense reasoning tasks with different LLMs.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (Dubey et al., 2024; Brown et al., 2020) have revolutionized the field of NLP by enabling state-of-the-art (SOTA) performance in several tasks merely by smart prompting (Wei et al., 2022a; Bubeck et al., 2023). A critical landmark in the art of prompting for multi-step reasoning tasks is the Chain-of-Thought (COT) prompting (Nye et al., 2022; Wei et al., 2022b), which elicits LLMs to generate step-by-step reasoning chains before providing the answer to a given question. Building upon COT prompting, several recent works incorporate a verifier mechanism to improve LLMs' performance. For instance, Cobbe et al. (2021); Li et al. (2023); Weng et al. (2023) use a verifier to evaluate the correctness and score each reasoning step, whereas Gandhi et al. (2023); Yao et al. (2023); Hao et al. (2023, 2024) use a verifier's scores to do a tree-search over the space of reasoning steps.\nHowever, these works have several drawbacks including: 1. most of them rely on few-shot prompting, i.e., they rely on exemplars (often manually handcrafted) of how the task is to be performed. This selection of the exemplars brings additional challenges as to which exemplars should be selected, how many, etc; 2. The verifier is also typically either fine-tuned on labeled datasets or rely on further verification-related few-shot exemplars. Properly fine-tuning a verifier may require a large number of human annotations which may be impractical in different tasks and domains. In contrast, in this paper, we focus on LLM-based self-verification of self-generated reasoning steps via COT prompts in a completely zero-shot regime. To achieve this, we propose a new zero-shot COT-based prompt, called COT STEP, for generating reasoning steps (i.e., to aid zero-shot decomposition of reasoning steps) and two new zero-shot verification prompts for using LLMs as verifiers to verify their own reasoning steps (i.e., evaluate whether the steps are correct or not). The zero-shot regime is valuable because in this setup, no manually selected or handcrafted examples are required. Moreover, in the complete zero-shot setting that we explore, there is no reliance on any additional data resource for exemplar construction. Furthermore, a general purpose zero-shot prompt can be used to set up a convenient model interface for users to request different kinds of reasoning tasks without having to worry about providing task-specific exemplars.\nIn exploring the zero-shot regime, we ask three research questions (RQ1-3) in this paper.\nRQ1: How do different zero-shot COT prompts compare against each other across different LLMs?\nRQ2: How well do LLM-based verifiers perform in evaluating the correctness of reasoning steps?\nRQ3: How well can LLM-based verifier scores be used in selecting the right answer among many or in guiding reasoning in real-time?\nWe address each of the questions in turn - RQ1 in \u00a73, RQ2 in \u00a74, and RQ3 in \u00a75 and evaluate them on various mathematical and commonsense reasoning tasks with different LLMs. While investigating RQ1, we find that our COT STEP prompt keep up competitively with other prompts alongside enabling automatic step decomposition. While investigating RQ2, we find that a zero-shot COT-variant of verification prompt tends to perform reasonably in verifying the correctness of mathematical reasoning steps. While investigating RQ3, we do find a noticeable benefit from using the verifier score in a step-level greedy search but the benefit vanishes when self-consistency (Wang et al., 2023b) is used."}, {"title": "2 Main Components", "content": "In this section, we discuss the main components of our experiments that are relevant for all our research questions (RQ1-3)."}, {"title": "2.1 Models", "content": "For our experiments, we primarily focus on two openly-licensed models in the main paper - (1) SOLAR (SOLAR 10.7B Instruct)\u00b9 (Kim et al., 2024) and (2) Phi3 (Phi 3 Mini 128K Instruct) (Abdin et al., 2024) and show additional experiments on other models such as Mistral (Mistral 7B Instruct v0.2) (Jiang et al., 2023) and Llama3 (LLama 3.0 8B Instruct) (Dubey et al., 2024) in Appendix B."}, {"title": "2.2 Existing Prompts", "content": "In our experiments, we consider four major existing zero-shot prompt strategies. We describe them here. Note that we use a simplified representation of user-assistant conversational turns to describe the prompt structure for generality and brevity.2\n(P1) Base: Base represents the baseline zero-shot prompting style for question answering that we use. Given a question, such as \"What is 2+2?\", the prompt to elicit an answer would look like: \"User: Q: What is 2+2? \n Assistant: A:\".\n(P2) COT: COT (Chain Of Thoughts) represents the standard zero-shot COT prompt (Kojima et al., 2022). This can be expressed as \"User: Q: What is 2+2? \n Assistant: Let's think step by step.\". Such a prompt is designed to prime the models to generate more explicit chain of reasoning steps before producing an answer.\n(P3) PS+: PS+ represents the plan-and-solve zero shot PS+ prompt presented by Wang et al. (2023a) as an extension over plain COT. This can be expressed as: \"User: Q: What is 2+2? \n Assistant: A: Let's first understand the problem, extract relevant variables and their corresponding numerals, and devise a plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numeral calculation and commonsense), solve the problem step by step, and show the answer.\"\n(P4) TAB COT: TAB COT represents another zero shot prompt originally presented by Ziqi and Lu (2023). This can be expressed as: \"User: Q: What is 2+2? \n Assistant: A: \n| step | subquestion | process | result\\n\". Such a prompt primes the model to perform chain of thought style reasoning in a tabular format.\nNote that we mainly focus on prompt strategies that do not require any additional data (label-free or not), pseudo-data generation, or involvement in any sophisticated meta-prompting (Yuan et al., 2024; Jin et al., 2024; Zhou et al., 2024; Chen et al., 2023; Wan et al., 2023b,a). Such works can be explored in future work. Besides the prompts discussed above (P1-4), Xu et al. (2024) proposed a zero-shot prompt for re-reading the question and Kong et al. (2024) recently proposed role-play-based zero-shot prompting. They can be orthogonal augmentations to any of the existing zero-shot prompts. We do not focus on them for this paper."}, {"title": "2.3 Our COT STEP Prompt for Thought Decomposition", "content": "Many of our experiments require LLM-based self-verification of the correctness of the reasoning steps induced by COT-styled prompts. However, this requires some strategy to decompose the reasoning steps (or thoughts) to be evaluated separately. In case of few-shot COT prompts, this can be done by providing exemplars with a specific reasoning format that can be exploited to automatically parse reasoning steps. However, that is not possible with zero-shot COT prompts. Most of the existing zero-shot COT prompts discussed above, do not provide any specific formatting constraint that would allow us to systematically demarcate reasoning steps.3\nTAB COT is an interesting exception compared to other existing prompts since it does induce the model to follow a specific tabular structure corresponding to the given columns. Thus, we can treat each generated row in the table (based on TAB COT) as a single reasoning step which can be decomposed based on new line markers (\u201c\\n\"). Nevertheless, we generally find the performance of TAB COT to be lower compared to other existing prompts as we will show later. We also find just the basic COT prompt to be a good enough contender (\u00a73.1). As such, we design a simple variant of the standard zero-shot COT prompt that makes the model always generate thoughts automatically in a structured manner where each thought (reasoning step) is separated cleanly into different numbered steps. We call this prompt as COT STEP.\n(P5) COT STEP: COT STEP is a novel variant of zero-shot COT prompt. This can be expressed as: \"User: Q: What is 2+2? \\n Assistant: A: Let's think step by step.\\n\\nStep 1: \". This prompt primes the model to organize its reasoning steps into numbered steps (Step 1, Step 2, ...). Given such structured outputs from COT-STEP, the reasoning steps (or \"thoughts\") can easily be decomposed automatically by using 'Step <number>:' as a delimiter."}, {"title": "2.4 Datasets", "content": "We evaluate our research questions on five Qusetion Answering-based reasoning datasets - (1) GSM8K (Cobbe et al., 2021) (2) GSM-HARD (GSMH) (Gao et al., 2023) (3) AQUA (Ling et al., 2017) (4) StrategyQA (StratQA) (Geva et al., 2021) (5) CommonsenseQA (CSQA) (Talmor et al., 2019), which are mathematical reasoning (first three) and commonsense reasoning datasets (last two)."}, {"title": "3 RQ1: COT Prompt Comparisons", "content": "In this section, we seek to compare the existing zero-shot prompts alongside our COT STEP prompt. We want to ensure that COT STEP maintains reasonable performance compared to other zero-shot prompts. Moreover, some prompts like PS+ are only evaluated for a single LLM, and typically not an openly-licensed LLMs. Thus, we compare the zero-shot prompts under the same settings across two openly-licensed models - SOLAR and Phi3, and show results with additional LLMs in Appendix B. Furthermore, self-consistency (Wang et al., 2023b) has shown to boost the performance of any prompt and we explore it in our setting.\nSelf-Consistency Self-Consistency (Wang et al., 2023b) creates multiple sample generations from a model given any prompt. Each sample is associated with an answer to the given question. After that, majority voting is applied to select the most frequent answer among the samples. We compare all the five prompts both with and without self-consistency. When using Self-Consistency, we use a temperature of 0.8 and draw 20 samples."}, {"title": "3.1 Results", "content": "We present our experiments in Table 1. The Base (Non-COT) prompt when using Phi3 without self-consistency excelled in AQuA and is oth-erwise competitive in CSQA where COT-style reasoning may not be as crucial given that it is a commonsense reasoning task. Besides that, it is the worst performer with or without using self-consistency. Surprisingly, PS+ and TAB COT (although they were shown to be better than original zero-shot COT (Kojima et al., 2022)) do not consistently outperform COT. Rather, without self-consistency, they often seem to perform significantly worse. Thus, these prompts may only work better compared to COT for specific models like GPT 3.5 Turbo, Davinci and such - but not generally across most models. Interestingly, when using self-consistency, these same prompts tend to become much more competitive and the results can flip. For example, PS+ on SOLAR performs the second worst on GSMH without self-consistency, but performs the best on the same task with self-consistency. Our COT STEP keeps up fairly well with other COT prompts. Without self-consistency COT STEP is even the best performer in most cases for SOLAR. With self-consistency COT STEP is still nearby to the best performer. Thus, COT STEP offers a good way to get structured outputs with cleanly separated reasoning steps in a zero-shot manner while maintaining competitive performance against most of the existing zero-shot COT prompts. We provide examples of generations from various prompts in Appendix C."}, {"title": "4 RQ2: Evaluating Self-Verification", "content": "Self-verification refers to the idea of using LLMs to verify the correctness of their own generated reasoning steps. Roughly, this is typically done by sending LLM-generated reasoning steps to the same LLM as an input alongside some appropriate prompt to evaluate those steps. The LLMs' response and its corresponding probabilities can be then used to get a \"score\" for the correctness of the generated reasoning steps that were sent as the input. In this paper, we evaluate each reasoning step separately (given the context of past reasoning steps) and assign each reasoning step with the evaluated score. Below, we describe our specific methods for obtaining the evaluation score."}, {"title": "4.1 Self-Verification: Task Setup and Prompts", "content": "Task Setup\nFor self-verification, given an input question and its associated (potentially partial) reasoning chain, the task is to determine whether the last step in the chain is correct or not.4 Thus, the self-verification happen at a step-level granularity.\nR-prompt\nFor this task, we design a zero-shot self-verification prompt as shown in Figure 1, which we call R-prompt. The prompt is adapted from the one-shot verification prompt used in Ling et al. (2023) for a simplified zero-shot context.\nCOTR-prompt\nSimilar to Ling et al. (2023), we also explore a COT variant of the R-prompt which we call the COTR-prompt. After the \"Assistant: \" in R-Prompt in Figure 1, the COTR-prompt variant replaces \"Answer: (\" with the following line: \u201cAnswer: Let's double-check step by step.\\n\u201d."}, {"title": "4.1.1 Verification Answer Extraction", "content": "From Figure 1, \u201c(A)\" represents \"Yes\" to the correctness of the given reasoning step whereas \u201c(B)\" represents \"No\". Thus, as a heuristic, we then check if \"A)\" is present in the generated response or not. If present, we consider that the LLM predicts the last (potentially partial) reasoning step to be correct, otherwise we consider that it predicts the last step to be incorrect. Given that the R-prompt is primed to immediately generate \u201cA\" or"}, {"title": "4.1.2 Verification Score Generation", "content": "Above we discussed how we get the LLM-based self-verification prediction. Here, we discuss how we get a score for the correctness (the answer \"(A)\") of the last reasoning step from the LLM. There are two components (C1 and C2) to generating an overall scalar score for a reasoning step.\nC1: Reasoning Step Generation\nThe component C1 returns the average log-probability (token-wise) for the generation of the last reasoning step in the input reasoning chain (that is to be verified). Let us call the returned score (obtained during the generation with COT prompts (from \u00a72.2 and \u00a72.3), e.g., with COT STEP) as SC1. Note C1 has nothing to do with the self-verification prompt (R-prompt or COTR-prompt) or verification prediction.\nC2: Reasoning Step Verification\nThe component C2 first calculates the average log-probability (token-wise) of the generation of the response to the verification prompt (R-prompt or COTR-prompt from \u00a74.1). Let us call the returned score as SC2. The final value returned by C2 is then:\n$\\frac{S_{c2}}{log(1-exp^{S_{c2}})} \\text{ prediction = (A)}$\n$\\frac{S_{c2}}{exp^{S_{c2}}} \\text{ prediction = (B)}$\nThis scoring is reliant on the prediction (answer extraction) from the verification prompt (R-prompt/COTR-prompt) as described in \u00a74.1.1. $S_{c2}$ is designed to correlate positively with the probability of correctness of the given reasoning step.\nUnification:\nThe two scores SC1 and SC2 are then averaged and exponentiated to return the final unified scalar score (sf):\n$S_f = exp(\\frac{S_{c1} + S_{c2}}{2})$\nWe take inspiration from Xie et al. (2023) in incorporating both the generation probability of the reasoning step and the probability of verification prediction for the final score of the reasoning step. However, unlike Xie et al. (2023), our approach is simpler in that we take a simple average instead of introducing new tunable hyperparameters. In Appendix B, we also discuss some ablation experiments where we use only C2 for scoring. There, we show that C1 is required for the best performance."}, {"title": "4.2 Correctness Classification Task Design", "content": "Now that we have presented our setup for the LLM verifier, the question arises on how do we evaluate the capacity of LLMs to self-verify. Since we do not have ground-truth labels for correctness of each reasoning step of an entire reasoning chain and because of the difficulty of creating such labels, we design the evaluation of LLMs self-verification as follows: we consider the specific task of correctness classification at the entire reasoning chain level despite our model-based self-verification being at reasoning step-level. In this task, the input is a question and the entire reasoning chain for the question. The output is a binary class: 1 if the reasoning chain has all correct reasoning steps (i.e., all intermediate reasoning steps are predicted as correct by the self-verifier) and the final answer matches the ground truth, and 0 otherwise.\nSpecifically, given a question and a reasoning chain as input we decompose it into multiple step-level inputs. A step-level input for step i will have the question and all reasoning steps in the chain from the beginning to step i. The LLM is used independently on each step-level input under the self-verification prompt templates (R-prompt, COTR-prompt). For each step-level input we get the LLM's classification on whether the reasoning step is correct or not based on the answer extraction strategy described before (\u00a74.1.1). The final prediction is 1 (indicating the correctness of the whole chain of reasoning) if the LLM answers affirmatively (option \"(A)\") to the correctness of each step-level input for the given reasoning chain and question. Otherwise the final prediction is 0 (indicating the incorrectness of at least one step in the chain of reasoning). Note that for this task, using the verification scores themselves are not necessary. Only the answer extractions are relevant. We evaluate the capability of LLMs to self-verify using verification scores in \u00a75."}, {"title": "4.3 Correctness Classification Dataset Design", "content": "Above we described the task of correctness classification for evaluating verification capabilities of LLMs. We also explained how we can use LLMs to predict chain-level correctness or incorrectness based on step-level predictions. However, we still need datasets for the task to actually perform the evaluations. To design the dataset we need three components: (1) the questions (2) the reasoning chain responses to the questions, and (3) the ground"}, {"title": "4.4 Self-Verification Results", "content": "We show our results on the four created datasets in Table 2. We can observe from the table that when the questions are sourced from the mathematical reasoning dataset (GSM8K), COTR-prompt consistently outperform R-prompt for verification for all the LLM models and reasoning chain prompts that are considered here. The R-prompt leads to a high bias in predicting any given chain as correct. Thus, we find that the R-prompt gets high true positive rate, but extremely low true negative rate. Interestingly, none of the verifier setup performs particularly well when the questions are sourced in CSQA - a commonsense reasoning dataset. This may be because chain of thought-style reasoning is distributionally non-standard for basic commonsense reasoning (which in natural settings, typically happens implicitly). We also share results with more correction classification datasets created from other zero-shot COT prompts and with other LLMs as verifiers in Appendix B."}, {"title": "5 RQ3: Utilizing LLM-based Verifier Scores for Reasoning", "content": "We now investigate different ways to utilize the verifier scores to boost performance of LLMs in different reasoning tasks. That is, we analyze how well can LLM-based verifier scores be used in selecting the right answer to a question among many (\u00a75.1) or in guiding reasoning in real-time (\u00a75.2)."}, {"title": "5.1 Verifier Score-based Self-Consistency", "content": "Several works (Cobbe et al., 2021; Li et al., 2023; Weng et al., 2023; Liang et al., 2024) have tried to utilize verifier scores to augment self-consistency. Similar to the spirit of such prior works, here, we consider three policies to augment self-consistency using verifier scores or verifier predictions. Each of these policies are different strategies to select a single answer from multiple sampled answer. Majority voting is the standard baseline strategy used in typical self-consistency for answer selection. It does not consider any verifier score. We describe below our three alternative policies (that do utilize the verifier score). For policies (1) and (2), we need the verifier score for an entire reasoning chain. To get that, we simply add up the verifier scores of each step, i.e., the sf scores from Eq (2) of each reasoning step (see \u00a74.1.2).\n1. Max: In this policy, given multiple sampled answers, we choose the one from the highest scoring reasoning chain according to the verifier.\n2. Weighted: In this policy, given multiple sampled answers, we conduct a weighted majority vot-ing, where the veto power of each sample is the score of its reasoning chain assigned by the verifier.\n3. Filter: In this policy, given multiple sampled answers, we predict the correctness of each answer using the verifier (based on the correctness prediction strategy described in \u00a74.2). We then filter out all the samples that are classified as incorrect and conduct a majority voting on the remaining samples. Note that the answers of samples that are classified as correct can still differ even if the entire reasoning chain is predicted as correct."}, {"title": "5.2 Verification-Guided Search", "content": "In this section, we consider approaches to use the verifier to guide the reasoning steps as they are getting generated rather than just filtering/weighing them post hoc. We provide hyperparameter details about the search methods in Appendix A.\nStep-wise Greedy Search:\nOur main approach is a stochastic Step-wise Greedy Search (SG) similar to Khalifa et al. (2023). In this strategy, at every turn - first, we use LLMs to generate k reasoning steps (using temperature-based sampling); second, we use the verifier with the COTR-prompt to score each step; and, third, we greedily select the reasoning step with the maximum verification score to condition the generation of the next step. We stop when a reasoning step with the end of sequence marker is generated.\nStep-wise Beam Search:\nWe also consider an extension of SG where instead of the stochastic stepwise greedy search, we use stochastic Step-wise Beam (SB) search guided by the verifier similar to Xie et al. (2023). The SB approach is also comparable to the breadth-first-search used in Tree of Thoughts (Yao et al., 2023). We use majority voting among the final beams - we call this overall strategy SB + SC (SC stands for Self-Consistency). We share more technical details in Appendix A.\nResults:\nIn Table 4, we compare basic COT STEP and COT STEP augmented with SG search. Here, we find that our verifier guided SG consistency outperforms the baseline when not using self-"}, {"title": "A Search Hyperparameters", "content": "For step-wise greedy search (SG), at each turn, we generate k = 5 reasoning steps. When combining SG with SC, at the first turn we generate 20 reasoning steps. We keep all the 20 reasoning steps and independently continue 20 reasoning chains. Each of the 20 chains use SG as before from the second turn (generating k = 5 candidates and greedily selecting the maximum scoring candidate). In the end, we get 20 reasoning chains with 20 corresponding answers. We then apply majority voting.\nFor step-wise beam search (SB), we use a beam width of 16. That is, at every turn, we generate 16 reasoning steps per b (let us say) beam (initially 1). If at least 16 beams have reached terminal status (i.e. the end of sequence marker is reached by each of the chains in the beams) among the 16b beams, we stop the search and return the majority voted result (thus, combining with self-consistency) among all the terminal beams. If at least 16 beams have not reached a terminal status, we stochastically sample 16 beams from the 16b beams based on their aggregated verifier scores as a pruning step. The sampled 16 beams become the b beams for the next step.\nLet us say, the verifier score at any step i for some beam m is $S_f^{(i,m)}$ calculated as described in \u00a74.1.2. In that case the aggregated verifier score for a beam m generated up to step j would be:\n$s_{bm} = \\frac{1}{d_m}\\sum_{i=0}^{j}log S_f^{(i,m)}$\n$s_{bm}$ represents the mean score for the beam m generated upto the last step so far (in this case, step j). $d_m$ represents the depth (number of reasoning steps) of beam m. We maintain the aggregation in logspace. Given multiple b beams, the aggregated score ($s_{bm}$) for any beam m can be normalized into a probability distribution via:\n$nS_{bm} = \\frac{exp(s_{bm}/T_m)}{\\sum_{i=1}^{b}exp(s_{bi}/T_i)}$\nThus, we get a probability distribution over all the available beams at a turn, and we can use this for sampling in the pruning step described before. Similar to Xie et al. (2023), we decay the temperature per turn in the beam search by some \u03b1: \u03c4\u2190\u03c4.\u03b1. Following Xie et al. (2023), we set both the initial temperature (7) and \u03b1 as 0.5."}, {"title": "B Additional Results", "content": "In this section, we share several additional experimental results."}, {"title": "B.1 Zero Shot Prompt Comparisons with Other LLMS", "content": "In Table 6, we compare the zero shot prompt performance (including step-wise greedy search with COT STEP) of other LLM models like Mistral (Mistral 7B Instruct v0.2) (Jiang et al., 2023) and Llama3 (LLama 3.0 8B Instruct) (Dubey et al., 2024). We observe the same patterns that we did with SOLAR 10.7B and Phi3 in the main paper."}, {"title": "B.2 Evaluating Self-Verification", "content": "In Table 7, we present an extended version of Table 2. We present the results on more correctness classification datasets generated from reasoning chains of other COT prompts (PS+, COT) and results by more LLM verifiers (Llama3, Mistral). In case of prompts like COT/PS+, we use new-line to decompose reasoning steps. For the mathematical datasets (based on GSM8K), we find that COT STEP reasoning chains lead to the best for most (if not all) LLM models as verifier (with COTR prompt) on balance (Accuracy, F1 measures). This further justifies the choice of COT STEP. R-prompt tends to remain highly biased towards either rejecting any reasoning chain as incorrect (increasing true negative rate at a high cost of true positive rate) or accepting any reasoning chain as correct (increasing true positive rate at a high cost of true negative rate). COTR-prompt, in contrast, tends to have a better balance. For commonsense reasoning-based datasets (based on CommonsenseQA as the question source), our observations remain similar to before as discussed in the main paper."}, {"title": "\u0412.\u0417 \u0422\u0410\u0412 COT vs COT STEP", "content": "Both TAB COT and COT STEP allows a principled way to automatically parse reasoning steps in a zero-shot regime. As such, we compare these two prompting strategy a bit more closely. First,"}, {"title": "B.4 Verification Score Ablation", "content": "In this section we conduct a short ablation study to check whether component C1 (described in \u00a74.1.2 for verifier score computation is important or not. We compare the performance of Phi3 and SOLAR while using them as verifiers with and without C1 for scoring and guiding them through the step-wise greedy search (SG). In Table 11, we report the results. The results show that removing C1 generally tend to harm the performance of SG - thus, underscoring the importance of the component."}, {"title": "C Generation Examples", "content": "We present examples of structured reasoning chain generations based on COT STEP prompt in Figure 2 and some example generations based on COT prompt in Figure 3. As can be seen while the fundamental content in the two cases are not two different, COT prompt do not have any consistent structure or pattern to exploit for zero-shot decomposition, whereas COT STEP prompt cleanly separates reasoning into numbered steps."}, {"title": "6 Related Works", "content": "Multiple works (Kadavath et al., 2022; Ling et al., 2023; Stechly et al., 2024; Hong et al., 2024) have studies of LLM-based verification and their limitations. However, most of them focus on few-shot verification. Several works studied the potential for LLMs to self-reflect and refine their answers (Shinn et al., 2023; Madaan et al., 2023; Paul et al., 2024; Zhou et al., 2023a; Zheng et al., 2024) which involves a form of verification of the initial answer. Contrary to those works, we currently focus on single-round prompting that involves no refinement of the initial answer. Cobbe et al. (2021); Li et al. (2023); Weng et al. (2023); Liang et al. (2024) trained a separate verifier to score answers or reasoning steps - ultimately to be used in Max or Weighted aggregation policies. Zhou et al. (2023b) used an external theorem prover for verification while using LLMs for auto-formalization. Khalifa et al. (2023) used a fine-tuned separate verifier score in a step-wise greedy search. Multiple works (Gandhi et al., 2023; Yao et al., 2023; Xie et al., 2023; Qiu et al., 2024; Wan et al., 2024; Wang et al., 2024a; Sun et al., 2024; Wang et al., 2024b; Gao et al., 2024; Besta et al., 2024; Hao et al., 2024) explored the utility of using verifier scores as rewards or costs in a tree-search or beam-search approach. However, they either use fine-tuned or few shot-based verifiers. One exception is Miao et al. (2024) who perform zero-shot self-verification to adjust self-consistency similar to us. However, they use a complicated multi-stage prompting strategy for verification which can become exorbitantly costly when used with tree search approaches (Gao et al., 2024). Moreover, we explore a fully zero-shot regime with zero-shot prompt-based thought decomposition as well."}, {"title": "7 Conclusion", "content": "We have the following takeaways: (1) prompts like PS+ or TAB COT are not necessarily systematically better than COT. (2) COT STEP offers an elegant zero-shot strategy to decompose reasoning steps virtually without any accuracy loss compared to COT. (3) Zero-shot COT prompt (COTR-prompt) is also useful for verification particularly in mathematical domain. (4) Zero-shot verifier scores do not particularly help in augmenting self-consistency. (5) Using zero-shot verifier scores to guide reasoning step search in a step-wise stochastic greedy manner can be helpful but its benefit compared to plain COT disappears when using self-consistency. Beam search did not help either. (6) The verifier works better with COT STEP-based decomposition than TAB COT."}, {"title": "8 Limitations", "content": "We utilize an automatic way of constructing correctness classification dataset. However, this method is not perfect - for it assumes that just because the predicted answer is correct the reasoning chain as a whole is correct. In some cases that may not be true (Lanham et al., 2023). However, this is a trade-off to make to get the benefit of automaticity. There exists some data with step-level ground truth verification scores (Cobbe et al., 2021; Lightman et al., 2024) however only the automated construction offers a way to conveniently evaluate the verification of different prompts for reasoning step generation."}]}