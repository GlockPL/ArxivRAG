{"title": "SGFormer: Single-Layer Graph Transformers with Approximation-Free Linear Complexity", "authors": ["Qitian Wu", "Kai Yang", "Hengrui Zhang", "David Wipf", "Junchi Yan"], "abstract": "Learning representations on large graphs is a long-standing challenge due to the inter-dependence nature. Transformers recently have shown promising performance on small graphs thanks to its global attention for capturing all-pair interactions beyond observed structures. Existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated architectures by stacking deep attention-based propagation layers. In this paper, we attempt to evaluate the necessity of adopting multi-layer attentions in Transformers on graphs, which considerably restricts the efficiency. Specifically, we analyze a generic hybrid propagation layer, comprised of all-pair attention and graph-based propagation, and show that multi-layer propagation can be reduced to one-layer propagation, with the same capability for representation learning. It suggests a new technical path for building powerful and efficient Transformers on graphs, particularly through simplifying model architectures without sacrificing expressiveness. As exemplified by this work, we propose a Simplified Single-layer Graph Transformers (SGFormer), whose main component is a single-layer global attention that scales linearly w.r.t. graph sizes and requires none of any approximation for accommodating all-pair interactions. Empirically, SGFormer successfully scales to the web-scale graph OGBN-PAPERS100M, yielding orders-of-magnitude inference acceleration over peer Transformers on medium-sized graphs, and demonstrates competitiveness with limited labeled data.", "sections": [{"title": "INTRODUCTION", "content": "LEARNING on on large graphs that connect interdependent data points is a fundamental challenge in machine learning and pattern recognition, with a broad spectrum of applications ranging from social sciences to natural sciences [1], [2], [3], [4], [5]. One key problem is how to obtain effective node representations, i.e., the low-dimensional vectors (a.k.a. embeddings) that encode the semantic and topological features, especially under limited computation budget (e.g., time and space), that can be efficiently utilized for downstream tasks.\nRecently, Transformers have emerged as a popular class of foundation encoders for graph-structured data by treat- ing nodes in the graph as input tokens and have shown highly competitive performance on graph-level tasks [6], [7], [8], [9], [10] and node-level tasks [11], [12], [13], [14] on graph data. The global attention in Transformers [15] can capture implicit inter-dependencies among nodes that are not embodied by input graph structures, but could potentially make a difference in data generation (e.g., the undetermined structures of proteins that lack known tertiary structures [16], [17]). This advantage provides Transformers with the desired expressivity for capturing e.g., long-range dependencies and unobserved interactions, and leads to superior performance over graph neural networks (GNNs) in small-graph-based applications [17], [18], [19], [20], [21].\nHowever, a concerning trend in current architectures is their tendency to automatically adopt the design philosophy of Transformers used in vision and language tasks [22], [23], [24]. This involves stacking deep multi-head attention layers, which results in large model sizes and the data-hungry nature of the model. However, this design approach poses a significant challenge for Transformers in scaling to large graphs where the number of nodes can reach up to millions or even billions, particularly due to two-fold obstacles.\n1) The global all-pair attention mechanism is the key component of modern Transformers. Because of the global attention, the time and space complexity of Transformers often scales quadratically with respect to the number of nodes, and the computation graph grows exponentially as the number of layers increases. Thereby, training deep Transformers for large graphs with millions of nodes can be extremely resource-intensive and may require delicate techniques for partitioning the inter-connected nodes into smaller mini-batches in order to mitigate computational overhead [12], [13], [14], [25].\n2) In small-graph-based tasks such as graph-level pre- diction for molecular property [26], where each instance is a graph, and there are typically abundant labeled graph instances, large Transformers may have sufficient supervision for generalization. However, in large-graph-based tasks such as node-level prediction for protein functions [27], where there is usually a single graph and each node is an instance, labeled nodes can be relatively limited. This increases the difficulty of Transformers with complicated architectures in learning effective representations in such cases.\nThis paper presents an attempt to investigate the necessity"}, {"title": "2 PRELIMINARY AND BACKGROUND", "content": "In this section, we introduce notations as building blocks of the analysis and the proposed model. In the meanwhile, we briefly review the literature related to the present work.\nNotations. We denote a graph as G = (V,E) where the node set V comprises N nodes and the edge set E = {(u,v) | Auv = 1} is defined by a symmetric (and usually sparse) adjacency matrix A = [auv]N\u00d7N, where auv = 1 if node u and v are connected, and 0 otherwise. Denote by D = diag({du}-1) the diagonal degree matrix of A, where du =  \u03a3\u03c5=1 1 auv. Each node has a D-dimensional input feature vector xu \u2208 RD and a label yu which can be a scalar or a vector. The nodes in the graph are only partially labeled, forming a node set denoted as Vtr CV (wherein |Vtr << N). Learning representations on graphs aims to produce node embeddings zu \u2208 Rd that are useful for downstream tasks. The size of the graph, as measured by the number of nodes N, can be arbitrarily large, usually ranging from thousands to billions."}, {"title": "2.1 Graph Neural Networks", "content": "Graph Neural Networks (GNNs) [28], [29] compute node embeddings through message passing rules over observed structures. The layer-wise message passing of GNNs can be defined as recursively propagating the embeddings of neighboring nodes to update the node representation:\nz(k+1) = n(k) (z(k+1)), z(k+1) = Agg({z(k) |v\u2208 R(u)}),\n(1)\nwhere zuk) \u2208 Rd denotes the embedding at the k-th layer, \u03b7 denotes (parametric) feature transformation, and Agg is an aggregation function over the embeddings of nodes in R(u). The latter is the receptive field of node u determined by G. Common GNNs, such as GCN [29] and GAT [30], along with their numerous successors, e.g., [31], [32], [33], [34], [35], typically assume R(u) to be the set of neighboring nodes in G. By stacking multiple layers of local message passing as defined by (1), the model can integrate information from the local neighborhood into the representation.\nHowever, since the number of neighboring nodes in- volved in the computation exponentially increases as the layer number goes up, the aggregated information from dis- tant nodes will be diluted with an exponential rate w.r.t. the model depth. This problem referred to as over-squashing [36] can limit the expressiveness of GNNs for learning effective representations. Moreover, recent evidence suggests that GNNs yield unsatisfactory performance in the case of graphs with heterophily [33], long-range dependencies [37] and structural incompleteness [38]. This urges the community to explore new architectures that can overcome the limitations of GNNs' local message passing."}, {"title": "2.2 Graph Transformers", "content": "Beyond message passing within local neighborhoods, Trans- formers have recently gained attention as powerful graph encoders [6], [7], [8], [9], [10], [12], [39], [40], [41]. These models use global all-pair attention, which aggregates all node embeddings to update the representation of each node:\nz(k+1) = n(k) (z(k+1)), z(k+1) = Agg({z(k)|v\u2208V}).\n(2)\nThe global attention can be seen as a generalization of GNNs' message passing to a densely connected graph where R(u) = V, and equips the model with the ability to capture unobserved interactions and long-range dependence.\nHowever, the all-pair attention incurs O(N2) complexity and becomes a computation bottleneck that limits most Transformers to handling only small-sized graphs (with up to hundreds of nodes). For larger graphs, recent efforts have resorted to strategies such as sampling a small (relative to N) subset of nodes for attention computation [42] or using ego-graph features as input tokens [11], [13]. These strategies sacrifice the expressivity needed to capture all- pair interactions among arbitrary nodes. Another line of recent works designs new attention mechanisms that can efficiently achieve all-pair message passing within linear complexity [10], [12]. Nevertheless, these schemes require approximation that can lead to training instability.\nAnother observation is that nearly all of the Transformers mentioned above tend to stack deep attention layers, in line with the design of large models used in vision and language tasks [22], [23], [24]. However, this architecture presents challenges for scaling to industry-scale graphs, where N can reach billions. Moreover, due to the complicated architecture, the model can become vulnerable to overfitting when the number of labeled nodes |Vtr | is much smaller than N. This is a common issue in extremely large graphs where node labels are scarce [27]. The question remains how to build an efficient and scalable Transformer model that maintains the desired expressiveness for learning effective graph representations."}, {"title": "2.3 Node-Level v.s. Graph-Level Tasks", "content": "Before going to our methodology, we would like to pinpoint the differences between two graph-based predictive tasks of wide interest. Node-level tasks (our focus) target a single graph connecting all the instances as nodes where each instance has a label to predict. Differently, in graph-level tasks, each instance (e.g., molecule) itself is a graph with a label, and graph sizes are often small, in contrast with the arbitrarily large graph in node classification depending on the number of instances in a dataset. The different input scales result in that the two problems often need disparate technical considerations [27]. While GNNs exhibit comparable competitiveness in both tasks, most of current Transformers are tailored for graph classification (on small graphs) [6], [7], [8], [9], [10], [39], [40], [41], [43], and it still remains largely under-explored to design powerful and efficient Transformers for node-level tasks on large graphs [11], [12], [44]."}, {"title": "3 THEORETICAL ANALYSIS AND MOTIVATION", "content": "Before we introduce the proposed model, we commence with motivation from the theoretical perspective which sheds"}, {"title": "3.1 A Hybrid Model Backbone", "content": "Denote by Z(k) = [z]N=1 \u2208 RNxd the stack of N nodes' embeddings at the k-th layer. We consider generic message- passing networks, which can unify the layer-wise updating rules of common GNNs and Transformers as a propagation layer with self-loop connection (a.k.a. residual link):\nZ(k+1) = P(k)Z(k)W(k) + \u03b2Z(k)W(k),\n(3)\nwhere \u03b2 \u2265 0 is a weight on the self-loop path, P(k) = [P]NXN denotes the propagation matrix at the k-th layer and W(k) \u2208 Rdxd denotes the layer-specific trainable weight matrix for feature transformation. For GNNs, the propagation matrix is commonly instantiated as a fixed sparse matrix, e.g., the normalized graph adjacency. For Transformers, P(k) becomes a layer-specific dense attention matrix computed by Z(k).\nPropagation Layers as Optimization Dynamics. The following theorem shows that, under mild conditions, the updating rule defined by Eqn. 3 is essentially an optimiza- tion step on a regularized energy that promotes a certain smoothness effect for graph signal denoising.\nTheorem 1. For any propagation matrix P(k) = [p]N\u00d7N and symmetric weight matrix W(k), Eqn. 3 is a gradient descent step with step size for the optimization problem w.r.t. the quadratic energy: E(Z; Z(k), P(k), W(k)) \uc2a4\n\u03a3Pzu - Zullw(k) +\u03a3||zu-(\u03b2+d(k))W(k)z(k) ||,\n(4)\n\u03ba,\u03c5\n\u03ba,\u03c5\nwhere dak) = 1 1p and the weighted vector norm is defined by ||x||w = x Wx.\nN\nN\nProof. We denote by D(k) = diag({d()}_1) and A(k) = (k) - P(k). The first term in E(Z; Z(k), P(k), W(k)) can be expressed as tr(ZTA(k) ZW(k)) and its gradient w.r.t. Z can be computed by\ntr(ZTA(k)ZW(k))\n\u2202Z\n=\n\u25b3(k)z. (W(k) + (W(k))).\n(5)\nGiven the symmetric property of W(k), we have the gradient of E(Z; Z(k), P(k), W(k)) evaluated at the point Z = Z(k):\nJE(Z; Z(k), P(k), W(k))\n\u018fZ\nZ=Z(k)\n=2(k)Z(k) W(k) +2 [2(k) \u2013 (BI + D(k))Z(k) W(k)]\n=2Z(k) - 2\u03b2Z(k)W(k) _ 2P(k)z(k)W(k),\n(6)\nwhere I denotes the N \u00d7 N identity matrix. Using gradient descent with step size to minimize E(Z; Z(k), P(k), W(k)) at the current layer yields an updating rule:\nZ(k+1) = Z(k) _ 1 dE(Z; Z(k), P(k), W(k))\n2\n\u018fZ\n= P(k)z(k)W(k) + \u03b2Z(k)W(k).\n(7)\nWe thus conclude the proof for the theorem.\nThe assumption of symmetric W(k) can, to some extent, limit the applicability of this theorem, whereas, as we show later, the conclusion can be generalized to the case involving arbitrary W(k) \u2208 Rd\u00d7d. Now we discuss the implications of Theorem 1. The first term of Eqn. 4 can be written as\n\u03ba,\u03c5\n\u03ba,\u03c5\n\u2211pe || Zu - Zollw (k) = \u03a3\u03a1) (Zu - Z) TW(k) (Zu - Zv),\n(8)\nwhich can be considered as generalization of the Dirichlet energy [48] defined over a discrete space of N nodes where the pairwise distance between any node pair (u, v) is given by puv puk and the signal smoothness is measured through a weighted space || . || w(*). The second term of Eqn. 4 aggregates the square distance between the updated (k)\nnode embedding zu and the last-layer embedding zu after transformation of (\u03b2 + d(k))W(k). Overall, the objective of Eqn. 4 formulates a graph signal denoising problem defined over N nodes in a system that aims at smoothing the node embeddings via two-fold regularization effects [49] (as illustrated in Fig. 1(a)): the first term penalizes the global smoothness among node embeddings through the proximity defined by P(k); the second term penalizes the change of node embeddings from the ones prior to the propagation.\nThe theorem reveals that while the layer-wise updating rule adopted by either GNNs or Transformers can be unified as a descent step on a regularized energy, these two model (k) backbones contribute to obviously different smoothness ef- fects. For GNNs that use graph adjacency as the propagation matrix, in which situation puk = 0 for (u, v)'s that are disconnected in the graph, the energy only enforces global smoothness over neighboring nodes in the graph. In contrast, Transformers using all-pair attention induce the energy regularizing the global smoothness over arbitrary node pairs.\nThe latter breaks the restriction of observed graphs and can facilitate leveraging the unobserved interactions for better representations. On the other hand, all-pair attention discards the input graph, which can play a useful inductive bias role in learning informative representations (especially when the observed structures strongly correlate with downstream labels). In light of the analysis, we next consider a hybrid propagation layer that synthesizes the effect of both models."}, {"title": "A Hybrid Model Backbone", "content": "We define a model backbone with the layer-wise updating rule comprised of three terms:\nZ(k+1) = (1-a)P(k)Z(k)W(k) +aPGZ(k)W(k) +\u03b2Z(k) W(k),\n(9)\nwhere Pk is an all-pair attention-based propagation matrix specific to the k-th layer, Pg is a sparse graph-based propa- gation matrix (associated with input graph G), and 0 < a < 1 is a weight. We assume P(k) = [C] NXN and PG = [Wuv]N\u00d7N. Particularly, the hybrid model can be treated as an extension of Eqn. 3 where P(k) = (1 \u2212 a)P(k) + P(k) and specifically\np(k) =\n{\n(1 \u2212 a)ck) + \u03b1\u03c9\u03bd, if (u, v) \u2208 \u0395\n(1-a)cu), if (u,v) \u2209 E.\n(10)\nWe can extend the result of Theorem 1 and naturally derive the regularized energy optimized by the hybrid model.\nCorollary 1. For any attention-based propagation matrix P(k) and graph-based propagation matrix PG, if W(k) is a symmet- ric matrix, then Eqn. 9 is a gradient descent step with step size for the optimization problem w.r.t. the quadratic energy E(Z; Z(k), P(K), PG, W(k))\n\u03a3[(1-a)|| zu \u2013 zvw(k) + awuv || Zu - Zvw()]+\n(11)\n\u03a3|| zu - (3+ (1 \u2212 a)d(k) + adu) W()()||,\n\u03ba,\u03c5\n\u03ba,\u03c5\n\u03ba,\u03c5\n2\nwhere (k) = 1 caw and du = 2-1 Wuv.\nN\nN\nProof. The proof for this corollary can be adapted by Theo- rem 1 with the similar reasoning line.\nThe hybrid model is capable of accommodating observed structural information and in the meanwhile capturing unobserved interactions beyond input graphs. Such an architectural design incorporates the graph inductive bias into the vanilla Transformer and is adopted by state-the- of-art Transformers on graphs, e.g., [10], [12], [14], that show superior performance in different tasks of graph representation learning.\nGeneralization to Asymmetric Weight Matrix. The above analysis assumes the weight matrix W(k) to be symmetric, which may limit the applicability of the conclusions since in common neural networks the weight matrix can potentially take any value in the entire Rd\u00d7d. In our context, it can be difficult to directly analyze the case of asymmetric W(k) and derive any closed form of the energy. However, the following proposition allows us to generalize the conclusion of Theorem 1 to arbitrary W(k) \u2208 Rdxd.\nProposition 1. For any weight matrix W(k) \u2208 Rdxd, there exists a symmetric matrix W(k) \u2208 Rdxd such that the up- dated embeddings \u017d(k+1) = P(k)Z(k)\u0174(k) + \u1e9eZ(k)\u0174(k) yield ||\u017d(k+1) - Z(k+1) || < \u20ac, \u2200\u20ac > 0.\nProof. We extend the proof of Theorem 9 in [50] to our case. The updating rule considered in Lemma 26 of [50] can be replaced by our updating rule Z(k+1) = P(k)Z(k)W(k) + B2(k) W(k), so that the updated embeddings Z(k+1) are continuous w.r.t. W(k). Then similar to Theorem 27 of [50], we can prove that for any weight matrix W(k) \u2208 Rdxd there exist \u017d(k+1) \u2208 CN\u00d7d, right-invertible T \u2208 Cd'\u00d7d and herimitia W(k) \u2208 CNxd' such that 2(k+1) = P(k)Z(k) W(k) +\n||\u017d(k+1) _ Z(k+1) || < \u20ac, \u2200\u20ac > 0.\n(12)\nBy assuming T\u2208 Cdxd, we prove the conclusion in the complex domain. Then we can use the same technique as the proof after Theorem 27 in Appendix E.3 of [50] to generalize the conclusion from the complex domain to the real domain.\nThis suggests that for any propagation layer (as defined by Eqn. 3) with W(k) \u2208 Rdxd that is even asymmetric, we can find a surrogate matrix W(k) that is symmetric, such that the latter produces the node embeddings which can be arbitrarily close to the ones produced by W(k). Pushing further, one-layer updates of the message passing model corresponds to a descent step on the regularized energy E(Z; Z(k), P(k), \u0174(k))."}, {"title": "3.2 Reduction from Multi-Layer to One-Layer", "content": "The analysis so far targets the updates on embeddings of one propagation layer, yet the model practically used for computing representations often stacks multiple propagation layers. While using deep propagation may endow the model with desired expressivity, it also increases the model complexity and hinders its scalability to large graphs. We next zoom in on whether using multi-layer propagation is a necessary condition for satisfactory expressiveness for learning representations. On top of this, the analysis suggests a potential way to simplify the Transformer architecture for learning on large graphs.\nMulti-Layer v.s. One-Layer Models. The analysis in Sec. 3.2 reveals the equivalence between the embedding updates of one propagation layer and one-step gradient descent on the regularized energy. Notice that since the attention matrix P(k) (dependent on Z(k)) and the feature transformation W() vary at different layers, the energy objective optimized by the model (Eqn. 3 or Eqn. 9) is also specific to each layer. In this regard, the multi-layer model, which is commonly adopted by existing Transformers, can be seen as a cascade of descent steps on layer-dependent energy objectives. From this viewpoint, there potentially exists certain redundancy in the optimization process for graph signal processing, since the descent steps of different layers pursue different targets and may interfere with each other. To resolve this issue, we introduce the next theorem that further suggests a principled way to simplify the Transformer model, and particularly, we can construct a single-layer model that yields the same denoising effect as the multi- layer counterpart.\nTheorem 2. For any K-layer model (where K is an arbitrary pos- itive integer) whose layer-wise updating rule is defined by Eqn. 9 producing the output embeddings Z(K), there exists a (sparse) graph-based propagation matrix P = [Wow]N\u00d7N, a dense attention-based propagation matrix P = [C] N\u00d7N, and a sym- metric weight matrix W* \u2208 Rd\u00d7d such that one gradient descent"}, {"title": "4 PROPOSED MODEL", "content": "In this section, we introduce our model, referred as Simplified Graph Transformer (SGFormer), under the guidance of our theoretical results in Sec. 3 (as distilled in Sec. 3.3). Overall, the architectural design adopts the hybrid model in Eqn. 19 and follows the Occam's Razor principle for specific instantiations. Particularly, SGFormer only requires O(N) complexity for accommodating the all-pair interactions and computing N nodes' representations. This is achieved by a simple attention function which has advanced computational efficiency and is free from any approximation scheme. Apart from the scalability advantage, the light-weighted architecture endows SGFormer with desired capability for learning on large graphs with limited labels."}, {"title": "4.1 Model Design", "content": "We first use a neural layer to map input features X = [x] N1 to node embeddings in the latent space, i.e., Z(0) = f(X) where f1 can be a shallow (e.g., one-layer) MLP. Then based on the result of Sec. 3.2, particularly the single-layer model presented in Corollary 2, we consider the following hybrid architecture for updating the embeddings:\nZout = (1 - a)AN(Z(0)) + aGN(Z(0), A),\n(21)\nwhere 0 < a < 1 again is a hyper-parameter, and AN and GN denote a global attention network and a graph- based propagation network, respectively. Then the node representations Zout are fed into an output neural layer for prediction Y = fo(Zout), where fo is a fully-connected layer in our implementation. We next delve into the detailed instantiations of AN and GN.\nSimple Global Attention Network. There exist many potential choices for global attention functions as the instanti- ation of AN(Z(0)), e.g., the widely adopted Softmax attention that is originally used by [15]. While the Softmax attention possesses provable expressivity [51], it requires O(N2) com- plexity for computing the all-pair attentions and updating the representations of N nodes. This computational bottleneck hinders its scalability for large graphs. Alternatively, we introduce a simple attention function which can reduce the computational complexity to O(N) and still accommodate all-pair interactions. Specifically, with the initial embeddings Z(0) as input, we first use feature transformations fQ, \u0130K and fv to obtain the key, query and value matrices, respectively, as is done by common Transformers:\nQ = fo (Z(0)), K = fk(Z(0)), V = fv (Z(0)),\n(22)\nwhere fQ, fk and fv are instantiated as a fully-connected layer in our implementation. Then we consider the attention function that computes the all-pair similarities:\nC = K\n\\||Q||F ||K||F\n, Q = Q + 1 diag Q(K1),\n(23)\nC = diag\u00af\u00b9 (C1) \u00b7 C,\n(24)\nwhere 1 is an N-dimensional all-one column vector. In Eqn. 23 the scaling factor can improve the numerical stability and the addition of a self-loop can help to strengthen the role of central nodes. Eqn. 24 serves as row-normalization which is commonly used in existing attention designs. If one uses the attention matrix C to compute the updated embeddings, i.e., ZAN = CV, the computation requires O(N2) complexity, since the computation of the attention matrix (Eqn. 23) and the updated embeddings both needs the cost of O(N2). Notably, the simple attention function allows an alternative way for computing the updated embeddings via changing the order of matrix products. In specific, assume Q = Q\nQF and K = K, and we can rewrite the computation flow of the attention-based propagation:\nN = diag-1 (1+Q(K1)),\n(25)\nZAN = N\u22c5 [V + Q(KTV)].\n(26)\nOne can verify through basic linear algebra that the result of Eqn. 26 is equivalent to the one obtained by ZAN = CV which explicitly computes the all-pair attention. In other words, while the computation flow of Eqn. 26 does not compute the all-pair attention matrix, it still accommodates the all-pair interactions as the original attention. More importantly, the computation of Eqn. (26) can be achieved in O(N) complexity, which is much more efficient than using the original computation flow. Therefore, such a simple"}, {"title": "Algorithm 1 Feed-forward and Training of SGFormer.", "content": "1: Input: Node feature matrix X, input graph adjacency matrix A, labels of training nodes Ytr, weight on graph- based propagation a.\n3:\n2: while not reaching the budget of training epochs do\nEncode input node features Z(0) = f1(X);\nCompute query, key and value matrices Q = f\u0119(Z(0)), K = fk (Z(0)) and V = fv(Z(0));\n4:\n5:\nCompute normalization Q = Q and K =\nK\nCompute denominator of global attention N\ndiag\u00af\u00b9 (I+Q(K1));\n6:\n=\n7:\nCompute updated embeddings by global attention ZAN = N\u22c5 [V + Q(KTV)];\nCompute final representations by graph-based propa- gation Zout = (1 - a)ZAN + QGN(Z10), A);\nCalculate predicted labels Y = fo (Zout);\nCompute the supervised loss L from Ytr and Ytr;\nUse L to update the trainable parameters;\n8:\n9:\n10:\n11:\n12: end while\ntheir architectures, expressivity, and scalability. Most existing Transformers have been developed and optimized for graph classification tasks on small graphs, while some recent works have focused on Transformers for node classification, where the challenge of scalability arises due to large graph sizes.\n\u2022 Architectures. Regarding model architectures, some ex- isting models incorporate edge/positional embeddings (e.g., Laplacian decomposition features [6], degree centrality [9], Weisfeiler-Lehman labeling [7]) or utilize augmented training loss (e.g., edge regularization [12], [41]) to capture graph information. However, the positional embeddings require an additional pre-processing procedure with a complexity of up to O(N\u00b3), which can be time- and memory-consuming for large graphs, while the augmented loss may complicate the optimization process. Moreover, existing models typi- cally adopt a default design of stacking deep multi-head attention layers for competitive performance. In contrast, SGFormer does not require any of positional embeddings, augmented loss or pre-processing, and only uses a single-layer, single-head global attention, making it both efficient and lightweight.\n\u2022 Expressivity. There are some recently proposed graph Transformers for large graphs [11], [13], [44] that limit the attention computation to a subset of nodes, such as neighboring nodes or sampled nodes from the graph. This approach allows linear scaling w.r.t. graph sizes, but sacrifices the expressivity for accommodating all-pair interactions. In contrast, SGFormer maintains attention computation over all N nodes in each layer while still achieving O(N) complexity. Moreover, unlike NodeFormer [12] and GraphGPS [10] which rely on random feature maps as approximation, SGFormer does not require any approximation or stochastic components and is more stable during training.\n\u2022 Scalability. In terms of algorithmic complexity, most existing graph Transformers have O(N2) complexity due to global all-pair attention, which is a critical computational bottleneck that hinders their scalability even for medium- sized graphs with thousands of nodes. While neighbor sampling can serve as a plausible remedy, it often sacrifices"}, {"title": "4.2 Comparison with Existing Models", "content": "We next provide a more in-depth discussion comparing our model with prior art and illuminating its potential in wide application scenarios. is a head-to- head comparison of current graph Transformers in terms of"}, {"title": "5 EMPIRICAL EVALUATION", "content": "We apply SGFormer to real-world graph datasets whose predictive tasks can be modeled as node-level prediction. The latter is commonly used for effectiveness evaluation of learning graph representations and scalability to large graphs. We present the details of implementation and datasets in Sec. 5.1. Then in Sec. 5.2, we test SGFormer on medium- sized graphs (from 2K to 30K nodes) and compare it with an extensive set of expressive GNNs and Transformers. In Sec. 5.3, we scale SGFormer to large-sized graphs (from 0.1M to 0.1B nodes) where its superiority is demonstrated over scalable GNNs and Transformers. Later in Sec. 5.5, we further compare the performance with different ratios of labeled data. In addition, we compare the model's time and space efficiency and scalability in Sec. 5.4. In Sec. 5.6, we analyze the impact of several key components in our model. Sec. 5.7 provides further discussions on how the single-layer model performs compared with the multi-layer counterpart."}, {"title": "5.1 Experiment Details", "content": "Datasets. We evaluate the model on 12 real world datasets with diverse properties. Their sizes", "55": "where the graphs have high homophily ratios", "56": "SQUIRREL", "57": "and DEEZER-EUROPE [58", "27": "the item co-occurrence network AMA- ZON2M [59", "60": ".", "29": "with shallow (e.g.", "training": "the whole graph dataset is fed into the model during training and inference. For large-sized graphs"}, {"27": "we feed the whole graph into the model using CPU", "space": "learning rate within {0.001, 0.005, 0.01, 0.05, 0.1}, weight decay within {1e \u2013 5, 1e \u2013 4, 5e \u2013 4, 1\u0435 \u2013 3, 1\u0435 \u2013 2}, hidden size within {32, 64, 128, 25"}]}