{"title": "ViPCap: Retrieval Text-Based Visual Prompts for Lightweight Image Captioning", "authors": ["Taewhan Kim", "Soeun Lee", "Si-Woo Kim", "Dong-Jin Kim"], "abstract": "Recent lightweight image captioning models using retrieved data mainly focus on text prompts. However, previous works only utilize the retrieved text as text prompts, and the visual information relies only on the CLIP visual embedding. Because of this issue, there is a limitation that the image descriptions inherent in the prompt are not sufficiently reflected in the visual embedding space. To tackle this issue, we propose ViPCap, a novel retrieval text-based visual prompt for lightweight image captioning. ViPCap leverages the retrieved text with image information as visual prompts to enhance the ability of the model to capture relevant visual information. By mapping text prompts into the CLIP space and generating multiple randomized Gaussian distributions, our method leverages sampling to explore randomly augmented distributions and effectively retrieves the semantic features that contain image information. These retrieved features are integrated into the image and designated as the visual prompt, leading to performance improvements on the datasets such as COCO, Flickr30k, and NoCaps. Experimental results demonstrate that ViPCap significantly outperforms prior lightweight captioning models in efficiency and effectiveness, demonstrating the potential for a plug-and-play solution. The source code is available at https://github.com/taewhankim/VIPCAP.", "sections": [{"title": "Introduction", "content": "Vision and language (V&L) tasks, such as image captioning, have advanced with large-scale models like SimVLM (1.4B), PaLi (3B), and REVEAL (2.1B) (Wang et al. 2022a; Chen et al. 2023; Hu et al. 2023). However, these advanced multimodal models require a large number of parameters, resulting in high computational and dataset construction costs.\nTo enhance training efficiency, recent works suggest focusing on learning a mapping network, such as MAPL and BLIP-2 (Ma\u00f1as et al. 2023; Li et al. 2023a), that bridges the modality gap between images and text or training learnable tokens like EVCap (Li et al. 2024). For example, BLIP-2 introduces the Qformer, which aligns the two modalities while keeping a V&L models frozen. Nevertheless, despite the efficiency of this approach, BLIP-2 still requires over 1B trainable parameters. Although EVCap trains learnable tokens with few trainable parameters, EVCap and MAPL depend on high-performance models like EVA-CLIP (Sun et al. 2023) and Vicuna (Chiang et al. 2023), with over 5B total parameters as shown in Fig. 1b.\nRecent retrieval based models like SmallCap, EXTRA, and Re-ViLM (Ramos et al. 2023; Ramos, Elliott, and Martins 2023; Yang et al. 2023) are emerging to reduce computational costs using external knowledge. These models retrieve texts semantically similar to input images and use them as only text prompts. However, there is a limitation in that visual information relies only on the CLIP vision encoder. As depicted in Fig. 2, SmallCap, which uses retrieval caption as a text prompt without visual prompts, encounters an issue where it can not contain detailed visual descriptions in a caption. We suspect this is because the image description in text prompt is not utilized as visual information.\nIn this paper, we propose a retrieval text-based visual prompt for lightweight image captioning (ViPCap), leveraging retrieved texts describing image information as visual prompts. First, given that the retrieved text provides a comprehensive image description, we encode the text prompt into the CLIP embedding space and transform it into patch-level hidden representations to extract semantic information. To effectively enhance local visual representations using a global text representation, we randomly augment the semantic representation from the text prompt. In particular, we assume that the embedding vector follows a randomized Gaussian distribution and extract M semantic features from this distribution as the basis for a visual prompt.\nPrevious works, including CapDec and LinCIR (Nukrai, Mokady, and Globerson 2022; Gu et al. 2024), address the modality gap in V&L tasks using Gaussian distributions. Based on this approach, we propose modeling text features following a randomized Gaussian distribution. Unlike the heuristic approaches like CapDec, our approach generates semantic features sampled from a learnable distribution, aiming for a high correlation with visual features. We assume these semantic features contain visual information and expect them to closely resemble the input image features. To achieve this, we employ a patch retrieval module that aligns semantic features with each input image patch.\nThe retrieved patch features are combined with the input image features to generate the visual prompt is added with the image features before decoder input. This approach aims to enhance the model's ability to capture relevant visual representations. To the best of our knowledge, our work is the first to utilize retrieval text as a visual prompt for lightweight image captioning.\nOur approach achieves superior performance on the COCO dataset (Lin et al. 2015) compared to our baseline model, SmallCap, and it significantly improves performance over previous lightweight models on the NoCaps dataset (Agrawal et al. 2019). In the experiments, we integrate our ViP module into retrieval-based models, text-only training models, and various prompts, resulting in consistent performance improvements.\nOur contribution can be summarized as follows: (1) We propose a novel visual prompt for lightweight image captioning models named ViPCap, which leverages retrieved texts to generate visual prompts. (2) We introduce the ViP module, which retrieves semantic information from text features and combines it with image features to generate the visual prompt. (3) Extensive experiments demonstrate that our method is efficient and outperforms previous models across datasets like COCO and NoCaps, regardless of the text prompt types used."}, {"title": "Related Work", "content": "CLIP in captioning. With the advent of vision-language models (Radford et al. 2021; Jia et al. 2021), significant advancements have been made in V&L tasks. Notably, CLIP achieves multimodal alignment with 400M image-text pairs. The image captioning models that use a clip-based encoder such as BLIP-2, LLaVA, and MiniGPT-4 (Li et al. 2023a; Liu et al. 2023; Zhu et al. 2023) create a mapping network and pass input features to the decoder. In our study, we map the CLIP text encoder to the randomized Gaussian distribution, desiring to achieve a high correlation with visual features and leveraging it to generate visual prompts,\nPrompt tuning. Initially used in NLP (Lester, Al-Rfou, and Constant 2021; Li and Liang 2021), this approach has extended to V&L and vision-only models (Jia et al. 2022; Wang et al. 2022b) to utilize pretrained knowledge for downstream without parameter training. Recent techniques focus on trainable tokens instead of handcrafted prompts (Gao, Fisch, and Chen 2021). Visual prompt tuning (Jia et al. 2022) adds learnable tokens to ViT for downstream tasks, while some methods (Bahng et al. 2022) integrate task-specific patches at the pixel level. Although previous methods achieved some success, they still face challenges with complex tasks like captioning. We introduce a visual prompt method suitable for captioning.\nRetrieval for captioning. External knowledge helps reduce the cost of creating large image-text datasets for Multimodal LLM. Methods like SmallCap (Ramos et al. 2023), LMCap (Ramos, Martins, and Elliott 2023), Knight (Wang et al. 2023), and MeaCap (Zeng et al. 2024) use text-based datasets and store captions in a datastore. SmallCap (Ramos et al. 2023), for example, generates captions using an input image and related text from a datastore with only 7 million trainable parameters, allowing fast and easy training,"}, {"title": "Proposed Method", "content": "Our model adopts SmallCap (Ramos et al. 2023) as a baseline, which retrieves enriched expressions from an external datastore and integrates the pretrained CLIP encoder (Radford et al. 2021) with GPT-2 (Radford et al. 2018) through cross-attention layers. SmallCap consists of the retrieved texts with the following hard prompt and inputs it into the decoder. Similarly, we utilize this approach in encoding our prompts: Similar images show {caption\u2081}...{captionk}. This image shows However, SmallCap primarily focuses on text prompts, while visual representation relies solely on the performance of the vision encoder.\nIn this work, we introduce ViPCap, a novel approach that enhances the performance of lightweight image captioning by generating visual prompts based on the semantic information in text prompts as illustrated in Fig. 3. ViP module encodes retrieved texts into the CLIP embedding space and converts them into patch-level representations. Then, assuming a multivariate Gaussian distribution, our method generates semantic features M times to obtain semantic features that are highly correlated to the local visual features. Unlike the heuristic Gaussian distribution approach in Capdec (Nukrai, Mokady, and Globerson 2022), our novel approach creates semantic features through a learnable distribution. Then, the semantic features are leveraged by the patch retrieval module to closely align with the input image patches. The matched semantic features are fused with the input image to generate the visual prompt. Finally, we train cross-attention layers to fuse the features in the decoder."}, {"title": "Randomized Gaussian Distribution Sampling", "content": "The ViP module aims to effectively sample the semantic information from the retrieved texts to enrich the input visual feature with semantic content in the form of visual prompts. Given retrieved texts T, our model encodes the retrieved text into D dimensional vector using pretrained CLIP text encoder $\\phi(\\cdot)$. Also, the CLIP image encoder embeds the input image into K dimensional visual features $V = {\\tilde{v_1}, \\tilde{v_2}, ...,\\tilde{v_N}} \\in \\mathbb{R}^{N\\times K}$ representing N number of patch-level visual features.\nWhen generating visual prompts, a single text feature might be insufficient to provide the necessary details to generate a visual prompt with complex patch-level local information. To address this, we employ a random augmentation techniques to sample semantic features from the Gaussian distribution. Also, instead of learning multiple mapping functions for local regions individually, we empirically find that sampling random vectors helps better match with visual local representations."}, {"title": "Patch Retrieval Module for Semantic Features", "content": "We estimate the parameters of distribution of the text embedding $\\tilde{\\mu}, \\tilde{\\sigma} \\in \\mathbb{R}^{K}$ assuming it follows a multivariate Gaussian distribution $\\mathcal{N}(\\tilde{\\mu}, \\tilde{\\sigma}^2I)$. We design functions $H_\\mu(\\cdot)$ and $H_\\sigma(\\cdot)$ to map text features into the mean and standard deviation of multivariate Gaussian distribution. These functions are implemented as MLP layers to map from D dimensions to K dimensions while sampling the mean and standard deviation ($H : \\mathbb{R}^{D} \\rightarrow \\mathbb{R}^{K}$). We empirically find that adding an additional learnable vector $w_{add}$ with a hyperparameter $\\alpha$ as a scaling factor to the MLP shows better performance and captures complex data structures more effectively. The $\\alpha$ is used to expand the range of the learnable vector. Let $\\tilde{\\mu}$ and $\\tilde{\\sigma}$ are computed via $\\tilde{\\mu} = H_\\mu(\\phi(T)) + \\alpha \\cdot w_{add}$, and $\\tilde{\\sigma} = H_\\sigma(\\phi(T))$, respectively.\nViP module samples M number of semantic features from this learnable Gaussian distribution to obtain semantic features that are highly correlated to the local visual embedding. We define the set of semantic representation $G \\in \\mathbb{R}^{M\\times K}$ obtained from the text features as:\n$G = {\\tilde{g_i} \\sim \\mathcal{N}(\\tilde{\\mu}, \\tilde{\\sigma}^2I; \\phi(T))}_{i=1}^M$                                                                                                (1)\nAdditionally, increasing M allows semantic features G to deliver better fine-grained visual information to the input feature (See more details in Ablation. ). Through the reparameterization trick (Kingma and Welling 2022), $\\tilde{g}$ can be re-formulated as $\\tilde{g} = \\tilde{\\mu} + \\tilde{\\sigma} \\cdot \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, I)$"}, {"title": "Patch Retrieval Module for Semantic Features", "content": "We hypothesize that the semantic features $G = {\\tilde{g_1}, \\tilde{g_2},...,\\tilde{g_M}}$ sampled from the Gaussian distribution contain the textual information describing the image. To effectively leverage random semantic features, we retrieve semantic features that contain useful visual information related to visual prompts based on feature similarity. In particular, we introduce a patch retrieval module, as depicted in Fig. 4, that compares the similarity between image patch-level representations V and the semantic features G. This module employs cosine similarity, denoted as sim(,), to effectively identify the most relevant semantic information for each patch representation $v_j \\in V$. Consequently, ViPCap chooses N relevant vectors from M candidates, one responsible for each patch vector, with high similarity to the input feature, and generates R:\n$R = {\\tilde{g}_{I(j)}}_{j=1}^N \\in \\mathbb{R}^{N \\times K},$\nwhere $I(j) = \\mathop{\\arg\\max}\\limits_{i \\in [1:M]} sim(\\tilde{g_i}, v_j)$.                                                                                                                                    (2)\nThis simple calculation process extracts valuable information through R without any additional training. The extracted vectors have the potential to provide semantic representations that the vision encoder cannot offer."}, {"title": "Feature Fusion Network and Visual Prompts", "content": "After obtaining the representations R containing semantic information relevant to the visual features, we integrate them with the input image to enhance the visual feature to generate the visual prompt Z. We introduce a Feature Fusion Network (FFN) designed to effectively fuse the visual representations V and the retrieved semantic features R, as shown in Fig. 4, by bridging the gap between them.\nFFN is designed with a transformer architecture that incorporates both self-attention and cross-attention layers. Unlike previous mapping networks (Li et al. 2023a; Mokady, Hertz, and Bermano 2021) that typically consist of 8 to 12 layers, this module uses only a single layer (l = 1). This efficiency results from earlier stages where features are sampled to closely align with visual representations, making a single layer sufficient for effective fusion. This network uses the input feature V as the query and the retrieved semantic representation R as the key.\nNote that a simple summation between V and R ignores distribution differences between features, and a simple concatenation cannot consider relational context between modalities (see Ablation. for more details).\nFinally, we obtain refined visual features V' with a simple summation (V' = V + Z). Because the visual prompt closely aligns with the distribution of image features, we enable model to generate the refined visual features through simple summation. This method allows models like SmallCap, which have a vision encoder and language decoder, to use refined visual feature V' instead of the input feature V, preserving the existing decoder design. This approach functions as a module that can be applied to various frameworks using encoders.\nThe decoder takes the input text embedding, while the refined visual feature V' is included conditionally when computing the loss function:\n$\\mathcal{L}_e = \\sum_{i=1}^{Q} \\log P_\\theta(y_i | y_{<i}, V'; \\theta)$.                                                                                                                                             (3)\nIn Eq. (3), the main difference in our approach is the use of refined visual feature V' instead of V. In the cross-attention layers ($\\theta$), weights are optimized by reducing the cross-entropy loss associated with forecasting the Q tokens in the given reference $y_1, ..., y_Q$."}, {"title": "Experiments", "content": "In this section, we conduct experiments to show the effectiveness of ViPCap over existing methods. First, we evaluate the advantages of our proposed model over the baseline in Table 1 for in-domain and out-of-domain, and then we apply the ViP module with the text-only training model to identify the role of ViP module in Table 2, respectively. In Table 3, ViP module evalutates performance of sampling method from different probability distributions. Moreover, we test the plug-and-play solution with different models in Table 4, Table 5, and with different prompt styles in Table 6."}, {"title": "Experimental Setup", "content": "Training dataset. We conduct experiments on image captioning benchmarks, i.e., COCO dataset (Lin et al. 2015), NoCaps (Agrawal et al. 2019), Flickr30k (Plummer et al. 2016). For COCO and Flickr30k, we follow the Karpathy split (Karpathy and Fei-Fei 2015) used in the image captioning. We evaluate our model on the COCO and Flickr30k test set and NoCaps validation and test datasets, as well as the cross-domain experiments.\nTraining setup. ViPCap includes a vision encoder (ViT-B/32) and a language decoder (GPT-2Base). Both are frozen during training. We train the ViP module and cross-attention layer. The FFN and cross-attention layer include a 12-head cross-attention layer with a single-layer block. To reduce the computational cost, we scale the dimension of cross-attention layers from 64 to 16. ViPCap requires 14M training parameters and is trained on a single NVIDIA 6000 GPU with a batch size of 128.\nDuring training, the ViP module uses a patch size of M=200, and hyperparameter \u03b1 is set to 5. The model selects three retrieved captions per image (k=3) from the COCO datastore due to the limitation of 77 context length size in CLIP. Captions are retrieved using CLIP ResNet-50x64 and processed with FAISS (Johnson, Douze, and J\u00e9gou 2017) for fast nearest neighbor search. Caption quality is evaluated using the metrics BLEU@4 (B@4) (Papineni et al. 2002), METEOR (M) (Denkowski and Lavie 2014), CIDEr (C) (Vedantam, Zitnick, and Parikh 2014), and SPICE (S) (Anderson et al. 2016)."}, {"title": "Main Results", "content": "In-domain. We evaluate ViPCap on the COCO, Flickr30k, and NoCaps datasets in Table 1. Table 1 shows the evaluation results on the COCO dataset. The upper part of Table 1 refers to the performance of large models trained on large datasets. In particular, compared to OSCARLarge, our model outperforms in B@4 score while using only 4% of the parameters. Our model, with 5 times fewer parameters than CaMEL, obtains the second-highest CIDEr score among lightweight models. In the COCO dataset, ViPCap exceeds the baseline, SmallCap, and outperforms SmallCapLarge, even though it has significantly fewer parameters (14M vs. 47M). The model also shows strong performance on the Flickr30k dataset.\nCross-domain. On the Nocaps validation dataset in the Table 1, it achieves a CIDEr score of 93.8 in in-domain data, surpassing all lightweight models. ViPCap exhibits superior performance in both in-domain and cross-domain, notably outperforming the previous SOTA models by over 3 points across most of metrics in the cross-domain. Additionally, it exceeds the large-scale training model, OscarLarge, by more than 10 points on in-domain data, indicating further performance in entire data. Table 1 shows that ViPCap is highly suitable for zero-shot tasks and real-world scenarios.\nAdditional vector design. We add vectors to enable the model to capture more complex data structures when estimating the Gaussian distribution. In the Table 3, we experiment by adding a vector to the semantic features across various design cases. No noise is adding nothing. We empirically find that our method enhances performance compared to not adding a vector or adding a vector sampled from a different distribution during the sampling process."}, {"title": "ViP Module Capability", "content": "We explore different model sizes and prompt styles to evaluate the capabilities of our model.\nPlug-and-Play manner. In Table 2, we evaluate the COCO, Flickr30k, and NoCaps test datasets to explicitly demonstrate the visual prompt ability by the ViP module. We combine our module with text-only training models.\nAs a result, applying the ViP module to CapDec and ViECap (Nukrai, Mokady, and Globerson 2022; Fei et al. 2023) improves performance, demonstrating that the ViP module can function as both a visual prompt and an image feature. The ViP module easily fuses with the vision encoders without modifying the framework. CapDec and ViECap with the ViP module result in an average increase of 3.5 points in CIDEr score on cross-domain in the NoCaps dataset. Our method shows the capability of ViP module in zero-shot tasks across real-world scenarios. We do not test with DeCap (Li et al. 2023b) due to its focus on memory efficiency, which does not align with our goals.\nModel-agnostic. Table 4 reveals that combining the ViP module with OPT (Zhang et al. 2022) and XGLM (Lin et al. 2022) as decoders, along with using both ViP and the retrieved text, leads to a notable improvement in performance."}, {"title": "Ablation Studies", "content": "We conduct ablation studies to evaluate the effects of various components in our work, including the design of ViP module, additional vector, and feature fusion network strategy.\nEffect of ViP module components. We explore the effect of the patch retrieval module, learnable vector, and scale factor in Table 7. Experimental results show that the patch retrieval module achieves better performance, which can be attributed to its close alignment with visual features. Adding a learnable vector during sampling improves performance. Scaling this vector with the \u03b1 parameter provides the optimal framework, and all experiments in this paper are conducted with the same scaling value.\nFeature Fusion Network design. In the Table 8, our FFN shows more effectiveness in mitigating the modality gap between image and text semantic features, and generating refined visual features with only a single layer. Compared to our approach, the summation, concatenation, and MLP methods struggle to capture distribution and relational context between modalities.\nEffect of M number sampling. We extracts semantic features through M number sampling. In Table 9, where M number of sampling ranges from 100 to 500, the highest score is observed at M = 200. When there are relatively too many patches (M > 200), the local representation G becomes scattered, and leads to no further improvement in performance."}, {"title": "Conclusion", "content": "In this work, we introduce ViPCap, a novel approach that generates visual prompts by leveraging semantic information from retrieved text embedding through the ViP module. ViPCap performs well across both in-domain and out-of-domain datasets. The ViP module proposes a plug-and-play method that generates visual prompts based on various models and prompt types. Future work will explore using learnable tokens as visual prompts for better flexibility."}]}