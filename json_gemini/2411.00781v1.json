{"title": "Hazards in Daily Life?\nEnabling Robots to Proactively Detect and Resolve Anomalies", "authors": ["Zirui Song", "Guangxian Ouyang", "Meng Fang", "Hongbin Na", "Zijing Shi", "Zhenhao Chen", "Yujie Fu", "Zeyu Zhang", "Shiyu Jiang", "Miao Fang", "Ling Chen", "Xiuying Chen"], "abstract": "Existing household robots have made significant progress in performing routine tasks, such as cleaning floors or delivering objects. However, a key limitation of these robots is their inability to recognize potential problems or dangers in home environments. For example, a child may pick up and ingest medication that has fallen on the floor, posing a serious risk.\nWe argue that household robots should proactively detect such hazards or anomalies within the home, and propose the task of anomaly scenario generation. We leverage foundational models instead of relying on manually labeled data to build simulated environments. Specifically, we introduce a multi-agent brainstorming approach, where agents collaborate and generate diverse scenarios covering household hazards, hygiene management, and child safety. These textual task descriptions are then integrated with designed 3D assets to simulate realistic environments. Within these constructed environments, the robotic agent learns the necessary skills to proactively discover and handle the proposed anomalies through task decomposition, and optimal learning approach selection. We demonstrate that our generated environment outperforms others in terms of task description and scene diversity, ultimately enabling robotic agents to better address potential household hazards.", "sections": [{"title": "1 Introduction", "content": "The development of Vision-Language Models (VLMs) has significantly improved household robots' ability to interact with the physical world in a more human-like manner (Liu et al., 2024b,a;\nCai et al., 2023; Majumdar et al., 2024). Among these models, the most popular paradigm for such robots is receiving instructions and performing corresponding operational tasks (Yang et al., 2024;\nDriess et al., 2023; Ahn et al., 2022).\nHowever, a critical yet often overlooked scenario arises when no instructions are provided. According to survey data, 31% of cooking fires are caused by unattended equipment (Ahrens, 2020). Meanwhile, unintentional injuries are the predominant cause of death among children, particularly those aged 1-14 years, encompassing incidents such as drowning, falls, and accidental poisonings (Worldwide, 2022). A lack of adequate supervision is often identified as a significant contributor to many of these fatalities, especially in cases involving younger children (Hymel et al., 2006; Williams and Kotch, 2023). It would greatly benefit humans if household robots could monitor whether stoves and other fire sources are properly turned off and detect potential hazards in the home that could lead to falls or accidental poisonings. Many of these fires and unintentional injuries could be prevented. However, to the best of our knowledge, such robotics have yet to be implemented.\nHence, in this work, we propose AnomalyGen, which can generate diverse anomaly settings covering household hazards, hygiene management, and child safety in 3D simulation environments, enabling robots to develop proactive detection and problem-solving abilities, as shown in Figure 1. Specifically, we first devised a group brainstorming setting, where LLM-based agents collaborate to generate diverse and comprehensive anomaly scenarios. The motivation comes from the observation that simply prompting an LLM to generate hazard scenarios results in repetitive and similar settings. In contrast, group brainstorming in real-life meetings often leads to novel and creative ideas. Based on these task settings, AnomalyGen automatically constructs simulated anomalous scenes through carefully designed 3D asset retrieval, configuration, and scene setup steps. Finally, AnomalyGen guides household robots in developing detection and resolution abilities for handling anomalies. It reads textual descriptions of the simulated environment, including the 3D coordinates of assets, and automatically identifies potential anomalous tasks that require attention. AnomalyGen then decomposes the task into fine-grained sub-tasks and selects the most appropriate learning method for the household robot. In general, our AnomalyGen leverages language-based approaches to bridge the domain gap between foundational models and robot interaction, enabling operations such as control inputs, operational trajectories, and physical interaction.\nFor the experiments, AnomalyGen constructs 111 diverse and comprehensive anomaly scenes, with human evaluation showing high quality and automatic metrics demonstrating greater diversity compared to previous human-crafted robotic datasets. Based on this simulation data, household robots are guided by AnomalyGen to learn and demonstrate a variety of skills across tasks such as rigid and articulated object manipulation and legged locomotion, achieving a task completion rate of 83%. Additionally, we conduct an error analysis highlighting the limitations of the current learning algorithm and VLM, identifying areas for future improvement and direction.\nOur contributions can be summarized as follows: Firstly, we introduce AnomalyGen, an unsupervised generative framework that enables household robots to autonomously detect and address anomalies without explicit instructions. Secondly, AnomalyGen creates a 3D simulation environment with 111 diverse hazard scenarios, generated through a collaborative brainstorming mechanism, significantly enhancing task diversity compared to previous datasets. Thirdly, AnomalyGen enables robots to autonomously identify anomalies, decompose tasks, and learn appropriate skills using an effective task decomposition and learning method with minimal human input."}, {"title": "2 Related Work", "content": "Household robotics have seen significant advancements, with researchers developing various benchmarks for embodied AI agents to tackle household tasks in simulation. Behavior1K (Li et al., 2021b;\nSrivastava et al., 2021) introduced a benchmark for AI agents to complete 100 household activities in a simulated environment. Housekeep (Kant et al., 2022) focuses on organizing homes by rearranging cluttered items, while TidyBot (Wu et al., 2023) emphasizes personalized household cleanup, aiming to understand and place items in their correct locations. However, limited work has been done on anomaly detection in household environments. The only notable dataset addressing safety-related topics in this domain is SafetyDetect (Mullen et al., 2024), which manually configured just seven distinct scenes and required substantial human effort for scene construction and data collection. Additionally, it is an image-based dataset, not a simulation. In contrast, AnomalyGen autonomously generates simulation environment and tasks without human intervention."}, {"title": "2.2 Foundation Models in Robotics", "content": "With the rapid development of foundation and generative models in multi-modal settings (Poole et al., 2022; Melas-Kyriazi et al., 2023; Touvron et al.,\n2023; Driess et al., 2023; OpenAI, 2023; Liu et al., 2023; Girdhar et al., 2023), a growing body of research has begun to harness the capabilities of foundational models across various domains, such as visual imagination for skill execution (Du et al., 2023), and sub-task planning (Ahn et al., 2022;\nHuang et al., 2022; Lin et al., 2023), among others. Some recent works have also attempted to fully harness the potential of LLMs for robotic manipulation, such as using LLMs for reward function generation (Yu et al., 2023; Ma et al., 2023), and sub-task and trajectory generation (Ha et al., 2023). Additionally, GenSim explored LLM-based robotic instruction tasks, but it primarily focused on object manipulation on desktops with a limited set of 3D assets. Gen2Sim (Katara et al., 2023) extends the range of task types by generating instead of only retrieving new 3D assets."}, {"title": "2.3 Simulation Environment", "content": "VirtualHome (Puig et al., 2018) and Alfred (Shridhar et al., 2020a) abstract physical interactions to concentrate on symbolic reasoning, yet they lack physical realism and a comprehensive scope of actions. Habitat (Savva et al., 2019), employing 3D scans of real homes, focuses on navigation tasks (Batra et al., 2020) but omits physics-based interactions. To augment physical realism, Habitat\n2.0 (Yenamandra et al., 2023) and iGibson (Srivastava et al., 2021; Li et al., 2021a) introduce realistic actions, interactions with environments, and object state simulations. Additionally, emerging simulation platforms such as ManiSkills (Gu et al., 2023), TDW (Gan et al., 2022), SoftGym (Lin et al., 2021), and RFUniverse (Fu et al., 2022) emphasize physical realism but still fall short on task diversity. To enrich task variety, several works have explored language-conditioned tasks (Zeng et al., 2021; James et al., 2020b; Mees et al., 2022).\nAnomalyGen focuses on constructing household anomaly scenes, which is an area that has not been covered in previous work."}, {"title": "3 Method", "content": "In this section, we demonstrate how our framework utilizes advanced generative models to automatically create anomalous scenarios and task-related data, as shown in Figure 2."}, {"title": "3.1 Brainstormed Anamoly Task Proposal", "content": "The most intuitive way to obtain anomaly scenarios would be to prompt an LLM to generate a list. However, in our preliminary experiments, the LLM tends to generate repetitive and lackluster scenarios, such as \"move the scissors to a drawer\", \"put the scissors into the box\" and \"store scissors safely.\"\nThis lack of diversity limits the range of potential hazards, resulting in scenarios that are too similar to each other.\nTo address this, we propose group brainstorming, a round-based divergent thinking framework that allows multiple agents to build upon each other's ideas. We also incorporate role-playing, where each agent adopts a unique perspective, encouraging a broader range of creative thoughts. This collaborative process not only enhances the variety of scenarios but also improves the realism and complexity of the generated anomalies.\nRole-play Initialization Stage. To ensure that each agent considers different perspectives and approaches the task from various angles, we assign distinct roles to each agent. This is especially useful in households, where different professions face diverse hazard scenarios daily. For example, a parent may focus on child safety, noticing hazards like sharp objects or unlocked cabinets, while a household maintenance worker may be more attuned to issues like electrical faults or fire hazards. Our role-play list includes roles such as homemaker, household safety advisor, and educational consultant, with the full list in the Appendix B.1. Based on their assigned role, each agent randomly selects a target object from an anomalous household asset list and proposes an initial anomaly scenario based on that object. This anomalous object list was curated from a subset of the PartNet Mobility dataset (Xiang et al., 2020; Wang et al., 2023b). The detailed categories and directory of these anomalous object assets can be found in the Appendix A.\nBrainstorming Stage. After each agent proposes its initial anomaly scenario, we facilitate multiple rounds of discussion to foster meaningful exchanges of ideas among the LLM agents. Each agent takes the outputs from other agents in the previous rounds, combines them with its own character and thoughts, and proposes new tasks. Each agent is informed that they are part of a collaborative brainstorming session, where teamwork and diverse perspectives are key to generating creative and comprehensive hazard scenarios. The detailed prompt can be found in Appendix B.2\nFor instance, as shown in Figure 2, the \"gardener\" and \"engineer\" agents propose a scenario where sharp tools left on a wet floor and exposed electrical wires near water pose significant safety risks. Motivated by the sharp tools mentioned by the gardener and the electrical hazards raised by the engineer, the \"homemaker\" suggests that leaving kitchen appliances plugged in near the sink could also lead to electrocution, expanding on the dangers of water-related hazards in the home. This iterative dialogue mimics human brainstorming, where participants build on each other's ideas for more creative and comprehensive outcomes. By encouraging LLM agents to collaborate, we achieve greater variety and depth in the generated anomaly scenarios."}, {"title": "3.2 3D Anomalous Scenarios Generation", "content": "The brainstormed ideas above are textual descriptions of the scenario, and our next step is to turn the text into vivid 3D environments.\nAuxiliary 3D Assets Retrieval. In the previous section, we compiled a list of 3D anomaly assets that serve as target objects for the robots to manipulate. However, to realistically simulate real-world scenarios, focusing only on these target objects is not enough. We also need auxiliary surrounding objects to construct realistic environments that mimic real-world object distributions. A straightforward approach to select these surrounding objects is to source them from Objaverse (Deitke et al., 2023). However, this dataset is extensive, containing up to 800K items, and the item names are often too short or duplicated, making selection challenging.\nTo address this, we first query LLM to generate names and descriptions of objects relevant to the anomalous task, from a textual perspective. We then employ Sentence-BERT (Reimers and Gurevych, 2019) to retrieve the top-k textually similar 3D assets from the Objaverse list based on these descriptions. From a visual perspective, we further validate the selection by using a VLM. The VLM takes the task name, detailed task description, and the assets annotations as input. It then determines whether the scene is valid and outputs \"yes\" to confirm alignment between the task description and the scene setup.\nIn this way, we compile a high-quality list of relevant 3D assets that undergo dual validation through both textual and visual steps.\nAsset Configuration. Assets configuration ensures that the retrieved 3D assets have physically plausible dimensions. To automate this process, we employ LLM to determine the size of each asset. The size of each object is calculated as a scalar value in meters, representing its largest dimension. In addition, we establish spatial relationship rules, where objects with relative size relationships must satisfy specific task requirements. For example, in a scenario where a \"bowl of soup\" needs to be placed inside a microwave, if the bowl's size is 0.15 meters, the microwave must have a dimension larger than 0.15 meters to accommodate it. We also define initial state rules, ensuring that objects are in the appropriate state for the task. For example, for the task \"turn off faucet,\" the faucet must be initialized in the \"on\" state to accurately simulate the conditions necessary for the task's execution.\nScene Configuration. After configuring the assets, the final step is to position them accurately while maintaining appropriate spatial relations. To achieve precise placement, we query an LLM to establish a 3D world coordinate system (x, y, z). Target assets required for specific tasks are strategically placed within the constrained space of (0,0,0) to (1,1,1), while auxiliary assets not involved in the current task are positioned outside this specified range.\nWe will demonstrate through human evaluation in \u00a75.4 that the environment constructed in this manner is of high quality."}, {"title": "3.3 Proactive Anomaly Detection", "content": "Although existing research extensively explores ways to enhance a robot's ability to follow instructions, a key limitation is their inability to actively detect anomalies or dangerous situations in daily life (Fan et al., 2024), an ability that is crucial for ensuring safety in dynamic environments (Lundstr\u00f6m et al., 2015; Wang et al., 2023b). In our simulated 3D environment, we aim to enable household robots to proactively detect hazards or anomalies and acquire the necessary skills to solve tasks related to these anomalies. Concretely, AnomalyGen employs an LLM to analyze and identify potential problems. The input for the LLM includes outputs from the previous steps, including the target object, retrieved assets, their configurations, and the overall scene setup. Note that LLM has no access to the task name and description, but only infers the potential task based on the environment observation. Based on this information, the LLM is prompted to generate possible problems that need to be solved and their solutions. For example, a problem could be that a folding knife on the table may cause cuts or injuries, and the solution would be to store the folding knife in a storage box."}, {"title": "3.4 Anomaly Task Learning", "content": "After confirming the task to be completed, AnomalyGen queries the LLM to decompose the detected solution into shorter-horizon sub-tasks, as illustrated in the bottom part of Figure 2.\nThen, different learning algorithms are selected, tailored to different subtasks: reinforcement learning (Schulman et al., 2017; Haarnoja et al., 2023) and action primitives with motion planning (Karaman and Frazzoli, 2011). Each algorithm has its strengths: Reinforcement learning is ideal for dynamic, contact-rich environments, like legged locomotion or adjusting appliance controls, and Action primitives with motion planning handle navigation through cluttered environments, ensuring safe and efficient paths. We introduce the strengths of each algorithm and provide three examples of action-algorithm pairs to the LLM, enabling it to select the most appropriate learning algorithm for each subtask by in-context learning.\nMeanwhile, for subtasks trained using reinforcement learning, the LLM is responsible for generating the corresponding reward functions. For tasks involving rigid manipulation and locomotion, these reward functions are derived from low-level states accessible to the LLM. In contrast, for tasks involving soft body manipulation, the reward functions are based on the earth-mover distance between the particles of the current and target shapes, ensuring precise shape matching. To simplify object grasping and approaching action in action primitives subtasks, we use a robot equipped with a suction cup. This setup streamlines the process of grasping. The simplified pseudo-algorithm for the grasping and approaching primitives is in Appendix C.1."}, {"title": "4 Experiment Setup", "content": ""}, {"title": "4.1 Implementation Details", "content": "Our proposed system is generic and agnostic to specific simulation platforms. However, considering the broad audience for simulation platforms, as following (Wang et al., 2023b; Kant et al., 2022;\nGu et al., 2023; Shridhar et al., 2020b; Savva et al., 2019). we choose Genesis (Katara et al., 2023), the most widely employed simulation platform for deployment. The model itself is general-purpose and independent of any specific simulation platform. We employ the state-of-the-art GPT-4-0314\nLLM and BLIP-2 (Li et al., 2023b) VLM by default. For anomaly task learning, we utilize the\nSoft Actor-Critic (SAC) (Haarnoja et al., 2018) as the reinforcement learning algorithm, employing learning rate $3e - 4$ for the actor, the critic, and the entropy regularizer. The horizon of manipulation sub-tasks is 100, with a frameskip of 2. For each sub-task, we train with 1M environment steps. We also employ Batch Informed Trees (Gammell et al., 2015) as the motion planning algorithm. More details are in the Appendix C.2."}, {"title": "4.2 Baselines", "content": "We compare our constructed environment with the latest benchmark environments, including RL-Bench (James et al., 2020a), which encompasses 100 distinct, meticulously crafted tasks, ranging in complexity from basic tasks like target-reaching and door-opening to more advanced, multi-step tasks. ManiSkill2 (Gu et al., 2023) includes 20 manipulation task families which cover stationary/mobile-base, single/dual-arm, and rigid/soft-body manipulation tasks with 2D/3D-input data. Meta-World (Yu et al., 2020), serves as a benchmark specifically designed for evaluating the performance of meta-reinforcement learning and multitask learning algorithms. Behavior-100\n(Li et al., 2023a) featuring 100 activities within simulated environments. These activities encompass a variety of routine household tasks, including cleaning, maintenance, and food preparation. Gen-Sim (Wang et al., 2023a) offers 100 robotic arm grasping scenarios, all set on a tabletop environment. RoboGen (Wang et al., 2023b) has generated 106 more diverse tasks, further expanding them to accommodate a wider range of robotic arm types. Note that RoboGen doesn't release its constructed dataset; therefore, we reimplement RoboGen based on the provided code."}, {"title": "4.3 Evaluation Metrics", "content": "We first evaluate the diversity of the generated anomaly task settings, including the semantic aspects of the tasks and the visual aspects of the scenes. For semantic view, we concatenate the task name and description from each anomalous task and calculate their similarity by Self-BLEU (Papineni et al., 2002) and SentenceBERT (Reimers and Gurevych, 2019) for quantitative analysis. For scene visual information, we assess scene diversity by measuring the embedding similarity of unrendered images of the scene from the initial camera state. Specifically, we utilize ImageNet pre-trained Vision Transformer (ViT) (Dosovitskiy et al., 2020) and CLIP models (Radford et al., 2021).\nNext, we assess whether the robot can proactively and accurately detect anomaly scenarios. We employ three undergraduate students specializing in computer-related fields to determine whether the task detected by the anomaly detection module aligns with the ground truth task.\nFinally, the annotators review task action videos to determine whether the task is successfully completed and evaluate the overall success rate of task completion. Note that even if the detected tasks do not fully align with the designed tasks in the task proposal, evaluations are still based on the detected tasks, as these tasks possess a certain degree of rationality, as explained in \u00a75.3."}, {"title": "5 Results and Analysis", "content": ""}, {"title": "5.1 Anomaly Task Statistics", "content": "Through our group brainstorming component, we can theoretically generate an unlimited number of diverse tasks. To facilitate the evaluation process, we limited the number of evaluated scenes to 111. These scenes are categorized into three general categories: household hazards, hygiene management, and child safety. Each category includes diverse tasks, as visualized in Figure 4, where \"store sharp objects safely\" represents the largest proportion of tasks at 28.8%, followed by other critical tasks such as \"reduce tripping hazards\" and \"manage chemical risks,\" all of which represent common dangers encountered in real life. Note that no human intervention was involved in the entire design process, so the distribution of tasks is the result of fully autonomous generation by our model."}, {"title": "5.2 Anomaly Task Diversity", "content": "In Group Brainstorming Ablation. To demonstrate that the diversity described above is due to our group brainstorming setting rather than the LLM's inherent abilities, we conduct an ablation study. The first setting removes both the brainstorming and role-play components introduced in \u00a73.1, directly querying LLM to generate task proposals. The second setting incorporates role-playing, where we provide the role settings in the prompt and query LLM to propose tasks from the perspective of the assigned character. To comprehensively evaluate the generation ability, we generate 300 anomalous task proposals. Concretely, we use identical query parameters for both experiments and randomly selected 10 assets as targets. Each method is run 10 times, generating 10 task proposals per iteration. We employ Self-BLEU, SentenceBERT, and Word Mover's Distance (WMD) (Huang et al., 2016) for evaluation.\nThe experiment results are presented in Table 2. It can be seen that with role-play, the Sentence-BERT and WMD scores decrease by 0.002 and 0.007, respectively, compared to the naive LLM approach. Then, brainstorming brings the greatest contribution, leading to the largest improvement across all three metrics, including a 0.182 decrease in Self-BLEU, 0.158 decrease in SentenceBERT, and 0.107 decrease in WMD. This demonstrates that the combination of brainstorming and role-play significantly enhances the model's performance by promoting better alignment and understanding, as reflected in the lower values across all metrics."}, {"title": "5.3 Anomaly Detection Performance", "content": "For the anomaly detection task, we allow AnomalyGen to come up with up to three solutions, and manually evaluate the hit accuracy for k solutions. As shown in Table 3, our model generally achieves high performance, with 76% accuracy on the first attempt and 82% accuracy when given three attempts. This demonstrates the effectiveness of the prompt we design for AnomalyGen to correctly identify the task, as well as the validity of our constructed simulation environment, where the anomaly scenes can be accurately detected.\nIn addition, we conduct an error analysis on the failed tasks. We find that the main reason for failure was that our environment closely mirrors real-life scenarios, including a variety of everyday clutter, which mislead the LLM into making incorrect detections. Meanwhile, we note that the tasks proposed by the detection module still possess a degree of rationality. For instance, while the ground truth involves picking up a pill from the floor and placing it on a table, our detection module instead proposes to discard it. Although this action does not align with the specified ground truth, the alternative task remains meaningful, as discarding a potentially misplaced pill could also be seen as a reasonable safety precaution."}, {"title": "5.4 Anomaly Solution Performance", "content": "Lastly, we can evaluate the performance of overall task performance. We first give some examples of the task execution process in Figure 3, including multi-step tasks, such as placing a medicine bottle on a safe desktop, and single-step tasks, such as closing the washing machine door. It can be observed that the model successfully follows the instructions and completes tasks of varying complexity and time steps.\nNext, we conduct a quantitative analysis of the anomaly resolution accuracy, using human evaluations to assess the completion rate across different categories, as in Figure 5. Among the 111 generated anomalous scenarios, the average success rate for resolving these tasks was 83%, highlighting AnomalyGen's strong execution performance. We also conduct an error analysis and summarize two main reasons for task failure. First, there is an overlap of scene assets. In the verification step described in \u00a73.2, we employ VLMs to verify that the assets are correctly positioned. However, there are 4 out of 111 instances where the VLMS fail to identify mispositioned items and incorrectly approve them. For example, when the mispositioned items are both white, accurate identification becomes more difficult. The misposition problem exists in other environments (Wang et al., 2023b), and we anticipate that advancements in VLM technology will address this limitation. Second, some tasks proposed by AnomalyGen include complex, multi-step actions that challenge the capabilities of current algorithms, making it difficult for them to perform effectively. We expect that improvements in learning algorithms will enable robots to better learn from their environments and handle more complex tasks in the future."}, {"title": "6 Conclusion", "content": "In this study, we present AnomalyGen, an innovative framework designed to enhance the proactive detection and resolution of household anomalies by robots. Our approach integrates advanced generative models to automatically create diverse and realistic 3D environments, which are essential for training robots to handle real-world tasks autonomously. We also propose a group brainstorming method, which generates a wide variety of anomalous scenarios, surpassing traditional methods that rely heavily on manual input. Furthermore, the AnomalyGen framework introduces a novel approach to anomaly detection, offering potential strategies for enabling robots to act without direct human commands. We hope our work will inspire further exploration into autonomous decision-making in real-world applications."}, {"title": "7 Limitation", "content": "While AnomalyGen has achieved certain milestones, it still encounters several limitations:\n1) In unsupervised settings, the validation of tasks within generated anomaly scenarios remains challenging, with a potential for scenarios that clearly do not meet task requirements. This issue is particularly exacerbated under conditions of large-scale generation. However, with future enhancements in the capabilities of multimodal large language models, we anticipate that this limitation will be addressed.\n2) The richness of the generated scenes is currently somewhat constrained by the scale of the 3D assets dataset. A limited dataset size may curtail the full potential of AnomalyGen.\n3) Regarding the deployment of AnomalyGen into real-world applications, there remains a significant sim-to-real domain gap. This gap constitutes an independent research domain that is beyond the scope of our current work. Given recent rapid advancements in physically accurate simulation (Li et al., 2020) and techniques such as domain adaptation (Tobin et al., 2017; Xu et al., 2023; Xiao et al., 2024) along with realistic sensory signal rendering (Zhuang et al., 2023), we anticipate a continual narrowing of this gap in the near future."}, {"title": "B Brainstorming Setting", "content": ""}, {"title": "B.1 Role List", "content": "In this section, we provide a detailed list of roles that are commonly found within a household setting, each accompanied by a specific role description. These roles encompass a variety of responsibilities and skills required to efficiently manage and maintain a home environment. Table 5 outlines the diverse roles ranging from daily household management to specialized services that enhance the functionality and comfort of home life."}, {"title": "B.2 Detail Prompt", "content": "We show our prompt template in Figure 6. The prompt outlines a brainstorming session focused on generating home safety tasks for the Franka Panda robotic arm, taking into account the articulated, semantically tagged movable objects in a household setting. These tasks are to be envisioned in scenarios that may pose potential hazards or unsanitary conditions within the home, which the robot is equipped to handle. The tasks are categorized into three primary areas: household hazard, hygiene management, and child safety measures. Each task is to be formatted to include the task name, an explanation, a description, any auxiliary items required, and the articulations and their specific functions. The brainstorming context should be collaborative, with a strong emphasis on the operational limits of the robotic arm, such as avoiding complex assembly, disassembly, or cleaning tasks. This ensures that the tasks are tailored to the robot's capabilities, focusing on practical and manageable interventions in household environment."}, {"title": "C Parameter Setting", "content": ""}, {"title": "C.1 Algorithm of Grasping and Approaching Primitives", "content": "In designing a robotic manipulator equipped with a suction cup to facilitate object grasping, the operational primitives for grasping and approaching are outlined as follows: Initially, a random point on the surface of the designated target object or link is selected. Subsequently, a gripper pose is calculated such that it aligns with the normal at the sampled point. Motion planning algorithms are then employed to devise a collision-free trajectory to the predetermined gripper pose. Upon attaining this pose, the manipulator advances along the normal vector until contact is established with the object. In this setup, AnomalyGen leverages LLM to determine the specific target object for either grasping or approaching, dependent on the given subtask. We show the simplified pseudo-algorithm in Algorithm 1."}, {"title": "C.2 Skill Learning Parameter", "content": "For anomaly task learning, we employ SAC algorithm for reinforcement learning. In object manipulation tasks, the observation space includes the low-level state of objects and the robot involved in the task. The SAC utilizes MLP with three layers, each having 256 units, for both the policy and Q networks. We set a learning rate of 3e-4 for the actor, critic, and entropy regularizer. Each manipulation task has a horizon of 100 steps and employs a frameskip of 2. The RL policy controls a 6-dimensional action space, where the first three dimensions dictate the translation\u2014either as delta translation or target location\u2014and the remaining three dimensions specify the delta rotation, represented as a delta-axis angle in the gripper's local frame. We train each sub-task over 1 million environment steps.\nFor locomotion tasks, we apply the Cross Entropy Method (CEM) for skill learning, which has proven more stable and efficient than traditional RL approaches. We use a ground-truth simulator as the dynamics model in CEM, focusing on optimizing the joint angle values of the robot. The horizon for locomotion tasks is set to 150, with frameskip of 4. Additionally, we integrate BIT (Gammell et al., 2015), implemented within the Open Motion Planning Library OMPL (Sucan et al., 2012), for action primitives in motion planning. Specifically, for grasping and approaching primitives, we begin by sampling a surface point on the targeted object or link. We then compute a gripper pose that aligns the gripper's y-axis with the normal of the sampled point. The pre-contact gripper pose is established 0.03 meters above the surface point along the normal direction. Utilizing motion planning, we identify a collision-free path to the target gripper pose, continuing the gripper's movement along the normal until contact is achieved."}, {"title": "D Data Statistics", "content": "We present the categories and numbers of all generated scenes. Detailed statistics are in Table 6."}]}