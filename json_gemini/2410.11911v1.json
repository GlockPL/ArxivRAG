{"title": "Transfer Learning Adapts to Changing PSD in Gravitational Wave Data", "authors": ["Beka Modrekiladze"], "abstract": "The detection of gravitational waves has opened unparalleled opportunities for observing the universe, particularly through the study of black hole inspirals. These events serve as unique laboratories to explore the laws of physics under conditions of extreme energies. However, significant noise in gravitational wave (GW) data from observatories such as Advanced LIGO and Virgo poses major challenges in signal identification. Traditional noise suppression methods often fall short in fully addressing the non-Gaussian effects in the data, including the fluctuations in noise power spectral density (PSD) over short time intervals. These challenges have led to the exploration of an AI approach that, while overcoming previous obstacles, introduced its own challenges, such as scalability, reliability issues, and the vanishing gradient problem. Our approach addresses these issues through a simplified architecture. To compensate for the potential limitations of a simpler model, we have developed a novel training methodology that enables it to accurately detect gravitational waves amidst highly complex noise. Employing this strategy, our model achieves over 99% accuracy in non-white noise scenarios and shows remarkable adaptability to changing noise PSD conditions. By leveraging the principles of transfer learning, our model quickly adapts to new noise profiles with just a few epochs of fine-tuning, facilitating real-time applications in dynamically changing noise environments.", "sections": [{"title": "Introduction", "content": "The discovery of gravitational waves (GWs) has revolutionized our approach to observing the universe, granting us access to phenomena that remain invisible to traditional astronomical instruments [1]. Among these phenomena, black hole inspirals are particularly significant. They serve as cosmic laboratories where the fundamental laws of physics can be tested under conditions of extreme gravity. The insights gained from these events have the potential to deepen our understanding of the universe's most fundamental principles, including the nature of spacetime itself.\nHowever, the path to unlocking these insights is fraught with challenges, chief among them being the significant noise that contaminates GW data [2]. This noise, a mixture of instrumental and environmental interferences [3], often masks the very signals we seek to analyze, making it difficult to extract clear and reliable information from the data collected by observatories such as Advanced LIGO [4] and Virgo [5].\nFurthermore, the substantial volume of data necessitates real-time analysis; however, traditional methodologies frequently lag, struggling to maintain pace [6]. In the context of signal detection, the presence of the non-stationary characteristics of noise [7] poses a significant challenge, given that traditional matched filtering techniques are designed and optimized for Gaussian noise [8]. Because noise in the data can drastically reduce the signal-to-noise ratio, this leads to challenges in accurately identifying genuine GW events [9].\nTraditional denoising methods in GW astronomy have predominantly centered on direct noise suppression techniques, incorporating a diverse array of approaches. These methods range from variational-based techniques [10], and wavelet-based strategies [11, 12], to those employing the Hilbert-Huang transform [13]. Despite their effectiveness in specific contexts, these traditional approaches fall short in addressing the intricate and evolving nature of noise within GW data.\nPrevious studies introducing AI for enhancing GW signal detection have revealed promising avenues [14, 15]. Innovations such as denoising autoencoders [16] have been utilized for reconstructing signals from noisy data, while Recurrent Neural Networks [17] are employed to capture temporal dependencies. These algorithms are distinguished by their computational efficiency, utilizing ac-celerated hardware to achieve rapid solutions [18]. Their scalability will ensure the capability to manage large datasets, thus offering reliable estimates of model performance [19]. Moreover, the modular design of these algorithms enhances their adaptability, enabling seamless integration of novel methodologies [20]. Additionally, the generalization abilities of deep learning models guarantee consistent performance across diverse gravitational wave data analysis scenarios [21].\nHowever, while autoencoders demonstrate effectiveness, their reliance on unsupervised learning may introduce reliability issues for scientific purposes in the future. Similarly, Recursive Neural Networks are susceptible to the vanishing gradient problem [22] as the network depth increases, which poses challenges for realistic signal detection. These obstacles highlight the need for a solution that incorporates supervised learning and employs a simpler neural network architecture. Such an approach would facilitate scalability and circumvent the vanishing gradient problem, offering a more robust framework for gravitational wave signal detection.\nIn response to these challenges, this paper focuses on a straightforward multi-layer perceptron (MLP) model and employs the ReLU activation function, which is crucial for circumventing the vanishing gradient problem. However, with simpler models, perceiving gravitational waves amidst highly non-trivial noise becomes almost impossible. To address this, we introduce a novel training methodology. We first train the model on clean, noise-free data to establish a robust foundation. Then, harnessing the principles of transfer learning, we fine-tune the model on noisy data, starting from the pre-trained model on clean data. This novel training methodology allowed us to effectively detect gravitational wave signals amidst highly non-trivial noise, demonstrating that simplicity, coupled with innovative training techniques, can lead to impactful outcomes.\nThe paper starts with Waveform generation, detailing the creation of a diverse set of gravitational waveforms. This is followed by AI Model Development and Training, which includes detailed discussions on Data preparation, Model architecture and Training process. Subsequently, the paper examines Realistic Noise and Fine Tuning where we simulate authentic observational conditions akin to those at LIGO detectors and adapt AI models through transfer learning to these realistic, variable noise conditions. Model performance and evaluation assesses how effectively our Al models detect gravitational wave signals amidst noise, demonstrating their potential to transform gravitational wave astronomy. Finally, Next steps and discussion advocates for the development of more extensive waveform banks and the use of generative models to surpass current limitations, potentially ushering in a new era of discovery-led science."}, {"title": "Methodology", "content": ""}, {"title": "Waveform generation", "content": "Our project simulates binary black hole mergers to explore a wide range of gravitational waveforms. Utilizing the pycbc library [23, 24], we varied key astrophysical parameters\u2014masses (m1, m2) from 10M to 30M and spins (81, 82) from 0 (non-spinning) to 0.99 (near-maximal spin)\u2014to produce plausible gravitational waveform morphologies, based on observed black hole binaries and theoretical predictions. [25].\nWe employed the SEOBNRv4 approximant [26] to generate accurate gravitational waveforms that represent the inspiral, merger, and ringdown phases. Waveforms were generated using the get_td_waveform function from the pycbc. waveform module at a sampling rate of 4096 Hz, starting from a 40 Hz lower frequency cutoff, aligning with the sensitivity range of Advanced LIGO."}, {"title": "Model development and training", "content": "Data preparation Our dataset comprised a collection of waveforms, each representing a variety\nof black hole inspiral events. To these, we added 'No Signal' samples, effectively augmenting our\ndataset with examples where GWs are absent. To ensure all waveforms were of consistent length,\nzeros were padded to the end of the input waveforms after the inspiral phase. This standardized\nwaveform length allows our model to operate effectively with real data, where the exact merger time\nmay be unknown. By employing uniformly sized segments, the model can systematically inspect all\ntemplate waveforms in a single run, sliding the same size segment to inspect for potential waveforms.\nModel architecture Our chosen architecture was a Multilayer Perceptron [27], celebrated for its\nproficiency in discerning patterns within high-dimensional data. The MLP featured an input layer\nto linearize the waveform data, followed by two hidden layers with ReLU activations 1. To prevent\noverfitting and to encourage the model to learn more generalized patterns, dropout regularization\n[28] was integrated into the hidden layers. Note also that the size of the hidden dimension is\nsignificantly smaller than the input data, which forces the model to learn intricate patterns instead of\njust memorizing the waveform bank. This design decision aimed to bolster the model's resilience\nwhen confronted with noisy data.\nTraining process To further regularize our model and encourage the learning of more robust fea-\ntures, we introduced weight decay into the optimization process. [29] This served as a countermeasure\nagainst overfitting by penalizing large weights. Throughout numerous epochs, our model engaged\nin an iterative learning process, aimed at reducing the loss on the training set, all the while being\nbenchmarked against the validation set to ensure reliable performance.\nEvaluation on clean data Upon completion of the training, the model was subjected to a test\ndataset, where it demonstrated high accuracy in identifying and classifying GW signals correctly.\nInterestingly, models trained under 'harsher' conditions\u2014such as with dropout regularization and\nsmaller hidden dimensions-couldn't reach over 99% accuracy on clean data. However, these models\nperformed significantly better when exposed to noise they had not previously encountered, compared\nto models that achieved over 99% accuracy on clean data. Thus, a training approach was selected\nthat would intentionally aim for models to achieve \"almost\" perfect performance, optimizing their\nrobustness in real-world noisy environments."}, {"title": "Realistic noise and transfer learning", "content": "Realistic noise To replicate the authentic observational conditions of LIGO detectors, we employed\nthe aLIGOZeroDetHighPower Power Spectral Density model to generate realistic noise patterns\n[24, 31]. The PSD is a critical tool in delineating the noise profile of LIGO detectors, which details\nthe power distribution over a range of frequencies. This model captures the multifaceted nature of\ndetector noise, that stem from a myriad of sources such as thermal fluctuations, quantum uncertainties,\nand environmental disturbances. Each waveform in our dataset was paired with noise, which was\ngenerated using a unique random seed, mirroring the unpredictable and variable noise encountered\nin actual GW detection. This approach to noise simulation enhances the validity of our tests and\nchallenges our AI models to perform under realistic conditions that include the full spectrum of\ndisturbances\u2014from seismic activity and thermal vibrations to instrumental artifacts. Such noise\nmodeling is crucial in evaluating the resilience and accuracy of our AI models in discerning GW\nsignals from the noisy background, which is typical of the data collected by GW observatories.\nChallenge: time varying PSD In the real world, the ultimate operational domain for our models,\nPSD is subject to change over time. This means that even if we train our model on a specific PSD, its\neffectiveness could diminish the very next day, which explains the cautious integration of AI models.\nIntriguingly, this challenge not only highlights the complexity of our initial problem but also points\nus toward the elegant solution that simultaneously addresses the issue of varying PSD. This solutions\ncould potentially enable the deployment of AI models that effectively detect signals in real-time\namidst fluctuating noise conditions.\nSolution: transfer learning and fine-tuning To enhance model performance in detecting GW\nsignals amidst noise, we employed transfer learning and fine-tuning. Models were initially pre-trained\non clean datasets to learn the fundamental patterns of gravitational waveforms, providing a solid\nfoundation. We then used transfer learning to adapt these models to our specific task by fine-tuning\nthem on datasets with realistic noise profiles. This fine-tuning adjusted model parameters to better\ndetect subtle features in noisy data, similar to real-world GW detection scenarios. Starting with\npre-trained weights rather than random ones allowed us to achieve sufficient accuracy in just a\nfew epochs, significantly speeding up the process. This approach enables real-time detection of\ngravitational wave signals by fine-tuning the model with changing PSD.\nModel performance and evaluation The final evaluation of our AI models was conducted on a\ndataset that reflected true operational challenges by incorporating realistic noise. Remarkably, the\nmodels demonstrated exceptional proficiency in signal detection, with over 99% accuracy, even in the\npresence of complex noise that is known to impede traditional analysis methods like matched filtering.\nWe illustrate a sample detection by our model, where a gravitational wave signal is accurately\nidentified within a noisy environment. This level of performance not only attests to the effectiveness\nof our training approach but also suggests that AI could play a transformative role in gravitational\nwave astronomy, particularly in scenarios where matched filtering is less effective."}, {"title": "Next steps and discussion", "content": "This success highlights AI's potential to reveal hidden signals in noise, opening new avenues for\ndiscoveries in GW astronomy. Moreover, this model can serve as a complement to matched filtering,\nrather than fully replacing it, facilitating a bidirectional enhancement of each other's capabilities. By\nassessing the probability of events in noisy data, our model identifies high-probability candidates,\nallowing matched filtering to conduct more focused and thorough investigations within shorter\nsegments. Conversely, starting with an algorithmic denoising process and potentially exploring\nadditional techniques, such as the method developed to correct, at first order, the effects of PSD\nvariation on the search background [34], prepares the data for more effective waveform detection by\nour model.\nHowever, the current approach to GWs detection, which relies on waveform banks for matched\nfiltering, leads us to a philosophically unsatisfactory situation: We will only see, what we expect to\nsee. To overcome this limitation, our next steps should involve using generative models [35] to\ncreate possible waveforms that could take us beyond our current expectations. This approach not only\neliminates the need for a traditional template bank but also opens the possibility of generating new\ntypes of waveforms that have not yet been hypothesized. Detecting these waveforms, which might\notherwise go unnoticed without specific candidate waveforms, opens the door to the new physics that\ncould account for these observations. Paradoxically, by adopting modern AI techniques, we may\nreturn to an 'ancient' way of doing science\u2014where experiments precede theory, and discovery drives\nunderstanding."}]}