{"title": "Domain-Aware Fine-Tuning of Foundation Models", "authors": ["U\u011fur Ali Kaplan", "Margret Keuper", "Anna Khoreva", "Dan Zhang", "Yumeng Li"], "abstract": "Foundation models (FMs) have revolutionized computer vision, enabling effective learning across different domains. However, their performance under domain shift is yet underexplored. This paper investigates the zero-shot domain adaptation potential of FMs by comparing different backbone architectures and introducing novel domain-aware components that leverage domain related textual embeddings. We propose domain adaptive normalization, termed as Domino, which explicitly leverages domain embeddings during fine-tuning, thus making the model domain aware. Ultimately, Domino enables more robust computer vision models that can adapt effectively to various unseen domains.", "sections": [{"title": "1. Introduction", "content": "Computer vision has made tremendous progress in recent years, thanks to the development of powerful neural network architectures and large-scale datasets (Deng et al., 2009; Simonyan & Zisserman, 2014; Long et al., 2015; Ronneberger et al., 2015; Hao et al., 2020; Dosovitskiy et al., 2020; Li et al., 2022; Schuhmann et al., 2022). However, when faced with domain shift - a common problem in real-world applications where the target domain has different characteristics than the training data - the performance of these models drops significantly. This is particularly problematic for tasks that require zero-shot domain adaptation, where during training there is no sample available in the target domain, though we might have an estimation of the potential domains (Farahani et al., 2021; Liu et al., 2022).\nFoundation models (FMs), have become pivotal in various applications, from natural language processing to computer vision. Due to the large-scale pretraining, FMs have generalizable representations. Despite showing promising results on zero-shot classification tasks (Brown et al., 2020; Oquab et al., 2023), it is yet challenging to employ these models directly for dense prediction tasks, e.g., semantic segmentation (see 1st row in Table 2), necessitating the need of fine-tuning for the specific task. However, this will inevitably lead to knowledge forgetting and raise the question of how robust the adapted models are under domain shifts.\nTo investigate the zero-shot domain adaptation performance of FMs, we first compare different FM backbone architectures on the challenging semantic segmentation task. We evaluate various vision backbones in Table 1, such as DINOv2 (Oquab et al., 2023; Darcet et al., 2023), ResNet-50 and ResNetRS-420 (He et al., 2016; Bello et al., 2021), CLIP-based fine-tuning methods leveraging MaskCLIP (Dong et al., 2023), and Stable Diffusion (SD) (Rombach et al., 2022) based fine-tuning, i.e., VPD (Zhao et al., 2023). We observe that these models are generally not robust to domain shifts, where there is a considerable performance drop when tested on unseen domains.\nWe hypothesized that visual embeddings can vary considerably with different domains, making the model vulnerable to changes such as weather and lighting conditions. To mitigate this issue, we propose to incorporate textual domain embeddings and introduce domain adaptive normalization, termed as Domino during fine-tuning. We employed CLIP"}, {"title": "2. Method", "content": "Stable Diffusion has demonstrated astonishing text-to-image synthesis capability, thanks to their large-scale pretraining. Naturally, it has learned rich multi-modal representations. Recent work VPD (Zhao et al., 2023) has explored its potential for downstream applications, e.g., depth estimation and semantic segmentation. More specifically, they fine-tune the denoising UNet and extract intermediate features and cross-attention maps from it, which can provide semantically meaningful features (Hertz et al., 2023; Li et al., 2023a). Note that semantic classes are used as prompts for semantic segmentation task, and a text adapter is introduced as well. Further, a lightweight task-specific decoder taking extracted features is incorporated.\nDespite showing promising results on the in-domain evaluation, it remains unclear how this model can perform under domain shift. In other words, it's not yet explored how the prior knowledge of powerful Stable Diffusion can help the downstream applications, which is of greater interest, and the focus of this work. Additionally, we propose a novel domain-aware fine-tuning strategy, where we extract the domain embeddings with CLIP (see Section 2.2), and incorporate them into the fine-tuning pipe to enhance domain awareness (see Section 2.3)."}, {"title": "2.2. Automatic Domain Embedding Extraction", "content": "The domain concept is quite often assigned manually. Instead, we seek a way to automatically obtain the domain information. Prior work (Wang et al., 2023) has shown that CLIP (Radford et al., 2021) is capable of assessing the look and feel of images. Thus, we propose to leverage CLIP to automatically extract the domain embeddings, which can be further utilized to enhance domain-awareness during FM fine-tuning.\nTo calculate the domain embedding, we begin by defining base cases that describe the aspects of interest. For instance, in autonomous driving, related domains involve different weather conditions and time of the day. Each of these domain descriptions are encoded using the CLIP text encoder:\n$d_i = \\text{CLIP}_{\\text{Text}}(k_i) \\forall i = 1, ..., N,$\nwhere $K = \\{k_1, ..., k_n\\}$ are defined domain descriptions and $D = \\{d_1, ..., d_N\\}$ is the resulting description embeddings. Given an image, we can obtain the image embedding through the CLIP image encoder and compute the similarity with each individual description embedding. Finally, the domain embedding of the given image is computed via a weighted sum of all description embeddings:\n$I = \\text{CLIP}_{\\text{Image}}(x)$\n$\\alpha_i = \\text{Softmax}\\left(\\frac{\\left|I \\cdot d_i\\right|}{\\left|I\\right| \\left|d_i\\right|}\\right)$\n$W = \\sum_i \\alpha_i d_i,$"}, {"title": "2.3. Domain Aware Fine-tuning", "content": "Stable Diffusion naturally is capable of utilizing textual embedding, and thus we can simply combine the domain embedding with the original prompt embedding, i.e., class embedding, as illustrated in Figure 2. There are two possible ways of combination, namely addition or subtraction. When adding the domain embedding, we encourage the model to explore the domain information for prediction. While for subtraction, we essentially try to remove domain-related information, thus enforcing domain-invariant learning. As experimented in Table 2, we found the latter is more effective in improving the generalization performance.\nIn addition to being utilized by the Stable Diffusion backbone, we propose to incorporate the domain embedding in the segmentation decoder head to further enhance domain awareness. Inspired by SPADE (Park et al., 2019), we introduce domain adaptive normalization, termed as Domino. Specifically, we map the domain embedding through simple MLP layers into modulation parameters $\\gamma$ and $\\beta$.\n$f_{adp} = \\frac{f - \\mu_f}{\\sigma_f} \\gamma(W) + \\mu(W),$"}, {"title": "3. Experiments", "content": "Experimental Setup. In this study, we focus on the challenging task of semantic segmentation, which requires per-pixel semantic class prediction. We train the models on the Cityscapes (Cordts et al., 2016) training set, and evaluate the model's generalization performance on ACDC (Sakaridis et al., 2021). Cityscapes is an urban scene dataset with 19 semantic classes, which is collected mainly in Germany"}, {"title": "4. Conclusion & Discussion", "content": "In this study, we conducted extensive experiments comparing different foundation models as the backbone for zero-shot domain adaptation in semantic segmentation. We empirically observe that Stable Diffusion based VPD model achieves better generalization performance. We then demonstrated the proposed Domino with explicit usage of the domain information can significantly boost the model's generalization further. Our results indicate that domain embedding addition encourages the use of domain cues, which can be beneficial for improving the in-domain segmentation performance. In contrast, domain embedding subtraction encourages the use of more domain-invariant features which can enhance the generalization performance. Furthermore, we have found that incorporating synthetic data with a proper ratio during the foundational model fine-tuning is also beneficial for domain generalization performance."}]}