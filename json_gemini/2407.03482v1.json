{"title": "Domain-Aware Fine-Tuning of Foundation Models", "authors": ["U\u011fur Ali Kaplan", "Margret Keuper", "Anna Khoreva", "Dan Zhang", "Yumeng Li"], "abstract": "Foundation models (FMs) have revolutionized computer vision, enabling effective learning across different domains. However, their performance under domain shift is yet underexplored. This paper investigates the zero-shot domain adaptation potential of FMs by comparing different backbone architectures and introducing novel domain-aware components that leverage domain related textual embeddings. We propose domain adaptive normalization, termed as Domino, which explicitly leverages domain embeddings during fine-tuning, thus making the model domain aware. Ultimately, Domino enables more robust computer vision models that can adapt effectively to various unseen domains.", "sections": [{"title": "1. Introduction", "content": "Computer vision has made tremendous progress in recent years, thanks to the development of powerful neural network architectures and large-scale datasets (Deng et al., 2009; Simonyan & Zisserman, 2014; Long et al., 2015; Ronneberger et al., 2015; Hao et al., 2020; Dosovitskiy et al., 2020; Li et al., 2022; Schuhmann et al., 2022). However, when faced with domain shift - a common problem in real-world applications where the target domain has different characteristics than the training data - the performance of these models drops significantly. This is particularly problematic for tasks that require zero-shot domain adaptation, where during training there is no sample available in the target domain, though we might have an estimation of the potential domains (Farahani et al., 2021; Liu et al., 2022).\nFoundation models (FMs), have become pivotal in various applications, from natural language processing to computer vision. Due to the large-scale pretraining, FMs have generalizable representations. Despite showing promising results on zero-shot classification tasks (Brown et al., 2020; Oquab et al., 2023), it is yet challenging to employ these models directly for dense prediction tasks, e.g., semantic segmentation (see 1st row in Table 2), necessitating the need of fine-tuning for the specific task. However, this will inevitably lead to knowledge forgetting and raise the question of how robust the adapted models are under domain shifts.\nTo investigate the zero-shot domain adaptation performance of FMs, we first compare different FM backbone architectures on the challenging semantic segmentation task. We evaluate various vision backbones in Table 1, such as DINOv2 (Oquab et al., 2023; Darcet et al., 2023), ResNet-50 and ResNetRS-420 (He et al., 2016; Bello et al., 2021), CLIP-based fine-tuning methods leveraging MaskCLIP (Dong et al., 2023), and Stable Diffusion (SD) (Rombach et al., 2022) based fine-tuning, i.e., VPD (Zhao et al., 2023). We observe that these models are generally not robust to domain shifts, where there is a considerable performance drop when tested on unseen domains.\nWe hypothesized that visual embeddings can vary considerably with different domains, making the model vulnerable to changes such as weather and lighting conditions. To mitigate this issue, we propose to incorporate textual domain embeddings and introduce domain adaptive normalization, termed as Domino during fine-tuning. We employed CLIP"}, {"title": "2. Method", "content": "Stable Diffusion has demonstrated astonishing text-to-image synthesis capability, thanks to their large-scale pretraining. Naturally, it has learned rich multi-modal representations. Recent work VPD (Zhao et al., 2023) has explored its potential for downstream applications, e.g., depth estimation and semantic segmentation. More specifically, they fine-tune the denoising UNet and extract intermediate features and cross-attention maps from it, which can provide semantically meaningful features (Hertz et al., 2023; Li et al., 2023a). Note that semantic classes are used as prompts for semantic segmentation task, and a text adapter is introduced as well. Further, a lightweight task-specific decoder taking extracted features is incorporated.\nDespite showing promising results on the in-domain evaluation, it remains unclear how this model can perform under domain shift. In other words, it's not yet explored how the prior knowledge of powerful Stable Diffusion can help the downstream applications, which is of greater interest, and the focus of this work. Additionally, we propose a novel domain-aware fine-tuning strategy, where we extract the domain embeddings with CLIP (see Section 2.2), and incorporate them into the fine-tuning pipe to enhance domain awareness (see Section 2.3)."}, {"title": "2.2. Automatic Domain Embedding Extraction", "content": "The domain concept is quite often assigned manually. Instead, we seek a way to automatically obtain the domain information. Prior work (Wang et al., 2023) has shown that CLIP (Radford et al., 2021) is capable of assessing the look and feel of images. Thus, we propose to leverage CLIP to automatically extract the domain embeddings, which can be further utilized to enhance domain-awareness during FM fine-tuning.\nTo calculate the domain embedding, we begin by defining base cases that describe the aspects of interest. For instance, in autonomous driving, related domains involve different weather conditions and time of the day. Each of these domain descriptions are encoded using the CLIP text encoder:\n$d_i = CLIP_{Text}(k_i) \\forall i = 1, ..., N,$\nwhere $K = \\{k_1, ..., k_n\\}$ are defined domain descriptions and $D = \\{d_1, ..., d_N\\}$ is the resulting description embeddings. Given an image, we can obtain the image embedding through the CLIP image encoder and compute the similarity with each individual description embedding. Finally, the domain embedding of the given image is computed via a weighted sum of all description embeddings:\n$I = CLIP_{Image}(x)$\n$\\alpha_i = Softmax(\\frac{I \\cdot d_i}{||I|| ||d_i||})$\n$W = \\sum_i \\alpha_i d_i,$"}, {"title": "2.3. Domain Aware Fine-tuning", "content": "Stable Diffusion naturally is capable of utilizing textual embedding, and thus we can simply combine the domain embedding with the original prompt embedding, i.e., class embedding, as illustrated in Figure 2. There are two possible ways of combination, namely addition or subtraction. When adding the domain embedding, we encourage the model to explore the domain information for prediction. While for subtraction, we essentially try to remove domain-related information, thus enforcing domain-invariant learning. As experimented in Table 2, we found the latter is more effective in improving the generalization performance.\nIn addition to being utilized by the Stable Diffusion backbone, we propose to incorporate the domain embedding in the segmentation decoder head to further enhance domain awareness. Inspired by SPADE (Park et al., 2019), we introduce domain adaptive normalization, termed as Domino. Specifically, we map the domain embedding through simple MLP layers into modulation parameters $\\gamma$ and $\\beta$.\n$f_{adp} = \\frac{f - \\mu_f}{\\sigma_f} \\gamma(W) + \\mu(W),$"}, {"title": "3. Experiments", "content": "Experimental Setup. In this study, we focus on the challenging task of semantic segmentation, which requires per-pixel semantic class prediction. We train the models on the Cityscapes (Cordts et al., 2016) training set, and evaluate the model's generalization performance on ACDC (Sakaridis et al., 2021). Cityscapes is an urban scene dataset with 19 semantic classes, which is collected mainly in Germany under good weather conditions during daytime. In contrast, ACDC contains more adverse weather conditions, such as fog, snow, rain and nighttime. Compared to Cityscapes, there is a significant domain shift, making it a challenging test dataset meanwhile a perfect testbed for assessing the model's generalization capability.\nFor evaluation, we use the mean intersection-over-union (mIoU) (Everingham et al., 2015). Similar to (Fahes et al., 2023), we also report the domain adaptation performance as the percentage of target domain mIoU (ACDC) over source domain mIoU (Cityscapes):\nmIoU% = $100 \\times \\frac{ACDC mIoU}{Cityscapes mIoU}$\nWe train all models on A100 GPUs using single-GPU training, using weighted cross-entropy loss. For a fair comparison, we train all models for 80,000 iterations. We use the AdamW optimizer (Loshchilov & Hutter, 2018) with a learning rate of 8e - 5 and weight decay le - 3. Diffusion transformer is using a different learning rate of 0.1 while the text encoder is frozen as in the VPD paper. We use poly learning rate scheduling to decrease the learning rate to 0 as training progresses.\nComparison of FM Backbones. In this comparison, we include a wide range of FMs, such as self-supervised DINOv2 (Oquab et al., 2023; Darcet et al., 2023), CLIP (Radford et al., 2021) and Stable Diffusion (Rombach et al., 2022).\nAdditionally, to provide a comparison against a similar model structure, we combine pre-trained ResNetRS-420's (Bello et al., 2021) in UNet form and fine-tune it on the Cityscapes dataset. We use CLIP-based methods with ViT and ResNet backbones, based on MaskCLIP+ (Dong et al., 2023).\nIn Table 1, we present the evaluation results of all models finetuned with cross entropy loss. We can see that VPD (built upon Stable Diffusion) performs the best on the Cityscapes validation set, as well as on the unseen ACDC, which presents a considerable domain shift. Notably, CLIP backbones and supervised models come close to the VPD performance on Cityscapes, but all of them show a significant performance degradation under domain shift. A surprising observation is the evident under-performance of the DINOv2 model. We attribute this to its need for a longer training time. In our experiments, we observed that while the validation performance of the other models kept increasing slowly nearing the end of our training schedule, DINOv2's validation scores kept increasing quickly, and it could have benefited from more training time. However, for a fair comparison, we have trained it using the same training schedule as the other models, which led to its underfitting on Cityscapes.\nAnother observation is that despite CLIP-based models also being pre-trained on large-scale image-text pairs, they have a substantial degradation on ACDC compared to Stable Diffusion. We hypothesize this is due to CLIP's tendency of catastrophic forgetting when fine-tuned fully, while being trained with a generative objective, Stable Diffusion is more tolerant in this regard. Ultimately, this validates our selection of Stable Diffusion as the FM backbone for further improvement of zero-shot domain adaptation.\nEffect of Domino. In Table 2, we present the comparison of the proposed domain-aware fine-tuning Domino with the original VPD (Zhao et al., 2023). We first examine the impact of architectural changes on performance, taking into account prior work suggesting that some foundational models perform better with frozen weights. To explore this phenomenon, we experimented using a frozen Stable Diffusion backbone (1st row in Table 2), which has the worst performance. This indicates the necessity of fine-tuning the entire backbone for the specific downstream task. We implemented both domain addition and subtraction, denoted as Domino-Add and Domino-Sub in Table 2. We observe that both variants outperform the baseline VPD in generalization performance on ACDC, which indicates the effectiveness of domain-aware fine-tuning. Notably, the Domino-Sub version achieves the best generalization results. This suggests that by encouraging domain-invariant representation learning during training, the model becomes more robust under domain shifts. We also see that Domino-Add can improve in-domain performance. We hypothesize that encouraging the usage of domain embedding can ease pattern learning as Cityscapes mostly consist of clear day images.\nEffect of Synthetic Data. We investigate the impact of incorporating synthetic data into our framework for zero-shot domain adaptation. Prior work has demonstrated that utilizing synthetic data during training can enhance performance for traditional models (Beery et al., 2020; Li et al., 2023b; 2024; Wang et al., 2024). However, the effectiveness of synthetic data with Foundation Models (FMs) is a topic of discussion, particularly as these models have been trained on massive real data. Yet, we hypothesized for improving domain generalization, it is crucial for the model to see diverse data from different domains, e.g., under different weather conditions. To explore this, we employ ALDM (Li et al., 2024) to generate synthetic data based on the labels of Cityscapes training set and diverse prompts.\nIn Table 3, we present the results of fine-tuning the base VPD model with varying ratios between real and synthetic data. Notably, combining synthetic data with real data leads to improved performance in both domains. As we increase the proportion of synthetic data, target domain performance also increases. However, when using synthetic data only, we observe a performance decrease in both domains. This finding suggests that relying solely on synthetic data is not beneficial, as the quality of synthetic might not be as good as real ones. This also highlights the importance of incorporating real data alongside synthetic data for foundational models during fine-tuning."}, {"title": "4. Conclusion & Discussion", "content": "In this study, we conducted extensive experiments comparing different foundation models as the backbone for zero-shot domain adaptation in semantic segmentation. We empirically observe that Stable Diffusion based VPD model achieves better generalization performance. We then demonstrated the proposed Domino with explicit usage of the domain information can significantly boost the model's generalization further. Our results indicate that domain embedding addition encourages the use of domain cues, which can be beneficial for improving the in-domain segmentation performance. In contrast, domain embedding subtraction encourages the use of more domain-invariant features which can enhance the generalization performance. Furthermore, we have found that incorporating synthetic data with a proper ratio during the foundational model fine-tuning is also beneficial for domain generalization performance."}]}