{"title": "Communication Compression for Tensor Parallel LLM Inference", "authors": ["Jan Hansen-Palmus", "Michael Truong Le", "Oliver Hausd\u00f6rfer", "Alok Verma"], "abstract": "Large Language Models (LLMs) have pushed the frontier of artificial intelligence but are comprised of hundreds of billions of parameters and operations. For faster inference latency, LLMs are deployed on multiple hardware accelerators through various Model Parallelism strategies. Our paper looks into the details on one such strategy - Tensor Parallel - and proposes to reduce latency by compressing inter-accelerator communication. We leverage fine grained quantization techniques to compress selected activations by 3.5 - 4.5x. Our proposed method leads up to 2x reduction of time-to-first-token (TTFT) with negligible model performance degradation.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have become essential across various applications due to their exceptional performance. As model performance tends to improve with increased parameter counts, LLMs have been significantly scaled in recent years, with contemporary models now reaching 500B+ parameters [Chowdhery et al., 2023].\nDeploying such large models for inference presents major challenges [Pope et al., 2022]. Tensor Parallel [Shoeybi et al., 2020] addresses this by splitting layers on multiple accelerators, enabling the execution of extremely large models and significantly reducing latency. However, Tensor Parallel demands accumulation of results from accelerators, as shown in Figure 1, and can lead to data communication bottlenecks [Zhuang et al., 2024, Agrawal et al., 2024], especially during the first auto-regressive inference step (the prefill phase).\nOne approach to mitigate these bottlenecks, and thus reduce model latency even further, is to quantize activations before communication, which reduces the time needed to accumulate results from accelerators in a Tensor Parallel group. However, the presence of outliers [Dettmers et al., 2022, Lin et al., 2023] complicates this strategy, necessitating fine-grained quantization approaches. We leverage such approaches proposed by Rouhani et al. [2023] to compress activations and demonstrate the potency of communication compression by measuring time-to-first-token (TTFT) in realistic inference scenarios using different inference hardware setups. We find that for hardware setups which have slower inter-accelerator bandwidths, the TTFT can be improved by 3.5 - 4.5x with negligible degradation of model performance."}, {"title": "Background", "content": ""}, {"title": "Parallel Inference", "content": "Given the massive parameter and operations requirement of LLMs [Dubey et al., 2024, Minaee et al., 2024], distributing their parameters and operations across a large number of accelerators is essential, which is achieved through different parallelism techniques. Among common parallelism techniques used to fit LLMs on multiple accelerators for inference [Shoeybi et al., 2020, Korthikanti et al., 2022, Rajbhandari et al., 2020, Zhuang et al., 2024], Tensor Parallel (TP) is usually the most widely used one because it allows a very effective way of reducing latency and scaling down model size per accelerator. Other parallelism strategies, such as Pipeline Parallelism or Sequence Parallel, are combined with TP to improve further upon it. TP has two possible sub-variants: Column-wise and Row-wise parallelism [Shoeybi et al., 2020], enable the partitioning of linear layers along the column or row dimension of weights respectively.\nWhile Tensor Parallel offers significant benefits, it also increases communication overhead, as results computed on individual accelerators must be frequently synchronized using collective operations [Clarke et al., 1994], [Nvidia, 2023]. This becomes particularly problematic during the prefill phase, where activation tensors for the entire token sequence need to be transmitted. This can lead to communication bottlenecks [Zhuang et al., 2024, Agrawal et al., 2024], especially if the inter-accelerator bandwidth is low. While Bian et al. [2024] explored various approaches to reduce communication overhead during training, to the best of our knowledge, this issue has not yet been addressed at inference-time by the research community."}, {"title": "Model Quantization", "content": "The quantization of weights and/or activations of LLMs [Zhao et al., 2024, Lin et al., 2023, Sheng et al., 2023, Kang et al., 2024, Xiao et al., 2024, Yuan et al., 2023, Frantar et al., 2023] has been a very active field of research. Quantization of weights to lower bit-widths reduces the memory needed to store parameters and also improves throughput due to better memory bandwidth utilization. Furthermore, quantization of activations and also computations allows reducing activation memory footprint and improves throughput as well by utilizing faster and cheaper computation engines on recent accelerators [NVIDIA, 2024].\nUnlike weights, activations are hard to quantize due to their dynamic nature [Zhao et al., 2024, Sheng et al., 2023] and the presence of outliers, which has been extensively studied in literature [Dettmers et al., 2022, Lin et al., 2023, Xiao et al., 2024]. Dettmers et al. [2022] found that accurately representing these outliers is critical for maintaining model performance: Even though outliers represent only a small fraction of input features, removing them leads to a significant degradation in Perplexity.\nA common approach to activation quantization is to normalize the values based on the tensor's absolute maximum. However, this can be problematic due to the presence of outlier values. Using the absolute maximum, which is dominated by these outliers, leads to poor representation of the remaining values in the tensor, as they become compressed into a narrow range and lose precision.\nTo address this issue, various solutions have been developed. One approach is to limit the impact of outliers by grouping tensor values and quantizing them together, which helps reducing the quantization error. Mixed precision methods [Zhao et al., 2024, Dettmers et al., 2022, Kang et al., 2024, Hooper et al., 2024], on the other hand, distinguish between outliers and non-outliers, using different data types for each group during quantization.\nSince we compress activations of specific layers, in the following section, we briefly review methods related to low-bit activation quantization."}, {"title": "Related Work", "content": ""}, {"title": "KV-cache Quantization", "content": "LLM inference involves generating tokens in an auto-regressive manner, and the majority of the computation time is spent accessing the KV-cache from memory [Hooper et al., 2024, Zirui Liu et al.,"}, {"title": "Weight and Activation Quantization", "content": "The primary objective of weights and activation quantization is to reduce the overall model size while accelerating inference [Wu et al., 2023, Zhao et al., 2024] by the use of specialized hardware, such as Nvidia's INT4 Tensor Cores [NVIDIA, 2024].\nZhao et al. [2024] proposed a mixed-precision technique that addresses the challenge of activation quantization by representing the 128 largest outlier channels with 8 bits per value. OCP Specification [2023] introduces low-bit data formats applicable to matrix multiplication operations. In this method, a block of values is encoded in a low-bit floating-point format along with a shared exponent. Given the strong performance of OCP Specification [2023] [Rouhani et al., 2023] data types, we based our work on their provided code [Mircosoft, 2024] and apply low-bit data types for activation compression experiments as well."}, {"title": "Method", "content": ""}, {"title": "Communication Compression", "content": "Figure la highlights our approach to communication compression in a TP setting. We aim to compress the partial results of each worker after row-wise TP linear layers, and decompress them before reduction in each worker. Since our method introduces extra computations which offsets slower inter-accelerator communication, we must strike a balance between computation and compression. We found that block-wise quantization as proposed in OCP Specification [2023] for low-bit compression is a strong contender to balance quantization error and compression latency. So, we evaluate a variety of low-bit data types using this fine-grained block-wise quantization scheme.\nWe also extended the data types available in [Mircosoft, 2024] to experiment with more variety of bit-widths to test higher compression. The following data types and parameters were evaluated:"}, {"title": "Model Evaluation", "content": "We measure the performance of compression approaches based on the compression rate, which is measured by the number of effective bits [Frantar et al., 2023, Lin et al., 2023], and the increase in Perplexity metric [Hooper et al., 2024, Zhao et al., 2024, Dettmers et al., 2022, Wei et al., 2023], relative to the compression-free model with 16 bit (FP16) activations. We show that each possible hyper-parameter of quantization data type proposed in OCP Specification [2023] can have varied effect on latency and model's Perplexity in Section 5.1."}, {"title": "Profiling", "content": "To show we can reduce communication bottlenecks by utilizing our most performant quantization approaches, we measure the TTFT of models of different sizes in a deployment scenario. We base our profiling on the code provided by IBM [2023] which uses torch.compile [Wen, 2023] to speed up inference of Llama 2 models along with TP. The architecture of the Llama 2 model family is very similar to other state-of-the-art LLMs and therefore provides valuable compression insights. We extended the code to add communication compression. In our profiling setup, each worker in a TP group of N workers compresses output activations of each row-wise linear layer before communication, and decompresses N-1 activations gathered from all other workers. We finally reduce the decompressed activations using torch. sum as shown in Figure 1b."}, {"title": "Experiments", "content": ""}, {"title": "Optimal Compression Scheme Search", "content": "To pick an optimal compression scheme for each model we first evaluate the Perplexity of various state-of-the-art LLMs on 10% of the Wikitext train dataset, which we deemed sufficient for this purpose, Merity et al. [2016] by testing a subset of combinations of data types and block sizes. Please refer to A.1 for further details how we restricted the search space of quantization hyper-parameters prior to these experiments.\nNext, we pick the final compression schemes for each model using the results from Table 1: To ensure minimal model performance degradation, we consider only combinations which lead to an increase in Perplexity of less than 3%. From the remaining candidates, we then choose the one with the largest compression ratio (lowest effective bits). This procedure ensures a good balance between compression ratio and quantization accuracy. Finally, to validate the performance of our chosen compression schemes, we evaluate them on the entire Wikitext test set (Table 2). We observe minimal performance degradation with respect to 16 bit uncompressed communication, while being able to compress tensors by a factor of roughly 3.3X."}, {"title": "Measuring TTFT Speedups", "content": "We demonstrate the potential speedups achievable by compressing communication in 3. As IBM [2023] only supports the Llama 2 model family, we decided to conduct measurements using the FP4 quantization scheme deemed best by Rouhani et al. [2023], which has a similar compression ratio to the schemes we picked in 5.1. We achieve a speed-up between 2x - 1.2x based on the TP degree and hardware setup. We profile our method on cutting-edge NVIDIA accelerators: L4, and A100 which are accessed through Google Cloud Platform [Google, 2024]. L4 GPUs in a node are connected with PCIe Gen4 x16 and have 64GB/s bandwidth, while A100 have 600 GB/s bidirectional any-to-any bandwidth. These both types of GPU provide good coverage of computing throughput(FLOPs/sec) and GPU-GPU bandwidth to showcase communication compression benefits. Generally, setups containing more than two L4 GPUs greatly benefit from our approach of communication compression. Due to the rather slow interconnect, these setups are heavily bottle-necked by communication"}, {"title": "SOTA Comparison", "content": "We compare our compression method with the relevant state-of-the-art method described in [Bian et al., 2024] which proposes multiple learned and non-learned methods for communication com-pression. Since our method targets inference-only non-learned optimization, we compare to the two fastest non-learned approaches: channel-wise INT quantization, and TopK compression which keeps the K largest magnitudes and zeroes all other values.\nIn Table 4, we show that both of their compression techniques are lead to much higher degradation of Perplexity metric. INT4 compression offers substantial speedups due to the minimal computational overhead, but shows higher Perplexity degradation when compared to fine-grained quantization approach. Improving the accuracy of this approach is left for future work."}, {"title": "Conclusion and Limitations", "content": "Our work builds on the foundation set by Rouhani et al. [2023] and OCP Specification [2023] by incorporating additional data types and tuning parameters to optimize compression. To achieve this, we developed a model-dependent hyper-parameter selection procedure that effectively balances the compression rate with model accuracy. The proposed compression method reduces activation sizes by a factor of 3.5 to 4.5x, with minimal degradation in performance which results in an improvement of LLMS' TTFT by a factor of 1.2 to 2x, depending on the hardware setup.\nWhile our compression methods shows substantial TTFT improvements for some hardware configu-rations, it does not lead to improvement when inter-accelerator bandwidth is substantially high and inference is not bandwidth-bound anymore. Furthermore, the latency introduced by compression algorithms can offset any communication compression speedup, so compressing more but with slower algorithms is not helpful. It could be possible to accelerate compression by utilizing specialized hardware, but such strategies are beyond the current scope and are left for future investigation. Furthermore, by improving model quantization techniques and adopting 8 bit matrix multiplications, the communication size of TP linear layers could be reduced easily without extra compression costs.\nOur current profiling setup could also be enhanced further to take into account in-flight batching and other parallelism strategies and provide more extensive details of throughput improvements."}, {"title": "Appendix / supplemental material", "content": ""}, {"title": "Ablation over quantization hyper-parameters", "content": "We ablate over the extended space of possible data types and parameters (4.1). We aim at restricting our parameter space by picking parameters (Table 5) which promise good quantization accuracy for different compression ratio requirements. As scale data type, E5M0 was generally the best option, as lower bits for scale introduce unacceptable Perplexity degradation while higher do not lead to any improvement. For value bits we"}]}