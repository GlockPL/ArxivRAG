{"title": "An Empirical Study of Mamba-based Pedestrian Attribute Recognition", "authors": ["Xiao Wang", "Weizhe Kong", "Jiandong Jin", "Shiao Wang", "Ruichong Gao", "Qingchuan Ma", "Chenglong Li", "Jin Tang"], "abstract": "Current strong pedestrian attribute recognition models are developed based on Transformer networks, which are computationally heavy. Recently proposed models with linear complexity (e.g., Mamba) have garnered significant attention and have achieved a good balance between accuracy and computational cost across a variety of visual tasks. Relevant review articles also suggest that while these models can perform well on some pedestrian attribute recognition datasets, they are generally weaker than the corresponding Transformer models. To further tap into the potential of the novel Mamba architecture for PAR tasks, this paper designs and adapts Mamba into two typical PAR frameworks, i.e., the text-image fusion approach and pure vision Mamba multi-label recognition framework. It is found that interacting with attribute tags as additional input does not always lead to an improvement, specifically, Vim can be enhanced, but VMamba cannot. This paper further designs various hybrid Mamba-Transformer variants and conducts thorough experimental validations. These experimental results indicate that simply enhancing Mamba with a Transformer does not always lead to performance improvements but yields better results under certain settings. We hope this empirical study can further inspire research in Mamba for PAR, and even extend into the domain of multi-label recognition, through the design of these network structures and comprehensive experimentation.", "sections": [{"title": "1. Introduction", "content": "Pedestrian Attribute Recognition (PAR) [48] is a widely exploited research topic in the computer vision (CV) community. It aims to recognize human attributes from a set of attribute descriptions, such as short black hair, wearing hats, with back bag, etc. On the one hand, pedestrian attribute recognition can be used to describe the appearance and motion characteristics of pedestrians, playing a crucial role in understanding them; on the other hand, pedestrian attributes can serve as mid-level semantic representations, assisting in improving the performance of other visual tasks, including pedestrian detection [55], person re-identification [54, 59], and more.\nWith the help of deep learning, various deep PAR models are proposed, including CNN (Convolutional Neural Networks), RNN (Recurrent Neural Networks), GNN (Graph Neural Networks), Transformer, etc. Specifically, MTCNN [1] uses CNN to extract features and combine them in a multi-task manner to solve the PAR task. JRL [44] and GRL [56] are solving PAR as a sequence prediction task using LSTM to model the association between attributes. PARformer [7] and VTB [3] use Transformer to solve PAR task, the difference is that VTB introduces text modality. Although existing PAR models work well in simple scenarios, the recognition performance in challenging scenarios is still limited (e.g., low illumination, cross-domain). In addition, the training/inference cost (O(N2)) is rather high due to the utilization of self-attention in Transformer networks. Based on the observations above and reflections, it is natural to raise the following question: how can we design a new pedestrian attribute recognition framework that achieves a comparable or even better performance compared to the Transformer-based models, meanwhile, reducing the cost significantly?\nRecently, the State Space Model (SSM) has drawn more and more attention in the artificial intelligence community due to its linear complexity (O(N)) and good performance. It has been widely utilized for visual tracking [17], image-/video-based classification [27, 60], and time series analysis [49]. The experimental results reported in the SSM survey [47] demonstrate that the Vim-S [60] based attribute recognition model performs better than the ViT-S based version on the PA100K [31] dataset, but achieves worse results on the PETA [5] dataset. Hence, crafting innovative Mamba-inspired PAR architectures that deliver uniform performance enhancements across benchmarks, while also maintaining low training and testing overhead, poses an ongoing challenge.\nInspired by these works, in this paper, we attempt to conduct extensive experiments to validate the effectiveness of Mamba-based pedestrian attribute recognition. Generally speaking, as illustrated in Fig. 1, we design two representative Mamba-based PAR frameworks, i.e., image-based multi-label classification PAR framework and image-text fusion-based PAR framework. Our experimental results demonstrate that the pure Mamba-based model performs better than the image-text fusion based model when the VMamba [33] is adopted, but the opposite when the Vim [60] is utilized. Furthermore, we consider the design of introducing Transformers to create a more powerful and efficient hybrid Mamba-Transformer backbone architecture for better feature representation extraction, in order to improve the performance of attribute recognition, as illustrated in Fig. 3. By conducting extensive experiments, we find that when utilizing Mamba for the dense fusion of hierarchical Transformer features, i.e., the Fig. 3 (e), the final results can beat the ViT-B based PAR framework. Also, when distilling from Transformer to Mamba for PAR, i.e., the Fig. 3 (g, h), the performance can also be improved over the Vim-S and ViT-S based version.\nTo sum up, we conclude the key contributions of this paper with the following three aspects:\n1). We exploit the integration of the recently released Mamba architectures with the PAR task by proposing two representative Mamba-based PAR frameworks, i.e., the image based multi-label classification framework, and image-text fusion based PAR. Our experimental results indicate that different visual Mamba models are suitable for different PAR frameworks.\n2). We propose several variations of hybrid Transformer-Mamba networks to improve the final attribute recognition performance further and hope it provides comprehensive guidelines for Mamba-based attribute recognition.\n3). We conduct extensive experiments on multiple PAR benchmark datasets, including PA100K [31], PETA [5], RAP-V1 [26], RAP-V2 [25], WIDER [29], PETA-ZS [20], and RAP-ZS [20], to demonstrate the effectiveness and efficiency of the proposed Mamba-based PAR frameworks.\nThis paper is organized as follows: Firstly, we review the related works on pedestrian attribute recognition and SSM; then, we will introduce the pure Mamba-based and Mamba-based vision-language fusion PAR frameworks; we also propose different variations of hybrid Mamba-Transformer networks for the PAR. After that, we will jump into the experiments, compare them with other state-of-the-art PAR models, and discuss the influence of different settings for Mamba-based PAR. We also give some analysis of the model efficiency and visualizations. Finally, we conclude this paper and propose possible future works worth to be studied."}, {"title": "2. Related Works", "content": "In this section, we will introduce the related works on pedestrian attribute recognition, and state space model. More details can be found in the following surveys [45, 47, 48]."}, {"title": "2.1. Pedestrian Attribute Recognition", "content": "Pedestrian attribute recognition has made significant progress over the years, with many methods proposed by researchers achieving promising results. Early methods primarily involved feeding images into CNN networks for multi-task training, and sharing parameters between networks to obtain richer features. For example, MTCNN [1] adopted this multi-task training approach, using CNN to extract image features and allowing knowledge sharing among different attribute categories. Due to the correlations between different attributes, sequential prediction models were introduced into pedestrian attribute recognition research. JRL [44] modeled the contextual information between pedestrians and the contextual information between attributes using LSTM, transforming pedestrian attribute recognition into a sequence prediction problem. Building upon JRL, GRL [56] divided the attribute list into multiple attribute groups and modeled the spatial and semantic correlations between groups using LSTM. With the development of deep learning networks, attention models began to be used in pedestrian attribute recognition. Specifically, HydraPlus-Net [32] introduced a multi-directional attention module to extract multi-scale attention features for obtaining more comprehensive pedestrian features. Transformer, a model proposed by Vaswani et al. [43], has been widely used in the field of natural language processing and has achieved great success. Some researchers have also applied it to pedestrian attribute recognition. Fan et al. [7] proposed a pure Transformer-based PAR network, using a Transformer-based backbone network for image feature extraction to obtain more discriminative features. Cheng et al. [3] proposed VTB, encoding pedestrian images into visual features and encoding attribute annotations into text features using pre-trained text encoders, and using Transformer for multimodal fusion. However, due to the global attention mechanism of the Transformer, its complexity grows quadratically with the length of the input sequence. Gu et al. [9] introduced Mamba, a type of State Space Model that optimizes the complexity of sequence processing to linear. Therefore, we introduce a state space modal for optimization, improving computational efficiency while ensuring the accuracy of pedestrian attribute recognition."}, {"title": "2.2. State Space Model", "content": "The State Space Model (SSMs) [9, 11\u201313] is a mathematical framework used to model dynamic systems, which has received widespread attention in recent years due to its excellent performance in handling long sequence data. S4 [12], as a pioneering work in using state space modal for long sequence processing, introduced Hippo [10] to address the issue of long-range dependencies in sequence modeling, significantly improving model performance. S4ND [34] extends the spatial state model to multidimensional signals, enabling it to model large-scale visual data as continuous multidimensional signals, laying the foundation for SSMs\u2019s use in multimodal tasks. Mamba [9] introduces a selective mechanism for information processing while balancing training, inference efficiency, and model effectiveness, outperforming Transformer [43] in natural language processing. After Mamba, there appeared works applying Mamba to the visual domain, such as Vim [60] and VMamba [33]. Both of these works show the potential of Mamba in vision tasks. Vim proposes a bidirectional Mamba to model images, and VMamba proposes a four-direction cross-scan module. Through this multi-direction sequence order, it is more effective to model two-dimensional image data and maintain linear computational complexity. Mamba-2 [4] is the improvement of Mamba, and proposes SSD (State Space Duality) framework, which improves Mamba by 2-8 times speedup and obtains better performance. Our work explores the potential of Mamba in handling visual-text fusion features and comprehensively applies Mamba and its derivative modules to pedestrian attribute recognition."}, {"title": "3. Methodology", "content": "In this section, we will first give a brief review of the State Space Model (SSM), then, we will introduce two Mamba-based PAR frameworks, including the pure image-based multi-label classification and image-text fusion based PAR framework. Further, we design eight variations of hybrid Mamba-Transformer networks for the PAR task."}, {"title": "3.1. Preliminary: State Space Model", "content": "The State Space Model (SSM) originates from the classic Kalman filter [22] algorithm which introduces linear filtering. It converts a one-dimensional sequence into an N-dimensional hidden state for output. The calculation formula is as follows,\n$h'(t) = Ah(t) + Bx(t),$\n$y(t) = Ch(t) + Dx(t).$\nwhere $x(t) \\in R^1$ and $h'(t) \\in R^N$ are the input sequence and the derivative of the hidden state respectively. $A \\in R^{N \\times N}$ is the state matrix, $B \\in R^{N \\times L}$ is the input matrix, $C \\in R^{L \\times N}$ is the output matrix, and $D \\in R^{L \\times L}$ is the feed-through matrix. The above formula is a continuous-time SSM. In order to facilitate the deep learning algorithm's understanding of the input, we need to discretize the matrixes through some methods, such as zero order hold (ZOH), etc. Specifically, Gu et al. [12] propose structured state-space sequence models (S4), which convert continuous parameters into discrete ones by introducing timescale parameter $\u0394$. The formula is as follows,\n$A = exp(\\Delta A),$\n$B = (\\Delta A)^{-1}(exp(\\Delta A) - I) \\cdot \\Delta B,$\n$C = C.$\nwhere $A, B$, and $C$ are discrete parameters of the system from $A, B$ and $C$ respectively. Furthermore, the formula can be shown that,\n$h_t = Ah_{t-1}+ Bx_t,$\n$y_t = Ch_t.$\nwhere the $D$ matrix can sometimes be ignored as a residual. Based on the aforementioned models, in the field of computer vision, Vim [60] and VMamba [33] have been proposed and received considerable attention. This paper will explore SSM-based pedestrian attribute recognition frameworks using these two models."}, {"title": "3.2. When Mamba Meets PAR", "content": "In this section, we mainly focus on two widely used PAR frameworks, i.e., the image-based multi-label classification framework [7, 24, 42], and the image-text fusion based PAR framework [3, 46], as shown in Fig. 2. Considering that image-based multi-label recognition can be seen as a special case of the image-text integration framework, that is, by removing the attribute label encoding module, this paper will focus on elaborating the PAR process of the image-text integration framework. The details of multi-label recognition will not be further discussed.\nGenerally speaking, our proposed Mamba-based visual-language fusion PAR architecture consists of three main modules, i.e., the Mamba vision backbone which is composed of a Vim [60] network, a Mamba-based text encoder for extracting the attribute semantic information, and a visual-semantic fusion (VSF) Mamba module for image-text feature interaction and fusion. Finally, we adopt a classification head for pedestrian attribute recognition. We will introduce these modules in the subsequent paragraphs respectively.\nMamba for Input Representation. Given a pedestrian image $I$, we need to predict its attributes from an attribute set $A = \\{a_1, a_2, ..., a_L \\}$, $L$ is the number of person attributes, and the ground truth annotations $Y$ are provided for the supervised learning of our PAR framework. In our framework, we adopt the vision Mamba network Vim [60] as an example to demonstrate how to encode the given person image into visual features $X_v$. Note that, both the Vim [60] and VMamba [33] can be adopted for the encoding of pedestrian images, as validated in our experimental results. Firstly, we divide the image into equally sized image patches based on a specified patch size, then, these patches are flattened and fed into a linear projection to obtain the patch tokens $X_v \\in R^{P^2 \\times D}$. Positional embeddings $P.E.$ are added to each token to better capture the spatial positions of each patch.\nNext, we feed the tokens into a stack of Vim [60] blocks, $MV = \\{MY_1,MY_2,..., MY_N \\}$, where N is the number of Vim blocks, thus, we can obtain the visual tokens $X_v$. More in detail, as shown in Fig. 2 the input tokens are firstly processed using the normalization layer and then transformed into $x$ and $z$ using projection layers. For the $x$, the forward and backward processing branches are adopted for the vision feature learning, and each branch contains both Conv1d and SSM layers. For the $z$, an activation layer $\\sigma(\\cdot)$ is adopted to enhance the feature and multiply with the output of the forward and backward processing branches respectively. The compute procedure can be simply written as:\n$\\sigma(z) * FSSM(Conv1d(x)) + \\sigma(z) * BSSM(Convld(x))$\nThe output features are added and projected as the output of each Mamba block.\nTo better help the framework understand what is human attributes, the attribute labels defined on the whole dataset are usually directly integrated into the PAR framework. Given the attribute set $A$, we use the pre-trained BERT [23] tokenizer to embed the attributes into semantic tokens $X_t$. These semantic tokens are further enhanced by the text Mamba [9] blocks $MT = \\{MT_1,MT_2,..., MT_N \\}$ and the output attribute tokens are fed into vision-language aggregation module which will be introduced in the following paragraphs.\n[Optional] Mamba for Vision-Language Aggregation. Previous purely visual PAR methods struggle to effectively correlate attributes with visual features, and the inconsistency of optimization objectives across multiple attribute classifications sharing the same features leads to interference issues. We address these two problems through visual-language interaction. In our visual-language interaction processing, we integrate the extracted visual and textual features, denoted as $X = [X_v, X_t]$, and input them into the Vision-Semantic Fusion (VSF) Mamba module. This module, consisting of a stack of Mamba blocks, is designed to model the relationship between attributes and visual data across various layers. Ultimately, we employ the textual features $X_t$ that are aggregated with visual information for classification.\nPAR Classification Head and Loss Function. By using the Mamba networks to obtain effective feature representations, we input the features to the pedestrian attribute recognition prediction head for multi-label prediction. Specifically, our prediction head is composed of multiple feed-forward networks (FFN). It inputs features of pedestrian attributes to FFNs, outputs the respective category of each input, and completes the final prediction. Specifically,\n$P = Sigmoid(FFN(X))$\nwhere $P = \\{P_1,P_2,...,P_L\\}$, and $L$ is the number of attributes. We use the weighted cross-entropy loss as the loss function for our prediction which can be formulated as:\n$L = - \\sum_{j=1}^L w_j (y_j log(p_j) + (1 - y_j) log(1 - p_j))$\nwhere $w$ is positively correlated with the attribute positive ratio in the training set."}, {"title": "3.3. Hybrid Mamba-Transformer for PAR", "content": "In our experiments, we find that the pure VMamba [33] based PAR framework performs better than the image-text fusion based one. Therefore, in this section, we exploit new network architectures to aggregate the VMamba [33] with Transformer networks for better pedestrian attribute recognition. Specifically speaking, we design eight variations of hybrid Mamba-Transformer networks to validate which one performs better for the PAR task. Due to the limited space in this paper, we briefly introduced the computation procedures of these hybrid Mamba-Transformer networks for the PAR task. For more details on these models, please check our source code.\n\u2022 Parallel Fusion (PaFusion) is shown in Fig. 3 (a), where we connect the corresponding layers in the two models ViT-B and Vim-S in parallel, then add them together and feed into the next layer uniformly. It should be noted that the feature dimension of Vim-S is 384-D, while the feature dimension of ViT-B is 768-D. Thus, linear mapping is performed on the Vim features before feature addition each time. The integrated features are also subjected to reverse linear mapping for input to the next layer of Vim. Since Vim-S contains 24 Mamba layers and ViT-B contains 12 Transformer layers, we connect two Vim layers and one ViT layer in parallel in our practical implementation.\n\u2022 Non-alternating Serial Fusion (N-ASF) As shown in Fig. 3 (b), we directly connect the Vim-S and ViT-B to build the N-ASF PAR model. After a couple of Vim-S layers, we perform a dimension transformation and feed it into four ViT-B blocks for the feature processing in the next step.\n\u2022 Alternating Serial Fusion (ASF) is shown in Fig. 3 (c), where we alternately connect two models of ViT-B and Vim-S layers in serial. There is a linear mapping between the two-layer connections due to the inconsistent number of layers. We connect one layer of ViT to two layers of Vim in series.\n\u2022 Mamba-enhanced Transformer (MaFormer) is shown in Fig. 3 (d), where we use Vim-S as a complementary information extractor to integrate the features extracted by Vim and the current layer input before feeding into ViT.\n\u2022 Mamba for Hierarchical Dense Fusion of Transformer (MaHDFT) is shown in Fig. 3 (e), where we consider Vim-S as a processor that integrates the outputs of all layers of ViT. We use the ViT-B model trained in advance on the PA100k dataset and then freeze it. Following this, we integrate the outputs of each layer of ViT into four layers of Vim for integration, and then feed the outputs of Vim into the classification head for label prediction. Due to the limited memory of our used RTX3090 GPUs, it is not possible to integrate all tokens directly (2352 total tokens), so each output layer is first passed through a convolutional layer to reduce the number of tokens to 49 (588 total tokens). It should be mentioned here that we retain the classification head of ViT-B on the PA100K dataset for generating the labels corresponding to the final output features of ViT. The classification head used for Vim is recreated, and after obtaining the two sets of label distributions, we average them to obtain the final label.\n\u2022 Adapter for Mamba-Transformer Fusion (AdaMTF) is shown in Fig. 3 (f), where we connect Vim and ViT backbone as a whole in parallel, and introduce adapter in the corresponding layer to perform feature interaction, where the adapter is an MLP in our implementation.\n\u2022 Knowledge Distill (KDTM) is shown in Fig. 3 (g), where we focus on Vim and use ViT as the teacher network to perform feature- or logit-level distillation on Vim to enhance the feature representations.\n\u2022 Mamba-guided Knowledge Distillation Fusion (MaKDF) is shown in Fig. 3 (h), where we introduce a Vim module on top of the direct distillation to perform a distillation transition from the ViT teacher network and then integrate this feature back into the original Vim feature. This avoids the difference between ViT and Vim feature extraction, which brings performance damage to Vim."}, {"title": "4. Experiments", "content": "In our experiments, seven widely used PAR benchmark datasets are evaluated, including PA100K [31], PETA [5], RAP-V1 [26], RAP-V2 [25], WIDER [29], PETA-ZS [20], and RAP-ZS [20]. A brief introduction to these datasets is given below:\n\u2022 PA100K [31] dataset is the largest pedestrian attribute recognition dataset, which contains 100,000 pedestrian images and 26 binary attributes. In our experiments, we split them into a training and validation set of 90,000 images, and a test subset of the remaining 10,000 images.\n\u2022 PETA [5] dataset contains 19,000 outdoor or indoor pedestrian images and 61 binary attributes. These images are divided into 9500 as the training subset, 1900 as the validation subset, and 7600 as the test subset. In the experiment, we selected 35 pedestrian attributes according to the method of [5].\n\u2022 RAP-V1 [26] dataset contains 41585 pedestrian images and 69 binary attributes, of which 33,268 images are used for training. In the current work, 51 attributes are usually selected for training and evaluation.\n\u2022 RAP-V2 [25] dataset has 84,928 pedestrian images and 69 binary attributes, of which 67,943 are used for training. We selected 54 attributes to train and evaluate our model.\n\u2022 WIDER [29] dataset is divided into a training and validation set of 28,345 images and a test set of 29,179 images with a total of 14 attribute labels. Following the default setup, the train-val set is used for training and the performance on the test set is evaluated.\n\u2022 PETA-ZS [20] dataset is proposed by Jia et al., based on the PETA dataset, following a zero-shot protocol. The training, validation, and test sets consist of 11,241 | 3,826 | 3,933 samples respectively. For our experiments, we selected 35 common attributes according to Jia et al. [20].\n\u2022 RAP-ZS [20] dataset is developed based on the RAPv2 dataset, where the training, validation, and testing sets contain 17062, 4628, and 4928 pedestrian images, respectively. There is no shared personal identity between training and inference data. In the experiment, we selected 53 attributes for evaluation following Jia et al. [20].\nTo evaluate our proposed Mamba-based pedestrian attribute recognition algorithm and other SOTA PAR models, we adopt the following evaluation metrics, including mA, Accuracy, Precision, Recall, and F1-measure. For the mA can be expressed as:\n$mA = \\frac{\\sum_{i=1}^C AP_i}{C}$\nwhere $AP$ is the area under the precision-recall curve for the attribute\u017c, and $C$ is the total number of attributes. The Accuracy can be expressed as:\n$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$\nwhere TP is a positive sample predicted correctly, TN is a negative sample predicted correctly, FP is a negative sample predicted incorrectly, and FN is a positive sample predicted incorrectly. The Precision, Recall, and F1-measure can be expressed as:\n$Precision = \\frac{TP}{TP + FP}, Recall = \\frac{TP}{TP + FN},$\n$F1 = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}$"}, {"title": "4.2. Implementation Details", "content": "In our experiments, both VMamba-B and Vim-S versions are used as visual feature extractors. VMamba-B is a vision Mamba network in four stages with 2, 2, 27, and 2 layers in each stage, and the feature dimension is 768-D. Vim-S is a 24-layer vision Mamba model with 384 feature dimensions. For the text modality, the original Mamba model Mamba-130M, with a feature size of 768 is used. Note that, random cropping and flipping operations are employed for data augmentation.\nFor the detailed parameters, we train the model for 100 epochs using the Adam [37] optimizer. When using VMamba-B, we set the learning rate to 8e-5 and the batch size to 16. When using Vim-S, we set the learning to 2.5e-5 and the batch size to 64. Our source code is implemented using Python and the deep learning framework PyTorch [35]. The experiments were conducted on a server with GPU RTX3090s. More details can be found in our source code."}, {"title": "4.3. Comparison on Public PAR Benchmarks", "content": "In this section, we report the recognition results on seven datasets and compare them with existing state-of-the-art pedestrian attribute recognition algorithms. Note that the results for VTB* were obtained by replacing VTB\u2019s backbone network with ViT-L/14. Visualization of attribute predictions of some person images is given in Fig. 5.\nResults on PETA dataset. As shown in Table 1, the evaluated MambaPAR model performs well on most evaluation metrics, with different branches and backbone networks showing varying results. Specifically, the VMamba-B visual branch model achieved mA, Acc, Prec, Recall, and F1 metrics of 86.28, 80.54, 87.45, 87.95, and 87.45, respectively; the VMamba-B visual plus language branch model achieved 85.01, 78.47, 85.12, 87.49, and 86.00; the Vim visual plus language branch model achieved 81.45, 74.92, 82.68, 84.27, and 83.15; and the Vim visual plus text branch model achieved 84.25, 76.07, 82.73, 87.20, and 84.56. Interestingly, we can find that the attribute labels are useful for the Vim-S based PAR, but not for the VMamba-B based version. Compared to the Transformer-based pedestrian attribute recognition models, our VMamba visual branch model surpassed VTB (ViT-B/16, 85.31/79.60/86.76/87.17/86.71) and VTB* (ViT-L/14, 86.34/79.59/86.66/90.67/88.86) in the mA, Acc, Prec, Recall, and F1 metrics. Therefore, we can conclude that our model achieves competitive results on the PETA dataset, verifying the feasibility of the Mamba model for pedestrian attribute recognition.\nResults on PA100K dataset. As shown in Table 1, our MambaPAR model also performs well on the PA100K dataset. The VMamba-B visual branch model achieved mA, Acc, Prec, Recall, and F1 metrics of 83.63, 81.11, 87.59, 89.9, and 88.40, respectively; the VMamba-B visual plus language branch model achieved 81.50, 79.55, 87.54, 88.03, and 87.35; the Vim visual plus language branch model achieved 79.28, 76.77, 85.24, 86.58, and 85.44; and the Vim visual plus language model achieved 80.87, 79.33, 86.48, 88.79, and 87.21. The VMamba-B visual branch model's performance is close to that of VTB* (85.3/81.76/87.87/90.67/88.86). These experimental results indicate that the exploited Mamba-based PAR frameworks we explored have achieved preliminary success.\nFrom the experimental results reported in the PETA and PA100K datasets, we can find that the VMamba-B achieves the best results over other Mamba-based PAR models. Therefore, we only report the results of pure VMamba-B based PAR framework for the rest of the datasets.\nResults on RAP-V1 dataset. As shown in Table 2, our VMamba-B visual branch model achieved mA, Acc, Prec, Recall, and F1 metrics of 83.12, 69.47, 78.03, 84.81, and 80.88, respectively. In comparison, the strong baseline VTB* achieved 83.69, 69.78, 78.09, 85.21, and 81.10 on these metrics, and our results are very close to theirs. Meanwhile, our model's performance is also close to the DRFormer [42], which is also developed based on Transformer. This fully demonstrates that the performance of our Mamba-based pedestrian attribute recognition model is comparable to or even surpasses Transformer-based models.\nResults on RAP-V2 dataset. As shown in Table 2, our VMamba-B visual branch model achieved mA, Acc, Prec, Recall, and F1 metrics of 81.91, 68.08, 76.41, 84.38, and 79.83, respectively, outperforming the Transformer-based VTB* (81.36/67.58/76.19/84.00/79.52). This further proves the effectiveness of our proposed pedestrian attribute recognition model.\nResults on WIDER dataset. As shown in Table 3, our model achieved mA of 89.38, surpassing VTB (ViT-B/16, 88.2), which further validates the feasibility of our model. Compared with the pre-trained vision-language CLIP [36] model based PAR, e.g., the VTB* and PromptPAR, the VMamba-based model is still inferior to these models. In our future works, we will consider knowledge distillation strategies to further augment the performance of VMamba-based PAR.\nResults on PETA-ZS dataset. In addition to standard evaluations, we also evaluated the model's performance on datasets using a zero-shot setting. According to the results reported in Table 4, our model achieved mA, Acc, Prec, Recall, and F1 metrics of 74.43, 60.56, 73.12, 74.51, and 73.37, respectively, which are close to or even better than VTB (ViT-B/16, 75.13/60.50/73.29/74.40/73.38) on certain metrics.\nResults on RAP-ZS dataset. As shown in Table 4, the experimental results on the RAP-ZS dataset show that VTB based on ViT-B/16 achieved mA, Acc, Prec, Recall, and F1 metrics of 75.76, 64.73, 74.93, 80.85, and 77.35, respectively, while our experimental results were more excellent, reaching 77.34, 66.64, 75.37, 83.57, and 78.86, respectively. These results indicate that our model also performs excellently in zero-shot settings, and we believe the key reason is the high adaptability of the Mamba model to pedestrian attribute recognition."}, {"title": "4.4. Ablation Study", "content": "Effects of Vision-Semantic Fusion for PAR. As shown in Table 1, we conducted ablation experiments on the PA100K and PETA datasets for the visual semantic fusion (VSF) module. The effectiveness of this module has been validated by integrating with the Transformer networks, but not with the Mamba networks. Thus, we exploit two vision Mamba models, i.e., the VMamba-B and Vim-S, to check whether VSF still works for the Mamba-based PAR framework. It is easy to find that the pure Vim-S based model achieves (PA100K) 81.45, 74.92, 82.68, 84.27, 83.15 and (PETA) 79.28, 76.77, 85.24, 86.58, 85.44 in the five indicators of mA, Acc, Prec, Recall, and F1, respectively. After introducing the VSF module highlighted in the red dashed rectangle, the results reach 80.87, 79.33, 86.48, 88.79, 87.21 on the PA100K dataset and 84.25, 76.07, 82.73, 87.20, 84.56 on the PETA dataset, respectively, which proves the effectiveness of the semantic labels for the Vim-based PAR framework. However, we also find that similar conclusions do not hold for the VMamba-B based PAR framework, that is to say, the performance drops when adopting the semantic labels of all attributes.\nTransformer vs Mamba for PAR. As shown in Table 5, we compare the performance of ViT-S, Vim"}]}