{"title": "An Empirical Study of Mamba-based Pedestrian Attribute Recognition", "authors": ["Xiao Wang", "Weizhe Kong", "Jiandong Jin", "Shiao Wang", "Ruichong Gao", "Qingchuan Ma", "Chenglong Li", "Jin Tang"], "abstract": "Current strong pedestrian attribute recognition models\nare developed based on Transformer networks, which are\ncomputationally heavy. Recently proposed models with lin-\near complexity (e.g., Mamba) have garnered significant at-\ntention and have achieved a good balance between accu-\nracy and computational cost across a variety of visual tasks.\nRelevant review articles also suggest that while these mod-\nels can perform well on some pedestrian attribute recog-\nnition datasets, they are generally weaker than the cor-\nresponding Transformer models. To further tap into the\npotential of the novel Mamba architecture for PAR tasks,\nthis paper designs and adapts Mamba into two typical\nPAR frameworks, i.e., the text-image fusion approach and\npure vision Mamba multi-label recognition framework. It\nis found that interacting with attribute tags as additional\ninput does not always lead to an improvement, specifi-\ncally, Vim can be enhanced, but VMamba cannot. This\npaper further designs various hybrid Mamba-Transformer\nvariants and conducts thorough experimental validations.\nThese experimental results indicate that simply enhancing\nMamba with a Transformer does not always lead to per-\nformance improvements but yields better results under cer-\ntain settings. We hope this empirical study can further in-\nspire research in Mamba for PAR, and even extend into\nthe domain of multi-label recognition, through the design\nof these network structures and comprehensive experimen-\ntation. The source code of this work will be released at\nhttps://github.com/Event-AHU/OpenPAR.", "sections": [{"title": "1. Introduction", "content": "Pedestrian Attribute Recognition (PAR) [48] is a widely\nexploited research topic in the computer vision (CV) com-\nmunity. It aims to recognize human attributes from a set\nof attribute descriptions, such as short black hair, wearing\nhats, with back bag, etc. On the one hand, pedestrian at-\ntribute recognition can be used to describe the appearance\nand motion characteristics of pedestrians, playing a cru-\ncial role in understanding them; on the other hand, pedes-\ntrian attributes can serve as mid-level semantic representa-\ntions, assisting in improving the performance of other vi-\nsual tasks, including pedestrian detection [55], person re-\nidentification [54, 59], and more.\nWith the help of deep learning, various deep PAR\nmodels are proposed, including CNN (Convolutional Neu-\nral Networks), RNN (Recurrent Neural Networks), GNN\n(Graph Neural Networks), Transformer, etc. Specifically,\nMTCNN [1] uses CNN to extract features and combine\nthem in a multi-task manner to solve the PAR task. JRL [44]\nand GRL [56] are solving PAR as a sequence prediction task\nusing LSTM to model the association between attributes.\nPARformer [7] and VTB [3] use Transformer to solve PAR\ntask, the difference is that VTB introduces text modality.\nAlthough existing PAR models work well in simple scenar-\nios, the recognition performance in challenging scenarios is\nstill limited (e.g., low illumination, cross-domain). In addi-\ntion, the training/inference cost (O(N2)) is rather high due\nto the utilization of self-attention in Transformer networks.\nBased on the observations above and reflections, it is natural\nto raise the following question: how can we design a new\npedestrian attribute recognition framework that achieves a\ncomparable or even better performance compared to the\nTransformer-based models, meanwhile, reducing the cost\nsignificantly?\nRecently, the State Space Model (SSM) has drawn more\nand more attention in the artificial intelligence community\ndue to its linear complexity (O(N)) and good performance.\nIt has been widely utilized for visual tracking [17], image-\n/video-based classification [27, 60], and time series analy-\nsis [49]. The experimental results reported in the SSM sur-\nvey [47] demonstrate that the Vim-S [60] based attribute\nrecognition model performs better than the ViT-S based"}, {"title": "2. Related Works", "content": "In this section, we will introduce the related works\non pedestrian attribute recognition, and state space model.\nMore details can be found in the following surveys [45, 47,\n48].\n2.1. Pedestrian Attribute Recognition\nPedestrian attribute recognition has made significant\nprogress over the years, with many methods proposed by\nresearchers achieving promising results. Early methods\nprimarily involved feeding images into CNN networks for\nmulti-task training, and sharing parameters between net-\nworks to obtain richer features. For example, MTCNN [1]\nadopted this multi-task training approach, using CNN to\nextract image features and allowing knowledge sharing\namong different attribute categories. Due to the correlations\nbetween different attributes, sequential prediction models\nwere introduced into pedestrian attribute recognition re-\nsearch. JRL [44] modeled the contextual information be-\ntween pedestrians and the contextual information between\nattributes using LSTM, transforming pedestrian attribute\nrecognition into a sequence prediction problem. Building"}, {"title": "3. Methodology", "content": "In this section, we will first give a brief review of the\nState Space Model (SSM), then, we will introduce two\nMamba-based PAR frameworks, including the pure image-\nbased multi-label classification and image-text fusion based\nPAR framework. Further, we design eight variations of hy-\nbrid Mamba-Transformer networks for the PAR task.\n3.1. Preliminary: State Space Model\nThe State Space Model (SSM) originates from the clas-\nsic Kalman filter [22] algorithm which introduces linear fil-\ntering. It converts a one-dimensional sequence into an N-\ndimensional hidden state for output. The calculation for-\nmula is as follows,\n$\\begin{aligned}\nh'(t) &= Ah(t) + Bx(t), \\\\\ny(t) &= Ch(t) + Dx(t).\n\\end{aligned}$$\nwhere $x(t) \\in \\mathbb{R}^1$ and $h'(t) \\in \\mathbb{R}^N$ are the input se-\nquence and the derivative of the hidden state respectively.\n$A \\in \\mathbb{R}^{N \\times N}$ is the state matrix, $B \\in \\mathbb{R}^{N \\times L}$ is the input ma-\ntrix, $C \\in \\mathbb{R}^{L \\times N}$ is the output matrix, and $D \\in \\mathbb{R}^{L \\times L}$ is the\nfeed-through matrix. The above formula is a continuous-\ntime SSM. In order to facilitate the deep learning algo-\nrithm's understanding of the input, we need to discretize\nthe matrixes through some methods, such as zero order hold\n(ZOH), etc. Specifically, Gu et al. [12] propose structured\nstate-space sequence models (S4), which convert continu-\nous parameters into discrete ones by introducing timescale\nparameter \u0394. The formula is as follows,\n$\\begin{aligned}\n\\overline{A} &= \\exp(\\Delta A), \\\\\n\\overline{B} &= (\\Delta A)^{-1}(\\exp(\\Delta A) - I) \\cdot \\Delta B, \\\\\n\\overline{C} &= C.\n\\end{aligned}$$\nwhere $\\overline{A}$, $\\overline{B}$, and $\\overline{C}$ are discrete parameters of the system\nfrom $A$, $B$ and $C$ respectively. Furthermore, the formula\ncan be shown that,\n$\\begin{aligned}\nh_t &= \\overline{A}h_{t-1} + \\overline{B}x_t, \\\\\ny_t &= \\overline{C}h_t.\n\\end{aligned}$$\nwhere the D matrix can sometimes be ignored as a residual.\nBased on the aforementioned models, in the field of com-\nputer vision, Vim [60] and VMamba [33] have been pro-\nposed and received considerable attention. This paper will\nexplore SSM-based pedestrian attribute recognition frame-\nworks using these two models."}, {"title": "3.2. When Mamba Meets PAR", "content": "In this section, we mainly focus on two widely used PAR\nframeworks, i.e., the image-based multi-label classification\nframework [7, 24, 42], and the image-text fusion based PAR\nframework [3, 46], as shown in Fig. 2. Considering that\nimage-based multi-label recognition can be seen as a spe-\ncial case of the image-text integration framework, that is,\nby removing the attribute label encoding module, this paper\nwill focus on elaborating the PAR process of the image-text\nintegration framework. The details of multi-label recogni-\ntion will not be further discussed.\nGenerally speaking, our proposed Mamba-based visual-\nlanguage fusion PAR architecture consists of three main\nmodules, i.e., the Mamba vision backbone which is com-\nposed of a Vim [60] network, a Mamba-based text en-\ncoder for extracting the attribute semantic information, and\na visual-semantic fusion (VSF) Mamba module for image-\ntext feature interaction and fusion. Finally, we adopt a clas-\nsification head for pedestrian attribute recognition. We will\nintroduce these modules in the subsequent paragraphs re-\nspectively.\nMamba for Input Representation. Given a pedestrian im-\nage $I$, we need to predict its attributes from an attribute set\n$A = \\{a_1, a_2, ..., a_L \\}$, $L$ is the number of person attributes,\nand the ground truth annotations $Y$ are provided for the su-\npervised learning of our PAR framework. In our framework,\nwe adopt the vision Mamba network Vim [60] as an exam-\nple to demonstrate how to encode the given person image\ninto visual features $X_v$. Note that, both the Vim [60] and\nVMamba [33] can be adopted for the encoding of pedestrian\nimages, as validated in our experimental results. Firstly,\nwe divide the image into equally sized image patches based\non a specified patch size, then, these patches are flattened\nand fed into a linear projection to obtain the patch tokens\n$X_v \\in \\mathbb{R}^{P^2 \\times D}$. Positional embeddings P.E. are added to\neach token to better capture the spatial positions of each\npatch.\nNext, we feed the tokens into a stack of Vim [60] blocks,\n$MV = \\{M_1^V,M_2^V,..., M_N^V\\}$, where N is the number of\nVim blocks, thus, we can obtain the visual tokens $X_v$. More\nin detail, as shown in Fig. 2 the input tokens are firstly pro-\ncessed using the normalization layer and then transformed\ninto $x$ and $z$ using projection layers. For the $x$, the for-\nward and backward processing branches are adopted for\nthe vision feature learning, and each branch contains both\nConv1d and SSM layers. For the $z$, an activation layer $\\sigma(\\cdot)$\nis adopted to enhance the feature and multiply with the out-\nput of the forward and backward processing branches re-\nspectively. The compute procedure can be simply written\nas:\n$\\sigma(z) * FSSM(Conv1d(x)) + \\sigma(z) * BSSM(Convld(x))$$\nThe output features are added and projected as the output of\neach Mamba block.\nTo better help the framework understand what is hu-\nman attributes, the attribute labels defined on the whole\ndataset are usually directly integrated into the PAR frame-\nwork. Given the attribute set A, we use the pre-trained\nBERT [23] tokenizer to embed the attributes into seman-\ntic tokens $X_F$. These semantic tokens are further enhanced"}, {"title": "3.3. Hybrid Mamba-Transformer for PAR", "content": "In our experiments, we find that the pure VMamba [33]\nbased PAR framework performs better than the image-text\nfusion based one. Therefore, in this section, we exploit new\nnetwork architectures to aggregate the VMamba [33] with\nTransformer networks for better pedestrian attribute recog-\nnition. Specifically speaking, we design eight variations of\nhybrid Mamba-Transformer networks to validate which one\nperforms better for the PAR task. Due to the limited space\nin this paper, we briefly introduced the computation proce-\ndures of these hybrid Mamba-Transformer networks for the\nPAR task. For more details on these models, please check\nour source code.\n\u2022 Parallel Fusion (PaFusion) is shown in Fig. 3 (a), where\nwe connect the corresponding layers in the two models ViT-\nB and Vim-S in parallel, then add them together and feed\ninto the next layer uniformly. It should be noted that the fea-\nture dimension of Vim-S is 384-D, while the feature dimen-\nsion of ViT-B is 768-D. Thus, linear mapping is performed\non the Vim features before feature addition each time. The\nintegrated features are also subjected to reverse linear map-\nping for input to the next layer of Vim. Since Vim-S con-\ntains 24 Mamba layers and ViT-B contains 12 Transformer\nlayers, we connect two Vim layers and one ViT layer in par-\nallel in our practical implementation.\n\u2022 Non-alternating Serial Fusion (N-ASF) As shown in\nFig. 3 (b), we directly connect the Vim-S and ViT-B to build\nthe N-ASF PAR model. After a couple of Vim-S layers, we\nperform a dimension transformation and feed it into four\nViT-B blocks for the feature processing in the next step.\n\u2022 Alternating Serial Fusion (ASF) is shown in Fig. 3 (c),\nwhere we alternately connect two models of ViT-B and\nVim-S layers in serial. There is a linear mapping between\nthe two-layer connections due to the inconsistent number of\nlayers. We connect one layer of ViT to two layers of Vim in\nseries.\n\u2022 Mamba-enhanced Transformer (MaFormer) is shown\nin Fig. 3 (d), where we use Vim-S as a complementary infor-\nmation extractor to integrate the features extracted by Vim\nand the current layer input before feeding into ViT.\n\u2022 Mamba for Hierarchical Dense Fusion of Transformer\n(MaHDFT) is shown in Fig. 3 (e), where we consider Vim-\nS as a processor that integrates the outputs of all layers of\nViT. We use the ViT-B model trained in advance on the\nPA100k dataset and then freeze it. Following this, we in-\ntegrate the outputs of each layer of ViT into four layers of\nVim for integration, and then feed the outputs of Vim into\nthe classification head for label prediction. Due to the lim-\nited memory of our used RTX3090 GPUs, it is not possible\nto integrate all tokens directly (2352 total tokens), so each\noutput layer is first passed through a convolutional layer to\nreduce the number of tokens to 49 (588 total tokens). It\nshould be mentioned here that we retain the classification\nhead of ViT-B on the PA100K dataset for generating the la-\nbels corresponding to the final output features of ViT. The\nclassification head used for Vim is recreated, and after ob-\ntaining the two sets of label distributions, we average them\nto obtain the final label.\n\u2022 Adapter for Mamba-Transformer Fusion (AdaMTF)\nis shown in Fig. 3 (f), where we connect Vim and ViT back-\nbone as a whole in parallel, and introduce adapter in the\ncorresponding layer to perform feature interaction, where\nthe adapter is an MLP in our implementation.\n\u2022 Knowledge Distill (KDTM) is shown in Fig. 3 (g), where\nwe focus on Vim and use ViT as the teacher network to per-\nform feature- or logit-level distillation on Vim to enhance"}, {"title": "4. Experiments", "content": "4.1. Datasets and Evaluation Metric\nIn our experiments, seven widely used PAR benchmark\ndatasets are evaluated, including PA100K [31], PETA [5],\nRAP-V1 [26], RAP-V2 [25], WIDER [29], PETA-ZS [20],\nand RAP-ZS [20]. A brief introduction to these datasets is\ngiven below:\n\u2022 PA100K [31] dataset is the largest pedestrian attribute\nrecognition dataset, which contains 100,000 pedestrian im-\nages and 26 binary attributes. In our experiments, we split\nthem into a training and validation set of 90,000 images,\nand a test subset of the remaining 10,000 images.\n\u2022 PETA [5] dataset contains 19,000 outdoor or indoor\npedestrian images and 61 binary attributes. These images\nare divided into 9500 as the training subset, 1900 as the\nvalidation subset, and 7600 as the test subset. In the exper-\niment, we selected 35 pedestrian attributes according to the\nmethod of [5].\n\u2022 RAP-V1 [26] dataset contains 41585 pedestrian images\nand 69 binary attributes, of which 33,268 images are used\nfor training. In the current work, 51 attributes are usually\nselected for training and evaluation.\n\u2022 RAP-V2 [25] dataset has 84,928 pedestrian images and\n69 binary attributes, of which 67,943 are used for training.\nWe selected 54 attributes to train and evaluate our model.\n\u2022 WIDER [29] dataset is divided into a training and vali-\ndation set of 28,345 images and a test set of 29,179 images\nwith a total of 14 attribute labels. Following the default\nsetup, the train-val set is used for training and the perfor-\nmance on the test set is evaluated.\n\u2022 PETA-ZS [20] dataset is proposed by Jia et al., based on\nthe PETA dataset, following a zero-shot protocol. The train-\ning, validation, and test sets consist of 11,241 | 3,826 | 3,933\nsamples respectively. For our experiments, we selected 35\ncommon attributes according to Jia et al. [20].\n\u2022 RAP-ZS [20] dataset is developed based on the RAPv2\ndataset, where the training, validation, and testing sets con-\ntain 17062, 4628, and 4928 pedestrian images, respectively.\nThere is no shared personal identity between training and\ninference data. In the experiment, we selected 53 attributes\nfor evaluation following Jia et al. [20].\nTo evaluate our proposed Mamba-based pedestrian at-\ntribute recognition algorithm and other SOTA PAR models,\nwe adopt the following evaluation metrics, including mA,\nAccuracy, Precision, Recall, and F1-measure. For the mA\ncan be expressed as:\n$mA = \\frac{\\sum_{i=1}^{C} AP_i}{C}$$\nwhere AP is the area under the precision-recall curve for\nthe attribute\u017c, and C is the total number of attributes. The\nAccuracy can be expressed as:\n$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\nwhere TP is a positive sample predicted correctly, TN is a\nnegative sample predicted correctly, FP is a negative sample\npredicted incorrectly, and FN is a positive sample predicted\nincorrectly. The Precision, Recall, and F1-measure can be\nexpressed as:\n$\\begin{aligned}\nPrecision &= \\frac{TP}{TP + FP} \\\\\nRecall &= \\frac{TP}{TP + FN}, \\\\\nF1 &= \\frac{2 \\times Precision \\times Recall}{Precision + Recall}\n\\end{aligned}$"}, {"title": "4.2. Implementation Details", "content": "In our experiments, both VMamba-B and Vim-S ver-\nsions are used as visual feature extractors. VMamba-B is\na vision Mamba network in four stages with 2, 2, 27, and\n2 layers in each stage, and the feature dimension is 768-\nD. Vim-S is a 24-layer vision Mamba model with 384 fea-\nture dimensions. For the text modality, the original Mamba\nmodel Mamba-130M, with a feature size of 768 is used.\nNote that, random cropping and flipping operations are em-\nployed for data augmentation.\nFor the detailed parameters, we train the model for\n100 epochs using the Adam [37] optimizer. When using\nVMamba-B, we set the learning rate to 8e-5 and the batch\nsize to 16. When using Vim-S, we set the learning to 2.5e-\n5 and the batch size to 64. Our source code is imple-\nmented using Python and the deep learning framework Py-\nTorch [35]. The experiments were conducted on a server\nwith GPU RTX3090s. More details can be found in our\nsource code."}, {"title": "4.3. Comparison on Public PAR Benchmarks", "content": "In this section, we report the recognition results on seven\ndatasets and compare them with existing state-of-the-art\npedestrian attribute recognition algorithms. Note that the\nresults for VTB* were obtained by replacing VTB's back-\nbone network with ViT-L/14. Visualization of attribute pre-\ndictions of some person images is given in Fig. 5.\nResults on PETA dataset. As shown in Table 1, the\nevaluated MambaPAR model performs well on most eval-\nuation metrics, with different branches and backbone net-\nworks showing varying results. Specifically, the VMamba-\nB visual branch model achieved mA, Acc, Prec, Re-\ncall, and F1 metrics of 86.28, 80.54, 87.45, 87.95, and\n87.45, respectively; the VMamba-B visual plus language\nbranch model achieved 85.01, 78.47, 85.12, 87.49, and\n86.00; the Vim visual plus language branch model achieved\n81.45, 74.92, 82.68, 84.27, and 83.15; and the Vim vi-\nsual plus text branch model achieved 84.25, 76.07, 82.73,\n87.20, and 84.56. Interestingly, we can find that the at-\ntribute labels are useful for the Vim-S based PAR, but\nnot for the VMamba-B based version. Compared to the\nTransformer-based pedestrian attribute recognition models,\nour VMamba visual branch model surpassed VTB (ViT-\n\u0392/16, 85.31/79.60/86.76/87.17/86.71) and VTB* (ViT-\nL/14, 86.34/79.59/86.66/90.67/88.86) in the mA, Acc, Prec,\nRecall, and F1 metrics. Therefore, we can conclude that\nour model achieves competitive results on the PETA dataset,\nverifying the feasibility of the Mamba model for pedestrian\nattribute recognition.\nResults on PA100K dataset. As shown in Table 1,\nour MambaPAR model also performs well on the PA100K\ndataset. The VMamba-B visual branch model achieved\nmA, Acc, Prec, Recall, and F1 metrics of 83.63, 81.11,\n87.59, 89.9, and 88.40, respectively; the VMamba-B vi-\nsual plus language branch model achieved 81.50, 79.55,\n87.54, 88.03, and 87.35; the Vim visual plus language\nbranch model achieved 79.28, 76.77, 85.24, 86.58, and\n85.44; and the Vim visual plus language model achieved\n80.87, 79.33, 86.48, 88.79, and 87.21. The VMamba-B vi-\nsual branch model's performance is close to that of VTB*\n(85.3/81.76/87.87/90.67/88.86). These experimental results\nindicate that the exploited Mamba-based PAR frameworks\nwe explored have achieved preliminary success.\nFrom the experimental results reported in the PETA and\nPA100K datasets, we can find that the VMamba-B achieves\nthe best results over other Mamba-based PAR models.\nTherefore, we only report the results of pure VMamba-B\nbased PAR framework for the rest of the datasets.\nResults on RAP-V1 dataset. As shown in Table 2, our\nVMamba-B visual branch model achieved mA, Acc, Prec,\nRecall, and F1 metrics of 83.12, 69.47, 78.03, 84.81, and\n80.88, respectively. In comparison, the strong baseline\nVTB* achieved 83.69, 69.78, 78.09, 85.21, and 81.10 on\nthese metrics, and our results are very close to theirs.\nMeanwhile, our model's performance is also close to the\nDRFormer [42], which is also developed based on Trans-\nformer. This fully demonstrates that the performance of\nour Mamba-based pedestrian attribute recognition model is\ncomparable to or even surpasses Transformer-based mod-\nels.\nResults on RAP-V2 dataset. As shown in Table 2, our\nVMamba-B visual branch model achieved mA, Acc, Prec,\nRecall, and F1 metrics of 81.91, 68.08, 76.41, 84.38,\nand 79.83, respectively, outperforming the Transformer-\nbased VTB* (81.36/67.58/76.19/84.00/79.52). This further\nproves the effectiveness of our proposed pedestrian attribute\nrecognition model.\nResults on WIDER dataset. As shown in Table 3, our\nmodel achieved mA of 89.38, surpassing VTB (ViT-B/16,\n88.2), which further validates the feasibility of our model.\nCompared with the pre-trained vision-language CLIP [36]\nmodel based PAR, e.g., the VTB* and PromptPAR, the\nVMamba-based model is still inferior to these models. In\nour future works, we will consider knowledge distillation\nstrategies to further augment the performance of VMamba-\nbased PAR.\nResults on PETA-ZS dataset. In addition to standard\nevaluations, we also evaluated the model's performance on\ndatasets using a zero-shot setting. According to the results\nreported in Table 4, our model achieved mA, Acc, Prec,\nRecall, and F1 metrics of 74.43, 60.56, 73.12, 74.51, and\n73.37, respectively, which are close to or even better than\nVTB (ViT-B/16, 75.13/60.50/73.29/74.40/73.38) on certain\nmetrics.\nResults on RAP-ZS dataset. As shown in Table 4, the ex-\nperimental results on the RAP-ZS dataset show that VTB"}, {"title": "4.4. Ablation Study", "content": "Effects of Vision-Semantic Fusion for PAR. As shown in\nTable 1, we conducted ablation experiments on the PA100K\nand PETA datasets for the visual semantic fusion (VSF)\nmodule. The effectiveness of this module has been val-\nidated by integrating with the Transformer networks, but\nnot with the Mamba networks. Thus, we exploit two vision\nMamba models, i.e., the VMamba-B and Vim-S, to check\nwhether VSF still works for the Mamba-based PAR frame-\nwork. It is easy to find that the pure Vim-S based model\nachieves (PA100K) 81.45, 74.92, 82.68, 84.27, 83.15 and\n(PETA) 79.28, 76.77, 85.24, 86.58, 85.44 in the five in-\ndicators of mA, Acc, Prec, Recall, and F1, respectively.\nAfter introducing the VSF module highlighted in the red\ndashed rectangle, the results reach 80.87, 79.33, 86.48,\n88.79, 87.21 on the PA100K dataset and 84.25, 76.07,\n82.73, 87.20, 84.56 on the PETA dataset, respectively,\nwhich proves the effectiveness of the semantic labels for\nthe Vim-based PAR framework. However, we also find that\nsimilar conclusions do not hold for the VMamba-B based\nPAR framework, that is to say, the performance drops when\nadopting the semantic labels of all attributes.\nTransformer vs Mamba for PAR. As shown in Table 5,\nwe compare the performance of ViT-S, Vim-S, ViT-B, and\nVMamba-B on the PA100K dataset using only the visual\nbranch. One can find that the vision Mamba-based PAR\nframework achieves comparable or even better performance\nthan vision Transformer based models at the same size.\nThese comparisons demonstrate that it is still a promising\nresearch direction to adopt the Mamba for pedestrian at-\ntribute recognition."}, {"title": "4.5. Efficiency Analysis", "content": "Analysis of Testing Time. As shown in Fig. 4(a), we test\nthe running time of Vim-S, VMamba-B, ViT-S, and ViT-\nB to check their efficiency. Obviously, the Mamba-based\nmodels are slower than the ViT-based models. Also, the\nVMamba-B needs more time than the Vim-S due to larger\nparameters. Thus, research on further improving the effi-\nciency of Vision Mamba is needed.\nAnalysis on GPU Memory Cost. As shown in Fig. 4(c),\nwe compared the memory usage of different PAR models on\nvarious datasets. It is easy to find that the VMamba signifi-\ncantly costs more GPU memories (about \u00d72 times) than the\nVim-S, ViT-S, and ViT-B. Interestingly, we can also see that\nthe ViT-B performs the best with batch size 24 or 64. These\nexperimental results indicate that Mamba does not gain an\nadvantage regarding memory usage on pedestrian attribute\nrecognition tasks with limited image resolutions.\nAnalysis of Parameters and FLOPs. As illustrated in\nFig. 4(b, d), the Vim-S model exhibits a notable reduction\nin parameters compared to the ViT-S and ViT-B models,\nwith no appreciable decline in performance. Additionally,\nthe VMamba-B model displays a more compact parame-\nter count than the ViT-B model, while maintaining com-\nparable performance. It has been demonstrated that the\nMamba-based pedestrian attribute recognition method has a\nreduced number of parameters and facilitates the process of\nfine-tuning. Furthermore, the computational costs of these\nmethods under different batch sizes were compared. It was\nobserved that as the batch size increased, the Transformer-\nbased PAR method exhibited a notable reduction in compu-\ntational cost. However, the computational cost of VMamba-\nB was significantly lower than that of ViT-B, while that of\nVim-S remained largely unchanged. This demonstrates that\nthe Mamba-based pedestrian attribute recognition method\ncontinues to exhibit reduced computation at high batch sizes\nand is more hardware-friendly when deployed."}, {"title": "4.6. Comparison of Hybrid Mamba-Transformer", "content": "In this paper, we also exploit different versions of hy-\nbrid Mamba-Transformer networks for pedestrian attribute\nrecognition, as discussed in sub-section 3.3. We will report\nand discuss the experimental results of these variations in\nthis sub-section.\nAs shown in Table 6, among the six hybrid methods we\nproposed, method (e) achieves the best results, even outper-\nforming the largest-scale ViT models. Others, such as (b)\nand (d), do not outperform ViT-B in performance, but also\noutperform ViT-S and Vim-S. Other than these, none of the\nother approaches resulted in any performance improvement,\neither because of design issues or because of the hybrid ap-"}, {"title": "4.7. Limitation Analysis", "content": "Although we have explored some hybrid architectures\npurely visually and obtained some improvements, we have\nnot explored a suitable scheme for how to use Mamba for\nmulti-modal fusion. From the experimental results, directly\napplying Mamba to modal fusion cannot obtain a consis-\ntent improvement. Therefore, how to apply Mamba to the"}, {"title": "5. Conclusion", "content": "In this paper"}, {"title": "An Empirical Study of Mamba-based Pedestrian Attribute Recognition", "authors": ["Xiao Wang", "Weizhe Kong", "Jiandong Jin", "Shiao Wang", "Ruichong Gao", "Qingchuan Ma", "Chenglong Li", "Jin Tang"], "abstract": "Current strong pedestrian attribute recognition models\nare developed based on Transformer networks, which are\ncomputationally heavy. Recently proposed models with lin-\near complexity (e.g., Mamba) have garnered significant at-\ntention and have achieved a good balance between accu-\nracy and computational cost across a variety of visual tasks.\nRelevant review articles also suggest that while these mod-\nels can perform well on some pedestrian attribute recog-\nnition datasets, they are generally weaker than the cor-\nresponding Transformer models. To further tap into the\npotential of the novel Mamba architecture for PAR tasks,\nthis paper designs and adapts Mamba into two typical\nPAR frameworks, i.e., the text-image fusion approach and\npure vision Mamba multi-label recognition framework. It\nis found that interacting with attribute tags as additional\ninput does not always lead to an improvement, specifi-\ncally, Vim can be enhanced, but VMamba cannot. This\npaper further designs various hybrid Mamba-Transformer\nvariants and conducts thorough experimental validations.\nThese experimental results indicate that simply enhancing\nMamba with a Transformer does not always lead to per-\nformance improvements but yields better results under cer-\ntain settings. We hope this empirical study can further in-\nspire research in Mamba for PAR, and even extend into\nthe domain of multi-label recognition, through the design\nof these network structures and comprehensive experimen-\ntation. The source code of this work will be released at\nhttps://github.com/Event-AHU/OpenPAR.", "sections": [{"title": "1. Introduction", "content": "Pedestrian Attribute Recognition (PAR) [48] is a widely\nexploited research topic in the computer vision (CV) com-\nmunity. It aims to recognize human attributes from a set\nof attribute descriptions, such as short black hair, wearing\nhats, with back bag, etc. On the one hand, pedestrian at-\ntribute recognition can be used to describe the appearance\nand motion characteristics of pedestrians, playing a cru-\ncial role in understanding them; on the other hand, pedes-\ntrian attributes can serve as mid-level semantic representa-\ntions, assisting in improving the performance of other vi-\nsual tasks, including pedestrian detection [55], person re-\nidentification [54, 59], and more.\nWith the help of deep learning, various deep PAR\nmodels are proposed, including CNN (Convolutional Neu-\nral Networks), RNN (Recurrent Neural Networks), GNN\n(Graph Neural Networks), Transformer, etc. Specifically,\nMTCNN [1] uses CNN to extract features and combine\nthem in a multi-task manner to solve the PAR task. JRL [44]\nand GRL [56] are solving PAR as a sequence prediction task\nusing LSTM to model the association between attributes.\nPARformer [7] and VTB [3] use Transformer to solve PAR\ntask, the difference is that VTB introduces text modality.\nAlthough existing PAR models work well in simple scenar-\nios, the recognition performance in challenging scenarios is\nstill limited (e.g., low illumination, cross-domain). In addi-\ntion, the training/inference cost (O(N2)) is rather high due\nto the utilization of self-attention in Transformer networks.\nBased on the observations above and reflections, it is natural\nto raise the following question: how can we design a new\npedestrian attribute recognition framework that achieves a\ncomparable or even better performance compared to the\nTransformer-based models, meanwhile, reducing the cost\nsignificantly?\nRecently, the State Space Model (SSM) has drawn more\nand more attention in the artificial intelligence community\ndue to its linear complexity (O(N)) and good performance.\nIt has been widely utilized for visual tracking [17], image-\n/video-based classification [27, 60], and time series analy-\nsis [49]. The experimental results reported in the SSM sur-\nvey [47] demonstrate that the Vim-S [60] based attribute\nrecognition model performs better than the ViT-S based"}, {"title": "2. Related Works", "content": "In this section, we will introduce the related works\non pedestrian attribute recognition, and state space model.\nMore details can be found in the following surveys [45, 47,\n48].\n2.1. Pedestrian Attribute Recognition\nPedestrian attribute recognition has made significant\nprogress over the years, with many methods proposed by\nresearchers achieving promising results. Early methods\nprimarily involved feeding images into CNN networks for\nmulti-task training, and sharing parameters between net-\nworks to obtain richer features. For example, MTCNN [1]\nadopted this multi-task training approach, using CNN to\nextract image features and allowing knowledge sharing\namong different attribute categories. Due to the correlations\nbetween different attributes, sequential prediction models\nwere introduced into pedestrian attribute recognition re-\nsearch. JRL [44] modeled the contextual information be-\ntween pedestrians and the contextual information between\nattributes using LSTM, transforming pedestrian attribute\nrecognition into a sequence prediction problem. Building"}, {"title": "3. Methodology", "content": "In this section, we will first give a brief review of the\nState Space Model (SSM), then, we will introduce two\nMamba-based PAR frameworks, including the pure image-\nbased multi-label classification and image-text fusion based\nPAR framework. Further, we design eight variations of hy-\nbrid Mamba-Transformer networks for the PAR task.\n3.1. Preliminary: State Space Model\nThe State Space Model (SSM) originates from the clas-\nsic Kalman filter [22] algorithm which introduces linear fil-\ntering. It converts a one-dimensional sequence into an N-\ndimensional hidden state for output. The calculation for-\nmula is as follows,\n$\\begin{aligned}\nh'(t) &= Ah(t) + Bx(t), \\\\\ny(t) &= Ch(t) + Dx(t).\n\\end{aligned}$$\nwhere $x(t) \\in \\mathbb{R}^1$ and $h'(t) \\in \\mathbb{R}^N$ are the input se-\nquence and the derivative of the hidden state respectively.\n$A \\in \\mathbb{R}^{N \\times N}$ is the state matrix, $B \\in \\mathbb{R}^{N \\times L}$ is the input ma-\ntrix, $C \\in \\mathbb{R}^{L \\times N}$ is the output matrix, and $D \\in \\mathbb{R}^{L \\times L}$ is the\nfeed-through matrix. The above formula is a continuous-\ntime SSM. In order to facilitate the deep learning algo-\nrithm's understanding of the input, we need to discretize\nthe matrixes through some methods, such as zero order hold\n(ZOH), etc. Specifically, Gu et al. [12] propose structured\nstate-space sequence models (S4), which convert continu-\nous parameters into discrete ones by introducing timescale\nparameter \u0394. The formula is as follows,\n$\\begin{aligned}\n\\overline{A} &= \\exp(\\Delta A), \\\\\n\\overline{B} &= (\\Delta A)^{-1}(\\exp(\\Delta A) - I) \\cdot \\Delta B, \\\\\n\\overline{C} &= C.\n\\end{aligned}$$\nwhere $\\overline{A}$, $\\overline{B}$, and $\\overline{C}$ are discrete parameters of the system\nfrom $A$, $B$ and $C$ respectively. Furthermore, the formula\ncan be shown that,\n$\\begin{aligned}\nh_t &= \\overline{A}h_{t-1} + \\overline{B}x_t, \\\\\ny_t &= \\overline{C}h_t.\n\\end{aligned}$$\nwhere the D matrix can sometimes be ignored as a residual.\nBased on the aforementioned models, in the field of com-\nputer vision, Vim [60] and VMamba [33] have been pro-\nposed and received considerable attention. This paper will\nexplore SSM-based pedestrian attribute recognition frame-\nworks using these two models."}, {"title": "3.2. When Mamba Meets PAR", "content": "In this section, we mainly focus on two widely used PAR\nframeworks, i.e., the image-based multi-label classification\nframework [7, 24, 42], and the image-text fusion based PAR\nframework [3, 46], as shown in Fig. 2. Considering that\nimage-based multi-label recognition can be seen as a spe-\ncial case of the image-text integration framework, that is,\nby removing the attribute label encoding module, this paper\nwill focus on elaborating the PAR process of the image-text\nintegration framework. The details of multi-label recogni-\ntion will not be further discussed.\nGenerally speaking, our proposed Mamba-based visual-\nlanguage fusion PAR architecture consists of three main\nmodules, i.e., the Mamba vision backbone which is com-\nposed of a Vim [60] network, a Mamba-based text en-\ncoder for extracting the attribute semantic information, and\na visual-semantic fusion (VSF) Mamba module for image-\ntext feature interaction and fusion. Finally, we adopt a clas-\nsification head for pedestrian attribute recognition. We will\nintroduce these modules in the subsequent paragraphs re-\nspectively.\nMamba for Input Representation. Given a pedestrian im-\nage $I$, we need to predict its attributes from an attribute set\n$A = \\{a_1, a_2, ..., a_L \\\\}$, $L$ is the number of person attributes,\nand the ground truth annotations $Y$ are provided for the su-\npervised learning of our PAR framework. In our framework,\nwe adopt the vision Mamba network Vim [60] as an exam-\nple to demonstrate how to encode the given person image\ninto visual features $X_v$. Note that, both the Vim [60] and\nVMamba [33] can be adopted for the encoding of pedestrian\nimages, as validated in our experimental results. Firstly,\nwe divide the image into equally sized image patches based\non a specified patch size, then, these patches are flattened\nand fed into a linear projection to obtain the patch tokens\n$X_v \\in \\mathbb{R}^{P^2 \\times D}$. Positional embeddings P.E. are added to\neach token to better capture the spatial positions of each\npatch.\nNext, we feed the tokens into a stack of Vim [60] blocks,\n$MV = \\{M_1^V,M_2^V,..., M_N^V\\}$, where N is the number of\nVim blocks, thus, we can obtain the visual tokens $X_v$. More\nin detail, as shown in Fig. 2 the input tokens are firstly pro-\ncessed using the normalization layer and then transformed\ninto $x$ and $z$ using projection layers. For the $x$, the for-\nward and backward processing branches are adopted for\nthe vision feature learning, and each branch contains both\nConv1d and SSM layers. For the $z$, an activation layer $\\sigma(\\cdot)$\nis adopted to enhance the feature and multiply with the out-\nput of the forward and backward processing branches re-\nspectively. The compute procedure can be simply written\nas:\n$\\sigma(z) * FSSM(Conv1d(x)) + \\sigma(z) * BSSM(Convld(x))$$\nThe output features are added and projected as the output of\neach Mamba block.\nTo better help the framework understand what is hu-\nman attributes, the attribute labels defined on the whole\ndataset are usually directly integrated into the PAR frame-\nwork. Given the attribute set A, we use the pre-trained\nBERT [23] tokenizer to embed the attributes into seman-\ntic tokens $X_F$. These semantic tokens are further enhanced"}, {"title": "3.3. Hybrid Mamba-Transformer for PAR", "content": "In our experiments, we find that the pure VMamba [33]\nbased PAR framework performs better than the image-text\nfusion based one. Therefore, in this section, we exploit new\nnetwork architectures to aggregate the VMamba [33] with\nTransformer networks for better pedestrian attribute recog-\nnition. Specifically speaking, we design eight variations of\nhybrid Mamba-Transformer networks to validate which one\nperforms better for the PAR task. Due to the limited space\nin this paper, we briefly introduced the computation proce-\ndures of these hybrid Mamba-Transformer networks for the\nPAR task. For more details on these models, please check\nour source code.\n\u2022 Parallel Fusion (PaFusion) is shown in Fig. 3 (a), where\nwe connect the corresponding layers in the two models ViT-\nB and Vim-S in parallel, then add them together and feed\ninto the next layer uniformly. It should be noted that the fea-\nture dimension of Vim-S is 384-D, while the feature dimen-\nsion of ViT-B is 768-D. Thus, linear mapping is performed\non the Vim features before feature addition each time. The\nintegrated features are also subjected to reverse linear map-\nping for input to the next layer of Vim. Since Vim-S con-\ntains 24 Mamba layers and ViT-B contains 12 Transformer\nlayers, we connect two Vim layers and one ViT layer in par-\nallel in our practical implementation.\n\u2022 Non-alternating Serial Fusion (N-ASF) As shown in\nFig. 3 (b), we directly connect the Vim-S and ViT-B to build\nthe N-ASF PAR model. After a couple of Vim-S layers, we\nperform a dimension transformation and feed it into four\nViT-B blocks for the feature processing in the next step.\n\u2022 Alternating Serial Fusion (ASF) is shown in Fig. 3 (c),\nwhere we alternately connect two models of ViT-B and\nVim-S layers in serial. There is a linear mapping between\nthe two-layer connections due to the inconsistent number of\nlayers. We connect one layer of ViT to two layers of Vim in\nseries.\n\u2022 Mamba-enhanced Transformer (MaFormer) is shown\nin Fig. 3 (d), where we use Vim-S as a complementary infor-\nmation extractor to integrate the features extracted by Vim\nand the current layer input before feeding into ViT.\n\u2022 Mamba for Hierarchical Dense Fusion of Transformer\n(MaHDFT) is shown in Fig. 3 (e), where we consider Vim-\nS as a processor that integrates the outputs of all layers of\nViT. We use the ViT-B model trained in advance on the\nPA100k dataset and then freeze it. Following this, we in-\ntegrate the outputs of each layer of ViT into four layers of\nVim for integration, and then feed the outputs of Vim into\nthe classification head for label prediction. Due to the lim-\nited memory of our used RTX3090 GPUs, it is not possible\nto integrate all tokens directly (2352 total tokens), so each\noutput layer is first passed through a convolutional layer to\nreduce the number of tokens to 49 (588 total tokens). It\nshould be mentioned here that we retain the classification\nhead of ViT-B on the PA100K dataset for generating the la-\nbels corresponding to the final output features of ViT. The\nclassification head used for Vim is recreated, and after ob-\ntaining the two sets of label distributions, we average them\nto obtain the final label.\n\u2022 Adapter for Mamba-Transformer Fusion (AdaMTF)\nis shown in Fig. 3 (f), where we connect Vim and ViT back-\nbone as a whole in parallel, and introduce adapter in the\ncorresponding layer to perform feature interaction, where\nthe adapter is an MLP in our implementation.\n\u2022 Knowledge Distill (KDTM) is shown in Fig. 3 (g), where\nwe focus on Vim and use ViT as the teacher network to per-\nform feature- or logit-level distillation on Vim to enhance"}, {"title": "4. Experiments", "content": "4.1. Datasets and Evaluation Metric\nIn our experiments, seven widely used PAR benchmark\ndatasets are evaluated, including PA100K [31], PETA [5],\nRAP-V1 [26], RAP-V2 [25], WIDER [29], PETA-ZS [20],\nand RAP-ZS [20]. A brief introduction to these datasets is\ngiven below:\n\u2022 PA100K [31] dataset is the largest pedestrian attribute\nrecognition dataset, which contains 100,000 pedestrian im-\nages and 26 binary attributes. In our experiments, we split\nthem into a training and validation set of 90,000 images,\nand a test subset of the remaining 10,000 images.\n\u2022 PETA [5] dataset contains 19,000 outdoor or indoor\npedestrian images and 61 binary attributes. These images\nare divided into 9500 as the training subset, 1900 as the\nvalidation subset, and 7600 as the test subset. In the exper-\niment, we selected 35 pedestrian attributes according to the\nmethod of [5].\n\u2022 RAP-V1 [26] dataset contains 41585 pedestrian images\nand 69 binary attributes, of which 33,268 images are used\nfor training. In the current work, 51 attributes are usually\nselected for training and evaluation.\n\u2022 RAP-V2 [25] dataset has 84,928 pedestrian images and\n69 binary attributes, of which 67,943 are used for training.\nWe selected 54 attributes to train and evaluate our model.\n\u2022 WIDER [29] dataset is divided into a training and vali-\ndation set of 28,345 images and a test set of 29,179 images\nwith a total of 14 attribute labels. Following the default\nsetup, the train-val set is used for training and the perfor-\nmance on the test set is evaluated.\n\u2022 PETA-ZS [20] dataset is proposed by Jia et al., based on\nthe PETA dataset, following a zero-shot protocol. The train-\ning, validation, and test sets consist of 11,241 | 3,826 | 3,933\nsamples respectively. For our experiments, we selected 35\ncommon attributes according to Jia et al. [20].\n\u2022 RAP-ZS [20] dataset is developed based on the RAPv2\ndataset, where the training, validation, and testing sets con-\ntain 17062, 4628, and 4928 pedestrian images, respectively.\nThere is no shared personal identity between training and\ninference data. In the experiment, we selected 53 attributes\nfor evaluation following Jia et al. [20].\nTo evaluate our proposed Mamba-based pedestrian at-\ntribute recognition algorithm and other SOTA PAR models,\nwe adopt the following evaluation metrics, including mA,\nAccuracy, Precision, Recall, and F1-measure. For the mA\ncan be expressed as:\n$mA = \\frac{\\sum_{i=1}^{C} AP_i}{C}$$\nwhere AP is the area under the precision-recall curve for\nthe attribute\u017c, and C is the total number of attributes. The\nAccuracy can be expressed as:\n$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\nwhere TP is a positive sample predicted correctly, TN is a\nnegative sample predicted correctly, FP is a negative sample\npredicted incorrectly, and FN is a positive sample predicted\nincorrectly. The Precision, Recall, and F1-measure can be\nexpressed as:\n$\\begin{aligned}\nPrecision &= \\frac{TP}{TP + FP} \\\\\nRecall &= \\frac{TP}{TP + FN}, \\\\\nF1 &= \\frac{2 \\times Precision \\times Recall}{Precision + Recall}\n\\end{aligned}$"}, {"title": "4.2. Implementation Details", "content": "In our experiments, both VMamba-B and Vim-S ver-\nsions are used as visual feature extractors. VMamba-B is\na vision Mamba network in four stages with 2, 2, 27, and\n2 layers in each stage, and the feature dimension is 768-\nD. Vim-S is a 24-layer vision Mamba model with 384 fea-\nture dimensions. For the text modality, the original Mamba\nmodel Mamba-130M, with a feature size of 768 is used.\nNote that, random cropping and flipping operations are em-\nployed for data augmentation.\nFor the detailed parameters, we train the model for\n100 epochs using the Adam [37] optimizer. When using\nVMamba-B, we set the learning rate to 8e-5 and the batch\nsize to 16. When using Vim-S, we set the learning to 2.5e-\n5 and the batch size to 64. Our source code is imple-\nmented using Python and the deep learning framework Py-\nTorch [35]. The experiments were conducted on a server\nwith GPU RTX3090s. More details can be found in our\nsource code."}, {"title": "4.3. Comparison on Public PAR Benchmarks", "content": "In this section, we report the recognition results on seven\ndatasets and compare them with existing state-of-the-art\npedestrian attribute recognition algorithms. Note that the\nresults for VTB* were obtained by replacing VTB's back-\nbone network with ViT-L/14. Visualization of attribute pre-\ndictions of some person images is given in Fig. 5.\nResults on PETA dataset. As shown in Table 1, the\nevaluated MambaPAR model performs well on most eval-\nuation metrics, with different branches and backbone net-\nworks showing varying results. Specifically, the VMamba-\nB visual branch model achieved mA, Acc, Prec, Re-\ncall, and F1 metrics of 86.28, 80.54, 87.45, 87.95, and\n87.45, respectively; the VMamba-B visual plus language\nbranch model achieved 85.01, 78.47, 85.12, 87.49, and\n86.00; the Vim visual plus language branch model achieved\n81.45, 74.92, 82.68, 84.27, and 83.15; and the Vim vi-\nsual plus text branch model achieved 84.25, 76.07, 82.73,\n87.20, and 84.56. Interestingly, we can find that the at-\ntribute labels are useful for the Vim-S based PAR, but\nnot for the VMamba-B based version. Compared to the\nTransformer-based pedestrian attribute recognition models,\nour VMamba visual branch model surpassed VTB (ViT-\n\u0392/16, 85.31/79.60/86.76/87.17/86.71) and VTB* (ViT-\nL/14, 86.34/79.59/86.66/90.67/88.86) in the mA, Acc, Prec,\nRecall, and F1 metrics. Therefore, we can conclude that\nour model achieves competitive results on the PETA dataset,\nverifying the feasibility of the Mamba model for pedestrian\nattribute recognition.\nResults on PA100K dataset. As shown in Table 1,\nour MambaPAR model also performs well on the PA100K\ndataset. The VMamba-B visual branch model achieved\nmA, Acc, Prec, Recall, and F1 metrics of 83.63, 81.11,\n87.59, 89.9, and 88.40, respectively; the VMamba-B vi-\nsual plus language branch model achieved 81.50, 79.55,\n87.54, 88.03, and 87.35; the Vim visual plus language\nbranch model achieved 79.28, 76.77, 85.24, 86.58, and\n85.44; and the Vim visual plus language model achieved\n80.87, 79.33, 86.48, 88.79, and 87.21. The VMamba-B vi-\nsual branch model's performance is close to that of VTB*\n(85.3/81.76/87.87/90.67/88.86). These experimental results\nindicate that the exploited Mamba-based PAR frameworks\nwe explored have achieved preliminary success.\nFrom the experimental results reported in the PETA and\nPA100K datasets, we can find that the VMamba-B achieves\nthe best results over other Mamba-based PAR models.\nTherefore, we only report the results of pure VMamba-B\nbased PAR framework for the rest of the datasets.\nResults on RAP-V1 dataset. As shown in Table 2, our\nVMamba-B visual branch model achieved mA, Acc, Prec,\nRecall, and F1 metrics of 83.12, 69.47, 78.03, 84.81, and\n80.88, respectively. In comparison, the strong baseline\nVTB* achieved 83.69, 69.78, 78.09, 85.21, and 81.10 on\nthese metrics, and our results are very close to theirs.\nMeanwhile, our model's performance is also close to the\nDRFormer [42], which is also developed based on Trans-\nformer. This fully demonstrates that the performance of\nour Mamba-based pedestrian attribute recognition model is\ncomparable to or even surpasses Transformer-based mod-\nels.\nResults on RAP-V2 dataset. As shown in Table 2, our\nVMamba-B visual branch model achieved mA, Acc, Prec,\nRecall, and F1 metrics of 81.91, 68.08, 76.41, 84.38,\nand 79.83, respectively, outperforming the Transformer-\nbased VTB* (81.36/67.58/76.19/84.00/79.52). This further\nproves the effectiveness of our proposed pedestrian attribute\nrecognition model.\nResults on WIDER dataset. As shown in Table 3, our\nmodel achieved mA of 89.38, surpassing VTB (ViT-B/16,\n88.2), which further validates the feasibility of our model.\nCompared with the pre-trained vision-language CLIP [36]\nmodel based PAR, e.g., the VTB* and PromptPAR, the\nVMamba-based model is still inferior to these models. In\nour future works, we will consider knowledge distillation\nstrategies to further augment the performance of VMamba-\nbased PAR.\nResults on PETA-ZS dataset. In addition to standard\nevaluations, we also evaluated the model's performance on\ndatasets using a zero-shot setting. According to the results\nreported in Table 4, our model achieved mA, Acc, Prec,\nRecall, and F1 metrics of 74.43, 60.56, 73.12, 74.51, and\n73.37, respectively, which are close to or even better than\nVTB (ViT-B/16, 75.13/60.50/73.29/74.40/73.38) on certain\nmetrics.\nResults on RAP-ZS dataset. As shown in Table 4, the ex-\nperimental results on the RAP-ZS dataset show that VTB"}, {"title": "4.4. Ablation Study", "content": "Effects of Vision-Semantic Fusion for PAR. As shown in\nTable 1, we conducted ablation experiments on the PA100K\nand PETA datasets for the visual semantic fusion (VSF)\nmodule. The effectiveness of this module has been val-\nidated by integrating with the Transformer networks, but\nnot with the Mamba networks. Thus, we exploit two vision\nMamba models, i.e., the VMamba-B and Vim-S, to check\nwhether VSF still works for the Mamba-based PAR frame-\nwork. It is easy to find that the pure Vim-S based model\nachieves (PA100K) 81.45, 74.92, 82.68, 84.27, 83.15 and\n(PETA) 79.28, 76.77, 85.24, 86.58, 85.44 in the five in-\ndicators of mA, Acc, Prec, Recall, and F1, respectively.\nAfter introducing the VSF module highlighted in the red\ndashed rectangle, the results reach 80.87, 79.33, 86.48,\n88.79, 87.21 on the PA100K dataset and 84.25, 76.07,\n82.73, 87.20, 84.56 on the PETA dataset, respectively,\nwhich proves the effectiveness of the semantic labels for\nthe Vim-based PAR framework. However, we also find that\nsimilar conclusions do not hold for the VMamba-B based\nPAR framework, that is to say, the performance drops when\nadopting the semantic labels of all attributes.\nTransformer vs Mamba for PAR. As shown in Table 5,\nwe compare the performance of ViT-S, Vim-S, ViT-B, and\nVMamba-B on the PA100K dataset using only the visual\nbranch. One can find that the vision Mamba-based PAR\nframework achieves comparable or even better performance\nthan vision Transformer based models at the same size.\nThese comparisons demonstrate that it is still a promising\nresearch direction to adopt the Mamba for pedestrian at-\ntribute recognition."}, {"title": "4.5. Efficiency Analysis", "content": "Analysis of Testing Time. As shown in Fig. 4(a), we test\nthe running time of Vim-S, VMamba-B, ViT-S, and ViT-\nB to check their efficiency. Obviously, the Mamba-based\nmodels are slower than the ViT-based models. Also, the\nVMamba-B needs more time than the Vim-S due to larger\nparameters. Thus, research on further improving the effi-\nciency of Vision Mamba is needed.\nAnalysis on GPU Memory Cost. As shown in Fig. 4(c),\nwe compared the memory usage of different PAR models on\nvarious datasets. It is easy to find that the VMamba signifi-\ncantly costs more GPU memories (about \u00d72 times) than the\nVim-S, ViT-S, and ViT-B. Interestingly, we can also see that\nthe ViT-B performs the best with batch size 24 or 64. These\nexperimental results indicate that Mamba does not gain an\nadvantage regarding memory usage on pedestrian attribute\nrecognition tasks with limited image resolutions.\nAnalysis of Parameters and FLOPs. As illustrated in\nFig. 4(b, d), the Vim-S model exhibits a notable reduction\nin parameters compared to the ViT-S and ViT-B models,\nwith no appreciable decline in performance. Additionally,\nthe VMamba-B model displays a more compact parame-\nter count than the ViT-B model, while maintaining com-\nparable performance. It has been demonstrated that the\nMamba-based pedestrian attribute recognition method has a\nreduced number of parameters and facilitates the process of\nfine-tuning. Furthermore, the computational costs of these\nmethods under different batch sizes were compared. It was\nobserved that as the batch size increased, the Transformer-\nbased PAR method exhibited a notable reduction in compu-\ntational cost. However, the computational cost of VMamba-\nB was significantly lower than that of ViT-B, while that of\nVim-S remained largely unchanged. This demonstrates that\nthe Mamba-based pedestrian attribute recognition method\ncontinues to exhibit reduced computation at high batch sizes\nand is more hardware-friendly when deployed."}, {"title": "4.6. Comparison of Hybrid Mamba-Transformer", "content": "In this paper, we also exploit different versions of hy-\nbrid Mamba-Transformer networks for pedestrian attribute\nrecognition, as discussed in sub-section 3.3. We will report\nand discuss the experimental results of these variations in\nthis sub-section.\nAs shown in Table 6, among the six hybrid methods we\nproposed, method (e) achieves the best results, even outper-\nforming the largest-scale ViT models. Others, such as (b)\nand (d), do not outperform ViT-B in performance, but also\noutperform ViT-S and Vim-S. Other than these, none of the\nother approaches resulted in any performance improvement,\neither because of design issues or because of the hybrid ap-"}, {"title": "4.7. Limitation Analysis", "content": "Although we have explored some hybrid architectures\npurely visually and obtained some improvements, we have\nnot explored a suitable scheme for how to use Mamba for\nmulti-modal fusion. From the experimental results, directly\napplying Mamba to modal fusion cannot obtain a consis-\ntent improvement. Therefore, how to apply Mamba to the"}, {"title": "5. Conclusion", "content": "In this paper, our study has explored the potential of the\nMamba architecture in the context of pedestrian attribute\nrecognition (PAR) tasks. While Mamba, with its linear\ncomplexity, has shown promise in balancing accuracy and\ncomputational cost across various visual tasks, our find-\nings suggest that its advantage in terms of memory usage\nis not pronounced when dealing with limited resolutions\nof input images in PAR tasks. The adaptation of Mamba\ninto two typical PAR frameworks yielded mixed results; the\ntext-image fusion approach with Vim showed enhancement\npotential, whereas the VMcontinue generating json"}]}]}