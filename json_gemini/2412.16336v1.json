{"title": "REAL FAULTS IN DEEP LEARNING FAULT BENCHMARKS: HOW REAL ARE THEY?", "authors": ["Gunel Jahangirova", "Nargiz Humbatova", "Jinhan Kim", "Shin Yoo", "Paolo Tonella"], "abstract": "As the adoption of Deep Learning (DL) systems continues to rise, an increasing number of approaches\nare being proposed to test these systems, localise faults within them, and repair those faults.\nThe best attestation of effectiveness for such techniques is an evaluation that showcases their capability\nto detect, localise and fix real faults. To facilitate these evaluations, the research community has\ncollected multiple benchmarks of real faults in DL systems. In this work, we perform a manual\nanalysis of 490 faults from five different benchmarks and identify that 314 of them are eligible for\nour study. Our investigation focuses specifically on how well the bugs correspond to the sources\nthey were extracted from, which fault types are represented, and whether the bugs are reproducible.\nOur findings indicate that only 18.5% of the faults satisfy our realism conditions. Our attempts to\nreproduce these faults were successful only in 52% of cases.", "sections": [{"title": "Introduction", "content": "The growing popularity of deep learning (DL) systems and their widespread adoption makes ensuring that they are\nfault-free a task of utmost importance. In response to this challenge, in the last few years, research in the Software\nEngineering (SE) field has strongly focused on proposing approaches for testing, fault localisation, and repair of such\nsystems. An obvious requirement for evaluation of these techniques is to demonstrate that they can detect, localise or\nfix faults in faulty DL systems. The need for faulty versions of programs has long been essential for evaluating similar\napproaches that target traditional software systems, the initial response to which was to apply mutation testing tools\nto generate versions of a program with small syntactic changes. Later, the discussion on whether the mutants are the"}, {"title": "Fault Dataset Extraction", "content": ""}, {"title": "Database Search", "content": "We performed a systematic literature search to identify the list of works that include reproducible real faults in DL\nsystems. The search was conducted using Scopus, a large, multi-disciplinary database of peer-reviewed literature\nthat offers an advanced search functionality. Our search string is designed to look for the terms related to the real\nbugs in any part of a paper while checking for the terms associated with DL or ML systems only in the abstract. We"}, {"title": "Abstract and Full Text Analysis", "content": "The application of the search string returned 282 results. For each obtained paper, we had to ensure that it indeed\nincludes reproducible real faults in DL systems. This task is hard to automate and therefore we proceeded with\nthe manual analysis of the abstract of each paper by one of the authors. Our inclusion criterion was decided to be\ndeliberately broad in this phase, encompassing any work that proposes an approach for any software engineering activity\nrelated to DL systems. For each excluded paper, the assessor (one of the authors) wrote a summary justifying the\nexclusion. The majority of the papers were removed from further analysis as they focused on approaches that use\nDL for tasks such as fault localisation, program repair, mutation testing, testing adequacy, and oracle generation for\ntraditional software programs. While not being directly relevant, such papers easily fulfil the requirements of our search\nstring: they use DL-related terms and the evaluation of the proposed approaches often requires the presence of real\nfaults. Another category of papers that we eliminated was the works focusing on real faults in DL libraries, which as\nmentioned before are not in the scope of our study."}, {"title": "Forward and Backward Snowballing", "content": "To ensure that our search string did not miss any relevant studies, we manually applied both forward and backward\nsnowballing [11] to the identified 10 papers. Backward snowballing refers to going through the list of references of\na given paper, while forward snowballing focuses on the papers that cite the given paper. We used Google Scholar\nfor forward snowballing. Our first point of judgement was the title of the paper and we included any paper that had a\nDL-related term in its title. This gave us a pool of 147 titles which were reduced to 70 after removing the duplicates.\nWe then checked how many of these titles were already covered by our search string and identified that only 48 of them\nwere new. We performed abstract and full-text analysis for these papers as described in the previous subsection and\nobtained a list of 6 additional papers."}, {"title": "Final Pool of Papers", "content": "We proceeded with the analysis of our pool of final 16 papers (10 from the initial search + six from snowballing). The\ngoal of this step was to perform the final analysis and to identify whether these papers come with a dataset suitable for\nthe purpose of our study. For a benchmark to be deemed suitable the following conditions need to be satisfied:\n1. The dataset should provide a source from which each fault has been extracted (such as a link to a corresponding\nGitHub issue commit or a unique ID for an SO post that raises/discusses this fault). This condition must be\nstrictly fulfilled because, without the source of a fault, it is impossible to check whether the version of the fault\nspecified in a fault benchmark matches the original version.\n2. For each fault in the benchmark, the source code and training dataset for the faulty and the fixed version need\nto be provided. The presence of the fixed version is required because, in the absence of such a fix, it is not\npossible to claim that the reported issue is indeed a fault and that it is possible to fix it. Note that the fixed\nversion also needs to come from the source of the fault to be considered realistic. If it is from GitHub, then the\nfix should be present in the issue discussion or in the committed code. If the fault originates from SO, then the"}, {"title": "Excluded Datasets", "content": "The work by Nikanjam et al. [5] introduces NeuraLint, a model-based fault detection approach for DL programs that\nuses meta-modeling and graph transformations. The authors evaluate NeuraLint on 34 real-world DL models extracted\nfrom SO posts and GitHub repositories. However, our inspection of its replication package identified that the source\ncode is provided only for the buggy version and not for the fixed version. Zhang et al. [15] propose a static analysis\napproach named DEBAR for detecting numerical bugs in DNNs based on abstract interpretation. DEBAR is evaluated\non DNNs with confirmed bugs (collected from works by Zhang et al. [16] and Odena et al. [17]) and real-world neural\narchitectures collected from TensorFlow Models GitHub repository [18]. Similarly to NeuraLint [5], the replication\npackage of DEBAR contains files only for the buggy DNNs, and therefore it also had to be excluded from our study.\nYan et al. [19] proposes GRIST, a technique for generation of inputs that expose numerical bugs in DL programs. The\nevaluation of GRIST was performed on 63 real-world DL programs that authors collected from GitHub. However, this\nevaluation does not rely on any ground truth fixed DL programs. Instead, the authors report the bugs detected by GRIST\nin the issue repositories of DL programs and expect the developers to fix them. This leads to the exclusion of this work,\nwhich then creates a chain reaction of also excluding the work by Li et al. [20], as this work indicates that it uses the\nGRIST dataset for its evaluation.\nThe work by Liu et al. [21] has a specific goal of detecting TensorFlow bugs in industrial systems and proposes a\nconstraint-based approach named ShapeTracer. As the subject programs come from industry, the evaluation dataset,\nconsisting of 60 TensorFlow programs, could not be made available in their replication package. The work by Zhang\net al. [22] proposes AutoTrainer, a rule-based automatic repair tool that supports detecting and fixing five commonly\nseen training problems. The evaluation of AutoTrainer is performed on 495 models which as reported by the authors\nwere \"collected from reported buggy models on GitHub, SO, existing papers and personal blogs, and some of them are\ngathered from machine learning experts within our organisation\u201d. However, no further details are provided on this,\npreventing us from accessing the source for each fault.\nThe work by Wardat et al. [6] introduces DeepDiagnosis, a debugging approach that localises faults, reports error\nsymptoms and suggests fixes for DNN programs. To evaluate DeepDiagnosis, the authors employed a dataset of 53\nreal-world DNNs from GitHub and SO, and 391 from AutoTrainer. While the ones from AutoTrainer are excluded\nfor the reasons mentioned before, we proceeded to look into the 53 DNNs from GitHub and SO. Unfortunately, the\nreplication package of DeepDiagnosis only provides the IDs of posts from SO, while providing no point of reference\nfor the GitHub issues. Moreover, there is no source code provided for the faulty and ground truth versions of each\nanalysed bug.\nThe work by Li et al. [23] performs an empirical study to investigate the impact of existing data cleaning on ML\nclassification tasks. For this purpose, the authors use 14 real-world datasets with real errors. The replication package\nfor this work contains a pdf document that provides a high-level textual description of the errors. However, there are\nno pairs of faulty and cleaned ground truth datasets provided, making it impossible for us to explore these datasets as\nexample of real faults in training data. The work by Liang et al. [8] presents gDefects4DL, a dataset of 64 bugs. Along\nwith the buggy/fixed versions, the authors claim to provide an isolated environment to replicate each bug. The paper\ncontains links to both a cloud service using which the bugs can be downloaded and a web page that can be used to\nnavigate the dataset. However, both links are no longer active. We have contacted the authors of the benchmark and\nthey have confirmed the problem with the links. Our request to provide access to the dataset did not receive a response\nat the time of submission.\nOverall, after removing the works that did not satisfy the criteria for the benchmark of real reproducible faults or were\nnot available, we were left with five benchmarks to perform our realism analysis on."}, {"title": "List of Fault Benchmarks", "content": "The five benchmarks that survived our multi-step elimination process are: DeepLocalize [24], DeepFD [4], De-\nfects4ML [7], SFData [13, 12], and the dataset provided in the study by Zhang et al. [16]. In Table 1, we provide some\nbrief information on these benchmarks such as their publication venue(s), main purpose, overall number of bugs and in\nbrackets the number of analysed bugs (the reasons why we excluded some bugs from the analysis are detailed later\nin Section 4.4). As the table shows, all the benchmarks have been published at prestigious SE venues and contain a\nsignificant number of bugs.\nThe work by Zhang et al. [16] focuses on studying the characteristics of faults in DL programs built on top of the\nTensorFlow framework. The authors investigate the symptoms and root causes of the faults, as well as the main\nchallenges associated with detecting and localising them. This analysis has resulted in a benchmark of 151 faults (75\nfrom GitHub and 76 from SO) classified across seven root causes. We refer to this dataset as TFBugs2018.\nDeepLocalize [24] is a fault localisation tool that automatically determines whether a model under test is faulty and\nidentifies the root causes of the bug detected. It does so by adding a callback mechanism to the model's training process\nand analysing the historic trends in values propagated between layers. The evaluation of DeepLocalize is performed on\n41 bugs, 11 coming from GitHub and 30 from SO.\nDeepFD [4] is a fault diagnosis and localisation framework which maps the fault localisation task to a learning problem\ninstead of employing a rule-based approach as in DeepLocalize. The evaluation of DeepFD is performed on 52 real\nfaults (5 from GitHub and 47 from SO). As the starting point for mining the faults, the authors indicate previous\nstudies that focused on analysing types of DL faults without reproducing them [10, 25] as well as the benchmark\nof DeepLocalize. A closer look into the benchmark reveals that 34 out of its 52 faults come from DeepLocalize.\nHowever, we checked whether the files provided for faulty and fixed versions are identical and this was not the case.\nOur understanding is that while the authors used the same SO posts and GitHub commits as sources of the faults, they\nperformed the process of reproducing the bug independently.\nSFData [13, 12] is a fault benchmark that has been collected as part of the works by Wu et al., who proposed Tensfa [13]\nand Tensfa2 [12]. Both approaches aim to automatically repair tensor shape faults in DL programs. The 146 faults in\nSFData all come from SO and are collected by the authors to evaluate the proposed approaches.\nThe work by Morovati et al. [7] focuses on the challenges attributed to the process of extracting reproducible and\nverifiable faults in DL-based systems. The main contribution of this study is a Defects4ML benchmark of 100 pairs of\nthe faulty and fixed programs originating from GitHub or SO that were either mined by the authors or extracted from\nexisting literature [25, 10, 24, 16, 5]. In their selection process, the authors considered fault types that fall under the\nmain categories of the taxonomy of real faults in DL systems [10]. Out of the five benchmarks analysed in our study,\nDefects4ML is the only one that has been published in a paper that specifically aims to create a DL fault benchmark,\nwhile all the others were generated to be used in the evaluation of the approach proposed in their original publications."}, {"title": "Analysis of Fault Benchmarks", "content": ""}, {"title": "Research Questions", "content": "The goal of our study is to critically evaluate the reproducibility and realism of fault benchmarks used in the research\narea of \"Software Engineering for DL\", as well as provide some insights into how representative they are of DL faults.\nTo this extent, we have performed a set of analyses and experiments to answer the following research questions:\nRQ1 [Correspondence to Source]: Do the faulty versions in the benchmarks and their fixes correspond to the source\nfrom which the fault was extracted?"}, {"title": "Experimental Procedure", "content": ""}, {"title": "RQ1 [Correspondence to Source]", "content": "To extract the source code of the faulty and fixed code pairs in the benchmarks, we first locate the links to the replication\npackages in the corresponding papers. The replication packages usually contain spreadsheets listing all the faults with\ntheir source indicated and the pairs of source code files for each fault.\nFor a fault to be considered \"real\", its faulty version in the benchmark must correspond to the source it was extracted\nfrom. For each item in the list, we follow the links to SO posts or GitHub commits. In GitHub, the link takes us to a\ncommit that includes changes in the files associated with the fault. For each file, we extract the source code before the\ncommit. In SO, we copy the code provided in the post. We then generate a diff between the code obtained from GitHub\nor SO and the faulty version from the benchmark, and we manually analyse it. If the changes in the diff are only pieces\nof code added for the purpose of debugging (e.g., print statements; different verbosity levels in model.fit statement) or\ndifferent syntax that leads to the same semantics (e.g., indicating the activation as the parameter of the layer instead of\nadding it as a separate layer or declaring the hyperparameter value as a separate variable instead of passing its value\ndirectly to the model.compile statement), we ignore them. However, if the diff contains alterations to the code that lead\nto a different model structure, hyperparameter value or training process, we mark the fault as not corresponding to its\nsource. It should be noted that sometimes in SO not the whole code is available, but only the model structure. In such\ncases, we require only the available code not to be changed, as this part is reported to be the cause of the bug. The\nremaining missing parts of the code can be adjusted by the authors of the benchmark as they see fit, since there is no\nsource that we can use to check the correspondence to.\nOur next step is to check whether the fix applied to the faulty model also corresponds to the source of the fault. In\nGitHub, we extract the code after the commit and compare it to the fixed source code provided in the benchmark. In\nSO, very often there is no fixed source code available and the fixes are provided as textual suggestions. Whether the\nprovided suggestion indeed solves the reported problem is usually indicated by the post author marking the answer with\nthe suggestion as \"accepted\". Therefore, the first point of our analysis is to check whether an SO post has an accepted\nanswer. If this is not the case, then it is not possible to claim that there exists a ground truth fix for the given fault.\nHowever, we still check if any of the \u201cnon-accepted\u201d answers correspond to the fix applied in the benchmark, as they\nmight still be fixing the reported problem. Note that as there is not necessarily the source code of the fix in SO, we read\nthe answers and manually check whether the suggestions provided in them are indeed applied in the fixed files in the\nbenchmark. Given that the answers are written in free text format, their direct application to the source code might be\nprone to subjectivity. To ensure we keep this process systematic, we apply the following rules:"}, {"title": "RQ2 [Training Dataset]", "content": "For each fault in the benchmark we manually check whether the training dataset used in the benchmark is the same as\nthe one indicated in its source (GitHub or SO). For both GitHub and SO, the employed dataset can be identified in the\nparts of code that load the training data. In SO, sometimes the datasets are just mentioned in the text of the post and not\nin the code. Once we check that the datasets are the same, we then proceed with analysing whether a real or a fake\ndataset has been used. We consider a dataset real if it is a known existing dataset used previously for a learning task or\nif it is a meaningful data generated using some automation (such as data generated using mathematical formulas for\na regression task). In contrast, a dataset is considered fake if it does not refer to any existing dataset or meaningful\ndata (e.g., it just contains randomly generated values). If a dataset is real, we record its name. If a dataset is fake, we\nfurther analyse the size of the dataset and whether it is generated randomly or not."}, {"title": "RQ3 [Fault Types]", "content": "In this RQ, we aim to identify and analyse types of the faults that affect real faulty models through the applied fixes. As\nthe reference for the list of possible fault types in DL systems, we use the taxonomy by Humbatova et al. [10]. This\ntaxonomy was created by manually analysing 1,059 SO posts and GitHub issues and interviewing 20 professional\ndevelopers. The taxonomy was further validated by surveying with another set of 21 developers. It is organised into five\ntop-level categories (three of which are divided into inner subcategories) and contains 92 types of faults.\nWe manually analyse the diffs between the buggy and fixed source code as provided in the benchmark and list the fault\ntypes that comprise each diff. The diffs that contain changes to multiple elements of the DL model (e.g. changing both\nthe activation function for the last layer and the loss function for the model) are assigned with multiple fault types. If\nthe same fault type takes place in different parts of the DL model (e.g., activation is changed for multiple layers), we list\nthis fault type multiple times."}, {"title": "RQ4 [Similarity to Artificial Faults]", "content": "To extract the list of fault types that can be simulated by injecting artificial faults we use the DeepCrime [26, 27]\nmutation testing tool. We choose DeepCrime as it is the only available tool that introduces changes to the source code of\nthe DL model before training (instead of altering weights and biases of an already trained model [28]). DeepCrime has\n24 mutation operators extracted from 793 real faults in DL systems and has been implemented for the Keras platform.\nThese mutation operators introduce changes to the training data, hyperparameters, activation function, regularisation,\nweights initialisation, loss function, optimisation function and validation process.\nHowever, DeepCrime injects only single-order mutants, i.e., it can change either the activation function or the loss\nfunction, but not both of them at the same time. In contrast, the faults in the benchmarks can contain changes to\nmultiple elements. We use the analysis performed in RQ4 that extracts the list of fault types present in each fault of the\nbenchmarks. We then manually compare each of these fault types (that correspond to single-order mutants) against the\n24 mutation operators of DeepCrime and check whether each of them could have been injected by DeepCrime. In fact,\napplying DeepCrime multiple times could lead to generating the composite faults present in the benchmark. As a result\nof this analysis, for a composite fault X that contains n changed elements ($X_1$, $X_2$, ..., $X_n$), we will obtain a list of\nsize n where each i-th element has a boolean value indicating whether the change to element $X_i$ can be simulated by\nDeepCrime. We will refer to the elements $X_i$ as fault components."}, {"title": "RQ5 [Fault Reproducibility]", "content": "For each analysed fault, we identify whether it satisfies our four conditions for realism. If that is the case, then we\nattempt to reproduce the bug. It should be noted that as SFData has a specific focus on tensor shape faults and the\nusage of realistic training data is not relevant to the reproduction of such faults. Therefore, we remove the two realism\nconditions on training data for the bugs from this dataset.\nThe reproducibility of the bugs is highly dependent on the versions of Python and of the frameworks/libraries. For\nTFBugs2018 and SFData no versions are provided. For DeepLocalize and DeepFD the versions to run the proposed"}, {"title": "Manual Analysis", "content": "As indicated in the experimental procedure, our first 4 RQs require careful manual analyses by a human. To make\nthis analysis more systematic, we identified the information that needs to be extracted for each RQ and organised this\ninformation into columns of a spreadsheet that is available as part of our replication package [31].\nWe divided all the faults between two authors. To ensure that they have a shared understanding of the task, we conducted\na pilot study. We randomly selected 10 SO and 10 GitHub faults from the Defects4ML dataset. Each of the authors\nperformed the analysis on these 20 faults independently and then they had a meeting to discuss the disagreements.\nThe disagreements were mostly due to grey areas on 1) how to convert free text fix suggestions in SO into changes in\nthe code, 2) when to consider a dataset real or fake, and 3) how to handle composite bugs. The discussion of these cases\nled to more precise rules and definitions already discussed in the experimental procedure for the corresponding RQs.\nMoreover, the correspondence of the buggy code in the benchmark to the source from which it was extracted (analysed\nas part of RQ1) has been identified as an important factor during this meeting.\nAfter the pilot study, the two authors proceeded with the manual analysis. Any bugs that refer to unique situations that\nrequire further discussion were taken note of and discussed in periodic meetings. This was the case for 41 faults out of\n490 analysed."}, {"title": "Excluded Bugs", "content": "A closer investigation of the benchmarks revealed that not all bugs could be analysed as part of our study. The value in\nbrackets in column #Bugs' of Table 1 reports the number of bugs that were kept for our analysis. In DeepLocalize,\nDeepFD and SFData, overall 17 bugs were excluded. The most common reasons for the exclusion of bugs were as\nfollows: 1) the bugs referred to a GitHub link that was no longer valid, 2) the bugs were extracted from SO posts with\nno code or with no responses, and 3) the bugs were extracted in a way that misrepresented the SO post.\nWe have excluded 30 bugs out of 100 in Defects4ML. In seven of these cases there was no actual bug represented, as\nthe buggy versions were either fully identical to the fixed one or the changes included only variable renaming or added\nprint statements. In one case the files in the benchmark did not correspond to the code in the source at all, while in two\ncases the source of the bugs was not indicated. Moreover, the benchmark contained three duplicate entries of other\nissues from the benchmark. The remaining of the excluded bugs were either referring to simple 'how to' questions or to\nbugs not in a DL system (such as general programming errors or changes not related to DL).\nLastly, for TFBugs2018 we had to exclude all the bugs originating from GitHub and 3 bugs from SO as there were\nno links to corresponding sources available. A careful analysis of the remaining 73 bugs revealed further reasons for\nexclusion: 'how to' questions (20), issues related to APIs and framework versions/compatibility (18) and bugs not in\nDL systems (7). In 3 bugs, the benchmark entry did not bear similarities to the source. Finally, we found 3 duplicates of\nexisting issues from the same dataset."}, {"title": "Results", "content": ""}, {"title": "RQ1 [Correspondence to Source]", "content": "Table 2 reports results on the correspondence of the faults in the benchmark and their fixes to the sources they were\nextracted from. Column \u2018#Analysed bugs' reports the number of analysed faults, and column \u2018Buggy m.' reports the\nnumber of cases when the buggy code in the dataset fully corresponds to its source. Similarly, column \u2018Fix m.' reports\nthe number of instances in which the fixed version in the benchmark matches the fix provided in the source. The column"}, {"title": "RQ2 [Training Dataset]", "content": "Column 'TD m.' in Table 2 reports the number of cases in which the training data provided as the source of each bug\nmatches the data used in the benchmark. The correspondence to the original training dataset varies between 45.4% and\n74.3% across the datasets. The main cause of mismatches, ranging from 42% to 85% of cases across benchmarks, is the\nlack of training dataset information in SO posts. SFData is most affected because it focuses on tensor shape faults, and\ntherefore very often its faults are reported either using placeholders or randomly generated fake data.\nThe number of faults in each benchmark for which the used training data is real is reported in Column \u2018TD real' of\nTable 2. The instances of training data not being real stem from the previously discussed cases of the original training\ndata not being mentioned or the SO posts using toy training data. For both DeepFD and DeepLocalize, the size of fake\ntraining datasets varies between 8 and 110,000 elements (they share a lot of the SO posts). For TFBugs2018, the fake\ndataset can be as small as 1 input and as big as 88,041. For Defects4ML, the smallest generated dataset has 5 inputs,\nwhile the largest has 60,000. Lastly, the size of generated data for SFData varies between 1 and 11,200 elements."}, {"title": "RQ3 [Fault Types]", "content": "Table 3 reports statistics on the composition of the bugs in the benchmarks, such as the maximum/average number of\nfaults associated with each reported bug (i.e., the \"order\" of the bug, similarly to the order of a mutant in mutation\ntesting), as well as their sum. Column \u2018#FT' reports the number of unique fault types across the whole benchmark. The\nrow 'ALL' contains information for the union of all benchmarks. Note that the numbers indicated in brackets are for the\nsubset of bugs that satisfy all the conditions for realism."}, {"title": "RQ4 [Similarity to Artificial Faults]", "content": "Table 4 presents results on whether the bugs in the benchmarks can be simulated using existing mutation operators.\nColumns 'RSB A.' and 'RSB R.' report the ratio of bugs that can be generated by applying the DeepCrime mutation tool\nto simulate each of its faults for the whole benchmark and for the subset of faults that satisfy the conditions for realism,\nrespectively. In contrast, columns 'RSFC A.' and 'RSFC R.' provide a more fine-grained picture and report the ratio of\nfault components that can be simulated."}, {"title": "RQ5 [Reproducibility]", "content": "The number of bugs that have fulfilled our conditions on realism and which we therefore attempted to reproduce are\nreported in column \u2018#Bugs' in Table 5. Our attempts were first challenged by the version problems. For most of the\nbugs, we were able to adapt the code to run on newer versions with minor adjustments. However, some bugs depended\non outdated APIs and required a complete rewrite to be compatible with newer versions. The number of these issues is\ndetailed in the \u2018#Version Problem' column. The number of bugs that we could successfully reproduce is reported in\ncolumn '#Reproduced'. Column \u2018#Stable' reports the number of bugs that were stable across all 20 performed runs."}, {"title": "Threats to Validity", "content": "Construct. The main threat to the construct validity is our choice of characteristics to consider a DL fault in a\nbenchmark as \u201creal\u201d. To minimise this threat we use straightforward and unambiguous criteria such as correspondence\nto source for training data and buggy/fixed versions. We also provide a precise definition for what we consider a realistic\ntraining data.\nInternal. One possible threat to the internal validity is that our literature analysis may not include all relevant works.\nTo ensure that we covered the list of works referring to reproducible real DL faults, we employed a comprehensive\nsearch string and performed both forward and backward snowballing.\nAnother possible threat is the possibility of human error when performing the manual analysis. To minimise this risk\nwe have conducted a pilot with two of the authors to align their understanding of the task. We have also employed\nautomated tools for comparison of the files. Lastly, the authors took note of any non-trivial issues and ensured the\ndecision-making for those cases was performed via consensus."}, {"title": "Discussion", "content": ""}, {"title": "Towards More Realistic Benchmarks", "content": ""}, {"title": "Representativeness", "content": "During our analysis, along with the data required to answer our research questions, when available, we also collected\ndata on what type of task (classification or regression) the DL model associated with the bug is focused on and what DL\nframework it uses. As the results in Table 6 show, in 80% of cases, the task was the classification (Column 'C'). When\nit comes to the DL frameworks, 2 out of 5 datasets focus specifically on Keras, one on TensorFlow (TF), while the\nremaining two use both Keras and TF. This translates into 71% of the faults implemented with Keras and 29% with TF."}, {"title": "Sources of Bugs", "content": "84% of the faults we analysed originate from SO and only 16% come from GitHub. It should be noted that using SO as a\nsource of bugs is something specific to DL research, as for traditional software systems, bugs are commonly mined from\nGitHub. This is mostly due to the fact that GitHub repositories for DL models very often do not include intermediate\npoints in the DL model development process, but are committed once a well-performing model has been trained. While\nthe usage of SO is understandable in terms of convenience and providing information on such intermediate problems,\nit comes with a lot of drawbacks. As SO is a discussion forum, posts very often do not contain the full code and\ninformation on the used training dataset. This creates a need for the benchmark authors to fill the gaps. As the symptom\nfor DL models is usually also indicated in very imprecise terms (such as low accuracy, slow training process, etc.) it is\nhard to ensure that the changes introduced to fill such gaps match the root causes of the reported symptoms. As there is\na low rate of match between the buggy version and the initial source code in SO, our assumption that the information in\nthe posts is very often not sufficient to reproduce the reported problem. Hence the authors sometimes used SO posts\nas an inspiration of an example code and then introduced some changes that reproduce to some degree the reported\nsymptoms. The same applies to the performed fixes that also have a low match rate to the answers provided in SO,\nmost likely due to the vagueness in the answers and their failure to really fix the fault. While faults generated in such\na manner might indeed provide pairs of buggy and fixed models, this approach is not very different from (manual)\nmutation testing and such benchmarks create a false promise of real bugs.\nWe believe the usage of SO for the purpose of extracting real bugs can only be reliably performed if strict criteria are\napplied to the posts: relevant posts must report the whole code and indicate the training dataset used, as well as contain\nan accepted answer proposing a precise fix solution. However, we understand that the number of such posts is limited\nand that their identification requires significant manual effort."}, {"title": "Benchmark Independence", "content": "4 out of the 5 benchmarks that we have analysed have been generated specifically to evaluate the approach proposed by\nthe authors of the benchmark. In the absence of any dataset of DL faults, this is an understandable choice. Moreover,\nthe authors provide a systematic procedure on how SO and Github issues have been mined. However, since for many\nfaults, both the buggy and the fixed versions have changes not indicated in the respective source, there is a risk of\nunconscious bias towards introducing faults and fixes that would highlight the strengths of the proposed approach. For\nthis reason, it is important that independent benchmarks such as Defects4ML are created and maintained, to allow for\nan evaluation on a dataset not generated by the same authors who are proposing and evaluating a novel approach."}, {"title": "Benchmark Maintainability", "content": "Our attempts to reproduce the bugs in these benchmarks show problems with the maintainability of the bugs, as they\nrely on old versions of Python and the DL frameworks and often can not be run with more recent versions and hardware.\nOne effective strategy to address this problem is containerisation using platforms like Docker. This would allow for\npreserving the conditions under which a bug was originally identified and ensure that legacy software in these bugs\nremains functional despite updates to the underlying frameworks."}, {"title": "Mutants vs. Real Faults", "content": "Our results have important implications for mutation testing tools of DL systems. As mentioned before, the existing\ntools are capable of applying only first-order mutants, while our analysis of real faults indicates that 4"}]}