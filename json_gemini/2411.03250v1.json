{"title": "DIFFLM: CONTROLLABLE SYNTHETIC DATA GENERATION VIA DIFFUSION LANGUAGE MODELS", "authors": ["Ying Zhou", "Xinyao Wang", "Yulei Niu", "Yaojie Shen", "Lexin Tang", "Fan Chen", "Ben He", "Le Sun", "Longyin Wen"], "abstract": "Recent advancements in large language models (LLMs) have significantly en-hanced their knowledge and generative capabilities, leading to a surge of inter-est in leveraging LLMs for high-quality data synthesis. However, synthetic data generation via prompting LLMs remains challenging due to LLMs' limited un-derstanding of target data distributions and the complexity of prompt engineering, especially for structured formatted data. To address these issues, we introduce DiffLM, a controllable data synthesis framework based on variational autoencoder (VAE), which further (1) leverages diffusion models to reserve more information of original distribution and format structure in the learned latent distribution and (2) decouples the learning of target distribution knowledge from the LLM's generative objectives via a plug-and-play latent feature injection module. As we ob-served significant discrepancies between the VAE's latent representations and the real data distribution, the latent diffusion module is introduced into our framework to learn a fully expressive latent distribution. Evaluations on seven real-world datasets with structured formatted data (i.e., Tabular, Code and Tool data) demon-strate that DiffLM generates high-quality data, with performance on downstream tasks surpassing that of real data by 2%-7% in certain cases. The data and code will be publicly available upon completion of internal review.", "sections": [{"title": "INTRODUCTION", "content": "Data Synthesis has become an indispensable technique in current machine learning research, en-abling rapid generation and modification of datasets (Bauer et al., 2024), allowing researchers to experiment with various scenarios and model architectures without the extensive processes associ-ated with real-world data collection. Meanwhile, with the rapid advancements in large language models (LLMs), recent research in natural language processing (NLP) has increasingly focused on leveraging LLMs for synthetic data generation. Early efforts attempted to fine-tune LLMs to align with real data distributions (Keskar et al., 2019; Anaby-Tavor et al., 2020; Borisov et al., 2023). As the in-context learning capabilities of LLMs have improved, some studies have explored zero-shot or few-shot prompting of LLMs to generate synthetic data (Ye et al., 2022a; Wei et al., 2024).\nDespite the progress achieved, generating high-quality synthetic textual data using LLMs remains challenging, particularly for structured data (Josifoski et al., 2023; Li et al., 2022). First, LLMs often lack a global understanding of the target data distribution when generating synthetic data. Even after fine-tuning, it is difficult to inject information about complex and varied distributions into current LLM architectures, often resulting in outputs with low diversity and instances of data copying (Wu et al., 2024; Yu et al., 2023). Moreover, existing LLM-based synthetic data generation methods typically involve complex pipelines and post-processing mechanisms, such as prompt engineering, multi-agent frameworks, and iterative sampling (Dekoninck et al., 2024; Wu et al., 2024). These complexities hinder the rapid adaptation of LLMs to new tasks, limiting their utility in dynamic"}, {"title": "RELATED WORKS", "content": "Large Language Models in Data Synthesis. The recent advancement in the generative capabil-ities of LLMs has motivated numerous exploratory works aiming to leverage these models for data augmentation in areas such as text classification (Ye et al., 2022a; Li et al., 2023), information ex-traction (Tang et al., 2023; Josifoski et al., 2023), and tabular data generation (Borisov et al., 2023; Xu et al., 2024). A comprehensive survey conducted by Long et al. (2024) proposes a prompt-based generic workflow for synthetic data generation, curation, and evaluation. And multiple advanced works have attempted to fine-tune language models for data synthesis in recent years (Anaby-Tavor et al., 2020; Kumar et al., 2020; Dinh et al., 2022; Borisov et al., 2023; Xu et al., 2024). Specifically, these methods involve fine-tuning LLMs on a small amount of gold data for language modeling, followed by the use of various sampling methods to generate data. However, a major challenge remains in ensuring that synthetic data accurately reflects real-world distributions. Veselovsky et al. (2023) has shown that LLM-generated data can sometimes diverge from actual data distributions, leading to unfaithful representations that may hinder model training. Some studies have explored data selection (Puri et al., 2020) or data augmentation (Ye et al., 2022b) to address this distribution gap, but there remains significant room for improvement.\nLatent Variable Models in Text Generation. Latent variable models have made significant ad-vances in computer vision in recent years (Yu et al., 2022; Gu et al., 2022; Luo et al., 2023; Gulrajani et al., 2017), achieving high-quality generation results, flexibility and effectiveness, as well as ro-bustness to noise perturbations. In particular, latent diffusion models, such as DALL-E (Betker et al., 2023) and Stable Diffusion (Rombach et al., 2022), operate their diffusion processes in a la-"}, {"title": "METHODOLOGY", "content": "Figure 1 illustrates the main pipeline of our proposed DiffLM. First, we define an encoder to map discrete data into a continuous latent space (Section 3.2). Second, although text features are ex-tracted and compressed, vanilla latent embeddings in VAEs often lead to decoding failures due to underutilized or empty regions in the latent space. To address this issue, we adopt a latent diffusion model (Section 3.3). Finally, to incorporate the prior knowledge into the decoding stage of LLMs, we propose a novel soft prompt injection method to steer the decoding process (Section 3.4).\n3.1 PROBLEM FORMULATION\nWe begin by defining $D$ as a known small set of real-world distribution data, where each element $x$ represents a real sample. We define $G$ as the synthetic data generator, which learns the distribution of $D$ and generates a set of synthetic samples, $D_{syn}$, ensuring that the model does not simply memorize and reproduce the same real samples, meaning $D \\cap D_{syn} = \\O$. It should be noted that we focus on the task of unconditional data synthesising using LLMs, where $G$ generates synthetic samples independently of any additional context, i.e., without using explicit prompt text.\n3.2 VAE-BASED REPRESENTATION LEARNING\nFeature Encoding: In standard VAEs, an encoder is typically employed to map input data into a latent space. Given structured text data $s_i$, we utilize a learnable Transformer-based pre-trained lan-"}, {"title": "LATENT SPACE DENOISING", "content": "Although VAE can learns latent space representations of data, directly sampling from the prior distribution $p(z)$ often exhibit low quality generated samples. In our preliminary experiments, we observed that directly utilizing the latent features learned by the VAE frequently produces text that is unrelated to the target data distribution. This issue arises due to the discrepancy between the encoder's learned posterior distribution $q_{\\phi}(z|x)$ and the prior $p(z)$. To address this problem, we introduce a diffusion model in the latent space to more accurately model the true distribution of the latent features. Inspired by Zhang et al. (2024), we extract the latent vectors $z \\in Z$ from the trained VAE for each data point $x \\in D_{train}$. Starting from the initial latent vector $z_0$, we progressively add noise over time following a linear schedule to get $z_t$. During the reverse diffusion process, we employ a standard continuous denoising network to recover $z_0$ (Song et al., 2021). For the training objective, we optimize the diffusion model through denoising score matching (Karras et al., 2022):\n$z_t = z_0 + \\sigma(t)\\epsilon, \\epsilon \\in N(0, I),$  (5)\n$dz_t = -\\delta(t)\\sigma(t)\\nabla_{z_t} log p(z_t)dt + \\sqrt{2\\delta(t)\\sigma(t)}dw_t,$ (6)\n$L_{diff} = E_{t~p(t), z_0~p(z_0), \\epsilon~N(0,1)} ||\\epsilon_\\theta(z_t, t) - \\epsilon||^2,$ (7)\nIn forward process Eq.5, $z_t$ is the latent variable at time $t$, and $\\sigma(t)$ is a time-dependent noise scale function. As for backward process Eq.6, $\\delta(t)$ stands for the time derivative of $\\sigma(t)$, and $\\nabla_{z_t} log p(z_t)$ is the gradient of the log probability density with respect to $z_t$, also known as the score function, and $dw_t$ is an increment of the Wiener process (standard Brownian motion). For diffusion model training loss Eq.7, $\\epsilon_\\theta(z_t, t)$ is the neural network that predicts the noise $\\epsilon$ given $z_t$ and $t$. The detailed description for diffusion model could be found in Appendix A.1."}, {"title": "LATENT FEATURE INJECTION", "content": "After constructing a latent space that captures the true data distribution, two challenges remain: 1) Aligning latent space with LLM's input space. How can the decoding LLM process the latent vector"}, {"title": "EXPERIMENTS", "content": "In this section, we evaluate the generation quality of the DiffLM method on multiple public bench-marks across three tasks: 1) Tabular Data Generation: We compare DiffLM with SoTA tabular generation algorithms, demonstrating its strong capability in structured data generation. 2) Code Generation: DiffLM showcases the ability to integrate structured data priors with its internal knowl-edge. The results on synthetic data are even better than real ones. 3) Tool Generation: DiffLM can quickly adapt to complex function call scenarios, highlighting its flexibility and adaptability.\n4.1 TABULAR DATA GENERATION\nBenchmarking. We selected five publicly available datasets for evaluation, encompassing both classification and regression tasks: Adult, Beijing, Default, Magic, and Shoppers. The properties of datasets are presented in Table 5. To assess the quality of synthetic data, we employed two perspec-tives: 1) Low-order statistical metrics, where we quantified column-wise density estimation using the Kolmogorov-Smirnov Test for numerical columns and the Total Variation Distance for categor-ical columns; 2) Downstream task performance, where we measured the predictive accuracy on test data of classifiers or regressors trained on the generated data.\nBaselines. We selected a comprehensive set of classic and SoTA tabular data generation models with diverse architectures for comparison. First, we consider the performance on real data as the upper bound for evaluation. Secondly, we included the classic method, synthetic minority over-sampling technique (SMOTE) (Chawla et al., 2002), which generates new synthetic data patterns by performing linear interpolation between minority class samples and their k nearest neighbors. Additionally, for neural network-based tabular generation algorithms, we considered six baseline"}, {"title": "CODE GENERATION", "content": "Benchmarking. In the code generation scenario, to simplify the problem, we focus on Python code and use the Flytech\u00b9 dataset as real data, which contains 24,813 unique real user queries and the corresponding Python code fulfilling those requests. We discard the user queries and use only the code to train DiffLM. After generating synthetic code data, we continue pre-training the Mistral 7B v0.3 base model (Jiang et al., 2023) using a smaller learning rate, i.e., 1e-5, in a causal language"}, {"title": "TOOL GENERATION", "content": "Evaluation. To address more complex structured data generation scenarios, we further conduct a tool synthesis task. Specifically, we select the ToolBench (Qin et al., 2024) dataset as a benchmark for comparison, which is constructed based on the RapidAPI\u00b2 platform by crawling APIs created by real users and synthesizing related dialogue SFT data using GPT3. We use the its toolset to train DiffLM and then sample an equal number of tools for comparison. We assess the usability of the generated tools from two perspectives: 1) Single-Tool Quality: We use GPT-4 as an annotator to score the real and synthetic data across multiple dimensions on a scale from 0 to 10, where the"}, {"title": "ANALYSIS", "content": "5.1 ABLATION STUDY\nThe effect of adaptive $\\beta$ adjustment. As described in Section 3.2, we use a decreasing $\\beta$ ad-justment strategy to train the VAE latent space. Here, we compare this with another method that uses a cyclical schedule to anneal $\\beta$ (Fu et al., 2019), evaluating both the loss decline curves and downstream task performance to demonstrate the effectiveness of our decreasing strategy. Firstly, as shown in Figure 3, the KL-divergence loss trends under decreasing $\\beta$ exhibit a pattern where the loss first increases, then decreases, and then increases again. This indicates that during the early stages of VAE training, DiffLM uses a larger $\\beta$ to focus on the divergence between the embedding distribution and the standard Gaussian. This helps quickly learn a standard latent space to stabilize the training of the LLM module. Subsequently, when the reconstruction loss reaches a bottleneck, it gradually reduces the weight of the KL-divergence loss. At this point, the training objective shifts"}, {"title": "TRAINING DATA PLAGIARISM", "content": "Data copying is a significant challenge for overfitted generative models in practical applications. To verify that the data generated by DiffLM is not merely copied from the training data, we compute the Distance to Closest Record (DCR) metric. Specifically, for each row in the tabular data, we represent the categorical columns using one-hot vectors and perform min-max normalization on the numerical columns. We then define DCR as the minimum L1-distance between a synthetic data point and each training sample point:\n$DCR(X_{syn}) = \\underset{X_{real} \\in D_{train}}{min} L1(X_{syn}, x_{real}).$ (8)\nThe DCR distribution is shown in Figure 5. We observe that the LLM-based GReaT generates results that differ significantly from the training data, indicating that vanilla fine-tuning struggles to adapt LLMs to real data distributions and generate high-quality results. DiffLM demonstrates a DCR distribution similar to that of the SoTA method TabSyn on both datasets. This further indicates that our proposed general-purpose data synthesis framework can achieve performance on par with domain-specific models on specific tasks."}, {"title": "VISUALIZATION", "content": "Figure 6 presents 2D t-SNE visualizations of the latent space for multiple datasets, including four categorical tabular datasets, one numerical tabular dataset, and one tool dataset. We use DiffLM trained on the corresponding datasets to encode their validation sets, obtaining latent features. It can be observed that data of the same class encoded by DiffLM exhibit clustering characteristics in the latent space, as seen in the Adult and Magic. Notably, in the numerical dataset Beijing, different target values display a clear transitional distribution: the upper part of the 2D space corresponds to data with larger target values, i.e., 157 to 858, while the lower part corresponds to data with smaller target values, i.e., 1 to 23. These results demonstrate that DiffLM's latent space learning strategy can effectively capture the real data distribution."}, {"title": "CONCLUSION", "content": "In this paper, we introduce DiffLM, a novel framework designed to enhance LLM's understanding of real-world data distributions in synthetic data generation tasks. DiffLM leverages a VAE to map real data into a latent space, which is then injected into the decoding process of LLM, enabling end-to-end training through causal language modeling objective. Additionally, we incorporate a dif-fusion process to further refine the learning of the latent distribution, mitigating the sampling failures caused by latent space discrepancies. To flexibly and non-intrusively control the structure and qual-ity of the generated data, DiffLM integrates real data information with LLMs' internal knowledge by freezing the LLM parameters and using the latent features as plug-in modules. Experimental results demonstrate that DiffLM produces highly robust and consistent outputs. In all datasets, the performance of downstream models trained on the generated data is comparable to or even surpasses that of models trained on real data."}, {"title": "DETAILS ON MODEL DESIGN", "content": "A.1 DIFFUSION PROCESS\nIn this section, we will introduce the general process of latent diffusion models. Latent Diffusion Models (LDMs) are a class of diffusion probabilistic models that operate in the latent space of an autoencoder rather than directly on the high-dimensional data space. By performing diffusion in a compressed latent representation, LDMs significantly reduce computational complexity while maintaining high fidelity in data generation. An LDM consists of two primary components:\n1. Autoencoder: Encodes input data $x_0$ into a latent representation $z_0$ = $E(x_0)$ and decodes latent variables back to data space $x$ = $D(z)$.\n2. Diffusion Model: Defines a diffusion process on the latent variables ${z_t}_{t=0}^{T}$.\nIt should be noted that the variable used here is independent with main text.\nForward Process (Diffusion). The forward diffusion process in latent space progressively adds Gaussian noise to the latent representation over T timesteps. Starting from the initial latent code $z_0$ = $E(x_0)$, obtained by encoding the data $x_0$, the forward process is defined as:\n$q(z_t | z_{t-1}) = N(z_t; \\sqrt{ 1 - \\beta_t } z_{t-1}, \\beta_tI),$ (9)\nwhere $\\beta_t \\in (0,1)$ is a predefined variance schedule that controls the amount of noise added at each step $t$, and $N$ denotes a Gaussian distribution. By recursively applying this process, we can express $z_t$ directly in terms of $z_0$:\n$q(z_t | z_0) = N(z_t; \\sqrt{\\bar{a_t}} z_0, (1 - \\bar{a_t})I),$ (10)\nwhere $a_t = 1 - \\beta_t$ and $\\bar{a_t} = \\prod_{i=1}^{t} a_i $. This formulation allows efficient sampling of $z_t$ at any arbitrary timestep $t$ without iterating through all previous steps. In this paper, we adopt the Variance Exploding defined perturbation kernels, whereas setting $s_t = \\sqrt{1 - \\beta_t}$ and $\\sigma_t = \\sqrt{\\frac{1 - \\bar{a_t}}{1}} I$. Also, we set $s_t = 1$ to directly add noise to the data rather than weighted mixing, convert Eq.10 to:\n$q(z_t | z_0) = N(z_t; 0, \\sigma_t^2 I)$ (11)\nReverse Process (Denoising). The reverse diffusion process aims to recover $z_0$ from a noisy la-tent variable $z_t \\sim N(0, I)$. It is parameterized by a neural network $\\epsilon_\\theta$, which predicts the noise component at each timestep:\n$p_\\theta(z_{t-1} | z_t) = N(z_{t-1}; \\mu_\\theta(z_t, t), \\Sigma_\\theta(z_t, t)).$ (12)\nTypically, the model predicts the mean $\\mu_\\theta$ while the covariance $\\Sigma_\\theta$ is fixed or simplified. By lever-aging the properties of the forward process, the mean can be parameterized to predict the original noise $\\epsilon$ added during the forward diffusion:\n$\\mu_\\theta (z_t, t) = \\frac{1}{\\sqrt{a_t}} ( z_t - \\frac{\\beta_t}{\\sqrt{1- \\bar{a_t}}} \\epsilon_\\theta(z_t, t) ) $ (13)\nThis formulation enables the model to denoise $z_t$ step by step, ultimately reconstructing $z_0$.\nLearning Objective. The training objective for LDMs focuses on minimizing the difference be-tween the true noise $\\epsilon$ added during the forward process and the noise predicted by the model $\\epsilon_\\theta$. The simplified loss function is:\n$L_{latent} = E_{x_0,\\epsilon,t} [||\\epsilon - \\epsilon_\\theta(z_t, t)||^2],$ (14)\nwhere $z_t$ is sampled as:\n$z_t = \\sqrt{\\bar{a_t}} z_0 + \\sqrt{1 - \\bar{a_t}} \\epsilon, \\epsilon \\sim N(0, I).$ (15)\nThis objective encourages the model to learn the conditional distribution $p_\\theta (z_{t-1} | z_t)$ by accurately predicting the noise component at each timestep."}, {"title": "EXPERIMENTAL SETUP", "content": "Noise Scheduling. The noise schedule ${\\beta_t}_{t=1}^{T-1}$ plays a critical role in the diffusion process. It dictates how quickly noise is added in the forward process and, consequently, affects the difficulty of the reverse denoising task. Common strategies for setting $\\beta_t$ include linear, cosine, and quadratic schedules. We use use linear noise schedule, i.e., the perturbation kernel $\\sigma(t) = t$. As it is an effective schedule, ensuring that the data is sufficiently diffused by timestep $t$, while still allowing the model to learn meaningful reverse transitions.\nB DETAILS ON EXPERIMENTAL SETUP\nB.1 TABULAR DATA GENERATION\nB.2 TOOL JUDGEMENT PROMPTS\nWe present the evaluation prompts used for assessing tool generation quality in Figure 7 and Fig-ure 8.\nB.3 INSTRUCTIONS FOR REPRODUCTION\nIn this section, we present the experimental details of DiffLM, including data preprocessing, training hyperparameter settings, and data post-processing filtering methods.\nData Preprocessing. Real-world NLP datasets often exhibit inherent structures, such as the con-text, question, and answer in machine reading comprehension tasks, or key-value pairs in tabular generation tasks. In DiffLM, we convert all structured data into JSON format. For instance, tabular data in a CSV file is transformed into lines of JSON, and tools from ToolBench are abstracted into JSON structures comprising tool_name, tool_description, api_name, and api_description. For code data, we use the raw code directly without any preprocessing as input for DiffLM training.\nHyperparameter Settings.\nVAE Encoder: bert-base-uncased\nVAE Decoder: mistralai/Mistral-7B-Instruct-v0.3\nSoft Prompt Tokens k: 64\nSoft Prompt Embedding Dimension d: 4096\n$\\beta_{max}$ = 0.1\n$\\beta_{min}$ = 0.001\nDiffusion Noise Dimension: 4096"}]}