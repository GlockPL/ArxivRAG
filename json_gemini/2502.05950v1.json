{"title": "Survival Concept-Based Learning Models", "authors": ["Stanislav R. Kirpichenko", "Lev V. Utkin", "Andrei V. Konstantinov", "Natalya M. Verbova"], "abstract": "Concept-based learning enhances prediction accuracy and interpretability by leveraging high-level, human-understandable concepts. However, existing CBL frameworks do not address survival analysis tasks, which involve predicting event times in the presence of censored data a common scenario in fields like medicine and reliability analysis. To bridge this gap, we propose two novel models: SurvCBM (Survival Concept-based Bottleneck Model) and SurvRCM (Survival Regularized Concept-based Model), which integrate concept-based learning with survival analysis to handle censored event time data. The models employ the Cox proportional hazards model and the Beran estimator. SurvCBM is based on the architecture of the well-known concept bottleneck model, offering interpretable predictions through concept-based explanations. SurvRCM uses concepts as regularization to enhance accuracy. Both models are trained end-to-end and provide interpretable predictions in terms of concepts. Two interpretability approaches are proposed: one leveraging the linear relationship in the Cox model and another using an instance-based explanation framework with the Beran estimator. Numerical experiments demonstrate that SurvCBM outperforms SurvRCM and traditional survival models, underscoring the importance and advantages of incorporating concept information. The code for the proposed algorithms is publicly available.", "sections": [{"title": "1 Introduction", "content": "Concept-based learning (CBL) [1, 2, 3, 4, 5, 6] is an approach aimed at improving classification or regression accuracy. More importantly, it addresses the challenge of interpreting predictions generated by deep learning models [7, 8, 9]. Concepts represent semantic descriptions of input images and can be viewed as high-level, human-understandable attributes or abstractions [1, 10]. In most cases, concept labels are annotated by humans or domain experts [11]. Unlike conventional black-box models, which establish a direct relationship between input data and predictions, CBL models first predict concept values from the input data and then use these predicted concepts to determine target class labels [12].\nOne of the key architectures within the CBL framework is the concept bottleneck model (CBM), introduced by Koh et al. [12]. This model consists of two predictors: a concept predictor and a class predictor. The concept predictor explicitly generates concept labels from images, while the class predictor determines the final label based on these concept predictions. The concept predictor is typically implemented using a convolutional neural network (CNN), which extracts a feature vector encoding the concepts from the input image. The class predictor, often implemented as a downstream layer (usually a linear fully-connected layer), predicts the final class using only the predicted concepts. This downstream layer serves as a tool for interpreting the final class prediction in terms of the underlying concepts [13, 14]. Additionally, the CBM architecture allows for interactive intervention during the interpretation and prediction process [13]. The CBM and its numerous modifications, such as [15, 16, 17, 18, 19, 20], are considered some of the most notable representatives of CBL models.\nAn important type of data that arises in many fields, including medicine, reliability, safety, and economics, is event time data. Event time observations can be approached within the framework of conventional regression problems. However, unlike standard observations, some events may remain unobserved because they occur after a fixed time point. Such data is referred to as censored observations. Survival analysis [21] is a framework designed to handle two types of event time data: censored (where the event of interest is not observed) and uncensored (where the event of interest is observed). Unlike many traditional machine learning models, the predictions of survival models are typically expressed as probabilistic functions of time. For example, the survival function represents the probability of an event not occurring up to a predefined time point.\nIt is worth noting that Forest et al. [22] introduced a concept bottleneck model (CBM) for predicting the remaining useful life. In their work, concepts are represented by different degradation modes associated with the remaining useful life. However, this approach remains within the standard regression framework of CBMs and does not address censored data or incorporate survival analysis. To the best of our knowledge, no existing method combines concept-based learning (CBL) with survival analysis. Given"}, {"title": "the importance of tasks involving interpretable predictions and improved prediction accuracy, we propose a novel approach that integrates survival analysis into the CBM framework. This model, called SurvCBM (Survival Concept-based Bottleneck Model), is designed to address survival analysis tasks.", "content": "The first idea underlying the proposed approach is to implement the second predictor in the CBM as the Cox proportional hazards model [23], where concepts serve as covariates with a linear relationship between them. In this setup, the coefficients of the concept linear combination in the Cox model, along with the covariates, can be interpreted as measures of the concepts' impact on the predictions. These predictions are expressed in terms of survival functions or other probabilistic measures within the survival analysis framework [24]. Alternatively, the second predictor can be implemented using the Beran estimator [25], which also provides a means to interpret predictions [26]. When using the Beran estimator, we can adopt an approach within the framework of example-based explanations. This approach involves selecting several instances from the training set that are closest to the explainable instance, based on the proximity of their predicted survival functions as measured by a distance metric. The importance of concepts in the prediction is then determined by the number of matching concepts between the closest instances and the explainable instance. The more concepts that coincide, the greater their significance in the prediction. This explanation approach is universal and can be applied to many survival models integrated into SurvCBM for predicting the probability distributions of event times.\nThe second idea behind the proposed approach involves two distinct architectures for CBL models. The primary architecture is a CBM with a bottleneck layer consisting of concept logits, followed by either the Beran or Cox models. The secondary architecture, introduced for comparison purposes, also employs the Beran or Cox models but uses concepts as regularization to enhance prediction accuracy. This architecture is referred to as SurvRCM (Survival Regularized Concept-based Model). Both neural network architectures are trained in an end-to-end manner.\nOur contributions can be summarized as follows:\n1. New concept-based survival models, SurvCBM and SurvRCM, are proposed for dealing with censored data in the framework of survival analysis. These models not only improve the accuracy of predictions but also provide a tool for interpreting these predictions in terms of concepts. The models are based on the Cox proportional hazards model and the Beran estimator.\n2. Two approaches for interpreting the predicted survival functions are considered, depending on the survival model used in SurvCBM or SurvRCM. The first approach is based on the assumption of a linear relationship between covariates in the Cox model. Using the trained regression coefficients and the logits of the concepts corresponding to the explainable instance, the importance of each concept is"}, {"title": "3. The proposed models are compared with each other and with a survival model that does not incorporate concepts.", "content": "4. Various numerical experiments are conducted to compare the proposed concept-based survival models under different conditions. The results demonstrate that the first architecture, SurvCBM, outperforms the other models. Moreover, they demonstrate the importance and advantage of applying concept information. The corresponding codes implementing the proposed models are publicly available at: https://github.com/NTAILab/SurvCBM.\nThe paper is organized as follows. Related work considering the existing explanation methods can be found in Section 2. A short description of basic concepts of survival analysis, including the Cox model and the Beran estimator, as well as concept-based learning is given in Section 3. A general idea of the concept-based survival models is provided in Section 4. Numerical experiments comparing different architectures of the survival models are given in Section 5. Approaches to interpreting predictions of concept-based survival models are studied in Section 6. Concluding remarks are provided in Section 7."}, {"title": "2 Related work", "content": "Concept-based learning models. Many CBL models were proposed in recent years [2, 27, 6] in order to improve the classification and regression performance of machine learning models and to interpret their predictions in terms of the high-level concepts. A large part of the models are CBMs [12] which have attracted special attention due to a number of remarkable properties. In particular, we point out stochastic CBMs [19], interactive CBMs [13], editable CBMs [11], semi-supervised CBMs [28], probabilistic CBMs [15], label-free CBMs [29], CBMs without predefined concepts, [30] [31], post-hoc CBMs [32], incremental residual CBMs [33], concept bottleneck generative models [16], any CBMs [34]. concept complement bottleneck models [30]. The above CBMs are a small part of various CBL models proposed in literature.\nConcept-based models, their advantages and disadvantages are considered in the survey papers [1, 10, 35, 36, 37]."}, {"title": "Survival analysis in machine learning.", "content": "Many survival machine learning models have been developed [38] due to their importance in several application areas, for example, in medicine, safety, reliability, economics. Detailed reviews of many survival models can be found in [38, 39, 40]. Deep survival machine learning models were reviewed in [41]. A practical introduction to survival analysis was provided by Emmert-Streib and Dehmer in [42].\nA large part of survival models can be regarded as extensions of conventional machine learning models under condition of censored data. For example, several survival models are based on applying neural networks and deep learning [41, 43, 44, 45, 46, 47, 48, 49, 50], several survival models are based on the transformer architectures [51, 52, 53, 54, 55, 56, 57], attention-based deep survival models were proposed in [58, 59]. A part of models is based on extending the random forest [60, 61]. Convolutional neural networks also used in survival analysis [62]. Many machine learning survival models extend the Cox model [23]. They mainly relax or modify the linear relationship assumption used in the Cox model [63, 64].\nDespite the intensive development of survival models, their application to concept-based learning is currently not reflected in the literature. Therefore, this work can be considered as the first attempt to create a survival concept-based model."}, {"title": "3 Background", "content": ""}, {"title": "3.1 Formal problem statement of concept-based learning", "content": "In the conventional supervised concept-based model setting, a training set consists of triplets $(x_i, y_i, c_i), i = 1,...,n$, where $x_i \\in R^D$ or $x_i \\in R^{d_1 \\times d_2}$ is the input instance represented as a vector or a matrix; $y \\in Y = \\{1, ...,s\\}$ is the corresponding target defining s-class classification task; $c_i = (c_i^{(1)},...,c_i^{(m)}) \\in C$ is a vector of m high-level concepts which describe the i-th instance x with $c_i^{(j)} \\in \\{0, ..., k_j \u2013 1\\}$; $k_j$ is the number of the j-th concept values. Concepts are usually represented as a vector consisting of binary elements such that the j-th unit element corresponds to the case when the j-th concept is presented in the description of the i-th instance. The main task of CBL is to construct a machine learning models (a classifier or a regressor) $h : R^D \\rightarrow (C, Y)$ to predict concepts and the target for a new input instance x.\nAnother task is to explain the predicted target in terms of concepts. One of the approaches for solving this task is the CBM proposed by Koh et al. [12], which consists of two parts: the first part explicitly predicts concept labels from instances and implements the map $g : X \\rightarrow C$, the second part predicts the target class using only predicted concepts and implements the map $f: C \\rightarrow Y$. The vector of concepts c or their probabilities (logits) predicted by the first part can be viewed as a bottleneck. As a result, the target class y of a new input instance x is defined as $y = f(g(x))$ ."}, {"title": "3.2 Elements of survival analysis", "content": "Instances in survival analysis are represented by a triplet $(x_i, \\delta_i, T_i)$, where $x_i^T = (x_i^{(1)},...,x_i^{(d)}) \\in R^d$ is a vector of d features characterizing the i-th instance whose time to an event of interest is $T_i$; $\\delta_i$ is the censoring indicator taking the value 1 if the event of interest is observed at time $T_i$ (uncensored observation), and 0 if the event is not observed (censored observation) [21]. For censored observations, it is only known that the time to the event exceeds the duration of observation. Survival analysis aims to estimate the time to the event T for a new instance x by using the training set $A = \\{(x_i, \\delta_i, T_i), i = 1, ..., n\\}$.\nOne of the important elements of survival analysis is the survival function (SF) denoted as $S(t | x)$, which is the probability of surviving beyond time t, that is $S(t | x) = Pr\\{T > t | x\\}$. Another element is the cumulative hazard function (CHF) denoted as $H(t | x)$ an expressed through the SF as follows:\n$S(t | x) = exp(-H(t | x)) .\n$\nAnother element of survival analysis used for comparison of different survival models is the C-index [65]. It estimates the probability that event times of a pair of instances are correctly ranking. Let $I$ be a set of all pairs (i, j) satisfying conditions $\\delta_i = 1$ and $T_i < T_j$. The C-index is formally computed as [66, 38]:\n$C = \\frac{\\sum_{(i,j)\\in I} 1[T_i < T_j]}{\\sum_{(i,j)\\in I} 1} ,\n$\nwhere $\\hat{T}_i$ and $\\hat{T}_j$ are expected (predicted) event times obtained from the predicted SFs $S(t | x_i)$ and $S(t | x_j)$.\nA popular semi-parametric regression model for analysis of survival data is the Cox proportional hazards model that calculates the effects of observed covariates on the risk of an event occurring [23]. The model assumes that the log-risk of an event of interest is a linear combination of covariates or features. According to the Cox model, the CHF at time t given instance x is defined as:\n$H(t | x, b) = H_0(t) exp (b^Tx),\n$\nwhere $H_0(t)$ is the baseline CHF estimated by using the Nelson-Aalen estimator [21, 42]; $b^T = (b_1,..., b_m)$ is a vector of the model parameters in the form of the regression coefficients which can be found by maximizing the partial likelihood function for the dataset A.\nThe SF $S(t | x, b)$ in the framework of the Cox model is computed as\n$S_C(t | x, b) = (S_0(t))^{exp(b^Tx)}.\n$"}, {"title": "Here $S_0(t)$ is the baseline SF which is also determined by using the Nelson-Aalen estimator. It is important to note that functions $H_0(t)$ and $S_0(t)$ do not depend on x and b.", "content": "Another important model which is used below is the Beran estimator [25] which estimates the SF on the basis of the dataset A as follows:\n$S_B(t | x) = \\prod_{t_i \\leq t} \\left\\{ 1 - \\frac{\\alpha(x, x_i)}{\\sum_{j=1}^{i-1} \\alpha(x, x_i)} \\right\\}^{\\delta_i} ,\n$\nwhere time moments $t_1, ..., t_n$ are ordered; the weight $\\alpha(x, x_i)$ conforms with relevance of the i-th instance $x_i$ to the vector x and can be defined by using kernels $K(x, x_i)$ as\n$\\alpha(x, x_i) = \\frac{K(x, x_i)}{\\sum_{j=1}^n K(x, x_j)}\n$.\nIn particular, if to use the Gaussian kernel with the parameter $\\tau$, then weights $\\alpha(x, x_i)$ are of the form:\n$\\alpha(x,x_i) = softmax \\left(\\frac{||x - x_i||^2}{\\tau} \\right)\n$.\nThe Beran estimator is reduced to the Kaplan-Meier estimator [38] when all weights are identical, i.e., $\\alpha(x, x_i) = 1/n$ for all $i = 1, ..., n$."}, {"title": "4 Description of concept-based survival models", "content": ""}, {"title": "4.1 Concept-based survival problem statement", "content": "In the survival concept-based learning setting, a training set A is represented by a set of n quadruplets $(x_i, c_i, T_i, \\delta_i), i = 1, ..., n$, which combines concept-based and survival information considered above. Unlike most concept-based learning models, predictions in the survival CBL are SFs $S(t | x,c)$ or CHFs $H(t | x,c)$. For simplicity, we will consider only SFs because CHFs are expressed through the corresponding SFs. The concept-based survival analysis aims to estimate the SF $S(t | x) $ as well as the concept vector c for a new instance x by using the training set A. Other elements of survival analysis, for example, the mean time to event, can be calculated on the basis of the SF. If we denote the set of SFs as S, then our task is to construct machine learning models which implement the map $G : R^D \\rightarrow (C, S)$. Two models are proposed and compared in this work."}, {"title": "4.2 The first model: SurvCBM", "content": "The second proposed model is derived from the architecture of the CBM. Its mathematical formulation can be expressed as follows:\n$S(t | x, A) = F(G(x)),\n$\nwhere $S(t | x, A)$ is the SF obtained for a feature vector x using training set A; $F(c)$ is a survival model defined over the concept space; $G(x)$ is a neural network (or a set of neural networks) that predicts the vector of concepts for the feature vector x.\nThe architecture of the model is illustrated in Fig. 1. It can be seen from Fig. 1 that the input image x is processed by a set of m convolutional neural networks (CNN-1,...,CNN-m) where each network outputs a vector of concept logits $p_i = (p_{i,1}, ..., p_{i,k_i})$. Here, it is assumed that the i-th concept can take $k_i$ possible values. The use of CNNS is crucial for reducing the dimensionality of the images and enabling the training of Cox or Beran models on lower-dimensional vectors. In SurvCBM, the Cox model and the Beran estimator are trained on the concatenated logits\n$p = (p_1, \u2026, p_m) = (p_{i,1}, ..., p_{i,k_i}, i = 1,..., m) \\in R^M, M = \\sum_{i=1}^m k_i\n$.\nThis is a very important peculiarity of SurvCBM, as it achieves a balance between two objectives: accurately defining the concept values and computing the SF. Furthermore, this design ensures that the model remains interpretable, and the computed SF can be explained in terms of the underlying concepts.\nThe general form of the loss function is given by:\n$L = \\alpha L_{surv} + (1 - \\alpha) L_{CE},\n$\nwhere $\\alpha \\in (0,1)$ is a hyperparameter.\nThe term $L_{CE}$ corresponds to solving the concept classification task. For a single data point x with true concept values $(y_1, \u2026\u2026\u2026, y_m)$, it is defined as:\n$L_{CE} = -\\frac{1}{m} \\sum_{i=1}^m  \\log  \\frac{exp (p_{i,y_i})}{\\sum_{j=1}^{k_i} exp(p_{i,j})} \n$.\nThe term $L_{surv}$ corresponds to solving the survival analysis task. To implement We apply $L_{sury}$, we use the smoothed C-index function. For data points $x_1, ..., x_n$ with true event times $T_1, ..., T_n$, censoring indicators $\\delta_1, ..., \\delta_n$ and estimated expected event times $\\hat{T}_1, ..., \\hat{T}_n$, the loss function can be expressed as:\n$L_{surv} = \\frac{\\sum_{i, j: T_i<T_j} \\sigma(\\hat{T}_i - \\hat{T}_j) \\cdot \\delta_i}{\\sum_{i,j: T_i<T_j} \\delta_i} ,\n$"}, {"title": "where $\\sigma$ is the sigmoid function with a temperature parameter $\\omega$, which is treated as a hyperparameter.", "content": "The loss function $L_{ce}$ is averaged over a batch during the optimization process, while the smoothed C-index is computed over all instances in the batch.\nSurvCBM is trained in an end-to-end manner. It is worth noting that the use of a set of convolutional neural networks is one possible implementation of the model. Alternatively, a single network could be used to compute the vector p of logits. However, our numerical experiments have demonstrated that using a set of networks yields more robust results."}, {"title": "4.3 The second model: SurvRCM", "content": "The SurvRCM does not follow the architecture of the CBM because it lacks a bottleneck layer. Nevertheless, we consider SurvRCM as a simplified variant of concept-based"}, {"title": "5 Numerical Experiments", "content": "Two types of numerical experiments are conducted. The first type evaluates the performance of the proposed models, while the second type demonstrates their explanation mechanisms.\nTo assess the model performance, we compare the proposed models with each other and with a baseline survival model. Due to the absence of known survival CBMs, we compare the proposed models with a baseline model that solves the same survival analysis task without incorporating concept data. This comparison allows us to study the importance of correctly selected concepts for solving survival tasks when the input instances are images. In the baseline model, denoted as SurvBase, a CNN generates an embedding z, which is used to train either the Cox model or the Beran estimator. The model is trained in an end-to-end manner, and its loss function consists solely of the term $L_{surv}$, as defined in (12). The architecture of SurvBase is illustrated in Fig. 3. It can seen from Fig. 3 that an image x is fed into the CNN to produce an embedding $z \\in R^d$. The Cox or Beran model is then applied to the set of embeddings to generate the SF S(t | x).\nThe following model components and hyperparameters are used: the temperature $\\omega$ of the sigmoid function in the smoothed C-index; parameter $\\alpha$ in the loss function;"}, {"title": "the number of instances in the background set of the Beran estimator; the parameter $\\tau$ of the Gaussian kernel in the Beran estimator; the number of tasks, generated on each training epoch; the number of epochs; the optimizer and its parameters, including learning rate and weight decay; the size d of the embedding z. Different values of the hyperparameters are tested, choosing those leading to the best results.", "content": "Experiments are conducted in two directions: comparison of the model performance depending on the number of training instances and depending on the number of un-censored data. The performance measures for comparison of the models are C-index, characterizing the survival model, and F1-measure, characterizing the concept classification accuracy. At that, values of the F1-measures indicated on graphs below are averaged over all concepts for brevity.\nThe cross-validation in all experiments is performed with 100 repetitions. Intervals on the graphs are the average value of these repetitions and the standard deviation."}, {"title": "5.1 Datasets", "content": "We consider the following composite images constructed from the datasets MNIST and CIFAR-10 with synthetically generated concepts and target event times:\n1. MNIST. First, we apply MNIST dataset which is a commonly used large dataset of 28 \u00d7 28 pixel handwritten digit images [67]. It has a training set of 60,000 instances, and a test set of 10,000 instances. The dataset is available at http://yann.lecun.com/exdb/mnist/. To construct the concept-based MNIST dataset, images from the original MNIST dataset are combined into sets of four different digits. Each digit in its position defines a separate concept, where the concept's value corresponds to the digit itself. As a result, each data point in the dataset is described by four concepts, each taking one of 10 possible values. The event time is generated according to the Weibull distribution and is computed as follows:\n$T= \\left( \\frac{In(u)}{\\lambda exp(b^Tc)} \\right)^{\\frac{1}{\\nu}} ,\n$\nwhere $u \\sim U(0, 1)$ is the uniform random variable, c is the vector of concepts; $\\nu$ and $\\lambda$ are parameters of the Weibull distribution.\nThe generation parameters are $b^T = (0.5, 1.5, -1, 0.001)$, $\\nu = 2$, $\\lambda = 10^{-4}$. Values of the censoring indicator $\\delta$ are generated in accordance with the Bernoulli distribution. Examples of the generated instances as well as vectors of concepts c are shown in Fig. 4.\n2. MNIST-sin. The second dataset is identical to the first one, but the event times are generated using a probability distribution different from the Weibull distribu-"}, {"title": "tion. The event times are generated according to the following formula:", "content": "$T= \\left( \\frac{In(u)}{\\lambda (sin (b^Tc) + 1.001)} \\right)^{\\frac{1}{\\nu}} ,\n$\nwhere the generation parameters are $b^T = (0.5, 1.5, -1, 0.001)$, $\\nu = 4$, $\\lambda = 0.01$.\n3. CIFAR-10. The third dataset consists of four images from the well-knwon CIFAR-10 dataset [68], which contains 60000 color images 32 \u00d7 32 drawn from 10 categories (50,000 training and 10,000 test images each). The dataset is available at https://www.cs.toronto.edu/~kriz/cifar.html. The following concepts vector of instances consisting of four CIFAR-10 images is used: (1 - number of animals, 2 - number of vehicles, 3 - number of flying objects, 4 - is there a cat on the picture). Concepts 1, 2, 3 take five values, the last one is binary. The event time is generated according to the Weibull distribution in the same way as for the MNIST dataset. The generation parameters are b = (-0.7, 1.5, \u22122, 5), $\\nu = 2$, $\\lambda = 0.01$. Examples of the dataset can be found in Fig. 5. As an example, it can be seen from the first instance in Fig. 5 that it contains three animals, one vehicle, no flying objects, and there is a cat. This implies that the corresponding vector of concepts is (3, 1, 0, 1)."}, {"title": "5.2 MNIST dataset", "content": "First, we conduct numerical experiments using a synthetic dataset derived from the MNIST dataset. All metrics presented in the graphs are computed on the test set, which comprises 40% of the instances from the dataset. For experiments involving varying sample sizes, we set the proportion of uncensored instances to 33%.\nFirst, we study how the sample size n impacts the accuracy (the C-index) of the models. Fig. 6 illustrates how the C-index of the compared models depends on the number of training instances n, when the Beran estimator (the left graph) and the Cox model (the right graph) are used as the survival models. It can be seen from the both graphs in Fig. 6 that SurvCBM outperforms the other models. It is interesting to point out that the most impressive results are demonstrated when the Cox model is used. This is due to the relatively simple feature space and the Weibull distribution used for generating the event times. It is also interesting to note that SurvRCM provides worse results even in comparison with SurvBase. We can also observe that the C-index of SurvBase is much smaller than that of SurvCBM. This implies that the concept information may significantly impove the model performance.\nIf the C-index characterizes the performance of models as survival models, then F1-measure characterizes the concept classification accuracy. Fig. 7 shows how the F1-measures of the models depend on the number of training instances under the same"}, {"title": "conditions. We consider only two models because SurvBase does not deal with concepts. It can be seen from Fig. 7 that SurvCBM significanlty outperforms the Mixture model. Moreover, results for SurvCBM for different survival models (the Beran and Cox models) are almost the same. It is important to note that the variance of SurvCBM results is extremely small compared with the variance of SurvRCM results. This fact illustrates a high robustness of SurvCBM.", "content": "The next question is how the model performance depends on the proportion of un-censored data p. Figs. 8 and 9 address this question. In particular, Fig. 8 shows that the C-index of SurvCBM significantly exceeds that of other models when the Cox model is used as the survival model. This can be explained by the fact that the Weibull distribution is used for generating the event times. One can also see from Fig. 8 that SurvBase provides unsatisfactory results. This fact again demonstrates that concept information may significantly improve the model performance. Fig. 9 is similar to Fig. 7. The F1-measure of SurvCBM weakly depends on the survival models, whereas SurvRCM strongly depends on both the survival models and the parameter p."}, {"title": "5.2.1 MNIST-sin", "content": "Let us study the model performance when the models are trained on the MNIST-sin dataset. In this dataset, the Weibull distribution of the event times is violated, and the proportionality of risk is also violated. The consequences of these violation are clearly seen from Fig. 10 where the Beran estimator demonstrates superior results compared"}, {"title": "to the Cox model when SurvCBM is used. This is because the Beran model effectively handles complex data structures by taking into account relationships between instances.", "content": "It is interesting to note that all models have similar C-indices when the Cox model is used. Moreover, increasing the number of training instances does not significantly improve the performance of any of the models.\nFig. 11 shows how the F1-measures of the models depend on the number of training instances. It can be seen from Fig. 11 that SurvCBM outperforms SurvRCM for both the Beran estimator and the Cox model. We again observe that the F1-measures for both models, using either the Beran or Cox models, are almost the same.\nResults depicted in Figs. 12 and 13 are consistent with the similar results shown in Figs. 10 and 11, respectively. We again observe that the C-index increases with the number of training instances. However, this increase is observed only for SurvCBM when the Beran estimator is used. At the same time, there is almost no improvement in the concept classification performance (the F1-measure) as the proportion of uncensored observations increases (see Fig. 13)."}, {"title": "5.2.2 CIFAR-10", "content": "Similar numerical experiments are performed with the CIFAR-10 dataset. Figs. 14 and 15 demonstrate that the more complex data structure and principles of concept formation lead to worse results. The complex structure of the dataset does not allow satisfactory results to be obtained using the Cox model, even when the Weibull distribution is used"}, {"title": "6 SurvCBM as an interpretation tool", "content": "Depending on the survival model used in SurvCBM, we consider two methods for interpreting predictions which are in the form of SFs.\nThe first method is applied when the Beran estimator is used in SurvCBM. The goal of this method is to explain which concepts significantly impact the predicted SF S(t | x) for a new instance x. A key element of this interpretation is the predicted SF itself. However, determining a direct relationship between concepts and the SF is challenging. To address this, the following approach is proposed for interpreting the predicted SF.\nThe model selects instances from the training set that are closest to the instance being explained in the concept space. Closeness is defined as the distance between the predicted SF of the explainable instance and the SFs of other instances from the"}, {"title": "training data. The number of concepts that coincide with those of the instance being explained, among the nearest instances, determines the degree of importance of these concepts in the prediction corresponding to the eplainable example. The more concepts that coincide, the greater their importance. This interpretation can be viewed within the framework of example-based explanations [69].", "content": "For the MNIST dataset", "follows": "b_1 = (0.5", "as": "b_1 = (0.001, ..., 0.001) \\in R^5$"}]}