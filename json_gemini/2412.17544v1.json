{"title": "Retention Score: Quantifying Jailbreak Risks for Vision Language Models", "authors": ["Zaitang Li", "Pin-Yu Chen", "Tsung-Yi Ho"], "abstract": "The emergence of Vision-Language Models (VLMs) is significant advancement in integrating computer vision with Large Language Models (LLMs) to enhance multi-modal machine learning capabilities. However, this progress has made VLMs vulnerable to advanced adversarial attacks, raising concerns about reliability. Objective of this paper is to assess resilience of VLMs against jailbreak attacks that can compromise model safety compliance and result in harmful outputs. To evaluate VLM's ability to maintain robustness against adversarial input perturbations, we propose novel metric called Retention Score. Retention Score is multi-modal evaluation metric that includes Retention-I and Retention-T scores for quantifying jailbreak risks in visual and textual components of VLMs. Our process involves generating synthetic image-text pairs using conditional diffusion model. These pairs are then predicted for toxicity score by VLM alongside toxicity judgment classifier. By calculating margin in toxicity scores, we can quantify robustness of VLM in attack-agnostic manner. Our work has four main contributions. First, we prove that Retention Score can serve as certified robustness metric. Second, we demonstrate that most VLMs with visual components are less robust against jailbreak attacks than corresponding plain VLMs. Additionally, we evaluate black-box VLM APIs and find that security settings in Google Gemini significantly affect score and robustness. Moreover, robustness of GPT4V is similar to medium settings of Gemini. Finally, our approach offers time-efficient alternative to existing adversarial attack methods and provides consistent model robustness rankings when evaluated on VLMs including MiniGPT-4, InstructBLIP, and LLaVA.", "sections": [{"title": "Introduction", "content": "Recent advances have led to the widespread use of Vision Language Models (VLMs) capable of handling a range of tasks. There has been great interest in incorporating vision modules into Large Language Models (LLMs), consisting of GPT-4V (OpenAI 2023) and Gemini Vision (Team et al. 2023). Although the introduction of visual input to Large Language Models (LLMs) has improved the ability of the language model to understand multi-modal knowledge, it also exposes an additional dimension of the visual input domain that expands the threat landscape for adversarial attacks. This expands the attack vectors available to adversaries, who now have two domains to exploit: the high-dimensional visual space and the discrete textual space. The shift from purely textual to multi-modal text-visual interaction significantly increases the possible ways for adversarial attacks to occur.\nTo help language models avoid generating harmful responses, prior work such as model alignment ensures that LLMs are aligned with their developers' intentions (Bai et al. 2022; Ouyang et al. 2022), thus ensuring that harmful content is not generated in response to prompts. However, there is always the possibility for users to craft adversarial perturbations from both image and text avenues to undermine alignment and induce malicious behavior. Previous research has shown the ease with which VLMs can be tricked into producing malicious content through image (Qi et al. 2023a; Carlini et al. 2023) or text strategies (Zou et al. 2023; Liu et al. 2023b). Accordingly, it is important to address concerns about the toxicity potential of VLMs. In line with Carlini's interpretation (Carlini et al. 2023), we define toxicity as the susceptibility (lack of robustness) of models to be goaded into emitting toxic output (i.e., jailbreak risks).\nWhile most of the works focus on guiding harmful responses (i.e., jailbreak) or preventing VLMs from improper content, we aim to provide a qualified margin-based robustness evaluation metric for each VLM. Previous studies on adversarial robustness in computer vision (Carlini et al. 2019) have already concluded that robustness evaluation based on adversarial attacks may not be persistent because stronger attacks may exist and are yet to be discovered. On the other hand, certified robustness guarantees that no attacks can break the certificate. Our proposed jailbreak risk evaluation of VLMs falls into the category of margin-based certificates.\nThe task of assessing jailbreak risks of VLMs is full of challenges. (i) First, VLMs are trained on large, web-scale datasets, which complicates the feasibility of performing robust accuracy evaluations on test sets. (ii) Second, the discrete nature of textual data defies the establishment of a secure boundary in the context of text attacks. (iii) Third, the computational and monetary costs associated with evaluating adversarial robustness via optimization-based jailbreak attacks are impractical due to their cost and time consumption.\nWe address these challenges by introducing Retention Score, a novel conditional robustness certificate to evaluate the toxicity resilience of VLMs. The Retention Score, with its subcategories Retention-I and Retention-T, provides"}, {"title": "Background and Related Works", "content": "The advent of LLMs such as GPT-3 (Brown et al. 2020) has revolutionized artificial intelligence, enabling context-aware learning and chain-of-thought reasoning by exploiting abundant web data and numerous model parameters. VLMs represent the convergence of computer vision and natural language processing, combining visual perception with linguistic expression. Examples such as GPT-4V (OpenAI 2023) and Google Gemini (Team et al. 2023) use both visual and textual information. In addition, open-source VLMs such as MiniGPT-4 (Zhu et al. 2023), InstructBLIP (Dai et al. 2023), and LLaVA (Liu et al. 2023a) enhance multi-modal integration by generating text in response to visual and textual cues."}, {"title": "Alignment of Vision-Language Models", "content": "In the quest for responsible AI, ensuring alignment with human values such as helpfulness and harmlessness presents a significant challenge (Askell et al. 2021). When a language model fails to align with the user's intent, it can be attributed to two main factors: (i) insufficient question-answer pairs in the training dataset, and (ii) language models, despite their ability to predict based on Internet data, may inadvertently reflect biases and toxicities present in that data. Alignment methods aim to recalibrate language models to ensure that their outputs meet ethical guidelines and societal expectations (Wei et al. 2022; Ouyang et al. 2022). Techniques such as reinforcement learning with human feedback (RLHF) and instruction tuning are used to fine-tune these models and teach them to avoid generating biased content. RLHF (Ouyang et al. 2022) fine-tunes the model based on generations preferred by human annotators, while instruction tuning (Wei et al. 2022) refines the model to better understand and perform tasks described by instructions."}, {"title": "Adversarial Examples for Jailbreaking Aligned VLMs and LLMs", "content": "The field of adversarial machine learning studies inputs designed to fool AI models, subtle to the human eye yet powerful enough to disrupt algorithmic predictions. In textual contexts, adversaries craft prompts that trick LLMs into producing dangerous outputs, thereby \u201cjailbreaking\" the boundaries of their biases. In the image domain, (Qi et al. 2023a) discovers that a single visual adversary example can universally jailbreak an aligned VLM, forcing it to obey malicious instructions and generate malicious content beyond the narrow scope of a \"few-shot\" derogatory corpus used to optimize the adversary example. (Carlini et al. 2023) developed a fully differentiable version of the VLM extending from raw image pixels to the output logits generated by the language model component. Through this differentiable implementation, typical optimization strategies associated with teacher forcing are used to achieve the adversarial example generation process. Unlike the white-box settings of the former work, (Zhao et al. 2023) introduces a technique where adversaries have only black-box access to VLMs. By targeting CLIP (Sun et al. 2023) and BLIP (Li et al. 2022) as surrogate models, (Zhao et al. 2023) achieves transfer attacks on other VLMs. In the text domain, existing work such as GCG attacks (Zou et al. 2023) and AutoDAN (Liu et al. 2023b) emerge as breakthroughs in this area. They both show great ability in transferability settings across models. GCG attacks generate adversarial suffixes, while AutoDAN uses sophisticated genetic algorithms to generate jailbreaking prefixes."}, {"title": "Attack-agnostic Robustness Certificate", "content": "Previous evaluations of neural network classifiers, such as the CLEVER Score (Weng et al. 2018), have provided assessments based on a closed form of certified local radius involving the maximum local Lipschitz constant around a neighborhood of a data sample x. However, the robustness guarantee of VLMs remains unexplored. The GREAT Score (Li, Chen, and Ho 2023) derives a global statistic representative of distribution-wise robustness to adversarial perturbation for image classification tasks. While the GREAT Score evaluates global robustness, our method evaluates conditional robustness for given images and texts."}, {"title": "Retention Score: Methodology and Algorithms", "content": "Our methodology defines notational preliminaries for characterizing the robustness of VLMs against adversarial visual and text attacks. We begin by defining \"jailbreaking\" for VLMs in Section. We then propose the use of a generative model to obtain the Retention Score, which includes both image-focused Retention-I and text-centric Retention-T in Section. Then We briefly claim the certification for Retention Score in Section. In Section, we explain algorithmic mechanisms and computational complexities. To ensure clarity, we systematically enlist pertinent notations and their corresponding definitions in Appendix ."}, {"title": "Formalizing Image-Text Jailbreaking", "content": "To explain the process of jailbreaking in the context of VLMs, we introduce a model $\\mathcal{V} : \\mathbb{R}^d \\times \\mathcal{A} \\rightarrow \\mathcal{A}$, which accepts visual data of dimension $d$ and linguistic prompts denoted by $\\mathcal{A}$. An image-text pair is represented by $(I,T)$, where $I$ is a visual sample and $T$ is the corresponding textual prompt.\nFor the assessment of toxicity in the generated outputs, we define a judgment function $\\mathcal{J} : \\mathcal{A} \\rightarrow \\Pi^2$ that assigns probabilities to the potential for toxicity within responses, with $\\Pi^2$ symbolizing the two-dimensional probability simplex representing toxic and non-toxic probabilities. Let the notations 't' and 'nt' stand for toxic and non-toxic categories, respectively. We then characterize the complete VLM with an integrated judgment classifier, $\\mathcal{M} : \\mathbb{R}^d \\times \\mathcal{A} \\rightarrow \\Pi^2$. This mapping embodies the transformation from the VLM's initial response $\\mathcal{V}(I, T)$ to the evaluated judgment $\\mathcal{J}(\\mathcal{V}(I,T))$ which we denote concisely as $\\mathcal{M}$.\nPrior to discussing robustness, it is crucial to establish a continuous space for both images and text. Images inherently exist in a continuous space, whereas text, due to its discrete nature, necessitates an additional definition to facilitate its embedding into a continuous semantic space. We define a semantic encoder $s$ that maps token sequences $Y = [Y_1, Y_2, ..., Y_n]$, with each $y_i$ belonging to a vocabulary $\\mathcal{V}$, into a $k$-dimensional space such that $s : \\mathcal{A} \\rightarrow \\mathbb{R}^k$. Here, $\\mathcal{A}$ includes all possible token sequences derived from $\\mathcal{V}$, and $\\mathbb{R}^k$ represents the continuous vector space. Additionally, we define a semantic decoder $\\psi : \\mathbb{R}^k \\rightarrow \\mathcal{A}$, which maps the continuous representations back to the discrete token sequences.\nWith continuous spaces for image and text established, we are now in a position to define the minimum perturbation required to alter the toxicity assessment in each modality.\nFor an image-text pair $(I, T)$, the classification of a non-toxic pair depends on a non-toxic score of $\\mathcal{M}_{nt}(I,T) \\geq 0.5$. We define an adversarial jailbreaking instance as a perturbed image or text that can transition this non-toxic pair to toxic. In terms of image perturbations, we denote $\\mathcal{A}_{min}(I, T)$ as the smallest perturbation that, among all adversarial jailbreaking candidates, reduces the non-toxic score of the perturbed pair $(I,T)$ to the threshold of 0.5 or below. Formally, it is expressed as: $\\mathcal{A}_{min} (I,T) = \\arg \\min_{\\Delta} \\{||\\Delta||_p : \\mathcal{M}_{nt}(I + \\Delta,T) \\leq 0.5\\}$ where $||\\Delta||_p$ denotes the $l_p$-norm of the perturbation $\\Delta$, which is a measure of the magnitude of the perturbation according to the chosen $p$-norm.\nThe search for the minimum text perturbation requires us to move through the semantic space. Employing a semantic encoder $s$, we convert a textual prompt $T$ into this space. The smallest perturbation $\\mathcal{A}_{min} (T)$ that results in a borderline non-toxic score is formalized as: $T_{in} (I,T) = \\arg \\min_{\\Delta} \\{||\\Delta||_p : \\mathcal{M}_{nt}(I,\\psi(s(T) + \\Delta)) \\leq 0.5\\}$ where $\\Delta$ symbolizes a perturbation in the semantic space and $s(T)+\\Delta$ the perturbed representation."}, {"title": "Establishing the Retention Score Framework", "content": "Revisiting concepts introduced in Section, minimal perturbations for Image-Text pair in context of VLMs were established. We proposed that greater values of $\\mathcal{A}_{min} (I,T)$ and $\\mathcal{A}_{min} (I, T)$ correlate with enhanced local robustness of model $\\mathcal{M}$ for pair $(I,T)$. Consequently, estimating lower bounds for these minimal perturbations provides measure of VLMs' robustness. To quantify robustness, we introduce Retention Score, denoted as $\\mathcal{R} : \\mathbb{R}^d \\times \\mathcal{A} \\rightarrow \\mathbb{R}$, which aims to provide assessment of VLM resilience against input perturbations. Higher Retention Scores signify model's inherent robustness, indicative of safeguards against adversarial toxicity manipulation. Retention Score is multimodal measure capable of assessing conditional robustness of VLMs across"}, {"title": "Retention-Image Score (Retention-I)", "content": "Building on the foundation laid out previously, we dedicate this subsection to formulating the Retention-I Score. This metric serves as a robustness certificate and is designed to evaluate a model's ability to resist adversarial image perturbations. The Retention-I Score is developed to evaluate robustness given a set of text prompts and a specific image $I$, which we approach by initially defining a local pair score estimate function for each $(\\mathcal{I},T)$ and subsequently deriving a conditional robustness score for the given image $I$ and a collection of text prompts, denoted as $X = \\{T_1, T_2, ...,T_m\\}$.\nThe local score function is predicated on the VLM with an integrated judgment mechanism $\\mathcal{M}$ and a specified textual prompt $T$. We incorporate a continuous diffusion-based image generation model $G_1(z|I)$, which, given a zero-mean isotropic Gaussian-distributed input $z \\sim \\mathcal{N}(0, \\mathcal{I})$, synthesizes a semantically similar image to $I$. The local score function $g_I$ evaluates the non-toxicity of the generated image associated with the given prompt $T$ and is defined by:\n$g_I(\\mathcal{M},G_1(z|I), T) = \\frac{\\pi}{2} \\cdot \\{M_{nt}(G_1(z|I),T) - M_{t}(G_1(z|I),T)\\}$.\nWith this local score estimate, the conditional robustness for images, representing the mean robustness across all image-text pairs, can be approximated using a finite sample set $G_1(z_i|I)_{i=1}^n$ produced by the generator $G_1(\\cdot|I)$ applied to each text prompt. The Retention-I Score is formalized as:\n$R_I(\\mathcal{M}, I, X) = \\frac{1}{m\\cdot n} \\sum_{j=1}^m \\sum_{i=1}^n g_I(\\mathcal{M},G_1(z_i|I),T_j))$."}, {"title": "Retention-Text Score (Retention-T)", "content": "In a manner similar to Retention-I, the Retention Text Score (Retention-T) is introduced as a measure of VLM vulnerability to textual adversarial endeavors. Given the high success rate of attacks targeting single images, we direct our evaluation towards a fixed image $I$ and a set of prompts. The model $G_T(z|T)$ refers to a text generator founded on paraphrasing diffusion techniques, conditioned on a text prompt $T$ and Gaussian-distributed input $z$.\nWe define the local score function $g_T$, which assesses the non-toxicity of a given image $I$ associated with the paraphrased text prompt $T$, as:\n$g_T(\\mathcal{M}, I, s(G_T(z|T))) = \\frac{\\pi}{2} \\cdot \\{M_{nt}(I,(\\psi(s(G_T(z|T))))) - M_{t}(I,\\psi(s(G_T(z|T))))\\}$.\nHere, $s$ and $\\psi$ represents a semantic encoder and decoder, such as BART (Lewis et al. 2019), that translates discrete textual information into a continuous vectorial representation and vice versa.\nSimilar to Retention-I, the Retention-T Score can be estimated from a set of generated samples $G_T(z_i|T)_{i=1}^n$ corresponding to each prompt. The Retention-T Score, $R_T$, is computed as the mean of the local scores $g_T$ across all paraphrased prompts in conjunction with the given image $I$:\n$R_T(\\mathcal{M}, I, X) = \\frac{1}{m\\cdot n} \\sum_{j=1}^m \\sum_{i=1}^n g_T(\\mathcal{M}, I, (s(G_T(z_i|T_j)))))."}, {"title": "Establishing the Robustness Certification for Retention Scores", "content": "Consider $\\mathcal{M}$ as a VLM equipped with a judgment classifier. We assert that the previously score function constitutes a robustness certification. This claim is strengthened by the theorem below.\nTheorem 1 (Robustness Certification via Retention Score). For a given image I and a collection of text prompts X, consider Retention Image Score $R_I$ defined in (2) and Retention Text Score $R_T$ defined in (4). For each $T \\in X$ satisfying the condition $M_{nt}(I,T) \\geq M_{t}(I,T)$, indicating a non-toxic classification of the original prompt. As the number of generated samples $n$ from a generative model $G(\\cdot)$ approaches infinity, the following statements hold almost surely:\n(I) Given any perturbation $\\delta_1$ within $R_I$ range applied to the image $I$, the worst-case non-toxic score maintains a lower bound as follows:\n$\\min_{\\|\\delta_1\\|_2<R_1} M_{nt}(I+ \\delta_1,T) \\geq 0.5$.\n(II) Similarly, for perturbations within the semantic space of T, the worst-case non-toxic score is bounded by:\n$\\min_{\\|\\delta_T\\|_2<R_T} M_{nt}(I, (s(T) + \\delta_T)) \\geq 0.5$.\nThe theorem implies that Retention Scores $R_I$ and $R_T$ act as thresholds beyond which VLM maintains its non-toxic output for respective perturbations, thus certifying robustness of $\\mathcal{M}$ with respect to image and text modifications. The proof delineating details and assumptions underpinning this theorem is elucidated in Appendix .\nThe theorem provides a guarantee that for perturbations whose magnitudes are within the radius defined by the respective Retention Scores, the VLM can be considered provably robust against potential toxicity-inducing alterations. This robustness certificate serves as a crucial asset in affirming the defensibility of VLMs when encountering adversarial perturbations, thereby reinforcing trust in their deployment in sensitive applications."}, {"title": "Computation and Complexity for Retention-I and Retention-T", "content": "The detailed descriptions of the algorithms for estimating Retention Score are in Algorithm 1 and Algorithm 2 in Appendix . Consider a set of evaluated text prompts, represented as $X = \\{T_1, T_2, ..., T_m\\}$ and a given image $I$. Both Retention-I and Retention-T must conditionally generate on samples $N_s$ times total and take forward pass into VLM to aggregate resulting confidence scores using model $\\mathcal{M}$. The remark governing computational complexity states that the total computational cost is linear with respect to the number of samples $m$ in $X$ and times of generation $N_s$.\nRemark 1. The time complexity $T(R)$ of computing the Retention Score for a model $M$ with respect to a sample set $S$ and generator $G(\\cdot)$ is given by:\n$T(R) = 0 (m \\times N_s \\times T(M) + N_s \\times T(G(\\cdot))$ where $T(M)$ is the time complexity of toxicity inference and $T(G(\\cdot))$ is the time complexity of sample-generation."}, {"title": "Performance Evaluation", "content": "Models. We assess the robustness of various Vision-Language Models (VLMs), including MiniGPT-4 (Zhu et al. 2023), LLaVA (Liu et al. 2023a), InstructBLIP (Dai et al. 2023), and their base LLMs in a 13B version. Our evaluations also encompass the VLM APIs for GPT-4V (OpenAI 2023) and Gemini Pro Vision (Team et al. 2023).\nMiniGPT-4 integrates vision components from BLIP-2 (Li et al. 2023) with ViT-G/14 from EVA-CLIP (Sun et al. 2023; Fang et al. 2023) and a Q-Former network for encoding images into Vicuna (Chiang et al. 2023) LLM's text embedding space. A projection layer aligns the visual features with the Vicuna model. In the absence of visual input, MiniGPT-4 is equivalent to Vicuna-v0-13B LLM. This model shares ChatGPT's instruction tuning and safety guardrails, ensuring consistency in generation and adherence to ethical guidelines.\nLLaVA leverages a CLIP VIT-L/14 model with a linear layer to encode visual features into Vicuna's embedding space. Unlike MiniGPT-4, the Vicuna component of LLaVA is fine-tuned, further refining its response accuracy. Originating from LLaMA-2-13B-Chat, LLaVA exhibits a sophisticated alignment due to its hybrid tuning involving instructional data and reinforcement learning from human feedback. This model sets a new benchmark for aligned VLMs.\nInstructBLIP is based on the Vicuna-v1.1-13B and enhances BLIP-2 by incorporating instruction-directed visual feature extraction. The Q-former module integrates instruction text tokens with image queries, utilizing self-attention layers to prioritize relevant feature extraction. The model employs a ViT-based visual encoder from CLIP, underscoring task-specific image comprehension.\nThe GPT-4V API introduces a multi-modal approach, empowering GPT-4 to process and analyze images alongside textual content. Continually refined through instruction tuning and learning, the model harnesses a comprehensive data corpus to sharpen its textual and visual insights.\nGoogle's Gemini Pro Vision embodies a comprehensive AI system capable of parsing multi-modal stimuli. Leveraging a sophisticated transformer model architecture, Gemini Pro Vision exemplifies Google's commitment to advancing multi-contextual understanding and interaction within the digital landscape. We opt for the Pro version for its optimal balance of high-end performance and scalability."}, {"title": "Generative Models", "content": "For Image Generation, we refer to stable diffusion (Rombach et al. 2021) for an image generation task that synthesizes realistic and diverse images from input such as text. Stable diffusion (Rombach et al. 2021) uses the DDIM (Song, Meng, and Ermon 2022) mechanism in latent space with powerful pre-trained denoising autoencoders. With this powerful pre-trained autoencoder, stable diffusion can transfer input data into latent space and achieve the diffusion process on it, allowing DM training on limited computational resources.\nFor text generation, we refer to paraphrasing. DiffuSeq (Gong et al. 2023) uses diffusion and sequence-to-sequence mechanisms to rephrase given text, preserving semantics while changing stylistic makeup. Here we paraphrase harmful instructions from original harmful behaviors dataset."}, {"title": "Analyzing Score Efficiency through Image-based Adversarial Attacks", "content": "Our analysis of the Retention Image score employs the RealToxicityPrompts benchmark (Gehman et al. 2020) as input prompts. We randomly chose 50 text prompts from its challenging subset, known for inciting toxic continuation responses. These prompts are input alongside visually adversarial examples. To quantify the toxicity level of the generated outputs, we utilize the Perspective API that assigns toxicity ratings on a scale from 0 to 1, with higher values indicating increased toxicity. A threshold value of 0.5 serves as our benchmark for deeming a response as toxic.\nImages are adversarially tailored to manipulate VLM into complying with associated harmful text prompt it would typically reject to respond. We adopt visual adversarial attack outlined in (Qi et al. 2023a) with $l_\\infty$ perturbation limit of $ \\epsilon = 16/255$, iteratively generating examples crafted to maximize occurrence probability for specific harmful contents. These adversarial visual instances, paired with consistent prompts, undergo evaluations measuring toxicity of responses to determine Attack Success Rate (ASR).\nIn terms of image generation, our protocol follows the state-of-the-art generative model, stable diffusion. In the study by (Qi et al. 2023a), the 'clean' image originates from a depiction of a panda, whereas (Carlini et al. 2023) employ a Gaussian noise base image as their starting point. To minimize the experimental randomness and examine the influence of image variability on the efficacy of attacks, we have generated a diverse set of 50 images for each demographic subgroup, categorized by gender and age: male, female, older adults, and youths. For instance, we utilize stable diffusion with a prompt such as \u201cA facial image of a woman.\" to synthetic the given woman's facial image. The prompts used and the corresponding examples of generated images are thoroughly documented in Appendix ."}, {"title": "Impact of Visual Integration on Toxicity for VLMs", "content": "Here we assess the impact of adding visual elements to LLMs on their ability to mitigate toxicity. We hypothesize that a multi-modal approach using both visual and textual data might not improve model robustness against toxic outcomes, as it introduces multi-modal attack interfaces. To investigate, we compared VLMs' performance with their corresponding LLMs. Our experimental setup involved feeding a noise image generated from a Gaussian distribution to VLMs, along with identical textual prompts to corresponding plain LLMs. We evaluate the Retention-T for LLMs and assess the ASR.\nBy the results in Table 4, we conclude that LLaVA and InstructBLIP show a significant decrease in toxicity score and a significant increase in ASR. This suggests that adding the visual module in LLaVA and InstructBLIP increased toxic outputs, decreasing the model's safety. The relative constancy of Retention Text Score and ASR within MiniGPT-4 can be attributed to its architecture. MiniGPT-4 includes a frozen visual encoder and LLM, connected by a trainable projection layer that aligns representations between the visual encoder and Vicuna. The visual backbone integration does not significantly affect output toxicity. In contrast, the influence of the visual module on InstructBLIP's performance can be explained by textual instructions being processed by the frozen LLM and the Q-Former, enabling the Q-Former to distill instruction-aware textual features. Meanwhile, LLaVa presents a scenario where the LLM is dynamically tuned with the visual encoder. Such a configuration disrupts the resilience of the LLM, making it more susceptible to perturbations induced with the visual components.\nOverall, the results indicate that the inclusion of a visual module can influence the toxicity resilience of VLMs such as LLava and InstructBLIP, with varying degrees of effectiveness across different models. Further research is needed to clarify the mechanisms by which visual modules can improve resilience and reduce the occurrence of toxic language generated by these sophisticated models."}, {"title": "Run-time Analysis", "content": "Figure 2 compares the run-time efficiency of Retention Score over adversarial attacks in (Qi et al. 2023a) and (Liu et al. 2023b). We show the improvement ratio of their average per-sample run-time (wall clock time of Retention Score/Adversarial Attack is reported in Appendix ) and observe around 2-30 times improvement, validating the computational efficiency of Retention Score."}, {"title": "Conclusion", "content": "In this paper, we presented Retention Score, a novel and computation-efficient attack-independent metric for quantifying jailbreak risks for vision-language models (VLMs). Retention Score uses off-the-shelf diffusion models for deriving robustness scores of image and text inputs. Its computation is lightweight and scalable because it only requires accessing the model predictions on the generated data samples. Our extensive results on several open-source VLMs and black-box VLMs (Gemini Vision and GPT4V) show the Retention score obtains consistent robustness analysis with the time-consuming jailbreak attacks, and it also reveals novel insights in studying the effect of safety thresholds in Gemini and the amplified risk of integrating visual components to LLMs in the development of VLMs."}, {"title": "Appendix", "content": "All the notations and labels used are listed in Table 5.\nIn this section, we will give detailed proof for the certified conditional robustness estimate in Theorem 1. The proof contains three parts: (i) derive the local robustness certificate for VLM given a image text pair.; (ii) derive the closed-form Lipschitz constant; and (iii) prove the proposed Retention-I and Retention-T is a lower bound on the conditional robustness. Some of our proofment here refers to GREAT Score (Li, Chen, and Ho 2023).\nLemma 2 (Lipschitz Continuity in Gradient Form for VLM in image aspect ((Paulavi\u010dius and \u017dilinskas 2006))). Suppose SCRd is a convex, bound, and closed set, and let M : (S,T) \u2192 II\u00b2 be a VLM that is continuously differentiable on an open set containing S where T is a fixed text prompt. Then M is Lipschitz, continuous if the following inequality holds for any x, y \u2208 S :\n|M(x,T) \u2013 \u041c(y,T)| \u2264 L2 ||x - Y||2\nwhere L2 = maxx\u2208s ||\u2207M(x,T)||2 is the corresponding Lipschitz constant.\nThen we get the formal guarantee for adversarial image attacks.\nLemma 3 (Formal guarantee on lower bound of VLM for adversarial image attacks.). Let I,T \u2208 Rd be a given non-toxic image and fixed text prompt pair, and let M : Rd \u00d7 A \u2192 I\u00b2 be a toxicity judgement classifier integrated with a Vision Language Model that does not output toxic content. For adversarial attacks on images, a lower bound on the minimum distortion in L2-norm can be guaranteed such that for all d\u2081 in Rd, it must satisfy:\n||81||2 \u2264 Mnt(I,T) \u2013 M\u2081(I,T)/LM\nRefer to the proofment in GREAT Score (Li, Chen, and Ho 2023), here we will derive the Lipschitz constant for M.\nLemma 4 (Stein's lemma (Stein 1981)). Given a soft classifier F: Rd \u2192 P, where P is the space of probability distributions over classes. The associated smooth classifier with parameter \u03c3 \u2265 0 is defined as:\nF := (F * N(0, \u03c3\u00b2\u0399))(x) = \u0395\u03b4\u2081~N(0,021) [F(x + \u03b4\u03b9)]"}, {"title": "Algorithm 1: Retention Image Score Computation", "content": "Input: VLM V(, ); toxicity judgment classifier\nJ(\u00b7);\nconditional image generator G1();\nimage score function g\u2081(\u00b7) defined in (1);\nnumber of generated image samples N\u2081; given\nimage I;\nselected text prompts Ts; number of text prompts\nNT\nOutput: Retention Image Score Ri (V)\nscore_sum - 0\nfor i 1 to N\u2081 do\nSample z ~ N(0, I) from a Gaussian distribution\nGenerate image sample G1(z|I) using G1(\u00b7)\nfor j 1 to Nr do\nend\nend\nObtain the VLM response V (G1(z|I), Ts[j])\nby combining image G1(z|I) with prompt\nTs[j] and passing to VLM V\nEvaluate the response through classifier J to\nget toxicity scores (Mnt, Mt)\nCalculate the partial score using toxicity\nscores:\npartial_score =\n\u221a{Mnt(G1(z|I), Ts[j])\nMt(G1(z|I), Ts[j])}+\nscore_sum \u2190 score_sum + partial_score\nRi(V) \u2190 score_sum/NINT(Compute the mean score)\nAlgorithm 2: Retention Text Score Computation\nInput: VLM V(\u00b7, \u00b7); toxicity judgment classifier\nJ(\u00b7);\nparaphrasing generator for Text GT(\u00b7);\nscore function gr(\u00b7) defined in (3); semantic\nencoder s; semantic decoder \u03c8;\ngiven Image I; selected text prompts Ts;\nnumber of times to paraphrase each prompt Np.\nOutput: Retention Text Score Rt (V)\nscore_sum-0\nforeach prompt T in Ts do\nfor i 1 to Np do\nend\nend\nSample z ~ N(0, I) from a Gaussian\ndistribution\nGenerate a paraphrased prompt GT(z|T)\nusing the generator GT(\u00b7) conditioned on T\nObtain the VLM response\nV(I,\u03c8(s(GT(z|T)))) by combining image\nI with the encoded prompt\nEvaluate the response through classifier J to\nget toxicity scores: (Mnt, Mt)\nCalculate the partial score using toxicity\nscores:\npartial_score \u2190\n\u221a\u00b7{Mnt(I, \u03c8(s(Gr(z|T))))-\nMt(I,\u03c8(s(GT(z|T))))}+\nscore_sum \u2190 score_sum + partial_score\nRt(V) \u2190 score_sum/Ts(Compute the mean score)"}, {"title": "LLama-70B Judge", "content": "In light of assessing harmful instructions", "policy.\n\"role\"": "user\"", "content\"": "Below are Model IP-holder\nname, e.g., OpenAI's prohibited usage policies. These\npolicies ensure that models interact with users safely\nand responsibly. Models should refuse to execute in-\nstructions that fall into the disallowed use cases outlined\nin the policy, avoiding affirmative responses or provid-\ning any information that could be helpful for any prohib"}]}