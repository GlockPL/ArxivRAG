{"title": "DIFFERENTIAL LEARNING KINETICS GOVERN THE TRANSITION FROM MEMORIZATION TO GENERALIZATION DURING IN-CONTEXT LEARNING", "authors": ["Alex Nguyen", "Gautam Reddy"], "abstract": "Transformers exhibit in-context learning (ICL): the ability to use novel information presented in the context without additional weight updates. Recent work shows that ICL emerges when models are trained on a sufficiently diverse set of tasks and the transition from memorization to generalization is sharp with increasing task diversity. One interpretation is that a network's limited capacity to memorize favors generalization. Here, we examine the mechanistic underpinnings of this transition using a small transformer applied to a synthetic ICL task. Using theory and experiment, we show that the sub-circuits that memorize and generalize can be viewed as largely independent. The relative rates at which these sub-circuits learn explains the transition from memorization to generalization, rather than capacity constraints. We uncover a memorization scaling law, which determines the task diversity threshold at which the network generalizes. The theory quantitatively explains a variety of other ICL-related phenomena, including the long-tailed distribution of when ICL is acquired, the bimodal behavior of solutions close to the task diversity threshold, the influence of contextual and data distributional statistics on ICL, and the transient nature of ICL.", "sections": [{"title": "INTRODUCTION", "content": "Large transformer models trained to predict the next token exhibit powerful generalization capabilities. One signature of such generalization capabilities is in-context learning (ICL): the ability to solve a task based on new information presented in the context without additional weight updates (Brown et al. (2020); Dai et al. (2022); Dong et al. (2022); Garg et al. (2022); Xie et al. (2021); Olsson et al. (2022)). Arguably, the ability to interpret novel inputs on-the-fly is a core feature of any intelligent system. However, updating synaptic weights on rapid behavioral timescales is challenging, both for natural and artificial systems. The emergence of ICL in large language models (LLMs) shows that finding network states that learn on-the-fly is indeed possible. Understanding how ICL emerges in LLMs promises insights into how such algorithms may be implemented in the brain and how the data distribution, training objective and network architecture interact to enable ICL acquisition at scale.\nVarious methods have been used to probe the ICL capabilities of LLMs (Brown et al. (2020); Dong et al. (2022); Pan (2023); Min et al. (2022); Olsson et al. (2022)). A common ICL paradigm is to present exemplars as a sequence of item-label pairs, and measure the network's response to a target item (Chan et al. (2022); Kirsch et al. (2022); Garg et al. (2022); Aky\u00fcrek et al. (2022); Von Oswald et al. (2023); Ravent\u00f3s et al. (2023); Bai et al. (2023)). While LLMs display remarkable capabilities on such ICL tasks, interpreting the underlying network mechanisms that give rise to these capabilities remains challenging (but see Wang et al. (2022)). Recent work has approached this challenge by examining how small transformer models solve synthetic ICL tasks (Reddy (2023);\nBietti et al. (2024); Aky\u00fcrek et al. (2022); Ahn et al. (2023); Von Oswald et al. (2023); Edelman et al. (2024)). We highlight two notable aspects of ICL phenomenology relevant for our current work: the influence of task diversity on whether the network memorizes a finite dataset or acquires ICL (i.e., generalizes), and how ICL is acquired (and lost) during training."}, {"title": "TASK FORMULATION", "content": "We consider a simplified version of an ICL task proposed by Chan et al. (2022), which allows for disentangling ICL and IWL performance (Figure 2a). Before training, we generate a dataset D that contains K item-label pairs, D = {(x\u2081,l\u2081), (x\u2082, l\u2082), ..., (x\u2096,l\u2096)}. Each item x\u1d62 is a D-"}, {"title": "RESULTS", "content": ""}, {"title": "A ONE-LAYER TRANSFORMER MODEL RECAPITULATES ICL PHENOMENOLOGY", "content": "We begin with a one-layer attention-based network followed by a multi-layer perceptron (MLP). Given tokens t\u2081, t\u2082, ..., t N+1, we first apply a LayerNorm operation to obtain t\u2081 = LayerNorm(t\u1d62)."}, {"title": "DISENTANGLING ICL AND IWL IN A MINIMAL MODEL", "content": "Figure 2(c-f) shows that, despite its simplicity, the one-layer transformer model captures the core features of the memorization to generalization transition and ICL training dynamics observed in more complex models. However, a mechanistic analysis is still challenging due to the nonlinearities in the attention head, the MLP and how these two operations interact. To make progress, we further reduce our one-layer transformer model into a disentangled model (which we refer to as the \u201cminimal model\" hereafter) by proposing two ansatz. We will show empirically that the minimal model also reproduces the phenomena in Figure 2(c-f). This minimal model is amenable to a theoretical analysis and leads to specific quantitative predictions. We then validate our ansatz by empirically testing these predictions using our original transformer model (Section 4).\nTo motivate the ansatz, we observe that ICL in this task involves a simple match-to-sample operation implemented by the attention head. The attention paid by the target token is determined by its dot-product similarity with the content (the first D dimensions) of the tokens in the context. The value matrix reads the labels of the tokens weighted by the attention paid to those tokens and passes it on"}, {"title": "THE LOSS LANDSCAPE OF THE MINIMAL MODEL", "content": "We consider the asymptotic limit K \u00bb N \u00bb 1 and the infinite-dimensional limit D \u2192 \u221e (recall, K > 10\u00b3, N = 10\u00b2, D = 63 in our experiments). From equation 3, ICL is acquired when w, \u03b2 \u00bb 1. Our goal is to compute the time taken for the network to acquire ICL starting from w = w\u2080, \u03b2 = \u03b2\u2080 with |w\u2080|, |\u03b2\u2080| \u226a 1.\nIn the limit D \u2192 \u221e, the dot product x\u2c7c\u22c5XN+1 is 1 if x\u2c7c is a copy of the target and 0 otherwise. It is unlikely there is more than one copy of the target in the context when K \u00bb N. Let c denote the index of this copy. From equation 3, we have\nZATT \u2248 w(e^(\u03b2))/(e^(\u03b2) + N \u2212 1) \u22c5 lc + 1/(e^(\u03b2) + N \u2212 1) \u03a3_(j\u2260c)(2n_+ \u2212 N - lc),  (4)\nwhere n+ is the (binomally distributed) number of tokens with label +1 amongst the N tokens in the context. When N \u226b 1, n+/N \u2248 1/2 + \u03b7/2\u221aN, where \u03b7 ~ N(0, 1).\nNext, the MLP's contribution to the average loss appears only through the distribution of logits obtained by applying the MLP to each of the K items in D. In particular, denote P+ as the distribution of logits obtained when the MLP is applied to the items in D with a +1 label. We use the fact that the two labels are symmetric, and average over n+ and P+ to show that the average binary cross-entropy loss L is (see Appendix)\nL\u2248 -((1/\u221aN)+(\u03b7\u221aN/N))*\u03c3'((6+ + w)(e^(\u03b2) \u2212 1)/(e^(\u03b2) + N \u2212 1))  (5)"}, {"title": "THE DYNAMICS OF ICL ACQUISITION", "content": "To examine the dynamics of ICL acquisition, we find an expression for the loss when |w| < \u221aN and e^(\u03b2) \u2212 1 < N (for arbitrary P+). Both these conditions are satisfied at initialization (|w\u2080|, |\u03b2\u2080| \u226a 1). From equation 5, a few steps of simplification leads to (Appendix)\nL\u2248 L_(MLP) + c\u2081/N e^w \u2212 c\u2082w^2/2 (6)\nwhere c\u2081 = (\u03c3(\u2212\u03c6+))$+ and c\u2082 = 1 \u2212 (\u03c3(\u2212\u03c6+)2)$+/c\u2081. Here, we used |w| < \u221aN and e^(\u03b2) \u2212 1 \u00ab\nN to Taylor expand equation 5 and retained terms to order 1/N (terms of order 1/\u221aN vanish in expectation). The distribution from which $+ is drawn has been dropped for notational convenience.\nEquation 6 allows us to make several important inferences. The first term in the r.h.s of equation 6 is the loss incurred by the MLP. It does not involve w, \u03b2 and thus does not affect ICL learning dynamics. Since the second term in the r.h.s of equation 6 is small at initialization, the rate at which the MLP memorizes is not affected by ICL learning. That is, IWL proceeds without any competition from ICL until ICL is acquired (which happens abruptly).\nThe scalar variables c\u2081 and c\u2082 depend on P+ and thus depend on the time t since training began. Their evolution in general depends on multiple factors, including MLP architecture, initialization scheme and the number of tasks K to be memorized. Importantly, IWL influences ICL acquisition only through c\u2081(t) and c\u2082(t), which in turn depend only on how the MLP memorizes class labels. We proceed with our analysis by retaining c\u2081(t) and c\u2082(t) as yet-to-be-determined MLP-specific dynamical \u201corder parameters\", keeping in mind that their dependence on t and K will play an important role in our analysis further below.\nGradient descent dynamics over the loss in equation 6 gives\ndw/dt = c\u2081/N (e^(\u03b2) \u2212 c\u2082w), (7)\nd\u03b2/dt = c\u2081/N (we^(-\u03b2)). (8)\nLearning initially proceeds at a slow rate c\u2081/N (since N\u226b 1 and 0 < c\u2081 < 1). Since $+ on average increases as the MLP memorizes, c\u2081 decreases and slows down ICL acquisition. If the MLP (near)\nperfectly memorizes the K item-label pairs before ICL is acquired, then ICL is never acquired. In other words, the loss \"explained away\" due to MLP memorization creates an effective competition between IWL and ICL acquisition despite the additive contributions of the MLP and the attention head to the logit. Since 0 < c\u2082 < 1, equation 7 shows that w eventually converges from its initial value to a positive value w = e^(\u03b2)/c\u2082. \u03b2 increases monotonically when w is positive until ICL is acquired. Thus, equation 7 and equation 8 imply that ICL will always be acquired, however slowly, if the MLP is unable to perfectly memorize the K item-label pairs (i.e., c\u2081(\u221e) > 0).\nHowever, the choice of label statistics in the context matters. For example, consider the case when N is even and there are exactly N/2 tokens with +1 and \u22121 labels in the context. To compute the mean loss L' in this scenario, we set n = 0 in equation 5 and Taylor expand w.r.t (e^(\u03b2) \u2212 1)/N to get\nL'\u2248 C1/(log(1+e^($+))) + 1/N w(e^(\u03b2) \u2212 1). (9)"}, {"title": "EXPONENTIAL DEPENDENCE OF ICL ON INITIAL CONDITIONS", "content": "Equation 7 and equation 8 allow us to estimate the number of iterations it takes to acquire ICL (Appendix). Note that \"time\" t here is a proxy for the number of iterations, which we can only determine up to a constant pre-factor. Exact integration of equations 7 and 8 is infeasible, but an approximate expression is obtained when |w\u2080|, |\u03b2\u2080| < 1. We fix w\u2080 = 0 hereafter, though the more general case of w\u2080 \u2260 0 can be solved (Appendix). We find that the number of iterations it takes for ICL acquisition (denoted TK) satisfies\nN\u221a2\u03c0\u03b5^(-\u03b2\u2080) \u2248 I_K(T_K), where I_K(t) = 2 \u222b_0^t c\u2081(t')dt'.  (10)\nThe subscript K is introduced to highlight that c\u2081 depends on K.\nWe first consider the case K = \u221e so that the MLP is unable to memorize D. The MLP logit $+ is distributed symmetrically around 0, in which case c\u2081(t) = (\u03c3(\u2212\u03c6+))$+ \u2248 1/2 and I_\u221e(t) = t. Solving for T_\u221e (which we call t_ICL hereafter) using equation 10, we get\nt_ICL ~ N\u221a2\u03c0\u03b5^(-\u03b2\u2080). (11)\nThe dynamics are qualitatively different when -\u03b2\u2080 is large and e^(\u03b2\u2080) \u00ab 1. In this case, we obtain\nt_ICL \u2248 Ne^(-2\u03b2\u2080) (Appendix). We numerically verify the exponential dependence of t_ICL on the initial values of \u03b2\u2080 (Supplementary Figure A.1a). A consequence of this exponential dependence is that normal-distributed values of \u03b2\u2080 will lead to a long-tailed distribution of t_ICL. In pictorial terms, due to the nearly flat loss landscape close to initialization (Figure 4), small variation in the initial parameters w\u2080, \u03b2\u2080 leads to large variation in when ICL is acquired."}, {"title": "MEMORIZATION SCALING LAWS AND THE TRANSITION FROM MEMORIZATION TO GENERALIZATION", "content": "Equation 10 shows that the behavior of an MLP-specific quantity, c\u2081(t) (via I_K), determines when ICL is acquired for different values of K. It is useful to introduce the quantity I_K (\u221e), which can be interpreted as the time taken for the MLP to memorize a dataset of size K. Equations 10 and 11 together with the monotonicity of I_K (t) imply that ICL is acquired if\ntICL < I_K(\u221e). (12)\nWe delineate two distinct mechanisms depending on whether I_K(\u221e) is finite or not:\n1. Capacity-constrained: We call the network capacity-constrained if I_K(t) diverges as t \u2192\n\u221e, i.e., the network never fully memorizes the dataset. Equation 12 then implies that the\nnetwork generalizes when K > Kcc, where Kcc is the smallest K at which the network is\ncapacity-constrained.\n2. Differential learning kinetics: It is possible that I_K(\u221e) is finite. In this case, the network\ntransitions from memorization to generalization at K = K* such that t\u2081CL \u2248 I_K* (\u221e). In\nother words, when K > K*, it takes longer for the network to memorize the dataset (even\nthough it has the capacity to do so) than it takes for the network to generalize. We call\nthis case the differential learning kinetics regime as the relative rates at which the network\nmemorizes and generalizes determine when the transition occurs.\nThe divergence of I_K(t) as t \u2192 \u221e may occur either because the network has limited capacity to memorize the K samples or because of the data distribution. For example, if the rank-frequency distribution of item-label pairs follows a Zipf's law p(f) ~ f^(-\u03b1) with exponent \u03b1 < 1, then the network's loss is dominated by rare item-label pairs that are not memorized. Previous work has shown that such skewed data distributions indeed favor ICL acquisition (Chan et al. (2022))."}, {"title": "SLOW IWL EXPLAINS TRANSIENT ICL", "content": "We now explain why transience appears in our minimal model (Figure 3c) when the attention head is regularized more heavily compared to the MLP. For simplicity, we impose L2 regularization with parameter \u03bbw only on w. We return to equation 5 for the loss, which applies throughout training. Since w, \u03b2 \u00bb 1 after ICL is acquired, we can simplify equation 5 to obtain (Appendix)\nL\u2248 (e^(-\u03c6+))/(1 + \u221aN \u03b7) e^(-w) + \u03bb_w w^2/2. (14)\nOnce ICL is acquired, memorization slows down dramatically due to the small factor e^(-w). Without L2 regularization on w, w continues to increase (at a decreasing rate) and ICL is not transient. However, when w is regularized, w after ICL acquisition tracks wtr, where\nW_tr(t) \u2248 W(c3(t)/\u03bb_w), c3(t) = (e^(-\u03c6+))_\u03c6+ (15)\nThe Lambert W function W(x) is monotonic in x when x is positive. c3 decreases as the network memorizes the dataset (Figure A.2c). Thus, Wtr decreases as c3 decreases. Wtr decays to zero (and\nICL fades away) when the dataset is sufficiently memorized, i.e., when c3 \u2248 \u03bbw. Thus, the analysis\nsuggests that extremely slow memorization coupled with regularization leads to ICL transience.\nWe note however that in more complex models the effects of a global regularization parameter on\ndifferent sub-circuits are hard to disentangle, which may explain the puzzling observations in Singh\net al. (2023).\nEquation 15 hints at a relationship between the loss on ICL sequences (L_ICL) and the loss of IWL sequences (L_IWL) after ICL is acquired. We use a heuristic argument (Appendix) to show that\nL_IWL \u2248 log(L_ICL), when L_ICL < 1,\nL_ICL \u2248 log(L_IWL), when L_IWL < 1. (16)\nThese approximate relations between L_ICL and L_IWL are consequences of our two ansatz. If our\nansatz are valid, the theory predicts that these relations should hold from the moment ICL is acquired\nuntil it fades due to gradual IWL."}, {"title": "EMPIRICAL VALIDATION", "content": "The theory makes a number of new quantitative predictions related to ICL acquisition. We empirically test six nontrivial predictions that span various aspects of ICL phenomenology using the original transformer model in equation 1.\nPower-law scaling of the task diversity threshold with context length. Equation 13 predicts a highly non-trivial power law relationship between the task diversity threshold K* and N. To test this prediction, we train our original transformer model (equation 1) at varying N and K. At each N, we observe a sharp transition from memorization to generalization as K increases (Supplementary Figure A.7). For each N, we determine K* by fitting a sigmoidal curve to ICL performance as a function of K. As \u03bd \u2248 0.7, Equation 13 predicts an exponent of 1/\u03bd \u2248 1.43, closely matching our measured exponent \u2248 1.41 (Figure 6a).\nLinear scaling of the time taken to acquire ICL with context length. Equation 11 predicts that\ntICL (time taken to acquire ICL) scales linearly with N. To test this, we train our original transformer\nmodel (equation 1) at varying N and take the limit K \u2192 \u221e by resampling our dataset D at every\ntraining iteration. We then determine tICL as the epoch at which ICL accuracy exceeds 95%. We\ntrain \u2248 100 seeds for each N to obtain the full distribution of tICL. Figure 6b confirms a linear"}, {"title": "CONCLUSION", "content": "Here, we propose a theory based on the ansatz that the network contains sub-circuits that are independently involved in memorization and generalization. A trade-off arises simply because the rate at which one sub-circuit is optimized depends on how much loss is already explained by the other sub-circuit(s). Building on this theory, we show that the transition from memorization to generalization in our model is determined by the relative rates at which these sub-circuits memorize and generalize. However, the theory does not rule out the possibility that capacity constraints play a role in other scenarios.\nThis ansatz, despite being cast in the context of a simplified one-layer model, explains a surprising variety of ICL-related phenomena observed with much larger models. These include a long-tailed distribution in when ICL is acquired, an MLP memorization scaling law, the bimodality of solutions at the task diversity threshold, the transient nature of ICL, amongst other novel quantitative relations that our theory identifies. The two most striking predictions are (1) the non-trivial relationship between an MLP-specific memorization scaling law (I_K(\u221e) ~ K^\u03bd) and a task diversity threshold scaling law w.r.t context length (K* ~ N^(1/\u03bd)), and (2) the long-tailed distribution of when ICL is acquired and its linear scaling with context length (t_ICL ~ N). Both these predictions have been validated in our experiments.\nOur results offer some hope that seemingly intractable phenomena observed in large models can be reproduced and analyzed using simpler, tractable models through careful experimental design. However, further work is necessary to examine to what extent such insights provided by small models remain valid for larger models (that potentially contain many sub-circuits) and for more naturalistic tasks (where a clear distinction between memorization and generalization cannot be made) (Min et al. (2022); Wei et al. (2023); Pan (2023); Shi et al. (2024))."}, {"title": "APPENDIX", "content": "We present a detailed analysis of the minimal model outlined in the main text. Suppose the (D+1)-dimensional input tokens are t1, t2, ..., tN+1, where ti = (xi, li) for i \u2264 N and tN+1 = (xN+1,0). In the minimal model, we consider two logits ZMLP = \u03c6(xN+1) and\nZ_ATT = \u03a3_(l=1)^N  e^(\u03b2x_l \u22c5 x_(N+1))/(\u03a3_(l=1)^N e^(\u03b2x_l \u22c5 x_(N+1))) * wl_j. (17)\n\u03c6 is a multi-layer perceptron (MLP), which throughout our paper is a three-layer ReLU network with hidden dimension 512. The final logit z used for classification is the sum of the contributions from the\nMLP and the attention head, z = ZMLP + ZATT. The minimal model can be obtained from the\none-layer transformer model by (1) removing the LayerNorm operation, (2) the interaction strength\nansatz, i.e., by assuming the query, key and value matrices in the attention head have the form\nK^TQ = (\u03b2I_(DxD)   \u03b2I_(Dx1) // 0_(1xD)   w_(1x1)), V = (0_(DxD) // 0_(1xD)), (18)\nand (3) the independence ansatz, where the residual term xN+1 is processed by the MLP to produce\nZMLP, the output of the attention head is ZATT and these two logits are summed to produce the final\nlogit z. However, we stress that the minimal model serves as a phenomenological model and is not\nderived from the one-layer transformer model.\nTo reproduce the phenomenology shown in Figure 3, we optimize \u03b2, w and the parameters of the MLP \u03c6 using the same procedure used to train the full model. In particular, we use stochastic gradient descent (SGD) with batch size 128, learning rate 0.01, weight-decay 10^(-10), D = 63, N = 100 and MLP hidden dimension d = 512. To reproduce transience in Figure 3c with a fewer number of iterations, we increase the weight-decay parameter on w and \u03b2 to 10^(-3)."}, {"title": "THEORETICAL ANALYSIS OF THE MINIMAL MODEL", "content": "To derive an analytical expression for the loss landscape of the minimal model, we assume (1) that only one copy (say xc) of the target xN+1 is present in the context, (2) that xN+1 \u22c5 xi = 1 if i = c and 0 otherwise, and (3) that the distributions of logits obtained when the MLP is applied to all the"}, {"title": "EXPANDING THE LOSS BEFORE ICL ACQUISITION", "content": "w, \u03b2 are initialized at w\u2080, \u03b2\u2080 where |w\u2080|, |\u03b2\u2080| < 1. Define \u03b3 = e^(\u03b2) - 1. Close to initialization, w, \u03b2 satisfy \u03b3/N < 1 and |w|/\u221aN \u00ab 1. We compute the time it takes for the network to acquire ICL."}, {"title": "ICL ACQUISITION", "content": "We consider ICL to be acquired when w, \u03b2 \u00bb 1. To compute the time taken to acquire ICL, we consider gradient descent dynamics of w and \u03b2 close to initialization. From equation 28, we get\ndw/dt = c\u2081/N (e^(\u03b2) \u2212 c\u2082w), (31)\nd\u03b2/dt = c\u2081/N (we^(-\u03b2)). (32)\nNumerical simulations (Figure A.1) with c\u2081, c\u2082 fixed at 1/2 reveal qualitatively different dynamics depending on whether |\u03b2\u2080| < 1 or \u2212\u03b2\u2080 \u226b 1. Note that e^(\u03b2\u2080) \u2212 1 \u00ab N in both these cases, so that our approximation equation 28 is still valid. We first examine the relevant case |\u03b2\u2080| \u226a 1. The analysis of the second case -\u03b2\u2080 > 1 is presented further below for completeness.\nEquations 31 and 32 cannot be solved exactly and we resort to approximations. When |\u03b2\u2080| \u226a 1\nand |w\u2080| < 1, we have e^(\u03b2) \u2248 1 and |w| \u226a 1 close to initialization. By definition, c\u2082 < 1. Put"}, {"title": "TRANSIENCE", "content": "We examine the influence of applying L2 regularization to the parameters of the attention head. For simplicity, we assume regularization (with L2 regularization parameter \u03bbw) is applied only to w. Our goal in this section is to show that such a regularization parameter (however small) is necessary to induce transience, and to delineate the values of \u03bbw for which transience is recapitulated in the minimal model. A sufficiently large regularization parameter will of course also affect ICL acquisition (Supplementary Figure A.6). Further analysis could delineate the range of values of w for which regularization will have a significant effect on ICL acquisition; this analysis is beyond the scope of this paper.\nWe write down the dynamics of w after ICL is acquired. Re-writing the expression for the loss in equation 22,\nL\u2248 -(1+(\u03b7/N))*log \u03c3'((6+ + w)(e^(\u03b2) \u2212 1)/(e^(\u03b2) + N \u2212 1) + (\u03b7\u221aN)/(N-1)), (38)\nNote that the terms involving \u03b7 inside the logarithm are at most of order 1/\u221aN. Since w, \u03b2 \u00bb 1 after ICL is acquired, the first term (e^(\u03b2) \u2212 1)/(e^(\u03b2) + N \u2212 1) \u2248 1 will be much larger than the term involving \u03b7. This leads to\nL~- (1/\u221aN)*(log \u03c3'( \u03c6+ + w)). (39)"}]}