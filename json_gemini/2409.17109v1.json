{"title": "Unveiling Ontological Commitment in Multi-Modal Foundation Models", "authors": ["Mert Keser", "Gesina Schwalbe", "Niki Amini-Naieni", "Matthias Rottmann", "Alois Knoll"], "abstract": "Ontological commitment, i.e., used concepts, relations, and assumptions, are a corner stone of qualitative reasoning (QR) models. The state-of-the-art for processing raw inputs, though, are deep neural networks (DNNs), nowadays often based off from multimodal foundation models. These automatically learn rich representations of concepts and respective reasoning. Unfortunately, the learned qualitative knowledge is opaque, preventing easy inspection, validation, or adaptation against available QR models. So far, it is possible to associate pre-defined concepts with latent representations of DNNs, but extractable relations are mostly limited to semantic similarity. As a next step towards QR for validation and verification of DNNs: Concretely, we propose a method that extracts the learned superclass hierarchy from a multimodal DNN for a given set of leaf concepts. Under the hood we (1) obtain leaf concept embeddings using the DNN's textual input modality; (2) apply hierarchical clustering to them, using that DNNs encode semantic similarities via vector distances; and (3) label the such-obtained parent concepts using search in available ontologies from QR. An initial evaluation study shows that meaningful ontological class hierarchies can be extracted from state-of-the-art foundation models. Furthermore, we demonstrate how to validate and verify a DNN's learned representations against given ontologies. Lastly, we discuss potential future applications in the context of QR.", "sections": [{"title": "1 Introduction", "content": "One of the basic ingredients of QR models is an ontology specifying the allowed concepts, relations, and any prior assumption about them; more precisely, the commitment to (a subset of an) ontology with associated semantic meaning of concepts and relations [20]. Thanks to years of research, large and rich ontologies like Cyc [30], SUMO [35], or ConceptNet [53] are readily available for building or verifying QR models.\nMeanwhile, however, DNNs have become the de-facto state of the art for many applications that hardly allow a precise input specification [42], such as processing of raw images (computer vision), e.g., for object detection [19], or processing of unstructured natural language text [37]. This machine learning approach owes its success to its strong representation learning capabilities: DNNs automatically learn highly non-linear mappings (encoding) from inputs to vectorial intermediate representations (latent representations or vectors) [11], and reasoning-alike processing rules [3, 23] from these to a desired output. Availability of large text and image datasets have further sparked the development of multimodal so-called foundation models [10, 28, 45]. These are large general-purpose DNNs trained to develop semantically rich encodings suitable for a variety of tasks [10]. This is oft achieved by training them to map textual descriptions and images onto matching vectorial representations (text-to-image alignment) [45], using multimodal inputs of both images and text.\nThe prospect. Foundation models come with some interesting prospects regarding their learned knowledge: (1) One can expect foundation models to learn a possibly interesting and useful ontology, giving insights into concepts [27, 29, 49, 62] and concept relations [16, 27] prevalent in the training data; and (2) such sufficiently large models can also develop sophisticated reasoning chains on the learned concepts [23, 44]. From the point of perspective of QR, this raises the question, whether this learned knowledge is consistent with the high quality available ontologies and QR models. This opens up well-grounded verification and validation criteria for safety or ethically critical applications. As a first step towards this, this paper defines techniques for extraction and verification of simple class hierarchies. Future prospects encompass to use the extracted knowledge from DNNs for knowledge retrieval, and ultimately gain control over the learned reasoning: This would enable the creation of powerful hybrid systems [14, 31] that unite learned encoding of raw inputs like images with QR models.\nThe problem. Unfortunately, the flexibility of DNNs in terms of knowledge representation comes at the cost of interpretability [22]; and, being purely statistical models, they may extract unwanted and even unsafe correlations [27, 47, 51]. The opaque distributed latent representations of the input do not readily reveal which interpretable concepts have been learned, nor what reasoning is applied to them for obtaining the output. This is a pity, not least because that hinders verification of ethical and safety properties. Take as an example the ontological commitment: Which hierarchical subclass-relations between concepts are considered? An example is shown in Fig. 3. This directly encodes the learned bias, which commonalities between classes are taken into account, and which of these are predominant for differentiating between classes. The same example also nicely illustrates the issue with wrongly learned knowledge: The models may focus on irrelevant but correlated features to solve a task, such as typical background of an object in object detection [47].\nA whole research field, explainable artificial intelligence (XAI), has evolved that tries to overcome the lack of DNN interpretability [22, 50]. To date it is possible to partly associate learned representations with interpretable symoblic concepts (1-ary predicates) [52], such as whether an image region is a certain object part (e.g., isLeg), or of a certain texture (e.g., isStriped) [16, 27]. However, extraction of learned relations is so far focused on simple semantic similarity of concepts [16, 48]; hierarchical relations that hold across subsequent layers, i.e., across subsequent encoding steps [27, 59, 60]; or hierarchies obtained when subdividing a root concept [33]. And while first works recently pursued the idea to extract superclass hiearchies from given leaves, these are still limited to simple classifier architectures [59]. A next step must therefore be: Given a set of (hierarchy leaf) concepts, how to extract (1) the unifying superclasses, and (2) the resulting class hierarchy with subclass relationships from any semantically rich intermediate output of a DNN, preferrably from the embedding space of foundation models.\nApproach. We here propose a simple yet effective means to get hold of these encoded class hierarchies in foundation models; thereby taking another step towards unveiling and verifying the ontological commitment of DNNs against known QR models respectively on ontologies. Building on [59] and [62], our approach leverages two intrinsic properties of the considered computer vision models:\n(1) Vision DNNs generally encode learned concept similarities via distances in their latent representation vector space [16]. This makes it reasonable to find a hierarchy of superclass representations by means of hierarchical clustering [59].\n(2) Foundation models accept textual descriptions as inputs, trained for text-to-image alignment. This allows to cheaply establish an approximate bijection of textual concept descriptions to representations: A description is mapped by the DNN to a vector representation, and a given representation is assigned to that candidate textual description mapped to the most similar (=close by) vector [62].\nContributions. Our main contributions and findings are:\n\u2605 An approach to extract and complete a simple learned ontology, namely a superclass hierarchy with given desired leaf concepts (Figure 2), from intermediate representations of any multimodal DNN, which allows to manually validate DNN-learned knowledge against QR models (see Figure 1);\n\u2605 An approach to test the consistency of multimodal DNNs against a given class hierarchy, e.g., from standard ontologies;\n\u2605 An initial experimental validation showing that the approach can extract meaningful ontologies, and reveal inconsistencies with given ontologies;\n\u2605 A thorough discussion of potential applications for QR extraction and insertion from / into DNNs."}, {"title": "2 Related Work", "content": "Extraction of learned ontologies. Within the field of XAI [22, 50], the subfield of concept-based XAI (c-XAI) has evolved around the goal to associate semantic concepts with vectors in the latent representations [29, 40, 49]. For analysis purposes, methods here allow to both extract representations which match given concept specifications (supervised approach) [16, 26, 27, 62] as well as mine meanings for the most prevalent representations used by the DNN (unsupervised approach) [18, 63]. Notably, we here utilize the supervised approach by Yuksekgonul et al. [62] which directly utilizes the text-to-image alignment in multimodal DNNs. Such associations have found manifold applications in the inspection of DNNs' learned ontology, such as: Which concepts from a given ontology are learned [2, 52]? And how similar are representations of different concepts [16, 48]? This was extended to questions about the QR of the models, such as sensitivity of later concept representations (or outputs) to ones in earlier layers [27], or compliance with pre-defined logical rules [52]. However, very few approaches so far explored more specific relations between concept representations within the same layer's representation space. In particular, specific relations beyond general semantic similarity, such as class hierarchies. This is a severe gap when trying to understand the learned ontological relations between concepts: DNNs develop increasing levels of abstraction across subsequent layers [16], rendering the concepts occurring in their representation spaces hardly comparable. Notably, Wan et al. [59] challenged this gap and applied hierarchical clustering on DNN representations. However, their association of given concepts to latent representations is limited to last layer's output class representations, which we want to resolve. Furthermore, existing work was devoted only to single kinds of relations. We here want to show that these efforts can be unified under the perspective of investigating ontological commitment of DNNS."}, {"title": "3 Background", "content": "3.1 Deep neural network representations\nDNNs. Mathematically speaking, deep neural networks are (almost everywhere) differentiable functions $F: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ which can be written in terms of small unit functions, the so-called neurons $f: \\mathbb{R} \\rightarrow \\mathbb{R}$, by means of the standard concatenation operation $f \\circ g : x \\rightarrow f(g(x))$, linear combination $x \\rightarrow Wx + b$, and product $a, b \\rightarrow ab$. Typically, the linear weights $W$ and biases $b$ serve as trainable parameters, which can be optimized in an iterative manner using, e.g., stochastic gradient descent. Neurons are typically arranged in layers, i.e., groups where no neuron receives outputs from the others. Due to this \"Lego\"-principle, DNNs are theoretically capable of approximating any continuous function (on a compact subspace) up to any desired accuracy [25], and layers can be processed highly parallel. In practice, this is a double-edged sword: DNNs of manageable size show astonishing approximation capabilities for target functions like detection or pixel-wise segmentation of objects in images [28, 56]. However, they also tend to easily extract irrelevant correlations in the data, leading to incorrect [47] or even non-robust [55] generalization respectively \"reasoning\" on new inputs.\nLatent representations. In the course of an inference of an input $x$, each layer $L$ of the DNN produces as intermediate output a vector $F^{\\rightarrow L}(x) \\in \\mathbb{R}^n$, each entry being the output of one of the $n$ neurons of $L$. This vectorial encoding of the input is called the latent representation of the input within $L$, and the vector space $\\mathbb{R}^n$ hosting the representations is called the latent space. Interestingly, it was shown that DNNs encode semantically meaningful information about the input in their latent representations, with abstraction increasing the more layers are passed (e.g., starting with colors and textures, to later develop notions of shapes and objects) [16, 36].\nConcept embeddings. An emergent property of these representations is that in some layers, a concept $C$ (e.g., color Red, or object part Leg), can be encoded as prototypical vector $e(C)$ within this latent space. These are called concept (activation) vectors [27] or concept embeddings [16]. The mapping $e: C \\rightarrow \\mathbb{R}^n$ from a set of human-interpretable concepts to their embeddings even preserves semantic similarities to some extend: Examples are the reflection of analogical proportions [43] in word vector spaces (DNNs with textual inputs trained for natural language processing), like \"$e(King) - e(Queen) = (Man) - e(Woman)$\" [32]; and their analogues in standard computer vision architectures trained for object classification or detection: \"$e(Green) + e(Wood) = e(Tree)$\" [16]. Our approach relies on these natural translation of semantic to vector operations/properties. In particular, we assume that the relation IsSimilarTo on input instances $x$ is mapped to some distance metric $d$ like Euclidean or cosine distance by the DNN representations:\n$\\forall c, c': IsSimilarTo(c, c') \\Rightarrow d(e(c), e(c') \\approx 0$.\nConcretely, we use the translation of similarity relations to find a superclass concept representation via interpolation.\nText-to-image alignment. In the case of multimodal DNNs that accept both textual and image inputs, the training often encompasses an additional (soft) constraint: Given textual descriptions of an input image, these must be mapped to the same/a similar latent representation as their respective image. While pure language models suffer from the impossibility to learn the true meaning of language concepts without supervision [9], this additional supervision might help the model to develop representations that better match the human understanding of the word/concept. We here leverage this intrinsic mapping to associate textual or graphical descriptions of our concepts with latent representations.\nWhen using textual decriptions, good text-to-image alignment is an important assumption; but, sadly, even with explicit training constraints this is not guaranteed [17] (cf. distance of image and text embeddings in Figure 4). We show both the influence of text-to-image alignment on our method, how it can be reduced, and how to use our method in order to identify issues with the learned meaning of concepts, which opens up options to fix the representations."}, {"title": "3.2 Ontologies", "content": "When modeling any problem or world, a basis of the model is to know \"what the model is talking about\". This is exactly answered by the underlying ontology, i.e., a definition of what categories/properties and relations are used in the model. We here adopt the definition from [20].\nDefinition 1 (Ontology). An ontology is a pair $(V, A)$ constituted by a vocabulary $V = C \\cup R$ of a set of unary predicates $C$ (the concepts corresponding to class memberships and other properties) and a set of binary predicates $R$ (the instance relations) used to describe a certain reality, and which are further constraint by a set $A$ of explicit assumptions in the form of a first- (or higher-)order logic theory on the predicates.\nA relation we will use further is $IsSimilarTo \\in R$. Also spatial relations like IsCloseBy [52] and LeftOf, TopOf, etc. [44] have been defined and used in literature for latent space representations of objects. Simple examples of assumptions that relate the concept sets are, e.g., the subclass relationship we investigate in this paper: $IsSuperclassOf(C',c) : (\\forall v: c(v) \\Rightarrow c' (v))$ (cf. Figure 3). This can also be seen as a relation between concepts, by interpreting the unary concept predicates $C$ as sets of objects (e.g., classes) via $v \\in C : C(v)$. The validity of concept embeddings also gives rise to assumptions about concepts ($\\forall v: C(v) - IsSimilarTo(v, e(C))$). Note that, given embeddings, we can formulate relations between concepts using instance relations $R \\in R$ via $R(c, c') : R(e(c), e(c'))$. An example would be isSimilarTo(cat, dog).\nThe first challenge in extracting learned QR from DNNs is to find/explain the ontology that is used within the reasoning process of the DNN. Unraveling an ontology as done in 1 above breaks this step roughly down into:\n(1) Find the concepts $C$ (and their embeddings) used by the model."}, {"title": "3.3 Hierarchical clustering", "content": "Hierarchical clustering [46] aims to find for a given set $M$ a chain of partitions $M_1 \\leq M_2 \\leq ... \\leq \\{M\\}$ connected by inclusion, i.e., assign each point in $M$ to a chain of nested clusters $M_{1, i_1} \\subset M_{2, i_2} \\subset ... \\subset M$, as illustrated in Figure 1. Such a hierarchy can be depicted using a dendrogram as in Figure 2. There are two regimes for hierarchical clustering: Divisive breaks up clusters top-down, while agglomerative starts from the leaves $M_1 = \\{\\{p\\} | p \\in M\\}$ and iteratively merges clusters bottom-up [46]. We here employ hierarchical clustering to find a hierarchy of subsets of latent representation vectors. Since we start with given leaf vectors, this work uses standard agglomerative hierarchical clustering [61]. This optimizes the partitions for small distance between the single points within a cluster (affinity) and a large distance between the sets of points making up different clusters (linkage), typically at a complexity of $O(|M|^3)$."}, {"title": "4 Approach", "content": "This section details our approach towards extracting a globally valid approximation of a DNN's learned concept hierarchy, given the hierarchy's desired leaf concepts. The goal is to allow manual validation or verification testing against existing ontologies from QR. Recall that this both requires a guided exploration of the learned concepts (which parent classes did the model learn?), as well as an exploration of the applicability of the superclass relation (which superclasses/features are shared or different amongst given concepts?). We will start in subsection 4.2 by detailing how to obtain the extracted class hierarchy (here simply referred to as ontology). This is followed by an excursion on how to conduct a kind of instance-based inference using the global taxonomy (subsection 4.2, which is then used in subsection 4.3 where we discuss techniques for validation and verification of DNN learned knowledge."}, {"title": "4.1 Extracting an ontology", "content": "Overview The steps to extract our desired ontology are (explained in detail further below): (1) obtain the embeddings $e(c_i)$, (2) apply hierarchical clustering to obtain superclass representations as superclass cluster centers, (3) decode the obtained superclass representations into a human-interpretable description.\nIngredients. We need as ingredients our trained DNN $F$, some concept encoder $e$ (in our case defined using the DNN, see Step 1 below), the finite set $(C_i)_{i = 1}^{i=L_{leaf}}$ of leaf concepts for which we want to find parents classes, and the choice of layer $L$ in which we search for them. Furthermore, to ensure human interpretability of the results, we constrain both our leaf concepts as well as our solution parent concepts to come from a given concept bank $C$ of human-interpretable concepts. We furthermore need per concept $c \\in C$: A textual description $toText(c)$ of $c$ as textual specification; optionally a set $toImages(c)$ containing the concept as graphical specification (see Step 1), as available, e.g., from many densely labeled image datasets [8, 24]; and optionally a set $Parents(c)$ of candidates for parent concepts of $c$ (for more efficient search). The following assumptions must be fulfilled, in order to make our approach applicable:\nAssumptions 1.\n(a) Text-to-image alignment: The DNN should accept textual inputs, and be trained for text-to-image alignment, such that for a suitable textual description $T$ of any concept $c \\in C$ one can reasonably assume $e(c) \\approx F^{\\rightarrow L}(T)$. We use this to find embeddings: The embedding of a visual concept $c$ can be set to the DNN's text encoding $F^{\\rightarrow L}(T)$ of a suitable textual description $T$ of $c$.\n(b) Existence of embeddings: For all leaf concepts, embeddings $e(c_i)$ of sufficient quality exist in the latent space of $L$.\n(c) Concentric distribution of subconcepts: Representations of subconcepts are distributed in a concentric manner around its parent. Generally, this does not hold [33], but so far turned out to be a viable simplification as long as semantic similarities are well preserved by the concept embedding function $e$ [18, 41]. I.e. for a superclass concept $Parent$ with children set $C_s$ we can choose\n$e(Parent) = \\frac{\\text{mean}\\; e(child)}{Child \\in C_s}$  (1)\n(d) Semantic interpolatability: Consider a latent representation $v$ that is close to or inbetween (wrt. linear interpolation) some embeddings $e(C_i)$ and $e(C_j)$. We assume that $v$ can be interpreted to correspond to some concept, i.e., $\\exists c \\in C: ||e(c) - v ||_2 < \\epsilon$ for some admissible error $\\epsilon$. This is needed to make the averaging in the parent identification in (1) above meaningful.\nNote that Assumption 1(d) is very strong, stating that there is a correspondence between the semantic relations of natural language concepts, and the metric space structure of latent spaces. This is by no means guaranteed, but according to findings in word vector spaces [32] and also image model latent spaces [16] a viable assumption for the structure of learned semantics in DNNs.\nStep 1: Obtain the embeddings $e(c_i)$. We here leverage the text-to-image alignment to directly define the concept-to-vector mapping $e$: $e(c) := \\text{mean}_{x \\in toDNNInput(c)} F^{\\rightarrow L}(x)$. Following [59, 62], the $toDNNInput$ function can be a mapping from concept to a single textual description [62] or to a set of representative images [59].\n* Textual concepts: The naive candidate for a textual description $toDNNInput(c) := toText (c)$. However, some additional prompt engineering may be necessary, i.e., manual adjustment and finetuning of the formulation [17, 45]. For example, following [45] we replace \"c\" by \"an image of c\" for the prompting.\n* Visual concepts: Here we take the graphical $to Images(c)$ specification of our concept. One could then employ standard supervised c-XAI techniques to find a common representing vector for the given images, e.g., as the weights of a linear classifier of the concept's presence [16, 27]. We here instead simply feed the DNN with each of the images and capture its respective intermediate latent representations, which is valid due to the concentricity assumption.\nIf the text-to-image alignment is low, we found image representations of concepts to yield more meaningful results.\nStep 2: Hierarchical clustering. Employ any standard hierarchical agglomerative clustering technique to find a hierarchy of partitionings of the set of given concept embeddings. Each partitioning level represents one level of superclasses, with one cluster per class (see the simple example in Figure 1). As of (1), the mean of the cluster's embedding vectors is the embedding of its corresponding superclass (the cluster center).\nNote that the hierarchical clustering in principle allows to: (a) start off with more than one vector per leaf concept, e.g., coming from several image representations or from jointly using embeddings from textual and image representations; (b) weight the contribution of each child to the parent. This, however, is only viable together with means to automatically determine the weights, and not further pursued here.\nStep 3: Decoding of cluster centers. We here use a two-step search approach to assign each cluster center a concept from the concept bank $C$. Given a cluster center $p$, the first optional step is to reduce the search space by selecting a subset of candidate concepts from $C$. Following [62], (1a) we collect for every leaf concept $c$ the set of those concepts that, according to the ConceptNet knowledge graph [53], are related to $c$ by any of the relations in $R_{concepts} = \\{hasA, isA, partOf, HasProperty, MadeOf\\}$:\n$Parents(C) := \\{P | \\underset{R \\in R_{concepts}}{R(P, C)}\\} .$ (2)\n(1b) The union $P = \\underset{c \\text{ leaf in cluster}}{\\cup} Parents(c)$ of these sets serves as candidate set for $p$. Note that this is a simplification that allows to capture as superclass any best fitting commonality between the leaf concepts (e.g., background context like indoor or biological relation like mammal for $\\{cat, dog\\}$ as in Figure 3). Generally, there is a trade-off between very specific relation definitions, and fidelity to the learned knowledge of the model. The trade-off can be controlled by the broadening or narrowing of the candidate set. The here chosen broad definition of the IsSuperClass relationship between concepts favors fidelity to the model's learned knowledge. Investigating effects of more narrow concept candidate sets is future work. (2) In the second step, the concept for $p$ is then selected from the candidate set $P$ to be the one with minimum distance embedding (embeddings again obtained as in Step 1): $e^{-1}(p) := \\text{argmin}_{p \\in P} ||p - e(p)||_2$.\nThe final result then is a hierarchy tree, where leaf nodes are the originally provided concepts, inner nodes are the newly extracted superclasses, and the connections represent the $IsSuperclassOf$ relation. In the experimental section we will more closely investigate the influence of the proposed variants with/without prompt engineering and with/without finetuning."}, {"title": "4.2 Inference of an ontology", "content": "The such obtained ontology can be used for outlier-aware inference, i.e., classification of new input samples to one of the leaf concepts. This will be useful not only as an interesting standalone application in safety-relevant classification scenarios, but in particular for the validation.\nThe baseline of the inference is the k-nearest neighbor classifier: It directly compares the latent representation of a new input with each available concept embedding; and then assigns the majority vote of the k nearest concept embeddings. To enrich the inference process with information from the ontology, one instead traverses the ontology tree, at each node branching off towards the closest child node.\nRemark 1. Note that this allows to easily insert an outlier criterion: If at a parent class $P$ none of the children nodes is closer than a threshold, the sample is considered an outlier of class $P$. This neatly preserves the maximum amount of information available about the properties of the sample, and, thus, eases subsequent handling of the unknown input. For example, an outlier of (parent-)class StaticObject should be treated differently than one of (parent-)class Animal.\nHyperparameters of this inference procedure are the choice of similarity, including whether to take into account the size (variance/width) of the cluster, e.g., by favoring wide over near-to-point-estimate clusters; and the threshold for being an outlier."}, {"title": "4.3 Validating and comparing learned ontologies", "content": "We now get to the core goal of this paper: Verify or validate a given DNN using QR. For this we start with validation of an extracted ontology from subsection 4.1, and discuss how to measure its fidelity to DNN learned knowledge, and alignedness to human prior knowledge, which here corresponds to the expected image-to-concept matching. Lastly, we show how one can encode a given ontology as contextualized embeddings to verify a DNN against given prior knowledge from QR.\nHuman-alignedness. One main desirable of a DNN's ontology is that it well aligns with the semantics that humans would expect and apply for the respective task. Any mismatch may either bring insights to the human on alternative solutions, or, more probably, indicates a suboptimal solution or even Clever Hans effect of the learned representations. A straight-forward way to measure the human-alignedness is to test the prediction accuracy of the ontology when used for inference (see subsection 4.2) on human-labeled samples. If human labels deviate often from the predictions, this indicates a bad alignment of the semantics the DNN has learned for the concepts from those a human would expect. Other means to estimate the human-alignedness (not yet investigated in this work) are direct qualitative user studies, where human evaluators manually check the consistency of the obtained ontology tree with their own mental model; or automatic checking of consistency against given world knowledge or common sense ontologies like Cyc [30]. Lastly, the improvement in humans' predictions about the behavior of the model, a typical human-grounded XAI metric [50], could quantify in how far humans can make sense of the ontology.\nA different aspect of human-alignedness is how well the ontology, in particular the inference scheme it defines, generalizes to novel concepts (semantic outliers) that so far have not occurred in leaves or nodes. The gerenalization can be measured as the performance in assigning a correct parent node. A special case here are blended cases where the novel concept unifies features of very different classes, such as a cat with wheel as walking support. The uncertainty of the model in such blended cases can be qualitatively compared against human one, potentially uncovering a bias.\nText-to-image alignment. The to-be-expected performance of cross-modal inference of the ontology (i.e., ontology defined using textual concepts, but inference done on images) directly depends on the quality of the text-to-image alignment. This motivates a use as an indicator for suboptimal text-to-image alignment.\nFidelity. Fidelity of the ontology, respectively shortcomings in the simplified modeling of the ontology, can be measured by the deviation between the baseline inference directly on the leaves, and the ontology inference. Inference on the leaf concepts $c_i$ means we predict for an image $x$ the output class $c$ for which the textual embedding is closest to the embedding of $x$, proximity measured with respect to some distance $d$ (here: cosine similarity):\n$c := \\text{argmin}_{c \\in (C_i)_i} d (F^{\\rightarrow L}(toText(c), F^{\\rightarrow L}(x)))$ (3)\nThis is referred to as naive zero-shot approach, following research on using foundation models on specialized tasks without finetuning (=with training on zero samples) [17, 45]. The reason to choose this as a baseline is that the ideal tree should sort samples into the same leaf neighborhood as direct distance measurement would do. Simplifications that may infringe this equality are unequal covariances (\u2248 widths) of sibling class clusters; the chosen similarity measure; or assuming perfect text-to-image alignment.\nVerification against a given ontology. The previous extraction techniques yield an inspectable representation of the ontology learned by a model. This allows manual validation of the learned knowledge against models from QR. Alternatively, one could directly verify a multimodal model against consistency with a given ontology: In short, we propose to modify the leaf concept embeddings from Step 1 such that they additionally encode their local part of the ontology, i.e., information about all desired parents of the leaf, as context. One can then measure the performance of naive inference (see subsection 4.2) on these contextualized leaf nodes as defined in (3). A higher performance then means a better alignment of the context of a leaf concept with its image representations. This even would allow to narrow down unalignedness to specific concepts (those with bad inference results). We suggest as point of attack for contextualization is the textual encoding: Let $c$ be a leaf concept at depth $d$ in the tree with chain of parents $(P_i)_{i=1}^d$ from root to leaf. We can now follow [17] and modify the original $t_T = toText$ function of a leaf concept to:\n$toText'(c) := \\text{``}t_T(P_1),...,t_T(P_d), t_T(c)\\text{''}$  (4)"}, {"title": "5 Experiments", "content": "5.1 Settings\nModels under test. In our experiments, we utilized CLIP [45], one of the first multimodal foundation model family accepting both text and images [13]. For text-to-image alignment CLIP was trained to map an image and its corresponding text descriptions onto a similar (with respect to cosine similarity) latent space representation. This general-purpose model captures rich semantic information, and achieves impressive performance compared to task-specific models across various applications, including image captioning [7, 12], recognition of novel unseen objects [5], and retrieval tasks [6, 54]. This makes it a common choice as basis for training or distilling more specialized models [12, 13], and thus a highly interesting target for validation and verification of its learned knowledge and internalized QR. In our experiments, we explored various CLIP backbones, including ResNet-50, as well as Vision Transformer (ViT) variants featuring different patch sizes and model capacities (e.g., ViT-B/32, ViT-L/14).\nDataset. The CIFAR-10 dataset [4, Chap. 3] is a benchmark in the field of computer vision, consisting of 60,000 32\u00d732 color images, split into 50,000 training and 10,000 test images. The images are equally distributed onto the 10 diverse classes airplane, ship, car, truck, bird, cat, dog, deer, horse, frog. The choice of classes suits our initial study well, as they both exhibit pairs of semantically similar objects (e.g., car, truck), as well as mostly unrelated ones (e.g., car, cat), so we can expect a deep class hierarchy. In our study, we conduct inference both of the baseline (naive zero-shot) and the proposed method on the CIFAR-10 test dataset [4].\nFidelity baseline. As discussed in subsection 4.3, the inference on the leaf concepts (naive zero-shot approach) serves as baseline (maximum performance) for fidelity measurements. The closer the tree inference gets to the naive zero-shot performance, the higher the fidelity. We here choose as distance metric the cosine distance $\\text{CosDist}(a, b) := 1 - \\frac{a \\cdot b}{||a|| \\cdot ||b||}$ (0 for $a, b$ parallel, 1 for orthogonal, 2 for $a = b$), going along with the training of CLIP.\nMetrics. Any quantitative classification performances are measured in terms of accuracy of the results on CIFAR-10 test images against their respective ground truth label."}, {"title": "5.2 Ablation Study: Influences on Human-Alignedness and Fidelity of Ontology Extraction", "content": "As detailed in subsection 4.3", "ontology": "n* Affinity: Affinity typically influences which data points are most similar", "Linkage": "This parameter determines the criterion used to merge clusters during the hierarchical clustering process", "similarity": "We use use the same choices as for affinity. Next, we compare different settings for obtaining the leaf embeddings. The following variants are considered"}]}