{"title": "HAMSTER: HIERARCHICAL ACTION MODELS FOR OPEN-WORLD ROBOT MANIPULATION", "authors": ["Yi Li", "Yuquan Deng", "Jesse Zhang", "Joel Jang", "Marius Memmel", "Raymond Yu", "Caelan Garrett", "Fabio Ramos", "Dieter Fox", "Anqi Li", "Abhishek Gupta", "Ankit Goyal"], "abstract": "Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is the lack of robotic data, which are typically obtained through expensive on-robot operation. A promising remedy is to leverage cheaper, \"off-domain\u201d data such as action-free videos, hand-drawn sketches or simulation data. In this work, we posit that hierarchical vision-language-action (VLA) models can be more effective in utilizing off-domain data than standard monolithic VLA models that directly finetune vision-language models (VLMs) to predict actions. In particular, we study a class of hierarchical VLA models, where the high-level VLM is finetuned to produce a coarse 2D path indicating the desired robot end-effector trajectory given an RGB image and a task description. The intermediate 2D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Doing so alleviates the high-level VLM from fine-grained action prediction, while reducing the low-level policy's burden on complex task-level reasoning. We show that, with the hierarchical design, the high-level VLM can transfer across significant domain gaps between the off-domain finetuning data and real-robot testing scenarios, including differences on embodiments, dynamics, visual appearances and task semantics, etc. In the real-robot experiments, we observe an average of 20% improvement in success rate across seven different axes of generalization over OpenVLA, representing a 50% relative gain. Visual results are provided at: https://hamster-robot.github.io/", "sections": [{"title": "1 INTRODUCTION", "content": "Developing general robot manipulation policies has been notoriously difficult. With the advent of large vision-language models (VLMs) that display compelling generalization capabilities, there is optimism that the same recipe is directly applicable to robot manipulation. A line of prior work (Brohan et al., 2023a; Kim et al., 2024; Black et al., 2024) builds open-world vision-language-action models (VLAs) by finetuning off-the-shelf pretrained VLMs to directly produce robot actions. These VLA models, which we refer to in this work as monolithic VLA models, rely crucially on large robotics datasets, complete with on-robot observations, e.g., images and proprioceptive states, and actions. However, on-robot data is expensive, since end-to-end observation-action pairs are typically collected on the robot hardware through, e.g., teleoperation. Despite recent community-wide efforts in building large-scale robotics datasets (Collaboration et al., 2023; Khazatsky et al., 2024), the size, quality, and diversity of existing robotics datasets are still limited, and monolithic VLA models have yet to demonstrate emergent capability comparable to VLMs and LLMs in other domains of study. Moreover, monolithic VLA models are constrained by their inference frequency to achieve dexterous and dynamic manipulation tasks (Brohan et al., 2023a; Kim et al., 2024).\nOn the other hand, relatively small robot policy models have shown impressive dexterity and robust-ness. Such models have demonstrated promise across a range of complex tasks involving contact-rich manipulation and 3D reasoning, spanning domains from tabletop manipulation (Shridhar et al.,"}, {"title": "2 RELATED WORK", "content": "LLMs and VLMs for robotics. Early attempts in leveraging LLMs and VLMs for robotics are through pretrained language (Jang et al., 2022; Shridhar et al., 2023; Singh et al., 2023) and visual (Shah & Kumar, 2021; Parisi et al., 2022; Nair et al., 2023; Ma et al., 2023) representations. However, these are not sufficient for complex semantic reasoning and generalization to the open world (Brohan et al., 2022; Zitkovich et al., 2023). Recent research has focused on directly leveraging open world reasoning and generalization capability of LLMs and VLMs, by prompting or fine-tuning them to, e.g., generate plans (Huang et al., 2022; 2023b; Lin et al., 2023; Liang et al., 2023; Singh et al., 2023; Brohan et al., 2023b), construct value (Huang et al., 2023a) and reward functions (Kwon et al., 2023; Sontakke et al., 2023; Yu et al., 2023; Ma et al., 2024; Wang et al., 2024). Our work is more closely related to the literature on VLA models, summarized below.\nMonolithic VLA models as language-conditioned robot policies. Monolithic VLA models have been proposed to produce robot actions given task description and image observations directly (Brohan et al., 2022; Jiang et al., 2023; Zitkovich et al., 2023; Team et al., 2024; Kim et al., 2024; Radosavovic et al., 2023). Monolithic VLA models are often constructed from VLMs (Liu et al., 2024c; Bai et al., 2023; Driess et al., 2023; Lin et al., 2024), and are trained on large-scale on-robot data (Brohan et al., 2022; Collaboration et al., 2023; Khazatsky et al., 2024) to predict actions as text or special tokens. However, due to the lack of coverage in existing robotics datasets, they must be finetuned in-domain on expensive on-robot data. Their action frequency is also constrained by inference frequency, limiting their capability to achieve dexterous and dynamic tasks. The most relevant monolithic VLA model to our work is LLARVA (Niu et al., 2024), which predicts end-effector trajectories in addition to robot actions. However, LLARVA does not use trajectory prediction to control the robot; rather, it uses it as an auxiliary task to improve action prediction. Therefore, LLARVA still suffers from the limitations of monolithic VLA models. In contrast, our work takes a hierarchical approach, enabling us to use specialist lower-level policies that take in additional inputs the VLMs cannot support, such as 3D pointclouds, to enable better imitation learning. Our predicted paths then enable these lower-level policies to generalize more effectively.\nVLMs for predicting intermediate representations. Our work bears connections to prior methods using vision-language models to predict intermediate representations. These methods can be categorized by the choice of predicted representations:\nPoint-based predictions: A common intermediate prediction interface has been keypoint affor-dances (Stone et al., 2023; Sundaresan et al., 2023; Nasiriany et al., 2024b; Yuan et al., 2024; Kuang et al., 2024). Keypoint affordances can be obtained through using open-vocabulary detectors (Min-derer et al., 2022), iterative prompting of VLMs (Nasiriany et al., 2024b), or fine-tuning detectors to identify certain parts of an object by semantics (Sundaresan et al., 2023). Perhaps most related to our work, Yuan et al. (2024) finetune a VLM to predict objects of interest as well as free space for placing an object, and Liu et al. (2024a) propose a mark-based visual prompting procedure to predict keypoint affordances as well as a fixed number of waypoints. As opposed to these, our work"}, {"title": "3 BACKGROUND", "content": "Imitation Learning via Supervised Learning. Imitation learning trains a policy $\\pi_{\\rho}(a \\mid s,o, z)$ from expert demonstrations, where $s$ denotes proprioceptive inputs, $o$ includes perceptual observations (e.g., RGB images, depth), and $z$ provides task instructions. Given an expert dataset $D = \\{(s_i, o_i, z_i, a_i)\\}_{i=1}^N$, the policy is optimized via maximum likelihood estimation, maximizing $E_{(s_i,o_i,z_i,a_i)\\sim D} [log \\pi_{\\theta} (a_i \\mid s_i, o_i, z_i)]$. Despite advancements in architectures such as 3D policy representations (Goyal et al., 2023; Ke et al., 2024), generalizing to novel semantic or visual variations remains challenging. In this paper, we explore how VLMs can enhance imitation learning models for better generalization.\nVision-Language Models. VLMs (Lin et al., 2024; Liu et al., 2024c) are large transformer models (Vaswani et al., 2023) that accept both vision and text tokens to generate text responses. They are pre-trained on extensive multimodal datasets (Zhu et al., 2023; Byeon et al., 2022) and later fine-tuned on high-quality, task-specific data (Shen et al., 2021; Lu et al., 2022). By tokenizing each modality into a shared space, these models autoregressively produce sequences of text tokens conditioned on an image and prior tokens. In our work, we assume access to such a pre-trained, text-and-image VLM (Lin et al., 2024; Liu et al., 2024c), further fine-tuned via a supervised loss that minimizes the negative log-likelihood of the target tokens."}, {"title": "4 HAMSTER: HIERARCHICAL ACTION MODELS FOR ROBOTIC LEARNING", "content": "In this work, we examine how VLA models can leverage relatively abundant data and demonstrate cross-domain transfer capabilities, as opposed to relying purely on expensive observation-language-action data collected on a robot. HAMSTER is a family of hierarchical VLA models designed for this purpose, exhibiting generalizable and robust manipulation. It consists of two interconnected models: first, a higher-level VLM that is finetuned on large-scale, off-domain data to produce intermediate 2D path guidance (detailed in Section 4.1), and second, a low-level policy that produces actions conditioned on 2D paths (detailed in Section 4.2)."}, {"title": "4.1 HAMSTER'S VLM FOR PRODUCING 2D PATHS TRAINED FROM OFF-DOMAIN DATA", "content": "The high-level VLM of HAMSTER predicts a coarse 2D path $p$ to achieve the task given a monocular RGB image $img$ and language instruction $z$, i.e., $p \\sim VLM(img, z)$. The 2D path $p$ describes a coarse trajectory of the robot end-effector, or human hand in the case of human videos, on the input camera image. It also contains information about the gripper state. Formally, the 2D path is defined as $p = [(x_t, y_t, gripper\\_open_t)]_t$ where $x_t, y_t \\in [0,1]$ are normalized pixel locations of the end effector's (or hand) position at step $t$, and $gripper\\_open_t$ is a binary value indicating the gripper state, i.e., open and close.\nAlthough, any pretrained text-and-image-input VLM (Lin et al., 2024; Liu et al., 2024c; Achiam et al., 2023) can be used to predict such a 2D path by casting an appropriate prompt, we find that pretrained VLMs struggle with predicting such a path in a zero-shot manner (see Table 5). Therefore, we finetune pre-trained VLMs on datasets that ground VLMs to robot scenes and path predictions collected from easier-to-obtain sources, i.e., internet visual-question-answering data, robot data from other modalities, and simulation data. This is in contrast to work such as Gu et al. (2023), where pre-trained VLMs are tasked with directly performing spatially relevant path generation.\nWe use VILA-1.5-13b (Lin et al., 2024) as our base VLM, a 13-billion-parameter vision language model trained on interleaved image-text datasets and video captioning data. Although it is possible to curate a dataset on path prediction $\\{(img_i, z_i, p_i)\\}$ and train the VLM only on the dataset, the literature (Brohan et al., 2023a; Yuan et al., 2024) has shown that co-training the VLM on a variety of relevant tasks, all framed as VQA tasks, can help retain the VLM's generalization capability. To this end, we curate a multi-domain dataset to finetune this model for effective 2D path prediction."}, {"title": "4.1.1 FINETUNING OBJECTIVE AND DATASETS.", "content": "Predicting the 2D path of the end-effector requires understanding what objects to manipulate in a given task in terms of their pixel positions, but also reasoning about how a robot should perform the task. To enable this understanding, we collate a diverse off-domain dataset $D_{off}$ from a wide range of modalities, including real-world data, visual question-answering data, and simulation data. Importantly, none of this off-domain data used to train the VLM comes from the deployment environment, thereby emphasizing generalizability.\nWe assemble a dataset $D_{off} = \\{(img_i, z_i, ans_i)\\}_{i=1}^M$ of image inputs $img_i$, language prompts $z_i$, and answer $ans_i$ consisting of three types of off-domain data: (1) pixel point prediction tasks (what); (2) simulated robotics tasks (what and how); (3) a real robot dataset consisting of trajectories (what and how). We detail each dataset below; see Figure 3 for visualization of each dataset's prompts and corresponding answers."}, {"title": "4.2 PATH GUIDED LOW-LEVEL POLICY LEARNING", "content": "The low-level policy of HAMSTER $\\pi_{\\theta}(a \\mid s, o, z, p)$ is conditioned on proprioceptive and perceptive observations, (optional) language instruction and, importantly, 2D path. While a low-level control policy can learn to solve the task without 2D path, the paths allow the low-level policy to forgo long-horizon and semantic reasoning and focus on local and geometric predictions to produce robot actions. As we find empirically (see Figure 4), 2D paths allow for considerably improved visual and semantic generalization of low-level policies.\nHAMSTER's general path-conditioning framework allows lower-level policies to take in proprio-ceptive and perceptual (e.g., depth images) observations, that are not input to the high-level VLM. We consider low-level policies based on 3D perceptual information, i.e., $o = (img, pointcloud)$, available at test time on a robotic platform with standard depth cameras. We study two choices of policy architecture, RVT-2 (Goyal et al., 2024) and 3D-DA (Ke et al., 2024) which has shown state-of-the-art results on popular robot manipulation benchmark (James et al., 2020).\nConditioning on Paths. Most policy architectures use the form $\\pi_{\\theta}(a \\mid s, o, z)$ without 2D path inputs. One na\u00efve option is to concatenate the path with proprioceptive or language inputs. However, because 2D paths vary in length, the architecture must handle variable-length inputs. To incorporate the 2D path $p$ from the VLM without major modifications, we alternatively overlay the 2D path onto the image observation (Gu et al., 2023). Our implementation follows this approach by drawing colored trajectories on all images in the trajectory of, . . ., of: points at each $(x_t, y_t)$ are connected with line segments using a color gradient to indicate temporal progression (see Figure 2(b)), and circles mark changes in gripper status (e.g., green for closing, blue for opening). If the policy architecture allows images with more than three channels, we can also include path drawing as separate channels, instead of overlaying it on the RGB channel. We empirically study both drawing strategies, overlay and concatenating channels, in section 5.3.\nPolicy Training. To train the policy, we collect a relatively small-scale task-specific dataset $D = \\{(s_i, o_i, z_i, a_i)\\}_{i=1}^N$ on the robot hardware. During training, we use oracle 2D paths constructed by proprioception projection, similar to how the 2D paths are constructed for the VLM training data, and construct path-labeled dataset $D_{path} = \\{(s_i, o_i, z_i, p_i, a_i)\\}_{i=1}^N$. We train a pol-icy $\\pi_{\\theta}(a \\mid s,o,z,p)$ with standard supervised imitation learning objectives on $D_{path}$ to maximize the log-likelihood of the dataset actions: $E_{(s_i,o_i,z_i,p_i,a_i)\\sim D_{path}} log \\pi_{\\theta}(a_i \\mid s_i, o_i, z_i, p_i)$. For further implementation details, see Appendix B.\nInference Speed. Monolithic VLAs query the VLM at every action step (Kim et al., 2024; Brohan et al., 2023a), which can be very expensive with large VLMs. For example, OpenVLA's 7B-parameter VLA only runs at 6Hz on an RTX 4090 (Kim et al., 2024). Instead, HAMSTER's hierarchical design allows us to query the VLM only one or few times during an episode to generate 2D paths $p$ that can be followed by low-level policy for multiple steps. Therefore, HAMSTER can be scaled to large VLM backbones without needing end-users to be concerned about inference speed."}, {"title": "5 EXPERIMENTAL EVALUATION", "content": "We evaluate our approach in both simulation and real-world experiments to the following key ques-tions. Do hierarchical VLAS:\nQ1 Generalize behaviors to unseen scenarios with significant visual and semantic variation?\nQ2 Achieve stronger cross-domain generalization than monolithic VLAs and low-level imita-tion learning methods?\nQ3 Facilitate learning of non-prehensile and long-horizon tasks?\nQ4 Exhibit strong demonstration efficiency?\nQ5 Have improved visual + semantic reasoning due to hierarchy and VLM fine-tuning?"}, {"title": "5.1 REAL WORLD EVALUATION ON TABLETOP MANIPULATION", "content": "To answer Q1, our real-world evaluation experiments aim to test the generalization capability of hierarchical VLA models across significant semantic and visual variations. In particular, we consider"}, {"title": "6 CONCLUSION AND LIMITATIONS", "content": "In summary, HAMSTER studies the potential of hierarchical VLA models, achieving robust gener-alization in robotic manipulation. It consists of a finetuned VLM that accurately predicts 2D paths for robotic manipulation and a low-level policy that learns to generate actions using the 2D paths. This two-step architecture enables visual generalization and semantic reasoning across considerable domain shifts, while enabling data-efficient specialist policies, like ones conditioned on 3D inputs, to perform low-level action execution.\nThis work represents an initial step towards developing versatile, hierarchical VLA methods, with numerous opportunities for future improvement and expansion. The proposed work only generates points in 2D space, without making native 3D predictions. This prevents the VLM from having true spatial 3D understanding. Moreover, the interface of just using 2D paths is a bandwidth limited one, which cannot communicate nuances such as force or rotation. In the future, investigating learnable intermediate interfaces is a promising direction. Moreover, training these VLMs directly from large-scale human video datasets would also be promising."}, {"title": "A VLM FINETUNING DATASET DETAILS", "content": "Pixel Point Pred Data. Our point prediction dataset comes from Robopoint (Yuan et al., 2024). 770k samples in our point prediction dataset contain labels given as a set of unordered points such as p\u00ba = [(0.25, 0.11), (0.22, 0.19), (0.53, 0.23)], or bounding boxes in [(cx, cy, w, h)] style. Other than that, following Robopoint (Yuan et al., 2024), we use the VQA dataset Liu et al. (2024b) with 660k samples which answer VQA queries in natural language such as \"What is the person feeding the cat?\" We keep these data as is because these VQA queries are likely to benefit a VLM's semantic reasoning and visual generalization capabilities; we fine-tune HAMSTER'S VLM on the entire Robopoint dataset as given.\nSimulation Data. We selected 81 RLBench tasks out of 103 to generate data by removing tasks with poor visibility on the front cam view in RLBench. We use the first image in each episode combined with each language instruction. The final dataset contains around 320k trajectories.\nReal Robot Data. For the Bridge (Walke et al., 2023) dataset, which only provides RGB images, we extract trajectories by iteratively estimating the extrinsic matrix for each episode. In each scene, we randomly sample a few frames and manually label the center of the gripper fingers. Using the corresponding end-effector poses, we compute the 3D-2D projection matrix with a PnP (Perspective-n-Point) approach. We then apply this projection matrix to the episodes and manually check for any misalignments between the projected gripper and the actual gripper. Episodes exhibiting significant deviations are filtered out, and a new round is started to estimate their extrinsic matrix.\nFor DROID (Khazatsky et al., 2024), a large portion of the dataset contains noisy camera extrinsics information that do not result in good depth alignment. Therefore, we filter out trajectories with poor-quality extrinsics as measured by the alignment between the projected depth images and the RGB images. This results in ~45k trajectories (~22k unique trajectories as trajectories each have 2 different camera viewpoints) which we use for constructing the VLM dataset Doff as described in Section 4.1."}, {"title": "B IMPLEMENTATION AND ARCHITECTURE DETAILS", "content": "B.1 VLM IMPLEMENTATION DETAILS\nVLM Prompt. We list the prompt for both fine-tuning on sim and real robot data and evaluation in Figure 10. We condition the model on an image and the prompt, except when training on Pixel Point Prediction data (i.e., from Robopoint (Yuan et al., 2024)) where we used the given prompts from the dataset. Note that we ask the model to output gripper changes as separate language tokens, i.e., Open Gripper/Close Gripper, as opposed to as a numerical value as shown in simplified depictions like Figure 2."}, {"title": "B.2 Low-LEVEL POLICY TRAINING DETAILS", "content": "We train RVT2 (Goyal et al., 2024) and 3D-DA (Ke et al., 2024) as our lower-level policies. We keep overall architecture and training hyperparameters the same as paper settings. Specific details about how the inputs were modified other than the 2D path projection follow.\nFor low-level policy training, we train the policies on ground truth paths constructed by projecting trajectory end-effector points to the camera image. In order to also ensure the policies are robust to possible error introduced by HAMSTER VLM predictions during evaluation, we add a small amount of random noise (N(0, 0.01)) to the 2D path (x, y) image points during training to obtain slightly noisy path drawings. No noise was added to the gripper opening/closing indicator values.\nRVT2 (Goyal et al., 2024). We remove the language instruction for RVT-2 when conditioning on HAMSTER 2D paths.\n3D-DA (Ke et al., 2024). In simulated experiments in Colosseum, no changes were needed. In fact, we saw a performance drop for HAMSTER+3D-DA when removing language for Colosseum tasks and a small drop in performance when using simplified language instructions. This is likely due to 3D-DA's visual attention mechanism which cross attends CLIP language token embeddings with CLIP visual features, therefore detailed language instructions are beneficial.\nIn real-world experiments, we simplify the language instruction in the same way as for RVT2 when conditioning on HAMSTER 2D paths to encourage following the trajectory more closely with limited data. In addition, we reduced the embedding dimension of the transformer to 60 from 120, removed proprioception information from past timesteps, and reduced the number of transformer heads to 6 from 12 in order to prevent overfitting."}, {"title": "CREAL WORLD EXPERIMENT DETAILS", "content": ""}, {"title": "C.1 TRAINING TASKS AND DATA COLLECTION", "content": "For our real-world experiments, we collected all data using a Franka Panda arm through human teleoperation, following the setup described in Khazatsky et al. (2024). Below, we describe the training tasks:\nPick and place. We collected 220 episodes using 10 toy objects. In most of the training data, 2 bowls were placed closer to the robot base, while 3 objects were positioned nearer to the camera. The language goal for training consistently followed the format: pick up the object} and put it in the {container}.\nKnock down objects. We collected 50 episodes with various objects of different sizes. Typically, 3 objects were arranged in a row, and one was knocked down. The language goal for training followed the format: push down the {object}.\nPress button. We collected 50 episodes with 4 colored buttons. In each episode, the gripper was teleoperated to press one of the buttons. The language goal followed the format: press the {color} button.\nWhen training RVT2, which requires keyframes as labels, in addition to labeling frames where the gripper performs the open gripper and close gripper actions, we also included frames that capture the intermediate motion as the gripper moves toward these keyframes."}, {"title": "C.2 BASELINE TRAINING DETAILS", "content": "OpenVLA (Kim et al., 2024). Following Kim et al. (2024), we only utilize parameter efficient fine-tuning (LoRA) for all of our experiments, since they showed that it matches full fine-tuning performance while being much more efficient. We follow the recommended default rank of r=32. We opt for the resolution of 360 x 360 to match all of the baseline model's resolutions. We also follow the recommended practice of training the model until it surpasses 95% token accuracy. However, for some fine-tuning datasets, token accuracy converged near 90%. We selected the model checkpoints when we observed that the token accuracy converged, which usually required 3,000 to 10,000 steps using a global batch size of either 16 or 32. Training was conducted with 1 or 2 A6000 gpus (which determined the global batch size of 16 or 32). Emprically, we observed that checkpoints that have converged showed very similar performance in the real world. For example, when we evaluate checkpoint that was trained for 3,000 steps and showed convergence, evaluating on a checkpoint trained for 5,000 steps of the same run resulted in a very similar performance.\nRT-Trajectory (Gu et al., 2023). We implement two versions of RT-Trajectory for the comparison in Table 5. The first (0-shot GPT-40) directly uses GPT-4o to generate 2D paths with a prompt very similar to the one we use for HAMSTER, displayed in Figure 11.\nThe second version implements RT-Trajectory on top of a Code-as-Policies (Liang et al., 2023), as described in RT-Trajectory. We use OWLv2 (Minderer et al., 2023) to perform open-vocabulary object detection on the image to generate a list of objects as the scene description and then prompt RT-Trajectory with the prompt shown in Figure 12. We also use GPT-40 as the backbone for this method."}, {"title": "C.3 EVALUATION TASKS", "content": "We evaluate our method on the tasks of pick and place, knock down object, and press button across various generalization challenges, as illustrated in Figure 4. Detailed results are available in Table 4. Following (Kim et al., 2024), we assign points for each successful sub-action. For VLM, human experts are employed to assess the correctness of the predicted trajectories."}, {"title": "D.1 IMPACT OF DESIGN DECISIONS ON VLM PERFORMANCE", "content": "To better understand the transfer and generalization performance of the proposed hierarchical VLA model, we analyze the impact of various decisions involved in training the high-level VLM. We con-duct a human evaluation of different variants of a trained high-level VLM on a randomly collected dataset of real-world test images, as shown in Figure 8. We ask each model to generate 2D path traces corresponding to instructions such as \u201cmove the block on the right to Taylor Swift\" or \"screw the light bulb in the lamp\u201d (the full set is in Appendix D.2). We then provide the paths generated by each method to human evaluators who have not previously seen any of the models' predictions. The human evaluators then rank the predictions for each method; we report the average rank across the samples in Table 5.\nWe evaluate the following VLM models: (1) zero-shot state-of-the-art closed-source models such as GPT-40 using a similar prompt to ours (shown in Figure 11), (2) zero-shot state-of-the-art closed-source models such as GPT-40 but using Code-as-Policies (Liang et al., 2023) to generate paths as described in Gu et al. (2023) (prompt in Figure 12), (3) finetuned open-source models (VILA-1.5-13b) on the data sources described in Section 4.1, but excluding the simulation trajectories from the RLBench dataset, (4) finetuned open-source models (VILA-1.5-13b) on the data sources described in Section 4.1, including path sketches from the RLBench dataset. The purpose of these evaluations is to first compare with closely related work that generates 2D trajectories using pretrained closed source VLMs Gu et al. (2023) (Comparison (1) and (2)). The comparison between (3) and (4) (our complete method) is meant to isolate the impact of including the simulation path sketches from the RLBench dataset. In doing so, we analyze the ability of the VLM to predict intermediate paths to transfer across significantly varying domains (from RLBench to the real world).\nThe results suggest that: (1) zero-shot path generation, even from closed-source VLMs Gu et al. (2023) such as GPT-40 with additional help through Code-as-Policies (Liang et al., 2023), under-performs VLMs finetuned on cross-domain data as in HAMSTER; (2) inclusion of significantly different training data such as low-fidelity simulation during finetuning improves the real-world per-"}, {"title": "D.2 VLM REAL WORLD GENERALIZATION STUDY", "content": "The full list of task descriptions for this study is below (see Appendix D.1 for the main experiment details). Duplicates indicate different images for the same task. We plot some additional comparison examples in Figure 13. Note that the path drawing convention in images for this experiment differ from what is given to the lower-level policies as described in Section 4.2 as this multi-colored line is easier for human evaluators to see.\n1. screw in the light bulb on the lamp\n2. screw in the light bulb on the lamp\n3. screw in the light bulb on the lamp\n4. screw out the light bulb and place it on the holder\n5. screw out the light bulb and place it on the holder\n6. screw in the light bulb\n7. screw in the light bulb on the lamp\n8. move the blue block on Taylor Swift\n9. pick up the left block and put it on Jensen Huang\n10. move the block on the right to Taylor Swift\n11. place the yellow block on Kobe\n12. pick up the blue block and place it on Jensen Huang\n13. move the red block to Kobe\n14. press the button on the wall\n15. press the button to open the left door\n16. press the button to open the right door\n17. open the middle drawer\n18. open the bottom drawer\n19. open the top drawer\n20. open the middle drawer\n21. open the bottom drawer\n22. press the button\n23. press the button\n24. press the orange button\n25. press the orange button with black base\n26. press the button\n27. pick up the SPAM and put it into the drawer\n28. pick up the orange juice and put it behind the red box\n29. pick up the tomato soup and put it into the drawer\n30. pick up the peach and put it into the drawer\n31. move the mayo to the drawer\n32. move the dessert to the drawer\n33. pick up the object on the left and place it on the left\n34. pick up the fruit on the left and put it on the plate\n35. pick up the milk and put it on the plate\n36. press the button with the color of cucumber, then press the button with color of fire\n37. press the button with color of banana\n38. press the button with color of leaf\n39. press the button with color of leaf, then press the one with color of banana\n40. press left button\n41. pick up the left block on the bottom and stack it on the middle block on top\n42. make I on top of C\n43. put number 2 over number 5\n44. stack block with lion over block with earth\n45. pick up the left block on the bottom and stack it on the middle block on top\n46. stack the leftest block on the rightest block\n47. stack the block 25 over block L\n48. put the left block on first stair"}, {"title": "D.3 HUMAN RANKING", "content": "Due to the variety of possible trajectories that accomplish"}]}