{"title": "Should You Use Your Large Language Model to Explore or Exploit?", "authors": ["Keegan Harris", "Aleksandrs Slivkins"], "abstract": "We evaluate the ability of the current generation of large language models (LLMs) to help a decision-making agent facing an exploration-exploitation tradeoff. We use LLMs to explore and exploit in silos in various (contextual) bandit tasks. We find that while the current LLMs often struggle to exploit, in-context mitigations may be used to substantially improve performance for small-scale tasks. However even then, LLMs perform worse than a simple linear regression. On the other hand, we find that LLMs do help at exploring large action spaces with inherent semantics, by suggesting suitable candidates to explore.", "sections": [{"title": "Introduction", "content": "There has been significant interest in the machine learning community to apply recent advances in generative AI and large language models (LLMs) to solve important decision-making problems. Early work in this direction has already produced impressive agentic behavior in both virtual [e.g., 41, 32] and physical-world environments [e.g., 7]. However, significant challenges remain. Beyond generalization (needed for supervised learning), decision-making under uncertainty requires two additional capabilities: exploitation (making the best decision given the current data) and exploration (trying new options for long-term benefit). Balancing the two has been a subject of an enormous literature, see books [e.g., see books 38, 25, 3]. A recent line of work [e.g., 23, 31] evaluates the ability of LLMs to balance exploration and exploitation entirely in-context, i.e., by specifying the problem description, parameters, and history in the LLM prompt. Focused on simple tasks in reinforcement learning, these results have been mixed. Both papers show that LLMs fail to solve these tasks adequately out-of-the-box, but they can be prompted to do so, e.g. by providing various summary statistics in-context, or by fine-tuning on data from algorithmic baselines on similar problem instances. Unfortunately, such mitigations do not readily extend beyond simple settings. For example, succinct summary statistics do not exist for many decision-making tasks, and fine-tuning for specific tasks may be prohibitive due to cost or lack of sufficient training data. Motivated by these observations, we study the ability of GPT-4 [2], GPT-4o [18], and GPT-3.5 [8] to explore and exploit in-context in silos, with an eye towards leveraging a pre- trained LLM (and the inductive bias therein) as a part of a larger decision-making agent. We focus on (contextual) bandits, as a standard abstraction for the explore-exploit tradeoff. In Section 3, we evaluate LLMs as exploitation oracles for contextual bandits. Given a history of (context, action, reward) tuples, the LLM is tasked with identifying the best action"}, {"title": "Related Work and Background", "content": "Our results belong to a small, but quickly growing line of work on using pre-trained LLMs for in-context reinforcement learning (RL). Coda-Forno et al. [13], Krishnamurthy et al. [23], Nie et al. [31], Monea et al. [30], Xia et al. [43], Park et al. [33], Wu et al. [42] evaluate the ability of LLMs to solve various multi-armed bandit and contextual bandit tasks, and find that the current generation of LLMs largely fail to solve these tasks in-context. Indeed, positive findings are restricted to very simple tasks and/or require substantial mitigations (which in turn do not readily extend beyond simple settings). Xia et al. [43] use LLMs to solve dueling bandit tasks, and Park et al. [33] also evaluate the ability of LLMs to learn in games. While our paper is primarily concerned with whether LLMs succeed as algorithms, several others [e.g., 36, 17, 14] use in-context bandits (and many other tasks) to study whether LLMs exhibit human-like behavior/biases in decision-making. A broader literature on in-context learning [8] aims to solve various tasks by providing all relevant information in the LLM prompt. The work on exemplar selection (selecting examples and other information to present in-context) [e.g., 19, 48, 44, 40] is relevant to our exploitation experiments. A growing line of work aims to use LLMs as a part of a larger decision-making agent [e.g., 28, 50, 49]. Our exploration experiments take inspiration from the work on using LLMs as \u201caction priors\u201d inside of a larger RL algorithm [46, 10, 47, 16]. Much of this work falls under the proposer-verifier framework of Snell et al. [39], where an LLM proposes several possible sequences from which a verifier selects suitable candidates. In comparison, our goal is a more systematic evaluation of LLMs' abilities to explore large action spaces, in isolation from other components of the decision-making task. Finally, a parallel line of work trains transformers from scratch to solve various RL tasks [e.g., 24, 26, 34, 45, 27]. Multi-armed bandits. We consider tasks based on multi-armed bandits (MAB) and contextual bandits (CB), well-studied special cases of RL that abstract the explore-exploit tradeoff, see books [38, 25] for background. In MAB, there are T rounds and K arms. In each round $t \\in [T]$, the learner chooses an action (arm) $a_t \\in [K]$ and observes a noisy reward $r_t$ drawn from some sub-Gaussian reward distribution for this arm. The reward distribution, and particularly its mean $\\mu(a_t)$, are unknown to the algorithm. In CB, the learner additionally observes a context $z_t$ before each round t, and the expected reward $\\mu(z_t, a_t)$ depends on both the context and the arm. The learner's goal is to balance exploration and exploitation to maximize cumulative reward."}, {"title": "LLMs as Exploitation Oracles", "content": "In this section, we evaluate the ability of LLMs to exploit in decision-making tasks with statistical uncertainty on the outcomes. We present LLMs with in-context exploit tasks inspired by multi- armed bandits (MAB) and contextual bandits (CB). In a CB exploit task, an LLM is given a history consisting of context-arm-reward tuples, and is instructed to take the best arm given the current history and the current context. A MAB exploit task is the same, but without contexts. These tasks are generated from some parameterized distributions called exploit puzzles. Our visualizations plot the fraction of correct answers against the task difficulty."}, {"title": "MAB Exploit Puzzles", "content": "Our MAB-based experiments on GPT-4 and GPT-3.5 provide a partial explanation for why the current LLMs fail to solve MAB tasks in-context when presented with raw (non-summarized) history, as first observed by Krishnamurthy et al. [23], Nie et al. [31]. Following these two papers, we try two prompts: one in which arms correspond to pushing different colored buttons and one where they correspond to showing different advertisements to users. The LLM is asked to choose an arm with the highest empirical reward. We also try chain-of-thought (CoT) prompts, for the total of 4 prompt designs: { buttons, adverts } \u00d7 { CoT, no-CoT }. See Appendix A for more details on our experimental setup."}, {"title": "CB Exploit Puzzles and Mitigations", "content": "While the history in K-armed bandits can be summarized using 2K numbers (the average reward for each arm and number of times it has been played), such succinct summary statistics may not be readily available (or even exist) in more complicated decision-making tasks such as CB. We focus on linear CB, a well-studied CB model [starting from 29, 12, 1] in which the expected reward of each arm is linear in the context. Specifically, given context $z \\in \\mathbb{R}^d$ and arm a, the expected reward is $\\mu(z, a) = \\langle z, \\theta_a \\rangle$, for some fixed (but unknown) parameters $\\theta_a \\in \\mathbb{R}^d$. We consider a CB exploit puzzle parameterized by the number of arms K, context dimension d, and history size T. The tasks are constructed as follows. We sample parameters $\\theta_a \\in [-1,1]^d$ and $\\gamma_a \\in [-0.25, 0.25]$ independently and u.a.r. for each arm a. Given context $z\\in \\mathbb{R}^d$, expected"}, {"title": "CB Exploit Puzzles (text-based)", "content": "As a robustness check, we repeat our CB experiments on a text-based exploit puzzle. In this puzzle, contexts are items in a room (e.g. animals, objects on a table), and actions have an associated semantic meaning (e.g. eat the food item, leave the room). Rewards are still presented numerically, and are non-linear functions of both the context and action. See Appendix A for full details on our experimental setup. Figure 5 shows the performance of GPT-4o (with mitigations) on this puzzle. While the reward function is non-linear (and thus the linear baseline only achieves 70% FracCorrect), we find that all configurations are still significantly out-performed by the linear baseline."}, {"title": "LLMs as Exploration Oracles", "content": "We now turn our attention to the ability of pre-trained LLMs to explore large action spaces. Specifically, we leverage the inductive bias of an LLM to generate a small set of candidate actions from a text-based action space, before running an off-the-shelf MAB algorithm on top of the candidate set. We consider two types of exploration tasks: answering an open-ended \"philosophical\" question (Q/A task) and suggesting a title for an arXiv research paper based on its abstract (arXiv task). Particular workloads within each task type are called explore puzzles. The Q/A task (resp. arXiv task) is constructed as follows. We define the \u201cbest arm\" as a contrarian answer generated by another LLM (resp. the actual title of the research paper). The expected reward $\\mu(a)$ of an arm a is the cosine similarity between this arm and the best arm in the embedding space. Here, we generate sentence embeddings using the Sentence-BERT"}, {"title": "Explore Puzzle: Open-Ended Questions", "content": "We used GPT-4 to generate a dataset of 10 open-ended questions with many reasonable answers, along with an intentionally contrarian answer for each question to serve as the ground truth. (E.g., \"What does it mean to live a fulfilling life?\" \"Fulfillment comes from embracing discomfort.\") Each question-answer pair yields a task, as defined above. We evaluate GPT-4o as an exploration oracle on these tasks. We prompt it to suggest $K \\in \\{1,2,3,4,5,7,10\\}$ candidate answers given a question. To grade the candidate set (the entire set, not just the best answer in this set), we run an off-the-shelf MAB algorithm for some large-but-realistic time horizon T over these K candidates. (We use the UCB1 algorithm [5] and $T = 1000$.) We record the expected time-averaged reward $rew := \\frac{1}{T} \\sum_{t \\in [T]} \\mu(a_t)$, where at is the arm selected in round t. We repeat this process (both selecting the candidates and running the bandit algorithm) 10 times for a given task and K, and record the average rew over these runs, denoted by $rew = rew(task, K)$. We experiment with several prompting strategies. Along one axis, we either ask the model to generate suggestions \u201call-at-once\u201d with temperature 0 or \u201cone-by-one\u201d with temperature 1 (we repeatedly show the LLM the list of candidate answers so far and ask it to generate one more). We also experiment with explicitly prompting the LLM to provide a diverse set of candidate answers"}, {"title": "Explore Puzzle: arXiv Abstracts and Titles", "content": "We run similar experiments on a larger-scale dataset of paper titles and abstracts from arXiv.org. Using the arXiv API [4], we collect 10 (paper abstract, paper title) pairs from each of the 41 different arXiv categories across various academic disciplines. To minimize the likelihood that these papers appear in GPT-4o's training corpus, we only use papers uploaded to arXiv after June 2024. Each abstract-title pair yields a task, as discussed earlier. We evaluate GPT-4o as an exploration oracle for these tasks much like in in Section 4.1. Given an abstract, we prompt GPT-4o to generate K alternative titles, which are then used to instantiate a bandit algorithm. We use the same algorithm (UCB1) and time horizon $T = 1000$. We record the expected time-averaged reward $rew := \\frac{1}{T} \\sum_{t \\in [T]} \\mu(a_t)$ and compute the average over tasks within the same arXiv category, called $rew = rew(category, K)$. We try \"all-at-once\" and \"one-by-one\" prompting strategies. We visualize our findings in Section 4.1, using the same conventions as Figure 6 and focusing on six arXiv categories. To assess LLM's ability to specialize to a task, we consider a stronger"}, {"title": "Conclusions", "content": "We evaluate the potential of GPT-4, GPT-4o, and GPT-3.5 to explore and exploit in decision- making tasks. We find that LLMs are useful as exploration oracles in large, semantically meaningful action spaces, where they effectively propose high-quality candidate actions. How- ever, LLMs cannot yet robustly replace (let alone surpass) traditional algorithmic methods for exploitation, particularly in larger or more complex settings. While we suggest several helpful mitigations, they consistently underperform relative to a simple algorithmic baseline such as linear regression. We highlight two directions for future work. First, LLMs trained to use tools like a calculator may be better at exploitation. However, it is unclear how much this would help in more complex scenarios, e.g., CB tasks with text-based contexts and actions, and which mitigations and/or prompting techniques would be needed. Second, while \"zooming\" bandit algorithms do not work for rich text-based action spaces (as discussed in Section 4), LLM-based exploration oracles may potentially help. The hope is to \"zoom in\" entirely in the space of \"potentially relevant\" actions (determined by the LLM), rather than in the space of all actions."}, {"title": "Appendix for Section 3: LLMs as Exploitation Oracles", "content": "In this section we give example prompts for each of our experimental setups. \"Buttons\" prompt for the MAB puzzle: [SYSTEM] You are in a room with 5 buttons labeled blue, green, red, yellow, purple. Each button is associated with a Bernoulli distribution with a fixed but unknown mean; the means for the buttons could be different. For each button, when you press it, you will get a reward that is sampled from the button's associated distribution. Then you must pick the button with the highest empirical average, which must be exactly one of blue, green, red, yellow, or purple. You must provide your final answer immediately within the tags <Answer>COLOR  where COLOR is one of blue, green, red, yellow, or purple and with no text explanation. [USER] The past rewards for each button are: round 1: blue button had reward 1, green button had reward 1, red button had reward 0, yellow button had reward 1, purple button had reward 0 round 2: blue button had reward 0, green button had reward 1, red button had reward 1, yellow button had reward 1, purple button had reward 0 Which button do you choose? Remember, YOU MUST provide your final answer within the tags  where COLOR is one of blue, green, red, yellow, or purple and with no text explanation. \"Adverts\" prompt for the MAB puzzle: [SYSTEM] You are recommendation engine that chooses advertisements to display to users when they visit your webpage. There are 5 advertisements you can choose from, named A, B, C, D, E. When a user visits the webpage you can choose an advertisement to display and you will observe whether the user would have clicked each of the ads. You model this by assuming that each advertisement has a certain click rate and users click on advertisements with their corresponding rates. I will show you the past clicks for each advertisement. Then you must pick the advertisement with the highest empirical click rate, which must be exactly one of A, B, C, D, or E. You must provide your final answer immediately and with no text explanation. within the tags  ADVERTISEMENT  where ADVERTISEMENT is one of A, B, C, D, or E. [USER] The past clicks for each advertisement are: round 1: advertisement A was clicked, advertisement B was clicked, advertisement C was not clicked, advertisement D was clicked, advertisement E was clicked round 1: advertisement A was not clicked, advertisement B was clicked, advertisement C was clicked, advertisement D was clicked, advertisement E was not clicked Which advertisement do you choose? Remember, YOU MUST provide your final answer within the tags  ADVERTISEMENT  where ADVER- TISEMENT is one of A, B, C, D, or E and with no text explanation. \"Buttons\" prompt for the numerical CB puzzle: [SYSTEM] You are in a room with a television and 2 buttons labeled blue, green. Each button is associated with a Bernoulli distribution with an unknown mean; the means for the buttons could be different from each other and may depend on the list"}, {"title": "Additional MAB Figures", "content": "See Figure 9, Figure 10, Figure 11 for additional results in our MAB exploit puzzle."}, {"title": "Additional Details for Text-Based CB Puzzles", "content": "Each context contains a time of day (belonging to {morning, afternoon, evening, night}), an animal ({bear, dog, cat, None}), a tool ({key, letter opener, hammer, None}), a food item ({cake, apple, nut, None}), and a button with a particular color ({red, orange, yellow, green}). The actions in each round are \"pet animal\", \"leave room\", \"use tool\", \"eat food\", and \"press button\". We experimented with two reward functions: an \"easy\" reward function, where the expected rewards for each action are as follows: \u2022 The expected reward of petting the animal is 0.01 if the animal is a bear, 0.7 if the animal is a dog, and 0.4 if the animal is a cat. Otherwise, the expected reward if 0.5. \u2022 The expected reward for leaving the room is always 0.5. \u2022 The expected reward for using the tool is 0.75 if it is a key, 0.6 if it is a letter opener, 0.45 if it is a hammer, and 0.2 otherwise. \u2022 The expected reward for eating food is 0.8 if it is cake, 0.6 if it is an apple, 0.2 if it is a nut, and 0.3 otherwise. \u2022 The expected reward for pressing the button is 0.89 if it is green, 0.62 if it is yellow, 0.39 if it is orange, and 0.27 if it is red. Our results under this reward function are summarized in Figure 12. We used hamming distance to implement our mitigations. Note that in higher-dimensional settings, distance in an embedding space may be used. The reward function we use in the main body is more complicated, and is detailed below: \u2022 The expected reward for petting the animal is 0.01 if it is a bear, 0.7 if it is a dog, 0.3 if it is a cat and the time of day is morning or afternoon, 0.7 if it is a cat and the time of day is evening or night, and otherwise 0.5. \u2022 The expected reward for leaving the room is always 0.5 \u2022 If the animal is a bear, the expected reward for using the tool is 0.1. Otherwise, if the tool is a key and the table item is a chest, the expected reward is 0.9. Otherwise, it is 0.4 \u2022 If the animal is a bear, the expected reward for eating food is always 0.5. Otherwise, the expected reward is 0.8 for cake, 0.6 for an apple, 0.2 for a nut, and 0.5 otherwise. \u2022 If the animal is a bear, the expected reward for pressing the button is 0.1. Otherwise if the button is green and the time of day is morning, or the button is yellow and the time of day is afternoon, or the button is orange and the time of day is evening, or the button is red and the time of day is night, then the expected reward is 0.9. In all other cases, the expected reward is 0.25."}, {"title": "Appendix for Section 4: LLMs as Exploration Oracles", "content": "In this section we give example prompts for each of our experimental setups. \"All-at-once\" prompt with encouragement for open-ended question puzzle: [SYSTEM] I will give you an open-ended question. Come up with 5 different candidate answers. Reply only with the 5 candidate answers, and put each candidate answer on a separate line. Each answer should only be a few words, skipping any introductory phrasing and going straight to the essence. Try to come up with answers that are very different in spirit from one another. [USER] Here is the question: \u201cWhat is the purpose of art?\" \"One-by-one\" prompt without encouragement for arXiv puzzle: [SYSTEM] I will give you an abstract and some candidate titles for a paper. Come up with a new candidate title that is relevant to the abstract, but different from the other candidate titles. Reply only with the candidate title. [USER] Here is the abstract: {abstract goes here} Here are the other candidate titles: {previous suggestions go here} Our open-ended question dataset consists of the following 10 questions and the corresponding \"ground-truth\" answers. Questions: 0. What is the meaning of freedom? 1. How should we define success? 2. What is the role of technology in society? 3. What is the nature of reality? 4. What is the purpose of art? 5. What does it mean to live a fulfilling life? 6. How do cultural differences shape our understanding of morality? 7. What is the relationship between happiness and wealth? 8. How can we balance individuality and community in modern society? 9. What is the role of education in personal and societal growth? Answers: 0. Freedom is an illusion shaped by societal norms and external influences. 1. Success should be defined as contributing to the greater good rather than personal achieve- ment. 2. Technology disrupts the natural balance of society and often creates more problems than it solves. 3. Reality is subjective, varying entirely based on individual perception and experience. 4. The purpose of art is to challenge conventions and disrupt established ideas. 5. Fulfillment comes from embracing discomfort. 6. Cultural differences create moral superiority. 7. Wealth detracts from true happiness. 8. Individuality thrives when shaped by community. 9. Education's purpose is to challenge authority."}, {"title": "Benchmarking Encoders", "content": "Here we benchmark the two encoders we use (Sentence-BERT and the universal sentence encoder) by measuring the cosine similarity between semantically similar/different words. The similarity scores of both models in Table 10 suggest that while the embeddings produced by both embedding models are generally \"in the ballpark\" of what one would consider \"simi- lar\"/\"different\", they are still a somewhat coarse measure of distance, which may explain the similar performance of our different prompting strategies."}]}