{"title": "Integrating LLMs with ITS: Recent Advances, Potentials, Challenges, and Future Directions", "authors": ["Doaa Mahmud", "Hadeel Hajmohamed", "Shamma Almentheri", "Shamma Alqaydi", "Lameya Aldhaheri", "Ruhul Amin Khalil", "Nasir Saeed"], "abstract": "Intelligent Transportation Systems (ITS) are crucial for the development and operation of smart cities, addressing key challenges in efficiency, productivity, and environmental sustainability. This paper comprehensively reviews the transformative potential of Large Language Models (LLMs) in optimizing ITS. Initially, we provide an extensive overview of ITS, highlighting its components, operational principles, and overall effectiveness. We then delve into the theoretical background of various LLM techniques, such as GPT, T5, CTRL, and BERT, elucidating their relevance to ITS applications. Following this, we examine the wide-ranging applications of LLMs within ITS, including traffic flow prediction, vehicle detection and classification, autonomous driving, traffic sign recognition, and pedestrian detection. Our analysis reveals how these advanced models can significantly enhance traffic management and safety. Finally, we explore the challenges and limitations LLMs face in ITS, such as data availability, computational constraints, and ethical considerations. We also present several future research directions and potential innovations to address these challenges. This paper aims to guide researchers and practitioners through the complexities and opportunities of integrating LLMs in ITS, offering a roadmap to create more efficient, sustainable, and responsive next-generation transportation systems.", "sections": [{"title": "I. INTRODUCTION", "content": "ITS represent a transformative approach to enhancing transportation networks' efficiency, safety, and sustainability [1]-\u2013[3]. For instance, the ITS market in the Middle East was valued at USD 2.82 billion in 2017, with an anticipated compound annual growth rate (CAGR) of 11.6% over the forecast period [4]. According to [5], there is significant growth in the U.S. Large Language Model market, projecting a rise from $50 million in 2020 to $1.4 billion by 2030, highlighting a robust CAGR of 37.2% from 2024 to 2030 as shown in Fig. 1. The rising demand for real-time traffic updates for drivers and passengers significantly drives this growth. ITS aims to tackle the increasing challenges of traffic congestion while emphasizing safety and efficiency in transportation systems [6]-[10]. Through intelligent decision-making, ITS can adapt to changing traffic conditions and implement strategies to alleviate congestion. For example, adaptive traffic signal control systems can adjust the timing and coordination of traffic lights based on real-time data, prioritizing the flow of vehicles and minimizing waiting times at intersections [11]. Similarly, variable message signs and ramp metering can be used to provide drivers with up-to-date information on traffic conditions and manage the flow of vehicles entering highways [12]. Traditional ITS techniques include traffic signal control, adaptive traffic management, and sensors and cameras to monitor and manage traffic flow [13], [14]. These methods effectively address congestion, improve traffic flow, and reduce accidents but often rely on predetermined algorithms and reactive measures, limiting their adaptability to rapidly changing conditions and unforeseen events.\nRecently, the advent of deep learning (DL) has significantly enhanced ITS capabilities, introducing predictive and adaptive traffic management [15]\u2013[21]. DL techniques use neural networks to analyze large datasets, enabling more accurate traffic predictions, real-time incident detection, and advanced driver assistance systems [22], [23]. For instance, convolutional neural networks (CNNs) process traffic camera images to detect anomalies, while recurrent neural networks (RNNs) predict traffic patterns based on historical data [24], [25]. These advancements allow for a proactive approach to traffic management, with systems capable of anticipating congestion and dynamically adjusting signals or rerouting traffic.\nMoreover, introducing LLMs has further expanded ITS potential by enhancing user interaction, predictive analytics, and situational awareness [26]-[30]. LLMs like generative pre-trained transformers (GPT) and their successors show remarkable versatility in addressing urban and interurban travel challenges [31]. With their advanced natural language processing (NLP) and machine learning (ML) capabilities, LLMs can interpret, predict, and respond to complex scenarios within transportation networks, improving decision-making and traffic management [32]. LLMs have found various applications in the ITS, including traffic prediction, sentiment analysis of social media data for traffic insights, emergency response, disaster management, and multi-modal transportation planning [30].\nLLMs can potentially revolutionize user interaction with transportation systems by offering personalized travel assistance through natural language interfaces [33], [34]. These models can provide real-time information and recommendations based on individual preferences and traffic conditions, suggesting efficient transportation modes and issuing alerts about potential delays or hazards [33], [34]. By analyzing extensive traffic data and meteorological conditions, LLMs can simplify the creation of accurate traffic prediction models, enabling real-time traffic control and alleviating congestion [32]. Research indicates that LLMs enhance traffic forecasting accuracy, leading to more effective routing and reduced travel times [32]. Furthermore, public sentiment plays a significant role in shaping transportation systems. Social media platforms serve as a rich source of information on traffic conditions, road closures, and public opinions [23]. LLMs can efficiently handle and analyze unstructured data for sentiment analysis [23]. Models like LLMLight have demonstrated superior performance to conventional traffic management systems through simulations and real-world implementations, reducing travel times and congestion while enhancing the interpretability of traffic control decisions [23]. By analyzing social media expressions, transportation officials can gain valuable insights into public sentiment regarding traffic events and road conditions, which proves invaluable for proactive traffic management and disaster response [23].\nBesides, effective communication is paramount during emergencies such as accidents, natural disasters, and road closures. In such scenarios, LLMs are vital in generating automatic emergency warnings and providing essential information to the public [35]. These models can scan data from various sources, including emergency calls, news stories, and social media, to deliver real-time updates to transportation officials and first responders [35]. Research indicates that LLMs improve the speed and accuracy of information sharing during crises, enhancing overall safety and response coordination [23]. Moreover, the integration of LLMs in autonomous vehicles can enhance vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communications, enabling these vehicles to understand and adapt to their surroundings more effectively [23]. By leveraging LLMs' NLP capabilities, autonomous cars can interpret and respond to various contextual information, improving their overall situational awareness and decision-making abilities.\nLLMs are transforming the ITS by improving user interaction and communication. Imagine a typical rush hour in a bustling urban center where thousands of commuters are stuck in traffic, frustrated by delays, and uncertain about their arrival times. Now, envision an ITS powered by LLMs that seamlessly integrates real-time data from traffic cameras, GPS devices, social media feeds, and weather forecasts. As vehicles approach congested intersections, the system analyzes and learns from past traffic patterns to predict bottlenecks and dynamically adjusts signals to optimize flow, reducing congestion while maintaining safety. Simultaneously, commuters receive personalized notifications on their smartphones, offering alternative routes, real-time traffic updates, and public transport options, minimizing travel time and stress. Beyond traffic management, LLMs can drive predictive maintenance for public transportation by analyzing sensor data to detect potential breakdowns in buses and trains, allowing maintenance teams to act proactively and prevent service interruptions. In the realm of autonomous vehicles, LLMs enable real-time data sharing between vehicles, allowing them to coordinate movements, avoid traffic jams, reduce accidents, and optimize routes for energy efficiency. These examples highlight the transformative potential of LLMs in ITS, where they enhance traffic management and improve safety, reliability, and sustainability, ultimately creating smarter, more adaptive transportation systems that respond dynamically to modern challenges.\nThe LLMs provide intuitive interfaces that mimic human conversation, offering real-time traffic updates, travel advice, and query responses, thus enriching user experience and fostering trust in ITS services [36]. In vehicle-to-everything (V2X) communication, LLMs can translate complex sensor data into actionable information, enhancing situational awareness and enabling advanced ITS applications like cooperative driving [37]. Furthermore, LLMs contribute significantly to predictive analytics, aiding traffic planners in mitigating congestion through accurate traffic flow predictions and infrastructure planning, facilitating sustainable urban development and resilient transportation networks [38]. They also enhance ITS through training and simulation by generating realistic traffic scenarios for training programs and augmenting autonomous vehicle training with diverse scenarios, ensuring preparedness for various conditions [37]. While the applications of LLMs in ITS are extensive and promising, navigating these developments with ethical, societal, and regulatory considerations at the forefront is crucial to ensure responsible and beneficial implementation of ITS technologies [39]. The use of LLMs in ITS faces various challenges, including regulatory issues, unforeseen capabilities, deployment safety, and widespread acceptance [39].\nThis paper presents a comprehensive review and analysis of the transformative potential of LLMs in optimizing ITS. By leveraging their advanced NLP and ML capabilities, LLMs can address significant challenges in traffic management, enhance situational awareness, and improve decision-making processes. Furthermore, we explore several applications in detail, discussing the integration of LLMs in various ITS components, including traffic prediction, signal optimization, and real-time user interaction. The paper also addresses associated ethical, societal, and regulatory considerations. Through this detailed exploration, we aim to highlight the innovative solutions that LLMs can bring to ITS and propose future research directions to overcome existing challenges and fully realize their potential."}, {"title": "A. Related Surveys", "content": "In recent years, numerous AI-based methods have been applied across various transportation-related fields, leading to a wealth of surveys and research. In [40], the authors conducted a comprehensive analysis of studies where AI techniques were employed to propose new services and applications or to address challenges in ITS. Their focus was on key ITS domains such as road safety, vehicle control, and traffic control and prediction. They evaluated the most widely used AI methods, particularly focusing on genetic algorithms (GAs), fuzzy logic (FL), expert systems (ESs), and artificial neural networks (ANNs), analyzing their advantages and limitations in different transportation scenarios. Similarly, in [41], a detailed survey was conducted on various AI methods used for accident prediction and analyzing risky driving patterns. This survey provided valuable insights into how AI can enhance safety measures on roads by predicting potential accident-prone situations.\nMoreover, in [42], the authors offered a thorough review of the application of generative AI in intelligent transportation systems. They provided a systematic overview of mainstream generative AI techniques, comparing these methods both horizontally and vertically. The study analyzed how generative AI can address critical ITS challenges such as traffic perception, traffic prediction, simulation, and decision-making, while also highlighting the open challenges in deploying generative AI in these systems. In [43], the focus shifts to explainable AI (XAI), particularly for autonomous driving. The author presents a framework that emphasizes the components necessary for explainable end-to-end autonomous driving systems, proposing potential future directions to improve the transparency, reliability, and societal acceptance of autonomous vehicles (AVs). In [44] provides a comprehensive review of AI's application in addressing research challenges within vehicle-to-everything (V2X) systems, showcasing the diverse ways AI is being used to enhance communication and decision-making in connected vehicle environments.\nRecently, several comprehensive surveys have explored the integration of LLMs into ITS and related fields. In [45], the authors delve into applying LLMs for enhancing intelligent traffic safety systems, emphasizing their potential in automating accident reports, augmenting traffic data, and analyzing multi-sensory safety data. They highlight the evolution from statistical methods to advanced DL approaches, noting the improvements in efficiency and accuracy. However, they also address significant risks such as model bias, data privacy issues, and artificial hallucination, suggesting mitigation strategies. Another survey by [46] introduces TrafficGPT, which combines ChatGPT with traffic foundation models to improve urban traffic management. This integration allows ChatGPT to analyze and process traffic data, support decision-making, and interact with simulations, effectively overcoming the limitations of LLMs in handling numerical data and simulations. Further, [47] provides an in-depth review of vision-language models (VLMs) in autonomous driving and intelligent traffic systems, categorizing existing VLM algorithms and exploring their applications. The paper discusses the integration challenges, such as data scale and quality, real-time processing, and safety alignment, emphasizing the need for large-scale datasets and improved hardware support. Similarly, [48] presents an overview of Open-TI, an augmented language agent for intelligent traffic analysis, detailing its architecture and robust capabilities in tasks like map data processing and traffic simulations. This paper showcases the potential of Open-TI to enhance traffic management and strategy planning through LLMs' contextual abilities and domain-specific tools. Additionally, [23] surveys the transformative impact of cutting-edge AI technologies on ITS, highlighting how frontier AI, foundation models, and LLMs enhance transportation intelligence and smart city development by leveraging vast textual data. Lastly, [49] focuses on multi-modal large language models (MLLMs) in autonomous driving, discussing their potential to revolutionize the field and the necessity for new, large-scale datasets and better hardware support to improve safety and user experience."}, {"title": "B. Contributions of this paper", "content": "Unlike the surveys mentioned above, our paper explores the innovative deployment of LLMs across various ITS applications, aiming to enhance user interaction, predictive analytics, and situational awareness. Leveraging advanced NLP and ML capabilities, LLMs are adept at interpreting, predicting, and responding to complex scenarios within transportation networks. Furthermore, we explore the innovative deployment of LLMS in ITS, enhancing user interaction, predictive analytics, and situational awareness. By leveraging models like GPT, BERT, T5, FalconLLM, and LlaMa, we demonstrate how LLMs can improve traffic prediction, real-time data analysis, and user interaction. This integration is set to revolutionize data acquisition, signal control, and overall traffic management, contributing to sustainable urban development. We address the ethical, societal, and regulatory considerations for responsible implementation and highlight future research directions. The main contributions of this paper are summarized as follows:\nFirst, we present a comprehensive overview of the ITS system, outlining its components, operational principles, and overall effectiveness. This foundational knowledge provides a ground for examining the integration and application of LLMs within ITS.\nNext, we present an in-depth theoretical background of various LLM techniques, including GPT, T5, CTRL, BERT, and others, within the context of ITS. This section is crucial for novice readers to grasp the technical aspects and foundational principles of these models, enabling a better understanding of how they can be leveraged to enhance different applications within ITS.\nThen, we conduct an in-depth analysis of the various applications of LLM techniques within ITS. These include LLM-assisted traffic flow prediction, vehicle detection and classification, autonomous driving, traffic sign recognition, and pedestrian detection. This detailed investigation demonstrates how LLMs can effectively tackle a range of challenges in ITS, thereby enhancing overall traffic management and safety.\nAfter that, we explore the challenges LLMs face within ITS, such as data availability, computational constraints, and ethical considerations. The paper also provides insights into overcoming these challenges to ensure responsible and effective implementation.\nFinally, we discuss future research directions and potential innovations, such as improving data integration techniques, enhancing real-time processing capabilities, interrogation with emerging technologies, and developing ethical frameworks to address privacy concerns. These future directions guide further advancements in integrating LLMs with ITS for more efficient, sustainable, and responsive transportation systems."}, {"title": "C. Organization of the paper", "content": "The remainder of the paper is structured as follows. In Section II, we provide an overview of the centralized LLMs, such as GPT, BERT, T5, FalconLLM, and LlaMa and their role in ITS. In Section III, we discuss the decentralized LLMs, including GPT-NeoX, OpenFlamingo, BLOOM, Colossoal-AI, Mesh TensorFlow and Petals, and their relevance to ITS. Next, in Section IV, we explore various applications of LLMs in ITS, including traffic prediction, signal optimization, route planning, autonomous vehicles, public transport, V2X communication, ADAS, and traffic control centers. Then, in Section V, we presented a few case studies to provide insight into the LLM models application in different ITS applications. Also, in Section VI, we provide the challenges/limitations of integrating LLMs in the ITS, such as data availability, computational constraints, ethical concerns, integration issues, and network reliability and also outline the future research directions and potential innovations to address these challenges. Finally, Section VII concludes the paper, summarizing the key findings and highlighting the transformative potential of LLMs in optimizing ITS."}, {"title": "II. OVERVIEW OF LLMS", "content": "NLP has been revolutionized by LLMs, which have shown remarkable performance for commonly spoken languages across various NLP applications [50]. Unlike earlier models limited to specific tasks, LLMs can solve many tasks, garnering significant attention in academic and industrial sectors [51], [52]. Pre-training Transformer models on large-scale corpora have led to the development of pre-trained language models (PLMs) that perform well across diverse NLP applications. Researchers have discovered that scaling model size can enhance performance, prompting further exploration of this scaling effect [53]. LLMs experience substantial performance improvements when their parameter scale surpasses a certain threshold, displaying unique abilities not found in smaller models [53]. These large PLMs demonstrate remarkable abilities in handling complex tasks such as step-by-step reasoning, instruction following, and in-context learning, achieving state-of-the-art performance in specific NLP problems [54], [55]. They can produce human-like text, respond to queries, and perform other language-related activities with high accuracy without additional training [56].\nThe advent of transformer-based LLMs represents a fundamental shift in NLP, introducing a new paradigm for learning and generating human language [57]. The Transformer architecture offers superior computational efficiency compared to Recurrent Neural Network (RNN)-based models [58]. Unlike traditional Convolutional Neural Network (CNN) and RNN architectures, the Transformer design, consisting of an encoder and decoder, each with its attention mechanism, provides distinct advantages [59], [60]. The attention mechanism is crucial for effective sequence modelling and transduction, enabling the modelling of dependencies regardless of their distance in the input or output sequences [61]. This allows the model to view entire phrases or paragraphs simultaneously, rather than one word at a time, as in RNNs [60]. In [60], the authors outline the sequence of steps in Transformers. First, the input text, such as a sentence or paragraph, is tokenized into smaller parts [62]. These tokens are then numerically encoded to become embeddings that retain meaning. Additionally, the input includes positional encoding to indicate word order. The encoder uses these token embeddings and positional data to create its visualization, where the output is provided by the decoder. During training, the decoder learns to predict the next word based on the preceding words [60]. Using learned embedding, the input and output tokens are transformed into vectors of various dimensions, and a multi-head attention layer enables the model to attend to different representation subspaces concurrently [59], [61]. Fig. 2 provides a generic illustration of the LLM transformer with the input/outputs. A significant advancement in LLMs is pre-training, where a language model is initially trained on a large dataset before being fine-tuned for a specific task. This method has proven effective in enhancing performance across various language-related tasks [63]. Selecting an appropriate amount of pre-training data helps to fine-tune target classification tasks [64]. Given the diverse domains of target data, different approaches to pre-training data reuse are applied. For fine-tuning, random sampling of pre-training data is practical when the target data closely relates to the pre-training data. Alternatively, if label information for the pre-training data is available, data with overlapping classes can be directly used for fine-tuning, improving performance with fewer resources [64]. LLMs, such as GPT-3 and BERT, are powerful Al models that can generate human-like text and understanding and are usually trained on textual data exhibiting NLP capabilities. Recent advancements have seen the development of various LLMs, each tailored for specific applications and improvements in NLP tasks. Different types of LLMs have been developed, each with strengths and capabilities. For example, GPT-3 excels in natural language generation, while BERT is known for its language understanding capabilities. Fig. 3 shows some well-known LLMs and their characteristics, and Table II provides an overview of different LLM models, their basic architecture, training methods, key capabilities, and applications in ITS. In the following, we briefly discuss these well-known LLMs."}, {"title": "A. Generative Pre-Trained Transformer (GPT)", "content": "GPT is a DL model that has been pre-trained on vast amounts of text data, enabling it to be fine-tuned for a variety of specific tasks, such as language generation, sentiment analysis, language modelling, machine translation, and text categorization [65]. The transformer architecture employed in GPT represents a substantial advancement over earlier NLP techniques, such as Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs). The critical feature of GPT is its use of a self-attention mechanism, which allows the model to consider the context of the entire sentence when generating the next word. This enhances GPT's ability to understand and produce language, as the decoder creates output text based on the input representation [65]. GPT's capabilities span a wide range of NLP tasks. It excels in natural language understanding (NLU), enabling it to analyze and interpret text and recognize entities and relationships within sentences. Additionally, GPT has natural solid language generation (NLG) capabilities, allowing it to generate text output, such as creative content or complete and informative responses to queries. Furthermore, GPT can generate computer code in various languages, including Python and JavaScript. The model also effectively answers questions, provides factual summaries, and develops stories based on the input text. The transformer architecture and the extensive pre-training of GPT on a vast corpus of data have endowed the model with remarkable versatility and performance across a diverse range of NLP applications [65].\nAfter being pre-trained on extensive generic text corpora, the GPT parameters were made publicly available [66]. \u03a4\u03bf optimize GPT for a specific downstream NLP task, it is often fine-tuned on a limited amount of in-domain data [67]. This process allows the language knowledge acquired during pre-training to be applied to a task using a small amount of task-specific data. The first iteration of GPT in 2018 had 110 million learning parameters (values that a neural network seeks to optimize during training) [68]. GPT-2, released a year later, utilized 1.5 billion parameters. GPT-3, the latest iteration, employs 175 billion parameters. The training is conducted on the AI supercomputer on Microsoft Azure, with an estimated cost of $12 million, making it extremely expensive [69] [70]. This computational approach can address numerous use cases, including question answering, chatbots, email composition, summarization, translation, grammar correction, and more.\nThe GPT family encompasses a range of models, including GPT-1, GPT-2, GPT-3, InstructGPT, ChatGPT, GPT-4, CODEX, and WebGPT [71], [72]. While earlier models like GPT-1 and GPT-2 were open-source, recent iterations such as GPT-3 and GPT-4 are closed-source and accessible only through APIs. GPT-1 demonstrated the potential of Generative Pre-Training, where a decoder-only Transformer model is pre-trained on unlabeled text to achieve substantial performance across various natural language tasks [71]. This self-supervised model predicts the next word or token and is then fine-tuned on downstream tasks with fewer samples. GPT-1 laid the groundwork for subsequent GPT models, each featuring enhanced architecture and improved performance on language tasks. Building upon this foundation, GPT-2 illustrated that language models can learn specific natural language tasks without explicit supervision when trained on a massive Web-Text dataset containing millions of web pages [72]. While similar to GPT-1, GPT-2 introduced minor modifications, such as moving layer normalization to the input of each sub-block, adding additional layer normalization after the final self-attention block, and adjusting the initialization to account for accumulation on the residual path and scaling the weights of residual layers.\nThe GPT-3 model marks a significant milestone in developing LLMs, distinguished by its pre-trained auto-regressive architecture and an extensive parameter count of 175 billion [73]. This substantial scale endows GPT-3 with emergent capabilities often absent in smaller models. One notable feature of GPT-3 is its ability to engage in in-context learning, enabling it to be applied to downstream tasks without needing gradient updates or fine-tuning [73]. Users can orchestrate tasks and provide few-shot examples through textual interactions with the model, allowing it to excel in a wide range of NLP tasks, including translation, question-answering, and cloze tests. Furthermore, GPT-3 demonstrates impressive performance in impromptu reasoning and domain adaptation tasks, such as word unscrambling, employing novel terms in sentences, and executing three-digit arithmetic [73]. These capabilities emerge from the model's extensive pre-training on a vast corpus of textual data, which enables it to draw upon a deep understanding of language and reasoning to tackle diverse challenges. The introduction of GPT-3 represents a significant advancement in the field of LLMs, showcasing their potential to tackle a broad spectrum of NLP tasks with remarkable versatility and adaptability [73]. In March 2023, GPT-4 emerged as the most advanced and potent LLM within the GPT family. It is a multi-modal model that accepts image and text inputs while generating text outputs. Although GPT-4 exhibits certain limitations compared to human performance, it achieves human-level results on numerous professional and academic benchmarks, including ranking in the top 10% of test takers on a simulated bar examination. Similar to its predecessors, GPT-4 underwent initial training to predict subsequent tokens across extensive text corpora, followed by fine-tuning through Reinforcement Learning from Human Feedback (RLHF) to ensure alignment with desired behaviours [74].\nBy leveraging its natural language comprehension skills, GPT can be crucial in transportation planning. Thanks to its ability to generate coherent language and understand context, GPT can aid in analyzing diverse transportation-related data, such as weather reports, traffic updates, and user feedback. This enables it to provide detailed insights into traffic patterns and recommend optimal routes for commuters and logistics operations. By simulating various scenarios, GPT's generative capabilities can allow transportation planners to anticipate potential bottlenecks and develop proactive solutions. Integrating GPT's processing of multiple information sources enhances decision-making in transportation management systems, reducing congestion and improving traffic flow. Additionally, GPT's language-generating capabilities facilitate natural language interactions with users, promoting seamless communication and enhancing the user experience in transportation applications."}, {"title": "B. Text-to-Text Transfer Transformer (T5)", "content": "T5 is a pre-trained encoder-decoder model that translates each task into a text-to-text format [75]. The main innovation of T5 is its formulation of every task as a text production problem. This approach allows various tasks, such as question answering, summarizing, text classification, and translation, to be converted into a unified text-to-text format. Consequently, the same loss function, model, hyper-parameters, and training protocols can be applied across different tasks, streamlining the training process [75].\nT5 is pre-trained using a masked language modeling objective called \"span-corruption.\u201d This approach replaces consecutive spans of input tokens with a mask token, and the model is trained to reconstruct the masked-out tokens. The model computes the probability of each possible word that can fit in the blank based on the preceding and following words, thereby following the context. This pre-training strategy enables the model to generate text for various tasks effectively. The large dataset used for T5's pre-training derives from the Colossal Clean Crawled Corpus (C4), one of the most enormous language datasets. C4 comprises over 365 million documents, totaling 173 billion tokens [76]. This corpus includes material extracted from Common Crawl, subjected to several filters to preserve high-quality English content [76]. As a result, C4 provides a vast, clean, and natural dataset for pre-training, significantly larger than most pre-training datasets [75].\nA well-known application of T5 is SciFive, a domain-specific T5 model. As proposed in [77], SciFive is extensively pre-trained on biological datasets. Unlike BERT, which is designed for tasks with a single prediction output, T5 is tailored for text generation tasks. T5 generates a text string for every input, making it suitable for summarizing, answering questions, and other tasks where a single output is insufficient. This adaptability allows T5 to achieve state-of-the-art results in clinical notes, biomedical literature, and general scientific literature.\nDue to the encoder-decoder architecture of T5, it demonstrates exceptional performance in traffic generalization and understanding. As proposed in [78], their model on T5 serves as a foundational model for network traffic, leveraging the T5 architecture to derive pre-trained representations from extensive unlabeled datasets. This design maintains the model's generative capabilities while effectively capturing global information. Consequently, their model can learn representations from raw data more efficiently, significantly reducing the amount of labeled data required for fine-tuning compared to existing methodologies."}, {"title": "C. BERT", "content": "BERT is one of the most prominent language models designed to tackle complex NLP tasks [79]. It represents the contextual interactions between words or word fragments in input sequences, capturing both short- and long-term dependencies [80]. This is achieved through the Transformer architecture, specifically utilizing bi-directional self-attention. Unlike traditional embedding techniques, BERT provides multiple context-aware representations of a single word in different positions by considering the bi-directional dependencies between words across sentences. By continuously adjusting the model parameters, BERT functions as a pre-trained language model, ensuring that the generated semantic representations accurately reflect the core meaning of the language [81].\nBERT is trained on a large unlabeled corpus to extract features encapsulating textual semantic information. The initialization techniques used in the pre-training process expedite the model's convergence and enhance its generalization capability. BERT learns a comprehensive language representation during pre-training using large-scale unlabelled text samples. This involves masked language modeling (MLM) and next-sentence prediction (NSP). MLM masks specific input tokens and trains the model to predict the original masked tokens, facilitating bidirectional context awareness [82]. NSP trains BERT to predict whether a given sentence follows another, thereby improving coherence and understanding after extensive training periods ranging from one to one hundred GPU days. The basic BERT model consists of twelve Transformer layers used to generate the final embeddings. Each layer uses only Transformer encoders to process the input data. Initially, tokenized phrases are fed into the first layer of the Transformer, which outputs the same number of tokens. This process continues through successive layers, each producing equivalent tokens. However, the feature value weights are adjusted as the tokens pass through each layer. Individual text sentences are input for BERT training on a specific dataset. These sentences are tokenized using the BERT tokenizer, translating them into BERT-specific tokens. Additional formatting is required to prepare the data for training. BERT can be leveraged for ITS applications by analyzing textual data from user inquiries, road conditions, and traffic patterns. BERT's understanding of complex language allows it to process real-time traffic updates from multiple sources, including social media and traffic reports, facilitating dynamic route adjustments. Integrating BERT into transportation systems can enable the development of more efficient navigation solutions, reducing fuel consumption and travel time. Additionally, BERT's contextual awareness can help provide personalized transportation recommendations based on user preferences and constraints."}, {"title": "D. LlaMa", "content": "Meta has introduced the LLaMA collection, a set of foundational language models with significant potential for ITS applications [83]. Unlike the GPT models, the LLaMA models are open-source, with the model weights provided to the research community under a non-commercial license. As a result, the LLaMA family is rapidly expanding as researchers utilize it to enhance open-source LLMs and develop task-specific models for mission-critical ITS applications [83]. The initial LLaMA models were released in February 2023, featuring parameter counts ranging from 7 billion to 65 billion [83]. These models have been pre-trained on trillions of tokens from publicly available datasets. While LLaMA adopts the transformer design of GPT-3, it incorporates minor modifications, such as using a SwiGLU activation function instead of ReLU and employing rotational positional embeddings [83]. These adaptations make LLaMA highly suitable for processing large-scale traffic data, predicting traffic patterns, and optimizing traffic flow in real time.\nIn July 2023, Meta and Microsoft introduced the LLaMA-2 collection, which includes foundational language models and LLaMA-2 Chat models optimized for dialogue [83]. The LLaMA-2 Chat models outperformed other open-source models in public benchmarks. Fig. 4 illustrates a generic architecture of the LLaMA-2 model. The training procedure for LLaMA-2 Chat begins with pre-training LLaMA-2 using publicly available web data. The initial version of LLaMA-2 Chat is created through supervised fine-tuning. The model is iteratively improved through reinforcement learning from human feedback (RLHF), rejection sampling, and proximal policy optimization. Collecting human feedback during the RLHF stage is crucial for adjusting the reward model and preventing excessive changes that could negatively impact the stability of LLaMA training. In the context of ITS, these models can enhance human-machine interactions, enabling more accurate and responsive traffic management systems [83]. Llama Guard is an input-output safeguard model based on LLM, designed for use in Human-AI conversations [84]. A valuable tool for classifying specific safety risks identified in LLM prompts is the safety risk taxonomy, which is integrated into the model (also known as prompt classification). This taxonomy significantly aids in response classification, the process of categorizing responses produced by LLMs in reaction to these prompts. A high-quality dataset has been meticulously assembled to facilitate prompt and response classification. Despite being a low-volume model, Llama Guard, an instruction-tuned Llama2-7B model, performs well on benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where it matches or surpasses readily available content moderation tools [84]. In ITS, Llama Guard could be instrumental in monitoring and moderating communication between various system components, ensuring safe and efficient operation.\nLlama Guard performs multi-class classification as a language model and generates scores for binary decisions. Through Llama Guard's instruction fine-tuning, tasks and output formats can be customized and adjusted. This feature enhances the model's performance by allowing taxonomy categories to be modified to fit specific use cases better and by facilitating zero-shot or few-shot responses with various taxonomies at the input. This flexibility is precious in ITS, where diverse data sources and evolving traffic conditions require adaptive and precise solutions."}, {"title": "E. FalconLLM", "content": "The Falcon series", "85": ".", "86": "resulting in minor modifications. Notably", "87": "."}, {"87": "."}, {"87": ".", "scalability": "hardware, data, and performance [87", "88": [89], "90": "."}]}