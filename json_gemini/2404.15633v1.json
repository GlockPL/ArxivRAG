{"title": "Artificial Intelligence for Multi-Unit Auction design*", "authors": ["Peyman Khezr", "Kendall Taylor"], "abstract": "Understanding bidding behavior in multi-unit auctions remains an ongoing\nchallenge for researchers. Despite their widespread use, theoretical insights into\nthe bidding behavior, revenue ranking, and efficiency of commonly used multi-\nunit auctions are limited. This paper utilizes artificial intelligence, specifically\nreinforcement learning, as a model free learning approach to simulate bidding\nin three prominent multi-unit auctions employed in practice. We introduce\nsix algorithms that are suitable for learning and bidding in multi-unit auc-\ntions and compare them using an illustrative example. This paper underscores\nthe significance of using artificial intelligence in auction design, particularly in\nenhancing the design of multi-unit auctions.", "sections": [{"title": "1 Introduction", "content": "Multi-unit auctions play a fundamental role in allocating goods and services\nacross diverse markets, including treasury bills, emission permits, spectrum licenses,\nand electricity (Khezr and Cumpston, 2022; Bichler and Goeree, 2017). These auc-\ntions are often characterized by asymmetric information and, due to the existence of\nmultiple units, result in a complex bidding environment. Although multi-unit auc-\ntions are commonly employed in practice, there is a lack of consensus in the literature\nregarding their performance in terms of revenue and efficiency. Moreover, economic\ntheory often lacks clear guidance on the outcomes of many multi-unit auctions (Kr-\nishna, 2009).\nA range of disciplines, including economics and computer science, have employed\nvarious methodologies to investigate multi-unit auctions. Auction theory, data ana-\nlytics, experimental approaches, and simulations represent some of the predominant\nmethods used in these studies (Ausubel et al., 2014; Roughgarden et al., 2017; Mor-\ngenstern and Roughgarden, 2016; Hailu and Thoyer, 2007). Each method offers\ndistinct advantages but also comes with limitations. A common shortfall across\nthese approaches is their inability to comparatively assess the performance of differ-\nent multi-unit auction variations under similar conditions. Additionally, traditional\nsimulation methods often lack clear guidance as they require an existing model or\ntheoretical prediction to feed into the simulation. In this article, we use artificial\nintelligence, specifically introducing several reinforcement learning (RL) algorithms,\nto simulate bidding behavior in various multi-unit auctions.\nReinforcement learning (RL) has been recognized as an effective means of simu-\nlating learning processes in uncertain environments, such as auctions (Silver et al.,\n2021). The capability of RL-based algorithms to learn in a model-free manner makes\nthem particularly well-suited to contexts like multi-unit auctions, where predictive\nmodels are often unclear or unavailable. This paper concentrates on three primary\nRL approaches: Q-Learning, Policy Gradient, and Actor-Critic. We discuss six algo-\nrithms that are suitable for simulating the learning and bidding processes in auctions.\nOur findings detail the principal advantages and disadvantages of each algorithm in\nthe context of bidding behavior simulation. This paper represents one of the initial\nattempts to apply artificial intelligence to analyze multi-unit auctions.\nOur approach leverages artificial intelligence as a complementary tool to previous\nmethodologies, aiming to emulate the human thought process and decision-making\nin complex bidding scenarios within multi-unit auctions. Using a standard private\nvalue model, we assume that buyers' valuations for each unit up for auction are pri-\nvately known. Our investigation focuses on three sealed-bid multi-unit auctions: the\nDiscriminatory Price (DP), the Generalized Second-Price (GSP), and the Uniform-\nPrice (UP) auctions. All three of these auctions have been employed extensively in\nreal-world markets to sell various goods and services.\nWe first provide a metric for learning and demonstrate through an example, in\nwhich each bidder demands two units, that almost all RL algorithms perform very"}, {"title": "Background", "content": "Since the seminal work by Vickrey (Vickrey, 1961), it is known that bidding\nbehavior in multi-unit auctions does not necessarily follow those in single unit auc-\ntions. Despite this, most of conventionally used multi-unit auctions are extensions\nof a standard single-unit format. For instance, the generalized second-price auction,\nas appears from its name, is a format that extend the second-price rule to a multi-\nunit case. There are several studies that show these multi-unit extensions do not\nnecessarily have the ideal properties of the single unit format (e.g. Back and Zender,\n1993; Edelman et al., 2007; Ausubel et al., 2014). Additionally, due to multiple equi-\nlbria and lack of closed form solutions for the bidding functions, theoretical studies\nhave no clear conclusion about the revenue and efficiency ranking of commonly used\nmulti-unit auctions (Krishna, 2009; Ausubel et al., 2014).\nRecently, the use of RL approaches, particularly Q-learning algorithms, has be-\ncome more prevalent in auction theory literature. For example, Banchio and Skrzy-\npacz (2022) employ a simple Q-learning algorithm to simulate bidding behavior in\ntwo different single-unit auction formats. Khezr et al. (2024) is another study that\nuses Q-learning to simulate a more complex type of auction, namely the knapsack\nauction. While various other RL approaches could potentially be used to simulate\nbidding in auctions, current studies are mainly limited to simple Q-learning algo-\nrithms. Therefore, our main aim is to explore other, more advanced algorithms that\ncould effectively simulate such complex bidding environments."}, {"title": "2 Model", "content": "A seller owns K > 2 units of a homogeneous object. There are n > 1 potential\nbuyers, each demanding up to k > 1 units of the object. Every buyer i has a distinct\nmarginal value, $v_i^k > 0$, for the kth unit of the object. We use $v^i = (v_1^i, v_2^i, ..., v_k^i)$\nto denote the vector of values for buyer i with diminishing marginal values, that is,\n$v_1^i \\geq v_2^i \\geq ... \\geq v_k^i$. Each buyer privately knows these values. However, it is publicly\nknown that these values are independently and identically distributed according to\na distribution function $F_i(.)$ with finite bounds. Finally, to avoid trivial cases, we\nassume that the sum of the units demanded by all buyers exceeds the number of\navailable units, that is, $\\Sigma_{i=1}^n k_i > K$.\nThe allocation mechanism\nThe seller uses a sealed-bid multi-unit auction to allocate the objects. Each\nbidder submits a vector of bids b' for all the units they demand. The auctioneer\nranks the bids from the highest to the lowest, and allocate object to the K highest\nsubmitted bids. We explore three types of payment rules as follows.\nDiscriminatory Price Auction (DP)\nIn this auction, every buyer i submits up to k bids, represented by $b^i = (b_1^i, b_2^i, ..., b_k^i)$.\nThe auctioneer arranges all the n\u00d7k bids from the highest to the lowest. The top\nk bids each win a unit and pay the amount they bid for that particular unit. For\nexample, if the first bid from bidder three, $b_1^3$, holds the top rank, she secures the\nfirst unit and pays $b_1^3$ for it.\nGeneralized Second-Price Auction (GSP)\nMuch like the DP auction, each buyer i submits up to k bids. The top k bids each\nwin a unit.However, in this case, winners pay the amount of the bid that is ranked\nimmediately below their own (excluding their own subsequent bids). For instance, if\nbidder three's first bid, $b_1^3$, holds the highest rank, she secures the first unit but pays\nthe amount of the second-highest bid."}, {"title": "Uniform-Price Auction (UP)", "content": "In this auction every winning bidder pays a uniform price for each unit equal to\nthe highest losing bid, that is the K + 1th highest bid."}, {"title": "3 Reinforcement Learning approaches", "content": "We use Reinforcement Learning (RL) methods to model bidder behavior in multi-\nunit auctions with multiple bidders. These algorithms are inspired by behavioral\npsychology, where an agent learns to achieve a goal by receiving feedback through\nrewards or penalties. An agent, in the context of this work, is a simulated human\nbidder operating within an environment, an auction, in which they interact with other\nagents via bidding for available items. The environment encompasses everything the\nagent interacts with and can change over time, either due to the agent's actions or\nindependently. Interactions between an agent and the environment are defined by\nthe terms: state, action, and reward.\nA state is a complete description of the agent's situation within the environment.\nAn observation, which might be a partial view of the state, provides the agent with\ninformation about the current situation. Actions are the set of all possible decisions\nthe agent can make. A reward is a feedback signal indicating the immediate effect\nof the agent's previous action. The agent's objective is to maximize the cumulative\nreward, or return, over time. This objective and the associated decision-making\nstrategy are encapsulated in an agent's policy, a mapping of states (or observations)\nto actions.\nThe RL problem is fundamentally about learning the optimal policy that maxi-\nmizes the expected return when starting from any given state. The agent gradually\nimproves its decision-making abilities by exploring and exploiting different actions.\nThese methods are adapted to the complexity and uniqueness of each auction sce-\nnario presented in this paper. Our approach involves multiple agents (bidders) in a\ndynamic, discrete environment. We evaluate three main approaches for this study:"}, {"title": "1. Q-Learning:", "content": "Traditional tabular and deep learning approaches are assessed, where an agent\nlearns from their own actions, states, and rewards. In the tabular version, each\nagent has a Q-table (developed over many auction episodes), which guides\nthem to the best action for a given state that maximises rewards. Initially,\nexploration is emphasised, but over time, exploiting known actions becomes\ndominant. The deep Q-learning version (DQN) uses a neural network instead"}, {"title": "2. Policy Gradient:", "content": "Again, traditional and deep learning versions are considered. The policy gradi-\nent approach models action probabilities for given states, aiming to maximise\nrewards. The policy is parameterised and updated based on the reward gra-\ndient. A neural network represents the policy using deep learning, enhancing\ngeneralisation and scalability and allowing for more complex policy learning."}, {"title": "3. Actor-Critic:", "content": "This method combines value-based estimates (in ways similar to Q-learning)\nand policy-based functions (similar to those used in Policy Gradient algo-\nrithms). The actor proposes actions, while the critic evaluates them and guides\npolicy updates using neural networks. Two popular and successful methods are\nevaluated: the Advantage Actor-Critic (A2C) and Proximal Policy Optimiza-\ntion (PPO) algorithms. These methods differ in their approaches to estimating\nthe value of actions and updating the policy."}, {"title": "3.1 Environment setup and evaluation", "content": "A typical RL problem trains an agent using multiple independent sets of agent\nand environment interactions called an episode. An episode is a complete sequence\nof states, actions, and rewards that ends with a terminal state. It consists of a series\nof steps, where at each step, the agent receives an observation of the environment's\ncurrent state, takes an action based on that observation, and receives a reward from\nthe environment. For example, in a chess game, an episode would be one complete\ngame from the initial board setup until checkmate, resignation, or a draw is reached.\nThe agent and environment, therefore, interact in a loop:\n1. The environment presents a state to the agent\n2. The agent takes an action based on the state\n3. The environment transitions to a new state and provides a reward to the agent\n4. The process repeats from step 1 (until the termination condition is met)\nIn this paper's multi-agent, multi-unit auction scenario, an episode includes only\na single step. The number of bids an agent can submit is the same for all agents for all"}, {"title": "3.2 Q-Learning", "content": "Q-Learning is an off-policy model-free RL method developed by Watkins (1989).\nThe method learns the value of an action in a given state without requiring a model"}, {"title": "3.2.1 Deep Q-Learning Network", "content": "A Deep Q-learning network (DQN) combines the principles of Q-learning with\nthe capabilities of Deep Neural Networks (DNN) (Mnih et al., 2015). It addresses\nthe limitations of traditional Q-learning, especially in environments with large or\ncontinuous state spaces where maintaining a Q-table becomes impractical. A DNN\nrefers to a subset of machine learning algorithms that use multi-layered structures\nof nodes or neurons to simulate the decision-making capabilities of the human brain.\nThe use of multiple layers in the network allows for the processing of complex, high-\ndimensional data.\nIn place of a Q-table, DQN uses DNNs as function approximators that estimate\nthe Q-values for each state-action pair. The Q-value function is initialised with a\nprimary neural network and a target network. It selects actions using an e-greedy\npolicy and stores experience tuples in a replay memory buffer. The Q-network is\nupdated periodically to minimize the loss between predicted and target Q-values.\nThrough repeated interaction and learning, the Q-network gradually converges to\nthe true Q-values, resulting in an optimal policy that maximizes expected returns."}, {"title": "3.3 Policy Gradient", "content": "Known as 'Vanilla' Policy Gradient (VPG) or \u2018REINFORCE', this RL algorithm\noptimises policies directly rather than using a value function as Q-Learning does.\nPolicy gradient methods aim to learn a policy $\u03c0 : S \u2192 A$ that maps states to actions.\nIt is optimised by adjusting policy parameters in the direction that increases the\nexpected return. This is achieved through gradient ascent on the expected return,\nwith the policy represented as a probability distribution over actions.\nThe algorithm uses a loss function that guides the update of policy weights based\non the expected return function $J(\u03b8)$. The loss function involves the log probabilities\nof actions taken in each state multiplied by the discounted rewards, averaged over\ntime steps (Sutton and Barto, 2018). The REINFORCE algorithm is synonymous\nwith Policy Gradient approaches and is typically expressed as the following gradient\nupdate rule (Williams, 1992):\n$0 \u2190 0 + \u03b1yrt\u2207oln \u03c0\u03bf(at|St),$\nwhere 0 represents the parameters or the policy $\u03c0$, $\u03b1$ is the learning rate, y the\ndiscount factor, rt the return from time step t, at is the action taken at time t, and\nst is the state at time t. The term $Veln\u03c0\u03bf(at|st)$ represents the gradient of the\nlogarithm of the policy's probability of taking action at in state st, with respect to\nthe policy parameters \u03b8.\nVPG and REINFORCE both suffer from high variance in gradient estimates,\nwhich delays convergence and negatively impacts exploration of the search space.\nGenerally, the principal source of variability in policy gradient approaches stems\nfrom the variability of accumulated rewards over long multi-step episodes. While\nthe RL scenarios in this paper comprise single-step episodes, the non-deterministic\nand dynamic environments used ensure the problem of high variance continues to\ndegrade results."}, {"title": "3.3.1 Deep Policy Gradient", "content": "Similar to how DQN extends traditional Q-Learning through the use of DNNs,\ndeep policy gradient methods replace the parameterized function of VPG with DNN\nfunction approximations. The increased representation power of DNNs allows for\nefficient search of larger state and action spaces and facilitates the use of techniques\nto reduce variance and encourage convergence.\nThe DNN used in Deep Policy Gradient approaches takes the current state of\nthe environment as an input and outputs a probability distribution over actions\n(for discrete action spaces) or parameters of a distribution from which actions are\nsampled (for continuous action spaces). The DNN's weights and biases are the\npolicy parameters that are then iteratively adjusted during training. Sophisticated\noptimisers can also be used with Deep Policy methods, which handle noisy gradient\nestimates much better than VPG approaches. An entropy term is commonly used\nwith the loss function to assist convergence and avoid sub-optimal policies.\nWhile an improvement over VPG, Deep Policy Gradient is very sensitive to the\nhyperparameter selection. The choice of learning rate, discount factor, and neu-\nral network architecture can result in considerable result variation. Unfortunately,\nchoosing the settings is a non-trivial task requiring repeated experimentation and\nadjustment."}, {"title": "3.4 Actor-Critic", "content": "The Actor-Critic approach combines aspects of both policy-based (the \u201cActor\u201d)\nand value-based (the \u201cCritic\") learning. The actor component is responsible for se-\nlecting actions based on the current policy, that is, a mapping from states to actions.\nThe critic evaluates the actions taken by the actor by estimating the value function,\nwhich measures the expected return from a given state using the current policy. The\ncritic's evaluation is then used to update the actor's policy towards more rewarding\nactions. One key advantage of actor-critic methods is their ability to handle contin-\nuous and discrete action spaces. In addition to this versatility, separating the policy\nand value function estimates can stabilize the learning process when compared to\nmethods using only policy or value function estimation.\""}, {"title": "3.4.1 Advantage Actor-Critic", "content": "The Advantage Actor-Critic (A2C) method is a variant of the Actor-Critic ap-\nproach that improves the actor's policy by using the advantage function. The advan-"}, {"title": "4 Simulations", "content": "In this section, we conduct various simulations using the six algorithms described\nabove. Since the primary aim of this paper is to assess various RL algorithms that are\nsuitable for simulating bidding behavior, we maintain a consistent number of bidders\nacross all simulations and only vary the total number of units available to allow for\na tractable analysis. We assume each auction features six bidders, each demanding\ntwo units. We explore three scenarios with regard to the total units available for\neach auction: one with four total units, one with six units, and one with eight units.2\nThe main variable used to assess the learning process in each simulation is called\nthe 'learning ratio', which is calculated as the value minus the bid, divided by the\nvalue. Intuitively, one expects an agent following an optimal strategy to submit\nbids that proportionally differ from their value in a stable manner. Therefore, the\nlearning ratio is a good metric to test such performance. Once we confirm that the\nlearning patterns demonstrate reasonably consistent learning, we then use revenue\nand efficiency as the metrics to evaluate auction performance. Revenue is simply the\nsum of payments made by all winning bidders, and efficiency is calculated as the sum\nof the K highest values (irrespective of the bids), where K is the number of units\navailable minus the sum of the values of those who won an item. When this term is\nzero, the auction has allocated the objects in a fully efficient manner."}, {"title": "4.1 Implementation", "content": "All simulations were programmed using Python 3.10 combined with the \u201cGymna-\nsium\" library (for developing and comparing reinforcement learning algorithms)Towers\net al. (2023), and \u201cStable Baselines 3\" (for the deep learning and actor-critic algo-\nrithm implementations) Raffin et al. (2021). All algorithms are implemented with\ndefault settings, and no hyper-parameter tuning was conducted.\nCustom Gymnasium environments were created for the simulated auctions, and\nalgorithms from Stable Baselines were modified for use in multi-agent auction scenar-\nios. All simulations were performed on the same computer, running the Linux-based\nUbuntu 22.04 operating system with an Intel Core i9-9900KS 4.00GHz CPU, 64Gb\nRAM, and a Nvidia RTX 3080 GPU with 10Gb RAM (all deep-learning algorithms\nwere run using the GPU)."}, {"title": "4.2 Preliminary simulations", "content": "To compare the performance of each algorithm, we aim to run them in an envi-\nronment where each method competes against the others. Prior to this, we pre-train\neach algorithm individually against agents with a random bidding strategy. The\nobjective of this initial training is to facilitate each agent's exploration of the search\nspace and learning the basic mechanics of the auction environments. In addition,\ntraining against agents that make random decisions helps prevent overfitting and\nco-adaption to other agent's bidding strategies (Zhu et al., 2024; Zang et al., 2023).\nAll six RL approaches were initially trained against five agents who made random\nbids at or below their item values. A total of 54 training sessions were performed\nwith the resultant model (or Q-table) saved. For the six algorithms, training runs\nwere performed using 100,000 episodes for each combination of auction type (DP,\nGSP and UP) and number of items on offer (4, 6 and 8)."}, {"title": "4.3 Simulation results", "content": "We begin with the learning results for the three auctions in three different scenar-\nios. Figure 1 shows the learning ratios of the six algorithms for the Discriminatory\nPrice auction across three scenarios. As shown in this figure, Proximal Policy Op-\ntimization (PPO) is the most stable algorithm in terms of the learning ratio and\nis, in some sense, the fastest. However, all six algorithms demonstrate reasonable\nconvergence to similar strategies by the final episodes. It should be noted that since\nbids are ranked from highest to lowest, the order in which an agent submits their\nhighest bids (first or second) does not impact the outcome. In all cases, agents bid\nbelow their values, a straightforward conclusion from the theory (Krishna, 2009).\nMoreover, when more units are available, agents tend to reduce their second bids\nfurther, which is also very intuitive.\nA similar pattern of learning is observed in Figure 2 for the Generalized Second\nPrice auction. Again, in all cases, the agents bid below their values, aligning with\ntheoretical predictions (Edelman et al., 2007). The DPN algorithm struggles to learn\nan optimal strategy, and as the number of items increases, its performance worsens.\nThis is mainly because DPN is significantly influenced by stochastic outcomes, and\nin the GSP format, where the payoff is determined by someone else's bid, DPN\nstruggles to find a consistent learning path. This issue is most pronounced in the\nUniform Price auction, where computing an expectation for the highest losing bid\nfrom a probabilistic perspective is much more challenging.\nFinally, Figure 3 illustrates the learning patterns for the Uniform Price auction."}, {"title": "5 Conclusions", "content": "In this article, we introduced six reinforcement learning algorithms to simulate\nbidding behavior in three different multi-unit auctions. While this paper extensively\ninvestigates RL-based approaches for simulating auctions, the task is far from com-\nplete. There is a pressing need for future studies to explore additional modeling\nassumptions for agents, including the possibility of allowing more than two bids per\nagent and increasing the number of items available. We note that such expansions re-\nquire significant computational resources and time commitment, which were beyond\nthe scope of this research."}]}