{"title": "GPA: Grover Policy Agent for Generating\nOptimal Quantum Sensor Circuits", "authors": ["Ahmad Alomari", "Sathish A. P. Kumar"], "abstract": "This study proposes a GPA for designing optimal\nQuantum Sensor Circuits (QSCs) to address complex quantum\nphysics problems. The GPA consists of two parts: the Quantum\nPolicy Evaluation (QPE) and the Quantum Policy Improvement\n(QPI). The QPE performs phase estimation to generate the search\nspace, while the QPI utilizes Grover search and amplitude\namplification techniques to efficiently identify an optimal policy\nthat generates optimal QSCs. The GPA generates QSCs by\nselecting sequences of gates that maximize the Quantum Fisher\nInformation (QFI) while minimizing the number of gates. The\nQSCs generated by the GPA are capable of producing entangled\nquantum states, specifically the squeezed states. High QFI\nindicates increased sensitivity to parameter changes, making the\ncircuit useful for quantum state estimation and control tasks.\nEvaluation of the GPA on a QSC that consists of two qubits and\na sequence of Rx, Ry, and S gates demonstrates its efficiency in\ngenerating optimal QSCs with a QFI of 1. Compared to existing\nquantum agents, the GPA achieves higher QFI with fewer gates,\ndemonstrating a more efficient and scalable approach to the\ndesign of QSCs. This work illustrates the potential computational\npower of quantum agents for solving quantum physics problems.", "sections": [{"title": "I. INTRODUCTION", "content": "RL is a machine learning approach that allows autonomous\nintelligent agents to learn by directly interacting with\nenvironment. These agents are rewarded for performing\nactions, and their goal is to find an optimal policy to maximize\nthese rewards, which results in solving the task of the\nenvironment [1, 4, 38]. As artificial intelligence progresses,\nthere is a greater need for algorithms that can learn rapidly and\neffectively, and speedups are more welcome than ever.\nQuantum control refers to the use of classical or quantum RL\nagents to automatically design or optimize quantum circuits to\naddress optimization tasks. The optimization objectives for the\nagent may include minimizing the number of gates, optimizing\nquantum states or entanglement, improving gate fidelity, and\nachieving other goals [1-14]. The agent uses optimization\ntechniques to explore the vast design space of possible quantum\ncircuits and selects designs that best meet the specified\noptimization objectives.\nOne set of metrologically useful states are squeezed states\nwhich give mild performance gains through pairwise\nentanglement generation [15]. In this case, a more complex\nsensor circuit is necessary to generate the quantum states that\nwe need. The task therefore is to generate the specific kind of\nentanglement that will lead to an optimal quantum advantage\nfor parameter estimation, for example, for the precision\nmeasurement of a phase shift. This phase shift could be\ngenerated by an inertial rotation, a magnetic field, or a variety\nof other possible terms in the system Hamiltonian of interest.\nThe resulting technology will allow for a better understanding\nof the physical world with a breadth of applications that bridge\nmany fields of science. We encapsulate this problem in a\ngeneral conceptual framework referred to as a QSC [16, 17, 18].\nA QSC executes a generalized Ramsey measurement on an\narray of qubits with encoding and decoding sequences\nrepresented by a chain of elementary gate operations, and the\nquantum design task is to select the optimal sequences [19]. The\ngoal is to reveal the absolute quantum limit for measurement\nsensitivity when the circuit is taken as a whole. The design of\nsuch a QSC is difficult when the circuit is deep (meaning the\ncascade of many consecutive elementary gate operations) due\nto the large number of possible permutations of gates and\nmeasurements that should amplify the correct amplitudes for a\nsensitive signal while mitigating the adverse effects of noise\nand decoherence. While there are a variety of alternate\napproaches in optimal control theory, they all require a cycle of\nmeasurement and feedback, or open loop control, where\nexquisite modeling of the system is essential. This can lead to\nthe degradation of the design utility when non-modelled\nperturbations are present [20]. These may include such\nimperfections as unitary errors due to gate inaccuracies,"}, {"title": "II. RELATED WORK", "content": "Existing QRL approaches can be classified into two types:\nquantum agents that learn in classical environments and\nscenarios where the agent and environment are fully quantum."}, {"title": "A. Quantum Agents in Classical Environments", "content": "Examples of the first type are found in [1, 2, 4, 6, 8, 10, 14].\nHeimann et al. developed a Deep Quantum Reinforcement\nLearning (DQRL) for training a wheeled robot to navigate\nthrough complex environments [1]. The wheeled robot is a\nDouble Deep Q-Network (DDQN) that interacts with an\nenvironment represented using Variational Quantum Circuits\n(VQCs). The authors used the data-reuploading method to\ntransform the classical features into the VQCs. The robot was\ntested using three different scenarios of the Turtlebot 2\nenvironment, such that the complexity of the environment\nincreased in each scenario. The results of the proposed DQRL\nshow that quantum machine learning can be applied for\nautonomous robotic enhancements.\nSkolik et al. developed a Variational Quantum Algorithm\nbased on Deep Q-Learning (VQA-DQL) for enhancing the\ntraining process of Parametrized Quantum Circuits (PQCs) that\ncan be used to solve discrete and continuous state space RL\ntasks [2]. The authors tested the proposed QRL approach using\nthe frozen lake and carte pole environments, and the results\nshow that QRL can outperform classical RL in terms of\ngenerating q-values for better learning performance agents.\nSamuel et al. developed a hybrid quantum-classical approach\nthat consists of quantum circuits with tunable parameters to\nenhance the performance of Noisy Intermediate Scale Quantum\n(NISQ) devices [6]. The proposed approach consists of VQCS\nthat represent classical DRL algorithms (e.g., experience replay\nand target network). The circuits represent a Quantum Neural\nNetwork (QNN) that is used to estimate the Q-value function,\nwhich is used to improve the decision-making and policy\nselection of NISQ systems by reducing energy costs.\nYun et al. presented a Centralized Training and\nDecentralized Execution Quantum Multi-Agent Reinforcement\nLearning framework (CTDE-QMARL), which addresses\nchallenges related to NISQ and non-stationary properties [8].\nThe proposed framework incorporates innovative VQCs and\ndemonstrates its effectiveness in a single-hop environment,\nimproving the performance of rewards received by agents\ncompared to classical frameworks.\nChen et al. introduced two deep quantum reinforcement\nlearning frameworks: one utilizes amplitude encoding for the\nCartPole problem, while the other employs a hybrid Tensor-\nNetwork VQC (TN-VQC) architecture to handle high-\ndimensional inputs of the MiniGrid [10]. The results show the\nadvantage of parameter saving with amplitude encoding and the\npotential for quantum reinforcement learning on NISQ devices\nthrough efficient input dimension compression.\nSequeira et al. presented a low depth policy for a\nreinforcement learning agent in a VQC [14]. The study\ndemonstrates an efficient approximation of the policy gradient\nwith logarithmic sample complexity relative to the number of\nparameters. Empirical results confirm the comparable\nperformance of the proposed VQC policy gradient to classical\nneural networks in benchmarking environments and quantum\ncontrol, utilizing few parameters, while also investigating the\nbarren plateau phenomenon in quantum policy gradients\nthrough analysis of the fisher information matrix.\nDong et al. developed a QRL approach based on quantum\nsuperposition and parallelism for autonomous state value\nupdating of agents [4]. The proposed QRL technique represents\nthe action as a quantum superposition state, such that the\neigenstate of the action is obtained by randomly observing the\nsuperposition state according to the collapse postulate of\nquantum measurement. The eigen action (eigen state)\nprobability is determined by the probability amplitude and\nparallelly updated according to rewards. The proposed QRL\nprovides a balance between exploration and exploitation and\ncan speed up the learning process through quantum parallelism."}, {"title": "B. Quantum Agents in Quantum Environments", "content": "Examples of quantum agents learning in quantum\nenvironments are found in [3, 5, 7, 9, 11, 12, 13]. Wei et al.\npresented a control RL algorithm called Deep Reinforcement\nLearning Quantum Experience Replay (DRL-QER) [7]. This\nquantum technique allows agents to receive experiences from\nthe replay buffer according to the complexity and the replay\ntimes of the surrounding environment, which results in a\nbalance between exploration and exploitation of their\nenvironment.\nAlomari & Kumar presented a Reinforcement Learning\nAutonomous Quantum Agent (ReLAQA) that integrates a\nGrover Autonomous Quantum Agent (GAQA) with a Quantum\nTicTacToe (QTTT) game to outperform classical methods in\naction selection using quantum techniques [5]. The ReLAQA\ndemonstrated faster and more efficient performance than\nclassical techniques using fewer computational resources. This\nwork paves the way for future developments in quantum\ncircuits for reinforcement learning robotics and metrological\nenhancements in NISQ devices.\nWu et al. implemented a Quantum Deep Deterministic\nPolicy Gradient (QDDPG) algorithm for efficient resolution of\nclassical and quantum sequential decision problems [9]. The\nproposed QDDPG enables one-shot optimization for generating\ncontrol sequences to achieve arbitrary target states, surpassing\nthe need for optimization per target state as required by standard\nquantum control methods. Additionally, the algorithm\nfacilitates the physical reconstruction of unidentified quantum\nstates.\nWiedemann et al. proposed a QPE method that combines\namplitude estimation and Grover search for solving quantum\nreinforcement learning tasks, achieving quadratically greater\nefficiency compared to classical Monte Carlo methods [11].\nUsing QPE, the authors developed a QPI approach that\niteratively improves policies using the Grover search. The\nauthors provide implementation and simulation for a two-\narmed bandit markov decision process to showcase the\neffectiveness of the proposed approach.\nBorah et al. developed a Deep Reinforcement Learning\nArtificial Neural Agent (DRLANA) to control highly nonlinear\nquantum systems toward the ground state [12]. By\nincorporating weak continuous measurements into the\nproposed DRLANA, successfully learns counterintuitive\nstrategies and achieves high fidelity. This approach\ndemonstrates effective control techniques for nonlinear\nHamiltonians, surpassing traditional methods.\nSivak et al. proposed a Model-Free Circuit-based\nReinforcement Learning (MFCRL) approach for training an\nagent on quantum control tasks, addressing the issue of model\nbias [13]. The agent learns the parameters of a control PQC\nthrough trial-and-error interaction with the quantum system,\nutilizing measurement outcomes as the sole source of\ninformation. The proposed approach enables rewarding the\nagent using experimentally available observables, facilitating\nthe preparation of nonclassical states, and executing logical\ngates on encoded qubits.\nDong et al. proposed the Quantum-inspired Reinforcement\nLearning (QiRL) algorithm for autonomous mobile robot\nnavigation control [3]. The proposed technique uses a\nprobabilistic action selection technique and a reinforcement\npolicy, which are inspired, respectively, by the quantum\nmeasurement collapse phenomenon and amplitude\namplification."}, {"title": "III. METHODOLOGY", "content": "Our proposed QRL approach consists of the GPA and the\nQSC environment represented. Fig. 1 illustrates the workflow\nof the proposed GPA approach."}, {"title": "A. The QSC Environment", "content": "To illustrate the efficient performance of the proposed GPA,\nwe consider a QSC that consists of two qubits and Rx, Ry, and\nS gates. Fig. 2 shows the structure of the QSC. This QSC has\nan optimal solution that we intend the GPA to generate, namely\nit should produce the N00N state since that maximizes the QFI\n[27].\nFirst, we initialize the qubits to the state |0). Then, we apply\na generalized Ramsey sequence to measure the QFI, which\nrepresents the accumulation of a relative phase between the two\nqubits collective dipole and a stable local oscillator that\nsynchronizes the timing. This phase is associated with the\ngenerator of the Ramsey sequence such as the Pauli-Z operator\n(Z) [28, 29]. The accumulation consists of a sequence of Rx,\nRy, and S gates that the GPA optimally generates to maximize\nthe QFI. The QFI and S are given in equations 1 and 2 as\nfollows.\n$QFI = \\frac{4((\\psi|Z^2|\\psi) \u2013 (\\psi|Z|\\psi)^2)}{n}$ (1)\nn is the number of the qubits in the QSC, which is 2. (4) is\nthe quantum state that is manipulated using the quantum gates\nthat are generated by the GPA. Z is the generator related to the\nrotation angle \u03b8. Z represents the projection of the angular\nmomentum of a quantum state along the z-axis by $\\frac{\\pi}{2}$ . It is\nused to measure the z-component of the quantum state angular\nmomentum.\n$S = exp(-i\\theta Z^2)$ (2)\nTo calculate the QFI using a fully quantum approach,\nwithout relying on classical techniques, we extended the QSC\nto incorporate 4 qubits. We then developed a quantum\nsubtraction operation to represent the QFI. This operation\ncomputes the difference between the sum of qubits 0 and 1 and\nthe sum of qubits 2 and 3. An additional 2 qubits were used to\nexecute this subtraction process, resulting in a 6-qubit circuit,\nas illustrated in Fig. 2."}, {"title": "B. The QPE", "content": "The idea of the QPE is to generate the search space using\nphase estimation, while the QPI utilizes Grover search and\namplitude amplification techniques over the search space to\nidentify an optimal policy that generates optimal QSCs. Fig. 3\nrepresents the phase estimation circuit of the QPE. It consists\nof four counting qubits |0), four Hadamard (H) gates, four\ncontrolled-unitary (CU) gates with rotation angles of $\\frac{\\pi}{4}$ ,\nQuantum Fourier Transform (QFT), four measurement gates,\nand the environment |4) that represents the QSC in Fig. 2 [30,\n31]. The output of the circuit X represents the value function of\neach estimated policy of the environment.\nThe QPE circuit performs a phase estimation on |4) to\ngenerate the search space, which is the evaluated policies.\nThese policies are generated by evaluating their value functions\nusing the following equation.\n$v_{\\pi}(s) = E_{\\tau \\sim H}[G(\\tau)|S_0 = s]$ (3)"}, {"title": "C. The QPI", "content": "The QPI consists of the Grover search and amplitude\namplification. It will perform a Grover search on the output\ngenerated by the QPE to find an optimal policy that will be used\nto generate quantum QSCs. For a fixed policy \u03c0, QPE can be\nrepresented as one unitary operation that maps\n|X)|0) |\u03a8QPE). \u03a4o implement the QPI, we need to prepare\nthe search space that is generated by the QPE using the\nfollowing equation.\n$A_{QPI} = \\prod_{\\pi \\epsilon P} OPE_{\\pi} \\cdot (H \\otimes Id)$ (6)\n$OPE_{\\pi}$ represents the value function that is generated by the\nQPE for a policy \u03c0\u2208 P, where P represents all the generated\npolicies. After generating the search space using AQPI, we\nimplement an oracle and use it to amplify the amplitudes of\npolicies that have higher returns. For a policy n and its\nestimated value function $\\nu_{\\pi}$, the oracle $O_{\\pi}$ is given by the\nfollowing equation, where t is the number of the qubits in the\nQPE circuit.\n\nelse\nUsing the oracle $O_{\\pi}$ that will target the optimal policy, we\nrepresent the QPI by the following equation.\n$OOPI = -AQPI \\circ S_0 \\circ AQPI + (Idp \\otimes O_{\\pi})$ (8)\nBased on the concept of amplitude amplification that is used\nin the Grover search, applying OOP1 a certain number of times\nto the search space generated by AQPI will amplify the\namplitude of the states that satisfy the condition (sin\u00b2(\u03c0x/\n2t)) > \u03bd\u03c0 [11, 26, 32, 33]. This will result in increasing the\nprobability of measuring a policy \u03c0 that has better performance\nthan \u03bd\u03c0.\nTo ensure the effectiveness of the amplitude amplification, it\nis essential to define the number of Grover rotations. This\nindicates how many times we run the QPE, which determines\nthe scaling of amplitudes for the desired states. While there\nexists an efficient number of Grover rotations in theory,\ndetermining this value in QPI is difficult due to the necessity of\nknowing the probability of achieving the desired state upon\nmeasuring the current state [19, 34, 35]. To select the number\nof Grover rotations L, we used L = int(k(r + V(s'))). This\nmethod takes in the reward r received by the GPA and the\nestimated value function V(s') of the new state s' visited by the\nGPA and returns L as an integer number. k is a learning\nhyperparameter that shows that L is proportional to (r + V(s'))\n[4, 5]. Here, sin(2L + 1)0 is a periodical function around\n(2L + 1)0. Too many iterations may generate small probability\namplitudes, then we select L = min[int(k(r +\nV(s'))), int(\\frac{\\pi}{4})]. This ensures that L remains within an\noptimal range, which efficiently maximizes the probability\namplitude. See section IV for more detailed information."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "We implemented the proposed GPA using local simulation.\nThe simulation is done on an Intel Core I-7 Dell computer with\n16 GB of RAM using Qiskit through Python. All experiments\nuse the statevector simulator. Furthermore, we compared the\nperformance of the GPA with the GAQA, which is a quantum\nagent that we developed, utilizing Grover search and amplitude\namplification [5]. The main difference between the proposed\nGPA and the GAQA is that the GPA uses phase estimation to\ngenerate the policies, which reduces the search space. Also, the\nGPA generates the QFI using the subtraction operations in Figs.\n2 and 7. These enhancements are not implemented in the\nGAQA. The results illustrate the efficient performance of the\nproposed GPA in generating optimal QSCs compared to the\nother approaches."}, {"title": "A. The Performance of the Proposed GPA", "content": "To demonstrate the computational power of the proposed\nGPA we integrated the QSC in Fig. 2 into the QPE phase\nestimation circuit as 4) to generate the policies, and then the\nQPI was applied to find the best policy that represents the\noptimal QSC in Fig. 1. Figs. 4, 5, 6, and 8 show the\nexperimental results that are generated using the statevector\nsimulator."}, {"title": "B. The Results of Comparing the GPA with the GAQA", "content": "We compared the performance of the GPA with the GAQA,\nwhich is a quantum agent that we developed, utilizing Grover\nsearch and amplitude amplification [5]. The main difference\nbetween the proposed GPA and the GAQA is that the GPA uses\nphase estimation to generate the policies by estimating their\nvalue function. This reduces the search space and efficiently\nallows the QPI to find the best policy that represents an optimal\nQSC. Additionally, the GPA executes the two episodes shown\nin Fig. 7 in parallel, unlike the GAQA, which runs each episode\nsequentially. Fig. 9 shows the workflow of the GAQA."}, {"title": "V. CONCLUSION", "content": "Quantum control involves the utilization of classical or\nquantum RL agents for designing and enhancing quantum\ncircuits to address optimization challenges. The optimization\nobjectives for the agent may include minimizing the number of\ngates, optimizing quantum states or entanglement, improving\ngate fidelity, and achieving other goals. In this proposed work,\nwe have developed a GPA approach for generating optimal\nQSCs capable of solving complex quantum physics problems.\nThe QSC performs a generalized Ramsey measurement on\nqubits using sequences of quantum gates, and the task of the\nGPA is to select the optimal sequences. The proposed GPA\nconsists of the QPE and the QPI. The idea of the QPE is to\ngenerate the search space, which consists of the evaluated\npolicies. QPI then uses Grover search and amplitude\namplification techniques to find an optimal policy that will be\nused to generate QSCs with high QFI and few quantum gates.\nHigh QFI indicates that the circuit is more sensitive to\nparameter changes and therefore more informative or useful for\nquantum state estimation or other quantum control tasks. Few\nquantum gates mean that the circuit is not complex and can be\nimplemented in quantum computers.\nTo evaluate the performance of the proposed GPA, we\nconsidered a QSC that consists of two qubits and a sequence of\nRx, Ry, and S gates. This circuit has an optimal N00N state that\nmaximizes the QFI, and the task of the GPA is to generate this\ncircuit while minimizing the number of gates. The results show\nthe efficient performance of the proposed GPA by generating\noptimal QSCs with a QFI of 1. Next, we simplified the QSC to\nenable the GPA to run for multiple episodes, and then we\ncompared its performance with the GAQA. The results show\nthat the GPA outperforms the GAQA by generating higher QFI\nvalues. The implementation details and simulations we\nconducted will illustrate how quantum agents can be utilized to\nsolve quantum physics problems.\nThe practical application of the GPA extends to quantum\nsensing and metrology, fields where critical measurements are\nessential. High sensitivity and fidelity are necessary to measure\nphysical parameters accurately, and by generating circuits with\nhigh QFI and few gates, the GPA supports the development of\nefficient and scalable QSCs. This can significantly enhance\nsensitivity in measurement systems and advance metrological\nprecision, pushing the boundaries of what can be detected or\nquantified in quantum systems. For future work, we intend to\nevaluate the performance of the proposed GPA on different\nquantum physics problems, which require the design of\ncomplex QSCs. Moreover, we intend to explore Hybrid\nClassical-Quantum agents (HCQAs) methods that combine\nclassical algorithms with quantum techniques to enhance\ndecision-making processes and optimize the design of QSCs."}]}