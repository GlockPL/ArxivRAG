{"title": "Generative AI for Health Technology Assessment: Opportunities, Challenges, and Policy Considerations", "authors": ["Rachael L. Fleurence", "Jiang Bian", "Xiaoyan Wang", "Hua Xu", "Dalia Dawoud", "Tala Fakhouri", "Mitch Higashi", "Jagpreet Chhatwal"], "abstract": "This review introduces the transformative potential of generative Artificial Intelligence (AI) and foundation models, including large language models (LLMs), for health technology assessment (HTA). We explore their applications in four critical areas, evidence synthesis, evidence generation, clinical trials and economic modeling: (1) Evidence synthesis: Generative AI has the potential to assist in automating literature reviews and meta-analyses by proposing search terms, screening abstracts, and extracting data with notable accuracy; (2) Evidence generation: These models can potentially facilitate automating the process and analyze the increasingly available large collections of real-world data (RWD), including unstructured clinical notes and imaging, enhancing the speed and quality of real-world evidence (RWE) generation; (3) Clinical trials: Generative AI can be used to optimize trial design, improve patient matching, and manage trial data more efficiently; and (4) Economic modeling: Generative AI can also aid in the development of health economic models, from conceptualization to validation, thus streamlining the overall HTA process. Despite their promise, these technologies, while rapidly improving, are still nascent and continued careful evaluation in their applications to HTA is required. To ensure their responsible use and implementation, both developers and users of research incorporating these tools, should familiarize themselves with their current limitations, including the issues related to scientific validity, risk of bias, and consider equity and ethical implications. We also surveyed the current policy landscape and provide suggestions for HTA agencies on responsibly integrating generative AI into their workflows, emphasizing the importance of human oversight and the fast-evolving nature of these tools.", "sections": [{"title": "Introduction", "content": "Artificial intelligence (AI) has been defined as \u201cthe science and engineering of making intelligent machines\u201d 1. Advancements in AI and especially machine learning (ML) have been rapidly progressing for several decades 2,3. However, the recent emergence of generative AI technologies such as large language models (LLMs) has ushered in a transformative era across numerous fields, including science, medicine, and other areas of human activity 4-6 (Box 1)\nGenerative Al employs sophisticated ML models, particularly a class of deep neural networks, that can generate language, images, and code in response to free text prompts provided by users. These models are trained using self-supervised learning techniques on vast amounts of existing data, allowing them to identify patterns and relationships in the underlying data autonomously. Their training requires substantial computational power, often relying on specialized hardware, especially graphics processing units (GPUs) 7.\nIn the field of Health Economics and Outcomes Research (HEOR), generative AI has the potential to transform evidence generation methods used in health technology assessments (HTA). Despite this promise, the exploration of opportunities, limitations, and risks associated with developing, evaluating, and deploying these models for supporting different approaches to evidence generation remains sparse, given the recency of these new tools. This paper seeks to fill this gap by offering a review of the landscape of generative AI's applications in methodological areas relevant to HTA development. The immediate audience is HEOR professionals, but we anticipate that it will also be useful to other stakeholders, particularly HTA evidence developers and users, regulators, and manufacturers.\nThe manuscript begins with a concise history of AI and overview of generative AI. Next, we examine the opportunities and challenges of utilizing generative AI to support significant aspects of HEOR. This includes (1) evidence synthesis methods, (2) evidence generation methods including Real-World Evidence (RWE), (3) clinical trials and (4) health economic modelling. We continue with a description of the current limitations and risks associated with generative AI tools. Next, we provide a summary overview of the current policy landscape on AI. We conclude with the results of a brief survey of HTA agencies gauging their current use of tools with generative AI and provide suggestions to HTA agencies on preparing for a more widespread adoption of generative AI tools."}, {"title": "A brief history of AI and generative Al", "content": "Al has evolved significantly since its conception, tracing a path of remarkable technological advancements. Introduced by Alan Turing in 1950, the Turing Test challenged the ability to distinguish between responses from a machine and a human during a natural language conversation, setting a foundational criterion for machine intelligence 8. In the following decade, significant progress was made with the development of symbolic methods to process natural language, such as the ELIZA chatbot in the 1960s 9. In the medical field during the 1970s, expert systems like Stanford University's MYCIN demonstrated capabilities in reasoning under uncertainty and providing decision rationales, using rule-based AI 10.\nThe 1990s ushered in the era of ML algorithms with the development of techniques like Support Vector Machines (SVM) and Random Forests. These methods excelled in tasks across various domains by effectively handling pattern recognition and decision-making processes. This period marked a shift from rule-based to data-driven AI, setting the stage for the next big leap. The introduction of deep learning in the 2000s further revolutionized AI capabilities 11. Deep learning models based on neural networks, enabled a wide range of complex applications from image recognition to natural language processing (NLP). It also led to breakthroughs such as IBM Watson and AlphaGo-systems that famously outperformed humans in Jeopardy and Go, respectively 12,13. While IBM Watson was not eventually successful in healthcare applications, AlphaGo led to the development of AlphaFold, which revolutionized the field of structural biology by solving the protein folding problem 14,15.\nThe emergence of generative AI technologies, especially the LLMs from OpenAI's GPT series, and the debut of ChatGPT in November 2022, marked a paradigm shift in the field of AI. Generative Al focuses on generating new content (e.g., text, images, videos) by learning from large existing data and it differs substantially from earlier AI technologies in several key aspects. First, LLMs employ the innovative transformer architecture, which features self-attention mechanisms to evaluate the relevance of each word in a sentence, regardless of their position. This enhances the model's grasp of context and linguistic subtleties 16. Pre-trained on vast text corpora, LLMs utilize billions of parameters to excel across a variety of tasks, such as summarization, translation, question answering, and code generation. These models are often called \"foundation models\" as the fundamental basis for a wide variety of downstream tasks. Since the development of ChatGPT, numerous LLMs have been created, consistently setting new benchmarks in performance across many tasks 7. Additionally, these generative AI technologies offer user-friendly interfaces that allow for direct interaction through natural language text or speech, significantly reducing the need for structured data preparation typical of earlier ML models like SVMs. LLMs can also quickly adapt to new tasks through natural language instructions and in-context learning.\nIn summary, generative AI technologies like LLMs represent a fundamental shift in how machines can understand and interact with human language, offering a breadth of capabilities and flexibility that were not possible with earlier AI models. Many applications of LLMs to different data modalities (e.g., text, images, and videos) have been investigated across different industrial sectors including healthcare and life science 17,18."}, {"title": "Applications of generative AI in literature reviews and evidence synthesis", "content": "This section identifies applications of generative AI, including the use of foundation models such as LLMs, in evidence synthesis studies such as systematic literature reviews (SLRs) and meta-analyses. LLMs can assist in generating a search strategy by proposing Mesh terms and keywords to input in biomedical search engines such as PubMed 19. Nevertheless, \u201challucinations\u201d can occur, where LLM-generated citations can be fabricated, manually verifying the terms proposed by the models is still necessary 19,20. Search engines are also improving their search returns with the integration of AI supported tools 21. Researchers have evaluated the capacities of foundation models to automate abstract and full text screening. When screening full text literature using highly reliable prompts, GPT-4's has been shown to have comparable performance to humans 22. For example, Guo et al. assessed the performance of OpenAI GPT in automating screening and found a high level of agreement between the model-generated results and a human reviewer 23. They also found that the model was able to explain its reasoning for excluding papers and correct its initial decisions 23. Robinson et al. have also evaluated LLMs' capacity to provide exclusion reasoning for abstracts and full texts 24. Hasan et al compared the agreement between GPT-4 and human reviewers in assessing the risk of bias using the Cochrane Collaboration's risk of bias tool. Their case study demonstrated a fair agreement between the LLM and the human reviewer 25. Several studies have tested the data extraction capacities of LLMs. Reason et al. found that GPT-4 >99% accuracy in replicating the data extraction in 4 network meta-analyses 26. Gartlehner et al. assessed the performance of CLAUDE-2 in extracting data elements from published studies compared to human extraction 27 and found that CLAUDE-2 achieved an overall accuracy of 96.3%. Schopow et al. found a high degree of concordance between ChatGPT (GPT-3.5 and 4.0) and human researchers on the data extraction task 28. Nevertheless, using GPT-4, another study only found moderate performance in assisting with data abstraction 22. In addition to data extraction, the foundation model can also be directed to generate the code to conduct meta-analyses. Reason et al. found that the LLM used was able to generate error-free code in R for network meta-analyses 26. However, Qureshi et al. found errors in code generated in an earlier GPT model, GPT-3.5 19. Finally, report writing has also been evaluated with researchers finding that the LLM is able to generate reasonable drafts 26.\nIn summary, these early applications show that there is promise in using foundation models to support a range of tasks required in SLRs but this rapid overview indicates that LLMs still have limitations in automating SLR tasks and human verification is necessary. Comparisons with human conducted tasks show reasonable accuracy in many case studies but not all, emphasizing the need for continued human validation at all steps 20. It is expected that both user expertise with foundation models, and the actual performance of foundation models themselves, will improve rapidly thereby likely augment the conduct SLRs in the near future."}, {"title": "Applications of generative AI to evidence generation", "content": "Increasingly, RWE is being used by a wide range of stakeholders in the HTA process 29. The potential benefits of using generative AI in RWE are profound, including increased efficiency and speed (e.g., faster data processing and analysis, and reduction in manual efforts), enhanced accuracy and consistency (e.g., minimizing human errors and standardizing the evidence generation processes), and the ability to combine multimodal data sources (e.g., structured clinical variables, unstructured clinical texts, imaging, and omics), enabling more holistic assessments.\nGenerative Al has the potential to improve the efficiency and quality of RWE generation. It is well-known that unstructured notes in EHRs, such as radiology reports and physician notes, are important for RWE generation 30 and the use of LLMs can make information extraction from notes easier and more efficient 31. This also leads to the development of domain-specific LLMs like GatorTron, GatorTronGPT and Me LLaMA, which are trained using large clinical texts 32-34. Additionally, generative AI is increasingly being customized for images and other formats of real-world data to generate evidence 35. However, one study also found that a range of foundation models had less than 50% accuracy in mapping descriptive text to the correct ICD and CPT codes 36. Like other areas, this field is still nascent although likely to improve rapidly. AI, including ML methods, has already been widely used in predicting patient health outcomes and modeling disease progression, especially with RWD, both of which are important components of the evidence generation process; recent advancements in generative AI have made it possible to be more accurate and efficient in creating synthetic or virtual patient cohorts and simulate treatment outcomes 37. This has potential applications such as creating synthetic or external control arms to optimize clinical trials, accelerating the evidence generation process 37. However, as discussed in the limitations section below, many of these areas are still nascent and continue to need human oversight and validation within each step of the process."}, {"title": "Applications of generative AI to clinical trials", "content": "Clinical trials, the gold standard for measuring clinical efficacy and safety of medical interventions, are another important component of evidence generation. Applications of generative AI to improve and accelerate the planning and conduct of clinical trials are still nascent and are likely to expand 37-39. First, in the area of clinical trial design, exploratory work is underway to leverage the summarizing and extracting capabilities of LLMs to optimize clinical trial protocols. This might be done by efficiently summarizing previous trials in the same disease area, reviewing their designs and outcomes 38, or by extracting the eligibility criteria of previous clinical trials from free text 40. LLMs might also be used to aid the design of clinical trial eligibility criteria for patients. For example, AutoTrial generates eligibility criteria for clinical trials using natural language to prompt LLMs 41. Second, LLMs are being explored quite extensively to improve and accelerate patient-matching for clinical trials. Several methods using LLMs have been proposed 42-44, some offering innovative approaches to increase patient confidentiality in the process 45. One approach uses ChatGPT to extract patient information from unstructured clinical notes and generate queries to search for potentially eligible clinical trials 46. Other approaches include InstructGPT to assist physicians in determining the eligibility of patients for clinical trials based on the patient's summarized medical profile 47 and TrialGPT to match patients to trials 48. Future areas also include approaches to predict which patients might be more likely to complete a trial, and approaches to increase patient retention in trials, for example through the provision of specialized trained chatbots to answer patient questions 38. Finally, foundation models are likely to be able to assist in clinical trial data management, for example by extracting data from clinical trial documents in structured format, such as Population, Intervention, Comparator, and Outcome (PICO) 49, matching images to text, or standardizing unstructured data but these remain exploratory approaches at this time. In a comprehensive review of ML techniques, Weissler et al. provide a helpful list of aspects of clinical trials that will likely be improved by ML tools which can be generalized to generative AI tools 50. Overall, the integration of generative Al has the potential to significantly enhances clinical trial development and execution, including design, optimization, emulation, and recruitment, signaling a potentially transformative shift in the landscape of clinical trials."}, {"title": "Applications of generative AI to health economic modeling", "content": "Generative AI has the potential to support the phases of economic model development, including model conceptualization, parameterization, model implementation, and evaluation and validation of model results 51-53. Chhatwal et al. explored the use of foundation models\u2014GPT-4 and Bing Chat-in conceptualization of a Markov model for hepatitis C and parameterization of the model 54. They found significant variability in how these models conceptualized disease progression. Despite GPT-4 performing better than Bing Chat, the study highlighted that foundation models alone are insufficient for robust conceptual model development. This underscores the likely need for specialized models that are specifically trained to develop more consistent and reliable health economic models. Reason et al. demonstrated the capability of GPT-4 to recreate published three-state partition survival models for non-small cell lung cancer and renal cell carcinoma 55. While the model construction was replicated with high accuracy, this approach required human intervention to provide contextual information on top of information specifying the model assumptions, methods and parameter values. This suggests that although generative AI can create a model code, significant human expertise is still necessary to guide the model conceptualization and parameterization. Generative AI could play an important role in structural uncertainty analysis, a typically resource-intensive process 52. By automating this aspect, generative AI can help in identifying and mitigating potential weaknesses in model structure more efficiently.\nIn summary, generative AI has the potential to support all stages of health economic model development, from conceptualization to validation. This transformation could lead to more efficient model development, reducing time and resource expenditure. By automating different phases of the modeling, generative AI could lower the barriers to conducting comprehensive health economic evaluations, making these tools more accessible to a broader range of healthcare decision-makers. Future research should explore the accuracy and reliability of fully automating all phases of health economic modeling. As AI becomes more integrated into health economic modeling, it will be crucial to establish rigorous standards to ensure the validity and reliability of Al-generated models. Future collaborations should focus on establishing guidelines and frameworks for the review of such models."}, {"title": "Limitations of generative AI in HTA applications", "content": "While generative AI offers promising applications in areas relevant to HTA, caution should be exercised when evaluating studies employing these technologies 4. Several frameworks have been developed to help guide the responsible use of AI in health care, many of which remain relevant to generative AI 56-58. Here, we briefly review and focus on three different categories of limitations of generative AI and foundation models: 1) Scientific validity and reliability, 2) Bias, equity and fairness and 3) Regulatory and ethical concerns."}, {"title": "Scientific validity and reliability", "content": "Scientific validity and reliability are critical aspects of research. Because LLMs are often trained on large corpora of publicly available information from the web, errors may be introduced especially in domain specific areas like health 4. A well noted limitation has been the generation of \u201challucinations\u201d where the model generates text that seems plausible, but it is in fact fabricated 20,59. This phenomenon occurs because models generate context based on learned statistical associations between words and sentences, not from an understanding of the knowledge in the corpora. Several strategies might be employed to improve the factual correctness of model outputs. First, improving the guidance used in the prompts provided to the models, a process known as prompt engineering, has been shown to improve the accuracy of the results. For example, chain-of-thought prompting is a technique in which a model is guided to generate step-by-step explanation or its reasoning process while arriving at an answer 60. Few-shot learning, a method where the model learns and generalizes from a very limited amount of data and examples, is another approach to improve model performance 61. Second, the use of retrieval augmented generation (RAG) allows the model to retrieve relevant information from external sources 21. This has been shown reduce the incidence of hallucinations and factually incorrect output 62. Third, foundation models can be fine-tuned with domain-specific data and knowledge, thus improving their performance on answering domain-specific questions 32,63,64. In science, findings should also be reproducible. Challenges to reproducibility with research using generative AI, include the randomness inherent in the training of foundation models and in the generation of the model parameters. Researchers are also proposing interesting approaches to support the reproducibility of studies and the accuracy of results. For example, Hasan et al. propose a framework for integrating foundation models into SLRs 25. Reason et al. run their models over 20 times to compare the results from different runs 26. Open sharing of data, code and results as well as the adoption of standards for reporting and transparency will likely help improve reproducibility 65,66. Finally, transparency in this area is challenging because of the 'black box' nature of close-source foundation models (e.g. GPT, Gemini), meaning it is unclear how models generate answers from the input queries and the model algorithms. Strategies proposed to improve transparency have included requirements for the outputs to cite which part of the dataset contributed to the answer, and \u2018explainable' AI 4. To achieve these goals and address the challenges above, the involvement of all stakeholders, with consideration of human-Al teaming, including human oversight, is critically needed in using these models 67."}, {"title": "Bias, Equity and Fairness", "content": "Foundation models can propagate or amplify biases introduced at many steps from model inception to deployment, resulting in systematic differences that may exacerbate inequities and cause harms to individuals and communities 68,69. First, systemic bias (or institutional or historical bias) may occur in the use of historical data to train foundation models, where marginalized groups may be underrepresented in datasets because of barriers to access to resources, perpetuating existing systemic biases in society 70. Second, computational and statistical biases may stem from errors that occur when the sample is not representative of the population and the algorithm produces a result that differs from the true underlying estimate 71,72. Bias may also occur when specific population groups are excluded from data collection, training, testing or subsequent analyses 68. Several strategies have been proposed to assess and mitigate the risk of bias in foundation models. Surveys on this topic have been published 73-75. One review of methods for addressing AI fairness and bias in biomedicine identified distributional and algorithmic approaches for addressing bias 73. Distributional approaches include strategies such as data augmentation, data perturbation and data reweighting, such as the generation of synthetic datasets to ensure balanced representation of all demographic groups 76. Federated learning may offer some potential to address biases, where models are trained on multi-institutional data without the need for data sharing, with the idea that access to more diverse populations in many different health systems may mitigate biases inherent to localized datasets 68. Algorithmic approaches change the algorithm itself and include approaches such as adversarial learning and loss-based methods, where the loss function is adjusted to penalize biased predictions 73."}, {"title": "Regulatory and Ethical Considerations", "content": "Regulatory frameworks focused on generative AI in biomedical research are under development in many countries. However, established data privacy and confidentiality laws such as the Health Insurance Portability and Accountability Act (HIPAA) in the United-States and General Data Protection Regulation (GDPR) in the European Union remain applicable. However, foundation models require a large amount of training data which could contain Protected Health Information (PHI) where absolute de-identification is not attainable, and re-identification risks remain non-zero 77,78. Further foundation models have the ability to memorize the data that they were trained on, posing a risk of reproducing PHI that they memorized during training 79. Enhancements such as synthetic data that replicates real patient data without personal identifiers 80 and enabling computations on encrypted data 81, are being explored to augment privacy protections. The issue of informed consent for AI driven research is becoming increasingly critical and is underscored by recent legal disputes over the use of patient EHR data to train LLMs 82. Ethical concerns will arise from the concerns around scientific validity, bias and fairness. Inaccurate information can lead to patient harm, for example if used to support an HTA submission that erroneously exclude underserved populations. In addition, emergent capabilities of LLMs may raise significant ethical challenges, particularly when the model's accurate performance cannot be transparently explained. For example, Giyocha et al. found that the AI model could identify a patient's self-reported race from imaging alone, raising ethical issues that will need to be thoughtfully addressed 83. In summary, given the early stages of the development and advances of generative AI, the themes discussed above will continue to be important and the object of warranted attention from many stakeholders within society."}, {"title": "Policy landscape", "content": "A rapid review of the policy landscape on the use of generative AI, including foundation models, in healthcare was conducted. Many governments have issued laws or policy guidance with respect to the use of AI in their countries that are not specific to healthcare or health research. The EU has issued the most comprehensive \u201chorizontal\u201d regulatory framework to date, the EU AI Act, which applies to all applications of AI across sectors 84. The regulation is binding on its member states and offers a risk-based approach to the use of AI technologies, with higher risk technologies associated with the most stringent requirements. Other countries have adopted \u201cvertical\u201d regulation, which applies to specific applications of AI or a specific sector. The UK has opted for a more flexible regulatory approach, with no centralized regulatory body; instead, different regulators will oversee AI applications in different sectors 85,86. In Japan, the government has encouraged voluntary governance by companies with sector-specific guidelines rather than AI-specific laws 87. In the US, there is no comprehensive federal AI regulation framework, but the Biden administration has issued an executive order on safe, secure and trustworthy AI, tasking multiple agencies to develop guidelines for AI systems and address the risk associated with federal government use of AI 88,89. International organizations have also issued guidance or statements on AI, including the Organization for Economic Co-operation and Development (OECD), the United Nations (UN) and United Nations Educational, Scientific and Cultural Organization (UNESCO) 90-92. In the health care sector, the World Health Organization (WHO) has provided guidance to member states on large multimodal models (LMMs), describing the benefits and risks of deploying LMMs in health care 93,94.\nIn the regulatory space, the European Medicines Agency (EMA) has issued a reflection paper on the use of AI/ML in the medicinal product life cycle, including product development, authorization, and post-authorization, emphasizing the need to meet all existing legal requirements, employ a risk-based approach, and promote AI trustworthiness 95.\nThe FDA has issued several discussion documents related to AI, specifically on the use of AI/ML-based software as a medical device 96,97 and more generally on AI use across the drug development process, including drug discovery, nonclinical phase of drug development, the clinical phase including in clinical trials, post-marketing surveillance and in manufacturing 98-100.\nIn the field of HTA, we reviewed the websites of the main HTA bodies and did not identify statements or guidance on the use of AI in HTA submissions at the time of writing (May 2024). The ISPOR HTA Roundtable conducted an informal survey with 18 members from North America, Latin America, Australasia and Europe, in April 2024 to gauge the level of internal use of generative AI in HTA agencies (They were not queried about the use of AI in submissions). Several agencies mentioned that they were still in an exploratory phase with generative AI, seeking to better understand how it may support their work and be used by manufacturers in their submissions. Some groups were not using AI at this time and one agency blocks the use of AI tools such as Chat-GPT at this time. Several agencies are evaluating software offered by private companies in the space of Systematic Literature Reviews (5 agencies) and one agency described experimenting with the use of generative AI in economic modelling. Finally, some agencies mentioned internal use of AI tools for translation and for general education purposes. The learnings from these explorations of AI tools will likely inform the methods and processes guidance HTA agencies will need to develop to guide companies in using generative AI in HTA submissions 101. Multi-stakeholder groups are also working on frameworks for the responsible and trustworthy use of AI, including for example, the National Academies of Medicine, the US National Institute of Standards and Technology (NIST), the Coalition for Health AI and the European Commission's HTx H2020 (Next Generation Health Technology Assessment) project 56-58,72,101. Finally, we note that scientific and medical journals are in the process of developing their policies with respect to the use of AI tools in publications submitted to their journals 102. Of note, the new NEJM AI journal encourages the use of LLMs in submissions with the assumption that they will enhance the quality of research and support its democratization 103."}, {"title": "Conclusion", "content": "The field of generative AI provides potentially transformative tools to augment and support the efficient generation of evidence to support HTA. In this article, we have provided a brief overview of the development of generative AI and reviewed applications relevant to HTA including: evidence synthesis, evidence retrieval and generation, clinical trials and economic modeling. Despite their promise, it is important to acknowledge that these technologies, while rapidly improving, are still nascent and continued careful evaluation in their application to H\u03a4\u0391 is required. To ensure their responsible use and implementation, both developers and users of research incorporating these tools, should familiarize themselves with their current limitations, including the issues related to scientific validity, risk of bias, equity implications and ethical considerations. This is a transformative and exciting time in scientific and medical research, but scientific research is a complex endeavor that impacts human health and will continue to require careful validation and human oversight for the foreseeable future."}, {"title": "Glossary", "content": "Artificial intelligence (AI): a broad field of computer science that aims to create intelligent machines capable of performing tasks typically requiring human intelligence.\nDeep Learning: a subset of machine learning algorithms that uses multilayered neural networks, called deep neural networks. Those algorithms are the core behind the majority of advanced Al models.\nFoundation model: a large scale pretrained models that serve a variety of purposes. These models are trained on broad data at scale and can adapt to a wide range of tasks and domains with further fine-tuning.\nGenerative AI: AI systems capable of generating text, images, or other content based on input data, often creating new and original outputs.\nGenerative Pre-trained Transformer (GPT): a specific series of LLMs created by OpenAI based on the Transformer architecture, which is particularly well-suited for generating human-like text.\nLarge Language Model: a specific type of foundation model trained on massive text data that can recognize, summarize, translate, predict, and generate text and other content based on knowledge gained from massive datasets.\nMachine learning (ML): a field of study within AI that focuses on developing algorithms that can learn from data without being explicitly programmed.\nMultimodal AI: an AI model that simultaneously integrates diverse data formats provided as training and prompt inputs, including images, text, bio-signals, -omics data and more.\nPrompt: the input given to an Al system, consisting of text or parameters that guide the AI to generate text, images, or other outputs in response.\nPrompt engineering: creating and adapting prompts (input) to instruct AI models to generate specific output."}]}