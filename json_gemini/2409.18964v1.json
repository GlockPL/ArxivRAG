{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "authors": ["Shaowei Liu", "Zhongzheng Ren", "Saurabh Gupta", "Shenlong Wang"], "abstract": "We present PhysGen, a novel image-to-video generation method\nthat converts a single image and an input condition (e.g., force and\ntorque applied to an object in the image) to produce a realistic, phys-\nically plausible, and temporally consistent video. Our key insight is to\nintegrate model-based physical simulation with a data-driven video gen-\neration process, enabling plausible image-space dynamics. At the heart\nof our system are three core components: (i) an image understanding\nmodule that effectively captures the geometry, materials, and physical\nparameters of the image; (ii) an image-space dynamics simulation model\nthat utilizes rigid-body physics and inferred parameters to simulate real-\nistic behaviors; and (iii) an image-based rendering and refinement mod-\nule that leverages generative video diffusion to produce realistic video\nfootage featuring the simulated motion. The resulting videos are realis-\ntic in both physics and appearance and are even precisely controllable,\nshowcasing superior results over existing data-driven image-to-video gen-\neration works through quantitative comparison and comprehensive user\nstudy. PhysGen's resulting videos can be used for various downstream", "sections": [{"title": "1 Introduction", "content": "Looking at the images in Fig. 1, we can effortlessly predict or visualize the poten-\ntial outcomes of various physical effects applied to the car, the curling stone, and\nthe stack of dominos. This understanding of physics empowers our imagination\nof the counterfactual: we can mentally imagine various consequences of an action\nfrom an image without experiencing them. We aim to provide computers with\nsimilar capabilities - understanding and simulating physics from a single im-\nage and creating realistic animations. We expect the resulting video to produce\nrealistic animations with physically plausible dynamics and interactions.\nTremendous progress has been made in generating realistic and physically\nplausible video footage. Conventional graphics leverage model-based dynamics\nto simulate plausible motions and utilize a graphics renderer to simulate images.\nSuch methods provide realistic and controllable dynamics. However, both the dy-\nnamics of physics and lighting physics are predefined, making it hard to achieve\nour goal of animating an image captured in the real world. On the other hand,\ndata-driven image-to-video (I2V) generation techniques [6,8,58] learn to cre-\nate realistic videos from a single image through training diffusion models over\ninternet-scale data. However, despite advancements in video generative mod-\nels [6, 8, 58], the incorporation of real-world physics principles into the video\ngeneration process remains largely unexplored and unsolved. Consequently, the\nsynthesized videos often lack temporal coherence and fail to replicate authentic\nobject motions observed in reality. Furthermore, such text-driven methods also\nlack fine-grained controllability. For instance, they cannot simulate the conse-\nquences of different forces and torques applied to an object.\nIn light of this gap, we propose a paradigm shift in video generation through\nthe introduction of model-based video generative models. In contrast to existing\npurely generative methods, which are trained in a data-driven manner and rely\non diffusion models to learn image space dynamics via scaling laws, we propose\ngrounding object dynamics explicitly using rigid body physics, thereby inte-\ngrating fundamental physical principles into the generation process. As classical\nmechanics theory reveals, the motion of an object is determined by its physical\nproperties (e.g., mass, elasticity, surface roughness) and external factors (e.g.,\nexternal forces, environmental conditions, boundary conditions).\nWe tackle our image-to-video generation problem via a computational frame-\nwork, PhysGen. PhysGen consists of three stages: 1) image-based physics un-\nderstanding; 2) model-based dynamics simulation; and 3) generative video ren-\ndering with dynamics guidance. Our method first infers object compositions\nand physical parameters from the input image through large visual foundation\nmodel-based reasoning (Sec. 3.1). Given an input force, we then utilize the in-\nferred physical parameters to perform realistic dynamic simulations for each ob-\nject, accounting for their rigid body dynamics, collisions, frictions, and elasticity"}, {"title": "3 Approach", "content": "Given a single input image, our goal is to generate a realistic T-frame video fea-\nturing rigid body dynamics and interactions. Our generation can be conditioned\non user-specific inputs in the form of an initial force Fo and/or torque To. Our\nkey insight is to incorporate physics-informed simulation into the generation pro-\ncess, ensuring the dynamics are realistic and interactive, thus producing large,\nplausible object motion. To achieve this, we present a novel framework consist-\ning of three stages: physical understanding, dynamics generation, and generative\nrendering. The perception module aims to reason about the semantics, collision\ngeometry, and physics of the objects presented in the image (Sec. 3.1). The dy-\nnamics module then leverages the input force/torque, as well as the inferred\nshape and geometry, to simulate the rigid-body motions of the objects, as well\nas object-object and object-scene interactions, such as friction and collisions\n(Sec. 3.2). Finally, we convert the simulated dynamics into pixel-level motion\nand combine image-based warping and generative video to render the final video\n(Sec. 3.3). Fig. 2 depicts the overall framework."}, {"title": "3.1 Perception", "content": "Simulating dynamics on an image requires a holistic understanding of object\ncompositions, materials, geometry, and physics. Toward this goal, we designed"}, {"title": "3.2 Image space dynamics simulation", "content": "Given the foreground objects with physical properties, we use rigid-body physics\nfor dynamics simulation in image spaces. We choose to perform simulations in\nimage spaces for three reasons: 1) image-space dynamics are better coupled with\nour output video; 2) a full 3D simulation requires complete 3D scene recon-\nstruction and understanding, which remains an unsolved problem from a single\nimage; 3) image-space dynamics can already cover various object dynamics and\nhave been widely used in prior work [4, 20, 36, 47, 81]. Specifically, at a given\ntime $t$, each rigid object $i$ is characterized by its 2D pose and velocity at its\ncenter of mass. The position includes a translation $t^{i}(t) \\in \\mathbb{R}^{2}$ and a rotation\n$R^{i}(t) \\in SO(2)$ specified in a world coordinate. The velocity includes linear ve-\nlocity $v^{i}(t) \\in \\mathbb{R}^{2}$ and angular velocity $w^{i}(t) \\in \\mathbb{R}^{2}$. Hence, the state of the each\nobject $i$ at time $t$ can be represented as $q^{i}(t) = [t^{i}(t), R^{i}(t), v^{i}(t), w^{i}(t)]$.\nFollowing [26], the rigid body motion dynamics is given by Eq. (1) for each\nobject. For simplicity, we omit the object index $i$ in the following:\n$\\frac{d}{dt} \\begin{bmatrix}\nt(t) \\\\\nd R(t) \\\\\nv(t) \\\\\nw(t)\n\\end{bmatrix} = \\begin{bmatrix}\nv(t) \\\\\nw(t) \\times R(t) \\\\\n\\frac{F(t)}{M} \\\\\nI(t)^{-1}(\\tau - w(t) \\times I(t)w(t))\n\\end{bmatrix}$                                        (1)\nwhere $F$ is the force, $\\tau$ is the torque, $M$ is the mass of the object, $I(t) =$\n$R(t)IR(t)^{\\intercal}$ is the rotation inertia in world coordinates, $I \\in \\mathbb{R}^{2\\times 2}$ is the iner-\ntia matrix in its body coordinates. This inertia indicates its resistance against\nrotational motion. The state $q(t)$ is given by integrating Eq. (1):\n$q(t) = q(0) + \\int_{0}^{T} \\frac{d}{dt} q(t) dt = q^{i}(0) + \\sum_{i=1}^{T} \\frac{d}{dt}q(t)=t_{i} \\Delta t$                              (2)\nwhere $q(0)$ is the initial condition specified in the input image. We compute\nthe integral using numerical ODE integrations, e.g. Euler method [21]. Given\nthe initial state of the objects, the physical motion simulation module synthesis\neach foreground object future motion. The location of each object is updated by\nthe affine transformation $T(t) = [R(t), t(t)]$ from the initial location.\nExternal force and torque. For each object, the external forces $F(t)$ and\ntorques $\\tau$ include gravity, friction, and elasticity between the object's surface and\nthe environment. Specifically, we consider the initial external force and torque\nfrom user input, as well as gravity, rolling friction, and sliding friction."}, {"title": "3.3 Rendering", "content": "Given the simulated motion sequence {$q^{i}(0), ..., ., q^{i}(t), q^{i}(t), \\dots. \\dots. \\dots. \\dots.$} and the input im-\nage C, the rendering module outputs the rendered video {..., C(t), . . . }. There\nare a few desiderata for generating realistic video: it needs to be temporally\nconsistent, complete, reflect the lighting changes as the object moves, and accu-\nrately represent the simulated image space dynamics. To achieve this goal, we\ndesign a novel motion-guided rendering pipeline consisting of an image-based\ntransformation and composition module to ensure motion awareness, a relight-\ning module to ensure the plausibility of lighting physics and a generative video\nediting module that retouches the composed and relit video to ensure realism.\nComposited video. Given the object state from the physical motion module,\nwe compose an initial video by alpha-blending the foreground scene with the\nstatic inpainted background from Sec. 3.1. The foreground scene is rendered\nby performing forward-warping from the input image to future frames using\nthe affine transformation $T(t)$. The alpha channel is computed using the same\nprocedure, with the input being the segmentation mask in Sec. 3.1:\n$X(t) = composite(B, warp(X^{0}, T^{0} (t)) . . . warp(X^{i}, T^{i} (t)) . .....)$\nwhere $X^{i}$ are segmented input image of i-th object. Apart from the RGB se-\nquence V = {... X(t) . . . }, we also use the same affine warping and composition\nto compute the blended albedo map $\\hat{A}(t)$ and surface normal $\\hat{N}(t)$ from $\\hat{A}(0)$\nand $\\hat{N}(0)$ respectively, which is later used for relighting."}, {"title": "Relighting", "content": "Our relighting module simulates the changes in shading due to\nobject movement. Specifically, it takes the RGB image X(t), the transformed\nalbedo $\\hat{A}(t)$ and surface normal $\\hat{N}(t)$ at each time t, as well as the estimated\ndirectional light L as input. We then perform reshading using the the Lambertian\nshading model f for foreground objects: $\\hat{X}(t) = f(\\hat{x}(t), \\hat{A}(t), \\hat{N}(t), L)$, which\nreturns the relit image X(t) for each frame t."}, {"title": "3.4 Generative refinement with latent diffusion models", "content": "One limitation of the single-image based relighting model is that it neglects\nthe background and cannot handle complicated lighting effects due to complex\nobject-object interactions, such as ambient occlusion and cast shadows. More-\nover, the composition results in unnatural boundaries. To address these issues\nand enhance realism, we incorporated a diffusion-based video to refine the relit\nvideo, denoted as V = {. . ., X(t), . . . }, and obtain the final video output.\nSpecifically, we incorporate a pretrained video diffusion model to refine our\nvideo V. We first use the pretrained latent diffusion encoder [63] to encode the\nguidance video V into a latent code z. Inspired by SDEdit [54], we add noise\nto the guided latent code and gradually denoise it using the pretrained video\ndiffusion. Given that the content in the guided latent code is already satisfactory,\nwe do not perform the denoising process from scratch. Instead, we define an\ninitial noise strength s where s\u2208 [0,1] controls the amount of noise added to\nz. In practice, we find that a certain approach finds a good trade-off between\nfidelity and realism. At each denoising step, we ensure the foreground objects\nare as consistent as possible with the guidance (copied from the guided latent\ncode) while synthesizing new content (e.g., shadows) in the background (using\nthe denoised latent code). To this end, we use a fusion weight w to control how\nmuch of the generated latent to inject into the foreground; the fusion weight is\ngradually increased during the denoising as the perturbation of noise decreases.\nWe also define a fusion timestamp d as the signal to stop the fusion. The final\ndenoised latent code z* is then decoded to our final video output V. Please see\nAppendix A.3 for a detailed algorithm."}, {"title": "4 Experiments", "content": "Implementation details. For physical body simulation, we use a 2D rigid\nbody physics simulator Pymunk [13]. For each experiment, we run 120 simu-\nlation steps and uniformly sample 16 frames to form the videos. We set the\ngeneration resolution at 512 \u00d7 512 so that frames can easily fit into the diffusion\nmodels. For video diffusion prior model, we use SEINE [19]. For inference, DDIM\nsampling [68] is used and the noise strength i set to s = 0.5, i.e., executing 25/50\nsteps for denoising. The latents fusion stops at timestamp \u03b4 5."}]}