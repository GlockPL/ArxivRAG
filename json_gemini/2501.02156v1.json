{"title": "The Race to Efficiency: A New Perspective on AI Scaling Laws", "authors": ["Chien-Ping Lu"], "abstract": "As large-scale AI models expand, training becomes costlier and sustaining progress grows harder. Classical scaling laws (e.g., Kaplan et al. [9], Hoffmann et al.[10]) predict training loss from a static compute budget yet neglect time and efficiency, prompting the question: how can we balance ballooning GPU fleets with rapidly improving hardware and algorithms? We introduce the relative-loss equation, a time- and efficiency-aware framework that extends classical AI scaling laws. Our model shows that, without ongoing efficiency gains, advanced performance could demand millennia of training or unreal-istically large GPU fleets. However, near-exponential progress remains achievable if the \"efficiency-doubling rate\" parallels Moore's Law. By for-malizing this race to efficiency, we offer a quantitative roadmap for balanc-ing front-loaded GPU investments with incremental improvements across the AI stack. Empirical trends suggest that sustained efficiency gains can push AI scaling well into the coming decade, providing a new perspective on the diminishing returns inherent in classical scaling.", "sections": [{"title": "Introduction", "content": "The future trajectory of AI scaling is widely debated: some claim that ever-growing models and datasets are nearing practical and theoretical limits [1, 2, 3], while others maintain that ongoing innovations will continue driving exponential growth [4, 5, 6]. For organizations weighing these divergent views, a central question arises: should they \"front-load\" GPU capacity\u2014relying on the predictable (yet potentially plateau-ing) gains promised by static scaling laws or invest in R&D for (possibly unpre-dictable and hard-to-measure) efficiency breakthroughs, model innovations, and fu-ture hardware enhancements? Ultimately, if diminishing returns do indeed loom, how severe might they be in terms of both time and hardware capacity (see Table 2 for an illustrative range of outcomes)?\nTo address this conceptual gap, we note that any truly enduring \u201cexponential\u201d trend hinges on improving an efficiency metric that reflects both the outcomes and the costs (time, energy, etc.). Historically, Moore's Law embodied such progress by showing that transistor count per unit area could approximately double every two years [7], while Dennard Scaling [8] kept power usage in check. Turning to AI, classical scaling laws quantify how training loss predictably decreases with increasing compute, provided balanced model, data, and training configurations\u2014referred to as the compute-optimal condition [9, 10]. However, these laws are inherently static: they do not account for the severity of diminishing returns or specify how quickly efficiency must improve to offset these trends over time."}, {"title": "Key Idea: Making Scaling Time- and Efficiency-Aware", "content": "Classical scaling laws [9, 10] posit that \\(L_0 \\propto C_0^{-\\kappa}\\) for a given static compute budget \\(C_0\\). We extend this snapshot into a time- and efficiency-aware framework. Let \\(L_0\\) represent the \u201cbaseline\u201d loss associated with an initial compute budget. If \\(\\gamma\\) denotes the annual efficiency-doubling rate (in yr\\(^{-1}\\)), reminiscent of the 0.5 times per year doubling of transistor density in Moore's Law, we derive a relative-loss equation that captures how loss evolves over time:\n\\(L(t) = L_0 R(t), \\quad R(t) = \\left(1 + \\frac{2^{\\gamma t} - 1}{\\gamma \\ln(2) \\times 1 \\text{ yr}} \\right)^{-\\kappa} \\)\nHere, \\(L(t)\\) represents the training loss at time \\(t\\) (in years), \\(L_0\\) is the initial loss, \\(R(t)\\) is the relative loss, and \\(\\kappa\\) is the unitless scaling exponent. The equation captures how training loss evolves over time as efficiency improves. Even with diminishing returns (\\(\\kappa \\ll 1\\)), rapid efficiency gains (\\(\\gamma > 0\\)) can sustain near-exponential progress in AI scaling."}, {"title": "Mathematical Foundation", "content": "We now formalize how to extend classical, static AI scaling laws into a dynamic, time-dependent framework. In particular, we derive a relative-loss equation that unifies traditional loss-compute relationships with a \u201cMoore's Law-like\" perspective on efficiency gains."}, {"title": "Key Parameters and Notation", "content": "Table 1 summarizes the main parameters and variables. In brief, we measure:\nLogical (Model) FLOPs, defined to remain stable, vendor-agnostic, and con-sistent with industry standards;\nPower and time, representing real-world usage averaged over a suitable timescale (mean-field assumption);\nA scaling exponent k, which captures how loss decreases with total compute, and an efficiency-doubling rate y, quantifying how rapidly \u201cusable compute\" can grow per unit time and power."}, {"title": "Continuous Efficiency Gains (E(t))", "content": "We model efficiency as a continuously evolving resource, reminiscent of how Moore's Law once described periodic doubling in transistor density. Concretely, let\n\\(E(t) = E_0 \\times 2^{\\gamma t}, \\quad \\text{(units: PFLOP/yr/MW)},\n\\)\nwhere y denotes the annual rate at which efficiency doubles, and \\(E_0\\) is the baseline efficiency at \\(t = 0\\). Although real improvements may come in discrete jumps, this continuous approximation is mathematically convenient and mirrors how large-scale phenomena (e.g., population growth) are often modeled as exponentials.\nRelation to Power and Hardware. Efficiency, as used here, is dimensionally Logicatimex Power. In practice, raising E(t) can come from:\nBetter hardware (e.g., next-gen accelerators, lower-precision logic, advanced mem-ory or networking),\nAlgorithmic gains (e.g., quantization, expert routing),\nSoftware optimizations (kernel-level efficiency, distributed training overheads), or\nAny combination of the above.\nWe simply aggregate all these factors into a single, time-varying E(t)."}, {"title": "Cumulative Compute as an Integral (C(t))", "content": "Classical scaling laws treat compute C as a static budget. Here, we let C(t) accu-mulate over time:\n\\(C(t) = C_0 + \\Delta C(t),\n\\)\nwhere \\(C_0\\) is the initial snapshot of compute (reflecting prior investments), and\n\\(\\Delta C(t) = \\int_0^t E(\\tau) P(\\tau) d\\tau.\n\\)\nIf P(\u03c4) denotes the power allocated to training, then \\(E(\\tau) P(\\tau)\\) is the instantaneous compute throughput (PFLOP/yr). Integrating from 0 to t yields the total additional compute AC(t) beyond the original \\(C_0\\).\nMean-Field Assumption. Rather than modeling P(\u03c4) at every instant, we ap-proximate\n\\(P(\\tau) \\approx P_0,\n\\)\nthe average power over one year. This \u201cmean-field\u201d approach is common in physics (e.g., average particle collisions) and engineering (e.g., duty cycles). Importantly, this assumption represents an upper bound on performance, as any deviations\u2014such as fluctuations in power usage or suboptimal resource allocation\u2014will result in slower progress in reducing training loss. This makes the mean-field assumption not only mathematically convenient but also practically significant, as it provides an opti-mistic baseline for evaluating the impact of efficiency improvements.\nFor example, consider a training run for LLaMA 3 with 405B parameters, which used approximately 30.8 million GPU-hours across 16,000 H100 GPUs. The peak power might reach 16MW over a few months. However, spreading this total energy over an entire year yields an average power \\(P_0\\) of approximately 3.5 MW. Instead of modeling short-lived peaks, we \u201csmooth\u201d usage across 12 months to adopt a single constant \\(P_0\\), simplifying the analysis.\nBecause classical scaling laws directly link \\(C_0\\) to \\(L_0\\), we define\n\\(C_0 = E_0 P_0 \\times 1 \\text{ yr}, \\quad \\frac{C_0}{E_0 P_0} = 1 \\text{ yr}.\\)\nHere, \\(C_0\\) is simply the compute obtained by running efficiency \\(E_0\\) at power \\(P_0\\) for one year. Changing hardware details (e.g., front-loading more GPUs) merely rescales"}, {"title": "Deriving the Relative-Loss Equation", "content": "In the static regime, scaling laws state that the training loss L decreases as a power-law of compute, \\(L \\propto C^{-\\kappa}\\). Introducing time into the compute accumulation C'(t) transforms this into a time-varying equation:\n\\(L(t) = L_0 \\left(1 + \\frac{\\Delta C(t)}{C_0} \\right)^{-\\kappa}.\\)\nUsing the integral form for AC(t) and noting \\(C_0 = E_0 P_0 \\times 1 \\text{ yr}\\), plus the integral \\(\\int 2^{\\gamma t} dt = \\frac{2^{\\gamma t}-1}{\\gamma \\ln 2}\\), we obtain:\n\\(\\Delta C(t) = \\frac{E_0 P_0}{\\gamma \\ln(2)} (2^{\\gamma t} - 1).\n\\)\nHence,\n\\(L(t) = L_0 \\left(1 + \\frac{\\Delta C(t)}{C_0} \\right)^{-\\kappa} = L_0 \\left(1 + \\frac{2^{\\gamma t} - 1}{\\gamma \\ln(2) \\times 1 \\text{ yr}} \\right)^{-\\kappa},\\)\nwhich can be rewritten in relative-loss form:\n\\(R(t) = \\frac{L(t)}{L_0} = \\left(1 + \\frac{2^{\\gamma t} - 1}{\\gamma \\ln(2) \\times 1 \\text{ yr}} \\right)^{-\\kappa}.\\)\nThe relative-loss equation captures how a baseline loss \\(L_0\\) evolves over time, provided efficiency improves at a rate y. As \u03b3 increases, R(t) declines more rapidly.\nInterpretation.\nStatic vs. Dynamic. The relative-loss equation extends static scaling laws into a time- and efficiency-aware domain. When efficiency does not improve (y = 0), the system effectively reverts to \u201cstatic\u201d scaling. One could, in principle, keep training on the same hardware for a very long time, making AC grow linearly with time.\nMoore's Law-Like Perspective. By letting \"efficiency\u201d double over time (instead of having a single snapshot), the analysis aligns with the historical notion of transistor-density doubling. Here, y denotes how quickly one can \u201crefresh\u201d hard-ware and/or optimize software."}, {"title": "Timescale and Cross-Project Scope", "content": "One-Year Baseline. Our derivation adopts a one-year baseline (via the mean-field assumption (3.2)), so AC(t)/Co measures how compute accumulates beyond that one-year mark. In principle, any timescale\u2014weeks or months\u2014could be used, yielding the same curve shape; but one year naturally aligns with budgeting cycles and hardware-refresh periods. Thus, statements like \u201cdoubling efficiency every six months\u201d or \u201cit takes five years to reduce loss below 0.68\" gain clear operational meaning for R&D planning.\nMulti-Year, Cross-Project Context. Although the equations might appear to describe a single, multi-year training run, organizations typically develop AI systems iteratively across multiple releases\u2014upcycling existing models [16, 17], refining data pipelines, and introducing new hardware. Each iteration effectively raises efficiency (\u03b3 > 0), while training loss (e.g., cross-entropy) offers a monotonic yardstick: newer\""}, {"title": "Analysis of Scaling Behaviors", "content": "Having established a time-based framework for AI scaling, we now examine how its two principal parameters\u2014the scaling exponent k and the annual efficiency-doubling rate y shape long-term performance."}, {"title": "Reduction to Classical Scaling Laws at \u03b3", "content": "Starting from the relative-loss equation:\n\\(R(t) = \\left(1 + \\frac{2^{\\gamma t} - 1}{\\gamma \\ln(2) \\times 1 \\text{ yr}} \\right)^{-\\kappa},\\)\nwhere L(t) = LoR(t), we now set y = 0. To handle the limit 2^\\gamma t \u2212 1 \u2192 ytln(2) for small yt, we recall the first-order expansion 2^x \u2248 1 + x ln(2) for x \u2192 0. Thus,\n\\[\\frac{2^{\\gamma t}-1}{\\gamma \\ln(2) \\times 1 \\text{ yr}} \\xrightarrow{\\gamma \\to 0} \\frac{\\gamma t \\ln(2)}{\\gamma \\ln(2) \\times 1 \\text{ yr}} = t.\\]\nHence, at y = 0,\n\\(L(t) = L_0 \\left(1 + \\frac{t}{1 \\text{ yr}} \\right)^{-\\kappa}.\\)\nInterpretation. When y = 0 (no time-based efficiency improvements), this out-come reduces to the static-scaling form \\(L \\propto C^{-\\kappa}\\). However, we now see how running the same hardware and software for an additional time t merely accumulates compute in a linear fashion. As the nearly flat y = 0 curve in Figure 1 shows, one must either (a) train for an exceedingly long duration or (b) invest in a massive up-front cluster at t = 0 to further reduce loss. Thus, the original diminishing returns \\(L \\propto C^{-\\kappa}\\) are now made explicit in both time (t) and space (Lo), underscoring why progress inevitably stalls without ongoing efficiency gains (\\(\\gamma > 0\\))."}, {"title": "Asymptotic Behaviors", "content": "Recall that\n\\(R(t) = \\left(1 + \\frac{2^{\\gamma t} - 1}{\\gamma \\ln(2) \\times 1 \\text{ yr}} \\right)^{-\\kappa} \\quad \\text{(Equation 3.4)};\n\\)\nand hence,\nR(t) \\propto 2^{-\\kappa \\gamma t} \\quad \\text{for large } t.\nSince \\(\\kappa \\gamma > 0\\), R(t) declines exponentially as \\(t \\to \\infty\\), mirroring the vanishing returns one encounters when investing ever more compute. This parallels the leveling-off observed in Figure 1.\nFor added intuition, consider a hypothetical analogy to historical Moore's Law: if one estimates an effective \\(\\kappa \\approx 0.4\\), then a doubling rate of \\(\\gamma = 0.5\\) (doubling roughly every two years) might suffice to maintain improvements for a surprisingly long time.\nBy contrast, modern AI scaling laws typically have much smaller \\(\\kappa \\approx 0.05\\). Achiev-ing equally robust gains within a decade may therefore require \\(\\gamma \\geq 2\\) (efficiency doubling every six months) or faster. As we increase y, we effectively prolong what could be termed the \u201cproductivity cycle\"-the window in which near-exponential improvements remain viable."}, {"title": "Sensitivity Analysis", "content": "Our model assumes a constant average power budget \\(P_0\\) under the mean-field as-sumption (Equation (3.2)). In reality, infrastructure, workloads, and hardware may fluctuate, introducing uncertainty. To quantify how such changes affect predictions, we add a perturbation \u03c4via\n\\[\\frac{C_0}{E_0 P_0} = 1 + \\tau \\text{ yr}.\\]\nIf y is a target relative loss (say, \\(R(t(\\tau)) = y\\)), then near \\(\\tau = 0\\), the time-to-target t(\u03c4) is approximately 1/(\\gamma ln 2), regardless of y. This implies a consistent first-order sensitivity across different baselines."}, {"title": "Efficiency Doubling Rates and Time Horizons", "content": "Finally, consider how changing y impacts the time needed to attain specific relative-loss targets. Figure 3 tracks the time to achieve various y \u2208 [0.5, 09] for different \u03b3 values. From a practical standpoint:\nHistorical Moore's Law (\\(\\gamma = 0.5\\)): Doubling every two years may suffice for modest goals over long timescales, but it can push more stringent targets (e.g., 0.7 or lower) out to 15-20 years- far beyond most industry planning cycles.\nModern Demands (\\(\\gamma > 2\\)): Efficiency doublings every 6\u201312 months (y = 2 or 3) compress the entire schedule to a handful of years, in line with contemporary AI's rapid iteration."}, {"title": "Implications and Case Studies", "content": "Having introduced a time- and efficiency-aware perspective on AI scaling, we illus-trate its consequences through several thought experiments. First, we examine two theoretically static scenarios (\\(\\gamma = 0\\)), where compute is \u201cunfolded\u201d either in space (front-loading all GPUs simultaneously) or in time (running a fixed-size cluster for millennia). Next, we consider scenarios with positive \u03b3, exploring how a balance be-tween up-front GPU investment and sustained efficiency gains can shape multi-year outcomes.\nIllustrative Scenario. To ground these theoretical insights, consider a target of achieving about 50% token-prediction accuracy (L = 0.68 nats/token). This per-formance level implies that, on average, the model correctly predicts the next token roughly half the time\u2014no small feat when dealing with large vocabularies and highly nuanced contexts."}, {"title": "How Severe Are the Diminishing Returns in AI Scaling?", "content": "A simplified numeric example demonstrates how purely static assumptions (\\(\\gamma = 0\\)) can result in extremely large hardware requirements or extended training durations, primarily due to the small scaling exponent \\(\\kappa\\) (e.g., \\(\\kappa = 0.048\\)).\nFrom Lo = 1.0 to L = 0.68. Under the static law (i.e., the relative-loss equation at y = 0), we have:\n\\[\\left(1 + \\frac{t}{1 \\text{ yr}} \\right)^{-\\kappa} = \\left(1 + \\frac{\\Delta C}{C_0} \\right)^{-\\kappa} = 0.68,\\)\nwhich implies t \u2248 3000 and AC \u2248 3000 \u00d7 Co.\nInterpretation:\nLonger Training (unfold in time): If Co denotes running a baseline cluster for one year, reducing loss from L\u2081 = 1.0 to L = 0.68 under a y = 0 assumption would require an additional 3,000 years of training on the same hardware.\nBigger GPU Fleet (unfold in space): If the loss reduction must be achieved within a single year, AC \u2248 3000 \u00d7 Co implies that the hardware must be scaled by a factor of 3,000. This would push GPU requirements into the 300 million range, consuming power comparable to the electricity use of an entire continent."}, {"title": "A Multi-Year Case: Baseline, Turtle, and Hare", "content": "We now consider three illustrative scenarios, each aiming to reduce training loss from an initial loss Lo to a final target of L = 0.68. Although they share the same scaling exponent k and ultimate objective, they differ in:\nStarting Training Loss (Lo), which depends on how many GPUs were ini-tially allocated, and\nAnnual Efficiency-Doubling Rate (\u03b3).\nIn particular:\nBaseline: Begins with 100,000 GPUs at y = 0.5, reflecting a historical rate of doubling roughly every two years.\nTurtle: Starts with fewer GPUs (10,000) but targets a higher \u03b3 = 3.0 (tripling annually) to see if an \u201cextreme efficiency push\" can catch up.\nHare: Begins with 150,000 GPUs (about 1.5\u00d7 the Baseline's cluster) at y = 2.0, balancing stronger up-front capacity with a still-robust annual doubling rate.\nCommon Setup and Baseline Loss Lo. We assume each scenario runs for one initial year, after which the training loss is recorded as a new baseline Lo. Formally, if Linit is the loss before this year of training, and AC is the additional compute gained in that year (relative to a baseline Co), then:\n\\(L_0 = L_{\\text{init}} \\left(1 + \\frac{\\Delta C}{C_0} \\right)^{-\\kappa}.\\)\nFewer initial GPUs yield a negative AC, as the compute achieved falls short of the baseline Co, raising Lo. Conversely, more GPUs yield a positive AC, as the compute achieved exceeds the baseline, lowering Lo. This relationship underscores the trade-off between initial resource allocation and the resulting training loss.\nTurtle vs. Hare in Practice. A scenario inspired by DeepSeek-V3 [18] could exemplify the Turtle approach:\nSmaller (Possibly Older) GPU Fleet: Starting with fewer GPUs (e.g., 2,000 rather than 10,000) and aiming for a higher efficiency-doubling rate (y > 2) to offset the lower initial capacity.\nHigh-Impact Optimizations: Even on mid-range devices, leveraging ad-vanced hardware features (e.g., FP8) or specialized software (e.g., Mixture-of-Experts) can systematically increase \u03b3.\""}, {"title": "Scaling Laws as the Driving Force for Innovations", "content": "Sustaining progress in AI scaling demands a deliberate focus on innovation to accel-erate efficiency gains and counteract diminishing returns. Interestingly, this driving force stems from the same principles that underpin classical scaling laws, once ex-tended into a time- and efficiency-aware framework.\nLogical Compute as Optimization-Agnostic. Because scaling laws inherently treat compute in a model-agnostic manner, logical compute is defined as though the model were both dense and full-precision. This prevents conflating compute with efficiency, the latter reflecting tangible gains from optimizations like spar-sity or reduced-precision formats [15, 14]. Without this separation, an architecture such as DeepSeek-V3 [18]\u2014which achieves a 17\u00d7 higher real-world efficiency than Llama 3 (405B) by leveraging sparsity (Mixture-of-Experts), FP8 arithmetic, and other refinements (see Appendix A)\u2014would appear artificially \u201csmaller.\u201d\nBy anchoring \u201ccompute\" to the model's full architectural capacity and attribut-ing actual speedups to Time \u00d7 Power, the optimization-agnostic nature of scaling laws remains intact. Researchers can innovate freely\u2014balancing accuracy, power, and training cost\u2014without altering the fundamental compute measure itself. Mean-while, the annual efficiency-doubling rate y quantifies how rapidly these real-world optimizations accumulate, fueling near-exponential progress over multi-year devel-opment cycles.\nCumulative Compute as a Compounding Process. Classical AI scaling laws (e.g., Lo x Co", "snapshot": "or training loss under a fixed compute budget. Once we generalize to\n\\(L(t) = L_0 \\left(1 + \\frac{\\Delta C(t)}{C_0} \\right)^{-\\kappa},\\)\nthe additional compute AC(t) builds over time, transforming that static snapshot into a compounding process. Every phase of progress leverages all prior compute in-vestments, allowing near-exponential improvements when efficiency (\u03b3 > 0) continues rising. This view clarifies why ongoing innovation is indispensable: each incremental gain can magnify the returns of earlier investments, thereby sustaining AI scaling despite inherently small exponents (\u03ba)."}, {"title": "Outlook", "content": "Connection to Industry Trends. Recent NVIDIA data shows a 45,000\u00d7 leap in token-generation efficiency over eight years\u2014effectively doubling every six months (\u03b3\u2248 2) [20]. Meanwhile, organizations like OpenAI, Google, and Meta continuously refine hardware, software, data pipelines, and infrastructure to maintain similarly rapid efficiency-doubling rates. For instance, DeepSeek-V3 [18] achieves 17\u00d7 higher efficiency (see Appendix A) compared to Llama 3 (405B) through innovations such as FP8 arithmetic, sparsity, and Mixture-of-Experts, demonstrating how architectural optimizations can drive significant efficiency gains. The success of DeepSeek-V3 un-derscores the importance of prioritizing efficiency improvements in AI development, particularly for organizations with limited compute budgets or sustainability goals. This y > 0 trajectory is far from theoretical: it underpins the near-exponential performance gains needed for training modern large-scale models.\nPolicy Implications. By explicitly embedding the efficiency-doubling rate y within AI scaling laws, our framework elevates innovation across the AI stack from an im-plicit assumption to a measurable driver of progress. Rather than a \u201ccompute arms race,", "race to efficiency\\\": leaders and policymakers can set explicit targets (e.g. doubling efficiency every six months) and synchronize develop-ment roadmaps to sustain a high y. Much as Moore's Law once provided concrete milestones for transistor scaling, AI practitioners can now anchor multi-year plans on tangible efficiency benchmarks, fueling the compounding gains crucial for sustained, real-world AI advances.\"\n    },\n    {\n      \"title\": \"Conclusion\",\n      \"content\": \"This work presented a time- and efficiency-aware foundation for AI scaling, revealing how a \u201crace to efficiency": "aturally emerges once classical, static scaling laws account for ongoing efficiency gains. Three key insights form a mutually reinforcing cycle:\nTime-Extended Perspective. While classical laws link loss to compute at a single snapshot, recognizing \u201cefficiency doubling\" over months or years shifts that static view into a dynamic, near-exponential trajectory.\nEfficiency-Centric Focus. Because scaling laws reliably map compute to performance gains, the crucial question becomes how efficiently compute ac-cumulates over time. In this light, efficiency doubling emerges as both a com-putational and practical necessity to mitigate steep diminishing returns.\nInnovation as Core to Scaling. Once efficiency is central, continual op-timizations across the AI stack no longer appear as external \u201cfixes\" but as integral parts of the scaling process. These incremental improvements com-pound over multiple training cycles and product generations, reinforcing the time-extended perspective.\nLooking ahead, constraints akin to those that once challenged transistor scaling may ultimately call for new paradigms. Yet the tension between diminishing returns and time-extended efficiency gains will likely remain a defining force in AI's technological evolution."}, {"title": "Limitations and Future Work", "content": "While the relative-loss equation offers a unified perspective on AI scaling progress, its practical value and generality warrant further exploration. Below, we highlight several avenues for extending and refining this framework.\nEmpirical Validation and Transparency\nThe insights in this paper rest on theoretical constructs and empirically observed scaling exponents. Nevertheless, comprehensive validation against real-world data is crucial. Greater transparency in reporting relative training loss, cumulative com-pute, and efficiency-doubling rates could enable more rigorous cross-study compar-isons. Industry-wide data sharing, standardized benchmarks, and consistent evalua-tion protocols akin to those once used for guiding semiconductor progress\u2014would help verify the predictive power of the relative-loss equation. Such efforts could also inform resource-allocation decisions, model architectures, and targeted efficiency im-provements.\nGeneralizing to Multi-Phase Growth\nAlthough our framework focuses on AI training, it could be generalized to capture early, sub-exponential \u201ckick-off\u201d phases or logistic transitions in other domains\u2014ranging from technology adoption to broader economic processes. Such a generalization would provide a unified view of how systems evolve from an initial ramp-up to po-tentially exponential (or S-curve [21]) growth trajectories.\nOptimal Usage vs. Raw Compute\nThe relative-loss equation assumes compute-optimal usage, where model size, dataset size, and training strategy are balanced to fully exploit available resources. Simply increasing GPUs or peak compute does not guarantee improved performance if scal-ing principles are not followed.\nFor instance, training a large model without adequate data or failing to tune hyper-parameters may not yield the expected loss reductions. Likewise, imbalanced scaling between model and data can prevent the envisioned gains. The equation thus reflects a best-case trajectory, assuming that each increment in compute efficiency translates directly into effective training progress.\nExtending the Framework to Inference and Test-Time Scaling\nWhile our current formulation focuses on training dynamics, extending the relative-loss equation or developing analogous constructs for inference-time scaling [13, 22] could yield a more holistic view of AI system performance. In particular, because large models are increasingly used to generate new training data and perform on-the-fly or iterative refinement, higher inference efficiency can accelerate the training pipeline rather than merely reduce deployment costs.\nAs a result, understanding how inference-time efficiency improvements translate into faster throughput, lower latency, or expanded data pipelines may be crucial, given that deployment considerations (and the downstream feedback loop into training) increasingly shape large-model design choices. A unified framework that addresses both training and inference could thus clarify how hardware roadmaps, data en-gineering, and algorithmic optimizations interact across the entire lifecycle of AI systems.\nEvolving Concepts of Compute-Optimality\nTraditionally, compute-optimal scaling assumes a static dataset and a fixed model configuration. In reality, both datasets and models evolve over time. Approaches such as upcycling pretrained models [23, 16, 17], and dynamically adapting model size or precision introduce new optimization strategies. Similarly, dataset generation and curation leveraging synthetic data [24] or reasoning-based selection [25]-blur the boundary between model development and data sourcing.\nAs these strategies mature, future frameworks must treat datasets, models, and com-pute budgets as interconnected and evolving. Such adaptability ensures the relative-loss equation and similar scaling models remain relevant."}, {"title": "Invariance of Logical Compute", "content": "This appendix explains why we define logical compute as dense and full-precision and how it underpins a fair comparison\u2014illustrated with DeepSeek-V3 and Llama 3 (405B)- -even when actual GPU hours differ across hardware or optimization strate-gies.\nWhy \u043a Remains Unchanged by Optimizations\nSuppose the training loss depends on model size N and dataset size D as\n\\(L(N, D) = AN^{-\\alpha} + BD^{-\\beta} + E,\n\\)\nwhere \u03b1, \u03b2 > 0. For a fixed compute budget C, the compute-optimal pairs (N*, D*) satisfy\n\\[N^* \\propto C^{\\frac{\\beta}{\\alpha + \\beta}}, \\quad D^* \\propto C^{\\frac{\\alpha}{\\alpha + \\beta}},\\]\nso substituting back yields\n\\(L(C) \\propto C^{\\frac{-\\alpha \\beta}{\\alpha + \\beta}} = C^{-\\kappa}, \\quad \\kappa = \\frac{\\alpha \\beta}{\\alpha + \\beta}.\n\\)\nHence, k depends only on \u03b1, \u03b2, not on how we optimize model parameters or numeric formats. In other words, whether one uses low-precision arithmetic, sparsity, or a Mixture-of-Experts design, the fundamental exponent k stays invariant.\nDefining Logical Compute\nWe define logical compute as though the model is both dense and full-precision. Specifically:\nLogical Compute (FLOPs) = 6 \u00d7 N \u00d7 D,\nwhere\nN total model parameters,\nD = total training tokens,\nThe factor 6 accounts for forward/backward passes and parameter updates.\nThis ensures that \u201ccompute\u201d consistently reflects the full architecture, independent of sparsity or precision. Actual speedups (e.g., from FP8 or MoE) appear separately in reduced Time \u00d7 Power, rather than shrinking the fundamental compute measure.\nCase Study: DeepSeek-V3 vs. Llama 3 (405B)\nTo illustrate why logical compute is kept dense, consider two models that achieve broadly similar large-scale outcomes yet differ in real-world GPU usage:\nDeepSeek-V3 [18]:\nN\u2248 671 B (parameters),\nD\u2248 14.8T (training tokens),\nReal GPU usage: ~ 2.78M GPU-hours on H800, adjusted to ~ 2.224 M GPU-hours at H100 equivalence.\nLlama 3 (405B) [19]:\nN \u2248 405 B,\nD\u2248 2.0T,\nReal GPU usage: ~30.84 M GPU-hours on H100."}, {"title": "Counterarguments and Responses", "content": "Should We Adjust Logical Compute for Sparsity or Precision? Some sug-gest reducing FLOPs to reflect only the fraction of parameters activated per token (experts-per-token in MoE) or the lower numeric cost (e.g., from FP8). However, that merges two concepts:\nFull Model Complexity: The entire parameter space at full precision, rep-resenting the model's theoretical capacity.\nReal-World Efficiency Gains: Achieved by using only a subset of parame-ters, or fewer bits per operation, thus lowering time and power.\nConflating them penalizes architectures that are inherently more efficient (like Mixture-of-Experts). Instead, we keep the definition of \u201clogical compute\u201d dense and record any real-world speedups in the denominator (Time \u00d7 Power). This way, a model reaps the benefits of advanced routing or quantization (\\(\\gamma > 0\\)) without artificially reducing its fundamental FLOP count.\nIs \"Effective Model Size\u201d More Accurate? Although some frameworks define an effective size Neff for MoE [14] or reduced precision [15], such an approach can hide the full parameter space. Not all parameters are active per token, but they still exist, providing capacity for generalization and future scaling. By keeping \u201clogical compute\" dense, we preserve fairness across architectures. If a model invests in sophisticated routing or quantization, the advantage should appear as a lower Timex Power, not by discarding parameters from the total.\nSummary\nBecause k depends solely on the power-law slope linking model size, dataset size, and compute, it is invariant to whether a model employs sparsity, lower-precision arithmetic, or mixture-of-experts routing. Defining logical compute as though it were both dense and full-precision provides a consistent baseline for comparing very different architectures. Real-world efficiency gains, meanwhile, show up in Time \u00d7 Power, thereby highlighting the genuine speedups achieved by hardware or software improvements. This framework allows us to preserve the foundational exponent \u043a from classical scaling laws while giving proper credit for engineering advances in accelerators, memory systems, or training algorithms."}]}