{"title": "Pencils to Pixels: A Systematic Study of Creative Drawings across Children, Adults and AI", "authors": ["Surabhi S Nath", "Guiomar del Cuvillo y Schr\u00f6der", "Claire E. Stevenson"], "abstract": "Can we derive computational metrics to quantify visual creativity in drawings across intelligent agents, while accounting for inherent differences in technical skill and style? To answer this, we curate a novel dataset consisting of 1338 drawings by children, adults and AI on a creative drawing task. We characterize two aspects of the drawings-(1) style and (2) content. For style, we define measures of ink density, ink distribution and number of elements. For content, we use expert-annotated categories to study conceptual diversity, and image and text embeddings to compute distance measures. We compare the style, content and creativity of children, adults and AI drawings and build simple models to predict expert and automated creativity scores. We find significant differences in style and content in the groups children's drawings had more components, AI drawings had greater ink density, and adult drawings revealed maximum conceptual diversity. Notably, we highlight a misalignment between creativity judgments obtained through expert and automated ratings and discuss its implications. Through these efforts, our work provides, to the best of our knowledge, the first framework for studying human and artificial creativity beyond the textual modality, and attempts to arrive at the domain-agnostic principles underlying creativity.", "sections": [{"title": "Introduction", "content": "Human visual creative expression emerges early on children start drawing before they can write (Levin & Bus, 2003), and cave paintings predate the written word (Ardila, 2004). However, empirical research in creativity, especially in the context of AI, has focused more on verbal than visual creativity. This discrepancy arises in part due to the complexities of producing and evaluating visual outputs. Evaluating visual creativity involves subjective perceptual judgments, for example aesthetic considerations and challenges of separating creativity from technical skill (Chan & Zhao, 2010). For this reason, visual creativity is commonly assessed using simple shape completion creative drawing tasks (e.g., TTCT Picture Completion, (Torrance, 1966); TCT-DP, (Urban, 2005); MTCI, (Barbot, 2018)). Unlike other forms of visual expression (e.g., paintings, digital art), creating such drawings requires limited technical or artistic expertise, without compromising on creative potential (Barbot & Tinio, 2015) and serves as a useful cognitive tool (Fan, Bainbridge, Chamberlain, & Wammes, 2023).\nHowever, evaluating such drawings are still challenging due to the lack of a well-formalized framework for assessment, especially across intelligent agents, a crucial comparison in the present era where human and machine creativity increasingly intersect (Acar, 2023; O'Toole & Horv\u00e1t, 2024; Marr, 2023). For example, how can we meaningfully compare the creativity of a child's pencil drawing to that of a 1024 \u00d7 1024 pixel image generated by an AI model? To bridge this gap, we make two key contributions: (1) we curate a novel dataset of creative drawings and evaluations spanning different intelligent agents, (2) we develop a computational framework that quantifies two core aspects of the drawings\u2014content, and style, and use them to study drawing creativity.\nDataset. In order to create a diverse dataset, we take note of the vast literature studying individual differences in drawing abilities (Chan & Zhao, 2010), particularly across development (Lowenfeld, 1957; Philippsen, Tsuji, & Nagai, 2022; Heard, 1988; Hart et al., 2022; Narvaez, Polsley, & Hammond, 2024). Further, we note that in AI text-to-image models, prompting can lead to significant diversity in outputs (Oppenlaender, Linder, & Silvennoinen, 2024). Therefore, we curate drawings from children in two age groups, adults, and AI models prompted using a collection of prompts.\nFor each drawing, we obtain human ratings from two experts raters and get automated scores from two recently released tools for automated assessment of drawing creativity, AuDrA (Patterson, Barbot, Lloyd-Cox, & Beaty, 2024) and OSC-figural (Acar, Organisciak, & Dumas, 2023). Research suggests that AI-based evaluation methods favor AI-generated responses, whereas human evaluators prefer human-created outputs (Laurito et al., 2024). Therefore, by incorporating both expert (Kaufman & Baer, 2012) and automated evaluation (Cropley & Marrone, 2022), we obtain a balanced perspective on creativity assessment.\nFramework. To analyze this diverse dataset, we begin by distinguishing two key aspects of creative drawings: content (what is depicted) and style (how it is rendered). This distinction between content and style is studied in vision research, for example in the context of art perception (Augustin, Leder, Hutzler, & Carbon, 2008; Augustin, Defranceschi, Fuchs, Carbon, & Hutzler, 2011) and in generative artificial intelligence (Kotovenko, Sanakoyeu, Lang, & Ommer, 2019; Zhang, Zhang, & Cai, 2018).\nWe explore various computational metrics for characterizing content and style in creative drawings."}, {"title": "Methods", "content": "Our dataset consists of 1338 drawings by children, adults and AI on stimuli from the Multi-trial Creative Ideation (MTCI) Task (Barbot, 2018) (Fig 1 left panel).\nChildren Data comprises of 444 drawings from 148 children, 84 from kindergarten (4-6 year olds) and 64 from lower elementary level (7-9 year olds) from a public Montessori school. These two groups align with two stages of drawing development, where ages 4-6 are considered pre-schematic and ages 7-9 schematic (Lowenfeld, 1957).\nData collection took place in the classroom in small groups. A trained research assistant explained the task. Then each child was given a thick pencil and a piece of paper with the stimulus printed on it. They were given 5 minutes to complete their drawing, after which the next stimulus was given. Each child completed three drawings, one for each of the stimuli shapes G, I and R.\nSince the children drew freely on paper, they sometimes ignored instructions and flipped the paper by 180\u00b0, drawing on the inverted stimulus. Stimulus 'R' (resembling an inverted house) was flipped most often, with 60% drawings on 'R' being inverted. Across the whole dataset, about 30% drawings (nearly the same ratio in both pre-schematic and schematic) were drawn on inverted stimuli.\nAdults Data comprises of 444 drawings from 148 participants, who each completed drawings for stimuli shapes G, I, and R on the MTCI hosted by Barbot's Crealyx platform, an open-access, online testing platform dedicated to the assessment of creativity.\nAI We treated the MTCI task as an inpainting task. We prompted Open AI's Dall-e (v2), through their API in image editing mode using three prompts (see box below) for each of three stimuli shapes G, I, R. We collect a total of 50 images per prompt per stimuli, resulting in a final dataset of (50+ 50+50) \u00d7 3 = 450 images.\nThe first prompt, based on Chen (2023) contained clear stylistic instructions (colour, pen type, thickness, art style etc.) with no explicit instruction for content (to match the task instructions given to humans), and was set as the base prompt (prompt 1). Prompts 2 and 3 extended the base prompt with explicit content instructions for creating objects/scenes (prompt 2) and living figures (prompt 3)."}, {"title": "Preprocessing", "content": "Since the children, adults and AI data came from different sources, it was crucial to preprocess the images for valid comparisons. We controlled for size, colours and line thickness using computer vision techniques. The children and Dall-e drawings are cropped and resized to 400 \u00d7 400 size to match the adults. The cropping was specified in a way to ensure the stimulus size and position was aligned across all drawings. The children's drawings were made with pencil and are therefore shades of gray. All drawings were binarized and cast to black-and-white. Children drawings also had tiny scattered pencil spots which were removed using image erosion. The lines for children and AI drawings were dilated to match the line thicknesses of adult drawings. Post processing, we confirmed a non-significant difference in line thickness across the three groups using a Kruskal-Wallis test (p > 0.1)."}, {"title": "Measures", "content": "We developed computational measures of style and content to characterise drawing creativity.\nStyle We quantify (1) ink density, (2) fraction of ink inside the stimuli shape, (3) number of components, and (4) number of lines.\n(1) Ink density: The percentage of the drawing covered in ink, measured by dividing the number of black pixels by the total number of pixels times 100.\n(2) Fraction of Ink inside the Stimulus Shape: Quantified by the amount of ink inside the stimulus's bounding box divided by the total amount of ink. Bounding boxes for the base stimuli were obtained by identifying the extreme ink points defining their boundaries.\n(3) Number of Components: Counts the number of visually distinct regions in the drawing, based on the graph theoretic property of reachability.\n(4) Number of Lines: Skeletonizes the drawing to extract its structural outline and then applies the Hough Transform to detect and count straight lines.\nContent We use the clip image-embedding model (OpenAI, clip-vit-large-patch14) to obtain image embeddings per drawing. We compute the cosine distance between the embeddings of the drawing and the corresponding base stimulus shape (G, I or R). Based on the nature of clip's training data, we know the distance measures induced by these encoders are largely influenced by semantic rather than stylistic attributes.\nFor captions, we use GPT40 to generate image descriptions for each drawing. We explicitly state in the prompt to give short captions (<15 words), describing the content (and not style) of the drawing, and to caption \"hard to interpret\" drawings as such. We then used gtelarge text-embedding model to obtain caption-embeddings per caption per drawing. Using these, we (1) cluster the embeddings hierarchically to arrive at core semantic themes expressed in the drawings. (2) We define a measure of semantic uniqueness for each drawing by computing the mean cosine distance of the caption-embedding to its ten nearest neighbours. The nearer a caption-embedding is to others, the more popular and less unique the drawing concept is in the dataset of drawings."}, {"title": "Annotation", "content": "Drawing content was also annotated by an expert. Each drawing was classified into a minimum of one and a maximum of three (from most salient to least salient) concept categories. If the drawing was hard to interpret, it was assigned to a \u201chard to interpret\" category. Using these categories, we evaluate conceptual diversity for the different groups by diving the number of unique categories per group by the total number of unique categories.\nDrawing captions generated by GPT40 were also validated by an expert with a correct/incorrect label per caption per drawing. We found that GPT40 captions scored nearly 83% correct across all drawings.\nTo measure process aspects, we obtained flexibility (conceptual diversity within the outputs of the same agent) scores by two expert raters. Flexibility was scored in a range of [0-2] for the three drawings by the same participant. Since there are no explicit participants for the Dall-e drawings, we randomly sample sets of three Dall-e drawings, one from each stimulus shape under the same prompt. We sample 50 random samples sets per prompt (without replacement so that each image is sampled once) and obtain flexibility scores for each set."}, {"title": "Creativity Scoring", "content": "Creativity was scored by four sources. Two experts rated each drawing on a scale of [0-4] following the MTCI scoring protocol. Two automated scoring tools AuDrA (Patterson et al., 2024) and Open Creativity Scoring - Figural (Acar et al., 2023) produced originality ratings in the range [0-1] per drawing.\nOne expert rater also scored utility in the range [0-2] per drawing, depending on how well the drawing incorporated the base shape."}, {"title": "Results", "content": "We compare the drawings of children, adults and AI based on style and content. We present the subgroups of children and AI separately to note differences across childhood development and prompt-guided content manipulation.\nStyle Figure 2 presents the boxplots comparing the style measures. We see significant differences in ink densities across the three groups (the within group differences were not significant, Figure 2a). We find that AI drawings have the highest ink density followed by children and then adults with the least. From Figure 2b we see that most of this density resides inside the stimulus shape for children and adults with no significant difference between them (there was a significant difference within the subgroups of children), but resides outside for AI drawings. Further, the children's drawings had a higher number of components compared to adults, and AI had the least (Figure 2c). This means the children draw more separable elements in their drawings whereas the AI produces a connected chunk of ink. Finally, from Figure 2d we see that AI drawings contain a large number of straight lines, significantly higher than children or adult drawings (with no significant difference between children and adults groups or subgroups). Within AI, prompt 3 drawings had significantly lesser lines than prompt 1 or 2, suggesting that the instruction of producing living figures resulted in more curved strokes. Lastly, we found that while children and adults nearly always seamlessly incorporated the stimulus shape in their drawings, AI only did so about 50% of the times.\nContent To study the content of the drawings, we identify the common themes across drawings using two methods.\nWe use expert annotated categories to calculate concept diversity as the number of unique categories per subgroup divided by the total number of unique categories across subgroups (counted to be 253). Adult drawings displayed the maximum content diversity, with their drawings encompassing nearly 50% of all identified categories, followed by children (~30%) and then AI (~18%) (wherein prompt 2 had a significantly higher diversity than other prompts). In the AI drawings, prompt 2 produced more object themes and prompt 3 produced more living themes, compared to prompt 1, suggesting that the prompting was effective.\nTo understand the classes of themes more closely, we visualise them by hierarchical clustering of GPT40 caption-embeddings (Figure 3). We see that adults display the maximum number of themes, some of them also encompassing complex ideas e.g. gravestone, music or leisure activities. The children and AI drawings had a roughly similar number of categories but the themes differed significantly. Children themes included many imaginative concepts such as cartoon characters, wings, antennae, rockets, ice cream or ghosts. The large proportion of houses can be attributed to 60% of 'R' stimuli drawings being inverted. Many of the AI drawings contained abstract themes such as abstract shapes, figures or faces, while some others were complex themes such as furniture or captivity. Children and AI had a relatively similar fraction of drawings which were hard to interpret (~ 25%), higher than those in adults (16%).\nWe plot the mean flexibility scores for each group to study process level differences (Figure 4). We note are adults are generally highly flexible with mean flexibility scores largely above 1. Within children, the pre-schematic group has relatively low flexibility with many children scoring a mean under 1. This means these children redrew the same themes across their drawings, disregarding the stimulus. However, in the schematic group, children are strikingly more flexible (p < 0.01), rarely repeating themes across their drawings. A similar shift towards higher flexibility is visible as a result of prompting. The base prompt yielded inflexible drawings, whereas prompt 2 and 3 sees a rightward shift in the values, yielding a significant difference in case of prompt 2 (also in line with the greater conceptual diversity in prompt 2)."}, {"title": "Who is More Creative?", "content": "To test which group received the highest creativity scores, we first check the agreement within and between the expert and automated scores. Between the two expert raters, and between the two automated methods, the average fixed raters ICC score was respectively 0.82 and 0.90 (p < 0.01), indicating high inter-rater reliability and consistency, and therefore we use mean expert score (normalised using min-max scaling to the range 0-1) and mean automated score for analyses. Interestingly, the ICC between mean expert score and mean automated score is only 0.48 and Spearman correlation coefficient is 0.36 (p < 0.01). This indicates that the experts and automated scores, though both reliable, do not agree with each other fully on how creative the drawings are.\nFurther, we test the agreement within the groups to find that the Spearman correlation between mean expert score and mean automated score is higher in adults (0.66, p < 0.01), compared to children (0.59, p < 0.01, with 0.56 in pre-schematic), and lowest in AI (~ 0.30, p < 0.01 for each prompt subgroup). This can be explained by the fact that the automated models were trained on adult drawings and can therefore not generalize easily to other subdomains such as drawings by a different group of humans like pre-schematic children, or to non-human, artificial drawings by AI."}, {"title": "What do human experts and automated tools value when rating visual creativity?", "content": "We use linear mixed effects regression to test the role of our style and content measures in explaining variance in mean creativity scores by experts and automated tools. We perform stratified, 3-fold cross validation and report $R^2$ and Spearman correlation $C_{ortest}$ metrics for the models with the lowest BICs (Table 2a). We find that the expert score depended on originality, in both image and text space-i.e. on how far the drawing is from the stimulus in image space (dist_from_stim), and on how similar the caption is to its ten nearest neighbours in text space (10NN text). Importantly, their scores also depended on how seamlessly the drawing incorporated the stimulus (used_stim), and whether the drawing was easy to interpret (hard_to_interpret), which can be considered as evaluations of drawing utility. On the other hand, the automated score is largely influenced by the amount of ink (ink_density) and originality measures operating purely largely in image space (10NN_image), ignoring the multimodal nature of creativity.\nWe see this dissociation clearly in Table 2b, which reports the regression coefficients for a combined linear model with all the predictors from the individual best models (VIFs are under 5). We see that the predictors useful for explaining experts scores, namely used_stim, hard_to_interpret and 10NN text are not effective predictors of mean automated score, while ink density and 10NN_image are not effective predictors of mean expert score. Dist_from_stim was the only predictor contributing significantly to both models."}, {"title": "Discussion", "content": "Our work develops a novel dataset and a computational framework based on style and content to study creative drawings of children (across two age groups), adults and AI (across three prompts). This framework was useful in studying differences in drawings across the groups children's drawings had more visual components, and depicted imaginative themes (Latham & Ewing, 2018). Adult drawings displayed more conceptual diversity, and AI drawings had the highest ink density. We saw a striking increase in flexibility from pre-schematic to schematic children (Spensley & Taylor, 1999). Also, content-driven prompting improved creativity in AI (Hao, Chi, Dong, & Wei, 2024). Unlike most textual creative tasks (Bellemare-Pepin et al., 2024; Marco, Rello, & Gonzalo, 2024), Dall-e drawings still differ greatly from human outputs. Methods beyond prompting, for example agentic interactions (Vinker et al., 2024) or fine-tuning could help align AI drawings more with humans' (Liang et al., 2024).\nOur framework also helps understand creativity and its measurement. Interestingly, which group was most creative depended on the evaluator. We confirm a self-bias where human experts preferred human drawings, and automated models preferred AI drawings (Magni, Park, & Chao, 2024). As with textual creativity, both the automated methods and expert ratings value originality (distance from stimulus) (Kenett, 2019) and novelty (distance from other responses) (Runco & Acar, 2012). But, the automated tools preferred ink density while expert scores valued utility. This highlights an important shortcoming of automated tools which lack understanding of underlying concepts and are therefore unable to incorporate the utility (effectiveness) dimension of creativity evaluation. However, this reflects positively on our search for domain-agnostic determinants of creativity\u2014in line with past work, originality and utility interacted to predict creativity (Diedrich, Benedek, Jauk, & Neubauer, 2015).\nDespite challenges in scale unification across intelligent agents and manual curation of features, our framework, decomposing creativity into what and how, is general enough to extend beyond the visual domain to other forms of creative expression."}]}