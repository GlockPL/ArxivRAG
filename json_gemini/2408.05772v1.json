{"title": "An analysis of HOI:\nusing a training-free method with multimodal visual foundation models when only\nthe test set is available, without the training set", "authors": ["Chaoyi Ai"], "abstract": "Human-Object Interaction (HOI) aims to identify the pairs of\nhumans and objects in images and to recognize their relation-\nships, ultimately forming (human, object, verb) triplets.\nUnder default settings, HOI performance is nearly saturated,\nwith many studies focusing on long-tail distribution and\nzero-shot/few-shot scenarios. Let us consider an intriguing\nproblem: \"What if there is only test dataset without train-\ning dataset, using multimodal visual foundation model in\na training-free manner? \" This study uses two experimen-\ntal settings: grounding truth and random arbitrary combina-\ntions. We get some interesting conclusion and find that the\nopen vocabulary capabilities of the multimodal visual foun-\ndation model are not yet fully realized. Additionally, replac-\ning the feature extraction with grounding DINO further con-\nfirms these findings.", "sections": [{"title": "Introduction", "content": "The objective of Human-Object Interaction (HOI) detection\nis to identify the human pairs in images and discern the rela-\ntionships between them, which is crucial for various down-\nstream tasks, e.g., visual question answering, robotic vi-\nsion, embodied intelligence, and video analysis. Although the per-\nformance of HOI detection has approached saturation, there re-\nmains room for improvement in areas such as zero-shot/few-\nshot learning, and long-tail distribution. Previous work has\nachieved promising results by leveraging multimodal visual\nfoundation models for the initialization of textual labels, fea-\nture distillation, and so on.\nLet us consider an interesting question: What results can\nbe achieved using a training-free method with multimodal\nvisual foundation models when only the test dataset is avail-\nable, without training dataset? We implemented three dis-\ntinct experimental setups. The first setup involves inputting\n(human, object) pairs from ground truth into a multimodal\nvisual foundation model and comparing these with text\nprompts containing various verbs to derive a probability dis-\ntribution of verbs, thus determining the verb outcomes. The\nsecond setup disrupts the paired features of ground truth;\nhere, \"human\" comprises all ground truth bounding boxes\nfor humans, and \u201cobject\u201d includes all bounding boxes. These\nare then inputted in combinations into a subsequent query\nmodule to generate verb results like the first setup. The\nthird setup employs bounding boxes extracted by grounding\nDINO, which are unpaired, using a method\nsimilar to the second setup to ascertain the verb outcomes.\nWhen only test dataset is available and no training dataset\nexists, all samples are treated equally. In this context,\ndistinctions such as rare/non-rare, seen/unseen combina-\ntion/object/verb, which become irrelevant.\nIn the default experimental setting, rare classes are in-\nsensitive to random combinations of humans and objects,\nwhereas non-rare classes demonstrate sensitivity. Under the\nRF-UC (Rare First Unseen Combinations) setting, tail HOI\n(rare classes) categories are designated as unseen classes.\nThese unseen classes(rare classes) are insensitive to ran-\ndom combinations, while seen classes(non-rare) are sensi-\ntive to random combinations. Conversely, the NF-UC (Non-\nrare First Unseen Combinations) setting identifies head\nHOI categories(non-rare) as unseen classes, where unseen\nclasses(non-rare) are sensitive to random combinations, and\nseen classes(rare) are not sensitive. In experiments involv-"}, {"title": "Related Work", "content": "HOI\nHOI (Human-Object Interaction) detection primarily consists of two\nsubtasks: detecting human-object pairs (including their lo-\ncations and types) and categorizing the types of human-\nobject interactions. HOI detection methodologies are gen-\nerally divided into two categories: two-stage and one-stage\napproaches. In the two-stage approach, a separate detector\nis employed to identify the locations and classes of objects.\nThis is followed by specially designed modules that handle\nthe association of humans and objects and the recognition of\ntheir interactions. Conversely, the one-stage approach involves directly detecting human-object pairs\nalong with their interactions, thereby identifying the cor-\nresponding HOI categories in a single step. This paradigm\neliminates the need for complex post-processing for human-\nobject matching, enabling end-to-end training.\nHOI Detection with Linguistic Guidance /\nzero-shot / few-shot\nRecent advancements in Vision-Language Models (VLMs)\nhave exhibited a promising ability to transfer to downstream\ntasks. The visual\nrepresentations derived from natural language supervision\nfacilitate zero-shot and open vocabulary tasks. A practi-\ncal approach to achieve high performance without substan-\ntial effort is to utilize information from pre-trained mod-\nels. An effective strategy involves leveraging linguistic guid-\nance. With the emergence and demonstrated powerful per-\nformance of large-scale pre-trained visual-linguistic mod-\nels, methods that harness linguistic guidance have shown\nsignificant potential in interaction recognition tasks. These\ntasks necessitate a profound understanding of image con-\ntext and relational inference. A common technique to incor-\nporate linguistic guidance is to initialize interaction classi-\nfiers using text embeddings generated by pre-trained visual-\nlinguistic models. Ad-\nditionally, some studies extract information through knowl-\nedge distillation techniques. Directly using predictions from\npre-trained visual-linguistic models as constraints is also a\nfavored approach.\nModel\nOur study focuses on model performance evaluation in the\nabsence of a training dataset and only test dataset existing,\nfocusing specifically on zero-shot/few-shot, and long-tail\ndistribution recognition capabilities. We operate under the\nassumption that the feature extraction component of our de-\ntection model is predefined. The extracted features are sub-\nsequently input into a multimodal visual foundation model\nto discern relationships between humans and objects. The\nfeature extraction is categorized into three distinct types:"}, {"title": "Experiment", "content": "datasets\nOur research utilizes the publicly available HICO-DET\ndataset. HICO-Det comprises 47,776 im-\nages, divided into 38,118 for training and 9,658 for testing\npurposes. It annotates 600 categories of human-object inter-\nactions (HOIs), derived from 80 object categories and 117\naction categories. Of these, 138 HOI categories, character-\nized by fewer than 10 training samples each, are classified\nas Rare. The remaining 462 categories are classified as Non-\nRare."}, {"title": "results", "content": "The results can be seen in Table 1. The the result in\nblip2_coco_vitH/14@364px can be seen in Table 2, Table 3,\nand Table 4."}, {"title": "analysis", "content": "When only the test dataset is available, without a corre-\nsponding training dataset, all samples are treated uniformly.\nIn this scenario, distinctions such as rare versus non-rare or\nseen versus unseen combinations of objects and verbs be-\ncome irrelevant.\nIn the default setting, rare classes display insensitivity to\narbitrary human-object interactions, while non-rare classes\nare sensitive to these combinations. Under the Rare First\nUnseen Combinations (RF-UC) setting, rare Human-Object\nInteraction (HOI) categories are classified as unseen. These\nunseen (rare) classes exhibit insensitivity to arbitrary com-\nbinations, whereas the seen (non-rare) classes remain sen-\nsitive. Conversely, the Non-rare First Unseen Combinations\n(NF-UC) setting assigns non-rare HOI categories as unseen,\nwhere these unseen (non-rare) classes are sensitive to arbi-\ntrary combinations, while the seen (rare) classes show no\nsensitivity. Experiments that differentiate between unseen\nand seen objects or verbs maintain a consistent pattern of\nsensitivity across these classifications."}, {"title": "Conclusion", "content": "We can deduce that both rare and non-rare classes are in-\nherently present in the environment, each endowed with dis-\ntinctive characteristics. Moreover, our experiments indicate\nthat current models have not yet fully realized the poten-\ntial of zero-shot and few-shot learning in multimodal vision\nfoundation models."}]}