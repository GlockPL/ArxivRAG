{"title": "An analysis of HOI: using a training-free method with multimodal visual foundation models when only the test set is available, without the training set", "authors": ["Chaoyi Ai"], "abstract": "Human-Object Interaction (HOI) aims to identify the pairs of humans and objects in images and to recognize their relationships, ultimately forming (human, object, verb) triplets. Under default settings, HOI performance is nearly saturated, with many studies focusing on long-tail distribution and zero-shot/few-shot scenarios. Let us consider an intriguing problem: \"What if there is only test dataset without training dataset, using multimodal visual foundation model in a training-free manner? \" This study uses two experimental settings: grounding truth and random arbitrary combinations. We get some interesting conclusion and find that the open vocabulary capabilities of the multimodal visual foundation model are not yet fully realized. Additionally, replacing the feature extraction with grounding DINO further confirms these findings.", "sections": [{"title": "Introduction", "content": "The objective of Human-Object Interaction (HOI) detection is to identify the human pairs in images and discern the relationships between them, which is crucial for various downstream tasks, e.g., visual question answering, robotic vision, embodied intelligence, and video analysis. Although the performance of HOI detection has approached saturation, there remains room for improvement in areas such as zero-shot/few-shot learning, and long-tail distribution. Previous work has achieved promising results by leveraging multimodal visual foundation models for the initialization of textual labels, feature distillation, and so on.\nLet us consider an interesting question: What results can be achieved using a training-free method with multimodal visual foundation models when only the test dataset is available, without training dataset? We implemented three distinct experimental setups. The first setup involves inputting (human, object) pairs from ground truth into a multimodal visual foundation model and comparing these with text prompts containing various verbs to derive a probability distribution of verbs, thus determining the verb outcomes. The second setup disrupts the paired features of ground truth; here, \"human\" comprises all ground truth bounding boxes for humans, and \u201cobject\u201d includes all bounding boxes. These are then inputted in combinations into a subsequent query module to generate verb results like the first setup. The third setup employs bounding boxes extracted by grounding DINO, which are unpaired, using a method similar to the second setup to ascertain the verb outcomes.\nWhen only test dataset is available and no training dataset exists, all samples are treated equally. In this context, distinctions such as rare/non-rare, seen/unseen combination/object/verb, which become irrelevant.\nIn the default experimental setting, rare classes are insensitive to random combinations of humans and objects, whereas non-rare classes demonstrate sensitivity. Under the RF-UC (Rare First Unseen Combinations) setting, tail HOI (rare classes) categories are designated as unseen classes. These unseen classes(rare classes) are insensitive to random combinations, while seen classes(non-rare) are sensitive to random combinations. Conversely, the NF-UC (Non-rare First Unseen Combinations) setting identifies head HOI categories(non-rare) as unseen classes, where unseen classes(non-rare) are sensitive to random combinations, and seen classes(rare) are not sensitive. In experiments involving unseen/seen objects or verbs, the sensitivity to random combinations remains consistent between unseen and seen classifications.\nWe can conclude that both rare and non-rare classes naturally exist in the environment, each possessing unique properties. Additionally, our experiments demonstrate that the existing models have not fully developed the zero-shot/few-shot capabilities of multimodal vision foundation models"}, {"title": "Related Work", "content": "HOI\nHOI (Human-Object Interaction) detection primarily consists of two subtasks: detecting human-object pairs (including their locations and types) and categorizing the types of human-object interactions. HOI detection methodologies are generally divided into two categories: two-stage and one-stage approaches. In the two-stage approach, a separate detector is employed to identify the locations and classes of objects. This is followed by specially designed modules that handle the association of humans and objects and the recognition of their interactions. Conversely, the one-stage approach involves directly detecting human-object pairs along with their interactions, thereby identifying the corresponding HOI categories in a single step. This paradigm eliminates the need for complex post-processing for human-object matching, enabling end-to-end training.\nHOI Detection with Linguistic Guidance / zero-shot / few-shot\nRecent advancements in Vision-Language Models (VLMs) have exhibited a promising ability to transfer to downstream tasks. The visual representations derived from natural language supervision facilitate zero-shot and open vocabulary tasks. A practical approach to achieve high performance without substantial effort is to utilize information from pre-trained models. An effective strategy involves leveraging linguistic guidance. With the emergence and demonstrated powerful performance of large-scale pre-trained visual-linguistic models, methods that harness linguistic guidance have shown significant potential in interaction recognition tasks. These tasks necessitate a profound understanding of image context and relational inference. A common technique to incorporate linguistic guidance is to initialize interaction classifiers using text embeddings generated by pre-trained visual-linguistic models. Additionally, some studies extract information through knowledge distillation techniques. Directly using predictions from pre-trained visual-linguistic models as constraints is also a favored approach.\nModel\nOur study focuses on model performance evaluation in the absence of a training dataset and only test dataset existing, focusing specifically on zero-shot/few-shot, and long-tail distribution recognition capabilities. We operate under the assumption that the feature extraction component of our detection model is predefined. The extracted features are subsequently input into a multimodal visual foundation model to discern relationships between humans and objects. The feature extraction is categorized into three distinct types:"}, {"title": "Experiment", "content": "datasets\nOur research utilizes the publicly available HICO-DET dataset. HICO-Det comprises 47,776 images, divided into 38,118 for training and 9,658 for testing purposes. It annotates 600 categories of human-object interactions (HOIs), derived from 80 object categories and 117 action categories. Of these, 138 HOI categories, characterized by fewer than 10 training samples each, are classified as Rare. The remaining 462 categories are classified as Non-Rare."}, {"title": "analysis", "content": "When only the test dataset is available, without a corresponding training dataset, all samples are treated uniformly. In this scenario, distinctions such as rare versus non-rare or seen versus unseen combinations of objects and verbs become irrelevant.\nIn the default setting, rare classes display insensitivity to arbitrary human-object interactions, while non-rare classes are sensitive to these combinations. Under the Rare First Unseen Combinations (RF-UC) setting, rare Human-Object Interaction (HOI) categories are classified as unseen. These unseen (rare) classes exhibit insensitivity to arbitrary combinations, whereas the seen (non-rare) classes remain sensitive. Conversely, the Non-rare First Unseen Combinations (NF-UC) setting assigns non-rare HOI categories as unseen, where these unseen (non-rare) classes are sensitive to arbitrary combinations, while the seen (rare) classes show no sensitivity. Experiments that differentiate between unseen and seen objects or verbs maintain a consistent pattern of sensitivity across these classifications."}, {"title": "Conclusion", "content": "We can deduce that both rare and non-rare classes are inherently present in the environment, each endowed with distinctive characteristics. Moreover, our experiments indicate that current models have not yet fully realized the potential of zero-shot and few-shot learning in multimodal vision foundation models."}]}