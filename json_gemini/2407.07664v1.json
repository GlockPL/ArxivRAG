{"title": "A Coding-Theoretic Analysis of Hyperspherical Prototypical Learning Geometry", "authors": ["Martin Lindstr\u00f6m", "Borja Rodr\u00edguez-G\u00e1lvez", "Ragnar Thobaben", "Mikael Skoglund"], "abstract": "Hyperspherical Prototypical Learning (HPL) is a supervised approach to representation learning that designs class prototypes on the unit hypersphere. The prototypes bias the representations to class separation in a scale invariant and known geometry. Previous approaches to HPL have either of the following shortcomings: (i) they follow an unprincipled optimisation procedure; or (ii) they are theoretically sound, but are constrained to only one possible latent dimension. In this paper, we address both shortcomings. To address (i), we present a principled optimisation procedure whose solution we show is optimal. To address (ii), we construct well-separated prototypes in a wide range of dimensions using linear block codes. Additionally, we give a full characterisation of the optimal prototype placement in terms of achievable and converse bounds, showing that our proposed methods are near-optimal.", "sections": [{"title": "1. Introduction", "content": "Representation learning addresses the problem of learning a mapping from a high-dimensional input space to a lower-dimensional representation space subject to suitable inductive biases. These biases are imposed on learning algorithms as additional constraints on, for example, the network architecture or the optimisation algorithm. Geometry-based inductive biases have long been popular in representation learning. For instance, imposing unit norm constraints on the representations has been employed in different unsupervised learning methods, either through explicit normalisation or norm-invariant loss functions in variational autoencoders, or in self-supervised learning. Imposing representation separation is another common inductive bias used, for example, in contrastive learning and supervised representation learning.\nIn the supervised learning setting, one way to impose representation separation is through prototypical learning. Each class is assigned a prototype, and these are specified a priori to maximise their separation and are held fixed during training, where the algorithm attempts to map input samples to their class prototypes. Therefore, the representations are biased towards being separated based on their class. Recently, Hyperspherical Prototypical Learning (HPL) started imposing unit norm constraints to prototypical learning. This places the representations on the hypersphere and thus enhances representation separation bias in a scale invariant and known geometry.\nTo illustrate the idea behind HPL, consider the na\u00efve approach of selecting prototypes as the familiar one-hot encoding that, for K classes, picks the canonical basis vectors {e1,..., ek} in dimension K as prototypes. As illustrated in Figure 1 (left), this results in a suboptimal class separation on the hypersphere. Instead, HPL attempts at placing K maximally separated prototypes on the n-dimensional hypersphere Sn-1, hopefully with n < K. This combinatorial and non-convex problem is well-studied, but even on S2 the problem is unsolved for general K, and optimal solutions are only known for K = 1, ..., 14, and 24. Despite this, approximate solutions have been proposed. Mettes et al. propose a relaxation of the problem that however only achieves suboptimal separation. Kasarla et al., on the other hand, propose a closed-form solution that we show to be optimal; however, it is only applicable in dimension n = K \u2212 1.\nIn this paper, we propose two new methods for designing hypershperical prototypes and present sharp bounds on the optimal separation that can be achieved by placing an arbitrary number of prototypes K < 2n on a hypershpere of dimension n. Our approach rests on theory and concepts from error correcting codes, and our contributions are threefold:\n(i) We provide a new design approach for hyperspherical prototypes that maps binary linear codes defined over the n-dimensional Hamming space onto the n-dimensional hypersphere Sn-1. Our approach provides guarantees on the class separation by design, at the same time that it enables a more flexible trade-off between separation and the dimension n for a given number of classes K.\n(ii) We derive a converse bound on the guaranteed minimum prototype separation as well as an achievable bound that certifies that well-separated code-based prototypes exist. These bounds imply that for a large number of classes K and in high dimensions n, the worst-case cosine similarity converges to zero. The bounds also show that our code-based prototypes closely approach optimal separation for n \u2248 K/2.\n(iii) Finally, we provide alternative optimization-based hyperspherical prototypes which achieve the converse bound through a convex relaxation. These improve on the prototypes obtained by Mettes et al., which do not achieve the converse bound.\nThe paper is organised as follows. In Section 2, we motivate the connection between binary codes and hyperspherical prototypes and give a primer on the theory of error correcting codes. In Section 3, we apply this theory in order to give both coding-theoretic prototype constructions and bounds thereon. Also in Section 3, we provide a novel optimisation-based prototype scheme. The performance of the proposed schemes is evaluated and compared to the state of the art in HPL in Section 4, and Section 5 concludes the paper."}, {"title": "2. Background", "content": "We start with a formal problem formulation for designing a codebook of hyperspherical prototypes in the HPL setting. Then, we connect binary error correcting codes defined in the Hamming space to hyperspherical prototypes. After that, we provide a brief overview of fundamental concepts in coding theory that are used in this paper to derive code-based hyperspherical prototypes with good separation properties. The interested reader is encouraged to consult for a more detailed treatment.\n2.1. Problem Formulation\nWe consider the HPL setting with K classes and n dimensions; that is, we are interested in placing K prototypes C1,...,ck on the n-dimensional unit hypersphere Sn-1, where the dimension n is a hyperparameter. Our objective is maxmising the Euclidean distance between every pair of prototypes ci, cj \u2208 Sn\u22121 (i \u2260 j). Clearly, the Euclidean distance de (ci, cj) is bounded in the range [0, 2] and satisfies that $||c_i - c_j||_2^2 = 2 - 2\\langle c_i, c_j \\rangle$ for all ci, cj \u2208 Sn-1. Hence, maximising the Euclidean distance is equivalent to minimising the cosine similarity $\\langle c_i, c_j \\rangle$. In turn, this is equivalent to maximising the angle \u03b1 between ci and cj; since \u03b1 = arccos $\\langle c_i, c_j \\rangle$. Therefore, we will use the cosine similarity as a notion of separation throughout this paper. The objective in HPL is then designing a codebook C := {ci \u2208 Sn\u22121 : i = 1,..., K} of K well-separated hyperspherical prototypes, which can be summarized in the following optimisation problem:\n$\\min_C \\max_{i\\neq j} \\langle c_i, c_j \\rangle$.                                                                 (P)\nUnfortunately, this problem is both non-convex due to the unit norm constraint, and combinatorial due to the search of the worst pair of prototypes ci, cj (i \u2260 j), requiring tractable relaxations that yield approximate solutions.\n2.2. Connecting Codes to Protypes\nThe approach in this paper leverages coding theory to design n-dimensional binary vectors (that is, members of the n-dimensional Hamming space), which are mapped onto the n-dimensional hypersphere Sn-1, thereby creating prototypes with good separation. To provide some intuition as to how error correcting codes relate to placing prototypes that are maximally spaced apart, consider the mapping \u03c0: {0,1}n \u2192 Sn-1 that maps n-dimensional binary vectors b from the n-dimensional Hamming space to points c on the hypersphere. More precisely, the mapping is defined as\nc = \u03c0(b) := $\\frac{2(b-1/2)}{\\sqrt{n}}$                                                                   (1)\nand enforces that c(l) \u2208 {\u22121/\u221an,+1/\u221an} and $||c||_2 = 1$. This approach allows us to place 2n points on the unit hypersphere Sn-1 with a cosine similarity of at most $\\langle c, c' \\rangle \\leq 1 - 2/n$ for every pair c = \u03c0(b) and c' = \u03c0(b') with b \u2260 b'. For K < 2n, we can improve the separation guarantees by carefully selecting the K binary vectors placed on the hypersphere via the mapping \u03c0. Error correcting codes provide a systematic way to achieve this, and the concept is illustrated for n = 3 in Figure 1.\nAs a baseline, consider one-hot encoding (Figure 1, left), which provides K = 3 orthogonal prototypes. Using carefully selected corners of the unit cube and the mapping \u03c0, we can substantially improve on one-hot encoding. Reducing to K = 2 prototypes (Figure 1, centre), the unit cube corners corresponding to b\u2081 = (0,0,0) and b2 = (1, 1, 1) are diametrically opposed with a cosine similarity of -1, which is optimal. From a coding theory perspective, this corresponds to a repetition code with Hamming distance dH(b1, b2) = 3 between codewords; that is, the codewords differ in 3 bits. Increasing to K = 4 prototypes (Figure 1, right), the unit cube corners corresponding to b\u2081 = (0,0,0), b2 = (0,1,1), b\u2083 = (1,0,1), and b\u2081 = (1,1,0) have a mutual cosine similarity of -1/3, which is an improvement over one-hot encoding (Figure 1, left) while increasing the number of classes at the same time. Again, the set B = {b1, b2, b3, b4} constitutes a binary linear code with Hamming distance d\u00ed(bi, bj) = 2 between its codewords.\nThis example demonstrates that due to the definition of the mapping function \u03c0, there exists a relation between the separation of vectors c = \u03c0(b) and c' = \u03c0(b') on the unit hypersphere and the Hamming distance d\u2081(b, b') of the binary vectors b and b' in the sense that a large Hamming distance implies a large separation. We will make this result explicit in Section 3.1. Error correcting codes, designed to have a large Hamming distance d\u00ed (b, b') between every pair of codewords b and b', are hence a well suited tool for designing hyperspherical prototypes, and coding theory provides us with useful bounds on the achievable separation.\n2.3. A Primer on Coding Theory\nThe systematic study of error correcting codes dates back to the seminal work of Hamming (1950). By introducing redundancy in a structured way, error correcting codes allow for error detection and correction in messages and data, and are essential for guaranteeing the reliability of today's digital communication, computation, and storage systems. Linear codes defined over the Galois field GF(q), where q = pm and p is a prime, are of special interest as they offer a structure that can be used for efficient encoding, decoding, and analysis of the distance properties of the code.\nIn this paper, we mainly restrict ourselves to binary linear block codes (that is, q = p = 2), and only briefly discuss extensions to q-ary codes with q > 2. A binary block code with parameters [n, k] is specified by a codebook B of 2k binary codewords of length n and a bijective encoder mapping that maps the set of all length-k binary vectors into the code B. This adds n \u2212 k bits of redundancy, which we can use to detect and correct errors in the codeword. The rate of the code is defined as R = k/n, where a low rate corresponds to a high redundancy. The error detection and correction capabilities rely on the separation of codeword pairs bi, bj \u2208 B in Hamming distance, namely\ndH (bi, bj) := $\\sum_{l=1}^n I\\{b_i(l) \\neq b_j(l)\\}$.\nA binary code with minimum Hamming distance\ndmin := $\\min_{b_i,b_j \\in B, i\\neq j} d_H(b_i, b_j)$\ncan detect dmin \u2212 1 errors and correct [(dmin-1)/2] errors.\nIn this paper, we fix k = [log2(K)] given K classes, suggesting that we are in the low-rate or high-redundancy regime if n is of the order of K. Noting that low-rate codes offer large separation in terms of minimum Hamming distance, we can expect good separation by adopting a code-based approach. To this end, a fundamental result in coding theory is that good codes exist. This is formalised by the well-known Gilbert-Varshamov bound.\nLemma 2.1 (Gilbert-Varshamov Bound). There exists an [n, k] code with minimum distance at least dmin, provided that\n2n-k >= $\\sum_{i=0}^{d_{min}-2} {n-1 \\choose i}$                                                                 (2)\nThe Gilbert-Varshamov bound for the largest dmin gives a lower bound on dmin. However, the bound only guarantees that good codes exist, but not how to find them. Luckily, as we will show in Section 3, several linear binary codes with better minimum distance exist.\nRemark 2.2. To derive some of our results, the bound in (2) needs to be evaluated carefully in order to avoid overflow problems. Notice that the bound can be rewritten as\n$\\sum_{i=0}^{d_{min}-2} {n-1 \\choose i} = 2^{n-1} F_{bin}(d_{min}-2; n-1, \\frac{1}{2})$ = Fbin (dmin \u2013 2; n \u2212 1, \u00bd),\nwhere Fbin (\u00b7 ; n \u2212 1, 1/2) denotes the cumulative distribution function of a binomial distribution with n \u2212 1 trials with success probability 1/2. This is implemented in a numerically stable manner in many computational libraries."}, {"title": "3. Hyperspherical Prototype Design", "content": "In this section, we present our main contributions towards solving the optimization problem (P). As mentioned earlier, this problem is both non-convex due to the unit norm constraint, and combinatorial due to the search of the worst pair of prototypes ci and cj with i \u2260 j. Our contributions are focused on relaxations to the problem (P) and bounds on the optimal solution. Section 3.1 formalises the coding-theoretic approach introduced with the example in Figure 1; Section 3.2 uses coding-theoretic tools to bound the optimal solution to (P); and Section 3.4 presents a relaxation to (P) which achieves the bound on the optimal solution.\n3.1. Coding-Theoretic Prototypes\nWe begin by formalising the intuition provided in Section 2.2, namely that a binary [n, k] code with the codebook B and a large minimum distance dmin gives good prototypes. Firstly, notice that if we want K prototypes, then we need |B| = 2k > K. Additionally, recall that the mapping c = \u03c0(b) defined in (1) produces a unit-norm vector for any binary vector b. Then, we can guarantee that binary code-based constructions guarantee good separation.\nProposition 3.1. Assume that B is the codebook of a binary [n, k] code with minimum distance dmin. Then, for every pair bi, bj \u2208 B with i \u2260 j, the cosine similarity between ci = \u03c0(bi) and cj = \u03c0(bj) is upper bounded by\n$\\langle c_i, c_j \\rangle = 1 - \\frac{2d_H(b_i, b_j)}{n} \\leq 1 - \\frac{2d_{min}}{n}$\nProof. To show the bound on the cosine similarity, notice that two binary vectors differing in dH (bi, bj) positions obey that $\\sum_{l=1}^n (b_i(l) - b_j(l))^2 = d_H(b_i, b_j)$. Expanding $||c_i - c_j ||^2$ in two ways gives\n$||c_i - c_j ||^2 = 2 - 2\\langle c_i, c_j \\rangle$\n$\\sum_{l=1}^n \\frac{\\langle 2(b_i(l) - \\frac{1}{2}) - 2(b_j(l) - \\frac{1}{2}) \\rangle ^2}{\\sqrt{n}}$.\nRearranging this and recalling that dH (bi, bj) \u2265 dmin yields the desired result.\nSince good codes exist (see Lemma 2.1), we provide two examples of binary codes whose minimum distance is close to dmin = n/2 in dimensions where n < K. That is, there exist no worse than orthogonal prototypes with zero worst-case cosine similarity in dimensions n \u2248 K/2. Additionally, these codes are easy to implement in Python, and hence are easily integrable in modern machine learning software. The codes are the Bose-Chaudhuri\u2013Hocquenghem (BCH) and Reed-Muller (RM) codes. Other code families like low-density parity-check (LDPC) codes and sparse graph codes are not competitive in terms of minimum distance guarantees since their minimum distance is usually lower than the one predicted by the Gilbert-Varshamov bound, see e.g., Mitchell et al., Figure 9. These popular code families are hence not further considered in this paper. Polar codes, on the other hand, belong to the same code family as RM codes, see e.g., Abbe et al., Section IV-D, and are hence implicitly covered.\nPrototypes from BCH Codes BCH codes are known to have good minimum distance in low dimensions, and although the exact minimum distance is not known in general , we find empirically that it approaches dmin = n/2 in dimensions n < K. They are implemented in the Galois Python library (v0.3.8).\nPrototypes from RM Codes The distance properties of RM codes are easier to characterise. Their construction is simple and gives straight-forward distance guarantees. In fact, they provide no worse than orthogonal prototypes in dimension n \u2248 K/2.\nLemma 3.2 (Separation Guarantees for RM Codes). Let K be the smallest power of 2 such that K > K. Then, RM codes in dimension n \u2265 K/2 have minimum distance dmin = n/2 and guarantee that $\\langle c_i, c_j \\rangle \\leq 0$.\nProof. By construction, RM codes are [n, k] codes with n = 2m, k = $\\sum_{i=0}^r {m \\choose i}$, and minimum distance dmin = 2m-r. Hence, we have cosine similarity $\\langle c_i, c_j \\rangle < 0$ if r = 1, namely if 1 + m = 1 + log2 n = k > log2 K > log2 K or if n \u2265 K/2 \u2265 K/2.\nRemark 3.3. It is important to note that the cosine similarity guarantee $\\langle c_i, c_j \\rangle \\leq 0$ is not equivalent to pairwaise orthogonality. However, every codeword is locally orthogonal to all its minimum-distance neighbours and has no worse than orthogonal separation globally. We investigate the global cosine similarity distribution for all prototype generation schemes in Figure 3.\nRealisable Dimensions with Codes As has been shown, binary codes provide a flexible way to derive prototypes. However, there is a restriction on the dimensions which are realisable: RM codes are defined for n = 2m, and BCH codes are defined for n = 2m \u2212 1 for every m\u2208N+. Moreover, additional dimensions are realisable: codes can be punctured by removing dimensions, thereby creating a lower-dimensional code, and they can be extended by adding more dimensions. In general, puncturing a code by 1 bit will reduce its minimum distance by 1. Similarly, extending a code can (but is not guaranteed to) increase the minimum distance. Therefore, RM and BCH codes can have good distance properties around dimensions n = 2m and n = 2m \u2212 1, respectively, but not for general n. Compared to , which is only valid in n = K \u2212 1, coding-based prototypes hence improve flexibility by guaranteeing good separation for a larger set of admissible dimensions.\n3.2. Coding-Theoretic Bounds\nIn this section, we provide both upper and lower bounds the worst-case cosine similarity of hyperspherical prototypes in (P). Our achievable (upper) bound is based on Lemma 2.1, which states that good binary codes exist. Our converse (lower) bound is based on results from spherical coding theory, which shows that the minimum separation cannot be improved beyond near orthogonality. We begin by recalling the Rankin bound from spherical coding theory .\nLemma 3.4 (Rankin Bound). Any set of K hyperspherical prototypes C satisfies that\n$\\max_C \\min_{i\\neq j} ||c_i - c_j ||^2 \\leq \\frac{2K}{K-1}$\nNow, we may state the achievable and converse bounds.\nTheorem 3.5. There exists a set of hyperspherical prototypes C with cosine similarity at most (separation at least)\n$\\max_{i\\neq j} \\langle c_i, c_j \\rangle \\leq 1 - \\frac{2d_{GV}}{n}$                                                                 (3)\nwhere day denotes the largest solution to the Gilbert-Varshamov bound in (2). Conversely, no set of prototypes exists with maximum cosine similarity smaller (better separation) than\n$\\max_{i\\neq j} \\langle c_i, c_j \\rangle \\geq \\frac{-1}{K-1}$                                                                  (4)\nProof. The achievable bound (3) follows directly from combining Proposition 3.1 and Lemma 2.1. For the converse bound, recalling that $||c_i \u2013 c_j||\u00b2 = 2 \u2013 2\\langle c_i, c_j \\rangle$, applying Lemma 3.4, and simplifying yields (4).\n3.3. Beyond Binary Codes\nIn this section, we briefly discuss the generalisation of our results to the case of q-ary codes and motivate the choice of restricting our attention to binary codes.\nAssume a construction that combines an [nq, kq] code U over the Galois field GF(q), with minimum Hamming distance $d_H^{(U)} = d_{min}^{(U)}$, with a mapping $\u03c0_q^{(\u03c0)}$ that maps q-ary symbols u \u2208 {0,...,q \u2013 1} to points $\u0113 =  \u03c0_q^{(\u03c0)}(u)$ on the l-dimensional unit hypersphere Sl\u22121, and with pairwise Euclidean distance of at least $d_{E,min}^{(\u03c0)}$. Then, an nq-dimensional q-ary vector u can be mapped to the unit hypersphere Sn-1 in n = nq \u00b7l dimensions by realising the mapping\nc = \u03c0q(u) := $\\frac{1}{\\sqrt{n_q}} ((\u03c0_q^{(1)}(u^{(1)})),..., (\u03c0_q^{(n_q)}(u^{(n_q)})))$.                                                                 (5)\nThen, similarly to the binary case, by a proper choice of the code parameters, hyperspherical prototypes with good separation guarantees can be obtained.\nProposition 3.6. Assume U is the codebook of a q-ary $[k_q, n_q]$ code with a minimum Hamming distance $d_{H,min}^{(U)}$ that is mapped into the unit hyperspehere Sn\u22121 in n = nq\u00b7l dimensions with the mapping \u03c0q from (5). Furthermore, let $d_{E, min}^{(\u03c0)}$ be the minimum Euclidean distance achieved by the component mapping $\u03c0_q^{(\u03c0)}$. Then, for every codeword pair ui, uj \u2208 U with i \u2260 j, the cosine similarity between ci and cj is upper bounded by\n$\\langle c_i, c_j \\rangle \\leq 1 - \\frac{2 d_{H,min}^{(U)}}{n_q} d_{E,min}^{(\u03c0)}$.\nProof. The proof follows along the same lines as the proof of Proposition 3.1.\nAssume we want to minimise the upper bound on the cosine similarity. We then want to find a q-ary code with as large minimum distance as possible. The Singleton bound states that every [nq, kq] linear code has minimum distance dmin <ng-kq +1. Reed-Solomon (RS) codes with parameters log\u0105 K < kq \u2264 nq \u2264 q achieve the Singleton bound with equality, and the only binary code achieving the Singleton bound is the repetition code. Note that for both one-hot encoding and the Kasarla et al. mapping, the mutual cosine similarity is constant for all prototype pairs, and hence the class to prototype mapping does not matter.\nConsider now combining RS codes with the Kasarla et al. mapping, which has $d_{E,min}^{(\u03c0)} = \\frac{2q}{(q-1)}$. Then, the cosine similarity for this prototype construction is guaranteed to be upper bounded by\n$\\langle c_i, c_j \\rangle \\leq 1 - \\frac{q}{n_q} \\frac{n_q-k_q+1}{(q-1)}$.\nFrom this result, it follows that the cosine similarity of this construction becomes strictly negative if, and only if kq < nq/q + 1 \u2264 2, where the second inequality comes from the requirement ng \u2264 q on the length of RS codes. That is, RS codes only guarantee a strictly negative cosine similarity for kq = 1, given that q > K. However, in that case, the mapping by already guarantees the optimal separation, and there is no benefit by further extending the dimensions beyond nq = 1 with an additional code. For kq = 2, nq = q, and under the condition q\u00b2 > K, we can guarantee a cosine similarity $\\langle c_i, c_j \\rangle \\leq 0$ for n = (q - 1) \u00b7 q dimensions. In the favourable case where q = 2m and K = 22m, the construction achieves $\\langle c_i, c_j \\rangle \\leq 0$ for n = 22m - 2m = K - \u221aK, which for m > 1 is larger than n=22m-1 = K/2. n=K/2 is however obtained by the RM-code-based construction as demonstrated in Lemma 3.2. Hence, there is no benefit employing RS codes in conjunction with the mapping by compared to the RM-code-based construction.\n3.4. Optimisation-Based Prototypes\nIn this section, we compare numerical approaches to approximately solve the non-convex and combinatorial problem (P). Throughout, we employ projected gradient descent to deal with the non-convexity introduced by the unit norm constraint, and compare different relaxations to the combinatorial part of the problem.\nMinimising the Average Worst-Case Similarity Mettes et al. note that solving the combinatorial minimisation in (P) is numerically inefficient. Instead, they minimise the average maximum cosine similarity per prototype. More specifically, they define the matrix of prototypes C := [C1, \u2026\u2026\u2026, CK] \u2208 Rn\u00d7K describing the codebook C and propose the problem\n$\\min_C \\frac{1}{K} \\sum_{i=1}^K \\max_{j} M_{i,j}$,\ns.t. M = CTC \u2212 2I,\n$||c_i|| = 1$,                                                                                (PAVG)\nwhere Mi,j denotes the (i, j)-th element of the matrix M and I denotes the identity matrix. Since the diagonal elements of CTC are always 1, subtracting twice the identity matrix avoids selecting these. The improvement over the original problem (P) is that multiple prototypes are updated at each gradient step, which improves the convergence speed. However, no proof of convergence or optimality is presented.\nLog-Sum-Exp Relaxation We propose a convex relaxation to the combinatorial problem, which we show numerically that it closely approximates the converse bound in Theorem 3.5. Specifically, we propose to use the log-sum-exp approximation of the maximum. It is folklore knowledge that for x \u2208 Rn we have\n$\\max_i x_i \\leq \\frac{1}{t} log \\sum_{i=1}^n exp(tx_i) \\leq max_i x_i + \\frac{log \\ n}{t}$,\nfor any temperature t > 0. Moreover, the function is convex. For a large temperature t, this problem approaches the original problem (P). By carefully choosing a scheduler for the temperature, we are able to balance the need to update multiple prototypes, and to approximate the original problem. Hence, we propose the problem\n$\\min_c \\frac{1}{t} log \\sum_{i\\neq j} exp(t\\langle c_i, c_j \\rangle)$,                                                                             (PLSE)\nwhich can be rewritten as a sum over the upper (or lower) triangular part of exp(tCTC), excluding the diagonal.\n3.5. Computational Complexity\nIn this section, we comment on the computational complexity of the prototype generation schemes in order to give a complete characterisation of the methods. We note however that the computational complexity of generating prototypes is negligible in comparison to network training.\nOptimisation-Based Prototypes The optimisation-based prototypes from (PLSE) and (PAVG) both require O(nK2) operations per gradient step, since all the K2 inner products $\\langle c_i, c_j \\rangle$ of n-dimensional vectors need to be calculated. In practice, the wall-clock time spent on these calculations is small: Even on a laptop, the computations take on the order of seconds even for K = 1000 and n \u2248 K."}, {"title": "4. Experiments", "content": "In this section, we evaluate the separation in terms of maximum cosine similarity for the considered prototype generation schemes and present numerical results on CIFAR-100. We also present supplementary results on MNIST in Appendix C. We emphasise that our aim with these experiments is to investigate the relation between prototype separation and performance, and not necessarily to find the best performing realisations of the algorithms.\n4.1. Experimental Setup\nFor optimisation-based prototypes, we follow Mettes et al. and use stochastic gradient descent (SGD) with learning rate 0.1 and momentum 0.9 over 1 000 epochs. For the log-sum-exp prototypes, we scale the temperature linearly with epochs from 1 to K. For CIFAR-100, we use a ResNet-34 backbone as implemented by Kasarla et al., and we also use the same hyperparameters (SGD with a cosine annealing learning rate scheduler, learning rate 0.1, momentum 0.9, weight decay 5 \u00d7 10-4, and batch size 512 over 200 epochs), with standard data augmentation schemes (random 32 \u00d7 32 crops with padding 4, random horizontal flips with probability 1/2, and random rotations with up to 15\u00b0). We use cross-entropy loss on the cosine similarities C\u012bz between the prototypes C and the output z from the ResNet-34 backbone. At test time, classification is done via nearest-neighbour decoding, or equivalently, maximum cosine similarity decoding, by choosing the class of the nearest prototype. All our results are averaged over 5 runs, and we use a randomised validation set (20 % of the training set) for every run. For MNIST, we use the lightweight network proposed by , with the same hyperparameters and data augmentations as for CIFAR-100, except that we rotate by up to 30\u00b0 (instead of 15\u00b0) and do not flip horizontally.\nSome additional remarks on prototype generation are in order. For BCH and RM codes, the mapping between class and prototype is fixed, while the optimisation-based mappings from (PLSE) and (PAVG) randomises the assignment across different runs. Therefore, for a fair comparison for BCH and RM codes, we both average over different class to prototype mappings, as well as over a fixed class to prototype mapping. Note that for both one-hot encoding and the Kasarla et al. mapping, the mutual cosine similarity is constant for all prototype pairs, and hence the class to prototype mapping does not matter.\n4.2. Prototype Separation Guarantees\nWe evaluate the achieved prototype separation in terms of maximum cosine similarity over the dimension n for \u039a\u2208 {10,100,1000} classes. The results for K = 100, corresponding to the models trained on CIFAR-100 in Sec-tion 4.3, are presented in Figure 2 along with the achievable and converse bounds from Theorem 3.5. Plots for K = 10 and K = 1 000 classes are provided in Appendix A.\nFor K = 100, the results confirm our theoretical analysis in Sections 3.1 and 3.2. For n = 99, the mapping by Kasarla et al. achieves the lower bound as expected. RM codes achieve zero worst-case cosine similarity for n \u2265 64, and the maximum cosine similarity achieved by BCH codes for n \u2265 63 is slightly above zero. That is, the code-based designs give close-to-optimal separation guarantees with only approximately half the number of dimensions. Below n = 63, the optimisation-based schemes outperform the code-based designs, where the proposed log-sum-exp relaxation gives a slight advantage over Mettes et al.. For fewer classes (K = 10, see Appendix A), the optimisation-based methods outperform coding-based approaches, and solving the proposed relaxation (PLSE) instead of (PAVG) results in a big improvement. For a large number of classes (K = 1000, see Appendix A), the optimisation-based methods perform poorly, and coding-theoretic methods guarantee better separation in lower dimensions n < K. We therefore conclude that code-based prototypes are beneficial if the number of classes is large, in which case the achievable dimension compression also becomes an attractive feature.\nTo provide further insights, we provide histograms of the cosine similarities for the different prototype schemes in Figure 3. We compare the schemes for K = 100 classes in dimension n = 16 and, for the sake of illustration, include prototypes created uniformly at random as a baseline. As the histograms show, the optimisation moves probability mass from the right tails towards lower cosine similarity, where the log-sum-exp relaxation achieves a slightly lower maximum cosine similarity. For the coding-based methods, the cosine similarities concentrate in a few different values which can be directly calculated from the weight distribution (or weight-enumerator) function of the linear codes.\n4.3. Experiments on CIFAR-100\nWe now turn to results on CIFAR-100, where we compare the performance of different prototype schemes. Figure 4 shows classification accuracy on the test set for different prototype schemes in different dimensions. We notice a thresholding effect around n = K, indicating that little is gained by adding dimensions, which is consistent with the observed worst-case similarity in Figure 2. For n \u2208 {31, 63, 127}, the performance of BCH-code-based prototypes averaged over the label mapping dominates the optimization-based schemes. For n = 63, the performance is close to the performance of one-hot encoding. The mapping by Kasarla et al. still performs best at n = 99, which can be expected since it guarantees a slightly lower worst-case cosine similarity.\nTo illustrate the separation/accuracy tradeoff for the different prototype schemes, Figure 5 plots the accuracy over the maximum cosine similarity across all our trained models. Through linear regression we find that, as expected, more dissimilar prototypes tend to yield better results. However, there is a significant variance in the accuracy within models trained with the same class of prototypes, and moreover, across different methods with the same maximum similarity. Part of the variance is explained by our use of a randomised validation set. However, as the difference in BCH code performance between the fixed mapping and the average mappings, and the lower performance of RM codes show (see Figure 4), the alignment of prototype similarities with semantic similarities of classes appears important.\nIn particular, the lower accuracy of RM codes in dimensions n = 64 and n = 128 is insightful. In these dimensions, they provide no worse than orthogonal prototypes (see Lemma 3.2), as well as K/2 prototype pairs which are diametrically opposed with cosine similarity -1. Investigating pairs of classes which were assigned diametrically opposed prototypes, we find several pairs with high semantic similarity, for example leopard and lion; shrew and skunk; and seal and shark. Compared to Mettes et al., who argued for the importance of the maximum and average similarity, our results indicate that additional properties beyond maximum and average similarity are important. Hence, we argue that further investigation on incorporating the semantics in the data in the labelling of the prototypes is needed."}, {"title": "5. Conclusion", "content": "In this paper, we have analysed the geometry of hyperspherical prototypical learning with tools from coding theory. Firstly, we have presented"}]}