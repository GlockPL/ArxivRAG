{"title": "A Coding-Theoretic Analysis of Hyperspherical Prototypical Learning Geometry", "authors": ["Martin Lindstr\u00f6m", "Borja Rodr\u00edguez-G\u00e1lvez", "Ragnar Thobaben", "Mikael Skoglund"], "abstract": "Hyperspherical Prototypical Learning (HPL) is a supervised approach to representation learning that designs class prototypes on the unit hypersphere. The prototypes bias the representations to class separation in a scale invariant and known geometry. Previous approaches to HPL have either of the following shortcomings: (i) they follow an unprincipled optimisation procedure; or (ii) they are theoretically sound, but are constrained to only one possible latent dimension. In this paper, we address both shortcomings. To address (i), we present a principled optimisation procedure whose solution we show is optimal. To address (ii), we construct well-separated prototypes in a wide range of dimensions using linear block codes. Additionally, we give a full characterisation of the optimal prototype placement in terms of achievable and converse bounds, showing that our proposed methods are near-optimal.", "sections": [{"title": "1. Introduction", "content": "Representation learning addresses the problem of learning a mapping from a high-dimensional input space to a lower-dimensional representation space subject to suitable inductive biases. These biases are imposed on learning algorithms as additional constraints on, for example, the network architecture or the optimisation algorithm. Geometry-based inductive biases have long been popular in representation learning. For instance, imposing unit norm constraints on the representations has been employed in different unsupervised learning methods, either through explicit normalisation or norm-invariant loss functions in variational autoencoders, or in self-supervised learning. Imposing representation separation is another common inductive bias used, for example, in contrastive learning and supervised representation learning.\nIn the supervised learning setting, one way to impose representation separation is through prototypical learning. Each class is assigned a prototype, and these are specified a priori to maximise their separation and are held fixed during training, where the algorithm attempts to map input samples to their class prototypes. Therefore, the representations are biased towards being separated based on their class. Recently, Hyperspherical Prototypical Learning (HPL) started imposing unit norm constraints to prototypical learning. This places the representations on the hypersphere and thus enhances representation separation bias in a scale invariant and known geometry.\nTo illustrate the idea behind HPL, consider the na\u00efve approach of selecting prototypes as the familiar one-hot encoding that, for K classes, picks the canonical basis vectors {e1,..., ek} in dimension K as prototypes. As illustrated in Figure 1 (left), this results in a suboptimal class separation on the hypersphere. Instead, HPL attempts at placing K maximally separated prototypes on the n-dimensional hypersphere Sn-1, hopefully with n < K. This combinatorial and non-convex problem is well-studied, but even on S2 the problem is unsolved for general K, and optimal solutions are only known for K = 1, ..., 14, and 24. Despite this, approximate solutions have been proposed. propose a relaxation of the problem that however only achieves suboptimal separation. Kasarla et al. (2022), on the other hand, propose a closed-form solution that we show to be optimal; however, it is only applicable in dimension n = K \u2212 1.\nIn this paper, we propose two new methods for designing hypershperical prototypes and present sharp bounds on the optimal separation that can be achieved by placing an arbitrary number of prototypes K < 2n on a hypershpere of dimension n. Our approach rests on theory and con-"}, {"title": "2. Background", "content": "We start with a formal problem formulation for designing a codebook of hyperspherical prototypes in the HPL setting. Then, we connect binary error correcting codes defined in the Hamming space to hyperspherical prototypes. After that, we provide a brief overview of fundamental concepts in coding theory that are used in this paper to derive code-based hyperspherical prototypes with good separation properties. The interested reader is encouraged to consult for a more detailed treatment.\n2.1. Problem Formulation\nWe consider the HPL setting with K classes and n dimensions; that is, we are interested in placing K prototypes c1,...,ck on the n-dimensional unit hypersphere Sn-1, where the dimension n is a hyperparameter. Our objective is maxmising the Euclidean distance between every pair of prototypes ci, cj \u2208 Sn\u22121 (i \u2260 j). Clearly, the Euclidean distance de(ci, cj) is bounded in the range [0, 2] and satisfies that ||ci \u2212 cj||\u00b2 = 2 \u2212 2\u27e8ci, cj\u27e9 for all ci, cj \u2208 Sn\u22121. Hence, maximising the Euclidean distance is equivalent to minimising the cosine similarity \u27e8ci, cj\u27e9. In turn, this is equivalent to maximising the angle \u03b1 between ci and cj since \u03b1 = arccos \u27e8ci, cj\u27e9. Therefore, we will use the cosine similarity as a notion of separation throughout this paper. The objective in HPL is then designing a codebook C := {ci \u2208 Sn\u22121 : i = 1,..., K} of K well-separated hyperspherical prototypes, which can be summarized in the following optimisation problem:\nUnfortunately, this problem is both non-convex due to the unit norm constraint, and combinatorial due to the search of the worst pair of prototypes ci, cj (i \u2260 j), requiring tractable relaxations that yield approximate solutions."}, {"title": "2.2. Connecting Codes to Protypes", "content": "The approach in this paper leverages coding theory to design n-dimensional binary vectors (that is, members of the n-dimensional Hamming space), which are mapped onto the n-dimensional hypersphere Sn-1, thereby creating prototypes with good separation. To provide some intuition as to how error correcting codes relate to placing prototypes that are maximally spaced apart, consider the mapping \u03c0: {0,1}n \u2192 Sn-1 that maps n-dimensional binary vectors b from the n-dimensional Hamming space to points c on the hypersphere. More precisely, the mapping is defined as\nand enforces that c(l) \u2208 {\u22121/\u221an,+1/\u221an} and ||c||\u00b2 = 1. This approach allows us to place 2n points on the unit hypersphere Sn-1 with a cosine similarity of at most \u27e8c, c'\u27e9 \u2264 1 \u2212 2/n for every pair c = \u03c0(b) and c' = \u03c0(b') with b \u2260 b'. For K < 2n, we can improve the separation guarantees by carefully selecting the K binary vectors placed on the hypersphere via the mapping \u03c0. Error correcting codes provide a systematic way to achieve this, and the concept is illustrated for n = 3 in Figure 1.\nAs a baseline, consider one-hot encoding (Figure 1, left), which provides K = 3 orthogonal prototypes. Using carefully selected corners of the unit cube and the mapping \u03c0, we can substantially improve on one-hot encoding. Reducing to K = 2 prototypes (Figure 1, centre), the unit cube corners corresponding to b\u2081 = (0,0,0) and b\u2082 = (1, 1, 1) are diametrically opposed with a cosine similarity of -1, which is optimal. From a coding theory perspective, this corresponds to a repetition code with Hamming distance dH(b1, b2) = 3 between codewords; that is, the codewords differ in 3 bits. Increasing to K = 4 prototypes (Figure 1, right), the unit cube corners corresponding to b\u2081 = (0,0,0), b\u2082 = (0,1,1), b\u2083 = (1,0,1), and b\u2084 = (1,1,0) have a mutual cosine similarity of -1/3, which is an improvement over one-hot encoding (Figure 1, left) while increasing the number of classes at the same time. Again, the set B = {b1, b2, b3, b4} constitutes a binary linear code with Hamming distance d\u00ed(bi, bj) = 2 between its codewords.\nThis example demonstrates that due to the definition of the mapping function \u03c0, there exists a relation between the separation of vectors c = \u03c0(b) and c' = \u03c0(b') on the unit hypersphere and the Hamming distance dH (b, b') of the binary vectors b and b' in the sense that a large Hamming distance implies a large separation. We will make this result explicit in Section 3.1. Error correcting codes, designed to have a large Hamming distance dH (b, b') between every pair of codewords b and b', are hence a well suited tool for designing hyperspherical prototypes, and coding theory provides us with useful bounds on the achievable separation."}, {"title": "2.3. A Primer on Coding Theory", "content": "The systematic study of error correcting codes dates back to the seminal work of Hamming (1950). By introducing redundancy in a structured way, error correcting codes allow for error detection and correction in messages and data, and are essential for guaranteeing the reliability of today's digital communication, computation, and storage systems. Linear codes defined over the Galois field GF(q), where q = pm and p is a prime, are of special interest as they offer a structure that can be used for efficient encoding, decoding, and analysis of the distance properties of the code.\nIn this paper, we mainly restrict ourselves to binary linear block codes (that is, q = p = 2), and only briefly discuss extensions to q-ary codes with q > 2. A binary block code with parameters [n, k] is specified by a codebook B of 2k binary codewords of length n and a bijective encoder \u03c6 that maps the set of all length-k binary vectors into the code B. This adds n \u2212 k bits of redundancy, which we can use to detect and correct errors in the codeword. The rate of the code is defined as R = k/n, where a low rate corresponds to a high redundancy. The error detection and correction capabilities rely on the separation of codeword pairs bi, bj \u2208 B in Hamming distance, namely\nA binary code with minimum Hamming distance\ncan detect dmin \u2212 1 errors and correct \u230a(dmin\u22121)/2\u230b errors.\nIn this paper, we fix k = \u2308log2(K)\u2309 given K classes, suggesting that we are in the low-rate or high-redundancy regime if n is of the order of K. Noting that low-rate codes offer large separation in terms of minimum Hamming distance, we can expect good separation by adopting a code-based approach. To this end, a fundamental result in coding theory is that good codes exist. This is formalised by the well-known Gilbert-Varshamov bound."}, {"title": "3. Hyperspherical Prototype Design", "content": "In this section, we present our main contributions towards solving the optimization problem (P). As mentioned earlier, this problem is both non-convex due to the unit norm constraint, and combinatorial due to the search of the worst pair of prototypes ci and cj with i \u2260 j. Our contributions are focused on relaxations to the problem (P) and bounds on the optimal solution. Section 3.1 formalises the coding-theoretic approach introduced with the example in Figure 1; Section 3.2 uses coding-theoretic tools to bound the optimal solution to (P); and Section 3.4 presents a relaxation to (P) which achieves the bound on the optimal solution.\n3.1. Coding-Theoretic Prototypes\nWe begin by formalising the intuition provided in Section 2.2, namely that a binary [n, k] code with the codebook B and a large minimum distance dmin gives good prototypes. Firstly, notice that if we want K prototypes, then we need |B| = 2k \u2265 K. Additionally, recall that the mapping c = \u03c0(b) defined in (1) produces a unit-norm vector for any binary vector b. Then, we can guarantee that binary code-based constructions guarantee good separation.\nProposition 3.1. Assume that B is the codebook of a binary [n, k] code with minimum distance dmin. Then, for every pair bi, bj \u2208 B with i \u2260 j, the cosine similarity between ci = \u03c0(bi) and cj = \u03c0(bj) is upper bounded by\nProof. To show the bound on the cosine similarity, notice that two binary vectors differing in dH (bi, bj) positions obey that \u2211nl=1(bi(l) \u2013 bj(l))\u00b2 = dH(bi, bj). Expanding ||ci \u2212 cj||\u00b2 in two ways gives\nRearranging this and recalling that dH(bi, bj) \u2265 dmin yields the desired result.\nSince good codes exist (see Lemma 2.1), we provide two examples of binary codes whose minimum distance is close to dmin = n/2 in dimensions where n < K. That is, there exist no worse than orthogonal prototypes with zero worst-case cosine similarity in dimensions n \u2248 K/2. Additionally, these codes are easy to implement in Python, and hence are easily integrable in modern machine learning software. The codes are the Bose-Chaudhuri\u2013Hocquenghem (BCH) and Reed-Muller (RM) codes. Other code families like low-density parity-check (LDPC) codes and sparse graph codes are not competitive in terms of minimum distance guarantees since their minimum distance is usually lower than the one predicted by the Gilbert-Varshamov bound, see e.g., Mitchell et al. (2015, Figure 9). These popular code families are hence not further considered in this paper. Polar codes, on the other hand, belong to the same code family as RM codes, see e.g., Abbe et al. (2021, Section IV-D), and are hence implicitly covered.\nPrototypes from BCH Codes BCH codes are known to have good minimum distance in low dimensions and although the exact minimum distance is not known in general we find empirically that it approaches dmin = n/2 in dimensions n < K. They are implemented in the Galois Python library.\nPrototypes from RM Codes The distance properties of RM codes are easier to characterise. Their construction is simple and gives straight-forward distance guarantees In fact, they provide no worse than orthogonal prototypes in dimension n \u2248 K/2.\nLemma 3.2 (Separation Guarantees for RM Codes). Let K be the smallest power of 2 such that K > K. Then, RM codes in dimension n \u2265 K/2 have minimum distance dmin = n/2 and guarantee that \u27e8ci, cj\u27e9 \u2264 0.\nProof. By construction, RM codes are [n, k] codes with n = 2m, k = \u2211rm=0 (mi), and minimum distance dmin = 2m\u2212r. Hence, we have cosine similarity \u27e8ci, cj\u27e9 < 0 if r = 1, namely if 1 + m = 1 + log2 n = k > log2 K > log2 K or if n \u2265 K/2 \u2265 K/2.\nRemark 3.3. It is important to note that the cosine similarity guarantee \u27e8ci, cj\u27e9 \u2264 0 is not equivalent to pairwaise orthogonality. However, every codeword is locally orthogonal to all its minimum-distance neighbours and has no worse than orthogonal separation globally. We investigate the global cosine similarity distribution for all prototype generation schemes in Figure 3."}, {"title": "Realisable Dimensions with Codes", "content": "As has been shown, binary codes provide a flexible way to derive prototypes. However, there is a restriction on the dimensions which are realisable: RM codes are defined for n = 2m, and BCH codes are defined for n = 2m \u2212 1 for every m \u2208 N+. Moreover, additional dimensions are realisable: codes can be punctured by removing dimensions, thereby creating a lower-dimensional code, and they can be extended by adding more dimensions. In general, puncturing a code by 1 bit will reduce its minimum distance by 1. Similarly, extending a code can (but is not guaranteed to) increase the minimum distance. Therefore, RM and BCH codes can have good distance properties around dimensions n = 2m and n = 2m \u2212 1, respectively, but not for general n. Compared to which is only valid in n = K - 1, coding-based prototypes hence improve flexibility by guaranteeing good separation for a larger set of admissible dimensions.\n3.2. Coding-Theoretic Bounds\nIn this section, we provide both upper and lower bounds the worst-case cosine similarity of hyperspherical prototypes in (P). Our achievable (upper) bound is based on Lemma 2.1, which states that good binary codes exist. Our converse (lower) bound is based on results from spherical coding theory, which shows that the minimum separation cannot be improved beyond near orthogonality. We begin by recalling the Rankin bound from spherical coding theory.\nLemma 3.4 (Rankin Bound). Any set of K hyperspherical prototypes C satisfies that\nNow, we may state the achievable and converse bounds.\nTheorem 3.5. There exists a set of hyperspherical prototypes C with cosine similarity at most (separation at least)\nwhere dGV denotes the largest solution to the Gilbert-Varshamov bound in (2). Conversely, no set of prototypes exists with maximum cosine similarity smaller (better separation) than\nProof. The achievable bound (3) follows directly from combining Proposition 3.1 and Lemma 2.1. For the converse bound, recalling that ||ci \u2013 cj||\u00b2 = 2 \u2013 2\u27e8ci, cj\u27e9, applying Lemma 3.4, and simplifying yields (4).\nThe bounds are numerically evaluated in Section 4. A number of remarks are in order. In many practical settings, the converse bound \u22121/(K-1) is close to 0. It is therefore impossible to achieve a maximum cosine similarity that is notably better than one-hot encoding. However, as Lemma 3.2 shows, it is possible to have no worse than orthogonal prototypes in low dimension n = K/2. Moreover, as we will show with numerical examples, it is possible to have approximately orthogonal prototypes in much lower dimension than n = K. Finally, we note that the upper bound can be tightened for n > K by recalling one-hot encoding. The bounds are therefore sharp, meaning that orthogonal prototypes are achievable and near-optimal for a large number of classes K. Finally, it is interesting to note that the mapping proposed by  is optimal since it achieves the converse bound.\n3.3. Beyond Binary Codes\nIn this section, we briefly discuss the generalisation of our results to the case of q-ary codes and motivate the choice of restricting our attention to binary codes.\nAssume a construction that combines an [nq, kq] code U over the Galois field GF(q), with minimum Hamming distance d(U)H,min = dmin, with a mapping \u03c0(\u03c0)q that maps q-ary symbols u \u2208 {0,...,q \u2212 1} to points \u0113 = \u03c0\u03b1q(u) on the l-dimensional unit hypersphere Sl\u22121, and with pairwise Euclidean distance of at least d(\u03c0)E,min. Then, an nq-dimensional q-ary vector u can be mapped to the unit hypersphere Sn\u22121 in n = nq \u00b7l dimensions by realising the mapping\nThen, similarly to the binary case, by a proper choice of the code parameters, hyperspherical prototypes with good separation guarantees can be obtained.\nProposition 3.6. Assume U is the codebook of a q-ary [kq, nq] code with a minimum Hamming distance d(U)H,min = dH that is mapped into the unit hyperspehere Sn\u22121 in n = nq\u00b7l dimensions with the mapping \u03c0q from (5). Furthermore, let d(\u03c0)E,min be the minimum Euclidean distance achieved by the component mapping \u03c0\u03b1). Then, for every codeword pair ui, uj \u2208 U with i \u2260 j, the cosine similarity between ci and cj is upper bounded by\nProof. The proof follows along the same lines as the proof of Proposition 3.1."}, {"title": "3.4. Optimisation-Based Prototypes", "content": "In this section, we compare numerical approaches to approximately solve the non-convex and combinatorial problem (P). Throughout, we employ projected gradient descent to deal with the non-convexity introduced by the unit norm constraint, and compare different relaxations to the combinatorial part of the problem.\nMinimising the Average Worst-Case Similarity  note that solving the combinatorial minimisation in (P) is numerically inefficient. Instead, they minimise the average maximum cosine similarity per prototype. More specifically, they define the matrix of prototypes C := [c1, \u2026\u2026\u2026, cK] \u2208 Rn\u00d7K describing the codebook C and propose the problem\nwhere Mi,j denotes the (i, j)-th element of the matrix M and I denotes the identity matrix. Since the diagonal elements of CTC are always 1, subtracting twice the identity matrix avoids selecting these. The improvement over the original problem (P) is that multiple prototypes are updated at each gradient step, which improves the convergence speed. However, no proof of convergence or optimality is presented.\nLog-Sum-Exp Relaxation We propose a convex relaxation to the combinatorial problem, which we show numerically that it closely approximates the converse bound in Theorem 3.5. Specifically, we propose to use the log-sum-exp approximation of the maximum. It is folklore knowledge that for x \u2208 Rn we have\nfor any temperature t > 0. Moreover, the function is convex. For a large temperature t, this problem approaches the original problem (P). By carefully choosing a scheduler for the temperature, we are able to balance the need to update multiple prototypes, and to approximate the original problem. Hence, we propose the problem\nwhich can be rewritten as a sum over the upper (or lower) triangular part of exp(tCTC), excluding the diagonal.\n3.5. Computational Complexity\nIn this section, we comment on the computational complexity of the prototype generation schemes in order to give a complete characterisation of the methods. We note however that the computational complexity of generating prototypes is negligible in comparison to network training.\nOptimisation-Based Prototypes The optimisation-based prototypes from (PLSE) and (PAVG) both require O(nK2) operations per gradient step, since all the K2 inner products \u27e8ci, cj\u27e9 of n-dimensional vectors need to be calculated. In practice, the wall-clock time spent on these calculations is small: Even on a laptop, the computations take on the order of seconds even for K = 1000 and n \u2248 K.\nCoding-Theoretic Prototypes Very efficient implementations of error correcting codes exist: They are implemented and run in real time on billions of light-weight wireless devices. Since the codebooks are fixed, they need only be computed once, and can later be re-used across runs. For BCH codes, tables of their so called generator polynomials are available, and with those, all the K codewords can be enumerated quickly with a complexity of O(nK log(K)) operations. As an example, the generator polynomials of BCH codes are tabulated up to n = 210 \u2212 1 = 1023 in and up to n = 216 \u2212 1 = 65 535 in the MATLAB function bchgenpoly For RM codes, due to their simple structure, it is fast to generate the entire codebook, again with a complexity of O(nK log(K)) operations. This is done in fractions of a second even for K = 1000 and n \u2248 K on a laptop.\nKasarla et al. (2022) Prototypes The prototypes from also take less than a second to generate on a laptop. However, their implementation is recursive, and therefore requires increasing the recursion limit for large K. Similar to the coding-theoretic prototypes, they can be pre-computed and re-used across runs."}, {"title": "4. Experiments", "content": "In this section, we evaluate the separation in terms of maximum cosine similarity for the considered prototype generation schemes and present numerical results on CIFAR-100 We also present supplementary results on MNIST We emphasise that our aim with these experiments is to investigate the relation between prototype separation and performance, and not necessarily to find the best performing realisations of the algorithms.\n4.1. Experimental Setup\nFor optimisation-based prototypes, we follow Mettes et al. (2019) and use stochastic gradient descent (SGD) with learning rate 0.1 and momentum 0.9 over 1 000 epochs. For the log-sum-exp prototypes, we scale the temperature linearly with epochs from 1 to K. For CIFAR-100, we use a ResNet-34 backbone as implemented by and we also use the same hyperparameters (SGD with a cosine annealing learning rate scheduler, learning rate 0.1, momentum 0.9, weight decay 5 \u00d7 10-4, and batch size 512 over 200 epochs), with standard data augmentation schemes (random 32 \u00d7 32 crops with padding 4, random horizontal flips with probability 1/2, and random rotations with up to 15\u00b0). We use cross-entropy loss on the cosine similarities C\u012bz between the prototypes C and the output z from the ResNet-34 backbone. At test time, classification is done via nearest-neighbour decoding, or equivalently, maximum cosine similarity decoding, by choosing the class of the nearest prototype. All our results are averaged over 5 runs, and we use a randomised validation set (20 % of the training set) for every run. For MNIST, we use the lightweight network proposed by with the same hyperparameters and data augmentations as for CIFAR-100, except that we rotate by up to 30\u00b0 (instead of 15\u00b0) and do not flip horizontally.\nSome additional remarks on prototype generation are in order. For BCH and RM codes, the mapping between class and prototype is fixed, while the optimisation-based mappings from (PLSE) and (PAVG) randomises the assignment across different runs. Therefore, for a fair comparison for BCH and RM codes, we both average over different class to prototype mappings, as well as over a fixed class to prototype mapping. Note that for both one-hot encoding and the mapping, the mutual cosine similarity is constant for all prototype pairs, and hence the class to prototype mapping does not matter.\n4.2. Prototype Separation Guarantees\nWe evaluate the achieved prototype separation in terms of maximum cosine similarity over the dimension n for K\u2208 {10,100,1000} classes. The results for K = 100, corresponding to the models trained on CIFAR-100 in Section 4.3, are presented in Figure 2 along with the achievable and converse bounds from Theorem 3.5. Plots for K = 10 and K = 1 000 classes are provided in Appendix A.\nFor K = 100, the results confirm our theoretical analysis in Sections 3.1 and 3.2. For n = 99, the mapping by achieves the lower bound as expected. RM codes achieve zero worst-case cosine similarity for n \u2265 64, and the maximum cosine similarity achieved by BCH codes for n \u2265 63 is slightly above zero. That is, the code-based designs give close-to-optimal separation guarantees with only approximately half the number of dimensions. Below n = 63, the optimisation-based schemes outperform the code-based designs, where the proposed log-sum-exp relaxation gives a slight advantage over For fewer classes (K = 10, see Appendix A), the optimisation-based methods outperform coding-based approaches, and solving the proposed relaxation (PLSE) instead of (PAVG) results in a big improvement. For a large number of classes (K = 1000, see Appendix A), the optimisation-based methods perform poorly, and coding-theoretic methods guarantee better separation in lower dimensions n < K. We therefore conclude that code-based prototypes are beneficial if the number of classes is large, in which case the achievable dimension compression also becomes an attractive feature.\nTo provide further insights, we provide histograms of the cosine similarities for the different prototype schemes in Figure 3. We compare the schemes for K = 100 classes in dimension n = 16 and, for the sake of illustration, include prototypes created uniformly at random as a baseline. As the histograms show, the optimisation moves probability mass from the right tails towards lower cosine similarity, where the log-sum-exp relaxation achieves a slightly lower maximum cosine similarity. For the coding-based methods, the cosine similarities concentrate in a few different values which can be directly calculated from the weight distribution (or weight-enumerator) function of the linear codes (MacWilliams & Sloane, 1977, Chapter 2, \u00a71).\n4.3. Experiments on CIFAR-100\nWe now turn to results on CIFAR-100, where we compare the performance of different prototype schemes. Figure 4 shows classification accuracy on the test set for different prototype schemes in different dimensions. We notice a thresholding effect around n = K, indicating that little is gained by adding dimensions, which is consistent with the observed worst-case similarity in Figure 2. For n \u2208 {31, 63, 127}, the performance of BCH-code-based prototypes averaged over the label mapping dominates the optimization-based schemes. For n = 63, the performance is close to the performance of one-hot encoding. The mapping by still performs best at n = 99, which can be expected since it guarantees a slightly lower worst-case cosine similarity.\nTo illustrate the separation/accuracy tradeoff for the different prototype schemes, Figure 5 plots the accuracy over the maximum cosine similarity across all our trained models. Through linear regression we find that, as expected, more dissimilar prototypes tend to yield better results. However, there is a significant variance in the accuracy within models trained with the same class of prototypes, and moreover, across different methods with the same maximum similarity. Part of the variance is explained by our use of a randomised validation set. However, as the difference in BCH code performance between the fixed mapping and the average mappings, and the lower performance of RM codes show (see Figure 4), the alignment of prototype similarities with semantic similarities of classes appears important.\nIn particular, the lower accuracy of RM codes in dimensions n = 64 and n = 128 is insightful. In these dimensions, they provide no worse than orthogonal prototypes (see Lemma 3.2), as well as K/2 prototype pairs which are diametrically opposed with cosine similarity -1. Investigating pairs of classes which were assigned diametrically opposed prototypes, we find several pairs with high semantic similarity, for example leopard and lion; shrew and skunk; and seal and shark. Compared to who argued for the importance of the maximum and average similarity, our results indicate that additional properties beyond maximum and average similarity are important. Hence, we argue that further investigation on incorporating the semantics in the data in the labelling of the prototypes is needed."}, {"title": "5. Conclusion", "content": "In this paper, we have analysed the geometry of hyperspherical prototypical learning with tools from coding theory. Firstly, we have presented new code-based constructions to generate hyperspherical prototypes with strong minimum separation guarantees (in terms of worst-case cosine similarity). Secondly, we fully characterised the worst-case cosine similarity of these prototypes (in terms of achievable and converse bounds). Our prototypes are flexible and near-optimal in low dimension, thereby enabling a tradeoff between dimension and separation for a given number of classes. Our experimental results furthermore indicate that the classification accuracy does not only depend on the worst-case separation of prototypes, but also depends on the mapping from class labels to prototypes. We thus conclude that the alignment of semantic similarity with prototype separation is an important problem for further investigation. Additionally, the impact of prototype distance on prototype-based self-supervised learning schemes is also an important future consideration."}, {"title": "A. Separation Guarantees for K = 10 and K = 1 000 Prototypes", "content": "In this section, we provide additional results for prototype generation schemes for K = 10 and K = 1 000 prototypes, see Figures 6 and 7. For K = 10 prototypes, the optimisation-based approaches work well: in particular, solving (PLSE) provides no worse than orthogonal prototypes in dimension n = 8, and optimally separated prototypes in n = 16. However, the improvement over one-hot encoding and the Kasarla et al. (2022) mapping is small. On the other hand, for K = 1 000 prototypes, the coding-theoretic approaches can guarantee no worse than orthogonal (and therefore near-optimal) separation in n \u2248 K/2, while optimisation-based prototypes require high dimension to give approximately orthogonal prototypes."}, {"title": "B. Cosine Similarity Histograms for K = 10 and K = 1 000 prototypes", "content": "In this section, we show cosine similarity histograms for K = 10 and K = 1 000 prototypes, see Figures 8 and 9. The figures illustrate that optimisation-based methods are effective for a small number of prototypes, where the density can be moved around effectively to create good maximum cosine similarity properties. The coding-theoretic approaches concentrate the density in a few cosine similarities, which is beneficial for a large number of prototypes K, where they outperform the optimisation-based approaches in lower dimensions n."}, {"title": "C. Results on MNIST", "content": "In this section, we provide results on MNIST, similar to the ones on CIFAR-100, in Figures 10 and 11. We notice that while all the schemes perform well on MNIST, we are able to beat one-hot encoding and the mapping using prototypes obtained from solving (PLSE) in lower dimension. We also find that a smaller maximum cosine similarity is correlated with better performance, although the correlation is weaker here than for CIFAR-100.\nSimilar to CIFAR-100, we notice a high variance. In particular, notice the averaged mappings outperforms the fixed mappings for both BCH and RM codes. This again indicates that the semantic class to prototype mapping is important. Again, investigating mappings for RM codes for n = 8, where the prototypes are no worse than orthogonal, we find the following diametrically opposed pairs in the first trained model. For the fixed mapping, the pairs 4 and 5; and 8 and 9 were diametrically opposed. These pairs are semantically similar, especially when hand-written. A randomised class to prototype assignment has a chance of avoiding these semantically similar pairings."}]}