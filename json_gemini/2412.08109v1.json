{"title": "Unseen Horizons: Unveiling the Real Capability of LLM Code Generation Beyond the Familiar", "authors": ["Yuanliang Zhang", "Yifan Xie", "Shanshan Li", "Ke Liu", "Chong Wang", "Zhouyang Jia", "Xiangbing Huang", "Jie Song", "Chaopeng Luo", "Zhizheng Zheng", "Rulin Xu", "Yitong Liu", "Si Zheng", "Xiangke Liao"], "abstract": "Recently, large language models (LLMs) have shown strong potential in code generation tasks. However, there are still gaps before they can be fully applied in actual software development processes. Accurately assessing the code generation capabilities of large language models has become an important basis for evaluating and improving the models. Some existing works have constructed datasets to evaluate the capabilities of these models. However, the current evaluation process may encounter the illusion of \"Specialist in Familiarity\", primarily due to three gaps: the exposure of target code, case timeliness, and dependency availability. The fundamental reason for these gaps is that the code in current datasets may have been extensively exposed and exercised during the training phase, and due to the continuous training and development of LLM, their timeliness has been severely compromised.\nThe key to solve the problem is to, as much as possible, evaluate the LLMs using code that they have not encountered before. Thus, the fundamental idea in this paper is to draw on the concept of code obfuscation, changing code at different levels while ensuring the functionality and output. To this end, we build a code-obfuscation based benchmark OBFUSEVAL. We first collect 1,354 raw cases from five real-world projects, including function description and code. Then we use three-level strategy (symbol, structure and semantic) to obfuscate descriptions, code and context dependencies. We evaluate four LLMs on OBFUSEVAL and compared the effectiveness of different obfuscation strategy. We use official test suites of these projects to evaluate the generated code. The results show that after obfuscation, the average decrease ratio of test pass rate can up to 62.5%.", "sections": [{"title": "I. INTRODUCTION", "content": "With the rapid development of the Large Language Model (LLM), the code generation capability of LLMs has attracted lots of attention [1]-[8]. However, how to accurately assess the code generation capability of LLMs in production-level software development is still an open question. A variety of benchmark tests for code generation have been proposed, however, there are still gaps between these benchmark tests and the actual software development process.\nTraditional datasets as evaluation benchmarks [9]-[12] play key roles in evaluating the capabilities of LLMs. However, they focus mainly on standalone functions based on algorithmic problems, which cannot reflect the complexity of real software development, as LLMs may face potential challenges in handling code that is interdependent with other contextual elements of the project [13]. There have been several benchmarks [13]-[16] which are built based on real-world production projects, while there remain three gaps for these benchmarks to objectively evaluate the code generation capability of LLM: exposure of target code, case timeliness, and dependency availability.\nGap 1: Target code has been exposed in the pre-training stage. Previous benchmarks [13], [15], [16] have only rewritten the functional descriptions without modifying the code itself. This could result in the target code being exposed to LLMs during training, making the evaluation results not objective as the code has been exercised extensively.\nGap 2: Collected case is time-sensitive. With the rapid development of LLMs, data will be continuously trained. Although previous benchmark [14] has collected modified code from history after LLMs training, the modified code may still have been exposed to LLMs because of the existence of code clones. In addition, benchmarks that rely on historical data will suffer from the timeliness problem and cannot be used to evaluate subsequent releases of LLMs.\nGap 3: Precise dependencies are difficult to provide in real usage scenarios. Existing benchmarks [13], [15] directly provide the model with all the dependencies needed to generate the target code. However, such conditions can not be always satisfied in real development."}, {"title": "II. BACKGROUND", "content": "In this section, we first conduct a comprehensive examination of the latest advancements in Large Language Models (LLMs) within code generation. Subsequently, we delve into the related work of evaluations crafted for code generation, along with the limitations and challenges encountered in assessing the performance of LLMs. Finally, we introduce the motivation for incorporating code obfuscation in the evaluation of large language models."}, {"title": "A. Large Language Models for Code Generation.", "content": "The process of code generation, which involves the automatic creation of complete program code or the completion of code snippets from higher-level representations, such as natural language descriptions, models, or specifications, plays a pivotal role in enhancing programming efficiency and mitigating human error [18]\u2013[21]. Recent advancements in Large Language Models (LLMs) for code generation have garnered significant attention in the realm of computer science research. These LLMs, such as GPT-4 [22], ChatGLM [23], CODEX [9], and CodeGen [24], have demonstrated remarkable capabilities not only in general natural language processing tasks [25] but also in the specific area of code generation. Notably, GPT-4 achieved the highest pass rate on the HumanEval benchmark [9], indicating a growing trend to evaluate the code generation capacity of general LLMs [26]. Code-specific LLMs, which are trained primarily on massive code-specific corpora, often outperform general LLMs in code generation tasks [26]\u2013[33]. Diverse training approaches have been employed, with some models like InCoder [34] and StarCoder [35] being trained with the \"filling-in-the-middle\" capability for infilling missing code based on context. Varieties of code LLMs have been proposed, such as WizardCoder [29], Instruct-StarCoder [36], and Instruct-CodeGen [37], each designed with different training objectives."}, {"title": "B. Evaluations for LLM's Code Generation", "content": "Benchmark construction. Current benchmarks built based on real projects usually rewrite only the functional descriptions without modifying the code [11], [13]\u2013[15], [38], [39], which may lead to the \"code leak\u201d issue. SWE-BENCH [14] collects modified code from the project's history. However, the code may be in the training set of subsequent releases of LLMs. Future LLMs may become experts in solving problems in SWE-BENCH, but they may still struggle with new code problems in real-world scenarios. EvoCodeBench [40] periodically update the dataset, but there is still possibility of introducing similar code due to the existence of code clones.\nContext dependencies. Traditional benchmarks focus on generating independent code units, ignoring the contextual relationships between code [9], [41]\u2013[45]. For example, SWE-BENCH does not provide the complete dependencies of the generated code, so it is hard to distinguish the capability of LLMs to generate the target code and its dependencies. However, current studies indicate that only about 30% of methods in open-source projects are relatively independent [13]. In real-world scenarios, methods often depend on each other or share variables, which is not considered in these traditional benchmarks. Some works [13], [15] provide complete dependencies but do not include useless and obfuscated dependencies. This discrepancy does not align with software development scenarios and fails to accurately measure the ability of LLMs to assist developers in practical settings. Therefore, we need evaluation methods closer to real-world scenarios to comprehensively assess LLMs' performance in real-world software development.\nEvaluation methods. When evaluating the code generation capabilities of LLMs, existing studies and benchmarks focus on the basic correctness of the code, which is usually verified by executing simple test cases (e.g., unit tests) [46]\u2013[48]. In our work, we try to assess both syntactic and functional correctness of LLMs-generated code in real scenarios, by leveraging both compile checking and systematic testing, which aligns more closely with the requirements of real-world development. In addition, previous benchmarks were compared to Humaneval [9] to prove their validity [13], [15], [49], [50]. However, since the length of code to be generated by these benchmark tests does not match the distribution of code lengths in Humaneval, this comparison is inherently unfair. It therefore does not accurately reflect the validity of the benchmark tests."}, {"title": "C. Motivation of Using Code Obfuscation", "content": "As LLMs are continuously trained and released, traditional datasets would constantly be learned and trained by these models. Therefore, the timeliness of code may be a crucial factor when testing the generation capacity of LLM. In other words, code may become easier to generate because it already exists (or similar) in the training set.\nTo validate our conjecture, we conduct a pilot study to check whether the timeliness of code will affect the result of LLM generation capability. We use GPT-3.5-turbo (released at 2023.06.13) and GPT 4.0-preview (released at 2023.11.06) to do code competition problems from LeetCode [51]. We collect the problems from 2018.09 to 2018.11 (90 problems) and the problems from 2023.11 to 2024.01 (127 problems) as our test data. The later 127 problems came out after the release of two models, which were theoretically not presented in the model's training data.\nRelying solely on the latest code as a dataset is not a sustainable approach in the long run (as models will continuously train and evolve), and due to the existence of code reuse, even code written after the training cutoff date may still be similar to code in the training set. During the construction of our dataset, to mitigate the impact that LLMs may have seen the code, we have drawn inspiration from code obfuscation techniques [52]\u2013[54], which are originally used for making applications difficult to be decompiled or disassembled. After obfuscation, the code will become different from the training set while maintaining its functionality and output."}, {"title": "III. BENCHMARK CONSTRUCTION AND TESTING", "content": "In this section, we describe the approach of building and testing OBFUSEVAL to show the capability of LLM, and the workflow is shown in Fig. 3. The building process mainly includes two phases: collecting raw data from open-source projects (Section III-A), and obfuscating code (Section III-B). After that, we construct a testing framework to evaluate the effectiveness of code generation and code completion by the models on OBFUSEVAL (Section III-C)."}, {"title": "A. Raw Data Collection", "content": "To construct OBFUSEVAL, we first need to collect the raw data. The raw data includes functions modified after the large model training data cutoff date from real-world open source projects, as well as the context information these functions depend on within the project. Overall, we divide the raw data collection process into two parts: function collection and contextual dependency provision."}, {"title": "1) Functions Collection:", "content": "Function collection includes three main steps:\nStep 1: Repository Selection. The existing dataset mainly covers Java and Python projects. To demonstrate the code generation capabilities of the large language models in other programming languages, we chose C projects to build OBFUSEVAL. We set two conditions to filter repositories from GitHub: 1) The repository should be mature and well-maintained; 2) The project should have a comprehensive test suite for systematic testing. Finally, we selected five projects [55]\u2013[59], with over 20k average stars.\nStep 2: PRs Extraction and Function Filtering. We selected merged PRs from the chosen repositories and extract functions that meet the task criteria. Specifically, we extracted PRs and filter functions based on the following four criteria: 1) The PRs are merged after a certain time point to coincide with the training data cutoff date of specific models we tested; (more discussion in Section IV-A2); 2) The PRs modify the repository's test files to verify the modified code's functional correctness; 3) The functions are modified in the PR to ensure that LLM had not seen modified code in previous code base; 4) The functions are covered by the test suites to ensure that the functionality of the functions is effectively verified."}, {"title": "2) Contextual Dependency Provision:", "content": "Previous work [13] has shown that more than 70% of the functions depend on other contextual information in the project, therefore, the inability to provide dependencies can lead to a significant decline in the generative capabilities of large language models. However, accurately providing the dependencies required for code generation is extremely difficult and does not align with real-world development scenarios. Consequently, we adopt a conservative approach to providing dependencies.\nWe first use syntax tree analysis to identify and collect all relevant contextual dependencies in the code. we extract dependencies from project files, including the names of functions, declarations, function bodies, global variables, structures, macros, as well as function comments. Next, we compile the code to obtain the LLVM IR intermediate code representation of the files. By matching keywords in the IR syntax (such as \"call\" to indicate a function invocation), we traverse the IR files to acquire the names of those dependencies. By cross-referencing the results from the first step, we can obtain the contextual information of the target function. We also provide similar but different dependencies for each contextual dependency to simulate the disturbances caused by irrelevant information in the actual development process."}, {"title": "B. Code Obfuscation", "content": "Despite selecting code from the project revision history that was modified after the training time of the large model, we also applied additional code Obfuscation techniques to the dataset to enhance protection against \"code leakage\" and ensure the applicability of our benchmarks in future releases of the large model. Fig. 4 shows the example process of Obfuscating the dataset with three strategies. To objectively evaluate the effectiveness of the LLM when dealing with obfuscated code, we constructed code generation and code completion scenarios, considering the code completion task as a subtask of the code generation task. We apply symbol obfuscation and structure obfuscation strategies in code generation scenarios and symbol obfuscation and semantic obfuscation strategies in code completion scenarios (the strategies can be used in combination).\nDistinguishing different obfuscation strategies for various task scenarios is due to the fact that only in the code completion scenario can semantically obfuscated code fragments be retained and a LLM be required to generate complete code. This is to test the LLM's true generation capability when faced with semantically obfuscated code. We do not fully integrate the three types of obfuscation together because different obfuscation strategies are suitable for different types of code. Next, we will discuss in detail the effects and practice of each obfuscation."}, {"title": "1) Symbol Obfuscation:", "content": "We first use a comprehensive symbol obfuscation strategy to not only rewrite the functional descriptions of the functions but also perform thorough identifier rewrites for all meaningful identifiers in the target code and the provided context. This means that all identifiers for functions, variables, class names, etc. in the code are obfuscated regardless of the context in which they appear in the source code. We use NLTK [60] to do the word segmentation and replacement. This strategy is designed at the token-level to change the LLM's familiarity to the target code."}, {"title": "2) Structure Obfuscation:", "content": "After symbol obfuscation, we further change the code structure automatically using a structure obfuscation strategy. In this stage, we employ the strategy to adjust and integrate the calling structure of the target function so that the execution path and organization of the function are changed. Specifically, we use LLVM [61] to unfold and integrate the functions that are called in the objective function to change the structure of the code. We extract all the called functions and their implementations within the target function based on the context, and then utilize the abstract syntax tree (AST) to handle parameter passing and automatically unfold the called functions."}, {"title": "3) Semantic Obfuscation:", "content": "In semantic obfuscation, we meticulously rewrite code snippets within functions to ensure that the new code is semantically equivalent to the original, yet the implementation logic differs. The goal of this process is to maintain the functional consistency while introducing a new implementation method, effectively obfuscating the code at the method-level. Through semantic transformation, we ensure that the code can still achieve the same functionality, but for the model, its generative logic is completely different from the original code. This semantic obfuscation provides a more challenging task to test the code understanding and generative capabilities of LLM. Since this obfuscation method relies on specific semantics of the code, the current approach involves manually rewriting the code and conducting a double-check. We highly recommend that future research should delve deeper into semantic obfuscation strategies and design templates to automate this process.\nThrough raw data collection and code obfuscation, we constructed the OBFUSEVAL, which is shown in Table III. Apart from symbol obfuscation, we did not apply structure and semantic obfuscation to all the original data. This is partly because the characteristics of the code may be suitable for specific obfuscation methods (such as nested structures), and partly due to the cost of manual inspection. The code examples from Redis are more numerous and regular, so we applied more semantic obfuscation to them. Future research could design automated code obfuscation framework for obfuscation and inspection."}, {"title": "C. Systematic Testing and Code Review", "content": "To evaluate the LLM's code generation capabilities on the OBFUSEVAL and the effectiveness of our code obfuscation methods, we designed an automated code execution and verification platform. The platform is built on Docker images, providing an isolated sandbox environment to ensure that the tested codes do not interfere with each other. The evaluation process of generated code mainly includes two parts: systematic testing and code review."}, {"title": "1) Systematic Testing:", "content": "We use the systematic testing process to evaluate the model and the dataset. We utilize compilation checks and official test suites to detect syntax and semantic errors. We construct the prompt to guide the large model in code generation scenarios and code completion scenarios. The composition of the prompt is detailed below:\nInstruction: We provide explicit instructions to guide the LLMs to generate code related to the software.\nContext: We provide detailed context information, including structs, macros, functions, global variables, etc. These contexts include both the necessary dependencies for implementing the target function and the context that is irrelevant to the target function.\nFunction description: We provide a functional description of the target function to guide LLMs in generating code. Based on this description, we provide only the declaration of the target function for code generation scenarios and partial code implementation details of the target function for code completion scenarios.\nAfter obtaining the code generated by the model, we integrate it into the software for compilation and system testing. If errors occur, we separately record compilation errors and test errors, then analyze the error information. For code that passes the tests, we will conduct a manual code review."}, {"title": "2) Code Review:", "content": "We used compilation checks and test suites to evaluate the syntactic correctness and functional correctness of the generated code, respectively. However, through manual inspection, we found that even the code was functionally correct, i.e., it was able to pass the tests, there were still some non-functional code quality issues. These problems may include deficiencies in code efficiency, code robustness, etc. Therefore, we manually analyzed these non-functional code quality issues (Section IV-B3)."}, {"title": "IV. EVALUATION", "content": "In this section, we describe our experimental setup and the evaluation results of four LLMs on our dataset. Our evaluation focuses primarily on the performance of LLMs on our dataset, and whether code obfuscation can further reveal the true capabilities of these LLMs."}, {"title": "A. Evaluation Setup", "content": "1) Model selection: We chose a general-purpose large language model, ChatGPT, and a code-focused large language model, DeepSeek. Both of them are mature and widely-used LLMs. For ChatGPT, we use the \"gpt-3.5-turbo-1106\" [62], \"gpt-4-turbo-1106\" [63], and \"gpt-4-turbo-0125\" [63] in our experiments. For DeepSeek, we use DeepSeek-coder-v2 [64] with the default settings. We use default value for LLM's parameters when generating code.\n2) Raw data selection: Note that the training datasets for gpt-4-turbo-1106 were finalized as of April 2023 (gpt-3.5-turbo-1106 is also before that time). We selected the code starting from May 2023 to Dec 2023 as the original data. Therefore, for gpt-3.5-turbo-1106 and gpt-4-turbo-1106, these codes were not included in the training set, while for gpt-4-turbo-0125 and DeepSeek-coder-v2, they might have encountered some of these code snippets during training. By doing so, we not only ensure a balanced distribution in our dataset (with both seen and unseen data), but also objectively and authentically demonstrate the generative capabilities of LLMs across different types of code.\n3) Evaluation Metrics: To assess the correctness of the generated code snippet, we employed two key performance metrics to measure the code generation capabilities of the LLMs in real-world development scenarios: Compile Pass Rate (CPR) and Test Pass Rate (TPR). We first replace the original function with the function generated by the LLMs and then compile the software. After that, we perform system tests associated with that function.\nCPR and TPR are representative metrics that can illustrate generated code that passes compilation and tests respectively. CPR demonstrates the basic ability of large language models to generate syntactically correct code, while TPR reflects the capability of large language models to understand and correctly generate complex functional code. Specifically, TPR can demonstrate whether the LLM can be used directly in production scenarios. Both CPR and TPR use pass@5 rate to eliminate fluctuations."}, {"title": "B. Results and Analysis", "content": "To evaluate the performance of large models on our dataset, we conduct experiments on four large models using OBFUSE-VAL. Specifically, we explore the effectiveness of code obfuscation and the various among different obfuscation strategies.\nWe investigate the following three research questions:\nRQ1: How effective are large language models in generating code on our datasets?\nRQ2: How does code obfuscation further reveal the capabilities of LLMs, and how effective are different obfuscation strategies?\nRQ3: What are the issues hidden in LLM-generated code?\n1) RQ1: How effective are large language models in generating code on our datasets: The model usually performs better on the code similar to training data. Thus, we collected code data after model training and utilized code obfuscation methods to further eliminate code leakage. In this research question, we conduct experiments with several widely used and proven effective large language models on our collected dataset. The overall results are presented in Table IV and Table V.\nThe CPRs are among 16.0% to 49.0% on original raw code, with an average of 32.8%, which is significantly lower than the LLM's performance on traditional datasets (e.g., humaneval). The average TPRs are 21.1% and 20.0% on original code generation and completion. After we applied different levels of code obfuscation, this result dropped to 6.9% and 15.6%. This indicates that the code generated by LLMs is difficult to pass the official tests of software and be directly used in production environments. Developers still need to manually fix and adjust the code.\n2) RQ2: How does code obfuscation further reveal the capabilities of LLMs, and how effective are different obfuscation strategies: We use three obfuscation strategies to modify the collected raw code to minimize the possibility that the code might resemble the training data. We specifically compare the TPR before and after different code obfuscations to evaluate the syntactic and functional correctness of generated code. We try them separately and also their combinations.\nOverall effect. All obfuscation strategies have reduced the TPR of code generation by LLMs. Since we did not deliberately increase the difficulty and complexity of the code in code obfuscation (discussed in Section V), code obfuscation can make the tested code less familiar to large models. Even though the decline may not be particularly significant, it represents the trend of the LLMs' capabilities, and if applied in a large-scale production environment, it could still have a considerable impact.\nAs for tested LLMs, GPT4-1106, GPT4-0125 and DeepSeek Code are obviously better than GPT3.5. However, the ranking of their capability on the Original code is inconsistent. After obfuscation, their ranking becomes more stable (GPT4-0125>DeepSeek Code>GPT4-1106>GPT3.5). This further proves that using code obfuscation can more accurately demonstrate the capabilities of the models.\nStrategy comparison. In this paper, we intuitively determine the effectiveness of code obfuscation strategies by the TPR decrease ratio comparing to the Original code tasks (eliminate the inflated capabilities of the large model). Symbol and structure obfuscation can be very effective with the average TPR decrease ratio of 24.6% and 32.1%. Semantic obfuscation is not that effective, with an average decrease ratio 15.3%. This is mainly because our current semantic obfuscation strategies are still relatively simple (using some heuristics to rewrite code manually), without making in-depth modifications to the code. We also find that the use of mixed strategies can lead to a further decrease in TPR (Symbol + Structure can have an average decrease ratio of 62.5%), demonstrating the effectiveness of mixed strategies.\nAlthough different obfuscation methods have different effectiveness, we still recommend that future researchers should try various obfuscation strategies and their combinations, which can not only ensure the fairness of code obfuscation but also explore the performance of LLMs on more types of code implementations."}, {"title": "3) RQ3: What are the issues hidden in LLM-generated code:", "content": "To enable developers to better utilize the code generated by large language models for downstream development tasks, we further analyzed common issues hidden in the generated code that may affect development tasks. Due to the large size of the OBFUSEVAL, we employed stratified sampling for manual code review. The sample size was calculated through the finite population correction, ensuring our sample accurately represents the dataset. We set the confidence level to 95% and the margin of error to 5%, which are standard statistical thresholds. Through this method, we extracted 299 pieces of LLM-generated code from five software and identified the following three categories of issues:\nSyntax errors: Codes lead to compilation errors.\nFunctional errors: Codes fail to meet the functional requirements of development (fail the official tests).\nNon-functional code quality issues: Codes potentially causing performance and reliability issues.\nDue to the target software being from diverse domains, the functionalities and the code semantics are various, so we mainly focus on syntax errors and non-functional code quality issues.\nSyntax Errors. We refer to the LLM-generated code that causes compilation errors in the software as syntax error code. Such errors significantly impact development efficiency. We extract error logs and manually analyze the code that failed to compile, ultimately categorizing the syntax errors into 5 major categories and 21 subcategories, as shown in Table VI.\nIn summary, large language models perform worst in handling function and type declarations, particularly implicit function declarations and type conflicts. And LLMs perform poorly in generating code related to structures, often resulting in issues such as accessing non-existent members. These errors may be due to the presence of similarly named functions or structures in the training data of the LLMs, leading to the use of functions or structures outside the provided context in code generation tasks. We recommend that developers focus on checking external dependencies such as called functions and structures when using LLMs to generate code. Additionally, LLMs often perform poorly when handling code involving pointers, leading to errors such as converting integers to pointers without casting, and incompatible pointer types.\nNon-functional code quality issues. For LLM-generated codes that have passed systematic testing, we conducted a manual code quality review and found that the code often had non-functional quality issues such as poor performance, and security vulnerabilities. These issues do not directly affect the normal execution of the code, but they can pose potential threats to the software. For example, poor performance may cause system response time delays, affecting user experience and overall performance; security vulnerabilities may be exploited by malicious attackers, leading to data breaches or system crashes. We have summarized common non-functional code quality issues into the following three categories:\nResource Management: In LLM-generated code, resource management code is often inadequate, especially in terms of memory or file management. For example, LLM-generated code often does not free dynamically allocated memory at the end of the program. Such code can pass compilation and testing, but it may lead to potential memory leaks, resulting in software performance issues.\nCode Efficiency: shows an example of low code efficiency. Wtiff_pack2tiff is an image data conversion function in the libvips software. In the original code, the function adopts corresponding processing methods according to different image encoding formats, while the LLM-generated code uses an inefficient loop to process each pixel, modifying and copying pixel by pixel. Although the LLM-generated code can meet functionality, its performance is far inferior to the original code, affecting the overall performance of the software.\nCode Robustness: The robustness of the generated code is not guaranteed, including missing error checking code, incomplete error handling and inadequate feedback message."}, {"title": "V. THREATS TO VALIDITY", "content": "Raw data collection. To simulate real-world development scenarios, we select mature system software from the real world as the target software for our research. We chose these software systems because they are widely used, have a rich development history, and contain mature test suites. We believe our study is representative, although some results may not apply to all kinds of software and all kinds of code language. There are other human efforts evolved in the data collection process (e.g., description rewriting). To minimize the impact of human error, we organize a team of seven senior software engineers, each with at least five years of experience in C programming. Additionally, we conduct a double-check progress in each step.\nCode complexity. Obfuscation strategies can introduce changes in code complexity, primarily in structural and semantic obfuscation. For example, function inlining in structural obfuscation may lead to an increase in the number of lines of code, potentially affecting the generation capabilities of large models. In practice, we did not intentionally increase the complexity of the code in any obfuscation method; our guiding principle was to ensure that the semantics of the code remained the same before and after obfuscation.\nTesting process. Due to the presence of flaky tests, the testing environment of real software projects can be unstable. Even correct code may fail tests due to contextual or environmental issues. Therefore, for each case, we run tests at least five times to mitigate intermediate test results. Official test suites may not comprehensively test the correctness of functionality, but this represents the best efforts, allowing for a better assessment of whether the code meets development requirements."}, {"title": "VI. CONCLUSION", "content": "Accurately assessing the code generation capabilities of LLMs is crucial for their evaluation and improvement. While existing works have constructed datasets to gauge these capabilities, three main gaps persist in objectively evaluating LLMs' real potential: the exposure of target code, case timeliness, and dependency availability. These gaps arise because the code in current datasets may have been exposed during the training phase of LLMs, and the continuous training and development of LLMs severely compromise their timeliness. To address the problem, this paper adopts the concept of code obfuscation, altering code at various levels while preserving its functionality and output. We developed a code-obfuscation-based benchmark, OBFUSEVAL, by collecting 1,354 raw cases from five real-world projects, which include function descriptions and code. We then obfuscated descriptions, code, and context dependencies using a three-level strategy (symbol, structure, and semantic). Evaluating four LLMs on OBFUSE-VAL and comparing the effectiveness of different obfuscation strategies, we found that after obfuscation, the average test pass rate can decreased by 15.3%-62.5%."}]}