{"title": "Exploring the Design Space of Cognitive Engagement Techniques with Al-Generated Code for Enhanced Learning", "authors": ["Majeed Kazemitabaar", "Oliver Huang", "Sangho Suh", "Austin Z. Henley", "Tovi Grossman"], "abstract": "Novice programmers are increasingly relying on Large Language Models (LLMs) to generate code for learning programming concepts. However, this interaction can lead to superficial engagement, giving learners an illusion of learning and hindering skill development. To address this issue, we conducted a systematic design exploration to develop seven cognitive engagement techniques aimed at promoting deeper engagement with AI-generated code. In this paper, we describe our design process, the initial seven techniques and results", "sections": [{"title": "1 INTRODUCTION", "content": "Novice programmers often encounter challenges as they learn to code. Initially, learners struggle with understanding fundamental programming concepts and syntax [8, 22, 58, 78, 93], and as they advance, they face further difficulties mastering skills such as algorithmic thinking and software design principles [43, 80, 94, 123]. Recently, Large Language Models (LLMs) are becoming increasingly integrated into novice programmers' workflow, influencing their help-seeking behaviors [50], and changing the perspectives of both students and educators [71, 121]. Novice programmers now frequently use LLMs for various tasks, such as writing and explaining code, debugging, understanding concepts, and obtain example solutions [33, 50, 121]. One prominent use of LLMs is for code generation [33] as demonstrated by large-scale self-report studies [99] and analyses of student prompts [62, 100]. Learners often cite the ability to frame questions easily and obtain faster, contextually relevant solutions as key advantages of using LLMs over traditional methods like web searches [64]. However, traditionally, learners proactively searched for and found relevant code examples to address gaps in their knowledge, meaningfully engaging themselves in a learning process [10, 15, 70, 101]. But with LLMs, learners can bypass the effortful yet beneficial process of adapting generic code examples to specific contexts [55, 112]. While this shift can boost productivity, it can lower their engagement with the learning process and hinder the development of critical programming and Computational Thinking (CT) skills [117]. Currently, there are many concerns from both learners and educators about over-reliance on AI-generated code which could lead to skill degradation [71, 120, 121]. Learners might accept generated code without fully understanding it, giving them the illusion of learning [92]. This can negatively impact their ability to write, modify, or debug code independently without AI [2, 62]."}, {"title": "2 RELATED WORK", "content": "This section discusses the challenges faced by novice programmers, established interventions that address these difficulties, and the evolving role of Large Language Models (LLMs) in programming education, including their potential impact on metacognitive skills and over-reliance on AI."}, {"title": "2.1 Challenges in Learning to Code", "content": "Despite the growing interest and accessibility, novice programmers face persistent challenges when they start learning to code [1, 4, 11, 23, 52, 57, 103]. They must simultaneously learn conceptual knowledge, syntactic knowledge, and strategic knowledge like planning, problem-solving, and debugging [93]. End-user programmers, who write code for their own use, face six learning barriers, identified by Ko et al.: uncertainty about next steps, selecting appropriate programming constructs, combining constructs, using code construct correctly, understanding code failures, and inspecting program behavior [66], with similar mistakes and misconceptions identified by other research [1, 65, 67, 76, 79]. Furthermore, as novice programmers make progress, they face difficulties in acquiring advanced skills like algorithmic thinking and software design [43, 80, 94, 123]. To address these challenges, researchers have introduced and experimented with various pedagogical strategies and software tools. A common approach in introductory programming courses is practicing with traditional coding tasks, where students write programs from scratch. While beneficial, these tasks often lead to frustration due to difficulties with syntax and conceptual understanding. Parsons problems were introduced as an alternative, requiring learners to arrange shuffled lines of code into the correct order, emphasizing logical reasoning over syntax and error correction [87]. Studies show that Parsons problems take less time to complete than fixing buggy code or writing code from scratch, without sacrificing learning outcomes [25]. Further research has investigated variations such as faded [115] and adaptive Parsons problems [24] with similar results. Worked examples are another alternative, which guide learners step-by-step through solving a problem, helping them understand how expert programmers approach coding tasks. They have been found particularly effective in reducing the time needed to reach proficiency [81], leading to the development of interactive versions [32]. Tracing exercises, where learners must follow the execution steps in a program, are frequently used to build understanding of programs execution and develop debugging skills [69, 77, 109]. Similarly, visualization tools allow learners to observe program execution and state changes, making abstract concepts like recursion and memory more tangible and accessible [102]. A notable example is Python Tutor, which displays visualizations of the program's data structures at each step of the code [40]. Other tools include Omnicode, which displays a scatter plot matrix of all runtime values for every variable in the program [59], Theseus, which annotates functions in the code editor with the number of times it was called during the current execution [73], and an extension that displays a small graph of how each variable changes over time during execution [47]. Another approach involves generating content to assist learners, such as hints [31, 46, 56, 88], examples [53, 54, 85], tutorials [44], and recommendations [27, 45, 82, 104, 113]. Enhanced error messages provide more informative feedback on syntax and runtime errors, though results on their effectiveness have been mixed [3, 18, 29, 84, 89, 91]. However, as AI tools become increasingly accessible and learners rely on them for generating code solutions, they risk missing out on practicing and mastering essential higher-order cognitive skills like computational thinking [117]. To address this, our work builds on established interventions introduced through programming education research to re-engage learners with AI-generated"}, {"title": "2.2 The Changing Landscape with Generative AI", "content": "The rise of generative AI, especially Large Language Models (LLMs) is transforming programming education. These models can generate code from natural language, explain code, correct errors in code, and offer code completions (e.g., Github Copilot [34]). They are now widely accessible through tools like ChatGPT and Claude Al and can serve as personalized coding assistants and tutors [42]. Recent research demonstrates that OpenAI GPT-3 outperformed students in multiple computer science exams [28] and GPT-4 passing exercises of three Python programming courses without human involvement [98]. Even multi-modal AI models can solve visual tasks like Parsons problems with high accuracy [49]. Although Al-powered tools can arguably offer exciting promises including improved engagement and personalized feedback [60], they also present challenges that limit learning opportunities. One major concern is over-reliance on AI-generated solutions [90, 92]. For novice programmers, tools like Copilot or ChatGPT provide quick answers to coding problems, which limits the opportunity for learners to critically engage with the problem and develop solution strategies [5]. Researchers are already providing empirical evidence on the effect of access to Al on learning outcomes and over-reliance. Kazemitabaar et al. conducted a controlled experiment with 69 high-school students with no prior Python experience, comparing learning outcomes between two groups: one with access to AI and one without, over seven sessions, followed by two evaluation sessions without AI. Their results showed that the AI group experienced less frustration, and improved learning outcomes for students with stronger prior conceptual skills. But a follow-up analysis revealed various types of over-reliance on AI. Particularly, when students relied on AI to solve tasks autonomously, their subsequent coding skills without AI were consistently diminished [61]. One approach to address over-reliance has been to place guardrails around AI so that it would avoid generating direct code solutions. Bastani et al. compared three groups of students, one with AI, one without, and one with an AI that safeguards learning (using prompt engineering) in a high school math class with nearly a thousand students. Their results demonstrate that while access to the Al boosts performance, students who later lose access to it perform worse than those who never had access to AI-except for those who used Al with guardrails [2]. Therefore, researchers and educators have called out for the development of pedagogical AI tools [71] which has led to the development of coding assistants like CodeAid [64] and CodeHelp [74] that avoid displaying direct code solutions. Although these solutions might help, they are common in limiting and discouraging the use of AI-generated code. However, generative Al tools are here to stay and many acknowledge that we have crossed a point of no return with Al integration in programming education, and therefore, we should embrace this new paradigm [71]. In our work, we take an alternative approach and instead promote learners in deep, cognitive engagement with Al-generated code."}, {"title": "2.3 Cognitive Engagement with AI", "content": "Over-reliance on AI is a challenge that is being observed in many domains and workflows beyond learning to code. For example novice designers often take on AI suggestions without putting the effort to explore the alternatives [21, 118]. This over-reliance stems from cognitive biases and tendencies. One prominent example is automation bias, which describes the human tendency to favor suggestions from automated systems, often ignoring contradictory information from non-automated sources, even when the latter is correct. Extensive research, such as studies involving doctors using clinical decision support systems, has demonstrated the pervasiveness of this bias [35]. Another contributing factor is cognitive offloading, which suggests that people rely on external aids-in this case, AI-in order to reduce their cognitive load [95]. Together, these concepts-automation bias and cognitive offloading-form the motivation of our work, highlight the challenges in integrating Al into human workflows and the need for interventions that strike the right balance between engagement and cognitive load. While this challenge has existed since the development of automated systems, the explosive adoption of Al in everyday life and work has amplified it into a widespread problem, affecting millions of people-not just a few experts using decision support systems. Thus, a growing number of researchers are recognizing the need for interventions that help users engage more deeply with AI. For example, Tankelevitch et al. advocated that all generative Al-powered systems should provide metacognitive support strategies, such as explainability and customizability, and suggested mechanisms for eliciting user reflections on both their own decisions and those of AI [106]. Similarly, Gajos et al. conducted three experiments with nutrition-related decisions and found that providing Al explanations without direct recommendations led to users learning more, as users had to derive their own conclusions from the explanations rather than relying on an 'answer' from the AI [13, 30]. Buccinca et al. tested three cognitive forcing functions-interventions that require users to explain why they accept or reject Al recommendations-and found that such interventions can help reduce over-reliance on AI. However, they also observed that people rated these designs less favorably compared to those where they might over-rely on AI, understanding again that it is important to carefully design these interventions [12]. In programming education, Kazemitabaar et al. studied the issue of learners passively accepting AI-generated code by developing CodeAid [64], which generates pseudo-code, or highlights incorrect lines with suggested fixes, rather than directly displaying code solutions. Similarly, Hou et al. developed CodeTailor [51], which responds to learners' help requests by presenting code solutions as Parsons problems for them to solve, instead of directly providing the fixed solution. Their evaluation study showed how this approach improved learning outcomes compared to a baseline where generated code was directly provided. Our work builds on these insights in the context of AI-assisted programming and explore new interventions that can help learners"}, {"title": "3 COGNITIVE ENGAGEMENT TECHNIQUES WITH AI-GENERATED CODE", "content": "To properly explore the design space of cognitive engagement techniques, we developed a set of initial prototypes, each with distinct types of user involvement based on the capabilities of LLMs and prior literature in supporting novices in learning to code. We then iteratively refined each technique through design probe sessions with five CS educators. Additionally, to guide our design, we constructed a design space based on three key dimensions relevant to our context. First, Bloom's taxonomy which classifies learning objectives by cognitive complexity [7]: Remember, Understand, Apply, Analyze, Evaluate, and Create. Specifically, we use definitions adapted for programming education [108]. Second, the ICAP framework [16], which links active learning outcomes to four categories of engagement level: Interactive, Constructive, Active, and Passive. And third, engagement timing: whether to require engagement with the AI-generated code before (Engage&Reveal) or after revealing the code solution (Reveal&Engage). We mapped each technique within the design space according to the above three dimensions, illustrated in Figure 2. We acknowledge that some degree of subjective interpretation is inherent in this process. Our design space is not meant to imply that these techniques can fit neatly into distinct areas. Rather, it was to guide us in making intentional design decisions. Below, we describe each technique, including the prior literature that inspired it and its design."}, {"title": "3.1 T1. BASELINE (Minimal | Passive | Reveal & Engage)", "content": "In the BASELINE technique, after the user provides the input prompt, the AI generates code and a detailed explanation using an initial prompt, and it is displayed to the user, similar to tools like ChatGPT. The level of engagement is completely optional and up to users. This technique induces little engagement, therefore classified as passive engagement."}, {"title": "3.2 T2. GUIDED-WRITE-OVER ( Remember | Active | Engage & Reveal)", "content": "For this technique, we drew inspiration from prior research in computing education, which found that requiring learners to type over worked examples improves learning [32]. This technique increases learning opportunities by forcing deliberate practice [26], preventing passive copying of code without engaging one's attention to understand it. In our technique, ghost text for each line of line of the generated code is displayed, similar to Github Copilot [34], but it requires users to type over it. A novel aspect of our technique is that, as users type, explanations for each expression (generated by a separate LLM prompt) are shown below the expression. Additionally, a high-level description of how the line contributes to the overall solution is displayed to the right of the line. Our technique also highlights incorrectly typed characters in yellow, prompting users to retype them. After two incorrect attempts, the system moves to the next character while marking the error in red. After the user successfully types over each line, they can proceed to use the code in their editor. This technique aligns with the Remember level of Bloom's taxonomy. It prompts users to actively retrieve previously learned concepts and recall relevant programming constructs, reinforcing memory retention and prevents shallow engagement."}, {"title": "3.3 T3. SOLVE-CODE-PUZZLE (Analyze | Constructive | Engage & Reveal)", "content": "This technique adapts Parsons problem, which has been widely used in computing education for their demonstrated effectiveness. It requires learners to rearrange mixed code blocks to form a correct program [19, 87]. Empirical research has shown that various forms of these problems, including faded [115], adaptive [24], and personalized [51], can reduce cognitive load while maintaining learning performance [25]. In this technique, each line of the AI-generated code is turned into a draggable block and is mixed inside a drag-and-drop pane, and the indentation is removed in the left pane. The user has to then drag each code block from the left, and put it in the correct spot on the right, as well as its correct level of indentation (horizontal spot). After using all code blocks, they can hit the check button that will only tell the user whether their solution is correct or not, and if all blocks are correctly placed they can proceed to use the code in their editor. There is also a hint button that will highlight all blocks that are placed incorrectly, and provide arrows to which"}, {"title": "3.4 T4. VERIFY-AND-REVIEW (Evaluate | Constructive | Engage & Reveal)", "content": "This technique draws inspiration from the use of debugging-a fundamental CT skill [117]\u2013to help learners grasp programming concepts [36, 72, 122]. Research beyond programming shows that incorporating incorrect solutions in worked examples can enhance learning transfer for advanced learners, but may overwhelm novices unless additional support, such as error highlighting, is provided [37]. We applied these design guidelines to inform the design of the technique. In this technique, the AI-generated code undergoes an additional LLM prompt that deliberately injects 3-4 errors, depending on the code's length and complexity. The user is then presented with this editable, incorrect code in a code editor, where they can run and debug it. The user is asked to identify and fix the injected errors. Furthermore, to assist users who might struggle with identifying the problems, clicking the check button highlights any remaining lines of code that needs to be fixed (using an LLM prompt). If they need help with how to fix the incorrect part, clicking on the hint button offers progressively more direct guidance, starting with hints about potential problems and eventually showing the correct code for each line. The user can use the editor to fix the problematic code and then check it with the check button. Once the code is error-free, the user can proceed to using the code in the editor. This technique engages users at the Evaluate level by requiring them to actively check whether the code meets requirements or produces the correct output. Moreover, by fixing errors, they are required to critique the logic and quality of the code. They must determine which portions of the code are incorrect and fix those, while keeping the correct parts of the code."}, {"title": "3.5 T5. INTERACTIVE-PSEUDO-CODE (Understand | Constructive | Engage & Reveal)", "content": "We were inspired by the use of pseudocode in computing education to support algorithmic thinking before introducing complex syntax [75, 96, 107], as well as by CodeAid, an LLM-powered assistant that encourages active learning by generating pseudocode rather than providing direct code solutions [64]. This technique uses an additional LLM prompt to generate a subgoal-labeled, hierarchical pseudocode, displayed fully to the user without showing the code solution. The pseudocode is organized into subgoals, grouping several lines of code with a descriptive title. Users are provided with an empty code editor beside the pseudocode to write the corresponding code. Two buttons-check and review-use an LLM prompt to help guide the user by checking their code and offering feedback if errors are found. Initially, only subgoal titles are shown, with an expand button to reveal the"}, {"title": "3.6 T6. EXPLAIN-BEFORE-USAGE ( Understand | Interactive | Reveal & Engage)", "content": "This technique is motivated by prior work on self-explanation, where learners were encouraged to explain the code. Researchers found that this can be a successful intervention, increasing engagement with worked examples [110], and that the ability to explain code has a significant relationship with the ability to write code [77, 83, 109]. In this technique, after the AI generates the code, it is immediately displayed to the user. Immediately afterwards, an important part of the code (determined by a second LLM prompt) is then highlighted, followed by a short-answer question for the user to answer. The user's response is then evaluated via another LLM prompt with feedback and a score from 0 to 5, based on accuracy and completeness. The user has up to 3 attempts for each question, with the correct answer automatically shown after the third try. A total of 3-5 questions are asked from various parts of the generated code, depending on the code's complexity, after which the user can proceed to using the code in the editor. This technique also reaches the Understand level as it engages users to interpret, explain, and infer meaning from existing code, without moving towards higher cognitive processes. However, it does this interactively, as users interact with Al to answer and receive feedback on their explanations."}, {"title": "3.7 T7. LEAD-AND-REVEAL (Create | Interactive | Engage & Reveal)", "content": "This technique draws inspiration from CS Unplugged activities [6], which teach core concepts without any coding, and have been shown to improve computational thinking skills [9]. We integrate this with scaffolded self-explanation using Socratic questioning, which has proven to enhance coding comprehension [105]. After the code is generated, it is not displayed to the user. Instead, a second prompt is used to generate Socratic multiple-choice questions which are then displayed on one side of the interface. These questions scaffold the step-by-step process of solving the task and prompting the user in what needs to be done at each step of the algorithm/solution in a structured format. After the user picks the correct answer (where they have up to three retries), its corresponding line of code (which is the next line) will be displayed on the right with an explanation about how their answer and that line contributes to the overall solution. As each question is answered, its corresponding line of code will be progressively revealed on the right of the interface. This process is continued until all lines of the generated code is revealed and then the user can proceed to use the code in their editor."}, {"title": "3.8 T8. TRACE-AND-PREDICT (Apply | Constructive | Reveal & Engage)", "content": "This technique is inspired by research showing that visualizing the notional machine is an effective method for learning programming concepts [41, 102]. Particularly, several studies have found a positive correlation between learners' performance on code tracing tasks and their code writing tasks [69, 77, 109]. Based on these findings, we aimed to strengthen learners' understanding by engaging them in code tracing tasks with integrated questions to sustain their engagement. In this technique, the AI-generated code is displayed to the user with a sample input for execution. The user is then required to trace the code in a debugger-like system that includes buttons to step forward or backward through the code execution, and real-time variable values value of variables as they change. At certain complex parts of the code (identified by an LLM prompt), the technique does not allow the user to proceed. It requires the user to predict the value of a variable after that line of code is executed. The user can retry each variable prediction question three times and on the third try, the answer is displayed. This process is continued until several important parts of the code are covered with value prediction questions. After the user finishes tracing the entire code, and answering all the prediction questions along the way, they can then proceed to using the code in their editor. Lastly, this technique reaches the Apply level as it requires users to examine the control flow, understand variable states, and identify how code executes line by line. Users have to actively solve questions about unfamiliar code, applying the rules of execution in a guided environment."}, {"title": "4 STUDY 1: COMPARING ALL TECHNIQUES", "content": "We conducted a between-subjects study with 82 participants to evaluate each technique against the BASELINE technique without forced cognitive engagement. Our goal was to evaluate:\n\u2022 RQ1 [Performance]: How effective are the cognitive engagement techniques in supporting participants' ability to transfer learned programming concepts to isomorphic coding tasks without AI assistance?\n\u2022 RQ2 [Friction]: How do the techniques impact perceived friction, measured through time on task and cognitive load?\n\u2022 RQ3 [Perceptions]: What are participants' perceptions about technique and the type of user involvement that they require?"}, {"title": "4.1 Experiment Tool Design", "content": "To evaluate the effectiveness of our techniques, we built a web-based application that allows users to log in and complete AI-assisted or manual coding tasks. This also enabled specific tasks to be assigned to each user by the experimenter. The tool was designed to provide a self-paced and consistent experiment for all participants. In AI-assisted tasks, users interact with a chatbot to generate code. To avoid generating incorrect or unrelated code, the chatbot compared the user's prompt with the current task and provided feedback if details were missing. Additionally, to ensure consistent outputs across participants, pre-generated responses for each task and technique were used. In the BASELINE technique, code is immediately shown with an accompanying explanation. For other techniques, a modal that covers the entire screen displays the technique after a 5-second delay to simulate generation time. Once users finish interacting with the technique, the modal disappears and users can see the code and its explanation. The tool is developed using TypeScript with a client-server architecture. The server, implemented in Node.js, stores user data and logs using a MongoDB database, and interacts with OpenAI APIs (GPT-4). It parses LLM outputs and communicates with a Python shell and language server for running Python code and providing real-time autocomplete and error-detection for the client-side code editor. The Monaco code editor is used for code editing, execution, and submission. The user interfaces can be seen in 1. Full details, including prompts and source code, are available on GitHub: https://github.com/MajeedKazemi/code-engagement-techniques."}, {"title": "4.2 Methodology", "content": "The study design posed two primary challenges: (1) ensuring participants had similar backgrounds to minimize variance in prior knowledge, and (2) designing programming tasks that would uniformly challenge participants, requiring assistance and enabling both learning and the transfer of learned concepts to isomorphic tasks. The following sections outline the methodology used to address these challenges. An overview of the study procedure is displayed in Figure 3."}, {"title": "4.2.1 Participants.", "content": "We recruited 82 participants (44 male, 37 female, 1 non-binary), ages 18-23 (M = 19), from an undergraduate programming course about data structures at a large public uni-versity. To ensure a consistent level of knowledge, all participants were recruited from the same class. Participants reported their frequency of using AI tools like ChatGPT for programming as daily (n=14), weekly (n=33), monthly (n=8), rarely (n=22), and never (n=5). Additionally, they reported using ChatGPT primarily for fixing code (n=59), explaining concepts (n=54), providing code snippet explanations (n=47), and generating code from descriptions (n=32). The study was approved at our institutes ethics review board, and informed consent was obtained from all participants prior to the study. Each participant received $25 as compensation."}, {"title": "4.2.2 Study Procedure.", "content": "As shown in Fig. 3, our study procedure consisted of three phases: pre-test, training, and evaluation. We describe each phase in detail below.\n\u2022 Phase 1. [Pre-Test] 5 Code-Tracing Questions. All participants initially attended an online 30-minute session over MS Teams to perform a pre-test, including five multiple-choice code tracing questions about the stack and queue data structures. The scores were then used to create eight balanced groups, each being randomly assigned to one of the seven experimental or the BASELINE techniques. The mean pre-test scores ranged from 43.5% to 49.0%,"}, {"title": "Phase 2. [Training] 4 Tasks with Assigned Cognitive Engagement Technique.", "content": "Participants then attended an in-person 2-hour session (including a 10-minute break) for the main study in which they used the web-based experiment tool. Participants logged in with their given credentials, where they were automatically assigned to the technique determined in advance. They then were instructed about how to use their assigned technique, followed by a warm-up task using the technique, and four training tasks with their technique on slightly complex tasks: two using stacks, and two using queues. The tasks were designed to be slightly outside of participants' zone of proximal development [111]. We consulted with the instructor of the course from which participants were recruited to ensure that the tasks were useful for their learning, were sufficiently challenging to solve independently, and could potentially be learned through sufficient, deep engagement with the code solution. Each task in our experiment tool first displayed the task description, several test-cases, and a multiple-choice question that asked participants how well they understand the task as a way to prompt them and to make sure they understood the task before starting to work on it. While working on the AI-assisted tasks, participants were given the option to either copy the task description as provided or rephrase it in their own words. Additionally, since engaging with the AI-generated code beyond the requirements of each technique was optional, participants were advised to move on to the next task once they felt confident in their understanding, without exceeding 15 minutes per task."}, {"title": "Phase 3. [Evaluation] Survey + 2 Coding Tasks without AI Assistance.", "content": "After completing the four training tasks, participants were asked to first fill out a survey, followed by working on two manual coding tasks without Al assistance. The survey included questions about perceived learning after using each technique, their perceived task load evaluated using the NASA Task Load Index (TLX) [68], and several Likert-questions about their willingness to use the techniques, and open-ended questions about what they liked and disliked about the process of using the technique that they were assigned to. Participants then proceeded to work on two evaluation tasks, one being isomorphic to the second task about stacks and the other being isomorphic to the fourth training task on queues, with minimal changes in task requirements. No starter code was provided for the evaluation tasks. A timer was displayed on top of these tasks and participants were instructed to skip the task if they were not able to finish the task within 20 minutes."}, {"title": "4.2.3 Data Analysis.", "content": "Learners' responses to the manual coding tasks were graded using two detailed rubrics, each tailored to one of the two tasks. These rubrics focused on key concepts and the use of data structures and how they were used in the training tasks. After the first author tested and refined the rubric on 25% of the responses, it was applied to all 82 submitted codes for each task by the first author. For each of the training tasks we also collected the time they spent on each task, which could be an indicator of how much friction the technique caused. For statistical analysis, we used a generalized linear model (GLM) to examine the effect of the intervention techniques on learners' evaluation scores while controlling for pre-test scores. One-way ANOVA is used for normally distributed data with equal variances, while the Kruskal-Wallis H test is the non-parametric alternative for non-normal data. For pairwise comparisons, an independent t-test is applied for normally distributed data, and the Mann-Whitney U test for non-normal data. In this study, we focused on pairwise comparisons between each of the seven experimental techniques and the BASELINE technique, yielding a total of seven comparisons. To control for the risk of Type I error due to multiple-comparisons, we applied a Bonferroni correction, setting the threshold for statistical significance at \\(a = \\frac{0.05}{7}= 0.007\\)."}, {"title": "4.3 RQ1 [Performance] How effective are the cognitive engagement techniques?", "content": "Regarding the effect of the techniques on learners' ability to write code manually without AI assistance, the generalized linear model explained 17.7% of the variance in post-test manual coding scores (Pseudo R2 = 0.177). Pre-test scores were a significant predictor of post-test performance (b = 0.34, SE = 0.12, z = 2.78, p = .005), indicating that higher pre-test scores were associated with higher post-test scores. Compared to the BASELINE technique (M = 31.5, SD = 28.2) which involved no forced engagement and demonstrated nearly the lowest mean performance, the LEAD-AND-REVEAL technique (M = 58.3, SD = 39.8) demonstrated the greatest improvement (b = 26.8, p = .058), though this did not reach statistical significance. The second most effective technique was SOLVE-CODE-PUZZLE (M = 54.1, SD = 38.2; b = 22.8, p = .108), followed closely by TRACE-AND-PREDICT (M = 54.1, SD = 37.9; b = 21.8, p = .123). A post-hoc power analysis (n\u00b2 = 0.112) showed the study was underpowered with 82 participants. To reach 90% power, a sample size of 153 would be required, suggesting the non-significant findings may be due to insufficient power. Future studies should aim for a larger sample."}, {"title": "4.4 RQ2 [Friction] How do the techniques impact perceived friction?", "content": "To compare participants' perceived friction across techniques, we analyzed both task completion time and NASA Task Load Index (TLX) scores. A Kruskal-Wallis H test was used to examine differences in the six TLX dimensions across the eight techniques. Significant differences were found in frustration (\\(x^{2}(2, N = 82) = 19.5\\), p = .006), physical demand (\\(x^{2}(2, N = 82) = 16.0\\), p = .025), and effort (\\(x^{2} (2, N = 82) = 14.1\\), p = .05). A Mann-Whitney U test showed that VERIFY-AND-REVIEW (M = 72.7, SD = 19.6; p = .035) had significantly higher frustration compared to the BASELINE (M = 46.7, SD = 27.9). Higher physical demand was reported for INTERACTIVE-PSEUDO-CODE (M = 60.0, SD = 19.9; p = .005) and GUIDED-WRITE-OVER (M = 65.7, SD = 28.7; p = .015) compared to the BASELINE (M = 36.4, SD = 14.8). Task completion time was analyzed using a one-way ANOVA, which revealed significant differences (F(7,74) = 3.64, p = .002). The BASELINE had the shortest time (M = 596s, SD = 268s), while SOLVE-CODE-PUZZLE (M = 1055s, SD = 272s; t(19) = 3.89, p < .001, d = 1.7) and GUIDED-WRITE-OVER (M = 1033s, SD = 280s; t(19) = 3.64, p = .001, d = 1.6) took the longest. LEAD-AND-REVEAL (M = 914s, SD = 347s) was closest to the BASELINE, with the smallest effect size difference (t(19) = 2.36, p = .029, d = 1.0). Lastly, the number of times that participants tested and executed the AI-generated code after they finished the technique was statistically significant, indicated by a Kruskal-Wallis H test (\\(x^{2} (2, N = 82) = 23.16\\), p = .001). The BASELINE technique had the most number of runs (M = 3.9, SD = 4.7), while the INTERACTIVE-PSEUDO-CODE (M = 0.3, SD = 0.6), followed by TRACE-AND-PREDICT (M = 0.6, SD = 0.7), and VERIFY-AND-REVIEW (M = 1."}]}