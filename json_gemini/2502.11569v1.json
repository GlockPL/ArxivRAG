{"title": "Towards Reasoning Ability of Small Language Models", "authors": ["Gaurav Srivastava", "Shuxiang Cao", "Xuan Wang"], "abstract": "Reasoning has long been viewed as an emergent property of large language models (LLMs), appearing at or above a certain scale (~100B parameters). However, recent studies challenge this assumption, showing that small language models (SLMs) can also achieve competitive reasoning performance. SLMs are increasingly favored for their efficiency and deployability. However, there is a lack of systematic study on the reasoning abilities of diverse SLMs, including those trained from scratch or derived from LLMs through quantization, pruning, and distillation. This raises a critical question: Can SLMs achieve reasoning abilities comparable to LLMs? In this work, we systematically survey, benchmark, and analyze 72 SLMs from six model families across 14 reasoning benchmarks. For reliable evaluation, we examine four evaluation methods and compare four LLM judges against human evaluations on 800 data points. We repeat all experiments three times to ensure a robust performance assessment. Additionally, we analyze the impact of different prompting strategies in small models. Beyond accuracy, we also evaluate model robustness under adversarial conditions and intermediate reasoning steps. Our findings challenge the assumption that scaling is the only way to achieve strong reasoning. Instead, we foresee a future where SLMs with strong reasoning capabilities can be developed through structured training or post-training compression. They can serve as efficient alternatives to LLMs for reasoning-intensive tasks.", "sections": [{"title": "Introduction", "content": "For a long time, reasoning in language models was considered an emergent property of large language models (LLMs), appearing at or above a certain scale (~100B parameters). Early studies (Wei et al., 2022b; Chowdhery et al., 2023; Brown et al., 2020) suggested that multi-step reasoning only emerges in models exceeding 100B parameters, as shown by models like GPT-4 (OpenAI et al., 2024) and Gemini (Team et al., 2024). However, recent findings challenge this assumption. Phi-3.5-mini (Abdin et al., 2024), with just 3.8B parameters, performs comparably to GPT-3.5, which suggests that reasoning ability can be achieved in small language models (SLMs) as well.\nA more recent breakthrough, DeepSeek-R1 (DeepSeek-AI et al., 2025), has shown impressive reasoning ability. While DeepSeek-R1 is a large model (671B), its reasoning abilities were distilled into smaller models (1.5B-70B parameters, Qwen Family (Qwen et al., 2025)). This further challenges the assumption that reasoning ability only comes from scaling and raises an important question: Can SLMs also develop strong reasoning capabilities? Before answering this, we need first to define what qualifies as an SLM. The definition of SLM varies widely, depending on model size, efficiency, and deployment constraints.\nIn this work, we define SLMs as models significantly smaller than state-of-the-art LLMs, typically ranging from a few hundred million to tens of billions of parameters, or models that achieve similar computational efficiency through compression (e.g., quantization, pruning).\nThere has been growing interest in SLMs due to their lower inference costs, reduced latency, and local deployment feasibility. Unlike LLMs that rely on cloud APIs for deployment, SLMs can be deployed locally (Wang et al., 2024), mitigating data exposure risks. However, their reasoning capabilities remain underexplored, particularly in compressed (Zhu et al., 2024b) variants. For example, can a quantized LLaMA-70B outperform an 8B variant? This raises another question: Can SLMs retain reasoning ability after undergoing compression (e.g., Quantization)? And to what extent? Prior research has lacked a detailed benchmarking effort that quantifies how different SLM strategies impact reasoning. In this work, we aim to fill this gap by systematically benchmarking SLMs' reasoning ability and providing clear guidance for researchers developing or deploying SLMs.\nFirst, we establish a reliable evaluation metric for assessing reasoning performance. Since reasoning is a generative task, defining an objective evaluation metric is non-trivial. Different methods often produce conflicting results compared to human evaluation, which makes it difficult to assess the model's actual reasoning ability. Manual evaluation is impractical, whereas rule-based evaluation expects the model to follow specific instructions (Huang et al., 2024). Sometimes, it can be unfair since we are testing the model's \"reasoning,\" not \"instructions following\" ability. Studies (Wei et al., 2022a) further show that this instruction following ability appears when scaled to ~100B parameters. To determine the best evaluation framework, we systematically compare different parsing-based methods, LLM-as-a-Judge, and widely used benchmarks like Im-eval-harness to our human evaluation. Our results show that GPT-4-Turbo and GPT-4o align most closely with human judgment (98% agreement), which we use as the main evaluation metric to benchmark SLM reasoning.\nSecond, we conduct a comprehensive evaluation of 72 SLMs of six different families (such as Llama and Qwen), including their quantized, pruned, and distilled variants. We evaluate across eight widely used reasoning benchmarks: GSM8K, MATH, MathQA ARC-C, ARC-E, CommonsenseQA, OpenBookQA, Hellaswag and six sorting tasks: 8, 16, 32 numbers with only positive and mixed randomly generated numbers to ensure that performance reflects the model's actual reasoning ability rather than memorization. We observed that all models do not respond similarly to different prompting strategies. Recent findings (Plaat et al., 2024; Qwen et al., 2025; Yang et al., 2024) suggest that some language models internally generate step-by-step reasoning (Wei et al., 2022c), even when prompted directly. So, on GSM8K, we tested SLMs prompt sensitivity using 5 different prompting strategies: Direct I/O, COT, 5-Shot, COT 5-Shot, and 8-Shot. We conduct all experiments three times and report the standard deviation to ensure a robust evaluation of the model's performance.\nFinally, we test the robustness of SLM reasoning on three specialized benchmarks: MR-Ben, which evaluates the ability to locate and analyze potential errors in reasoning steps (Zeng et al., 2024b); MR-GSM8K, which evaluates intermediate reasoning ability (Zeng et al., 2024a); and GSM-Plus, which measures resilience to adversarial perturbations (Li et al., 2024). To evaluate actual reasoning and not memorization, we select these datasets, released after the models' knowledge cut-off time, to ensure no prior exposure. Our results indicate that certain open-sourced SLMs like Qwen2.5-32B rival proprietary LLMs like GPT-4-Turbo in intermediate reasoning. This suggests that reasoning is not solely a function of scale but also structured training and optimization.\nThe remaining sections of this paper are structured as follows: Section 2 reviews the recent work on SLMs' reasoning and evaluation methodologies. Section 3 discusses our benchmarking setup, evaluation process, and reasoning tasks. Section 4 presents experimental results and insights, analyzing reasoning performance and its robustness. Finally, Section 5 concludes with key takeaways and directions for future research."}, {"title": "Related Work", "content": "Recent Surveys on SLMs Recent surveys provide insights into SLM advancements. Some focus on reasoning and task-specific improvements (Subramanian et al., 2025; Wang et al., 2024), while others survey SLM performance across various applications (Lu et al., 2024; Nguyen et al., 2024). These efforts highlight the increasing viability of SLMs as efficient alternatives to LLMs, particularly in resource-constrained settings. However, existing surveys lack a systematic benchmarking of diverse SLMs to quantify their performance across multiple reasoning benchmarks.\nSLM Reasoning Recent studies have explored the reasoning abilities of SLMs, such as Hymba-1.5B (Dong et al., 2025) and Llama-3-1B (Fedorov et al., 2024), particularly for mathematical and logical tasks. Some approaches train SLMs directly on reasoning tasks, such as rStar-Math (Guan et al., 2025), which uses Monte Carlo Tree Search (MCTS) and a process preference model. Specialization through fine-tuning on specific datasets also enhances reasoning (Fu et al., 2023) but may reduce generalization.\nAnother line of research uses knowledge distillation (Gou et al., 2021; Phuong and Lampert, 2019) to transfer reasoning capabilities from LLMs to SLMs (Zhu et al., 2024a). Similarly, distillation strategies, like feedback-driven (Zhu et al., 2024c) and counterfactual distillation (Feng et al., 2024), refine reasoning abilities and improve generalization to out-of-distribution tasks. Instruction-tuning-CoT (Ranaldi and Freitas, 2024) and fine-tuning on CoT-generated outputs (Magister et al., 2023) have also shown improvements in multi-step reasoning.\nFurthermore, structural modifications, such as equation-only formats (Kim et al., 2024) and synthetic data training (e.g., Orca-Math (Mitra et al., 2024)), have also improved performance. Efficient architectures like Phi-3-mini (Abdin et al., 2024) match the performance of larger models while being deployable on edge devices. Self-correction mechanisms like SCORE (Zhang et al., 2024) enhance reasoning reliability, while models like Orca 2 (Mitra et al., 2023) and OpenELM (Mehta et al., 2024) optimize efficiency through improved training strategies. In this paper, we evaluate a broad spectrum of SLMs, including trained-from-scratch, via different methods and their quantized, pruned, and distilled variants.\nReasoning Evaluation Assessing reasoning in language models is challenging due to the complexity of evaluating open-ended, multi-step responses. Various evaluation methods have been explored, including rule-based parsing, human evaluation, and LLM-as-a-Judge frameworks. Parsing-based methods provide precise accuracy but struggle with models that generate responses in unpredictable formats, often penalizing correct answers due to formatting inconsistencies. Human evaluation remains the gold standard but is expensive, time-consuming, and prone to subjectivity.\nRecently, LLM-as-a-Judge has gained popularity as an alternative, with studies showing that models like GPT-4 Turbo and Llama-3.1 70B align closely with human judgments, validating their effectiveness in evaluation tasks (Thakur et al., 2024). LLM-based assessments have been particularly effective for structured tasks, where models like InstructGPT and ChatGPT produce results comparable to expert human raters (Chiang and Lee, 2023). Similar trends are observed in summarization and grammatical error correction, where GPT-4 demonstrates strong agreement with human rankings (Sottana et al., 2023). Beyond accuracy, studies on NLG evaluation (Wang et al., 2023) highlight ChatGPT's strong correlation with human assessments in creative text generation. Recent surveys (Gu et al., 2025; Chang et al., 2024) further validate LLM-as-a-Judge as a reliable benchmarking tool.\nHowever, these evaluation methods for assessing reasoning in SLMs have not been systematically compared. It remains unclear which approach best reflects true reasoning capabilities across different models. Our work addresses this gap by assessing multiple evaluation methods, comparing LLM-as-a-Judge against human evaluation, rule-based parsing, and widely used open-source frameworks like Im-eval-harness. Then, we systematically benchmark SLMs across multiple reasoning tasks using a metric that closely aligns with human evaluation."}, {"title": "Benchmarking Experiment Setup", "content": "Unless stated otherwise, each experiment was repeated three times, and we reported the mean and standard deviation of model performance across all datasets to ensure the reliability of the results. Appendix C details all parsing scripts and prompt templates, including those used for different prompting strategies and GPT-based evaluations."}, {"title": "Evaluation Process", "content": "Our first step was to select a reliable assessment method. Instead of using standard parsing techniques to compare model responses with ground truth, we opted for LLM-as-a-Judge, using GPT-4 as the primary evaluator for most tasks.\nParsing Issues Standard parsing techniques rely on fixed patterns, which can be challenging for generative models to follow consistently. We observed that smaller models, in particular, struggle to follow strict output formats. This leads to cases where a model provides a correct answer but is penalized for deviating from the expected structure. Prior work (Wei et al., 2022c) also shows that instruction-following capabilities improve with model scale (~100B), making parsing an unfair metric for smaller models.\nTo establish a more reliable evaluation metric, we conducted three rounds of human evaluation on 100 randomly sampled data points from the GSM8K, ARC-E, ARC-C, and CommonsenseQA datasets."}, {"title": "Reasoning Tasks", "content": "Task 1 - Math Reasoning We evaluated mathematical reasoning using GSM8K (Cobbe et al., 2021), an arithmetic and word problems benchmark. We also evaluate on MathQA (Amini et al., 2019) and MATH (Hendrycks et al., 2021) dataset using Im-eval-harness (Results in Appendix \u0412).\nModels were tested under five prompting strategies: Direct I/O, Chain-of-Thought (CoT), 5-shot, 5-shot CoT, and 8-shot.\nTask 2 - Science Reasoning We used ARC-Easy and ARC-Challenge (Clark et al., 2018) for science reasoning, which includes multiple-choice questions requiring logical deduction. Unlike GSM8K, where CoT and multi-shot prompting are effective, these tasks rely more on factual knowledge retrieval. Therefore, we used direct I/O prompting for consistency in science reasoning.\nTask 3 - Commonsense Reasoning We assessed commonsense reasoning using CommonsenseQA (Talmor et al., 2019), which tests everyday knowledge and inference. Similar to science reasoning, we use direct I/O prompting for consistency. We also evaluate on OpenBookQA (Mihaylov et al., 2018a) and Hellaswag (Zellers et al., 2019) dataset using Im-eval-harness (Results in Appendix B)\nTask 4 - Sorting Numbers We designed a custom dataset (randomly generated) to evaluate logical reasoning in structured numerical tasks. The task was divided into two categories: sorting positive integers and sorting mixed integers (positive and negative). We use positive numbers in the range [1, 100] and mixed numbers in the range [-100, 100], testing lists of length 8, 16, and 32. Ground truth labels were generated using merge sort algorithm. This task measures the models' logical reasoning abilities and capability to handle sequential numerical data. Unlike datasets such as GSM8K, ARC-E, and ARC-C, which may have been seen during pre-training, the sorting task consists of randomly generated numbers. This ensures that performance reflects a model's reasoning ability rather than memorization. Direct I/O prompts were used, with responses evaluated using our regex-based parsing.\nTask 5 - Robustness To test the SLMs' reasoning robustness, we used three benchmarks (as below) that were published after June 2024, ensuring that models trained earlier had no exposure to them. 1) MR-Ben evaluates the model's ability to locate and analyze potential errors in reasoning steps. 2) MR-GSM8K assesses step-by-step intermediate reasoning. 3) GSM-Plus introduces adversarially perturbed inputs to test resilience."}, {"title": "Results and Insights", "content": "We evaluated 72 SLMs across six families: (1) SLMs trained from scratch, (2) Llama-3.2, (3) Llama-3.1, (4) Mistral and Mistral-Nemo, (5) Qwen2, and (6) Qwen2.5. Additionally, we reported computational requirements (GPU and Disk Space) to provide a holistic comparison. Complete results are detailed in Appendix A. In addition to the GPT-based judge for evaluation, we also evaluated all the model performance with the widely used framework, Im-eval-harness, on eight benchmarks. Complete Results are detailed in Appendix B."}, {"title": "Overall Performance", "content": "Our analysis show that emergent properties, i.e., performance improvements not observed in smaller models are highly model family-dependent (Table 2) rather than a universal trend across families. For example, Qwen2.5 (7B) outperforms Mistral (7B) by nearly 35 points on GSM8K despite having nearly the exact parameter count. This is primarily due to Qwen2.5's extensive pre-training data (18T tokens) (Qwen et al., 2025) and a post-training recipe using supervised fine-tuning and multi-stage reinforcement learning to align output better with humans. This suggests that training data and recipes are more critical than parameter size alone.\nOn ARC-E, larger models such as Llama-3.1 (70B) consistently achieve near-perfect scores, where the more straightforward reasoning tasks align well with their knowledge recall capabilities. In contrast, smaller models like Qwen2.5 (0.5B) and SmolLM2 (1.7B) exhibit performance drops, especially on ARC-C, which demands more nuanced reasoning. The performance gap between ARC-E and ARC-C suggests that factual recall (ARC-E) strongly depends on model size. In contrast, reasoning under ambiguity (ARC-C) benefits more from diverse training data and dataset quality than just scale."}, {"title": "Effect of Prompting", "content": "We find that prompt complexity had a minimal impact on performance across recent models. Recent models exhibit strong internal reasoning capabilities when either direct I/O or COT multi-shot prompts are used. This suggests that recent training methodologies have already embedded reasoning capabilities, diminishing the effect of explicit reasoning prompts. This calls for more advanced prompting techniques to enhance reasoning further. More analysis on why explicit COT does not work is detailed in Appendix D.1.\nOn GSM8K, Direct I/O prompts consistently outperform or match closely with complex prompts such as CoT and multi-shot settings. This suggests that excessive instructions can confuse SLMs rather than improve their reasoning.  shows that providing too many instructions or few-shot examples does not improve performance. SLMs perform better with straightforward queries rather than complex prompts. This suggest the need for task-specific prompt engineering rather than relying on general-purpose strategies like CoT."}, {"title": "Sorting Task Results", "content": "For the sorting tasks,  demonstrates that SLMs perform well on shorter lists but struggle as the list length increases. This highlights limitations in their ability to handle long-context numerical reasoning. Larger models like Llama-70B manage long sequences better, but even they show declining performance as task complexity increases (introducing -ve numbers).\nFurthermore, models struggle more when sorting mixed numbers (both +ve and -ve), with accuracy dropping compared to positive-only sorting. For example, Llama-3.1 (70B) achieves near-perfect accuracy on positive-only datasets, but performance drops on mixed numbers. This suggests that handling negative numbers introduces an additional layer of complexity that current architectures do not adequately address. Also, as the number length increases, performance drops significantly. This indicates a scalability bottleneck in current architectures for algorithmic reasoning.\nWe also observe failures where models introduce numbers not present in the input or simply repeat the entire input as the output. These errors become more frequent as list length increases, particularly for 32-number lists. More details are provided in Appendix D.5."}, {"title": "Quantization Effects", "content": "We find that quantization has minimal impact on the reasoning performance of larger models, whereas smaller models suffer more from compression. However, quantizing smaller models is often unnecessary since they are already compact and computationally efficient. On the other hand, compressing larger models allows them to match the efficiency of smaller models while retaining superior reasoning capabilities.\n shows that even with aggressive quanti- zation (4-bit), larger models retain their reasoning ability with near-identical performance as their uncompressed counterparts. For example, Llama-3.1 (70B) with W8-A8 quantization retains ~100% of its original accuracy while reducing computational cost (GPU) by over 75% (Table 2). Notably, a GPTQ 4-bit quantized Qwen2.5 (14B) outperforms Qwen2.5 (7B) while requiring ~1/3rd computational resources (GPU), which demonstrate that quantized versions of larger models are often more effective than standalone small models.\nThese findings show that quantization can make models significantly more efficient without a substantial drop in accuracy. This could be a viable strategy for deploying models in resource-constrained environments while preserving strong reasoning capabilities."}, {"title": "Pruning and Distillation Effects", "content": "Compared to quantization, pruning and distillation have a more detrimental impact on reasoning performance. After pruning, knowledge distillation is typically used with a recovery dataset to mitigate performance loss. In most cases, GSM8K was used as the recovery dataset. We observed that pruned models performed reasonably well on GSM8K (Table 2) when evaluated with direct prompting. However, on datasets such as ARC-E, ARC-C, and CommonsenseQA, pruned models frequently produce nonsensical outputs unrelated to the input query. Example cases are reported in Appendix D.2. Similarly, for sorting tasks, pruned models perform worse than their quantized counterparts, with frequent errors such as misplaced numbers, duplicated inputs, or missing elements in the final output. Some models, like Llama-3.1 and Mistral-7B, retain partial reasoning ability but remain far behind quantized models in overall performance. This indicates that pruning disrupts their ability to generalize reasoning across tasks."}, {"title": "Robustness", "content": "Adversarial Robustness (GSM-Plus) From Table 3, we observe that models generally experience a drop in accuracy on adversarial data but not drastically. Results reveal a clear performance hierarchy: Larger models (Qwen2.5-32B, Llama-3.1-70B) demonstrate higher resilience to adversarial perturbations, with relatively small accuracy drops (12.69 and 11.45 points, respectively). Smaller models (Qwen2.5-3B, Llama-3.1-8B) experience larger drops (16.41 and 16.35 points, respectively), highlighting their fragility in adversarial settings. This shows that larger models are inherently more robust in adversarial settings.\nWe observe that quantization does not significantly impact adversarial robustness. This suggests that quantized models retain their original reasoning resilience, further reinforcing their viability. Conversely, pruned models struggle significantly with adversarial robustness. Sparse-Llama-8B-2of4 and OpenHermes-7B, for example, experience drastic accuracy declines. Their inability to generalize under adversarial conditions likely stems from reduced model capacity and insufficient training diversity, making them vulnerable to edge-case scenarios.\nIntermediate Reasoning (MR-GSM8K) Table 3 also reports MR-GSM8K scores, which measure models' ability to generate and refine intermediate reasoning steps. Quantization has minimal impact on intermediate reasoning for larger models. For example, Qwen2.5-32B and its GPTQ-INT8 variant achieve identical MR-Scores, indicating that precision reduction does not degrade logical consistency. In contrast, pruned models fail to maintain coherent reasoning, with MR-Scores dropping to zero. This suggests that pruned architectures are more vulnerable to reasoning degradation when architectural integrity is compromised. Interestingly, the open-source Qwen2.5-32B (55.6) even surpasses closed propriety models like GPT-4-Turbo (53.0) and Claude3-Sonnet (20.8) in intermediate reasoning, based on reported results (Zeng et al., 2024a).\nIdentifying Errors in Reasoning (MR-Ben) Table 3 also reports MR-Ben scores. The trends observed are consistent: 1) Qwen-32B and Llama-70B outperform other models, 2) Quantization has minimal overall impact, and 3) Pruned models perform worse. Models perform best in biology and math, while coding and logic remain challenging. Findings from these 3 datasets further support the argument that SLMs are not just retrieving answers from pre-training but engaging in structured reasoning. These findings also contribute to the debate on whether neural networks genuinely reason. The original CoT paper (Wei et al., 2022c) raised concerns that \u201cCoT elicits reasoning but does not confirm whether the model is actually reasoning.\" These three benchmarks also help address this question, as they were released after many of the SLMs we tested, ensuring no contamination."}, {"title": "Conclusions and Future Directions", "content": "In this work, we systematically evaluated the reasoning ability of 72 SLMs, including their compressed variant, across 14 benchmarks. We also examine their robustness under adversarial conditions and intermediate reasoning. Overall, we observed: 1) LLMs tend to outperform SLMs in reasoning, but certain SLMs, such as the Qwen2.5 family, perform on par with LLMs. This is primarily attributed to their extensive pre-training (18T tokens, more than double that of Qwen2's 7T) and a robust post-training recipe using supervised fine-tuning and multi-stage reinforcement learning. 2) Among compression techniques, quantization proves to be a safer approach, preserving reasoning capabilities with minimal trade-offs. However, pruning drastically degrades performance, often leading to nonsensical or incomplete outputs. This suggests that compressing pre-trained LLMs is more effective than training SLMs from scratch. 3) LLMs exhibit stronger robustness in adversarial settings and intermediate reasoning tasks. However, quantization does not significantly impact a model's resilience in these scenarios, reinforcing its practicality as a compression method. 4) SLMs lag behind LLMs in instruction following, which may limit their applicability in tasks requiring precise adherence to input constraints. We hope these insights provide practical guidance for researchers in selecting an SLM. Future research should focus on improving the instruction-following capabilities of SLMs and exploring compression strategies that maintain reasoning performance while enhancing efficiency and robustness."}, {"title": "Limitations", "content": "In this work, we tried our best to ensure a rigorous and fair evaluation, but we acknowledge that some limitations should be considered when interpreting the results. First, our reliance on GPT-4 as an evaluator introduces potential biases and errors. While GPT-4 is a strong baseline for evaluation, it is not 100% accurate and may misclassify responses, especially in edge cases (shown in D.8). We observed instances where models producing many nonsensical responses were sometimes marked as correct by GPT-4, leading to potential overestimations of performance. Although we tried to mitigate this issue by limiting token generation and applying pre-evaluation parsing, it was not feasible to manually supervise the entire evaluation process.\nSecond, our sorting task evaluations relied on regex-based parsing to assess correctness. There could be cases where a model's response was correct but misclassified due to parsing errors. Although we tried to account for most of the variations in model outputs, ensuring 100% accuracy in automatic parsing remains a challenge. Additionally, our study focuses primarily on widely used benchmarks. However, reasoning abilities could be further assessed on more diverse datasets, including real-world problem-solving tasks and domain-specific reasoning benchmarks. Exploring these additional settings could provide a more comprehensive understanding of how different compression techniques impact model performance."}, {"title": "Ethics Statement", "content": "This study evaluates small language models using standardized benchmarks and publicly available datasets, ensuring transparency and reproducibility. No private or sensitive data was used, and all models were assessed under fair conditions. We acknowledge potential biases in LLM-based evaluations and encourage further research for mitigation."}, {"title": "Acknowledgement", "content": "This work was supported by the NSF NAIRR Pilot with PSC Neocortex and NCSA Delta, Cisco Research, Amazon, Schmidt Science, the Commonwealth Cyber Initiative, the Amazon-Virginia Tech Center for Efficient and Robust Machine Learning, the Sanghani Center for AI and Data Analytics at Virginia Tech, the Virginia Tech Innovation Campus, Children's National Hospital, and the Fralin Biomedical Research Institute at Virginia Tech. S. C. acknowledges support from Schmidt Science. The views, findings, conclusions, and recommendations expressed in this work are those of the authors and do not necessarily reflect the opinions of the funding agencies."}, {"title": "Quantization", "content": "Given a full-precision weight matrix $W \\in \\mathbb{R}^{m \\times n}$, quantization maps each weight $w \\in W$ to a lower-precision representation $\\hat{w}$:\n$$\\hat{w} = S \\cdot \\text{round}\\left(\\frac{w}{S}\\right),$$(1)\nwhere $S$ is a scaling factor that determines how real-valued weights are mapped to discrete levels."}, {"title": "Pruning", "content": "Formally, given a weight matrix $W$, pruning removes elements below a threshold $\\tau$, setting them to zero:\n$$W_{ij} =\\begin{cases} W_{i,j}, & \\text{if } |W_{ij}| \\geq \\tau \\\\ 0, & \\text{otherwise} \\end{cases}$$(2)"}, {"title": "Distillation", "content": "Given a teacher model output $z^T$ and a student model output $z^S$, distillation minimizes the loss:\n$$\\mathcal{L} = \\alpha \\mathcal{L}_{CE}(z^S, y) + (1 - \\alpha) \\mathcal{L}_{KD}(z^S, z^T),$$\nwhere $\\mathcal{L}_{CE}$ is the standard cross-entropy loss, $\\mathcal{L}_{KD}$ is the knowledge distillation loss:\n$$\\mathcal{L}_{KD} = \\sum_i p_i^T \\log p_i^S,$$(4)\nand $p_i^T$, $p_i^S$ are the softened class probabilities from the teacher and student, respectively. The temperature parameter $T$ controls how much the logits are softened before computing probabilities:\n$$p_i = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}$$ (5)"}]}