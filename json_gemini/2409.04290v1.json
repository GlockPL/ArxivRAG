{"title": "CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis", "authors": ["William Knottenbelt", "Zeyu Gao", "Rebecca Wray", "Woody Zhidong Zhang", "Jiashuai Liu", "Mireia Crispin-Ortuzar"], "abstract": "Survival analysis is a branch of statistics used for modeling the time until a specific event occurs and is widely used in medicine, engineering, finance, and many other fields. When choosing survival models, there is typically a trade-off between performance and interpretability, where the highest performance is achieved by black-box models based on deep learning. This is a major problem in fields such as medicine where practitioners are reluctant to blindly trust black-box models to make important patient decisions. Kolmogorov-Arnold Networks (KANs) were recently proposed as an interpretable and accurate alternative to multi-layer perceptrons (MLPs). We introduce CoxKAN, a Cox proportional hazards Kolmogorov-Arnold Network for interpretable, high-performance survival analysis. We evaluate the proposed CoxKAN on 4 synthetic datasets and 9 real medical datasets. The synthetic experiments demonstrate that CoxKAN accurately recovers interpretable symbolic formulae for the hazard function, and effectively performs automatic feature selection. Evaluation on the 9 real datasets show that CoxKAN consistently outperforms the Cox proportional hazards model and achieves performance that is superior or comparable to that of tuned MLPs. Furthermore, we find that CoxKAN identifies complex interactions between predictor variables that would be extremely difficult to recognise using existing survival methods, and automatically finds symbolic formulae which uncover the precise effect of important biomarkers on patient risk.", "sections": [{"title": "1 Introduction", "content": "Survival analysis - also called time-to-event analysis - is a set of statistical methods used for modelling the time until a specific event occurs, such as death, failure, or relapse. It is crucial to various fields, including medicine, engineering, economics, and insurance, where understanding the timing and probability of events can significantly impact decision-making. For example, survival models are used extensively in oncology (the study of cancer) to identify biomarkers/prognostic factors [1-3], assess treatment efficacy [4-7], and develop personalized treatment plans [8].\nArguably, the most common survival model is the Cox proportional hazards model (CoxPH) [9], which assumes a linear relationship between the patient's \u00b9 covariates (e.g., age, blood pressure etc.) and the log-partial hazard, which is a measure of the patient's risk of event-occurrence (see Section 2.1.1). This model has the benefit of interpretability (we can see exactly how each covariate impacts risk), but the linear assumption is often overly simplistic and can cause significant bias error. Methods based on machine learning generally have less bias and therefore potentially better performance. These include models such as random survival forests [10, 11], Bayesian models based on Gaussian processes [12, 13] and dependant logistic regression [14].\nThe most powerful survival models are those based on deep learning, which was first shown with \"DeepSurv\" [8], a deep neural network based on CoxPH. Deep learning models also have the advantage of being able to handle diverse input modalities from unstructured data such as images to structured datasets like tabular health records\u2014, making them highly adaptable for multiple healthcare applications. Deep learning has been used extensively for survival analysis, achieving state-of-the-art performance on numerous datasets across many domains [15-21]. However, the increased complexity associated with deep learning comes at the expense of interpretability, with multi-layer perceptrons (MLPs) being sometimes referred to as a \"black-box\". As a result, these methods have had limited clinical adoption and the search for more interpretable techniques is an active area of research [22-24].\nKolmogorov-Arnold Networks (KANs) [25] were recently introduced as an alternative to MLPs, demonstrating enhanced interpretability and accuracy. This approach differs from MLPs by using learnable activation functions on edges of the network instead of linear weights, and summing those activation functions on nodes (\"neurons\"). These learnable activation functions are parameterised as a B-spline curve with learnable coefficients (see Section 2.2.1) to allow them to approximate any univariate function. The interpretability of KANs stems from the ability to fit symbolic operators to the learned activation functions, leaving a symbolic formula in-place of the network. In the original paper, KANs were shown to be useful in physics for solving partial differential equations and extracting mobility edges in the context of Anderson localization. Since then, extensive applications of KANs have been found, including time series analysis [26, 27], medical image segmentation [28] and satellite image classification [29]."}, {"title": "2 Preliminaries", "content": "Survival time is typically described using the survival function and the hazard function. Let T be the time until the event of interest occurs, with probability density function f(t). The survival function S(t) = P(T > t) is the probability that a patient survives longer than time t. The hazard function h(t) is the instantaneous event probability density at time t, given the patient has survived up to at least that time. Formally, it is written\n$h(t) = \\lim_{\\Delta t\\rightarrow\\infty} \\frac{P(t < T < t + \\Delta t\\vert T > t)}{\\Delta t}$  (1)\nThis gives us the probability density function as f(t) = h(t)S(t). It can be shown that the survival function is related to the hazard function by:\n$S(t) = exp(-\\int_0^t h(s) ds).$  (2)\nSurvival data for a given patient is comprised of three parts: i) covariates x (predictor variables), ii) time duration t, and iii) event indicator d. If the event was observed then \u03b4 = 1 and t is the time between the covariates being collected and the event occurring. If the event was not observed then the patient is said to be right-censored, \u03b4 = 0, and t is the time between the covariates being collected and the last contact with the patient. For example, this could happen if we are conducting a study on the survival of cancer patients, and some of the patients drop out of the study at random"}, {"title": "2.1 Survival Analysis", "content": "Survival time is typically described using the survival function and the hazard function. Let T be the time until the event of interest occurs, with probability density function f(t). The survival function S(t) = P(T > t) is the probability that a patient survives longer than time t. The hazard function h(t) is the instantaneous event probability density at time t, given the patient has survived up to at least that time. Formally, it is written\n$h(t) = \\lim_{\\Delta t\\rightarrow\\infty} \\frac{P(t < T < t + \\Delta t\\vert T > t)}{\\Delta t}$  (1)\nThis gives us the probability density function as f(t) = h(t)S(t). It can be shown that the survival function is related to the hazard function by:\n$S(t) = exp(-\\int_0^t h(s) ds).$  (2)\nSurvival data for a given patient is comprised of three parts: i) covariates x (predictor variables), ii) time duration t, and iii) event indicator d. If the event was observed then \u03b4 = 1 and t is the time between the covariates being collected and the event occurring. If the event was not observed then the patient is said to be right-censored, \u03b4 = 0, and t is the time between the covariates being collected and the last contact with the patient. For example, this could happen if we are conducting a study on the survival of cancer patients, and some of the patients drop out of the study at random"}, {"title": "2.1.1 Cox proportional hazards model (CoxPH)", "content": "A proportional hazards model is one which assumes the hazard function takes the form\n$h(t, x) = h_0(t) exp(\\Theta(x)),$ (3)\nwhere t is time, $h_0(t)$ is the baseline hazard function (same for all patients) and $\\Theta(x)$ is the log-partial hazard. The log-partial hazard can be thought of as an overall measure of patient risk that is independent of time.\nThe original proportional hazards model is called the Cox proportional hazards model (CoxPH) [9] and is still perhaps the most common survival regression model used today. It models the log-partial hazard in (3) as a linear combination of the patient's covariates:\n$\\Theta_{CPH}(X) = \\beta x = \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n.$ (4)\nSuppose we have a dataset of N patients, {(xi, ti, \u03b4i)}N1. The weights, \u03b2, are tuned to optimize the Cox partial likelihood, given by\n$L(\\beta) = \\prod_{i:\\delta_i = 1} \\frac{exp(\\Theta(x_i))}{\\sum_{j\\in R(t_i)} exp(\\Theta(x_j))},$ (5)\nwhere the risk-set $R(t_i)$ is the set of patients with observed time t > ti (ie. those who are alive as of time ti)."}, {"title": "2.1.2 DeepSurv", "content": "We can construct a proportional hazards model based on deep learning by using a neural network to predict the log-partial hazard [8, 31]. It is trained using the \u201cCox loss\" function, which is the negative log of (5):\n$l_{Cox} = - \\sum_{i:\\delta_i=1}  [\\Theta(x_i) - log(\\sum_{j\\in R(t_i)} exp(\\Theta(x_j)))]$ (6)\nThis model is known as DeepSurv."}, {"title": "2.2 Kolmogorov-Arnold Networks", "content": "Kolmogorov-Arnold Networks (KANs) [25] are similar to Multi-Layer Perceptrons (MLPs) in that they consist of consecutive layers of neurons (nodes), where each layer"}, {"title": "2.2.1 Activation Functions", "content": "Each activation function \u03c6(x) is given by\n$\\varphi(x) = w_b b(x) + w_s spline(x),$ (11)\nwhere wb, ws are trainable weights that control the magnitude of the activation, b(x) is a (non-trainable) basis function used for training stability (analogous to a"}, {"title": "2.2.2 Regularization", "content": "For efficiency and interpretability, we would ideally like our KAN to be as small and simple as possible. However, we may not know in advance the appropriate shape for the problem. Hence, [25] proposed a regularization and pruning scheme to simplify a KAN from an initially large network. First, regularization terms are added to the loss function to encourage sparsity of the KAN neurons and spline coefficients.\nThe L1 norm of an activation function \u03c6 is defined to be its average magnitude over the training batch of NB inputs,\n$||\\varphi||_1 = \\frac{1}{N_B} \\sum_{s=1}^{N_B} |\\varphi((x^s))|,$ (13)\nand that of its spline coefficients c is $|c|_1 = \\sum_{i=0}^{G+k-1}|c_i|$."}, {"title": "3 CoxKAN", "content": "CoxKAN is a novel proportional hazards model where the log-partial hazard is estimated by a KAN with a single output node:\n$\\Theta_{KAN}(x) = KAN(x).$  (17)\nThe CoxKAN training pipeline can be summarised by the following: Hyperparameter search \u2192 train with sparsity regularization \u2192 auto-prune network \u2192 fit symbolic representation to result. The latter two steps are visualized in Fig. 2."}, {"title": "Loss and Optimization", "content": "CoxKAN is trained using the following objective:\n$l_{total} = l_{Cox} + \\lambda R,$ (18)\nwhere R is the regularization in (16), \u03bb controls overall regularization strength and lCox is a fast approximation to the Cox Loss in (6), where the risk-set R(ti) is slightly inaccurate when many patients have the same observed duration (we refer readers to our well-documented software for further details). This is useful since KANs are slow"}, {"title": "Hyperparameter Tuning", "content": "We implement random hyperparameter optimization [33] with the Python package Optuna [34] using the Tree-structured Parzen Estimator [35] algorithm to efficiently search the hyperparameter space. The objective function we optimize on is the average C-Index of the pruned CoxKAN over a 4-fold cross-validation of the experiment's training set."}, {"title": "Early Stopping", "content": "We conduct early stopping based on validation set C-Index. For smaller datasets we may want to train on the full training set instead of reserving an extra validation set, hence we include early stopping as an optimizable hyperparameter. For datasets where early stopping is determined to be most optimal, we reserve 20% of the designated training set as the validation set."}, {"title": "Pruning", "content": "After training CoxKAN, we prune activation functions in the network by removing those that have L1 norms below a certain threshold. This allows for automatic feature selection and control of the network shape. The L1 threshold is a tunable hyperparameter, but when using a validation set for early stopping, we instead select the optimal threshold based on validation performance."}, {"title": "Symbolic Fitting", "content": "For interpretability, we would like the activation functions of CoxKAN to be clean symbolic formulas rather than parameterised B-spline curves.\nReference [25] proposed the following procedure to convert a KAN to a symbolic representation: If we suspect a given activation function \u03c6(x) is approximating a known symbolic operator f (e.g., sin or exp), then we can set the activation function to \u03c6(x) = cf(ax + b) + d. The affine parameters (a, b, c, d) are found by fitting them to a set of pre- and post-activations {x(s),y(s)}M1, such that y \u2248 cf(ax + b) + d. This is done by iterative grid search for (a, b) and linear regression for (c, d). The quality of the fit is measured by the coefficient of determination, R\u00b2 (AKA \u201cfraction of variance explained\"). We can either visualize the activations by eye and choose a suitable function to fit, or we can use pykan's auto_symbolic method, which simply fits all symbolic operators from a large library and selects the operator that achieves the highest R2.\nIn this work, we used auto_symbolic with a library of 22 functions (see Appendix A), with a few additional improvements. Firstly, several of these functions can become linear with the right choice of affine parameters, but if a learnt activation is linear then we want this to be reflected in the symbolic formula. Hence, after training and pruning CoxKAN, we first fit the linear function f(x) = ax + b (special case with two affine parameters instead of four) to all activation functions and accept the fit if R2 > 0.99, otherwise we proceed normally (auto_symbolic or recognition by eye)."}, {"title": "4 Results", "content": "To evaluate CoxKAN as comprehensively as possible we conducted experiments on both synthetic and real datasets (13 in total). For each experiment, we train CoxKAN using the procedure described in Section 3 (hyperparameter search \u2192 train with sparsity regularization \u2192 auto-prune \u2192 fit symbolic). The hyperparameters found in each case are detailed in Appendix A.\nOn the real datasets we compare CoxKAN to CoxPH and DeepSurv. To ensure a fair comparison, we use the same hyperparameter searching strategy for DeepSurv as used for CoxKAN, and we boost performance of DeepSurv as much as possible by enabling modern deep learning techniques such as early stopping, dropout, batch normalization and weight decay (L2 regularization).\nWe evaluate all models using the concordance index c (C-Index) [36], which is the most common metric to judge predictive accuracy of a survival model. It measures how well the model predictions agree with the ranking of the patient's survival times, where c = 0.5 corresponds to random raking and c = 1 is a perfect ranking. We obtain 95% confidence intervals by bootstrapping [37] the test set (sampling with replacement), and characterise the difference in performance between two models as statistically significant if the confidence intervals do not overlap."}, {"title": "4.1 Evaluation with Synthetic Data", "content": "In the original paper [25], KANs were shown to recover exact symbolic formulas that were used to generate toy regression datasets. However, the authors did not simulate any noise in these datasets. Survival data typically provides a very noisy signal, not only because time-to-event is a random variable, but also because the censoring distribution adds an additional layer of uncertainty.\nTo ascertain whether KANs can successfully recover symbolic formulas from survival data, we generated four datasets based on a proportional-hazards model (3), using custom symbolic formulas for the log-partial hazard, a constant baseline hazard of 0.01 and a uniform censoring distribution. The covariates were sampled uniformly in [-1,1] unless stated otherwise. We also added two irrelevant noisy covariates to each dataset. Further details about the data generation are found in Appendix B.\nIn each case, we observe that the pruning of CoxKAN successfully removes the irrelevant features (demonstrating automatic feature selection), and leaves CoxKAN with a shape that is most appropriate for the problem (unless the hyperparameter search already yielded the \u201ccorrect\" shape). The results 3 are given in Table 1 and the pruned CoxKANs (before symbolic fitting) are visualized in Fig. 3. In all four cases, CoxPH fails to accurately predict the hazard function."}, {"title": "4.1.1 Gaussian Formula", "content": "We first set the log-partial hazard to be a Gaussian function:"}, {"title": "4.1.2 Shallow Formula", "content": "It is common in survival data to encounter covariates which satisfy the linear CoxPH assumptions after some non-linear transformation. That is, they have non-linear relationships to the patient's risk but they do not interact with each other.\nTo determine whether CoxKAN can automatically detect and solve this situation, we set the log-partial hazard to\n$\\theta(x) = tanh(5x_1) + sin(2x_2) + x_3.$\nFollowing the same procedure as above, CoxKAN predicts the following formula (affine parameters rounded to 1 d.p.):\n$\\Theta_{KAN} = tanh (5.1x_1) - sin (6.3x_2 - 9.4) + x_3.$\nUpon first glance the sin term appears incorrect, but we note that sin (6.3x2 - 9.4) \u2248 - sin(2x2 - 3\u03c0) = sin(2x2)."}, {"title": "4.1.3 Deep Formula", "content": "To contrast with the previous example, we next set the log-partial hazard to an expression that requires a deep KAN (2 hidden layers) to capture:\n$\\theta(x) = 2\\sqrt{(x_1-x_2)^2 + (x_3 - x_4)^2}.$\nCoxKAN predicts the formula:\n$\\Theta_{KAN} = 4 |\\frac{x_3 +0.8x_2 + 0.9 (0.1 - x_1)^2}{- 0.7 (x_3 +0.7x_4 + 0.1)^2 + 0.6} + \\frac{0.9 (0.1 - x_2)^2 - 0.5 (x_1 + x_2 - 0.1)^2}{- 0.7 (x_3 +0.7x_4 + 0.1)^2 + 0.6}|.$\nBy multiplying this out and making some liberal approximations to the affine parameters, we recover the original formula:\n$\\Theta_{KAN} \\approx 4|\\sqrt{(x_1^2 - 2x_1x_2 + x_2^2 + x_3 - 2x_3x_4 + x_4)} = 2\\sqrt{(x_1-x_2)^2 + (x_3 - X_4)^2}."}, {"title": "4.1.4 Difficult Formula", "content": "Finally, we selected an formula for the log-partial hazard that we hypothesized would be difficult to recover exactly:\n$\\theta(x) = tanh(5(log(x_1) + |x_2|)),$\nwhere x1 \u2208 [0.1,1], x2 \u2208 [-1,1]. The intuition here is that tanh(5z) has a shallow gradient in most of its input domain, hence large portions of the input space have similar survival functions, which should cause the data to have a weak training signal."}, {"title": "4.2 Evaluation on Real Clinical Data", "content": "Real survival data is complex and may not always be appropriately modeled by simple symbolic formulas like those seen in the previous section. To assess the real-world application of CoxKAN, we first compare its performance on 5 clinical datasets to CoxPH and DeepSurv [8]. The results are presented in Table 2. Three of the datasets (SUPPORT, GBSG, METABRIC) were obtained from the DeepSurv GitHub repository [8], hence for these experiments, we quote the published DeepSurv results. We provide results for CoxKAN straight after training (\u201cCoxKAN Trained\u201d), after pruning (\"CoxKAN Pruned\"), and after symbolic fitting (\u201cCoxKAN Symbolic\"). We find that CoxKAN Symbolic outperforms CoxPH and DeepSurv on all datasets except FLCHAIN.\nWe also observe that CoxKAN Symbolic generally achieves a higher C-Index than CoxKAN Trained. This is not statistically significant since the confidence intervals"}, {"title": "4.2.1 Study to Understand Prognoses and Preferences for Outcomes and Risks of Treatment (SUPPORT)", "content": "The Study to Understand Prognoses and Preferences for Outcomes and Risks of Treatment (SUPPORT) investigates the survival of hospitalized, seriously-ill adults [38]. The dataset consists of 7,098 patients for training and 1,775 for testing. Each patient is equipped with the following covariates: age, race, number of comorbidities, diabetes indicator, dementia indicator, cancer status, mean blood pressure (meanbp), heart rate (hr), respiration rate (rr), temperature (temp), serum sodium, white blood cell count (wbc), and serum creatinine.\nFollowing the usual procedure, we train and auto-prune CoxKAN. The resulting network has three hidden neurons and can be considered to consist of two main sub-networks. The first subnetwork is that which involves the first and third hidden neurons, and has linear activations in the 2nd layer that are equivalent to 'skipping a layer', hence it represents non-interacting terms that contribute to the log-partial hazard in isolation. We perform the default auto-symbolic fitting on this sub-network. The second sub-network is that which involves the second hidden neuron, and encodes a complex interaction between the patient age and cancer status. The single activation function in the 2nd layer of this sub-network, which we denote \u03c61,1,2 = \u03c6interact, has no obvious symbolic form so for now we leave it as non-symbolic (we will return to this soon)."}, {"title": "4.2.2 Rotterdam & German Breast Cancer Study Group (GBSG)", "content": "Next, we use breast cancer data from the Rotterdam tumor bank [39] for training (1,546 patients), and data from a study by the German Breast Cancer Study Group (GBSG) [40] for testing (686 patients). The covariates include hormonal therapy indicator, tumor size, menopausal status, age, number of positive lymph nodes, and the"}, {"title": "4.2.3 Molecular Taxonomy of Breast Cancer International Consortium (METABRIC)", "content": "The Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) is a research project investigating gene expression in breast cancer [42]. The dataset was preprocessed by [8], and consists of the expression of 4 genes (EGFR, PGR,"}, {"title": "4.2.4 Assay of serum free light chain (FLCHAIN)", "content": "The FLCHAIN dataset (obtained from [45]) contains 7,874 subjects from a study on the relationship between the concentration of immunoglobulin light chains (serum free light chain, FLC) and mortality [46, 47]. We reserved 20% of the patients at random for testing. The covariates include age, sex, year of blood sample, the kappa and lambda portion of serum free light chain, FLC group, serum creatinine, and indicator of monoclonal gammapothy (MGUS).\nAs can be seen in Table 2, all models achieve very similar performance with heavily overlapping confidence intervals, which suggests that the linear CoxPH assumption"}, {"title": "4.2.5 National Wilm's Tumor Study (NWTCO)", "content": "The National Wilm's Tumor Study [48] investigated the treatment and survival outcomes of children with a type of kidney cancer known as Wilms' tumor. Our dataset (obtained from [45]) consists of 4,028 subjects from the 3rd and 4th clinical trials of the study. The event of interest is cancer relapse (not death) and there are 6 covariates: histology readings (\u201cFavourable Histology (FH)\" or \"Unfavourable Histology (UH)\") from local institutions and again from a central lab, cancer stage, clinical trial, age in months and a binary indication of whether the patient was included in the subcohort from [49] (subsample stratified jointly on outcome and covariates). We reserved 20% at random for testing.\nCoxKAN has 5 hidden neurons in this case and its symbolic representation is depicted in Fig. 9(a). The activations \u03c61,1,1, \u03c61,1,2, \u03c61,1,5 are linear and \u03c61,1,3, \u03c61,1,4 are non-linear, thus the resulting formula is a mixture of isolation and interaction terms:"}, {"title": "5 Discussion and Conclusion", "content": "This paper presented the novel CoxKAN framework, which is the first application of Kolmogorov-Arnold Networks to interpretable survival regression. We demonstrated that CoxKAN achieves (i) sophisticated interpretability by obtaining symbolic formulas for the hazard function and visualizing KAN activation functions and (ii) high performance due to the ability to flexibly capture any function (low bias error). We were also able to mitigate CoxKAN overfitting, which can be attributed to the explicit regularization in the loss function, early stopping, and the inductive bias of the pruning and symbolic fitting pipeline that encourages simpler functions, which generalize better than the original network.\nIn the first series of experiments, we generated synthetic datasets using custom symbolic formulas for the hazard function and found that in 3/4 examples CoxKAN was able to recover the correct symbolic form. In the last example (which was made to be intentionally difficult to recover), CoxKAN found a formula that was shown to be a highly accurate approximation to the ground truth; we claim that CoxKAN still possesses the properties of interpretability and high performance in this case. Additionally, CoxKAN automatically pruned the irrelevant, noisy features added to all synthetic datasets, demonstrating successful feature selection. We then evaluated CoxKAN on 5 clinical datasets and 4 high-dimensional genomics datasets. On the clinical data, CoxKAN Symbolic achieved a statistically significant improvement in performance over CoxPH in 4/5 cases and over DeepSurv in 3/5 cases. On the genomics data, CoxKAN Symbolic achieved a statistically significant performance improvement over the DeepSurv in 2/4 cases and outperformed CoxPH with heavy Lasso regularization twice (though only once was this statistically significant). On datasets that CoxKAN did not outperform CoxPH or DeepSurv, the performance difference was generally not statistically significant as characterised by overlapping confidence intervals. CoxKAN also uncovered useful insights from the survival data. For example, on the SUPPORT dataset, CoxKAN identified that the risk of cancer patients in metastasis decreases with age until about 60 years old, then starts to increase, but for patients with non-metastatic cancer or no cancer at all, their risk only increases with age. This kind of variable interaction would be extremely difficult to identify using existing survival models. On the genomics datasets, CoxKAN uncovered a number of important biological associations between cancer risk and genomic features such as specific CNVs and mRNA transcripts, offering valuable insights that can guide further biological studies and the development of targeted therapeutic strategies."}, {"title": "Potiental Applications of CoxKAN", "content": "Given that CoxKAN is the essentially first survival model with sophisticated interpretability and low bias, we believe it has far-ranging applications, both within the medical field and in other disciplines. In medical research, CoxKAN could be used to discover complex biomarkers involving multi-variable interactions and assess treatment efficacy by providing insights of how treatment conditions impact survival and interact with patient features. In a clinical setting, CoxKAN could be used for personalized medicine by using its predictions/insights to inform treatment plans. Outside of the medical field, CoxKAN could be used to understand and address underlying factors that impact the time to mechanical failure in engineering (helping to inform construction of equipment), customer churn in business (guiding the development of retention strategies), loan default in finance (improving risk assessment models) and insurance claims (allowing actuaries to justify premiums)."}, {"title": "Weaknesses and Future Work", "content": "CoxKAN does not work straight out of the box and has several weaknesses that we believe are solvable. Firstly, CoxKAN is exposed to bias of certain assumptions of CoxPH such as \"the baseline hazard is the same for all patients\" and \"the relationship between covariates and risk does not change over time\". An exciting future direction would be to construct a KAN-based framework that bypasses these assumptions while retaining precise interpretability. Secondly, CoxKAN is vulnerable to overfitting and thus for the high dimensional genomics datasets the hyperparameter search typically yielded low-capacity KANs with no hidden layers. This meant that interactions between genomic features were not learned, even though it is well known that genomics features do experience interactions. Additional effort to mitigate overfitting while retaining the ability to capture interactions is a promising future direction. Furthermore, the performance of DeepSurv and CoxKAN was fairly unstable with respect to initialization on the high dimensional genomics datasets, hence CoxPH with Lasso regularization could be considered a more reliable choice in this case. CoxKAN is also sensitive to hyper-parameters and can be unstable to train. These flaws could be addressed by experimenting with more techniques related to hyperparameter tuning, regularization, and optimization."}, {"title": "6 Data availability", "content": "The clinical datasets METABRIC, SUPPORT and GBSG are available at https://github.com/jaredleekatzman/DeepSurv/tree/master/experiments/data and NWTCO, FLCHAIN are available at https://vincentarelbundock.github.io/Rdatasets/. TCGA genomic data (BRCA, STAD, GBM/LGG, and KIRC) are available at https://portal.gdc.cancer.gov."}, {"title": "7 Code availability", "content": "The code for training and evaluating CoxKAN is available at https://github.com/knottwill/CoxKAN, and can be installed using the following command: \"pip install coxkan\"."}, {"title": "C STAD and KIRC hazards", "content": "On the STAD dataset, CoxKAN predicted the following log-partial hazard:\n$\\Theta_{KAN} = +0.2 tanh(CALM2_{RNA} \u2212 0.4)$ (\u03c3 = 0.15)\n+ 0.1 PRR15L_{RNA}$ (\u03c3 = 0.10)\n+ 0.2 TOMM20_{RNA} (\u03c3 = 0.09)\n- 0.09 MUC16_{mut} (\u03c3 = 0.09)\n+0.8 arctan(0.4 C3_{RNA} +0.2) (\u03c3 = 0.08)\n- 0.1 HNRNPK_{RNA} (\u03c3 = 0.08)\n- 0.2 MISP_{RNA} (\u03c3 = 0.08)\n+ less significant terms\nOn the KIRC dataset, CoxKAN predicted:\n$\\Theta_{KAN} = + 0.43 \\cdot MT1X_{RNA}$ (\u03c3 = 0.42)\n+0.34 DDX43_{RNA} (\u03c3 = 0.34)\n+0.23 CWH43_{RNA}$ (\u03c3 = 0.31)\n+0.22 CILP_{RNA} (\u03c3 = 0.31)\n- 0.24 LOC153328_{RNA} (\u03c3 = 0.29)\n- 0.21 CYP3A7_{RNA} (\u03c3 = 0.28)\n+ less significant terms,"}]}