{"title": "BOLIMES: Boruta\u2013LIME optiMized fEature Selection for Gene Expression Classification", "authors": ["Bich-Chung Phan", "Thanh Ma", "Huu-Hoa Nguyen", "Thanh-Nghi Do"], "abstract": "Gene expression classification is a pivotal yet challenging task in bioinformatics, primarily due to the high dimensionality of genomic data and the risk of overfitting. To bridge this gap, we propose BOLIMES, a novel feature selection algorithm designed to enhance gene expression classification by systematically refining the feature subset. Unlike conventional methods that rely solely on statistical ranking or classifier-specific selection, we integrate the robustness of Boruta with the interpretability of LIME, ensuring that only the most relevant and influential genes are retained. BOLIMES first employs Boruta to filter out non-informative genes by comparing each feature against its randomized counterpart, thus preserving valuable information. It then uses LIME to rank the remaining genes based on their local importance to the classifier. Finally, an iterative classification evaluation determines the optimal feature subset by selecting the number of genes that maximizes predictive accuracy. By combining exhaustive feature selection with interpretability-driven refinement, our solution effectively balances dimensionality reduction with high classification performance, offering a powerful solution for high-dimensional gene expression analysis.", "sections": [{"title": "1 Introduction", "content": "Gene expression classification [1,17,9,10,18] has emerged as a fundamental tool in bioinformatics, enabling the identification of disease subtypes, prediction of patient outcomes, and discovery of potential therapeutic targets. With the advent of high-throughput sequencing technologies, researchers can now analyze vast gene expression profiles across thousands of genes simultaneously. However, this progress comes with a significant computational and analytical challenge: the curse of dimensionality [22,2]. In typical gene expression datasets, the number of genes (p) far exceeds the number of samples (n), often by several orders of magnitude. This imbalance leads to severe overfitting in machine learning models, where classifiers struggle to generalize due to the overwhelming presence of irrelevant or redundant features. Additionally, the high dimensionality increases computational complexity, making conventional classification models inefficient and impractical for real-world applications."}, {"title": "2 Background", "content": "In this section, we present the foundation of our approach with integrating Boruta and Lime. We also provide the ML algorithm that will be used for the gene expression classification task."}, {"title": "2.1 Gene Expression Classification with ML models", "content": "Gene expression classification [9,10,18] lies at the forefront of biomedical research, offering profound insights into the molecular mechanisms underlying various diseases. ML models have become indispensable in this domain, as they can uncover complex patterns within vast and high-dimensional gene expression datasets. However, these datasets often contain a plethora of features, many of which are redundant or irrelevant, potentially obscuring the most critical biological signals and leading to overfitting. Consequently, feature selection becomes imperative it refines the dataset by isolating the most informative genes, thereby enhancing model accuracy, interpretability, and computational efficiency. By focusing solely on the pivotal biomarkers, this research is able to achieve more reliable predictive outcomes. In this paper, we investigate and evaluate the classification with various ML techniques. Namely, we experiment our selected features with ML algorithms, i.e., SVM [32], Random Forest [4], XGB [6], Gradient Boosting [15].\nDefinition 1 (Classification). Let D = (X,y) be a dataset where X \u2286 Rn is the feature space and y \u2208 Y = {1, 2, ..., k} represents the class labels. A classifier is a function\n\\(f : X \u2192 \u0423\\),\nthat assigns a predicted label \u0177 = f(x) to each input x \u2208 X. The function f is learned from the labeled examples\n\\(D 0 = {(xi, yi) | Xi \u2208 X, yi \u2208 Y, i = 1, ..., N}\\),\nby minimizing a loss function l : Y \u00d7 Y \u2192 R>0 that quantifies the error between the predicted and true labels. Once trained, f is used to classify new, unseen inputs.\nWhile numerous feature selection techniques exist, our study concentrates on two: Boruta and LIME. We choose Boruta for its robust ability to identify all truly relevant features within high-dimensional datasets, ensuring that no significant predictor is overlooked. Complementing this, LIME is employed for its"}, {"title": "2.2 Leveraging Boruta for Robust Feature Extraction", "content": "Boruta [24,36] is a powerful wrapper-based feature selection algorithm designed to identify all truly relevant variables in a dataset. By comparing the importance of actual features with that of randomly generated \"shadow\" features, Boruta systematically filters out irrelevant variables while preserving essential predictors. This rigorous selection process is particularly valuable in high-dimensional applications, such as gene expression classification, where capturing meaningful signals is crucial. Rather than directly improving predictive accuracy, Boruta refines the feature set, which can indirectly enhance model interpretability and performance. For clarity, we formally define Boruta as follows:\nDefinition 2 (Boruta Feature Selection). Let D = (X,y) be a dataset with features X = {X1,X2,...,Xp} and target y. The Boruta algorithm identifies all relevant features in X as follows:\n1. Shadow Feature Generation: For each xi \u2208 X, create a shadow feature xshadow by randomly permuting its values, forming the set Xshadow.\n2. Importance Estimation: Train a classifier (e.g., Random Forest) on the combined set X \u222a X shadow and compute the importance score I(z) for each z.\n3. Feature Comparison: For each xi, define\n\\(Ishadow =\\underset{ZEX shadow}{max} I(z)\\).\nThen classify xi as relevant if I(xi) is significantly greater than Ishadow, irrelevant if significantly lower, or tentative otherwise.\n4. Iteration: Remove irrelevant features and repeat until all features are decisively classified.\nThe final selected subset X* \u2286 X comprises all features deemed relevant."}, {"title": "2.3 \u03a7\u0391\u0399 for Feature Selection", "content": "Explainable A\u0399 (\u03a7\u0391\u0399) [12,35] represents a forefront of AI research, aiming to elucidate the decision-making processes of complex models. In the context of gene expression classification, where feature selection is pivotal to model performance and interpretability, our study leverages LIME-Local Interpretable Model-Agnostic Explanations\u2014to demystify and extract critical features. LIME approximates the behavior of a sophisticated, black-box model with a simpler, locally interpretable surrogate, thereby pinpointing the most influential predictors in the vicinity of a given instance. This approach enhances the transparency"}, {"title": "3 BOLIMEX algorithm", "content": "Our methodology begins with applying the Boruta algorithm to sift through the high-dimensional gene expression dataset, effectively filtering out irrelevant features and isolating those that are truly significant. This initial reduction is critical because LIME, our subsequent interpretability tool, involves generating numerous perturbed samples and calculating distances-a computationally in- tensive process, especially in large feature spaces. By narrowing the focus to a refined subset via Boruta, we significantly reduce the computational burden and enhance the precision of LIME's local explanations. This sequential approach streamlines the overall feature selection process and fortifies the reliability and clarity of our model's interpretability, ultimately leading to improved classification performance.\nHowever, the key question remains: how many features should be selected for optimal classification? In our proposed BOLIMES algorithm, which integrates Boruta and LIME, we first reduce the high-dimensional feature set by eliminating irrelevant variables with Boruta. Then, we further refine this subset using LIME to assess the local importance of each feature. Finally, we determine the optimal number of features by evaluating classification performance-selecting the subset that yields the highest accuracy for model training. In general, our model is both efficient and robust, relying only on the most informative features for gene expression classification."}, {"title": "4 Experiment and results", "content": "This section offers a brief description of the gene expression datasets while delivering a detailed comparative analysis of the classification models. Further-"}, {"title": "4.1 Dataset and Configurations", "content": "The gene expression datasets summarized in exemplify the inherent chal- lenges of high-dimensional biomedical data. With sample sizes ranging from 53 to 575 and feature counts spanning from approximately 11,950 to over 54,600, these datasets present a significant imbalance between the number of available samples and the vast dimensionality of gene expression profiles. Additionally, the variability in the number of classes from as few as 3 to as many as 14-further complicates the classification task by introducing diverse and complex biological signals. This high dimensionality coupled with limited sample sizes accentuates the risk of overfitting and underscores the critical need for effective feature selection. Robust feature selection methods are essential to isolate the most informative genes, thereby enhancing model interpretability and predictive accuracy in gene expression classification. In this study, our feature selection strategy is specifically designed to address these challenges, ensuring that only the most relevant features are retained for subsequent classification tasks."}, {"title": "4.2 Boruta's Feature Selection", "content": "In this study, we adopt Boruta as our primary feature selection method to substantially reduce the dimensionality of our gene expression datasets before applying LIME for model interpretation. The rationale behind this choice is to prevent an explosion in computational complexity and potential loss of interpretability when LIME is applied to an excessively high-dimensional feature space. By using Boruta, we are able to effectively eliminate irrelevant features while retaining those that are truly informative for classification. As evidenced in datasets such as E-GEOD-20685 were reduced from 54,627 dimensions to only 545 confirmed features, thereby rendering the subsequent LIME analysis both feasible and efficient. To achieve this, Boruta was executed with the following parameters via the BorutaPy library: a random forest classifier ('rf') with 300 estimators, a maximum of 200 iterations, an alpha value of '0.01', a percentile threshold of 95, two-step feature selection enabled, and a fixed random state of 42, with verbose output enabled. These parameter settings were meticulously chosen to ensure a rigorous and robust selection process, ultimately facilitating a more interpretable and high-performing classification model."}, {"title": "4.3 Classification results", "content": "In our study, we conducted extensive experiments with four advanced algorithms to determine the method yielding the highest accuracy across various gene expression datasets. We configured an SVM using SVC with a radial basis function"}, {"title": "5 Conclusion and future work", "content": "In conclusion, our study introduces BOLIMES, a novel feature selection algo- rithm that integrates the robustness of Boruta with the interpretability of LIME to tackle the challenges inherent in gene expression classification. By systemati- cally filtering out non-informative genes and ranking the remaining ones based on their local importance, BOLIMES effectively reduces the dimensionality of genomic data while mitigating the risk of overfitting. The iterative evaluation process further refines the feature subset by identifying the optimal number of genes that maximize predictive accuracy, thereby ensuring both high classifica- tion performance and enhanced interpretability.\nLooking ahead, several promising avenues exist for further advancement of this work. Future research could explore the incorporation of additional inter- pretability methods alongside LIME to offer a more comprehensive assessment of feature relevance. Moreover, extending BOLIMES to accommodate multi-omics data, such as proteomic and metabolomic profiles, may broaden its applicability in complex biological analyses. Integrating deep learning-based classifiers could also be investigated to further boost performance in intricate gene expression scenarios. Finally, validating the approach on larger, more diverse datasets and real-world clinical samples will be crucial to establish its robustness and gener- alizability in practical settings."}]}