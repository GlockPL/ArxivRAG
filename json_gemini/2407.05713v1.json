{"title": "Short-term Object Interaction Anticipation with Disentangled Object Detection\n@ Ego4D Short Term Object Interaction Anticipation Challenge", "authors": ["Hyunjin Cho", "Dong Un Kang", "Se Young Chun"], "abstract": "Short-term object interaction anticipation is an impor-\ntant task in egocentric video analysis, including precise\npredictions of future interactions and their timings as well\nas the categories and positions of the involved active ob-\njects. To alleviate the complexity of this task, our pro-\nposed method, SOIA-DOD, effectively decompose it into 1)\ndetecting active object and 2) classifying interaction and\npredicting their timing. Our method first detects all po-\ntential active objects in the last frame of egocentric video\nby fine-tuning a pre-trained YOLOv9. Then, we combine\nthese potential active objects as query with transformer\nencoder, thereby identifying the most promising next ac-\ntive object and predicting its future interaction and time-to-\ncontact. Experimental results demonstrate that our method\noutperforms state-of-the-art models on the challenge test\nset, achieving the best performance in predicting next ac-\ntive objects and their interactions. Finally, our proposed\nranked the third overall top-5 mAP when including time-\nto-contact predictions. The source code is available at\nhttps://github.com/KeenyJin/SOIA-DOD.", "sections": [{"title": "1. Introduction", "content": "Short-term object interaction anticipation is a complex com-\nputer vision task that aims to predict human interactions\nwith objects in the near future and estimate the timing of\nthese interactions from an egocentric perspective (i.e., the\nviewpoint of the person). The Ego4D dataset [2] is a col-\nlection of massive and diverse egocentric videos to provide\nseveral benchmark challenges for understanding the first-\nperson visual experience in the past, present and future.\nHere, we focus on the short-term anticipation task, specifi-\ncally forecasting future human-object interaction. Success-\nfully predicting the next human actions would enable robots\nand augmented reality systems to better assist and protect\nhumans. However, the requirement for accurate forecast-"}, {"title": "2. Method", "content": "In this section, we introduce our proposed SOIA-DOD,\nwhich anticipates future object interaction from an egocen-\ntric video. Our method consists of two cascaded stages: 1)\nDetection of potential active objects and 2) Selection of the\nmost likely active object, along with predicting each action\nand its time-to-contact. The overall process of our SOIA-\nDOD is visualized in the Figure. 1."}, {"title": "2.1. Potential Active Objects Detection", "content": "The first stage of our SOIA-DOD aims to detect poten-\ntial active object present in the image. Using the Ego4D\ndataset, we fine-tune a pre-trained real-time object detector\nYOLOv9 [8] to estimate the location and class of next ac-\ntive object. Taking the last frame of given egocentric video,\nthe fine-tuned YOLOv9 [8], which is an active object de-\ntection model, outputs bounding box $b_i$, class label $c_i$, and\ndetection score $\\sigma_i$ for all potential active objects ${0_i}^N_{i=1}$.\n$N$ denotes the number of predictions.\nBefore proceeding to the next stage, we select top-k ac-\ntive object candidates based on the detection score $\\sigma_i$. We\nthen encode the bounding boxes $b_i$ and class names $c_i$ of\nthese selected objects separately. The bounding boxes are\nencoded by single linear layer ($B \\in R^{k\\times d}$), while the class\nnames are encoded by embedding layer ($C \\in R^{k\\times d}$) where"}, {"title": "2.2. Anticipating Short-term Object Interaction\nwith Transformer", "content": "In the second stage, we identify the most likely next active\nobject along with its interaction and time-to-contact pre-\ndictions by combining the visual features and active object\nquery $Q$ from the last egocentric frame. For the visual fea-\nture extraction, we use the pre-trained CLIP visual encoder\n(ViT-L/14@336px) [5]. As demonstrated in [3], we ex-\ntract the grid feature before the last transformer layer taking\nthe last egocentric frame for improved visual understand-\ning. These visual features are projected through a linear\nlayer, forming a visual token for each patch, as denoted as\n$V\\in R^{T\\times d}$ where $T$ refers to the number of CLIP visual to-\nkens. Then, we concatenates the active object query $Q$ from\nthe first stage and the visual features $V$ for constructing the\nintegrated active object query $Q' \\in R^{(k+T)\\times d}$.\nNext, we utilize a standard transformer encoder to fuse\nthe visual features and potential active objects features. The\ninitial query $Q'$ is projected to the layers of transformer\nencoder. In each layer, the query is updated through the\nself-attention mechanism, applying layer normalization be-"}, {"title": "2.3. Training Objective", "content": "We first fine-tune the pre-trained object detector and then\ntrain the rest of our model while freezing the weight of ob-\nject detector. For fine-tuning, we follow the training objec-\ntive outlined in [8] to detect potential active objects. For\ntraining the rest of the model, we use three types of losses:\na binary cross entropy $\\mathcal{L}_{obj}$ for future active object, a cross\nentropy $\\mathcal{L}_{int}$ for interaction prediction, and a smooth L1 loss\n$\\mathcal{L}_{ttc}$ for time-to-contact regression.\n$\\mathcal{L}_{total} = \\lambda_1 \\mathcal{L}_{obj} + \\lambda_2 \\mathcal{L}_{int} + \\lambda_3 \\mathcal{L}_{ttc}$ (1)\nWe also incorporate auxiliary layer losses during train-\ning, inspired by [1]. These losses provide additional super-\nvision for the model. The loss $\\mathcal{L}_{total}$ is calculated for each\noutput from the second-to-last layer of the transformer."}, {"title": "3. Experiments", "content": "We conduct the quantitative experiments on Ego4D Short-\nterm Interaction Anticipation benchmark to demonstrate the\neffectiveness of our proposed SOIA-DOD. While ranking third\noverall, it outperforms other methods in noun prediction by\n1.39% and in combined noun and verb prediction by 0.35%. In Table 2, we investigated the impact of the number of ob-\nject candidates used to construct active object query $Q$ dur-\ning training. Using the top-10 candidates yielded the best\noverall prediction performance during training. We also vi-\nsualize the success and failure cases of our proposed SOIA-\nDOD in Figure. 2. While SOIA-DOD demonstrates accu-\nrate detection of active objects and their interactions, it often\nstruggles with precisely estimating time-to-contact."}, {"title": "3.1. Implementation Details", "content": "We use version 2.0 of the Ego4D dataset for training and\nvalidation. We employ YOLOv9-E as the object detector\nand utilize the image encoder of CLIP ViT-L/14@336px for\nvisual feature extraction. The input image is resized to a"}, {"title": "3.3. Limitations", "content": "While our proposed SOIA-DOD excels in predicting the\nnext active objects and their interactions, it exhibits lower\naccuracy in forecasting time-to-contact compared to other\nstate-of-the-art methods. This limitation likely stems from\nthe model's reliance solely on last egocentric frame as in-\nput, consequently hindering its ability to leverage temporal\ninformation for precise time-to-contact prediction."}, {"title": "4. Conclusion", "content": "We present a novel cascaded approach, SOIA-DOD, for\nshort-term object interaction anticipation in egocentric\nvideos. SOIA-DOD disentangles active object detection\nfrom the prediction of its human interaction and time-to-\ncontact. By avoiding the challenges of simultaneously pro-\ncessing very different tasks, our method achieves accurate\nlocalized active object detection. In the short-term anticipa-\ntion in Ego4D challenge, we demonstrates that SOIA-DOD\noutperforms state-of-the-art methods in predicting next ac-\ntive objects and their interactions, though it ranks third\nwhen including time-to-contact. Future work should focus\non integrating more temporal information from past frames\nto enhance time-to-contact predictions."}]}