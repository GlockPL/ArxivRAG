{"title": "Short-term Object Interaction Anticipation with Disentangled Object Detection @ Ego4D Short Term Object Interaction Anticipation Challenge", "authors": ["Hyunjin Cho", "Dong Un Kang", "Se Young Chun"], "abstract": "Short-term object interaction anticipation is an important task in egocentric video analysis, including precise predictions of future interactions and their timings as well as the categories and positions of the involved active objects. To alleviate the complexity of this task, our proposed method, SOIA-DOD, effectively decompose it into 1) detecting active object and 2) classifying interaction and predicting their timing. Our method first detects all potential active objects in the last frame of egocentric video by fine-tuning a pre-trained YOLOv9. Then, we combine these potential active objects as query with transformer encoder, thereby identifying the most promising next active object and predicting its future interaction and time-to-contact. Experimental results demonstrate that our method outperforms state-of-the-art models on the challenge test set, achieving the best performance in predicting next active objects and their interactions. Finally, our proposed ranked the third overall top-5 mAP when including time-to-contact predictions.", "sections": [{"title": "1. Introduction", "content": "Short-term object interaction anticipation is a complex computer vision task that aims to predict human interactions with objects in the near future and estimate the timing of these interactions from an egocentric perspective (i.e., the viewpoint of the person). The Ego4D dataset [2] is a collection of massive and diverse egocentric videos to provide several benchmark challenges for understanding the first-person visual experience in the past, present and future. Here, we focus on the short-term anticipation task, specifically forecasting future human-object interaction. Successfully predicting the next human actions would enable robots and augmented reality systems to better assist and protect humans. However, the requirement for accurate forecasting in both spatial (location) and temporal (timing) domains makes short-term object interaction anticipation particularly challenging.\nRecently, several works have explored anticipating human behavior in the most recent frame of the video clip using Ego4D dataset [2]. StillFast [6] introduced an end-to-end model that utilizes a two-branch architecture to process the high-resolution last observed frame and the low-resolution past frames simultaneously, fuses the spatial and temporal features, and then predicts the next active objects, actions, and time-to-contact. GANOv2 [7], following the two-branch architecture of StillFast, employs a guided-attention mechanism to fuse spatial and temporal features and object detections from the video. TransFusion [4] proposed multimodal fusion, integrating language summaries of the video with the visual features of the last observed frame. While existing methods focused on simultaneously handling with spatial and temporal features for predicting the objects, actions, and time together, effectively training a model for these different multi-task learning aspects remains challenging, particularly for next active object detection, interaction classification, and timing prediction in the near future.\nIn this work, we propose a Short-term Object Interaction Anticipation with Disentangled Object Detection (SOIA-DOD) to address the challenge of multi-task learning by decomposing the task into two cascaded stages: active object detection, and prediction of interaction and its occurrence time. This approach alleviates the difficulty of learning both tasks simultaneously, and enables accurate local-izing the potential active objects. The detected results of these objects are then used as initial queries and combined with a transformer encoder to select the most likely next active object, classify the interaction, and predict its timing. In the Ego4D Short-Term Object Interaction Anticipation challenge, we demonstrate the effectiveness of our proposed SOIA-DOD, achieving state-of-the-art performance in predicting the next active objects and their interactions. Furthermore, it achieved the third place in the overall top-5 mean average precision (mAP) including time-to-contact."}, {"title": "2. Method", "content": "In this section, we introduce our proposed SOIA-DOD, which anticipates future object interaction from an egocentric video. Our method consists of two cascaded stages: 1) Detection of potential active objects and 2) Selection of the most likely active object, along with predicting each action and its time-to-contact. The overall process of our SOIA-DOD is visualized in the Figure. 1."}, {"title": "2.1. Potential Active Objects Detection", "content": "The first stage of our SOIA-DOD aims to detect potential active object present in the image. Using the Ego4D dataset, we fine-tune a pre-trained real-time object detector YOLOv9 [8] to estimate the location and class of next active object. Taking the last frame of given egocentric video, the fine-tuned YOLOv9 [8], which is an active object detection model, outputs bounding box $b_i$, class label $c_i$, and detection score $\\sigma_i$ for all potential active objects $\\{o_i\\}_{i=1}^N$ denotes the number of predictions.\nBefore proceeding to the next stage, we select top-k active object candidates based on the detection score $\\sigma_i$. We then encode the bounding boxes $b_i$ and class names $c_i$ of these selected objects separately. The bounding boxes are encoded by single linear layer ($B \\in \\mathbb{R}^{k\\times d}$), while the class names are encoded by embedding layer ($C \\in \\mathbb{R}^{k\\times d}$) where d denotes the embedding dimension. Lastly, we construct the active object query $Q \\in \\mathbb{R}^{k\\times d}$ by summing the embeddings of bounding box $B$ and class names $C$."}, {"title": "2.2. Anticipating Short-term Object Interaction with Transformer", "content": "In the second stage, we identify the most likely next active object along with its interaction and time-to-contact predictions by combining the visual features and active object query Q from the last egocentric frame. For the visual feature extraction, we use the pre-trained CLIP visual encoder (ViT-L/14@336px) [5]. As demonstrated in [3], we extract the grid feature before the last transformer layer taking the last egocentric frame for improved visual understanding. These visual features are projected through a linear layer, forming a visual token for each patch, as denoted as $V\\in \\mathbb{R}^{T\\times d}$ where T refers to the number of CLIP visual tokens. Then, we concatenates the active object query Q from the first stage and the visual features V for constructing the integrated active object query $Q' \\in \\mathbb{R}^{(k+T)\\times d}$.\nNext, we utilize a standard transformer encoder to fuse the visual features and potential active objects features. The initial query $Q'$ is projected to the layers of transformer encoder. In each layer, the query is updated through the self-attention mechanism, applying layer normalization be-"}, {"title": "2.3. Training Objective", "content": "We first fine-tune the pre-trained object detector and then train the rest of our model while freezing the weight of object detector. For fine-tuning, we follow the training objective outlined in [8] to detect potential active objects. For training the rest of the model, we use three types of losses: a binary cross entropy $\\mathcal{L}_{obj}$ for future active object, a cross entropy $\\mathcal{L}_{int}$ for interaction prediction, and a smooth L1 loss $\\mathcal{L}_{ttc}$ for time-to-contact regression.\n$\\mathcal{L}_{total} = \\lambda_1 \\mathcal{L}_{obj} + \\lambda_2 \\mathcal{L}_{int} + \\lambda_3 \\mathcal{L}_{ttc}$ (1)\nWe also incorporate auxiliary layer losses during training, inspired by [1]. These losses provide additional supervision for the model. The loss $\\mathcal{L}_{total}$ is calculated for each output from the second-to-last layer of the transformer."}, {"title": "3. Experiments", "content": "We conduct the quantitative experiments on Ego4D Short-term Interaction Anticipation benchmark to demonstrate the effectiveness of our proposed SOIA-DOD. While ranking third overall, it outperforms other methods in noun prediction by 1.39% and in combined noun and verb prediction by 0.35%. In Table 2, we investigated the impact of the number of object candidates used to construct active object query Q during training. Using the top-10 candidates yielded the best overall prediction performance during training. We also visualize the success and failure cases of our proposed SOIA-DOD While SOIA-DOD demonstrates accurate detection of active objects and their interactions, it often struggles with precisely estimating time-to-contact."}, {"title": "3.3. Limitations", "content": "While our proposed SOIA-DOD excels in predicting the next active objects and their interactions, it exhibits lower accuracy in forecasting time-to-contact compared to other state-of-the-art methods. This limitation likely stems from the model's reliance solely on last egocentric frame as input, consequently hindering its ability to leverage temporal information for precise time-to-contact prediction."}, {"title": "4. Conclusion", "content": "We present a novel cascaded approach, SOIA-DOD, for short-term object interaction anticipation in egocentric videos. SOIA-DOD disentangles active object detection from the prediction of its human interaction and time-to-contact. By avoiding the challenges of simultaneously processing very different tasks, our method achieves accurate localized active object detection. In the short-term anticipation in Ego4D challenge, we demonstrates that SOIA-DOD outperforms state-of-the-art methods in predicting next active objects and their interactions, though it ranks third when including time-to-contact. Future work should focus on integrating more temporal information from past frames to enhance time-to-contact predictions."}]}