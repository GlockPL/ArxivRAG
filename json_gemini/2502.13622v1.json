{"title": "REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large Language Models", "authors": ["DongGeon Lee", "Hwanjo Yu"], "abstract": "Hallucinations in large language model (LLM) outputs severely limit their reliability in knowledge-intensive tasks such as question answering. To address this challenge, we introduce REFIND (Retrieval-augmented Factuality hallucINation Detection), a novel framework that detects hallucinated spans within LLM outputs by directly leveraging retrieved documents. As part of the REFIND, we propose the Context Sensitivity Ratio (CSR), a novel metric that quantifies the sensitivity of LLM outputs to retrieved evidence. This innovative approach enables REFIND to efficiently and accurately detect hallucinations, setting it apart from existing methods. In the evaluation, REFIND demonstrated robustness across nine languages, including low-resource settings, and significantly outperformed baseline models, achieving superior IoU scores in identifying hallucinated spans. This work highlights the effectiveness of quantifying context sensitivity for hallucination detection, thereby paving the way for more reliable and trustworthy LLM applications across diverse languages.", "sections": [{"title": "Introduction", "content": "Detecting hallucinated information in responses generated by large language models (LLMs) has emerged as a critical challenge in the field of natural language generation (Ji et al., 2023; Zhang et al., 2023). Hallucination, in this context, refers to the generation of content that is factually incorrect or lacks grounding in verifiable sources (Li et al., 2024). This issue is particularly pronounced in knowledge-intensive tasks that demand high factual accuracy, such as question answering (Lee et al., 2022; Sun et al., 2024). The consequences of unmitigated hallucination are significant, ranging from the propagation of misinformation to a decline in trust in AI systems, underscoring the need for effective hallucination detection for the development of safe and trustworthy AI.\nPrior research has explored various approaches for hallucination detection. Token-level classifiers, for example, leveraging pre-trained language models like ROBERTa (Liu et al., 2019), have been employed for binary classification, labeling individual tokens as either factual or hallucinated (Liu et al., 2022). However, these models often exhibit limitations when applied to low-resource languages and tend to rely heavily on internal knowledge without effectively utilizing external evidence, which can hinder their performance. Extrinsic methods, such as retrieval-augmented models, aim to mitigate hallucinations by integrating external knowledge. Nevertheless, existing retrieval-augmented approaches, such as FAVA (Mishra et al., 2024), can potentially lead to inaccuracies in aligning the modified responses with the original LLM output, due to their multi-step processes involving retrieval, comparison, and editing.\nTo address these limitations, we introduce REFIND (REtrieval-augmented Factuality hallucINation Detection), a novel framework specifically designed to identify hallucinated spans within LLM-generated text. REFIND achieves this by quantifying the context sensitivity of each token at the token level. By leveraging retrieved documents, REFIND calculates a Context Sensitivity Ratio (CSR) for each token in the LLM's response, measuring the token's dependence on external contextual information. Tokens exhibiting high CSR values are identified as likely hallucinations, offering a more direct and efficient approach to factuality verification.\nOur contributions can be summarized as follows:\n\u2022 We present REFIND, a novel framework for detecting hallucinated spans in LLM responses by leveraging an external retriever and calculating the CSR at the token level.\n\u2022 We conduct a comprehensive evaluation of REFIND using the SemEval 2025 Task 3: Mu-SHROOM dataset (V\u00e1zquez et al., 2025), a multilingual benchmark for detecting hallucinated spans. REFIND is rigorously tested across nine diverse languages \u2013 Arabic, Czech, German, Spanish, Basque, Finnish, French, Italian, and English - demonstrating its robustness in both high- and low-resource settings.\n\u2022 Experimental results demonstrate that REFIND significantly outperforms baseline models such as token-level classifiers and FAVA, achieving superior Intersection-over-Union (IoU) scores. This highlights the efficacy of the CSR in accurately identifying hallucinated content."}, {"title": "Related Work", "content": "Detection of Hallucinated Responses Several studies have proposed methods to detect whether a response contains hallucinated information. Farquhar et al. (2024); Han et al. (2024); Arteaga et al. (2025) leveraged semantic entropy (Kuhn et al., 2023) to estimate uncertainty and identify hallucinations. These approaches utilize entropy-based metrics to assess the reliability of generated responses. SelfCheckGPT (Manakul et al., 2023) introduces a method that employs the language model itself to sample multiple responses and detect inconsistencies among them, thus identifying hallucinated outputs. However, this method relies solely on the internal knowledge of the language model, making it less effective when the model's knowledge is limited or incomplete.\nDetection of Hallucinated Spans Beyond identifying whether a response is hallucinated, other works aim to detect specific spans of hallucinated content within a response of LLMs. Token-level classification approaches (Liu et al., 2022) utilized pre-trained language models to classify individual tokens as factual or hallucinated. These methods focus on analyzing attention patterns, demonstrating that query input tokens (defined as constraint tokens) exhibit strong correlations with factual answer tokens (Yuksekgonul et al., 2024).\nFAVA (Mishra et al., 2024) proposes a retrieval-augmented pipeline that integrates retrieval, comparison, and editing steps to identify and correct hallucinated spans. While effective, the multi-step process introduces complexity and alignment challenges, particularly in ensuring that the corrected responses remain consistent with the semantics of the original output."}, {"title": "Method", "content": "The SemEval 2025 Task 3: Mu-SHROOM (V\u00e1zquez et al., 2025) focuses on detecting hallucinated spans in responses generated by LLMs. Given an input question q and its corresponding LLM-generated response (along with the model's identifier), the goal is to identify spans in the response that are hallucinated. Details of the Mu-SHROOM dataset are provided in Section 4.1."}, {"title": "Retrieval-Augmented Factuality Hallucination Detection", "content": "To address the challenge of factual hallucination detection in LLM outputs, we introduce REFIND (REtrieval-augmented Factuality hallucINation Detection). The overall workflow of the REFIND method is illustrated in Figure 1. REFIND leverages external knowledge retrieved from a relevant document set to assess the context sensitivity of each generated token.\nThe core principle behind REFIND is to quantify the influence of external context on the token generation process. We do this by measuring the change in the conditional probability of generating a token as information from retrieved documents is incorporated. This change is captured by the Context Sensitivity Ratio (CSR). It quantifies the degree to which the conditional probability of generating a token is altered by the inclusion of external contextual information from retrieved documents.\nLet $M_\\theta$ denote an LLM parameterized by $\\theta$, q represent the input question, and $t_i$ denote the i-th token in the LLM's response to q. We use $p_\\theta(t_i | \\cdot)$ to represent the probability of generating token $t_i$ given the input. Furthermore, let R be a retriever that provides relevant documents based on q, and let $D = R(q)$ be the set of retrieved documents. The CSR for each token $t_i$ is defined as:\n$CSR(t_i) = \\frac{\\log p_\\theta (t_i | D, q, t_{<i})}{\\log p_\\theta(t_i | q, t_{<i}) + \\varepsilon}$\nwhere $t_{<i}$ represents the sequence of preceding tokens. The numerator computes the log-probability of generating $t_i$ conditioned on the question q, the preceding tokens $t_{<i}$, and the retrieved document set D. The denominator computes the log-probability of generating $t_i$ conditioned solely on the question q and preceding tokens $t_{<i}$, excluding the retrieved documents.\nBy comparing these two probabilities, the CSR effectively quantifies the sensitivity of $t_i$ to the external context provided by the D. A higher CSR indicates a stronger influence of the retrieved context on the generation of the token.\nFinally, to determine whether a token is a hallucination, we compare its CSR value to a predefined threshold, denoted as $\\delta$. If the CSR value for the given token $t_i$ is greater than or equal to the threshold $\\delta$, we classify that the token as a hallucination. Conversely, if the CSR value is less than $\\delta$, the token is not considered a hallucination. This threshold $\\delta$ serves as a hyperparameter that can be tuned to optimize the balance between precision and recall in hallucination detection."}, {"title": "Experimental Setup", "content": "We conduct our experiments on the Mu-SHROOM dataset (V\u00e1zquez et al., 2025), which consists of outputs generated by various LLMs in response to specific input questions. Each output is annotated by human annotators to identify spans that correspond to hallucinations.\nThe dataset includes multiple languages, and for our study, we focus on the following nine languages: Arabic (AR), Czech (CS), German (DE), English (EN), Spanish (ES), Basque (EU), Finnish (FI), French (FR), and Italian (IT). This multilingual diversity enables a comprehensive evaluation of our method across diverse linguistic contexts.\nEach data point in the dataset contains the language identifier, the input question posed to the LLM, the model name, the generated output text, and its token-level probabilities. Additionally, binary annotations specify the start and end indices of hallucinated spans, marking each such span as a hallucination."}, {"title": "Evaluation Metric", "content": "To evaluate the performance of our hallucination detection method, we adopt the IoU metric, a standard measure for span-based evaluation.\nGiven the set of character indices predicted as hallucinations, $H_{pred}$, and the set of character indices labeled as hallucinations in the gold reference, $H_{gold}$, the IoU is calculated as:\n$IoU = \\frac{|H_{pred} \\cap H_{gold}|}{|H_{pred} \\cup H_{gold}|}$\nThis metric quantifies the overlap between the predicted and ground truth hallucinated spans. To handle cases where both $H_{pred}$ and $H_{gold}$ are empty (i.e., no hallucinations are present in either prediction or reference), we define IoU = 1.0 to signify perfect agreement."}, {"title": "Baseline Models", "content": "We employ a token-level hallucination classifier (Liu et al., 2022) based on XLM-ROBERTa (XLMR) (Conneau et al., 2020), a multilingual transformer model. The model is fine-tuned to perform binary classification at the token level, where each token is labeled as either hallucinated or non-hallucinated."}, {"title": "Result and Analysis", "content": "Table 1 presents the evaluation results of our proposed method, alongside the baseline models, XLM-R and FAVA, on the Mu-SHROOM dataset. The results are reported across nine languages (AR, CS, DE, EN, ES, EU, FI, FR, IT) and averaged to provide an overall assessment of performance.\nREFIND outperforms the baseline models in terms of average IoU scores. The improvements are particularly notable in low-resource languages such as Arabic, Finnish, and French, where REFIND achieves IoU scores of 0.3743, 0.5061, and 0.4734, respectively, compared to significantly lower scores from the baselines. This indicates that REFIND effectively leverages retrieval-augmented information to enhance hallucination detection in diverse linguistic settings."}, {"title": "Baseline Comparison", "content": "The XLM-R-based token classifier performs poorly on average, with an IoU of 0.0345. Its reliance solely on intrinsic model knowledge without leveraging external context limits its ability to identify hallucinated spans accurately, particularly in low-resource languages.\nFAVA exhibits better performance than XLM-R, with an average IoU of 0.2787. This improvement can be attributed to its use of retrieval-augmented information for detecting and editing hallucinated text. However, FAVA's two-step process introduces complexity and potential inaccuracies in aligning the edited text with the original output.\nREFIND outperforms both baselines with an average IoU of 0.3633, highlighting its superior ability to integrate retrieved context directly into the token generation process for hallucination detection. This streamlined approach ensures accurate and efficient identification of hallucinated spans."}, {"title": "Analysis of Multilingual Performance", "content": "REFIND demonstrates robust performance across both high-resource and low-resource languages. This indicates the generalizability of its retrieval-augmented approach to varying linguistic contexts. Notably, performance varies considerably across languages for all methods; for instance, XLM-R and FAVA struggle significantly with low-resource languages like Arabic, Finnish, and French. In contrast, REFIND's integration of external retrieval with the LLM's internal knowledge helps mitigate performance drops in these settings."}, {"title": "Analysis of Threshold Sensitivity", "content": "Figure 3 illustrates the performance of REFIND across varying threshold values (0.1-0.4) for nine languages. Most languages exhibit consistent IoU scores, indicating robustness to threshold changes. High-resource languages like English and German maintain stable scores around 0.35, while low-resource languages such as Arabic and Finnish show slightly larger variations, especially at lower thresholds. This suggests that the choice of threshold may have a more significant impact on low-resource languages, potentially due to their inherent linguistic challenges and data scarcity. Overall, these findings emphasize REFIND's ability to maintain reliable performance across a range of threshold values while highlighting potential areas for optimization in low-resource scenarios."}, {"title": "Case Study", "content": "Figure 2 illustrates REFIND's ability to detect hallucinations by utilizing retrieved evidence. The question asks about Chance the Rapper's debut year. The LLM's output contains a hallucinated span (\"2011\"), which is inconsistent with the retrieved documents. By comparing the generated output with external knowledge, REFIND effectively identifies spans that deviate from factual information."}, {"title": "Conclusion", "content": "In this study, we introduced REFIND, a novel framework for detecting hallucinated spans in LLM-generated outputs by leveraging retrieved documents to compute the Context Sensitivity Ratio (CSR) at the token level. REFIND was rigorously evaluated on the multilingual SemEval 2025 Task 3: Mu-SHROOM dataset, demonstrating superior performance across nine languages, including low-resource settings, compared to baseline approaches. By directly integrating retrieved context into the token probability calculation, REFIND effectively identifies hallucinated spans with greater precision and efficiency.\nOur experimental results highlight the robustness and scalability of REFIND in multilingual environments, offering a promising solution for enhancing the factuality of LLM outputs. Moreover, the streamlined detection process avoids the complexities associated with multi-step frameworks, enabling practical deployment in real-world applications.\nFor future work, we aim to extend REFIND by exploring adaptive thresholding mechanisms to further optimize the balance between precision and recall in hallucination detection."}, {"title": "Limitations", "content": "While REFIND achieves notable improvements in hallucination detection, there are limitations to consider. First, the reliance on retrieved documents means that the quality of the retriever directly impacts performance. Errors in retrieval or limited availability of relevant documents may lead to suboptimal CSR calculations and misclassification of hallucinated spans. Second, the approach involves computational overhead associated with calculating token probabilities with and without retrieved context, which could pose challenges in low-latency applications. Lastly, REFIND focuses on detecting factual hallucinations, and its performance in non-factoid question answering (Bolotova et al., 2022) remains unexplored. Further studies are needed to assess its ability to detect hallucinations in non-factoid QA tasks."}, {"title": "Implementation Details", "content": "All experiments are conducted using NVIDIA A100 80GB GPUs.\nFor training the XLM-R-based (Conneau et al., 2020) system, we leverage the Trainer from the Hugging Face Transformers library (Wolf et al., 2020). We train the model using token-aligned hallucination annotations from our dataset, with the model parameters optimized using cross-entropy loss and AdamW optimizer with a learning rate of 2e-5 for 5 epochs.\nInference for FAVA (Mishra et al., 2024) is conducted using vLLM (Kwon et al., 2023), adhering to the original settings with temperature=0, top_p=1.0, and max_tokens=1024. The prompt template used for FAVA inference is detailed in Figure 5 (Appendix A.1)."}]}