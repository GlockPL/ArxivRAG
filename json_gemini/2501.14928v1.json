{"title": "Decision Making in Changing Environments: Robustness, Query-Based Learning, and Differential Privacy", "authors": ["Fan Chen", "Alexander Rakhlin"], "abstract": "We study the problem of interactive decision making in which the underlying environment changes over time subject to given constraints. We propose a framework, which we call hybrid Decision Making with Structured Observations (hybrid DMSO), that provides an interpolation between the stochastic and adversarial settings of decision making. Within this framework, we can analyze local differentially private (LDP) decision making, query-based learning (in particular, SQ learning), and robust and smooth decision making under the same umbrella, deriving upper and lower bounds based on variants of the Decision-Estimation Coefficient (DEC). We further establish strong connections between the DEC's behavior, the SQ dimension, local minimax complexity, learnability, and joint differential privacy. To showcase the framework's power, we provide new results for contextual bandits under the LDP constraint.", "sections": [{"title": "1 Introduction", "content": "The Decision-Estimation Coefficient (DEC) [Foster et al., 2021, 2023b] has been recently shown to capture the difficulty of exploration in a wide range of problems in which a learning agent interacts with an unknown environment by making decisions and observing outcomes. Such problems include structured bandits, contextual bandits, and reinforcement learning, among others. The interaction protocol, termed Decision Making with Structured Observations (DMSO) in [Foster et al., 2021], assumes that the unknown model is fixed over the length of the interaction, i.e. the learning agent faces a stationary environment. This is often referred to as a stochastic setting, or stochastic DMSO. In contrast, the adversarial DMSO, studied in [Foster et al., 2022b], is a more complex task where the model may change arbitrarily between the rounds of the interaction.\nIn this paper, we study a setting that interpolates between the stochastic and adversarial DMSO. This interpolation is achieved by placing constraints on the way the model may change over time. Within the constraint set, the model is allowed to change arbitrarily, and we refer to the setting as that of constrained adversaries, or hybrid DMSO. In parallel with such constraints on the adversary, we additionally study constraints placed on the information received by the decision-maker, for instance due to privacy requirements or a specific oracle model of computation. The specification of constraints allows us to study-under the same umbrella-decision making with Statistical Queries (SQ) [Kearns, 1998], local differential privacy (LDP) [Kasiviswanathan et al., 2011, Duchi et al., 2013], robustness with respect to model corruption [Huber, 1965, Huber and Ronchetti, 2011], and smooth decision making [Rakhlin et al., 2011]. For example, in SQ learning, the decision-maker obtains information by issuing queries; since the response to these queries is only approximately"}, {"title": "1.1 Contributions", "content": "We formulate decision making in the setting of hybrid DMSO, generalizing the Decision-Estimation Coefficient framework [Foster et al., 2021, 2023b]. Our proposed notion of hybrid DEC allows us to understand, under the same umbrella, minimax behavior of statistical estimation and interac- tive decision making under such seemingly different settings as local differential privacy, query- based learning (in particular, statistical queries), robust learning, and smoothness. In particular, hybrid DECs for PAC learning and no-regret learning yield both lower and upper bounds for the corresponding learning goals. Our upper bounds are achieved by the unified Exploration-by- Optimization Algorithm (ExO+, cf. Lattimore and Szepesv\u00e1ri [2020], Lattimore and Gyorgy [2021], Foster et al. [2022b]).\nAs instantiations of our framework, we derive the hybrid DECs and corresponding upper and lower bounds for query-based learning (Section 2.2), locally private learning (Section 2.3), robust decision making (Section 2.4), and decision making against smooth adversaries (Section 2.5.1). The problem of contextual bandits with adversarial contexts also naturally falls under our hybrid formulation (Section 5.5), and we provide novel results for this setting as well.\nOur primary goal is to understand the complexity of learning problems at some level of gener- ality, rather than specific examples. Still, as a concrete application, our framework provides a near-optimal VT-regret for linear contextual bandits with local privacy (without well-conditioned assumptions), settling the open problem of the optimal regret in this setting [Zheng et al., 2020, Han et al., 2021, Li et al., 2024].\nIn addition, we make the following connections to other previously studied notions:\n\u2022 SQ dimension. The SQ dimension proposed by Feldman [2017] provides both lower and upper bounds for the optimal query complexity of SQ learning of distribution search problems. Not surprisingly, we show that there is quantitative equivalence between the SQ dimension"}, {"title": "1.2 Related work", "content": "Decision-Estimation Coefficient Framework. Towards a unifying framework for interactive decision making, Foster et al. [2021] propose Decision-Estimation Coefficient (DEC), which provides both lower and upper bounds for any decision making problem. An active line of research [Foster et al., 2022b, Chen et al., 2022, Foster et al., 2023b,a, Glasgow and Rakhlin, 2023, Chen et al., 2024] has extended the DEC framework to various more general learning goals, including adversarial decision making [Foster et al., 2022b], PAC decision making [Chen et al., 2022, Foster et al., 2023b], reward-free learning and preference-based learning [Chen et al., 2022], multi-agent decision making and partial monitoring [Foster et al., 2023a], and interactive estimation [Chen et al., 2022, 2024]. The present work further extends the DEC framework to handle changing environments and constraints on the decision maker, and our results heavily draw on the techniques developed in these previous papers.\nExploration-by-Optimization. The Exploration-by-Optimization technique is powerful machin- ery developed in Lattimore and Szepesv\u00e1ri [2020], Lattimore and Gyorgy [2021] for partial moni- toring in adversarial environments and later extended by Foster et al. [2022b] to decision making in adversarial environments, achieving upper bounds in terms of the generalized Information Ra- tio [Russo and Van Roy, 2014, 2018, Lattimore and Gyorgy, 2021] or the DEC [Foster et al., 2021, 2022b]. In the present work, we further extend this technique by incorporating the notion of in- formation sets, allowing a more granular quantification of the information and model equivalences that the decision-maker can take advantage of. The idea of using information sets in the context of posterior sampling was proposed by Dylan Foster back in 2022, and was considered by the authors of [Foster et al., 2022a] as a way of improving DEC-based results for reinforcement learning.\nLocal differential privacy. The notion of local differential privacy (LDP) was formalized by Kasiviswanathan et al. [2011], Duchi et al. [2013], with some earlier work on this subject dating back to Warner [1965]. A line of research has been investigating the statistical complexity of locally private learning for various statistical estimation problems [Duchi et al., 2013, 2018, Duchi and Rogers, 2019], including mean estimation [Asi et al., 2022, 2024], functional estimation [Rohde and Steinberger, 2020, Butucea and Issartel, 2021, Butucea et al., 2023, Duchi and Ruan, 2024], hypothesis testing [Berrett and Butucea, 2020, Li et al., 2023] and selection [Gopi et al., 2020, Pour et al., 2024], and regression [Wang and Xu, 2019, Berrett et al., 2021], to name a few. Beyond the setting of statistical estimation, recent research studies the complexity of interactive decision making with local privacy constraints, including contextual bandits [Zheng et al., 2020, Han et al.,"}, {"title": "2 Overview of Results", "content": "We start this section by formulating the hybrid DMSO framework (Section 2.1), a generalization of the Decision Making with Structured Observation (DMSO) framework proposed by Foster et al. [2021]. We then show how this generalization encompasses query-based learning (Section 2.2), locally differentially private learning (Section 2.3), and robust decision making (Section 2.4). For each setting, we formulate a corresponding variant of DMSO, the corresponding DEC, and the ensuing PAC guarantees. We also present regret guarantees for hybrid DMSO (Section 2.5), with application to smooth learning (Section 2.5.1)."}, {"title": "2.1 Hybrid DMSO", "content": "In the DMSO formulation, studied in [Foster et al., 2021], the learner (or, the decision maker) interacts for T rounds with the environment described by an underlying model M*, unknown to the learner (detailed discussion in Appendix A.1). While the DMSO formulation is general enough"}, {"title": "Learning objective", "content": "In PAC learning, the goal of the learner is to select an output decision\n$\\pi_{T+1} \\in \\Pi$ after T rounds of interaction, with the performance measured by\n$\\mathsf{Risk}_{\\mathsf{DM}}(T) := \\mathbb{E}_{\\pi_{T+1}\\sim p}[L(P^*, \\pi_{T+1})],$\nwhere $\\pi_{T+1} \\sim p$ is the randomized decision of the learner, $L : \\mathcal{P} \\times \\Pi \\rightarrow \\mathbb{R}$ is a known loss function.\nTo simplify the presentation in this section, we mainly focus on the PAC formulation, deferring the study of regret to Section 2.5. Further, we present all the results in terms of a metric-based loss function, which is specified by a certain pseudo-metric structure over the decision space $\\Pi$.\nDefinition 1 (Metric-based loss function). A loss function $L : \\mathcal{P} \\times \\Pi \\rightarrow \\mathbb{R}$ is induced by a metric (or simply metric-based) if the decision space $\\Pi$ can be equipped with a pseudo-metric $\\rho$ such that $L(P,\\pi) = \\rho(\\pi^\\circ, \\pi)$, where $P \\rightarrow \\pi^\\circ$ is a map from $\\mathcal{P}$ to $\\Pi$.\nFor many applications in statistics, the loss function is naturally metric-based, e.g., hypothesis testing and estimation [Casella and Berger, 2002]."}, {"title": "PAC hybrid DEC and guarantees", "content": "For any hybrid DMSO problem specified by the constraint\nclass $\\mathcal{P}$, we define the hybrid DEC of $\\mathcal{P}$ with respect to a reference model $\\mathcal{M}$ as\n$\\begin{aligned}\n    \\mathsf{p\\text{-}dec}(\\mathcal{P}, \\mathcal{M}) := \\left{\n\\begin{array}{c} \n        \\inf_{\\mathbb{P} \\in \\Delta(\\Pi)} \\sup_{\\mathbb{M} \\in \\mathcal{P}} \\left| \\mathbb{E}_{\\pi\\sim \\mathbb{P}}L(\\mathbb{M}, \\pi) \\right| \\\\ \n        \\Phi \\in \\Delta(\\mathbb{I}) \n\\end{array}  \\middle|  \\inf_{\\mathbb{M} \\in co(\\mathcal{P})} D_H^2 (\\mathbb{M}(\\pi), \\widetilde{\\mathbb{M}}(\\pi)) \\leq \\epsilon^2 \\right}.        \\quad (2)\n\\end{aligned}$\nand $\\mathsf{p\\text{-}dec}(\\mathcal{P}) = \\sup_{\\mathcal{M} \\in \\mathcal{M}^+} \\mathsf{p\\text{-}dec}(\\mathcal{P}, \\mathcal{M})$, where the supremum is taken over the class of reference models $\\mathcal{M}^+ := co(\\bigcup_{\\mathbb{P} \\in \\mathcal{P}} \\mathbb{P})$.\nWe now present the first result, which states that under the hybrid DMSO framework, hybrid DEC provides both lower and upper bounds for the minimax risk. The minimax risk quantifies the fundamental limit of learning, as it measures the best possible performance of an algorithm in the face of a worst-case environment constrained by $\\mathcal{P}$ (see Section 3 for details).\nTheorem 1 (PAC lower and upper bounds; Informal). Let $T > 1$, and $L$ be metric-like. Under mild growth assumption, the following holds:\n$\\mathsf{p\\text{-}dec}^{(\\epsilon,\\epsilon')}(\\mathcal{P}) \\lesssim \\inf_{\\mathsf{Alg}} \\sup_{\\mathsf{Env}} \\mathbb{E}_{\\mathsf{Env}, \\mathsf{Als}} [\\mathsf{Risk}_{\\mathsf{DM}}(T)] \\lesssim \\mathsf{p\\text{-}dec}^{(\\epsilon',\\epsilon)}( \\mathcal{P}),$\nwhere $\\inf_{\\mathsf{Alg}}$ is taken over all T-round algorithms Alg, $\\sup_{\\mathsf{Env}}$ is taken over all environments Env constrained by $\\mathcal{P}$, $\\epsilon(T) = \\frac{1}{\\sqrt{T}}$, $\\epsilon'(T) = \\sqrt{\\frac{\\log |\\mathcal{P}|}{T}}$ and we omit poly-logarithmic factors.\nWe note that the lower bound applies to the stationary adversaries, while the upper bound (achieved by ExO+) applies to arbitrary (adaptive) adversarial environments.\nLet us now discuss the qualitative behavior of $\\mathsf{p\\text{-}dec}(\\mathcal{P})$ with respect to the constraint class $\\mathcal{P}$. To start, consider stochastic DMSO, where each constraint is given by a singleton $\\mathcal{P} = {\\mathbb{M}}$. In this case, the infimum over $\\mathbb{M} \\in co(\\mathcal{P})$ disappears, recovering the definition of the original PAC DEC in Foster et al. [2023b] (see also Eq. (22)). As constraints become less stringent (informally, $\\mathcal{P}$'s become larger), the value of the DEC increases as the Hellinger-based constraint becomes easier to satisfy. Similarly, constraints on the learner are also reflected in the Hellinger term through the amount of information the measurements provide, as will be evident in the forthcoming calculations.\nIn the rest of this section, we detail how both types of constraints result in the corresponding measures of complexity and the guarantees for the settings of query-based learning (Section 2.2), locally differentially private learning (Section 2.3), and robust decision making (Section 2.4)."}, {"title": "2.2 Query-based learning", "content": "In query-based learning, the environment responds to the learner's measurements (or, queries) with answers that are close to the answer under the ground-truth model $\\mathbb{M}^* : \\Pi \\times \\Phi \\rightarrow V$, and we recall that we denote $\\mathbb{I} := \\Pi \\times \\Phi$.\nWe formulate the interaction protocol of ($\\tau$-correct) SQ DMSO as follows. For each $t = 1,\\ldots ,T$:\n\u2022\nThe learner selects a decision $\\pi_t \\in \\mathbb{I}$ and a measurement $\\phi_t \\in \\Phi$.\n\u2022\nThe environment selects (possibly adversarially) $v_t \\in V$ such that $||v_t - \\mathbb{M}^*(\\pi_t, \\phi_t)|| \\leq \\tau$ and reveals $v_t$ to the learner, where $V$ is a fixed normed vector space, and $\\tau \\geq 0$ is a known tolerance parameter.\nIn SQ DMSO, the underlying model $\\mathbb{M}^*$ is a deterministic map $\\Pi\\times \\Phi \\rightarrow V$, and the learner is assumed to have access to a known model class $\\mathcal{M} \\subseteq (\\Pi \\times \\Phi \\rightarrow V)$ that contains $\\mathbb{M}^*$.$^1$ After T rounds of interaction, the learner selects an output decision $\\pi_{T+1} \\sim p$ and incurs the PAC risk\n$\\mathsf{Risk}_{\\mathsf{DM}}(T) := \\mathbb{E}_{p} [L(\\mathbb{M}^*, \\pi_{T+1})], \\qquad (3)$\nwhere $L:\\mathcal{M}\\times \\Pi \\rightarrow \\mathbb{R}$ is a given loss function. This formulation encompasses the commonly studied Statistical Query (SQ) learning [Kearns, 1998] and its various variants [Bshouty and Feldman, 2002, Feldman, 2017, etc.]. Further examples are detailed in Section 4.\nThe setting we just described combines constraints on both the learner and the adversary. Indeed, the class $\\Phi$ represents constraints on the decision maker, limiting the information it receives. Since answers to the measurements may be imprecise (up to the tolerance level $\\tau$), the interaction can be modeled as decision making with a constrained adversary. Before we discuss the details of specializing the hybrid DMSO framework, we first present the definition of the DEC specific to query-based learning and its main guarantees."}, {"title": "SQ DEC", "content": "For a given model class $\\mathcal{M} \\subseteq (\\Pi \\rightarrow V)$ and a (randomized) reference model $\\widetilde{\\mathbb{M}} \\in \\mathcal{M}^+$,\nwe define the SQ DEC at $\\mathbb{M}$ as\n$\\begin{aligned}\n  \\mathsf{p\\text{-}dec}_{\\mathcal{M}}^{(\\epsilon,\\tau)}(\\mathbb{M}, \\widetilde{\\mathbb{M}}) :=   \\inf_{\\mathbb{P} \\in \\Delta(\\Pi)}  \\sup_{\\mathbb{M} \\in \\mathcal{M}} \\left{ \\mathbb{E}_{\\pi\\sim \\mathbb{P}}[L(\\mathbb{M}, \\pi)] \\middle|   \\mathbb{P}_{\\pi\\sim \\mathbb{P}, v \\sim \\widetilde{\\mathbb{M}}(\\pi)} (||\\mathbb{M}(\\pi) - v|| > \\tau) \\leq \\epsilon^2 \\right}.        \\quad (4)\n\\end{aligned}$\nWe further define the SQ DEC of $\\mathcal{M}$ as $\\mathsf{p\\text{-}dec}_{\\mathcal{M}}^{\\mathcal{T-SQ}}(\\mathbb{M}) = \\sup_{\\widetilde{\\mathbb{M}}} \\mathsf{p\\text{-}dec}_{\\mathcal{M}}^{(\\epsilon,\\tau)}(\\mathbb{M}, \\widetilde{\\mathbb{M}})$, where the supre- mum is taken over all randomized reference models $\\widetilde{\\mathbb{M}} : \\Phi \\rightarrow \\Delta(V)$.\nFor query-based learning, our main result is given by the following theorem:\nTheorem 2 (SQ DEC lower and upper bounds; Informal). Let $T > 1$, $\\mathcal{M}$ be a given model class, and the loss function $L$ be metric-based. Then under certain growth conditions, it holds that\n$\\mathsf{p\\text{-}dec}^{(\\epsilon',\\epsilon')}(\\mathbb{M}) \\lesssim \\inf_{\\mathsf{Alg}} \\sup_{\\mathsf{Env}} \\mathbb{E}_{\\mathsf{Env}, \\mathsf{Alg}} [\\mathsf{Risk}_{\\mathsf{DM}}(T)] \\lesssim \\mathsf{p\\text{-}dec}^{(\\epsilon,\\epsilon)}( \\mathbb{M}),$\nwhere $\\sup_{\\mathsf{Env}}$ is taken over all environments satisfying query correctness with tolerance $\\tau$ for a model $\\mathbb{M}^* \\in \\mathcal{M}$, $\\epsilon(T) = \\frac{\\tau}{\\sqrt{T}}$, $\\epsilon'(T) = \\sqrt{\\frac{\\log |\\mathcal{M}|}{T}}$"}, {"title": "From hybrid DMSO to SQ DMSO", "content": "To frame the ($\\tau$-correct) SQ DMSO within hybrid DMSO,\nwe can consider the constraint $\\mathcal{P}_{\\mathbb{M}^*}$ specified by a model $\\mathbb{M}^* \\in \\mathcal{M}$:\n$\\mathcal{P}_{\\mathbb{M}^*} := \\{ \\mathbb{M} \\in \\mathcal{M}^+ : \\forall \\pi \\in \\Pi, \\forall v \\in \\mathsf{supp}(\\mathbb{M}(\\pi)), ||v - \\mathbb{M}^*(\\pi)|| \\leq \\tau \\},       \\qquad (5)$\nand the constraint class corresponding to $\\mathcal{M}$ is given by $\\mathcal{P}_{\\tau\\text{-}query} = {\\mathcal{P}_{\\mathbb{M}^*} : \\mathbb{M}^* \\in \\mathcal{M}}$, with loss function $L(\\mathcal{P}_{\\mathbb{M}^*},\\pi) := L(\\mathbb{M}^*,\\pi)$.\nWhile our characterization of query-based learning (Theorem 2) is derived by a direct proof (cf. Appendix H), we can also obtain it by applying Theorem 1. Specifically, under the above choice (5), for any model $\\mathbb{M} \\in \\mathcal{M}$, we have\n$\\inf_{\\mathbb{M}' \\in co(\\mathcal{P}_{\\mathbb{M}})}  \\mathbb{E}_{\\pi\\sim q_D} D_H (\\mathbb{M}'(\\pi), \\mathbb{M}(\\pi)) = \\mathbb{P}_{\\pi\\sim q, v \\sim \\mathbb{M}(\\pi)} (||\\mathbb{M}(\\pi) - v|| > \\tau),$\nwhere $\\approx$ here means lower and upper bounds up to constant factors (cf. Lemma E.4). Hence,\n$\\mathsf{p\\text{-}dec}^{(\\epsilon,\\tau)}(\\mathbb{M}) \\lesssim \\mathsf{p\\text{-}dec}^{(\\epsilon,\\epsilon')}(\\mathcal{P}_{\\tau\\text{-}query}) \\lesssim \\mathsf{p\\text{-}dec}_{\\mathcal{M}}^{\\tau\\text{-SQ}}(\\mathbb{M})$.\nTherefore, under SQ DMSO, the hybrid DEC is equivalent to the SQ DEC, and the general guar- antees of Theorem 1 apply. Details are postponed to Appendix E.3.1."}, {"title": "2.3 Locally differentially private learning", "content": "The second example of hybrid DMSO is locally differentially private (LDP) learning. We first define the differentially private (DP) channels as follows.\nDefinition 2 (Differentially private channels). For the latent observation space Z and the noisy\nobservation space O, a channel Q is a (measurable) map from $Z \\rightarrow \\Delta(O)$. A channel Q is a $\\alpha$-DP if\nfor $z, z' \\in Z$ and any measurable set $E \\subseteq O$,\n$Q(E|z) \\leq e^{\\alpha}Q(E|z').$\nFor a fixed pair $(Z,O)$ of spaces, we denote by $Q_\\alpha$ the class of all $\\alpha$-DP channels. To simplify the presentation, we assume that $\\alpha < \\alpha_0$ for a pre-specified universal constant $\\alpha_0$, and we will hide dependence on $\\alpha_0$. We also assume the observation space O is non-trivial, i.e., $|O| \\geq 2$.\nDMSO with local privacy constraint (Private DMSO). We consider the following private variant of the DMSO framework, with the local privacy constraint formalized by a class of private channels $\\mathcal{Q}$. For each round $t = 1, ..., T$:\n\u2022\nThe learner selects a decision $\\pi_t \\in \\Pi$ and a private channel $Q_t \\in \\mathcal{Q}$, where $\\Pi$ is the decision space.\n\u2022\nThe environment generates $z_t \\in Z$ sampled via $z_t \\sim \\mathbb{M}^*(\\pi_t)$, where $Z$ is the observation space.\n\u2022\nThe learner receives a noisy observation $o_t \\in O$ sampled via $o_t \\sim Q_t(\\cdot | z_t)$.\nIn private DMSO, the environment is stationary and specified by an underlying model $\\mathbb{M}^* : \\Pi \\rightarrow \\Delta(Z)$, and the learner is assumed to have access to a known model class $\\mathcal{M} \\subseteq (\\Pi \\rightarrow \\Delta(Z))$ that contains $\\mathbb{M}^*$. As such, private DMSO is encompassed by the stochastic DMSO framework.\nIn this paper, we focus on $\\mathcal{Q} = \\mathcal{Q}_\\alpha$, the class of $\\alpha$-DP channels. We call a T-round algorithm as preserving $\\alpha$-LDP (or simply $\\alpha$-LDP) if it is a learner in the above sense. This formulation is equivalent to the commonly studied model of sequential LDP channel [Duchi et al., 2018]. Detailed discussion is deferred to Appendix B.1."}, {"title": "Locally Private Learning", "content": "Let $\\mathcal{L} = (Z \\rightarrow [0,1])$ be the class of functions from Z to [0,1]. For any\n$l \\in \\mathcal{L}$, we define the $l$-divergence between distributions $P, Q \\in \\Delta(Z)$ as\n$D_l(P,Q) := |\\mathbb{E}_{z \\sim P}[l(z)] - \\mathbb{E}_{z \\sim Q}[l(z)]| .        \\qquad (6)$\nFor a model class $\\mathcal{M}$ and a reference model $\\widetilde{\\mathbb{M}} \\in co(\\mathcal{M})$, the convex hull of $\\mathcal{M}$, we define private PAC-DEC at $\\mathbb{M}$ as\n$\\begin{aligned}\n        \\mathsf{p\\text{-}dec}_{\\mathcal{M}}^{DP}(\\mathbb{M}, \\widetilde{\\mathbb{M}}) := \\inf_{\\mathbb{P} \\in \\Delta(\\Pi)} \\sup_{\\mathbb{M} \\in \\mathcal{M}} \\left{  \\mathbb{E}_{\\pi \\sim \\mathbb{P}}[L(\\mathbb{M}, \\pi)] \\middle| \\mathbb{E}_{(\\pi,l) \\sim q}D_l^2(\\mathbb{M}(\\pi), \\widetilde{\\mathbb{M}}(\\pi)) \\leq \\epsilon^2 \\right},      \\qquad (7)\n        \\Phi \\in \\Delta(\\Pi \\times \\Sigma)\n\\end{aligned}$\nand the private PAC-DEC of $\\mathcal{M}$ as $\\mathsf{p\\text{-}dec}_{\\mathcal{M}}^{DP} = \\sup_{\\mathbb{M} \\in co(\\mathcal{M})} \\mathsf{p\\text{-}dec}_{\\mathcal{M}}^{DP}(\\mathbb{M}, \\widetilde{\\mathbb{M}})$. The $l$-divergenceis\na measure of closeness of two distributions that is weaker than the Hellinger distance from the DEC framework for non-private learning (cf. Eq. (22)). This divergence is closely connected to the notion of statistical queries (SQ), but we postpone this discussion until Section 4.3.\nFor learning with LDP constraints, the private PAC-DEC provides both lower and upper bounds for the expected risk, as stated in the following theorem.\nTheorem 3 (Private PAC-DEC lower and upper bounds; Informal). Let $T > 1$. If the loss function $L$ is reward-based or metric-based, the following holds:\n$\\begin{aligned}\n  \\mathsf{p\\text{-}dec}_{LDP}^{(\\epsilon,\\epsilon')}(\\mathcal{M}) \\lesssim \\inf_{\\mathsf{Alg}} \\sup_{\\mathbb{M} \\in \\mathcal{M}} \\mathbb{E}_{\\mathbb{M}, \\mathsf{Als}} [\\mathsf{Risk}_{\\mathsf{DM}}(T)] \\lesssim \\mathsf{p\\text{-}dec}_{LDP}^{(\\epsilon',\\epsilon)}(\\mathcal{M}),\n\\end{aligned}$\nwhere $\\inf_{\\mathsf{Alg}}$ is taken over all T-round $\\alpha$-LDP algorithms, $\\epsilon(T) = \\sqrt{\\frac{\\log |\\mathcal{M}|}{T}}$, $\\epsilon'(T) = \\frac{1}{\\sqrt{T}}$ and we omit poly-logarithmic factors.\nApplications. By further specializing the above result, we provide concrete guarantees for various locally-private learning tasks, including regression (Section 5.2) and particularly linear regression (Section 5.3). Our lower and upper bounds also provide a tight characterization of the local- minimax complexity under LDP (Section 6.1), recovering the characterization in Duchi and Ruan [2024]. We also provide regret guarantees under LDP constraint, with applications to contextual bandits (Section 5.5), where the contexts can be chosen adversarially by the environment. In par- ticular, we derive a near-optimal VT-regret for linear contextual bandits with local privacy through the private DEC theory, settling the open problem of the optimal regret in this setting [Zheng et al., 2020, Han et al., 2021, Li et al., 2024]."}, {"title": "From hybrid DMSO to private DMSO", "content": "For each model $\\mathbb{M} : \\Pi \\rightarrow \\Delta(Z)$, $\\mathbb{M}$ induces a\nmap $\\mathbb{M}^{\\#} : \\Pi\\times \\mathcal{Q} \\rightarrow \\Delta(O)$ given by $\\mathbb{M}^{\\#}(\\pi,Q) = Q \\circ \\mathbb{M}(\\pi)$, where for any channel $Q \\in \\mathcal{Q}$ and\nany distribution $P \\in \\Delta(Z)$, we denote $Q \\circ P \\in \\Delta(O)$ to be the marginal distribution of o under\n$z \\sim P, o \\sim Q(z)$. Therefore, the private DMSO is encompassed by the hybrid DMSO with\nmeasurement class $\\Phi = \\mathcal{Q}$ and constraint class $\\mathcal{P}_{LDP} = \\{\\{\\mathbb{M}^{\\#}\\} : \\mathbb{M} \\in \\mathcal{M}\\}$ induced by $\\mathbb{M}$. Using\nthe strong data-processing inequality (Proposition 20), for any distribution $q \\in \\Delta(\\Pi \\times \\mathcal{Q})$, there\nexists a distribution $q' \\in \\Delta(\\Pi \\times \\mathcal{L})$, such that\n$\\mathbb{E}_{(\\pi,l)\\sim q}D_H(\\mathbb{M}^{\\#}(\\pi), \\mathbb{M}^{\\#}(\\pi)) = a^2\\mathbb{E}_{(\\pi,l)\\sim q'}D_l^2(\\mathbb{M}(\\pi), \\mathbb{M}(\\pi)),$\nwhere $\\asymp$ denotes equivalence up to constant factors. Therefore, it holds that\n$\\mathsf{p\\text{-}dec}_{LDP}^{\\epsilon,\\tau} (\\mathbb{M}) \\leq \\mathsf{p\\text{-}dec}_{LDP}^{(\\epsilon,\\epsilon')}(\\mathcal{P}_{LDP}) \\leq \\mathsf{p\\text{-}dec}_{LDP}^{(\\epsilon,\\tau)}(\\mathbb{M}),$\nwhere $c_0, c_1 > 0$ are absolute constants. Details are deferred to Appendix E.3.2."}, {"title": "2.4 Robust decision making", "content": "We now introduce the following formulation of decision making in the presence of adversarial con- tamination (or, robust decision making). We mainly focus on Huber's contamination model [Huber, 1965, Huber and Ronchetti, 2011], as the application to other types of contamination (e.g. model mis-specifications) is analogous.\nRobust DMSO. Let \u03b2\u2208 [0, 1] be a fixed rate of contamination. In robust DMSO, the interaction protocol is as follows. For each round t = 1,2,\u2026\u2026,T:\n\u2022\nThe learner selects a decision \u03c0\u0165 \u2208 \u03a0 from the joint decision space.\n\u2022\nThe environment generates o\u2021 \u2208 O sampled via o\u2021 ~ M*(\u03c0t).\n\u2022\nWith probability 1 \u03b2, the environment reveals Ot = o to the learner. Otherwise, the environment selects of \u2208 O arbitrarily (potentially depending on the interactions up to round t).\nSimilar to private DMSO, we assume the ground truth model M* belongs to a given model class \u039c \u2286 (\u03a0 \u2192 \u0394\u2206(0)). In the formulation above, the environment is allowed to be adaptive, mak- ing the learning task harder than the Huber contamination model [Huber, 1965, 1992], where the environment is stationary, i.e., M1 =\n= MT = (1 \u2212 \u03b2)M* + \u03b2M' for an arbitrary but fixed con- tamination model M' (cf. Definition 4). Indeed, the environment under the Huber contamination model falls within the purview of the stochastic DMSO framework. Further discussion is deferred to Appendix A.3.\nTo frame the above setting within hybrid DMSO, we can consider the constraint specified by a model M* EM:\nP\u043c* := {(1 \u2212 \u03b2)M* + \u03b2M' : M' \u2208 (\u03a0 \u2192 \u2206(0))}, (8)\nand the constraint class (induced by M) as given by PB-Huber := {PM* : M* \u2208 M}, with loss func- tion L(PM*, \u03c0) = L(M*,\u03c0). Then, the robust DMSO described above is exactly hybrid DMSO with constraint class PB-Huber. By instantiating the general theory in Section 2.1, we arrive at the fol- lowing (simpler) DEC formulation for robust decision making."}, {"title": "Robust DEC", "content": "For \u03b2\u2208 [0", "1": "and distributions P", "divergence\n$D_{\\beta}^{\\text{-Huber}}(P,Q)": "inf_{P' \\in \\Delta(O)"}, "D_H((1-\\beta)P + \\beta P', Q) .        \\qquad (9)$\nFor a model class $\\mathcal{M}$ and a reference model $\\widetilde{\\mathbb{M}}$, we define robust DEC at $\\mathbb{M}$ as\n$\\begin{aligned}\n  \\mathsf{p\\text{-}dec}^{(\\epsilon,\\beta)}(\\mathbb{M}, \\widetilde{\\mathbb{M}}) :=   \\inf_{\\mathbb{P} \\in \\Delta(\\Pi)}  \\sup_{\\mathbb{M} \\in \\mathcal{M}} \\left{ \\mathbb{E}_{\\pi\\sim \\mathbb{P}}[L(\\mathbb{M}, \\pi)"]}