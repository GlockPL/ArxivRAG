{"title": "May the Forgetting Be with You: Alternate Replay for Learning with Noisy Labels", "authors": ["Monica Millunzi", "Lorenzo Bonicelli", "Angelo Porrello", "Jacopo Credi", "Petter N. Kolm", "Simone Calderara"], "abstract": "Forgetting presents a significant challenge during incremental training, making it particularly demanding for contemporary AI systems to assimilate new knowledge in streaming data environments. To address this issue, most approaches in Continual Learning (CL) rely on the replay of a restricted buffer of past data. However, the presence of noise in real-world scenarios, where human annotation is constrained by time limitations or where data is automatically gathered from the web, frequently renders these strategies vulnerable. In this study, we address the problem of CL under Noisy Labels (CLN) by introducing Alternate Experience Replay (AER), which takes advantage of forgetting to maintain a clear distinction between clean, complex, and noisy samples in the memory buffer. The idea is that complex or mislabeled examples, which hardly fit the previously learned data distribution, are most likely to be forgotten. To grasp the benefits of such a separation, we equip AER with Asymmetric Balanced Sampling (ABS): a new sample selection strategy that prioritizes purity on the current task while retaining relevant samples from the past. Through extensive computational comparisons, we demonstrate the effectiveness of our approach in terms of both accuracy and purity of the obtained buffer, resulting in a remarkable average gain of 4.71% points in accuracy with respect to existing loss-based purification strategies. Code is available at https://github.com/aimagelab/mammoth.", "sections": [{"title": "1 Introduction", "content": "Despite the latest breakthroughs, modern AI still struggles to learn in a continuous fashion and suffers from catastrophic forgetting [40], i.e. the new knowledge quickly replaces all past progress. Therefore, Continual Learning (CL) has recently gathered an increasing amount of attention: among the others, one prominent strategy is to interleave examples from the current and old tasks (rehearsal). To do so, a small selection of past data is retained in a memory buffer [17, 56], as in Experience Replay (ER) [48, 50].\nIntuitively, the effectiveness of these methods depends strictly on the memory content: the larger the gap between the memory and the true distribution underlying all the previous tasks, the lower the chances of learning a reliable model. In this respect, several factors may intervene and degrade the snapshot portrayed by the buffer. Several works have highlighted the shortcomings of low-capacity buffers and their link to severe overfitting [9, 57]. More recently, the plausible presence of annotation errors has emerged as an engaging factor [6, 31], due to the subsequent poisoning the memory buffer would be subjected to. Indeed, not only would a few observations of past tasks be available for the learner, but they might be erroneously annotated. It is noted that the presence of noisy annotations [8, 34, 36, 61] is an inescapable characteristic of CL: to allow the learner to digest incoming training examples on-the-fly, data has to be annotated within a restricted temporal window, leading to poor human annotations and hasty quality controls. In light of this, preliminary works [6, 31] focus on purifying the memory buffer and then consolidate their knowledge at the end of the current task (or while learning the task itself [29]). To do so, they spot clean samples by leveraging the popular small-loss criterion [4, 22, 28] and the fact that the most trustworthy examples are those favoured during the first training stages (memorization effect), thus exhibit a lower value of the loss function. However, despite its effectiveness in the offline scenario, such a criterion may be weak in incremental settings. Indeed, as learning does not re-start from scratch but builds upon previous knowledge, the adaption is faster and hence the loss-value separation between clean and noisy samples tends to vanish [4, 63].\nTo overcome this limitation, we explore a radically different approach which could be summarized by a quote ascribed to Julius Caesar \"if you can't defeat your enemy, have him as your friend\u201d: while existing methods see forgetting only as an issue to solve, we use it to identify noisy examples within the data stream. We build upon the work of [39, 55], which theoretically demonstrates that mislabeled examples are quickly forgotten, whereas complex or rare instances tend to be retained for longer periods or may not be forgotten at all.\nTo illustrate such a phenomenon, we depict the loss trend of clean and noisy samples in a memory buffer produced by a rehearsal baseline (ER-ACE [16])."}, {"title": "2 Related works", "content": "Continual Learning methods can be broadly categorized into regularization-based \u2013 these limit changes to key task-related parameters [32, 62] \u2013 and rehearsal-based methods [56].\nRehearsal. In most existing CL scenarios [14, 17, 56], it has been shown that supplementing current training data with past samples is more effective at mitigating forgetting than any regularization-based methods. A simple yet effective method is Experience Replay (ER) [48, 50] which interleaves the current training batch with past examples. Otherwise, GDumb [47] pushes this concept to the extreme by greedily storing incoming samples and subsequently training a model from scratch using only the samples stored in the buffer.\nSampling strategies. Given their low capacity, buffers need to contain a balanced outlook of all seen classes. For this purpose, many employ reservoir sampling [58] to update the memory [5, 14, 16]. The outcome is an independent and identically distributed snapshot of the incoming tasks. However, not every sample comes with the same significance or robustness against forgetting. As highlighted by [2, 5, 15], retaining complex samples is crucial for preserving the performance, which they detect through their loss value or model uncertainty, respectively."}, {"title": "2.1 Learning with Noisy Labels", "content": "Noisy data can originate from multiple sources, including systematic or measurement errors encountered when retrieving historical data [10, 30, 46], errors introduced by human annotators [24, 54], or the presence of outliers [13]. Furthermore, noisy labels pose a significant challenge in medical imaging [30], where small and noisy validation sets can hinder the effectiveness of model calibration techniques [19, 44]. Various approaches have been proposed to mitigate the impact of data noise, including adversarial training, regularization and robust loss functions [26, 37, 64]. Additionally, ensemble methods [38, 42, 45] have been proven effective in reducing the impact of noisy data on model performance."}, {"title": "Noisy label detection.", "content": "The prevalent approach for identifying noisy data is grounded on the memorization effect [4, 28], according to which correctly labelled (clean) instances tend to produce a smaller loss than mislabeled (noisy) ones during the initial stages of training. However, as the training ensues and the model starts to learn wrong patterns from the noisy data, its predictions become less reliable (confirmation bias). In this regard, [23] performs gradient ascent on the noisy samples, building on top of existing sample selection strategies and enhancing loss correction algorithms. Other works exploit separate models to perform sample selection, training either on a probably clean subset (CoTeaching [22], MentorNet [28]) or on all seen samples with semi-supervised objectives (DivideMix [35], [3])."}, {"title": "2.2 Continual Learning under Noisy Labels", "content": "Recent studies [6, 29, 31] conducted in the online CL setup have shown that existing sampling strategies fail to produce meaningful gains in noisy scenarios. In this respect, the authors of PuriDivER [6] propose a sampling strategy that promotes a trade-off between purity and diversity for samples in the buffer. Methods like SPR [31] and CNLL [29] use multiple buffers to gradually isolate clean samples: an auxiliary \u2013 usually larger \u2013 buffer gathers data from the stream, while a refinement procedure based on the small-loss criterion extracts only the clean samples into a purified buffer. Afterwards, SPR trains a network using a self-supervised loss on samples from both buffers, while CNLL adopts a semi-supervised approach inspired by FixMatch [52]. These models are limited to online settings due to either high computational demands or the tendency to overfit, losing effectiveness."}, {"title": "3 Method", "content": "Problem setting. We define the Continual Learning framework as the process of learning from a sequential series of T tasks. During each task t \u2208 {0,1,..., T \u2013 1}, input samples Xt and their annotations Yt are drawn from an i.i.d. distribution D\u2081. We follow the well-established class-incremental scenario [14, 20, 56] in which Yt-10Y\u2081 = 0 and at task t the learner fe is required to distinguish between all observed classes. Ideally, we wish to minimize:\n$\\theta^* = \\underset{\\theta}{\\text{argmin}} E_{B\\sim D_t} [\\mathcal{L}(f_\\theta(x),y)]],$\nwhere L is the cross-entropy loss and B = (x, y). As in CL the objective above is inaccessible, we leverage a fixed-size buffer M to store and replay part of the incoming samples. As a result, the generalized objective for rehearsal CL can be defined as:\n$\\theta^* = \\underset{\\theta}{\\text{argmin}} E_{B\\sim D_t} [\\mathcal{L}(f_\\theta(x),y)] + L_R,$\nwhere the replay regularization term LR depends on the choice of the replay-based method. Although our approach can be equally applied to advanced choices of LR [9, 12, 14, 27] (see Sec. 5), in this work we build upon the simplest strategy and leverage Experience Re-play [48, 50]:\n$L_R = E_M [\\mathcal{L}(f_\\theta(x), y_r)]$\nAs the objective in Eq. (3) could result in bias accumulation toward the current task [1], we adopt the asymmetric cross-entropy loss introduced in [16]."}, {"title": "3.1 Alternate Experience Replay (AER)", "content": "As discussed above, we tackle the challenge of Continual Learning under Noisy Labels (CLN), where each incoming dataset is affected by noise in the labeling process, leading to mislabeled training examples. For a given instance x; \u2208 D\u2081, we indicate with \u1ef9; ~ \u1ef8; the labels corrupted with annotation noise and with Pr (\u1ef9; \u2260 yi) the respective noise rate. In this setting, we must simultaneously address the challenges posed by both noisy labels \u1ef8, and the problem of forgetting. To achieve this, our main focus is on constructing a memory set M that is as clean and representative as possible. Since this objective involves distinguishing between noisy and clean examples when populating the memory set, our methodology seeks to maintain a significant gap between the losses of clean and noisy samples. This gap is indeed crucial for filtering examples through the widely used small-loss criterion. However, with no countermeasure, the loss gap starts to deteriorate as the replay of a small selection of data ensues (see Fig. 1, left). This effect is exacerbated in the popular offline (i.e. multi-epoch) CL setting [14, 49, 60], where we might be forced to trade-off convergence on the current task to avoid overfitting the mislabeled samples [4, 63].\nTo counteract the vanishing effect of the small-loss criterion and encourage the separation between the losses of noisy and clean samples, our novel methodology named Alternate Experience Replay (AER) induces forgetting of buffer datapoints. We refer the reader to Algo. 1 for a summary of the overall procedure. Specifically, we divide the training epochs for the current task into two categories: buffer learning and buffer forgetting epochs. The training process involves alternating between these two modes of learning.\n\u2022 Buffer learning. In this regime, we train the model with standard replay (line 6) as in Eq. (2). Importantly, we do not modify the samples stored in the memory buffer M (no insertion or removal operations are performed).\n\u2022 Buffer forgetting. In this case (line 8), we omit regularization on the memory buffer and focus the training exclusively on data from Dr. By halting regularization and causing the subsequent forgetting of buffer datapoints, the loss of noisy examples is likely to increase more rapidly than that of clean ones [39, 55]. This, in turn, makes the small-loss criterion reliable once again (see Fig. 1, right). On top of that, we update M (line 9) through a loss-based selection strategy during these epochs (see Sec. 3.2)."}, {"title": "3.2 Asymmetric Balanced Sampling (ABS)", "content": "In this section, we outline the sampling strategy used to insert and delete examples into and from the memory buffer during each buffer forgetting epoch.\nSample insertion. Given a batch of data B from the current task, the first step is to determine which examples should be included in the buffer. To encourage the inclusion of clean examples, we exploit the memorization effect and employ a simple criterion that involves applying a threshold to the loss value. Formally, let a denote the percentage of samples within the current batch that we intend to exclude from the insertion procedure, we compute:\nR = {(x, y) \u2208 B : L(x,y) < La}\nwhere La is the loss value at the a-th percentile of the loss distribution over B. For our experiments, we set a to 75, thus discarding the 75% of samples with the highest loss and treating the remaining 25% as candidates to be inserted in the buffer (lines 9-10 of Algo. 1).\nSample selection. We approach the selection process by sampling from a probability distribution p(x) defined over all exemplars \u2200x \u2208 M in the buffer. To model such a distribution, as carried out by most methods [5, 6, 15], we leverage the score s(x) = L(x, y) \u2265 0 given by the loss function. It is noted that a valid distribution can be then obtained by normalizing these scores, such that p(x) = s(x)/Z where Z = \u03a3xems(x). We refer to this strategy as Loss-Aware Symmetric Sampling (LASS). In LASS, examples with higher loss are more likely to be replaced, leading to a memory buffer that maintains greater purity.\nHowever, we argue that a replacement criterion based on loss value like LASS could be detrimental in terms of diversity, as it discourages the retention of complex yet clean samples into the memory buffer. Indeed, these examples tend to exhibit higher loss value, as they lie"}, {"title": "3.3 Buffer consolidation", "content": "While combining AER and ABS achieves a balance between sample purity and complexity preservation, a further reduction in noise levels can be achieved by optimally selecting samples from M at the end of each task. We employ a MixMatch-based [7] approach to enhance model robustness, utilizing uncertain samples as unlabeled ones (i.e. buffer consolidation in the following). Further details about this phase are provided in the supplementary materials."}, {"title": "4 Experiments", "content": "Datasets and noise settings. We conduct experiments on five distinct datasets and various levels of noise. Specifically, we use the Seq. CIFAR-100 dataset [33], which contains 32 \u00d7 32 images from 100 categories, split into 10 tasks, and the Seq. NTU RGB+D [51] dataset for 3D skeleton-based human action recognition, featuring 60 classes divided into 6 tasks. On these datasets, we inject two types of synthetic noise commonly employed in literature [22, 28, 35]: symmetric and asymmetric noise. In the first scenario, we replace the ground-truth label with probability r\u2208 [0,1] determined by the designated noise rate. The asymmetric or class-dependent noise setting, instead, is an approximation of real-world corruption patterns, altering labels within the same superclass as in [43, 64]. To further address real-world label noise, we evaluate our method on Seq. Food-101N [34] (5 tasks), composed of images gathered from the web, thus containing instance-level annotation noise. Additionally, ResNet18 [25] is used for Seq. CIFAR-100 with 50 epochs per task, ResNet34 [25] for Food-101N with 20 epochs, and EfficientGCN-B0 [53] for Seq. NTU-60 with 30 epochs."}, {"title": "4.1 Comparison with State-of-the-Art", "content": "The results of our main evaluation are presented in Tab. 1. To streamline the discussion, we first compare our approach with traditional continual learning baselines, followed by an analysis of methods designed for continual learning under noisy labels (e.g. PuriDivER).\nComparison with rehearsal baselines. As outlined by Tab. 1, the approaches relying solely on buffer consolidation \u2013 such as ER and GDumb are poorly effective, especially as noise levels rise. Regarding GDumb, its training phase is limited to the content of the memory buffer, preventing it from utilizing the data variety available throughout the task. This limitation is also evident from the comparison with standard ER, which consistently outperforms GDumb when noise levels are low. These outcomes highlight the benefits of performing multiple training iterations. However, this advantage turns into a double-edged sword as the stream becomes noisier, leading to a significant drop in performance.\nComparison with CNL methods. Firstly, we highlight the substantial improvement achieved by our adapted PuriDivER.ME, which outperforms PuriDivER by an average of 8.36%. Both versions perform buffer consolidation [6] at the end of each CL task; however, PuriDivER relies on a model trained over multiple epochs, which leads to the degradation of the small-loss criterion, an issue outlined in Sec. 3.1. Moreover, both PuriDivER.ME and ER + DivideMix are consistently surpassed by our proposal. In particular, we measure an average 1.50% gain over the best competitor's performance without any buffer consolidation, suggesting that our proposal improves the purity and diversity of samples in the buffer. However, as the sample selection is not perfect, applying an additional buffer consolidation technique tends to be more effective in more complex noise scenarios, with an average improvement of 4.71%.\nWe conduct additional comparisons with CNLL and SPR (Tab. 2) and PuriDivER.ME (Tab. 4). For the latter, we adopt the more realistic Food-101N dataset (i.e., images collected from the web and automatically labeled). Even in these scenarios, our approach remains superior, both with and without buffer consolidation. We remark that these considerable gains come with a remarkable speed-up in terms of both time and resources used (see supplementary materials), making it more suitable for a multi-epoch incremental scenario."}, {"title": "5 Model analysis", "content": "Ablative study. We herein aim to investigate the impact of each component. Starting from the base rehearsal method used in our research, i.e. ER-ACE [16], we gradually introduce our two main contributions, AER and ABS, one at a time. As seen from the results in Tab. 3, each additional feature produces an increase in performance on Seq. CIFAR-100. For an in-depth analysis of the effects of the asymmetric cross-entropy loss function (ACE), we compare against the standard cross-entropy (i.e. ER in Tab. 3). The results indicate that the contribution of ACE is significant, aligning with both [16] and our initial expectations.\nPurity of the buffer. Considering Seq. CIFAR-10 (40% noise), Fig. 4 depicts the purity and the diversity of the buffer produced by ABS, PuriDivER.ME, and LASS. For each class, purity is defined as the ratio of examples labeled correctly within the memory buffer. Instead, we model diversity as the intra-class variation within each class, thereby computing the average std. deviation of the features produced by the Joint ideal model. Finally, we scale all metrics according to their occurrence rate to account for potential imbalances in the number of examples from different classes. As shown in Fig. 4, ABS clearly outperforms both LASS and PuriDivER.ME in terms of purity and diversity. Unexpectedly, LASS yields a particularly unbalanced buffer, with only the most recent classes showing a good balance. In contrast, PuriDivER.ME achieves better balance but falls short in terms of purity.\nApplicability to other methods. To evaluate whether AER/ABS can enhance other rehearsal methods, we apply them on DER++ [14] and conduct tests on Seq. CIFAR-100. We also report the results with and without the consolidation phase (Sec. 4.1). The gains shown in Fig. 3 support the validity of our AER/ABS on enhancing other CL baselines.\nAdditional results. In the supplementary materials, we provide: i) details about the experimental settings, the adapted baselines, noise injection process and hyperparameters, ii) the evaluation of Final Forgetting (FF), iii) an analysis of the computational costs, iv) an evaluation of the speed at which the model learns the noisy data, v) a sensitivity analysis conducted on the hyperparameter a, which controls the purity within the sample insertion strategy."}, {"title": "6 Conclusions", "content": "We present an innovative framework for Continual Learning in the presence of Noisy Labels, a common issue in real-world AI applications. We focus on the multi-epoch class-incremental scenario, arguing the shortcomings of current methods leveraging the small-loss criterion. We hence appeal to a long-standing enemy of continual learning \u2013 forgetting \u2013 and propose Alternate Experience Replay to maintain a clear separation between mislabeled and clean samples. Additionally, we introduce Asymmetric Balanced Sampling to enhance sample diversity and purity within the buffer. We demonstrate the merits of our approach through extensive experiments, showcasing its potential in noisy incremental scenarios."}, {"title": "A On the effectiveness of buffer consolidation", "content": "By combining AER with ABS we obtain a balance between purity \u2013 for samples of the current task \u2013 while preserving the complexity of those from the past. To achieve this, the backbone network had to be trained on a stream of noisy data. While we find that the effect of noise from the current task is mitigated by AER (Appendix C), we can further reduce its influence with the help of the memory buffer.\nIn principle, with an ideal sample selection strategy we could simply train on samples from M to adjust the predictions of the network at the end of the task in a fully-supervised fashion (buffer fit.). While we empirically find in Sec. 4 that such a strategy delivers remarkable results, we can refine it to handle more complex noise scenarios.\nIn particular, we use a modified version of MixMatch [7] to obtain a more robust model, using the most uncertain samples as a source for unlabeled data. Similarly to [3], we fit a two-component Gaussian Mixture Model (GMM) g(L) on the loss L of each (x,\u1ef9) \u2208 M. Then, we compute the perceived uncertainty of each sample u(x) as the posterior g(I|L), where I indicates the Gaussian component with the smaller mean. Samples are then separated into pure P and uncertain U with a simple threshold on g(I|L).\nFrom this, samples in P have label \u1ef9 \u2248 y, thus we can use them to compute a supervised loss term. Instead, for x \u2208 U we compute y using the model's response on different augmentations T of x:\n$\\hat{y} = \\frac{u(x)}{u(x) + \\eta} y + \\frac{1 - u(x)}{\\eta} \\sum_{i=1}^\\eta f_\\theta(T(x)),$\nFinally, we obtain the refined set Q = {(x, y) : (x, y) \u2208 U} and follow up with the Mix-Match procedure to compute the supervised and self-supervised loss terms Ls and Lu respectively. The overall loss term is computed as Ls + \u03bbuLu, where \u03bbu is a regularization hyperparameter."}, {"title": "B On the influence of the hyperparameter \u03b1", "content": "In this section, we want to carry out a sensitivity analysis targeting the value of a. Recall that alpha controls the proportion of samples to be discarded from the insertion phase within the buffer. We here report the results yielded by several a values under three different noise settings (asymm. 40%, symm. 40%, symm. 60%). The experiments, reported in Tab. A, are conducted on Split CIFAR-100, with performance measured in terms of Final Average Accuracy. It can be concluded that a >= 50% is a good choice, with gains that stabilize around 60% - 70%. In our experiments, we remark that we avoided tuning a and set the same value for every dataset/noise ratio/noise type."}, {"title": "C On the effectiveness of AER as a regularizer for CNL", "content": "Here, we further provide evidence of the impact of AER on the overall performance of the model. In particular, in Fig. A we depict the final accuracy (FAA) and the loss of the noisy samples from the current task of ER-ACE with and without AER during the second task of Split CIFAR-10.\nSurprisingly, we find that AER vastly reduces the rate of convergence of noisy samples, which just by itself improves over the baseline in terms of FAA. Indeed, in rehearsal CNL providing a purified and diverse set of examples to counter forgetting is only part of the challenge: as the model is subjected to a continuous stream of noisy data from the current task, an important effect is to reduce the speed with which noisy samples from the present are learned."}, {"title": "D On the computational demands of CLN methods", "content": "We perform all the experiments on a Tesla V100-SXM2-16GB GPU. In Tab. B we report the computational costs of different methods in our setting, in terms of runtime and consumed memory.\nNot only our method achieves superior performance, as shown in Tab. 1, but also exhibits a lower overall training time."}, {"title": "E Additional details on SPR and CNLL", "content": "In the main paper we provide a comparison between our proposal and SPR and CNLL. Nevertheless, as these methods were originally designed for the single-epoch setting, we had"}, {"title": "F Additional details on the experimental settings and Final Forgetting", "content": "To evaluate our proposal we build upon the open-source codebase provided by Mammoth [11, 14, 16], a CL framework based on PyTorch.\nOn the choice of datasets and noise\nWe empirically validate our method on four different classification benchmarks as mentioned in the main paper. For experiments on CIFAR-10/100 [33] and NTU-60 [51], we corrupt the labels of the datasets at hand to obtain different noise configurations, which we then keep fixed for each of the experiments for fairness of results comparison across multiple methods.\nIn the process of injecting symmetric noise, we replace the ground-truth label with probability r\u2208 [0, 1] determined by the designated noise rate. The asymmetric or class-dependent noise setting is an approximation of real-world corruption patterns, which alters labels within the same superclass. For example, in the CIFAR-100 dataset, each image comes with a \"fine\" label (specific class) and a \"coarse\" label (superclass). Here, label transitions are parameterized by r such that the wrong class and true class have probability r and 1 r, respectively. This results in sample ambiguity occurring only between similar classes, as it would in a realistic scenario."}, {"title": "G Hyperparameters", "content": "We choose to use different buffer sizes relying on the dataset length. For experiments conducted on CIFAR-10 and CIFAR-100, the buffer size is set to 500 and 2000, respectively. We set the buffer size to 500 for experiments on NTU. Finally, we use a buffer size of 2000 for Food-101.\nWe select the other hyperparameters by performing a grid search and using the Final Average Accuracy (FAA) as the selection criterion for the best parameters. Here, we report the best values for each model, categorized by dataset and noise type.\nCIFAR-10\nNoise type: sym \u2013 20%\n\u2022 Joint: lr: 0.03\n\u2022 SGD: lr: 0.03\n\u2022 OURS: lr: 0.03"}]}