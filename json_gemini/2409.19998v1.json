{"title": "DO INFLUENCE FUNCTIONS WORK ON LARGE LANGUAGE MODELS?", "authors": ["Zhe Li", "Wei Zhao", "Yige Li", "Jun Sun"], "abstract": "Influence functions aim to quantify the impact of individual training data points on a model's predictions. While extensive research has been conducted on influence functions in traditional machine learning models, their application to large language models (LLMs) has been limited. In this work, we conduct a systematic study to address a key question: do influence functions work on LLMs? Specifically, we evaluate influence functions across multiple tasks and find that they consistently perform poorly in most settings. Our further investigation reveals that their poor performance can be attributed to: (1) inevitable approximation errors when estimating the iHVP component due to the scale of LLMs, (2) uncertain convergence during fine-tuning, and, more fundamentally, (3) the definition itself, as changes in model parameters do not necessarily correlate with changes in LLM behavior. Our study thus suggests the need for alternative approaches for identifying influential samples. To support future work, our code is made available at https://github.com/plumprc/\nFailures-of-Influence-Functions-in-LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) such as GPT-4 (Achiam et al., 2023), Llama2 (Touvron et al., 2023), and Mistral (Jiang et al., 2023) have demonstrated remarkable abilities in generating high-quality texts and have been increasingly adopted in many real-world applications. Despite the success in scaling language models with a large number of parameters and extensive training corpora (Brown et al., 2020; Kaplan et al., 2020; Hernandez et al., 2021; Muennighoff et al., 2024), recent studies (Ouyang et al., 2022; Bai et al., 2022; Wang et al., 2023; Zhou et al., 2024) emphasize the critical importance of high-quality training data. High-quality data is essential for LLMs' task-specific fine-tuning and alignment since LLMs' performance can be severely compromised by poor-quality data (Qi et al., 2023; Lermen et al., 2023; Kumar et al., 2024). Thus, systematically quantifying the impact of specific training data on an LLM's output is vital. By identifying either high-quality samples that align with expected outcomes, or poor-quality (or even adversarial) samples that misalign, we can improve LLM performance and offer more transparent explanations of their predictions.\nUnfortunately, efficiently tracing the impact of specific training data on an LLM's output is highly non-trivial due to their large parameter space. Traditional methods, such as leave-one-out validation (Molinaro et al., 2005) and Shapley values (Ghorbani & Zou, 2019; Kwon & Zou, 2021), necessitate retraining the model when specific samples are included or excluded, a process that is impractical for LLMs. To address this challenge, influence functions (Hampel, 1974; Ling, 1984) have been introduced as an alternative to leave-one-out validation by approximating its effects using gradient information, thereby avoiding the need for model retraining. These methods have been applied to traditional neural networks (Koh & Liang, 2017; Guo et al., 2020; Park et al., 2023) and more recently to LLMs (Grosse et al., 2023; Kwon et al., 2023; Choe et al., 2024). However, existing methods on applying influence functions to LLMs have primarily concentrated on efficiently computing these functions rather than assessing their effectiveness fundamentally across various tasks. Given the complex architecture and vast parameter space of LLMs, we thus raise the question: Are influence functions effective or even relevant in explaining LLM behavior?"}, {"title": "2 PRELIMINARIES", "content": "Let $f_{\\theta} : \\mathcal{X} \\rightarrow \\mathcal{Y}$ be the prediction process of language models where $\\mathcal{X}$ represents the input space; $\\mathcal{Y}$ denotes the target space; and the model $f$ is parameterized by $\\theta$. Given a training dataset $D = \\{ z_i = (x_i, y_i) \\}_{i=1}^N$ and a parameter space $\\Theta$, we consider the empirical risk minimizer as\n$\\theta^* = \\arg \\min_{\\theta \\in \\Theta} \\sum_{i=1}^N \\mathcal{L}(z_i, \\theta)$, where $\\mathcal{L}$ is the loss function and $f_{\\theta^*}$ is fully converged at $\\theta^*$."}, {"title": "2.1 INFLUENCE FUNCTION", "content": "The influence function (Hampel, 1974; Ling, 1984; Koh & Liang, 2017) establishes a rigorous statistical framework to quantify the impact of individual training data on the model's output. It describes the degree to which the model's parameters change when perturbing one specific training sample. Specifically, we consider the following up-weighting or down-weighting objective as:\n$\\theta_{\\epsilon,k} = \\arg \\min_{\\theta \\in \\Theta} \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(z_i, \\theta) + \\epsilon \\mathcal{L}(z_k, \\theta),$  (1)\nwhere $z_k$ is the k-th sample in the training set. The influence of the data point $z_k \\in D$ on the empirical risk minimizer $\\theta^*$ is defined as the derivative of $\\theta_{\\epsilon,k}$ at $\\epsilon = 0$:\n$I_{\\theta^*}(z_k) \\approx -H_{\\theta^*}^{-1} \\nabla_{\\theta} \\mathcal{L}(z_k, \\theta^*),$ (2)\nwhere $H_{\\theta^*} = \\frac{1}{N} \\sum_{i=1}^N \\nabla_{\\theta}^2 \\mathcal{L}(z_i, \\theta^*)$ is the Hessian of the empirical loss. Here we assume that the empirical risk is twice-differentiable and strongly convex in $\\theta$ so that $H_{\\theta^*}$ must exist. If the model has not converged or is working with non-convex objectives, the Hessian may have negative eigenvalues or be non-invertible. To address this, we typically apply a \"damping\" trick (Martens et al., 2010), i.e., $H_{\\theta^*} \\leftarrow H_{\\theta^*} + \\lambda I$, to make the Hessian positive definite and ensure the existence of $H_{\\theta^*}^{-1}$. According to the chain rule, the influence of $z_k$ on the loss at a test point $z_{\\text{test}}$ has the following closed-form expression.\n$I(z_{\\text{test}}, z_k) = -\\nabla_{\\theta} \\mathcal{L}(z_{\\text{test}}, \\theta^*)^{\\top} H_{\\theta^*}^{-1} \\nabla_{\\theta} \\mathcal{L}(z_k, \\theta^*).$  (3)\nAt a high level, the influence function $I(z_{\\text{test}}, z_k)$ measures the impact of one training data point $z_k$ on the test sample $z$ based on the change of model's parameters. Larger influence thus means larger change of parameters $\\Delta \\theta = \\theta_{\\epsilon,k} - \\theta^*$ when perturbing $z_k$. This way, the influence function \u201cintuitively\u201d measures the contribution of $z_k$ to $z_{\\text{test}}$."}, {"title": "3 EMPIRICAL STUDY", "content": "In this section, we empirically investigate the effectiveness of influence functions on LLMs through three tasks: (1) harmful data identification, (2) class attribution, and (3) backdoor trigger detection. All the experiments are conducted using publicly available LLMs and datasets.\nSetup. Recall that computing the influence functions on LLMs accurately is costly due to the high complexity for computing iHVP. Hereafter, we use three state-of-the-art methods for calculating the influence, i.e., DataInf (Kwon et al., 2023), LiSSA (Agarwal et al., 2017; Koh & Liang, 2017), and GradSim (Charpiat et al., 2019; Pruthi et al., 2020). Additionally, we include RepSim (i.e., representation similarity match) in our study since it is efficient to compute and has reported good performance (Zou et al., 2023a; Zheng et al., 2024). We use Llama2-7B-Chat (Touvron et al., 2023) as a representative LLM for all tasks for our evaluation. During training, we adopt LoRA (Hu et al., 2021) (Low-Rank Adaptation) to reduce the number of trainable parameters, making fine-tuning and computing influence more efficient. We use two metrics to evaluate the performance of a calculated influence: accuracy (Acc.) that measures the likelihood of correctly identifying the most influential data point, and coverage rate (Cover.) that measures the proportion of correctly identified influential data points within the top $c$ most influential samples, where $c$ represents the amount of data for a single category in the training set. Detailed experimental settings are provided for each evaluated task individually. See Appendix B for more implementation details and dataset showcases. All experiments are conducted on a single Nvidia A40 48GB GPU."}, {"title": "3.1 HARMFUL DATA IDENTIFICATION", "content": "In this task, we apply influence functions to identify harmful data in the fine-tuning dataset. Recent studies (Qi et al., 2023; Ji et al., 2024) revealed that the safety alignment of LLMs can be compromised by fine-tuning with a few harmful training examples. Fine-tuning with even a small number of harmful examples can undo the model's alignment, while fine-tuning with benign examples does not reduce the safety alignment significantly. Fine-tuning with a mix of benign and harmful examples can also significantly degrade the model's safety alignment. In this task, given a prompt which induces certain harmful response from a fine-tuned model, we aim to evaluate whether the influence functions can be used to identify harmful data in the mixed fine-tuning dataset. Note that in such a setting, the harmful data in the mixed fine-tuning dataset are intuitively influential (in inducing the harmful responses).\nExperimental settings. In this task, we use TinyLlama (Zhang et al., 2024) to generate harmful responses for fine-tuning Llama2, as TinyLlama has not undergone safety alignment. To construct a mixed fine-tuning dataset, we select the first 20 harmful prompts from Advbench (Zou et al., 2023b), and randomly select 20 benign prompts from Alpaca (Taori et al., 2023) to construct a small mixed data. We further conduct a large mixed data with 20 harmful prompts and 240 benign ones. We use a BERT-style classifier (Wang et al., 2024) to evaluate the attack success rate (ASR) on LLMs using the remaining harmful prompts in Advbench. In this experiment, we regard the harmful prompts in the fine-tuning data as the most influential data, i.e., the ground truth."}, {"title": "3.2 CLASS ATTRIBUTION", "content": "According to the Equation 3, training data samples that help minimize a validation sample's loss should have a negative value. A larger absolute influence value indicates a more influential data sample. In this task, we set up multiple experiments where the validation samples belong to several well-defined classes, and assess whether influence functions can accurately attribute validation samples to training samples within the same class. Note that we expect those training samples in the same class to be the most influential data."}, {"title": "3.3 BACKDOOR POISON DETECTION", "content": "Backdoor attacks (Rando & Tram\u00e8r, 2023; Hubinger et al., 2024; Zeng et al., 2024) can be a serious threat to instruction tuned LLMs, where malicious triggers are injected through poisoned instructions to induce unexpected response. In the absence of the trigger, the backdoored LLMs behave like standard, safety-aligned models. However, when the trigger is present, they exhibit harmful behaviors as intended by the attackers. To mitigate such threats, it is crucial to identify and eliminate those poisoned instructions in the tuning dataset. Our question is: can influence functions be used to identify them?\nExperimental settings. In this task, we follow the settings from previous studies (Qi et al., 2023; Cao et al., 2023) to perform post-hoc supervised fine-tuning (SFT), injecting triggers into instructions at the suffix location. We craft three datasets based on Advbench (Zou et al., 2023b), each containing a different number of triggers such as \"sudo mode\" and \"do anything now.\" Note that, given a validation sample obtained after triggering a backdoor, we consider the training samples poisoned with the same trigger as the most influential data."}, {"title": "4 WHY INFLUENCE FUNCTIONS FAIL ON LLMS", "content": "As shown in the previous section, influence functions consistently perform poorly across three different tasks. The data they identify as most influential often does not match our expectations, while representation-based matching consistently does a better job. These empirical observations suggest that influence functions may not be suitable for explaining LLMs' behavior. In this section, we identify and discuss why influence functions may fail on LLMs from three perspectives: 1) inevitable approximation error caused by calculating iHVP; 2) uncertain convergence state during fine-tuning; and 3) the definition of influence functions itself."}, {"title": "4.1 APPROXIMATION ERROR ANALYSIS", "content": "Given the large parameter space and the amount of data sampled used in LLMs, computing the influence accurately becomes infeasible and thus we must resort to approximation. The question is whether it is the approximation errors of existing influence-computing methods that make them ineffective. To assess the approximation error introduced by estimating iHVP, we conduct two simulate experiments on a subset of the MNIST dataset (Deng, 2012), using a single linear layer with limited parameters, so that we can accurately compute the influence function. As expected, the error increases with the amount of data samples and parameters. While increasing the number of iterations of the LiSSA method can reduce this error, it also introduces additional computational overhead, especially as the data size and parameters grow. Even with limited data, computing the accurate influence function still takes significantly longer than the approximation methods. Note that as the data size and parameters grow, LiSSA requires more iterations to gradually approximate the actual influence function, which is infeasible for LLMs."}, {"title": "4.2 UNCERTAIN CONVERGENCE STATE", "content": "According to the Equation 1 and 2, we should first find the well-converged parameters $\\theta^*$ and then compute the influence. In practice, determining whether a model has converged is however non-trivial and especially so for LLMs. The question is thus: Is the poor performance of the influence-computing methods due to the fact that these models may not have converged? To answer the question, we meticulously record the checkpoints and data gradients at each stage of fine-tuning to study the impact of model convergence on the performance of the influence functions."}, {"title": "4.3 EFFECT OF CHANGES IN PARAMETERS", "content": "Based on the definition in Equation 2 and the derivation in Appendix A of the influence function, it is clear that the influence function quantify the influence of each data sample based on the change in model's parameters as $I_{\\theta^*}(z_k) \\sim \\Delta \\theta (\\theta^* - \\theta_{\\epsilon,k})$. While the definition is somewhat reasonable, it is slightly different from our goal of identify influential data samples based on the change in the model's behavior (e.g., performance on downstream tasks). The question is then whether this mismatch may explain the poor performance of existing influence-computing methods, i.e., whether they have climbed the wrong ladder.\nTo analysis the correlation between parameter change and model behavior change, we conduct a simple experiment. Furthermore, Figure 6 illustrates the parameter changes during Llama2 fine-tuning across different datasets. As the training and validation loss converges, the model's performance on the validation set stabilizes, yet parameter changes continue to increase with training epochs. This indicates that $\\Delta \\theta$ may not accurately reflect changes in the LLM's behavior.\n Theoretically speaking, it is entirely possible that for a parameter abundant complex function, such as LLMs, different parameter sets may yield similar behavior, as discussed in Mingard et al. (2023). To study whether the model complexity is indeed a factor here, we conduct further experiments to study the correlation between change in model parameters and model behaviors. We observe that for smaller models, the changes in parameters closely align with changes in the model's"}, {"title": "5 CONCLUSION", "content": "In this work, we conduct a comprehensive evaluation of influence functions when applied to LLMs, revealing their consistent poor performance across various tasks. We identify and analyze several key factors contributing to this inefficacy, including approximation errors, uncertain convergence state, and misalignment between changes in parameters and LLM's behaviors. The findings challenge the previously reported successes of influence functions, suggesting that these outcomes were more likely driven by specific case studies than by accurate computations. We underscore the instability of gradient-based explanations and advocate for a comprehensive re-evaluation of influence functions in future research to better understand their limitations and potential in various contexts. Furthermore, our research highlights the need for alternative approaches to effectively identify influential training data."}]}