{"title": "Exploring the Use of Abusive Generative AI Models on Civitai*", "authors": ["Yiluo Wei", "Yiming Zhu", "Pan Hui", "Gareth Tyson"], "abstract": "The rise of generative AI is transforming the landscape of digital imagery, and exerting a significant influence on online creative communities. This has led to the emergence of AI-Generated Content (AIGC) social platforms, such as Civitai. These distinctive social platforms allow users to build and share their own generative AI models, thereby enhancing the potential for more diverse artistic expression. Designed in the vein of social networks, they also provide artists with the means to showcase their creations (generated from the models), engage in discussions, and obtain feedback, thus nurturing a sense of community. Yet, this openness also raises concerns about the abuse of such platforms, e.g., using models to disseminate deceptive deepfakes or infringe upon copyrights. To explore this, we conduct the first comprehensive empirical study of an AIGC social platform, focusing on its use for generating abusive content. As an exemplar, we construct a comprehensive dataset covering Civitai, the largest available AIGC social platform. Based on this dataset of 87K models and 2M images, we explore the characteristics of content and discuss strategies for moderation to better govern these platforms.", "sections": [{"title": "1 INTRODUCTION", "content": "The commodification of AI Generated Content (AIGC) has had a significant impact on online creative communities [4, 12]. For example, the Generative Diffusion Model (GDM) [35] has achieved state-of-the-art outcomes in the realm of image generation, with open-source implementations like Stable Diffusion [33] easily accessible. Their open-source nature further enables fine-tuning and extension of the models.\nThis has driven the emergence of AIGC social platforms such as Civitai, PixAI, and Tensor.art. These are online platforms for sharing models, images and discussing open-source generative AI. They are designed akin to social media services, allowing users to showcase their creations, participate in discussions, and receive feedback, thereby creating a sense of community. Uniquely, they also allow users to develop and share their own generative AI models. For instance, bespoke models can be developed for generating particular types of images (e.g., containing particular people or artistic styles) and, subsequently, other users can then share the outputs (images) from these models for further social discussion. These unique features have attracted a significant number of creators sharing numerous novel models and artworks, catalyzing new trends in AI content creation [3, 23].\nHowever, the unrestricted proliferation of diverse models represents a double-edged sword: while they can help unleash creativity, they also pose challenges and risks that require careful consideration. Numerous issues concerning the abuse of generative AI have already been reported, including flooding online communities with not-safe-for-work (NSFW) images [38], disseminating deceptive deepfakes [44], and infringing upon copyright [14]. Anecdotally these platforms have often been the origin of the generative AI models that produce the aforementioned abusive content, and also where the abusive content is initially shared [17, 22]. Thus, the proliferation of abusive content from these platforms can exert a broader influence, permeating other social media communities.\nAs a result, there is an arguable need to somehow moderate the use of these models on such platforms. However, to date, there have been no prior studies that could inform the debate. With this in-mind, we conduct the first large-scale empirical study of an emerging AIGC social platform, focusing on the Civitai - the largest social platform for image models [34]. As of November, 2023, it has attracted 10 million unique visitors each month. We compile a dataset comprising all metadata (for both images and"}, {"title": "2 PRIMER ON CIVITAI", "content": "As a social platform, Civitai enables users to share their AI models and generated images, as well as receive feedback, comments and even tips from other users. In this section, we introduce the necessary pre-knowledge about Civitai.\nModels and images. Civiati hosts diffusion models and AI-generated images, uploaded by creators. Every model/image is associated with a unique ID and a preview web page public to any users. Various social metadata is visible as well, involving tags (assigned by users or Amazon Rekognition [9, 10]), statistics (e.g., number of downloads/views, likes, and rating scores), and text comments by other registered users. Creators can also attach descriptive information to their models and images, i.e. textual descriptions of models' usage and images' configurations, resources, and prompts used for generation.\nUsers. Similar to common social platforms, Civitai users have profile pages displaying their self-reported information and all their models/images. Users can also attach external links to their profile page, as promotion for their accounts on other social platforms (e.g., Instagram and X) or profitable platforms (e.g., Ko-fi and Patreon). Furthermore, users can follow each other and leave rating scores to the profile pages."}, {"title": "3 DATA COLLECTION METHODOLOGY", "content": "We compile a dataset containing the metadata of all models, images, and creators on Civitai. To accomplish this, we utilize the Civitai REST API\u00b9 and python Selenium WebDriver.\nModel data. We collect 87,042 models' metadata. The metadata contains the number of downloads, likes, comments, and rating score (range from 0 to 5), amount of tips, tags, a textual model description, and flags for whether the model is a real-human deepfake or NSFW. Of these models, 8.0% are checkpoint models (base models), 84.4% are LoRA [11, 19] (or LyCORIS [46]) models (fine-tune models), 5.8% are embeddings and 1.8% are other models.\nImage and prompt data. We collect 2,740,149 images' metadata. These are images generated from the shared models. The metadata contains the number of consumers' five reactions (cry, laugh, like, dislike, and heart), number of comments, views, amount of tips, tags, flags for whether the image is NSFW, and the model used to generate the image. Note, as Civitai API does not identify deepfake images, we annotate an image as real-human deepfake if it is generated by any one model explicitly reported as real-human deepfake. Importantly, the metadata also includes the text prompts used for 1,534,922 images.\nCreator data. We extract 56,779 creators, with 11,632 model creators and 52,546 image creators (7,399 are both model and image creators). We also gather their lists of followers and followees."}, {"title": "3.2 Data Augmentation", "content": "We augment our data with two types of annotations: (i) labeling models and images with their content themes (\u00a74); and (ii) identifying person names as well as occupations, using the models'"}, {"title": "3.3 Baseline Prompt Datasets", "content": "Our study also contains a later comparative analysis of the usage of NSFW content in prompts on Civitai vs. two mainstream AIGC platforms: Stable Diffusion and Midjourney. For this, we make use of two existing prompt datasets: DiffusionDB (1,528,512 distinct prompts from Stable Diffusion Discord) [39] and JourneyDB (1,466,884 distinct prompts from Midjourney) [36]. Each of the two datasets contains a large volume of user-generated prompts, allowing us to understand the prompts used on those platforms.\nFor this, we also employ OpenAI's moderation API, configured with the text-moderation-006 model, to quantify the degree of NSFW content exposed in each prompt's text across our Civitai dataset, plus DiffusionDB and JourneyDB. OpenAI's moderation API takes a prompt's text as an input and then reports if it is NSFW content (confidence score ranging from 0 to 1), as well as a flag defining whether the prompt finally classified as NSFW. We choose this moderation model because it is effective in detecting NSFW content [25]. Indeed, it is already used to moderate ChatGPT's prompt input [31]."}, {"title": "4 EXPLORING MODEL AND IMAGE THEMES (RQ1)", "content": "In this section, we investigate RQ1, inspecting the themes of models and images. Our ultimate goal is to explore the potential prevalence of abusive content on Civitai."}, {"title": "4.1 Overview of Themes", "content": "Recall, we use ChatGPT to identify the theme of each model and image, based on their tags. Thus, we begin by examining the distribution of the six identified themes. Overall, while \"Human attributes\" is the most common themes across both models and images, we observe that the focus on themes varies between the creation of models vs. images. Model development predominantly revolves around three themes: \"Virtual character, fictional content, entertainment media\" (65.03% models), \"Human attributes\" (53.85% models), and \"Style of art and culture\" (34.58% models). In contrast, image creation focuses on \"Scenery"}, {"title": "4.2 Overview of Creators' Themes", "content": "We next examine the specific themes that each creator (primarily) focuses on. To do this, for each user, we calculate the proportion of each theme within their content portfolio. To reduce noise, we exclude less active creators, who have fewer than 3 models or images.\nOverall, the results confirm that deepfake and NSFW content are"}, {"title": "4.3 Relationships between Themes", "content": "The prior subsection has revealed clear preferences for certain themes of models and images within the Civitai community. Abusive themes (NSFW & deepfakes) seem particularly prevalent for images. That said, deepfakes themselves are not inherently abusive; rather, they only become abusive when intertwined with other themes. Therefore, we next examine the co-occurrence of themes, measured using the phi coefficient (\u03d5)."}, {"title": "5 EXPLORING THE CREATION OF ABUSIVE IMAGES (RQ2)", "content": "The previous section has exposed the presence of models specifically designed to generate abusive content, alongside a wealth of images generated using those models. Next, we focus on the popularity of these models, and who they target.\nPopularity of deepfake and NSFW models.\nBased on this classification, we see that NSFW and deepfake models are commonly used by image creators. We find that celebrities from three industries are the main targets of deepfakes on Civitai: entertainment (e.g., actress/actor, model, and singer), adult (e.g., pornstar and adult model), and social media (Internet influencer and streamer). Whereas prior work has revealed a prominence of victims from entertainment and politics [7], only 841 (1.71%) deepfake images target politicians on Civitai. Instead, our findings highlight that it is far more common to target online celebrities."}, {"title": "6 EXPLORING USER ENGAGEMENT WITH ABUSIVE MODELS AND IMAGES (RQ3)", "content": "Prior literature has underscored the importance social engagement in encouraging users to share more abusive media [26, 29]. Civitai allows users to interact and, for example, post comments and likes on models or the images that they generate. Inspired by this, we inspect the level of social engagement received by models and images labeled as abusive.\nMetrics to quantify engagement. We rely on several metrics to quantify social engagement with models and images:\nEngagement with deepfake and NSFW models and images.\nThat NSFW content existing within a wider community of active users."}, {"title": "7 EXPLORING THE NETWORK POSITIONS OF ABUSIVE CREATORS (RQ4)", "content": "One potential explanation for the above prevalence of abusive models and images is that help creators to gain a greater social status. To explore this, we next investigate whether the sharing of abusive models and images is correlated with a creator's social network position (e.g., centrality)."}, {"title": "7.1 Social Network Definition", "content": "We induce a follower network based on users' following connections. Among these users, we focus on active creators who have shared at least 3 models or images. We further categorize these creators as NSFW/deepfake if they have contributed at least 3 NSFW/deepfake models or images."}, {"title": "7.2 Centrality Analysis", "content": "Graph centrality is a metric to assess users' positions on a social network [5]. We calculate three centrality metrics to quantify the creators' network positions [30, 47]:"}, {"title": "8 RELATED WORK", "content": "Platforms for Al models. Previous studies have looked at online platforms for AI models. To the best of our knowledge, this is the first large-scale empirical study of an emerging AIGC social platform.\nAbuse of generative AI. Several studies have examined the abuse of generative AI. Overall, our research complements prior studies by providing insights not only from the image angle, but also from the model and creator perspective. We argue this can help in better regulating and moderating potentially abusive models and images."}, {"title": "9 CONCLUSION AND DISCUSSION", "content": "Summary and implications. This paper performed a large-scale study of the creative ecosystem of Civitai. Our analysis reveals abusive use of generative models, mainly revolving around NSFW content and deepfakes. This motivates the need to better develop moderation tools by analysing creators' network positions."}, {"title": "Limitations and future work.", "content": "Our research is based solely on Civitai. Moving forward, we hope to include additional AIGC platforms such as PixAI and Tensor.art. Furthermore, so far, our focus is limited to the exploration of NSFW content and deepfake abuses through generative models. We wish to explore other forms of misuse, such as copyright infringement, the creation of offensive memes, and the production of false information."}, {"title": "Ethics consideration.", "content": "Our study is based on public data from the Civitai RESTful API. We do not attempt to de-anonymise users. We only use the data to understand users' behaviors associated with abusive AIGC and discuss moderation strategies for Civitai. Our analyses follow Civitai's data policies."}]}