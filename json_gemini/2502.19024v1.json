{"title": "Ground-level Viewpoint Vision-and-Language Navigation in Continuous Environments", "authors": ["Zerui Li", "Gengze Zhou", "Haodong Hong", "Yanyan Shao", "Wenqi Lyu", "Yanyuan Qiao", "Qi Wu"], "abstract": "Vision-and-Language Navigation (VLN) empowers agents to associate time-sequenced visual observations with corresponding instructions to make sequential decisions. However, generalization remains a persistent challenge, particularly when dealing with visually diverse scenes or transitioning from simulated environments to real-world deployment. In this paper, we address the mismatch between human-centric instructions and quadruped robots with a low-height field of view, proposing a Ground-level Viewpoint Navigation (GVNav) approach to mitigate this issue. This work represents the first attempt to highlight the generalization gap in VLN across varying heights of visual observation in realistic robot deployments. Our approach leverages weighted historical observations as enriched spatiotemporal contexts for instruction following, effectively managing feature collisions within cells by assigning appropriate weights to identical features across different viewpoints. This enables low-height robots to overcome challenges such as visual obstructions and perceptual mismatches. Additionally, we transfer the connectivity graph from the HM3D and Gibson datasets as an extra resource to enhance spatial priors and a more comprehensive representation of real-world scenarios, leading to improved performance and generalizability of the waypoint predictor in real-world environments. Extensive experiments demonstrate that our Ground-level Viewpoint Navigation (GVnav) approach significantly improves performance in both simulated environments and real-world deployments with quadruped robots.", "sections": [{"title": "I. INTRODUCTION", "content": "Vision-and-Language Navigation (VLN) is a challenging cross-domain research field that requires an agent to interpret natural language instructions from humans and navigate in unseen environments by executing a sequence of actions.\nThere have been significant advancements in understand-ing and aligning vision, language, and action in navigation tasks [1], [2], [3], [4], [5], nevertheless, the effectiveness of these developments is limited when applied to practical scenarios, as they are primarily designed in discrete envi-ronments, where the agent can only navigate on predefined navigation graph by teleporting between adjacent nodes. Therefore, Krantz et al. [6] proposed a benchmark that sets the VLN task in a continuous photo-realistic reconstructing 3D environment where visual agents are required to execute low-level discrete actions. Irshad et al. [7] introduced a hier-archical model to better simulate real robotic actions by esti-mating the agent's linear and angular velocities as continuous actions within the Robo-VLN environment. Recently, based on the close-to-human [1] performance of VLN tasks both in discrete settings [5] (over 80% successful rate) and contin-uous settings [8] (over 60% successful rate), researchers are extending the VLN task into real robot experiments [9], [10], [11], [12]. However, a significant performance gap between simulation and real-world deployment has been identified.\nOne of the primary reasons for this gap is the mismatch of panoramic observation in VLN research and monocular observation on real robots. Most existing Sim-to-Real VLN models rely on monocular RGBD cameras as visual sensors, limiting the agent's field of view and preventing panoramic observation. This restricted visual input hinders the agent's ability to perceive the environment and make informed decisions. Zhang et al. [13] demonstrated that panoramic visual input significantly outperforms monocular input across various performance metrics, further emphasizing the limi-tations of monocular sensors in real-world applications.\nMoreover, in real-world applications where humans issue commands and robots execute actions, there is often a significant discrepancy in viewpoint height between humans and most robots, such as a robot dog. Humans typically have a much higher line of sight, allowing them to observe a broader and more comprehensive view of the environment. In contrast, the robot dog's lower viewpoint limits its field of vi-sion, focusing more on ground-level obstacles and localized surroundings. This height disparity introduces an information asymmetry: humans issue commands based on a global understanding of the environment, while the robot dog, constrained by its limited perspective, makes decisions based on partial, localized information. This mismatch can lead to errors in command interpretation, particularly in complex environments where the robot lacks sufficient information to execute tasks accurately such as shown in Figure 1, and this mismatch can not be solved by simply raise the height of the dog's sensor since it will decrease the passability in constrained environments, potentially impeding its ability to maneuver through narrow spaces or under obstacles.\nTo the best of our knowledge, current VLN research has not adequately addressed the impact of this visual informa-tion gap on performance, which poses practical challenges in applications involving various forms, such as assistive robots autonomous vehicles. Exploring these gaps is crucial for improving VLN tasks in real-world robots, which vary in shape and visual perspective. In this paper, we identify several challenges in deploying VLN systems on real robots, using the Xiaomi Cyberdog-a typical small dog-shaped robot with a low line of sight as a case study: (1) VLN methods are navigating through panoramic observation, but most of the robots are constructed with monocular RGBD cameras as visual sensors. (2) There are numerous visual domain variances to transfer the VLN model from a simula-tor to the real world: Firstly, small-sized dog-like robots such as Unitree Gol and Xiaomi Cyberdog are only around 30cm in height. The reduction in the height of the viewpoint leads to a different understanding of landmarks. Secondly, ground-level viewpoint also results in a significant performance drop in depth-only waypoint prediction used by most VLN-CE approaches [8], [14], [15]. Thirdly, instructions in existing datasets are primarily designed based on human's line of sight, which are not always suitable for quadruped robots. (3) The generalizability of waypoint prediction has been underestimated in VLN-CE R2R benchmarks. The waypoint prediction does not perform well in more complex real environments. Our contribution includes:\n1) We assessed the impacts of notable differences in visual information between human-issued instructions and robot dogs' execution by reconstructing the Xi-aomi Cyberdog with a programmable motor to spin an RGBD camera to get panoramic visual input.\n2) We assessed the impact of waypoint prediction on ground-level viewpoint between depth-only and RGBD waypoint predictions. We further transfer the con-nectivity graphs from public 3D scans as extra data to power up the generalization ability of waypoint predictors in real-world complex environments.\n3) We proposed an adaptive information-gathering mod-ule to handle obstruction in local observation by as-signing appropriate weights to identical features across different viewpoints, significantly enhancing perfor-mance in both simulated environments and real-world deployments with quadruped robots."}, {"title": "II. RELATED WORK", "content": "A. Vision-and-Language Navigation\nIn recent years, significant efforts have been devoted to enabling navigation in previously unvisited environments based on human instructions. This research is often con-ducted within discretized simulated scenes that utilize prede-fined navigation graphs [1], [16], [2], [17]. To facilitate the alignment of language and visual cues for decision-making, Fried et al. [18] introduced the concept of navigation through panoramic actions. This method allows the agent to teleport between adjacent nodes on the graph by selecting an image oriented toward the target node. Building on this foundation, research in VLN has made steady progress in improving model performance towards human-level capabilities [19], [20], [18], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [14]. Recently, Wang et al. [5] achieved an 80% single-run success rate on the widely recognized R2R-VLN bench-mark [31]. However, these advancements remain constrained by the limitations of the high-level panoramic action space when applied to real-world scenarios. To address this, Krantz et al. [6] proposed a benchmark that shifts the VLN task from a discrete to a continuous environment, more closely resembling real-world settings. Despite this shift, directly transferring VLN methods into continuous environments has resulted in substantial performance declines [7], [32], [33]. To overcome these challenges, several studies [4], [34], [35] have introduced waypoint models that bridge the gap between VLN and VLN-CE, maintaining the simplicity of learning cross-modal alignment in discrete environments. Notably, Hong et al. [4] highlight that the choice of waypoint direction and step size significantly impacts VLN policy decision-making. In this work, we aim to optimize waypoint prediction under conditions of limited line-of-sight.\nB. Vision-and-Language Navigation in Real Environments\nRecently, researchers have been trying to extend the VLN task in real robots. Navid [9] proposed a video-based large vision language model (VLM), it only requires an on-the-fly video stream from a monocular RGB camera equipped on the robot to output the next-step action with human instructions, to showcase the capability of VLMs to achieve state-of-the-art level navigation performance without any maps, odometers, or depth inputs. Wang et al. [10] propose an approach to endow the monocular robots with panoramic traversability perception and panoramic semantic understand-ing. This method transfers the high-performance panoramic VLN models to the common monocular robots and tested in real robots. Li et al. [11] extended traditional VLN by incorporating dynamic human activities and relaxing key assumptions, and introduced a Human-Aware 3D (HA3D) simulator and also tested in a real robot. However, none of them indicates the performance dropping by generalization gap in different height field of view."}, {"title": "III. PRELIMINARIES", "content": "A. VLN Background\nAs the navigation graph assumption cannot reflect the challenges a deployed system would experience in a real world environment. This paper focuses on the VLN-CE, an agent tasked with navigating through a continuous 3D environment based on natural language instructions. The environment represented as a continuous 3D space E, where the agent's position at any time t is given by its 3D coordinates $x_t = (x_t, y_t, z_t) \\in E$, where $x_t$, $y_t$, and $z_t$ represent the agent's location in a continuous space. At each position $x_t$, the agent perceives its surroundings through visual observations $o_t$, where $o_t$ has RGBD images $o_{t}^{rgb} \\in R^{H \\times W \\times 3}$ and depth $o_{t}^{depth} \\in R^{H \\times W}$. The agent is provided with a natural language instruction $L = \\{l_1, l_2, ..., l_n\\}$, where $l_i$ are tokens (words) in the instruction. This instruction guides the agent from a start position $x_{start} \\in E$ to the goal position $x_{goal} \\in E$ with discrete low-level actions.\nB. Cross-modal Planning with Topological Map\nWaypoint Prediction Network: Let $P_t = \\{p_1, p_2, ..., p_n\\}$ represent the 3D waypoint positions at time step t, where each $p_i \\in R^3$. Similarly, let $V_t = \\{v_1, v_2, ..., v_n\\}$ denote the corresponding d-dimensional visual features. At each time step, the visual encoders process the panoramic input to generate $V_t$, and a Transformer operates on $V_t$ to establish spatial and contextual relationships among the neighboring sectors, enriching the visual feature representation and in-forming the generation of candidate waypoints $P_t$, where each waypoint is associated with a direction encoded in $v_i$. The agent selects the most promising waypoint $p_i$ based on its visual feature and spatial position, simplifying navigation by moving directly toward the chosen waypoint.\nTopological Navigation Policy: To enable effective back-tracking and planning in the continuous environment, we follow the previous SoTA method ETPNav [36] on VLN-CE and perform language-guided navigation based on topolog-ical mapping. The environment is represented as a graph-based topo map $G_t = \\{N_t, E_t\\}$ keeps track of all observed nodes along the path $\\Gamma'$. Given $\\Gamma'$, we initialize $G_t$ by deriving its corresponding sub-graph from the predefined graph $G^*$. The nodes $N_t$ are divided into three categories:\n*   Visited Node is the agent has already visited\n*   Current Node is where the agent is currently located\n*   Ghost Node is a hypothetical node representing an uncertain or predicted location in the environment, not yet confirmed\nThe edges $E_t$ record the Euclidean distances among all adjacent nodes. The feature vectors $V^p$ are mapped onto the nodes as their visual representations. Taking time step $t$ as an example, $V$ are first fed into a panoramic encoder [15] to obtain contextual view embeddings $V^p$. Visited Node and Current Node have been visited and can access panoramas, they are represented by an average of panoramic view embeddings. Ghost Node is partially observed and therefore is represented by accumulated embeddings of views from which Ghost Node can be observed. $G_t$ is equiped with a global action space $A_G$ for long-term planning, which consists of all observed nodes.\nThe graph $G^*$ is updated continuously based on the agent's predictions and spatial relationships between nodes. If a visited node is localized, the input waypoint is deleted, and an edge is added between the current node and the localized visited node. If a ghost node is localized, the position and visual representation of the input waypoint are accumulated to the localized ghost node. This means that the ghost node's position and features are updated based on the accumulated observations of the waypoint. If no node is localized, the input waypoint is added to the graph as a new ghost node. This newly added ghost node will remain unconfirmed until future localization attempts. To ensure the graph $G^*$ remains efficient, nodes that are too close together or redundant are pruned. If the distance between nodes $v_i$ and $v_j$ is less than a threshold $\\epsilon$, then prune $v_i$ if $d(v_i, v_j) < \\epsilon$, where $d(v_i, v_j)$ is a distance function between two nodes."}, {"title": "IV. METHODS", "content": "A. Scaling up Waypoint Prediction Network Training\nThe first challenge posed by the ground-level viewpoint is the substantial degradation in waypoint prediction per-formance. This is not only due to the downward-shifted line of sight, which limits the visual field, but also the inherently low generalizability of the waypoint predictor in complex, real-world environments. Fig. 2 illustrates the candidate waypoints predicted by a waypoint predictor re-trained exclusively on the R2R dataset using low line-of-sight visual inputs, denoted by red crosses. Despite being re-trained to account for the robot's lower viewpoint, the predictions exhibit suboptimal performance. To solve this problem, we follow ScaleVLN [5] and construct a large waypoint prediction dataset in 800 scans from HM3D [37], 491 scans from Gibson [38], and 61 scans from MP3D [39] under low-angle observation. Despite heuristically sampled viewpoints can also estimate feasible navigation paths based on depth information, it often lack the flexibility to handle complex, language-guided navigation tasks and oversimplify the navigation process by focusing primarily on depth cues, neglecting the rich semantic and visual information.\nSpecifically, we adopt the connectivity graph constructed in ScaleVLN [5] and discretize the environments into undi-rected graphs. At each node of the graphs, we annotate the distance and orientation of the connected nodes as ground truth supervision for the Waypoint Prediction Network. This provides 212924 training samples in total. Compared to the original training data for the waypoint prediction network [4], this raised \u00d722.02 in training data amount. Moreover, we set the rendering height to 80 cm from the ground in the Habitat simulator, and captured the depth images from ground-level observation at each node.\nB. Multi-view Information Gathering\nThe second challenge posed by the ground-level viewpoint is the discrepancy between the oracle and the agent's local observation, caused by environmental obstructions. This cre-ates difficulties for the agent when attempting to predict the next action based on limited local observations. In Figure 2, the robot's observation at position A is limited, while histor-ical data from position B offers an unobstructed view. This disparity between local observations and the oracle's ideal perspective becomes critical when the agent is tasked with selecting the next action or viewpoint in its decision-making. We propose to adaptively gather information from previous unobstructed angles along the trajectory from the previous SoTA method ETPNav [36] on VLN-CE. As illustrated in Figure 2, during the update of the topo map with the predicted waypoints, we introduce a trainable transformer encoder layer that adaptively selects the optimal visual representation $\\tilde{v}_g$ for each ghost node g. At each time step t, the visual representations $VV_t = \\{v_1, v_2, ..., v_h\\}$ are processed through the trainable transformer encoder layer, which applies self-attention to capture dependencies between the visual features:\n$V' = SelfAttn(V^p)$\nHere, $V' \\in R^{n \\times d}$ is the output matrix of the transformer, which incorporates the contextual relationships between the visual features. Instead of averaging the visual fea-tures, the transformer encoder layer uses a learned atten-tion mechanism. Specifically, after applying the transformer block, the layer computes a set of learned weights $W = \\{w_1, w_2, ..., w_n\\}$for each input feature:\nThis generates attention weights, $W \\in R^{n \\times 1}$, used to select the most relevant feature representations. The final representation for the ghost node is computed as a weighted sum of the transformed features $v'_i$, where the weights are derived from the attention mechanism:\n$\\tilde{v}_g = \\sum_{i=1}^{n} Softmax(Linear(v'_i)) v'_i$\nThus, the transformer encoder layer hence then learns to emphasize more informative features for the current context, enabling adaptive selection of the visual representations from multiple viewpoints (A and B). As shown in Figure 2, the navigation policy identifies the optimal next viewpoint in the topological graph (selecting C as the next viewpoint after A). This prediction is based not only on the robot's current observation at A, but also on previous, unobstructed views (from B), allowing the robot to mitigate occlusions and plan more robust navigation strategies."}, {"title": "V. EXPERIMENTS", "content": "A. Experiment Setup\nIn this study, we aim to evaluate the performance of serval VLN models under varying line-of-sight perspectives, transitioning from a high line-of-sight perspective (represen-tative of human vision) to a low line-of-sight perspective (representative of small quadruped robots). This evaluation is designed to identify the performance gap caused by viewpoint discrepancies and to assess the limitations of existing VLN models in low line-of-sight scenarios. Our im-plementation is based on the Habitat simulator [40] and uses the Matterport3D (MP3D) dataset [39], which offers photo-realistic 3D environments with both panoramic and line-of-sight variations, effectively simulating real-world conditions. We employed a two-stage training process, with the first stage involving learning on a scaled dataset generated from the HM3D, Gibson, and MP3D datasets for waypoint pre-diction, and the second stage training on navigation task-specific data (R2R [6]). The learning rate was set to 0.0001 with a batch size of 32. Our approach was benchmarked against multiple baselines, including Seq2Seq [6], CMA (mono) [6], BEVBert [8], and ETP [15]. Evaluation metrics included Trajectory Length (TL), Navigation Error (NE), Overall Success Rate (OSR), Success Rate (SR), and Success weighted by Path Length (SPL), which collectively provided a comprehensive assessment of model performance. In our evaluation, a navigation attempt was considered successful if the robot reached within 3 meters of the target location. Additionally, we deployed our proposed method on a Xiaomi Cyberdog for real-world tests, comparing its performance against two monocular methods and two panoramic methods, to demonstrate its robustness in diverse environments.\nB. Comparison on Simulated Environments\nImpact of Changing Line-of-Sight: We evaluated VLN models trained exclusively on high line-of-sight data (ap-proximately 1.7 meters, simulating a human perspective) and tested them under both high and low line-of-sight condi-tions. This comparison revealed a substantial performance gap between the two settings, illustrating the difficulty of applying models trained on high line-of-sight visual data to small quadruped robots. As shown in Table I, models M#1 and M#5 scored 13% lower SR scores, respectively, for the CMA and RecurrentBert. Comparing M#3 to M#9 and M#4 to M#11, the SR scores are reduced by 32% and 36% for BEVBert and ETPNav, respectively. The disparity between these two perspectives, particularly in landmark recognition, depth perception, and spatial awareness, leads to navigation errors, underscoring that models trained on high line-of-sight data are not directly transferable to small robots with a pronounced downward-shifting line of sight. Especially for BEVBert(M#9, M#10) and ETPNav(M#11, M#12), as they heavily rely on depth information for spatial accessibility to predict waypoint [15], [8]. To address this, we re-trained exclusively on the same model configuration using low line-of-sight visual inputs and there remains a noticeable drop in performance when compared to high line-of-sight tasks that are shown in Table I(M#6, M#8, M#10 and M#12). This highlights the inherent limitations of re-training the model on low-perspective data without intro-ducing additional architectural adjustments or compensatory mechanisms. The reduced performance underscores that the discrepancy between viewpoints introduces a domain gap, which cannot be bridged solely through data re-training. This finding aligns with the results discussed in Section III, Part B, which highlighted the mismatch between the oracle and local observations due to occlusions. This reflects a critical limitation in the design of existing VLN datasets, which primarily focus on high-level, human-like visual data, leaving a performance gap when applied to low perspectives. In M#13 we compared our GVNav with current state-of-the-art methods on the R2R-CE dataset. The results demonstrate that our model outperforms the existing models on all splits in terms of NE: 0.26-0.72, nDTW: 1%-5%, OSR: 2%-5%, SR: 3%-8%, and SPL: 2%-7%.\nNavigator vs. Waypoint Predictor: Table II presents abla-tion experiments designed to isolate the contributions of the waypoint predictor and the navigator as individual compo-nents. The experiments demonstrate that under challenging low line-of-sight conditions, the waypoint predictor has a more pronounced impact on performance compared to the navigator. We tested ETPnav in VLN-CE R2R and freeze/re-train individual components including the navigation policy and waypoint prediction networks. The results show that Re-training the waypoint predictor significantly improves the model's generalization and navigation accuracy, from 21% SR to 39% SR, even when the navigator remains frozen. In contrast, re-training the navigator without updating the waypoint predictor yields only marginal performance improvements, from 21% SR to 32% SR as the static waypoints limit the agent's ability to navigate effectively under low-visibility conditions. To address this limitation, we constructed a larger waypoint prediction dataset, allowing for more comprehensive training of the waypoint prediction\nC. Comparison on Real-world Environments\nWe deployed our GVNav approach on a Xiaomi Cyberdog to demonstrate its capability to navigate in real-world envi-ronments based on given instructions. Our pipeline allows the robot to perform low-level point navigation, effectively enabling it to navigate in unseen environments without prior mapping. The hardware was upgraded with an Intel RealSense D455 camera for more accurate depth sensing. We integrated a 360\u00b0 TTL programmable gear motor to rotate the camera in precise 30\u00b0 increments, capturing 12 images to form a full panoramic view. These images were fed into our navigation model for processing. All models, including CLIP, the waypoint predictor, and our navigation policy, were executed in real-time on a laptop equipped with an NVIDIA RTX 3080 Mobile GPU (16 GB VRAM).\nWe evaluated our method in four distinct environments: a gaming room, a kitchen, a laboratory, and an office area, with 25 unique instructions provided for each scene. As shown in Table III, our approach was successfully deployed on a real robot for Vision-and-Language Navigation (VLN) tasks in real-world settings with a low line-of-sight, and the robot effectively navigated through these diverse environments. In addition to the metrics outlined in Table III, our approach outperformed other methods in both simulated and real-world environments under low line-of-sight conditions.The gaming room represented a particularly challenging environment due to its cluttered layout and limited open space. In contrast, the kitchen was a smaller but relatively open area with few branching paths. The laboratory offered a more spacious and less cluttered environment, while the office area presented a highly expansive space with numerous branching paths. These diverse environments allowed for a thorough evalu-ation of the robustness and adaptability of our method in various spatial and navigational complexities."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we address the key challenges of deploying VLN models on robots with low viewpoints in continuous environments. Using the Xiaomi Cyberdog as a case study, we examine the discrepancy between human commands and the robot's limited visual input, focusing on the restricted field of view due to its low camera height. Our analysis highlights significant performance gaps caused by differences in human and robot perspectives and the limitations of monocular sensors. To address these issues, we reconstruct panoramic inputs, enhance waypoint prediction, and develop an information-gathering strategy to improve navigation per-formance. Our results demonstrate that bridging the visual gap between human and robot perspectives is crucial for im-proving the generalization and performance of VLN models."}]}