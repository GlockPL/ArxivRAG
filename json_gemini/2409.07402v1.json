{"title": "WHAT TO ALIGN IN MULTIMODAL CONTRASTIVE LEARNING?", "authors": ["Benoit Dufumier", "Javiera Castillo Navarro", "Devis Tuia", "Jean-Philippe Thiran"], "abstract": "Humans perceive the world through multisensory integration, blending the information of different modalities to adapt their behavior. Contrastive learning offers an appealing solution for multimodal self-supervised learning. Indeed, by considering each modality as a different view of the same entity, it learns to align features of different modalities in a shared representation space. However, this approach is intrinsically limited as it only learns shared or redundant information between modalities, while multimodal interactions can arise in other ways. In this work, we introduce CoMM, a Contrastive Multimodal learning strategy that enables the communication between modalities in a single multimodal space. Instead of imposing cross- or intra- modality constraints, we propose to align multimodal representations by maximizing the mutual information between augmented versions of these multimodal features. Our theoretical analysis shows that shared, synergistic and unique terms of information naturally emerge from this formulation, allowing us to estimate multimodal interactions beyond redundancy. We test CoMM both in a controlled and in a series of real-world settings: in the former, we demonstrate that CoMM effectively captures redundant, unique and synergistic information between modalities. In the latter, CoMM learns complex multimodal interactions and achieves state-of-the-art results on the six multimodal benchmarks.", "sections": [{"title": "1 INTRODUCTION", "content": "Multisensory or multimodal learning involves extracting and processing information from multiple sources (or modalities, e.g. text, audio, images, tabular data, etc.) to perform a task. The whole human experience is inherently multimodal: we simultaneously see, hear, smell, taste and feel, and these different sensory signals are combined to give us the necessary information to explore our environment. Indeed, many of the simplest tasks we tackle in our daily lives are multimodal. For example, the way we perceive the flavor of our food or drinks does not depend solely on our taste, but also on what we see or even what we hear while we eat. McGurk & MacDonald (1976) have also demonstrated that visual stimuli interact with audio signals to perform human speech recognition.\nDespite the inherent multimodality of sensory systems, machine learning has largely concentrated on single-modality models, with few exceptions in areas like audio-visual speech recognition, multimedia content retrieval, and video-based human behavior analysis. Nowadays, with the emergence of self-supervised strategies and their impressive capacities for learning representations in computer vision, NLP or audio, the paradigm has shifted to learning general multimodal representations from unlabeled data and then fine-tune the models to specific multimodal tasks. Recent works have shown success at training multimodal representations by using cross-modal contrastive objectives to align the representations in a shared embedding space. However, this training strategy only works under the multiview redundancy assumption, i.e., considering that all task-relevant information is shared between modalities and redundantly contained in either one of them separately. In particular, for"}, {"title": "2 QUANTIFYING MULTIMODAL INTERACTIONS", "content": "Problem setup. Let $X_1, X_2,..., X_n$ be random variables representing n different data modalities (e.g. images, text, audio, tabular data, etc.), and a given task Y. Our goal is to learn a latent variable $Z = f(X)$ that is a good representation of $X = (X_1, ..., X_n)$ for Y.\nFor our theoretical analysis, we set n 2, as multimodal interactions have not been characterized yet by PID for larger n. All proofs can be found in Appendix G.\nFor Z to be a good representation of X it should capture the task-relevant information that X contains. Therefore, we need to model the information between the joint variable X and the task Y: $I(X; Y) = I(X_1, X_2; Y)$.\nPartial information decomposition (PID) states that multivariate mutual information $I(X_1, X_2; Y)$ is decomposed into three forms of interactions:\n(i) Uniqueness. This term appears when the task Y can be completed by leveraging only one of the modalities. $U_1$ (resp. $U_2$) refers to the case when $X_1$ (resp. $X_2$) contains all task-relevant information.\n(ii) Redundancy. When $X_1$ and $X_2$ contain the same information about Y. R corresponds to this redundant or shared information.\n(iii) Synergy. Noted by S, this term only emerges when $X_1$ and $X_2$ are simultaneously present, because they bring different and complementary task-relevant information.\nThus, the information that $(X_1, X_2)$ has about Y can be written as the contribution of four terms:\n$I(X_1, X_2; Y) = R + S + U_1 + U_2$ (1)\nMoreover, Eq. (1) and the application of the chain rule of mutual information yield the following consistency equations between R, S, $U_1$ and $U_2$:\n$I(X_1; Y) = R + U_1$, $I(X_2; Y) = R + U_2$, $I(X_1; X_2; Y) = R - S$ (2)\nExisting methods using contrastive objectives to learn multimodal representations impose cross-modal constraints by maximizing an estimator of $I(X_1; X_2)$ as an approximation of $I(X_1, X_2; Y)$. However, this strategy is limited by the multiview redundancy assumption.\nDefinition 1 (Multi-view redundancy) $\\exists\\varepsilon > 0$ such that $I(Y; X_1|X_2) < \\varepsilon$ and $I(Y; X_2|X_1) < \\varepsilon$.\nThis assumption states that most task-relevant information is shared across modalities and the non-shared information is (at most) a small $\\varepsilon$. In other words, any of the modalities contains enough information to fulfill the downstream task Y, and they can provide some kind of \u201csupervision\" to one another, which explains their success for zero-shot image classification.\nLemma 1 Under the multiview redundancy assumption, cross-modal contrastive learning methods are limited to only learn the redundant information R.\nWhat happens when other sources of multimodal information intervene? FactorCL is a good initial step to integrate uniqueness and redundancy into multimodal contrastive learning by applying multimodal augmentations. However, it heavily relies on the assumption that optimal multimodal augmentations can be obtained by applying a two-step process based on conditional augmentations. We argue that this hypothesis is unrealistic, as the first unique augmentation is strongly related to task-relevant information of different modalities. For example, if the text caption is \"a yellow flower\", then color jittering should not be applied to an image depicting a flower. Besides, the factorized formulation of FactorCL is impractical as it is prone to cumulative errors. Finally, this method does not consider the synergy between modalities. In contrast, we propose a model that relies solely in the main hypothesis of contrastive learning, extended to the multimodal case, without relying on strong assumptions between multimodal relationships nor conditional aug-mentations."}, {"title": "3 COMM: CONTRASTIVE MULTIMODAL LEARNING", "content": "We aim to learn multimodal representations that are transferable to any multimodal task. Contrastive learning has shown promising results in multimodal learning. However, current approaches fail to capture multimodal interactions other than redundancy, as shown in Section 2.\nOur strategy builds upon multiview contrastive learning theory and extends it to the multimodal case. It is based on two main components:\n(i) A multimodal architecture, with specialized encoders to process any data type, and an attention-based fusion module to obtain a final multimodal representation.\n(ii) A contrastive objective that naturally captures unique, redundant, and synergistic interactions between different data modalities."}, {"title": "3.1 TOWARDS EFFECTIVE MULTIMODAL REPRESENTATIONS", "content": "To obtain robust, task-agnostic and common representations Z that capture uniqueness, redundancy and synergy from different input modalities, we design $f_\\theta$ \u2013a neural network parameterized by $\\theta$\u2013 such that $Z_\\theta = f_\\theta(X) = f_\\theta(X_1, X_2)$.\nWe define $X' = t(X)$ with $t \\in T$ a stochastic mapping (multimodal augmentation) of X and $Z' = f_\\theta(X')$.\nGiven data processing inequalities for Markov chains $X \\rightarrow X' \\rightarrow Z'$ and $Z' \\rightarrow X \\rightarrow Z_\\theta$, we have:\n$I(Z_\\theta; Z') \\leq I(X, Z') \\leq I(X, X')$ (3)\nWith these inequalities, we can prove the following lemmas:\nLemma 2 By optimizing $f_\\theta$ to maximize $I(Z_\\theta; Z')$, and if we assume an expressive enough network $f_\\theta$, we have at optimum:\n$I(Z_\\theta, Z') = I(X, X')$ (4)\nLemma 3 Let $f_\\theta^*$ be optimal, i.e. $f_\\theta$ maximizes $I(Z_\\theta, Z')$. Then, we have the equality $I(Z_\\theta^*;Y) = I(X';Y)$. If we consider the special case $T = {t_i}$ such that $X' = t_i(X) = X_i$ and $Z_\\theta^* = f_\\theta(X_i) = Z_i$ for $i \\in {1,2}$, then it follows:\n$I(Z_i; Y) = I(X_i; Y) = R + U_i$ (5)\nLemma 3 implies that optimal representations $Z_i$ preserve all the task-relevant information contained in modality i. Interestingly, we do not require Assumption 1 for this equality to hold.\nThe previous theoretical developments lead us to the key ingredients for CoMM's contrastive objec-tives to succeed at capturing multimodal interactions (see Section 3.3 for practical implementation):\n(i) Following Lemma 2, $U_1 + U_2 + R + S = I(X,Y)$ can be learned by optimizing the term $I(Z_\\theta, Z')$ for $T = T^*$, since $I(X, X') = I(X, Y)$ by Assumption 1;\n(ii) Thanks to Lemma 3, $R + U_i$ for $i \\in {1,2}$ can be directly learned by optimizing the term $I(Z_\\theta, Z_i)$ for $T = {t_i}$."}, {"title": "3.2 MULTIMODAL ARCHITECTURE", "content": "Our architecture for multimodal representation learning is presented in Fig. 2. In order to capture multimodal interactions, the model consists of mainly three components:\nEncoders. Each modality is encoded independently by one of the n modality-specific encoders.\nLatent converters. Linear modules that transform features into modality-specific sequences of embeddings. After the latent converters, a concatenation operation gathers these sequences to be fed into a transformer architecture.\nTransformer block. The goal of this module is to perfom the fusion of the modality-specific embeddings through the multihead self-attention layers in the transformer block, obtaining the final multimodal embedding Z.\nMore information about the specific modules used as modality encoders, about the latent converters and the transformer block architecture can be found in Appendix B."}, {"title": "3.3 TRAINING", "content": "Given a multimodal input $X = (X_1, ..., X_n)$ and a set of label-preserving multimodal transformations $T^*$, two augmentations t',t\" are drawn from $T^*$ and applied to X, obtaining $X' = t'(X)$ and $X\" = t\"(X)$. We also consider projections (with a slight abuse of notation) $X_i = ([MSK], ..., X_i, ..., [MSK])$ for $i \\in {1, ..., n}$, where every modality is masked except for the i-th.\nThese terms are encoded by the network to obtain (n+2) embeddings, namely: Z', Z\", and $\\{Z_i\\}_{i=1}^n$. (2n + 1) mutual information terms are then optimized through backpropagation: $I(Z', Z\")$ to max-imize $I(Z, Z')$ in Eq. (3) and $I(Z_i, Z')$ and $I(Z_i, Z\")$ to better approximate $R + U_i$ in Eq. (5) for $i \\in {1, ..., n}$.\nWe use the InfoNCE estimator of mutual information due to its simplicity and strong results in the self-supervised learning literature:\n$\\widehat{I}_{NCE} (Z, Z') = \\mathbb{E}_{(z, zpos) \\sim p(Z, Z')} \\log \\frac{\\exp sim(z, z_{pos})}{\\sum_{z_{neg} \\sim p(Z')} \\exp sim(z, z_{neg})}$ (6)\nGiven this estimator, our final training loss can be written as:\n$\\mathcal{L}_{COMM} = \\widehat{I}_{NCE} (Z', Z'') - \\sum_{i=1}^n (\\widehat{I}_{NCE} (Z_i, Z') + \\widehat{I}_{NCE} (Z_i, Z'')) =: \\mathcal{L}_{R+S} + \\sum_{i=1}^n \\mathcal{L}_{\u2248R+U_i}$ (7)\nFig. 3 illustrates CoMM's training process for the case n = 2. The pseudocode for the general case n\u2265 2 is available in Appendix D. It is worth to notice that the loss terms in Eq. (7) grow linearly with the number of modalities n.\nAt inference, no augmentations are applied. The multimodal input $X = (X_1, ..., X_n)$ is processed through the network to obtain the multimodal feature $Z_\\theta = f_\\theta(X)$. This multimodal representation can then be directly transferred to any task either by performing linear probing or by fine-tuning the whole architecture."}, {"title": "4 EXPERIMENTS", "content": "We design different sets of experiments to evaluate our model: first, over a controlled environment inspired by the Trifeature dataset, we carefully formulate tasks that"}, {"title": "5 ABLATION STUDIES", "content": "We test three main components of our framework (the loss, fusion module and augmentation strategy) against important control baselines on bimodal Trifeature dataset (see Section 4.1).\nLoss function. First, we check our claim that optimizing both $\\mathcal{L}$ and $\\sum_{i=1}^n \\mathcal{L}_i$ -as in Eq. (7)- is required to accurately capture uniqueness, synergy, and redundancy. In Fig. 5, we show that maximizing $\\sum_{i=1}^n \\mathcal{L}_i$ improves redundancy and uniqueness, as guaranteed by our Lemma 3, but fails for synergy. Conversely, maximizing $\\mathcal{L}$ allows one to learn all information terms but very slowly. In particular, for synergy, we argue it is because the model has to learn modality-specific features first (phase 1) before learning their interactions (phase 2). The former is learned through $I(Z_i, Z') + I(Z_i, Z\")$ while the latter is captured with $I(Z', Z\")$. Hence, $\\mathcal{L}_{COMM}$ speeds up phase 1 and can learn synergy more efficiently in phase 2.\nFusion module. We compare our attention-based latent fusion module with shallow linear fusion. We project modality-specific representations to the common latent space using only linear layers and remove latent converters. Table 4 shows that synergy is not captured with linear fusion. This is expected as the XOR gate (typical example of synergistic interactions) cannot be approximated by a linear function. Additionally, uniqueness accuracy is also degraded compared to CoMM (-6%), suggesting that only modeling linear interactions limits the model's representation power.\nData augmentation. Finally, we show in Table 5 that strong data augmentation is crucial for learning multi-modal inter-actions, in line with the liter-ature on unimodal contrastive methods. Contrary to Fac-torCL, ap-plying strong augmentation on both modalities is beneficial for CoMM and we do not require task-dependent augmentations, highlighting the versatility of our framework."}, {"title": "6 RELATED WORK", "content": "Multimodal learning refers to methods that connect and integrate information from multiple sources of data. Early works focused on training"}, {"title": "7 CONCLUSIONS", "content": "Multisensory integration is at the core of human perception, allowing us to build coherent represen-tations of our environment. In this paper, we introduce CoMM, a contrastive multimodal method that enables the integration of multiple modalities in a single multimodal representation space. Unlike existing multimodal contrastive models, CoMM is designed to learn multimodal interactions be-yond redundancy, through PID theory. Our controlled experiments on the bimodal Trifeature dataset demonstrate that CoMM successfully learns redundant, unique and synergistic information. In real-life multimodal datasets from Multibench with two and three input modalities, CoMM outperforms existing methods by large margins in almost every case, showing the efficiency and versatility of CoMM to handle data across diverse domains (robotics, healthcare, affective computing, multime-dia) and structures (time series, image, text, audio, tabular).\nThis work offers large avenues for future research on multimodal representation learning, in partic-ular for crafting label-preserving multimodal augmentations not limited to unimodal augmentations. Limitations and perspectives for future research are further discussed in Appendix A. Overall, the simplicity and versatility of CoMM's design make it a good candidate to learn deep representations of several modalities across domains. It offers promises in better solving real-world problems rang-ing from neuroscience and medical imaging to remote sensing."}]}