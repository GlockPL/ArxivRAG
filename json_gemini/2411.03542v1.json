{"title": "Exploring the Benefits of Domain-Pretraining of Generative Large Language Models for Chemistry", "authors": ["Anurag Acharya", "Shivam Sharma", "Robin Cosbey", "Megha Subramanian", "Scott Howland", "Maria Glenski"], "abstract": "A proliferation of Large Language Models (the GPT series, BLOOM, LLaMA, and more) are driving forward novel development of multipurpose Al for a variety of tasks, particularly natural language processing (NLP) tasks. These models demonstrate strong performance on a range of tasks; however, there has been evidence of brittleness when applied to more niche or narrow domains where hallucinations or fluent but incorrect responses reduce performance. Given the complex nature of scientific domains, it is prudent to investigate the trade-offs of leveraging off-the-shelf versus more targeted foundation models for scientific domains. In this work, we examine the benefits of in-domain pre-training for a given scientific domain, chemistry, and compare these to open-source, off-the-shelf models with zero-shot and few-shot prompting. Our results show that not only do in-domain base models perform reasonably well on in-domain tasks in a zero-shot setting but that further adaptation using instruction fine-tuning yields impressive performance on chemistry-specific tasks such as named entity recognition and molecular formula generation.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) and foundation models are becoming increasingly ubiquitous (Bommasani et al., 2021) across traditional natural language processing applications (Aharoni et al., 2019; Brown et al., 2020) as well as a wide array of domains that can leverage natural language knowledge, reasoning, or enrichments (e.g., law, medicine, biology). Architectures leveraging self-supervised training at scale have become more common as well (Ranftl et al., 2021). This has resulted in a vast increase in use of LLMs, and more broadly AI, across a variety of domains from commonsense reasoning to law and from natural sciences to cultural motifs (Sap et al. 2019, Kell et al. 2020, Acharya et al. 2021, Yarlott et al. 2021, Gu et al. 2021, Xiao et al. 2021, Moor et al. 2023).\nWhile foundation models have displayed a strong capability to generalize to unseen tasks and to perform in-context reasoning, they struggle to produce consistent, factual output on many tasks (Xiao and Wang, 2021). This weakness is of particular importance in various scientific or otherwise technical domains, where factual incoherence is a critical impediment to adoption or impactful use where they could otherwise prove incredibly beneficial to domain experts and practitioners as a means of augmenting human expertise for accelerated advances.\nDue to the vast compute required to train or tune a model at scale, it is critical to understand the trade-off of different strategies to boost performance in domain whether through pre-training from scratch on domain-rich pre-training data, approaches to fine-tune a pre-trained model, or stacking self-supervised pre-training and task-focused fine-tuning. In addition, there is an increasing reliance on interactions with these foundation models using API access (where requests are sent to server-backed instantiations rather than processed locally).\nReliance on state-of-the-art models via API access may raise concerns or impede adoption of these methods e.g., cost concerns (cost of API requests), privacy concerns (information contained within the request being shared externally), and memorization impacts (a subsequent query of the model hitting a version that has been retrained or updated using the previous requests for settings where that would be undesirable). As these (non-empirical) trade-offs that may also motivate a potential choice to leverage the technology of foundation models at a smaller scale for heightened control (on what data models are trained on, what architectures are used, or what scale the models are trained to) increase in consideration, it is important to evaluate performance of both local instantiation strategies and off-the-shelf models.\nIn this work, we focus on a specific scientific field chemistry and, leveraging scientific literature from a multitude of sources, we curate a large and diverse chemistry publication dataset to train several chemistry-focused foundational models. We introduce these foundational models for chemistry as AISLE (AI from Scientific Literature) models and present an analysis that quantifies the benefit of in-domain pre-training or fine-tuning compared to off-the-shelf baselines. Our experiments analyze the benefit of in-domain pre-training from scratch and/or task fine-tuning leveraging instruction based prompts for domain-specific tasks with general benchmarking provided for context.\nAdditionally, we adapt the models further by instruction fine-tuning on some chemistry-specific tasks. We see that the performance of models instruction fine-tuned on a combination of all these tasks perform exceptionally well, indicating that is not necessary to perform instruction fine-tuning for each individual task to adapt a model to a domain. We also observe some limitations of these systems, suggesting a potential need to expose the models to perhaps an even more diverse collection of data in the future, e.g., going beyond scientific literature to structural representations of molecular structure and other chemical properties. Overall, this work introduces the following novel contributions:"}, {"title": "2 Related Work", "content": "The field of Large Language Models (LLMs) has seen an avalanche of recent developments. GPT3 (Brown et al., 2020) took the world by storm with abilities extending beyond what many would consider the \"regular\" uses of languages models."}, {"title": "3 Methods", "content": "In this work, we explore the benefits of pre-training a model with scientific texts and compare this performance to that of general-purpose large language models. We explore two major generative models as baselines and train them on chemistry-domain scientific literature data, referred to as our in-house AISLE models. Additionally, after training these models on our scientific data, we perform instruction fine-tuning across the baseline and AISLE models."}, {"title": "3.1 Data Collection and Processing", "content": "We leverage scientific literature from a variety of data sources es when constructing our chemistry-focused pre-training dataset. This included samples from several existing academic literature datasets the Semantic Scholar Open Research Corpus (S2ORC) (Lo et al., 2019), the Microsoft Academic Graph (MAG) (Wang et al., 2020a), ArnetMiner's \"AMiner\" dataset (Tang, 2016), PubMed publications from the pile (Gao et al., 2020), the Connecting REpositories (CORE) dataset (Pontika et al., 2016), and the CORD-19 dataset (Wang et al., 2020b) - and API-based sampling from three publication databases or scholarly search systems: the dblp computer science bibliography (DBLP) (https://dblp.org), Clarivate's Web of Science (WoS), and the Office of Scientific and Technical Information's OSTI.gov engine (OSTI). We also sample publications from two pre-print archives: arXiv and bioRxiv as described by Horawalavithana et al. (2022).\nIn total, our aggregated dataset comprises 53M scientific publication abstracts containing 10B tokens.\nDeduplication Recent research has shown that duplicates in training data can significantly affect downstream task performance (Lee et al., 2021; Carlini et al., 2022). In order to detect and remove duplicates, we casefold and strip punctuation from the titles to create a simplified T' and consider two articles $A_1$ and $A_2$ to be duplicates if they had the same processed title ($T_{A_1} = T_{A_2}$).\nSegmentation and Tokenization The large size of our input text made it unfeasible to include the entirety of one text in a single batch for pre-training. However, since truncation of data would result in loss of information and the potential for poorer overall performance (Koh et al., 2022), we chose not to truncate the input data. Instead, apace with others (Wang et al., 2019; Dong et al., 2023), we concatenate all tokenized strings together and split them into batches, with each batch having a maximum sequence length of 1024.\nOnce the documents were segmented, we encoded the text into dense vector embeddings for self-supervised pre-training from scratch. We use the Byte Pair Encoding (BPE) algorithm (Shibata et al., 1999) to train a tokenizer with a vocabulary size of 64K reflecting the process developed for the GPT2 tokenizer."}, {"title": "3.2 Instruction Fine-tuning Data", "content": "Previous research has shown that the wording and the structure of fine-tuning instructions has a large impact on the performance of fine-tuned foundation models (Wei et al., 2022).\nWe used the format of ALPACA to create instruction prompts for five chemistry-specific task types using two open-source datasets. The CHEMDNER (Krallinger et al., 2015) tasks include Chemical Entity Extraction (CEE) and Chemical Entity Recognition (CER). The PubChem (Kim et al., 2019) tasks include Molecular Formula Generation (MFG), Isomeric SELFIE String Generation (ISG), and Molecular Weight Estimation (MWE) The prompt templates for these tasks are shown in Table 1.\nCHEMDNER Tasks\nWe construct instructions for the CER and CEE tasks based on the text, entities and entity classes from the CHEMDNER dataset. Each text provided to the model contained one or more chemical named entities from one of seven classes of chemical entities (Trivial, Family, Systematic, Formula, Abbreviation, Multiple, Identifier). For the CEE task, the model had to identify all entities present in the text for a specific entity class. For the CER task, the model had to identify all entity classes for the entities present in the provided text. Our custom metrics for CEE and CER tasks based on the methodology described in Chinchor and Sundheim (1993) allow us to consider partial matches between the gold and predicted entities. We used four different scoring categories, as described below:\n\u2022 Correct (COR): predicted entities that are exact matches for gold entities.\n\u2022 Partially Correct (PAR): predicted entities with partial overlap with gold entities.\n\u2022 Incorrect (INC): predicted entities that are not present in the gold entities.\n\u2022 Missing (MIS): gold entities that are not present in the list of predicted entities.\nand construct two additional sets:\n\u2022 Possible Entities: $POS = COR + PAR + MIS$\n\u2022 Actual Entities: $ACT = COR + PAR + INC$\nto calculate partial scores for precision, recall, and F1 using:\n$Precision = (COR + 0.5 * PAR)/(ACT)$\n$Recall = (COR + 0.5 * PAR)/(POS)$\n$F1 = (2 * Prec. * Rec.)/(Prec. + Rec.)$\nWe also define two evaluation schemas, constrained and unconstrained, indicating whether we constrain the comparison between gold and predicted entities to a specific entity class. For the CEE task, we report results on both constrained and unconstrained schema. For CER, we only report results on the constrained schema.\nPubChem Tasks\nWe constructed instructions for the MFG, ISG, and MWE tasks using the PubChem corpus. For each task, we provided the model with the IUPAC name and instruct it to generate the corresponding molecular formula, generate the isomeric SELFIE string, or estimate the molecular weight."}, {"title": "3.3 Models", "content": "In our experiments we compare two core model architectures: the Generative Pre-trained Transformer Model (GPT-2) (Radford et al., 2019) and the BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) (Scao et al., 2022). We collect or train the following configurations for each model architecture:\n\u2022 the off-the-shelf baseline model,\n\u2022 an AISLE model pre-trained from scratch with chemistry-focused data,\n\u2022 a baseline model with instruction fine-tuning,\n\u2022 and an AISLE pre-trained from scratch base model with instruction fine-tuning."}, {"title": "Baseline Models", "content": "We employ the GPT2-XL model with 1.5B parameters and the BLOOM-3B model with 3B parameters for both the first round of analysis contrasting off-the-shelf models with domain pre-trained models as well as for our fine-tuning analyses. We used the pre-trained weights and standard GPT-2 or BLOOM tokenizers available from HuggingFace (Wolf et al., 2020)."}, {"title": "AISLE Models Trained from Scratch", "content": "We leverage our aggregated scientific data to train seven models from scratch across the two architectures (GPT and BLOOM). We trained these models for three epochs each over 10B tokens from 53 million scientific documents. All AISLE models were pre-trained with a 95/5 train/validation split."}, {"title": "Instruction Fine-tuned Models", "content": "In order to better adapt models for the chemistry domain, we perform instruction fine-tuning across a variety of tasks. Fine-tuning for two epochs with early stopping, we conduct experiments with both the off-the-shelf baseline models and our domain pre-trained AISLE models of each architecture and fine-tune on a combination of the training data for all five tasks we consider resulting in four fine-tuned models: Baseline GPT2, Baseline BLOOM, AISLEGPT2, and AISLEBLOOM ."}, {"title": "4 Results", "content": "Next, we discuss the results of three experiments each investigating a novel approach to adapting LLMs for domain-specific use."}, {"title": "4.1 Impacts of pre-training from scratch", "content": "First, we analyze the benefits of pre-training with domain-focused datasets for LLM adaptation to a given domain i.e., addressing the question what performance gains do trained-from-scratch models offer over off-the-shelf models? In these experiments we consider both zero-shot and few-shot (n = 3) evaluations across tasks.\nWhen we evaluate the model's ability to answer chemistry exam questions at the high school (HT-HC) and college (HT-CC) level, using the chemistry tasks from the MMLU benchmark (Hendrycks et al., 2020, 2021), we see that our domain pre-trained AISLE models outperform off-the-shelf baseline models, as shown in Table 3 that illustrates consistent strong performance by the AISLEGPT2 model.\nWhen we evaluate the models across the MMLU benchmark as a whole, comparing average performance across all tasks regardless of topic, we also see that our strongest AISLE model (AISLEGPT2 ) outperforms both off-the-shelf models by a small margin."}, {"title": "4.2 Impacts of Instruction Fine-tuning", "content": "For our next two analyses, we examine the performance of off-the-shelf Baseline models and pre-trained AISLE models to identify what benefit instruction fine-tuning brings to chemistry-focused extractive and generative tasks. First, we evaluate on held-out data from the five tasks incorporated during the instruction fine-tuning and then we examine the impacts instruction fine-tuning can have on unseen, in-domain multiple choice tasks (i.e., MMLU) described in the previous section.\nEvaluation on Instruction Tasks\nWe present the performance of models on instruction fine-tuning tasks in Table 4. When we evaluate performance of the models on CHEMDNER tasks, we see that our from-scratch trained model with added instruction fine-tuning AISLEGPT2 surpasses the performance of the baseline models (i.e., those that have only been instruction fine-tuned without previous training) on the CER task. For CEE, the AISLEGPT2 models are very close in performance to Baseline models. As expected, the performance on unconstrained schema for CEE tasks is consistently higher for all the models than the constrained schema.\nAISLEGPT2 models also surpass performance of Baseline models for the MFG and MWE PubChem tasks. The variability in edit distance across models for MFG and ISG is displayed in Figure 3 where we see that both AISLE models surpass that of the Baseline GPT2. For the MFG task, we see that the AISLE models illustrate stronger performance at low edit distances (in particular perfect or near perfect [0,1]) over both baselines although Baseline BLOOM shows slightly higher numbers for edit distance of 0.\nEvaluation on Multiple Choice Tasks\nAcross the architectures, we find that the AISLE models trained from scratch on in-domain data with few-shot prompting outperform Baseline off-the-shelf models indicating that while instruction fine-tuning can provide a reasonable performance improvement for domain adaptation, it still lags behind models trained fully from scratch. However, because training models from scratch can acrue high cost in terms of time, money, and compute, instruction fine-tuning can offer a potential alternative for low-resource use cases."}, {"title": "5 Discussion", "content": "In this work we provide an exploration of performance trade-offs for three key methods of domain-adaption. Our results have highlighted some of the trade-off space for performance in compute (e.g., resource-constrained environments that may inhibit the use of an extremely large LLM) or API-access constrained environments. We highlight the strength of domain pre-training from scratch to support varied unseen tasks, with three key findings that we highlight to prompt future research.\nFirst, we see that while the performance of the BLOOM models increases for perplexity-based tasks when instruction fine-tuned on the chemistry tasks, GPT models don't exhibit such consistent results. Their performance varies with seemingly no pattern as to why this occurs. One possible explanation is because the instruction fine-tuning set contained molecular formula and SELFIE strings, which are different (structurally and semantically) from regular written prose tokens, causing the model to produce much less coherent text when asked to generate regular words. But this does not explain why this effect is not seen when using BLOOM models (off-the-shelf, or using domain-focused text), warranting further study.\nWe also highlight the poor performance of our instruction fine-tuned models on the PubChem tasks, in contrast to much better performance on the ChemDNER tasks. We believe this is because the tasks require the models to generate or interpret molecular formulae again suffer from the aforementioned fact that the patterns for those are dissimilar from scientific writing text (e.g., similar to out of vocab or unseen languages).\nFinally, we see that for the BLOOM models, the performance actually decreases for the instruction-based CHEMDNER and PubChem tasks when instruction fine-tuned on these tasks. In several instances, the model simply generates an empty string as the output for the queries. We have not conducted any study into why this might have occurred beyond replicating training and evaluation to confirm it is consistent with GPT2-XL model evaluation, and leave it as a future direction."}, {"title": "6 Limitations", "content": "The experiments in this work have explored in detail the impacts of the three key methods to adapt models for the scientific domains. During these experiments, one main factor of limitation was the computational cost of training large models from scratch. Given more time and resources, we would have liked to train our models for greater number of epochs than our current limit of three. Additionally, the sparsity of appropriate benchmarks for LLMs in the science domain led us to adapt existing benchmarks for evaluation that were not designed for this specific purpose.\nIn the future, this work can be expanded such that the training data includes not just articles, but a varied form of data. It would also be interesting to see how several other new LLMs perform in our experimental settings. Finally, it would be a good study to see if retriever-augmented language models can outperform these performances given the same training data."}, {"title": "7 Ethical Consideration", "content": "It has generally been the norm to assume that previously published work can be used as-is without having to consider the inherited ethical issues. However, in present times, researchers should not \"simply assume that [...] research will have a net positive impact on the world\" (Hecht et al., 2021). We acknowledge that this applies not just to new work, but also when using existing work in the way that we have done.\nWhile we do not anticipate the novel work presented here to introduce new ethical concerns in and by themselves, we do recognize that there may also be pre-existing concerns and issues of the data, models, and methodologies we have used for this paper. In particular, it has been seen that Large Language Models (LLMs), like the ones used in this work, exhibit a wide variety of bias - e.g., religious, gender, race, profession, and cultural - and frequently generate answers that are incorrect, misogynistic, antisemitic, and generally toxic. (Abid et al., 2021; Buolamwini and Gebru, 2018; Liang et al., 2021; Nadeem et al., 2021; Welbl et al., 2021) However, when used within the parameters of our experiments detailed in this paper, we did not see such behaviour from any of the models. To our knowledge, when used as intended, our models do not pose additional ethical concerns than any other LLM."}]}