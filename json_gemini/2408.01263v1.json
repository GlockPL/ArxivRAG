{"title": "The virtual CAT: A tool for algorithmic thinking assessment in Swiss compulsory education", "authors": ["Giorgia Adorni", "Alberto Piatti"], "abstract": "In today's digital era, holding algorithmic thinking (AT) skills is crucial, not only in computer science-related fields. These abilities enable individuals to break down complex problems into more manageable steps and create a sequence of actions to solve them.\nTo address the increasing demand for AT assessments in educational settings and the limitations of current methods, this paper introduces the virtual Cross Array Task (CAT), a digital adaptation of an unplugged assessment activity designed to evaluate algorithmic skills in Swiss compulsory education.\nThis tool offers scalable and automated assessment, reducing human involvement and mitigating potential data collection errors. The platform features gesture-based and visual block-based programming interfaces, ensuring its usability for diverse learners, further supported by multilingual capabilities.\nTo evaluate the virtual CAT platform, we conducted a pilot evaluation in Switzerland involving a heterogeneous group of students. The findings show the platform's usability, proficiency and suitability for assessing AT skills among students of diverse ages, development stages, and educational backgrounds, as well as the feasibility of large-scale data collection.", "sections": [{"title": "1. Introduction", "content": "Algorithmic thinking (AT) is the systematic approach to breaking down complex problems into manageable steps and devising sequential actions to solve them [9, 23, 74, 86, 99].\nIt is an indispensable skill in today's digital era, transcending its origins in computer science [7, 41, 99, 100, 102]. AT empowers individuals to excel in various personal and professional domains by enhancing problem-solving abilities, logical reasoning, and creativity [52]. Consequently, it has become increasingly important in educational contexts [67], where it serves as a foundational pillar for comprehending essential concepts such as algorithms and data structures [17, 48, 49, 68]. As a result, a growing need exists to assess this skill to measure students' development and tailor teaching methods to their needs [22, 72, 83].\nIn the current landscape of AT assessment, educators have a spectrum of approaches at their disposal. Traditional methods, such as closed-ended questions and multiple-choice tests, are commonly used in education and training. They are often used to assess students' knowledge, skills and understanding in various educational contexts. Closed-ended questions elicit brief, direct responses, while multiple-choice tests require selecting answers from pre-defined options. Such methods can assess mastery of specific concepts, memorisation of information and the ability to answer questions clearly and concisely. However, they have been criticised for their limitations in evaluating AT comprehensively, as they often prioritise rote memorisation and basic knowledge recall, failing to align with the real-world problem-solving nature of AT [13, 16, 69, 87, 97].\nAlternative approaches like open-ended questions have the advantage of showcasing problem-solving skills and critical thinking but can be time-consuming and subject to grading inconsistencies [16]. Programming assignments and coding challenges emphasise practical application and offer hands-on evaluation of algorithmic skills. While they provide valuable real-world learning experiences, grading can be labour-intensive and evaluating code quality may require significant effort [90]. Robotic activities provide a hands-on approach to AT assessment, engaging students in real-world problem-solving with physical robots [44, 57]. Despite their benefits, challenges arise concerning access to robotic equipment, resource requirements, and the need for specialised knowledge to facilitate these activities effectively. The domain of automatic assessment systems offers a promising avenue to mitigate some of the challenges associated with standardisation and subjectivity [75, 80]. These systems provide immediate feedback to students, ensuring more consistent evaluations and contributing to a more dynamic and interactive learning experience [75]. Automated assessment tools are well-suited for large-scale educational programs. However, research is still refining their ability to comprehensively assess AT, especially in monitoring learners' progress [89]. Finally, these assessment methods sometimes overlook developmental aspects, social and environmental contexts, and the availability of appropriate educational resources [12, 49, 50, 56, 73, 78, 79, 81, 93, 100]. These emphasise the need for a comprehensive, reliable, and objective assessment tool that can be broadly applied and scaled to accommodate diverse age groups and educational settings [9, 30, 65, 96].\nTo overcome the inherent limitations in existing approaches, this study builds upon previous work, introducing the Cross Array Task (CAT) [71], an unplugged assessment activity designed to align seamlessly with the distinctive educational landscape of Switzerland. This activity considers tasks' developmental and situated nature in social and artefactual environments, aspects that are often overlooked in the literature. The CAT's focus on these characteristics complements the Swiss educational philosophy, which strongly emphasises diversity and integration. Swiss schools are known for accommodating students with diverse backgrounds, capabilities, and linguistic proficiencies, and the CAT's flexibility enables it to support these goals effectively. While the CAT has demonstrated its effectiveness in evaluating AT abilities in K-12 students, nonetheless, it presents certain limitations. The CAT was conceived as a bilateral activity between a student and a specialist. Thus, it cannot be feasibly administered to an entire class simultaneously by a single expert. Consequently, it proves to be time-consuming and impractical when applied to large-scale assessments. Furthermore, the presence of a human agent in the assessment process can introduce additional challenges. It can make the process raise the potential for errors, given the possibility of inconsistent interpretations of instructions.\nThis paper introduces a digital version of the CAT activity designed to overcome its limitations and enable large-scale assessment, focusing on Switzerland's compulsory education system. While our main objective in this study is to assess AT on a large scale, it also encompasses the design of a tool that can effectively achieve this goal and lays the foundation for understanding how to develop such a tool. This virtual CAT offers multiple interfaces as interaction modes and multilingual support, making it adaptable and accessible to a wide range of pupils. It can be administered through individual devices, allowing multiple pupils to take the assessment simultaneously. This streamlines and automates the assessment process, making it faster and more efficient while reducing human involvement and minimising errors in data collection and interpretation inconsistencies. Given the characteristics of the virtual adaptation of the unplugged CAT, it shows promise in addressing the limitations of current assessment tools and facilitating larger-scale assessments of AT in compulsory education."}, {"title": "2. Design process", "content": "This section discusses the design process of the virtual CAT. We employ a formative evaluation approach, drawing from various methodologies, focused on usability, to continuously refine our design [14, 18, 34, 85, 98].\nOur design process comprises three distinct User Experience (UX) Design Lifecycles. In the initial phase, we define objectives, analyse prior experiences, and establish tool requirements [47]. We proceed to the designing and prototyping phase, prioritising user experience and usability. During the second phase of our design process, we aimed to improve the user-friendliness of our initial prototype. To achieve this, we enlisted the expertise of a UX inspector. With their knowledge of UX design guidelines and professional experience, they carefully examined our prototype to identify any potential issues. Based on their findings, we developed an improved prototype that addressed these concerns and provided a better user experience [34].\nIn the final cycle, we engage children in participatory design, considering their needs and preferences to extract new user requirements [28, 29, 76, 77, 84]. Usability remains central as we actively involve children as informants and evaluators, with teachers facilitating the process [8, 11, 19, 27, 35, 42, 63, 64, 82]. Multiple data elicitation techniques are used to gain insights into usability and effectiveness [31, 32]. More in-depth details are provided in Sections 4 and 5."}, {"title": "2.1. The CAT: from unplugged to virtual interactions", "content": "The Cross Array Task (CAT), illustrated in Figure 1, is an assessment activity for K-12 pupils conceived using the CT-cube [71]. This theoretical framework guarantees that any Computational Thinking Problem (CTP) designed includes the necessary cognitive processes involved in problem-solving while also considering the social and environmental factors that influence these processes.\nThe CAT revolves around the concept of a cross array - a cross-shaped array of 20 dots forming a 2-thick cross, consisting of five 2 \u00d7 2 square arrays of coloured dots, with colours chosen from a set of k colours, usually yellow, green, blue, or red. Each student in this activity receives 12 reference schemas, essentially coloured cross arrays exhibiting different regularities and increasing complexities. The problem solver's task is to devise a set of instructions, referred to as algorithm, to replicate these intricate patterns on a blank cross array. These instructions need to be conveyed to an agent for execution. To communicate the algorithm, the problem solver has the flexibility to use various artefacts, depending on the version of the CAT used. The human or artificial agent is tasked with interpreting the student's instructions and replicating the colouring pattern on a blank cross array, also known as a colouring schema. Typically, during this interaction, a visual barrier prevents the problem solver from seeing the colouring schema to enhance the task's challenge. However, if necessary, the problem solver can remove this barrier and rely on visual clues of the colouring process.\nWithin the CT-cube framework, the CAT is structured around three dimensions:\n1. Type of activity: The CAT focuses on algorithm development, or breaking down complex processes into more straightforward instructions that a human or an artificial agent can execute to solve the problem.\n2. Cognitive artefacts: These are the tools and resources used during the activity. The CAT employs two main types of artefacts: embodied (rooted in sensory experiences) and symbolic (based on abstract representations).\n3. Level of autonomy: The degree of independence exhibited by students during the activity. In the CAT, inactive pupils do not attempt to solve the task; non-autonomous ones rely on visual feedback to do it; autonomous ones provide intelligible instructions without clues.\nThe CAT assessment process is designed to evaluate pupils based on these dimensions: algorithm complexity, artefact choice, and autonomy level. Specifically, the complexity of the algorithm is assessed based on the operations' dimensions involved: OD operations involve colouring dots individually, 1D operations involve colouring multiple dots with the same colour based on patterns such as rows, diagonals, and squares, 2D involve more complex patterns with alternating colours, repetition or mirroring of operations. The most complex operation used in an algorithm determines its final classification.\nThe task is successful if the student conceives a complete and correct algorithm, regardless of its complexity, the artefactual environment or autonomy. A comprehensive metric called the CAT score is used to quantify this multi-facet performance, consisting of two components: the combination of artefact used and autonomy level, referred to as the interaction dimension, and the algorithm dimension. Each component is assigned a numerical score, with a higher score indicative of a student who has navigated the complexities of challenging artefacts, assumed an autonomous role, and/or conceived a higher-dimensional algorithm.\nTo better understand the evolution of the CAT activity from its original unplugged domain to the virtual one and easily allow for a visual comparison of them, Figure 1 visually illustrates the setup of the two versions, while Table 1 highlights their differences.\nThe CAT was conceived as an unplugged activity, characterised by a face-to-face interaction between the problem solver and a human agent. In this task version, the problem solver's primary objective was to conceptualise an algorithm and effectively communicate it to the agent.\nWithin this setup, two different representational artefacts are at the student's disposal. The first was a symbolic artefact, where students could verbally communicate their instructions, describing the process using words. This includes using natural language, which is considered symbolic as it employs words and phrases to represent ideas and concepts. Alternatively, an embodied artefact allowed students to augment voice instructions with physical gestures. For instance, they could use hand movements to point to specific dots on an empty cross array schema, physically illustrating the commands they wished to convey.\nRegarding the interaction dynamics between the problem solver and the agent, they were seated in front of each other across a table, with a physical barrier separating them. This screen obstructed the direct visual exchange of actions between the student and the agent. It effectively prevented the student from observing how the agent executed the task of colouring the empty cross array and vice versa.\nThe agent has the role of interpreting student instructions, documenting them on a protocol, and finally replicating the colouring pattern on a blank cross array based solely on the algorithm communicated by the student.\nThe evolution from the CAT unplugged version to its virtual counterpart required a transformation in the interaction methods between the problem solver and the artificial agent. The problem solver now interacts with a device, a virtual agent, replacing face-to-face interaction with a human agent.\nThe original idea of using two different representation methods to convey the algorithm remains, but they have been adjusted. A gesture-based interface is available to replicate hand gestures, maintaining the embodied artefact aspect. The original voice interaction was removed as it posed multiple challenges in a multilingual context and with younger learners. Speech recognition technology struggled to accurately interpret spoken instructions in these scenarios, resulting in a less practical learning experience. As a replacement, we introduced an alternative symbolic artefact, a visual programming interface, hereafter referred to as CAT-VPI, with ready-made building blocks, reproducing commands akin to those encountered in the unplugged CAT. These blocks come in two variations: one conveys commands through textual instructions, while the other employs symbols. The advantage of this block-based coding approach lies in its user-friendly nature, enabling students to construct instructions through simple drag-and-drop actions, enhancing accessibility and reducing the likelihood of syntax errors.\nRegarding interaction dynamics, as in the unplugged version, the problem solver cannot observe the executed code or the cross-colouring points reached, however, the interfaces allow the user to activate the visualisation of the progress of the colouring.\nThe agent maintains the role of interpreting the student's instructions. A programming language interpreter is used to translate gesture interaction and visual blocks into a formal programming language that mimics the operations observed in the unplugged activity, which we assumed the student would reuse in the virtual version. The algorithms are thus automatically recorded by the virtual agent."}, {"title": "2.2. The first prototype", "content": "In the design of our platform, we prioritised accessibility and usability. In educational technology, accessibility centres on crafting solutions to meet users' needs from various backgrounds, regardless of their physical or cognitive abilities [43]. In contrast, usability focuses on the user experience, aiming at delivering an intuitive and effective learning environment [6, 26]. To achieve these goals, we made a series of strategic decisions, including considerations such as the choice of compatible devices, language support, and the design and layout of various interfaces, following guidelines and best practices found in [34].\nFor practical reasons, we opted to skip certain stages, such as crafting paper prototypes, and directly develop a functional prototype. This decision was influenced by limited access to schools and children, along with time constraints, which made a streamlined approach necessary. Additionally, the involvement of very young children in the participatory design, who may find abstract reasoning challenging, makes it necessary to opt for a different type of prototype [19, 31, 33, 55].\nThe app is tailored for iPads, primarily due to their touchscreen interaction, which closely aligns with the intuitive and interactive learning experiences sought in educational settings. This design choice ensures that students, particularly those in K-12 educational contexts, can engage with the application user-friendly and pedagogically effectively, fostering an enriching educational experience [82].\nThe app is available in four languages: Italian, French, and German to cater to the diverse linguistic landscape of Switzerland; English to extend the app's utility to a broader range of educational institutions, ensuring that a wider student demographic can benefit from the learning experience it offers, paving the way for potential adoption beyond Switzerland's borders (see Figure A.1).\nThe platform provides users with two distinct methods to engage with it a gesture interface (CAT-GI) and a visual programming interface (CAT-VPI). We adhered to standard mobile application design principles when crafting the interfaces to ensure their usability and effectiveness [15, 34, 61, 92]. This includes incorporating common elements, like a top bar and a left-side menu list, to create an interface familiar to users with experience with other applications. We prioritised the legibility and readability of text, ensuring that font sizes were large enough for all users and that the background had a good contrast. We ensured our system was accessible to everyone, including people with visual impairments and colour blindness. To achieve this, we used high-contrast visuals and colours that are easy to see. Users can switch to a colour-blind mode and also use our text-to-speech feature. We maintained consistency by using similar names and labels for similar objects and functions and employing precise wording in menus, icons, and data fields to enhance clarity. We avoided synonyms to ensure an intuitive user experience. To align with established conventions, we placed commonly used features in easily accessible locations. Our platform features a responsive design that adapts seamlessly to different devices for optimal usability. Overall, our adherence to these mobile application design principles aimed to create a user-friendly and accessible, enhancing the overall experience for all users, regardless of their abilities or prior experience with similar applications."}, {"title": "2.3. Expert evaluation and design of a refined prototype", "content": "After developing the initial prototype of the platform, we collaborated with experts to enhance its usability and accessibility. First, we consulted with an interaction design researcher and teacher, who recommended more intuitive icons and a restructuring of the interfaces to establish a shared uniform page layout. This resulted in adopting a consistent design featuring three columns, each optimised for specific functions: the left column featured predefined code blocks or buttons, the central column served as the workspace for user interactions, and the right column displayed the coloured schemas to be replicated. Additionally, we sought the expertise of three pedagogical specialists in computer science education and pedagogy to refine the educational aspects of our initiative. They provided strategies for introducing the activity and platform to pupils in an engaging and age-appropriate manner.\nVisual programming interface The development of the CAT-VPI has given rise to a visual programming language (CAT-VPL) designed to make coding accessible to a broad audience, particularly K-12 students, including those with no prior programming experience. The CAT-VPI is depicted in Figure 2a. Within this virtual environment, users engage with a drag-and-drop mechanism, which facilitates the organisation of predefined building blocks for constructing a colouring algorithm. These building blocks inherently possess predefined functions that mirror their counterparts in the formal programming language. Nevertheless, users can customise these blocks by adjusting parameters such as colour or pattern choices. To ensure inclusivity for learners of diverse backgrounds, ages, and cognitive abilities, we provided two different representations for commands, one textual, offering explicit linguistic command expressions (see Figure 2b) and one symbolic, enabling non-literate learners to interact effectively with the interface (see Figure 2c).\nGesture interface The CAT-GI is intentionally designed to emulate the hand gestures observed during the unplugged CAT activity. This interface allows engaging in a hands-on, tactile experience similar to interacting with the physical empty cross array, bridging the gap between physical and virtual learning environments. Users can directly interact with the cross by selecting colours and touching the dots, mimicking the physical engagement of the unplugged activity. By dragging their fingers across the screen, users can create patterns, while additional complex actions such as repeating instructions or mirroring patterns can be executed using action buttons, adding layers of complexity to their algorithm, as showcased in Figure 3."}, {"title": "3. Implementation", "content": "The final application [4] was developed using Flutter [25]. This choice of framework yielded multiple advantages. Its extensive platform support, including Android, iOS, Linux, macOS, Windows, and the web, ensures compatibility across various devices. Although the app is designed to be used on an iPad, thanks to the cross-platform framework, we developed a single codebase that runs seamlessly on multiple platforms, saving us much time and effort in platform-specific development. The application design is responsive, which means it has a consistent look across different platforms and screen sizes. Another benefit is the streamlined and effective development experience provided by the hot reload feature, which allows developers to see the effects of code changes, making iterative development more seamless and increasing productivity. Additionally, Flutter provides ample pre-built widgets and libraries, offering the necessary tools to create visually engaging and interactive user interfaces. The latest version of the application, along with its comprehensive source code and documentation, is available online and can be accessed through reference [4]."}, {"title": "3.1. Definition of a formal programming language", "content": "To establish a standardised set of instructions that users could employ within the application interfaces to design the algorithm, we defined the CAT programming language, which codifies and formalises all the commands and actions observed during the original experimental study with the unplugged CAT.\nCross representation The cross-board dots are manipulated and referenced using a coordinate system, where rows are labelled from bottom to top using letters (A-F), and columns are numbered from left to right (1-6).\nMoves Moving around the cross-board can be done in two ways. The `goCell(cell)` method allows jumping directly to a specific coordinate. Alternatively, the `go(move, repetitions)` method allows traversing a certain number of dots in one of the eight available directions (either cardinal or diagonal) to reach the desired destination.\nBasic colouring Colouring the board is a fundamental aspect of the CAT application, and we offer various methods to achieve this. The `paintSingleCell(color)` method allows colouring the dot they are currently positioned on with a single colour. The `paintPattern(colors, repetitions, pattern)` method allows colouring multiple dots according to predefined patterns. A sequence of colours can be specified, which will alternate following the selected pattern. Additionally, users can choose from five pattern types (cardinal, diagonal, square, L, zigzag), each with various directions. The `paintMultipleCells(colors, cellsPositions)` method enables colouring multiple dots with custom patterns, defined by specifying the coordinates of the cells to be coloured. The `fillEmpty(color)` method colours all the uncoloured dots on the board with the same colour.\nRepetition-based colouring Moving beyond the basics, other methods allow for more complex operations. The `repeatCommands(commands, positions)` method allows specifying a sequence of commands (e.g., a series of go and paint operations) and applying them to specific coordinates. The `copyCells(origin, destination)` method copies the colours from origin coordinates to destination coordinates.\nSymmetry-based colouring Finally, symmetrical colouring approaches are available. The `mirrorBoard(direction)` method, which reflects the coloured dots on the board onto the non-coloured ones, following the principle of symmetry. This mirroring can be done horizontally on the x-axis or vertically on the y-axis. The `mirrorCells(cells, direction)` method performs similar mirroring operations but on a specified set of dots. The `mirrorCommands(commands, direction)` method applies the mirroring to a list of commands."}, {"title": "3.2. Definition of the virtual CAT interpreter", "content": "The virtual CAT programming language interpreter [3] is a dedicated Dart package that can be integrated into any Flutter project, in our case, the virtual CAT app [4]. It translates student actions, including gesture interactions and arranged visual programming blocks, into executable machine-readable instructions. It analyses the user's input, converting actions into a formal algorithm specified using the CAT programming language.\nEach command that composes the algorithm, such as colour selections and other operations, undergoes a validation process to identify and address semantic errors. Notably, the interface's design, featuring predefined programming blocks and buttons, obviates the need for syntax checking, as it inherently eliminates the possibility of such errors, significantly streamlining the process. However, semantic errors can still occur during command execution, for instance, when users attempt to move outside the board boundaries using invalid directions or apply an inappropriate pattern for a colouring command.\nUpon validation, the code is executed, and real-time feedback is provided to the user, including the display of current progress on the colouring cross and the CAT score. If the interpreter detects errors, it handles them and provides users with error notifications and potential suggestions for correction."}, {"title": "3.3. Server and data handling", "content": "Considering the often limited availability of secure networks in educational settings, we implemented a technical framework to prioritise participant privacy and responsible data management [2]. To use this system, a local network infrastructure should be established using a router to connect all devices involved in the activity to a designated computer, which serves as the data collection point. Within this network, a database should be configured to securely receive and store the acquired data from the iPads. Afterwards, all collected data can be transferred from the local database to a dedicated repository through a private network connection."}, {"title": "4. Experimental", "content": "This section provides an in-depth overview of the formative evaluation conducted to evaluate the platform's usability, proficiency, and suitability.\nWe organised our pilot study as a participatory design involving three roles: a researcher, students and teachers [84]. Pupils were at the heart of the study, and their interactions with the platform were crucial for assessing our tool [20, 51, 94, 95]. The inclusion of children in the design and evaluation of their artefacts aimed at empowering them to take an active role in driving [37, 40, 45, 46] and critically reflecting on the developed tool, as well as making the process enjoyable and rewarding [10, 58, 59, 60]. In this phase, children are encouraged to share their evolving ideas as they perform the activity and test the tool [36, 38, 39, 101]. Teachers also played a vital role in facilitating the study, providing assistance as needed, and ensuring a smooth classroom experience.\nThe researcher, responsible for administering the activity, closely monitored pupil progress and interactions, collected empirical data on task performance and gathered feedback from students and teachers. Multiple data elicitation techniques, including think-aloud and observation, are employed to gain insights into the usability and effectiveness of the design [32, 34].\nWe gathered empirical data through a combination of user-centred techniques, including think-aloud sessions for real-time verbalised feedback, note-taking to capture observations and essential points, and observational analysis to understand underlying phenomena and user reactions, all following Human-Computer Interaction (HCI) principles and UX design practices [21, 32, 34, 36, 53, 54]."}, {"title": "4.1. Selection and participation", "content": "In March 2023, we conducted a pilot study to evaluate the virtual CAT as an assessment tool. It involved a sample of 31 students, 21 girls and 10 boys, from two schools in the Ticino canton (see Table 3). In each class, we conducted the activity with all the pupils who were prone to participate and were explicitly allowed by their parents.\nTo ensure a representative sample, we randomly selected schools, including a preschool class (4-6 ys) and two low secondary classes (11-12 ys), representing opposite ends of the compulsory education system in Switzerland. This aimed to demonstrate the platform's effectiveness for diverse school types, which can be extended to cover compulsory education.\nGiven the participation of young students, we strictly adhered to ethical guidelines and maintained transparent communication with pupils, parents, and schools [5, 70]. Initially, we obtained authorisation from school directors and teachers to conduct the research within their schools and classes. Next, we provided parents with an exhaustive document explaining the research project, data collection and storage procedures, and the personnel involved. We also requested their consent for their children's participation and publishing the collected data. Teachers obtained informed consent from parents without recording pupils' full names to safeguard privacy, ensuring data anonymity from the outset."}, {"title": "4.2. Training module", "content": "In our study, we recognised the importance of ensuring pupils' familiarity with the assessment tool. To achieve this, we designed and integrated a training module within the app that serves as a preparatory step for scholars, allowing them to become acquainted with the tool's interface and functionalities (see Figure A.2). This module included 15 sample cross-array schemas to solve. Additionally, a step-by-step guide to the app's features and functions was provided by a researcher during live, in-person sessions held in every class. During this phase, we fostered a collaborative and interactive environment to ensure pupil understanding of the activity. Teachers played a crucial role in assisting the researcher and guiding the class, ensuring they received clear guidance. The training sessions were held in groups based on device availability and lasted about 30-45 minutes.\nTo ensure a respectful testing environment for children, we tailored the training approach to their developmental level, including age-appropriate guidance, particularly directing kindergarten pupils to use only the gesture interface [33]. This adaptation involved simplifying instructions, ensuring comfort, and pacing the test accordingly. No data collection occurred during this phase, allowing students to concentrate on becoming comfortable with the tool and its functions."}, {"title": "4.3. Validation module and data collection", "content": "In the pilot study, data collection occurred within the validation session, a module integrated into the app to mimic the original activity with the 12 schemas to solve. Session and student details must be manually input into the app to start the data collection process (see Figures A.4 and A.5). Specifically, we recorded the date, the canton, the school where data was collected, and the class's grade level. Pupils' details were limited to their gender and date of birth. To uphold anonymity, each student was assigned a unique identifier.\nThroughout the entire activity, the application's log meticulously tracked every action performed by the pupils. This included recording the timestamp and type of operation carried out, such as adding, confirming, removing, or reordering commands, updating command properties like colours or directions, resetting the algorithm, changing the mode of interaction or visibility, confirming task completion, or surrendering. This comprehensive data collection was instrumental in assessing pupil performance, providing valuable insights for the study's analysis and findings.\nWe followed the current open science practice in Switzerland [88] and pseudonymised all the data. The data has been made available through Zenodo for public access [1] after removing any information that could identify the pupil (e.g., school and class)."}, {"title": "5. Results", "content": "This section presents the results of our formative evaluation aimed at assessing the tool and the interface prototypes, shown in Section 2.3, all in the context of our educational objectives. Additionally, we explore the tool's practicality for assessing algorithmic skills among K-12 pupils on a large scale. For this assessment, we established specific criteria and defined corresponding measures, customised to our educational context, as detailed in Tables 4 and 5, drawing upon established usability standards in the literature [34, 62, 66].\nWe evaluate usability by analysing the completion time across different interaction dimensions. Table 6 reveals that the gesture interface, both with and without feedback, leads to quicker task completion times than the visual programming interface counterparts. This observation aligns with expectations, as the gesture interface represents a less complex dimension of the artefactual environment, making it more intuitive and efficient for students. However, it's interesting to note that when considering the maximum completion times, the gesture interface, particularly with feedback, recorded the longest time. One possible explanation is that less proficient students may gravitate towards the gesture interface, which could lead to longer completion times. This suggests that interface choice may not solely reflect usability but also user proficiency. It is important to note that students who rely on visual feedback take more time to complete their tasks on average. This could indicate that while feedback aids pupils in task comprehension, it might extend the overall interaction duration as they process and respond to the feedback.\nAdditionally, Figure 4 provides insights into younger and older pupils' strategies to solve the tasks based on the algorithmic and interaction dimensions. Pupils across different age groups display a balanced usage of multiple interfaces and autonomy levels on the platform. Notably, pupils in the younger age group consistently used all available interfaces. In contrast, the distribution of interaction dimensions concerning the algorithm dimension in the older age category shows noteworthy variations. Older pupils tend to employ simpler algorithms more frequently with the gesture interface, while they conceive more complex algorithms with the visual programming interface. These differences highlight the nuanced ways in which students engage with different interfaces and how this impacts their problem-solving strategies. To evaluate proficiency, Figure 4 provides insights into younger and older pupils' strategies to solve the tasks. Pupils exhibit proficiency in generating algorithms across all three dimensions. In both age groups, 1D algorithms are the most used, consistent with findings from the unplugged CAT. Among younger pupils, about 31% employed straightforward OD algorithms, while only a tiny portion used 2D algorithms, indicating a preference for simplicity. For older pupils, 1D algorithms were even more prevalent, with a higher percentage of pupils adopting this approach. Additionally, there was a reliance on 2D algorithms, demonstrating a combination of sequential and branching strategies in their problem-solving."}, {"title": "5.1. User-driven platform refinements", "content": "Delving into the process of refining the virtual CAT, in this section, we explore how active collaboration with teachers and pupils from different age groups and schools guided the improvement of the platform. In particular, by collecting feedback and documenting pertinent observations about the users' interactions with the tool, including their command preferences and progress throughout the activity phases, we identified areas for refinement, leading to significant adjustments to enhance usability, proficiency and suitability. The final design of the user interfaces is visually depicted in Appendix A in Figures A.6, A.7 and A.8.\nCollaborating with the students yielded invaluable insights for enhancing the platform. First, we noticed that as time was running out, concluding the activity required individually confirming each schema. This process was unnecessary and time-consuming. Moreover, it led to schemas being marked as failed instead of not attempted. In response, we introduced a surrender button, allowing users to skip specific schemas (see Figure 5). This feature also allows users if they feel stuck on a task and want to move on. Additionally, to satisfy the curiosity of some pupils who wanted to explore forthcoming schema, we included navigation arrows (see Figure 5). In response to feedback provided by some students who found the visual feedback button unclear and difficult to interpret, we took measures to improve it. We replaced the button with two new icons, one with an open eye indicating active visual feedback and one with a closed eye implying that visual feedback is turned off (see Figure 5).\nDuring the course of the activities, some students showed interest in knowing the number of remaining schemas to be completed. For this reason, we decided to add a progress bar at the top of the central column of the interfaces (see Figure 6). This enhancement was aligned with the usability principle of \"progress indicators\" to help users plan and manage task sequencing while staying motivated and organised throughout the activities [34", "34": "."}, {"34": "."}, {"34": "."}, {"34": "."}]}