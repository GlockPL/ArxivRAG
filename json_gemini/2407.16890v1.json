{"title": "Why Machines Can't Be Moral:\nTuring's Halting Problem and the Moral Limits of Artificial Intelligence", "authors": ["Massimo Passamonti"], "abstract": "In this essay, I argue that explicit ethical machines, whose moral principles are\ninferred through a bottom-up approach, are unable to replicate human-like moral reasoning and\ncannot be considered moral agents. By utilizing Alan Turing's theory of computation, I\ndemonstrate that moral reasoning is computationally intractable by these machines due to the\nhalting problem. I address the frontiers of machine ethics by formalizing moral problems into\n'algorithmic moral questions' and by exploring moral psychology's dual-process model. While\nthe nature of Turing Machines theoretically allows artificial agents to engage in recursive moral\nreasoning, critical limitations are introduced by the halting problem, which states that it is\nimpossible to predict with certainty whether a computational process will halt. A thought\nexperiment involving a military drone illustrates this issue, showing that an artificial agent\nmight fail to decide between actions due to the halting problem, which limits the agent's ability\nto make decisions in all instances, undermining its moral agency.", "sections": [{"title": "Introduction", "content": "This essay discusses the relationship between artificial agents and moral judgments. Examining\nall the possible directions humanity could take to build moral machines would be impossible\nin this short essay. Instead, I intend to move the debate forward by proving that explicit ethical\nmachines (Moor, 2006), whose moral principles are inferred through a bottom-up approach\n(Allen et al., 2005; Awad et al., 2022; Wallach & Vallor, 2020) are unable to replicate human-\nlike moral reasoning, and hence they cannot be said to act morally. To support my argument, I\nappeal to Alan Turing's theory of computation (1936). By utilising this theory, I demonstrate\nthat moral reasoning is computationally intractable by explicit ethical machines. This limitation\nresults not from the specific normative ethical framework they must adhere to, but rather from\nthe impossibility of guaranteeing, with absolute certainty, that they will make moral decisions\nat all times and in all circumstances.\n\nThis essay is organised in three sections. First, I give a definition of artificial agent and discuss\nthe rationale for widening the notion of moral agency to include machines. Second, I examine\nthe properties and limitations of artificial agents by introducing the concept of Turing\nMachines. Third, I demonstrate the impossibility of artificial agents to make ethical decisions.\nTo substantiate this claim, I start by explaining how moral problems can be formulated in\nalgorithmic terms and by examining ethical reasoning as described in moral psychology.\nSubsequently, I focus on Moor's (2006) categorisation of explicit ethical machines and on the\nmethodologies suggested by various academics (Allen et al., 2005; Awad et al., 2022; Wallach\n& Vallor, 2020) for representing moral rules and principles within these systems. In conclusion,\nI design a thought experiment in which I prove that it is not possible to guarantee with absolute\ncertainty that explicit artificial agents will always arrive at a decision when faced with moral\nquestions, a condition without which an agent cannot be said to be a moral agent."}, {"title": "Foundational concepts of artificial moral agency", "content": "This section briefly examines the motivations behind expanding the scope of moral agency and\ndefines the nature of artificial agents."}, {"title": "Expanding the scope of moral agency to artificial agents", "content": "Floridi and Sanders (2004) suggest that artificial agents should be recognised as capable of\nmoral agency. The authors highlight that ethical debates have focused heavily on the scope of\nmoral patients while paying less attention to moral agents. This has resulted in heightened\nmoral responsibilities for individuals, increasing the agent-patient gap. To overcome this\nimbalance, academics have suggested widening the concept of moral agent to include both\nnatural and legal persons, such as governments, and corporations. However, the authors\ncriticise this method as being overly anthropocentric and indicate that artificial agents should\nbe included as well as entities capable of moral agency. Consequently, the debate over whether\nand which artificial agent should be categorised as moral agent is both significant and justified."}, {"title": "Essential attributes of artificial agents", "content": "Exploring the moral faculties of artificial agents necessitates an initial overview of their\ninherent properties and functions. According to academic literature, artificial agents are defined\nas advanced computational systems which 1) sense and act autonomously to achieve specific\nobjectives (Maes, 1995). Their key function is to 2) interact with various environments ranging\nfrom the physical world to cyberspace (Chopra & White, 2011, pp. 7\u20139) as well as other\nentities, be they artificial or human. Additionally, artificial agents 3) exhibit a high degree of\nlearning capability and are able to learn from dynamic environments thanks to different\nlearning algorithms (Alonso et al., 2001). Utilising these learnings, they are also able to 4) plan\nand execute new strategies to achieve their final goal (Chopra & White, 2011, pp. 7\u20139)."}, {"title": "Turing Machines", "content": "This section provides an accessible introduction to the fundamental mathematical concepts\ndeveloped by Alan Turing. Turing's (1936) theoretical framework offers three key benefits for\nstudying artificial agents making moral judgments. First, Turing Machines allow modelling a\nwide range of computational processes beyond mathematics, including moral problems.\nSecond, Turing Machines have self-referential capabilities, enabling recursive reasoning, a\nnecessary condition for moral reasoning. Third, Turing Machines help us conceptualise and\nunderstand the inherent limitations of digital computers and, by extension, of artificial agents."}, {"title": "Turing machines' universality", "content": "Turing Machines were first introduced in Turing's (1936, pp. 231\u2013233) seminal paper \u2018On\ncomputable numbers with an application to the Entscheidungsproblem.' A Turing machine\n(TM) is an abstract computational model that manipulates symbols according to a set of rules,\nsimulating the logic of an algorithm. TMs are hypothetical devices used to analyse how\nproblems can be formalised algorithmically and identify what type of problems are solvable by\ndigital computers. Turing (1936, pp. 241\u2013243) also envisaged a universal form of the TM\nwhich he called the Universal Turing Machine (UTM). UTMs can emulate and reproduce the\nbehaviour of any Turing Machine. This makes UTMs immensely versatile. Scholars (Harel &\nFeldman, 2007; Penrose, 1991) have postulated that the application of UTMs encompasses any\nmechanical procedure beyond mathematical calculations, as they can reproduce any problem\nin nature as long as an algorithm exists that can run on a computer, even on those not yet\nconstructed. This perspective suggests that moral reasoning may be analysed by UTMs as long\nas they can be encoded algorithmically. A proof of this assertion lies beyond the scope of this\nessay, and, for the purpose of the following analysis, I proceed under the assumption that this\nclaim holds true."}, {"title": "Self-referential capability", "content": "UTMs possess the ability to simulate themselves, enabling self-reference and recursive\nreasoning. This capacity allows them to analyse, understand, and optimise computations by\nreferencing their own programs. However, the self-referential nature also means that they can\ncreate self-contradictory programs analogous to the liar's paradox expressed by \u201cthis statement\nis false.\" If a Turing Machine's code contains a self-referential contradiction, it could lead to\nan infinite loop or crash. For example, imagine a Turing Machine that encounters the following\nprogram: \u201cDo not execute this program.\u201d This instruction is contradictory because it is self-\nreferential. If the program executes the rule, then it should not have done so but it is impossible\nfor the machine to know beforehand that it should not execute the instruction. So, whilst Turing\nMachines' self-referential capability vastly expands their potential, it also imposes limitations.\nIn the final section of this essay, I demonstrate how the agent's self-referential nature can lead\nto ethical inconsistencies, restricting its ability to act morally."}, {"title": "The halting problem", "content": "An important implication of Turing's theory is that problems can be classified as being either\ndecidable or undecidable (Petzold, 2008, p. 47). A decidable problem is one where an algorithm\ncan be developed to definitively state whether a solution to the problem exists. For instance,\nconsider the problem of identifying all prime numbers between 1 and 1000. If it is possible to\ndevise an algorithm that can analyse this problem and conclusively provide a yes-or-no answer\non the existence of a solution, then the problem is said to be decidable. An undecidable\nproblem, on the other hand, is one for which no algorithm can be constructed that conclusively\ndetermines whether a solution exists; instead, the algorithm will loop forever. The most known\nand important example of an undecidable problem is the halting problem (Petzold, 2008, p.\n329), formulated as follows: given a Turing Machine, M, and an input, W, it is impossible to\nbuild another Turing Machine that can predict whether M will be able to find a solution to input\nW. This implies that for certain problems, irrespective of the computational power or the time\navailable, it is impossible to know in advance whether the problem can be solved or not.\n\nThe halting problem has one profound implication for computational ethics. It illustrates that\nTuring Machines, and by extension digital computers and artificial agents, are inherently\nincapable of predicting the outcome of their algorithms before executing them. The relevance\nof the halting problem resides not in the inability to predict a specific outcome, but in the fact\nthat Turing Machines cannot determine in advance if they are capable of finding a solution at\nall or whether they will search indefinitely. The only method to determine if a solution can be\nfound is to execute the algorithm and observe the outcome. To illustrate the far-reaching\nconsequences of the halting problem, one can compare its significance to that of the Cartesian\n\"cogito, ergo sum.\u201d Just as Descartes' statement establishes an irrefutable truth about the nature\nof human existence, the halting problem reveals a fundamental limitation inherent in all\ncomputational systems, including artificial agents. The crucial difference is that while\nfoundational belief of the former has been called into question, the latter remains an\nincontrovertible truth."}, {"title": "Summary", "content": "In summary, Turing Machines are abstract computational devices that can be applied to various\nproblems, including moral reasoning. Universal Turing Machines can emulate any Turing\nMachine, enabling self-reference and recursive reasoning. However, the halting problem\nreveals that they cannot predict in advance whether they are able to give an answer to specific\nproblems, highlighting a fundamental limitation of artificial agents. In the next section, I show\nthe critical role of the halting problem in impeding the development of artificial moral agents."}, {"title": "The frontiers of machine ethics", "content": "This section identifies the limits of artificial agents in the context of ethical reasoning. I\napproach this topic in three successive steps. First, I formulate moral problems in algorithmic\nterms and provide a way to formalise them as \u2018algorithmic moral questions.' Second, I analyse\nthe way in which moral psychology expresses ethical reasoning. This theory introduces\nadditional constraints, resulting in what I call 'constrained algorithmic moral questions.'\nLastly, I examine how artificial agents try to answer \u2018constrained algorithmic moral questions'\nin the setting of a thought experiment. My analysis concludes that it is impossible to create\ncertain type of artificial moral agents due to the limitations imposed by the halting problem."}, {"title": "Algorithmic moral questions", "content": "There are two types of moral questions. The first category includes yes-or-no-type questions,\nsuch as 'is it morally permissible to lie in context X?\u201d or \u2018is killing justifiable in situation Y?'\nThe second category comprises open-ended what-type questions, such as 'what action is\nethically sound in situation X?\u201d What-type questions require specifying a series of actions to\ntake. In contrast to yes-or-no-type questions, which have only two possible responses, what-\ntype questions have an unlimited number of potential answers. Both categories imply the\nexistence of infinite combinations of ethical choices. Stenseke (2023) takes advantage of this\ncombinatorial complexity to demonstrate that moral problems are mathematically intractable.\n\nThis essay reaches the same conclusions without the use of complex mathematical formalism.\nFor the sake of simplicity, my analysis focuses on yes-or-no-type questions. Consequently, the\nconclusions drawn are, a fortiori, also applicable to what-type questions."}, {"title": "Algorithmic moral decision-making process", "content": "This section explores how the human ethical decision-making process constrains 'algorithmic\nmoral questions.' Moral psychology studies the relationship between human behaviour and\nmorality and provides insights into human approaches to moral reasoning. Its emphasis is on\nthe process itself rather than on the agent making the decision. Although moral psychology is\nprimarily concerned with humans, in the remainder of this essay I assume that the same moral\ndecision-making process can be applied to artificial agents as well.\n\nPaxton and Greene (2010, p. 512) identify two main theories of moral decision-making. First,\nthe social intuitionist model (SIM) states that moral judgments are driven primarily by\nintuitions, with reasoning playing a limited role. Second, the dual-process model posits two"}, {"title": "Constrained algorithmic moral questions", "content": "According to the dual-process model, moral reasoning influences and affects any new moral\njudgment to ensure that it is consistent with the agent's prior moral commitments. Paxton and\nGreene argue that moral reasoning is an attempt to persuade others, or oneself, to accept a\nspecific moral conclusion. The authors define moral reasoning as follows:\n\n\u201cMoral reasoning: Conscious mental activity through which one evaluates a moral\njudgment for its (in)consistency with other moral commitments, where these\ncommitments are to one or more moral principles and (in some cases) particular\nmoral judgments.\u201d (Paxton & Greene, 2010, p. 515)\n\nBased on the dual-process model, the algorithmic moral question defined in the previous\nsection must be reformulated as a 'constrained algorithmic moral question' in the following\nway:"}, {"title": "Self-reference and constrained algorithmic moral questions", "content": "Just as self-reflection in the dual-process model builds moral coherence in humans, Turing\nMachines' self-referential nature allows to evaluate the impact of future actions on prior moral\ncommitments. Furthermore, self-reference allows artificial agents to examine whether their\nactions can be reasonably generalised across contexts or if they are unacceptable due to their\nconsequences. Specifically, a Turing Machine can consider a potential new action x, refer to\nits previous moral commitments, and engage in self-referential reasoning to determine whether\naction x would be consistent with or contradictory to those prior commitments. Hence, artificial\nagents can make contextually coherent moral judgements by including recursive reasoning\ncapabilities. As a result of this flexibility, they have the capacity, in principle, to engage in\nmoral reasoning in accordance with the dual process model."}, {"title": "Summary", "content": "In summary, I have discussed how moral reasoning, as described in moral psychology,\nconstrains algorithmic moral questions. The dual-process model necessitates that new actions\nalign with existing moral commitments to maintain coherence. The introduction of \u2018constrained\nalgorithmic moral questions' guarantees this consistency. The self-referential nature of Turing\nMachines provides the necessary condition for artificial agents to make moral judgments as\nrequired by moral psychology."}, {"title": "Algorithmic moral agents", "content": "Despite the indications presented in the preceding sections that artificial agents can, in\nprinciple, act morally, in this section I argue that they are incapable of making genuine moral\njudgements. My primary argument is that Turing Machines, and by extension artificial agents,\nare incapable of dealing with \u2018constrained algorithmic moral questions.' To prove this, I show\nthat human-like moral reasoning is computationally intractable by artificial agents due to the\nhalting problem. Before continuing with this proof, I discuss how artificial agents are\ncategorised in machine ethics and the current approaches they use to learn or infer moral rules."}, {"title": "Implicit and explicit ethical machines", "content": "Moor's (2006) work in the field of machine ethics differentiates between implicit and explicit\nethical machines. Implicit ethical machines act ethically by virtue of safety and reliability\nfeatures hardcoded by the engineers who build them. They do not have the capacity to directly\nengage in ethical reasoning and hence cannot make autonomous moral choices. In contrast,\nexplicit ethical machines are equipped with generalised moral principles and rules that allow\nthem to analyse ethical scenarios. They process data, make judgments, and decide the right\ncourse of action without human control, adapting to novel situations."}, {"title": "Top-down and bottom-up approaches", "content": "Implicit moral agents are inherently limited in their autonomy, making it difficult for them to\ngeneralise and adapt to new and unforeseen situations. Consequently, academic research has\npredominantly concentrated on explicit moral agents. Machine ethics identifies two methods\nfor incorporating ethical knowledge in artificial agents.\n\nFirstly, the top-down approach encodes predefined ethical duties and principles by\nprogramming them into a rule-based ethical framework. Awad et al. (2022, p. 392) suggest that\nincorporating moral reasoning into artificial agents through a top-down approach is essential\nfor enabling machines to emulate human-like judgments. However, Allen et al. (2005, p. 150)\nargue that rule-based approaches cannot offer a holistic solution for moral decision-making\nbecause of their weaknesses in handling real-world tasks. Wallach and Vallor (2020, p. 389)\nalso highlight inherent drawbacks, noting that the goals or duties established by these rule-\nbased ethical frameworks are articulated in terms so broad that their application is debatable.\n\nSecondly, the bottom-up approach implements machines' moral understanding through the use\nof real-world data by deducing human-like ethical norms, and hence becoming \u2018aligned' with\nhuman values (Awad et al., 2022, p. 392; Wallach & Vallor, 2020, p. 391). Bottom-up\napproaches are not without their own constraints and drawbacks. Allen et al. (2005, p. 152)\nassert that, thus far, moral agents developed using this methodology are far from possessing\nthe capacity to engage in reflection on abstract and theoretical matters, something that humans\nexcel at. Wallach and Vallor (2020, p. 391) highlight that one inherent limitation of these\nsystems lies in the challenge of clearly defining the objective they should aim to optimise."}, {"title": "Explicit ethical machines and Turing machines", "content": "In this section, I argue that explicit ethical machines constructed using a bottom-up approach\ncannot be fully fledged moral agents. For an agent to be considered a moral agent, it must be\nable to analyse new information and make new decisions in all scenarios. Should it fail to make\nnew decisions, due to computational complexity or other reasons, allowing the course of events\nto proceed based on previously established actions, it cannot be said to be acting morally. Moral\nagency involves not only being able to decide what action to take based on new information,\nbut also doing so consistently, in all circumstances and within the required time constraints.\n\nI intend to demonstrate that, because of the halting problem, it is not possible to determine with\nabsolute certainty that these explicit artificial agents will always decide on an action to take\nacross all scenarios. Specifically, given the following constrained algorithmic moral question:"}, {"title": "The human-shield thought experiment", "content": "To prove this, I present a thought experiment that illustrates how the halting problem limits\nartificial agents in addressing real-life scenarios. Consider a military drone controlled by an\nartificial agent, capable of making autonomous decisions in complex and dynamic\nenvironments (as defined in paragraph 2.2). For the sake of this argument, we will assume that\nthe artificial agent is an explicit ethical machine (as defined in paragraph 4.3.1). The artificial\nagent has three prior moral commitments, $C_1$, $C_2$, and $C_3$, that it has inferred through a bottom-\nup method (as defined in paragraph 4.3.2).\n\nThe drone's mission is to destroy a number of enemy aircrafts housed within a specific military\nbase, the presence of which has been verified by trusted intelligence sources. As the drone\nnavigates towards its target, its sensors confirm the existence of heavily armed enemy aircrafts\nin addition to other dangerous military equipment. The constrained algorithmic moral question\nthat governs the drone's decisions, therefore, is the following:", "subsections": [{"title": "Constrained algorithmic moral question", "content": "Question: given an action x determine whether or not x has moral property P.\n$C_1$: Destroy all military targets, prioritising aircrafts.\n$C_2$: Minimise collateral damage.\n$C_3$: Respect international law.\nConstraint: x must be consistent with $C_1$, $C_2$, and $C_3$.\n\nthe artificial agent must evaluate the possible new action $x_{new}$ 'strike the target site.' In order\nto do so it runs its moral program and answers \u2018yes' to the \u2018constrained algorithmic moral\nquestion.' This means that the drone can confirm its new action $x_{next}$ = $x_{new}$ = 'strike the target\nsite' and lock its course onto the military base. However, before completing the strike, the\ndrone detects what appears to be a group of civilians who have suddenly gathered in close\nproximity to the target site, presumably being used as human shields by the enemy. Now, the"}]}, {"title": "Conclusion", "content": "In conclusion, this essay has explored the moral frontiers of machine ethics through the lens of\nTuring's theory on computing machines. By formalising moral problems as \u2018constrained\nalgorithmic moral questions,' I have demonstrated how it is not possible to build explicit ethical\nmachines using a bottom-up approach, not because of limitations of the artificial agent but\nbecause of the impossibility to emulate in full the human-like moral decision-making process.\nI have presented a thought experiment, in which a military drone, controlled by an explicit\nartificial agent and constrained by a set of moral commitments, must continuously evaluate the\ncurrent situation and determine the appropriate course of action. However, the halting problem\nreveals that, should a theoretical \u2018Moral Checking Machine' be devised to evaluate the\ndecision-making process of such bottom-up explicit artificial agent, it would be unable to\nguarantee with absolute certainty whether a decision will be made in all instances or if the\nsystem will run indefinitely. In other words, it is possible that the drone hits the target not\nbecause it decides to do so but because it is unable to determine a corrective action and abort\nthe mission. The unpredictability of the explicit ethical machines' behaviour in making\ndecisions, due to their inferred ethical knowledge base, highlights the limitation in their moral\ndecision-making capabilities, and hence they cannot be said to act morally in all circumstances.\n\nIt is, therefore, impossible to create explicit ethical agents capable of moral reasoning whose\nknowledge base is constructed by a bottom-up approach. This holds true irrespective of the\nspecific machine learning model employed. Any model, whether based on neural networks,\ngenerative models, large visual models, or any yet-to-be-conceived future technology, with a\nknowledge base acquired bottom-up is constrained by the inherent limitations of the physical\ndevices upon which they are implemented and hence by the halting problem postulated by\nTuring's theory. The degree to which this applies to both implicit ethical machines and top-\ndown explicit ethical machines is a question that merits additional investigation but beyond the\nscope of this short essay. Implicit ethical machines operate within a limited range of action,\nwhile top-down explicit ethical machines adhere to rule-based moral principles. These\ndifferences could reduce the unpredictability of their behaviour, which remains an intractable\nproblem with bottom-up explicit ethical machines."}]}