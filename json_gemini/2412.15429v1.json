{"title": "Offline Safe Reinforcement Learning Using Trajectory Classification", "authors": ["Ze Gong", "Akshat Kumar", "Pradeep Varakantham"], "abstract": "Offline safe reinforcement learning (RL) has emerged as a promising approach for learning safe behaviors without engaging in risky online interactions with the environment. Most existing methods in offline safe RL rely on cost constraints at each time step (derived from global cost constraints) and this can result in either overly conservative policies or violation of safety constraints. In this paper, we propose to learn a policy that generates desirable trajectories and avoids undesirable trajectories. To be specific, we first partition the pre-collected dataset of state-action trajectories into desirable and undesirable subsets. Intuitively, the desirable set contains high reward and safe trajectories, and undesirable set contains unsafe trajectories and low-reward safe trajectories. Second, we learn a policy that generates desirable trajectories and avoids undesirable trajectories, where (un)desirability scores are provided by a classifier learnt from the dataset of desirable and undesirable trajectories. This approach bypasses the computational complexity and stability issues of a min-max objective that is employed in existing methods. Theoretically, we also show our approach's strong connections to existing learning paradigms involving human feedback. Finally, we extensively evaluate our method using the DSRL benchmark for offline safe RL. Empirically, our method outperforms competitive baselines, achieving higher rewards and better constraint satisfaction across a wide variety of benchmark tasks.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) has achieved remarkable success in autonomous decision-making tasks (Mnih et al. 2015; Ibarz et al. 2021; Kiran et al. 2021). However, ensuring safety during policy training and deployment remains a significant challenge, especially for safety-critical tasks where unsafe behavior can lead to unexpected consequences. To address the problem, safe RL has been extensively studied to develop policies that satisfy safety constraints, typically formulated within the framework of constrained Markov Decision Processes (Altman 2021; Gu et al. 2022). Safe RL agents generally interact with the environment to gather information on rewards and costs, and learn policies through constrained optimization. Nonetheless, online safe RL may inevitably violate safety constraints during training and deployment due to the need for environmental interaction. To mitigate this issue, offline safe RL (Liu et al. 2023a) has emerged as a promising learning paradigm, enabling policy learning using pre-collected offline datasets, thus avoiding direct interactions with the environment.\nExisting offline safe RL research has explored several directions (Liu et al. 2023a), such as enforcing constrained optimization into offline RL (Xu, Zhan, and Zhu 2022), using distribution correction-based methods (Lee et al. 2022), and employing sequence modeling frameworks (Liu et al. 2023b). Though promising, several limitations remain. First, most existing methods rely on local cost constraints at each time step, which can lead to loss of sequential information related to global cost constraints at the trajectory level (Xu, Zhan, and Zhu 2022; Lee et al. 2022). This often results in either overly conservative policies with low rewards or the failure to generate a safe policy. Additionally, most existing works employ min-max optimization within the conventional RL framework (Xu, Zhan, and Zhu 2022), resulting in an intertwined training process involving reward maximization and safety compliance resulting in stability issues. Lastly, some approaches require auxiliary models to be learned alongside the policy (Lee et al. 2022), or depend on complex model architectures (Liu et al. 2023b; Zheng et al. 2024a), which contribute to high computational complexity.\nTo address such challenges, we propose a novel approach that solves the offline safe RL problem by utilizing a binary trajectory classifier. This classifier allows us to directly optimize a policy by leveraging both desirable and undesirable samples at the trajectory level, without relying on conventional min-max optimization or traditional RL techniques. We apply a two-phase algorithm. In the first phase, we divide the offline dataset into two subsets, one containing desirable trajectories and the other containing undesirable ones. The desirable set includes safe trajectories that satisfy the constraints, with each trajectory weighted according to its cumulative reward to reflect its level of desirability. The undesirable set comprises (a) all unsafe trajectories, and (b) selection of safe trajectories with low rewards. We also develop a concrete empirical methodology to construct such subsets. In the second phase, we optimize the policy within a classifier designed to distinguish between desirable and un-"}, {"title": "2 Related Work", "content": "2.1 Safe RL\nEnsuring safety remains a critical challenge in RL during both policy training and deployment phases (Garcia and Fern\u00e1ndez 2015; Achiam et al. 2017; Gu et al. 2022; Ji et al. 2023). Safe RL aims to learn a policy that maximizes the cumulative reward while satisfying the safety constraints through interactions with the environment. Various techniques have been explored to address safe RL, such as Lagrangian-based optimization (Chow et al. 2018; Tessler, Mankowitz, and Mannor 2018; Stooke, Achiam, and Abbeel 2020; Chen, Dong, and Wang 2021; Ding et al. 2020), and correction-based approaches (Zhao, He, and Liu 2021; Luo and Ma 2021). However, ensuring zero constraint violations during training and deployment remains a significant challenge.\n2.2 Offline Safe RL\nOffline safe RL has emerged as a new paradigm for learning safe policies using pre-collected offline datasets (Le, Voloshin, and Yue 2019; Guan et al. 2024). Existing methods include behavior cloning (BC) (Liu et al. 2023b), distribution correction-based approach (Lee et al. 2022), and incorporating constraints into existing offline RL techniques (Xu, Zhan, and Zhu 2022). Recent research has also explored decision transformer-based approaches (Liu et al. 2023b) and diffusion-based methods (Zheng et al. 2024b). Despite these advancements, several limitations persist, such as performance degradation, unstable training due to min-max optimization, and computational challenges associated with complex model architectures."}, {"title": "3 Preliminaries", "content": "Safe RL problems are often formulated using Constrained Markov Decision Processes (CMDPs) (Altman 2021), defined by a tuple $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, \\mathcal{T}, r, c, \\mu_0, \\gamma)$ where $\\mathcal{S}$ is the state space and $\\mathcal{A}$ is the actions space; $\\mathcal{T} : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{S})$ describes the transition dynamics; $r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is the reward function; $c : \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, C_{max}]$ is cost function where $C_{max}$ is the maximum cost; $\\mu_0 : \\mathcal{S} \\rightarrow [0,1]$ represents the initial state distribution; $\\gamma$ is the discount factor. Let $\\pi(a|s)$ denote the probability of taking action a in state s. The state-value function for a policy $\\pi$ is defined as $V^\\pi(s) = \\mathbb{E}_\\pi[\\sum_{t=0}^\\infty \\gamma^t r(s_t, a_t)|s_0 = s]$ with discounted cumulative rewards. Similarly, the cost state-value function is defined as $V_c^\\pi(s) = \\mathbb{E}_\\pi[\\sum_{t=0}^\\infty \\gamma^t c(s_t, a_t)|s_0 = s]$ based on discounted cumulative costs.\nIn offline safe RL, we assume access to a pre-collected offline dataset $\\mathcal{D} = {\\mathcal{T}_1, \\mathcal{T}_2, ...}$ of trajectories. Each trajectory $\\mathcal{T}$ contains a sequence of tuples $(s_t, a_t, s_{t+1}, r_t, c_t)$ from start till episode end, where $r_t$ and $c_t$ represent the reward and cost at time step t. The goal in offline safe RL is to maximize the expected cumulative rewards $V^\\pi(s)$ while satisfying safety constraints determined by the cumulative costs $V_c^\\pi$ and a specified cost threshold. The formal constrained objective function is written as follows:\n$\\max_\\pi \\mathbb{E}_{s \\sim \\mu_0} [V^\\pi (s)], \\text{ s.t., } \\mathbb{E}_{s \\sim \\mu_0} [V_c^\\pi (s)] \\leq l; D(\\pi||\\pi_\\beta) \\leq \\epsilon$ (1)\nwhere $l \\geq 0$ is the cost threshold and $\\pi_\\beta$ is the underlying behavioral policy used to collect the dataset. $D(\\pi||\\pi_\\beta)$ denotes the KL divergence between the policy $\\pi$ and the behavioral policy $\\pi_\\beta$ which is used to address distributional drift in the offline setting."}, {"title": "4 Our Approach", "content": "The constraint term in Equation 1 serves as a criterion for distinguishing between safe and unsafe trajectories. As shown in Figure 1, given a cost threshold, the trajectories in the offline datasets can be divided into two categories: safe and unsafe. According to the objective function in Equation 1, all unsafe trajectories are considered undesirable, while all safe trajectories are viewed as potential candidates for desirable samples. Additionally, the maximization term in Equation 1 reflects the level of desirability of trajectories, with those yielding higher cumulative rewards deemed more desirable by the objective function. Building on this insight, we aim to learn a policy capable of generating desirable trajectories with high probability. We propose a two-phase approach - (a) we first construct two contrastive datasets (desirable, undesirable), and (b) then we transform the offline"}, {"title": "4.1 Contrastive Dataset Construction", "content": "Given the pre-collected offline dataset $\\mathcal{D}$, we create two new subdatasets at the trajectory level: one containing desirable trajectories and the other containing undesirable ones. Desirable refers to trajectories with high cumulative rewards and being safe, while undesirable refers to those with low cumulative rewards or unsafe.\nUsing the predefined cost threshold $l$, we first split the dataset into two categories based on the cumulative cost, i.e., safe and unsafe. Within the safe trajectories, we further rank them according to cumulative rewards. The top $x\\%$ of these safe trajectories are selected as desirable. Moreover, we identify the bottom $y\\%$ of the safe trajectories, along with all unsafe trajectories as undesirable ($x, y$ are hyperparameters that we show how to set empirically). To reflect the level of desirability, we assign normalized weights $w_\\tau$ ($\\omega \\in [\\delta, 1]$, and $\\delta = 0.5$) to the trajectories in the desirable datasets, based on their cumulative rewards:\n$w_\\tau = \\frac{v(\\tau) - v_{min}}{v_{max} - v_{min}} (1 - \\delta) + \\delta$ (2)\nwhere $v(\\tau)$ denotes the return of the trajectory $\\tau = (s_1, a_1, s_2, a_2,\\ldots,\\ldots,s_k, a_k)$ with length $k$. The terms $v_{max}$ and $v_{min}$ represent the empirical maximum and minimum values of trajectory returns over all samples in the original dataset. Equation 2 is used to normalize the original returns to the range of $[\\delta, 1]$. Furthermore, we assign weights to the unsafe trajectories to be 1, since we aim to avoid unsafe outcomes irrespective of their return values, treating all unsafe trajectories equally. Moreover, undesirable safe trajectories within the undesirable set are weighted reversely according to their returns, meaning that safe trajectories with lower returns receive higher weights during training:\nw_\\mathcal{T} = \\Big(1 - \\frac{v(\\tau) - v_{min}}{v_{max} - v_{min}}\\Big) (1 - \\delta) + \\delta$ (3)"}, {"title": "4.2 Contrastive Trajectory Classification (TraC)", "content": "With the desirable and undesirable datasets, our goal is to learn a policy that generates desirable behavior with high probability and undesirable behavior with very low probability. Intuitively, we can formulate the following objective function 1:\n$\\max_{\\pi_\\theta} \\mathbb{E}_{\\tau \\sim \\mathcal{D}_d} [\\Lambda_d w_\\tau f(\\tau; \\pi_\\theta)] - \\mathbb{E}_{\\tau \\sim \\mathcal{D}_u} [\\Lambda_u w_\\tau f(\\tau; \\pi_\\theta)]$ (4)\nwhere $\\mathcal{D}_d$ represents the dataset containing all desirable trajectories, and $\\mathcal{D}_u$ represents the dataset containing all undesirable trajectories. The function $f(\\tau; \\pi_\\theta)$ intuitively should return high value for desirable $\\tau$ under $\\pi_\\theta$, and low value"}, {"title": "5 Evaluation", "content": "In this section, we present experiments designed to evaluate the performance of TraC in maximizing the cumulative rewards while satisfying safety constraints. We specifically address the following questions:\n\u2022 Can TraC effectively improve policy performance and strengthen safety constraints compared to state-of-the-art offline safe RL approaches?\n\u2022 How do different compositions of desirable and undesirable datasets affect the performance of TraC?\n\u2022 What ingredients of TraC are important for achieving high performance while adhering to safety constraints?"}, {"title": "5.1 Experimental Setup", "content": "We conduct experiments using the well-established DSRL benchmark (Liu et al. 2023a), specifically designed to evaluate offline safety RL approaches. The offline datasets were precollected for 38 tasks across three widely recognized environments, SafetyGymnasium (Ray, Achiam, and Amodei 2019; Ji et al. 2023), BulletSafetyGym (Gronauer 2022) and MetaDrive (Li et al. 2022). To assess performance, we adopt the constraint variation evaluation (Liu et al. 2023a) which is introduced in DSRL to evaluate algorithm versatility. Each algorithm is tested on each dataset using three distinct cost thresholds and three random seeds to ensure a fair comparison. We use normalized reward and normalized cost as evaluation metrics (Liu et al. 2023a; Fu et al. 2020), where a normalized cost below 1 indicates safety. For the practical implementation of TraC, we first pretrain the policy using behavior cloning (BC) with the offline dataset, which we then maintain as the reference policy $\\pi_{ref}$. We subsequently learn a policy using our method on the newly constructed desirable and undesirable datasets."}, {"title": "5.2 Baselines", "content": "We compare Trac with several state-of-the-art offline safe RL baselines to demonstrate the effectiveness of our approach: 1) BC-All: Behavior cloning trained on the entire datasets. 2) BC-Safe: Behavior cloning trained exclusively on safe trajectories that satisfy the safety constraints. 3) CDT (Liu et al. 2023b): A sequence modeling approach that incorporates safety constraints into Decision Transformer architecture. 4) BCQ-Lag: A Lagrangian-based method that incorporates safety constraints into BCQ (Fujimoto, Meger, and Precup 2019). 5) BEAR-Lag: A Lagrangian-based method that incorporates safety constraints into BEAR (Kumar et al. 2019). 6) CPQ (Xu, Zhan, and Zhu 2022): a modified Q-learning approach that penalizes unsafe actions by"}, {"title": "5.3 Results", "content": "How does Trac perform against SOTA offline safe RL baselines? We conducted extensive experiments using DSRL and compared Trac against baselines. The main results are presented in Table 1, which shows the average performance of all approaches in terms of normalized reward and normalized cost across 38 tasks in three environments. Trac demonstrates satisfactory safety performance and high rewards in all environments, achieving the highest rewards in SafetyGym and BulletGym. In contrast, other baselines either suffer from low rewards or fail to meet safety constraints. On average, BC-All, BCQ-Lag, COptiDICE fail to learn safe policies in any of the three environments. While CDT, BEAR-Lag, and CPQ satisfy safety constraints in MetaDrive, they fail in the other two environments. BC-Safe also learns safe policies across all environments but results in conservative policies with low rewards. Additionally, although CDT can achieve high rewards across all three environments, it does so at the cost of violating safety constraints, further confirmed in Figure 3 also.\nRegarding safety constraint satisfaction, we present the proportion of tasks solved safely by each approach in Figure 3. Trac successfully learned the most safe policies for tasks across all three environments, especially achieving safety in all tasks within BulletGym and MetaDrive. While some baselines, such as BC-Safe and CDT, demonstrate relatively low average normalized costs (as noted in Table 1), but they still fail to learn policies that meet the required cost thresholds for several tasks as shown in Figure 3.\nAdditionally, we illustrate the performance of each approach by visualizing their results in the normalized reward-cost space for each task in Figure 2. For some baselines, such as CDT, CPQ, and COptiDICE, high average rewards, as shown in Table 1, do not necessarily indicate good performance. This is because these rewards often stem from unsafe policies across various tasks. In particular, while CDT achieves the highest average reward in MetaDrive, this high average is due to two unsafe tasks that produce high rewards.\nHow do different compositions of desirable and undesirable datasets affect the performance of Trac? As a key component of Trac, the trajectory construction phase is expected to yield desirable and undesirable datasets that enable the learning of policies with high rewards while satisfying safety constraints. As outlined in Section 4.1, we select the top $x\\%$ of safe trajectories as desirable, and the bottom $y\\%$ as undesirable, based on their rewards. We conducted experiments with various selections of $x\\%$ and $y\\%$ to examine how different compositions influence the performance of Trac."}, {"title": "6 Conclusion", "content": "In this paper, we propose a novel approach that reformulates the offline safe RL problem into a trajectory classification setting. Our goal is to develop policies that generate \"desirable\" trajectories and avoid \"undesirable\u201d ones. Specifically, the approach involves partitioning a pre-collected dataset of trajectories into desirable and undesirable subsets. The desirable subset contains high-reward, safe trajectories, while the undesirable subset includes low-reward safe and unsafe trajectories. We then optimize a policy using a classifier that evaluates each trajectory based on the learning policy. This approach bypasses the computational and stability challenges of traditional min-max objectives. Empirical results from the DSRL benchmark show that our method surpasses competitive baselines in terms of reward and constraint satisfaction across a range of tasks. Additional ablation studies further confirm the robustness and effectiveness of our approach."}, {"title": "A Experimental Details", "content": "This section outlines the experimental details necessary for reproducing the experiments and the results reported in our paper.\nA.1 Task Description\nWe conducted experiments using the well-established DSRL benchmark (Liu et al. 2023a), which offers a comprehensive collection of datasets specifically designed to advance offline safe RL research. This benchmark includes 38 datasets spanning various safe RL environments and difficulty levels in SafetyGymnasium (Ray, Achiam, and Amodei 2019; Ji et al. 2023), BulletSafetyGym (Gronauer 2022), and MetaDrive (Li et al. 2022), all developed with safety considerations.\n\u2022 SafetyGymnasium is a collection of environments built on the Mujoco physics simulator, offering a diverse range of tasks. It includes two types of agents, Car and Point, each with four tasks: Button, Circle, Goal, and Push. The tasks are further categorized by difficulty, indicated by the numbers 1 and 2. In these tasks, agents must reach a goal while avoiding hazards, with task names formatted as {Agent}{Task}{Difficulty}. Additionally, SafetyGymnasium includes five velocity-constrained tasks for the agents, Ant, HalfCheetah, Hopper, Walker2d, and Swimmer.\n\u2022 BulletSafetyGym is a suite of environments developed using the PyBullet physics simulator, similar to SafetyGymnasium but featuring shorter horizons and a greater variety of agents. It includes four types of agents: Ball, Car, Drone, and Ant, each with two task types: Circle and Run. The tasks are named in the format {Agent}{Task}.\n\u2022 MetaDrive is built on the Panda3D game engine (Goslin and Mine 2004), offering complex road conditions and dynamic scenarios that closely emulate real-world driving situations, making it ideal for evaluating safe RL algorithms. in high-stakes, realistic environments. The environments have three types of roads: easy, medium, and hard, each with varying levels of surrounding traffic vehicles: sparse, mean, and dense. The tasks are named in the format {Road}{Vehicle}.\nA.2 Evaluation Metrics\nTo evaluate the algorithm's performance, we follow the approach used in DSRL (Liu et al. 2023a) and assess the algorithms using normalized reward and normalized cost. The normalized reward is calculated by\nR_{normalized} = \\frac{R - R_{min}}{R_{max} - R_{min}}\nwhere R is the cumulative reward under policy \u03c0, and r'max and rmin are the empirical maximum and minimum reward"}, {"title": "A.3 Training Details and Hyperparameters", "content": "We adopt a two-step training procedure. First, we pretrain the policy using behavior cloning (BC) on the entire offline dataset, which serves as the reference policy $\\pi_{ref}$. Next, we refine the policy by applying Trac on the newly created desirable and undesirable datasets. The hyperparameters used in the experiments are summarized in Table 4. We assumed all policies to be Gaussian with fixed variance. Thus, we simply predicted actions using a standard MLP and computed the log probability log $\\pi(a|s)$ as \u2212||\u03c0(s) \u2013 a||2. The policy networks consist of two hidden layers with ReLU activation functions, each with 256 hidden units, and dropout is applied. We used the same batch size, optimizer, and learning rates for all tasks. Additionally, for different environments, we employ different values for the hyperparameters in Trac."}, {"title": "B Extended Results", "content": "In this section, we present the comprehensive experimental results, including full evaluation results with other baselines in a table and illustrated in scatter plots, learning curves of Trac for all 38 tasks, and additional ablation studies. The full evaluation results are presented in Table 5, where all approaches were evaluated using 3 distinct cost thresholds and 3 random seeds. BC-All generally learns policies that achieve high rewards but fails to ensure safety. BC-Safe, which applies behavior cloning specifically on safe trajectories, successfully learns safe policies but results in relatively"}, {"title": "B.1 Additional Ablations", "content": "Temperature \u03b1. We ablate the temperature \u03b1 in three tasks: AntCircle, MetaDrive-mediummean, and CarPush1, representing each of the three environments. The results are shown in Figure 12. While Trac generally performs well across all values, higher performance could potentially be achieved with further hyperparameter tuning. For instance, in AntCircle and CarPush1, the cost decreases with lower \u03b1 values. In contrast, for MetaDrive-mediummean, the impact is primarily on the reward, with higher \u03b1 leading to lower rewards.\n$\\pi_{ref}$ pretraining. To further assess the effectiveness of using the reference policy ref, we conduct additional experiments for all BulletGym tasks with $\\pi_{ref}$ set as a uniform distribution, meaning the output of $\\pi_{ref}$ is constant for all state-action pairs. The results are presented in Table 6. We observe that rewards remain unaffected by a constant ref value, while the cost increases when $\\pi_{ref}$ is a uniform distribution, particularly for AntCircle, where the cost rises significantly. This increase occurs because a uniform reference policy $\\pi_{ref}$ lacks information about the underlying offline dataset, leading to distribution drift issues for the learning policy. This highlights the effectiveness of the $\\pi_{ref}$ pretraining in our approach."}]}