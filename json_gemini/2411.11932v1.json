{"title": "Reviving Dormant Memories: Investigating Catastrophic Forgetting in Language Models through Rationale-Guidance Difficulty", "authors": ["Huashan Sun", "Yang Gao"], "abstract": "Although substantial efforts have been made to mitigate catastrophic forgetting in continual learning, the intrinsic mechanisms are not well understood. In this paper, we discover that when a forgetting model passively receives an externally provided partial appropriate rationale, its performance on the forgotten task can be restored. Furthermore, by simply adding a task-agnostic prefix to the original instruction, the forgetting model can actively generate an appropriate rationale to reach the correct answer. These findings suggest that the model does not actually \"forget\" the task knowledge; instead, the degraded performance can be attributed to the failure of the original instructions in guiding the model to generate the appropriate rationales. Based on this insight, we propose the Rationale-Guidance Difficulty metric to evaluate how effectively a given instruction guides the model in generating appropriate rationales. We apply this metric to optimize the allocation of replay data in replay-based continual learning algorithm. Experimental results demonstrate that our data allocation method effectively mitigates catastrophic forgetting and maintains better model plasticity simultaneously across models.", "sections": [{"title": "Introduction", "content": "While large language models (LLMs) acquire extensive knowledge during pre-training (Brown et al., 2020; Touvron et al., 2023a; Yang et al., 2023), in reality, both knowledge and data are dynamic, necessitating that models adapt to different tasks or domains continuously (Zheng et al., 2024). Accordingly, continual learning can assist models in acquiring new knowledge incrementally, thereby enhancing their capabilities over time. However, a key challenge models face during continual learning is \"catastrophic forgetting,\" which refers to the phenomenon where a model's performance on old tasks declines after learning new tasks (McCloskey and Cohen, 1989; Goodfellow et al., 2014).\nDespite the numerous methods proposed to mitigate catastrophic forgetting (Wang et al., 2024, 2023a; Zhao et al., 2024) (discussed in Section 2.2), few studies have begun to investigate the intrinsic mechanisms underlying this phenomenon. Kotha et al. (2024) propose the \"task inference\" hypothesis, which suggests that fine-tuning a model changes which of its abilities it tends to use, rather than causing it to actually forget those abilities. However, this hypothesis has been primarily validated on synthetic datasets rather than directly on natural language datasets and LLMs. Similarly, Jiang et al. (2024) investigate forgetting in LMs through the lenses of instruction-following and task-related knowledge, proposing that forgetting stems from a decline in instruction-following capabilities rather than an actual loss of task-related knowledge. However, the expression of \"task-related knowledge\u201d varies between probing and practical use, leaving the conclusion insufficiently clarified.\nIn this work, we hypothesize that the model does not truly forget task knowledge; rather, its performance degradation on previous tasks is primarily attributable to the original instructions' inability to guide the generation of relevant knowledge. The first question that naturally arises for a forgetting model is: How does the model perform when passively provided with appropriate knowledge, such as the rationale of the Chain of Thought (CoT)? Specifically, for a forgetting model, we concatenate k part of the rationale (ground truth) to the original instruction (see Figure 3 where k = 0.1) and evaluate the model's performance. Our findings reveal that for models of varying sizes, providing even a portion of the appropriate rationale as guidance allows the model's performance to recover (shown in Figure 2). Moreover, as k increases, the model's performance can recover to pre-forgetting levels. There are two potential explanations: (1) the instructions fail to guide the generation of the corresponding knowledge, or (2) the knowledge has truly been forgotten. To investigate the underlying reason further, we pose the second question: Can we help the model to actively generate the appropriate knowledge by modifying its prompt? To explore this, we add a Task-Agnostic Prefix (Ye et al., 2024) before the original instruction (see Figure 4) and assess the performance of the forgetting model. This approach ensures that knowledge is generated and expressed in a same manner before and after forgetting. The experimental results (shown in Figure 5) indicate that, across models of various scales, the task-agnostic prefix can facilitate the forgetting model in generating relevant knowledge, thereby partially restoring its performance on forgotten tasks. The above experiments provide direct evidence for our hypothesis regarding LLMs: the model's forgetting primarily stems from the original instructions' inability to facilitate appropriate rationales, rather than an actual loss of task-related knowledge.\nBuilding on the above conclusion, we propose the Rationale-Guidance Difficulty metric to evaluate how effectively a given instruction can guide a model in generating an appropriate rationale. Utilizing this metric, we dynamically allocate replay data for each previous task to optimize data utilization. Experiments conducted across various scales and datasets demonstrate the effectiveness of our approach.\nOur contributions can be summarized as follows:\n1.  We directly verify on LLMs that task-related knowledge, also expressed in the same format as in instruction-following scenarios, is not actually forgotten. Instead, the deterioration in model performance arises from the inability of the original task instructions to facilitate the generation of appropriate rationale (Section 3).\n2.  We propose the Rationale-Guidance Difficulty metric and implement it for data allocation within a replay-based framework (Section 4). Experiments validate the effectiveness of our approach in mitigating catastrophic forgetting in the model (Section 5)."}, {"title": "Related Work", "content": ""}, {"title": "Mechanism of catastrophic forgetting", "content": "Catastrophic forgetting refers to the tendency of models to lose previously acquired knowledge when learning new tasks (McCloskey and Cohen, 1989; Goodfellow et al., 2014), a challenge that has been widely studied with numerous attempts to mitigate it (Section 2.2). Nevertheless, a substantial gap persists in comprehending the internal mechanisms that lead to these knowledge losses. Kotha et al. (2024) hypothesize that fine-tuned models do not \"forget\" prior abilities but rather \u201csuppress\" them. They suggest that models first perform \u201ctask inference\" before applying the relevant capability, and fine-tuning biases this inference towards tasks aligned with the fine-tuning distribution, thereby suppressing performance on other prior capabilities. Jiang et al. (2024) hypothesize that a model's task ability comprises both understanding task-related knowledge and following instructions. Their experiments reveal that \"forgetting\" in models is primarily due to a decline in the ability to follow instructions, rather than a loss of knowledge."}, {"title": "Traditional methods in continual learning", "content": "Continual Learning (CL) seeks to progressively acquire knowledge from a sequence of tasks while retaining what has been previously learned (Zheng et al., 2024). Numerous continual learning methods have been proposed to address catastrophic forgetting: (1) Regularization-based methods constrain the features learned from previous tasks (Zhang et al., 2023a; Huang et al., 2021) or penalize changes to weights critical for those tasks (Zhou and Cao, 2021; Aljundi et al., 2018), ensuring that new learning minimally interferes with prior knowledge thus maintaining performance on earlier tasks. For example, O-LoRA (Wang et al., 2023a) mitigates catastrophic forgetting by learning tasks in different (low-rank) vector subspaces (LoRA parameters) using an additional orthogonal parameter loss. (2) Architecture-based methods aim to reduce interference between new and prior tasks by either dynamically increasing the model's capacity (Zhao et al., 2024) or isolating the existing weights (Hu et al., 2024). SAPT (Zhao et al., 2024) aligns parameter-efficient tuning blocks with selection modules via a shared attention mechanism, effectively tackling both catastrophic forgetting and knowledge transfer. (3) Replay-based methods retain a small subset of prior training examples or features and revisit them when a new task is introduced (Wang et al., 2024; Guo et al., 2024; Liu et al., 2021). Rather than retaining the original data, PCLL (Zhao et al., 2022), LFPT5 (Qin and Joty, 2022) and SSR (Huang et al., 2024) generate pseudo data samples that mimic the old data, either by leveraging the model itself or through a separate generative model."}, {"title": "Impact of Appropriate Rationale on \"Pseudo-Forgetting\"", "content": "Recall our assumption is that the model does not truly forget; rather, after learning new tasks, the instructions for the old tasks fail to \"guide\" the model in generating an \u201cappropriate reasoning process,\u201d which ultimately manifests as apparent \"forgetting\" of the old tasks. Thus, we aim to investigate the following two questions:\n1.  Q1: How does a forgetting model perform when passively provided with externally supplied \"appropriate rationale\u201d?\n2.  Q2: Can changing the prompt method enable the model to generate the \u201cappropriate rationale\" actively?"}, {"title": "Appropriate Rationale Meeting \"Pseudo-Forgetting\"", "content": "In this section, we address Q1. We selected the model from the final stage of sequential learning and chose the test set of tasks with a high forgetting rate for the experiment. To offer \u201cappropriate knowledge\" guidance, as shown in Figure 3, we append k part of the rationale directly after the original instruction. It is important to note that the added portion with small k does not directly provide task-specific or answer-relevant information but instead serves to guide the model in shaping the overall direction of its predictions. The detailed description of the experimental data and model is provided in Section 5.1.\nResults and Analysis Results are presented in Figure 2.\nFirstly, when a \u201cforgotten\u201d model passively receives partial guidance on \u201cappropriate rationale,\" it can regenerate the \u201cforgotten knowledge\" and gradually restore its \"pre-forgetting\" task performance. Specifically, across various \"forgotten\" tasks, performance improves with increasing k-values, suggesting that the model's knowledge remains intact. The issue lies in the original instructions' inability to elicit the appropriate reasoning processes. With some \u201cappropriate guidance,\u201d the model can access the relevant task knowledge and complete the task.\nSecondly, the degree of recovery of the model's \"task memory\u201d is related to the task difficulty and the scale of the model. For instance, in the CB task, Mistral-7B returns to its pre-forgetting performance level at k = 0.2, while the MNLI task requires k\n0.4 to achieve the same recovery level. Meanwhile, to restore the pre-forgetting performance level for Qwen2-0.5B and Llama2-13B, values of k = 0.6 and k = 0.1 are required, respectively."}, {"title": "Reviving Dormant Knowledge via Task-Agnostic Prefix Prompting", "content": "In the experiment in Section 3.1, the model passively received some \u201cappropriate rationale\" as guidance, allowing it to recover its performance gradually. To further verify and demonstrate our hypothesis, we address Q2 in this section. We utilize Task-Agnostic Prefix Prompting (Ye et al., 2024), which enables the model to generate a reasoning process for a task based on parameterized knowledge. As illustrated in Figure 4, this method uses examples unrelated to the testing task to guide the model in producing reasoning processes and answers through context-based learning. Notably, the generated rationale relies on parameterized knowledge rather than knowledge acquired from context. This approach simply adds prefixes to the original instructions, making the detected knowledge more closely resemble the original 'forgotten knowledge' in QA format, rather than adopting other formats (Jiang et al., 2024). This similarity highlights that the knowledge itself is intact, and the challenge lies in the differences in \"guidance\" methods for generating an \u201cappropriate rationale.\"\nResults and Analysis Results are presented in Figure 5. Adding an appropriate prefix to the original instruction enables the model to generate an appropriate rationale and recover its performance on forgotten tasks. Specifically, for different forgetting models, a Task-Agnostic Prefix added before the original instructions partially restores performance across various forgotten tasks. For optimal prefix selection, we employed a grid search to identify the best demonstrations and demonstration count for each task, highlighting the dependency of task recovery on prefix design. We hypothesize that an optimal Task-Agnostic Prefix can restore performance to pre-forgetting levels. Notably, larger models, such as Llama2-13B, exhibit higher recovery levels than smaller models like Llama2-7B and Mistral-7B, suggesting that model size correlates with resistance to \"forgetting\".\nSummary Experimental results indicate that the performance drop on previous tasks is primarily attributable to limitations in the prompting method. The model has not truly forgotten task-specific knowledge; rather, the prompts fail to effectively guide the generation of the appropriate rationale, giving the impression of forgotten knowledge."}, {"title": "Replay Based on Rationale-Guidance Difficulty", "content": "While it is possible to recover the performance of a forgotten model using appropriate prompts, this approach is less cost-effective. Building on our assumption, we argue that replay-based algorithms (Wang et al., 2024; Guo et al., 2024; Liu et al., 2021) provide the most straightforward and efficient solution to mitigate catastrophic forgetting. In this section, we first introduce an evaluation metric, Rationale-Guidance Difficulty, to assess the difficulty of guiding a model toward an 'appropriate rationale' for a given instruction. Based on this metric, we dynamically allocate replay data for each previous task to optimize data utilization."}, {"title": "Rationale-Guidance Difficulty", "content": "The Rationale-Guidance Difficulty (RGD) score for a given data pair (x, r, y) is calculated as follows:\n$RGD(x, r, y) = \\frac{PPL_\\theta(r|x)}{PPL_\\theta(r)}$\nHere, x denotes the prompt, r represents the inference process, y is the answer, and \u03b8 refers to the parameters of the model being tested. $PPL_\\theta(r)$ reflects the difficulty for the model to generate the rationale r independently, while $PPL_\\theta(r|x)$ measures the difficulty when generating r given the prompt x. The RGD score indicates how well a given prompt x facilitates the generation of rationale r. A higher RGD score signifies greater difficulty for a prompt in guiding the model to produce the rationale, and vice versa. The calculation method of this metric follows the Instruction Following Difficulty (IFD) score proposed by Li et al. (2024b), though the IFD score was primarily used for fine-tuning data selection (Li et al., 2024a,b).\nFor a given previous task t, we estimate its RGD score using a set of test data V:\n$RGD_t = Mean(RGD_V)$"}, {"title": "Understanding Forgetting via Rationale-Guidance Difficulty", "content": "Table 1 shows the RGD scores for the Mistral-7B model on the QQP task, before and after forgetting. Before forgetting, the model exhibits lower RGD scores, indicating that the instructions for the QQP task easily guide the model to the relevant knowledge, resulting in better performance. However, after forgetting, the RGD scores increase, suggesting that the instructions become less effective at guiding the model to the relevant knowledge, leading to a decline in task performance."}, {"title": "Dynamic Replay Strategy", "content": "Wang et al. (2024) propose determining the proportion of replay data based on task similarity, suggesting that tasks with greater differences from the current learning task should have more replay data. In contrast, we argue that the model's capacity to learn and execute tasks is crucial, making it more reasonable to allocate replay data based on the model's difficulty in generating rationales from prior instructions.\nWhen training the model on the current task $T_i$, the amount of replay data required for the previous task $T_j$ calculated as follows:\n$\\alpha^*_j = \\frac{RGD_j^{-1}}{\\sum_{k=1}^{i-1} RGD_k} \\times \\alpha, j \\in [1, i - 1]$\nwhere $RGD_j^{-1}$ represents the rationale-guidance difficulty of task j after the model has completed training on task i - 1, and \u03b1 represents the total amount of replay data."}, {"title": "Experiments", "content": ""}, {"title": "Experiments Setup", "content": ""}, {"title": "Datasets", "content": "Long Sequence Benchmark (Razdaibiedina et al., 2023) A continual learning benchmark of 15 classification datasets. Following (Razdaibiedina et al., 2023; Wang et al., 2023b), we select 1,000 random samples for training each task and hold out 500 samples per class for validation and testing.\nWe employed Qwen2.5-72B-Instruct\u00b9 to generate a rationale r for each data point (x, y), ensuring that the initial part of the rationale did not directly reveal the final answer. Following prior work (Zhao et al., 2024), we employ two different training orders in our experiment."}, {"title": "Models", "content": "We utilize Qwen2 (0.5B, 1.5B) (Yang et al., 2024), Llama2 (7B, 13B) (Touvron et al., 2023b), and Mistral (7B) (Jiang et al., 2023) as the backbone models for our experiments."}, {"title": "Baselines", "content": "We primarily evaluate various data allocation methods for replay. For comparison, we use the following baseline allocation methods, employing random sampling to select replay samples for each task.\nEqual Allocation (EA) For each previous task, we replay the same amount of samples while learning a new task to maintain the previous capability.\nInsCL InsCL (Wang et al., 2024) dynamically replays previous data based on task similarity, calculated by Wasserstein Distance with instructions.\nMulti-task Learning We use multi-task learning as the upper bound for continual learning."}, {"title": "Metrics", "content": "According to previous works (Zhang et al., 2023b; Lopez-Paz and Ranzato, 2017; Biesialska et al., 2020), we use the following CL-related metrics that concentrate on catastrophic forgetting (stability) and knowledge transfer (plasticity). Let ai,j be the testing performance (Accuracy for classification task and Rouge-L for others) on the j-th task after training on i-th task, the metrics for evaluating are:\nFinal Average Performance (FAP) The average performance of all tasks after the final task $t_T$ is learned, i.e., $FAP_T = \\frac{1}{T} \\sum_{t=1}^{T} a_{T,t};$\nForgetting Rate (F.Ra) It measures how much knowledge has been forgotten across the first T \u2013 1 tasks, i.e., $F_T = \\frac{1}{T-1} \\sum_{t=1}^{T-1} (max_{k=1} a_{k,t} - a_{T,t});$"}, {"title": "Results and Analysis", "content": "Table 2 presents a performance comparison of different replay data allocation methods on the Long Sequence benchmark. All results are averaged across two different task orders. Detailed results for each order and task within a specific order are provided in the Appendix.\nThe equal allocation method significantly reduces catastrophic forgetting compared to no data replay For example, with Qwen2-0.5B, random data replay improved task performance by 20.03 and reduced the forgetting rate by 19.73. Similarly, for Mistral-7B, task performance increased by 6.92 and the forgetting rate decreased by 7.15. These results support our hypothesis that model performance degradation is due to ineffective task instructions, not forgotten knowledge. Simple data replay, using a small amount of data, helps the model leverage old knowledge more effectively, alleviating catastrophic forgetting.\nThe model exhibits inherent resistance to forgetting, which improves with larger model sizes Specifically, larger models show lower forgetting rates during continual learning without data replay. For example, the forgetting rates for Llama2-13B and Qwen2-0.5B are 13.02 and 23.13, respectively.\nUsing a data allocation algorithm can further alleviate model forgetting Our model-based data allocation method, along with the InsCL method that allocates data based on similarity, both more\nThe equal allocation method significantly reduces catastrophic forgetting compared to no data replay. For example, with Qwen2-0.5B, random data replay improved task performance by 20.03 and reduced the forgetting rate by 19.73. Similarly, for Mistral-7B, task performance increased by 6.92 and the forgetting rate decreased by 7.15. These results support our hypothesis that model performance degradation is due to ineffective task instructions, not forgotten knowledge. Simple data replay, using a small amount of data, helps the model leverage old knowledge more effectively, alleviating catastrophic forgetting.\nThe model exhibits inherent resistance to forgetting, which improves with larger model sizes. Specifically, larger models show lower forgetting rates during continual learning without data replay. For example, the forgetting rates for Llama2-13B and Qwen2-0.5B are 13.02 and 23.13, respectively.\nUsing a data allocation algorithm can further alleviate model forgetting. Our model-based data allocation method, along with the InsCL method that allocates data based on similarity, both more"}, {"title": "Backward Transfer (BWT) BWT measures", "content": "the impact that continually learning on subsequent tasks has on previous tasks, i.e., $BWT_T = \\frac{1}{T-1} \\sum_{t=1}^{T-1} (a_{T,t} - a_{t,t}).$\nForward Transfer (FWT) FWT measures how much knowledge from previous tasks transfers to a new task, i.e., $FWT_T = \\frac{1}{T} \\sum_{t=1}^{T} (a_{t,t} - a_{0,t})$ where $a_{0,t}$ refers to the performance of training task t individually;\nCurrent Average Performance (CAP) The average performance of all tasks in their respective training stage, i.e., $CAP_T = \\frac{1}{T} \\sum_{t=1}^{T} a_{t,t}.$\nBetter scores on FAP, F.Ra, and BWT indicate improved model stability, while better scores on FWT and CAP reflect enhanced model plasticity."}, {"title": "Results and Analysis", "content": ""}, {"title": "Discussion and Future Work", "content": "What does the rationale generated by a \"pseudo-forgetting\" model look like under the guidance of old instructions? As shown in Table 3, our preliminary observations suggest that the inappropriate rationales which we call this Rationale Drift generated by \"pseudo-forgetting\u201d models primarily fall into two categories:\n1.  Instruction Drift (Misalignment): When there is a significant difference between old and new tasks (e.g., the old task is Natural Language Inference (NLI), the new task is Topic Classification (TC)), the reasoning process tends to completely rely on TC-relevant knowledge due to instruction misalignment, leading to incorrect results.\n2.  Knowledge Drift: When old and new tasks are similar (e.g., both NLI but from different domains), the rationale remains related to NLI knowledge but is influenced by the new domain's knowledge. This causes deviation from the appropriate reasoning path, resulting in errors.\nAdditionally, can these types of drift be detected by specific methods? For instance, could differences in model hidden layer representations reveal their presence? Is there a big difference between the Rationale-Guidance Difficulty in the two cases?\nMore Complex Scenarios Current continual learning benchmarks rely on traditional NLP tasks, which may be relatively simple and thus less likely to induce forgetting in the model. To enhance the credibility of our conclusions, more complex domain adaptation datasets, such as TRACE (Wang et al., 2023c), could be utilized to replicate the experiments.\nDesign of Continual Learning Algorithm Our study assumes that the model retains prior knowledge during continual learning and inherently resists forgetting, making simple data replay effective in mitigating catastrophic forgetting. Another key goal of continual learning is to maximize asynchronous knowledge transfer across tasks to enhance model performance. Given that prior knowledge is parameterized, we propose that a combined approach, integrating both parametric and data replay perspectives, may be the most effective."}, {"title": "Conclusion", "content": "This study sheds light on the intrinsic mechanism behind catastrophic forgetting in continual learning, revealing that task knowledge is not truly lost but hindered by inadequate instruction guidance for appropriate rationales. We validate this hypothesis within LLMs and with consistent knowledge expression by subjecting the forgetting model to passive external knowledge guidance and incorporating a task-agnostic prefix into the original instruction for an active generation, both of which effectively recover the performance of the forgetting model. We introduce a Rationale-Guidance Difficulty, which evaluates the difficulty of guiding the model to generate appropriate rationales\u2014a critical factor in overcoming forgetting. Using this metric, our proposed replay data allocation method effectively mitigates forgetting while maintaining model plasticity."}]}