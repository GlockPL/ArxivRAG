{"title": "REVISITING GENERALIZATION POWER OF A DNN\nIN TERMS OF SYMBOLIC INTERACTIONS", "authors": ["Lei Cheng", "Junpeng Zhang", "Qihan Ren", "Quanshi Zhang"], "abstract": "This paper aims to analyze the generalization power of deep neural networks (DNNs) from the\nperspective of interactions. Unlike previous analysis of a DNN's generalization power in a high-\ndimensional feature space, we find that the generalization power of a DNN can be explained as\nthe generalization power of the interactions. We found that the generalizable interactions follow a\ndecay-shaped distribution, while non-generalizable interactions follow a spindle-shaped distribution.\nFurthermore, our theory can effectively disentangle these two types of interactions from a DNN. We\nhave verified that our theory can well match real interactions in a DNN in experiments.", "sections": [{"title": "1 Introduction", "content": "Analyzing and quantifying the generalization power of deep neural networks (DNNs) is a crucial issue in deep learning.\nFor example, numerous achievements have been made to analyze the generalization power in high-dimensional feature\nspace. [Petrini et al., 2022, Boopathy et al., 2023, Dyballa et al., 2024, Nikolikj et al., 2024]\nIn this paper, we revisit the generalization power of a DNN from a new perspective. Unlike analyzing a DNN's\ngeneralization power in the high-dimensional feature space, people usually have a more efficient way to evaluate the\ntrustworthiness of another person, i.e., directly understanding the internal logic behind his/her behavior. However,\nthis strategy has not been conducted on DNNs, because the lack of a sophisticated theoretical system to faithfully\nquantify/explain detailed logic (or inference patterns) encoded by a DNN has hampered all attempts of using detailed\ninference patterns to analyze a DNN's generalization power.\nBackground: faithful disentanglement of interaction patterns used by a DNN for inference. The first challenge to\nthe above strategy is the theoretically guaranteed faithfulness of inference patterns. To this end, the interaction theory\nwith lots of theorems and empirical findings has been built up [Ren et al., 2023a, Li and Zhang, 2023a, Ren et al.,\n2024a] to guarantee the faithfulness of defining and disentangling a DNN's inference patterns, and it has been widely\napplied to various scenarios [Ren et al., 2021, Wang et al., 2021, Deng et al., 2022, Liu et al., 2023, Zhou et al., 2024a,\nZhang et al., 2024, Ren et al., 2024b].\nSpecifically, as Figure 1 shows, given a DNN and an input sample x, it is proven that there exists a logical model\nconsisting of AND-OR interaction patterns, which can accurately predict the DNN's outputs on all 2n masked states of\nthe input sample. Such a strong theorem enables us to roughly consider the set of AND-OR interactions in the logical\nmodel as inference patterns equivalently encoded by the DNN. For example, in Figure 1, the logical model encodes\nan AND interaction between the tokens in S = {Theory, Relativity}. Only when all tokens in S are present in the"}, {"title": "2 Analyzing interactions of two distributions", "content": "The contribution of this paper can be summarized as follows. In this study, we find that the generalizable interactions,\nencoded by a DNN often follow a decay-shaped distribution, while the non-generalizable interactions often follow\na spindle-shaped distribution. Furthermore, we propose a method to formulate and disentangle interactions of a\nspindle-shaped distribution and interactions of a decay-shaped interactions from a DNN. Preliminary experiments have\nshowed the effectiveness of our theory."}, {"title": "2.1 Preliminaries: interactions represent primitive inference patterns used by a DNN", "content": "To analyze the DNN's generalization power from the perspective of detailed inference patterns, the first challenge is\nhow to faithfully extract and quantify the inference patterns encoded by a DNN.\nAs a typical theoretical guarantee for the faithfulness of explaining inference patterns used by a DNN, recent progresses\nRen et al. [2023a], Li and Zhang [2023a], Ren et al. [2024a] have proved that a DNN's all detailed inference patterns on\na given input sample can be rigorously mimicked by a logical model consisting of AND-OR interactions. Let us be\ngiven a DNN and an input sample x = [x1,x2,..., xn] consisting of n input variables\u00b2. N = {1, 2, ..., n} denotes the\nset of indices of the n input variables. We use v(x) \u2208 R to represent the scalar output of the DNN. Typically, we can\nfollow Deng et al. [2022] to set v(x) as the scalar classification confidence\u00b3 of the sample x.\n$v(x) \\stackrel{\\text{def}}{=} \\log \\frac{p(y = y_{\\text{truth}}|x)}{1 - p(y = y_{\\text{truth}}|x)} \\in \\mathbb{R}$.                                                                                                               (1)\nwhere p(y = ytruth|\u00e6) represents the probability\u00b3 of classifying the input sample \u00e6 to the ground-truth category ytruth.\nTheorem 2.1 theoretically guarantees that given an input sample x, we can construct the following AND-OR logical\nmodel to accurately predict the DNN's scalar outputs on all masked samples xmask when we enumerate all 2n possible\nmasked states.\n$h(x_{\\text{mask}}) \\stackrel{\\text{def}}{=} \\sum_{S \\in \\Omega_{\\text{AND}}} 1_{\\text{AND}}(S|x_{\\text{mask}}) \\cdot I_{\\text{AND}} +\\sum_{S \\in \\Omega_{\\text{OR}}} 1_{\\text{OR}}(S|x_{\\text{mask}}) \\cdot I_{\\text{OR}} + b$                                                                                                                                                    (2)\n\u2022 The trigger function 1AND(S|xmask) represents an AND interaction between a subset S \u2264 N of input variables.\nIf all input variables in S are present (not masked) in the masked sample xmask, the AND trigger function is activated\nand returns 1. Otherwise, it returns 0. IAND is the scalar weight. The bias term b is set to v(xg), i.e., the network\noutput when masking all input variables in \u00e6. QAND and QOR represent the set of AND interactions and the set of OR\ninteractions, respectively.\n\u2022 The trigger function 10R (S|xmask) represents an OR interaction between a subset S \u2264 N of input variables. If\nat least one input variable in S is present (not masked) in xmask, then the OR trigger function is activated and returns 1.\nOtherwise, it returns 0. IOR is the scalar weight.\nTheorem 2.1. (Universal matching property, proved by Chen et al. [2024] and Appendix C) Given an input sample\nx, if we set all weights $\\forall S \\subseteq N$, $I_{\\text{AND}} = \\sum_{T \\subseteq S} (-1)^{|S|-|T|} v_T^{\\text{AND}}$ and $I_{\\text{OR}} = - \\sum_{T \\subseteq S} (-1)^{|S|-|T|} v_T^{\\text{OR}}$, then we\nhave\n$\\forall T \\subseteq N$,  $v(x_T) = h(x_T)$                                                                                                                                                                                                                                                                                                                         (3)\nwhere x is the masked sample only containing the input variables in T. All other input variables in the set\nN\\T are masked.4 The overall network output v(xT) is decomposed, subject to uAND = 0.5 \u00b7 v(x) + \u03b3T and\nur = 0.5 \u00b7 v(x) \u03b3\u0442. {T} is a set of learnable parameters to determine the decomposition (See Appendix D for\nthe details).\nOR\nT\nThis theorem ensures that each logical model can predict DNN's outputs on an exponential number of masked samples.\nThis guarantees that we can roughly consider AND-OR interactions in the logical model as primitive inference\npatterns equivalently used by the DNN."}, {"title": "2.2 Two perspectives to define the generalization power of interactions", "content": "Because the output of a DNN can be faithfully decomposed into effects of AND-OR interactions, we can explain the\ngeneralization power of a DNN as the ensemble of the generalization power of its compositional interactions.\nThe first perspective to understand the generalization power of an interaction. Definition 2.2 shows a typical\ndefinition of the generalization power of interactions proposed by Zhou et al. [2024b]. If an interaction, which frequently\nappears on the training samples, also frequently appears on the testing samples, then this interaction is considered to be\ngeneralized to testing samples; otherwise, not.\nDefinition 2.2. Given a set of AND interactions and a set of OR interactions, the generalization power of all these\nAND-OR interactions is defined as the Jaccard similarity Sim(Dtrain, Dtest) between these interactions' distributions\n$\\|| \\min\\{D_{\\text{train}}, D_{\\text{test}} \\} ||_1$\nDtrain over the training set and their distributions Dtest over the testing set, i.e., Sim(Dtrain, Dtest) = $\\|| \\max\\{D_{\\text{train}}, D_{\\text{test}} \\} ||_1$,\nwhere || \u00b7 ||1 denotes the l\u2081 norm.\nIn this way, Zhou et al. [2024b] have found the following empirical findings on interactions of each order. The order\n(or complexity) of an interaction is defined as the number of input variables in the set S, i.e., order(S) = |S|. The\nexperimental verification of the following claim is provided in Appendix G.\nClaim 1 (Generalization power of interactions of different orders.). The generalization power of high-order\n(complex) interactions is lower than that of low-order (simple) interactions, i.e., low-order interactions have a higher\nvalue of Sim(Dtrain, Dtest) than high-order interactions.\nThe second perspective to understand the generalization power of an interaction. In this study, we find that\nthe above claim does not explain the fundamental ways for a DNN to encode generalizable interactions and non-\ngeneralizable interactions. Instead, we propose and later verify the following hypothesis on how a DNN encodes\ngeneralizable interactions and non-generalizable interactions.\nHypothesis 1. Generalizable interactions encoded by a DNN usually follow a decay-shaped distribution over different\norders, while non-generalizable interactions encoded in the DNN usually follow a spindle-shaped distribution over\ndifferent orders.\nWe follow Ren et al. [2024b] to represent the distribution of interactions mentioned in Hypothesis 1. Given all\ninteractions of the m-th order {S \u2286 N : |S| = m}, we compute the strength of all positive interactions A(m),+ and all\nnegative interactions A(m),\u2014 as follows:\n$A^{(m),+} = \\sum_{\\substack{S:|S|=m}} \\max\\{I_{\\text{AND}}, 0\\} + \\max\\{I_{\\text{OR}}, 0\\}$\n$A^{(m),-} = -\\sum_{\\substack{S:|S|=m}} \\min\\{I_{\\text{AND}}, 0\\} + \\min\\{I_{\\text{OR}}, 0\\}$                                                                                                                                   (4)\nIn Figures 1, 2 and 4, we visualize A(m),+ and A(m),+ over different orders to represent the decay-shaped distribution\nof interactions and the spindle-shaped distribution of interactions in Hypothesis 1.\nIn order to verify Hypothesis 1, we conducted experiments in Sections 2.2.1 and 2.2.2 to compare (1) the distribution of\ninteractions extracted from a well-trained DNN with (2) the distribution when we revise this DNN towards overfitting.\nThen, Hypothesis 1 can be verified if we find that (1) the well-trained DNN usually encodes decay-shaped interactions;\n(2) interactions of spindle-shaped distributions newly emerge when a well-trained DNN is revised to encode additional\nnon-generalizable features."}, {"title": "2.2.1 Verifying the hypothesis in the two-stage dynamics of interactions", "content": "Our hypothesis is grounded on the two-stage dynamics of interactions during the learning process of a DNN, which was\nfound and proven by Ren et al. [2024b], Zhang et al. [2024]. Specifically, people can use the gap between the training\nloss and the testing loss to divide the entire training process into a normal learning phase and an overfitting phase. In\nthe normal learning phase, the loss gap remains close to zero, and the DNN mainly encodes low-order interactions. In\nthe overfitting phase, the loss gap suddenly begins to increase, and the DNN learns interactions of increasing orders.\nIn this subsection, we go beyond interactions of a single order and analyze the distribution of interactions in the\ntwo-stage dynamics, so as to verify Hypothesis 1.\n\u2022 Stage 1: Normal learning phase. We discover that the normal learning phase mainly pushes the DNN to encode\ninteractions that follow a decay-shaped distribution over different orders. This discovery partially verifies Hypothesis 1.\nExperiments. We conducted experiments to illustrate the distributions of interactions over different orders in the normal\nlearning phase. We trained VGG-11, VGG-16 [Simonyan and Zisserman, 2014], AlexNet Krizhevsky et al. [2012]\non the CIFAR-10 datasets [Krizhevsky et al., 2009], CUB200-2011 datasets Wah et al. [2011] and Tiny-ImageNet\ndatasets Le and Yang [2015], respectively. We also trained BERT-Medium models Devlin et al. [2019] on the SST-2\ndatasets Socher et al. [2013]. For image data, we followed Ren et al. [2024b] to set a random set of ten image patches\nas input variables. For natural language data, we set all embeddings related to a word as an input variable. Please see\nAppendix I.2 for the details. We selected two sets of salient interactions, and obtained QAND = {S \u2286 N : |IND| > \u03c4}\nand NOR = {S \u2286 N : |IGR| > \u03c4} s.t. \u03c4 = 0.02 \u00b7 Ex [v(x) \u2013 v(x\u00f8)]. Then, we measured the distributions of salient\ninteractions in \u03a9AND and OOR by following Equation (4)."}, {"title": "2.2.2 verifying the spindle-shaped distribution of non-generalizable interactions", "content": "In this subsection, we revised a well-trained DNN by injecting non-generalizable representations into it to to obtain a\nDNN with non-generalizable features. In this way, we could examine whether interactions newly emerged in the revised\nDNN followed a spindle-shaped distribution. This experimental setting was also supported by the finding in [Zhang\net al., 2024], i.e., the spindle-shaped distributions could also be derived by considering all newly emerged interactions\nacted like random noises on most training samples (in fact, they were learned to fit very few hard samples during the\noverfitting phase). Appendix E provides further discussions on mathematical evidences of adding noises to generate\nnon-generalizable representations.\nTherefore, in this subsection, we conducted two experiments to examine the newly emerged interactions, in which we\nrevised the well-trained DNN from two perspectives.\nIn the first experiment, we revised the well-trained DNN by adding Gaussian noises \u03b5 ~ N(0, \u03c3\u00b2) to the network\nparameters of a well-trained DNN. Then, we computed the interactions in the well-trained DNN, denoted by IAND\nand IGR, and the interactions in the revised DNN, denoted by Ibise and Ior. Thus, \u0394\u0399\u0391\u039d\n= IAND - IAND and\nAIOR = IOR - IOR Noise represented the interaction effects newly emerged in the revised DNN. We measured the\ndistribution of newly emerged interactions in the revised DNN \u2206A(m),+ and \u2206A(m),\u2014 by following Equation (4).\n\u0394\u0399\u0391\u039d\nnoise\ns, Noise\nnoise\n$\u0394A^{(m),+} = \\sum_{\\substack{S:|S|=m}} \\max\\{\u0394I_{\\text{AND}}, 0\\} + \\max\\{\u0394I_{\\text{OR}}, 0\\}$\n$\u0394A^{(m),-} = -\\sum_{\\substack{S:|S|=m}} \\min\\{\u0394I_{\\text{AND}}, 0\\} + \\min\\{\u0394I_{\\text{OR}}, 0\\}$                                                                                                                                           (5)\nIn the second experiment, given a specific sample x, we revised this sample by adding adversarial perturbations8\nand generate xnoise, as the injection of non-generalizable representations. Then, we used the same setting in the first\nexperiment to measure the distribution of interactions newly emerged in the revised sample.\nTo this end, we conducted experiments on different DNNs, including ResNet-20 [He et al., 2016], VGG-11 and VGG-16\non various datasets, including MNIST [LeCun et al., 1998], CIFAR-10 and CUB200-2011, respectively."}, {"title": "2.3 Modeling two distributions in the hypothesis", "content": "In this subsection, we derive the analytic solutions of the decay-shaped distribution of generalizable interactions and the\nspindle-shaped distribution of non-generalizable interactions, in order to better explain the observations in Sections 2.2.1\nand 2.2.2. Specifically, we propose two parameterized models to represent the above two distributions."}, {"title": "2.3.1 Modeling the spindle-shaped distribution", "content": "Given a DNN with randomly initialized parameters, all AND-OR interactions in the DNN represent fully non-\ngeneralizable noises. Then, Zhang et al. [2024] have found that the effect of each non-generalizable interaction actually\nfollows a Gaussian distribution."}, {"title": "2.3.2 Modeling the decay-shaped distribution", "content": "In this subsection, we use the optimal interactions under weight uncertainty proven by Ren et al. [2024b] to formulate\nthe decay-shaped distribution of interactions.\nTheorem 2.3. (Experimental verification is shown in Appendix H\u00ba) Let I* = [I51, I52,..., I52n] denote the effects of\nall 2n interactions extracted from a fully converged DNN, which is probably overfitted. Then, injecting uncertainty/noises\nof the magnitude of 8 to data during the training process would further reduce high-order interactions and yield the\nfollowing set of interactions.\n$\\hat{I}(\u03b4) = M(\u03b4) I^*$                                                                                                                                (7)\nwhere M(\u03b4) \u2208 R2n \u00d72n is a matrix. The detail formula of M is shown in Appendix H due to the limit of page length.\nThe larger value of 8 more penalizes the strength of high-order interactions.\nTheorem 2.3 shows that the ubiquitous uncertainty in network parameters and training data will eliminate the mutually\ncounteracting high-order interactions, thereby avoiding overfitting. In this way, we formulate the strength of positive\ninteraction effect and that of negative interaction effect of each m-th order, as follows.\n$A_{\\text{decay}}^{(m),+} (\u03b4, \u03b3) = \u03b3 \\sum_{S: |S|=m} \\max\\{\\hat{I}_{AND} (\u03b4), 0\\} + \\max\\{\\hat{I}_{OR} (\u03b4), 0\\}$\n$A_{\\text{decay}}^{(m),-} (\u03b4, \u03b3) = -\u03b3 \\sum_{S: |S|=m} \\min\\{\\hat{I}_{AND} (\u03b4), 0\\} + \\min\\{\\hat{I}_{OR} (\u03b4), 0\\}$                                                                                                                     (8)"}, {"title": "2.3.3 Disentangling the generalizable and non-generalizable interactions.", "content": "We propose a method to disentangle the two distributions of interactions from a real DNN. Although the distributions\nin Equation (8) and Equation (6) are derived on some simplifying assumptions and heuristic settings, experiments in\nSection 3 will verify the effectiveness of our theory. Given a DNN, let A(m),+ and A(m),- denote the sum absolute\nstrength of m-order positive interactions and negative interactions, respectively. Then, the distributions of generalizable\ninteractions $A_{\\text{decay}}^{\\text{decay}}$ and non-generalizable interactions $A_{\\text{spindle}}^{\\text{spindle}}$ can be disentangled as the optimization\nof the following parameter model:\n$\\min_{\\alpha,\\beta,\\delta,\\gamma} \\sum_{m=1}^n (A^{(m),+} - A_{\\text{theory}}^{(m),+})^2 + (A^{(m),-} - A_{\\text{theory}}^{(m),-})^2$\n$\ns.t. A_{\\text{theory}}^{(m),+} = A_{\\text{decay}}^{(m),+} (\u03b4, \u03b3) + A_{\\text{spindle}}^{(m),+} (\u03b1, \u03b2)$\n$\nA_{\\text{theory}}^{(m),-} = A_{\\text{decay}}^{(m),-} (\u03b4, \u03b3) + A_{\\text{spindle}}^{(m),-} (\u03b1, \u03b2)$                                                                                                                                   (9)"}, {"title": "3 Experimental verification", "content": "Fitness to the distribution of non-generalizable interactions. We conducted experiments to verify whether the\nproposed formulation for the spindle-shaped distribution in Equation (6) really represented the true distribution of\nnon-generalizable interactions in a real DNN. Specifically, let us revisit the two experiments in Section 2.2.2, in which\nwe proposed two ways to inject non-generalizable representations to a well-trained DNN. Thus, let us be given the\ndistribution of interactions newly emerged in the revised DNN (or the revised samples), denoted by \u2206A(m),+, \u2206A(m),-,\nwhich were measured in two experiments in Section 2.2.2. We further used the derived distribution $A_{\\text{spindle}}^{\\text{spindle}}$\nin Equation (6) to match these interactions, i.e., computing parameters a and \u1e9e that best matched the distribution of\nthese interactions."}, {"title": "4 Conclusion and discussions", "content": "In this paper, we have analyzed the generalization power of a DNN in terms of the generalization power of interactions\nencoded by the DNN. We have proposed and further experimentally verified that generalizable interactions in a DNN\nusually follow a decay-shaped distribution, while non-generalizable interactions usually follow a spindle-shaped\ndistribution over different orders. Given a DNN, we further propose a method to disentangle the two distributions of\ninteractions encoded by the DNN. The experiments have verified that the theoretically disentangled distributions well\nmatch the true distribution of interactions in a real DNN. Moreover, various experiments have shown that the spindle-\nshaped distribution and the decay-shaped distribution successfully reflect the generalization power of interactions.\nHowever, our work is still far from a ultimate theory towards the explanation of the generalization power of interactions.\nOur findings only describe a general trend of how generalizable interactions and non-generalizable interactions\ndistribute over different orders. Whereas, given a specific sample, our theory cannot explain a microscopic perspective.\nSpecifically, We cannot ensure that every detail interaction in a decay-shaped distribution is generalizable, or ensure\nevery interaction in a spindle-shaped distribution is non-generalizable."}, {"title": "Impact statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A Related work", "content": "Analyzing and quantifying the generalization power of deep neural networks (DNNs) is a crucial issue in deep learning.\nExisting explanations often either analyze the loss gap Neyshabur et al. [2017], Bousquet et al. [2020], Deng et al.\n[2021], Haghifam et al. [2020, 2021] or focus on the smoothness of the loss landscape Keskar et al. [2016], Li et al.\n[2018], Foret et al. [2021], Kwon et al. [2021]. Additionally, there are some researches focus on analyzing the DNN's\ngeneralization power in high-dimensional feature spaces [Petrini et al., 2022, Boopathy et al., 2023, Dyballa et al., 2024,\nNikolikj et al., 2024].\nHowever, recent advances in interaction-based theory provide a more direct perspective to analyze the generalization\npower of DNNs. Specifically, the interaction-based methods define and quality the interaction effect encoded by DNNs.\nSince the output of a DNN can be faithfully explained as the sum of all AND-OR interactions, the generalization power\nof the entire DNNs can be also roughly regard as the integration of the generalization power of interactions encoded by\nthe DNN.\nLiterature in guaranteeing the faithfulness of defining and disentangling a DNN's inference patterns. Ren et al.\n[2023b] have discovered and Ren et al. [2024a] have proved that, given a DNN, there always exists a logical model\nconsisting of AND-OR inference patterns, which can using a small set of AND-OR interactions to accurately predict\nthe DNN's outputs on all 2n masked states of the input sample. Furthermore, Li and Zhang [2023b] have shown that the\nsalient interactions extracted from a DNN can be shared across different samples. Besides, they have also discovered that\nsalient interactions exhibited remarkable discrimination power in classification tasks. Chen et al. [2024] have proposed\na method to extract interactions that are generalizable across different models. These findings suggest that interactions\nact as primitive inference patterns encoded by DNNs, forming the theoretical foundation of interaction-based theoretical\nframeworks.\nLiterature in explaining the generalization power of DNNs from the perspective of interactions. Recent achieve-\nments have shown that interactions play a crucial role in explaining the hidden factors affecting a DNN's adversarial\nrobustness Ren et al. [2021], adversarial transferability Wang et al. [2021], and generalization power Zhou et al. [2024a]\nof a DNN. Deng et al. [2022] have discovered and theoretically proved the existence of a representation bottleneck\nin DNNs, which limits their ability to encode interactions of intermediate complexity. Ren et al. [2023c] have found\nthat Bayesian neural networks (BNNs) tend to avoid encoding complex interactions, which helps explain the good\nadversarial robustness of BNNs. Liu et al. [2023] have discovered that DNNs learn simple interactions more easily than\ncomplex interactions. Zhang et al. [2024] and Ren et al. [2024b] have discovered and analyzed the two-phase dynamics\nin DNNs' learning process."}, {"title": "B Three common conditions for sparse interactions", "content": "Ren et al. [2024a] have formulated three mathematical conditions for the sparsity of AND interactions, as follows.\nCondition 1. The DNN does not encode interactions higher than the M-th order: \u2200 S \u2208 {S \u2286 N | |S| > M + 1}, IAND.\nCondition 1 implies that the DNN does not encode extremely high-order interactions. This is because extremely\nhigh-order interactions usually represent very complex and over-fitted patterns, which are unnecessary and unlikely to\nbe learned by the DNN in real applications.\nCondition 2. Let us consider the average network output $\\overline{u}^{(k)} \\stackrel{\\text{def}}{=} E_{|s|=k}[v(x_s) - v(x_{\\O})]$ over all masked samples xs\nwith k unmasked input variables. This average network output monotonically increases when k increases: \u2200 k' \u2264 k,\nwe have $\\overline{u}^{(k')} <\\overline{u}^{(k)}$.\nCondition 2 implies that a well-trained DNN is likely to have higher classification confidence for input samples that are\nless masked.\nCondition 3. Given the average network output $\\overline{u}^{(k)}$ of samples with k unmasked input variables, there is a polynomial\nlower bound for the average network output of samples with k'(k' < k) unmasked input variables: \u2200 k' < k, $\\overline{u}^{(k')} >\n(\\frac{k'}{k})^p\\overline{u}^{(k)}$, where p > 0 is a positive constant.\nCondition 3 implies that the classification confidence of the DNN does not significantly degrade on masked input\nsamples. The classification/detection of masked/occluded samples is common in real scenarios. In this way, a well-\ntrained DNN usually learns to classify a masked input sample based on local information (which can be extracted from\nunmasked parts of the input) and thus should not yield a significantly low confidence score on masked samples."}, {"title": "C Proof of Universal Matching property (Theorem 2.1)", "content": "Proof. (1) Universal matching property of AND interactions.\nWe will prove that output component vand(xs) on all 2n masked samples {xs : S \u2286 N} could be universally explained\nby the all interactions in S \u2286 N, i.e., $\\forall \\neq S \\subseteq N$, $v_{\\text{and}}(x_s) = \\sum_{\\O\\neq T \\subseteq S} I_{\\text{AND}} + v(x_{\\O})$. In particular, we define\nvand(x) = v(xp) (i.e., we attribute output on an empty sample to AND interactions).\nSpecifically, the AND interaction is defined as $I_{\\text{AND}} = \\sum_{L \\subseteq T}(-1)^{|T|-|L|} u_L^{\\text{AND}}$ in Theorem 2.1 To compute the sum\nof AND interactions $\\sum_{\\O\\neq T \\subseteq S} I_{\\text{AND}} = \\sum_{\\O\\neq T \\subseteq S} \\sum_{L \\subseteq T}(-1)^{|T|-|L|} u_L^{\\text{AND}}$, we first exchange the order of summation\nof the set LCTC S and the set T \u2283 L. That is, we compute all linear combinations of all sets T containing L\nwith respect to the model outputs uAND given a set of input variables L, i.e., $\\sum_{T:L \\subseteq T \\subseteq S}(-1)^{|T|-|L|} u_L^{\\text{AND}}$. Then, we\ncompute all summations over the set LC S.\nIn this way, we can compute them separately for different cases of L \u2286 T \u2286 S. In the following, we consider the cases\n(1) L = S = T, and (2) L C T \u2286 S, L \u2260 S, respectively.\n(1) When L = S = T, the linear combination of all subsets T containing L with respect to the model output $\\overset{AND}{U}$ is\n(-1)|S|-|S|UAND = UAND\n(2) When L T \u2286 S, L \u2260 S, the linear combination of all subsets T containing L with respect to the model output\nUAND is ET:LCTCS(-1)|T|-|L|u^ND. For all sets T : S \u2287 T \u2283 L, let us consider the linear combinations of all sets\nT with number |T| for the model output UAND, respectively. Let m := |T| \u2013 |L|, (0 < m \u2264 |S| \u2013 |L|), then there\nare a total of C'S|-|L| combinations of all sets T of order |T|. Thus, given L, accumulating the model outputs UAND\ncorresponding to all T\u2283 L, then $\\sum_{T:L \\subseteq T \\subseteq s}(-1)^{|T|-|L|} u_L^{\\text{AND}}$=\n$\\sum_{m=0}^{|S|-|L|} C_{|S|-|L|}^m(-1)^m = 0$. Please see\nthe complete derivation of the following formula.\n$\\sum_{\\O/TC S} I_s(T|x)$\n$\n= \\sum_{\\O/TC S} \\sum_{L C T} (-1)^{|T|-|L/} I^{\\text{AND}}$\nAND\n-LAND\nLC S=0\n$ \\sum_{L=S} u_{0}^{\\text{AND}}$\n-uAND\n1 \\Sigma\nu S\\\\Sigma\nu (A (x))\n\\nu(C\n-\\Sigma\nu\nC\\\\Sigma (-1) A \\text{-}\\\\u{1}\\\\\\\\Text{AND}\nu\n,\\\\\\Text{AND}\n0-nN\\0 A\\\\\\\nS\n(\\Text{AND}\n(10)\n\nThus, we have V\u00d8 \u2260 S \u2286 N, vand(xs) = \u22110\u2260TCS IAND + v(xp).\n(2) Universal matching property of OR interactions.\nAccording to the definition of OR interactions, we will derive that \u2200S \u2286 N, ugr = \u2211T:T0S\u22600 IGR, where we define\nOR\nu R = 0 (recall that in Step (1), we attribute the output on empty input to AND interactions).\nSpecifically, the OR interaction is defined as $I_{\\text{OR}} = - \\sum_{L C T}(-1)^{|T|-|L|} u_L^{\\text{OR}}$ in Theorem 2.1 Similar to the\nabove derivation of the Universal matching property of AND interactions, to compute the sum of OR interactions\n\\Sigma\n1OR=0 (\\Text{-} A \\Text{-} I\n\\Sigma\\nu O \\O\\\\\u03a31(- A \\Text{-} \\\\\nu\\\\[\\Text\\\\[\u03a3u_t,\\Text(-\\Text\\\\\n21 (-(-4)) A A\\\\\n\\Sigma\\nu\\Sigma \\Sigma\n\\n\\\\\n= A\\\n(AND\\n(A\\\n(A\\\nIn this way, we can compute them separately for different cases of L \u2286 T \u2286 N,T \u2229 S \u2260 \u00d8. In the following, we\nconsider the cases (1) L = N \\ S, (2) L = N, (3) L \u2229 S \u2260 0, L \u2260 N, and (4) L \u2229 S = 0, L \u2260 N \\ S, respectively."}, {"title": "D Details of optimizing {\u04af\u0442} to extract the sparsest AND-OR interactions", "content": "A method is proposed Li and Zhang [2023c", "2024": "to simultaneously extract AND interactions IAND\nand OR interactions IOR from the network output. Given a masked sample \u00e6r, Li and Zhang [2023c"}]}