{"title": "Differentiating between human-written and AI-generated texts using\nlinguistic features automatically extracted from an online computational tool", "authors": ["Georgios P. Georgiou"], "abstract": "While extensive research has focused on ChatGPT in recent years, very few studies have\nsystematically quantified and compared linguistic features between human-written and Artificial\nIntelligence (AI)-generated language. This study aims to investigate how various linguistic\ncomponents are represented in both types of texts, assessing the ability of AI to emulate human\nwriting. Using human-authored essays as a benchmark, we prompted ChatGPT to generate essays\nof equivalent length. These texts were analyzed using Open Brain AI, an online computational\ntool, to extract measures of phonological, morphological, syntactic, and lexical constituents.\nDespite AI-generated texts appearing to mimic human speech, the results revealed significant\ndifferences across multiple linguistic features such as consonants, word stress, nouns, verbs,\npronouns, direct objects, prepositional modifiers, and use of difficult words among others. These\nfindings underscore the importance of integrating automated tools for efficient language\nassessment, reducing time and effort in data analysis. Moreover, they emphasize the necessity for\nenhanced training methodologies to improve the capacity of AI for producing more human-like\ntext.", "sections": [{"title": "Introduction", "content": "The revolutionary advancements in artificial intelligence (AI) and natural language processing\n(NLP) have given rise to increasingly sophisticated and capable language models known as Large\nLanguage Models (LLMs) (Kasneci et al., 2023). These LLMs, a subset of generative AI, are\ndesigned to produce new information by leveraging patterns and structures learned from existing\ndata, allowing machines to comprehend and generate human language (Zhou et al., 2023).\nChatGPT, abbreviated from Generative Pretrained Transformers, is a leading LLM created by\nOpenAI, utilizing NLP to produce text responses based on user prompts. This versatility allows it\nto function effectively in various fields, including education (Adeshola & Adepoju, 2023),\nhealthcare (Javaid et al., 2023), language teaching and learning (Kohnke et al., 2023), and customer\nservice (Kos et al., 2023) among others.\nOne of ChatGPT's most important advantages is the capability to learn through human feedback.\nThis process helps the model understand the meaning and intent behind user queries, providing\nrelevant and useful responses (Mindner et al., 2023). ChatGPT was founded on the GPT-3.5 LMM\nof Open AI, which is an adapted version of GTP-3, trained using a huge dataset featuring 175\nmillion parameters and 499 billion tokens of text (Mindner et al., Nazir & Wang, 2023). By\nlearning the subtleties of human language from this extensive dataset, ChatGPT can produce text\nthat closely mimics human writing (Chukwuere, 2024).\nHowever, AI language does not follow the exact patterns found in human language. A small body\nof work compared human and AI languages to detect potential differences. For example,\nAlexander et al. (2023) concluded that ChatGPT-generated essays were highly recognizable\ncompared to student-written essays since they presented with divergent language patterns. Herbold\net al. (2023) examined human-written versus ChatGPT-generated essays in an attempt to identify\nthe linguistic devices that are characteristic of student versus AI-generated content among others.\nThe AI-generated essays were assessed by teachers. The results indicated significant linguistic\ndistinctions between human-written and AI-generated content. AI-generated essays exhibited a\nhigh degree of structural uniformity, exemplified by identical introductions to concluding sections\nacross all ChatGPT essays. Furthermore, initial sentences in each essay tended to start with a\ngeneralized statement using key concepts from the essay topics, reflecting a structured approach\ntypical of argumentative essays. This contrasts with human-written essays, which display greater\nvariability in adhering to such structural guidelines on the linguistic surface. In another study, Cai\net al. (2023) examined how LMMs including ChatGPT and Vicuna replicated effectively the\nhuman language. Various prompts examining phonetic, syntactic, semantic, and discourse patterns\nwere given directly to the chatbots. The models were found to replicate well human language\nacross all levels. However, some discrepancies occurred since, unlike humans, neither model\nshowed a preference for using shorter words to express less informative content, nor did they\nutilize context to resolve syntactic ambiguities. A more comprehensive comparative examination\nof the human and AI languages with a special focus on particular linguistic features is needed to\nunderstand better the differences between them."}, {"title": "Automatic elicitation of linguistic features", "content": "Analyzing linguistic features in language, speech, and communication provides valuable insights\ninto linguistic choices and aids in language assessment. Such analyses have heavily relied on\nmanual assessments in both typical and clinical populations. For example, the mapping of speech\nproduction divergences of second language speakers often requires the collection of speech\nrecordings, the segmentation of sounds, and the implementation of statistical analysis (see\nGeorgiou & Kaskampa, 2024). In clinical settings involving children with developmental language\ndisorder, extracting speech patterns usually demands the use of a narrative assessment tool\nfollowed by audio analysis (see Georgiou et al., 2024). These methods, although reliable, can be\nunwieldy and time-consuming, potentially causing stress for patients or students undergoing the\nassessments (Themistocleous, 2024b).\nThe evolution of Al technologies has brought automated computational applications to the\nforefront, simplifying the extraction of speech and language measures. These tools utilize machine\nlearning technologies, such as deep neural networks, and NLP to provide algorithms for linguistic\nanalysis and pattern interpretation. Open Brain AI is one of these tools. Open Brain Al\n(Themistocleous, 2024a) is an open-source online computational platform application designed to\nprovide automated linguistic and cognitive assessments. It serves researchers, clinicians, and\neducators by streamlining their daily tasks through advanced AI methods and tools. Educators can\nutilize the application to evaluate students' speech and language, extract meaningful markers from\nessays and other materials, estimate performance, and assess the effectiveness of teaching\nmethodologies. For clinicians, Open Brain Al automates the analysis of spoken and written\nlanguage, offering valuable linguistic insights into the language of patients. Since language can be\nindicative of potential speech, language, and communication disorders, early screening and\nassessment can be decisive for diagnosis and treatment (Georgiou & Theodorou, 2023).\nResearchers may benefit from Open Brain AI by generating quantitative measures of speech,\nlanguage, and communication that can assist their research.\nThe analysis provided by the application involves the objective measurement of written speech\nproduction features, allowing the comparison of an individual with a targeted population across\nvarious linguistic domains. It specifically analyzes texts or transcripts generated from the speech-\nto-text module, conducting assessments in the following linguistic domains:\n\u2022\tPhonology: Measures include the number and ratios of syllables, vowels, words with\nprimary and secondary stress, consonants per place and manner of articulation, and voiced\nand voiceless consonants.\n\u2022\tMorphology: Includes the counts and ratios of parts of speech (e.g., verbs, nouns,\nadjectives, adverbs, conjunctions, etc.) relative to the total number of words.\n\u2022\tSyntax: Calculates the counts and ratios of syntactic constituents (e.g., modifiers, case\nmarkers, direct objects, nominal subjects, predicates, etc.).\n\u2022\tLexicon: Provides metrics such as the total number of words, hapax legomena (words that\noccur once), Type Token Ratio (TTR), and others.\n\u2022\tSemantics: Estimates counts and ratios of semantic entities within the text (e.g., persons,\ndates, locations, etc.).\n\u2022\tReadability Measures: Assessments of text readability and grammatical structure."}, {"title": "This study", "content": "This study aims to investigate the representation of various linguistic constituents in both human-\nwritten and AI-generated texts. The main objective is to find potential differences between human\nand AI essay texts in the occurrence of various phonological, morphological, syntactic, and lexical\ncomponents. This is one of the very few studies to focus on how particular features can discern\nhuman-written and AI-generated texts. Such an investigation is crucial for advancing multiple\naspects of language technology and its applications. It is essential for understanding AI capabilities\nand limitations, enabling the refinement of algorithms to produce more natural and coherent\nlanguage. By identifying where AI text diverges from human norms, researchers can improve\ntraining methods and design better models, enhancing the quality of AI-generated content. These\nimprovements have broad applications in NLP tasks such as machine translation, text\nsummarization, and dialogue systems, and can significantly benefit content creation in fields such\nas marketing, journalism, education, and health service. Furthermore, understanding linguistic\ndiscrepancies enhances user experience and fosters trust and acceptance of AI technologies by\nensuring that generated content meets human expectations. Moreover, this study will effectively\nhighlight the role of LMMs in supporting the development of automated linguistic assessments\nthrough a user-friendly tool, potentially revolutionizing the fields of education and clinical therapy."}, {"title": "Methodology", "content": "We obtained five text samples from IELTS exam writing tasks, which were written by professional\nteachers. This approach mitigated the chance of language errors within the texts. The samples\ncover areas such as education, society, technology, and arts among others. We then used ChatGPT-\n3.5 to generate another five texts based on the same instructions as those of the topics. We provided\na prompt to generate texts with a word count similar to that of human-generated texts.\nSubsequently, all texts were added to the Open Brain AI online application. The software was set\nto elicit readability, phonological, morphological, syntactical, and lexical features from the text\nsamples. Readability scores were used descriptively. The output provided 22 phonological, 15\nmorphological, 44 syntactical, and 13 lexical measures. However, we selected for analysis only\nthose pertaining to the counts of linguistic constituents and those constituents with a remarkable\npresence in the texts."}, {"title": "Statistical Analysis", "content": "To compare the distribution of outcomes between the human and the AI texts, we employed the\nbinomial test, a statistical method suited for analyzing categorical data with binary outcomes. The\ntest was conducted using the binom.test() function in R (R Core Team, 2024). This type of test is\nmore appropriate given that the data forms a 2 \u00d7 1 contingency table (Richardson, 1994). Our\ndataset comprised counts from the two categories of texts, representing the number of occurrences\nwithin each category for each linguistic component across four levels (phonology, morphology,\nsyntax, and lexicon). The binomial test is a valuable tool for evaluating the null hypothesis that the\nprobability (\u03c0) of occurrence is equal for two categories (H0: \u03c0 = 0.5). It relies on the formula\n$\\Pr(X=k)=(\\binom{n}{k}) p^k.(1-p)^{n-k}$, where n is the total number of observations, k is the number of\nsuccesses and p is the probability of success for each observation under the null hypothesis.\nTherefore, the binomial test assesses whether the observed counts in each type of texts significantly\ndeviate from what would be expected if both types had the same count distribution. Upon obtaining\nthe results, a p-value below the conventional significance level (a = 0.05) would lead to rejection\nof the null hypothesis. We also report the effect sizes using Cohen's h. An h value of \u2248 0.2 indicates\na small practical effect, an h value of \u2248 0.5 indicates a moderate effect, and an h value of \u2248 0.8\nshows a large effect."}, {"title": "Results", "content": "Table 1 presents the average readability measures across the human and AI texts. For the\nphonological condition, the results of the descriptive statistics indicated the use of a higher number\nof approximants, fricatives, laterals, nasals, and plosives by the AI text compared to the human\ntext. Also, alveolar, bilabial, and postalveolar consonants were preferred to a greater degree by the\nAI text, while the opposite occurred for dental consonants. The number of voiced and voiceless\nconsonants was larger in the AI text, as well as the use of primary and secondary stress Figure 1\nillustrates the number of occurrences of phonological components for each phonological measure\nin both the human-written and the AI-generated text. In the morphological condition, the\nproportions of adpositions, adverbs, auxiliaries, coordinating and subordinating conjunctions, and\nverbs were higher in the human text, while the number of adjectives, nouns, and pronouns was\nhigher in the AI text. In the syntactic condition, determiners, object of prepositions, and\nprepositional modifiers were more prevalent in the human than the AI text, while adjectival\nmodifiers, conjuncts, and nominal and direct objects were more evident in the AI text. Figure 2\nshows the number of morphological and syntactic constituents across the two types of texts.\nFinally, in the lexical condition, the human text included more easy words and less difficult words\nthan the AI text, whereas the human text contained a smaller number of content words and a larger\nnumber of function words compared to the AI text. The number of occurrences of lexical\ncomponents for each lexical measure in both the human and the AI-generated texts is shown in\nFigure 3.\nTo investigate potential differences between the human and the AI texts, we used statistical\nanalyses. The binomial tests demonstrated significant differences for approximants, laterals,\nnasals, and plosives between the human and AI texts, indicating a tendency of AI to favor using a\nhigher number of these consonants. However, the effect sizes were small. Significant differences\noccurred for alveolar, bilabial, dental, and postalveolar consonants. All consonants but dentals\nwere used to a greater extent by the AI text. The majority of these sounds exhibited small to\nmoderate effect sizes. In addition, the tests revealed that the AI text used significantly a higher\nnumber of voiced and voiceless consonants (small and moderate to small effect sizes) and a higher"}, {"title": "Discussion", "content": "This study compared the occurrence of various phonological, morphological, syntactical, and\nlexical constituents in human-written versus Al-generated essay texts. The goal was to identify\ncritical linguistic features which distinguish between the two types of texts. Linguistic features\nwere elicited through an online platform application that has the capacity to extract linguistic\ninformation from written texts among others. This is one of the few studies quantifying linguistic\nconstituents produced by human and AI languages.\nOur findings indicated that AI-generated text tended to favor the use of approximants, laterals,\nnasals, and plosives more than human text. This preference was statistically significant but the\neffect sizes were small. Similarly, significant differences were observed in the usage of alveolar,\nbilabial, dental, and postalveolar consonants, with AI text using all except dental consonants more\nfrequently. Furthermore, the analysis revealed a statistically significant increase in the use of both\nvoiced and voiceless consonants, as well as words with primary stress, in the AI-produced text.\nWhile the effect sizes were generally small to moderate, these trends warrant further investigation.\nThe training data provided to Al models might influence their stylistic choices, potentially favoring\nsentences with a higher consonant density or stronger stress patterns. The internal algorithms\ngoverning AI text generation might prioritize specific phonological features during the\nconstruction of sentences. These results suggest that AI models may have inherent biases in\nconsonant production, possibly due to the training data or the specific algorithms used. While the\ndifferences were small, they highlight an area where AI text generation could be fine-tuned for\nmore human-like phonological characteristics. According to Suvarna et al. (2024), the acquisition\nof phonological skills by LLMs is still in doubt since there is no access to speech data. The authors\nsuggest that although LLMs perform well in tasks such as songwriting, poetry generation, and\nphonetic transcription, they lack deep phonological understanding.\nThe analysis of morphological features showed significant variations in the usage of adpositions,\nauxiliaries, coordinating conjunctions, nouns, pronouns, and verbs. Notably, AI text employed\nmore coordinating conjunctions, nouns, and pronouns, whereas human text contained more\nadpositions, auxiliaries, and verbs. These differences, with effect sizes ranging from small to large,\nmay reflect the tendency of AI to produce more noun-heavy and conjunction-rich sentences,\npossibly making the text appear more formal or structured. The above findings are consistent with\nthe results of Liao et al. (2023), who reported greater usage of nouns and coordinating conjunctions\nby ChatGPT-generated compared to human-written medical texts. Also, Johansson (2023) found\nChatGPT to use almost twice as many pronouns in the generation of an essay compared to an essay\nwritten by a student. In contrast, human text, with its higher usage of adpositions and verbs, might\nconvey more fluid and dynamic narratives. These findings suggest that while AI-generated texts\ncan closely mimic human language, there are still distinct morphological patterns that differentiate\nthem from human writing. Moreover, the syntactic analysis uncovered significant differences in\nthe use of adjectival modifiers, conjuncts, direct objects, object prepositions, and prepositional\nmodifiers. Al text was found to employ a higher number of conjuncts, adjectival modifiers, and\ndirect objects, whereas human text utilized more object prepositions and prepositional modifiers.\nThe effect sizes ranged from small to large, indicating varying degrees of divergence in syntactic\nstructure. The preference of AI for more conjuncts and direct objects may contribute to a more\nsegmented and explicit sentence structure, while the greater use of prepositional phrases of the\nhuman text could reflect a more subtle and descriptive approach. According to the lexical analysis,\nAI text tend to use more difficult words and content words, whereas human text is inclined to use\neasier words and function words. More advanced vocabulary of AI text compared to human text\nwas also found by Alexander et al. (2023). This difference highlights the contrasting approaches\nin vocabulary selection, with AI possibly generating more sophisticated and varied vocabulary,\nwhile human authors may prioritize clarity and accessibility.\nBy examining linguistic patterns in the AI-generated language, LLMs can be trained in such a way\nas to improve their language development capabilities. This process involves scrutinizing various\nlinguistic constituents within the generated text. By identifying and understanding these patterns,\ndevelopers can adjust the training algorithms to better mimic natural language usage. This\nadvancement has the potential to benefit various domains significantly. For example, in healthcare,\nchatbots offer valuable medical advice and guidance to individuals. The technology program of\nthe World Health Organization, for instance, has developed a chatbot to assist in combating\nCOVID-19 (Walwema, 2021). This chatbot delivers information on virus protection, provides\naccess to the latest news and facts, and helps users prevent the spread of the virus. Therefore, it is\ncrucial for the language used by these chatbots to be as precise as possible.\nAutomatically and effortlessly eliciting linguistic features through an intuitive tool is crucial.\nAdvances in Al have made it possible to seamlessly gather linguistic data from speakers. For\nexample, Open Brain AI, which relies on AI and computer technology, provides a convenient tool\nfor analyzing written texts. Through this online tool, we managed to extract measures of\nphonology, morphology, syntax, and lexicon of human and AI texts. This can be particularly useful\nto educators and clinicians (Themistocleous, 2024a). Educators can aid students with speech,\nlanguage, and communication challenges by using automated AI tools for assessment in these\nareas. Furthermore, clinicians can promptly screen and assess individuals with disorders,\nconsidering that early diagnosis can help prevent or slow the progression of these conditions. There\nare also economic benefits, as automation can reduce costs by requiring less effort and time to\nextract the data (Georgiou & Theodorou, 2023)."}, {"title": "Conclusion", "content": "The results of this study have several implications for the development and refinement of AI text\ngeneration models. Overall, while AI-generated texts exhibit a high degree of linguistic\ncompetence, there are still discernible differences that set them apart from human writing based\non the automated measures we gathered from Open Brain AI. The observed differences in\nphonology, morphology, syntax, and lexicon underscore potentially the need for more refined\ntraining approaches that can produce more human-like text. Future research should explore the\nunderlying causes of these differences, such as the influence of training data and algorithmic\ndesign. By addressing these areas, we can enhance the naturalness and effectiveness of Al-\ngenerated content, making it more indistinguishable from human-produced text. Any conclusions\nshould be treated with caution since the analysis of a larger number and different types of texts\nwould offer a more holistic understanding of the differences between human-written and AI-\ngenerated texts."}, {"title": "Data availability", "content": "Data is available at request."}]}