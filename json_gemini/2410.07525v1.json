{"title": "OFFLINE INVERSE CONSTRAINED REINFORCEMENT LEARNING FOR SAFE-CRITICAL DECISION MAKING IN HEALTHCARE", "authors": ["Nan Fang", "Guiliang Liu", "Wei Gong"], "abstract": "Reinforcement Learning (RL) applied in healthcare can lead to unsafe medical decisions and treatment, such as excessive dosages or abrupt changes, often due to agents overlooking common-sense constraints. Consequently, Constrained Reinforcement Learning (CRL) is a natural choice for safe decisions. However, specifying the exact cost function is inherently difficult in healthcare. Recent Inverse Constrained Reinforcement Learning (ICRL) is a promising approach that infers constraints from expert demonstrations. ICRL algorithms model Markovian decisions in an interactive environment. These settings do not align with the practical requirement of a decision-making system in healthcare, where decisions rely on historical treatment recorded in an offline dataset. To tackle these issues, we propose the Constraint Transformer (CT). Specifically, 1) we utilize a causal attention mechanism to incorporate historical decisions and observations into the constraint modeling, while employing a Non-Markovian layer for weighted constraints to capture critical states. 2) A generative world model is used to perform exploratory data augmentation, enabling offline RL methods to simulate unsafe decision sequences. In multiple medical scenarios, empirical results demonstrate that CT can capture unsafe states and achieve strategies that approximate lower mortality rates, reducing the occurrence probability of unsafe behaviors.", "sections": [{"title": "1\nINTRODUCTION", "content": "In recent years, the doctor-to-patient ratio imbalance has drawn attention, with the U.S. having only 223.1 physicians per 100,000 people (Petterson et al., 2018). AI-assisted therapy emerges as a promising solution, offering timely diagnosis, personalized care, and reducing dependence on experienced physicians. Therefore, the development of an effective AI healthcare assistant is crucial.\nReinforcement learning (RL) offers a promising approach to develop AI assistants by addressing sequential decision-making tasks. However, this method can still lead to unsafe behaviors, such as administering excessive drug dosages, inappropriate adjustments of medical parameters, or abrupt changes in medication dosages. These actions, including \u201ctoo high\u201d or \u201csudden change\", may significantly endanger patients, potentially resulting in acute hypotension, arrhythmias, and organ damage, with fatal consequences (Jia et al., 2020; Shi et al., 2020). For example, in sepsis treatment, vasopressor (vaso) doses above 1\u00b5g/(kg\u00b7min) are linked to a 90% mortality\""}, {"title": "2 RELATED WORKS", "content": "Reinforcement Learning in Healthcare. RL has made great progress in the realm of healthcare, such as sepsis treatment (Huang et al., 2022; Raghu et al., 2017a; Peng et al., 2018; Do et al., 2020), mechanical ventilation (Kondrup et al., 2023; Gong et al., 2023; Yu et al., 2020), sedation (Eghbali et al., 2021) and anesthesia (Calvi et al., 2022; Schamberg et al., 2022). However, the works mentioned above have not addressed potential safety issues such as sudden changes or too high medications. Therefore, the development of policies that are both safe and applicable across various healthcare domains is crucial.\nInverse Constrained Reinforcement Learning. Previous works inferred constraint functions by determining the feasibility of actions under current states. In discrete state-action space, Chou et al. (2020) and Park et al. (2020) learned constraint sets to differentiate constrained state-action pairs. Scobee & Sastry (2019) proposed inferring constraint sets based on the principle of maximum entropy, while some studies (McPherson et al., 2021; Baert et al., 2023) extended this approach to stochastic environments using maximum causal entropy (Ziebart et al., 2010). However, discrete approaches often face limitations when scaling to high-dimensional problems. As the state-action space increases, the computational cost rises significantly. This makes inference in large, discrete spaces challenging, requiring additional optimization techniques or assumptions. In continuous domains, Malik et al. (2021), Gaurav et al. (2022), and Qiao et al. (2024) used neural networks to approximate constraints. Some works (Liu et al., 2022; Chou et al., 2020) applied Bayesian Monte Carlo and variational inference to infer the posterior distribution of constraints in high-dimensional state space. Xu & Liu (2023) modeled uncertainty perception constraints for arbitrary and epistemic uncertainties. However, these methods can only be applied online and lack historical dependency.\nTransformers for Reinforcement Learning. Transformer has produced exciting progress on RL sequential decision problems (Zheng et al., 2022; Chen et al., 2021; Janner et al., 2021; Liu et al., 2023). These works no longer explicitly learn Q-functions or policy gradients, but focus on action sequence prediction models driven by target rewards. Chen et al. (2021) and Janner et al. (2021) perform auto-regressive trajectories modeling to achieve offline policy learning. Furthermore, Zheng et al. (2022) unify offline pretraining and online fine-tuning within the Transformer framework. Liu et al. (2023) and Kim et al. (2023) integrate the transformer architecture into constraint learning and preference learning. With its sequence modeling capability and independence from the Markov assumption, the transformer architecture can capture temporal dependencies in medical decision-making. Thus, it is well-suited for trajectory learning and personalized learning in medical settings."}, {"title": "3 PROBLEM FORMULATION", "content": "Constrained Reinforcement Learning (CRL). We model the medical environment with a Constrained Non-Markov Decision Process (Constrained Non-MDP) $\\mathcal{N}$, which can be defined by a tuple $(S, A, h_t, P, R, C, \\gamma, \\kappa, \\rho_0, T)$ where: 1) $s \\in S$ denotes the state indicators of the patient at each time step. 2) $a \\in A$ denotes the administered drug doses or instrument parameters of interest. 3) $h_t = \\{s_0, a_0, s_1, a_1, ..., s_t\\}$ is the treatment history, where t represents the current time step. 4) $P(s_{t+1} | h_t, a_t)$ defines the transition probabilities. 5) The reward function $R(h_t, a_t)$ is used to describe the quality of the patient's condition and provided by experts based on prior work (Huang et al., 2022; Kondrup et al., 2023). 6) The constraint function $C(h_t, a_t)$ describes the risk or cost associated with taking a particular action given the current historical information. 7) $\\gamma$ denotes the discount factor. 8) $\\kappa \\in \\mathbb{R}^+$ denotes the bound of cumulative costs. 9) $\\rho_0$ defines the initial state distribution. 10) T is the length of the trajectory T. At each time step t, an agent acts at at a patient's state st. This process generates the reward $r_t \\sim R(h_t, a_t)$, the cost $c_t \\sim C(h_t, a_t)$ and the next state $s_{t+1} \\sim P(s_{t+1} | h_t, a_t)$. The goal of the CRL policy $\\pi$ is to maximize expected discounted rewards while limiting the cost in a threshold $\\kappa$:\n$\\arg \\max_{\\pi} \\mathbb{E}_{\\pi,\\rho_0} [\\sum_{t=1}^{T} \\gamma^t r_t]$,\ns.t. $\\mathbb{E}_{\\pi,\\rho_0} [\\sum_{t=1}^{T} \\gamma^t c_t] \\leq \\kappa$.\nCRL commonly assumes that constraint signals are directly observable. However, in healthcare, such signals are often difficult to obtain due to the variability in individual patient characteristics and"}, {"title": "4 METHOD", "content": "To infer constraints and achieve safe decision-making in healthcare, we introduce the Offline Constraint Transformer (shown in Figure 2), a novel ICRL framework.\nIn practice, ICRL can be conceptualized as a bi-level optimization task (Liu et al., 2022). We can 1) update this policy based on Equation 1, and 2) employ Equation 3 for constraint learning. Intuitively, the objective of Equation 3 is to distinguish between trajectories generated by expert policies and imitation policies that may violate the constraints. Specifically, task 1 involves updating the policy using advanced CRL methods. Significant progress has been made in some works such as BCQ-Lagrangian (BCQ-Lag) (Fujimoto et al., 2019), COpiDICE (Lee et al., 2022), VOCE (Guan et al., 2024), and CDT (Liu et al., 2023). Task 2 focuses on learning the constraint function, as shown in Figure 2. Our research primarily improves the latter process, addressing two key challenges that ICRL faces in healthcare: challenge 1 pertains to the limitations of the Markov property, and challenge 2 involves the issue of inferring constraints only from offline datasets. To address these challenges, we propose the offline CT as our solution.\nOffline Constraint Transformer. To address the first challenge, we delve into the inherent issues of applying the Markov property to healthcare and draw inspiration from sequence modeling tasks, redefining the representation of the constraints. To realize the offline training, we consider the essence of ICRL updates, proposing a model-based RL to generate unsafe behaviors used to train CT. We outline three parts: establishing the constraint representation model (Section 4.1), creating an offline RL for violating data (Section 4.2), and learning safe policies (Section 4.3)."}, {"title": "4.1 CONSTRAINT TRANSFORMER", "content": "ICRL methods relying on the Markov property overlook patients' historical information, focusing only on the current state. However, both current and historical states, along with vital sign changes are crucial for a human doctor's decision-making process (Plaisant et al., 1996).\nTo emulate the observational approach of humans, we draw inspiration from the existing historical sequence model (such as Long Short-Term Memory (LSTM) (Graves & Graves, 2012) and Transformer (Vaswani, 2017)) to incorporate historical information into constraints for a more comprehensive observation and judgment. Compared to"}, {"title": "4.2 MODEL-BASED OFFLINE RL", "content": "To train CT offline, we introduce a model-based offline RL method that simultaneously learns a policy model and a generative world model via auto-regressive imitation of the actions and observations in healthcare. The model processes a trajectory, $\\tau_e \\in D_e$, as a sequence of tokens encompassing the return-to-go, states, and actions, defined as $\\{R_0, s_0, a_0, ..., R_T, s_T, a_T\\}$. Notably, the return-to-go $R_t$ at timestep t is the sum of future rewards, calculated as $R_t = \\sum_{t'=t}^{T} \\gamma^{t'}r_{t'}$. At each timestep t, it employs the tokens from the preceding K timesteps as its input, where K represents the context length. Thus, the input tokens for it at timestep t are denoted as $o_t = \\{R_{-K:t}, s_{-K:t}, a_{-K:t-1}\\}$, where $R_{-K:t} = \\{R_K, ..., R_t\\}$, $s_{-K:t} = \\{s_K, ..., s_t\\}$ and $a_{-K:t-1} = \\{a_K, ..., a_{t-1}\\}$."}, {"title": "4.3 SAFE-CRITICAL DECISION MAKING WITH CONSTRAINTS", "content": "To train offline CT, we gather the medical expert dataset $D_e$ from the environment. Then, we employ gradient descent to train the model-based offline RL, guided by Equation 7 and Equation 8, continuing until the model converges. Using this RL model, we automatically generate violating data denoted as $D_v$. Subsequently, CT is optimized based on Equation 6 to get the cost function C, leveraging samples from both $D_e$ and $D_v$. To learn a safe policy, we train the policy $\\pi$ using C until it converges based on Equation 1. The detailed training procedure is presented in Algorithm 1."}, {"title": "5 EXPERIMENT", "content": "In this section, we first provide a brief overview of the task, as well as data extraction and preprocessing. Subsequently, in Section 5.1, we demonstrate that CT can describe constraints in healthcare and capture critical patient states. We emphasize its applicability to various CRL methods and its ability to approach the optimal policy for reducing mortality rates in Section 5.2. Section 5.3 discusses the realization of the objective of safe medical policies. Finally, we use offline policy evaluation (OPE) methods to estimate our policy in the field of dynamic treatment regimes in Section D.2.2.\nTasks. We primarily use the sepsis task that is commonly used in previous works (Huang et al., 2022; Raghu et al., 2017a; Komorowski et al., 2018; Do et al., 2020), and supplement some experiments"}, {"title": "5.1 CAN OFFLINE CT LEARN EFFECTIVE CONSTRAINTS?", "content": "In this section, we primarily assess the efficacy of the cost function learned by offline CT in sepsis, focusing on its capability to evaluate patient mortality rates and capture critical events. First, we employ the cost function to compute cost values for the validation dataset. Subsequently, we statistically analyze the relationship between these cost values and mortality rates. As shown in Figure 5, there is an increase in patient mortality rates with rising cost values. It is noteworthy that such increases in mortality rates are often attributed to suboptimal medical decisions. Therefore, these experimental findings affirm that the cost values effectively reflect the quality of medical decision-making. To observe the impact of the attention layer (Non-Markovian layer), we conduct experiments by removing the attention layer from CT. The results reveal that the penalty values do not correlate proportionally with mortality rates (shown in Figure 5). This indicates that the attention layer plays a crucial role in assessing constraints."}, {"title": "5.2 CAN OFFLINE CT IMPROVE THE PERFORMANCE OF CRL?", "content": "Baselines. We adopt the DDPG method as the baseline in sepsis research (Huang et al., 2022). Since no other offline inverse reinforcement learning works are available for reference, we have included three additional settings: no cost, custom cost, and LLMs cost. In the case of no cost, the cost is set to zero, while the design of custom constraints and LLMs cost are outlined in Appendix B. These settings help evaluate whether CT can infer effective constraints.\nMetrics. To assess effectiveness, we use w to indicate the probability that the policy is optimal and analyze the relationship between DIFF and mortality rate through a graph.\nResults. We combine our method CT with CRL algorithms (e.g., VOCE, COpiDICE, BCQ-Lag, and CDT), and compare them with both no-cost and custom cost settings. Each CRL model is trained using no cost, custom cost, and CT separately, with other parameters set the same during training. For evaluation metrics, we use IV difference (IV DIFF), vaso difference (VASO DIFF), and combined [IV, VASO] difference (ACTION DIFF) as the met-rics to be ranked. We measure the mean and variance of w% in 10 sets of random seeds, and the results are shown in Table 2. From the results, we can conclude: 1) CT makes the strategies in the VOCE, CopiDICE, and CDT methods closer to the lower mortality strategies. 2) CDT+CT achieves better results on all three metrics. CDT is also a transformer-based method, which indicates that transformer-based ar-chitecture indeed exhibits more outstand-ing performance in healthcare."}, {"title": "5.3 CAN CRL WITH OFFLINE CT LEARN SAFE POLICIES?", "content": "We have confirmed the existence of two unsafe strategy issues, namely \"too high\" and \"sudden change\" in the treatment of sep-"}, {"title": "5.4 OFF-POLICY EVALUATION", "content": "Baselines. 1) Naive baselines. A naive baseline can provide worst-case scenario benchmarks for algorithm evaluation (Luo et al., 2024a), including random policy $\\pi_r$, zero-drug policy $\\pi_{min}$, max-drug policy $\\pi_{max}$, alternating policy $\\pi_{alt}$, and weight policy $\\pi_{weight}$. 2) RL methods baselines. We select common RL methods such as Deep Q-Network (DQN), Conservative Q-Learning (CQL), Implicit Q-Learning (IQL), and Batch Constrained Q-Learning (BCQ) as baseline models.\nMetrics. A recent series of studies have applied offline policy evaluation techniques to dynamic treatment regimes, including Weighted Importance Sampling (WIS) (Kidambi et al., 2020; Nambiar et al., 2023) and Doubly Robust (DR) estimators (Raghu et al., 2017a; Wu et al., 2023; Wang et al., 2018). To more accurately evaluate the policy, we use metrics such as RMSE and F1 score to describe the deviation from the clinician's policy.\nWe used the same reward function to compare the policy results under different evaluation metrics, as shown in Table 4. Our findings present that the CDT+CT method outperforms other methods in terms of $RMSE_{IV}$, WIS, $WIS_t$, and $WIS_{bt}$ evaluation metrics. Since CDT+CT produces more safe and conservative policies, there is a certain distance from the clinician's policy, so it does not perform as well in terms of $RMSE_{vaso}$ and F1 score."}, {"title": "6 CONCLUSION", "content": "In this paper, we propose offline CT, a novel ICRL algorithm designed to address safety issues in healthcare. This method utilizes a causal attention mechanism to observe patients' historical information, similar to the approach taken by actual doctors, and employs Non-Markovian importance weights to effectively capture critical states. To achieve offline learning, we introduce a model-based offline RL for exploratory data augmentation to discover unsafe decisions and train CT. Experiments in sepsis and mechanical ventilation demonstrate that our method avoids risky behaviors while achieving strategies that closely approximate the lowest mortality rates."}, {"title": "A.1 SEPSIS PROBLEM DEFINE", "content": "Our definition is similar to (Raghu et al., 2017b). We extract data from adult patients meeting the criteria for sepsis-3 criteria (Singer et al., 2016) and collect their data within the first 72 hours of admission.\nState Space. We use a 4-hour window and select 48 patient indicators as the state for a one-time unit of the patient. The state indicators include Demographics/Static, Lab Values, Vital Signs, and Intake and Output Events, detailed as follows (Raghu et al., 2017b):\n\u2022 Demographics/Static: Shock Index, Elixhauser, SIRS, Gender, Re-admission, GCS - Glasgow Coma Scale, SOFA - Sequential Organ Failure Assessment, Age\n\u2022 Lab Values Albumin: Arterial pH, Calcium, Glucose, Hemoglobin, Magnesium, PTT - Partial Thromboplastin Time, Potassium, SGPT - Serum Glutamic-Pyruvic Transaminase, Arterial Blood Gas, BUN Blood Urea Nitrogen, Chloride, Bicarbonate, INR - International Normalized Ratio, Sodium, Arterial Lactate, CO2, Creatinine, Ionised Calcium, PT - Prothrombin Time, Platelets Count, SGOT Serum Glutamic-Oxaloacetic Transaminase, Total bilirubin, White Blood Cell Count\n\u2022 Vital Signs: Diastolic Blood Pressure, Systolic Blood Pressure, Mean Blood Pressure, PaCO2, PaO2, FiO2, PaO/FiO2 ratio, Respiratory Rate, Temperature (Celsius), Weight (kg), Heart Rate, SpO2\n\u2022 Intake and Output Events: Fluid Output - 4 hourly period, Total Fluid Output, Mechanical Ventilation\nAction Space. Regarding the treatment of sepsis, there are two main types of medications: intravenous fluids and vasopressors. We select the total amount of intravenous fluids for each time unit and the maximum dose of vasopressors as the two dimensions of the action space, defined as (sum(IV), max(Vaso)). Each dimension is a continuous value greater than 0.\nReward Function. We refer to the reward function used in (Huang et al., 2022), as shown in the following equation:\nr (St, St+1) = A1 tanh (SOFA \u2013 6) + 12 (SOFA \u2013 SOFA))\n(St+1\nWhere \u03bb\u03bf and \u03bb\u2081 are hyperparameters set to -0.25 and -0.2, respectively. This reward function is designed based on the SOFA score, as it is a key indicator of the health status of sepsis patients and is widely used in clinical settings. The formula describes a penalty when the SOFA score increases and a reward when the SOFA score decreases. We set 6 as the cutoff value because the mortality rate sharply increases when the SOFA score exceeds 6 (Ferreira et al., 2001)."}, {"title": "A.2 MECHANICAL VENTILATION TREATMENT PROBLEM DEFINE", "content": "The RL problem definition for Mechanical Ventilation Treatment is referenced from (Kondrup et al., 2023).\nState Space. We also use a 4-hour window and select 48 patient indicators as the state for a one-time unit of the patient. The state indicators are as follows:\n\u2022 Demographics/Static: Elixhauser, SIRS, Gender, Re-admission, GCS, SOFA, Age\n\u2022 Lab Values Albumin: Arterial pH, Glucose, Hemoglobin, Magnesium, PTT, BUN Blood Urea Nitrogen, Chloride, Bicarbonate, INR, Sodium, Arterial Lactate, CO2, Creatinine, Ionised Calcium, PT, Platelets Count, White Blood Cell Count, Hb"}, {"title": "B DESIGN AND ANALYSIS OF THE CUSTOM AND LLMS COST FUNCTION", "content": "We base our design on prior knowledge that intravenous (IV) intake exceeding 2000mL/4h or vasopressor (Vaso) dosage surpassing 1g/(kg\u00b7 min) is generally considered unsafe in sepsis treatment (Shi et al., 2020). To design a reasonable constraint function, we refer to the constraint function designed by Liu et al. in the Bullet safety gym environments (Liu et al., 2023). We define the cost function as shown in Equation 11. Thus, during the treatment of sepsis, if the agent exceeds the maximum dosage thresholds of the two medications, it incurs a cost due to constraint violation."}, {"title": "LLMS COST FUNCTION", "content": "We provide prior knowledge to GPT-4.0, and the cost function it designs is as shown in Equation 12. Based on the self-designed constraint function, LLMs added a penalty for Vaso doses mutations, giving the agent a certain penalty when the change in Vaso doses exceeds the threshold."}, {"title": "C THE EVALUATION OF MODEL-BASED OFFLINE RL", "content": "Regarding the selection of trajectory length, we consider the relationship between the average pre-diction error, the error of the last point in the trajectory, and the trajectory length. We use the model-based offline RL to generate trajectories and compare them with expert data using the Eu-clidean distance to measure their differences. We evaluate the average error and the error of the last point in the trajectory, as shown in Figure 9. We observe that with an increase in trajectory length, the average prediction error at each time step decreases, while the state error stabilizes. Taking into account the observation length and prediction accuracy, we ultimately choose to generate trajectories with lengths ranging from 10 to 15."}, {"title": "D THE EVALUATION OF COST FUNCTION", "content": "To validate that the CT method captures key states, we conduct statistical analysis on the relationship between state values and penalty values. We collect penalty values under different state values for all patients, and the complete information is shown in Figure 13. We find that the CT method successfully captures unsafe states and imposes higher penalties accordingly. The safe range of state values is shown in Table 7."}, {"title": "D.1.2 CAPTURE UNSAFE HIDDEN VARIABLES", "content": "In a medical context, mortality rates may be influenced by various factors. The dataset often contains numerous unaccounted features (hidden variables), such as epinephrine, dopamine, medical history, and phenotypes. As noted in (Jeter et al., 2019), clinicians typically set a mean arterial pressure (MAP) target (e.g., 65) and administer vasopressors until the patient reaches a safe pressure level. Additionally, Luo et al. (2024a) suggest using the NEWS2 score as evidence for clinical rewards. To validate whether our penalty function captures changes in hidden variables (NEWS and MAP), we conducted supplementary experiments, as shown in Figure 12. When the NEWS score is excessively high, the penalty value increases accordingly; similarly, when MAP falls outside the normal range, the penalty also rises. This indicates that the penalty function successfully captures changes in hidden variables and compensates for the reward function's omission of certain parameter variables. Therefore, we can rely on a simple reward function and use the penalty function to achieve safe policy learning."}, {"title": "EXPERIMENTAL SETTINGS", "content": "To train the CRL+CT model, we use a total of 3 NVIDIA GeForce RTX 3090 GPUs, each with 24GB of memory. Training a CRL+CT model typically takes 5-6 hours. We employ 5 random seeds for validation. We use the Adam optimization algorithm to optimize all our networks, updating the learning rate using a decay factor parameterization at each iteration. The main hyperparameters are summarized in Table 10 and 11."}]}