{"title": "FGCE: Feasible Group Counterfactual Explanations for Auditing Fairness", "authors": ["Christos Fragkathoulas", "Vasiliki Papanikou", "Evaggelia Pitoura", "Evimaria Terzi"], "abstract": "This paper introduces the first graph-based framework for generating group counterfactual explanations to audit model fairness, a crucial aspect of trustworthy machine learning. Counterfactual explanations are instrumental in understanding and mitigating unfairness by revealing how inputs should change to achieve a desired outcome. Our framework, named Feasible Group Counterfactual Explanations (FGCEs), captures real-world feasibility constraints and constructs subgroups with similar counterfactuals, setting it apart from existing methods. It also addresses key trade-offs in counterfactual generation, including the balance between the number of counterfactuals, their associated costs, and the breadth of coverage achieved. To evaluate these trade-offs and assess fairness, we propose measures tailored to group counterfactual generation. Our experimental results on benchmark datasets demonstrate the effectiveness of our approach in managing feasibility constraints and trade-offs, as well as the potential of our proposed metrics in identifying and quantifying fairness issues.", "sections": [{"title": "I. INTRODUCTION", "content": "AI-driven technologies increasingly influence various aspects of society shaping significant decisions that impact our lives. Ensuring the fairness of their decisions and providing a clear understanding of why they yield specific outputs is becoming a societal imperative. Various types of explanation methods have emerged for providing transparency of such decisions [1, 2, 3, 4]. Among them, counterfactual explanations (CFEs) have attracted significant attention [5]. CFEs offer insights into how altering the features of an instance could lead to a different outcome for this instance.\nWhile most previous work focuses on CFEs for individual instances [6, 7, 8, 9, 10], there is increasing interest in group counterfactuals (GCEs). Unlike individual CFEs, which alter features for single instances, GCFEs explore how modifying features can impact entire groups. This approach is particularly valuable in real-world applications such as fairness in lending, hiring, and criminal justice, where decisions affect groups collectively. GCFEs help assess and gain insights into unfairness by revealing how different changes in features might influence outcomes across different groups [11, 12, 13]. However, existing methods often fall short of producing GCFEs that are feasible and actionable. GCFEs should be coherent with the data distribution, representative of the population, and propose feasible feature alterations.\nIn this paper, we introduce Feasible Group Counterfactual Explanations (FGCE), a novel graph-based approach to generating group counterfactuals. Our approach utilizes a density-weighted graph that encapsulates feasibility constraints [6]. We call this graph feasibility graph. The graph facilitates feasible transformations between instances while considering the associated costs of these transformations. We represent feasibility through reachability in the feasibility graph. The connected components of the graph represent subgroups of the group to be explained, which are covered by the same set of counterfactuals.\nWe propose efficient algorithms for identifying a small set of counterfactuals that cover many instances in the group (i.e., are feasible transformations) and capture the various trade-offs in group counterfactual generation. FGCE ensures that the generated CFEs are aligned with the underlying data distribution and offers practical and feasible insights for the decision-making process.\nThen, we use FGCE to explain and measure unfairness. Specifically, we consider groups of instances defined based on the value of one or more of their sensitive attributes, and generate FGCEs for each group. We introduce novel measures of the fairness of each group by comparing the FGCEs of each group. Our measures encapsulate the trade-offs of the size, coverage, and cost of the FGCEs and are applicable at both the group and the subgroup level.\nFinally, we present experimental results of the fairness of three real datasets using the proposed metrics that show the effectiveness of FGCE in auditing unfairness. We also compare experimentally our method with related research. Despite incorporating stringent feasibility constraints, our method outperforms existing baselines, demonstrating its ability to propose counterfactuals that align with the underlying data distribution.\nThe remainder of this paper is structured as follows. In Section 2, we introduce the problem, in Section 3 our algorithms, and in Section 4, the FGCE-based fairness measures. In Section 5, we present experimental results, in Section 6 related work, and in Section 7, we offer conclusions."}, {"title": "II. PROBLEM DEFINITION", "content": "We consider a binary classifier $f : R^d \\rightarrow \\{0,1\\}$, which maps instances in a d-dimensional feature space into two classes, labeled 0 and 1. Let $U \\subseteq R^d$ denote the input space.\nA model prediction on an individual instance $x \\in U$, called factual, is explained by crafting a counterfactual (CF) instance $x' \\in R^d$ that is similar to $x$ but its outcome $f(x')$ differs from the initial prediction $f(x)$ [5]. The changes in feature values from $x$ to $x'$ should be feasible, adhering to real-world constraints. For instance, alterations to immutable features, such as race or height, should be prohibited. Formally, a CF $x'$ for $x$ is defined as: $x' = arg \\, min_{x'\\in A_x} cost(x, x') \\, s.t. \\, f (x') \\neq f(x)$, where $cost(x, x')$ is a distance function and $A_x$ restricts the set of counterfactuals to those attainable from $x$ through feasible changes.\nCounterfactuals not only explain a decision but also suggest the actions to be applied to a factual for getting a favorable outcome. For example, consider a classifier that decides whether a person is eligible for a loan, using features such as age, education, or income. Say Alice is refused the loan. A counterfactual for Alice could for example indicate that Alice should have received the loan, if she had a MSc degree, or her income was increased by 10K.\nIt would be hard to trust a counterfactual if it resulted in a combination of features that were unlike any observations the classifier has seen before [5], thus counterfactuals should also be coherent with the underlying data distribution. To ensure feasibility and trustworthiness, we adopt a graph-based approach. We construct a weighted directed graph $G_u = (V, E, W)$, termed feasibility graph, whose nodes correspond to instances in $U$ and there is an edge between nodes $x_i$ and $x_j$, if changes between the corresponding instances meet feasibility constraints and the distance between them falls within a predetermined threshold denoted as $e$. This ensures that alterations between instances are both feasible and small. The weight function $W$ is defined following the density-based approach introduced in FACE [6] to ensure that counterfactuals lie in dense areas of the input space and avoid outliers. Each edge in $G_u$ is assigned a weight $W_{ij}$, calculated as the product of the distance between the instances (capturing the difficulty of transitioning) and the density of the instances around the midpoint of $x_i$ and $x_j$, where density is estimated using a Kernel Density Estimator (KDE) [14] :\n$W_{ij} = KDE(\\frac{X_i + X_j}{2}) ||X_i - X_j ||$\nGiven a feasibility graph $G_u$, we define the feasibility set $A_x$ of instance $x$ to be the set of instances $x'$ for which there is a path in $G_u$ from $x$ to $x'$, i.e., the set of instances that are reachable from $x$: $A_x = \\{x' \\in U|x'$ is reachable from $x$ in $G_u\\}$. We call instances in this set feasible counterfactuals for $x$.\nIn this paper, instead of finding a CF for a single instance $x$, we are interested in providing counterfactuals for a set $X \\subseteq U$ of instances mapped to the same class. Let $X' \\subseteq U$ be the set of instances mapped to the opposite class. Our goal is to identify a small subset $S$ of $X'$ of size $k$ that best explains $X$. We constrain the number $k$ of CFs for interpretability, so that the produced CFs are manageable. To select $S$, we consider the trade-off between coverage and cost.\nFor a group of counterfactuals $S \\subseteq X'$, coverage is defined as:\n$coverage(X, S) = |\\{x \\, | \\, x \\in X \\, and \\, \\exists x' \\in S \\cap A_x\\}|$\nWe overload notation and use function cost to define both the cost between an instance and a set of instances and the cost between two sets of instances as follows:\n$cost(x, S) = min_{x'\\in S} cost(x, x')$\n$cost(X, S) = min_{X \\in X} max \\, cost (x, S)$\nThe cost function $cost(x, x')$ between two instances $x$ and $x'$ can be any function that captures the cost of transforming $x$ to $x'$ offering flexibility to adapt to specific problem requirements. For example, cost can be defined as the vector distance (e.g., L2 distance) between $x$ and $x'$. Alternatively, cost can be defines as the sum of edge weights along the shortest path from $x$ to $x'$ in the feasibility graph, or as the number of hops on this path. By emphasizing proximity in feature space and considering dense paths, these definitions ensure that the derived counterfactual explanations are both feasible and closely aligned with real data instances. Our approach works with any definition of cost.\nWe provide two definitions of the Feasible Group Coun- terfactual Explanation (FGCE) problem. Our first definition prioritizes cost over coverage setting a threshold on cost.\nProblem 1 (Cost-Constrained FGCE): Given $X, X', k\\in N^*$, and distance threshold $d \\in R$, find $S\\subseteq X'$ with $|S| \\leq k$ and $Q \\subseteq X$ such that for every instance $x \\in Q$ there exist an instance $x' \\in S$ such that $cost(x, x') \\leq d$ and $|Q|$ is maximized.\nOur second problem definition prioritizes coverage over cost, asking for a set that provides a specified coverage degree c.\nProblem 2 (Coverage-Constrained FGCEs): Given $X, X', k \\in N$, and coverage degree $c, 0 < c \\leq 1$, find $S \\subseteq X'$ with $|S| \\leq k$ such that $coverage(X,S) \\geq c|X|$ and $cost(X, S)$ is minimized."}, {"title": "III. ALGORITHMS", "content": "Our approach to generating feasible counterfactuals hinges on the feasibility graph $G_u$ that inherently captures the dataset constraints. It is easy to see that a necessary condition for $x'$ to be a feasible counterfactual for $x$ is that $x$ and $x'$ belong to same weakly connected component (WCC) of $G_u$. Thus, $G_u$ splits the set of factuals $X$ into $m, m > 0$, disjoint subsets $X_i$, where each $X_i$ includes the instances of $X$ that belong to the same WCC of $G_u$ and whose feasible counterfactuals are also disjoint subsets $X_i'$ belonging to the same WCC with $X_i$. This partition of $X$ into $m$ subgroups with distinct feasible explanations offers an opportunity to understand the behavior of the model both at the group and at the subgroup level.\nIn the following, we present two versions: (a) a global version that generates counterfactuals for the whole set $X$ and (b) a local version that generates counterfactuals per subgroup $X_i$.\nWe also show how the local version can be used to generate counterfactuals for the whole group $X$."}, {"title": "A. Cost-Constrained FGCE", "content": "We formulate the global version of the cost-constrained problem as a greedy optimization problem, where we maximize coverage by iteratively selecting counterfactuals that cover the maximum number of factual instances from $X$ that have not yet been covered (Algorithm 1). In Lines 2-4, we compute for each candidate counterfactual $x'$ the number of factuals covered by it. The greedy selection is then applied to these candidates to determine the final set $S$ of counterfactuals.\nAlgorithm 1 can also be used to provide a counterfactual explanation for a subgroup $X_i$ by applying it only to the corresponding WCC. We can also utilize this local version to provide counterfactuals for the whole group $X$ by applying the greedy algorithm iteratively to all $m$ WCC as follows. Initially, we apply a single step of the greedy algorithm at each WCC. Then we select the CF that provides the overall best coverage and apply an additional step of the algorithm to the WCC from where the CF was selected. We repeat this until the maximum number $k$ of counterfactuals is reached or all factual instances are covered. It is easy to see that this local version provides the same result as the global one."}, {"title": "B. Coverage-Constrained FGCE", "content": "The coverage-constrained FGCE problem is similar to the classical k-center problem that is known to be NP-hard [16]. While there are known polynomial 2-approximations for this problem, in this paper, we adopt a mixed integer programming (MIP) formulation which proves to be very efficient in practice [17]. Let $r_{xx'} = 1$ if $x'$ covers $x$; and $r_{xx'} = 0$, otherwise. Let $u_{x'}$ = 1 if counterfactual $x'$ covers any instance in $X$, and $u_{x'}$ = 0 otherwise. The goal is to minimize the maximum cost $d$ of the farthest instance, while ensuring coverage$(X, S) \\geq c |X|$:\n$minimized$\n$\\sum_{x' \\in X'} cost(x,x')r_{xx'} \\leq d, \\forall x \\in X$ (1)\n$\\sum_{x' \\in X'} r_{xx'} \\leq 1, \\forall x \\in X$ (2)\n$\\sum_{x' \\in X'} u_{x'} \\leq k$ (3)\n$r_{xx'} \\leq u_{x'}, \\forall x' \\in X' \\, and \\, \\forall x \\in X$ (4)\n$\\sum_{x \\in X} \\sum_{x' \\in X'} r_{xx'} \\geq cX$ (5)\n$u_{x'}, r_{xx'} \\in \\{0,1\\}, \\forall x \\in X, x' \\in X'$ (6)\nConstraint 1 establishes that cost d must be the maximum of the pairwise costs. Constraint 2 ensures that the sum of the $r_{xx'}$ variables over $x'$ is at most 1. Constraint 3 limits the number of selected instances $x' \\in X'$ at k. Constraint 4 specifies that if $u_{x'}$ = 1 (instance $x'$ covers at least one instance $x$) then instances $x' \\in X'$ for which $r_{xx'}$ = 1 should assigned to them. Constraint 5 ensures that the desired coverage percentage is achieved. Finally, Constraint 6 imposes binary restrictions. Note that for full coverage, i.e, c = 1, Constraint 2 becomes an equality constraint, and Constraint 5 is unnecessary.\nIn the global version, we invoke the MIP algorithm on the $G_u$ graph. To improve performance, we add constraints only for instances $x$ and $x'$, such that $x' \\in A_x$. In the local version, to get counterfactuals for a specific subgroup $X_i$ of $X$, we invoke the MIP algorithm at the corresponding WCC of $G_u$.\nWe now describe how the local version can be applied to solve the global version. Let us first consider the simplest case of full coverage, i.e., c = 1. Suppose we have m WCCs ordered arbitrarily as $C_1, C_2, ..., C_m$. In this case, achieving full coverage simplifies to distributing k counterfactuals among these m components. Note that the maximum number of counterfactuals that can be allocated to each WCC is at most k-m, since for full coverage we need at least one counterfactual per WCC. First, we run MIP at each WCC with k varying from 1 up to k - m. Let $l_i$ denote the minimum number of counterfactuals needed to fully cover the i-th WCC. We start by allocating $l_i$ counterfactuals to each $C_i$. Next, we keep allocating one counterfactual at a time to the component with the highest cost until the total number of allocated counterfactuals equals k.\nWhen c < 1, the task becomes more complex as we have to allocate both k and coverage c across the WCCs. Let F(1...i, k,n) be the minimum cost of allocating k counterfactuals that cover a total of n factuals considering connected components $C_1, ...., C_i$."}, {"title": "IV. FGCE FOR AUDITING FAIRNESS", "content": "In this section, we examine algorithmic fairness through the lens of FGCEs. In a high level, group (algorithmic) fairness refers to a set of criteria aimed at ensuring that protected groups, such as those defined by the values of sensitive attributes, like gender, race, or age, are treated similarly by the classifier. These criteria can be categorized into [18]: those based on demographic parity, which require that the proportion of groups receiving favorable outcomes reflects their representation in the input, and error-based ones, which focus on equalizing classification errors, such as false negatives, across groups.\nPrevious research has explored counterfactual explanations towards measuring and understanding group unfairness [4]. FGCES offer novel tools in both directions. The main idea is to generate FGCEs for each of the groups.\nLet $X$ be one of the groups. We generate FGCEs for subsets of $X$, depending on the type of group fairness that we want to explore. For example, for demographic parity, we generate counterfactuals for the instances of $X$ that are classified to the negative class, while for error-based fairness, for e.g., the instances of $X$ that are falsely classified to the negative class, i.e., the false negatives. Disparities among the FGCEs generated for each of the groups can signal underlying biases warranting deeper investigation.\nOne of the distinctive features of FGCE is that it allows addressing fairness at multiple levels. FGCE partitions each group into subgroups based on the connected components of the feasibility graph, resulting in distinct FGCEs for each subgroup $X_i$. This provides a partition of $X$ into subgroups with distinct feasible explanations and offers an opportunity to understand unfair behavior both at the group and at the subgroup level.\nWe define the area under the curve for k, kAUC(k), as:\n$kAUC(k) = \\int_{dmin}^{dmax} coverage(X, S_{k,d})dd$\nThis metric measures the area under the curve when plotting coverage over a range of cost values for a given number of counterfactuals k. This offers insights into how effectively a group can achieve coverage across varying costs for a given k. There is a value of cost that provides the largest possible coverage for k, we call it saturation point for k and denote it as sp(k). That is, it holds, for any d \u2265 sp(k), coverage(X, $S_{k,d}$) = coverage(X, $S_{k,sp(k)})$.\nSimilarly, for a given cost d, we define the area under the curve for d, dAUC(d), as:\n$dAUC(d) = \\int_{kmin}^{kmax} coverage(X, S_{k.d})dk$\nThis metric computes the area under the curve of coverage when plotting coverage against the number of counterfactuals, given a fixed maximum cost d. It helps in understanding the difficulty of a group in achieving coverage across various numbers of counterfactuals when restricting the cost to be at most d. There is a value of k that provides the largest coverage possible, called saturation point for d. That is, it holds, for any k \u2265 sp(d), coverage(X, $S_{k,d}$) = coverage(X, $S_{sp(d),d}$).Finally, for a given c, we define the area under the curve for c, cAUC(c), as:\n$CAUC(c) = \\int_{kmin}^{kmax} cost(X, S_{k,c})dk$\nThis metric quantifies the area under the curve of the minimum maximum cost over a number of counterfactuals given a specific coverage, indicating the effort needed to achieve a specific coverage in terms of minimum maximum cost. There is a value of k that provides the smallest cost possible called saturation point for c. That is, for any k \u2265 sp(c), cost(X, $S_{k,c}$) = cost(X, $S_{sp(c),c}$).For computing the kAUC and the dAUC measures, we solve the cost-constrained FGCE problem, and for computing the CAUC measures, the coverage-constrained FGCE problem.\nLet $ACF(X,S,A) = \\frac{1}{|X|} \\sum_{x \\in X}(1-\\delta(x_A, x'_A))$, where $\\delta(x, x')$ is the Kronecker delta which returns 1 if $x_A = x'_A$, and 0 otherwise."}, {"title": "V. EXPERIMENTS", "content": "The goal of our experiments is twofold: (a) to showcase the efficacy of our approach in auditing fairness, and (b) to compare our approach with related research in generating group counterfactuals.\nWe use three widely used real datasets, namely Student\u00b9, COMPAS\u00b2 and Adult\u00b3. As a first step, we create the feasibility graph $G_u$ that encodes possible transitions between instances in the dataset. There is an edge from node, i.e., instance, $x_i$ to $x_j$, if the transition from $x_i$ to $x_j$ is feasible and the distance between them is smaller than $e$. For each dataset, we manually specify the feasibility constraints (see Supplementary material).\nIn the following, we use as default e the minimum value to achieve full connectivity, namely, 3, 0.3, and 0.3 respectively for the Student, COMPAS, and Adult dataset.\nWe use the Gender attribute to define groups and denote the created groups $G_0$ and $G_1$. $G_0$ corresponds to females and $G_1$ to males. Table VII presents related statistics."}, {"title": "A. Auditing Fairness", "content": "In this set of experiments, we exploit our algorithms to audit fairness. Without loss of generality, we focus on finding counterfactual explanations for the false negative (FN) instances of a classification model (logistic regression) of groups $G_0$ and $G_1$. We include only instances in $G_0$ and $G_1$ for which at least one feasible counterfactual exists and use as a cost function, the $L_2$ vector distance.\n1) Burden: Let us first study the subgroups of groups $G_0$ and $G_1$ as formed by the WCCs of $G_u$. The number of WCCs per group is a lower bound for the minimum number $k_0$ of counterfactuals required for the full coverage of the group.\nNext, we use the AUC-related fairness measures to account for the various trade-offs between the number k of counterfactuals, cost d, and coverage c."}, {"title": "B. Comparison with Baselines", "content": "In this set of experiments, we compare the performance of our approach to previous research on generating counterfactuals. We conduct two types of evaluations: (a) against FACE [6], a graph-based method that generates individual counterfactuals, and (b) against two state-of-the-art baselines, AReS [11] and GLOBE-CE [12], which generate group counterfactuals. For this set of experiments, we generate counterfactuals for the whole group $G = G_0 \\cup G_1$.\n1) Comparison with Individual Counterfactuals: We first compare our approach with FACE [6], an approach that produces counterfactuals for individual factuals using a similar graph-based approach. We apply FACE to locate for each $x \\in X$ the counterfactual $x' \\in A_x$ such that the cost(x, x') is minimized.\nWe solve the coverage-constrained FGCE problem to generate a set S of counterfactuals for varying values of k. For FACE, we generate individual counterfactuals for all factuals covered by the set S returned by FGCE. We then compare the average cost between x and its optimal counterfactual generated using FACE with the average cost between x and its counterfactual in set S generated by FGCE."}, {"title": "VI. RELATED WORK", "content": "Explanations have become a central focus in recent research [2, 3, 4, 5, 20, 21, 22, 23, 24], particularly as machine learning systems are increasingly employed to inform decisions in critical societal domains such as finance, healthcare, and education.\nCounterfactual explanations hold a significant place in the explanations commonly used in the literature. Wachter et al. [25], one of the first works on counterfactual explanations, formulates CEs as an optimization problem that minimizes the distance between the original input and a counterfactual instance while ensuring the desired class prediction."}]}