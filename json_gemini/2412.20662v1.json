{"title": "Enhancing Table Recognition with Vision LLMs: A Benchmark and Neighbor-Guided Toolchain Reasoner", "authors": ["Yitong Zhou", "Mingyue Cheng", "Qingyang Mao", "Qi Liu", "Feiyang Xu", "Xin Li", "Enhong Chen"], "abstract": "Pre-trained foundation models have recently significantly progressed in structured table understanding and reasoning. However, despite advancements in areas such as table semantic understanding and table question answering, recognizing the structure and content of unstructured tables using Vision Large Language Models (VLLMs) remains under-explored. In this work, we address this research gap by employing VLLMs in a training-free reasoning paradigm. First, we design a benchmark with various hierarchical dimensions relevant to table recognition. Subsequently, we conduct in-depth evaluations using pre-trained VLLMs, finding that low-quality image input is a significant bottleneck in the recognition process. Drawing inspiration from these findings, we propose the Neighbor-Guided Toolchain Reasoner (NGTR) framework, which is characterized by integrating multiple lightweight models for low-level visual processing operations aimed at mitigating issues with low-quality input images. Specifically, we utilize a neighbor retrieval mechanism to guide the generation of multiple tool invocation plans, transferring tool selection experiences from similar neighbors to the given input, thereby facilitating suitable tool selection. Additionally, we introduce a reflection module to supervise the tool invocation process. Extensive experiments on public table recognition datasets demonstrate that our approach significantly enhances the recognition capabilities of the vanilla VLLMs. We believe that the designed benchmark and the proposed NGTR framework could provide an alternative solution in table recognition\u00b9.", "sections": [{"title": "1 Introduction", "content": "Tables are ubiquitous for organizing and communicating structured information across diverse domains, ranging from scientific literature and business reports to web pages and financial documents [1, 2]. They encapsulate a wealth of information essential for numerous applications, such as knowledge discovery, decision support, and data-driven analytics [3, 4]. In the context of intelligent table applications, one fundamental yet challenging task is table recognition [5]: converting image-based table representations into structured digital formats. Over the years, substantial efforts [6] have been made to tackle this problem, introducing various approaches to address challenges such as segmentation techniques and object detection methods.\nRecently, the advent of Large Language Models (LLMs) [7] and Vision Large Language Models (VLLMs) [8] has revolutionized natural language processing and computer vision, respectively. For LLMs, their ability to process serialized data has facilitated numerous tabular data mining tasks, such as table-to-text generation [9-11], table question answering [12-14], and table semantic understanding [15-17]. Meanwhile, several VLLM-based methods have emerged to bypass traditional OCR pipelines for visual table analysis and understanding [18, 19].\nDespite these advancements, our investigation reveals a noticeable gap: the application of VLLMs to table recognition remains underexplored. This task serves as a foundational building block for table-related applications. Some existing work [20, 21] has focused on pre-training or fine-tuning VLLMs to accomplish this task. However, Fine-tuning VLLMs for specific tasks is often computationally expensive and risks catastrophic forgetting of general capabilities. To address this, we focus on leveraging a prompt-based paradigm for table recognition using pre-trained VLLMs, exploring a generative approach without requiring additional fine-tuning. Recognizing the absence of dedicated benchmarks in this domain, we design an evaluation framework based on a hierarchical design philosophy [22-24], comprising recognition tasks for table recognition. Through extensive evaluations, we identify a critical bottleneck: low-quality input images significantly hinder the table recognition capabilities of the evaluated VLLMs. This is understandable given the various characteristics associated with different table styles.\nTo overcome this limitation, we propose the Neighbor-Guided Toolchain Reasoner (NGTR) framework with the goal of effective table recognition. One of the main characteristics of the NGTR framework is that it integrates the capacity of lightweight models and the strategy of retrieval-augmented generation to enhance input image quality and guide structured data recognition. Specifically, we propose a preprocessing toolkit equipped with various lightweight models designed to enhance input image quality. For each input instance, we retrieve a similar neighbor from the training data, and the experience gained from these neighbors is used to inform the guidance for the generation of tool invocation plans. This experience based guidance helps select and integrate suitable tools for the given input. Furthermore, we incorporate a reflection-driven tool selection module at each step of tool invocation to iteratively refine table recognition outputs. This enables VLLMs to produce higher-quality structured data.\nTo validate the effectiveness of the proposed NGTR framework, we conduct extensive experiments on multiple public table recognition datasets. The key observations are as follows: (1) Our NGTR framework significantly enhances the table recognition performance of naive VLLM-based approaches; (2) While VLLMs achieve competitive accuracy on specific datasets compared to traditional models, a noticeable performance gap remains in favor of traditional models. Nonetheless, we have preliminarily revealed the performance boundaries of VLLMs in several representative table recognition datasets. As is shown in Figure 1, the VLLM-based table recognition approach demonstrates the capability for universal modeling. This method facilitates a paradigm shift in design objectives from a model-centric to a data-centric focus, presenting significant potential for further exploration. We hope this work will inspire more research efforts in the future.\nIn summary, the contributions of this paper are as follows:\n\u2022 We pioneer the exploration of VLLM-based table recognition tasks by introducing a comprehensive benchmark with hierarchical evaluation settings.\n\u2022 We propose the NGTR framework to address critical bottlenecks in table recognition, such as low-quality input images.\n\u2022 We conduct extensive experiments to report the promising performance and potential of VLLMs for table recognition, along with interesting observations that highlight areas for future research."}, {"title": "2 Related Work", "content": "Table Recognition. Earlier table recognition (TR) methods predominantly rely on heuristic rules [25-27] or statistical learning techniques [28, 29], which were generally effective on simple and standard structured tables. However, these approaches rely heavily on handcrafted features or implicit rules and show limited generalization ability. In the era of deep learning, numerous studies have made impressive progress in handling more intricate and heterogeneous table structures. Top-down methods [30-33] first predict table lines and boundaries to infer the overall structure, and then locate cells based on row-column intersections. Bottom-up methods [2, 34, 35] first identify table cells with object detection models [36, 37], and then predict the cell relations to organize the row-column structures to form the overall tables. These methods follow an explicit two-stage learning paradigm with relatively strong transferability and explainability, yet the risks of ambiguous contents and boundless structures may cause unstable and incorrect predictions.\nRecently, sequence-based methods [38-41] have been widely explored to directly generate markup sequence defining structures with specific decoders. Although these approaches require a massive of training data and computing resources, they have demonstrated substantial potential to unify visual-text parsing tasks [42].\nLarge Language Models. In recent years, LLMs have made significant progress in the field of natural language processing. In particular, LLMs such as GPT [43] and Llama [44] have demonstrated exceptional performance in tasks such as multi-task learning [45], zero-shot learning [46], and text generation [47]. These models have not only broken through the limitations of traditional technologies in generating natural language text, but have also shown capabilities in reasoning [48-50] and planning [51, 52]. Meanwhile, VLLMs combine the capabilities of visual and language understanding, enabling LLMs to process visual information. For multimodal understanding scenarios (e.g., scene text recognition [53], visual question answering [54]), VLLMs have been widely validated as effective [55-57]. With continuous progress in text-rich scenarios, some studies [18, 20, 58] have also focused on enabling VLLMs to handle multimodal table understanding tasks.\nDespite their promising success in a wide range of domains, VLLMs applied to table recognition remain under-evaluated and under-explored. Our study presents a comprehensive benchmark for VLLM-based table recognition evaluation. Subsequently, we propose a novel framework to address the bottleneck of VLLMs, thereby enhancing their capabilities in table recognition."}, {"title": "3 Problem Definition and Proposed Benchmark", "content": "3.1 Problem Definition\nWe employ the generation paradigm of VLLMs to address the table recognition (TR) task, which is formulated as a format mapping problem from images to sequences. Formally, given a TR dataset D = {(I\u00b9, H\u00b9)}\u2081\u207f with n samples, we predict the corresponding structured form \u0124\u00b9 for each table image I\u00b9. Specifically, we provide the image table I\u00b9 along with a prompt P as input to the VLLMs, which generates the structured data form \u0124 = VLLM(P, I\u00b9).\n3.2 Benchmark Evaluation Setup\nThis section proposes an evaluation benchmark for TR based on VLLMs, outlining the hierarchical recognition tasks to assess their performance and the evaluation setup, including the recognition tasks and VLLMs being evaluated. Details of the evaluation metrics and datasets are discussed in Section 5.1.\n3.2.1 Recognition Task Design. We design several hierarchical recognition tasks to conduct a more in-depth assessment of VLLMs' table recognition capability. In addition to the general table recognition task, we extend our approach by designing recognition tasks that focus on the structural composition of the table, including the cell-level, row-level, column-level, and global table-level. This hierarchical recognition tasks design comprehensively assesses VLLM's table recognition capability at different levels."}, {"title": "3.3 Benchmark Evaluation Results", "content": "Comprehensive evaluations of hierarchical structural recognition tasks are presented in Figure 2. The results show that, among all the VLLMs we selected, GPT and Gemini demonstrate the strongest performance, consistently outperforming the other VLLMs. Furthermore, open-source Llama demonstrates a significant performance gap compared to the closed-source VLLMs. We give some highlights associated with the benchmark results as follows:\nRow-column Sensitivity Analysis. We found that all models show inconsistent performance between row/column-related tasks. To mitigate the influence of uneven distributions of rows and columns in the original data leading to varying difficulty levels, we further refine the experimental results by selecting samples where the difference between the number of rows and columns does not exceed three. The refined results are presented in Figure 3. The results of row-column sensitivity analysis experiments show that the accuracy for the column-related tasks is higher than for the row-related tasks, suggesting that VLLMs prefer to process column-structured information.\nWe believe this phenomenon is because columns usually contain more substantial attribute information, allowing VLLMs to locate columns more accurately. Rows tend to have higher similarities, so VLLMs faces more significant challenges locating rows or completing row-related tasks. The differences between rows and columns in visual table size detection further verify this view.\nImage Quality Analysis. We conduct in-depth experiments and analysis on the TR task. Detailed experimental results and analysis are presented in Section 5. The results demonstrate that significant bottlenecks remain despite existing VLLMs achieving competitive"}, {"title": "3.4 Bottleneck Analysis", "content": "In this section, we further investigate the performance of VLLMs under varying image quality conditions and assess their visual robustness to these conditions through empirical analysis.\n3.4.1 Experimental Setup. To comprehensively evaluate the visual robustness of VLLM, we focus on three distinct visual challenges: the image quality challenge, the table border quality challenge, and the geometric deformation challenge. Specifically, We randomly selected 1,000 samples from the SciTSR [59] test set for our experiments and configure our image types according to these challenges. Details of these challenges are provided in Table 2.\nVisual Conditions. Visual conditions is a key factor affecting the accuracy of VLLMs in table recognition. To assess it, we systematically analyze the performance of VLLMs under various visual conditions across three scenarios: blur, underexposure, and overexposure. These analyses demonstrate the robustness of VLLMs in handling impaired visual conditions.\nTable Border Quality. Table borders indicate structural information and are critical for parsing tasks. We evaluate the impact of border visibility and completeness on table recognition performance by considering scenarios including unclear table borders and missing borders. Additionally, we explore the effect of border changes in the table by thickened table borders.\nGeometric Deformation. Geometric deformation caused by viewing angles or operations can disrupt the geometric consistency of tables, which is common in visual table parsing, especially when"}, {"title": "4 Neighbor-Guided Toolchain Reasoner", "content": "Through our in-depth analysis of VLLMs' performance on benchmark tasks and bottleneck analysis, we have identified that the quality of the input image emerges as a critical factor that constrains the performance of these VLLMs. Consequently, improving input image quality is essential for enhancing VLLMs' capability to recognize and interpret structured image data more effectively. To address this bottleneck, we propose the NGTR framework.\n4.1 Model Overview\nNGTR enhances input image quality by applying various tool combinations tailored to scenarios such as low resolution, overexposure, and noise interference, among others. To select a suitable combination of tools, we employ a similarity-based neighbor retrieval module to obtain samples that resemble each input instance. These samples guide the generation of multiple tool invocation plans. Subsequently, the tool invocation experience learning module executes each toolchain plan and generates the corresponding structured data based on the neighbor samples. It evaluates the effectiveness of different plans to select a relatively better plan. In the final processing stage, we utilize a reflection-driven tool selection module. This module integrates iterative tool invocation and dynamic feedback to refine the processing flow. The optimized image resulting from this process is subsequently input into the VLLMs, which utilize"}, {"title": "4.2 Toolkit Preparation", "content": "Inspired by the conclusions of the Bottleneck Analysis (Section 3.4), we employ five distinct tools to address various scenarios and potential issues that may arise in the table recognition task. These tools are shown in Table 3. By combining different tools, NGTR effectively addresses various challenging situations. For example, when the table border is faint, the VLLMs can invoke the border enhancement tool to strengthen the structural information by thickening the table border. Similarly, when the table occupies only a portion of the image, the VLLMs can invoke the table detection and cropping tool to identify and crop the relevant table area, thereby reducing noise interference."}, {"title": "4.3 Similarity-based Neighbor Retrieval", "content": "Neighbor retrieval methods enable VLLMs to retrieve similar neighbor samples, providing richer contextual information. In the NGTR framework, we hypothesize that images with similar features exhibit similar results after being processed by the same image preprocessing toolchain. Consequently, the processing results of neighbor samples could guide the selection of a potentially optimal toolchain for test samples. We first retrieve images that are similar to the target task images from a sample set (a subset of the training data containing 100 samples). Subsequently, we employ a prompting template to leverage the VLLM's planning capabilities to generate tool invocation plans. The retrieval process can be formally described as follows:\n$Retrieval(I_{test}, D', f) = \\arg \\max_{I^{i} \\in D'} [f (I_{test}, I^{i})],$   (1)\nwhere $I_{test}$ represents an image from the test set, $D'$ denotes a subset of the training dataset, and $f$ is the similarity measurement function. In this paper, we combine the ORB (Oriented FAST and Rotated BRIEF) algorithm with the Hamming distance as $f$ to measure the similarity between images. Specifically, the feature vectors are first extracted using the ORB algorithm for the task image and a candidate example image. Subsequently, the Hamming distance between the feature vectors of the two images is calculated as a measure of similarity. Then, we guide the VLLMs to generate multiple tool invocation plans for the image. The generation process can be formally expressed as follows:\n$VLLM(T, N(I_{test})) \\rightarrow \\{p_1, p_2, ..., p_n\\},$   (2)\nwhere $T$ represents the description information set of all available image preprocessing tools, including their functions, applicable scenarios, invocation identifiers, and other relevant details; $N(I_{test})$ denotes the neighbor image samples of the test sample $I_{test}$, along with their associated features, retrieved from the training set; and $\\{p_1, p_2,..., p_n\\}$ represents the generated candidate set of plans, which are then used to select an appropriate tool invocation plan."}, {"title": "4.4 Tool Invocation Experience Learning", "content": "In this module, we follow a sequential workflow to evaluate the multiple tool invocation plans. First, we execute each tool invocation plan generated by the previous module to obtain multiple processed images:\n$I_{p_i} = f_{p_i} (I), i \\in \\{1, 2, ..., n\\},$  (3)\nwhere $f_{p_i}$ denotes the image preprocessing process corresponding to the tool invocation plan $p_i$. Next, we use a simple prompt template to instruct VLLMs to generate an markup sequence.\n$H_{p_i} = VLLM(I_{p_i}, P),$  (4)\nwhere P represents the prompt to instruct VLLMs generation. Subsequently, we evaluate the prediction results based on the example labels. The evaluation process employs the tree edit distance-based similarity (TEDS) metrics to quantify the accuracy of the VLLMs output. The specific details of TEDS can be found in the appendix E. By following this process, we calculate a quantitative score for each toolchain, enabling selection of a suitable plan."}, {"title": "4.5 Reflection-driven Tool Selection", "content": "Although the tool invocation experience learning module provides a high-quality plan, mindlessly applying the tool invocation plan to new samples may result in the loss of critical information in the image, thereby affecting the accuracy of the final result. To address this, we introduce the reflection-driven tool selection module during the execution phase to refine the processing flow, reduce information loss, and thereby improve recognition accuracy. The formalized expression of the reflection module is as follows:\nLet $I^{(t-1)}$ denote the image before the t-th operation and $I^{(t)}$ denote the image after the t-th operation. The VLLMs computes a decision value $y^{(t)}$ to determine whether to accept the operation:\n$y^{(t)} = reflect(I^{(t-1)}, f^{(t)}),$  (5)\nwhere $y^{(t)}$ is a binary decision indicating the quality change between the before and after images. The function $reflect()$ evaluates the difference in quality. If $y^{(t)} = 1$, the operation is considered successful; otherwise, if $y^{(t)} = 0$, the operation is rejected, and the process proceeds to the next step.\nThis step-by-step module enhances the interaction between the VLLMs and the target image, ensuring the accuracy of the final task outcome. More importantly, introducing this module enables downstream researchers and developers to flexibly customize and expand the toolkit without worrying about the impact of poorly performing expanded tools on the final results, thereby significantly improving the versatility and transferability of our framework. In the last step, we use a simple prompt template to instruct VLLM to generate a markup sequence and obtain the result of table recognition."}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\n5.1.1 Datasets. In this study, we utilize three widely-used table recognition datasets: SciTSR [59], PubTabNet [38], and WTW [60], each offering unique characteristics and challenges, as shown in Table 4. SciTSR is a dataset comprising tables extracted from the scientific literature, and the image quality in this dataset is relatively high. In contrast, the image resolution of PubTabNet is 72 pixels per inch, and its overall image quality is relatively low. WTW contains images collected from the wild, introducing a variety of extreme cases, such as tilt, blur, and table curvature, which significantly increase the task's difficulty. These datasets encompass diverse table types as well as various unique visual challenges, providing a robust foundation for benchmarking in this study.\n5.1.2 Baselines. We select six Vision Large Language Models (VLLMs), including Phi, Llama, GPT-mini, Qwen, GPT, and Gemini, as baseline models for comparison. These models' selection and implementation details are described in appendix D. Additionally, we select three representative deep learning-based methods as baselines for comparison: EDD [38] based on sequence modeling, LGPMA [34] based on cell bounding box detection, and LORE [35] based on cell point center detection.\n5.1.3 Evaluation Metrics. As for the evaluation metrics of TR, we use a similarity metric based on Tree-Edit Distance (TEDS) [38] and the structure-aware TEDS-Struct metric. The specific details of TEDS can be found in the appendix E. We employ two evaluation metrics for the hierarchical tasks described in Section 3.2.1: accuracy (ACC) and micro-averaged F1 score (F1-score). Specifically, ACC is used to evaluate cell-level tasks (excluding merged cell detection) and table-level tasks; the F1-score is utilized for row- and column-level tasks and merged cell detection.\n5.1.4 Implementation Details. For the PubTabNet dataset, since the test set labels are not public, we choose to use the validation set for evaluation. We randomly select 1,500 images from the validation set of PubTabNet as research samples. The experimental results show that the EDD and LGPMA baseline model's performance on this sample is consistent with its initial reported results, which shows that our sampling is well-representative. For the SciTSR and WTW datasets, we use their complete test sets for evaluation. Since the WTW dataset does not provide content information for table recognition, we do not report its TEDS scores.\nFor LORE, since it is mainly aimed at table structure recognition but not table content recognition, we only report its performance scores for table structure recognition. As for EDD, since its model training requires a large amount of end-to-end annotated data, and SciTSR and WTW lack corresponding labeled data, its performance on these datasets has not been evaluated. For LGPMA, since the model relies on table content for training and the WTW dataset does not provide table content labels, we did not evaluate its performance on the WTW dataset."}, {"title": "5.2 Main Results Analysis", "content": "Tables 5 show our benchmark results on the table recognition and table structure recognition tasks. Based on the experimental results, we draw the following insights:\n5.2.1 Performance Analysis of NGTR. As shown in Table 5, the experimental results compare our NGTR framework with baseline methods. We use GPT and Gemini, which performs well in benchmark tests, as the framework backbone. The main results show that our framework achieves significant performance improvements on the PubTabNet dataset, mainly attributed to our framework\u2019s enhanced VLLMs robustness when dealing with low-quality inputs. On the SciTSR dataset, our framework also outperforms all VLLMs baselines, further verifying our framework\u2019s effectiveness.\nIn the WTW dataset, our method and all VLLM-based baselines demonstrate suboptimal performance compared to traditional lightweight baseline methods. However, despite these challenges, our approach significantly enhances the effectiveness of VLLMs, suggesting that our framework imparts a degree of robustness to VLLMs in complex environments. Nevertheless, it highlights persistent challenges that remain difficult to address in VLLMs. Section 5.2.4 presents a more detailed analysis of VLLMs performance on the WTW dataset.\n5.2.2 Open-source and Proprietary VLLMs. The advanced open-source VLLMs demonstrate capabilities comparable to proprietary VLLMs in this task. As a representative of open-source VLLMs, Llama exhibits outstanding performance, particularly in the table structure evaluation task (TEDS-Struct) on the PubTabNet dataset. Llama achieves a relatively better result, surpassing GPT by 2.73 points and Gemini by 2.36 points. These results confirm the exceptional performance of Llama but also underscore the potential of open-source VLLMs for further research.\n5.2.3 Model Scale. We observe that lightweight VLLMs face significant performance limitations in specific tasks. For instance, Phi, a lightweight VLLM with a parameter size of 4.2B, demonstrates consistently low performance across all evaluations. While another lightweight VLLM, GPT-mini, achieves commendable results on the simpler SciTSR dataset, its performance declined significantly on the more challenging PubTabNet dataset, dropping 33% and 21% in TEDS and TEDS-Struct metrics, respectively.\nWe attribute this phenomenon to two key factors. First, the TR task requires VLLMs to possess fine-grained detection capabilities and the ability to accurately recognize and extract structured information\u2014a difficult challenge. Due to their relatively small parameter sizes and limited computational power, lightweight VLLMs struggle to achieve the required recognition accuracy and input robustness. Second, the TR task demands the generation of accurate markup sequence representations of tables, imposing stringent requirements on the VLLMs\u2019 generation capabilities. We observe that lightweight VLLMs often fail to produce parsable markup sequences after multiple attempts, mainly when tasked with complex scenarios such as large, intricate tables or ambiguous image inputs. This limitation significantly reduces their final performance scores, a phenomenon not observed in larger-scale VLLMs.\n5.2.4 WTW Dataset: A Challenge for VLLMs. Table 5 presents the experimental results on the WTW dataset. These results indicate that prompt tuning based VLLMs methods still exhibit significant gaps compared to traditional lightweight OCR methods, highlighting the challenges VLLMs face when processing datasets with wild scenarios. An in-depth analysis of the experimental findings reveals that the WTW dataset primarily emphasizes table structure information. Performance declines notably when processing tables with numerous empty cells, unevenly distributed text, skewed, rotated, or densely packed text. Further analysis of the outputs suggests that VLLMs tend to ignore cells lacking semantic content. While this behavior helps avoid processing irrelevant data, it also limits their ability to effectively capture the structural information of tables with many blank cells.\nIn conjunction with the analysis of table borders in Section 3.4, we speculate that VLLMs primarily rely on cell content and its logical position in the image when completing the table recognition task, with relatively limited use of table border information. This dependence restricts the performance of VLLMs in recognizing complex table structures to some extent."}, {"title": "5.3 Ablation Study w.r.t Key Components", "content": "The results of our ablation experiments on the NGTR framework are shown in Table 6. We randomly select 1,000 images from the SciTSR dataset for ablation studies. We ablate two key components: tool invocation experience learning and reflection-driven tool selection module. Note that we do not ablate the toolkit, as it is the foundational module of our framework and cannot be removed. The results highlight the critical contributions of these components to the overall performance of the NGTR framework.\nEffectiveness of Tool Invocation Experience Learning. To evaluate the effectiveness of VLLMs in optimizing toolchains using neighbor samples, we performed an ablation study by removing the tool invocation experience learning module. In this experiment, we only prompted VLLMs to generate a tool invocation plan and applied it directly to the test samples. Without the tool invocation experience learning module, the experimental results showed that while the VLLMs was able to generate a valid tool invocation plan, the lack of effective validation of the generated plan led to a significant performance decline. This further validates the critical role of the tool invocation experience learning module in improving the NGTR framework performance.\nEffectiveness of Reflection-driven Tool Selection. To evaluate the effectiveness of introducing the reflection-driven tool selection module for stepwise backtracking validation in the final execution stage, we ablate this module. We apply all the tools in the toolchain to the task images in a single pass, using them as input for the generation stage. Experimental results show that, without the reflection module for stepwise backtracking validation, VLLMs cannot effectively supervise the processing procedure, which may lead to the incorporation of unsuitable tools for the current sample, thereby affecting performance. This performance drop is mainly reflected in the reduction of the TEDS indicator, which may be due to the loss of text details caused by the incorrect noise reduction and binarization application."}, {"title": "5.4 Tool Invocation Analysis", "content": "To further investigate the NGTR framework's preferences in tool invocation, we calculated the tool usage rates for each test dataset (i.e., the proportion of samples that invoke a particular tool among all samples where tools are invoked). The results are presented in Table 7. As shown, there are notable differences in the frequency of tool usage rates across different datasets.\nOn the PubTabNet dataset, the NGTR framework exhibits a marked preference for the image upscaling tool. This preference is consistent with this dataset's overall lower image quality characteristic, as improving image resolution can better support subsequent table recognition tasks. For the SciTSR and WTW datasets, the border enhancement tool is invoked more frequently. These two datasets primarily feature tables with complete borders, and enhancing border features enables VLLM to recognize the table structure information more accurately. Moreover, the detection and cropping tool has the highest invocation rate on the WTW dataset, which aligns with the dataset's characteristics in the wild scenarios. In such images, tables often occupy only a portion of the frame or appear displaced, making this tool effective in locating and extracting the target areas. In summary, the experimental results demonstrate that the NGTR framework adaptively selects appropriate image processing tools based on the characteristics of different datasets, thereby fully showcasing its flexible planning capabilities in cross-dataset scenarios."}, {"title": "5.5 Hyperparameter Sensitivity Analysis", "content": "The NGTR framework contains two core parameters: the maximum length of the toolchain execution plan L and the number of plans generated each time N. Based on GPT, Experimental results on the SciTSR dataset are presented in Figures 6.\nOur experimental results indicate that a moderate toolchain length achieves an adequate balance between complexity and performance, as excessive toolchain length increases combinatorial complexity and limits processing performance, thereby affecting the framework's ability to generate high-quality solutions. Similarly, generating a moderate number of execution plans effectively balances solution quality and generation efficiency, whereas generating too few or too many plans slightly reduces performance. Therefore, a moderate toolchain length and number of execution plans can balance complexity and performance well, providing valuable guidance for the tool invocation."}, {"title": "6 Conclusion and Limitation", "content": "This paper addressed the under-explored challenge of table recognition using VLLMs in a training-free reasoning paradigm. We proposed the NGTR framework, which enhanced input image quality through lightweight models and neighbor-guided tool invocation strategies. Extensive experiments demonstrated that NGTR significantly improved VLLM-based table recognition performance, addressing critical bottlenecks such as low-quality input images. This work not only established a benchmark for table recognition but also highlighted the potential of VLLMs in advancing table understanding, paving the way for future research and applications.\nDespite the strengths of our framework, we acknowledge several limitations that warrant further investigation. Firstly, its performance depends on the underlying toolkit. Secondly, when the available set of neighbor candidates does not sufficiently cover a wide range of scenarios, selecting inappropriate neighbors may lead to suboptimal performance. Nevertheless, we believe the NGTR framework demonstrates strong generalizability, serving as a versatile approach for tool invocation for various domains."}, {"title": "A Logical to Markup Sequence Conversion", "content": "Since the baseline methods (including LORE and LGPMA) output logical structures representing cell information, they cannot be directly used for the calculation of TEDS metrics. To address this, we use the following pseudocode to convert the logical structure into markup sequence format.\nWe divide the entire conversion process into two stages. In the first stage, we perform a preliminary preprocessing of the logical location information, associating the logical positions with the corresponding cell content and storing them in a tabular data matrix. The detailed implementation is shown in Algorithm 1."}, {"title": "B Study of Table Size", "content": "In this section, we analyze the scale of the tables involved in the benchmark evaluations. We quantify the size of tables in the SciTSR and PubTabNet datasets by counting the number of cells in each table. Furthermore, this section presents experiments designed to evaluate the impact of table size on table recognition performance. GPT is selected as a representative of VLLMs for these experiments, with the distribution details and experimental results illustrated in Figure 7. In small scale tables, the model demonstrates stable performance on both the SciTSR and PubTabNet datasets. However, as the table size increased, the model's performance experienced a moderate decline. This indicates that larger tables introduce longer context lengths, thereby affecting the VLLMs' TR task performance."}, {"title": "C Case Study", "content": "Figure 8 presents a case study that showcases evaluation samples from the PubTabNet dataset and analyzes the table recognition results using the traditional OCR models LORE and VLLM. The input table image lacks clear row borders, making it challenging for traditional OCR models to locate specific cells accurately. However, VLLM effectively comprehends the hierarchical structure of the table, thereby producing correct recognition results. This case highlights the advantages of VLLM in semantically rich table recognition tasks, demonstrating its superior adaptability and robustness compared to traditional OCR models.\nHowever, relying on the understanding of the table's hierarchical structure to perform table recognition tasks can also have negative consequences in certain scenarios. As illustrated in Figure 9, we analyzed the results of using VLLM for table recognition on evaluation samples from the SCITSR dataset. The results indicate that the output of VLLM in table recognition significantly deviates from the correct answers, particularly in the placement of the \"Linear\" and \"Kernel\" cells in the table. Specifically, VLLM tends to misidentify \"Linear\" and \"Kernel\" as belonging to the first row of the table, treating them as subheaders, even though these cells are located in the table's upper-left corner. This phenomenon may stem from"}, {"title": "D Parameter Settings of VLLMs", "content": "We report the parameter settings of six VLLMs. For open-source VLLMs, we select Phi-3.5-Vision-Instruct\u00b2 (Phi) and Llama-3.2-90B-Vision-Instruct\u00b3 (Llama) for evaluation. For closed-source VLLMs, we evaluate GPT-4o-mini\u2074 (GPT-mini), Qwen-VL-Max\u2075 (Qwen), GPT-4o\u2076 (GPT) and Gemini-1.5-Pro 7 (Gemini). Specifically, when generating multiple tool invocation plans, we set the temperature parameter to 0.8 to encourage the generation of more diverse tool invocation plans. For other experiments, the temperature parameter was kept at 0 to ensure the stability of the experimental results. Additionally, the top_p parameter was set to 0.2, and the n_samples parameter was set to 1."}, {"title": "E Details of TEDS Metric", "content": "TEDS is designed to measure the similarity between two tree structures. Specifically, TEDS is used to calculate the similarity between the HTML tree of the real tag and the HTML tree of the predicted tag. When applying the TEDS metric, the HTML table format must first be converted into a tree structure. The similarity is then calculated using the following formula:\n$TEDS(T_a, T_b) = 1 - \\frac{EditDist(T_a, T_b)}{max(|T_a|, |T_b|)}$   (6)\nwhere $T_a$ and $T_b$ represent the tree structures of the tables in HTML format. EditDist($T_a, T_b$) denotes the tree-edit distance, and |T| refers to the number of nodes in tree T. Additionally, we adopted a modified version of TEDS, referred to as TEDS-Struct. TEDS-Struct is designed to assess the accuracy of table structure recognition without considering the specific results generated by the table content."}, {"title": "F Implementation Details of Benchmark", "content": "We present two prompt templates designed to guide VLLM in generating markup sequence outputs for TR tasks in Figures 10 and 11. Prompt 1 is a simplified template that facilitates the execution of TR tasks by VLLM. In contrast, Prompt 2 introduces the concept of Chain-of-Thought by explicitly planning the table recognition process for VLLM. In our experiments, the more complex Prompt 2 achieved a slight improvement in TR tasks performance. However, since our benchmark evaluation aims to reflect VLLM's recognition capabilities more directly, we employ Prompt 1 as the prompt template in all experiments."}, {"title": ""}]}