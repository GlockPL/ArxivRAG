{"title": "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification", "authors": ["Ming Li", "Jike Zhong", "Chenxin Li", "Liuzhuozheng Li", "Nie Lin", "Masashi Sugiyama"], "abstract": "Recent advances in fine-tuning Vision-Language Models (VLMs) have witnessed the success of prompt tuning and adapter tuning, while the classic model fine-tuning on inherent parameters seems to be overlooked. It is believed that fine-tuning the parameters of VLMs with few-shot samples corrupts the pre-trained knowledge since fine-tuning the CLIP model even degrades performance. In this paper, we revisit this viewpoint, and propose a new perspective: fine-tuning the specific parameters instead of all will uncover the power of classic model fine-tuning on VLMs. Through our meticulous study, we propose ClipFit, a simple yet effective method to fine-tune CLIP without introducing any overhead of extra parameters. We demonstrate that by only fine-tuning the specific bias terms and normalization layers, ClipFit can improve the performance of zero-shot CLIP by 7.27% average harmonic mean accuracy. Lastly, to understand how fine-tuning in CLIPFit affects the pre-trained models, we conducted extensive experimental analyses w.r.t. changes in internal parameters and representations. We found that low-level text bias layers and the first layer normalization layer change much more than other layers. The code is available at https://github.com/minglllli/CLIPFit.", "sections": [{"title": "Introduction", "content": "Large pre-trained Visual-Language Models (VLMs) have been developed a lot in recent years. For example, CLIP (Radford et al., 2021) and ALIG (Jia et al., 2021) demonstrated remarkable performance for various tasks, e.g., image recognition in a zero-shot fashion. To further improve the performance on the specific downstream tasks, prompt tuning (Lester et al., 2021; Yao et al., 2023; Zhu et al., 2023; Zhou et al., 2022a) and adapter tuning (Gao et al., 2023; Zhang et al., 2021) methods have been proposed. As shown in Fig. 1, prompt tuning methods proposed to introduce a set of learnable prompt vectors as the input of the text encoder while adapter tuning approaches adopted an additional bottleneck layer to learn new features. During the fine-tuning procedure, both of these two strategies keep CLIP's parameters fixed. The performance of prompt tuning and adapter tuning methods are superior on various tasks (Zhou et al., 2022b; Gao et al., 2023), so research on fine-tuning the inherent parameters of VLMs has been barely touched.\nFor language models, fully fine-tuning with downstream data can achieve promising results (Zaken et al., 2021; Liu et al., 2022). Moreover, recent works in language model fine-tuning (e.g., BitFit (Zaken et al., 2021)) have demonstrated that, without introducing any external parameters, fine-tuning only the bias terms in a pre-trained model can perform competitively on downstream tasks compared with fine-tuning the entire model. For VLMs, however, it is believed that fine-tuning the parameters of VLMs corrupts the inherent pre-trained knowledge as fully fine-tuning degrades performance (Zhou et al., 2022b). In this paper, we revisit this viewpoint and ask if, without introducing any external parameters, fine-tuning the inherent parameters of VLMs can achieve competitive performance compared with prompt tuning.\nWe start with directly applying BitFit to fine-tuning the CLIP model. We explore two strategies: (i) applying BitFit to the text encoder alone, and (ii) applying BitFit to both the text and image encoder. We found that both two strategies can acquire task-specific knowledge but their performance to unseen class data can be poor (more discussed in Sec. 4.4), implying that directly fine-tuning the bias terms of a text or image encoder may harm the model's generalization ability. These findings motivate us to develop more effective and efficient fine-tuning techniques for VLMs.\nIn light of this, we propose CLIPFit, a simple yet effective method for efficiently fine-tuning VLMs. CLIPFit is orthogonal to previous prompt tuning and adapter tuning methods, as shown in Fig. 1 (c). For the text encoder, instead of fine-tuning all the bias terms, CLIPFit proposes to tune only the bias terms of projection linear layers in feed-forward networks (FFNs). Fine-tuning only the bias terms of projection linear layers in FFNs will reduce the number of training parameters compared with fine-tuning all the bias terms. Moreover, empirically, we discovered that our bias term tuning strategy can generalize better than BitFit (Zaken et al., 2021), as shown in Sec. 4.4. For the image encoder, as mentioned before, it may harm the model's performance if directly applying BitFit. In the image encoder, layer normalization (LayerNorm) (Ba et al., 2016) aims to normalize the distributions of intermediate layers. Since the distributions of pre-training and downstream data might be divergent, pre-trained LayerNorm might lead to sub-optimal performance for downstream data inference. Therefore, CLIPFit proposes to further update only the parameters of the image encoder's LayerNorm. Updating LayerNorm can yield a better image encoder for downstream data. Lastly, previous studies (Yao et al., 2023) have shown that generic pre-trained knowledge is easily forgotten in the fine-tuning stage. Therefore, we explored two different regularization strategies for alleviating forgetting: (i) using the knowledge distillation (KD) loss (Hinton et al., 2015) to guide CLIPFit to learn from the zero-shot CLIP; (ii) using the mean squared error (MSE) loss in bias terms to penalize changes in text encoder. We empirically found that both two strategies can alleviate forgetting problems and the KD loss performs better, thus we used the KD loss as the final solution for CLIPFit.\nFine-tuning is an empirical and black-box process. So, understanding how fine-tuning affects the pre-trained models is important for uncovering the black-box fine-tuning process. Previous works (Zhou and Srikumar, 2022; De Vries et al., 2020; Merchant et al., 2020) explored this for language models fine-tuning. However, very little work explored the internal black-box fine-tuning process for VLMs. In this paper, we conducted an initial exploration to analyze VLM fine-tuning process of CLIPFit, focusing on changes in internal parameters and representations. We found that for bias terms in the FNN of the text encoder, as the number of layers increases, the change in bias decreases, which means that during the fine-tuning process, low-level features in the text encoder change more than high-level features. For LayerNorm in the image encoder, we found that the first layer (patch embedding) changes much more than other layers. Experimentally, we showed that more changed layers play a more important role in adapting downstream knowledge than less changed layers. Moreover, we explored how KD loss affects the fine-tuning process for alleviating forgetting. We found that KD loss will reduce the changes for the more-changed low-level bias terms and enhance changes in less-changed high-level layers, which implies that penalizing changes for low-level bias terms is important for avoiding overfitting. Lastly, we found that tuning LayerNorm will form a better image feature space compared with zero-shot CLIP.\nWe conducted extensive experiments on 11 datasets in 4 different settings to show the effectiveness of the proposed CLIPFit. Overall, our main contributions can be summarized as follows:\n\u2022 We propose a CLIPFit method for efficiently fine-tuning the CLIP model to uncover the"}, {"title": "Methodolgy", "content": "In this section, we introduce CLIPFit. We first briefly review CLIP and then illustrate CLIPFit."}, {"title": "Review of CLIP", "content": "We first briefly review CLIP (Radford et al., 2021). During pre-training, CLIP aims to align image features and text features in the joint embedding space to capture the relationship between images and texts. Let $D = \\{(x_i, t_i)\\}_{i=1}^{b}$ be the sampled batch, where $x_i$ is the input image, $t_i$ is the input text and $b$ is the batch size. A CLIP model is comprised of two types of encoders: visual encoder $E_I(\\cdot, \\Theta_I)$ and text encoder $E_T(\\cdot, \\Theta_T)$. The visual encoder encodes image $x$ into $f_I$ and text $t$ into $g$, i.e.,\n$f_i = E_I(x_i, \\Theta_I)$, $g_i = E_T(t_i, \\Theta_T)$. (1)\nThen, a contrastive learning loss is applied to them for alignment.\nAfter pre-training, CLIP can perform zero-shot image recognition by comparing the image features with class weights $\\{w_i\\}_{i=1}^{K}$, where $K$ is the number of classes. The class weight $w_i$ is generated by text encoder $E_T(\\cdot, \\Theta_T)$ which takes the class descriptions (prompts) as input. These prompts usually take the form \"a photo of a [CLASS].\", where the class token will be replaced by the $i$-th class name (e.g., cat) for weight $w_i$. Formally, for an image feature $f$, the probability that it belongs to class $i$ is calculated by\np(y = i | x) = $\\frac{\\exp (\\cos (w_i, f) /\\tau)}{\\sum_{j=1}^{K} \\exp (\\cos (w_j, f) /\\tau)}$, (2)\nwhere $\\tau$ is a temperature parameter learned by CLIP during pre-training and $\\cos(\\cdot,\\cdot)$ denotes the cosine similarity function."}, {"title": "CLIPFit", "content": "The overall pipeline of the proposed CLIPFit is shown in Fig. 2. Without introducing any external parameters, CLIPFit involves fine-tuning only the bias terms of projection linear layers in FNNs of the text encoder and updating LayerNorm (Ba et al., 2016) in the image encoder.\nText Encoder. For the text encoder, instead of fine-tuning all bias terms, CLIPFit fine-tunes only the bias terms of projection linear layers (i.e., second layers) in the FFNs of the text encoder. Fine-tuning only part of bias terms will reduce the number of training parameters compared with fine-tuning all bias terms. Moreover, Sec. 4.4 will empirically show that our bias tuning method can achieve better performance compared with fine-tuning all bias terms (Zaken et al., 2021).\nImage Encoder. As mentioned in Sec. 1, directly applying BitFit (Zaken et al., 2021) to the image encoder may cause a negative impact on the model's performance. Instead of fine-tuning the bias terms of the image encoder, CLIPFit proposes to fine-tune LayerNorm. In LayerNorm, the two learniable parameters gain $g$ and bias $b$ are applied for affine transformation on normalized input vectors $x$ for re-centering and re-scaling, which are expected to enhance the expressive power by re-shaping the distribution (Ba et al., 2016). Different data distributions should produce different gains and biases in LayerNorm for distribution re-shaping during the training process. So, if shifted gains and biases in LayerNorm are applied during inference, it may lead to a sub-optimal solution. Therefore, CLIPFit proposes to fine-tune LayerNorm in the image encoder.\nLoss function. Previous works (Yao et al., 2023; Xuhong et al., 2018) have verified that during the fine-tuning stage, generic pre-trained knowledge is easily forgotten. Therefore, we explore two different strategies for alleviating such forgetting. The first one is to use the knowledge distillation (Hinton et al., 2015; Yao et al., 2023) loss to guide CLIPFit to learn from the original zero-shot CLIP. Let $\\{w^{clip}_i\\}_{i=1}^{K}$ and $\\{w_i\\}_{i=1}^{K}$ be the text features from original CLIP and text features from CLIPFit. The training loss and KD loss of CLIPFit are defined by\n$L = L_{ce} + \\beta L_{kg}$, (3)\n$L_{kg} = \\frac{1}{K} \\sum_{i=1}^{K} \\cos(w^{clip}_i, w_i)$, (4)\nwhere $L_{ce}$ is the cross entropy loss for classification (Zhou et al., 2022b,a) and $\\beta$ is a hyperparameter. The second strategy is using the MSE loss in bias terms to penalize changes in the text encoder. Let $\\{b^{clip}_i\\}_{i=1}^{L}$ and $\\{b_i\\}_{i=1}^{L}$ be the unfixed text bias terms from pre-trained CLIP and unfixed text bias terms from CLIPFit, where $L$ is the number of unfixed bias layers. The MSE loss is defined as\n$L_{mse} = \\frac{1}{L} \\sum_{i=1}^{L} || b^{clip}_i - b_i ||^2$. (5)\nWe found that both strategies can alleviate the forgetting problems and the KD loss performs better (as discussed in Sec. 4.3), thus we adopted the KD loss as the final solution for CLIPFit."}, {"title": "Experiments", "content": "In this section, we show and discuss the experimental results. To evaluate the effectiveness of our proposed method, we conducted extensive experiments and analyses on 11 datasets."}, {"title": "Experimental Setup", "content": "Datasets. Following CoOp, we conducted extensive experiments on 11 public classification benchmark datasets to evaluate CLIPFit. The datasets are ImageNet (Deng et al., 2009), Caltech101 (Fei-Fei et al., 2004), OxfordPets (Parkhi et al., 2012), StanfordCars (Krause et al., 2013), Flowers102 (Nilsback and Zisserman, 2008), Food101 (Bossard et al., 2014), FGVCAircraft (Maji et al., 2013), SUN397 (Xiao et al., 2010), DTD (Cimpoi et al., 2014), EuroSAT (Helber et al., 2019), and UCF101 (Soomro et al., 2012). Implementation details. We implemented our method with PyTorch (Paszke et al., 2019). All experiments were based on the vision backbone with Vit-B/16 (Dosovitskiy et al., 2020). We followed CoOp to preprocess input images. We used SGD optimizer with batch size set as 32, and set the learning rate as 0.002 (Zhou et al., 2022b). All results reported below are the average of three runs with different random seeds. The training epoch was set to 100 for all datasets except ImageNet and Food101. $\\beta$ was set to 8 for all datasets on the base-to-new and cross-dataset setting, and 2 for the distribution shift setting. For the few-shot setting, we set $\\beta$ to 2 for all datasets except SUN397 and DTD. More implementation details are provided in appendix B.\nComparisons. We compared our method against state-of-the-art methods: zero-shot CLIP, prompt tuning methods: CoOp, CoCoOp (Zhou et al., 2022a), ProGrad (Zhu et al., 2023), KgCoOp (Yao et al., 2023), MaPLe (Khattak et al., 2023) and adapter tuning methods: CLIP-adapter, Tip-adapter (Zhang et al., 2021). Detailed introductions to these methods can be found in appendix C."}, {"title": "Comparisons with State-of-the-arts", "content": "Results on base-to-new generalization setting Following Zhou et al. (2022b), we split each dataset into two disjoint groups: the base class dataset and the new class dataset. All compared methods and the proposed CLIPFit were trained on the base class dataset and evaluated on the new class dataset. We conducted 4/8/16-shot experiments, following Yao et al. (2023). We reported base and new class accuracies (Base and New) and their harmonic mean accuracy (HM). The 16-shot results are shown in Table 1, and 4/8-shot results are provided in appendix G. As shown in Table 1, CLIPFit achieves 6 best HM accuracies among 11 datasets and the best average HM accuracy, which demonstrates that CLIPFit can not only learn well on seen base class data but also can generalize well to data from unseen new classes. A notable issue with previous methods like CoOp, CoCoOp, ProGrad, and KgCoOp is that they usually perform well only on either the base or new class. To alleviate this issue, MaPLe proposes a multi-modal prompt learning strategy for CLIP tuning which improves a lot over HM compared to previous methods. Compared to MaPLe, our CLIPFit achieves better HM accuracy and average performance on the base class, with slightly lower average performance on the new class. It is important to note that CLIPFit only needs to tune nearly 46K parameters while MaPLe needs to tune nearly 3.55M parameters for each task, which is 77 times more than CLIPFit, meaning that CLIPFit fine-tunes significantly fewer parameters and is much more efficient.\nResults on few-shot learning setting. To verify whether the proposed CLIPFit can learn task-specific knowledge, we also compared CLIPFit with other existing methods on the few-shot learning setting. Following Zhou et al. (2022a), we used 1, 2, 4, 8, and 16-shot sets for training and reported accuracy performance. We report the results of the average accuracy of 11 datasets in Table. 2, and report all results on each dataset in appendix F. As shown in Table 2, compared with other methods, CLIPFit shows overall consistent improvements among all 1/2/4/8/16-shot settings. This demonstrates that CLIPFit can successfully learn task-specific knowledge. It is worth noting that CLIPFit outperforms other methods by a large margin in 1/2/4-shot settings, demonstrating CLIPFit's robust ability to learn with extremely few samples."}, {"title": "Results on the robustness to distribution shift setting", "content": "Following Zhang et al. (2021), we evaluated the robustness under distribution shift of CLIPFit and other methods by first training models on the 16-shot ImageNet dataset and then evaluating on ImageNet-V2 (Recht et al., 2019) and ImageNet-Sketch (Wang et al., 2019). The label sets of two evaluating datasets are subsets of the label set of ImageNet. Although the label sets are compatible, the distributions of these three datasets are different from each other. The results are shown in Table 3. As shown in Table 3, while TIP-adapter achieves the best performance on the ImageNet dataset, CLIPFit can achieve better average performance compared to existing methods, effectively underlining the robustness of CLIPFit.\nResults on cross-dataset transfer setting is provided in appendix D."}, {"title": "Fine-tuning Analysis", "content": "Analyzing parameter change. To understand the black-box fine-tuning process in CLIPFit, we first analyzed changes in the parameters of both the text encoder and image encoder. We computed the squared difference $||p^{pre} - p||^2$ for each layer, where $p^{pre}$ is the pre-trained parameter vector and $p$ is the fine-tuned parameter vector. We conduct experiments on the DTD dataset. The results are shown in Fig. 3. As observed Fig. 3 (a), for bias terms in the FNN of the text encoder, when the number of layers increases, the change in bias decreases, which implies that low-level features in the text encoder change more than high-level features during the fine-tuning process of CLIPFit. From Fig. 3 (b), we found that for LayerNorm in the image encoder, the first layer (i.e., patch embedding layer) changes much more compared with other layers for both bias and gain, showing that tuning patch embedding LayerNorm is crucial for shifted downstream tasks. Moreover, the gain of the last several LayerNorm layers has much changed and the most intermediate layers change much less. The difference in change between different layers may be caused by gradient difference. We visualize the squared sum of gradient from each text bias layer in Fig. 4 (a). As observed, the curve of the gradient sum is very similar to changes in parameters.\nTo verify whether more changed layers are more important in fine-tuning, we conducted experiments by freezing less (or more) changed LayerNorm layers on the 4-shot setting. We found that when only updating the first LayerNorm layer and freezing other LayerNorm layers, the average accuracy is 76.22%. For comparison, the average accuracy is 74.93% when only updating the twelfth LayerNorm, and 75.06% when updating the last LayerNorm. Both are much less than the first layer and these two layers change much less than the first layer, as shown in Fig. 3 (b). Moreover, when updating the top 6 most changed LayerNorm layers, the average accuracy is 77.03%, which is only a 0.15% drop, while only tuning 23% parameters of CLIPFit. The phenomenon for text encoder is similar and can be found in appendix H. These results demonstrate that the more changed layers are more important for knowledge adapting.\nAnalyzing regularization loss. We then analyze the two regularization losses: KD loss and bias term MSE loss. We found that both two losses can avoid overfitting and boost performance during fine-tuning. For the 4-shot learning task, fine-tuning w/ KD loss leads to a 77.18% average accuracy, and fine-tuning w/ KD loss leads to a 76.23% average accuracy. Both two performances are better than fine-tuning w/o regularization loss (76.13% average accuracy) and KD loss performs better. We then analyze how these two losses affect changes in parameters during fine-tuning of CLIPFit. The results are shown in Fig. 4 (b). As observed, KD loss will reduce the changes for the more-changed low-level bias terms and enhance changes in less-changed high-level layers, which implies that penalizing changes for low-level bias terms is important in avoiding overfitting. Compared with KD loss, MSE loss directly applying to text bias terms reduces more changes in low-level layers.\nAnalysing image encoder representations. We used t-SNE (Van der Maaten and Hinton, 2008) to visualize the image representation space of zero-shot CLIP and CLIPFit to analyze image encoder representations. We visualize the data from EuroSAT dataset. The visualization results are presented in Fig. 5. As observed, in high-dimensional classification feature space, CLIPFit has a much clearer separation of different class image features compared with zero-shot CLIP, which demonstrates that CLIPFit can better detect the similarities among images. These results verify that updating LayerNorm in the image encoder during fine-tuning will lead to a more separated and better similarity-detected image feature space.\nUpdating LayerNorm can also benefit other methods. We show that updating LayerNorm can also benefit prompt tuning methods and adapter tuning methods. We re-implemented CoOp, KgCoOp,"}, {"title": "Ablation Study", "content": "Comparison of different strategies of fine-tuning bias terms. We give an in-depth exploration of how to apply BitFit to fine-tune the CLIP model. Original BitFit fine-tunes all bias terms in language models. We conduct 4 strategies for fine-tuning bias terms of CLIP: (a) fine-tuning all bias terms of the text and image encoder; (b) fine-tuning all bias terms of the text encoder; (c) fine-tuning bias terms of FFNs of the text encoder; (d) fine-tuning bias terms of projection linear layers in FFNs of the text encoder. We trained these four strategies on the 16-shot base-to-new setting with Lce and reported average accuracy. The results are shown in Table 5. As shown in Table 5, both strategy (a) and strategy (b) can boost seen base class performance but will decrease significantly unseen new class performance, which implies that directly applying BitFit to CLIP may be harmful to model's generalization ability. Moreover, strategy (c) and strategy (d) can have similar performance among both the base and new class data, but strategy (d) fine-tunes only one-fifth of parameters compared with strategy (c), which speeds up training.\nEffectiveness of proposed components. We validated the effects of updating LayerNorm and KD loss by ablating them. The results are shown in Table 6. Fine-tuning bias terms with KD loss brings 2.35%, 1.22%, and 0.09% improvements for 1/4/16-shot setting, respectively. Fine-tuning bias terms in the text encoder and LayerNorm in the image encoder brings 1.89%, 1.41%, and 1.51% improvements for 1/4/16-shot setting, respectively. Together, CLIPFit brings 3.46%, 2.46% and 1.74% improvements for 1/4/16-shot setting, respectively. These results demonstrate the effectiveness of each CLIPFit components.\nTraining efficiency. We compare the training efficiency of CLIPFit and other methods w.r.t. parameters and training time per image (Yao et al., 2023). The results are shown in Table 7. It is noticed that CoOp and KgCoOp have the lowest number of training parameters and time. However, the performance of these two methods is not satisfactory. MaPLe improves accuracy performance compared with other methods but also increases the required tuning parameters to 3.55M, which is very time-consuming. CLIPFit achieves the best harmonic mean accuracy with only 44k parameters, which is much less than MaPLe. Also, the training time of CLIPFit is slightly higher than CoOp and KgCoOp. Given the large improvement of CLIPFit, a slight increase in training time is acceptable."}, {"title": "Discussion", "content": "Although our method is designed for contrastive encoder VLMS (CLIP), the core idea of CLIPFit and our model analysis may still provide insights for other large multimodal model (e.g., LLaVA (Liu et al., 2024)) fine-tuning. For example, the idea of tuning LayerNorm could be used when distributions of downstream and pretraining QA image data are divergent, and parameter change and importance analysis (Sec. 4.3) could provide insights for how to select fine-tuning parameters. We hope our work can provide insights for a broader range of VLM fine-tuning."}, {"title": "Conclusion", "content": "In this paper, we presented CLIPFit for fine-tuning visual-language models. Unlike existing prompt tuning and adapter tuning methods, CLIPFit does not introduce any external parameters and fine-tunes CLIP by updating only bias terms of projection layers in FFNs of the text encoder and the image encoder's LayerNorm. To understand the effect of CLIPFit fine-tuning on the pre-trained model, we conducted various analyses focusing on changes in internal parameters and representations. We conducted extensive experiments and analysis to evaluate CLIPFit on 11 datasets, whose performances show the superiority of our method."}, {"title": "Limitations", "content": "In this paper, we presented CLIPFit for VLM fine-tuning and conducted an exploration of how CLIPFit affects the pre-trained CLIP model. Our analyses found some interesting phenomena after fine-tuning, i.e., low-level bias terms in the text encoder change much more than high-level bias terms and the change in the first LayerNorm layer is much bigger than other LayerNorm layers in the image encoders. Moreover, we found that this may be caused by the difference in the magnitude of the gradient. Nevertheless, our analysis does not reveal why the difference in the magnitude of the gradient happens during fine-tuning. A deeper analysis of gradient back-propagation during fine-tuning is needed to understand this for future work.\nFurthermore, following previous works (Zhou et al., 2022a; Yao et al., 2023; Zhou et al., 2022b; Khattak et al., 2023), this paper focused on image classification for VLMs, so our study was constrained to classification tasks. Expanding CLIPFit for VLM fine-tuning to a broader range of tasks (e.g., image retrieval) could be the future work."}, {"title": "More Fine-tuning Analysis", "content": "Sec. 4.3 discussed the changes in unfixed parameters after fine-tuning the DTD dataset and the importance of more changed LayerNorm. In this subsection, we give more detailed analyses of other datasets and other aspects.\nImportance of low-level bias terms in text encoder. Sec. 4.3 presented that after the fine-tuning of CLIPFit, for bias terms in the FNN of the text encoder, as the number of layers increases, the change in bias decreases. In this subsection, we conducted experiments to verify whether more changed layers in the text encoder are more important. Similar to Sec. 4.3, we freeze less (or more) changed LayerNorm bias layers in the text encoder on the 4-shot setting. When updating only the first bias layer and freezing other layers, the average accuracy is 74.69%. For comparison, the average accuracy is 73.33% when only updating the sixth bias layer and 70.62% when only updating the last bias layer. Both are much lower than updating the first layer. We also found that when only updating the top-3 bias layers (changed more) and freezing other bias term layers, the average accuracy is 76.13%. For comparison, when only updating the last 3 bias layers (changed less) and freezing other bias term layers, the average accuracy is 70.86%, which is much lower than updating the top 3 bias layers. These results demonstrate that the more changed parameters are crucial for fine-tuning.\nAnalyzing LayerNorm with regularization loss. Sec. 4.3 analyzed the difference of changes in the text encoder bias terms between w/ and w.o regularization loss. In this subsection, we will analyze LayerNorm in the image encoder bias terms between w/ and w.o regularization loss. Noted that although the two regularization losses are applied to text features or text encoder, the image encoder or image features will also be affected since these two encoders are fine-tuned simultaneously. The results on the DTD dataset are shown in Fig. 8. When fine-tuning w/ KD loss, unlike in text encoder, changes in gain and bias increase compared with w/o KD loss. This phenomenon implies that image features will change more w/ KD loss compared with fine-tuning w/o KD loss. Moreover, we also found that the increases are almost in the more changed LayerNorm layers. When fine-tuning w/ MSE loss, changes in gain and bias are equal or slightly higher than fine-tuning w/o KD loss.\nLayerNorm gradient. We visualize the squared sum of gradient from each LayerNorm layer in the image encoder in Fig. 11 (a). As observed, the magnitude of gradient in the first LayerNorm layer is much bigger than other layers. So the difference in change may be caused by the difference in gradient.\nChange in each iteration. We visualize the change in first-layer text bias terms, first-layer LayerNorm gain, and first-layer LayerNorm bias for each iteration in Fig. 11 (b). As observed, the change will increase smoothly and converge to some values.\nAnalyses on other datasets. We also conducted analyses on other datasets. The results for the EuroSAT dataset are shown in Fig. 7, Fig. 9, and Fig. 10. The phenomena in the EuroSAT dataset are very similar to the DTD dataset."}]}