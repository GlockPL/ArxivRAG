{"title": "Human-compatible driving partners through data-regularized self-play reinforcement learning", "authors": ["Daphne Cornelisse", "Eugene Vinitsky"], "abstract": "A central challenge for autonomous vehicles is coordinating with humans. Therefore, incorporating realistic human agents is essential for scalable training and evaluation of autonomous driving systems in simulation. Simulation agents are typically developed by imitating large-scale, high-quality datasets of human driving. However, pure imitation learning agents empirically have high collision rates when executed in a multi-agent closed-loop setting. To build agents that are realistic and effective in closed-loop settings, we propose Human-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are trained through self-play with a small penalty for deviating from a human reference policy. In contrast to prior work, our approach is RL-first and only uses 30 minutes of imperfect human demonstrations. We evaluate agents in a large set of multi-agent traffic scenes. Results show our HR-PPO agents are highly effective in achieving goals, with a success rate of 93%, an off-road rate of 3.5%, and a collision rate of 3%. At the same time, the agents drive in a human-like manner, as measured by their similarity to existing human driving logs. We also find that HR-PPO agents show considerable improvements on proxy measures for coordination with human driving, particularly in highly interactive scenarios. We open-source our code and trained agents. Demonstrations of agent behaviors are shared at https://sites.google.com/view/driving-partners.", "sections": [{"title": "1 Introduction", "content": "Developing autonomous vehicles (AVs) that are compatible with human driving remains a challenging task, especially given the low margin for error in the real world. Driving simulators offer a cost- effective and safe means to develop and refine autonomous driving systems. The purpose of these simulators is to prepare AVs for real-world deployment, where they must smoothly interact and coordinate with a diverse set of human drivers. Therefore, a crucial aspect of both learning and validation in these simulators involves realistic simulations: the traffic scenarios and other simulation agents with which the controlled AV interacts. To identify where driving policies fall short, it is important to ensure that the simulated traffic conditions and driver agents closely resemble those in the real world (Gulino et al., 2023; Muhammad et al., 2020).\nExisting driving simulators typically provide a set of baseline agents to interact with, such as low-dimensional car following models, rule-based agents, or recorded human driving logs (Treiber et al., 2000; Gulino et al., 2023; Dosovitskiy et al., 2017). While these agents provide a form of interactivity, they are limited in their abilities to create interesting and challenging coordination scenarios, which requires driving agents that are reactive and sufficiently human-like. Having effective simulation agents that drive and respond in human-like ways would facilitate the controlled generation of human-AV interactions, which has the potential to unlock realistic training and evaluation in simulation at scale. Additionally, it would reduce the need for continuous real-world large-scale data collection."}, {"title": "Building", "content": "Building human-like driving policies is an ongoing challenge. Existing simulated agents are either (1) quite far from human-like behavior (2) struggle with achieving closed-loop stability or (3) frequently get stuck in deadlocks. A ubiquitous way to generate driving policies has been through imitation learning, where a driving policy is learned by mimicking expert behavior using recorded actions from human drivers (Pomerleau, 1988; Xu et al., 2023). Unfortunately, such policies still have high crash rates when put in a multi-agent closed-loop setting where they have to respond to the actions of other agents (Montali et al., 2024). Another approach that has been explored to achieve closed-loop stability is multi-agent RL (Vinitsky et al., 2022). While in principle perfect closed-loop driving may be achieved via self-play, there is no guarantee that the equilibrium the agents find will be at all human-like. For example, self-play agents have no a priori reason to prefer driving on the left side of the road vs. the right. Similarly, because every agent is aware that other agents are a copy of themselves, they may feel comfortable driving much closer to each other than human comfort and reaction times would allow.\nAs a step towards effective and realistic driving partners for simulation, we propose Human- Regularized PPO (HR-PPO). HR-PPO is an on-policy algorithm that includes an additional regularization term that nudges agents to stay close to human-like driving. Concretely, our contribu- tions are:\n\u2022\n\u2022\nWe show that adding a regularization term to PPO agents trained in self-play leads to agents that are more compatible with proxies for human behavior in a variety of scenarios in Nocturne, a benchmark for multi-agent driving.\nOur results also show that effectiveness (being able to navigate to a goal without colliding) and realism (driving in a human-like way) can be achieved simultaneously: Our HR- PPO agents achieve similar performance to PPO while experiencing substantial gains in human-likeness.\nWe also show the benefits of training in multi-agent settings: HR-PPO self-play agents outperform agents trained directly on the test distribution of agents. This suggests that multi-agent training may provide additional benefits over single-agent training (log-replay)."}, {"title": "2 Methods and background", "content": ""}, {"title": "2.1 Human-Regularized PPO", "content": "Let ot, at denote the observation and action at time step t and r(o, a) the instantaneous reward for the agent that executes action a in state o. The history up to time T is defined as xt = (01,01,...,\u0430\u0442-1,\u043e\u0442) (e.g. data collected from a rollout). The basic form of a KL-regularized expected reward objective is defined as:\n$E_{\\tau} \\sum_{t=0}^{T} \\gamma^{t}r(o_{t}, a_{t}) - \\lambda \\cdot D_{KL} \\left[ \\tau(\\cdot \\mid o_{t}) \\|\\pi(\\cdot \\mid o_{t}) \\right]$\nwhere is the most recent stochastic policy, is a stochastic behavioral reference policy obtained from a dataset D and A denotes the regularization weight. The KL divergence is defined as the expectation of the logarithmic differences between the pre-trained (fixed) human-policy and RL policy action probability distributions. For a single observation o and discrete actions, the KL Divergence between the action distributions is defined as:\n$D_{KL} (\\tau(\\cdot \\mid o) \\|\\pi(\\cdot \\mid o)) = \\sum_{a \\in A} \\tau(a)  \\log \\frac{\\pi(a)}{\\tau(a)}$\nwhere our action space |A| = 651. We use the KL-divergence between and as a regularization term added to the standard Proximal Policy Optimization (PPO) objective (Schulman et al., 2017)"}, {"title": "to obtain", "content": "to obtain Human-Regularized PPO:\n$C^{HR-PPO} (\\theta) = (1-\\lambda) \\cdot L^{PPO} (\\theta) + \\lambda \\cdot D_{KL}(\\tau||\\pi)$\nwhere X is a hyperparameter that determines the importance of both objectives. For details on the trained behavioral reference policy distributions, see Appendix C. For training and implementation details, see Appendix D. We implement our code based atop Stable Baselines3 (Raffin et al., 2021)."}, {"title": "Expert demonstrations", "content": "Expert demonstrations We obtain a dataset of observation-action pairs Dk a = {(o, a), ..., (o,a)}1 for N vehicles and T = 80 time steps, for a set of K traffic scenar- ios in the Waymo Open Motion Dataset (WOMD) (Ettinger et al., 2021). The human driver (\"expert\") actions (acceleration, steering) are inferred from the positions and velocity of the observed positions using a dynamic bicycle model (Gulino et al., 2023). As the scenarios are recorded by fusing sensors onboard an autonomous vehicle (AV), the inferred positions of the AV are of higher quality compared to those of surrounding non-AV vehicles, which tend to have more noise. Therefore, we only use the demonstrations from the AV vehicles. To illustrate the difference between AV and non-AV demonstrations, Table 5 contrasts the performance under different conditions, and Figures 10, 11, and 12 show several randomly sampled trajectories in the dataset."}, {"title": "Imitation Learning", "content": "Imitation Learning We train a Behavioral Cloning (BC) policy on the shuffled dataset of observation-action pairs to an open-loop accuracy of 97-99%. The dataset, D = {(0i, ai)}{TK) is obtained from K = 200 scenarios with T = 90 time steps, which is equal to just 30 minutes of driving data. We obtain the behavioral reference policy using the negative log-likelihood objective to the expert demonstrations:\n$T_{NLL} = arg min_{\\theta} \\sum_{i=1}^{N} -log \\tau_{\\theta} (a_{i} \\mid O_{i})$\nand implement the algorithm using the imitation package (Gleave et al., 2022). Table 1 compares the performance of BC policies trained and evaluated on randomly assigned vehicles to only AV vehicles. We also show the performance obtained with the discretized expert actions (top-row), which is an upper bound on performance with this action space. Our BC policy trained on only the AV demonstrations performs better when used to control either the AVs or the random (non-AV) vehicles in the scenarios. Therefore, we select this policy as a regularizer in the multi-agent human-regularized PPO setting."}, {"title": "2.2 Environment details", "content": ""}, {"title": "2.2.1 Dataset and simulator", "content": "We use Nocturne (Vinitsky et al., 2022), a 2D multi-agent driving simulator that runs at 2000+ FPS built on top of the Waymo Open Motion Dataset (WOMDB; (Ettinger et al., 2021)) for training and evaluation. For the training dataset, we partition 10,200 randomly chosen traffic scenarios into 200 for training and 10,000 for testing. Each traffic scenario is 9 seconds, which is discretized at 10 hertz. We use the first second as a warmup period that provides agents with context, so each episode has"}, {"title": "2.2.2 Partially observable driving navigation tasks", "content": "At initialization, every vehicle in a scenario starts at a fixed position x = (x, y) and is assigned a fixed goal position x = (x, y). A vehicle obtains the sparse reward when its center is within a tolerance region of its goal position: ||x - xg||2 < d before the end of the episode, which is at most 80 steps. The goal positions are fixed and set to the last point from every logged vehicle trajectory. We set the tolerance region to 8 = 2 meters. Vehicles are removed from the scene when they go off-road or collide with another agent."}, {"title": "2.2.3 State space", "content": "A vehicle i has two main sources of information about the environment. The first is the ego state, s\u00b2 \u2208 R10, which includes the speed, the vehicle length, and width, its current speed, the distance to the goal position, the angle to the goal position (target azimuth), the heading and speed at goal position from the logged trajectory, the current acceleration and the current steering position. Secondly, the vehicle has a partial view of the traffic scene which is constructed by parameterizing the view distance, head angle, and cone radius of the driver v\u00b2 \u2208 R6720 and contains the road graph information, vehicle objects and the positions and speeds of the other vehicles that are within its field of view. Figure 1 shows an example scene in Nocturne with the obstructed vehicle view. We denote the full observation for a vehicle i as o\u00b2 = [s\u00b2, v\u00b2]. The observations are all relative to every agent's own ego-centric frame. In this work the cone radius is always 180 degrees and the radius of the cone is 80 meters."}, {"title": "2.2.4 Action space", "content": "At each time step, all agents simultaneously take actions. An action is a 2-dimensional tuple with the vehicle's acceleration and steering wheel angle. We create a joint action space by discretizing the actions (acceleration, and steering) into a grid of 21 x 31 = 651 actions. The steering wheel angle lower bound is set to -0.3 radians and the upper bound to 0.3 radians. The acceleration bounds are -4 and 4 m/s\u00b2."}, {"title": "2.3 Reward function", "content": "In our agent-based simulation, we provide sparse rewards to agents when they reach their goal position before the end of the 80-step episode. If an agent reaches its goal, it receives a reward of +1. Otherwise, it receives a reward of 0. The goal-achieved condition is satisfied when the vehicle is within a tolerance region of 2 meters from the target position. If a vehicle collides with another vehicle, goes off the road, or achieves its goal, it is removed from the scene. The reward function"}, {"title": "is intentionally", "content": "is intentionally simplified, omitting common additions such as reducing the distance to the goal, maintaining a safe distance from other vehicles, or following road rules. This is done so that all of these components can emerge from imitation regularization, rather than being hardcoded in."}, {"title": "3 Experiments and results", "content": ""}, {"title": "3.1 Baselines and implementation details", "content": "We use self-play to train HR-PPO agents in scenarios where we control all the vehicles in the scene, with a maximum of 43 controlled vehicles. Full implementation details, including the architecture, hyperparameters, and compute used, are found in Appendix D. We compare HR-PPO agents with four different baseline training methods:\n\u2022 Multi-agent PPO: Self-play while controlling all vehicles in the scene, without regularization.\n\u2022 Single-agent PPO: Sample a random agent at reset to control, step the rest of the agents in log-replay.\n\u2022 Single-agent HR-PPO: Add regularization but all but one random agent is in log-replay.\n\u2022 Behavioral Cloning: The behavioral reference policy."}, {"title": "3.2 Evaluation metrics", "content": "We evaluate our driving agents based on two classes of metrics, as shown in Figure 2. We refer to the first category as Effectiveness, which measures how well driving agents can achieve their goal safely, without colliding or going off-road. The second category, Realism, assesses how closely the driving behavior of the agents matches that of human drivers in the dataset. We use a variation of the Average Displacement Error (ADE) to measure the deviation from the logged human trajectories. In contrast to the trajectory prediction setting, our agents are goal-conditioned and thus they don't have to do inference over their own target goal positions. To distinguish this from the metric used in trajectory prediction, we refer to the metric as the Goal Condtioned ADE (GC-ADE). Additionally, we examine the absolute differences between the human expert actions and the policy-predicted steering wheel angle and acceleration at each time step. Full details on the metrics are in Appendix E."}, {"title": "3.3 Aggregate performance", "content": "Table 2 shows our aggregate performance. We compare the performance of HR-PPO agents to the baselines on the full train dataset, which consists of 200 traffic scenarios, and the test dataset,"}, {"title": "3.4 Driving in a human-like way", "content": "Human-like and effective driving agents. We aim to construct useful driving agents that can navigate effectively and resemble human driving behavior. To test whether these two properties can be achieved simultaneously, we contrast several existing realism metrics against the effectiveness of agents (Details of the metrics in Section 3.2). Across all four human similarity metrics, we observe that significantly more human-like behavior can be achieved for a minimal or even no trade-off in performance. For instance, Figure 3 shows that HR-PPO with a regularization weight of X = 0.06 has a Goal-Conditioned Average Displacement Error (GC-ADE) of 0.54, which is a 60% improvement to PPO (GC-ADE is 1.32), for a decrease in goal rate of 1%, and increase in off-road rate of less than 1%. We observe the same pattern when we compare the policy-predicted actions to the logged human driving logs, as shown in Figures 4,20, and 19. These measures hold when evaluated in a single-agent setting where we control only the AV vehicles (shown in Table 4) as well as the setting where we control all vehicles in the scene (Table 3).\nNatural correction for bad actions. Datasets of human driving may contain noise or undesirable actions. For instance, in our dataset, the off-road rate of replaying the expert actions is quite high (> 12%). However, we observe that HR-PPO agents, which are trained with these imperfect behavioral cloning actions, learn to ignore a large fraction of them and instead achieve an off-road rate between 2-4%. This finding suggests that it may not be necessary to have a near-perfect BC policy as the regularizer as RL can compensate for some of the weaknesses of the regularization policy."}, {"title": "3.5 Coordinating with human drivers", "content": "We explore the ability of HR-PPO agents to coordinate with human drivers in interactive scenarios. Since we cannot directly interact with human drivers, we use the available driving logs as a proxy instead. We compare the collision rates between self-play mode, where all agents are controlled by a single policy, and log-replay mode, where a single random agent is controlled by our policy, and the rest of the agents are controlled by human driving logs. By swapping out only the agents in identical scenarios, we can isolate errors caused by the inability to anticipate other agents' actions.\nFigure 5 compares the effectiveness of BC, PPO, and HR-PPO agents in different evaluation modes. PPO performs well when interacting with agents of the same kind but struggles when facing unseen human driver replay agents. Overall, there's a significant increase in collision rates, exceeding 20%, when switching from self-play mode to log-replay mode. HR-PPO also experiences a rise in collision rates, but to a lesser extent, with an increase of 7%. In log replay, HR-PPO outperforms the base BC agent in terms of collision rates while also achieving a much higher goal rate.\nThe effectiveness of HR-PPO agents in coordinating is more visible when we examine the collision rate as a function of the number of intersecting paths vehicles encounter (Details in Section E.3), which is shown in Figure 6. Notably, the collision rate for PPO consistently increases as trajectories become more interactive, with collisions occurring between 40-65% of vehicles when encountering one"}, {"title": "or more intersecting", "content": "or more intersecting paths. In contrast, the collision rate for HR-PPO shows only a slight increase of approximately 5-8% compared to its self-play collision rate, remaining relatively stable regardless of scene interactivity. It is worth noting that this improvement is not quite evident based on the aggregated performance metrics because more than 70% of all agent trajectories in the dataset do not intersect with other vehicles. Altogether, our results suggest that HR-PPO agents are more compatible with human driving behavior."}, {"title": "What makes", "content": "What makes HR-PPO agents more compatible with the human logs? To find out, we conduct a qualitative analysis. After analyzing the driving behavior of PPO and HR-PPO agents in 50 randomly sampled scenarios, we conclude that the lower collision rates can be attributed to two main factors. First, the HR-PPO agent's driving style aligns better with human logs, enabling a higher level of anticipation of other agents' actions. Secondly, HR-PPO agents maintain more distance from other vehicles, which reduces the risk of collisions. A subset of videos are available at https://sites.google.com/view/driving-partners.\nRegularization ensures that policies are more consistent with a reference distribution, in our case the human driving logs. This is also evident when we plot the statistical divergence between policies during training as shown in Figure 7. On the left side, we see that the PPO and HR-PPO learning curves are similar, indicating that both agents learn to navigate effectively. On the right side, we plot the KL divergence between the human and RL policies across training. In the case of PPO, the divergence increases indefinitely, while for HR-PPO, the divergence remains small. Although both policies seem to converge from the reward curves, the resulting driving behaviors are fundamentally different."}, {"title": "4 Related work", "content": "Driving agents in simulation. There are four major approaches used in existing traffic simulators to model human drivers. One class of methods uses low-dimensional car following models to describe the dynamics of vehicle movement through a small number of variables or parameters (Kreutz & Eggert, 2021; Kesting et al., 2007; Treiber et al., 2000). Rule-based agents have a fixed set of behaviors. Examples of rule-based agents in driving simulators include car-following agents (Gulino et al., 2023; Caesar et al., 2021; Lopez et al., 2018; Casas et al., 2010) such as the IDM model and behavior agents that can be parameterized to drive more cautiously or aggressively such as CARLA's TrafficManager (Dosovitskiy et al., 2017). While car-following and rule-based agents can respond to other agents and thus provide interactivity, it can be challenging for them to capture the full complexity of human driving behavior and these agents frequently experience non-physical accelerations or come to a deadlock in complex interactions. Some simulators provide the recorded human driving logs which can be replayed to allow for interactions (Lu et al., 2023; Vinitsky et al., 2022; Gulino et al., 2023;, FAIR; Caesar et al., 2021). Although these static models produce realistic trajectories, they cannot respond to changes in the environment, such as other drivers. Finally, some driving simulators include learning-based agents using reinforcement learning (Li et al., 2022), however, these agents likely do not resemble human behavior. Our Human-Regularized PPO approach aims to produce simulation agents that meet all these criteria to allow for the controlled generation of challenging real-life interactions in simulation.\nImitation Learning and Supervised Learning. A canonical approach for developing learning- based driving policies for autonomous driving has been through Imitation Learning (IL) (Pomerleau, 1988; Bojarski et al., 2016; Xu et al., 2023) and other supervised methods such as trajectory prediction (Philion et al., 2023) and language-conditioned traffic scene generation (Tan et al., 2023). IL works by mimicking expert behavior using recorded actions from human drivers. There are two broad classes of IL: open-loop and closed-loop. Open-loop methods, like Behavioral Cloning (BC), learn a policy without taking into account real-time feedback. As such, one limitation of open-loop IL methods is that they suffer from compounding errors once deployed in closed-loop systems (Ross et al., 2011). Closed-loop IL (Ng et al., 2000; Ho & Ermon, 2016; Fu et al., 2017; Igl et al., 2022; Baram et al., 2017; Suo et al., 2021) improves upon this by letting the system adjust its actions through ongoing interaction with the environment during training. While these methods provide enhanced robustness, they have not yet achieved high closed-loop performance when all agents are controlled. In addition, our approach does not rely on large, high-quality datasets of human driving data.\nMulti-Agent Reinforcement Learning. Reinforcement learning techniques have been effective in developing capable agents without requiring human data (Silver et al., 2016; 2018; Vinyals et al., 2019) in zero-sum and collaborative games. While this approach has worked in a range of games (Strouse et al., 2021; Bard et al., 2020), many games have multiple equilibria such that agents trained in self-play do not perform well when matched with human-partners (Bakhtin et al., 2021; Hu et al., 2020). In the driving setting, this challenge can partly be ameliorated through the design of reward functions that encode how people drive and behave in traffic interactions (Pan et al., 2017; Liang et al., 2018). However, it is not entirely clear what reward function corresponds to human driving and the inclusion of this type of reward shaping can create undesired behaviors (Knox et al., 2023). An alternate approach tries to create human compatibility through the design of training procedures that restrict the set of possible equilibria (Hu et al., 2020; 2021) by ruling out equilibria that humans are unlikely to play.\nCombined IL + (MA)RL. Recent work has shown that augmenting IL with penalties for driving mistakes can create more reliable policies. This has been demonstrated in both closed-loop (Zhang et al., 2023) and open-loop (Lu et al., 2023) settings. Outside of the driving domain, augmenting goal- conditioned multi-agent reinforcement learning (MARL) with a small amount of observational data has been found to improve the likelihood of convergence to the equilibrium under some settings (Lerer"}, {"title": ""}]}