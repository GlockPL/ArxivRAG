{"title": "PAFFA: Premeditated Actions For Fast Agents", "authors": ["Shambhavi Krishna", "Zheng Chen", "Vaibhav Kumar", "Xiaojiang Huang", "Yingjie Li", "Fan Yang", "Xiang Li"], "abstract": "Modern AI assistants have made significant progress in natural language understanding and API/tool integration, with emerging efforts to incorporate diverse interfaces (such as Web interfaces) for enhanced scalability and functionality. However, current approaches that heavily rely on repeated LLM-driven HTML parsing are computationally expensive and error-prone, particularly when handling dynamic web interfaces and multi-step tasks. To overcome these challenges, we introduce PAFFA (Premeditated Actions For Fast Agents), a framework designed to enhance web interaction capabilities through an Action API Library of reusable, verified browser interaction functions. By pre-computing interaction patterns and employing two core methodologies - \"Dist-Map\" for task-agnostic element distillation and \"Unravel\" for incremental page-wise exploration PAFFA reduces inference calls by 87% while maintaining robust performance even as website structures evolve. This framework accelerates multi-page task execution and offers a scalable solution to advance autonomous web agent research.", "sections": [{"title": "Introduction", "content": "The rapid growth of AI systems capable of autonomously navigating increasingly complex tasks has opened up new opportunities for enabling AI to interact with intricate interfaces, such as web environments. While AI assistants have demonstrated remarkable proficiency in natural language processing and specialized API integration, fluid interaction with web interfaces remains a significant challenge, limiting their broader applicability and adoption. Current approaches to web interaction face three core challenges: efficiency, reliability, and scalability.\n\u2022 Efficiency: Existing web agents rely heavily on repeated Large Language Model (LLM) inference calls for HTML parsing and action decisions. Each interaction requires fresh parsing and context understanding, leading to high computational overhead. This becomes particularly problematic in multi-step tasks where each action requires multiple inference calls.\n\u2022 Reliability: Web interfaces are inherently dynamic, with elements changing positions and structures being updated in real-time. Content indexing systems become outdated quickly, while direct HTML parsing approaches are vulnerable to structural changes, leading to cascading errors in task execution.\n\u2022 Scalability: Existing solutions often rely on either website-specific implementations or comprehensive HTML understanding for each task. Custom API development is resource-intensive and difficult to maintain, while universal HTML parsing approaches struggle with the diversity of web implementations.\nThe research community has attempted to address these challenges through various approaches. Early frameworks like MiniWoB++ (Liu et al., 2018) introduced controlled environments for basic web tasks but failed to capture real-world complexity. Mind2Web (Deng et al., 2023) advanced this by extending to real-world websites, yet still faced challenges with dynamic content handling and state management. Multimodal approaches, exemplified by WebVoyager (He et al., 2024), enhanced web understanding by integrating visual and textual elements. However, these systems operate within what we term the \"short-horizon task paradigm\u201d\u2014they process each action independently, requiring separate LLM inference calls for HTML parsing and action selection at each step. This paradigm's limitations become increasingly apparent as task complexity grows. Navigation-focused frameworks like FLIN (Mazumder and Riva, 2020) and We-"}, {"title": "Related Works", "content": "The development of web agents has evolved across task automation, action planning, and efficiency optimization dimensions, with recent advances in foundation models and multimodal approaches accelerating progress."}, {"title": "Foundation Models and Web Interaction Frameworks", "content": "Early web automation frameworks like Mini-WoB++ (He et al., 2024) provided controlled environments for basic web tasks, but failed to capture real-world complexity. Mind2Web (Deng et al., 2023) advanced the field by incorporating real-world websites, though challenges remain in handling dynamic content. Pre-trained models have shown particular promise, with bidirectional models like HTML-T5 achieving state-of-the-art results in document parsing (Li et al., 2021)."}, {"title": "Multimodal and Vision-Based Approaches", "content": "Recent work has explored multimodal interactions for web navigation. WebVoyager (He et al., 2024) leverages multimodal models for understanding both visual and textual elements, while Pix2Act (Shaw et al., 2023) demonstrates success in screen-shot parsing and behavioral cloning using Monte Carlo Tree Search."}, {"title": "Navigation and Planning Systems", "content": "Frameworks like FLIN (Mazumder and Riva, 2020) and WebLINX (L\u00f9 et al., 2024) have advanced natural language-based navigation, though their step-by-step planning mechanisms create efficiency bottlenecks. Recent work (Gur et al., 2023) has shown promise in task decomposition and multi-step interactions, while MindSearch (Ma et al., 2023) introduces graph-based planning strategies."}, {"title": "Action Abstraction and Evaluation", "content": "Current approaches face challenges in balancing accuracy with computational efficiency, requiring repeated HTML parsing and LLM inference. Mind2Web-Live (Pan et al., 2024) introduces progress-aware evaluation allowing multiple valid paths to task completion. While frameworks have attempted to create reusable components, most focus on low-level actions. Recent work in self-experience supervision (Gur et al., 2023) and frameworks like TPTU-v2 (Kong et al., 2023) explore promising directions in tool use and planning, though gaps remain in developing flexible, high-level action APIs."}, {"title": "Methodology", "content": ""}, {"title": "Overview", "content": "Contemporary web agent architectures face two fundamental constraints that limit their effectiveness: context length limitations and cross-website generalization capabilities.\nThe first constraint stems from the inherent limitations of LLMs when processing HTML documents (Zhou et al., 2023; Deng et al., 2023). Modern websites present very complex DOM structures that vary a lot, making comprehensive context processing expensive, particularly acute in multi-page tasks, where cumulative context across pages dramatically increases computational overhead and reduces efficiency.\nThe second constraint emerges from the dynamic nature of web interfaces, as documented by (Pan et al., 2024) that demonstrates web interfaces and interaction patterns undergo frequent modifications over time. This is further supported by baseline performance metrics from Mind2Web (Table 1), which show significantly degraded performance when models encounter previously unseen websites (Cross-Website split); indicating that models struggle more with novel website structures than with new task domains which may be similar to training."}, {"title": "Dist-Map: Distillation then Mapping", "content": "Developed as our first methodological approach for task-specific script generation (Figure 1), Dist-Map builds upon established research in HTML document understanding (Gur et al., 2023; Zheng et al., 2023). While addressing the challenge of processing multiple HTML pages within a unified context, we introduce task-agnostic HTML document understanding to enable persistent actions across semantically similar tasks.\nDist-Map operates in two phases: element distillation and script generation. The distillation phase creates distilled functions that encapsulate DOM selectors and their functionalities, organized hierarchically by page-level operations. This systematic reduction in context enables focused task planning rather than requiring comprehensive page interpretation. By implementing task-agnostic distillation, the system can operate on any website without prior knowledge of specific annotated interactions, significantly reducing dataset dependence.\nOur experimental observations led to the implementation of a verification mechanism for ensuring DOM selector and attribute correctness, as initial distilled outputs may not achieve perfect accuracy. This verification process identifies non-interactive extracted functions and generates corrections using the Sonnet language model, leveraging:\n1. Language models' high precision in identifying and correcting specific inaccuracies\n2. The tractability of correcting a limited set of incorrect elements compared to comprehensive element identification\nThe script generation phase employs contextual mapping, where Sonnet maps each task to its required distilled element files. For instance, an airline check-in task selectively incorporates elements from homepage and check-in pages while excluding unrelated flight booking interfaces. This targeted approach enables precise context management and focused task execution, utilizing a two-step self-reflective prompting system to generate Selenium-based Python code with established best practices.\nThrough this methodology, Dist-Map enables the creation of task-specific scripts that form the foundation of our Action API Library (Figures 2, 3), particularly excelling at handling complex multi-page interactions while maintaining minimal context requirements."}, {"title": "Unravel: Solve in chunks", "content": "While implementing Dist-Map, we identified a significant limitation: the task-invariant element distillation process does not achieve perfect accuracy in identifying task-essential element selectors, even with corrective measures in place. This observation led to the development of Unravel, our second methodological approach to handling multi-page complex tasks.\nInstead of employing comprehensive selector distillation, Unravel implements a chunked execution strategy, decomposing tasks into smaller units that can be processed using individual webpages."}, {"title": "Library Creation", "content": "Once the scripts are created, we turn to creating an Action API Library by assimilating content from all related scripts. This process involves a two-step reasoning-based process. First, the model must create logical groupings for tasks of a website, with each group having one or more tasks, and intended to have as much overlap as possible for web execution on that page (Figure 2) Next, for each group, we direct the model (with a 2-step self-corrective reasoning prompt) to generate an API that can adapt to execute every step in the corresponding scripts (see Figure 3). That is, within a task group, the API generated would, with the right parameters, solve the task exactly like the original script would have. This prompt ensures that parameterization for necessary and optional arguments are clear and well-documented so that the API grounding in the downstream (Figure 4) is"}, {"title": "Results", "content": "In order to measure task completeness and compare to Mind2Web, there are two metrics - 'Element Accuracy', i.e. percentage of the elements interacted with are correct, and 'Step Accuracy', i.e. the percentage of correct combination of element and action taken on that element for every step of a task. There are various data splits we can view the baseline comparisons in - cross-task (trained on the same website, but different tasks), cross-website (trained on the different websites). We also have two domains we're considering - Airlines and Shopping.\nDuring evaluation, element accuracy and step accuracy (both macro averaged), are expected to exactly match the annotation (marked \u2018Exact' in Tables 1, 2). However, from qualitative analysis, it's found that many tasks actually have many correct 'paths' to completion, and hence, those inexact paths should also be considered, as well as annotation irregularities. Using these findings, we perform a human re-evaluation using human checks\nDuring evaluation, element accuracy and step accuracy (both macro average across websites), are expected to exactly match the annotation. However, from qualitative analysis, it is found that many tasks actually have many correct 'paths' to completion, and hence, those inexact paths should also be considered, as well as annotation irregularities. Using these findings, we perform a human re-evaluation, marked under 'Inexact' in the Tables 1, 2."}, {"title": "Finetuning versus Prompting SOTA", "content": "Another key point to acknowledge during baseline comparisons is the use of a fine-tuned set of DeBERTa (He et al., 2021) (86M)+ FLAN-T5 (Chung et al., 2022)(XL has 3B params). Sonnet 3.5, however, is currently the SOTA model, and is a lot bigger, and we use it with zero-shot prompting.\nIn order to understand how much of the model performance may be attributed to Sonnet 3.5, we use Sonnet in the MindAct framework, with 3-shot examples. These results are compiled and shown in Tables 1, 2. We can see that despite using Sonnet 3.5 in MindAct with 3-shot prompt, PAFFA framework methods outperforms the former. In particular, we note that the performance of Unravel on the Cross-Website split is on par with any other split for Unravel, while in MindAct, it struggles with lack of training in the Cross-Website and shows lower performance."}, {"title": "Cost Comparison", "content": "At deployment, whenever a new request comes in, we have a simpler query with just the API library, to ground the task to the right action API and execute it. By comparison, for MindAct, per task cost comparisons include MCQA-based element+action selection, stepwise. In Table 3, we can see the total tokens used to prompt for a single task at deployment is 87% less for PAFFA library calls than for MindAct . Further, this calculation does not take into account the time/tokens for ranking candidate elements from DeBERTa for each page in real time, further increasing costs for MindAct."}, {"title": "Discussion and Conclusion", "content": "Through PAFFA (Premeditated Actions For Fast Agents), we introduce a framework that significantly advances web agent capabilities through an Action API library of reusable functions. Our two core methodologies-Dist-Map's task-agnostic element distillation and Unravel's incremental pagewise exploration\u2014work in tandem to address the fundamental challenges of efficiency, reliability, and scalability in web automation.\nEmpirical evaluations on the Mind2Web benchmark demonstrate substantial improvements over existing approaches, with element accuracy increasing from 56% to 74% and step accuracy from 50% to 57%. Most significantly, PAFFA achieves an 87% reduction in inference tokens while maintaining robust performance across different data splits, including challenging cross-website scenarios. This improvement in efficiency, coupled with consistent performance, positions PAFFA as a practical solution for production environments.\nDespite these advances, we acknowledge several limitations: the reliance on human evaluation given multiple valid task completion paths, the need for robust verification modules, and the current scope of test datasets. These challenges, however, point to promising future research directions, including automated API grouping and grounding methods, integration with broader AI assistant capabilities, enhanced verification mechanisms, and automated API maintenance for evolving websites.\nThe Action API Library concept represents a fundamental shift in approach, moving from repetitive HTML parsing to pre-computed, reusable actions. By significantly reducing computational overhead while maintaining adaptability to dynamic websites, PAFFA provides a scalable foundation for advancing autonomous web agent research and practical deployment."}, {"title": "Common Prior Workflow", "content": "Existing solutions iteratively call the LLM after every single action, which is inefficient, and does not take into account planning that Agents can leverage to understand the web like a human would, by constructing viable workflows and following those action sequences. Previously executed tasks should inform similar future tasks."}, {"title": "Dataset", "content": "We evaluate our framework on the Mind2Web benchmark (Deng et al., 2023), which provides annotated actions for diverse tasks across real-world websites. This dataset's key advantage is its use of complex, real-world websites rather than simplified environments like Mini-WoB++ (Liu et al., 2018), enabling more realistic evaluation of generalization capabilities.\nFor our evaluation, we extract a focused subset of Travel-Airlines domain and Shopping-General Cross-Task splits. We additionally include the Cross-Website split of Shopping-General to assess generalization in our no-train setting (see Appendix A.3 for more details). From each website's training data, we extract 'unique web pages' - pages with distinct HTML structures whose content may change dynamically (e.g., a homepage's structure"}, {"title": "Dataset Details", "content": "The three datasets are:\n1. Cross-Task Split for Travel-Airlines : contains 31 test tasks, across 7 websites : American Airlines, Delta, Jetblue, Kayak, Qatarairways, Ryanair, United.\n2. Cross-Task Split for Shopping-General : contains 8 test tasks, across 3 websites : Amazon, Target, Instacart.\n3. Cross-Website Split for Shopping-General : contains 17 test tasks, for one website : Google Shopping.\nNote: There are no Cross-Website test cases for Travel-Airlines."}]}