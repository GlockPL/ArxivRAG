{"title": "Can visual language models resolve textual ambiguity with visual cues?\nLet visual puns tell you!", "authors": ["Jiwan Chung", "Seungwon Lim", "Jaehyun Jeon", "Seungbeen Lee", "Youngjae Yu"], "abstract": "Humans possess multimodal literacy, allowing them to actively integrate information from various modalities to form reasoning. Faced with challenges like lexical ambiguity in text, we supplement this with other modalities, such as thumbnail images or textbook illustrations. Is it possible for machines to achieve a similar multimodal understanding capability?\nIn response, we present Understanding Pun with Image Explanations (UNPIE), a novel benchmark designed to assess the impact of multimodal inputs in resolving lexical ambiguities. Puns serve as the ideal subject for this evaluation due to their intrinsic ambiguity. Our dataset includes 1,000 puns, each accompanied by an image that explains both meanings. We pose three multimodal challenges with the annotations to assess different aspects of multimodal literacy; Pun Grounding, Disambiguation, and Reconstruction. The results\u00b2 indicate that various Socratic Models and Visual-Language Models improve over the text-only models when given visual context, particularly as the complexity of the tasks increases.", "sections": [{"title": "1 Introduction", "content": "Humans can actively integrate information from multimodal sources without being explicitly told to. For example, a wink can reveal the insincerity behind a statement about dieting. Similarly, visual aids such as Venn diagrams help students understand abstract concepts such as set theory. This active understanding capacity is often denoted as multimodal literacy (Mills and Unsworth, 2017).\nIn contrast, current multimodal models lack this capacity for active understanding and typically operate under two assumptions: (1) all instructions require visual inputs, and (2) these inputs are relevant (Cui et al., 2023; Zhang et al., 2024). Such limitations hinder their applicability in real-world scenarios, such as summarizing long blog posts, where irrelevant images must be excluded, and only contextually significant visuals should be used to enhance the understanding of disparate text segments.\nAn essential component of multimodal literacy is the ability to resolve multimodal ambiguities effectively, which refers to the capacity to disambiguate conflicting or unclear information in modality with information from another modality (Kottur et al., 2021; Guo et al., 2022). Owing to its explicit requirement of multimodal information gathering, disambiguation can serve as a controlled benchmark for evaluating multimodal literacy.\nPuns stand as a unique challenge within ambiguity modeling. They are intrinsically ambiguous and understanding a pun requires grasping multiple interpretations of a single phrase or word simultaneously. Understanding puns can be difficult even for humans, often necessitating visual cues to clarify the intended interpretation, as demonstrated in Figure 1. Compared to verbose textual explanations, visual cues can deliver instant insight, preserving the humor and cleverness of the pun (Morreall, 1983). Therefore, puns provide an ideal testing ground for assessing models' capabilities in multi-"}, {"title": "2 Overview of UNPIE Benchmark", "content": "UNPIE is a new multimodal multilingual benchmark. Its primary aim is to assess machines' capacity to actively integrate information from visual sources to resolve ambiguity in text. Our dataset leverages puns that inherently contain such ambiguity to study the challenge of multimodal literacy in a natural environment.\nUNPIE extends puns in two directions: visual context and multilingual translations. First, we collect images for each pun that 1. describes both meanings of the pun to explain it and 2. depicts only one meaning of the pun to disambiguate the pun (section 2.1). While one can naturally retrieve images for disambiguation from the web, images that illustrate the ambiguity of the pun in a single canvas are rare. Thus, we use an off-the-shelf text-to-image model (Betker et al., 2023) to generate such images. We then employ human annotators to filter the images so that they correctly explain the given pun. Secondly, we ask human annotators to translate the English pun sentences into multilingual targets (section 2.2). Importantly, the ambiguity should not carry on to the translation target."}, {"title": "2.1 Collecting Puns with Visual Context", "content": "Base Text-Only Pun Data. We build our multimodal multilingual benchmark on top of the text-only English pun dataset of SemEval 2017 Task 7 (Miller et al., 2017). The dataset bounds the pun understanding problem in two ways to rely less on external requirements: first, each sentence contains a maximum of one pun. Hence, a sentence's lexical ambiguity is regulated, at least in terms of puns. Second, most pun has a lexical entry in WordNet 3.1 (81% of the whole data). This vocabulary limit keeps our pun generation problem from being dominated by many out-of-vocabulary words.\nThe data is divided into Homographic and Heterographic puns, depending on the surface form of the puns. As shown in Figure 3, homographic puns have identical spelling and pronunciation but different meanings, while heterographic puns differ in spelling and meanings. We inherit this categorization scheme and report our experiment results category-wise (Homographic and Heterographic).\nFrom the SemEval 2017 collection of 2,878 English pun sentences, we selected 500 homographic and 500 heterographic puns with concrete concepts that are more easily visualized through images.\nGenerating Pun Explanation Images. UNPIE is designed to assess a VLM's capability to resolve lexical ambiguity with visual context. In terms of a pun, the context should depict both meanings"}, {"title": "2.2 Translating Puns to Multilingual Targets", "content": "Evaluating a machine's ability to understand puns is a complex task. Without a rule-based algorithm to measure this capability, the assessment often relies on human judgment or other machines. However, relying on human evaluation can limit the scal- ability of the assessment process, while machine-based evaluation, such as using models like GPT-4 (OpenAI, 2023), may introduce undesirable biases (Liu et al., 2023c; Hada et al., 2023). To overcome these challenges, we suggest an alternative evaluation method via a downstream task in translation, intentionally aligning with previous research in the field of multimodal machine translation.\nTranslation with Machine Assistance. We translate the original English pun sentence into three languages (German, French, and Korean). Note that we should ensure that the ambiguity in English does not carry over into the translated targets.\nWe here design a cooperative framework between machines and humans for pun translation. Per each language pair (e.g. En \u2192 De), we recruit a bilingual worker whose native language is the target language (e.g. De). First, we use off-the-shelf translation models to generate three candidates. Then, the human workers select the best one and make further modifications to finalize the translation. This machine-assisted translation aligns with common practices in the industry (Federico et al., 2012). We chose machine-human cooperation for two reasons: firstly, we saw that our human translators find pun translation difficult. Machine suggestions can serve as starting points here. Secondly, this method expedited the annotation process and reduced costs.\nAddressing Lingering Ambiguity. Certain cases arise where the ambiguity in the source language is retained in the translated text in literal translation. For example, consider the sentence: \u201cA baseball player was a thief. He was always trying to steal.\u201d The pun in this sentence relies on the dual meanings"}, {"title": "2.3 Dataset Analysis", "content": "Our pipeline yields a dataset comprising 500 homographic and 500 heterographic pun sentences, each accompanied by one pun explanation image, two pun disambiguator images, and translations to three languages.\nHow natural are the generated images? Given the limited availability of real-world images accurately depicting puns, we opted to use AI-generated visuals. To gauge the difference between generated and authentic images, we conducted two human evaluation studies, comparing our generated images against natural image-pun pairs sourced from the web (https://www.reddit.com/r/puns/).\nIn the first study, human evaluators were asked to identify the correct text pun associated with each image from a set of potential matches. Results showed that natural images achieved an accuracy of 86%, while our generated images achieved a slightly higher accuracy of 92%. This test was conducted using a set of 50 randomly selected images. In the second study, we conducted an A/B comparison to assess the perceived naturalness of the images. To ensure consistency, natural images containing multiple panels, written text, or well-known characters were excluded from the evaluation. Across three independent evaluators, the naturalness test resulted in accuracy rates of 66%, 72%, and 74%, respectively, using another set of 50 random images. Overall, despite slight distributional differences between the generated and natural pun images, the disparity is considered acceptable. These findings indicate that evaluations performed within our benchmark can be reasonably"}, {"title": "3 Task Overview", "content": "We pose three multimodal pun understanding tasks on the collected annotations to test models' capability to use visual context in addressing lexical ambiguity, as illustrated in Figure 2. Each task evaluates different aspects: the easier Pun Grounding task can be solved without image input. It is aimed at determining if less advanced models, which might not fully resolve such challenges, can enhance their performance with added visual information. The second task of pun disambiguation is designed to necessitate the usage of visual context. Finally, the pun reconstruction task replicates a practical multimodal literacy scenario. This task necessitates that models not only use the given translation but also infer or extract the underlying pun meaning that the translation does not explicitly convey, potentially drawing on visual inputs to do so.\nPun Grounding. The first step in understanding a pun is to identify it. Our initial task examines whether visual context aids models in identifying pun phrases within sentences. Given the whole English sentence $x_i = [x_{i,0},...,x_{i,N}]$ containing a pun phrase $s_i = [x_{i,j},...,x_{i,k}]$ and its corresponding pun explanation image $v_e$, the model returns a pun phrase candidate $\\hat{s_i}$. Note that while the actual target phrase $s_i$ is part of the full sentence $x_i$, the model's output $\\hat{s_i}$ is not bound by this constraint. We purposefully formulate this task as a sequence-to-sequence problem to facilitate zero-shot evaluation across various baselines. The model's output is then assessed for exact text match with the actual pun phrase to determine accuracy.\nPun Disambiguation. Once models pinpoint a pun's location, they must then interpret its semantics. Understanding a pun hinges on recognizing the different meanings of the pun phrase, as its humor lies in this ambiguity. In this task, we assess the models' proficiency in correlating each meaning of the pun with its associated visual context. Given the English sentence $x_i$ and the pun disambiguator image $v_d$ aligned with one of the meanings constructing the pun, the model should produce a translation of the sentence into a target language (e.g. German $y_e$). Notably, the translated text should be free of any ambiguity stemming from the pun, closely aligning with the meaning depicted in the provided image. We compare the model-generated translation $\\hat{y}_e$ with two translation targets $y_{e,0}^{De}, y_{e,1}^{De}$, each corresponding to a different meaning of the pun. The model's output is considered correct if it more closely resembles the ground-truth translation $y_{e,j}^{De}$ that corresponds to the meaning depicted in the image $v_{i, j}$, $j \\in \\{0, 1\\}$. Refer to section 4.3 for the implementation details.\nPun Reconstruction. The final task is to reconstruct the complete pun sentence. To make the problem deterministic, we provide two types of inputs to the model: a non-English language translation of the original pun sentence that has been clarified of any ambiguities (e.g. German $y_{e}^{De}$) and the related pun explanation image $v_e$. The model then generates an output $z_i$, which we compare with the original English pun sentence $x_i$ to determine if both English sentences encapsulate the same pun. It is a complex task to determine whether two sentences contain the same pun, and we resort to machine-based evaluation with GPT-4 to obtain the binary decision. We verify GPTEval's validity here using human evaluation in appendix D."}, {"title": "4 Experiments on UNPIE benchmark", "content": ""}, {"title": "4.1 Models", "content": "LM. To measure the effectiveness of multimodal modeling, we establish baselines using unimodal text-only language models. We incorporate an open-source model (Vicuna-13B (Chiang et al., 2023)) and the advanced proprietary language model (GPT-4 (OpenAI, 2023)). Furthermore, we appropriate a visual-language model, LLaVA, for a text-only scenario by inputting only text prompts without the images. This approach assesses the concept of multimodal alignment tax (Chen et al., 2023) in the context of pun interpretation, implying that fine-tuning a model on visual data might"}, {"title": "4.2 Do Images Help Pun Grounding?", "content": "Metrics. We report accuracy based on the equality of the model-estimated pun phrase and the ground-truth pun phrase. To check the equality, we use the exact match of the surface text form and report the accuracy of the outputs.\nResults. As anticipated, the incorporation of visual context led to a consistent improvement in pun grounding performance across all models, including Socratic Models and Visual-Language Models (refer to Table 4). Also, GPT-4, a stronger model,"}, {"title": "4.3 Can VLMs Disambiguate with Images?", "content": "Metrics. We conduct a generative evaluation for the pun disambiguation test. The task for the machines is to translate a given pun sentence into a target language, using the accompanying image as a guide to disambiguate the meaning of the pun phrase. In this generative test, the model generates a sequence of text, which is then evaluated against two potential translation targets. The model's output is considered accurate if it aligns more closely with the translation that corresponds to the context of the provided image. We use BERTScore (Zhang et al., 2019) to measure the text similarity following the human evaluation results in appendix C.\nResults. All the considered baselines have demonstrated their ability to disambiguate translation outputs based on visual context, as illustrated in Table 5. Both strengthening the language model (Vicuna vs. GPT-4) and improving visual context processing (Vicuna with image captions from BLIP-2 vs. LLaVA) led to more accurate disambiguation. Still, comprehending puns in the textual form was a more decisive factor for pun disambiguation than a stronger visual understanding, as GPT-4 with image captions outperforms all other models. Interestingly, fine-tuning with the Multi30k multi-"}, {"title": "4.4 Do Images Help Pun Reconstruction?", "content": "Metrics. The pun reconstruction task involves machines using both the human-translated text and the image context to recreate the original pun sentence. Then, the reconstructed pun is compared with the original sentence for consistency in puns. Still, determining whether two sentences share the same pun is a complex task. To tackle this, we use a machine-based evaluation method with GPT-4 (OpenAI, 2023) to determine if the puns in both sentences are equivalent. To ensure the validity of this approach, known as GPTEval, we further compare it with human annotations in appendix D.\nAdditionally, we report on common text evaluation metrics, such as Bleu-4 (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005)\u2014metrics widely used in the machine translation domain.\nResults. The results in Table 6 affirm that visual context significantly enhances machines' ability to reconstruct puns and manage their inherent ambiguity. For all tested models, the inclusion of images consistently improved the accuracy of pun reconstruction. The only exception was the weakest model in both language processing and visual comprehension (SM based on Vicuna). Notably, unlike the main metric of correctness, the automatic text evaluation scores (Bleu-4 and METEOR) did not reflect a clear trend. Through manual inspection of the generated outputs, we saw that such scores were more aligned with changes in the surface form"}, {"title": "5 Related Work", "content": "Multimodal Machine Translation. By integrating backtranslation as a downstream task, UNPIE contributes to the literature on Multimodal Machine Translation (MMT), a widely studied area that extends neural machine translation with additional visual contexts (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Previous research argues that visual information can help resolve ambiguities in the source text (Li et al., 2022; Hatami et al., 2022). However, the primary dataset for MMT, Multi30K (Elliott et al., 2016), has limited examples of such ambiguities, leading to questions about the use of MMT for assessing multimodal literacy capacity (Elliott, 2018; Wu et al., 2021; Futeral et al., 2023). Another benchmark counteracts this phenomenon with manual annotation (Futeral et al., 2023; Bawden et al., 2018). Nevertheless, this dataset is relatively small (155 samples) due to the difficulty in pinpointing ambiguities within sentences. Additionally, the benchmark is limited to classification models.\nComputational Pun Understanding. After early research (Ritchie, 2005) pointed out ambiguity as a key in pun generation, numerous studies have investigated automatic pun generation regarding heterographic puns, which slackens the surface form identity requirement for each meaning of the pun (He et al., 2019; Yu et al., 2020; Mittal et al., 2022). Other research explored homographic pun generation which is based on multiple meanings of a polysemous word (Yu et al., 2018; Luo et al., 2019; Tian et al., 2022). Recently, Sun et al. (Sun et al., 2022) extended the pun generation problem to consider contextual cues. We extend this line of research with multimodal understanding.\nVisual-Language Models. The field has seen rapid growth since Flamingo (Alayrac et al., 2022) illustrated the advantages of applying large language models to the visual domain. BLIP-2 (Li et al., 2023), utilizing the OPT language model (Zhang et al., 2022), made significant strides in image captioning. The introduction of a stronger language model (Touvron et al., 2023) further enabled prompt-based control of the models. MiniGPT-4 (Zhu et al., 2023) and LLaVA (Liu et al., 2023b) pioneered the field of visual instruction tuning. InstructBLIP (Dai et al., 2023), an extension of BLIP-2, improved its capability to follow instructions more accurately. Further developments in this domain include other models such as LLAMA-Adapter (Zhang et al., 2023) and Qwen-VL (Bai et al., 2023). Our research puts visual language models (VLMs) to the test regarding their multimodal literacy capabilities."}, {"title": "6 Conclusion", "content": "We introduced UNPIE, a new benchmark for the multimodal literacy capability. Based on UNPIE, we craft three tests to measure how machines can utilize visual context to resolve inherent ambiguity in puns. Our findings indicate that machines can indeed leverage visual information to enhance their understanding of text, as shown by their improved performance across all tasks.\nHowever, achieving human proficiency in multimodal literacy is still a challenge. While our results are encouraging, there remains a considerable gap in machine capability to fully grasp and interpret the intricate relationship between text and visuals, particularly in more complex tasks like pun reconstruction. Therefore, we envision UNPIE as not only a platform for testing but also as a starting point for the development of future multimodal models to actively navigate and integrate information from multiple modalities."}, {"title": "7 Limitations and Ethical Considerations", "content": "UNPIE, while being a multilingual dataset, is built on the English-only pun corpus (Miller et al., 2017). As such, it primarily models lexical ambiguities unique to English, stemming from polysemies or similar surface forms of the language. To enhance its linguistic diversity and applicability, expanding the dataset to include ambiguities inherent in other languages would be beneficial. Such expansion would not only diversify the linguistic challenges in the dataset but also offer deeper insights into how lexical ambiguities manifest differently across various languages and cultures.\nAlthough UNPIE's size is much larger than that of the previous multimodal literacy dataset that features explicit ambiguities (Futeral et al., 2023), its total size is insufficient for creating a training split suitable for fine-tuning. This limitation stems from the scarcity of puns, which are inherently challenging for humans to create as well and are not readily available in large quantities online. We thus plan to expand the dataset for multilingual puns in the future.\nEthical Considerations. UNPIE, constructed using existing English puns, may inadvertently perpetuate cultural biases and stereotypes present within the humor. Although human annotators were instructed to eliminate any puns expressing explicit hatred, subtle biases can still be perpetuated through seemingly innocuous humor.\nTo address ethical concerns in the data curation process, we confirmed that all human annotators either volunteered willingly or were compensated fairly for their contributions. We defer the details to appendix B."}]}