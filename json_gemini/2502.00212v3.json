{"title": "STP: Self-play LLM Theorem Provers with Iterative Conjecturing and Proving", "authors": ["Kefan Dong", "Tengyu Ma"], "abstract": "A fundamental challenge in formal theorem proving by LLMs is the lack of high-quality training data. Although reinforcement learning or expert iteration partially mitigates this issue by alternating between LLM generating proofs and finetuning them on correctly generated ones, performance quickly plateaus due to the scarcity of correct proofs (sparse rewards). To keep improving the models with limited data, we draw inspiration from mathematicians, who continuously develop new results, partly by proposing novel conjectures or exercises (which are often variants of known results) and attempting to solve them. We design the Self-play Theorem Prover (STP) that simultaneously takes on two roles, conjecturer and prover, each providing training signals to the other. The conjecturer is trained iteratively on previously generated conjectures that are barely provable by the current prover, which incentivizes it to generate increasingly challenging conjectures over time. The prover attempts to prove the conjectures with standard expert iteration. We evaluate STP with both Lean and Isabelle formal versifiers. With 19.8 billion tokens generated during the training in Lean, STP proves 26.3% of the statements in the Lean Workbook dataset, doubling the previous best result of 13.2% achieved through expert iteration. The final model achieves state-of-the-art performance among whole-proof generation methods on miniF2F-test (61.7%, pass@3200), ProofNet-test (23.1%, pass@3200) and PutnamBench (8/644, pass@3200). We release our code, model, and dataset in this url: https://github.com/kfdong/STP.", "sections": [{"title": "1 Introduction", "content": "The reasoning capability of large language models (LLMs) is critical for various applications, including coding assistants, question-answering, and agents [Plaat et al., 2024, Shinn et al., 2023, Yao et al., 2022, Shao et al., 2024, Li et al., 2023, Nijkamp et al., 2022]. It is also a key criterion for achieving artificial general intelligence (AGI). Automated theorem proving with formal languages by LLMs stands at the forefront of reasoning research [Yang et al., 2024a], partly because it allows objective and reliable evaluation through classical verifiers such as Lean [Moura and Ullrich, 2021] and Isabelle [Nipkow et al., 2002]. Moreover, it arguably encapsulates the essence of advanced reasoning tasks while abstracting away the ambiguity of natural language, enabling meaningful studies on a relatively smaller scale.\nHowever, a fundamental challenge in improving reasoning performance-whether in natural or formal languages-lies in the lack of high-quality training data. Collecting reasoning data requires domain experts, making it expensive to scale. There are only a limited number of advanced math papers and theorems in existence, orders of magnitude smaller than other data sources.\nReinforcement learning (RL) on datasets without solutions (e.g., datasets with theorem statements or reasoning questions and answers) is a prominent approach for improving the reasoning capability, as seen in the recent development of OpenAI 01 Jaech et al. [2024], DeepSeek-Prover [Xin et al., 2024a] and DeepSeek R1 [Guo et al., 2025]. Often referred to as expert iteration Anthony et al. [2017], it partially mitigates the data scarcity issue by alternating between LLMs generating proofs and finetuning them on correctly generated ones [Kaliszyk et al., 2018, Wu et al., 2021, AlphaProof, 2024, Xin et al., 2024b, Ying et al., 2024].\nHowever, as Wu et al. [2024] pointed out, RL or expert iteration often saturates at a low pass rate because the number of samples required to generate a correct proof for an unproven theorem grows exponentially. As a result, a massive amount of computation is wasted on generating incorrect proofs that provide no training signal to the model.\nWe need an algorithm that can run and self-improve indefinitely without more data. To this end, we draw inspiration from how mathematicians learn and develop advanced mathematics; they refine their understanding and sharpen their proof skills by working on synthesized exercises-variants, extensions, or combinations of existing theorems. Additionally, they frequently propose and publish conjectures, a process widely regarded as just as important, if not more so, than solving them. In other words, unlike the current training of LLMs, mathematicians engage with far more exercises and conjectures (referred to collectively as conjectures in this paper) than the polished, published results found in academic papers and books. Moreover, the continuous generation of new conjectures keeps mathematical fields dynamic and moving forward.\nIn this paper, we design Self-play Theorem Prover (STP), which mimics how mathematicians learn and develop mathematics. It simultaneously assumes two roles conjecturer and prover-providing training signals to each other. As illustrated in Fig. 1, the conjecturer, given a seed theorem with proof, proposes a new, related conjecture (Step 1), while the prover attempts to prove conjectures and statements from an existing dataset (Step 2). Then, the verifier selects correct proofs (Step 3) to train the prover using standard RL and identifies correct, approachable, elegant, yet challenging conjectures to supervise the training of the conjecturer (Step 4). More concretely, in each iteration, the conjecturer is trained on previously generated conjectures that: (a) are barely provable by the current prover (i.e., the prover's success probability with respect to its random seed is positive but low), and (b) pass certain elegancy filters. This iterative process gradually increases the difficulty of conjectures and proofs without requiring additional data. Our method can be viewed either as a self-play algorithm between conjectures and provers or as automated curriculum learning [Portelas et al., 2020] with a self-generated adaptive curriculum (via conjecturers).\nWe empirically evaluate our method with both Lean [Moura and Ullrich, 2021] and Isabelle [Nipkow et al., 2002]."}, {"title": "2 Additional Related Works", "content": "We refer the readers to Bibel [2013], Loveland [2016] and the reference therein for classical automated theorem proving. Below, we discuss recent works on modern LLM-based theorem provers in addition to what has been discussed in the intro."}, {"title": "3 Method", "content": "On the high level, Self-play Theorem Prover (STP) involves three training stages: (1) model initialization by supervised finetuning, (2) self-play training (visualized in Fig. 1), and (3) final re-training. Unless otherwise stated, we use the term 'statement' to refer to the statements in given datasets, and 'conjecture' the generated conjectures."}, {"title": "3.1 Model initialization by supervised finetuning", "content": "In this stage, we initialize the model with two roles, conjecturer and prover, by finetuning a generic LLM (such as the Llama [Touvron et al., 2023]) on a SFT dataset constructed from existing proof libraries such as Mathlib [mathlib Community, 2020]. The proof libraries are organized into files containing human-written formal proofs of known mathematical theorems, and each file formalizes a relatively self-contained result, such as a chapter of a textbook. Our SFT data consists of the following two parts, for finetuning the prover and conjecturer, respectively. Also see concrete examples in Appendix A.1.\nProver SFT dataset. We construct a SFT dataset to teach the model to write formal proofs in the given format, where each example is the concatenation of a system prompt (to instruct the model to generate in formal language), a statement, and its corresponding proof. We only compute the next token prediction loss on the proof (which is"}, {"title": "3.2 Self-play training", "content": "Our self-play training stage of STP is shown in Fig. 1. The main difference compared to expert iteration is the conjecturer in Steps 1 and 4, highlighted in a yellow background."}, {"title": "Generating conjectures and proofs (Steps 1 & 2)", "content": "The self-play training starts with collecting a list of the conjecturer's inputs in the same format as in the conjecture SFT dataset (system prompt, lemma, and theorem), but from theorem-proof pairs where the theorems are from the given dataset without proofs and proofs are previously gener-ated. We extract a seed lemma from the proof, using the verifier. To prevent the model from only focusing on a few particular proof techniques, we de-duplicate the list based on the seed statement and lemma, and randomly drop some inputs whose lemma appears excessively. Then, the LLM generates conjectures from the inputs, and we randomly select a subset of the generated conjectures with size no larger than the number of remaining unproved statements in the given dataset, so that the prover's compute budget is split equally between the conjectures and statements. (See the pseudo-code and details in Appendix A.2.) For the prover's inputs, we combine the generated conjectures and the unproved statements in the existing dataset. Then, we independently sample K proofs per statement/conjecture in Step 2."}, {"title": "Reward assignments (Step 4)", "content": "The major technical challenge of STP is to design the reward function for the conjecturer (in other words, construct the conjecturer dataset in Step 4). The ultimate goal is to incentivize conjecturer to generate diverse, relevant, approachable yet challenging conjectures to provide enough training signals to the prover.\nIn Step 4, we first organize all generated conjectures and proofs into a list of examples \\(D = \\{(t_i, p, l_i, c_i, p'_i)\\}_{i=1}^n\\) where \\(t_i\\) and \\(p\\) represents a seed statement and its proof, \\(l_i\\) is a lemma used in the proof \\(p\\), and \\(c_i, p'_i\\) are the generated conjectures and the generated proof. We will filter D as described below and then use \\((t_i, p, l_i)\\) as the input to the conjecturer and \\(c_i\\) as the output, and \\(p'_i\\) as the output of the prover w.r.t. the input \\(c_i\\).\nTo decide whether a conjecture c is challenging, we use the (empirical) pass rate of the prover estimated by the K independently generated proofs:\n\\[P(c) \\equiv (\\#\\{i : c_i = c, p'_i \\text{ is correct}\\})/(\\#\\{i : c_i = c\\}).\\]\nThen, we select the examples in D where (a) lemma \\(l_i\\) is used in the proof of conjecture \\(p\\), and (b) the pass rate of the conjecture, P(c), is between (0,1/4]:\n\\[D\\leftarrow \\{(t_i, p_i, l_i, c_i) | (t_i, p_i, l_i, c_i, p'_i) \\in D, \\\\ P(c_i) \\in (0,1/4], p'_i \\text{ is correct}, l_i \\text{ is used in } p\\}.\\]"}, {"title": "3.3 Final re-training", "content": "To avoid training instability caused by the changing data distribution during self-play, we re-train the final model checkpoint from the base model (before the SFT stage) on a combination of the SFT dataset and all the correct proofs generated during the self-play training whose corresponding statement/conjecture has an empirical pass rate no larger than 1/4. For every statement/conjecture, we randomly keep at most 16 distinct proofs to speedup the training."}, {"title": "4 Experiments", "content": "This section presents our implementation details of STP, the results of Isabelle and Lean experiments, and the ablation studies, followed by examples of generated conjectures."}, {"title": "4.1 Implementation details", "content": "Training datasets. We use the de-duplicated Lean Workbook [Ying et al., 2024], which contains around 89K Lean4 statements, as the given dataset without proofs (see Appendix A.4 for details). For the Isabelle experiments, we translate the Lean4 statements to Isabelle using the DeepSeek V2.5 with few-shot prompting.\nThe SFT dataset for the Isabelle experiments is extracted from AFP3 and Isabelle built-in files such as HOL. For the Lean experiments, we first sample 32 proofs per statement in LeanWorkbook since our base model, DeepSeek-Prover-V1.5-SFT, is already trained on it, and combine the correct proofs with examples extracted from the proof library Mathlib4 [mathlib Community, 2020] as the SFT dataset."}, {"title": "4.2 Results with Lean", "content": "For the Lean experiments, we choose DeepSeek-Prover-V1.5-SFT as our base model, which was trained on proofs collected by expert iteration on a combination of public, such as LeanWorkbook, miniF2F-valid [Zheng et al., 2021], and ProofNet-valid [Azerbayev et al., 2023a], and other proprietary datasets. We ran 24 iterations of STP and gen-erated 2M conjectures, 120M proofs, and 19.8B tokens in total. We use the cumulative pass rate, defined by the fraction of statements proved during the entire training, as the main metric for training progress. \nTable 1 also compares STP with tree search methods such as InternLM2.5-StepProver [Wu et al., 2024], which use LLMs to generate single proof steps conditioned on the current verifier's proof state and then find a complete proof by best first search or MCTS. The sample budget of these methods are not directly comparable with whole-proof generation methods because (a) the number of steps in a generated proof varies significantly, (b) LLMs in tree search methods need to process additional tokens related to the verifier's proof state, and (c) methods like InternLM2.5-StepProver [Wu et al., 2024] require an additional LLM as the value function. Moreover, it's conceivable that tree search methods can also be used with STP, so essentially these are orthogonal methods. Nonetheless, we compute the total number of proof steps per statement generated by STP as an proxy for the total number of LLM output tokens for STP and tree search methods, ignoring the additional compute required by tree search methods to process the verifier's proof state and query the value function. Results in Table 1 indicate that STP also outperforms prior tree search methods with similar (estimated) inference-time budgets."}, {"title": "4.3 Results with Isabelle", "content": "For Isabelle experiments, we start with the Llemma-7b [Azerbayev et al., 2023b], math-focused model, and run 58 iterations of STP to study its scalability. We take several checkpoints during STP training and then switch to the expert iteration and parallel sampling baselines to study the scalability of the algorithm from checkpoints with various"}, {"title": "4.4 Ablation study", "content": "Generated conjectures provide denser training signals. Fig. 4 (Right) shows the histogram of empirical pass rates of the generated conjectures and the unproved statements in LeanWorkbook using a checkpoint in the Isabelle experiment. Only 131 out of 2.5M generated proofs for the 79K unproved statements are correct. As a result, finetuning the model on correct proofs has almost no effect, and thus expert iteration plateaus. In contrast, generated conjectures by STP offer has higher pass rates and thus more training signals, leading to better scaling.\nRe-training with generated conjectures still helps downstream performance. One may hypothesis that the self-play algorithm and generated conjectures only help improve the pass rate on LeanWorkBook. It turns out that in the final re-training stage, it is still beneficial to re-train with the generated conjectures in addition to the successfully proved statements in LeanWorkBook even for performance on miniF2F-test and ProofNet-test-it leads to about 1% performance gain (for pass@128) than re-training only on the latter (See Appendx B.1)."}, {"title": "4.5 Examples of generated conjectures", "content": "In this section, we list three manually selected examples of the generated conjectures at the last iteration of the Lean experiment to demonstrate the quality of generated conjectures.\nExample 1. The generated conjecture says (1 + x)^(2n) > 1 + x^n when n \u2265 1 is an integer and x \u2208 [0, 1]:\n\\['''theorem lean_workbook_36081' (x : R) (hx : 0 < x < x \u2264 1) : \u2200 n:N, n \u2265 1 \u2192 (1 + x)^(2*n) \u2265 1 + x^n\n\n'''\\]\nThe seed statement says 1 + x^2 < (1 + x)^2 when x \u2208 [0,1]:\n\\['''theorem lean_workbook_36081 (x : R) (hx : 0 < x < x \u2264 1) : 1 + x^2 < (1 + x)^2\n\n'''\\]\nExample 2. The generated conjecture says (x \u2013 1) mod (x \u2013 1) < 1 if x, n are integers:\n\\['''theorem lean_workbook_54038' (x : N) (n : N) (hn : 1 < n) : (x^n - 1) % (x - 1) < 1\n\n'''\\]\nThe seed statement says n \u2013 1 divides nk \u2013 1:\n\\['''theorem lean_workbook_54038 (n : N) (k : N) (hn : 2 \u2264 n) : n - 1 \\| n^k - 1\n\n'''\\]\nExample 3. The generated conjecture says \\(\\sum_{i>o}((1/4)^i \\cdot a) = \\frac{a}{1-\\frac{1}{4}}\\) if 0 < a < 1.\n\\['''theorem lean_workbook_plus_46203' (a : R) (ha : 0 < a \u2227 a \u2264 1) : \u2211' (i : N), (1 / 4)^i * a = a/(1-1/4)\n\n'''\\]\nThe seed statement is special case where a = \u221a5/3:\n\\['''theorem lean_workbook_plus_46203 : \u2211' k : N, (1 / 4)^k * (Real.sqrt 5 / 4) = (Real.sqrt 5 / 3)\n\n'''\\]"}, {"title": "5 Conclusion", "content": "This paper designs Self-play Theorem Prover (STP) that simultaneously has two roles, conjecturer and prover. By providing training signals to each other, STP goes beyond the statements in the given dataset and its performance continuously improves. Our final model significantly outperforms Deepseek-Prover-V1.5 series and achieves state-of-the-art performance among whole-proof generation methods on common formal proof benchmarks."}, {"title": "A Additional Implementation Details", "content": "In this section, we list the missing implementation details."}, {"title": "A.1 Examples of inputs and outputs of our model", "content": "Here we present some concrete examples to demonstrate the input and output formats of our model.\nExamples of the conjecturer. In the following, we show examples of the conjecturer's inputs and outputs. Note that <lemma>,<easy theorem>,<hard theorem>,</hard theorem> are the formatting tokens, and the system prompt is the first three lines in the input examples.\nInput:\nOutput:\nIn the following example, the lemma is replaced by a trivial statement, and the model can generate conjectures without focusing on any particular direction (see Section 4.1).\nInput:\nOutput:"}, {"title": "A.2 Pseudo-code for selecting the conjecturer's inputs", "content": "In the following, we present the pseudo-code for selecting the conjecturing imports. Recall that the input for the conjecturer consists of a statement, its proof, and a lemma used in the proof (c.f., Section 3.1). In Step 1, we construct the prompts by taking the correct proofs to statements in the given dataset, and extract the lemmas used in the proof by the formal verifiers. We also allow the model to propose conjectures without focusing on any particular lemma, which is implemented by replacing the lemma statement with a fixed trivial statement in the prompt (see Appendix A.1 for concrete examples). Finally, we de-duplicate the inputs by the (statement, lemma) pair. After generating the conjectures, we randomly select a subset whose size does not exceed the number of remaining unproved statements in the given dataset, so that the prover's sample budget is distributed equally between the conjectures and the statements.\nWe run two heuristic methods to ensure the diversity of the inputs. First, we make sure that each lemma l appears at most 0.1n times in the inputs because we observe that some lemmas (e.g., sq_nonneg, mul_self_nonneg) are much more likely to be included. Second, we make sure that every statement-lemma pair only appear at most once in the prompt, even if there are multiple correct proofs.\nAlg. 1 shows the complete pseudo-code for selecting the conjecturer's inputs."}, {"title": "A.3 Pseudo-code for preparing the conjecturer dataset.", "content": "The pseudo-code for preparing the conjecturer dataset is shown in Alg. 2. The motivations and explanations of each step in Alg. 2 can be found in Section 3.2."}, {"title": "A.4 Pre-processing LeanWorkbook", "content": "LeanWorkbook is a dataset that contains statements translated from natural language math statements (a.k.a., auto-formalization). The original dataset contains 140K (natural language statement, formal statement) pairs.\nWe de-duplicate the Lean Workbook dataset by keeping only one formal statements per natural language statement. After de-duplication, we get 89,221 formal Lean4 statements as our existing dataset w/o proofs for Lean experiments.\nFor the Isabelle experiments, we translate the Lean4 statements to Isabelle using DeepSeek-V2.5 API with few-shot examples. The prompt to the model is listed below."}, {"title": "A.5 Re-weighting the conjecturing dataset", "content": "In this section, we describe the motivations and implementation details of the re-weighting method for the conjecturing dataset."}, {"title": "Motivation", "content": "In our early experiments, we observe that the generated conjectures tend to have mode collapse issue after several iterations of self-play training. For example, the generated conjectures are mostly about algebraic manip-ulations even when the seed statements contain questions about, for example, number theory. This is partly because the Lean Workbook dataset contains a significant portion of inequality questions.\nTherefore, in addition to the particular conjecturing format where we require that the proof of the conjecture must use the lemma given in the input, we also re-weight the conjecturing examples at every iteration. Intuitively, if there is a distance function that can separate statements of different topics, the Wasserstein projection of the conjectures will have a similar distribution of topics, and therefore alleviates the mode collapsing issue."}, {"title": "Cost function", "content": "We compute the cost d(x, y) of matching conjecture x to a statement y by the negative of the cosine similarity between their embeddings, and the embedding is computed by the last hidden layer of the current model averaged over the sequence dimension. Since our model is trained to generate proofs of conjectures and statements, we expect that statements with similar proof techniques tend to have similar embeddings, and therefore smaller cost for the matching."}, {"title": "Algorithm", "content": "On the high level, our method computes a re-weighting of the generated conjectures that minimizes its Wasserstein distance to the unproved statements in the given dataset. Abstractly speaking, let X be the set of generated conjectures, and Q the set of unproved statements. Let d(x, y) be the distance between a conjecture x and a statement y. Then, the optimization problem can be written as\n\\[\\text{argmin}_{P:P \\text{ is a valid distribution, supp(P)}\\subset X} W(P,Q),\\]\nwhere W(P, Q) is the Wasserstein distance between P and Q (with little abuse of notation, we use Q to represent the uniform distribution over the unproved statements). The Wasserstein distance W(P, Q) is defined by the following optimal transportation problem where \u00b5 is a matching between the distribution P and Q:\n\\[W(P,Q) = \\text{min}_{\\mu} \\sum_{x \\in \\text{supp}(P), y \\in \\text{supp}(Q)} \\mu(x,y)d(x,y)\\]\ns.t.\n\\[\\sum_{y \\in \\text{supp}(Q)} \\mu(x,y) = P(x),\\]\n\\[\\sum_{x \\in \\text{supp}(P)} \\mu(x,y) = Q(y),\\]\n\\[\\mu(x,y) \\geq 0, \\forall x,y.\\]\nCombining the equations above, the re-weighting distribution P can be computed by\n\\['''\\text{argmin}_{P: \\text{supp}(P) \\subset X} \\text{min}_{\\mu} \\sum_{x \\in \\text{supp}(P), y \\in \\text{supp}(Q)} \\mu(x,y)d(x,y) \\\\\ns.t. \\sum_{y \\in \\text{supp}(Q)} \\mu(x,y) = P(x), \\\\\n\\sum_{x \\in \\text{supp}(P)} \\mu(x,y) = Q(y), \\\\\n\\mu(x,y) \\geq 0, \\forall x,y,\\\\\nP(x) \\geq 0, \\forall x,\\\\\n\\sum_{x \\in X} P(x) = 1, '''\\]\nwhere the last two constraint ensures that P is a valid distribution. Equivalently, we get the following program,\n\\['''\\text{argmin}_{P: \\text{supp}(P) \\subset X} \\text{min}_{\\mu} \\sum_{x \\in \\text{supp}(P), y \\in \\text{supp}(Q)} \\mu(x,y)d(x,y)\n'''\\]"}, {"title": "A.6 Implementation details for expert iteration", "content": "In this section, we describe two different implementations of expert iteration and compare their performance.\nVanilla expert iteration. For vanilla expert iteration, we only sample proofs to the unproved statements in the given dataset. The LLM training dataset consists of all the correct proofs generated in this and previous iterations, and in each iteration, the model is trained from the base model."}, {"title": "Optimized expert iteration", "content": "The most significant issue of vanilla expert iteration is the limited correct proofs gen-erated in each iteration. As a result, even though the model is re-trained at every iteration, the difference between two models in consecutive iterations are limited.\nTherefore, in our optimized implementation of expert iteration, we generate proofs to all statements in the given dataset, regardless of whether they are previously proved or not. Then, to construct the LLM training dataset, we randomly choose at most 16 proofs per statement (so that the model does not overfit to the easy problems with many correct proofs). Note that this implementation requires slightly more sample budget per iteration. However, since the pass rate on the given dataset is low (less than 30% even for our best model), this difference is not significant.\nIn Fig. 5 (Left), we plot the cumulative pass rate of two implementations of expert iteration, STP and parallel sampling. STP outperforms both implementations of expert iteration, and the optimized implementation of expert iteration is better than the vanilla implementation.\nFor the figures of Isabelle experiments, we always use the optimizes implementation of expert iteration. For Fig. 2, we use the vanilla implementation."}, {"title": "A.7 Additional details for interacting with the Isabelle verifier", "content": "For the Isabelle experiments, we have an additional filter for the conjectures - if the generated conjecture is equivalent to the statement in the prompt (tested by solve_direct in Isabelle), we consider it invalid.\nWe disallow the tactics sledgehammer, mason, smt, metis, sos by invalidating proofs that contain any of these sub-strings. However, following the implementation of Jiang et al. [2022b], we still use the keyword 'sledge-hammer' to replace the following simple tactics\n[by auto, by simp, by blast, by fastforce, by force, by eval, by presburger,\nby arith, by linarith, by (auto simp: field_simps)].\nDuring proof verification, we try these tactics sequentially to replace the keyword 'sledgehammer'. If any of the tactics succeed, we proceed to the remaining proof steps. Otherwise we flag the proof incorrect."}, {"title": "A.8 Additional details for interacting with the Lean4 verifier", "content": "During the self-play training, we use the same imports as the miniF2F Lean4 project instead importing the entire Mathlib to optimize the memory efficiency. This is because we do not have access to an additional CPU cluster for proof verification, and the available CPU memory in TPU-v4 VMs is limited."}, {"title": "A.9 Compute resources", "content": "Our experiments are primarily done on TPU-v4 VMs with 32 nodes. Each node contains 4 TPU chips (8 TPU cores), 240 CPU cores, and 400G memory. We use vLLM [Kwon et al., 2023] to generate LLM outputs, and Levanters to train the LLM. In both STP and expert iteration, since the generated proofs are heavily filtered (based on the correctness, elegancy, trivialness, etc.) when constructing the training dataset, the LLM training only takes less than 25% of the wall-clock time for TPU compute, and generating proofs takes the rest 75%. In the Lean experiments, running the verifier requires 2x wall-clock time than the TPU compute. Therefore, even though we parallelize the CPU and TPU compute, the TPU is still idle for more than 50% of the wall-clock time."}, {"title": "B Additional Experiment Results", "content": "In this section we show the additional experiment results with both Lean and Isabelle formal verifier."}, {"title": "B.1 Additional results with Lean", "content": "In Table 3, we compare the performance of our method with prior works on PutnamBench. Note that DSP [Jiang et al., 2022a] uses Isabelle verifier where PutnamBench only has 640 statements. Our model STP achieves state-of-the-art performance by solving 9 out of 644 problems.\nTable 2 compares the model obtained by final re-training with and without the proofs of generated conjectures, as discussed in the ablation study section (Section 4.4). The results show that it is still beneficial to re-train with the generated conjectures in addition to the successfully proved statements in LeanWorkBook even for performance on miniF2F-test and ProofNet-tess, which it leads to about 1% performance gain (for pass@128)."}, {"title": "B.2 Additional results with Isabelle", "content": "In Fig. 5 (Right), we plot the pass rates of STP and baseline methods on LeanWorkbook starting from iteration 0. The red crosses shows the points where we refresh the training process as described in Section 4.1. Our models are tested with PutnamBench [Tsoukalas et al., 2024], commit d49896f."}]}