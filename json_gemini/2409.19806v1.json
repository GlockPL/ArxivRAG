{"title": "PALM: Few-Shot Prompt Learning for Audio Language Models", "authors": ["Asif Hanif", "Maha Tufail Agro", "Mohammad Areeb Qazi", "Hanan Aldarmaki", "Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI)"], "abstract": "Audio-Language Models (ALMs) have recently\nachieved remarkable success in zero-shot audio\nrecognition tasks, which match features of au-\ndio waveforms with class-specific text prompt\nfeatures, inspired by advancements in Vision-\nLanguage Models (VLMs). Given the sensitiv-\nity of zero-shot performance to the choice of\nhand-crafted text prompts, many prompt learn-\ning techniques have been developed for VLMs.\nWe explore the efficacy of these approaches in\nALMs and propose a novel method, Prompt\nLearning in Audio Language Models (PALM),\nwhich optimizes the feature space of the text\nencoder branch. Unlike existing methods that\nwork in the input space, our approach results\nin greater training efficiency. We demonstrate\nthe effectiveness of our approach on 11 audio\nrecognition datasets, encompassing a variety\nof speech-processing tasks, and compare the\nresults with three baselines in a few-shot learn-\ning setup. Our method is either on par with or\noutperforms other approaches while being com-\nputationally less demanding.", "sections": [{"title": "1 Introduction", "content": "Inspired by the success of Vision-Language Mod-\nels (VLMs) (Zhang et al., 2024), Audio-Language\nModels (ALMs) have recently emerged, achieving\nstate-of-the-art performance on various zero-shot\naudio recognition tasks (Elizalde et al., 2023;\nDeshmukh et al., 2023; Kong et al., 2024; Das\net al., 2024). In zero-shot audio recognition,\nfeatures of the audio waveform are matched with\nfeatures of text prompts representing each class,\nand the highest matching class is assigned to the\naudio waveform. Zero-shot audio recognition\noffers significant advantages by eliminating the\nneed for extensive labeled datasets and allowing for\nthe recognition of new classes without additional\ntraining. This approach reduces training times\nand data annotation costs, leading to substantial\nsavings in computational resources.\nThe choice of text prompt is crucial for\npre-trained vision-language and audio-language\nmodels, but it becomes a drawback for zero-shot\nrecognition due to the requirement of hand-crafted\nprompts. This manual prompt-engineering can\nresult in performance variations (Zhou et al.,\n2022b,a). We confirm this observation, previously\nnoted in VLMs, within the context of ALMs (refer\nto Figure 2). To automate the learning of text\nprompts, various approaches have been introduced\nfor prompt learning in VLMs (Gu et al., 2023).\nThe domain of prompt learning in ALMs\nremains under-explored, lacking comprehensive\nstudies to evaluate the efficacy of prompt learning\ntechniques within this context. To bridge this\nresearch gap, we adapt prompt learning techniques\ndeveloped for VLMs and apply them to the domain"}, {"title": "2 Related Work", "content": "Prompt engineering involves adding task-specific\nhints, called prompts, to a large pre-trained model\nto adapt it to new tasks. Recently, significant ad-\nvancements have been made in prompt learning,\nparticularly in the fields of language and vision.\nBelow, we outline the recent developments in lan-\nguage, vision, and audio domains."}, {"title": "2.1 Audio Language Models (ALMs)", "content": "Taking inspiration from multimodal models like\nCLIP (Radford et al., 2021) in the vision domain,\nContrastive Language-Audio Pretraining (CLAP)\n(Elizalde et al., 2023) stands out as the first-of-its-\nkind audio language model. It connects natural\nlanguage and audio through dual encoders and con-\ntrastive learning, aligning audio and text descrip-\ntions in a shared multimodal space. Furthermore,\nCLAP introduces zero-shot prediction capabilities,\nremoving the necessity for training with predefined\nclass labels and allowing flexible class prediction\nduring inference.\nPENGI (Deshmukh et al., 2023), another audio\nlanguage Model, utilizes transfer learning by treat-\ning all audio tasks as text-generation tasks. It takes\naudio recordings and text inputs, generating free-\nform text as output. The input audio is represented\nby continuous embeddings from an audio encoder,"}, {"title": "2.2 Prompt Learning in Language Models", "content": "Extensive research has been conducted on prompt\nlearning techniques in natural language process-\ning. Pioneering work by (Brown et al., 2020)\nfocused on optimization strategies for zero-shot\nand few-shot learning scenarios, demonstrating\nthat prompts can enable generative models to per-\nform well across various tasks without extensive\ntask-specific training. Their method leverages the\nmodel's pre-trained knowledge and prompt-guided\ninteractions to achieve strong performance on new\ntasks. They also introduced GPT-3, which trans-\nformed the field of prompt learning in natural lan-\nguage processing. Petroni et al. (2019) integrated\ncontextual cues and constraints within prompts to\nguide model behavior, embedding task-specific in-\nformation to enhance output precision and rele-\nvance. Their technique improves interpretability\nand task-oriented performance by providing con-\ntextual guidance during inference."}, {"title": "2.3 Prompt Learning in Vision-Language\nModels", "content": "Inspired by advancements in prompt-based work\nin language models, several studies have been con-\nducted to adapt these methods to VLMs (Gu et al.,\n2023). Some focus exclusively on the language\ncomponent, such as COOP (Context Optimization)\n(Zhou et al., 2022a). In contrast, others integrate\ninsights from language and visual components,\nas seen in COCOOP (Conditional Context Opti-\nmization) (Zhou et al., 2022a). COOP enhances\nCLIP model's few-shot transfer learning capability\nby optimizing a continuous set of prompt vectors"}, {"title": "2.4 Prompt Learning in Audio-Language\nModels", "content": "Prompt learning with audio-language models is rel-\natively understudied. Previous work has explored\nenhancing language models with speech recog-\nnition by conditioning them on variable-length\naudio embeddings using a conformer-based au-\ndio encoder (Fathullah et al., 2024). Deshmukh\net al. (2024) propose a test-time domain adapta-\ntion method for Contrastive ALMs, using unla-\nbeled audio to adjust the model to new domains via\na domain vector, consistent predictions, and self-\nentropy fine-tuning, improving on traditional Test-\nTime Training. Li et al. (2024) introduce PT-Text,\nan audio-free prompt tuning scheme that optimizes\nprompt tokens from text, regularizing the model\nto avoid overfitting by training with captions and\nusing a multi-grained strategy to enhance perfor-\nmance. Despite these advancements, more research\nis needed to fully understand and exploit prompt\nlearning in audio-language models."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Audio-Language Model (ALM)", "content": "We demonstrate the efficiency of prompt learning\nin enhancing zero-shot performance using a\nstate-of-the-art audio-language model PENGI\n(Deshmukh et al., 2023). Our approach is\napplicable to all audio-language models that have\naligned audio and text encoders.\nPENGI takes an audio waveform and a text\nprompt as input and generates free-form text. It\nconsists of three branches. The first branch is\nan audio encoder that maps the audio waveform\nto an embedding space. The second branch is a\ntext encoder that transforms the input text into\nthe same embedding space. These embeddings\nare then concatenated to form an input prefix for\nthe third branch, a causal language model that\ngenerates tokens autoregressively, conditioned\non both the audio and text inputs. PENGI can be\nused for various audio-conditioned tasks, such\nas text completion, classification, audio caption\ngeneration, and question-answering (Deshmukh"}, {"title": "Zero-Shot Inference", "content": "Although PENGI is\nmultimodal-to-text generation model, however,\nwe use its audio and text encoder branches for\nzero-shot audio recognition. This is accomplished\nby comparing the embedding of the audio wave-\nform (extracted from the audio encoder) with the\nembeddings of text prompts for different classes\n(extracted from the text encoder). An overview of\nzero-shot inference is given in Figure 3(a). The\nzero-shot setup used by (Deshmukh et al., 2023)\ndiffers from ours, as they employ the model's\nfree-form text output for zero-shot inference.\nFormally, we denote the pre-trained ALM\nas $f_\\theta = \\{f_a, f_t\\}$, whereas $f_a$ and $f_t$ are audio\nand text encoders, respectively and $\\theta$ represents\nthe combined weights of both encoders. For classi-\nfication in zero-shot scenario, an audio waveform\nx is first passed to the audio encoder $f_a$ to produce\na d-dimensional feature vector $f_a(x) \\in R^d$. In\nparallel, text prompts representing each class\nlabel $y_i \\in \\{Y_1, Y_2 . . ., Y_c\\}$ are encapsulated within\nclass-specific handcrafted text templates, such as\n$t_i = \u201cAn audio recording of \\{CLASS y_i\\}\u201d$,\nwhere c is the total number of classes. Each text\nprompt, represented as $t_i$, is processed through\nthe text encoder $f_t$, resulting in a feature vector\n$f_t(t_i) \\in R^d$. The relationship between the audio\nwaveform x and a class-specific text prompt $t_i$ is\nquantified by computing the cosine similarity be-\ntween their corresponding feature vectors, denoted\nas $sim(f_a(x), f_t(t_i))$. The class with the highest\nsimilarity score is then assigned as the label $\\hat{y}$ for\nthe audio waveform, i.e."}, {"title": "3.2 PALM: Prompt Learning in ALM", "content": "In our proposed method, we do not use hand-\ncrafted prompts; instead, we simply use class\nnames as the input to the text encoder i.e. $t_i$ =\n\u201c\\{CLASS $y_i$\\}\\\u201d. Moreover, unlike COOP (Zhou\net al., 2022b), which learns the context of input text\nprompts in the token embedding space (see Figure\n3(b)), we learn the context in the feature space of\nprompts. Specifically, after obtaining the feature\nvector of the ith class text prompt via the text en-\ncoder, i.e., $f_t (t_i) \\in R^d$, we add a learnable vector\n$z_i \\in R^d$ to it to get the updated text feature vector\nas follows:\n$f'(t_i) = (1 \u2013 \\lambda_i)\\cdot f_t(t_i) + \\lambda_i\\cdot z_i$                                                   (2)\nwhere $\\lambda_i \\in [0, 1]$ is a learnable parameter that de-\ntermines the contributions of both vectors. Assum-\ning t = {$t_1, t_2, . . ., t_c$} denotes text prompts of all\nclasses, the raw/un-normalized prediction scores\n(logits), denoted as $f(x, t) \\in R^c$, for an audio\nwaveform (x) are obtained as follows:\n$f(x,t) = \\{sim(f_a(x), f'(t_i))\\}_\\{i=1\\}^\\{c\\}$\nwhere sim() is cosine-similarity function and\nc is the number of classes. $f_a(x)$ is the feature\nvector from the audio encoder, and $f'(t_i)$ is the\nupdated text feature vector (Equation 2) of ith class.\nWe optimize the following objective func-\ntion to learn feature-space context embeddings\n$Z = \\{z_1, z_2,..., z_c\\}$ and their corresponding\ncontributions $\u039b = \\{\\lambda_1, \\lambda_2, ..., \\lambda_c\\}$,\n$\\min_\\{Z, \u039b\\} \\sum_\\{(x, y) \\in D\\} L(f(x, t), y)$,                                                          (3)\nwhere D = {${x_i, y_i\\}_1^N$} is training dataset con-\nsisting of N audio-class pairs and L(\u00b7) denotes\ncross-entropy loss. We use few-shot setting during\ntraining, meaning that a fixed number of samples\n(e.g., 16) are randomly selected from each class\nin the training dataset. While optimizing objective\nin Equation 3, weights of both encoders {$f_a, f_t$}\nare kept in frozen state. The number of learnable\nparameters in our proposed method is c + (c \u00d7 d).\nAfter learning the parameters, we use Equation 4\nfor audio classification during inference stage.\n$\\hat{y} = \\arg\\max_\\{i \\in \\{1, 2, ..., c\\}\\} sim(f_a(x), f'(t_i))$                                                   (4)\nAn overview of our proposed approach can be\nfound in Figure 3(c)."}, {"title": "3.3 Difference with COOP and COCOOP", "content": "COOP (Zhou et al., 2022b) and COCOOP (Zhou\net al., 2022a) were originally introduced for vision-\nlanguage model; however, we adapted them for\naudio-language model (replacing the vision en-\ncoder branch with audio encoder branch) and pre-\nsented it as baseline methods. Both of these base-\nlines and our method aim to enhance zero-shot"}, {"title": "4 Experiments and Results", "content": ""}, {"title": "4.1 Datasets", "content": "We evaluate our methodology using datasets\nfrom various speech-processing tasks: instru-\nment classification, sound event classification,\nemotion recognition, vocal sound classification,\nsurveillance sound event classification, acoustic\nscene classification, and music analysis. Brief\ninformation of each dataset can be found in\nTable 1. For instrument classification, we use\nBeijing-Opera (Tian et al., 2014) dataset, which\nincludes audio examples of strokes from four\npercussion instrument classes used in Beijing\nOpera, and NS-Instruments (Engel et al., 2017)\ndataset, which consists of one-shot instrumental\nnotes with unique pitches, timbres, and envelopes,\nspanning ten classes. For sound event classifica-\ntion, we utilize three datasets: ESC50 (Piczak),\ncontaining environmental recordings across 50\nclasses; ESC50-Actions (Piczak), a subset with\n10 classes of non-speech human sounds; and\nUrbanSound8K (Salamon et al., 2014), with\nurban noise excerpts from 10 classes. Emotion\nrecognition is assessed with the CREMA-D (Cao\net al., 2014) and RAVDESS (Livingstone and\nRusso, 2018) datasets, covering 6 and 8 emotion\nclasses respectively, performed by actors. We\nemploy the VocalSound (Gong et al., 2021) dataset\nfor vocal sound classification, which includes 6\nclasses of human non-speech vocalizations. For\nsurveillance sound event classification, we use\nSESA (Spadini, 2019) dataset, which has 4 classes.\nAcoustic scene classification uses the TUT2017\n(Heittola et al., 2017) dataset, containing samples\nfrom 15 acoustic scenes. For music analysis, the\nGT-Music-Genre (Sturm, 2012) dataset is used,\nwhich includes 10 classes of music genres.\nWe adhere to the official train-test or multi-fold\nsplits for all datasets. We conduct cross-validation\nexperiments on datasets having multi-fold splits\nsuch as Beijing-Opera, ESC50, ESC50-Actions,\nUrbanSound8K, and TUT2017, and report the av-\nerage scores. We have publicly released all infor-\nmation regarding dataset preprocessing to ensure\nreproducibility of results."}, {"title": "4.2 Baseline Methods", "content": "For baselines, we consider PENGI model (Desh-\nmukh et al., 2023) (in ZERO-SHOT setup), COOP\n(Zhou et al., 2022b) and COCOOP (Zhou et al.,\n2022a). COOP and COCOOP are prompt learn-\ning approaches, originally introduced for VLMs.\nBoth of these approaches remove the requirement\nof providing handcrafted text prompts and they\noptimize the input token embedding space of text\nencoder to enhance accuracy. The only difference\nbetween COOP and COCOOP is that the latter\nincorporates a feedback loop from the output of\nthe audio encoder to the input of the text encoder.\nWe adapt these two approaches for audio-language\nmodels by replacing the vision encoder with an au-\ndio encoder and present them as baselines for our\nproposed method. Why PENGI, COOP and CO-\nCOOP as baselines? PENGI is an state-of-the-art\nALM that has demonstrated comprehensive evalua-\ntion across 21 downstream audio tasks, making it a\nrobust benchmark for comparison. COOP and CO-\nCOOP, on the other hand, are pioneering works on\nprompt learning in the domain of vision-language\nmodels, offering foundational techniques and in-\nsights that are highly relevant for our study."}, {"title": "4.3 Experimental Setup", "content": "We use pre-trained PENGI (Deshmukh et al., 2023)\nas the audio-language model for all methods. For\nall methods, except ZERO-SHOT, we conduct ex-\nperiments for 50 epochs. Following the few-shot\nevaluation setup, we use 16 randomly selected"}, {"title": "4.5 Ablative Analysis", "content": "For ablative analysis, we first show the impor-\ntance of incorporating learnable context embed-\ndings in text features. In Figure 4, we compare the\nperformance of our method with and without the\nlearnable context embeddings. The results clearly\ndemonstrate that removing the learnable context\nembeddings leads to a significant drop in perfor-\nmance, underscoring their crucial role in enhancing\nthe model's accuracy. This highlights the effec-\ntiveness of our approach in optimizing the feature\nspace of the text encoder.\nWe also show the impact of jointly optimizing\nthe input space and output space of the text encoder\nby applying PALM on top of COOP and COCOOP\nin Table 4. The results indicate that joint optimiza-\ntion results in slight performance degradation and\nis not beneficial. Moreover, we also show linear\nprobing results in Table 4. Since our approach is\nbased on few-shot setup, therefore, we show impact\nof number of shots (number of training samples per\nclass) on the PALM's performance across eight\ndatasets in Figure 5. As the number of shots in the\ntraining dataset increases, the performance of the\nmodel tends to improve."}, {"title": "5 Conclusion", "content": "In this study, we investigate the application of\nprompt learning techniques, originally developed\nfor vision-language models (VLMs), in the con-\ntext of audio-language models (ALMs). We in-\ntroduce PALM, a novel method that optimizes the\nfeature space of the text encoder branch, enhancing\ntraining efficiency compared to existing methods"}, {"title": "Limitations", "content": "Although we are the first, to the best of our\nknowledge, to integrate prompt learning techniques\noriginally designed for Vision-Language Models\n(VLMs) into Audio-Language Models (ALMs) and\npropose a new method, several aspects still need\nto be addressed. One critical aspect is to analyze\nprompt learning performance for domain gener-\nalization. This involves evaluating how well the\nprompts adapt to new, unseen domains and tasks,\nensuring robustness and effectiveness across vari-\nous applications. The second aspect is to analyze\nprompt learning performance under different types\nof perturbations in audio data to check its resilience\nagainst various types of noise. This analysis is\nessential for understanding the robustness of the\nmodels in real-world scenarios where audio data\ncan be contaminated with background noise, dis-\ntortions, and other audio artifacts. Thirdly, while\nour study shows results on audio classification, it\nis yet to be seen how prompt learning helps in\nother audio tasks such as speech recognition, audio\nsegmentation, and information retrieval. Investigat-\ning the effectiveness of prompt learning across a\nbroader range of audio tasks will provide a more\ncomprehensive understanding of its potential and\nlimitations."}]}