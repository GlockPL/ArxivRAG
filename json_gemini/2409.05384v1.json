{"title": "Look One and More: Distilling Hybrid Order Relational Knowledge for\nCross-Resolution Image Recognition", "authors": ["Shiming Ge", "Kangkai Zhang", "Haolin Liu", "Yingying Hua", "Shengwei Zhao", "Xin Jin", "Hao Wen"], "abstract": "In spite of great success in many image recognition tasks\nachieved by recent deep models, directly applying them to\nrecognize low-resolution images may suffer from low accu-\nracy due to the missing of informative details during res-\nolution degradation. However, these images are still recog-\nnizable for subjects who are familiar with the corresponding\nhigh-resolution ones. Inspired by that, we propose a teacher-\nstudent learning approach to facilitate low-resolution image\nrecognition via hybrid order relational knowledge distillation.\nThe approach refers to three streams: the teacher stream is\npretrained to recognize high-resolution images in high accu-\nracy, the student stream is learned to identify low-resolution\nimages by mimicking the teacher's behaviors, and the extra\nassistant stream is introduced as bridge to help knowledge\ntransfer across the teacher to the student. To extract sufficient\nknowledge for reducing the loss in accuracy, the learning of\nstudent is supervised with multiple losses, which preserves\nthe similarities in various order relational structures. In this\nway, the capability of recovering missing details of famil-\niar low-resolution images can be effectively enhanced, lead-\ning to a better knowledge transfer. Extensive experiments on\nmetric learning, low-resolution image classification and low-\nresolution face recognition tasks show the effectiveness of our\napproach, while taking reduced models.", "sections": [{"title": "1 Introduction", "content": "With the rapid development of deep learning, recent deep\nmodels have proven success in many image recognition\ntasks. For example, the ResNet model (He et al. 2016)\nhas achieved a high top-5 accuracy of 96.43% on Ima-\ngeNet (Deng et al. 2009) object classification task, while\nVGGFace2 (Cao et al. 2018) model gived a 99.63% accu-\nracy in face verification task on LFW benchmark (Huang et\nal. 2007) and ArcFace (Deng et al. 2019) reached 98.35%\nrank-1 accuracy in face identification task on the challeng-\ning MegaFace benchmark (Kemelmacher Shlizerman et al.\n2016). These successes may arise from that deep models\nwith massive parameters provide an effective way to ex-\ntract rich knowledge from large-scale data. However, a sharp\ndrop in accuracy may happen when directly deploying these\nmodels on recognizing low-resolution images that are dif-\nficult to annotate but widely encountered in real-world ap-\nplications, such as surveillance faces in the wild (Li et al.\n2019) and thumbnail images in the Internet. To meet real-\nworld requirements, it is necessary to explore an economic\nyet feasible solution that can address a key challenge: how\nto convert an existing complex image recognition model into\nan efficient one that still works effectively on low-resolution\nimages?\nAs shown in Fig. 1, low-resolution images can be rec-\nognizable for subjects that are familiar with the corre-\nsponding high-resolution images. Intuitively, it is helpful\nto improve the recognition capacity of a subject by show-\ning more similar or different images as well as more in-\nformation about their relationships. Thus, the knowledge\nfrom high-resolution images can help the extraction of dis-\ncriminative features for effective recognition. Inspired by\nthat, many models have been proposed to recognize low-\nresolution images, and can be grouped into two categories:\nreconstruction-based and representation-based models."}, {"title": "2 Related Works", "content": "Reconstruction-based models aim at reconstructing the\nhigh-resolution images before recognition (Luan, Xi, and\nLiu 2017; Kong et al. 2019). These models generally exhibit\nimpressive results in recognizing the reconstructed high-\nresolution images, but the super-resolution operation often\ntakes additional computational cost. Unlike reconstruction-\nbased models, representation-based models try to extract\ndiscriminative features from low-resolution images directly\nby using various external contexts. For example, (Biswas,\nBowyer, and Flynn 2012) proposed mapping low-resolution\nface images into Euclidean space, and then approximat-\ning high-resolution face images through distance dimension.\nIn (Ge et al. 2019), the authors proposed graph-based opti-\nmization algorithm that can extract the most discriminative\nfacial features from existing face models, which can super-\nvise the training process of low-resolution face models. Gen-\nerally speaking, the most important process in these models\nis transferring the knowledge from high-resolution images to\nlow-resolution ones. However, it needs to carefully address\na key issue in this process: how to represent the knowledge\neffectively for distilling more information cues and transfer\nit from high-resolution domain to low-resolution one.\nInspired by this fact, we propose a hybrid order relational\nknowledge distillation approach for low-resolution image\nrecognition. As shown in Fig. 2, the approach consists of a\nteacher stream, a student stream and an assistant stream. The\nteacher stream is initialized with pretrained high-resolution\nimage recognition models. Then, the structural knowledge\ncontaining one and more order relational information is ex-\ntracted from the teacher and then transfer to supervise the\ntraining of student. In this manner, the student is constructed\nby showing more information and thus can improve the ca-\npacity in recognizing low-resolution images. The assistant\nwith high-resolution image as input helps the student to\ntransfer knowledge when needed. Experimental results on\nseveral tasks show that the proposed approach performs im-\npressively in recognizing low-resolution images, with lower\nmemory footprint and faster speed.\nOur main contributions are three folds: 1) we propose a\nhybrid order relational knowledge distillation approach that\nis able to distill richer knowledge from pre-trained high-\nresolution models to facilitate low-resolution image recog-\nnition; 2) we propose a relation module to extract multiple\norder relational knowledge; 3) we conduct extensive experi-\nments to show the impressive performance of our approach\nin metric learning, low-resolution image classification and\nlow-resolution face recognition tasks."}, {"title": "2.1 Low-Resolution Image Recognition", "content": "Face Recognition. Low-resolution face recognition has be-\ncame one of the most difficult problem in the field of face\nrecognition. Low-resolution face image lacks plenty of face\ndetails, which results in unsatisfactory accuracy with normal\nface recognition models. Construction-based and projection-\nbased approaches are proposed to exploit the knowl-\nedge of high-resolution images. The construction-based ap-\nproaches explicitly reconstruct high-resolution facial details\nfrom low-resolution images, while the projection-based ap-\nproaches proposed to hallucinate or super-resolve the high-\nresolution faces before recognition by explicitly details. Al-\nthough, these approaches generally achieve good recogni-\ntion accuracy, they suffer from intensive computation due\nto the extra face reconstruction process. Another strategy\nis mapping the features learned from high-resolution im-\nages into a domain through transfer learning. The feature\nextracted by teachers with deeper and more complex net-\nworks, which are used to supervise training the student. In\norder to compensate for the lack of low-resolution image\ninformation, high-resolution and low-resolution images are\nusually input in pair-wise manner. The resulted simplified\nmodel can run faster while maintaining good performance\nand costing reduced memory.\nImage Classification. Compared to low-resolution face\nrecognition, low-resolution image recognition remains less\nrelatively explored. Previous studies show that there have\ntwo main ideas to solve the problem. One idea is mapping\nthe source space of low-resolution image to special space\nand making the largest similarity between high-resolution\nand low-resolution images. The representative method was\nproposed by (Wei et al. 2017), which found a robust lin-\near function to map different vector space getting the sparse\nstructure of each class. Another idea requires super resolu-\ntion process, which usually be used to get more detail in-\nformation about the low-resolution images. For example,\n(Wang et al. 2016) proposed incorporating the super res-\nolution domain adaptation and robust regression step by\nstep. Several works (Jaffe, Sundram, and Martinez-Nieves\n2017; Noor et al. 2019) introduced super-resolution for fine-\ngrained low-resolution image classification or other spe-\ncific low-resolution image classification task. There exist\nother methods to address low-resolution image classification\nproblem. For example, (Singh et al. 2019) proposed apply-\ning dual directed capsule network to low-resolution image\nclassification."}, {"title": "2.2 Knowledge Distillation and Transfer", "content": "With the improvement of model performance, the number of\nmodel parameters is also increasing. It often needs to occupy\nhuge memory resources and consume lots of time, lead-\ning to the issue of model compression. Knowledge distil-\nlation (Hinton, Vinyals, and Dean 2015; Romero et al. 2015;\nLiu et al. 2019; Park et al. 2019; Xue et al. 2019) provides a\nfeasible way to address that issue. Simply speaking, its idea\nis using a large parameter model (teacher) to supervise the\nlearning of a small parameter model (student). In practice,\nthe student model learns the output behaviors of the teacher\nmodel that has been pre-trained on some target dataset. Al-\nthough the student model can't reach the accuracy of the\nteacher model, it still is very powerful and yet efficient.\nThere are two key factors of knowledge distillation: what\nknowledge has been learned and how to effectively transfer\nknowledge from teacher model to student model.\nTraditional knowledge distillation tends to the direct\ntransmission of instance information, and the teacher and\nstudent often input in pairs, losing the structural information\nabout original data. Some recent approaches (Yu et al. 2019;"}, {"title": "3 Proposed Approach", "content": "Usually, the higher the order for information we focus\non, the more information we can get. Inspired by this\nand (Mirzadeh et al. 2019), we focus on different or-\nders for information attention and propose hybrid order\nrelation knowledge distillation (HORKD) for facilitating\nlow-resolution image recognition (see Fig. 2) via two-\nstage knowledge transfer. The first cross-structure distilla-\ntion stage transfers various order relational knowledge from\na high-resolution cumbersome model (teacher) $t(x; w_t)$ to\na high-resolution compact model (assistant) $a(x; w_a)$ that\nmimics the behaviors of the teacher. Then, we transfer var-\nious order relational knowledge from $a(x; w_a)$ to a low-\nresolution compact model (student) $s(x; w_s)$ that mim-\nics the behaviors of the the assistant with cross-resolution\ndistillation. Here, $x$ and $x'$ are a high-resolution and low-\nresolution image, respectively. $w_t$, $w_a$ and $w_s$ are the\nmodel parameters.\nTraditional knowledge distillation approaches pay more\nattention to the point-wise relationship between instances in\nrepresentation space, where the transfer of knowledge may\nbe inadequate. In contrast, our approach aims to achieve bet-\nter knowledge transfer by considering higher order relations.\nBy redefining the loss function, the student can learn the\nstructural knowledge extracted by the teacher well, and it\neffectively compensates for the lack of necessary informa-\ntion brought by resolution degradation, thus improving its\nrecognition performance. Toward this end, denoting $x^n$ and\n$\\hat{x}^n$ as a set of n-order tuple of distinct high-resolution and\nlow-resolution instances respectively, $f_i = \\phi_t(x_i; w_t)$ as\nthe teacher knowledge distilled from a high-resolution im-\nage $x_i$ and $g_i = \\phi_s(\\hat{x}_i; w_s)$ as the student knowledge from\nthe responding low-resolution image $\\hat{x}_i$, the distillation pro-\ncess for n-order can be formulated as\n$L_n = \\sum_{(x_1,...x_n) \\in x^n\\atop(\\hat{x}_1,...,\\hat{x}_n) \\in \\hat{x}^n} l(\\psi(f_1,...f_n), \\psi(g_1,...g_n))$,\nwhere $\\psi$ is a relational potential function that measures a\nrelational energy of the given n-tuple, and $l$ is a loss that\npenalizes difference between teacher and the student."}, {"title": "3.2 Hybrid Order Relational Knowledge", "content": "It is obvious that the relational potential function $\\psi$ plays a\nkey role in extracting relational knowledge, which affects\nthe effectiveness and efficiency of knowledge distillation\nprocess. Generally, a higher-order potential function may be\nmore powerful in capturing higher-level structure informa-\ntion when costing more computations. Suppose an output\nrepresentation space (e.g., a mini-batch) has m examples,\nthen the size of a n-tuple space is its combination $C_m^n$.\nOur distillation process tries to match the potential energy\ninformation between teacher stream and student stream.\nTherefore, small batch normalization is very useful, espe-\ncially when the difference between the two streams is sig-\nnificant. In order to compensating the information loss in\nresolution degradation, we expect to transfer various order\nrelational knowledge. Therefore, it needs to exploit an ef-\nfective solution to address this in an efficient way. In this\nwork, we first employ the traditional low-order relational\nknowledge: 1-order, 2-order and 3-order. Furthermore, we\npropose an effective approach to exploit higher-order rela-\ntional knowledge."}, {"title": "1-order relational knowledge.", "content": "It is also known as point-\nwise distillation loss, when $n = 1$ and the relation is unary.\nIt is popular with previous works, which uses the class prob-\nabilities produced from the teacher as soft targets for training\nthe student or transferring the intermediate feature maps. In\nthis work, we transfer features and the loss function is given\nas follows,\n$L_1 = \\sum_{x_i \\in x^1\\atop \\hat{x}_i \\in \\hat{x}^1} l_1(f_i,g_i)$,\nwhere $l_1$ is a function which measures the $L_1$ distance be-\ntween the feature instances of teacher and student."}, {"title": "2-order relational knowledge.", "content": "It is also known as pair-\nwise distillation loss. Recent works have used it in various\ntasks such as image classification, image retrieval and se-\nmantic segmentation. Its objective is transferring pair-wise\nrelations, specially pair-wise similarities in our approach,\namong instances. We adopt the square difference to formu-\nlate the 2-order relational knowledge distillation loss,\n$L_2 = \\sum_{(x_i, x_j) \\in x^2\\atop (\\hat{x}_i, \\hat{x}_j) \\in \\hat{x}^2} l_2(\\mathcal{V}_d(f_i, f_j), \\mathcal{V}_d(g_i, g_j))$,\nwhere $V_d$ is a normalized loss function that measures the L2\ndistance between the features from two instances in a mini-\nbatch space and having $a(f_i, f_j) = 1/\\mu||f_i-f_j||^2$, where\n$\\mu = 1/|x^2| \\sum_{(x_i,x_z) \\in x^2} ||f_i - f_j||^2$."}, {"title": "3-order relational knowledge.", "content": "It measures the relation\namong the examples in a triplet. Toward this end, Park et\nal. propose an angle-wise distillation loss that is formed by\nthree examples in the output feature space:\n$L_3 = \\sum_{(x_i, x_j, x_k) \\in x^3\\atop (\\hat{x}_i, \\hat{x}_j, \\hat{x}_k) \\in \\hat{x}^3} l_3(\\mathcal{V}_a (f_i, f_j, f_k), \\mathcal{V}_a (g_i, g_j, g_k))$,\nwhere $l_3$ is the Huber loss. The angle-wise potential function\n$V_a$ is represented as\n$\\mathcal{V}_a (f_i, f_j, f_k) = < \\frac{f_i-f_j}{||f_i - f_j||_2}, \\frac{f_k-f_j}{||f_k - f_j||_2} >$,\nwhere < ., . > is the dot-product operator. The 3-order re-\nlational knowledge is transferring the relationship of train-\ning instances embedding by penalizing angular difference,"}, {"title": "Center-based relational knowledge.", "content": "Typically, when the\norder is more than 3, it will bring in the following two is-\nsues: 1) the computational cost will increase, as we expect,\nand 2) the potential function is difficult to define. A feasi-\nble way to address the first issue is reducing the example\nnumber in each mini-batch when training. Clearly, though,\nthe relationship between an example and some other exam-\nples outside the mini-batch will be lost, reducing the suffi-\nciency of knowledge transferred. In this work, we propose\nthe class-centered relational knowledge to address these is-\nsues. Toward this end, an extra set of examples $U = {u_c}_{c=1}^C$\nis defined to describe the class centers in the output represen-\ntation space, which is represented as\n$\\hat{x} \\stackrel{\\text {def.}}{=} \\frac{\\sum_{i=1}^{x} \\delta(l_i = c) f_i}{\\sum_{i=1}^{x} \\delta(l_i = c)}$,\nwhere $\\delta(l_i = c)$ is an indicator function which equals 1 if\n$l_i = c$ and 0 otherwise. C is the total class number. In this\nway, each example is characterized by the feature instance\nof a specific class center, and the class center is represented\nby the average feature instances. Then, a support space is\nconstructed by using these class centers, which can be used\nto create the C-order relational knowledge for an instance:\n$L_c = \\sum_{x_i \\in x^1\\atop \\hat{x}_i \\in \\hat{x}^1} \\sum_{c=1}^{C} l_2(\\mathcal{V}_e(f_i, u_c), \\mathcal{V}_e(g_i, u_c))$,\nwhere $e$ measures the $L_2$ distance between two feature in-\nstances, having $We(f_i, u_c) = || f_i - u_c||_2$. In this way, this\nhigh order relation can be converted into a group of 2-order\nrelations, which can be addressed efficiently."}, {"title": "Total distillation loss.", "content": "With the hybrid order relations, the\nknowledge to be transferred can involve the relations in var-\nious levels, that is individual-level, pair-level, triplet-level\nand group-level knowledge. Finally, the total distillation loss\nis the weighted sum of these four losses:\n$L = L_1 + \\alpha L_2 + \\beta L_3 + \\gamma L_c$.\nwhere, $\\alpha$, $\\beta$ and $\\gamma$ are the tuning factors to balance the effects\nof different order relational knowledge."}, {"title": "3.3 Two-Stage Knowledge Transfer", "content": "As shown in Fig. 2, the knowledge transfer includes two\nstages. In the first cross-structure distillation stage, the input\nof the teacher and assistant network is high-resolution im-\nages. The objective is to alleviate structure redundancy like\ngeneral process of many knowledge distillation approaches\n(Hinton, Vinyals, and Dean 2015; Romero et al. 2015). After\ncross-structure distillation, there still exist redundancy in the\nresolution, meaning that a low-resolution image can still be\nrecognizable when its corresponding high-resolution images\nare learned adequately. In the second cross-resolution step,\nwe use the assistant network as a new teacher to guide the\ntraining of low-resolution student network that has the same\nstructure as assistant network. The objective is to reduce the\ninformation loss due to resolution degradation."}, {"title": "4 Experiments", "content": "To verify the effectiveness of our proposed approach, we\nconduct comprehensive experiments on three typical low-\nresolution image recognition tasks, including low-resolution\nimage classification, low-resolution face recognition and\ncross-resolution metric learning."}, {"title": "4.1 Low-resolution Image Classification", "content": "First, we check low-resolution image classification task\non the challenging CIFAR100 benchmark. The CIFAR100\ndataset consists of 60K 32\u00d732 images of 100 classes.\nAmong these images, 50K images are used for training\nand the remaining 10K images for testing. To benchmark\nour models, we further make the comparisons between four\nstate-of-the-arts: 1) KD (Hinton, Vinyals, and Dean 2015)\nserves as baseline that uses the final output class proba-\nbilities as soft targets to supervise the training of student,\n2) FitNet (Romero et al. 2015) uses the intermediate fea-\ntures as the knowledge and transfers to the student, 3) At-\ntention (Zagoruyko and Komodakis 2017) also uses feature\ninstances as the knowledge that is enhanced with attention\nmodule, and 4) RKD transfers the 2-order and 3-order re-\nlational knowledge to the student whose input resolution is\nthe same as teacher. For all the settings, we use the cross-\nentropy loss at the final loss in addition. For both the teacher\nand the student, we remove fully-connected layer(s) after the\nfinal pooling layer and append a single fully-connected layer\nas a classifier. The teacher is achieved by training in origi-\nnal 32\u00d732 training dataset with ResNet50, while the stu-\ndent is trained in the corresponding dataset with a reduced\nresolution of 16\u00d716 by using a simpler ResNet18 network.\nFor extensive comparisons, we also conduct the experiments\nwith the same input resolution and by using another VGG11\nnetwork (Simonyan and Zisserman 2015) for student (OUR-\nS-N, where S\u2208 {16,32, ...} stands for the resolution and\nN\u2208 {R,V} is the ResNet or VGGNet), resulting in four mod-\nels: OUR-16-R, OUR-16-V, OUR-32-R and OUR-32-V. Ad-\nditionally, 'L' and 'S' indicate the input resolution that is\nlower than the teacher and the same as teacher, respectively.\nIn this case, OUR-S-R and OUR-S-V can be considered as\nthe assistants.\nThe experimental results are shown in Tab. 1. First, it\nshows that the classification accuracy consistently decreases\nafter the data resolution is reduced from 32\u00d732 to 16\u00d716,\nas we expect. Specially, thank to the step-wise knowledge\ntransfer from the teacher to the student with the help of\nassistant, the accuracy drop of at an acceptable level, e.g.,\n3.11% from 77.76% to 74.65%, while the model get smaller.\nSecond, by using OUR-S-R with 76.68% accuracy as the\nassistant, the student OUR-L-R achieves an accuracy of\n74.65%, which is higher than the student OUR-L-V that uses\nan lower-accurate OUR-S-V as assistant. It implies that a\nhigher-accurate assistant may lead to better student. Third,\nour low-resolution student OUR-L-R gives a higher accu-\nracy than other state-of-the-art distillation approach, demon-\nstrating the effectiveness of our approach."}, {"title": "4.2 Low-resolution Face Recognition", "content": "After the promise achieved in general image classification\ntask, we focus on low-resolution face recognition task that\nis very helpful in many real-world applications, e.g., rec-\nognizing low-resolution surveillance faces in the wild. In\nour experiments, the teacher uses a recent state-of-the-art\nface recognizer VGGFace2 with ResNet50 structure. The\nstudent models use ResNet34 network and are trained in\nUMDFaces (Bansal et al. 2017) that is collected from In-\nternet and serves as the high-resolution face dataset. Then,\nthe trained student models are used to evaluate face verifi-\ncation task on LFW benchmark and face identification task\non UCCS dataset (Bansal et al. 2017). LFW benchmark con-\ntains 6K pairs where 3K positive pairs have the same iden-\ntities and the remaining is 3K negative pairs. UCCS con-\ntains 16,149 images in 1,732 subjects in the wild condition.\nWe follow the setting as (Ge et al. 2019), randomly select a\n180-subject subset, and randomly separate the images into\nthe 3,918 training images and 907 testing images according\nto a ratio of about 4:1, and report the results with the stan-\ndard top-K error metric. In order to verify the validity of our\nlow-resolution models, we emphatically check the accuracy\nwhen the input resolution is 32\u00d732 and 16\u00d716. In the fol-\nlowing, we report these results.\nFace verification on LFW. In this experiment, we conduct\nthe comparisons with 12 state-of-the-art face recognition\nmodels, including 6 models working at normal resolution\n(DeepFace (Taigman et al. 2014), DeepID (Sun, Wang, and\nTang 2014), DeepID2 (Sun et al. 2014), FaceNet (Schroff,\nKalenichenko, and Philbin 2015), VGGFace (Parkhi et al.\n2015) and ArcFace (Deng et al. 2019)), 6 models working at\nlow resolution (MobileID (Luo et al. 2016), SphereFace (Liu\net al. 2017), ShiftFaceNet (Wu et al. 2018), CosFace (Wang\net al. 2018), VGGFace2, and SKD (Ge et al. 2019)). For the\n6K face pairs, we extract the facial features for similarity\ncomparison. With a pre-set threshold, each faces pair is de-\ntermined to have the same identity if the similarity of the\ntwo faces is greater than the threshold and different iden-\ntity otherwise. The verification accuracy is reported as the\npercentage of the pairs that are correctly determined. The\nresults are listed in Tab. 2.\nFrom the results, we can find that the normal face recog-\nnition models always achieve a high accuracy but cost much\nmore parameters. For the DeepID and DeepID2 models\nwhich work at the medium resolution setting, they integrate\ndozens of models to achieve good results, leading to 2\u00d7 and"}, {"title": "4.3 Cross-Resolution Metric Learning", "content": "Beyond the low-resolution image classification and face\nrecognition tasks, we further evaluate our approach on low-\nresolution metric learning task which learns the similarity\nof objects under the resolution degradation. In the experi-\nment, we mainly carry out cross-resolution measurement of\ninterest correlation under three benchmarks: 1) Cars 196 con-\ntains 16,185 car images in 196 classes, 2) CUB2011 is an\nextended version of CUB200 that is a challenging dataset of\n200 bird species, and 3) SOP (Stanford Online Products) is\na product dataset having 120,053 images of 22,634 classes.\nThe input resolution is normalized to 256\u00d7256 for teacher\nand ranges from 256x256 to 96\u00d796 for students.\nOur main purpose is to verify the performance of our\nmodel in cross-resolution metric learning. In the experi-\nments, We use ResNet50 as teacher and ResNet18 as stu-\ndent, and then conduct the experiments to show the effect of\nusing original data to train teachers and different degraded\nresolution of images to feed students. The results are shown\nin Tab. 4, which are compared with several state-of-the-arts:\nincluding AT (Zagoruyko and Komodakis 2017), PKD (Pas-\nsalis and Tefas 2018), DarkRank (Chen, Zhang, and Wang\n2018) and MKD (Yu et al. 2019)).\nThe experimental results show that although the perfor-\nmance of student network will be limited by the resolution\ngap between teacher and student input, overall, our student\nis better than teacher. Specifically, when the resolution of\nthe student is maintained at more than one fourth of the res-\nolution of the teacher, that is 128\u00d7128 on Cars196 dataset,\nour approach shows good adaptability, even the accuracy in-\ncreases about 2% from the accuracy of our teacher. On the\nCUB-200-2011 and SOP dataset, the performance of stu-\ndents is slightly worse, but still better than the best results\nof other approaches. These results are based on the premise\nthat our teacher is simpler than their teacher. That is to say,\nwe have adopted simpler student to achieve better perfor-\nmance in cross-resolution metrics learning.\nFor cross-resolution metric learning, we use ResNet50\nas teacher with the input of high-resolution images, and\nResNet18 as assistant. Then we use the assistant to super-\nvise the training of student that also uses ResNet18 but takes"}, {"title": "4.4 Ablation Study", "content": "Generally speaking, the higher-order information will yield\nbetter knowledge transfer. But does learn more informa-\ntion mean that the model performance is better? In order\nto answer this question, we conducted a special experiment\non low-resolution face recognition task with cross-structure\ndistillation to verify the effect of the order of attention on\nthe model. The face images are taken from UMDFaces and\nUCCS datasets and resized to 32\u00d732. We use ResNet50 for\nteacher and the extremely simplified ResNet34 for student\nto study the performance of knowledge transfer. In our ex-\nperiments, the parameters \u03b1, \u03b2, \u03b3 are set to 0.02,0.01,1 re-\nspectively. The results are shown in Fig. 4.\nWith the increasing order of loss functions, the model per-\nformance is indeed getting better. However, the optimal re-\nsults come from $L = \\alpha L_2 + \\beta L_3 + \\gamma L_c$ rather than the\nhighest-order $L = L_1 + \\alpha L_2 + \\beta L_3 + \\gamma L_c$. In addition,\nwhen the number of loss terms is the same, the higher the\norder of information concerned by the loss function itself,\nthe better the performance is. We conclude that although the\nincrease of the order will help to improve the performance\nof the student while transferring knowledge, but it is not cer-\ntain. Although higher order means more information, we can\nnot guarantee that all of these information will help improve\nthe model. The redundancy of these information may have\na negative impact on the model probably. Therefore, for a\nspecified task, finding the optimal order is very important\nfor knowledge distillation. In our experiments, the student\nmodels perform best when $\\alpha = 0.02$, $\\beta = 0.01$ and $\\gamma = 1$.\nAdditionally, we check the merit of two-stage knowledge\ntransfer via cross-structure distillation and cross-resolution\ndistillation. We find that the introduction of the assistant\nas bridge can improve the performance for cross-resolution\nrecognition tasks. For example, we have achieved an accu-\nracy improvement of 3.03% in cross-resolution metric learn-\ning task when adding cross-structure distillation."}, {"title": "5 Conclusion", "content": "In this paper, we propose a hybrid order relational knowl-\nedge distillation approach for facilitating cross-resolution\nimage recognition tasks. The approach effectively trans-\nfers rich knowledge from high-resolution teacher to low-\nresolution student with the help of the assistant by carry-\ning out cross-structure distillation and cross-resolution dis-\ntillation step-wisely to remove the redundancy in the net-\nwork and image resolution. Extensive experimental results\nof three typical cross-resolution image recognition tasks\nshow the effectiveness of the proposed approach. In the fu-\nture, we will extend it for more extensive applications."}]}