{"title": "Explaining an image classifier with a generative model conditioned by uncertainty", "authors": ["Adrien Le Coz", "St\u00e9phane Herbin", "Faouzi Adjed"], "abstract": "Identifying sources of uncertainty in an image classifier is a crucial challenge. Indeed, the decision process of those models is opaque and does not necessarily correspond to what we might expect. To help characterize classifiers, generative models can be used as they allow the control of visual attributes. Here we use a generative adversarial network to generate images corresponding to how a classifier sees the image. More specifically, we consider the classifier maximum softmax probability as an uncertainty estimation and use it as an additional input to condition the generative model. This allows us to generate images that result in uncertain predictions, giving us a global view of which images are harder to classify. We can also increase the uncertainty of a given image and observe the impact of an attribute, providing a more local understanding of the decision process. We perform experiments on the MNIST dataset, augmented with corruptions. We believe that generative models are a helpful tool to explain the behavior and uncertainties of image classifiers.", "sections": [{"title": "Introduction", "content": "Context: explaining the behavior of image classifiers. The growing use of image classifiers in many, sometimes critical, applications (e.g., medical diag-nosis, autonomous driving, autonomous aircraft landing) reinforces the need to understand their behaviors. A key issue is to identify the conditions under which such systems are likely to fail, in order to ensure the safety of their use. With this objective in mind, one can consider uncertainty as a measure of potential failure: the question of failure condition identification can be translated into the problem of describing the nature of uncertain data for a given classifier.\nExplainability is currently thought of as a tool to improve the trustworthi-ness of AI predictive systems. [2,15]. In this paper, we propose to provide an explanation of the global classifier behavior as a representation of its uncertain data by using a generative model."}, {"title": "Method", "content": "Generative Adversarial Networks (GANs) [5] are a type of generative model known for their generation quality and the controllability offered by their la-tent (input) space. In particular, they can generate full size images. They are composed of two neural networks: a generator that generates fake images and a discriminator that distinguishes fake images from real ones from the train-ing data. The training is a competition between the two: the generator tries to fool the discriminator, which seeks not to be fooled. During this process, the two improve until the discriminator cannot distinguish any more real from fake data. GAN training is known to be unstable, so an equilibrium has to be found: if the discriminator becomes much better than the generator, it \"wins\", and it's hard for the generator to improve and fill the gap, and vice-versa. Losses and regularizations have been developed to fix the issue [1]. After training, we discard the discriminator and use the generator to generate images from noise vectors (\"latent codes\"). Interestingly, the model is structured so that interpo-lations between two latent codes result in a smooth semantic shift of an image into another; for instance, a digit image of \"8\" is progressively transformed into a mixture of \"3\" and \"8\" before ultimately becoming a \"3\", which is not the case if the interpolation is done in the pixel space.\nWe use the model StyleGAN2 [11,12], widely used for high-quality face gen-eration and edition. It has a unique architecture. The input latent space Z is mapped through fully connected layers to an intermediate latent space W. The image is generated progressively at different scales, starting from an ini-tial constant tensor with a size of 42 and 512 channels, which is up-sampled and transformed by residual convolution layers, and results in images of up to\n10242 pixels. Latent codes w \u2208 W are transformed into styles s \u2208 S through\nlearned affine transformations. Those styles will scale the convolution weights of each feature map for each generator layer. Styles applied at low resolution af-fect high-level aspects of the face (pose, hairstyle...), while at higher resolutions, they affect small details (microstructure...). Latent spaces W and S are highly disentangled, meaning that they encode distinct visual attributes along different dimensions. This allows image editing, one attribute at a time [22]. In particular, we can also use the latent space to characterize classifiers [13,17,14]. Here, we exploit generative models more straightforwardly by conditioning the generation with the classifier uncertainty, so that it becomes an input of the generator.\nOur model architecture is depicted in Fig. 1a. A conditional GAN [16] takes a noise vector as input and a condition. Typically, this can be a one-hot embedding of the class to generate samples of a selected class. A simple way of conditioning a GAN is to concatenate the condition, e.g., encoded as one-hot embedding, to the noise vector as inputs for the generator, and also concatenate the condition to the real or fake image as inputs for the discriminator.\nThere are several ways to define the prediction uncertainty, e.g., entropy, maximum softmax probability (MSP) [8], or true class probability [3]. We use the imperfect but simple MSP as an uncertainty estimation. We add it as an input condition to the generator. Then after training, the model can generate uncertain data to get a global overview of the uncertainty. We also manipulate data to increase or decrease the uncertainty and exhibit sources of uncertainty.\nMSP values are computed with the classifier (with frozen weights). For the discriminator used on real images, we compute their associated MSP first. For the discriminator used on fake images, we take the MSP used as a condition for the generator, which is not the same as what the classifier would output because of the imperfect generation. To condition the generator, we apply the MSPs of random real images using the classifier to track the real distribution of"}, {"title": "Preliminary experiments", "content": "Two-dimensional moons data. We first illustrate the approach with a simple problem using the moons dataset [18]. The data is 2-dimensional and looks like two interleaving half-circles corresponding to the upper and lower moon classes. The noise level can be adjusted, and we fix it to 0.3 to have an area where the two classes are mixed. We train a simple fully connected neural network as a classifier. We use a simple generator based on a fully connected network conditioned by one-hot class embedding and the MSP. The network has 5 layers with a latent space of dimension 8. The conditioning is a concatenation of the class information as a one-hot embedding vector (of dimension 2) and the MSP as a continuous value.\nFig. 2a on the left shows the data, with colors representing the MSP obtained when classifying the data. We can see that the MSP is close to 1, where the classes do not mix but gets lower in the middle area where the classes mix, representing higher uncertainty. We can note that this uncertainty is mostly aleatoric: data can be of either class in the middle region. Whereas, in Fig. 2a on the right shows synthetic data conditioned by MSP. The values are sampled from MSP computed on real data to follow the same distribution. We can see similarities\nbetween the locations of real data with high MSP and synthetic data conditioned by high MSP, and likewise for low MSP. The generator captured which kind of data is uncertain and can generate such data when conditioned with low MSP. For more quantitative results, we follow this process: fix some MSP values as conditions (\"input confidence\"), generate fake data, classify it, and obtain the MSP of the classifier (\"output confidence\"). Ideally, both values should be the same every time. As seen in Fig. 2b, it is not necessarily the case, especially for lower values. Yet, the two are correlated.\nMNIST. Let us now consider more complex data: images. We use the MNIST dataset [4], which contains black and white images of handwritten digits with ten classes (digits from 0 to 9). We train a Convolutional Neural Network to classify digits from images. We consider clean MNIST data, but to make the problem more realistic, we also choose to corrupt MNIST images. We use Gaus-sian blur and noise similarly to ImageNet-C [7]. These corruptions are applied on a random half of the images, with a random severity level out of 5 possible levels."}, {"title": "Conclusion", "content": "We created an explicit generator of uncertain data that can be used in several ways. It can give a global outlook of uncertain images by generating them on demand. It can also corrupt images (transform them into their more uncertain form) to visualize sources of local uncertainty. The results are preliminary but encouraging. Leveraging generative models is a promising way to improve ex-plainability when uncertain data is rare.\nWe identified some ideas for future work. The MSP might not contain sufficient information to capture the classifier behavior, so more information, like the full vector, could be considered. The constraint put on the condition during training should be reinforced, for instance with an additional loss term. Furthermore, the MSP might not be calibrated: the probability might be overestimated and require a recalibration."}]}