{"title": "TrojanDec: Data-free Detection of Trojan Inputs in Self-supervised Learning", "authors": ["Yupei Liu", "Yanting Wang", "Jinyuan Jia"], "abstract": "An image encoder pre-trained by self-supervised learning\ncan be used as a general-purpose feature extractor to build\ndownstream classifiers for various downstream tasks. How-\never, many studies showed that an attacker can embed a trojan\ninto an encoder such that multiple downstream classifiers built\nbased on the trojaned encoder simultaneously inherit the tro-\njan behavior. In this work, we propose TrojanDec, the first\ndata-free method to identify and recover a test input embed-\nded with a trigger. Given a (trojaned or clean) encoder and\na test input, TrojanDec first predicts whether the test input is\ntrojaned. If not, the test input is processed in a normal way\nto maintain the utility. Otherwise, the test input will be fur-\nther restored to remove the trigger. Our extensive evaluation\nshows that TrojanDec can effectively identify the trojan (if\nany) from a given test input and recover it under state-of-the-\nart trojan attacks. We further demonstrate by experiments that\nour TrojanDec outperforms the state-of-the-art defenses.", "sections": [{"title": "Introduction", "content": "Traditional transfer/supervised learning trains a feature ex-\ntractor using a labeled training dataset, which incurs large\ncosts and human effort to annotate the training dataset. More-\nover, it cannot leverage a large amount of unlabeled data\nthat can be collected from the Internet. Self-supervised learn-\ning (Devlin et al. 2019; Hadsell, Chopra, and LeCun 2006;\nHe et al. 2020; Chen et al. 2020; Hjelm et al. 2019; Grill\net al. 2020) has been designed to address those challenges. In\nparticular, a model provider can use self-supervised learning\nto pre-train an encoder using a large set of unlabeled data\n(called pre-training dataset). This paradigm can significantly\nsave the labeling effort and thus enables the model provider\nto significantly increase the size of its pre-training dataset.\nFor example, CLIP (Radford et al. 2021) was pre-trained by\nOpenAI on 400 million (image, text) pairs collected from\nthe Internet. The model provider can monetize its encoder\nby deploying it as a cloud service via providing an API to\nusers (Liu et al. 2022). Suppose a user has some training/test\ninputs for a downstream task. The user can query the API to\nobtain their feature vectors and then use those feature vectors\nto train/test a downstream classifier for its downstream task.\nDespite the success of self-supervised learning, many ex-\nisting studies show that it is vulnerable to trojan attacks (Jia,\nLiu, and Gong 2022; Saha et al. 2022; Carlini and Terzis\n2022). Specifically, an adversary can inject a trojan to self-\nsupervised learning encoders by either poisoning the pre-\ntraining dataset (Saha et al. 2022; Liu, Jia, and Gong 2022;\nCarlini and Terzis 2022) or directly manipulating pre-trained\nencoders' parameters (Jia, Liu, and Gong 2022). The trojaned\nencoder will produce normal feature vectors for clean inputs,\nbut output feature vectors similar to the feature vector of an\nattacker-chosen reference object for any test inputs with the\nattacker-chosen trigger. Furthermore, the trojan behavior will\nbe conveyed to downstream classifiers built upon the trojaned\nencoder, leading those classifiers to predict the test inputs\nwith the trigger to the ground-truth label (called target label)\nof the attacker-chosen reference object.\nMany defenses (Tran, Li, and Madry 2018; Chen et al.\n2018; Wang et al. 2019; Xu et al. 2021; Chen et al. 2019;\nLiu et al. 2019; Gao et al. 2019; Doan, Abbasnejad, and\nRanasinghe 2020; Li et al. 2021b) were proposed to mitigate\ntrojan attacks to machine learning models. Depending on\nwhich stage those defenses are used, we categorize them into\ntraining-phase defenses (Tran, Li, and Madry 2018; Chen\net al. 2018; Wang et al. 2019; Xu et al. 2021; Chen et al.\n2019; Liu et al. 2019; Li et al. 2021b) and testing-phase de-\nfenses (Doan, Abbasnejad, and Ranasinghe 2020; Li et al.\n2021b). A training-phase defense either trains a robust model\non a poisoned training dataset or detects/removes the trojan\nin a trained model. As a result, they require access to the\ntraining dataset or the model parameters of the model. In\nother words, they are not applicable when a defender only\nhas black-box access to a pre-trained model. In our work, we\nconsider a defender (i.e., user) has black-box access to an en-\ncoder (we discuss more details in our threat model) and thus\nthose defenses are not applicable in general. Moreover, when\nextending to self-supervised learning, some of these defenses\nare ineffective even if we assume the defender has white-box\naccess to the encoder as shown in previous studies (Jia, Liu,\nand Gong 2022) (we also confirm this in our experiments).\nTesting-phase defenses (Doan, Abbasnejad, and Ranasinghe\n2020; Li et al. 2021b) aims to detect whether a test input is\ntrojaned. When extended to self-supervised learning, those\ndefenses are either ineffective or require a defender to have\na clean validation dataset, which is less practical in the real\nworld. In our work, we relax such an assumption by propos-\ning a data-free defense. Moreover, our comparison results\nshow that our defense can achieve comparable performance\nby giving those defenses an advantage, i.e., assuming the\ndefender has a clean validation dataset.\nOur work: In this work, we propose TrojanDec, the first\nframework to identify and restore a trojaned test image in\nthe self-supervised learning context. Our framework consists\nof three main components: 1) metadata extraction, 2) trojan\ndetection, and 3) image restoration. In the first component, we\nextract the key metadata from a test image, which is critical\nin detecting if the image consists of a trigger. In the second\ncomponent, we perform a data-free statistical analysis on the\nmetadata to identify if the corresponding image is trojaned\nor not. In the third component, if the image is detected as a\ntrojaned example in the previous part, we further restore it\nusing a diffusion model.\nTo show the effectiveness of our TrojanDec, we conduct\nextensive experiments on multiple pre-training datasets and\ndownstream tasks under state-of-the-art trojan attacks (Jia,\nLiu, and Gong 2022; Saha et al. 2022; Liu, Jia, and Gong\n2022; Zhang et al. 2024) to self-supervised learning. Our\nresults some that our defense is consistently effective under\nthose attacks. We further generalize several representative\nbackdoor attacks (Chen et al. 2017; Salem et al. 2022) de-\nsigned for supervised learning to the self-supervised learning\ncontext as the adaptive attacks and show the effectiveness\nof our method in defending against them. To study the im-\npact of hyperameters, we perform a comprehensive ablation\nstudy in our evaluation. Finally, we compare our defense with\nexisting defenses (Gao et al. 2019; Doan, Abbasnejad, and\nRanasinghe 2020; Ma et al. 2023) against trojan attacks. Our\nresults demonstrate that TrojanDec outperforms all of them.\nTo summarize, we make the following contributions:\n\u2022 We propose the first generic data-free trojaned test input\ndetection and restoration framework for self-supervised\nlearning.\n\u2022 We demonstrate that our framework does not rely on any\nclean data or any prior knowledge to the pre-training or\ndownstream dataset.\n\u2022 We conduct extensive experiments to evaluate our\nTrojanDec and show that it is effective in defending\nagainst various types of trojan attacks and outperforms\nexisting defense methods."}, {"title": "Related Work", "content": "Trojan Attacks\nTrojan attacks to self-supervised learning: Trojan at-\ntacks (Carlini and Terzis 2022; Saha et al. 2022; Jia, Liu,\nand Gong 2022; Li et al. 2023; Zhang et al. 2024; Wu et al.\n2023; Jha, Hayase, and Oh 2023; Bober-Irizar et al. 2023;\nLi et al. 2024) to self-supervised learning aim to produce a\ntrojan encoder such that it outputs normal feature vectors for\nclean test inputs, while generating feature vectors that are\nsimilar to the feature vector of an attacker-chosen reference\nobject (called target object) for test inputs embedded with a\ntrigger. As a result, a downstream classifier built upon such a\nDefending against Trojan Attacks\nThere are many existing works that focus on defending\nagainst trojan attacks (Tran, Li, and Madry 2018; Chen et al.\n2018; Wang et al. 2019; Xu et al. 2021; Chen et al. 2019; Liu\net al. 2019; Gao et al. 2019; Doan, Abbasnejad, and Ranas-\ninghe 2020; Li et al. 2021b; Jia et al. 2022; Cheng et al. 2023;\nHuang et al. 2021). Most of them are designed for supervised\nlearning. Depending on which phase those defenses are ap-\nplied, they can be categorized into two genres: training-phase\ndefense and testing-phase defense.\nTraining-phase defenses: Training-phase defenses (Chen"}, {"title": "Threat Model", "content": "Attacker's goal: To perform trojan attacks to self-supervised\nlearning, an attacker selects one (or more) target classes in\none (or more) downstream tasks. The attacker then chooses a\ntrigger for each target class. In particular, the attacker aims\nat crafting a trojaned encoder to achieve two goals. Firstly,\nthe encoder generates normal feature vectors for clean test\ninputs (i.e., the inputs without triggers). This means that the\nfunctionality of the trojaned encoder is maintained for clean\ninputs, which makes the attack more stealthy and harder\nto be noticed. Secondly, when the trigger presents in a test\ninput, the trojaned encoder will produce a feature vector\nthat lies in the target class associated with that trigger. As a\nresult, a downstream classier built on the trojan encoder for\na downstream task will predict the target class for any input\nwith a backdoor trigger.\nAttacker's background knowledge and capability: We\nconsider both data-poisoning based attacks (Saha et al. 2022;\nCarlini and Terzis 2022) and model-poisoning based at-\ntacks (Jia, Liu, and Gong 2022). For data-poisoning based\nattacks, we consider an attacker can inject poisoned training\ninputs to the pre-training dataset used to pre-train an encoder.\nFor model-poisoning based attacks, we consider an attacker\ncan arbitrarily manipulate the parameters of an encoder. In\ngeneral, model-poisoning based attacks are more effective\nthan data-poisoning based attacks as they make a stronger\nassumption on the attacker.\nDefender's goal: We consider the defender as the down-\nstream user of the deployed self-supervised learning encoder.\nGiven a test input, the defender's goal is to detect whether it\ncontains a trigger. If this input is identified as trojaned, the\ndefender wants to further remove the trigger from the input.\nDefender's background knowledge and capability: We\nconsider a very weak defender. As mentioned, the defender is\na downstream user of the pre-trained encoder. In this scenario,\nthe defender only has the \"black-box\" access to the encoder,\nwhich means the following: 1) the defender can only query\nthe encoder by images and receive the produced feature vec-\ntors and 2) the defender does not have any knowledge to the\nencoder, including the pre-training dataset information, the\nencoder architecture, the encoder parameters, etc."}, {"title": "Our TrojanDec", "content": "We present TrojanDec as an end-to-end framework for\nidentifying and eliminating backdoors from test images. As\ndepicted in Figure 1, our framework consists of 3 key compo-\nnents: the extraction of metadata from a test input, backdoor\ndetection based on metadata, and test input restoration. We\nalso assume the attacker uses a patch-based trojan trigger,\nsince patch-based triggers are prevalent in real-world scenar-\nios as they are practical and easy to implement physically.\nNext, we delve into details of these components.\nMetadata Extraction\nThere are 3 steps to extract metadata from a test image. First,\nwe generate a set of masks and use them to mask the test im-\nage to create a set of masked images. Then, we query the en-\ncoder f (trojaned or not) to obtain feature vectors of masked\nimages and the original test image. Finally, we compute the\nsimilarity between the feature vectors of each masked image\nand the original test image to derive the metadata.\nFor simplicity, we use (m, p) to denote a patch-based\nmask, where m is a binary offset and p is the mask pattern.\nThe binary offset m controls two characteristics of the mask:\nlocation and size. Specifically, we denote the upper-left coor-\ndinate of a mask as (a, b) and the mask size as k. Given the"}, {"title": "Trojan Detection", "content": "In this section, we present our technique for detecting whether\na test image is trojaned or not, based on its metadata. The\nkey intuition of our TrojanDec is that the set of similarities in\nS can be divided into two clusters for a trojaned test image,\nwhereas there is only a single cluster for a clean test image.\nFigure 2 shows the distribution of S of a test image sampled\nfrom STL10 dataset. When the trigger is partially covered by"}, {"title": "Proposition 1", "content": "If the adversary sets the trojan trigger to be\ne whose height and width are \\(e_h\\) and \\(e_w\\), while defender has\na mask (m, p) such that the pattern p is randomly generated,\nthe probability of the existence of a part of the mask pattern\nsuch that l\u2081 distance of this part of the mask pattern and the\ntrojan trigger greater than \u03b2 is no smaller than\n\\[\n1 - \\frac{\\frac{(2\\beta)^T}{T!}}{1 - \\frac{\\frac{(2\\beta)^T}{T!}}{1 - \\frac{\\frac{(2\\beta)^T}{T!}}{1 - \\frac{\\frac{(2\\beta)^T}{T!}}{1 - \\frac{\\frac{(2\\beta)^T}{T!}}{1 - \\frac{\\frac{(2\\beta)^T}{T!}}{1 - \\frac{\\frac{(2\\beta)^T}{T!}}{1 - \\frac{\\frac{(2\\beta)^T}{T!}}{1 - \\frac{\\frac{(2\\beta)^T}{T!}}{1 - \\frac{\\frac{(2\\beta)^T}{T!}}{1 - \\frac{\\frac{(2\\beta)^T}{T!}}{1 - \\frac{\\frac{(2\\beta)^T}{T!}}{1 - \\frac{\\frac{(2\\beta)^T}{T!}}{1 - \\frac{\\frac{(2\\beta)^T}{T!}}{1}}}}}", "1\n    },\n    {": "itle", "Details of DDNM": "content\": \"The goal of the diffusion model in general is to train a model\nto estimate the noise added to \\(x_0\\) based on \\(x_t\\). To reach the\ngoal, it uses the following loss function:\n\\[\nL_e = ||\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{a}_t}x_0 + \\epsilon\\sqrt{1 - \\bar{a}_t}, t)||^2,\n\\]\nwhere \\(\\epsilon\\) is the neural backbone, t represents a time step, \\(\\bar{a}_t\\)\nis pre-defined scalar constant, and \\(\\epsilon ~ N(0, I)\\) represents the\nzero-mean Gaussian noise.\nFor simplicity, we denote \\(A = m^2\\). In the image restora-\ntion task, we have \\(y = Ax\\), where y is the degraded im-\nage and A is a linear degradation operation (i.e., masking).\nGiven the degraded image y, our goal switches to output x\nwhich is the estimation of x. From \\(Ax = y\\), we obtain that\n\\[\nx = A'y+ (I-A'A)\\bar{x},\n\\]\nwhere \\(A'\\) is the inverse of A. More-\nover, since y is known, our goal is to generate x such that\nthe null-space part \\((I - A'A)\\bar{x}\\) is in the same distribution\nas the range-space part \\(A'y\\). To achieve this goal, DDNM\nchanges the way that \\(x_{t-1}\\) is sampled. Instead of sampling\n\\(x_{t-1}\\) from \\(p(x_{t-1}|x_t, x_0)\\) which yields noisy intermediate\nstates, DDNM first outputs an estimation to \\(x_0\\) at time step t:\n\\[\nx_{0t} =\n\\frac{1}{\\sqrt{a_t}} (x_t - \\epsilon_\\theta(x_t, t)\\sqrt{1 -\\bar{a}_t})\n\\]\nThen, in order to finally output a \\(x_0\\) that satisfies \\(Ax_0 = y\\),\nDDNM fixes the range-space as \\(A'y\\) and yields a refined\nestimation \\(x_{0|t} = A'y + (I - A'A)x_{0|t}\\). Thus, instead of\nsampling from \\(p(x_{t-1}|tt, x_0)\\), DDNM samples \\(x_{t-1}\\) from\n\\(p(x_{t-1}|x_t, x_{0t})\\):\n\\[\nx_{t-1} =\n\\frac{\\sqrt{a_{t-1}b_t}}{1 - \\bar{a}_t}\n-\\frac{\\sqrt{a_t(1 - \\bar{a}_{t-1})}}{\\sqrt{1 - \\bar{a}_t}}\n\\bar{x}_{0|t} +\n\\sqrt{\\sigma} \\epsilon\n\\]\nBy applying Equations 6 and 7 iteratively, DDNM is pre-\ntrained on the loss function 5."}, {"title": "Experiment Settings", "content": "Dataset and models: We consider CIFAR10 (Krizhevsky\n2009) and STL10 (Coates, Ng, and Lee 2011) as the datasets\nto pre-train self-supervised learning encoder. When CI-\nFAR10 is used as the pre-training dataset, we use STL10,\nSVHN (Netzer et al. 2011), and EuroSAT (Helber et al. 2019)\nas the downstream datasets. When STL10 is applied to pre-\ntrained the encoder, we use CIFAR10, SVHN, and EuroSAT\nas the downstream dataset. We resize all images to be 32\u00d732\nto be consistent. The details of these datasets can be found"}, {"title": "Evaluation", "content": "Attack settings: We consider attacks that poison the pre-\ntraining dataset (Saha et al. 2022; Liu, Jia, and Gong 2022) or\nmanipulate the parameters of an encoder (Jia, Liu, and Gong\n2022). As the attack that manipulates the encoder parameters\nis usually more powerful, we consider (Jia, Liu, and Gong\n2022) by default. We adopt the publicly available implemen-\ntation of (Saha et al. 2022; Jia, Liu, and Gong 2022) and\nimplement (Liu, Jia, and Gong 2022) by ourselves. We use\nthe default parameter settings in those works. In particular,\nwe set the default trigger size to 10\u00d710. Moreover, we use\nrandomly generated trigger patterns.\nDefense settings: Our TrojanDec has two parameters: k\n(mask size) and s (step size). By default, we set k and s to\n15 and 1, respectively. We will study the impact of each of\nthem in the ablation study. For image restoration, we use a\npre-trained diffusion model from (ddn 2022).\nEvaluation metrics: To evaluate the effectiveness of our\napproach in detecting trojaned test images, we use the False\nPositive Rate (FPR) and False Negative Rate (FNR) as met-\nrics. FPR (or FNR) measures the fraction of clean (or trojan)\ntest images that are detected as trojan (or clean). Addition-\nally, to measure the effectiveness of TrojanDec in recovering\ntrojaned images, we apply our method to both the training\nand testing data of the downstream classifier and use the Ac-\ncuracy (ACC) and Attack Success Rate (ASR) to measure the\nclassifier's accuracy and attack success. Specifically, ACC\nmeasures the prediction accuracy of the downstream classi-\nfier for clean test images, while ASR quantifies the fraction\nof trojaned images predicted as the target class."}, {"title": "Defending against State-of-the-art Attacks", "content": "Parameter manipulation attacks: Table 2 and 1 present\nthe performance of our TrojanDec in trojan detection and\nremoval against trojan attack that manipulate the parameters\nof encoders (Jia, Liu, and Gong 2022). We have three ob-\nservations from the results. First, our TrojanDec effectively\ndetects trojaned test inputs, as evidenced by the low FNR in\nTable 2. Second, TrojanDec can successfully remove trojans\nfrom trojaned inputs. In Table 1, the ASR values signifi-\ncantly reduce after applying our method to trojaned encoders,\ncompared to trojaned encoders without TrojanDec. For in-\nstance, for the trojaned encoder pre-trained on CIFAR10\nand the downstream dataset being STL10, the ASR without\nTrojanDec is 100%, whereas the ASR with TrojanDec is de-\ncreased to 1.43%. It is important to note that while the ASR\nwith TrojanDec are not zero, they are comparable to the ASR\nof clean encoders. This is because downstream classifiers can\nmisclassify some test inputs, leading to slightly higher ASR.\nOur third observation is that TrojanDec successfully main-\ntains the utility of the encoders. Regardless of whether the\nencoder is clean or trojaned, the FPR values remain small and\nthe ACC values are comparable to those without TrojanDec.\nOverall, our TrojanDec shows promising performance in de-\ntecting and removing trojans from test images (if any), while\nmaintaining the utility of the encoders.\nPre-training dataset poisoning attacks: We also evaluate\nour method on trojan attacks that poison the pre-training\ndataset (Saha et al. 2022; Liu, Jia, and Gong 2022). For each\nattack, we craft a trojaned encoder pre-trained on CIFAR10\nand set the target downstream task to be STL10. The results\nin Table 11a and 11b in the Appendix show our TrojanDec\ncan defend against these trojan attacks effectively.\nComparing TrojanDec with Existing Defenses\nWe compare our TrojanDec with several state-of-the-art\ntesting-phase defenses (Gao et al. 2019; Doan, Abbasnejad,\nand Ranasinghe 2020; Ma et al. 2023). In addition, we show\nexisting state-of-the-art training-phase defenses (Wang et al.\n2019; Xu et al. 2021; Zeng et al. 2022) are not effective. We\nleverage their publicly available code in our experiments.\nBeatrix (Ma et al. 2023): Beatrix utilizes Gramian informa-\ntion to distinguish trojaned inputs from clean ones. Beatrix\nrequires the defender to have enough clean validation images.\nHowever, Beatrix is not effective when the defender does\nnot have enough clean validation images. To compare with\nBeatrix, we assume that the defender has varying numbers\nof clean validation images and plot the results on STL10 in\nFigure 4a. As TrojanDec does not rely on clean validation im-\nages, the FPR and FNR remain unchanged irrespective of the\nnumber of clean validation images. We have two observations\nfrom the results. First, our TrojanDec achieves comparable\nperformance with Beatrix when the defender has enough\nclean validation images (e.g., more than 30 clean validation\nimages). Second, our TrojanDec significantly outperforms\nBeatrix when the clean validation images are insufficient. We\nhave similar observations on other downstream datasets, as\nshown in Figures 5a and 6a in the Appendix.\nStrip (Gao et al. 2019): Strip also leverages clean validation\nimages to detect whether a test input is trojaned or not. Fig-\nure 4b compares Strip's performance on STL10 with that of\nTrojanDec. Similar to Beatrix, when the defender has limited\nclean validation images, Strip's FPR is high. As clean valida-\ntion images number increases, Strip's performance improves.\nWe find that our TrojanDec still outperforms Strip even if we\nassume the defender has 30 clean validation images. Simi-\nlar trends were observed on other downstream datasets, as\npresented in Figure 5b and 6b in the Appendix.\nFebruus: Februus (Doan, Abbasnejad, and Ranasinghe\n2020) is a defense method that uses GradCAM (Selvaraju\net al. 2017) to determine the potential trigger area. Then, it\napplies a Wasserstein GAN (Gulrajani et al. 2017) for in-\npainting the detected area. Table 6 in the Appendix shows\nthe comparison results. Our TrojanDec achieves slightly bet-\nter ACC and lower ASR for the trojaned inputs. Besides,\nwe note that Februus has two limitations compared to our\nmethod. Firstly, it cannot detect if a test input contains the\ntrojan trigger, but can only directly restore every test input.\nThis makes their defense method limited in scenarios where\nthe trojan behaviours are expected to be reported. Secondly,\nits restoration technique requires the defender's knowledge\nof the distribution of the real unmask testing data, which is\nnot available in the data-free setting we consider.\nOther defenses: We also evaluate the state-of-the-art\ntraining-phase defense methods (Wang et al. 2019; Xu et al.\n2021; Zeng et al. 2022) against trojan attacks. Table 7 in the\nAppendix shows these methods fail to defend against trojan\nattacks to encoders. Our experimental results confirm the\nobservations in (Carlini and Terzis 2022; Jia, Liu, and Gong\n2022; Feng et al. 2023)."}, {"title": "Real-world Encoders", "content": "To further evaluate the effectiveness of TrojanDec, we apply\nit on 2 real-world self-supervised learning encoders: 1) the\nencoder pre-trained on ImageNet released by Google (sim\n2020) and 2) CLIP encoder pre-trained on 400 million image-\ntext pairs released by OpenAI (cli 2021). Both encoders take\n224x224 images as inputs. To fit its input size, we set k to\n150 and s to 10. We directly apply our method on trojaned\nCLIP encoders released in (bad 2021). The trojan detection\nand removal results presented in Table 3 show that TrojanDec\nis effective in defending against trojan attacks to large, real-\nworld pre-trained self-supervised learning encoders. More\nexperimental results on larger-scale datasets are in Table 8 in\nthe Appendix."}, {"title": "Adaptive Attacks", "content": "In this section, we consider several adaptive attacks the adver-\nsary can take to evade our defense. In summary, we find that\nTrojanDec can effectively defend against adaptive attacks.\nTrigger with large size: The first countermeasure we con-\nsider is that the attacker uses a large trigger such that the\ntrigger size is bigger than the mask size set by TrojanDec. Ta-\nble 9 in the Appendix presents the defense results where the\ntrigger size is 16\u00d716. The FNR values have slightly increased\ncompared to Table 2 where the trigger size is 10\u00d710. Also,\nthe ASR after applying TrojanDec is decreased to small val-\nues. This is because our method does not necessarily require\nthe mask to fully cover the trigger. As shown in Figure 2,\nonly one fourth of the trigger being covered by the mask is\nenough to divide the metadata into two clusters. Our results\ndemonstrate that our TrojanDec can effective defend against\ntrojan attacks with large trigger sizes.\nTrigger with dynamic location: We also consider an at-\ntacker that performs a dynamic trojan attack to the encoder.\nSince there is no existing work studying dynamic trojan at-\ntacks to encoders, we generalize the idea from (Salem et al.\n2022) to perform the dynamic trojan attack. In particular, we\ninject the trojan into the encoder in the same way as (Jia,\nLiu, and Gong 2022) and in the testing phase, we embed\nthe trigger into the test input at a random location. We eval-\nuate the effectiveness of our TrojanDec against these two\nadpative attacks on 3 encoders pre-trained on CIFAR10 and\nuse STL10 as the downstream task. Table 4 presents the re-\nsults, which shows that our TrojanDec can effectively defend\nagainst trojan attacks with dynamic trigger location.\nTrojan attacks in supervised learning (Chen et al. 2017;\nYao et al. 2019; Ning et al. 2021): We notice that in super-\nvised learning, there exist other types of trojan attacks, e.g.,\nblending attack (Chen et al. 2017), latent attack (Yao et al.\n2019), and clean-label attack (Ning et al. 2021). For the blend-\ning attack, we generalize it into the self-supervised learning\ndomain by using the default trigger presented in (Chen et al.\n2017). However, as Table 10 in the Appendix reveals, the\nattack is not very successful, as the accuracy after the trojan\ninjection decreases too much, and the ASR is not high for\nSTL10 and EuroSAT. This indicates that it is not trivial to\ngeneralize the existing blending trojan attacks to the self-\nsupervised learning context. For latent and clean-label trojan\nattacks, since they require label information to perform the at-\ntack, they are not applicable in attacking encoders pre-trained\non unlabeled data."}, {"title": "Ablation Study", "content": "Impact of k and s: Our TrojanDec has two parameters: k\nand s, which control the mask size and the step size used to\ngenerate the mask set in the metadata extraction component.\nFigure 7 and 8 in the Appendix show the impact of k and s\nrespectively. When the value of k is small, e.g., less than 17,\nboth the trojan detection and removal achieve a consistently\ngood performance. When k is large, we find that FNR and\nASR increase. The reason for this trend is that when k is\nlarge, it is likely that almost all masks in the mask set will\ncover a significant portion of the real trigger, resulting in\nnone of the masked variants containing a complete trigger.\nThis leads to all masked variants being not similar to the\noriginal trojaned input, making the detection component less\neffective. We have a similar observation for s. Our results\ndemonstrate that we can set a smaller k and s in practice."}, {"title": "Conclusion and Future Work", "content": "In this work, we propose TrojanDec, a data-free framework\nto defend against trojan attacks to self-supervised learning.\nSpecifically, the defender first extracts metadata from a given\ntest input and identifies whether it has malicious trigger using\nstatistical analysis. Then, the defender can restore the tro-\njaned test input via a pre-trained diffusion model. Also, our\nextensive evaluation results show that TrojanDec is effective\nagainst multiple types of trojan attacks, and outperforms the\nexisting trojan defense mechanisms, especially in the setting\nwhere the defender only has access to a limited number of\nclean data. Moreover, we demonstrate that several counter-\nmeasures fail to break TrojanDec. Interesting future works\nwould be to extend our framework to defend against trojan\nattacks with non-patch-based triggers or other domains."}]}