{"title": "DarkSAM: Fooling Segment Anything Model to Segment Nothing", "authors": ["Ziqi Zhou", "Yufei Song", "Minghui Li", "Shengshan Hu", "Xianlong Wang", "Leo Yu Zhang", "Dezhong Yao", "Hai Jin"], "abstract": "Segment Anything Model (SAM) has recently gained much attention for its outstanding generalization to unseen data and tasks. Despite its promising prospect, the vulnerabilities of SAM, especially to universal adversarial perturbation (UAP) have not been thoroughly investigated yet. In this paper, we propose DarkSAM, the first prompt-free universal attack framework against SAM, including a semantic decoupling-based spatial attack and a texture distortion-based frequency attack. We first divide the output of SAM into foreground and background. Then, we design a shadow target strategy to obtain the semantic blueprint of the image as the attack target. DarkSAM is dedicated to fooling SAM by extracting and destroying crucial object features from images in both spatial and frequency domains. In the spatial domain, we disrupt the semantics of both the foreground and background in the image to confuse SAM. In the frequency domain, we further enhance the attack effectiveness by distorting the high-frequency components (i.e., texture information) of the image. Consequently, with a single UAP, DarkSAM renders SAM incapable of segmenting objects across diverse images with varying prompts. Experimental results on four datasets for SAM and its two variant models demonstrate the powerful attack capability and transferability of DarkSAM. Our codes are available at: https://github.com/CGCL-codes/DarkSAM.", "sections": [{"title": "1 Introduction", "content": "With the advancement of deep learning, large language models, such as GPT [2], LaMDA [33], and PaLM [6], have achieved tremendous success, yet the development of large vision models lags behind. Recently, Segment Anything Model (SAM) [19] was proposed as a foundational vision model, demonstrating exceptional generalization capabilities for handling complex segmentation tasks. Unlike traditional segmentation models [24, 42] that output pixel-level labels, SAM introduces a novel prompt-guided image segmentation paradigm by directly producing label-free masks for object segmentation. Benefiting from its powerful zero-shot capability, SAM has been rapidly deployed across various downstream scenarios, such as medical images [34], videos [36], and 3D point clouds [12]."}, {"title": "2 Background and Related Works", "content": null}, {"title": "2.1 Prompt-guided Image Segmentation", "content": "Segment Anything Model [19] is a cutting-edge advancement in computer vision, garnering widespread attention [34, 20, 5, 3, 22] for its powerful segmentation capabilities. Recent works have been dedicated to exploring various variants of SAM to further enhance performance, such as HQ-SAM [18], PerSAM [40] and MobileSAM [37]. Distinct from traditional semantic segmentation models [24, 42, 4] that predominantly focus on pixel-level label prediction, SAM undertakes the label-free mask prediction by generating object masks for a wide array of subjects using prompts. It consists of three components: an image encoder, a prompt encoder, and a lightweight mask decoder. The image encoder generates image representations in latent space and the prompt encoder utilizes positional embeddings for representing prompts, such as points and boxes. The mask decoder, combining outputs from both image and prompt encoders, predicts effective masks to segment targeted objects.\nGiven an image $x \\in R^{H\\times W\\times C}$ and a corresponding prompt $P$ to SAM, denoted as $f(x) \\in R^{H\\times W}$, the model returns a mask $m$ with the predicted segmentation. The prediction process of SAM can be represented as follows:\n$m = f_{\\theta}(x, P),$"}, {"title": "2.2 Universal Adversarial Perturbation", "content": "Deep neural networks have been shown vulnerable to adversarial examples [10, 25, 44, 45, 46], where attackers can deceive models by introducing subtle noise to images. Universal adversarial perturbation [27] (UAP) was first proposed to fool the victim model by imposing a single adversarial perturbation on a series of images. Existing works can be divided into data-dependent universal adversarial attacks [27, 14, 30] and data-free universal attacks [28, 29, 31], both designed for classification attacks. The former relies on the specific data characteristics of target dataset for UAP generation, while the latter provides a more generalized approach without relying on such data. Meanwhile, some works [15] have also explored UAPs for traditional segmentation models, but they rely on pixel-level labels, which are not applicable to emerging prompt-guided segmentation models. The concurrent works [8, 13] explore UAPs against SAM from the perspectives of direct noise optimization and perturbing the output of the image encoder of SAM, respectively. Different from them, we aim to comprehensively decouple and disrupt crucial object features in images from both spatial and frequency domains, thereby deceiving SAM into failing to segment input images."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Problem Formulation", "content": "As a fundamental vision model, SAM typically operates in an online mode, allowing users to set prompts randomly. Therefore, we define the threat model as a quasi-black-box setting, where ad-"}, {"title": "3.2 Intuition Behind DarkSAM", "content": "Unlike the standard deep learning paradigm that inputs a single image and outputs a one-hot label or pixel-level label, SAM requires both images and prompts as inputs, and then outputs label-free masks, indicating the shape information of critical objects. Therefore, a truly universal adversarial attack against SAM should implement a single perturbation to achieve ineffective segmentation for any combination between a series of images and different prompts. However, this task is hindered by the following challenges:\nChallenge I: The dual ambiguity in attack targets arising from varying images and prompts. Previous UAP works only need to optimize in the target images, hence the introduction of prompts could lead to invalid attacks, as different prompts for a fixed image yield distinct segmentation results. For instance, the image in the top-left corner of Fig. 2 shows a can and a spoon. For the same image, feeding different prompts will result in different masks output by SAM (see Fig. 2(b)). In conclusion, diverse variations in target images and prompts increase the uncertainty of attack targets. For varying images, existing UAP solutions (e.g., UAPGD [9]) can provide references, and the main challenge here is the uncertainty of the attack target brought by unknown prompts. To this end, we propose a shadow target strategy by increasing the number of prompts during the attack process to enhance the cross-prompt transferability of UAP. Specifically, for a given input image, we randomly select k prompts (e.g., points or boxes) to create a prompt auxiliary set. By merging their masks output by SAM, we form a semantic blueprint of the image, which serves as the target for our attack, as illustrated in Fig. 2(c). This semantic blueprint effectively encompasses the main semantic content of the original image, substantially reducing the ambiguity associated with unknown prompts.\nChallenge II: Suboptimal attack efficacy due to semantic decoupling deficiency. Since prompt-guided segmentation models output masks that are neither one-hot nor pixel-level labels, traditional attack methods that rely on label deviation for optimization guidance become ineffective. Another approach involves directly modifying the output, such as adjusting the adversarial examples' masks to diverge from their originals, potentially yielding marginal attack success as verified in Sec. 4.4. Nonetheless, the intrinsic sensitivity of segmentation models to pixel-level details significantly constrains the potency of these attacks, underscoring a notable limitation in their applicability.\nGiven the focus of prompt-guided segmentation models on local, critical object features rather than global image features, we are motivated to comprehensively decouple the key semantic features of an image from the perspective of both spatial and frequency domains, aiming to fool SAM by"}, {"title": "3.3 DarkSAM: A Complete Illustration", "content": "In this section, we present DarkSAM, a novel prompt-free hybrid spatial-frequency universal adversarial attack against the prompt-guided image segmentation models (i.e., SAM and its variants). The pipeline of DarkSAM is depicted in Fig. 3, encompassing a semantic decoupling-based spatial attack and a texture distortion-based frequency attack. We start by randomly generating k different prompts to form an auxiliary prompt set $P_a$, acquiring the semantic blueprints of the target images as the attack targets. By individually manipulating the semantic content of adversarial examples' foreground and background in the spatial domain, and increasing the distance between the HFC of adversarial and benign examples in the frequency domain, while also constraining the difference in their LFC, we enhance the attack performance and transferability of the UAP. The overall optimization objective $I_{total}$ of DarkSAM is as follows:\n$I_{total} = I_{sa} + \\lambda I_{fa},$ (3)\nwhere $I_{sa}$ and $I_{fa}$ are the spatial and frequency attack losses, and $\\lambda$ controls the importance.\nSemantic decoupling-based spatial attack. Initially, we utilize two Boolean mask $m_{fg}$ and $m_{\\bar{f}g}$ to separately extract the foreground and background mask of the adversarial examples based on the positive and negative values in the mask output by SAM. As for the foreground, our intention is to render it unidentifiable and unsegmentable by SAM. Thus, we optimize its mask towards a negative fake mask $\\Xi_{neg}$, enabling its fusion with the background to achieve segmentation evasion. The foreground evasion loss $I_{fe}$ can be described as:\n$I_{fe} = I_d(f_{\\theta}(x + \\delta, P_a) \\cdot m_{fg}, \\Xi_{neg}),$ (4)\nwhere $\\Xi_{neg}$ is a fake mask that conforms to the shape of the image, containing threshold values of -$\\tau$ in regions corresponding to the foreground, and 0 elsewhere. $I_d$ serves as the distance metric function, representing the mean squared error loss. For the background, we optimize its mask towards a positive fake mask $\\Xi_{pos}$ (opposite to $\\Xi_{neg}$), misleading SAM into interpreting it as a"}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experimental Setup", "content": "Datasets and Models. We evaluate our method using four public segmentation datasets: ADE20K [43], MS-COCO [23], CITYSCAPES [7], and SA-1B [19]. For each dataset, we randomly select 100 images for UAP generation and 2,000 images for testing purposes. All images are uniformly resized to 3\u00d71024\u00d71024. For victim models, we use the pre-trained SAM [19], HQ-SAM [18] and PerSAM [40] with the ViT-B backbone.\nParameter Setting. Following [27, 9, 32], we set the upper bound of UAP to 10/255. For our experiments, we adjust the hyperparameters k, \u03c4, \u03bb and \u03bc to 10, 1, 0.1 and 0.01, respectively, and set the batch size to 1. To evaluate the cross-prompt attack capabilities of DarkSAM, we employ three distinct prompt types: point, box, and segment everything (also abbreviated as \u201call\u201d) mode.\nEvaluation Metrics. To evaluate the effectiveness of DarkSAM, we use the mean Intersection over Union (mIoU) metric. To facilitate data presentation, we also use the attack success rate (ASR) as a metric to evaluate attack performance. ASR represents the difference between the mIoU values of benign and adversarial examples."}, {"title": "4.2 Attack Performance", "content": "To comprehensively evaluate DarkSAM's effectiveness, we perform experiments on three prompt-guided image segmentation models including SAM, HQ-SAM, and PerSAM, across four datasets. For each setup, we generate UAPs using point and box prompts, respectively, and then evaluate DarkSAM's attack performance using the corresponding single-point or single-box prompt. We first calculate the clean mIoU of different models across four datasets using point and box as prompts."}, {"title": "4.3 Transferability Study", "content": "We study the attack transferability of DarkSAM across data domain, prompt types and models, respectively. Cross-domain. The results in Tab. 1 demonstrate DarkSAM's excellent cross-domain transferability, where UAPs generated with the surrogate dataset (ADE20K) achieve a high ASR on datasets from various different domains. We also explore the role of the frequency attack (i.e., $I_{fa}$, denoted as FA) in enhancing cross-domain transferability. As shown in Fig. 4 (a), frequency attack can effectively improve the attack performance based on the spatial attack (i.e., $I_{sa}$, denoted as SA).\n\u25cf Cross-prompt. We examine the performance of DarkSAM across various types of prompts. As demonstrated in the last three columns of Fig. 5, UAPs created based on both point and box prompts perform well under the segment everything mode. Additionally, we provide results of transferability experiments between point and box prompts in Tab. 2. This includes testing UAPs created with point prompts in the box prompt setting and vice versa. Based on the observed results, it is discernible that UAPs crafted using box prompts generally demonstrate better transferability compared to those using point prompts. This increased efficacy can likely be attributed to the box prompts offering more integral and detailed prompt information. \u25cf Cross-model. We use UAPs created with points and boxes based on SAM to attack HQ-SAM and PER-SAM. The results in Fig. 4 (b) - (e) showcase DarkSAM's exceptional transferability across different models."}, {"title": "4.4 Comparison Study", "content": "To comprehensively demonstrate the superiority of our proposed method, we compare DarkSAM with popular UAP schemes, including UAP [27], UAPGD [9], and SSP [32]. We also consider the state-of-the-art adversarial attack against traditional segmentation models, SegPGD [11], and the latest sample-wise attack against SAM, Attack-SAM [39]. For a fair comparison, we adapt them to a UAP optimization strategy and keep other settings consistent with DarkSAM. We select SAM as the victim model and assess the effectiveness of these UAP methods across four datasets, using the same dataset for both generating and testing the UAPs. The results in Tab. 3 indicate that"}, {"title": "4.5 Abaltion Study", "content": "In this section, we explore the effect of different modules, prompt number, attack strengths, training data size, and threshold values on DarkSAM. We conduct experiments using point prompts on SAM across the ADE20K dataset.\nThe effect of different modules. We investigate the effect of various modules on the attack performance of DarkSAM. For clarity and convenience, we use A, B, C, and D to denote $I_{fe}$, $I_{bm}$, $\\tau_{hfc}$, and $\\tau_{lfc}$, respectively. The results in Fig. 6 (a) show that no variants can compete with the complete method, implying the indispensability of each component for DarkSAM.\nThe effect of prompt number. We study the effect of the prompt number in proposed shadow target strategy on attack performance of DarkSAM. We conduct experiments with varying numbers of point prompts, ranging from 1 to 100. The results in Fig. 6 (b) show a gradual increase in attack performance from 1 to 10 (default setting), followed by a downward trend. This could be attributed to an excess of random points leading to masks with redundant information, thereby impacting the attack efficacy.\nThe effect of perturbation budget. As shown in Fig. 6 (c) and Fig. A9, we evaluate DarkSAM's attack performance with $\\epsilon$ from 4/255 to 32/255. With the increase in $\\epsilon$, there is a corresponding enhancement in attack performance. Notably, our attack still maintains high efficacy at the 6/255 setting, with an average ASR exceeding 45%.\nThe effect of number of training samples. We explore the effect of varying the number of training images used to create UAP on DarkSAM. Utilizing a range from 10 to 1000 images to craft UAPs, the results in Fig. 6 (d) reveal that employing merely 100 images can achieve excellent attack performance, demonstrating a strong applicability advantage.\nThe effect of threshold values. We examine the effect of varying threshold values $\\tau$ in the fake mask $\\xi$ on DarkSAM. As illustrated in Fig. 6 (e), we test a range of values from 1 to 1000. The results indicate that these different values have a minimal overall effect on DarkSAM's performance."}, {"title": "5 Conclusions, Limitations, and Broader Impact", "content": "In this paper, we propose DarkSAM, the first truly universal adversarial attack against SAM. With a single perturbation, DarkSAM renders SAM incapable of segmenting objects across diverse images with varying prompts, thereby exposing its vulnerability. To tackle the challenge of dual ambiguity in attack targets, we present a shadow target strategy to obtain semantic blueprint as a attack target. We then design a novel prompt-free hybrid spatial-frequency universal attack framework, which consists of a semantic decoupling-based spatial attack and a texture distortion-based frequency attack. By disrupting the crucial object features in both the spatial and frequency domains of the"}, {"title": "A Datasets", "content": "\u2022 ADE20K: ADE20K [43] is a dataset for scene parsing that includes images from a variety of environments. It contains more than 20,000 images, classified into 150 categories, covering both natural landscapes and indoor settings. Each image in ADE20K is pixel-wise annotated, making it suitable for scene parsing and semantic segmentation tasks.\n\u2022 MS-COCO: MS-COCO [23] is a large-scale dataset for image recognition, segmentation, and image captioning. It contains more than 200,000 labeled images, 150,000 validation images, and over 80,000 test images. The dataset includes 80 different object categories and over 250,000 object instances. MS-COCO is known for its detailed annotations for each image, including object segmentation, object detection, and image captioning.\n\u2022 CITYSCAPES: CITYSCAPES [7] is a dataset for urban street scenes, primarily used for training and testing vision systems for autonomous driving. It includes street scenes from 50 different cities, with approximately 5,000 finely annotated images. These images include various urban scenarios and a range of traffic participants.\n\u2022 SA-1B: SA-1B [19] contains 11 million diverse, high-resolution, privacy-protected images and 1.1 billion high-quality segmentation masks. These masks were automatically generated by SAM. The dataset aims to facilitate computer vision research and is characterized by an average of 100 masks per image."}, {"title": "B Optimization", "content": "We provide the detailed optimization process of DarkSAM in Algorithm 1. Given a series of images, DarkSAM first acquires the attack targets through the shadow target strategy, and then optimizes a single UAP by disrupting their foreground and background semantic information in the spatial domain and distorting texture information in the frequency domain, thereby fooling SAM into failing to segment the content of the adversarial examples. The generated UAP can be applied to images from different datasets without being limited to a specific image."}, {"title": "C Platform", "content": "Experiments are conducted on a server running a 64-bit Ubuntu 20.04.1 system with an Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz processor, 125GB memory, and two Nvidia GeForce RTX"}, {"title": "D Supplementary Attack Performance", "content": null}, {"title": "D.1 Evaluation on MobileSAM", "content": "We evaluate the attack performance of DarkSAM against another SAM's variant model, Mobile-SAM [37], on four datasets. All experimental settings are kept consistent with Sec.4.2 of the manuscript. The results in Tab. Al demonstrate the effectiveness of DarkSAM against MobileSAM, further proving its strong attack capability. Notably, in line with the conclusions in Sec.4.2 of the manuscript, the choice of surrogate datasets has a certain impact on the attack performance. SA-1B serves as a notably superior surrogate dataset, while CITYSCAPES exhibits comparatively lower performance in certain scenarios. This discrepancy may be attributed to CITYSCAPES' limited scope, which solely encompasses the urban street scene, consequently restricting the transferability of the generated UAPs. Hence, adversaries gain an advantage by opting for a semantically diverse dataset, encompassing a wide range of categories, objects, and scenes, to augment the attack performance of UAPS."}, {"title": "D.2 Evaluation on SAM with ViT-L backbone", "content": "We present both quantitative and qualitative results of DarkSAM on SAM with a ViT-L backbone, denoted as SAM-L. The quantitative findings in Table A2 illustrate the effectiveness of DarkSAM in deceiving SAM-L. Notably, these results indicate that SAM-L exhibits greater robustness compared to SAM-B (SAM with a ViT-B backbone) due to its more intricate network architecture. Additionally, we offer visualization results of DarkSAM's attacks on SAM-L under point, box, and segment everything modes. Figs. Al and A2 demonstrate that adversarial examples generated based on point"}, {"title": "E Supplementary Transferability Study", "content": "In this section, we delve deeper into the cross-model transferability of DarkSAM, considering both different model types and diverse model backbones. We maintain uniformity with the experimental settings outlined in Sec. 4.3.\n1) Cross model type transferability. We explore the transferability of UAPs crafted by DarkSAM on PerSAM and HQ-SAM when attacking other types of models. These models all share the ViT-B backbone. The \"Point-\" and \"Box-\" prefixes indicate that the UAPs are crafted and tested under point and box prompts, respectively. The suffixes \u201cHQ2PER\u201d and \u201cHQ2SAM\u201d denote the UAPs crafted on HQ-SAM against PerSAM and SAM, respectively. Similar notations carry the same implications. The results in Fig. A5 further demonstrate the robust cross-model type transferability of DarkSAM.\n2) Cross model backbone transferability. We investigate the cross-model backbone transferability of DarkSAM. Specifically, we craft UAPs based on the ADE20K dataset on SAM-L and SAM-B, respectively, and test the transferability of these attacks between the two models. From Figs. A6 and A7, we can see that adversarial examples crafted on SAM-B effectively mislead SAM-L, and conversely, those crafted on SAM-L deceive SAM-B. These results demonstrate the cross-model backbone transferability of DarkSAM."}, {"title": "F Supplementary Ablation Study", "content": "1) The effect of random seeds. Considering the relationship between random seeds and the selection of images in training and testing, we investigate the effect of random seeds on DarkSAM. All our experiments default to a random seed setting of 100. As illustrated in Fig. A12, we select eight different random seeds and conduct experiments to attack SAM on the ADE20K dataset with these seeds. \"P2P\" and \"B2B\" respectively denote the creation and testing of UAPs using point and box prompts. The results in Fig. A12 indicate that DarkSAM consistently exhibits stable and superior attack performance across various random seed settings\n2) The mixed use of point and box prompts in the shadow target strategy. Tab. 1 of the manuscript showcases the impressive attack capabilities of UAPs generated by DarkSAM, utilizing ten randomly selected points and boxes. Building upon this, we delve into the effects of employing a hybrid approach of points and boxes as prompts in the shadow target strategy for DarkSAM. In this method, we craft UAPs using a balanced mix of five random points and five boxes. The outcomes, as detailed in Tab. A3, reveal that UAPs constructed with this mixed approach maintain robust attack performance. This finding accentuates the adaptability and effectiveness of our proposed shadow target strategy.\n3) Multipoint evaluation. We explore the effects of using multiple point prompts during the inference phase of SAM on the efficacy of DarkSAM's attacks. Compared to single-point prompts,"}, {"title": "G Defense", "content": "SAM is renowned for its powerful zero-shot capabilities, thus we believe an appropriate defensive measure is to refrain from making additional structural and parametric modifications to the pre-trained SAM to avoid compromising its original knowledge. Therefore, we consider employing input preprocessing methods to counter adversarial examples. We select two famous image corruption methods from the Imagecorruptions repository, contrast (C) and brightness (B), to test adversarial examples. Results in Fig. A11 demonstrate that DarkSAM effectively withstands such preprocessing-based defenses."}, {"title": "H Visualization", "content": "We provide visualizations of UAPs generated by DarkSAM under various settings in Fig. A13. Notably, since the images in these datasets are not square, we pad the images during resizing and crop out the redundant parts for display."}]}