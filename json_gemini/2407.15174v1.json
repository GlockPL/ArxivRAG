{"title": "TADA: Temporal Adversarial Data Augmentation for Time Series Data", "authors": ["Byeong Tak Lee", "Joon-myoung Kwon", "Yong-Yeon Jo"], "abstract": "Domain generalization involves training machine learning models to perform robustly on unseen samples from out-of-distribution datasets. Adversarial Data Augmentation (ADA) is a commonly used approach that enhances model adaptability by incorporating synthetic samples, designed to simulate potential unseen samples. While ADA effectively addresses amplitude-related distribution shifts, it falls short in managing temporal shifts, which are essential for time series data. To address this limitation, we propose the Temporal Adversarial Data Augmentation for time teries Data (TADA), which incorporates a time warping technique specifically targeting temporal shifts. Recognizing the challenge of non-differentiability in traditional time warping, we make it differentiable by leveraging phase shifts in the frequency domain. Our evaluations across diverse domains demonstrate that TADA significantly outperforms existing ADA variants, enhancing model performance across time series datasets with varied distributions.", "sections": [{"title": "Introduction", "content": "Most machine learning models are developed on the assumption that their training data reliably represents the broader population. Unfortunately, this assumption is often incorrect. The distribution of real-world data often deviates from that of the training dataset, and such discrepancies lead to performance degradation [1-5]. An adversarial data augmentation (ADA) is one of the approaches that addresses distribution shift problem by solving worst-case scenarios around the original distribution [1, 2]. By training the model with adversarial synthetic samples, which are made extremely difficult for the model to classify, we can ensure that the model performs reliably under less challenging conditions, such as distribution shift.\nTime series data can be depicted on a 2D graph, with the amplitude axis representing magnitude of data points and the temporal axis indicating the sequence and timing of these points. While ADA effectively simulates samples that reflect distribution shifts related to amplitude axis, it is limited in generating samples that account for distribution shifts along the temporal axis [6, 7]. Since distribution shifts in time series data are associated with both axes, it is crucial to develop methods that also account for changes along the temporal axis.\nTo address the challenge of distribution shift along the time axis, we propose Temporal Adversarial Data Augmentation (TADA) for time series data. This method integrates a time warping technique with the ADA process [8, 9]. A significant challenge in directly applying time warping within ADA is the non-differentiable nature of index mapping, which disrupts gradient computation with respect\nto the input loss. To overcome this, we introduce parameterized phase shifts in the frequency domain. This approach enables distinct, differentiable phase shifts for individual segments of the time series data, effectively approximating time warping in the time domain. Our method thus facilitates a differentiable form of time warping, enhancing the robustness of model training against temporal variations.\nTo evaluate the effectiveness of our method for distribution shifts, we used different types of time series datasets, such as electrocardiogram (ECG), electroencephalogram (EEG), and human activity recording (HAR) [10, 11]. Each dataset consists of multiple distinct subgroups, each distinguished by different distributions. We conducted experiments on a single source domain generalization scenario, where models were trained on one subgroup and then evaluated on the remaining subgroups [12].\nThe results clearly demonstrate that the TADA enhances model performance across target domain with distribution different from those in the source domain. TADA's straightforward implementation, which involves inserting a proposed time-warping function between the input and the encoder, ensures it is compatible with other variants of ADA. Our experiments reveal that incorporating the proposed time-warping function boosts the performance of existing ADA variants. Finally, we evaluated whether TADA can simulate real-world distribution shifts. Our observations confirm that TADA effectively simulates real-world distributions in latent space, emphasizing its value in addressing the distribution shifts problems in time series data.\nThe contributions of this paper are as follows: (1) We identify a significant issue with the existing ADA approaches in time series data, particularly their inability to effectively handle temporal dynamics and distribution changes. (2) We propose a novel ADA specifically designed to perturb data along the temporal axis, thus enhancing the robustness of models against time-related variations in data distribution. (3) We demonstrate that our proposed method is highly effective in extensive experiments on the single-source domain generalization framework."}, {"title": "Related works", "content": "Domain Generalization. The distribution shift between source domain and unseen target domain often degrades [13\u201315]. Domain generalization emerges to address these problems. The goal of domain generalization is to train models to effectively generalize across multiple unseen target domain [16, 3, 17, 18].\nDepending on the coverage of the source domain, domain generalization is categorized into single-source and multiple-source domain generalization [12, 18]. For single-source domain generalization, the objective is to generalize the model when only a single domain is available for training. Since identifying domain-specific characteristics within the training dataset is impossible, single-domain generalization rely on generating fictitious distribution that simulate unseen domains. This includes data augmentation-based methods [1-3, 12]. For the multiple-source domain generalization, the objective is to learn invariant features across multiple domains that are not affected by domain shift in the source domain. This is based on the idea that the unchanging characteristics will also remain consistent in the unseen target domain, and domain alignment is a widely adopted approach in multiple-source domain generalization [4, 5, 19].\nAdversarial Data Augmentation. Adversarial Data Augmentation (ADA) enhances a model's ability to generalize across other domain or unseen distributions. This method tackle the worst-case problem as detailed in Equation 1 [1]. By employing ADA, the model achieves robust performance even when the distribution P differs markedly from the initial domain Po, effectively handling scenarios of domain shift.\n$\\mathop{\\text{min}}\\limits_{\\theta}\\mathop{\\text{max}}\\limits_{P} \\{E_{P}[L(\\theta; X, Y)] - \\gamma c_{D_0}(P, P_0)\\},$ (1)\nwhere X and Y denote a sample and its corresponding label, respectively, and \\ensuremath{\\gamma} is a penalty parameter.\nIn practical, an iterative training procedure that alternates between the maximization phase and the minimization phase is conducted. For the maximization phase, data points are generated from fictitious distribution P, which is constrained to deviate within a distance \\ensuremath{P} from the source distribution Po. On the other hand, for the minimization phase, the network \\ensuremath{\\theta} is updated to minimize loss on the data sampled from both P and Po."}, {"title": "Methodology", "content": "ADA consists of maximization phase, which focuses on data generation, and a minimization phase, which involves updating the model. We will redesign the maximization phase of ADA to gen-erate samples with distribution shift in temporal axis. Equation 2 represents the objective of the maximization phase from Equation 1.\n$X^{k} \\in \\arg \\mathop{\\text{max}}\\limits_{X} {L(\\theta; (X^{k-1},Y)) - \\gamma c_{\\theta}((X^{k-1}, Y), (X, Y))},$ (2)\nwhere k refers the number of iteration for adversarial update, X\u00ba and Xk are original samples and adversarial perturbed samples at the iteration k, respectively. \\ensuremath{\\gamma} is a penalty parameter, and c\\ensuremath{\\theta} is a distance measure.\nThe maximization phase can simply be understood as the addition of adversarial noise to the input, expressed as x = x + \u03a6, where I represents the noise added. This process can be thought as passing the input sample through a function F, which is parameterized by \u03a6: x = F(x; \u03a6) = x + \u03a6. Now, we update & instead of X iteratively as following Equation 3:\n$\\Phi^{*} \\in \\arg \\mathop{\\text{max}}\\limits_{\\Phi} {L(\\theta; (F(X; \\Phi^{k-1}), Y)) - \\gamma c_{\\theta}((F(X; \\Phi^{k-1}), Y), (X, Y))}$ (3)\nOur objective is to design a function F that alters the temporal characteristic of the time series data X without affecting its amplitude. To achieve this, we can think of incorporating the time warping in the function F to transform the sequence according to a warping path. However, the time warping is non-differentiable since it uses the index mapping. This makes iterative updating of 4 infeasible."}, {"title": "Differentiable Time Warping", "content": "To address this issue, we propose a new time warping function \\ensuremath{F} that is equivalent to traditional time warping but differentiable. Leveraging the duality of time shift in the time domain, which is non-differentiable, and a phase shift in the frequency domain, which is differentiable, we introduce differentiable time warping.\nWe begin with the mathematical formulation of time warping. Time warping involves a specific warping path, which maps the index of original sequence to transformed sequence. Applying time warping to a sequence of the time series data X = {X1, X2, ..., Xn} transforms it into the transformed sequence X' = {X1+\\ensuremath{\\delta}1, X2+\\ensuremath{\\delta}2, ..., Xn+\\ensuremath{\\delta}n }, where d indicates the distance that an index is moved from its current position (e.g., time shift), and \u2206 = {\\ensuremath{\\delta}1, \\ensuremath{\\delta}2, ..., \\ensuremath{\\delta}n} means the warping path."}, {"title": "Function f: Transformation to frequency domain", "content": "We first split time series data X into overlapping segments S because applying a phase shift to a segment results in the same distance shift across all its points. Thus, A point xi in the time series data X can belong to multiple segments, depending on the window size and the degree of overlap. The number of segments that a single point xi belongs to can vary based on how the segments are defined and the chosen window size. The function f converts these segments into the frequency domain. In practice, function fis typically implemented using the Short-Time Fourier Transform (STFT) as the following Equation 5:\n$f(s)[m, k] = \\sum_{n=0}^{N-1} x[n]w[n - m] \\cdot e^{-j2\\pi kn/N} = x[m,k],$ (5)\nwhere k is the frequency bin index, m is the frame index, N is the number of frequency bins, and X is a output corresponding to segment s in the frequency domain.\nHere, the window function w is defined as follows:\n$w[n - m] = \\begin{cases}\n1, & \\text{if } |n-m| < M, \\\\\n0, & \\text{otherwise},\n\\end{cases}$ (6)\nwhere M is the maximum window length."}, {"title": "Function g: Time warping in frequency domain", "content": "To apply the time warping in frequency domain, we perturb the temporal characteristics of a output Xi for the segment si by adding the noise that induces a corresponding phase shift Pi, resulting in g(f(s), \u03a6) = f(s) + \u03a6\u00b7 \u03c9. \u0391 \u03a6 is interpreted as the warping path.\nThe warping path must fulfill several conditions [20]: (1) it must be monotonic, ensuring that the sequence progresses in a single direction without reversals; (2) it must align with the boundaries, meaning the start and end indices of the path should coincide the start and end of the sequences being aligned; and (3) it must adhere to a warping distance constraint, which limits the extent of the warping. To ensure these conditions, we impose several constraints on I using the function g. With constraints, \u03a6 is redefined as \u03a6 = (93 \u00b7 92 \u00b791)($), with 4 denoting the perturbation parameters."}, {"title": "Function f'-1: Reconstruction to time domain", "content": "After perturbing the signal in the frequency domain, we use Inverse STFT (ISTFT) to reconstruct the segment s in the time domain. ISTFT converts a frequency-domain component x into its corresponding segment s as Equation 10.\n$s[n] = \\frac{1}{N}\\sum_{m=0}^{N-1}\\sum_{k=0}^{N-1} x[m, k]w'[n - m]e^{j2\\pi kn/N},$ (10)\nIn Section 3.2.1, we described how the time series data X is divided into overlapping segments S to facilitate the application of distinct phase shifts across all segments. This process leads to interference among the time-shifted segments when aggregating them back into the time series data X. Instead of averaging the overlapping windows in the function f as described in Equation 5, we extract the central value from each transformed segment using a specific window function w' in the inverse function f'-1.\n$w'[n - m] = \\begin{cases}\n1, & \\text{if } n = m \\\\\n0, & \\text{otherwise}\n\\end{cases}$ (11)"}, {"title": "Training Procedure", "content": "We describe the full procedure on Algorithm 1. Our proposed algorithm incorporates adversarial data augmentation to improve model robustness through an iterative training process, which the"}, {"title": "Experiments", "content": "In this section, we benchmark our proposed method against leading techniques within the single domain generalization framework, which involves training models on data from one domain and deploying them in another. Additionally, we perform ablation studies to assess the effects of integrating our method with other approaches. Lastly, by analyzing the representations extracted from models trained using our methodology, we confirm that our method effectively simulates a valid distribution shift."}, {"title": "Experimental Setup", "content": "Datasets. Datasets used in our experiments consist of multiple sub-datasets. The sub-datasets were sourced from various regions or collected using different devices, leading inherent distribution shifts among them. This diversity among sub-dataset allows a comprehensive evaluation of methods robustness across varied data characteristics.\nPhysionet Challenge 2021 (Physionet) dataset [10] is a public dataset for electrocardiogram (ECG) research, aimed at diagnosing a wide range of cardiac abnormalities. It consists of seven sub-datasets. Each dataset presents differences in demographic profiles. Among them, five datasets containing over 1,000 ECGs were selected for our experiment: PTB-XL [21], Chapman-Shaoxing [22], Ningbo [23], G12EC, and CPSC2018 [24]. The datasets involve multi-label classification of 26"}, {"title": "Performance Evaluation on Datasets with Different Domains", "content": "In the following tables, scores in boldface indicate the best performance, and underlined scores represent the second best. A column includes ADA+TADA meaning a combination of ADA and TADA.\nPhysionet dataset. Table 1 shows the average F1 scores on the Physionet dataset. For the PTBXL dataset, methods including the proposed one like TADA and ADA+TADA are the most effective. The G12EC dataset shows a close performance among several methods, with TADA leading slightly. The CPSC2018 dataset, which generally presents lower scores, finds Mixup to be the most effective method. For the Chapman dataset, the combination of ADA and TADA acheive the best. Lastly, the Ningbo dataset shows that ADA+TADA outperforms other methods. Overall, in the ECG domain,\nthe average scores present ADA+TADA as the best method. We have confirmed that applying TADA or RandConv enhances the model performance.\nPCL dataset. Table 2 presents the average F1 scores on the PCL dataset. In Dataset1, RandConv emerges as the best method, with TADA following as the second one. Dataset2 and Dataset3 show that ADA+TADA achieve the best performance, respectively. Overall, ADA+TADA showed the best performance on EEG datasets, and methods that included TADA consistently led to high performance.\nHHAR dataset. Table 3 shows the experimental result on HHAR dataset. We observed that the best methods are distributed across the different datasets. In the overall results, we noticed that when TADA was used independently, its performance significantly dropped, contrary to trends seen in other datasets in ECG and EEG domains. This suggests that the HHAR dataset may not particularly account for temporal features. Fortunately, the combination of TADA and ADA ultimately resulted in the best performance."}, {"title": "Performance Evaluation in combining the variants of ADA with TADA", "content": "One advantage of our proposed method is its compatibility with existing adversarial data augmentation techniques. We have integrated techniques from extensions of ADA, such as M-ADA [32] and ME-ADA [2], into TADA, resulting in variants like ME-TADA and M-TADA. As shown in Tables 1, 2, and 3, TADA can be effectively combined with ADA, leading to the development of combinations such as ME-(ADA+TADA) and M-(ADA+TADA).\nTable 4 presents the average F1 scores for each domain dataset. The results suggest that merging ADA with TADA generally enhances performance across datasets. Specifically, ADA+TADA yields the highest F1 score in the Physionet dataset, demonstrating the advantages of this combination. In the PCL dataset, M-(ADA+TADA) provides a significant performance improvement. Additionally, in the HHAR dataset, M-TADA outperforms other combinations. These findings indicate that applying TADA to relax worst-case scenarios is more effective than adopting a maximum-entropy criterion. This underscores the importance of tailoring the combination of data augmentation techniques to the unique characteristics of each dataset for optimal performance."}, {"title": "Visualization of Distribution Shifts", "content": "We claim that TADA induces a different type of distribution shift (e.g., a distribution with temporally shifted samples) compared to the distribution shift caused by ADA (e.g., a distribution with amplitude-modified samples). We expect that these two distinct distributions could effectively enhance the robustness and generalization of models for time series data. For verification purposes, we first extract samples from both the training dataset (i.e., a single source sub-dataset) and the test dataset (i.e., other target sub-datasets). We then employ UMAP [33] to visualize their scatteredness.\nFigure 1 displays the UMAP visualization for the datasets. Gray dots represent original samples from the target sub-datasets, which are likely the data points that the model aims to generalize towards. Orange dots represent samples from the source sub-dataset; these are the original data points on which the model may have been initially trained. Blue dots represent samples that have been augmented using ADA, which include amplitude variations of the source data. Green dots are samples generated by TADA, focusing on temporal shifts not covered by ADA.\nThe visualization reveals that gray and orange dots are somewhat intermixed but also form distinct clusters, indicating some overlap yet distinguishable characteristics between the source and target datasets. The blue and green dots are interspersed among the original source and target samples. The placement of these augmented points suggests that they bridge the gap between the characteristics of the source and target datasets, potentially aiding in improving model generalization by providing a gradient of features between these datasets."}, {"title": "Conclusion", "content": "We introduces the Adversarial Data Augmentation for time series data (TADA) as a novel approach to address domain generalization challenges specific to time series data. TADA effectively supplements traditional Adversarial Data Augmentation (ADA), which has predominantly focused on amplitude-related distribution shifts but often overlooks temporal aspects crucial to time series analysis. By leveraging a differentiable time-warping technique that utilizes the duality between time warping in the time domain and phase shifts in the frequency domain, TADA is capable of addressing the temporal distribution shifts that ADA misses. Our extensive evaluations across diverse datasets such as electrocardiograms, electroencephalograms, and human activity recordings demonstrate that TADA not only outperforms existing ADA variants but also accurately simulates real-world data shifts. The integration of TADA and ADA provides a comprehensive solution that significantly enhances model robustness and generalization capabilities across unseen domains. Our findings suggest that\nTADA can be seamlessly integrated into existing ADA frameworks, improving their effectiveness and allowing them to better simulate and adapt to real-world distribution shifts. Ultimately, TADA represents a significant advancement in overcoming the challenges posed by distribution shifts in time series data, thereby enhancing the practical applicability and robustness of machine learning models in real-world scenarios."}, {"title": "Appendix", "content": "The figures in this section display samples generated by ADA and TADA. As shown in the figure, ADA perturbs the samples by adjusting their amplitude, whereas TADA their temporal characteristics. This confirms that TADA perturbs time series data as intended.\nDuring the data generation phase (the inner maximization phase), we can add samples generated from both ADA and TADA to the dataset, instead of relying solely on samples generated from either method. TADA can be implemented in conjunction with ADA by iteratively applying both methods during the data generation phase, as detailed in lines 10-18 of Algorithm 2."}]}