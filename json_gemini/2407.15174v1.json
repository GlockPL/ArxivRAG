{"title": "TADA: Temporal Adversarial Data Augmentation for Time Series Data", "authors": ["Byeong Tak Lee", "Joon-myoung Kwon", "Yong-Yeon Jo"], "abstract": "Domain generalization involves training machine learning models to perform robustly on unseen samples from out-of-distribution datasets. Adversarial Data Augmentation (ADA) is a commonly used approach that enhances model adaptability by incorporating synthetic samples, designed to simulate potential unseen samples. While ADA effectively addresses amplitude-related distribution shifts, it falls short in managing temporal shifts, which are essential for time series data. To address this limitation, we propose the Temporal Adversarial Data Augmentation for time teries Data (TADA), which incorporates a time warping technique specifically targeting temporal shifts. Recognizing the challenge of non-differentiability in traditional time warping, we make it differentiable by leveraging phase shifts in the frequency domain. Our evaluations across diverse domains demonstrate that TADA significantly outperforms existing ADA variants, enhancing model performance across time series datasets with varied distributions.", "sections": [{"title": "1 Introduction", "content": "Most machine learning models are developed on the assumption that their training data reliably represents the broader population. Unfortunately, this assumption is often incorrect. The distribution of real-world data often deviates from that of the training dataset, and such discrepancies lead to performance degradation [1-5]. An adversarial data augmentation (ADA) is one of the approaches that addresses distribution shift problem by solving worst-case scenarios around the original distribution [1, 2]. By training the model with adversarial synthetic samples, which are made extremely difficult for the model to classify, we can ensure that the model performs reliably under less challenging conditions, such as distribution shift.\nTime series data can be depicted on a 2D graph, with the amplitude axis representing magnitude of data points and the temporal axis indicating the sequence and timing of these points. While ADA effectively simulates samples that reflect distribution shifts related to amplitude axis, it is limited in generating samples that account for distribution shifts along the temporal axis [6, 7]. Since distribution shifts in time series data are associated with both axes, it is crucial to develop methods that also account for changes along the temporal axis.\nTo address the challenge of distribution shift along the time axis, we propose Temporal Adversarial Data Augmentation (TADA) for time series data. This method integrates a time warping technique with the ADA process [8, 9]. A significant challenge in directly applying time warping within ADA is the non-differentiable nature of index mapping, which disrupts gradient computation with respect"}, {"title": "2 Related works", "content": "Domain Generalization. The distribution shift between source domain and unseen target domain often degrades [13\u201315]. Domain generalization emerges to address these problems. The goal of domain generalization is to train models to effectively generalize across multiple unseen target domain [16, 3, 17, 18].\nDepending on the coverage of the source domain, domain generalization is categorized into single-source and multiple-source domain generalization [12, 18]. For single-source domain generalization, the objective is to generalize the model when only a single domain is available for training. Since identifying domain-specific characteristics within the training dataset is impossible, single-domain generalization rely on generating fictitious distribution that simulate unseen domains. This includes data augmentation-based methods [1-3, 12]. For the multiple-source domain generalization, the objective is to learn invariant features across multiple domains that are not affected by domain shift in the source domain. This is based on the idea that the unchanging characteristics will also remain consistent in the unseen target domain, and domain alignment is a widely adopted approach in multiple-source domain generalization [4, 5, 19].\nAdversarial Data Augmentation. Adversarial Data Augmentation (ADA) enhances a model's ability to generalize across other domain or unseen distributions. This method tackle the worst-case problem as detailed in Equation 1 [1]. By employing ADA, the model achieves robust performance even when the distribution P differs markedly from the initial domain Po, effectively handling scenarios of domain shift.\n$\\min_{\\theta} \\max_{P \\in \\{supp(P)} \\{E[L(\\theta; X, Y)] \u2013 \\gamma D_0(P, P_0)\\}\\},$"}, {"title": "3 Methodology", "content": "ADA consists of maximization phase, which focuses on data generation, and a minimization phase, which involves updating the model. We will redesign the maximization phase of ADA to gen-erate samples with distribution shift in temporal axis. Equation 2 represents the objective of the maximization phase from Equation 1.\n$\\mathbb{X}^k \\in arg \\max_{\\mathbb{X}} \\{\\mathcal{L}(\\theta; (\\mathbb{X}^{k-1},\\mathbb{Y})) \u2013 \\gamma c_{\\theta}((\\mathbb{X}^{k-1}, \\mathbb{Y}), (\\mathbb{X}, \\mathbb{Y}))\\},$\nwhere k refers the number of iteration for adversarial update, $X^0$ and $X^k$ are original samples and adversarial perturbed samples at the iteration k, respectively. y is a penalty parameter, and $c_0$ is a distance measure.\nThe maximization phase can simply be understood as the addition of adversarial noise to the input, expressed as $x = x + \\Phi$, where $\\Phi$ represents the noise added. This process can be thought as passing the input sample through a function F, which is parameterized by $\\Phi$: $x = F(x; \\Phi) = x + \\Phi$. Now, we update $\\Phi$ instead of $X$ iteratively as following Equation 3:\n$\\mathbb{\\Phi}^k \\in arg \\max_{\\mathbb{\\Phi}} \\{\\mathcal{L}(\\theta; (F(\\mathbb{X}; \\mathbb{\\Phi}^{k-1}), \\mathbb{Y})) \u2013 \\gamma c_{\\theta}((F(\\mathbb{X}; \\mathbb{\\Phi}^{k-1}), \\mathbb{Y}), (\\mathbb{X}, \\mathbb{Y}))\\}$\nOur objective is to design a function F that alters the temporal characteristic of the time series data X without affecting its amplitude. To achieve this, we can think of incorporating the time warping in the function F to transform the sequence according to a warping path. However, the time warping is non-differentiable since it uses the index mapping. This makes iterative updating of $\\Phi$ infeasible."}, {"title": "3.2 Differentiable Time Warping", "content": "To address this issue, we propose a new time warping function F that is equivalent to traditional time warping but differentiable. Leveraging the duality of time shift in the time domain, which is non-differentiable, and a phase shift in the frequency domain, which is differentiable, we introduce differentiable time warping.\nWe begin with the mathematical formulation of time warping. Time warping involves a specific warping path, which maps the index of original sequence to transformed sequence. Applying time warping to a sequence of the time series data X = {$X_1, X_2, ..., X_n$} transforms it into the transformed sequence $X'$ = {$X_{1+\\delta_1}, X_{2+\\delta_2}, ..., X_{n+\\delta_n}$ }, where \\delta indicates the distance that an index is moved from its current position (e.g., time shift), and \u2206 = {$\\delta_1, \\delta_2, ..., \\delta_n$} means the warping path."}, {"title": "Proposition 1.", "content": "Applying time warping to a sequence, X', is equivalent to applying phase shift A to the frequency domain components of a sub-sequence $S_i$, defined as a contiguous segment of the original sequence.\n$TimeWarping(X) = \\{f'^{-1}(g(f(s_1), \\Phi_1)), ..., f'^{-1}(g(f(s_n), \\Phi_n))\\}$"}, {"title": "3.2.1 Function f: Transformation to frequency domain", "content": "We first split time series data X into overlapping segments S because applying a phase shift to a segment results in the same distance shift across all its points. Thus, A point xi in the time series data X can belong to multiple segments, depending on the window size and the degree of overlap. The number of segments that a single point $x_i$ belongs to can vary based on how the segments are defined and the chosen window size. The function f converts these segments into the frequency domain. In practice, function fis typically implemented using the Short-Time Fourier Transform (STFT) as the following Equation 5:\n$f(s)[m, k] = \\sum_{n=0}^{N-1} x[n]w[n \u2013 m] \\cdot e^{-j2\\pi kn/N} = x[m,k],$\nwhere k is the frequency bin index, m is the frame index, N is the number of frequency bins, and X is a output corresponding to segment s in the frequency domain.\nHere, the window function w is defined as follows:\n$w[n - m] = \\begin{cases}\n1, & \\text{if } |n-m| < M, \\\\\n0, & \\text{otherwise,}\n\\end{cases}$"}, {"title": "3.2.2 Function g: Time warping in frequency domain", "content": "To apply the time warping in frequency domain, we perturb the temporal characteristics of a output $X_i$ for the segment $s_i$ by adding the noise that induces a corresponding phase shift $\\Phi_i$, resulting in $g(f(s), \\Phi) = f(s) + \\Phi \\cdot \\omega$. A $\\Phi$ is interpreted as the warping path.\nThe warping path must fulfill several conditions [20]: (1) it must be monotonic, ensuring that the sequence progresses in a single direction without reversals; (2) it must align with the boundaries, meaning the start and end indices of the path should coincide the start and end of the sequences being aligned; and (3) it must adhere to a warping distance constraint, which limits the extent of the warping. To ensure these conditions, we impose several constraints on I using the function g. With constraints, \u03a6 is redefined as \u03a6 = (93 \u00b7 92 \u00b791)($), with 4 denoting the perturbation parameters."}, {"title": "Maintaining Monotonicity.", "content": "The monotonicity constraint ensures that the original indices in the warping path consistently map forwarding indices to be transformed, never reversing direction. This means that if a point in the original sequence a\u017c is aligned with a point in a transformed sequence bj, then ai+1 can only be aligned with b; or its subsequent points bj+k, k \u2265 0. Fulfilling the requirement of monotonicity means that the difference between consecutive element of the warping path must be non-negative. This can be expressed by $\\delta_i$ \u2265 0. To ensure that each element of the warping path remains non-negative, we subtract the minimum value found in the warping path element from each element as follows:\n$g_1(\\Phi_i) = \\sum_{i=1}^{T} \\Phi_i \u2013 min(\\Phi)$"}, {"title": "Boundary Alignment.", "content": "To ensure indices of elements in the transformed sequence aligned in the boundary (e.g., starting and ending indices) of the original sequence, we first normalize i and then scale the normalized path to match the length of original signal, resulting in the warping path. On this warping path, each element is subtracted by the order of index, which gives the difference between the original index and the corresponding index in the shifted sequence. This is represented as below:\n$g_2(\\Phi_i) = \\frac{\\Phi_i - min(\\Phi)}{max(\\Phi) - min(\\Phi)}N-i,$\nwhere N denotes the total number of the segment (i.e. signal length)."}, {"title": "Warping Distance Constraint.", "content": "To prevent the transformed sequence from deviating excessively from the original, we impose a constraint on the level of deviation. As described in Equation 9, we define $\\Phi_{max}$ as a hyperparameter for the upper limit for the maximum allowable deviation between the indices of the original and the transformed sequences. For example, if max is set to 10, the index in the transformed sequence cannot deviate more than 10 indices far from its corresponding index in the original sequence. To preserve the fidelity of the transformation process, it is crucial to choose $\\Phi_{max}$ to be shorter than the window size used when transforming the signal to the frequency domain. This approach ensures that the deviation stays within acceptable boundaries.\n$g_3(\\Phi_i) = \\Phi_i \\cdot min(\\frac{\\Phi_{max}}{||\\Phi||_{\\infty}}, 1)$"}, {"title": "3.2.3 Function f'-1: Reconstruction to time domain", "content": "After perturbing the signal in the frequency domain, we use Inverse STFT (ISTFT) to reconstruct the segment s in the time domain. ISTFT converts a frequency-domain component x into its corresponding segment s as Equation 10.\n$s[n] = \\frac{1}{N} \\sum_{m=0}^{N-1} \\sum_{k=0}^{N-1} x[m, k]w'[n \u2013 m]e^{j2\\pi kn/N},$\nIn Section 3.2.1, we described how the time series data X is divided into overlapping segments S to facilitate the application of distinct phase shifts across all segments. This process leads to interference among the time-shifted segments when aggregating them back into the time series data X. Instead of averaging the overlapping windows in the function f as described in Equation 5, we extract the central value from each transformed segment using a specific window function w' in the inverse function f'-1.\nw'[n - m] = \\begin{cases}\n1, & \\text{if n = m}\n0, & \\text{otherwise}\n\\end{cases}"}, {"title": "3.3 Training Procedure", "content": "We describe the full procedure on Algorithm 1. Our proposed algorithm incorporates adversarial data augmentation to improve model robustness through an iterative training process, which the"}, {"title": "4 Experiments", "content": "In this section, we benchmark our proposed method against leading techniques within the single domain generalization framework, which involves training models on data from one domain and deploying them in another. Additionally, we perform ablation studies to assess the effects of integrating our method with other approaches. Lastly, by analyzing the representations extracted from models trained using our methodology, we confirm that our method effectively simulates a valid distribution shift."}, {"title": "4.1 Experimental Setup", "content": "Datasets. Datasets used in our experiments consist of multiple sub-datasets. The sub-datasets were sourced from various regions or collected using different devices, leading inherent distribution shifts among them. This diversity among sub-dataset allows a comprehensive evaluation of methods robustness across varied data characteristics.\nPhysionet Challenge 2021 (Physionet) dataset [10] is a public dataset for electrocardiogram (ECG) research, aimed at diagnosing a wide range of cardiac abnormalities. It consists of seven sub-datasets. Each dataset presents differences in demographic profiles. Among them, five datasets containing over 1,000 ECGs were selected for our experiment: PTB-XL [21], Chapman-Shaoxing [22], Ningbo [23], G12EC, and CPSC2018 [24]. The datasets involve multi-label classification of 26"}, {"title": "4.3 Performance Evaluation in combining the variants of ADA with TADA", "content": "One advantage of our proposed method is its compatibility with existing adversarial data augmentation techniques. We have integrated techniques from extensions of ADA, such as M-ADA [32] and ME-ADA [2], into TADA, resulting in variants like ME-TADA and M-TADA. As shown in Tables 1, 2, and 3, TADA can be effectively combined with ADA, leading to the development of combinations such as ME-(ADA+TADA) and M-(ADA+TADA)."}, {"title": "4.4 Visualization of Distribution Shifts", "content": "We claim that TADA induces a different type of distribution shift (e.g., a distribution with temporally shifted samples) compared to the distribution shift caused by ADA (e.g., a distribution with amplitude-modified samples). We expect that these two distinct distributions could effectively enhance the robustness and generalization of models for time series data. For verification purposes, we first extract samples from both the training dataset (i.e., a single source sub-dataset) and the test dataset (i.e., other target sub-datasets). We then employ UMAP [33] to visualize their scatteredness."}, {"title": "5 Conclusion", "content": "We introduces the Adversarial Data Augmentation for time series data (TADA) as a novel approach to address domain generalization challenges specific to time series data. TADA effectively supplements traditional Adversarial Data Augmentation (ADA), which has predominantly focused on amplitude-related distribution shifts but often overlooks temporal aspects crucial to time series analysis. By leveraging a differentiable time-warping technique that utilizes the duality between time warping in the time domain and phase shifts in the frequency domain, TADA is capable of addressing the temporal distribution shifts that ADA misses. Our extensive evaluations across diverse datasets such as electrocardiograms, electroencephalograms, and human activity recordings demonstrate that TADA not only outperforms existing ADA variants but also accurately simulates real-world data shifts. The integration of TADA and ADA provides a comprehensive solution that significantly enhances model robustness and generalization capabilities across unseen domains. Our findings suggest that TADA can be seamlessly integrated into existing ADA frameworks, improving their effectiveness and allowing them to better simulate and adapt to real-world distribution shifts. Ultimately, TADA represents a significant advancement in overcoming the challenges posed by distribution shifts in time series data, thereby enhancing the practical applicability and robustness of machine learning models in real-world scenarios."}]}