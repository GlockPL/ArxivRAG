{"title": "TransitGPT: A Generative AI-based framework for interacting with GTFS data using Large Language Models", "authors": ["Saipraneeth Devunuri", "Lewis Lehe"], "abstract": "This paper introduces a framework that leverages Large Language Models (LLMs) to answer natural language queries about General Transit Feed Specification (GTFS) data. The framework is implemented in a chatbot called TransitGPT with open-source code. TransitGPT works by guiding LLMs to generate Python code that extracts and manipulates GTFS data relevant to a query, which is then executed on a server where the GTFS feed is stored. It can accomplish a wide range of tasks, including data retrieval, calculations and interactive visualizations, without requiring users to have extensive knowledge of GTFS or programming. The LLMs that produce the code are guided entirely by prompts, without fine-tuning or access to the actual GTFS feeds. We evaluate TransitGPT using GPT-40 and Claude-3.5-Sonnet LLMs on a benchmark dataset of 100 tasks, to demonstrate its effectiveness and versatility. The results show that TransitGPT can significantly enhance the accessibility and usability of transit data.", "sections": [{"title": "1. Introduction", "content": ""}, {"title": "1.1. Background", "content": "The General Transit Feed Specification (GTFS) is an open data standard for transit data. Started in 2005 as a collaboration between Google and TriMet (Portland, OR's transit agency) (Roth, 2010; McHugh, 2013), today over 10,000 agencies from over 100 countries use GTFS to share and store their transit feeds publicly (MobilityData, 2024b). The scope of GTFS has expanded beyond schedules to include information about real-time updates, flexible services, and fares. Its widespread adoption has led to the development of software, programming libraries, and plugins for popular GIS software that help agencies create or visualize GTFS feeds.\nThe principal use of GTFS is for navigation applications such as Google Maps, Apple Maps, Transit, etc. But in addition to helping people use transit, GTFS data is increasingly used to analyze, measure and understand transit systems. A spatio-temporal decomposition of GTFS has been utilized to construct O-D travel time matrices, generate isochrones, and estimate travel time uncertainty (Pereira et al., 2021; Liu et al., 2024). This decomposition has been used to conduct transit accessibility analysis by examining socioeconomic and spatial-temporal inequalities (Fayyaz et al., 2017; Prajapati et al., 2020; Yan et al., 2022; Pereira et al., 2023). Devunuri and Lehe (2024a); Devunuri et al. (2024a) decomposes GTFS feeds into segments for all US and Canadian transit agencies to measure statistics about bus stop spacings. Fortin et al. (2016) modeled the transit timetables as graphs and developed graph-oriented indicators, including connectivity between stops and the proportion of active stop pairs over time. GTFS has also been used to create informative visualizations. Kunama et al. (2017) developed \u2018GTFS-Viz', a tool to animate the"}, {"title": "1.2. Plan of paper", "content": "This paper introduces an architecture that uses LLMs to answer questions about transit systems from GTFS data. This architecture is implemented in a chatbot interface called TransitGPT. The link to the chatbot as well as the code behind the TransitGPT is available on our GitHub repository located at https://github.com/UTEL-UIUC/TransitGPT. The reader is encouraged to experiment with TransitGPT by selecting one of the ten agencies we have prepared and asking questions about it. TransitGPT can answer questions that a rider might have, such as: \"When does the last Orange bus arrive at University and Victor on a Tuesday?\" It can also answer system-level questions such as: \"Which routes have headway shorter than 15 minutes on the weekend?\"\nA goal of this paper is to explain the architecture behind TransitGPT and how it was built. In a nutshell, what we do is (i) ask the LLM to write Python code that can answer a question given that the GTFS feed is available; then (ii) run that code on a server where the feed is stored. While this two-step combination conveys the gist, TransitGPT involves many embellishments which are explained more thoroughly below. In particular, we have created a large system prompt: a text file containing instructions that guide the LLM as it writes code to answer the questions. This system prompt can be passed to any public-facing LLM, without 'pre-training' or 'fine-tuning'. This emphasis on prompt engineering (rather than fine-tuning) aligns with the concept of \"Prompting Is All You Need\" (Feng and Chen, 2024), which avoids the need for extensive training data and compute resources. To evaluate different prompt designs and to compare the performance of LLMs, we employ a 100-question benchmark.\nAt the outset, it is important to distinguish our approach from another one which is perhaps more intuitive: passing an entire GTFS feed to an LLM and asking questions about the data. One reason we do not take this direction is that most GTFS feeds are too large to pass to the popular, public-facing LLMs currently due to context length limitations (Hsieh et al., 2024). Ostensibly, one could use Retrieval Augmented Generation (RAG) (Lewis et al., 2021; Gao et al., 2024) to retrieve the feed in chunks, but an obstacle to this approach is the inter-related nature of GTFS files (i.e., there is no single file which contains all information about, say, a route). In either case, a more fundamental problem is that many queries require not only looking up data but also manipulating data, doing calculations with data, and connecting different pieces of data according to the complex rules of GTFS. While LLMs excel at tasks such as translation, summarization, and classification, they often struggle with tasks that need precise numerical calculations or data manipulation (Ahn et al., 2024). For instance, it has been documented that LLMs make mistakes with simple tasks such as counting the number of r's in the word 'strawberry' or deciding whether '9.11' or '9.7' is a larger number. By contrast, it is straightforward for an LLM to write reliable code that solves those problems."}, {"title": "1.3. Scope and contribution", "content": "Note the following limitations on the scope of the project:\n(i) The scope is limited to the GTFS Static version of GTFS which conveys information that is planned in advance (e.g., stop locations, routes, schedules, etc.). TransitGPT does not draw information from GTFS Realtime feeds (about delays, vehicle locations, etc.).\n(ii) TransitGPT cannot generally answer questions that go beyond the capabilities of GTFS Static. For example, GTFS does not say how many seats are on each bus, so TransitGPT cannot answer questions about that. Geocoding is an exception; TransitGPT can use the Google Maps Geocoding API and the geocoding library Nominatim to answer queries such as \"How many stops are within 30 meters of Fisherman's Wharf?\"\u05d9\n(iii) Errors or limitations in a GTFS feed (which is prepared by a transit agency) will be reflected in the answers generated by TransitGPT. Devunuri and Lehe (2024b) surveys various errors (defined as deviations from GTFS standards) in GTFS Static feeds across the US and Canada and finds they are relatively common-although few of them are substantial. In addition to errors, many GTFS features are optional, so TransitGPT cannot answer questions that rely on an optional file (such as fare_-rules.txt and transfers.txt) or features (such as the wheelchair_accessible or bikes_allowed fields in trips.txt) that an agency has declined to include.\n(iv) TransitGPT is not intended for routing questions: e.g., \"How do I get from Chicago Union Station to the Harold Washington Library?\" This is simply not a space that TransitGPT meant to \"compete\" in. Mobile apps, such as Transit, are already optimized for routing: they utilize real-time data and forecasting which we do not access, and they have interfaces that show users when and where to walk (and wait/transfer) as well as choices among different options. TransitGPT's interface is designed for answering questions about systems."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. LLMs for Code Generation", "content": "The ability of Large Language Models (LLMs) to generalize and learn from context (in-context learning) has been effectively utilized for coding tasks, including code completion\u00b9 and code generation. While these tasks are complementary, this paper solely focuses on code generation. In the domain of data-related applications, Text2SQL (Khatry et al., 2023; Zhang et al., 2024a) has garnered significant attention for its ability to perform data extraction and analysis. Text2SQL allows users to query relational databases using natural language, which is then translated into SQL queries and executed. However, SQL lacks integration with existing libraries and does not offer visualization capabilities. Recently research has shown that Text2SQL performs poorly on tasks that require semantic or complex reasoning (Biswal et al., 2024).\nResearchers have explored alternative approaches of using programming languages such as Python (Haluptzok et al., 2023; Liu et al., 2023) or Java (Feng and Chen, 2024) and utilizing libraries written on top of them. Zan et al. (2022) demonstrated LLM's ability to generate code using popular data process-ing libraries numpy and pandas (using NumpyEval and PandasEval benchmarks) which are present in the corpus of training data of most LLMs. Furthermore, Patel et al. (2024) demonstrated that LLMs can learn and utilize libraries not present in their pre-training data through descriptions and raw code examples. This capability significantly expands the potential applications of LLMs in specialized domains such as transit, allowing the use of domain-specific libraries and tools. The ability to use libraries has also allowed LLMs to tackle much more complex tasks in related disciplines such as Data Science (Zhang et al., 2024e) and Machine Learning (Zhang et al., 2024c)."}, {"title": "2.2. LLMs for Transportation", "content": "The application of Large Language Models (LLMs) has expanded across various domains, including trans-portation. In the field of traffic safety, Zheng et al. (2023) investigated the potential of LLMs for automating accident reports and enhancing object detection through augmented traffic data. Similarly, Mumtarin et al. (2023) conducted a comparative analysis of LLMs in extracting and analyzing crash narratives, further demonstrating their utility in safety-related tasks. LLMs have also shown promise in autonomous vehicle technology. Research by Zhang et al. (2024b), Cui et al. (2023) and Fu et al. (2023) explored the use of LLMs to analyze, interpret, and reason about various driving scenarios, potentially improving the decision-making capabilities of self-driving systems. In traffic management, LLMs have been applied to several critical tasks. Zhang et al. (2024d), Movahedi and Choi (2024) and Da et al. (2023) investigated the use of LLMs for signal control optimization, while Zhang et al. (2024f) explored their application in demand forecasting. These studies highlight the versatility of LLMs in addressing complex transportation challenges, from infrastructure management to predictive analytics."}, {"title": "2.3. LLMs for Transit", "content": "Applications of LLMs to public transit have been more limited but have grown in the past year. Devunuri et al. (2024b) developed benchmarks to evaluate LLMs' capabilities (zero-shot and few-shot) and limitations in understanding GTFS semantics and retrieving information from GTFS data. The 'GTFS Semantics' benchmark tests LLMs on various GTFS elements, including terminology, data structures, at-tribute and category mapping, and common reasoning tasks. The\u2018GTFS Retrieval' benchmark assesses LLMs' performance on tasks ranging from simple lookups to complex operations involving sorting, group-ing, joining, and filtering information. In related work, Wang and Shalaby (2024) employed LLMs for transit information extraction and sentiment analysis from social media platforms. They also demonstrated that Retrieval Augmented Generation (RAG) improved the performance of LLMs on GTFS Semantics and Retrieval benchmarks. Syed et al. (2024) developed TransportBench, a comprehensive benchmark compris-ing questions from transportation planning, economics, networks, geometric design, and transit systems. Oliveira et al. (2024) developed a methodology to investigate and evaluate bus stop infrastructure and its surroundings using street-view images and automated image descriptions from LLMs."}, {"title": "3. Control Flow", "content": "This section describes how TransitGPT works as a series of steps that control the flow of information. Every step takes place on a remote server hosted by Streamlit, which is a company that provides a platform for building and sharing AI-centered web apps. The steps are executed by Python scripts on the Streamlit server. The control flow is described in broad strokes rather than in exact detail, but the reader may visit the documentation and code for TransitGPT to see exactly how things are carried out."}, {"title": "Step 0: Pre-Query", "content": "Before typing in a query (i.e., a question or command), the user selects a GTFS feed (e.g., SFMTA) and an LLM (e.g., GPT-40) from among our ten agencies and four LLMs.\nFor each GTFS feed, there is already a \u201cpickled\" (a serialized\u00b2 object saved to disk) Feed object which sits on the remote Streamlit server. A Feed is a Python object whose attributes correspond to the .txt files in a GTFS feed. For example, if feed is a Feed instance, then feed.routes has data from the file routes.txt. Each .txt file's data is stored as a pandas DataFrame whose columns are the fields in the .txt file. Hence, feed.routes.route_id yields a list of the strings given in the route_id column of routes.txt. The Feed objects were created using the Python library gtfs_kit. Appendix B details how the data from the actual GTFS feeds are pre-processed.\nA Python program we call the \"sandbox\" runs constantly on the Streamlit server, waiting for pieces of code to execute. When the user selects a GTFS feed, the sandbox will import the corresponding (pickled) Feed object from disk to create a variable named feed. The sandbox also has imported eight Python libraries\u00b3 which have been downloaded onto the Streamlit server."}, {"title": "Step 1: Moderation", "content": "Next, the user types a question or command into the TransitGPT interface: e.g., \"Show all the stops on Market St.\" We immediately send this query along with a document called the Moderation Prompt (Figure 4) to GPT-40-mini (a faster and cheaper version of GPT-40). The Moderation Prompt instructs GPT-40-mini to judge whether the user's query is relevant to transit. For example, the user cannot ask \"How tall is the Empire State Building?\" This Moderation Step defends against \"prompt injection\" attacks (Crothers et al., 2023). If necessary, the LLM returns a message that the query is not relevant to transit. Otherwise, we proceed to step 2. The Moderation step typically takes 0.5-1 seconds."}, {"title": "Step 2: Main LLM", "content": "If the user's query passes Moderation, then we build a prompt for the Main LLM that the user has selected (e.g., Claude Sonnet 3.5). This prompt contains four pieces of information:\n(i) The user's current query: e.g., Find the busiest date in the schedule based on the number of trips scheduled.\n(ii) The conversation history-consisting of previous questions and replies from the same conversation.\n(iii) Few-shot examples: We curated a list of twelve diverse question-answer pairs that serve as few-shot examples. For each query, we dynamically choose three examples that are most relevant to the current query. This strategy is known as dynamic few-shot prompting (Brown et al., 2020), and it helps show (rather than merely tell) the Main LLM specifically what we want it to do: e.g., how to format the expected output, what supporting information to include, etc. To identify the three ideal query/response pairs that are most similar to the user's query, we use a technique called TF-IDF (Term Frequency-Inverse Document Frequency) to compute \u201csimilarity scores\u201d between each query and example (Aizawa, 2003). We then select the three examples with the highest scores and present them in the decreasing order of their scores.\n(iv) The Main Prompt: a large text document that guides the Main LLM's response.\nThe Main Prompt, in turn, consists of five modules which each steer the LLM's response in a certain way.  First, the Role module sets the context and expected expertise for the model. Xu et al. (2023) and Salewski et al. (2023) show that instructing the LLM to assume a role as an expert can enhance its performance. Next, the Task Instructions module gives specific instructions for code generation including what libraries to use, the style of writing/documentation, validation of GTFS data, and code optimizations (such as vectorized operations). The Data Types module specifies the data type of each field in every GTFS file. Common data types such as 'Text', 'Integer', or 'Float' have straightforward one-to-one mappings with Python data types 'string', 'integer', and 'float'. However, date and time types are more complex due to their unique interpretation within GTFS. In GTFS, dates represent 'service days' rather than calendar dates and may extend beyond 24 hours (typically from 3 AM to 3 AM). Agencies may write time in either HH:MM:SS or H:MM:SS format. To address these complexities, we convert dates to Python's datetime.date format and convert times to \"seconds since midnight.\" Similarly, Data Types instructs the LLM on data types such as colors, coordinates, and identifiers that are specific to GTFS. Also, we specify the distance units (Meters, Kilometers, etc.) for fields such as 'shape_dist_-traveled'. To reinforce the datatypes, a Feed Samples module provides a sample feed that displays the first five rows of an example feed.\nFinally, the Custom Functions module explains how to use five custom Python functions we have written that help match users' natural language queries to particular stops and routes in the GTFS feed. This is important because users rarely use the exact identifiers given in GTFS feeds (e.g., one stop's name is 'Church St. & Victor St. (northwest corner)') and instead use landmarks (e.g., Pier 39), addresses, or intersections written without punctuation or street types (e.g., Broadway and Main). Moreover, users are prone to typographical errors, and agencies use different conventions for naming stops. These functions include:\n\u2022 find_route: Searches for a route by examining route IDs, short names, and long names using fuzzy matching.\n\u2022 find_stops_by_full_name: Locates stops by their full name, accommodating minor spelling varia-tions through fuzzy matching.\n\u2022 find_stops_by_street: Identifies stops on a specific street using the root word of the street name (e.g., \"Main\" for \"Maint Street.\").\n\u2022 find_stops_by_intersection: Finds stops near the intersection of two streets by providing the root words of both street names.\n\u2022 find_stops_by_address: Locates stops near a specific address by geocoding6 the address and finding nearby stops within a specified radius.\nDescriptions for these custom functions are passed to the LLMs within the Main prompt which the LLMs can use as tools (Schick et al., 2023). Similar to function calling, we describe the purpose of the function, its arguments, and expected output. We also show a brief example of how to use it."}, {"title": "Step 3: Code Execution", "content": "The next step is to execute the code snippet (returned in Step 2) in the 'sandbox' Python thread described in Step 0. Due to the instructions written in the Main Prompt, the code should expect there to be a 'Feed' object named feed and use any of the eight approved Python libraries. According to the directions given in the Main Prompt's Task Instructions module, the code is supposed to return a Python dictionary named result which has up to three properties:\n\u2022 answer: a string or a list of strings that answers the user's query.\n\u2022 additional_info: a string that provides information that the user may find useful as well as assump-tions made to arrive at the answer. Suppose, for example, that the user asks \"How many bus stops are near Newmark Civil Engineering Laboratory?\" In this case, the additional_info value may say what radius from the Laboratory was used to measure \"near.\"\n\u2022 visualization (optional): instructions for building tables, maps, charts, or diagrams using the Python libraries plotly and/or folium. This key only exists if the user asks for a visualization or if the Main LLM \"decides\" that a visualization would be helpful."}, {"title": "Step 3(b): Error Handling (contingent)", "content": "If the Main LLM's code snippet throws an error, we carry out an error handling/feedback mechanism inspired by the popular ReAct (Yao et al., 2023) framework. Nearly always, the Main LLM will correct the error and return executable code. This loop continues until the code executes successfully or until it reaches a predefined maximum number of 'retries.' By default, TransitGPT permits three retries."}, {"title": "Step 4: Summary", "content": "The result of the code execution in Step 3 is not in a useful format for humans to read. In the last step, we pass four pieces of information to a Summary LLM (GPT-40-mini):\n(i) The user's original query.\n(ii) The response object returned from Step 3.\n(iii) The code that generated the response object returned from Step 2.\n(iv) A Summary Prompt which has instructions on how to read (i)-(iii) and summarize the information usefully for human consumption.\nThe Summary LLM uses these four inputs to return a summary response\u2014such as the example shown in Figure 9. The response includes the answer to the user's query written in a natural way, alongside any assumptions made during the code execution."}, {"title": "4. Benchmark & Results", "content": ""}, {"title": "4.1. Benchmark", "content": "It is typical in the LLM literature to benchmark models on a dataset of tasks. The MMLU benchmark Hendrycks et al. (2021), for example, reveals how models perform across diverse subjects, offering insights into their breadth and depth of knowledge. The GSM-8k benchmark has diverse grade school math questions to test the mathematical reasoning capabilities of language models. Within transit data, the 'GTFS Semantics' benchmark tests LLMs on the various aspects of the GTFS specification. This section describes a benchmark dataset and how TransitGPT performs on it using different configurations. While designed specifically to test TransitGPT, the benchmark could also be used to identify other approaches to the problem of generating code that carries out GTFS queries.\nThe benchmark consists of 100 \u2018tasks,' indexed by i. Each task i is a tuple {Qi, Fi, Xi, Yi, Oi}, defined as follows:\n\u2022 Query (Qi): Describes the task in natural language.\n\u2022 Feed (F): A certain GTFS feed.\n\u2022 Input (Xi): Input variables or GTFS fields.\n\u2022 Code (Yi): Python code that provides the solution for this task.\n\u2022 Output (0\u2082): The intended output of the code snippet.\nThe benchmark was created in the following way. First, we manually write Qi (the query), which determines the topic and subject matter of the task. We designed queries to cover diverse sets of tasks- from basic data retrieval to more complex queries that may require, for instance, distance calculations or geocoding. Table 1 shows the different categories of tasks in the benchmark, alongside sample task descriptions, and how many tasks fall under each category. The 100 tasks are organized into 8 categories, with 20 that require generating visualizations. Some characteristics of benchmark are detailed in Appendix C including (i) wheel diagram that plots the root verbs and direct nouns objects used within the task queries (ii) table containing the GTFS files utilized across different tasks along with their counts.\nEach task is tested on a certain GTFS feed Fi, chosen from among five feeds: San Francisco Municipal Transportation Agency (SFM\u03a4\u0391, 20 tasks), Massachusetts Bay Transportation Authority (\u039c\u0392\u03a4\u0391, 13 tasks),"}, {"title": "4.2. Results", "content": "We use the task queries along with inputs from the benchmark generated to evaluate LLMs, specifically GPT-40 and Claude-3.5-Sonnet which our experience has shown to be the most capable LLMs. During the evaluation, we control four hyperparameters:\n\u2022 temperature: Controls how deterministic the LLM output is. For code generation tasks, lower temper-ature values (e.g., 0.2-0.5) are typically preferred to ensure more deterministic and focused responses. However, we observed that using values at the lower end of this range can impair the LLM's ability to self-correct when it makes mistakes. Consequently, we initially set the temperature to 0.3 for coding (i.e., Main LLM) but increased it to 0.5 for retries to prevent the model from repeating the same errors.\nFor 'Moderation' and 'Summary' LLMs that perform more creative tasks than code generation, we use a temperature of 0.7.\n\u2022 max_tokens: Limits the maximum number of tokens allowed for generation by the LLM. We set the \"max tokens\" parameter to the maximum possible output tokens the LLM supports for Main and Summary LLMs. For GPT-40, this is 16,384 tokens and for Claude-3.5-Sonnet, it is 8,192 tokens. For the Moderation LLM, we set the max tokens to '5'.\n\u2022 timeout: Implements time limit for code execution . We set the timeout to three minutes for all tasks.\n\u2022 max_retries: Limits the number of retries allowed for code execution . When enabled, we set the number of retries to three.\nWe evaluate the performance of both LLMs in two different configurations. As a baseline, we use TransitGPT with zero-shot code generation (i.e. no examples) and no (zero) retries. We compare the baseline against a 'TransitGPT+' configuration, which includes dynamic few-shot examples along with the Error Handling and Feedback Step 3(b). We evaluate the configurations using three performance metrics. They are:\n\u2022 \u03b1: The task accuracy rate. All grading is conducted manually for two reasons: (i) LLMs vary in their response structure and how they distribute information between the 'answer' and 'additional_info' portions of the output, and (ii) tasks involving visualizations require visual inspection. Each response"}, {"title": "5. Conclusion", "content": "In this study, we introduce TransitGPT, a framework that enables Large Language Models (LLMs) to interact with GTFS feeds and extract transit information. We demonstrate that the framework can perform a wide variety of GTFS retrieval tasks through text instructions alone, enabling users with limited knowledge of coding and GTFS specifications to access transit data. The framework's flexibility allows it to leverage existing Python libraries for data extraction, analysis, and visualization. Additionally, the generated code not only serves as a foundation for further analysis but also includes helpful comments that facilitate learning and understanding of the code.\nTransitGPT thus demonstrates the potential for AI to democratize transit data and analysis, empowering transit enthusiasts, professionals, practitioners, and planners. It provides a medium to interact with large datasets bypassing the context-length and lost-in-the-middle limitations of LLMS. The"}]}