{"title": "VELORA: A Low-Rank Adaptation Approach for Efficient RGB-Event based Recognition", "authors": ["Lan Chen", "Haoxiang Yang", "Pengpeng Shao", "Haoyu Song", "Xiao Wang", "Zhicheng Zhao", "Yaowei Wang", "Yonghong Tian"], "abstract": "Pattern recognition leveraging both RGB and Event cameras can significantly enhance performance by deploying deep neural networks that utilize a fine-tuning strategy. Inspired by the successful application of large models, the introduction of such large models can also be considered to further enhance the performance of multi-modal tasks. However, fully fine-tuning these models leads to inefficiency and lightweight fine-tuning methods such as LoRA and Adapter have been proposed to achieve a better balance between efficiency and performance. To our knowledge, there is currently no work that has conducted parameter-efficient fine-tuning (PEFT) for RGB-Event recognition based on pre-trained foundation models. To address this issue, this paper proposes a novel PEFT strategy to adapt the pre-trained foundation vision models for the RGB-Event-based classification. Specifically, given the RGB frames and event streams, we extract the RGB and event features based on the vision foundation model ViT with a modality-specific LoRA tuning strategy. The frame difference of the dual modalities is also considered to capture the motion cues via the frame difference backbone network. These features are concatenated and fed into high-level Transformer layers for efficient multi-modal feature learning via modality-shared LoRA tuning. Finally, we concatenate these features and feed them to a classification head to achieve efficient fine-tuning. The source code and pre-trained models will be released on https://github.com/Event-AHU/VELORA.", "sections": [{"title": "I. INTRODUCTION", "content": "PATTERN recognition targets learning and finding the common patterns from the given data directly and helps the downstream tasks, such as object detection [1], [2], [3] and tracking [4], [5], [6], [7], segmentation [8], [9], [10], etc. This research direction achieves significant improvements with the help of deep neural networks. However, the mainstream algorithms developed based on RGB cameras perform poorly in extremely challenging scenarios (e.g., low illumination and fast motion) due to the limited dynamic range and frame rate. Therefore, pattern recognition is still far from being solved in these cases.\nTo address the issues mentioned above, some researchers exploit bio-inspired event cameras (also termed Dynamic Vision Sensors, DVS) to assist RGB cameras for more accurate pattern recognition. From the perspective of the imaging principle, event cameras only emit a pulse/spike when the variation of light intensity exceeds a threshold. Note that, all pixel points are emitted asynchronously, meaning that whether each pixel point records information is independent and does not interfere with each other. Traditional frame cameras, on the other hand, record all pixel points in a scene synchronously. Event cameras have a higher dynamic range and higher temporal resolution, allowing them to work well in low-light or high-exposure scenes, with almost no motion blur. In addition, event cameras also have the advantage of low power consumption, which is beneficial for deployment on devices with limited energy consumption. The sparse spatial resolution allows for better handling of privacy-sensitive issues, such as human-centric visual tasks (pedestrian attribute recognition, person re-identification).\nMore in detail, Li et al. propose the SAFE [11] which exploits the fusion of RGB frames, event streams, and category names for high-performance RGB-Event based pattern recognition. Wang et al. propose a spatial-temporal feature learning framework based on Transformer, i.e., ESTF [12], for event-based human activity recognition. Chen et al. [13] propose a 2D-1T event cloud sequence which is a compact event representation for spatio-temporal information encoding."}, {"title": "II. RELATED WORKS", "content": "In this section, we review the related works on Event-based recognition, RGB-Event-based recognition, and Parameter-Efficient Fine-Tuning strategies. More related works can be found in the following surveys [22] and paper list 1.\nEvent based Recognition. Event-based recognition can be typically categorized into CNN-based [23], GNN-based [24] [25], Transformer [26] [27], and SNN-based [28] [29] approaches. Chen et al. [30] proposes a novel concept for gesture recognition by taking events as three-dimensional points data and employed into a dynamic graph CNN. Zhu et al. [31] proposes an novel approach that employs Time Surfaces to distill spatiotemporal characteristics from event-based data. Wang et al. [32] presents a gait recognition method named EV-Gait, which is based on Convolutional Neural Networks(CNNs). it addresses the challenge of noise in event streams by enforcing motion consistency. Graph Neural Networks (GNNs) are increasingly being utilized to analyze event data, with notable models such as the Asynchronous Event-based Graph Neural Network (AEGNN) [25], which treats event data as dynamic spatio-temporal graphs. it preserves sparsity and high temporal resolution by selectively updating nodes that are directly affected by incoming events while reducing computational complexity and latency. Xie et al. [24] introduced VMV-GCN, a voxel-centric geometric learning framework aimed at consolidating multi-view volumetric information. In addition, Spiking Neural Networks (SNNs), which mimic the behavior of biological neurons, provide a more energy-efficient solution. This is exemplified by models such as Spikformer [28], which integrates spiking neurons with Transformer architectures. Xiao et al. introduced the HMAX Spiking Neural Network (HMAX SNN) [33], utilizing multi-spike encoding to capture temporal features. Liu et al. proposed Motion SNN [34], which incorporates motion information into a multilayer SNN framework. Lee et al. [35] developed an event camera recognition method based on Spiking Neural Network(SNNs), leveraging computationally inspired supervised learning techniques, including backpropagation.\nIn addition, The SpikMamba [36] framework, as introduced by Chen et al., is designed to harness the energy efficiency of spiking neural networks(SNNs) and the long sequence modeling capability of Mamba. this integration is particularly effective for capturing global features from event data that is spatially sparse. Wang et al. [37] propose event-based Human Action Recognition(HAR) model Known as EVMamba, representing a significant advancement in the field by integrating a spatial plane multi-directional scanning mechanism along with an innovative voxel temporal information. Yang et al. [26] have developed a novel Mobile-Former network that emphasizes uncertainty-aware information propagation for the purpose of pattern recognition, which combines the MobileNet and Transformer network."}, {"title": "III. OUR PROPOSED APPROACH", "content": "In this section, we will first give a preliminary introduction to the LORA strategy, and then introduce the overview of our proposed VELORA-based recognition framework. After that, we focus on the network architecture and loss function."}, {"title": "A. Preliminary: LoRA Strategy", "content": "Low-Rank Adaptation (LoRA) is an efficient fine-tuning technique designed to enhance model performance by incorporating trainable low-rank matrices into pre-trained models. The central premise of LoRA is to maintain the integrity of the pre-trained model's weights while inserting trainable layers within each Transformer block. This approach dramatically reduces the number of parameters that need to be updated for downstream tasks, consequently lowering the GPU memory requirements. During the LoRA fine-tuning phase, the method prioritizes the linear layer within the Transformer block, achieving fine-tuning quality that is on par with full parameter fine-tuning, yet it is computationally more efficient and requires fewer resources. This attribute confers a significant practical advantage to the LoRA fine-tuning method.\nLORA emulates the effects of full-parameter fine-tuning by decomposing the weight matrix into two smaller matrices, which are then used to approximate the original matrix. The original matrix is kept frozen, and it is decomposed into matrices A and B, where the rank r is much smaller than the dimension d. Matrix A is initialized with values drawn from a random Gaussian distribution, while matrix B is initialized as a matrix of zeros. Given the input feature x, the compute procedure can be written as:\n$h = W_ox + \\Delta Wx, \\Delta W= BA$ (1)\nHere, $W_o$ represents the pre-trained weights that remain frozen throughout the training process. The matrices B and A are the ones that will be continuously optimized during training.\nAlthough the LoRA model has these advantages, there has been no work attempting to apply this strategy to the RGB-Event multi-modal classification task. This paper attempts to propose a multi-modal fine-tuning strategy based on LoRA in the hope of achieving a better balance between the cost of model fine-tuning and recognition performance."}, {"title": "B. Overview", "content": "As depicted in Fig. 3, our proposed VELORA framework takes RGB frames and event streams as inputs and employs pre-trained Large Vision Models (LVM) to encode the RGB frames and event streams independently, without parameter sharing. We consider the Transformer layers close to the input end as extracting low-level features, while the layers near the output end are seen as high-level features. We conduct modality-specific LoRA tuning on the low-level Transformer layer to obtain feature representations specific to each modality. Meanwhile, we also extract the frame difference information from the visible light and event stream to obtain motion cues. The outputs are concatenated and fed into a modality-shared LoRA tuning module to achieve multi-modal feature fusion and interaction. Finally, we adopt a classification head to predict the category labels. The VELORA model designed for this multi-modal fusion framework achieves high precision while maintaining a low number of parameters that require adjustment."}, {"title": "C. Network Architecture", "content": "\u2022 Input Encoding. Given the input image set $T_o = \\{U_1, U_2, ..., U_M\\}$ and the event stream $T_e = \\{e_1, e_2,..., e_N \\}$, where M and N represent the number of video frames and event points, respectively. Each event point in the stream is defined as $e_i = [x,y,t,p]$, with (x, y) indicating spatial coordinates, t the timestamp, and p the polarity. Initially, we convert the event stream $e_i$ into event images aligned with the timestamps of the video frames. To effectively capture the feature representations of these inputs, we employ the large vision model $LVM$ (ViT-B/16-based CLIP model [46]) for encoding, striking a balance between accuracy and efficiency."}, {"title": "D. Loss Function", "content": "In our study, we employ the cross-entropy loss function, a standard choice for classification tasks, to quantify the discrepancy between the model's predicted probability distribution and the actual distribution of the true labels. The loss function is defined as:\n$L(y, \\hat{y}) = -\\sum_{i=1}^N Y_i log(\\hat{y_i})$ (7)\nwhere N refers to the number of classes, and y and \u0177 denote the ground truth and predicted labels, respectively.\nTo generate higher-quality images by reducing noise and blurring, and to facilitate cross-modal information exchange, we focus on minimizing the reconstruction loss between RGB images and event streams [47]. This is achieved through the use of the Mean Squared Error (MSE) loss function in our reconstruction phase. The formulas for the reconstruction losses are as follows:\n$L_{RTE} = \\frac{1}{M} \\sum_{i=1}^M (y_e - \\hat{y_e})^2$ (8)\n$L_{ETR} = \\frac{1}{M} \\sum_{i=1}^M (y_r - \\hat{y_r})^2$ (9)\nHere, M represents the number of the patch tokens, $y_e$ and $\\hat{y_e}$ are the raw and reconstructed features for the event stream, while $y_r$ and $\\hat{y_r}$ are the original and reconstructed features for the RGB image. Therefore, the composite loss of our model is the sum of these individual losses, expressed as:\n$L_{total} = L_{RTE} + L_{ETR} + L(y, \\hat{y})$ (10)"}, {"title": "V. CONCLUSION", "content": "In this paper, we introduce a novel fine-tuning approach that integrates modality-specific and shared modality LoRA tuning strategy, enabling the model to retain sensitivity to individual modalities while also extracting shared information across various modalities, thereby improving the model's performance on multimodal tasks. Our model accepts three types of inputs, namely RGB, event, and frame differences. In the modality-specific tuning stage, we utilize a pre-trained large vision model to encode these inputs. It is important to note that although the encoders for these three branches are identical, their parameters are not shared. We contend that the three modalities are distinct and should not share parameters. In the modality-shared tuning stage, aimed at capturing shared information across different modalities, we incorporate the fused modality information into the final block of the Transformer and refine the original output. Our proposed method has indeed outperformed other state-of-the-art methods in terms of accuracy. Furthermore, our approach can dramatically reduce the number of training parameters in the model, thereby significantly conserving space and time resources."}, {"title": "G. Limitation Analysis", "content": "Although our model has achieved commendable experimental results, there is still a significant gap in recognizing every action with precision. The outcomes from these two datasets suggest that improving results solely through deep neural network models remains a formidable challenge. One possible reason is that the static representation of event streams may limit the effectiveness of temporal information. In future work, we plan to design a dynamic and learnable representation of event streams to further enhance the efficacy of the input data. Moreover, leveraging large models to analyze and reason about pedestrian actions is another approach worth considering to enhance the interpretability of our model."}]}