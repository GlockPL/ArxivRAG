{"title": "Robust Change Captioning in Remote Sensing: SECOND-CC Dataset and MModalCC Framework", "authors": ["Ali Can Karaca", "M. Enes \u00d6zelba\u015f", "Saadettin Berber", "Orkhan Karimli", "Turabi Yildirim", "M. Fatih Amasyali"], "abstract": "Remote sensing change captioning (RSICC) aims to describe changes between bitemporal images in natural language. Existing methods often fail under challenges like illumination differences, viewpoint changes, blur effects, leading to inaccuracies, especially in no-change regions. Moreover, the images acquired at different spatial resolutions and have registration errors tend to affect the captions. To address these issues, we introduce SECOND-CC, a novel RSICC dataset featuring high-resolution RGB image pairs, semantic segmentation maps, and diverse real-world scenarios. SECOND-CC which contains 6041 pairs of bitemporal RS images and 30 205 sentences describing the differences between images. Additionally, we propose MModalCC, a multimodal framework that integrates semantic and visual data using advanced attention mechanisms, including Cross-Modal Cross Attention (CMCA) and Multimodal Gated Cross Attention (MGCA). Detailed ablation studies and attention visualizations further demonstrate its effectiveness and ability to address RSICC challenges. Comprehensive experiments show that MModalCC outperforms state-of-the-art RSICC methods, including RSICCformer, Chg2Cap, and PSNet with +4.6% improvement on BLEU4 score and +9.6% improvement on CIDEr score. We will make our dataset and codebase publicly available to facilitate future research at https://github.com/ChangeCapsInRS/SecondCC.", "sections": [{"title": "I. INTRODUCTION", "content": "Remote Sensing Image Change Captioning (RSICC) has emerged as a pivotal field of research for providing meaningful natural language descriptions of detected changes in raw bitemporal remote sensing imagery [1], [2]. Unlike traditional change detection tasks, which primarily provide classes per pixel, RSICC interprets changes contextually, providing detailed information on objects, their spatial transformations, and the temporal relations involved. Accurate change interpretations become vital for many fields including disaster response, urban planning, environmental monitoring, and security, enabling descriptive insights for damage control, land use, environmental changes, and infrastructure changes for timely and informed action. RSICC frameworks typically consist of a vision encoder to extract visual features from multitemporal images and a caption decoder to translate these features into captions that describe spatial changes in the context of temporal evolution.\nEarly works, such as RSICCformer [3], laid the groundwork for remote sensing change captioning (RSICC) by introducing a dual-branch Transformer encoder. This architecture extracted high-level semantic features and leveraged bitemporal feature differences to improve change discrimination, establishing the LEVIR-CC dataset as a widely used benchmark in the field. PSNet [4] incorporated multi-scale feature extraction through progressive difference perception (PDP) layers and scale-aware reinforcement (SR) modules, focusing on identifying changes across varying object scales. MAF-Net [5] extended this approach by proposing Multi-Scale Change Aware Encoders (MCAE) with a Gated Attentive Fusion (GAF) module to aggregate change-aware features across multiple scales. Moreover, ICT-Net [6] introduced the Interactive Change-Aware Encoder (ICE) and Multi-Layer Adaptive Fusion (MAF) modules to selectively extract discriminative features while minimizing irrelevant information. Additionally, ICT-Net introduced the Cross Gated-Attention (CGA) module, which integrates multi-scale change-aware features to enhance captioning performance.\nRecently, Semantic-CC [7] and Change-Agent [8] have demonstrated innovative approaches by incorporating multimodal feature integration with foundation models using Large Language Models (LLMs). Semantic-CC employs the ViT-based Segment Anything Model (SAM) for feature extraction and the Vicuna-7B LLM for decoding. Change-Agent, on the other hand, introduces a dual-branch Multi-task Change Interpretation (MCI) model supported by an LLM for user interaction and additional analyses [8]. While Semantic-CC focuses on balancing semantic change understanding and detection to present a change captioning model in a traditional sense, Change-Agent expands functionality to include object counting and predictive insights with chatting tools for prompt-based user interaction. Although a large number of researches have been developed over the last two years [9], [10], these models have an extreme number of learnable parameters.\nWhile significant progress has been achieved in the RSICC domain, several challenges and opportunities remain for this task. Most existing frameworks focus on unimodal feature space (e.g., RGB, MS, SAR) [3], [11] and face challenges in distinguishing meaningful changes from complex scenes with subtle or small-scale variations [6], [12]. These difficulties are often caused by limitations in data quality, such as alignment issues, variations in image capture angles, and inconsistencies in resolution or lighting, which can obscure critical changes and introduce noise into the analysis. Additionally, establishing complex relationships between objects in image pairs is hindered by missing defining details, which limits the model to generate coherent and contextually accurate captions for intricate scenes. The recent advancements in LLMs have enabled the development of innovative methods that incorporate multimodal feature integration [7] and multiprompt strategies [8]. However, many of these approaches rely on computationally intensive, hard-to-balance architectures, limiting their scalability and applicability in real-world, time-sensitive scenarios. Furthermore, these models often demand large datasets to achieve their full potential, reducing their effectiveness when applied to smaller or domain-specific datasets. Addressing these issues necessitates a shift towards lightweight and efficient architectures that can achieve similar levels of captioning performance while reducing computational overhead. Integrating richer contextual information, such as semantic maps, and leveraging efficient cross-modal feature fusion techniques could bridge the gap between performance and scalability, paving the way for broader applicability of RSICC methods.\nRemote sensing change captioning (RSICC) faces critical challenges such as differences in illumination intensity, variations in viewpoints, blur effects, seasonal variations, and image misregistration. Moreover, images captured at different spatial resolutions and those with registration errors can further impact the accuracy of the captions. These issues often mislead RSICC methods, resulting in the generation of incorrect captions. In this paper, we introduce a publicly available RS-ICC dataset, SECOND-CC, which is specifically designed to address these challenges. Unlike the widely used LEVIR-CC dataset, SECOND-CC incorporates diverse scenarios impacted by these challenges. The dataset includes both color images and semantic segmentation maps, which have been previously utilized for change detection tasks [13], [14]. To produce detailed change captions under the aforementioned challenges, we propose a novel RSICC method, MModalCC (Multi-Modal Change Captioning), which effectively utilizes multimodal data, including color images and semantic segmentation maps. Our primary contributions are outlined as follows:"}, {"title": "II. SECOND-CC DATASET", "content": "The SECOND (SEmantic Change detectiON Dataset) dataset, a well-known comprehensive remote sensing dataset, contains bitemporal RS images and semantic maps and is designed to address challenges in change detection tasks [13], [14] and several research studies have been undertaken on this task [15], [16]. In this paper, we propose the SECOND-CC dataset that includes detailed captions for the SECOND change detection dataset. An example from the SECOND-CC dataset can be seen in Figure 1.\nSECOND-CC expands the contents of its predecessor by adding detailed change captions, which describe the variations between bitemporal remote sensing (RS) image pairs"}, {"title": "A. RS Images", "content": "The RS image pairs in the SECOND dataset capture the aerial views of the cities of Hangzhou, Chengdu, and Shanghai, each with a variety of terrain changes and distractors. Compared to the LEVIR-CC dataset [3], the image pairs in SECOND present greater challenges in the form of varying image resolutions, viewpoint change, contrast, luminance, and tonal differences, and alignment issues. Reflecting the complexity of real-world RS applications, these issues serve as a basis for creating a demanding RSICC dataset.\nThe SECOND dataset contains RS images that have a size of 512x512, and provide resolutions ranging from 0.5 to 3 meters per pixel. Similar to [3], we divide the images in SECOND equally into four distinct quadrants with dimensions of 256 \u00d7 256. By reducing the amount of change in the image pairs to be annotated, this step results in more manageable chunks for change captioning and increases the amount of available images."}, {"title": "B. Semantic Maps", "content": "The SECOND dataset includes high-quality semantic maps segmenting the changed regions of the scenes. The annotations are provided in terms of six different land-cover categories, including low vegetation, non-vegetated ground (n.v.g.) surface, tree, water, building, and playground. The regions of the scene that have not changed between bitemporal states are not labeled with semantic coloring. Notably, these semantic maps can represent multiple categorical changes within a single semantic pair (e.g., low vegetation to building, water to playground). The common transitions between the land-cover categories result in 30 different change categories in SECOND-CC."}, {"title": "C. Captions", "content": "The changes to the surface of the regions depicted in the RS image pairs are described with five human-labeled sentences. There is a total of 30205 sentences in the dataset, prepared with the help of seven contributors in a year. Through the analysis of the LEVIR-CC and SECOND datasets, several rules and decisions are established to guide the change captioning efforts. Key guidelines include:\nDetailing Change Characteristics: It is essential to provide specific information regarding change characteristics, including color, shape, location, intensity, and type, to facilitate a nuanced representation of changes.\nAvoiding Excessive Repetition: To improve the quality of data generation, it is important to minimize repetitive patterns in captioning.\nMinimizing Emphasis on Peripheral Changes: During the labeling process, the focus should be limited to significant changes instead of minor edge alterations.\nStacking the Presentation of Multiple Changes: Multiple principal alterations should be presented in sequential sentences.\nDictionary Size Considerations: The total number of distinct words should be maintained below 2000.\nConsistent Use of Directional Phrases: Directional phrases should be standardized (e.g., use \"top\" instead of \"up\", \"upper\u201d, or \u201cabove\u201d)."}, {"title": "D. Data Augmentation", "content": "In this paper, we prepared an augmented dataset SECOND-CC AUG to achieve higher and more stable performances in this challenging data. To this end, we manipulated the images with a hybrid augmentation method that uses well-known transformations such as blurring, brightening, mirroring, and rotating. Note that this augmentation approach was previously proposed for us and recently presented in [18]. Blurring operation is applied with Gaussian blur process with kernel size 5\u00d75 and brightening operation is performed by pixelwise intensity increase in the images. For blurring and brightening, the captions are directly copied. Since mirror and rotate operations change the position and orientation of objects in the images, it is crucial to update the captions to align with their corresponding updated image pairs. The hybrid augmentation method synthesizes a single image pair by randomly choosing one of the related four transformations for each image pair in training and validation sets."}, {"title": "E. Data Analysis", "content": "The SECOND-CC dataset contains 6041 data entries and 30 159 total captions, as stated in Table I. Each entry is composed of a pair of RS images, semantic maps, five captions, and a label that specifies the most significant change on which the annotations are focused. Some of the entries feature RS image pairs with no discernible land-cover change across them, which are referred in the No-Change category. The remaining entries, comprised of the change categories, are denoted under the Change category. According to the table, the number of samples under the Change category constitutes 71.74% of the entire dataset. The augmented dataset SECOND-CC AUG has twice as many image pairs and sentences as the source dataset, excluding the test split as is seen in Table I. Here, the total number of samples used in SECOND-CC AUG is increased to 10855 thanks to the augmentation process.\nThe dataset is divided into training, validation, and testing sets following a 7:1:2 ratio for model development. Figure 2 illustrates the probability distribution of sentence lengths in"}, {"title": "F. RSICC Dataset Comparison", "content": "The comparison between RSICC datasets, as shown in Table II, presents the unique properties of the SECOND-CC dataset and its augmented version in contrast to other prominent datasets such as LEVIR-CC, LEVIR-CCD, and Dubai-CCD. The proposed dataset offers several distinctive properties that make it particularly suited for advancing RSICC. First, other datasets do not contain any semantic maps whereas SECOND-CC and SECOND-CC AUG include semantic map pairs providing detailed pixel-level land-cover annotations providing additional information about the scene. Moreover, the SECOND dataset naturally incorporates spatial misalignment along with blur and brightness noise to mimic real-world remote sensing challenges. In addition, the images of the SECOND dataset have variable spatial resolutions which makes the dataset more challenging than the other dataset. Last, SECOND-CC exhibits greater variability in sentence structure, as evidenced by a higher average sentence length (10.40), a larger standard deviation (4.77) and a vocabulary size of 1060, which require models to process more complex syntactic and semantic patterns."}, {"title": "III. METHODOLOGY", "content": "The block diagram of the proposed method, MModalCC, is illustrated in Fig. 6, and the procedure of our MModalCC model is shown in Algorithm 1. MModalCC consists of three main components: (i) an encoder, (ii) a feature enhancement module, and (iii) a caption decoder. In the training step, MModalCC takes four inputs: semantic maps and color images collected before change at $t_0$, denoted as $S_{t_0}$ and $I_{t_0}$ and semantic maps and color images collected after change at $t_1$, denoted as $S_{t_1}$ and $I_{t_1}$, respectively. First, two CNN backbones, Encoder1 and Encoder2, extract deep features from the color images $I_{t_i}$ and semantic maps $S_{t_i}$, respectively. Next, features are fed into a novel feature enhancement module, including a series of Cross-Modal Cross Attention (CMCA), Unimodal Difference Cross Attention (UDCA), and Convolutional Block (CB) connected with residual connections. This part efficiently fuses the semantic segmentation and RGB color image information by considering cross-modal and unimodal relations simultaneously. Finally, enhanced features of RGB and semantic images $x_{rgb}$ and $x_{sem}$ were applied to Multimodal Gated Cross Attention (MGCA) based caption decoder. MCGA selectively integrates multimodal features with textual context, allowing the model to focus on the most relevant visual information. This design not only considers the differences between RGB images and semantic maps but also takes into account the relationship between the images, resulting in correct language descriptions."}, {"title": "A. Encoder", "content": "In change captioning literature, Siamese Convolutional Neural Networks (CNNs) are widely employed for image feature extraction [3], [11]. These networks ensure consistent processing of both inputs by utilizing shared weights between the twin CNNs. This design eliminates potential biases caused by differing feature extraction pathways, enabling a more robust and reliable comparison of the inputs.\nSince our model processes two image pairs, as described in Algorithm 1, two separate Siamese backbones are required to extract features for each modality: Encoder1 and Encoder2. For this purpose, we use the ResNet101 architecture pre-trained on the ImageNet dataset as the backbone. Specifically, the RGB image pair ($I_{t_0}$, $I_{t_1}$) is applied to Encoder1 whereas the semantic map image pair ($S_{t_0}$, $S_{t_1}$) is applied to Encoder2. By adding the positional embeddings, feature representations ($f_1$, $f_2$), and ($f_3$, $f_4$) feature pairs are obtained for RGB images and semantic maps, respectively. It is important to note that fine-tuning is applied for each encoder model, individually."}, {"title": "B. Feature Enhancement", "content": "Features obtained by encoders are processed by a Cross-Modal Cross-Attention (CMCA) module, which exploits the interaction between modalities to capture spatial and semantic relationships. Differences between temporal features are then computed and refined through a Unimodal Difference Cross-Attention (UDCA) module, enhancing the discriminative ability of the features. This process is iterated twice, alternating between CMCA and UDCA modules to refine and align the features further. In the subsequent step, the refined features are passed through a Convolutional Block (CB) with residual connections to consolidate RGB and semantic information while preserving feature hierarchies. Further details about each modules are provided in the subsections.\n1) Cross Attention Module: Cross-attention is a mechanism that enables the query of the source feature $f_{source}$ to focus on relevant parts of the key and queries in the target source $f_{target}$, as illustrated in Fig. 7. It computes attention weights by comparing the query with the key and then uses them to combine the corresponding value features, highlighting important information. This allows the model to learn dependencies between two different modalities, making it particularly useful in tasks like multimodal learning such as our problem. In our research, both Cross-Modal Cross Attention (CMCA) and Unimodal Difference cross-attention (UDCA) modules use Cross Attention Modules as shown in Fig. 6. The main difference between CMCA and UDCA has occurred in the determination of source feature $f_{source}$ and target feature $f_{target}$. Using the same module for CMCA and UDCA also enables parameter sharing which provides better optimization in the training step. Therefore, the parameters of the Cross Attention Module are shared in CMCA and UDCA.\nThe cross-attention output Z is computed as:\n$Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{D_k}})V$\n$Z = Attention(Q, K, V)$\nFor multi-head attention, h parallel attention heads are used. Each head computes attention using its own set of weight matrices $W_i^Q$, $W_i^K$, and $W_i^V$ for i=1, 2, ..., h. The outputs of all heads are concatenated and linearly transformed and cross attention output of each head is calculated as follows:\n$Q_i = f_{target}W^Q_i, K_i = f_{target}W^K_i, V_i = f_{target}W^V_i$\n$Z_i = softmax(\\frac{Q_i K^T_i}{\\sqrt{d_k}})V_i$\nFinally, the multi-head output is $Z_{multi-head}\\in \\mathbb{R}^{W^2\\times D}$:\n$Z_{multi-head} = MHA(f_{source}, f_{target}, f_{target})$\n$= Concat(Z_1, Z_2,..., Z_h) W_O$\n2) Convolutional Block: The Convolutional Block (CB) module, shown in Fig. 8, consists of a sequence of convolutional layers, batch normalization, and activation functions, providing feature enhancement for each modality separately. Specifically, CB includes a 1\u00d71 convolution to adjust the number of channels, followed by a 3\u00d73 convolution to capture broader spatial relationships, and another 1x1 convolution to refine the output. This process enhances the feature representations by learning both local and global patterns.\nAs is seen from 6th step of Algorithm 1, the RGB features (f1 and f2) and semantic features (f3 and f4) are channel-wise concatenated and passed through the CB module separately. In the subsequent step, residual connections are introduced by adding the output of the CB module to the previous feature representations. Due to the channel-wise concatenation process, the output feature map is doubled in dimension axis as $f_{output} \\in \\mathbb{R}^{W^2x2D}$ from $f_{output} = CB([f1'; f2']. Therefore, the feature dimensions of the color image output XRGB and semantic image output XSEM at the end of the feature enhancement part, respectively.\nResidual connections between CB help maintain information from earlier layers and facilitate faster convergence, enabling the network to learn more effectively. The combination of convolutional operations and residual connections before the caption decoder ensures improved performance and stability during the learning process."}, {"title": "C. Multimodal Gated Cross Attention-based Decoder", "content": "The proposed caption decoder is a transformer-based architecture that effectively integrates multi-modal information, including RGB features and semantic features, to generate descriptive captions. The decoder utilizes a Multimodal Gated Cross Attention (MGCA) mechanism to selectively fuse information from multiple modalities, followed by normalization, FFN, linear, and softmax layers for word-by-word prediction. This section describes the method with equations and detailed explanations.\nAs is seen from Fig. 6, the caption decoder (CD) consists of three main stages: (i) a word embedding layer with positional encoding, (ii) an MGCA module for fusing RGB and semantic features, and (3) rest of the network to take the fused representation $x_{fused}$ and produce the final prediction. The steps are detailed below.\nIn the first stage, the input word sequence is first mapped to an embedding vector WE, and positional encoding POS is added to incorporate sequence order: $X_{input}$ = WE+POS. This representation is passed through a masked multi-head attention (MHA) mechanism [20]. Masked MHA enables autoregressive decoding by computing attention while masking future tokens:\n$Y = softmax(\\frac{QK^T}{\\sqrt{d_k}}+M)$\n$X_{mha} = Concat(Y_1, Y_2, . . ., Y_h)W_O$\nwhere $Q_i = X_{input}W_i^Q$, $K_i = X_{input}W_i^K$, and $V_i = x_{input}W_i^V$. Similar to Eq. (below), each head computes attention using its own set of weight matrices $W_i^Q$, $W_i^K$, and $W_i^V$ for i=1,2,...,h. The matrix $M \\in \\mathbb{R}^{D \\times D}$ is employed to mask out similarities involving future frames, thereby maintaining causality. Since the subsequent operation is the softmax function, masking is achieved by adding -\u221e. After that, the outputs are aggregated and passed through residual connection and normalization:\n$X_{word} = Norm(X_{mha} + X_{input})$.\nIn the second stage, MCGA, illustrated in Fig. 9 fuses $x_{word}$, RGB features ($x_{rgb}$), and semantic features ($x_{sem}$). Two cross-attention heads compute attention using $X_{word}$ as follows:\n$x'_{rgb} = MHA(x_{word}, X_{rgb}, X_{rgb})$,\n$x'_{sem} = MHA(X_{word}, X_{sem}, X_{sem})$.\nHere, gate weights $g_{rgb}$, $g_{sem}$, and $g_{word}$ are calculated to determine the contribution of $X_{word}$, $x'_{rgb}$, and $x'_{sem}$ to the fused representation:\n$g_{word} = \\sigma(W_{word}X_{word})$,\n$g_{rgb} = \\sigma(W_{rgb}[x'_{rgb}; X_{word}])$,\n$g_{sem} = \\sigma(W_{sem}[x'_{sem}; n; X_{word}])$\nwhere $W_{sem}, W_{rgb} \\in \\mathbb{R}^{4D\\times 2D}$ and $W_{word} \\in \\mathbb{R}^{2D\\times 2D}$ are learnable parameters and $\\sigma$ is the sigmoid activation. The fused representation is computed as a weighted sum:\n$X_{fused} = g_{word} word + g_{rgby}x'_{rgb} + g_{sem} yx'_{sem}$\nwhere \u2299 denotes element-wise multiplication. Finally, the fused feature z is processed by the following equations:\n$X_{final} = Norm(Norm(x_{fused}) + FFN(Norm(x_{fused})))$,\n$p = softmax(W_{out} x_{final})$,\nwhere p represents the predicted probability distribution over the vocabulary."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "This section evaluates the MModalCC method through a series of experiments designed to explore its strengths and analyze its performance across different scenarios. The implementation details, including training configurations and evaluation metrics are introduced. The impact of data augmentation is examined along with the key ablation studies that explore the contributions of certain architectural components, such as the effect of multi-modal cross-attention and different decoder configurations, supported by visual representations of encoder and decoder attention maps. Finally, a comprehensive performance comparison table is provided, showing the advantages of MModalCC over baseline and state-of-the-art methods, and demonstrating how the proposed approach effectively addresses the challenges of remote sensing change captioning tasks. It is important to note that experiments in \"Ablation Study\" and \"Performance Comparison\u201d subsections are carried out by using augmented version of the dataset, SECOND-CC AUG, due to its effectiveness shared in \"Normal Data versus Augmented Data\u201d subsection."}, {"title": "A. Experimental Setup", "content": "We developed our models with PyTorch framework. All models are trained and results are evaluated on the NVIDIA GTX 4090 GPU with 24GB VRAM. In the training stage, we use the cross-entropy loss function and Adam [21] optimizer with 5 \u00d7 10-5 learning rate. The maximum epoch is set to 30. The vision layer we train in our model is ResNet-101 [22] convolution layer trained on ImageNet [23] dataset.\nSelecting the capable captioning metrics is essential to establish an effective and meaningful comparison of the performance of visual interpretation models. Following [24], the metrics are chosen as BLEU-N (N=1,2,3,4) [25], ROUGEL [26], METEOR [27], CIDEr-D [28], SPICE [29] and an average metric, denoted as Sm, to evaluate the quality of the generated captions. These metrics collectively provide a framework for evaluating the quality of captions with their ability to measure syntactic accuracy, contextual coherence and semantic relevance of the generated captions to the reference sentences. The CIDEr-D metric is not considered for no change results as this method penalize frequent words. Since the words in the no change reference captions are identical, the CIDEr-D metric assigns a score of 0 to every generated sentence in this category, thus losing its capability to provide meaningful"}, {"title": "B. Beam Size Search", "content": "Beam search is a search algorithm used to generate sequences by exploring multiple possible options at each step and selecting the most likely ones. Here, beam size determines how many of these options are kept for further exploration during the captioning. Through the experiments conducted with varying beam size hyperparameters, we observed that the performance of the MModalCC model improved as the beam size increased. Specifically, the model achieved a slightly higher Sm score when the beam size reached 4, which was the optimal value in our validation tests. This suggests that a beam size of 4 balances exploration and computational efficiency, leading to better performance. Smaller beam sizes, on the other hand, may limit the model's ability to explore diverse captioning possibilities. In comparison, larger beam sizes could introduce unnecessary computational overhead without a significant improvement in the quality of the results."}, {"title": "C. Normal Data versus Augmented Data", "content": "Considering the various challenges present in SECOND-CC, which involves change captioning across remote sensing images, we found it essential to employ data augmentation techniques. These challenges include image registration errors, brightness differences, blurriness, and variations in viewing angles, all of which can significantly hinder the model's ability to effectively detect and describe changes.\nWe explored various data augmentation procedures during training to enhance the diversity of our dataset. In our augmented dataset, we applied one of these techniques randomly to each image, generating transformed images and doubling the dataset size as described in Section 2."}, {"title": "D. Ablation Study", "content": "1) The effect of Multi-Modal Cross-Attention: This ablation study examines which configurations of cross-attention modules, detailed in Figure 7, are most effective and advantageous for integrating RGB images and semantic maps. The performance is evaluated across various metrics under these configurations:\n1) Single-Modality Cross-Attention: Cross-attention applied only to RGB features (CMCA) or semantic features (UDCA).\n2) Dual Cross-Attention: Cross-attention applied bidirectionally, allowing RGB features to attend to semantic features and semantic features to attend to RGB features.\nThe results of these configurations with and without cross-attention for each modality are shown in Table V.\nResults show that, in most cases, employing dual cross-attention (both CMCA and UDCA) achieves a higher performance across change and no-change cases.\nFor change cases, the dual-attention configuration consistently improves the metrics, increasing Sm significantly with respect to other settings, showing its ability to capture complementary information from RGB and semantic modalities. However, for no-change scenes, the impact of dual cross-attention is less pronounced. Interestingly, single-modality cross-attention configurations (particularly CMCA) demonstrate slightly better scores on certain metrics, such as SPICE and BLEU. Otherwise, the dual cross-attention configuration scores are comparable to the scores of single-modality configurations.\nIn the overall scores, the inclusion of dual cross-attention augments the model's capability to interpret change scenes, increasing Sm from 0.462 (CMCA only) and 0.455 (UDCA only) to 0.487, while maintaining decent performance in scenes with no-change. This suggests that cross-modal integration with CMCA and UDCA proves beneficial for handling the complex contextual relations between features in change captioning tasks."}, {"title": "E. The Effect of Decoder Configurations", "content": "This ablation study examines the impact of different decoder configurations, specifically the use of RGB ($x_{rgb}$) and semantic ($x_{sem}$) features, in the Multimodal Gated Cross Attention-based decoder. To fully exploit the complementary nature of RGB and semantic features, we retain both CMCA and UDCA modules active throughout this study. This configuration allows the decoder to integrate bidirectional cross-modal attention, providing an effective flow of learning exchange between the two modalities, as discussed in Section IV-D1. The performance is evaluated across various metrics under three configurations:\n1) Semantic-only ($x_{sem}$): Only semantic features are used.\n2) RGB-only ($x_{rgb}$): Only RGB features are used.\n3) Dual-modality ($x_{rgb}$ & $x_{sem}$): Both RGB and semantic features are combined.\nIn change cases, the dual-modality configuration shows notable improvements in capturing changes. This indicates a successfully established complementary relationship between semantic and RGB features in identifying and describing changes in the scenes. The most significant improvements with dual-modality enabled are observed in BLEU4 and CIDEr metrics, which emphasize capturing precise contextual alignment with the objects of the scene. Single-modality configurations struggle in change scenarios, falling short of achieving the same level of contextual detail and, consequently, lower performance in these metrics.\nIn no-change cases, all configurations perform comparably, with marginal differences in BLEU and Sm scores. The dual-modality setup achieves the highest Sm score, which is closely followed by the $x_{sem}$ configuration. Notably, the semantic-only approach yields slightly higher BLEU scores than the RGB-only configuration in no-change cases, indicating a potential susceptibility in the RGB feature space where minor pixel-level variations could be misinterpreted as changes.\nIn the overall scope, the dual-modality configuration, where both $x_{rgb}$ and $x_{sem}$ features included, consistently achieves highest scores compared to any single-modality setup across all metrics. The dual-modality configuration (Xrgb & Xsem) demonstrates the benefits of combining RGB and semantic features for handling complex changes and improving overall object-context relationship in the captions, as can be deducted from significantly higher BLEU4 and CIDEr scores of this configuration."}, {"title": "F. Performance Comparison", "content": "A performance comparison is conducted among state-of-the-art benchmark frameworks, including RSICCformer, Chg2Cap, and PSNet, using both RGB and semantic (SEM) versions of the proposed SECOND-CC AUG dataset. Since these frameworks are not inherently designed to integrate multi-modal data, the dataset is provided in separate RGB and SEM formats for fine-tuning within their existing architectures.\nThe results explain the significant advantages of the proposed MModalCC framework over existing methods.\nAccording to the table, across all evaluation metrics, MModalCC achieves superior scores, clearly outperforming RGB and SEM variants of RSICCformer, Chg2Cap, and PSNet. Using MModalCC provides substantial gains in captioning performance when compared to the next best-performing framework. Among the competing frameworks, PSNet-RGB and RSICCformer-RGB scores 2nd and 3rd highest scores across all metrics, respectively. Specifically, for the Sm metric, MModalCC achieves 0.487, surpassing the next best scores of 0.447 (Chg2Cap-RGB) and 0.438 (RSICCformer-RGB) by approximately 9%. Similarly, for the BLEU4 metric, MModalCC outperforms Chg2Cap-RGB by a notable margin of 13.20%, and for the CIDEr metric, it achieves an 11.48% improvement.\nThe consistency of MModalCC's performance can be further validated by the scene descriptions presented in Figure 12. This figure compares captions generated by various RSICC models for pairs of RS images and their corresponding semantic maps with the generated captions, across five diverse scenes. The figure focuses on cornerstones of change captioning such as capturing relative spatial directions, accurately describing pre- and post-change states with precise language, and maintaining contextual relevance along with logical consistency.\nMModalCC stands out for its reliability in identifying the absence of change, a distinction shared only by Chg2Cap & PSNet frameworks trained on semantic maps in this example (Scene-1). The added value of both semantic and RGB features, with CMCA and UDCA, further increases the accuracy of its descriptions as well as its positional precision.\nIn change cases, MModalCC demonstrates its strength in accurately describing spatial relationships, both in terms of relative directional phrases (in relation to other objects in the scene) and event positioning within the scene (e.g., \"in the middle of the scene\", \"next to the river\u201d, \"next to the residential area\u201d) (Scenes 2-4). Other models sometimes misrepresent or omit these critical details.\nMModalCC minimizes issues like redundancy in descriptions, where other models sometimes generate repetitive phrases by repeating object names or characteristics within a single caption decoding (as seen in Scene-3 (f), Scene-4 (c), and Scene 5 (\u0435)).\nA captioning model must generate descriptions with logical consistency by accurately using object-context relationships within a scene. This requires aligning actions and transformations with the inherent properties of objects, avoiding illogical statements such as \"building a green region\" (e.g. Scene-3 (f)). For example, logical consistency ensures that man-made structures are appropriately described as \"constructed\" or \"demolished\", while natural elements are correctly represented as \"transformed\", \"replaced\" or \"disappeared\". MModalCC achieves this logical consistency through its MGCA decoder. The MGCA selectively fuses RGB and semantic features with joined context, providing relevant details from each modality for generating logically sound sentences with strong object-context relationships."}, {"title": "V. CONCLUSION", "content": "In this paper, we addressed the critical challenges in remote sensing change captioning, such as illumination differences, viewpoint variations, blur effects, and image registration errors. To tackle these issues, we introduced SECOND-CC, a publicly available dataset that provides 6041 high-resolution RGB image pairs and semantic segmentation maps at mixed spatial resolutions (0.5-3 m/pixel). Detailed change descriptions offer a comprehensive benchmark for RSICC research. Furthermore, we also prepared an augmented dataset SECOND-CC AUG with 10855 image pairs and 54 275 change descriptions to reach better performances in this challenging dataset.\nWe also proposed MModalCC, a novel multimodal framework that effectively integrates semantic and visual data through advanced attention mechanisms. Extensive experiments demonstrated that MModalCC significantly outperforms existing methods, including RSICCformer, Chg2Cap, and PSNet with +4.6% improvement on BLEU"}]}