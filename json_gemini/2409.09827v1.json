{"title": "On the Effect of Robot Errors on Human Teaching Dynamics", "authors": ["Jindan Huang", "Isaac Sheidlower", "Reuben M. Aronson", "Elaine Schaertl Short"], "abstract": "Human-in-the-loop learning is gaining popularity, particularly in the field of robotics, because it leverages human knowledge about real-world tasks to facilitate agent learning. When people instruct robots, they naturally adapt their teaching behavior in response to changes in robot performance. While current research predominantly focuses on integrating human teaching dynamics from an algorithmic perspective, understanding these dynamics from a human-centered standpoint is an under-explored, yet fundamental problem. Addressing this issue will enhance both robot learning and user experience. Therefore, this paper explores one potential factor contributing to the dynamic nature of human teaching: robot errors. We conducted a user study to investigate how the presence and severity of robot errors affect three dimensions of human teaching dynamics: feedback granularity, feedback richness, and teaching time, in both forced-choice and open-ended teaching contexts. The results show that people tend to spend more time teaching robots with errors, provide more detailed feedback over specific segments of a robot's trajectory, and that robot error can influence a teacher's choice of feedback modality. Our findings offer valuable insights for designing effective interfaces for interactive learning and optimizing algorithms to better understand human intentions.", "sections": [{"title": "1 INTRODUCTION", "content": "As robots become increasingly integrated into daily life, they will need to learn from and engage with non-expert users. To enable this, human-in-the-loop learning allows users to leverage their intuitive understanding of real-world tasks, facilitating robot learning more effectively than pure trial-and-error methods. While human teachers will adapt to the teaching interface and the robot learner, not all teaching methods and robot behaviors are straightforward for the teachers to learn and respond to. Therefore, it is crucial to design algorithms and systems that leverage the natural approaches people use for teaching to both enhance the teaching experience and appropriately react to a teacher's feedback.\nWhen people teach, they communicate different intentions through a variety of teaching signals [32] and accommodate their teaching based on the performance of the learner [47]. Much of existing work has considered the dynamic nature of human teaching from an algorithmic perspective. In particular, work has focused on how to collect and process various sources and forms of human feedback for agent learning, such as demonstrations or evaluations [5]. These algorithmic approaches often assume that only the content of the feedback changes. However, understanding this dynamic from a fundamentally human-centered perspective (in terms of how users decide and convey their teaching choices) still remains in its early stages. While some research has identified common behavioral variations in human teaching [19], the factors contributing to these variations and their mechanisms are under-explored.\nIn this paper, we focus on one potential factor: robot errors. Human teachers will inevitably encounter robot errors as they instruct a robot in a new task. Previous studies have shown that robot errors can influence general human behavior, such as social responses [28], engagement with robots [51] and perceptions of robots [49]. There is a need to study the relationship between robot errors and"}, {"title": "2 BACKGROUND", "content": "Human-in-the-loop (HITL) learning is an approach that integrates human input and inherent expertise to facilitate agent learning. It has garnered significant attention in research due to its potential to improve agent performance across various real-world domains [38, 43]. For example, reinforcement learning from human feedback (RLHF) is a HITL learning paradigm that has proven effective for training or refining models, especially with online crowdsourced feedback [10, 24]. In this paper, we focus on an RLHF setting while taking into account findings from previous HITL learning research.\nTraditional HITL learning approaches usually restrict feedback to a single type [14, 54] or treat variation in human feedback as noises (e.g., using Boltzmann distributions [22]). However, recent work suggests that they may oversimplify the intricacies of human teaching and overlook valuable information given by human instructors [8, 13, 31], which potentially leads to deterioration in agent learning performance. In an attempt to address these challenges, researchers have begun taking into account the ways in which people naturally approach the task of teaching, such as adapting teaching based on the learner's performance by using multiple teaching signals and strategies. However, current research focuses primarily on the technical challenges of incorporating diverse feedback sources into agent learning [12, 18] and developing standardized encoding format for various feedback types [36, 56]. There is a growing need to understand a fundamental question: what factors influence dynamic human teaching behavior towards robots?\nOne factor likely to influence teaching behavior is errors from learners. In human-human teaching, when a student makes some mistake, people often switch among multiple teaching strategies such as clarification, correction and motivation [15, 40]. This shift occurs because errors prompt teachers to estimate the severity [20] and reassess their perception of the student's abilities [16]. Similarly, in human-robot teaching, human instructors exhibit different teaching styles when interacting with robots with varying performance [23] and adjust their teaching behavior accordingly as they dynamically update their impressions of robot capabilities [48].\nRobot errors could also result in this dynamic human-robot teaching. For instance, Chi et al. [9] found that as social agents made fewer inappropriate responses, human teachers provided less frequent instruction. However, most work related to robot errors focuses on general human-robot interaction contexts. Previous research has demonstrated that the presence of robot errors can impact user engagement [2, 27] and people's perceptions on robots such as reliability and trust [39, 52, 55]. Moreover, the severity of robot mistakes affects both human-robot collaboration performance [17, 44, 45] and human reaction intensity [6, 37]. As HITL robot learning becomes more prevalent, understanding how robot errors shape human teaching is increasingly critical. Therefore, this paper investigates the role of robot errors as a key factor influencing human-robot teaching dynamics."}, {"title": "3 METHODOLOGY", "content": "Feedback dynamics vary in many ways. Users might chose a different modality, e.g. evaluation, instruction, or demonstration; or they might adjust the specificity of their feedback, such as critiquing a single robot action or an entire trajectory. These variations may be a function of many things such as fatigue, preference, or the setting of the learning task. In this work, we focus on how robot error affects teaching dynamics in an online, RLHF-style teaching scenario. To do this, we design a within-subjects study and apply it to two different contexts: open-ended teaching and forced-choice teaching. Through these studies, we examine how robot errors can impact three important dimensions of a human teacher's feedback dynamics: feedback granularity, feedback richness, and teaching time.\nFeedback granularity refers to how specific a user's feedback is with respect to time. For example, a user may wish to critique a whole robot trajectory or a specific moment where the robot did something particularly good or bad. Feedback granularity as defined in this way is often predefined by the designers of the system as the either a window in the past for which the feedback is assigned to (this is known as \"credit assignment\" [46]), or implicitly determined by showing end-users trajectories of fixed length to elicit their preferences [42]. A user, however, may want to able to be specific about what part of the robot's behavior their feedback is referring to. Thus, to study this dynamic from a teacher's perspective, we explicitly design an interface which a user can specify a credit assignment window.\nFeedback richness refers to how much information is present in a piece of feedback. Intuitively, this represents the difference between broad judgements about a behavior, such as saying \"good/bad job\""}, {"title": "3.1 Experiment Setup", "content": "3.1.1 Environment. We implemented a robot environment using Kinova Gen3 arm for salad preparation (see Fig. 2). The goal of the robot arm is to pick up the cups with necessary ingredients based on the given recipe and pour out the ingredients from those cups into a mixing bowl in the center of the table. There are 4 cups in the environment, and each of them can contain useful ingredients (e.g., lettuce, cucumber, tomatoes, onion), inedible dishwasher pods, or is empty.\n3.1.2 Robot Trajectories & Error Severity. To understand how robot errors influence human teaching, we designed 6 robot trajectories and recorded them as videos: 4 with errors and 2 without. The length of all videos is 20 \u00b1 2 seconds. Two successful videos are:\n\u2022 add lettuce: Add lettuce to the mixing bowl correctly\n\u2022 add tomatoes: Add tomatoes to the mixing bowl correctly\nEach erroneous trajectory includes one of the following mistakes:\n\u2022 add dishpods: Add dishwasher pods to the mixing bowl\n\u2022 add nothing: Add nothing to the mixing bowl as a result of grabbing and attempting to pour from an empty cup\n\u2022 miss tomatoes: Miss grabbing the cup with tomatoes by failing to close the gripper\n\u2022 drop onion: Drop onion pieces outside of the mixing bowl as a result of attempting to pour out the onions only partially over the mixing bowl\nThese errors are informed by a prior user study [1], which offers a comprehensive list of robot mistakes with corresponding severity rankings within the context of food preparation. We ensured that a range of error severity was covered across the four failure trajectories, from a relatively low severity error, such as failing to add an ingredient by forgetting to close the robot gripper, to a relatively high severity error, such as adding cleaning product to the salad. In our study, participants assessed the severity of robot errors by rating the perceived robot performance in each trajectory using a 5-point Likert scale (1-excellent, 5-very poor). Despite our prior assumptions over how severe each error may be, by directly querying participants for their perception of the error severity, we can account for any unexpected individual differences in that perception. Furthermore, this rating is collected after participants provide their feedback to ensure that it is not perceived as part of the robot teaching process.\n3.1.3 Interactive teaching interface. We developed a user interface that prompts participants to provide feedback for improving robot performance (see Fig. 3). Since different query styles may affect people's perception on robot intelligence [7], we used a universal prompt to collect feedback from human teachers (\"Please help the"}, {"title": "3.2 Measures", "content": "Here we define how we translated the three feedback dynamics, feedback granularity, feedback richness, and teaching time, into measures for analysis.\n3.2.1 Feedback granularity. To measure feedback granularity, we utilize the time ranges that participants provided to specify the segment of the robot trajectory their feedback applies to. Smaller time ranges indicate finer granularity of feedback, while larger time ranges denote less granularity. The finest granularity occurs when the start time equals the end time, and the largest when the time range spans the entire trajectory video.\n3.2.2 Feedback richness. In the forced-choice teaching context, feedback richness is quantified by the size of the response choice space, as described in [29]. For the evaluation method, where participants choose between \"good\" and \"bad\" buttons (binary feedback), the response choice space size is 2. For the instruction method, participants select an action from the robot's action set A, resulting in a response choice space size of |A|, in this case |A| = 7. A larger response choice space size indicates greater feedback richness and vice versa.\nIn the open-ended teaching context, feedback richness is measured by calculating the word count of the text feedback. To control for an individual's overall verbosity, we normalize each participant's word count values by subtracting their minimum word count from the participant's own data. Longer text feedback corresponds to higher feedback richness, whereas shorter feedback implies lower richness. Naturally, there are cases where a participant's feedback may be long but contains information that could be said with fewer words. This does, however, serve as a proxy measure, providing an intuitive way for separate out broad feedback, such as \"good job,\" and more specific instructional feedback about what the robot should do.\n3.2.3 Teaching time. Teaching time refers to the duration of time spent by the participant in each trial. This is measured by recording the start and end times as the participant begins and finishes teaching the robot within a trial, then calculating the difference between these times."}, {"title": "3.3 Experiment Procedure", "content": "We conducted an online study and each experiment lasted ~15 minutes. Each participant signed an informed consent form to confirm their eligibility (fluent English speaker, a United States resident, and at least 18 years old). The participant continued to complete a brief survey collecting their demographics, technology background, and previous robot experience.\nNext, the participant would use the interactive teaching interface described in Section 3.1.3 to give feedback to the robot. The participant was randomly assigned to the forced-choice teaching context or the open-ended teaching context. In both cases, they would see six robot trajectories, as we described in Section 3.1.2. To mitigate the ordering effect, we also randomized the order of robot trajectories using the Latin Square method [25]. For each trial, the participant watched a robot trajectory video and was prompted to provide feedback to improve the robot performance.\nFinally, participants were asked to fill in a post-study questionnaire that included two Likert scale questions related to their teaching experience, and two open-ended questions asking about teaching strategies and general comments. The two Likert scale questions were: \"I think the teaching interface is easy to use.\" and \"I can communicate my thoughts well using the teaching interface.\" The two open-ended questions were: \"What teaching strategy did you use for providing feedback to the robot?\" and \"Do you have any overall comments about the study?\" Upon successful completion of all study components, the participants received a completion code for compensation."}, {"title": "4 RESULTS", "content": "The procedure of our study was approved by the University Institutional Review Board (IRB), and we published our study on the Prolific platform [41] to recruit participants. In both forced-choice and open-ended teaching contexts, participants were compensated $4 for participating in the study. For the forced-choice teaching context, we recruited 32 participants (17 females, 15 males; aged 18-64). For the open-ended teaching context, we recruited 30 participants (24 females, 5 males, 1 non-binary; aged 18-70).\nManipulation Check. To ensure that the design of the robot trajectories reflected different severity levels, we did a manipulation"}, {"title": "4.1 Influence on Feedback Granularity", "content": "4.1.1 Results of forced-choice teaching. To test H1a, we categorized the feedback granularity data based on the presence of robot errors in each robot trajectory into two groups: successful trajectory group and erroneous trajectory group. Figure 5 visualizes the data. The results indicate that participants provided feedback with lower granularity for erroneous robot trajectories compared to successful ones (p < 0.001, BF10 > 10000), supporting H1a. Additionally, the severity of robot errors significantly correlates feedback granularity (r = -0.287, p < 0.001, BF10 > 10000), supporting H1b. Specifically, when the robot makes a more severe mistake, participants tend to specify a narrower credit assignment window to evaluate or instruct the robot.\n4.1.2 Results of open-ended teaching. Similar to the analysis in the section 4.1.1, we found evidence that the presence of robot errors affects feedback granularity (p = 0.043, BF10 = 2.336), which supports H1a. Fig. 6 illustrates that participants were inclined to specify a finer feedback window for erroneous trajectories, though the evidence is less strong compared to forced choice teaching data."}, {"title": "4.2 Influence on Feedback Richness", "content": "4.2.1 Results of forced-choice teaching. In the forced-choice teaching context, we define feedback richness using the size of response choice space as described in section 3.2.2. We found that the presence of robot error significantly influences the feedback richness (p < 0.001, BF10 = 7857.122), which supports H2a (see Fig. 7). Additionally, we conducted a Mann-Whitney U t-test and found strong evidence that the severity of robot errors corresponds to the choices of teaching methods with different feedback richness. In particular, the severity ratings of the participants for the"}, {"title": "4.3 Influence on Teaching Time", "content": "4.3.1 Results of forced-choice teaching. We found strong evidence that the presence of robot errors has an impact on teaching time (p < 0.001, BF10 = 11.729), which supports H3a (see Fig. 9). It reveals that in forced choice teaching, participants spent more time to teach erroneous trajectories compared to successful ones. Moreover, the severity of robot errors correlates positively to teaching time (r = 0.141, p = 0.009, BF10 = 6.103), supporting H3b as well. That is to say, participants spent more time teaching the robot when it made a more severe error.\n4.3.2 Results of open-ended teaching. We failed to find a significant result that the presence of robot errors influences the amount of time participants spent on teaching in open-ended teaching (p = 0.672, BF10 = 0.203). Similarly, we did not find any evidence that the severity of robot correlates to teaching time (rr = -0.030, p = 0.587, BF10 = 0.116)."}, {"title": "4.4 Overall Teaching Experience in Forced-choice and Open-ended Teaching", "content": "In the post-study questionnaire, each participant was asked to rate the usability and expressiveness of the teaching interface using a 5-point likert scale. To understand whether there is any differences between the data collected from forced-choice and open-ended teaching, we conducted two-sample t-tests. We failed to find significant differences in interface usability (p = 0.079, BF10 = 0.983). However, there was a significant difference in interface expressiveness (p = 0.011, BF10 = 4.334). This finding indicates that participants perceived language feedback to be more expressive than structured evaluative and instructive feedback. Given that usability was not perceived differently by participants, a promising research direction could be to develop more expressive teaching interfaces for forced choice teaching scenarios."}, {"title": "5 DISCUSSION", "content": "In this paper, we demonstrate that the presence and severity of robot errors affect three dimensions of human teaching dynamics: feedback granularity, feedback richness, and teaching time. Compared to a successful example, when the robot displays erroneous behavior, people are inclined to use a narrower feedback window, provide more detailed information, and spend more time teaching the robot. In general, this tendency becomes more pronounced as the severity of the robot error increases.\nInterestingly, the effects of robot errors on human teaching are less pronounced in the open-ended teaching context than in the forced choice one. For instance, in the open-ended teaching, we saw a bimodal distribution of the feedback granularity data from the successful trajectory group (see Fig. 6). Some participants provided feedback over a long trajectory, while others focused on a small segment of the robot's actions or even a single action. This variability may be attributed to the nature of language feedback, which is less structured than binary feedback or action instructions, and to the diverse metrics that people use to assess robot performance. Although the robot may perform well in terms of task completion, participants might identify sub-optimalities in other areas, such as task planning (P1-open-ended: \"The robot should have started off with tomatoes, but this will still work.\") and fine manipulations (P4-open-ended: \"When pouring ingredients into the bowl, you want to make sure that you are pouring directly into the center of the bowl.\"). Language feedback allows flexibility in expressing teaching information but may be less straightforward for robot learners to interpret. From the robot learning perspective, ideal human feedback should specify what the error is, when it occurs, how it happens, and potential solutions to fix it. However, many participants only provided information related to error detection, such as \"P6-open-ended: the robot missed the bowl, spilling onions\" and \"P4-open-ended: Dishwasher pods do not belong in this salad\". This tendency introduces"}, {"title": "Limitations and Future Work", "content": "There is a wide range of teaching contexts and scenarios where robot error could impact a user's teaching style and behavior. In this work, we chose to focus on an online crowdsourcing setting where users provide feedback to pre-recorded robot trajectories. This setting is particularly important as large robot-behavior models begin to develop and incorporate RLHF into their training. However, the results and takeaways in this context may not apply to in-person human-robot teaching scenarios where the interaction may span a longer period and where the robot may be adapting its behavior in real-time. In that case, a user's perception of the robot error may change as a function of how the robot improves at the task for instance. This setting warrants its own investigation as well as analysis as to how it differs from the online setting.\nFuture work should investigate potential causes other than robot errors that influence teaching dynamics; for example, the nature of the task the robot is performing or legibility of the robot's motions. Future work should also study how a user's choice of teaching method is affected by robot error when a broader range of feedback modalities are available. In our forced-choice teaching study, for instance, we provided users with a choice of only binary evaluation or single-action instructions. This choice allowed us to compare the two feedback methods of evaluation and instruction with distinct levels of feedback richness and specificity. However, there are other ways of collecting feedback, such as collecting full-trajectory demonstrations, querying for preferences, or learning from implicit feedback like facial expressions, which can further enhance teachers' expression and choices. Lastly, a limitation with regard to our evaluation is that our analysis of feedback richness using word-count did not capture all of the nuances of the language feedback that was provided. For example, an equally long sentence could discuss multiple errors or focus on one error. This difficulty also prevented us, at least in this work, from more directly comparing the feedback results from forced choice and open-ended teaching. Defining feedback richness in a measurable way that captures both the user's intent as well as the amount of relevant information to the learning process itself is an on-going area of research."}, {"title": "6 CONCLUSION", "content": "As researchers design human-in-the-loop learning algorithms and as these systems are deployed, it is crucial to ensure that the teaching process is efficient, accessible, and comfortable. Robot errors are and will continue to be an inevitable part of this process. Developing a greater understanding of how these errors influence the teaching dynamics enables us to create better teaching experiences as well as learning algorithms. To that end, this paper presented results about how error can affect three dimensions of human teaching dynamics: feedback granularity, feedback richness, and teaching time. We studied this through the lens of an online teaching scenario and found that errors not only influence these teaching dynamics but often do so in a predictable way. Although there is more to be explored regarding how robot errors impact teachers in other human-robot teaching settings, this paper lays the groundwork for studying these phenomena and provides insights that can be applied to the design of human-in-the-loop learning systems."}]}