[{"title": "Force Matching with Relativistic Constraints: A Physics-Inspired Approach to Stable and Efficient Generative Modeling", "authors": ["Yang Cao", "Bo Chen", "Xiaoyu Li", "Yingyu Liang", "Zhizhou Sha", "Zhenmei Shi", "Zhao Song", "Mingda Wan"], "abstract": "This paper introduces Force Matching (ForM), a novel framework for generative modeling that represents an initial exploration into leveraging special relativistic mechanics to enhance the stability of the sampling process. By incorporating the Lorentz factor, ForM imposes a velocity constraint, ensuring that sample velocities remain bounded within a constant limit. This constraint serves as a fundamental mechanism for stabilizing the generative dynamics, leading to a more robust and controlled sampling process. We provide a rigorous theoretical analysis demonstrating that the velocity constraint is preserved throughout the sampling procedure within the ForM framework. To validate the effectiveness of our approach, we conduct extensive empirical evaluations. On the half-moons dataset, ForM significantly outperforms baseline methods, achieving the lowest Euclidean distance loss of 0.714, in contrast to vanilla first-order flow matching (5.853) and first- and second-order flow matching (5.793). Additionally, we perform an ablation study to further investigate the impact of our velocity constraint, reaffirming the superiority of ForM in stabilizing the generative process. The theoretical guarantees and empirical results underscore the potential of integrating special relativity principles into generative modeling. Our findings suggest that ForM provides a promising pathway toward achieving stable, efficient, and flexible generative processes. This work lays the foundation for future advancements in high-dimensional generative modeling, opening new avenues for the application of physical principles in machine learning.", "sections": [{"title": "1 Introduction", "content": "The field of generative modeling has witnessed significant progress with the advent of sophisticated techniques that leverage neural networks to synthesize high-quality data. Recent methods such as Diffusion Models (DM) [SDWMG15, HJA20a, DN21, RBL+22, ZLCZ23, ZLKE23], Flow Matching (FM) [LCBH+22, LGL22, EKB+24], and the more recent Equilibrium Dynamics Model (EDM) [KAAL22, KAL+24] have emerged as prominent approaches, each exploring distinct generative paradigms. These techniques differ fundamentally in how they utilize neural networks to evolve data representations over time: while Diffusion Models rely on iterative transformations of a Gaussian-initialized distribution, EDM employs an Ordinary Differential Equation (ODE) for continuous evolution, and FM directly predicts the data's velocity using a neural-network-based velocity field. Such diversity in generative approaches has motivated the need for a unified perspective that can bridge these conceptual differences.\nIn pursuit of this unification, TrigFlow [LS24] was proposed as a generalized framework that provides a continuous generative process capable of transitioning between the behaviors of EDM and FM. By leveraging trigonometric parameterization, TrigFlow formulates data generation through a combination of trigonometric components, allowing for a flexible representation that captures the strengths of both paradigms. The framework introduces a trigonometric-based parameterization of the generative process, a loss function aligning with diffusion methods, and a probability flow ODE, thereby offering a more comprehensive understanding of generative modeling techniques and providing a foundation for further advancements in this domain.\nBuilding on this unification perspective, this paper introduces Force Matching (ForM) as a novel generative framework inspired by principles of relativistic mechanics to stabilize the sampling process. By incorporating relativistic constraints through the Lorentz factor, ForM ensures stable sampling dynamics, limiting the velocity of generated samples to avoid instability. We establish that ForM is well-aligned with consistency models, suggesting its potential to enhance scalable generative modeling solutions. Our contributions can be summarized as follows:\n\u2022 We propose Force Matching (ForM), a novel generative modeling framework inspired by relativistic mechanics, which ensures stable sampling by constraining sample velocities through the Lorentz factor.\n\u2022 We establish theoretical foundations for ForM, highlighting its flexibility and scalability.\n\u2022 We conduct extensive empirical evaluations, showing that ForM outperforms baseline flow matching methods in generative tasks and validating the effectiveness of its velocity constraint through ablation studies.\nThese contributions illustrate the promise of force-based methods in generative modeling, emphasizing their capability for stable, efficient, and flexible sampling. This work not only extends our understanding of generative techniques but also lays the foundation for exploring novel high-dimensional generative frameworks that effectively integrate stability and efficiency.\nRoadmap. In Section 2, we introduce related work of generative models and flow matching. Then, Section 3 introduces the preliminary of Force Matching. We then propose the Force Matching architecture in Section 4. Section 5 demonstrates empirical experiments of Force Matching, and Section 6 performs ablation study of Force Matching. Finally, we conclude this paper in Section 8."}, {"title": "2 Related work", "content": "Generating Models. Generative models have made significant progress over the last decade, enabling diverse applications such as image synthesis, text generation, and data augmentation. One of the foundational models in this area is the Generative Adversarial Network (GAN) introduced by Goodfellow et al. (2014), which consists of a generator and a discriminator that compete in a zero-sum game, thereby leading to the generation of realistic data samples [GPAM+14]. GANs have inspired a variety of derivative architectures aimed at improving stability and quality, including Wasserstein GANs (WGAN), which address the instability issues of GANs by employing a different distance metric [ACB17]. Conditional GANs (cGANs) extend the GAN framework to generate data conditioned on additional information, making them more controllable [MO14].\nVariational Autoencoders (VAEs), proposed by Kingma and Welling (2013), offer another generative approach that combines variational inference with neural networks to learn a latent variable model of data [KW13]. Unlike GANs, VAEs maximize a lower bound on the log-likelihood of the data, allowing for a more principled probabilistic interpretation. The introduction of the reparameterization trick was key to making VAEs feasible to train with stochastic gradient descent, which has had a considerable impact on the field of deep generative models. Recently, autoregressive models such as PixelCNN [OKK16] and Transformer-based models [VSP+17] have demonstrated impressive performance in tasks like image and text generation. These models learn to predict the next element in a sequence, thereby allowing them to generate samples one step at a time, which has proven particularly effective for generating sequential data such as text and audio.\nThe development of large-scale language models like GPT-3 [BMR+20] has further showcased the power of autoregressive architectures in generating coherent and contextually relevant long text, significantly advancing the state-of-the-art in natural language processing. Another line of work explores diffusion models, such as Denoising Diffusion Probabilistic Models (DDPMs), which have gained attention for their ability to generate high-quality images by modeling the process of gradually adding noise to data and then learning to reverse this process [HJA20b]. These models provide an alternative to GANs by optimizing likelihood-based objectives, which makes training more stable. DDPMs have set new benchmarks for image generation quality, rivaling the output of GANs while avoiding some of their training difficulties. These developments collectively showcase the evolution of generative models from adversarial training with GANs to likelihood-based training with VAEs, autoregressive models, e.g, Visual autoregressive modeling (VAR) [TJY+24], and diffusion-based approaches, e.g., DDPM [HJA20b]. Each of these methods contributes unique strengths and capabilities, advancing the scope and quality of generated data.\nFlow Matching. Flow matching [DLT+24, YLP+24, GDB+24, CGL+25] is a key concept in fields such as optimal transport, computer vision, and machine learning, where it has been extensively studied and utilized to align two distributions effectively. The method has roots in the classic work on optimal transport theory, where Monge and Kantorovich initially laid out the foundational ideas for mapping mass distributions with minimal cost [Mon81, Kan42]. Building upon these ideas, Villani expanded the theoretical framework of optimal transport, leading to a rigorous mathematical foundation for flow matching and its related applications [Vil08].\nRecent advances in machine learning have leveraged flow matching for deep generative modeling tasks. Denoising diffusion probabilistic models (DDPMs), for example, have drawn inspiration from flow-based methods to improve the stability and efficiency of training [HJA20b]. Similarly, score-based generative models utilize a stochastic differential equation approach to approximate flows, effectively creating a flow-matching procedure for generating realistic data samples [SSDK+21]. This approach has demonstrated considerable success in capturing complex, high-dimensional data dis-"}, {"title": "3 Preliminary", "content": "In Section 3.1, we introduce all the notations we used in our paper. Then, in Section 3.2, we show the basic facts about flow matching. In Section 3.3, we present the basic background of special relativity and define the relativistic force."}, {"title": "3.1 Notations", "content": "For any positive integer $n$, we use $[n]$ to denote set $\\{1, 2, \\ldots, n\\}$. For two vectors $x \\in \\mathbb{R}^n$ and $y \\in \\mathbb{R}^n$, we use $\\langle x, y \\rangle$ to denote the inner product between $x, y$. For a vector $v \\in \\mathbb{R}^n$, we use $||v||_2$ to denote the $\\ell_2$-norm of $v$. We use $1_n$ to denote a length-$n$ vector where all the entries are ones. We use the symbol $\\perp$ to represent a component that is perpendicular to the direction of velocity, as exemplified by $a_{\\perp t}$, which denotes the perpendicular acceleration. Similarly, the symbol $\\parallel$ is employed to indicate a component parallel to the direction of velocity, such as $f_{\\parallel t}$, which represents the parallel force. We use $\\dot x_t$ to denote $\\frac{d x_t}{dt}$, and $\\ddot x_t$ to denote $\\frac{d^2 x_t}{dt^2}$."}, {"title": "3.2 Flow Matching", "content": "Flow Matching (FM) [LCBH+22, LGL22] is a generative modeling technique that constructs a smooth, invertible (i.e., diffeomorphic) mapping from a simple prior distribution to a complex target distribution. In FM, a time-dependent mapping $Z_t$ is defined to evolve according to an ordinary differential equation (ODE) driven by a vector field:\n$\\frac{dx_t}{dt} = V_t(x_t), \\quad t \\in [0, T]$.\nThe goal is to ensure that, at the terminal time $T$, the ODE transforms a sample $x_0$ from a simple distribution (e.g., a Gaussian) into a sample $x_f$ from the target data distribution $D$.\nTo achieve this, Flow Matching (FM) constructs a stochastic interpolation between a sample $x_1 \\sim D$ and a sample $x_0$ drawn from a known prior distribution, typically $\\mathcal{N}(0, I)$. The interpolation"}, {"title": "3.3 Background on Special Relativity", "content": "We first introduce several essential ideas of special relativity [E+05].\nDefinition 3.1 (Lorentz Factor). According to special relativity [E+05], the Lorentz factor at lab time $t$ is given by\n$\\gamma_t := \\frac{1}{\\sqrt{1 - ||v^{\\text{lab}}_t||^2 / c^2}}$\nwhere $v^{\\text{lab}}_t$ is the velocity at lab frame of reference, $c = 3 \\times 10^8$ is the speed of light in vacuum.\nThen, we introduce the proper time of special relativity.\nDefinition 3.2 (Proper Time). The proper time is defined as the time interval measured in the rest frame of a moving object according to special relativity [E+05]. The differential form of the proper time is given by\n$d \\tau = \\frac{dt}{\\gamma_t}$\nwhere $dt$ is the time interval in the laboratory frame of reference, and $\\gamma_t$ is the Lorentz factor at time lab time $t$ as defined in Definition 3.1."}, {"title": "4 Force Matching", "content": "In this section, we introduce Force Matching (ForM), a new architecture for generative models, and provide its theoretical analysis. In Section 4.1, we introduce the training objective of Force Matching. Then, in Section 4.2. In Section 4.3, we illustrate and discuss the speed limitation of ForM. In Section 4.4, We show the interpolation path for ForM."}, {"title": "4.1 Definition of Force Matching Objective", "content": "Next, we define the training objective of Force Matching.\nDefinition 4.1 (Force Matching Objective). The training objective of Force Matching (ForM) is defined by\n$\\mathcal{L}_{\\text{ForM}}(\\theta) := \\mathbb{E}_{t \\sim \\text{Uniform}[0, T], x_1 \\sim D}[||F_t(x_t) - f_t(x_t)||^2]$,\nwhere $D$ is the target data distribution, $f_t(x_t)$ is the target relativistic force defined in Definition 3.3, and $F_t(x)$ is a trainable neural network parameterized with $\\theta$."}, {"title": "4.2 Our Result I: Sampling ODE", "content": "We define an ordinary differential equation (ODE) in order to get the position based on a given relativistic force.\nTheorem 4.2 (Sampling ODE, informal version of Theorem A.1). Giving the force at position $x_t$ denoted as $f_t(x_t)$, we could solve for ForM sampling path $x_t$ by the following ODE\n$\\ddot x_t = \\frac{1}{m^{\\text{lab}} \\gamma_t} \\left(f^{\\text{local}}_t - \\frac{\\gamma_t^3 \\langle v^{\\text{lab}}_t, f^{\\text{local}}_t \\rangle}{c^2} v^{\\text{lab}}_t\\right)$,\nwhere $x_0 \\sim \\mathcal{N}(0, I)$, $\\dot x_0 = 0$.\nTheorem 4.2 shows how to derive the position $x_t$ from the relativistic force field $f_t(x_t)$. Unlike first-order flow-based methods, ForM naturally involves a second-order ODE because it encodes the evolution of both position and velocity under relativistic constraints. This allows for more expressive and physically-motivated sampling trajectories, where velocity constraints can help stabilize the generative process. Once a neural network $F_\\theta(x)$ is trained to approximate $f_t(x)$, the sampling procedure integrates this second-order ODE to produce samples consistent with the target distribution."}, {"title": "4.3 Our Result II: Speed Limit", "content": "One property of relativistic mechanics is the velocity will always be under the constant $c$, which is the speed of light.\nIn reality, the speed of light is $c \\approx 3 \\times 10^8$. For any $v_t$, the speed $||v_t||_2$ can approach but never exceed $c$. This property stabilizes the generating process. We formalize and prove this in Theorem 4.3.\nTheorem 4.3 (Speed Limit, informal version of Theorem A.3). For a ForM model with sampling path $x: [0, T) \\rightarrow \\mathbb{R}^n$, the velocity satisfies\n$||\\dot x_t||_2 < c, \\quad \\forall t \\in [0, T)$.\nIt shows that the sample velocity remains strictly below $c$ at all times under relativistic constraints. Practically, this upper bound on velocity helps mitigate risks of numerical instability or \"exploding gradients\" that can sometimes arise in diffusion- or flow-based generative models. By capping the speed of samples, ForM maintains a controlled and stable evolution in high-dimensional spaces. This provides a theoretical guarantee of safety against runaway behaviors, making the sampling process more robust."}, {"title": "4.4 Our Result III: ForM with TrigFlow", "content": "The interpolation path of ForM is given by the following theorem.\nTheorem 4.4 (ForM with TrigFlow, informal version of Theorem A.4). We let $m = 1$ for simplicity in ForM. Giving a the interpolation $x_t = \\alpha_t x_1 + \\sigma_t x_0$, where $\\alpha_T = 1$, $\\alpha_1 = 0$, $\\sigma_T = 0$, $\\sigma_0 = 1$. We let $F_t(x_t)$ denote a vector map of force, a trainable neuron network parameterized with $\\theta$. We select the $\\alpha_t$ and $\\sigma_t$ identical with TrigFlow [LS24], where $\\alpha_t = \\sin(t)$ and $\\sigma_t = \\cos(t)$, $T = \\frac{\\pi}{2}$. Then, force interpolation could be simplified to\n$f_t(x_t) = \\frac{(\\cos(t)x_1 - \\sin(t)x_0) \\cdot (-\\sin(t)x_1 - \\cos(t)x_0)}{c^2 - (\\cos(t)x_1 - \\sin(t)x_0)^2} (\\cos(t)x_1 - \\sin(t)x_0)).$"}, {"title": "5 Experiment", "content": "In this section, we conduct a systematic evaluation of Force Matching (ForM) effectiveness through extensive experimentation, emphasizing its significant role in advancing distribution generation. Section 5.1 describes the experimental framework, encompassing the Onedot, Halfmoons, and Spiral datasets, force dynamics, and a comparative analysis of various trajectory evolution"}, {"title": "5.1 Experiment setup", "content": "For an in-depth trajectory evolution analysis, we assess three approaches: the standard flow matching [LCBH+22] method utilizing only the first-order term, an improved approach integrating both first-order and second-order terms, and our proposed ForM model on three kinds of complex and challenging datasets.\nDatasets. (1) Onedot dataset: As illustrated in Figure 1, the Onedot dataset comprises 200 points sampled from a Gaussian distribution with variance 0.3 to establish the central source distribution. The target distribution is generated via a Lorentz field, where each point's initial velocity matches"}, {"title": "5.2 Results analysis", "content": "This section presents a comparative analysis of trajectory evolution on the Onedot and Halfmoons datasets. Figure 4 illustrates the trajectories generated by three approaches: (i) the original flow matching method that utilizes only the first-order term (denote as O1), (ii) an enhanced variant"}, {"title": "5.3 Euclidean distance loss", "content": "This section quantitatively assesses different approaches using Euclidean distance loss, which quantifies the deviation between transported and target distributions. Table 1 presents the loss values across three methods on the Onedot dataset and Halfmoons dataset, the unit we use is 0.1 light seconds. Lower values indicate higher precision in distribution transfer. The results demonstrate that incorporating the second-order term (O1 + O2) enhances performance beyond the first-order term (01) alone. Notably, ForM surpasses both baselines, achieving the lowest Euclidean distance loss, reinforcing its effectiveness in modeling transport trajectories."}, {"title": "6 Empirical Ablation Study", "content": "In this section, we empirically evaluate the performance of our proposed ForM model through a series of ablation experiments. We begin by illustrating the Spiral dataset and the corresponding transport objective, where samples are moved from the distribution $\u03c0_0$ (blue) to $\u03c0_1$ (pink) (see Figure 3). We then compare three approaches: the baseline Flow matching method using solely the first-order term (01), an enhanced variant that integrates both first-order and second-order loss metrics (O1 + O2), and our ForM model that leverages the Lorentz force to guide trajectory evolution. Qualitative results are shown in Figure 4, while Table 1 quantitatively demonstrates the superior performance of the ForM model."}, {"title": "7 Discussion", "content": "The core motivation behind the Force Matching (ForM) model stems from the need to stabilize generative modeling processes, particularly in flow-based methods where uncontrolled velocity magnitudes can lead to instability during sampling. Traditional approaches, such as Flow Matching (FM), offer different perspectives on modeling data evolution, yet they often lack explicit constraints that regulate sample movement, which can hinder both stability and efficiency. Inspired by special relativistic mechanics, we introduce a principled way to control velocity magnitudes through"}, {"title": "8 Conclusion", "content": "In this work, we introduce Force Matching (ForM), an innovative and comprehensive framework for generative modeling. ForM incorporates the principles of relativistic mechanics, higher-order flow matching, and TrigFlow, forming a unique and powerful synergy. The core idea behind ForM is to model the generative process in a way that accounts for both the geometric and dynamic aspects of flow, inspired by relativistic mechanics. We demonstrate that ForM not only preserves the structure of the data but also introduces an additional layer of stability to the generative process. Specifically, we theoretically prove that ForM bounds the velocity of the generative process under a hyperparameter c during the sampling procedure, which leads to improved control and stabilization of the process. This stabilization mechanism mitigates issues such as mode collapse and sampling instability that often plague other generative models. Through extensive empirical experiments, we demonstrate that ForM outperforms both Flow Matching and second-order Flow Matching in terms of generative quality and sample diversity. These results highlight the effectiveness of incorporating higher-order dynamics and relativistic principles into the generative process. Additionally, we conduct an ablation study to evaluate the individual components of ForM, further demonstrating its superiority over existing methods. This analysis confirms the contribution of each aspect of the framework, such as the higher-order flow matching and relativistic dynamics, to its overall performance. ForM offers a fresh perspective on the understanding of Flow Matching within the context of relativistic mechanics, presenting a new paradigm in the field of generative modeling. By redefining the conceptual and practical foundations of generative processes, ForM sets a new benchmark for the future development of generative models."}, {"title": "A Theoretical Analysis", "content": "In this section, we first provide the formal theorem and proof for the sampling ODE in Section A.1. Then, we formally proved the speed limit of ForM's sampling ODE in Section A.2. In Section A.3, we formally prove the derivation of the interpolation path of ForM with TrigFlow. Last, we illustrate the formal proof for relativistic force in Section A.4."}, {"title": "A.1 Sampling ODE", "content": "Here, we restate the Theorem 4.2 and state its proof.\nTheorem A.1 (Sampling ODE, formal version of Theorem 4.2). Giving the force at position $x_t$ denoted as $f_t(x_t)$, we could solve for ForM sampling path $x_t$ by the following ODE\n$\\ddot x_t = \\frac{1}{m^{\\text{lab}} \\gamma_t} \\left(f^{\\text{local}}_t - \\frac{\\gamma_t^3 \\langle v^{\\text{lab}}_t, f^{\\text{local}}_t \\rangle}{c^2} v^{\\text{lab}}_t\\right)$,\nwhere $x_0 \\sim \\mathcal{N}(0, I)$, $\\dot x_0 = 0$.\nProof. Recall $f^{\\text{local}}$ from Lemma A.5\n$f^{\\text{local}} = m^{\\text{lab}} \\left(\\gamma_t \\dot v^{\\text{lab}} + \\frac{\\gamma_t^3 \\langle v^{\\text{lab}}, \\dot v^{\\text{lab}} \\rangle}{c^2} v^{\\text{lab}}\\right)$,\nwhere $\\gamma_t$ is the Lorentz factor defined in Definition 3.1.\nTo solve for $\\dot v^{\\text{lab}}$, we could first decompose $\\dot v^{\\text{lab}}$ by\n$\\dot v^{\\text{lab}} = \\dot v^{\\text{lab}}_{\\perp t} + \\dot v^{\\text{lab}}_{\\parallel t}$,\nwhere $\\dot v^{\\text{lab}}_{\\parallel t}$ denotes the component of $\\dot v^{\\text{lab}}$ parallel with $v^{\\text{lab}}$, and $\\dot v^{\\text{lab}}_{\\perp t}$ denotes the component of $\\dot v^{\\text{lab}}$ perpendicular with $v^{\\text{lab}}$.\nAccording to the definition of parallel and perpendicular, we have\n$\\dot v^{\\text{lab}}_{\\parallel t} = \\frac{\\langle v^{\\text{lab}}, \\dot v^{\\text{lab}} \\rangle}{\\|v^{\\text{lab}}\\|^2} v^{\\text{lab}}$\n$\\dot v^{\\text{lab}}_{\\perp t} = \\dot v^{\\text{lab}} - \\dot v^{\\text{lab}}_{\\parallel t}$.\nThen we have\n$f^{\\text{local}} = m^{\\text{lab}} \\left(\\gamma_t \\dot v^{\\text{lab}} + \\frac{\\gamma_t^3 \\langle v^{\\text{lab}}, \\dot v^{\\text{lab}} \\rangle}{c^2} v^{\\text{lab}}\\right)$\n$= m^{\\text{lab}} \\left(\\gamma_t (\\dot v^{\\text{lab}}_{\\perp t} + \\dot v^{\\text{lab}}_{\\parallel t}) + \\frac{\\gamma_t^3 \\langle v^{\\text{lab}}, \\dot v^{\\text{lab}} \\rangle}{c^2} v^{\\text{lab}}\\right)$\n$= m^{\\text{lab}} \\left(\\gamma_t (\\dot v^{\\text{lab}}_{\\perp t} + \\dot v^{\\text{lab}}_{\\parallel t}) + \\frac{\\gamma_t^3}{c^2} \\langle v^{\\text{lab}}, \\dot v^{\\text{lab}}_{\\perp t} + \\dot v^{\\text{lab}}_{\\parallel t} \\rangle v^{\\text{lab}}\\right)$,\n$\\dot v^{\\text{lab}}_{\\parallel t} = \\frac{\\langle v^{\\text{lab}}, \\dot v^{\\text{lab}} \\rangle}{\\|v^{\\text{lab}}\\|^2} v^{\\text{lab}}$."}, {"title": "A.2 Speed Limit", "content": "In this subsection, we first calculate the derivative of the squared norm of velocity, then restate the Theorem 4.3 and provide its proof.\nLemma A.2 (Derivative of the squared norm of velocity). Let $X(t) := \\|v^{\\text{lab}}_t\\|^2$. Then we have\n$\\frac{dX(t)}{dt} = \\frac{2 \\gamma_t}{m^{\\text{lab}}} \\langle f^{\\text{local}}, v^{\\text{lab}} \\rangle \\left(1 - \\frac{\\gamma_t^2 \\|v^{\\text{lab}}\\|^2}{c^2}\\right)$,\nProof. Recall the sampling ODE\n$\\dot v^{\\text{lab}} = \\frac{1}{m^{\\text{lab}} \\gamma_t} \\left(f^{\\text{local}} - \\frac{\\gamma_t^3 \\langle v^{\\text{lab}}, f^{\\text{local}} \\rangle}{c^2} v^{\\text{lab}}\\right)$,\nwhere $\\gamma_t$ is the Lorentz factor at lab time $t$ defined in Definition 3.1.\nWe can show that\n$\\frac{dX(t)}{dt} = \\frac{d \\frac{1}{2} \\|v^{\\text{lab}}\\|^2}{dt}$\n$= \\langle v^{\\text{lab}}, \\dot v^{\\text{lab}} \\rangle$\n$= \\langle v^{\\text{lab}}, \\dot v^{\\text{lab}} \\rangle$\n$= \\langle v^{\\text{lab}}, \\frac{1}{m^{\\text{lab}} \\gamma_t} \\left(f^{\\text{local}} - \\frac{\\gamma_t^3 \\langle v^{\\text{lab}}, f^{\\text{local}} \\rangle}{c^2} v^{\\text{lab}}\\right) \\rangle$\n$= \\frac{1}{m^{\\text{lab}} \\gamma_t} \\left(\\langle v^{\\text{lab}}, f^{\\text{local}} \\rangle - \\frac{\\gamma_t^3 \\langle v^{\\text{lab}}, f^{\\text{local}} \\rangle \\langle v^{\\text{lab}}, v^{\\text{lab}} \\rangle}{c^2} \\right)$\n$= \\frac{\\langle v^{\\text{lab}}, f^{\\text{local}} \\rangle}{m^{\\text{lab}} \\gamma_t} \\left(1 - \\frac{\\gamma_t^2 \\|v^{\\text{lab}}\\|^2}{c^2}\\right)$,\nwhere the first step follows from the definition of $X(t)$, the second step follows from the chain rule, the third step follows from the basic fact, the fourth step follows from the Eq. (4), the fifth and last step follows from basic algebra.\nHere, we restate the Theorem 4.3 and state its proof.\nTheorem A.3 (Speed Limit, formal version of Theorem 4.3). For a ForM model with sampling path $x: [0, T) \\rightarrow \\mathbb{R}^n$, the velocity satisfies\n$\\|\\dot x_t\\|_2 < c \\quad \\text{for all } t \\in [0, T)$.\nProof. Let $X(t) := \\frac{1}{2} \\|v^{\\text{lab}}\\|^2$. By Lemma A.2, we have\n$\\frac{dX(t)}{dt} = \\frac{2 \\gamma_t}{m^{\\text{lab}}} \\langle f^{\\text{local}}, v^{\\text{lab}} \\rangle \\left(1 - \\frac{\\gamma_t^2 \\|v^{\\text{lab}}\\|^2}{c^2}\\right)$.\nObserve that the factor $(1 - \\|v^{\\text{lab}}\\|^2/c^2)$ becomes negative if ever $\\|v^{\\text{lab}}\\|^2 > c$, and it is zero when $\\|v^{\\text{lab}}\\|^2 = c$. Thus, if the velocity norm were to exceed $c$ at some time, the derivative of $X(t)$ at that moment would be negative, forcing $X(t)$ (i.e., $\\|v^{\\text{lab}}\\|^2$) to decrease rather than increase. In particular, once $\\|v^{\\text{lab}}\\|^2$ reaches $c^2$, it cannot increase further.\nConsequently, for all $t \\in [0, T)$ we must have $\\|v^{\\text{lab}}\\|^2 < c$, which proves the speed limit. Equivalently, since $\\dot x_t = v^{\\text{lab}}$ in our notation, we conclude\n$\\|\\dot x_t\\|_2 < c, \\quad \\forall t \\in [0, T)$.\nThus, we complete the proof."}, {"title": "A.3 ForM with TrigFlow", "content": "We restate the Theorem 4.4 and provide its proof.\nTheorem A.4 (ForM with TrigFlow, formal version of Theorem 4.4). We let $m = 1$ for simplicity in ForM. Giving a the interpolation $x_t = \\alpha_t x_1 + \\sigma_t x_0$, where $\\alpha_T = 1$, $\\alpha_1 = 0$, $\\sigma_T = 0$, $\\sigma_0 = 1$. We let $F_t(x_t)$ denote a vector map of force, a trainable neuron network parameterized with $\\theta$. We select the $\\alpha_t$ and $\\sigma_t$ identical with TrigFlow [LS24], where $\\alpha_t = \\sin(t)$ and $\\sigma_t = \\cos(t)$, $T = \\frac{\\pi}{2}$. Then, force interpolation could be simplified to\n$f_t(x_t) = \\frac{(\\cos(t)x_1 - \\sin(t)x_0) \\cdot (-\\sin(t)x_1 - \\cos(t)x_0)}{c^2 - (\\cos(t)x_1 - \\sin(t)x_0)^2} (\\cos(t)x_1 - \\sin(t)x_0)).$\nProof. We can show that\n$f_t(x_t) = \\gamma_t \\dot x_t + v_i \\frac{\\langle x_t, \\dot x_t \\rangle}{c^2} x_t$\n$ = (1 - \\frac{(\\cos(t)x_1 - \\sin(t)x_0)^2}{c^2})(-\\sin(t)x_1 - \\cos(t)x_0) +\n\\frac{(\\sin(t)x_1 - \\cos(t)x_0) \\langle x_t, x_t \\rangle}{c^2} x_t$\n$\\frac{d}{dt} [cos(t)x_1 - sin(t)x0)]$"}, {"title": "A.4 Relativistic Force Property", "content": "In this subsection, we restate Lemma 3.4, and show its proof.\nLemma A.5 (Equivalent Form of Relativistic Force, formal version of Lemma 3.4). Let $p^{\\text{lab}}$ be the momentum defined in Eq. (2), $\\gamma_t$ be the Lorentz factor at lab time $t$ defined in Definition 3.1, $\\tau$ denotes the proper time, $v^{\\text{lab}} = \\dot x_t$ denotes the velocity, $a^{\\text{lab}} = \\ddot x_t$ denotes the acceleration. The relativistic force, defined as the time derivative of the momentum in the lab frame, can be written as\n$f^{\\text{local}} = m^{\\text{lab}} \\left(\\gamma_t \\dot v^{\\text{lab}} + \\frac{\\gamma_t^3 \\langle v^{\\text{lab}}, \\dot v^{\\text{lab}} \\rangle}{c^2} v^{\\text{lab}}\\right)$.\nProof. We can show that\n$f^{\\text{local}} = \\frac{dp^{\\text{lab}"}]}, {}]