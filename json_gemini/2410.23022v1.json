{"title": "ONLINE INTRINSIC REWARDS FOR DECISION MAKING AGENTS FROM LARGE LANGUAGE MODEL FEEDBACK", "authors": ["Qinqing Zheng", "Mikael Henaff", "Amy Zhang", "Aditya Grover", "Brandon Amos"], "abstract": "Automatically synthesizing dense rewards from natural language descriptions is a promising paradigm in reinforcement learning (RL), with applications to sparse reward problems, open-ended exploration, and hierarchical skill design. Recent works have made promising steps by exploiting the prior knowledge of large language models (LLMs). However, these approaches suffer from important limitations: they are either not scalable to problems requiring billions of environment samples; or are limited to reward functions expressible by compact code, which may require source code and have difficulty capturing nuanced semantics; or require a diverse offline dataset, which may not exist or be impossible to collect. In this work, we address these limitations through a combination of algorithmic and systems-level contributions. We propose ONI, a distributed architecture that simultaneously learns an RL policy and an intrinsic reward function using LLM feedback. Our approach annotates the agent's collected experience via an asynchronous LLM server, which is then distilled into an intrinsic reward model. We explore a range of algorithmic choices for reward modeling with varying complexity, including hashing, classification, and ranking models. By studying their relative tradeoffs, we shed light on questions regarding intrinsic reward design for sparse reward problems. Our approach achieves state-of-the-art performance across a range of challenging, sparse reward tasks from the NetHack Learning Environment in a simple unified process, solely using the agent's gathered experience, without requiring external datasets nor source code.", "sections": [{"title": "1 INTRODUCTION", "content": "Reward functions are central to reinforcement learning (RL), and are often assumed to be given as part of the problem definition (Sutton & Barto, 2018). These functions are written to describe the task at hand, and often involve tradeoffs between ease of task definition and ease of policy optimization. For example, assigning a reward of +1 for solving the task and 0 otherwise is simple to define and accurately reflects the task goal, but is difficult to optimize due to providing zero gradients almost everywhere.\nThese difficulties have motivated the use of intrinsic rewards to aid policy optimization (Randlov & Alstr\u00f8m, 1998; Ng et al., 1999; Sorg et al., 2010; Singh et al., 2010). The reward designer can include additional reward shaping terms to create a denser learning signal, which can reflect task progress or guide the agent towards intermediate goals. However, designing intrinsic rewards can be remarkably challenging (Booth et al., 2023; Ibrahim et al., 2024) and places increased demands on human experts to provide task-specific knowledge.\nRecently, several works have been proposed to leverage the vast prior knowledge encoded in large language models (LLMs) to automate the reward design process, based on a task description in natural language. They can be broadly categorized into two families:\n1. Generating the reward function's code by LLM. A number of methods have been proposed to automatically generate executable code that computes the reward directly (Ma et al., 2023; Xie et al., 2023; Yu et al., 2023; Li et al., 2024). While they have demonstrated success in complex"}, {"title": "2 BACKGROUND", "content": "We consider a partially observed Markov decision process (POMDP) setting where the problem is defined by M = (S, A, O, po, P, O, r, \u03b3). At each episode, an initial state so \u2208 S is sampled from the initial state distribution po. At each time step t, the agents observes ot \u2208 O which is computed by the emission function O(st), and takes an action at \u2208 A. This action causes the environment to transition to a new state, st+1 ~ p(st, at). A new observation Ot+1 and a reward rt+1 = r(Ot+1) is given to the agent, and the process continues. The goal of the agent is to learn a policy \u3160 : 0 \u2192 \u0394(A) which maximizes the expected return \u0395\u03c0 [\u2211tytrt]. In this work, we additionally assume each observation of includes a textual caption ct, which could be empty. For observation spaces without textual captions, ct could in principle be provided by a captioning model as well.\nIn many situations, the extrinsic environment reward r is sparse, and the resulting return objective is challenging to optimize. We therefore consider methods which make use of an auxiliary intrinsic"}, {"title": "3 ONLINE INTRINSIC REWARDS", "content": "reward rint and define a composite surrogate reward:\n$r(ot) = r(ot) + \\beta \\cdot rint (ot)$.\n(1)\nA key research question is how to define or learn the intrinsic reward rint. A first option is to manually define rint based on task-specific goals, for example a measure of the distance between the agent's current state and the goal state. However, handcrafting the intrinsic reward function can require significant domain knowledge and must be redone for each new task. A second option is to define rint to measure some notion of observation novelty, which encourages the agent to systematically explore the environment. This can work well in smaller environments, but fails in ones that cannot be exhaustively explored in a tractable amount of time. A third class of methods, which we focus on in this work, leverage LLMs to automatically synthesize rint to reflect prior knowledge about the task. We discuss all three classes of methods in Section 4."}, {"title": "3.1 SYSTEM DESIGN: DISTRIBUTED PPO WITH LLM ANNOTATIONS", "content": "This section outlines the system we have built to learn online intrinsic rewards alongside the policy optimization. The engineering and design here is an important piece of our research, as the rest of our experimental studies are conducted within this system and influenced by the throughput of its interacting components.\nOur core system illustrated in Figure 3.1 is built on top of the Sample Factory library v1.0 (Petrenko et al., 2020) and their asynchronous variant of proximal policy optimization (Schulman et al., 2017), referred to as APPO. APPO operates on a single machine and concurrently runs many environment instances while asynchronously updating policy and value estimates with the usual PPO rules, and adequately handles policy staleness and data transfers between these components. Concretely for NetHack, by running 480 environment instances APPO collects approximately 32k environment interactions per second on a Tesla A100-80GB GPU with 48 CPUs.\nFor online reward learning via LLM annotations in this system\u00b9, we added 1) an LLM server hosted on a separate node, 2) an asynchronously running process that passes observation captions to the LLM server via HTTP request, 3) a hash table that stores the captions and corresponding LLM an-notations, and 4) and learning code that dynamically updates a reward model that learns on these annotations. Without being asynchronous, these components have the potential to block the concur-rent execution and can reduce the throughput of the overall system, because calling into an LLM and updating the reward model are time-consuming. We added them in a way that retains most of the throughput (!), approximately 80-95% of the original: the average throughput is 30k environment"}, {"title": "3.2 INTRINSIC REWARD FUNCTIONS", "content": "ONI offers flexible algorithmic choices for querying an LLM and distilling its feedback. In this work, we consider the following three methods.\nRetrieval The first and simplest approach we consider is based on binary labeling and retrieval. The LLM is asked to assign a binary label yi \u2208 {0, 1} indicating whether a caption ci is \"helpful\u201d or \"unhelpful\" for making progress on the task. The learner worker maintains a hash table H to store labeled pairs (ci, Yi), managed by the feedback processing thread. In the training thread, each time the RL agent receives an observation of with caption ct, we check if ct \u2208 H and the intrinsic reward is defined as:\n$rint (ot) = \\begin{cases}\nSH(ct) & \\text{if } cq \\in \\textrm{H} \\\\\n10 & \\text{if } ct \\notin \\textrm{H}\n\\end{cases}$\n(2)\nIf ct is unlabeled, it is placed into a last-in-first-out (LIFO) queue Q managed by the LLM annotation process, and then sent to the LLM server. The LLM continuously processes elements ci from Q and returns their labels y\u012b to the data processing thread of the learner worker, where the pairs (ci, Yi) are added into H. As described in Section 3.1, the training thread and the feedback processing thread run asynchronously. Therefore, editing the hash table does not slow down policy training. This retrieval-based approach does not generalize to observations with unlabeled captions. However, the resulting intrinsic reward is simple and hyperparameter-free, and may work well when the set of captions belongs to a relatively small set.\nClassification The second approach we consider is based on binary labeling together with training a classification model. Similarly to above, we label observation captions ci with labels yi \u2208 {0,1} indicating whether they are helpful or unhelpful via LLM. We simultaneously train a binary clas-sification model to predict yi from or. More precisely, we model P(y = 1|0) by rint : O \u2192 [0, 1], which is then used to compute the binary intrinsic reward by thresholding it at 7:\n$rint (ot) = [[rint (ot) > \u03b7]$,\n(3)\nwhere II is the indicator function. We study the impact of \u03b7 in Section 5.2. Unlike the previous approach, this method has potential to generalize to observations whose captions are similar, but not identical, to the captions labeled by the LLM. However, like the previous approach, it will assign a same reward to observations which are slightly positive (such as finding a few gold pieces in NetHack) and very positive (finding hundreds of gold pieces or a rare artifact).\nRanking The third approach we consider is based on ranking observations via pairwise classifi-cation, which is the approach taken by Motif. Here, pairs of observations (01,02) are sent to the LLM, which returns a preference label y \u2208 {1,2, \u00d8} indicating whether 01 or 02 is more desirable for accomplishing the task, or if they are equivalent. A reward model rint int : O \u2192 R is trained by minimizing the negative log-likelihood :\n$-E(01,02,y)~H [(I[y = 1] + I[y = 0]) log P\u2084(01 > 02) + (I[y = 2] + +I[y = 0]) log Po(01~02)]$\n(4)\nwhere we use average log-likelihood when y = \u00d8\u00b3 and use the Bradley-Terry model (Bradley & Terry, 1952) P(01 > 02) = 1-P\u00a2(01 <02) = exp (rint (01))/[exp (rint (01)) + exp (rint (02))].\nMotif computes the mean \u00b5\u00d0, standard deviation 5D, and a fixed quantile vo of rint over the offline dataset of annotations D. During RL training, it normalizes and thresholds ript to give the reward\n$rimotif(t) = [[(rint (ot) \u2013 HD)/OD > VD] \\cdot (ript (ot) \u2013 \u00b5D)/\u03c3\u03c0$\n(5)\nFor ON I-ranking, applying these steps directly is not possible since the annotation dataset is contin-uously changing. Therefore, we replace (\u00b5\u00d0, OD) by a running mean and standard deviation (\u03bc, \u03c3) computed over the agent's experience, and replace the quantile VD by a quantile of the standard normal distribution."}, {"title": "4 RELATED WORK", "content": "Exploration Based on Novelty Bonuses Learning from sparse or otherwise difficult-to-optimize reward functions is a long-standing problem in reinforcement learning. There is a large body of work which defines intrinsic rewards based on novelty bonuses (Schmidhuber, 1991; Kearns & Singh, 2002; Brafman & Tennenholtz, 2002; Stadie et al., 2015; Bellemare et al., 2016; Pathak et al., 2017; Burda et al., 2019; Shyam et al., 2019; Raileanu & Rockt\u00e4schel, 2020; Ecoffet et al., 2019; Agarwal et al., 2020; Zhang et al., 2021; Henaff et al., 2022; Lu et al., 2024). These methods tend to make minimal assumptions about the task at hand, operate online without requiring external data, and sometimes come with theoretical guarantees. However, since they are fundamentally tabula-rasa, they must rediscover much of the structure in the task that might already be encoded as prior knowledge in an LLM. Therefore, they tend to have difficulty exploring environments of very high complexity, such as NetHack, in a tractable amount of time (Klissarov et al., 2023).\nLLM-aided Reward Design In addition to Motif (Klissarov et al., 2023), several works have sought to leverage the prior knowledge encoded in LLMs to produce intrinsic rewards. Eureka (Ma et al., 2023), Auto-MC (Li et al., 2024), L2R (Yu et al., 2023) and Text2Reward (Xie et al., 2023) all use LLMs to generate executable code which computes intrinsic rewards from the under-lying environment state, conditioned on a task description. The generated reward function code is then iteratively improved based on aggregate statistics from agents trained with the current reward. However, a disadvantage with intrinsic rewards represented as code is that they require access to an interpretable underlying state representation, and it is unclear how to leverage non-numerical fea-tures such as those provided by unstructured language captions. The works of Kwon et al. (2023a);\nChu et al. (2023) also successfully used LLMs conditioned on task descriptions to directly generate binary rewards in an online manner, and did not train a reward model. This was possible due to eval-uating on environments and tasks which could be solved with a relatively small number of observa-tions, whereas we consider complex open-ended environments with billions of observations, where labeling them all with an LLM would be computationally infeasible. Also of note is the work of Wu et al. (2024), which additionally conditioned LLMs on user manuals to define the intrinsic reward.\nGoal-conditioned Reward Design A different approach to reward function design is to define re-wards as the distance between the agent's current state and the goal. For example, one line of work learns a state embedding in a self-supervised manner which converts geodesic distances in the original space to Euclidean distances in feature space (Wu et al., 2019; Wang et al., 2021; Gomez\net al., 2024). Another line of work of (Fan et al., 2022; Rocamonde et al., 2023; Adeniji et al.,\n2023; Kim et al., 2024) leverages pretrained image and text encoders, and defines rewards to be some measure of similarity between embeddings of visual observations and embeddings of textual task descriptions. Using a question generation and answering system, Carta et al. (2022) extract auxiliary objectives from the goal description and construct intrinsic rewards. An interesting combi-nation of goal-conditioned and LLM-aided reward design is the work of Du et al. (2023), who used an LLM to generate candidate goals which might be relevant to the environment at hand, and then used distance in embedding space to define the reward.\nLLM for RL Broadly Another way of leveraging the prior knowledge encoded in LLMs for de-cision making is to use the LLM directly as a policy. This approach has been successfully used in robotics (Ahn et al., 2022; Driess et al., 2023), as well as open-ended exploration in MineCraft (Wang et al., 2024). Both settings require the LLM to operate at a higher level of abstraction, by having it call upon a set of semantically grounded skills which handle the low-level sensorimotor activity. These are in turn produced by imitation learning on expert trajectories or hardcoded APIs. Jeurissen et al. (2024) prompt the LLM to choose a predefined skill to play NetHack. The prompts are constructed to represent past events, current observation, and the task description and hardcoded available skills are also included. More references on LLMs for decision making can be found in the survey paper of Cao et al. (2024)."}, {"title": "5 EXPERIMENTS", "content": "Environment We use the NetHack Learning Environment (NLE) (K\u00fcttler et al., 2020) as our experimental testbed, since it is one of the most challenging open-ended, long horizon and sparse reward environments available, and was also used as the main environment in the prior work we"}, {"title": "5.1 MAIN RESULTS", "content": "Task Performance We report the average performance and 95% confidence intervals computed via standard errors over 5 seeds in Figure 5.1a. The extrinsic reward agent performs reasonably well"}, {"title": "5.2 ABLATION STUDY", "content": "ONI-classification: the impact of the classification threshold As described in Section 3.2, ONI-classification predicts binary rewards by modeling P(yt = 1|ot) and then thresholding"}, {"title": "6 CONCLUSION", "content": "We have introduced ONI, a distributed online intrinsic reward and agent learning system. We showed that we are able to match the state of the art across a range of challenging sparse reward tasks from the NetHack Learning Environment, while removing the need for a large pre-collected dataset or auxiliary dense reward function required by previous work. We explored three different instan-tiations of our system of varying levels of complexity and generality, and study their tradeoffs. Our work paves the way for intrinsic reward methods which can learn purely from agent experience, are not constrained by external dataset size or quality, and can leverage high-performance RL training."}, {"title": "A EXPERIMENTAL DETAILS", "content": "Our architectures largely follow those used in Klissarov et al. (2023).\nOur policy network uses the Chaotic Dwarven GPT5 architecture originally introduced in Myf-fili (2021). This architecture combines convolutional layers processing the top-down visible map centered at the agent with fully-connected layers processing messages and bottom-line statistics including hit points, experience, hunger level and the like. The convolutional encoder has 3 con-volutional layers with 32, 64, 128 feature maps respectively, interleaved with exponential linear unit (ELU) non-linearities (Clevert et al., 2016). Messages and bottom-line statistics are each processed with 2-layer MLPs with 128 hidden units each. All embeddings are combined, passed through a single-layer MLP with 512 hidden units, and then passed to a recurrent GRU module (Cho et al., 2014) with 512 hidden units. Finally, this hidden representation feeds into linear critic and actor heads.\nOur reward model The reward model of ONI-ranking is based on the encoder from K\u00fcttler et al. (2020) that processes both state representation and messages. Messages are processed by a 5-layer character-level CNN (Zhang et al., 2015) with 64 feature maps at each layer. The first, second and last layers are interleaved with max-pooling layers with kernel size and stride 3. The output is then passed through a 3-layer MLP with 128, 256, 512 hidden units at each layer respectively, and ReLU non-linearities, followed by a scalar output. The reward model of ONI-classification only process messages, using the same architecture described above."}, {"title": "A.2 HYPERPARAMETERS", "content": "Following Klissarov et al. (2023), we scale the environment reward by 0.1 for the Score task and by 10 for the other sparse reward tasks, and use normalized intrinsic reward\n$rintmalized (Ot) = rint (ot)/N(ct)^z$,\nwhere N(ct) is the number of times the caption ct has been found in one episode. For all our experiments, we use z = 3.0.\nFor ONI-classification and ONI-ranking, we train the reward model using the Adam opti-mizer (Kingma, 2014) with batch size 256. ONI-classification is trained with learning rate 0.0001, classification threshold \u03b7 = 0.7, \u03b2 = 0.1 for the Score task and \u03b2 = 0.4 for the others.\nONI-ranking is trained with 0.00001, \u03b2 = 0.05 and vn = 1.96 (97.5-th quantile of the standard normal distribution). ONI-retrieval does not train a reward model, and we use \u03b2 0.1 for the Score task and \u03b2 = 0.5 for the others."}, {"title": "B PROMPTS", "content": "Here we include all the LLM prompts we use. The system prompt and the prompt for ONI-ranking are identical to the ones used in Klissarov et al. (2023). We adapted these to produce labels for single observations for ONI-retrieval and ONI-classification.\nSystem Prompt (used by all methods)\nYou are a helpful and honest judge of good gameplaying and progress in the NetHack game. Always answer as helpfully as possible, while being truthful.\nIf you don't know the answer to a question, please don't share false information.\nPrompt for ONI-retrieval and ONI-classification\nContext: I am playing NetHack and I will present you a message from the gameplay.\nInstruction: First, tell me about your knowledge of NetHack. Mention the goal of NetHack.\nThen, write an analysis describing the semantics of each message using your knowledge of NetHack. Finally, respond by assigning a score to the message, which measures whether the status represented by the message is helpful or unhelpful to make some progress towards the goal.\nHelpful here does not mean whether it is informative to the player to make decisions or not, but whether it is helpful for the agent to solve the game.\nI prefer agents that maximize the score in the game, for instance by killing monsters, collecting gold or going down the stairs in the dungeon\nFor extremely helpful messages, assign a label FOO. For unhelpful, neutral, and lukewarm messages, assign a label BAR.\nExample 1: \"You kill the goblin! Welcome to experience level 4\" indicates that the player has killed a goblin and gained experience. This led to a level up and increased abilities.\nGaining experience and leveling up are both extremely helpful and this message should be labelled as FOO.\nExample 2: \"You hear someone counting money.\" indicates that the player is close to a source of gold, as the sound of counting money suggests a nearby NPC or monster with treasure.\nThis is also extremely helpful and should be labelled as FOO.\nExample 3: \"5 gold pieces.\" indicates you collected 5 gold coins. This is extremely helpful should be labelled as FOO.\nExample 4: \"That door is closed.\" is neutral and should be labelled as BAR.\nPlease rate this message: {{\"{{}}\"}}\nConstraint: Please format your response strictly in this format without additional explanation:\n[your knowledge of NetHack) [your one-sentence analysis of the message)  [FOO/BAR]"}, {"title": "Prompt for ONI-ranking", "content": "I will present you with two short gameplay descriptions.\nFirst, tell me about your knowledge of NetHack. Mention the goal of NetHack. {}\nThen, write an analysis describing the semantics of each description strictly using information from the descriptions (which may be empty) and your knowledge of NetHack.\nProvide a comparative analysis based on first princicples.\nFinally, respond by explicitly declaring which one is the most likely to make some progress towards the goal, writing either (\"best_description\": 1), (\"best_description\": 2). You could also say (\"best_description\": None ).\n{{\n\"description_1\":\n\"{}\"\n}}\n{{\n\"description_2\":\n\"{}\"\n}}"}, {"title": "C ADDITIONAL RESULTS", "content": "C.1 LLAMA-3.1-70B-INSTRUCT VS LLAMA-3.1-8B-INSTRUCT\nIn Figure C.1, we compare the performance of Motif using two different sized LLMs on Score and Oracle (LLaMA-3.1-8B-Instruct and LLaMA-3.1-70B-Instruct). Interestingly, we do not observe a significant difference between the two. This is in contrast to the previous work of (Klissarov et al., 2023), who found a significant difference between using LLaMA-2-70B-chat and LLaMA-2-7B-chat. This suggest that the smaller 8B model is sufficient for evaluating messages on these tasks, hence we use it in our experiments.\nC.2 PERFORMANCE VS. LLM THROUGHPUT: TESLA V100 vs TESLA A100 GPU\nC.3 IMPACT OF INTRINSIC REWARD COEFFICIENT B"}]}