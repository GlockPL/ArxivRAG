{"title": "CASET: Complexity Analysis using Simple Execution Traces for CS* submissions", "authors": ["Aaryen Mehta", "Gagan Aryan"], "abstract": "The most common method to auto-grade a student's submission in\na CS1 or a CS2 course is to run it against a pre-defined test suite\nand compare the results against reference results. However, this\ntechnique cannot be used if the correctness of the solution goes\nbeyond simple output, such as the algorithm used to obtain the\nresult. There is no convenient method for the graders to identify\nthe kind of algorithm used in solving a problem. They must read\nthe source code and understand the algorithm implemented and its\nfeatures, which makes the process tedious.\nWe propose CASET (Complexity Analysis using Simple Exec\u0438-\ntion Traces), a novel tool to analyze the time complexity of algo-\nrithms using dynamic traces and unsupervised machine learning.\nCASET makes it convenient for tutors to classify the submissions\nfor a program into time complexity baskets. Thus, tutors can iden-\ntify the algorithms used by the submissions without necessarily\ngoing through the code written by the students. CASET's analysis\ncan be used to improve grading and provide detailed feedback for\nsubmissions that try to match the results without a proper algo-\nrithm, for example, hard-coding a binary result, pattern-matching\nthe visible or common inputs. We show the effectiveness of CASET\nby computing the time complexity of many classes of algorithms\nlike sorting, searching and those using dynamic programming par-\nadigm.", "sections": [{"title": "1 INTRODUCTION", "content": "In CS1/CS2, the correctness of the solutions of to programming\nassignments may go beyond producing correct outputs for a set of\ninputs. In many cases, it is also required to use a specific algorithm\nto solve a problem, for example a binary search instead of a linear\nsearch. In such cases, the graders have to manually go through\nthe submitted code to identify the actual algorithm used in the\nsubmission. Apart from being a tedious process, this method is\nalso prone to mistakes by the evaluator. We propose the CASET\nframework to make this evaluation process smooth and effective."}, {"title": "2 RELATED WORK", "content": "There have been many articles published that focus on improving\nthe overall teaching and learning experience of CS* courses. The\nASSYST system uses a simple form of tracing for counting exe-\ncution steps to gather performance measurements [7]. This was\nimplemented in an introductory course in which Ada was used\nas a teaching language. The number of evaluations is calculated\nlater used for complexity analysis. There has been a lot of work\ndone in the area of providing automated feedback for programming\nassignments. Prutor [3] is a cloud-based state-of-the-art tutoring\nplatform that helps in providing personalized feedback to individ-\nual students. Bob et al. [4] proposed a heat maps-based approach\nto provide feedback to visually guide student attention to parts\nof the code that is likely to contain the issue with the submission\nwithout giving so much direction effectively the whole answer is\ngiven. Sumit et al. [5] proposed a lightweight programming lan-\nguage extension that allows an instructor to define an algorithmic\nstrategy by specifying specific values that should occur during the"}, {"title": "3 METHODOLOGY", "content": "The submissions for an assignment are run against a pre-defined\ntest suite and compared against reference results. After this, we pass\nthe submission through CASET. CASET filters out any programs\nnot of the required time complexity. This is similar to unit tests\nexecuted within a continuous integration environment. Figure 1\nshows the grading pipeline setup for CASET.\nThe primary requirement for CASET is the presence of an in-\nstumentation framework for the generation of dynamic traces. We\nemployed Valgrind to demonstrate that time complexity analysis\nof programs is indeed possible with the presence of dynamic traces.\nSeveral experiments were undertaken to test this hypothesis and\nhas been discussed in detail in the Results section.\nOnce the traces are generated, we plot them against the input\nlength of the programs. Scipy's scipy.optimize.curve_fit is used\nto estimate the coefficients of the generalized curve equation and\nplot the curve that fits the best [11]. Pre-decided curve equations\nwere fit to the traces, and the program was classified to the basket\ncorresponding to which the curve gives the least mean squared\nerror. The curve equations are pre-decided based on the possible\ntime complexities of the algorithm being evaluated. For instance,\nwe will fit the curves $ax + b (O(n))$ and $log(ax + b) (O(log(n))$ for\nsearching algorithms because these are the only time complexity\nbaskets we expect the algorithm to lie in."}, {"title": "4 USE CASES", "content": "Let us consider different cases of the performance of programs on a\ntest suite and analyze how CASET framework would behave when\na program is passed through it."}, {"title": "4.1 Case 1 - All the test cases produce the\ncorrect result", "content": "When a program produces the same results as the reference results\nfor a test suite, CASET will check if the program is implemented\nin the required time complexity by classifying it to a complexity\nbasket. If the classified basket does not match the time complexity\nexpected from the submission, then the submission would be graded\nas a wrong solution."}, {"title": "4.2 Case 2 - Few of the test cases produce the\ncorrect result", "content": "In this case, even though a program produces the correct result for\na few test cases, it is possible that the implemented algorithm is\nnot of the required time complexity. So, before assigning a score\nto the submission, when the program is passed through CASET,\nit computes if the algorithm is of the required time complexity. If\nit isn't, the submission would be treated as an incorrect solution.\nCASET can also detect hard-coded programs designed to pass a\nfew visible test cases. Hardcoded programs are generally of linear\ntime complexity and wouldn't satisfy the required time complexity\n(unless the required complexity is linear). This can be detected\neasily with CASET.\nHowever, it is to be noted that it wouldn't be possible to compute\nthe algorithm's time complexity if it contains runtime, segmentation-\nfault, or any other memory errors. Because currently, CASET uses\nValgrind to generate traces, and trace generation from Valgrind is\nnot possible if the program contains memory-related errors. Other\ninstrumentation frameworks that can better handle memory errors\nshould be considered to handle this."}, {"title": "4.3 Case 3 - None of the test cases produce the\ncorrect result", "content": "In this case the program would be graded as incorrect even before\npassing through CASET"}, {"title": "5 DATA ANALYSIS", "content": "Valgrind traces can accurately estimate the time complexity of most\nof the sorting and searching algorithms. We were also able to fit\ncurves on a few dynamic programming algorithms and evaluate\ntheir time complexity. The mean squared errors upon plotting dif-\nferent curves with algorithms can be found in Table 1.\nIt can be seen in Table 1 that the algorithms that are of linear\ntime fit the best since their plots are a simple straight line(linear\nsearch and dynamic programming fibonacci algorithm), and the\nbest fit can be easily obtained. Even though the other curves seem\nto fit well in the graphs below, there is a substantial error in the\nactual curve, and the existing scatter plot, which is not evident due\nto scale. For instance, even though both $(ax + b) (log(cx + d))$ and\n$ax\u00b2 + bx + c$ seem to fit almost the same in the recursive merge sort\nplots(plots (e) and (f) in Figure 2), their mean squared errors differ\nby a factor of 10."}, {"title": "6 CHALLENGES FACED", "content": "Generating dynamic traces of the programs is a computationally\nexpensive task. It was not possible to generate traces for programs\neven when the size of the array was greater than ten on a machine\nwith an Intel i7 processor and 16GB RAM. So we had to make use\nof cloud resources. We used Amazon Web Services EC2 instances\nto generate traces for programs of input lengths greater than 10.\nOnce the generation of full-length dynamic traces of Valgrind was\nset up, we began to post-process the dynamic traces and fit them\ninto relevant time complexity baskets. The number of inversions in\nthe input array was also taken as a parameter since we only dealt"}, {"title": "7 CONCLUSION", "content": "In this paper, we have proposed a novel method to determine the\nasymptotic time complexity of a computer program. The results\nof this approach on various algorithms show the potential of this\napproach. However, currently, it is not possible to use them in a CS*\nlab environment because the trace generation in Valgrind is highly\ncomputationally expensive. Other instrumentation frameworks like\nDr. Memory [1] or gperf tools or one built solely for the cause of\nCASET may better fit the requirement of analyzing time complexity.\nHowever, Valgrind traces demonstrate that the time complexity\nanalysis with this setup is indeed possible. We believe that the\nsame philosophy can be generalized to other algorithms, including\nthose involving data structures like hash, heaps, and graphs. Apart"}]}