{"title": "A FRAMEWORK FOR COLLABORATING A LARGE LANGUAGE MODEL TOOL IN BRAINSTORMING FOR TRIGGERING CREATIVE THOUGHTS", "authors": ["Hung-Fu Chang", "Tong Li"], "abstract": "Creativity involves not only generating new ideas from scratch but also redefining existing concepts and synthesizing previous insights. Among various techniques developed to foster creative thinking, brainstorming is widely used. With recent advancements in Large Language Models (LLMs), tools like ChatGPT have significantly impacted various fields by using prompts to facilitate complex tasks. While current research primarily focuses on generating accurate responses, there is a need to explore how prompt engineering can enhance creativity, particularly in brainstorming. Therefore, this study addresses this gap by proposing a framework called GPS, which employs goals, prompts, and strategies to guide designers to systematically work with an LLM tool for improving the creativity of ideas generated during brainstorming. Additionally, we adapted the Torrance Tests of Creative Thinking (TTCT) for measuring the creativity of the ideas generated by AI. Our framework, tested through a design example and a case study, demonstrates its effectiveness in stimulating creativity and its seamless LLM tool integration into design practices. The results indicate that our framework can benefit brainstorming sessions with LLM tools, enhancing both the creativity and usefulness of generated ideas.", "sections": [{"title": "1 Introduction", "content": "Creativity isn't just about creating new ideas from scratch; it also involves redefining existing thoughts and synthesizing previous ideas (Gafour et al., 2020). It is a process that includes becoming sensitive to problems, deficiencies, information shortages, missing elements, identifying challenges, searching for solutions, making estimates and hypotheses, modifying these hypotheses in response to the identified shortcomings, and trying one of these solutions (Torrance, 1974). These elements of the process can be formulated into techniques that encourage creative idea generation or creative thinking. Many techniques have been recommended by previous studies (Herrmann et al. 2018). Research has shown that using those techniques results in more innovative ideas than not using any technique at all in practice (Chulvi et al., 2012; Dumas et al., 2015).\nOne of the frequently used techniques is Brainstorming (Diehl & Stroebe, 1991). It is an unstructured technique used in a group setting to generate creative ideas for a specific problem (Rossiter et al. 1994; Besant 2016). Brainstorming stimulates creativity and out-of-the-box thinking, which can lead to the exploration of unconventional solutions and the discovery of novel approaches to tackle a complex problem. It is also a versatile tool utilized in various fields, such as business and education. However, although barnstorming is unstructured in nature, it is important to have clear principles or tools to improve the group's working efficiency and increase the creativity and quality of the idea generated (Rossiter et al. 1994)."}, {"title": "2 Literature Review", "content": "Recent advances in Artificial Intelligence, particularly in the field of Natural Language Processing, have significantly impacted our lives through the development of Large Language Models (LLMs). Tools like ChatGPT have demonstrated their ability to assist users with a wide range of complex tasks. Their ability extends beyond information extraction (Shah et al. 2024) to data analysis, media creation, and reasoning (Chew et al. 2023). Many studies have explored the use of LLM tools for various applications, such as identifying relevant papers (Paroiu et al. 2023), synthesizing literature reviews (Antu et al. 2023), writing proposals (G\u00f3mez-Rodr\u00edguez et al. 2023), and analyzing data (Shen et al. 2023). The capabilities of LLMs continue to expand, explicitly indicating that these tools can open up new possibilities for different types of tasks, including brainstorming.\nThe effectiveness of using LLM for brainstorming lies in the instruction or prompts drafted by the users (Liu et al., 2021). The typical way to communicate with an LLM tool is through a text-based input, called prompt, which is fed to a language model to guide its output. Users of the tool expect LLM to respond to their prompts that can exactly meet what they want in their mind. For example, a prompt \"What does CMU stand for university?\" would be more likely to return a response \u201cCentral Michigan University (CMU)\u201d instead of \"Carnegie Mellon University (CMU)\". The response meets the user's expectation if they only have a vague idea about a university in Michigan. As a result, to obtain accurate, relevant, and coherent responses from an LLM tool, prompt engineering should be employed. Prompt engineering is the process of designing and refining prompts to elicit desired high-quality responses from LLMs. By creating effective prompts, users can get useful responses from an LLM and save valuable time and resources. The effectiveness of the prompts in prompt engineering research is measured by how accurate the LLM tool's responses are. Accordingly, current studies on prompt engineering focus on improving prompts' ability to generate accurate responses (which are thinking out of the box). However, how to use prompt engineering to generate creative ideas is neglected. As suggested by creativity research, usefulness and novelty are two essential aspects of creativity (Runco & Jaeger, 2012). It is important to consider not only the usefulness or accuracy but novelty as well for idea generation. This is particularly true for brainstorming. Therefore, our study aims to bridge this gap in prompt engineering by proposing a framework that focuses on improving the novelty of the ideas generated for brainstorming.\nTo create this framework, we first reviewed literature and other resources to form a theoretical understanding of how creativity is stimulated in design sessions and how these outcomes can be measured and evaluated. For testing our framework, four creativity measurements, inspired by the Tests of Creative Thinking (TTCT), are developed for assessing the results produced by LLM tools. We also focus on defining basic elements and procedures of the framework, specifically how to systematically compose and use prompts. These prompting elements were derived from several key insights gained through our explorations of student's brainstorming exercises and an investigation on numerous online prompt examples (e.g., HuggingFace database) over the past semesters. We demonstrate the practical application of our proposed framework with a design example, showing how it aids users during the ideation stage and specifying which prompting elements should be used according to various scenarios. We examine the framework's effectiveness in triggering creativity through a case study, which suggests the potential success of our approach. The results indicate that our framework can be beneficial for brainstorming with an LLM tool and can be seamlessly integrated into design practices."}, {"title": "2.1 Prompt engineering", "content": ""}, {"title": "2.1.1 Prompting LLMs for Complex Reasoning Tasks", "content": "The quality of the output from an LLM may vary significantly depending on the quality of the prompt. Prompt engineering is the process of developing and refining a prompt to get the desired output. Polverini and Gregoricic (2023) have shown that the performance of GPT on conceptual physics tasks can be significantly improved by using prompt engineering techniques. Wei et al. (2022) suggested dividing a complex task into multiple intermediate steps and applying chain-of-thought prompting to improve the accuracy of LLMs' response. In the studies (Chen et al., 2023; Madaan et al., 2023; Paul et al., 2023; Kim et al., 2023), self-reflection approaches are used progressively to guide the model to inspect its current output and refine it. Human prompt engineers usually examine the failure cases produced by the current prompt closely, make hypotheses, and compose a new prompt."}, {"title": "2.1.2 Automatic Prompt Engineering with LLMs", "content": "To alleviate the intensive efforts of human prompt engineering, recent works explore automating this process by meta-prompting LLMs to paraphrase the prompt (Zhou et al, 2022) or refine the prompt by inspecting failure examples (Pryzant et al., 2023). Pryzant et al. proposed the Prompt Optimization with Textual Gradients which is a general-purpose and nonparametric algorithm for automatic prompt optimization to improve prompts. Zhou et al. were inspired by classical program synthesis and the human approach in prompt engineering. They treated the instruction as the \"program,\u201d optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function and developed the Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In the study (White et. al., 2023), a key contribution of their paper is the introduction of prompt patterns to document successful approaches for. They focus largely on engineering domain-independent prompt patterns and introduce a catalog of essential prompt patterns to solve problems ranging from production of visualizations and code artifacts to automation of output steps that help fact check outputs."}, {"title": "2.1.3 Domain Applications of Prompt Engineering", "content": "Research in various fields also indicates prompt engineering is an essential skill for learners. Willey et. al (2023) discussed how to apply the Prompt Development Life Cycle (PDLC) model to educate students to make effective prompts. Woo, Guo, and Susanto in 2023 explored English as a foreign language (EFL) students' prompt engineering pathways to writing using ChatGPT. The results suggest that prompt engineering is an important emergent skill for EFL students to improve their writing. Heston and Khun in 2023 discussed the necessity, challenges, and concerns of using prompt engineering techniques in medical education."}, {"title": "2.2 Design Thinking", "content": "Design thinking is a human-centered design strategy proposed by Stanford d. school in early 1990 to help solve design-related problems (Dym et al., 2005), particularly for those ill-defined problems, where alternative solutions may exist. Design thinking summarizes the design process into two five phases (Empathy, Interpret, Ideate, Prototype, and Test) and two stages (the exploratory stage and the development stage). Design thinking encourages the creativity of the designers and maximizes the possibilities of generating creative ideas. For instance, during the exploratory stage, \"Empathy, Interpret, and Ideate\" encourages designers to widen the problem exploratory space by interacting with the users, identifying potential design problems, and conceptualizing as many solutions as possible. At the development stage, rapid prototyping and testing are used to narrow the problem space by developing and iteratively improving the design solutions. Research has shown the effectiveness of design thinking in identifying new opportunities and improving the innovation of the design products (Garbuio et al., 2018; Kurtmollaiev et al., 2018)."}, {"title": "3 Framework", "content": "As indicated above, the design thinking process involves both exploratory stage and development stages. For those two stages, divergent thinking and convergent thinking are essential for designers. As suggested by Cross's study (Cross, 2021), design processes involve both divergent and convergent thinking, where designers divergently explore possible ideas initially and ultimately converge, culminating in a final stage of evaluation and detailing. A later widely adopted version of the divergence-convergence design model is the Double Diamond, which further illustrates how those two thinking styles were applied at different stages of the design process (Design Council, 2005).\nWithin the design process, thinking strategies play a crucial role in guiding the designer's exploration of ideas and search for solutions. Cross (2021) identified two key design strategies: random search and prefabrication. The random search strategy embodies a primarily divergent approach, fostering the generation of numerous alternative solutions and thorough exploration of the design space, as described by Shah et al. (2021). Conversely, the prefabrication strategy exemplifies a primarily convergent approach, focusing on refining and narrowing down options. These perspectives underscore the importance of strategy in the design process, suggesting that strategies should be considered essential components of any design framework.\nDrawing inspiration from previous studies on the essential elements of design approaches and frameworks, we have developed the Goal-Prompt-Strategy (GPS) framework, which integrates prompt engineering for the utilization of LLM) tools. The purpose of this framework is to help develop effective prompts when using Generative AI for brainstorming purposes, particularly during the design process. The GPS framework comprises three principal levels: Goal, Prompt, and Strategy (see Fig. 1). The following explains the details for each level:\n1. Goal\nWhen using this framework, designers need to consider the brainstorming purpose and determine which design phase best fits the current design situation. The designer's goal is determined through a thorough analysis of the needs for exploration, driving the designer's thinking direction. This decision results in two distinct phases: divergence and convergence (see Fig.1). If the goal is to expand the thinking solution space or discover more creative possible options, the divergence phase should be selected. Otherwise, if the aim is to find more accurate answers from existing outcomes, the convergence phase should be chosen. Determining a phase is crucial because the selected phase will subsequently impact the choice of appropriate strategies.\n2. Prompt\nIt is important to learn the fundamentals of prompt writing to draft effective prompts for LLMs. Understanding the essential components that should be included in a prompt may increase the probability of the generated responses aligning with their specific focus. Based on the insights we learned from previous prompt engineering templates and our experience, we identified and summarized the key elements in a prompt as follows:\nWho (Role, Audience): It is critical for an LLM tool to understand the roles it needs to assume when conducting a task or solving a problem. This will let the LLM tool generate responses from specific perspectives and based on certain expertise. For instance, a prompt might begin with, \"Assuming you are a software engineer...\"\nWhat (Context, Task, Problem, Scenario, Situation, Constraint, Objective): The task and role are related. Users must clarify the specific task assigned to the LLM tool. In this section, users should also define the problem to be solved, describe the usage scenario, and outline any possible constraints. In addition, the desired goal should be clearly articulated.\nWhere (Location, Environment): Describe the environment and location where the designed system will be used.\nSupporting or Additional Information: This aims to help ChatGPT understand the background of the design problem, including relevant information. Providing these context cues ensures that the LLM tool remains focused on the topic and produces answers that meet the users' needs.\nOutput Requirement or Expectation: To increase the relatedness of the information generated, it is necessary to provide details about the expectations and requirements for the output. The designers can think about the type of answers they want the LLM tool to generate and any formatting requirements. In the context of brainstorming, if designers are in the exploratory or divergent phase, the number of ideas and their novelty might be more likely preferred at this stage. Users can consider including phrases such as \u201cgenerate ideas as many as possible\" in the prompt.\nWe summarize those elements into a prompt template (see Fig. 2). To increase the chance of receiving useful responses, we strongly recommend incorporating as many elements as possible in the initial prompt. Our experiences indicate that the responses are significantly influenced or constrained by the first prompt provided to the LLM tool. Therefore, we recommend to include all relevant elements in your prompt.\n3. Strategy\nStrategies play a pivotal role in our framework as established in prior research for guiding design thinking. However, within our framework, strategies serve a different purpose. Instead of directing the designer's thinking, they shape how an LLM tool generates responses. We recognize that an LLM tool operates based on interactions between questions asked by the users and answers generated by an LLM tool. Research on thinking skills (e.g., creativity, decision-making, problem-solving, and critical thinking) highlights the importance of questioning as a method for fostering human creativity and solution generation (Wayne Allison et al., 1986; Penick, 1996; National Advisory Committee on Creativity and Cultural Education, 1999; Raz et al., 2023). We believe that various question-asking strategies or techniques may not only influence LLM responses but also the flow of the conversation. Therefore, strategies in the context of question-and-answer interactions are used to form proper and purposeful questions that guide LLM's thinking. By treating an LLM tool as a collaborative partner, strategies introduce various ways of questioning for stimulating innovative ideas and solutions; that is, they influence how initial prompts may be adjusted or refined to elicit more effective responses. The following lists nine strategies that are essential for creative prompt engineering.\n(a) Chain-of-Thoughts (Multi-steps)\nThis strategy is designed to augment the reasoning capabilities of LLMs through a process of iterative evaluation. Rather than directly eliciting an answer, the model is prompted to assess and reflect on its reasoning at each stage of problem-solving. This method involves two primary techniques: either prompting users to decompose multi-step problems into intermediate steps, thereby enabling the LLM to effectively address complex reasoning tasks that surpass the limitations of standard prompting techniques, or instructing the LLM itself to systematically break down intricate tasks into sequential, step-by-step responses (Alkhatib, 2019). Therefore, a series of questions will be asked to facilitate the model in navigating a logical progression of thought, ultimately yielding outputs that are more accurate and reliable.\n(b) Unconventional Role in Context\nVygotsky (2004) identified some fundamental ways how creativity is connected to reality. His second type of association asserts that the final product of creativity is influenced by the experiences of others. In other words, an individual's creativity is not confined to their personal experiences but can also be enriched by drawing from the experiences of others. Similarly, Galindo (2009) suggests that the creative process can emerge from a diverse mix of ideas, as diversity introduces various backgrounds, cultures, experiences, and perspectives. Receiving input from individuals with diverse backgrounds can stimulate creative thinking. Based on the ideas of Vygotsky and Galindo, we propose that using the LLM tool to assume various roles can introduce a range of experiences and perspectives, thereby fostering creativity of the users. To enhance the novelty of the solutions generated, this strategy suggests LLM to play \"novel\" or \"unconventional\u201d roles, who typically not the person builds, uses, observes, or evaluates the system. This strategy drives LLM to view the problem or design from different perspectives, ultimately enriching the creative process.\n(c) Flipped Interaction\nTypically, users actively drive conversations. In contrast, Flipped Interaction assigns an active role to the LLM tool in soliciting inquiries to gather necessary data for task execution. The Flipped Interaction strategy aims to reverse the conventional interaction flow between the user and the LLM. Instead of the user asking questions and the LLM providing answers, this strategy positions the LLM as the questioner. This allows the LLM to extract relevant information from the user to achieve a specific objective. This strategy offers several advantages. Firstly, it enables the LLM to tailor its inquiries precisely, ensuring that no critical details are overlooked. Secondly, it prompts the user to consider information they may not have initially provided, ultimately leading to more efficient and accurate outcomes (White et al, 2023).\n(d) Analogy: One-shot or Few-shots Strategy\nAn analogy is a statement that establishes a similarity between one object, individual, situation, or action and another, highlighting their comparable characteristics or relationships (Hender et al., 2001). \u03a4\u03bf increase the accuracy of the responses AI generated, sometimes sample examples are always provided to help AI understand the needs of the user. Depending on the number of analogies provided in the prompt, analogy-based strategy can be further divided into one-shot prompting and few-shots (or multi-shots) prompting:\ni. One-Shot Prompting: In one-shot prompting, a specific prompt or context is provided to the model, serving as the guiding information for generating the desired output. This method leverages the model's ability to comprehend and utilize context effectively. By observing the given example, the model infers the necessary structure, tone, and content, producing a coherent and contextually relevant response. This strategy is particularly suitable for tasks requiring a specific format or context, where a single example can effectively guide the output. For example, one can add a sample job description for a \"car engine engineer\" position and then instruct the LLM to write a similar one.\nii. Few-Shot Prompting: Unlike One-shot strategy that provides one example to LLM, the Few-shot method provides the model with a small number of examples to quickly adapt to new instances. It is useful for complex tasks requiring multiple examples to provide broader context or to handle more nuanced queries.\nStudies about creativity (Rhodes, 2010; Acomi et al., 2023) show that creative thinking involves the ability to combine or synthesize existing ideas, images, or expertise to think and work imaginatively. Simply saying that creative thinking is an imaginative process aimed at producing originality outcomes. Therefore, to promote brainstorming, we also encourage using imaginary examples in the prompt and this imagined case could be impossible for current design's environment and conditions.\n(e) The Alternative Approaches\nGafour (2020) emphasized the necessity of exploring alternatives as a critical step in fostering creativity, drawing on insights from previous studies. In addition, humans often suffer from cognitive biases that lead them to favor certain approaches. The motivation behind this strategy is to dissolve these cognitive biases by making users aware of different problem-solving methods (White et al., 2023). This methodology aims to broaden the user's perspective, ultimately enhancing their creative thinking abilities. When applied in prompts, this strategy can also enforce an LLM tool to generate alternative solutions.\nHence, the Alternative Approaches strategy encourages an LLM to present multiple perspectives or solutions rather than a single answer. Instead of merely providing the most frequently mentioned response, the LLM explores a range of options or viewpoints. This strategy ensures that users are not confined to familiar methods and prompts them to consider alternative approaches. By presenting these alternatives, users are compelled to think critically about their results and assess whether their chosen approach is the most effective way to achieve their goals. Additionally, solving a task using different approaches can educate users and introduce them to new concepts for future reference.\n(f) Emphasis\nTo emphasize the importance of certain parts and ensure an LLM tool takes them seriously, we can highlight those parts using emotional stimuli (Li et al., 2023). Incorporating emotional phrases can enhance the LLM's performance, responsibility, and truthfulness. By applying this technique to specific portions of the input, we can achieve more accurate and detailed responses.\n(g) Reflection\nThe objective of this method is to prompt the model to autonomously clarify the rationale behind its responses to the user. Through reflective analysis, the LLM facilitates clarification of any areas of uncertainty, exposes implicit assumptions, and highlights gaps in knowledge or comprehension. Hence, by eliciting automated explanations from the LLM regarding its responses, users can enhance their comprehension of the model's input processing, underlying assumptions, and data utilization. Users can further troubleshoot their prompts and discern the reasons for any deviations from expected outcomes because LLMs may occasionally produce incomplete, inaccurate, or ambiguous answers. Such reflection strategy acts as a mechanism to mitigate these deficiencies and maintain the accuracy of the information provided by the LLM.\n(h) Environment Change\nIn the context of a task or question, this strategy stimulates the LLM by altering the environment in which the task typically takes place or where the resulting product will be utilized.\n(i) Self-Refinement\nIt is very possible that the LLM may not offer the best solution in the initial response. The strategy requests LLM to do the self-examination so that it can propose potentially better or more refined versions. Furthermore, users may frequently lack the sufficient background knowledge to obtain accurate answers. Through the process of self-refinement, the LLM can assist users in locating relevant information or achieving their objectives more efficiently and effectively.\nStrategies play a crucial role in shaping prompts to elicit more targeted responses and should be applied according to the specific situations or scenarios anticipated by users. To effectively generate solutions, we recommend employing these strategies in various phases, as determined by a thorough goal analysis. It is important to note that users may often need to combine multiple strategies to form a more productive interaction."}, {"title": "4 Framework Demonstration and Test", "content": "To demonstrate the collaboration with an LLM tool using our framework, we apply it to the task of \u201cdesign a car that can fly\" in the conversations with ChatGPT 3.5. We believe that using ChatGPT 3.5 sufficiently illustrates its application and effectiveness of our GPS framework, so we did not extend the framework to other LLM tools, such as Bard. This task involves creating a system that is not currently available in everyday life, thereby avoiding influences from existing examples to the user's thinking. This indicates the usefulness of brainstorming in generating innovative ideas for this task, also making it suitable for our test. Figure 3 illustrates the sequence of prompts generated throughout framework demonstration and test section in this paper."}, {"title": "4.1 Framework Usage", "content": "Considering the length of this paper, only P0, P1, P2 and P3 will be used for displaying how to use our framework. Prompt PO begins with a straightforward inquiry soliciting design solutions. Prompt P1 is then created using our template. Subsequently, prompt P2 is formed by incorporating the one-shot strategy into prompt P1. Similarly, prompt P3 adds an unconventional role into P2. presents the variations of the prompts after applying the GPS template and strategies. As well, we detail the process of composing prompts using our GPS framework.\nTable 3 to 6 present the responses generated by ChatGPT for prompts P0, P1, P2, and P3, respectively. Analyzing the results quantitatively, by counting the number of items generated, reveals an increase in the number of items following the application of the template and strategies."}, {"title": "4.2 Framework Test", "content": "This section provides a task testing the effectiveness of the framework on influencing ChatGPT's ability in generating creative ideas. The following introduces the test process and the method we used for measuring creativity."}, {"title": "4.2.1 Test Process", "content": "We would like to compare the difference in the creative idea generation of ChatGPT between using the framework and without framework. In addition, we also want to see how various prompting strategies might influence the results of the framework. Two professionals (the first author and the second author) first work together to develop the prompts based on the framework. In addition to P0, P1, P2, and P3 shown in our demonstration, P4, P5, P6, and P7 were also composed drawing on strategies of Unconventional Role, Environment Change, Flipped Interaction, and Alternative Approaches into the template, respectively (see Fig. 3). Detailed descriptions of prompts P4, P5, P6, and P7 are provided in Tables 7, 8, 9, and 10, respectively. Notably, P7 is employed after responses generated by P1 because the strategy (i.e., Alternative Approached) requires following another prompt's answers.\nThen, we develop creativity measurements to conduct the framework test. Each professional independently collects ratings according to each measurement. Since fluency measurement is only one that involves directly counting the number of generated items without human evaluation, the other three measurements are averaged while we examine the change rates. To determine whether the use of GPS enhances creativity, we employ the following equation (1) to calculate the change rate in each dimension of the measurement compared to the prompt without our framework (i.e., P0). In equation (1), $D_{j,i}$ represents the value of dimension j for prompt Pi. $D_{j,0}$ denotes the value of dimension j for prompt P0, and i = 1, 2, . . . 7.\nDimensional Change Rate $R_{j,0}$ = ($D_{j,i}$ - $D_{j,0}$)/$D_{j,0}$  (1)\nSimilarly, equation (2) calculates the change rate relative to the prompt that only uses the GPS template (i.e., P1). In equation (2), $D_{j,i}$ is the value of dimension j of the prompt Pi, $D_{j,1}$ is the value of dimension j of the prompt P1, and i = 2,... 7. Understanding this rate of increase (or decrease) is crucial for further investigating the influence of the strategy.\nDimensional Change Rate $R_{j,1}$ = ($D_{j,i}$ - $D_{j,1}$)/$D_{j,1}$  (2)"}, {"title": "4.2.2 Measurements for Creativity", "content": "The Torrance Test is widely recognized as the foundation for defining and assessing creativity. The test aims to measure creative thinking and potential across various dimensions, such as fluency, flexibility, originality, and elaboration (Kim, 2006). Therefore, we leverage the studies (Wang et al., 2002; Kartikasari et al., 2022; Trisnayanti et al., 2020; Hendrik et al, 2001; Zubaidah et al, 2017; Suryana et al., 2021) to create a set of criteria to evaluate our framework across these four dimensions (see Table 4). This evaluation will investigate how creativity is boosted after applying our framework.\nThe four dimensions are defined as follows: (a) fluency: the ability to develop large numbers of ideas; (b) flexibility: the ability to produce ideas in numerous categories; (c) originality: the ability to produce unusual or unique ideas; (d) elaboration - the ability to adapt abstract ideas into realistic implementations. Given the context of using an LLM tool, we have defined specific measurements for each of these dimensions, as outlined in Table 11."}, {"title": "4.2.3 Results and Discussion", "content": "ChatGPT always considers questions within the same question-and-answer thread to be contextually related, influencing subsequent responses. To avoid this effect, except for P1 and P7, each prompt is executed in a separate thread. From the evaluation, we observed more details were produced in all dimensions after applying our GPS framework. This outcome aligns with common expectations, as prompt engineering reports often indicate that more detailed prompts produce more detailed responses. But, this does not necessarily imply that all creativity dimensions will increase in value. Our evaluation indicated an increase in all creativity dimension values after applying our GPS framework (see Tables 12 and 13). We think that our GPS framework generally enhances creativity in ChatGPT's responses, as evidenced by increased values in all dimensions. We anticipated that the GPS framework would significantly boost fluency and flexibility, consistent with findings from previous practices.\nUpon examining the change rate in each dimension after applying the strategies, we discovered that prompts P2, P3, and P4 decrease fluency value while the other three dimensions increase (see Table 14). This means that prompts P2, P3, and P4 do not elicit more ideas but encourages ChatGPT to generate ideas from various categories. In terms of originality and elaboration, the use of strategies results in an increase in both. However, the rate of increase in elaboration is lower, indicating that more detailed explanations or in-depth descriptions can only be provided to a certain extent. This is particularly evident when similar responses are produced by different prompts.\nOne expert think it is difficult to consider some responses in P5 as useful ideas. For example, responses in the following might be too unrealistic for designing a flying car."}, {"title": "5 Conclusion", "content": "The advancement of large language models (LLMs) has significantly promoted the utilization of prompts in various domains. However, existing methods of creating prompts focus solely on increasing response accuracy. There has been a notable gap in addressing creativity in prompt design and evaluating its output. To address this gap, we developed a GPS framework for cooperating with an LLM tool in brainstorming. The framework is designed to augment creativity.\nOur study demonstrated the application of the GPS framework. Through our evaluation, we discovered that using the GPS template resulted in increased values across all dimensions. Implementing these strategies aids designers in enhancing flexibility, originality, and elaboration. Consequently, our GPS framework proves effective in boosting creativity when designers collaborate with ChatGPT 3.5. We also identified that the GPS framework not only supports the generation of more diverse and original ideas but also enhances the elaboration of responses. Our GPS framework ensures that creativity is systematically fostered, offering a tool for designers seeking to increase the creative responses from their prompts."}]}