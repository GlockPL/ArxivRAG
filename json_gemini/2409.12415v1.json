{"title": "Multichannel-to-Multichannel Target Sound Extraction Using Direction and Timestamp Clues", "authors": ["Dayun Choi", "Jung-Woo Choi"], "abstract": "We propose a multichannel-to-multichannel target sound extraction (M2M-TSE) framework for separating multichannel target signals from a multichannel mixture of sound sources. Target sound extraction (TSE) isolates a specific target signal using user-provided clues, typically focusing on single-channel extraction with class labels or temporal activation maps. However, to preserve and utilize spatial information in multichannel audio signals, it is essential to extract multichannel signals of a target sound source. Moreover, the clue for extraction can also include spatial or temporal cues like direction-of-arrival (DoA) or timestamps of source activation. To address these challenges, we present an M2M framework that extracts a multichannel sound signal based on spatio-temporal clues.\nWe demonstrate that our transformer-based architecture can successively accomplish the M2M-TSE task for multichannel signals synthesized from audio signals of diverse classes in different room environments. Furthermore, we show that the multichannel extraction task introduces sufficient inductive bias in the DNN, allowing it to directly handle DoA clues without utilizing hand-crafted spatial features.", "sections": [{"title": "I. INTRODUCTION", "content": "Humans naturally focus on a specific sound in complex auditory environments with multiple sound sources. This ability allows us to attend to a target sound using clues like its time-frequency pattern or direction [1]. Target sound extraction (TSE) aims to mimic this by extracting the desired sound source using various types of clues. Common clues include class-labels [2], [3], a signal resembling the target [4], images or videos [5], timestamps marking target occurrences [6], [7], text descriptions [8], [9], directions or regions indicating the locations of targets [10], and combinations of these clues [11]-[14].\nHowever, these methods primarily focus on extracting a single-channel target signal. Multichannel mixtures, often recorded using microphone arrays, include spatial characteristics of a sound field. In applications like 3-D audio and virtual reality (VR) audio, interchannel relationships provide important spatial cues for rendering realistic sound. Similarly, in acoustic surveillance systems, interchannel time delays or phase differences are key to determining the direction or location of a target sound source. To fully exploit the spatial information in multichannel recordings, TSE should extract the multichannel source signal as if the microphone array had recorded the target sound alone.\nThere have been many DNN models designed to capture the spatial features and interchannel relations from multichannel sound sources, especially in speech separation or enhancement [15]\u2013[22] and DoA estimation [23], [24]. More recently, directional speech extraction [10], [25] has also been proposed to extract speech from a multichannel mixture using its DoA clue. To incorporate DoA clue, its interchannel correlation features are extracted from a complex spectrogram of the mixture or individual one-hot embedding for each channel is integrated into spectral features. Nevertheless, these models utilize interchannel information to extract a single-channel clean sound source or to determine the DoA of the sound source by removing reverberations and noises.\nExamples of separating a multichannel source signal can be found in binaural speech enhancement and TSE approaches [26]\u2013[29]. These approaches have demonstrated the potential of M2M extraction by preserving binaural cues. However, the binaural speech enhancement model [27] focuses exclusively on the speech target, limiting its application to a specific source type. Binaural TSE [29] overcomes this limitation and also utilizes spatial losses to minimize the degradation in binaural cues. Despite these advances, the primary clue for extraction remains the class-label related only to the time-frequency (TF) characteristics. In complex mixtures of signals from the same class, i.e., such as multiple instruments or speech sources, the direction of sound or timestamps of activation becomes compelling clues for extraction.\nTo this end, we propose an M2M-TSE framework capable of extracting multichannel sounds from a complex, reverberant"}, {"title": "II. PROPOSED METHODS", "content": "Our research focuses on extracting an $M$-channel reverberant sound signal $X_i \\in \\mathbb{R}^{M \\times N}$ of temporal length $N$, from an input mixture $Y \\in \\mathbb{R}^{M \\times N}$ of $I$ source signals captured from a microphone array in a reverberant room. The input mixture can be described as\n$Y = \\sum_{i=1}^{I} X_i = \\sum_{i=1}^{I} S_i * R_i + V,$\\nwhere $s_i \\in \\mathbb{R}^{N}$ and $R_i \\in \\mathbb{R}^{M \\times N}$ are the $i$-th dry source signal and its $M$-channel room impulse response (RIR), respectively. Here, $V \\in \\mathbb{R}^{M \\times N}$ is the multichannel measurement noise, and $*$ indicates the temporal convolution. When the target sound is the $g$-th source $X_g$ ($g \\in \\{1, \\dots, I\\}$), the multichannel signal $X_g \\in \\mathbb{R}^{M \\times N}$ extracted from a TSE model with model parameters $\\theta$ can be written as\n$X_g = \\text{TSE}(Y, C_g; \\theta),$\\nwhere $C_g$ is a clue embedding derived from DoA and temporal activity (timestamps) of the target source. In this work, we consider the clue embedding $C_g$ given by either a one-hot or a cyclic positional vector, as described in section II-C."}, {"title": "B. Model Architecture", "content": "The backbone of the proposed network is the DeFTAN-II [22] architecture. DeFTAN-II is a transformer-based architecture that performs complex spectral mapping to extract a single-channel clean speech with suppressed noise and reverberation. Our TSE objective is similar, but the main difference is that a multichannel target signal should be extracted and a clue for identifying the target signal should be injected into the network. To achieve these, we introduce the following modifications to the backbone network.\nThe overview of the proposed architecture is depicted in Fig. 1 without the batch dimension. First, a multichannel input waveform is converted into a complex spectrogram of dimensions $2M \\times T \\times F$ by short-time Fourier transform (STFT), where $T$ and $F$ denote the number of time and frequency bins, respectively. This spectrogram is then transformed to a tensor with increased channel dimension (C) using a 2-dimensional split dense block (2D SDB) encoder. 2D SDB is a modified version of DenseNet [30] introduced for extracting spatial features and learning local spectral-temporal relations. Since the channel dimension includes encoded spatial features and local time-frequency (TF) information, we combine the clue embedding with the channel dimension of the encoded tensor to extract the target sound of a specific direction.\nIn a series of DeFTAN-II blocks, the F- and T-transformers analyze the relationships in spectral and temporal sequences. We repeatedly embed the clue during this stage, to gradually align the features developed by DeFTAN-II blocks with those of the target sound. Finally, the aligned features are decoded to a multichannel waveform through a decoder reducing the channel dimension from C to 2M as in the original STFT, and the inverse STFT (iSTFT) operation."}, {"title": "C. Spatio-temporal Clues", "content": "The clue utilized in this work is the direction and timestamps of a target source. For simplicity, we consider only azimuth angles as the direction clue and encode them in two different ways: one-hot or cyclic positional encoding. For the one-hot encoding, the clue embedding vector $1_{\\text{one-hot}}(\\phi) \\in \\mathbb{R}^{360}$ defined for direction $\\phi \\in [0, 360)$ with 1\u00b0 resolution has a value of one only in the index corresponding to the direction $\\phi$, with all other values being zero. That is, for the index $j \\in [0, 360)$,\n$1_{\\text{one-hot}}(\\phi, j) = \\begin{cases} 1 & \\text{where } j = \\phi, \\\\ 0 & \\text{otherwise}. \\end{cases}$\nWhile the one-hot vector contains unique information for each direction, it cannot represent periodicity correctly, showing the abrupt transition from 359\u00b0 to 0\u00b0. To address this problem, we employ the cyclic positional (cyc-pos) encoding [31]. The cyc-pos vector $\\text{PE}_{\\text{cyc-pos}}(\\phi) \\in \\mathbb{R}^{D}$ for embedding dimension D can be represented as:\n$\\begin{aligned} \\text{PE}_{\\text{cyc-pos}}(\\phi, 2j) &= \\sin(\\sin(\\phi) \\cdot \\frac{\\alpha}{10000^{2j/D}}), \\\\ \\text{PE}_{\\text{cyc-pos}}(\\phi, 2j + 1) &= \\sin(\\cos(\\phi) \\cdot \\frac{\\alpha}{10000^{2j/D}}), \\end{aligned}$,\nwhere $j \\in [0, \\frac{D}{2})$, and $\\alpha$ is the scaling factor controlling the angular range utilized for the positional encoding. In Fig. 2, examples of cyclic positional encoding are presented across various $\\alpha$. Both $D$ and $\\alpha$ are hyper-parameters determined empirically, and in this work, $D = 40$ and $\\alpha = 20$ were selected from the parameter study. The generated positional embedding is normalized by its L2 norm for each direction.\nTo further reduce ambiguity in signal extraction, the encoded positional embedding vector is combined with the timestamp clue. The timestamp indicates the occurrences of a target signal, so the positional embedding is broadcasted along the time dimension, such that rows of the final embedding matrix $C_g \\in \\mathbb{R}^{T \\times D}$ are nonzero only when the target source is active. One example of the embedding matrix is shown in the top left corner of Fig. 1. This embedding is encoded by linear layers, followed by layer normalization (LN) [32] and the parametric rectified linear unit (PReLU) [33] activation, and is then multiplied element-wise with the output of the encoder and DeFTAN-II blocks except for the final block, across the channel and the time dimensions."}, {"title": "III. EXPERIMENT AND ANALYSIS", "content": "Target signals of training and test datasets were collected from the FSD Kaggle 2018 [34] dataset, a set of sound sources with 41 classes. Following a setup similar to that used for generating reverberant speech in [35], a 4-channel circular microphone array with a radius of 10 cm was positioned in cuboid rooms of which width, depth, and height dimensions were randomly sampled from the uniform distribution within the ranges [5, 10] m, [5, 10] m, and [3, 4] m, respectively. The distance between the center of the microphone array and all sources was also randomly varied within the range of [0.75, 2.5] m, and the minimum angle between two sources relative to the array was set to 20\u00b0. The RIRs were generated using the image source method implemented in the pyroomacoustics\u00b9 [36] library, and the reverberation time (RT60) of each room was varied between [0.2, 1.3] s. All sound sources convolved with RIRs were mixed using the scaper\u00b2 [37] library depending on their timestamp, duration, and magnitude in dB. In addition, noise signals obtained from the 1st, 3rd, 5th, and 7th microphone noises in the REVERB challenge [38] dataset were added. All input mixtures were 6-second-long samples, sampled at 8 kHz for fast computation. The numbers of mixtures constituting the training, validation, and test datasets were 12.5K, 5K, and 2.5K, respectively."}, {"title": "B. Implementation Details", "content": "All experiments were conducted in PyTorch framework using automatic mixed precision (AMP) training on a GeForce RTX 4090. The training parameters included a batch size of 4, the Adam optimizer with an initial learning rate of 0.0005, multiplied with 0.1 when the scale-invariant signal-to-noise ratio (SI-SNR) [39] of the validation dataset did not increase after 5 consecutive epochs, and gradient norm clipping was set to 0.5 for 100 epochs. The model parameters were the same as a base model in [22] except for the number of output channels of the decoder (2 \u2192 2M). The loss function for training was the summation of phase-constrained magnitude (PCM) losses [40] calculated for individual channels of the multichannel output."}, {"title": "C. Analysis of Results", "content": "We evaluated the effectiveness of our method by measuring the improvement in SNR (SNRi) and SI-SNR (SI-SNRi) compared to the input mixture. These metrics assess the models' ability to suppress other sounds and noise while preserving the target signal. To assess how well the interchannel relations between microphone pairs were maintained, we also calculated mean-absolute-error (MAE) of interchannel metrics (Spatial Errors), such as $\\Delta \\text{ILD}$, $\\Delta \\text{IPD}$, $\\Delta \\text{ITD}$, and $\\Delta \\text{ITD-GCC}$, utilized in previous studies [26], [29]. The comparative results are shown in Table I.\nAs a baseline model for comparison, we modified the binaural extraction model4 [26] based on Waveformer [3] into a 4-channel model. Compared to the baseline, our model performs significantly better, yielding higher SNRs and lower spatial errors. This result emphasizes the importance of encoding spatio-temporal information using a complex spectrogram instead of a waveform directly for extracting a multichannel sound. Additionally, the extraction performance is noticeably better when using a cyc-pos vector rather than a one-hot-encoded vector. Since the cyc-pos vector has smooth variation across azimuthal angles and has periodicity unlike the one-hot vector, it better integrates into spatially encoded features. However, with large values of $\\alpha$ especially greater than the embedding dimension, performance is degraded because the embedding is no longer smooth and changes rapidly in a short cycle even for closely located directions. Meanwhile, most models employ similar AIPD values, encountering difficulties in analyzing interchannel phase differences directly, rather than the level or time difference of arrival.\nTo evaluate the sensitivity to incorrect directional clues, we measured extraction performance across gradually changing directional clues with a resolution of 10\u00b0 from the true target direction. Results presented in Fig. 3 indicate that cyc-pos vectors maintain robust extraction performance near the true target direction, showing less than 1dB decrease in SNRi for azimuth angle difference of \u00b120\u00b0. In contrast, the one-hot vector embedding showed no variation with changing azimuth angles, indicating that the spatial information was not utilized by the model. Thus, using cyc-pos vectors is more advantageous, as it not only reduces memory usage by lowering the embedding dimension but also maintains periodicity.\nOne example of M2M-TSE is presented in Fig. 4, for the cyc-pos vector with parameters D = 40 and $\\alpha$ = 20. The 4-channel waveforms extracted by the proposed model demonstrate that the target source signals are well extracted despite significant overlaps with other sound sources in time. Demo is available at https://choishio.github.io/demo_M2M-TSE/."}, {"title": "IV. CONCLUSION", "content": "We introduced an M2M-TSE framework for extracting multichannel sound from diverse sound mixtures using target direction and timestamp clues. The direction clue, embedded in form of cyclic positional encoding, was directly integrated with multichannel features of modified DeFTAN-II blocks to enable multichannel sound extraction. The proposed model demonstrated superior performance across multiple channels, outperforming the state-of-the-art extraction model using the same types of clues. This approach to extracting multichannel signals paves the way for separating and editing multichannel audio recordings by preserving spatial information of individual sources."}]}