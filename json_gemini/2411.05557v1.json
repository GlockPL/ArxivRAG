{"title": "A NERF-BASED COLOR CONSISTENCY METHOD FOR REMOTE SENSING IMAGES", "authors": ["Zongcheng Zuo", "Yuanxiang Li", "Tongtong Zhang"], "abstract": "Due to different seasons, illumination, and atmospheric conditions, the photometric of the acquired image varies greatly, which leads to obvious stitching seams at the edges of the mosaic image. Traditional methods can be divided into two categories, one is absolute radiation correction and the other is relative radiation normalization. We propose a NeRF-based method of color consistency correction for multi-view images, which weaves image features together using implicit expressions, and then re-illuminates feature space to generate a fusion image with a new perspective. We chose Superview-1 satellite images and UAV images with large range and time difference for the experiment. Experimental results show that the synthesize image generated by our method has excellent visual effect and smooth color transition at the edges.", "sections": [{"title": "1. INTRODUCTION", "content": "Orthophoto is one of the important achievements of satellite imagery, which plays an important role in surveying and mapping, land resource management, geographic monitoring and so on. Due to sensor design constraints, the coverage of a single image is limited. In order to obtain wide range of remote sensing images, mosaicking technology is often used on images from different sensors and at different times. Due to the impact of sunlight incidence angle, atmospheric environment, and illumination conditions, there are radiation differences in images obtained on different platforms. The visual continuity of the mosaiced satellite images is poor, and accompanied by obvious stitching seams, which seriously affects the use of fusion images. The purpose of the color consistency processing of satellite images is to make the tone transition of the surveyed area images smooth and natural without damaging the texture information of the images and affecting the interpretation of the features.\nAbsolute radiometric calibration enables accurate conversion of digital number value from satellite imagery to surface reflectance [1]. However, they require parameter values of atmospheric properties at the time of image acquisition for most absolute radiometric calibration methods, which are often difficult to obtain. The method of relative radiation normalization is to make the radiation information of the target image consistent with the reference image [2], such as selecting reference images, based on overlapping regions of images, and establishing a color reference database. Absolute and relative radiometric correction methods are frequently used schemes, and the relative radiometric correction method is more widely used because the absolute radiometric correction method requires strict input conditions. However, the relative radiation normalization method also has disadvantages. Firstly, the determination standard of reference image is not uniform. Secondly, the overlap area is usually too small or most objects in the overlap area change greatly, so the regression calibration parameters are difficult to obtain. Because the transient radiation conditions of imaging are very complex, these methods can only approximate the radiation process, so mosaic images generated using these methods always have stitching seams.\nThe recently proposed new rendering paradigm Neural Radiance Field (NeRF) [3] is an implicit neural representation of 3D scenes and capable of self-supervised training from calibrated images. NeRF employs global MLP to regress view-dependent radiance and volume densities at any location, and can render synthesize images by applying volume from new viewpoints. Therefore, NeRF can generate realistic views of novel scenes. Currently the technique has been extended to different tasks such as relighting, scene editing and dynamic scene modeling. We address the shortcomings of traditional methods and propose a NeRF-based color consistency (NeRF-CC) method, which can change the characteristics of scenes captured in different environments in a high-quality and semantically meaningful manner."}, {"title": "2. THE PROPOSED APPROACH", "content": "Our method NeRF-CC decomposes a scene into a neural scene representation of shadows, lighting, spatial occupancy, and diffuse albedo. NeRF-CC takes as input a set of images and generates as output a 3D representation that can render images from new viewpoints under arbitrary lighting conditions. NeRF-CC can direct access to scene lighting and explicitly estimates scene intrinsic. which includes a dedicated component for fusing volumes, an essential feature for scene fusion. The pipeline of NeRF-CC is shown in Figure 1."}, {"title": "2.1 Neural Radiance Fields", "content": "NeRF defines its color c(p) and density \u03c3(p) for any point p in 3D space. Rays are cast from the camera origin o along the direction d corresponding to each output pixel to render an image. The final color in image space C(o, d) is obtained by integrating the color and density along the ray (o, d), which can be defined as follows:\n$C(o, d) = C({pi} depth) = \\sum_{i=1}^{depth} T(t_i)a(\\sigma(p_i)d_i)c(p_i)$ (1)\nwhere T(t) = exp(-$\\sum_{i=1}^{depth} \\sigma(p_j)d_i$) is corresponding ray depth, $d_i = t_{i+1} - t_i$, $a(y) = 1 - exp(-y)$. Depth{ti} depth is chosen from a uniform distribution using stratified sampling, spanning depths along (o, d) which starts from the near and ends at the far camera plane. Both the color c(p) and the density \u03c3(p) are modeled using MLP, and the final rendering is trained in a self-supervised fashion by the per-pixel color of ground truth.\nWhile Eq. (1) allows view synthesis of high-quality free, c(p) is only defined by MLP which cannot encode illumination. In other words, Eq. (1) only acquires a Lambertian model of the scene under fixed illumination. A more generalized model with view direction dependence acquires a slice of the apparent bidirectional reflectance distribution function under fixed illumination. Nevertheless, this learned representation does not contain the underlying scene semantic meaning essence, nor direct control over the illumination.\nTo control relighting, we introduce an explicit second-order spherical harmonic (SH) lighting model [4] and redefine the rendering Eq. (1) as follows:\n$C({pi} depth, L) = A\\sum_{i=1}^{S} A({pi} depth) \\cdot L_i b(N({pi} depth))$ (2)\nwhere \u22c5 represents element-wise multiplication. A(x) is the cumulative albedo color, which is produced in a similar manner to Eq. (1) by combining the output of the diffuse albedo extraction. L stands for the deducible SH coefficient for each image, and b(n) is the SH basis. N(x) is the surface normal calculated from congregated ray density. To extract N, we distinguish the point density on the ray with respect to the raw x, y and z components of the ray samples, then congregate them over all N depth samples on the ray with weights \u03a4(ti)\u03b1(\u03c3(pi)di), and finally normalize the output vector to unit sphere. All terms in Eq. (2) are deducible, except the regular extraction operator N(x) and the SH basis b(n), which are based on fixed explicit models. The proposed ensemble of illumination models allows explicit re-illumination by varying L."}, {"title": "2.2 Volumes Fusion", "content": "To address the fusion of implicit representations of multiple images, we propose NeRFusion module (see Figure 2), which combines the advantages of TSDF-based and NeRF fusing techniques for photorealistic rendering and large-scale reconstruction. We predict the local radiance field of image sequences through direct network inference. These images are then fused together using a recurrent neural network capable of reconstructing a global sparse scene representation.\nThen we aggregate features to regress a local volume vn from multiple adjacent viewpoints, representing the local radiation field. We take advantage of computing the variance and mean of per-voxel features in \u00b5\u2081 across adjacent viewpoints, where the variance provides abundant correspondence clues for geometric inference and the mean can contain appearance information of per-view. These two operations are also invariant to the order and number for inputs, which can handle voxels with varying numbers of visible viewpoints. A deep neural network is employed to handle mean and variance features for per-voxel to regress each viewpoint reconstruction.\n$vt = NN([Meani\u2208\u03bat\u00b5i, Vari\u2208\u03bat\u00b5i])$ (3)\nwhere \u03bat denotes all k neighboring viewpoints used in each image; Mean and Var denote element-wise mean and variance operations respectively. To create scalable scene, efficient, and consistent reconstructions, we progressively fuse each frame's local feature volume vt into a global volume vousing a global volume fusion network."}, {"title": "2.3 Loss Function", "content": "Our entire pipeline is trained entirely on rendering supervision from ground truth images, which does not require any additional geometric supervision. We then train the entire pipeline by a rendering loss:\n$LnerF-CC = \\sum_t ||C_t \u2013 \\hat{C}||^2 + ||C^t_v \u2013 \\hat{C}||^2$ (4)\nwhere Ct stands for the pixel color rendered by the local reconstruction vt, $C$ stands for the pixel color of ground truth, and $C^t_v$ stands for the color rendered by the global volume ve after fusing images. Essentially, we use each intermediate local and global volume on each image to render new images of viewpoint and supervise them using the ground truth. Therefore, the fusion module can reasonably fuse local volumes from several input images."}, {"title": "3. EXPERIMENTS", "content": ""}, {"title": "3.1 Evaluation Metrics", "content": "The ground truth of remote sensing images is vital for quantitative evaluation experiments. Since it is hard to delimit what the ground truth is for a set of images with inconsistent colors, it is not feasible for this problem. Color consistency between images is a relative measure. Therefore, two metrics proposed by Xia et al. 2019 [5] will be used to evaluate the color calibration effects produced by different methods. The metrics include the color distance (CD) and the gradient loss (GL), which evaluate the color differences between overlap-corrected images and the gradient variations between the input image and the rectified image respectively. They can be defined as follows:\n$CD = \\sum_{I_i \\cap I_j \\neq \\emptyset} w_{ij} \\frac{\\Delta H(\\hat{I}_{ij}, \\hat{I}_{ji})}{N}$ (5)\n$GL = \\sum_{i=1}^{N} \\frac{\\sum_{p \\in I_i} \\Delta G(I_i, \\hat{I}_i)}{N_p}$ (6)\nwhere $I_i$ and $I_j$ are two overlapped images. $\\hat{I}_i$ and $\\hat{I}_j$ represent the corresponding corrected images. $I_{ij}$ stands for the region of $I_i$ overlapped with $I_j$. $w_{ij}$ is the normalized weight which is proportional to the area of $I_{ij}$. \u2206H stands for the difference between the color histograms extracted from the bin, and N is the bin number of the color histogram. AG stands for the difference between the gradient direction map extracted from $I_i$ and $I_j$ by pixel, Np and is the number of valid pixels in $I_i$. The smaller value of CD and GL, the higher the quality of the color calibration outcome."}, {"title": "3.2 Experimental Results of Different Methods", "content": "The first dataset used for testing consists of nine images acquired from the SuperView-1 satellites at urban area. The color differences between input images are relatively small in this dataset, as shown in Figure 3(a). Our method performs well on this dataset, where color differences are effectively removed between input images. We find that the left image is slightly darker than the right image for the first enlarged region by Yu et al. [6] method. For the second enlarged region by [6], we also observe that the color discrepancies between upper image and bottom image is larger than that generated by our method. There are still some tonal differences between the results of Brown et al. [7] method and Xiong et al. [8] method, especially in the lake and mountainous regions, as shown in Figure 3(b) and (c). Figure 3(d) shows the color correction results of Shen et al. [9] method, which produces the highest contrast. Especially in the right-most image area, the colors of the mountains are overly bright.\nThe second dataset captured by UAV at village area, which consists of more than one hundred images as shown in Figure 4(a). For this dataset, the results of traditional methods have some severe problems. The results of the Brown et al. method have a consistent global tone, however, there are still some tone discrepancies between adjacent images, as shown in the enlarged regions of Figure 4(b). The Xiong et al. method produces the worst color correction results, there are still serious color differences between the corrected images. Shen et al.'s method performs well in terms of color consistency and shows only small color differences in the corrected outputs, as shown in Figure 4(d). However, the results of Shen et al.'s method suffer from serious lighting inconsistencies, and some regions are too dark (the first enlarged region). Compared to the above methods, the method of Yu et al. provides better results, but it still has some color differences. Our method provides the most visually pleasing output which is notably outperforming those produced by other methods."}, {"title": "4. CONCLUSION", "content": "We demonstrate a method to recover replaceable neural volume representations from ambient and indirectly illuminated scene images by using visibility MLPS to approximate parts of the volume drawn integral. This paper proposes a novel neural rendering method that enables fast, high-quality, and large-scale multi-view scene reconstruction for photorealistic rendering. Our method uses a new recursive neural network to process the input multi-view images, and gradually reconstructs the global large-scale radiation field through the reconstruction and fusion of the local radiation field of each image. Compared with the traditional color consistency methods, we reconstruct the scene as a volumetric radiance field, and then obtain realistic view synthesis results."}]}