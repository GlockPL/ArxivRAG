{"title": "A comprehensive study of on-device NLP applications - VQA, automated Form filling, Smart Replies for Linguistic Codeswitching", "authors": ["Naman Goyal"], "abstract": "Recent improvement in large language models, open doors for certain new experiences for on-device applications which were not possible before. In this work, we propose 3 such new experiences in 2 categories. First we discuss experiences which can be powered in screen understanding i.e. understanding whats on user screen namely - (1) visual question answering, and (2) automated form filling based on previous screen. The second category of experience which can be extended are smart replies to support for multilingual speakers with code-switching. Code-switching occurs when a speaker alternates between two or more languages. To the best of our knowledge, this is first such work to propose these tasks and solutions to each of them, to bridge the gap between latest research and real world impact of the research in on-device applications.", "sections": [{"title": "Introduction", "content": "Recent advancements in large language models in Document AI (Xu et al., 2020; Li et al., 2021), Dialogue Generation (Peng et al., 2022), information extraction opens doors to much powerful and useful experiences on device application, much better than existing solutions. Currently the 2 main applications of NLP on a device are (1) digital assistants and (2) automated smart replies. This work gives a comprehensive idea of newer experiences which can be supported on a mobile device to ease user lives.\nThe first such experience are related to on-device screen understanding. The latest research on layout based understanding of screen content and help power newer capabilities on device including VQA, smarter form filling and information sharing and better accessibility usage for visually challenged users.\nThe second experience, is extending the capability of smart replies systems. Smart replies refers to automatically generating shorter responses for an incoming email or an ongoing conversation which a user assists with quicker response in a large number of scenarios. The current research in this domain is similar to dialogue generation, but the current responses are much more generic and only applicable in highly formal and limited languages setting. This works discusses new applications for smart replies (1) smart replies for users with linguistic code switching (2) personalized smart reply generation based on learning knowledge about a user from the conversation history\nIn linguistics, code-switching or language alternation occurs when a speaker alternates between two or more languages, or language varieties, in the context of a single conversation or situation. Multilingual (speakers of more than one language) sometimes use elements of multiple languages when conversing with each other. Thus, code-switching is the use of more than one linguistic variety in a manner consistent with the syntax and phonology of each variety.\nThe current challenges in the proposed space is lack of datasets and evaluation benchmarks for the newly proposed experiences including but not limited to, on-device screen understanding for form filling, smart replies for users with linguistic code switching, and personalized smart reply generation based on external grounded knowledge. Constructing such datasets requires large scale collections from user habits and special annotations for the task, and then coming up with good evaluation benchmarks for the tasks,\nThis is the first work which discusses such newer experiences and then proposes solutions based on the current research. The notable contributions being -"}, {"title": "Screen Understanding", "content": "Screen understanding is modelled as understanding the input screen image and associated text on the screen. It is a multimodal learning task. Recently, there has been recently a lot of progress in Document AI for work related to document understanding. If we model an input app view as document, we can use the recent techniques in document AI. Document AI refers to given a input document, techniques for automatically (1) reading, (2) understanding, (3) analyzing it. It's a challenging task due to the diversity of layouts and formats, inferior quality of scanned document images complexity of template structures. For rest of the discussion we will consider an app view (the screenshot + screen text) as an document."}, {"title": "Related Work", "content": "There are 2 families of models proposed - LayoutLM (Huang et al., 2022) and MarkupLM (Li et al., 2021) as seen in figure 1. The LayoutLM family used a rendered image and the document text to extract the answer. More useful for PDFS which are rendered the same irrespective of the viewing device. MarkupLM family uses the idea that the same HTML document could be rendered in different ways based on viewport screen size. Hence to generate they use XPath (XML Path Language) which is directly extracted from the view hierarchy.\nThe main idea for LayoutLM family is to do multiple pretraining tasks which closely align the image and text level tokens. For the latest LayoutLMv3 shown in figure 6, we take word level feature as token, divide Image as patch and project in latent sapce and append them to text. Then we do 3 level of pretraining\n1.  Mask Language Modelling with text token\n2.  Mask Image Modelling with Image token - reconstruct masked image patches, target tokens latent codes from a discrete VAE\n3.  Word Patch Alignment - predict from some text token if corresponding image patch is unmasked"}, {"title": "Tasks", "content": "We next discuss the 2 task in screen understanding. To best of our knowledge this is the first work done to propose such tasks in Screen Understanding.\n1.  Visual Question Answering for on screen context\n2.  Automated form filling using previous on screen context"}, {"title": "Visual Question Answering for on screen context", "content": "The task for visual question answering is to build a system which can answer questions based on information present on the screen. It is an extractive question answering task.\nSpecifically for a given screen view we need to understand the following to answer a question\n1. 'form' - information is in the form as key:value\n2. 'Layout' - require spatial/layout information like title, heading\n3. 'table/list' - question requires understanding of a table or a list"}, {"title": "Data and Challenge", "content": "The issue with building a system which works for Visual Question Answering should work for the large variety of apps on appstore. Hence we need need lot of diverse screenshots. We internally collected data from 100,000+ screens from more than 4,500 top-downloaded iOS apps.\nBut the challenge was this was unlabelled i.e. we need correct question, answer pair for a given"}, {"title": "Training Pipeline", "content": "Our training methodology had 4 stages.\n1.  We start with the LayoutLMv3 model which has been pre-trained on a large amount of open source scanned documents.\n2.  Add Question answering (QA) head on top LayoutLMv3. The model is then trained on DocVQA dataset.\n3.  Generate weak labels (question answer pairs) using Rule based system on the top apps dataset. to form a for training\n4.  Finetune on internal apps dataset using incremental learning.\nWe use DocVQA (Mathew et al., 2021) dataset to initialize QA head, because the question answer set of DocVQA is very close to our internal apps dataset training. Further DocVQA labels are clean and gold standard, while our labels are noisy."}, {"title": "Question Types", "content": "The following 6 types of questions were generated using the weak labelling procedure described earlier on tops apps dataset used for final training. Note if we found that a value has an associated key, we used the key as in forming question by replacing {} with the key. We call these questions keyed questions as highlighted in blue. Two examples of keyed questions are shown in figure 4\ntitle \"What is the document about?\", \"What is the title?\", \"What is it about?\"\nphone number \"What is the phone number?\", \"What is the number?\"\n\"What is the {} phone number?\", \"What is the {} number?\"\nemail \"What is the email?\", \"What is the email address?\"\n\"What is the {} email?\", \"What is the {} email address?\"\nurl \"What is the url?\", \"What is the link?\"\n\"What is the {} url?\", \"What is the {} link?\u201d\naddress \"What is the address?\", \"What is the location?\"\n\"What is the {} address?\", \"What is the {} location?\"\n'DateTime' \"What is the date?\", \"What is the time?\", \"When is it?\"\n\"When is the {}?\", \"What time is {} scheduled?\u201d, \"When is {} scheduled?\u201d, \"What date is {} scheduled?\""}, {"title": "Observations", "content": "We now analyze the fine-tuning on our top apps dataset starting from DocVQA checkpoint. We observe the following trends.\nObservation 1 The validation loss hasn't dropped much even though train loss has decreased. Similarly validation f1 hasn't increased much over training."}, {"title": "Results", "content": "Below we show 4 examples in figures 14, 15, 16, 17\nEach example highlights a particular challenge the model is able to solve without being explicitly trained on those samples.\nExamples include\n1.  Understand natural query and do structure based association in the cab example.\n2.  Work on generic new data types like length.\n3.  Do robust entity association in a list shown in the car race schedule example.\n4.  Understand tables"}, {"title": "Limitations", "content": "The following limitations were noticed while working on the model\n1.  The proposed model can't understand and reason about images. The model LayoutLM can understand the structure and layout but it can't reason nor understand if there is an image of a dog. This is because understanding the image was not part of its training objective, nor does it have any image-based backbone?\n2.  The model doesn't do intent classification of query (e.g. model that the question like \"How long was outdoor run?\" expects length as an answer) so the generated answer can be arbitrarily bad, and may not be even what user expects (e.g. model may predict date when asked about the same length question if a date is nearby the text \"outdoor run\" in the input screen.)\n3.  The latency of the model is quite large around 450 ms with 343M parameter. To make it work on device in real time, we have to shrink its runtime to less than 50M parameters."}, {"title": "Future Directions", "content": "One possible future direction for VQA is to look for predicting the bounding box of evidence when predicting an answer.\nSupport for Infographics/ Charts understanding. E.g. given an image Answer question which require understand image, and diagrams."}, {"title": "Automated form filling using previous on screen context", "content": "The task of automated form filling refers to the task when the user is filling a form and information required to fill the same exists in one of the recent screens user previously visited. Currently the user requires to go back and forth between screens, copy each information individually and then paste that information in the relevant fields one by one. This is a slow and repetitive process on user end. A way which automates the form filling process for user by suggesting the information from previous screen automatically relevant to the user in the current form.\nThe form filling suggestions can also made when the user is looking to input an information present in previous screen the user visited. To the best of our knowledge, this task has not been formally defined nor research previously."}, {"title": "Data and challenge", "content": "Since the task didn't exist before we need to create our own dataset for the task. We collect around 150 samples of different forms including flight reservations, hotel reservations, ticket creation and screenshot of previous screens. We also collect information of screen where the user is in a chat with an agent and requested some information present on previous screen in the current chat.\nAn example is shown in figure 18 where we have 2 questions about \"Reported problem\" and work order number which can filled from information in the previous screen.\nThe challenge here is to design a system which process each screen of user once, and stores an intermediate representation. We can then use multiple such screen representations to find appropriate form fields which could be filled, without having to recompute the intermediate representation every time."}, {"title": "Pipeline", "content": "The pipeline consists of keeping a buffer of previously visited screen and the current screen. First we pass the current screen and previous screen through LayoutLMv3 individually. Then the last layer representations are concatenated passed to a question extraction head which extractions spans of relevant questions which can be answered in the current screen (form screen) from the previous screen (info screen). The extracted question tokens along with representation of info screen are then passed to an answer extraction head which predicts the answer span for each of the extracted tokens.\nNote we use the same LayoutLMv3 model instance to extract representations for both form screen and info screen, which the representations are then differentiated when based to respective question or answer extraction head.\nThe latency of the model is increased by the faced that for each extracted question we need to run the answer extraction head to get span of answer predicted for a particular question.\nNote that both question answering and answer extraction head are 3 layer feedforward network with a cross attention layer at the start."}, {"title": "Results", "content": "Since we have 2 additional heads to train on top of LayoutLM we start evaluate each of the units individually as shown in table 2. We observe few trends -\nObservation 1 The question extraction is an easier task than answer extraction for a given question."}, {"title": "Limitations", "content": "The following limitations were noticed while working on the model\n1.  The latency of LayoutLMv3 is still very high to deploy on mobile in real time.\n2.  We need to run answer extraction head for each question we find. This increases our run time.\n3.  The system is currently only trained for a small sample of data and larger level study needs to be done to see its effectiveness."}, {"title": "Smart Replies", "content": "Smart replies are automated generated short responses to email or chat in a conversation especially for a phone application, which assists a user to quickly respond to large variety of messages which require similar response. This is meant to save the characters a user is supposed to type on a mobile device and hence save time."}, {"title": "Related Work", "content": "The earliest smart reply was for Gmail (Kannan et al., 2016) and used a Seq-to-Seq model to encode a message and then decode a response. To ensure only relevant emails get generated responses and the authors proposed a classifier which based on the email content and its metadata (origin, subject) will screen out marketing emails or emailing requiring more thoughtful longer responses. At the end only about 10-15% of the all emails were filtered to be used for further generating smart reply.\nTo ensure only higher quality messages get suggested as a response, a response set is pre-computed which is the set of all valid responses. And for a given input message, a reply is then searched during decoding only in the valid response set space. To ensure diversification of responses, each response is pre-assigned an intent. Now where searching for a reply to a message, it is ensured that messages which atleast 2 different intents are recommended to user."}, {"title": "Tasks", "content": "Smart replies models are currently great for English and high resource language conversations. Further they are only trained for short reply pairs and often the reply lacks diversity. Lastly the smart replies currently don't leverage external knowledge available about certain users during a reply.\nTo solve these issues we propose 2 new tasks in smart reply space. To best of our knowledge, this is the first work to propose such task.\n1.  Suggesting smart replies for multilingual speakers with code switching\n2. Suggesting smart replies based on learned knowledge about a user from different interactions with the same user."}, {"title": "Suggesting smart replies for multilingual speakers with code switching", "content": "In linguistics, code-switching or language alternation occurs when a speaker alternates between two or more languages, or language varieties, in the context of a single conversation or situation. Code switching happens where people use either same script for a language e.g. English - Hinglish/ English-Spanglish or different script to type both languages English - Hindi/ English - Spanish. Additionally, the challenge arises from the fact that there is large amount of data in monolingual setting i.e. exclusively using 1 language to type the message while there is limited amount of data for code-switching i.e. using 2 language in the same sentence.\nFor initial work we start with first generating a large corpus of m-r (message-replies) pair in code-switch format. Then we adapt the Smart replies pipeline to support multiple languages by change in architecture and adding few auxillary task to the bi-encoder approach."}, {"title": "Code-Switch Data", "content": "The code switch data is constructed by first taking the English m-r (message-replies) pair data. Then each m-r is translated in the monolingual second language e.g. Hindi using an existing solution like Google Translate. Then we take an input English message break it into subordinate clauses and out of all subordinate clauses we apply clause substitution to Hindi based on estimated frequency of code-switch. Hence we generate code-switch samples in English-Hindi-CS (code switch) format.\nWhile the m-r pair data is usually retrieved from user collected data in a commercial application, we use the public topical chat data where different turkers discuss on an open ended conversation with varying background knowledge provided via wikipedia and reddit article. The data consists of 8628 conversations and over 184,000 messages across 7 sentiments - Angry, Curious to Dive Deeper, Disguised, Fearful, Happy, Sad, and Surprised."}, {"title": "Code-switch Smart Replies model", "content": "To adapt the smart replies model to multiple language we first start with a multilingual BERT as m-r encoders. Then apart from the normal cosine similarlity between the m-r pair embedding, we add additional auxiliary tasks by adding a translation head and task using the learned embedding to make the model work better. We then train the model on the all the 3 tasks."}, {"title": "Results", "content": "To evaluate the results of our Code-switch smart replies. We evaluate the rank of each reply in the response set R, based on the score function given in equation 2.\nThen we sort the set each reply in the in descending order based on the score. Finally, we find the rank of the actual response with respect to all elements in R.\nUsing this value, we can compute the Mean Reciprocal Rank:\nMRR = 1/N \u03a3 (from i=1 to N) 1/rank_i\nAnalysing the results in table 3 we find that the multilingual BERT trained on Code-Switch m-r data performed the best in retrieving the correct response. While using BERT trained on English m-r pair data performed a little worse when we used translation over top of it. This shows that using language based training helps better response generation."}, {"title": "Future Directions", "content": "We constructed Code-switch data for training using simple clause based substitution while there exists better methods like embedded matrix theory (EMT) for code-switching data generation which could have been used to construct more natural Code-switch data.\nFurther we still need to a do a much larger experiments on commercial level data to validate our hypothesis of using multilingual representations over language specific representations.\nAnother future direction to explore in smart reply space is about Suggesting smart replies based on learned knowledge about a user during the conversation history.\nWhile there has been a lot of research (Peng et al., 2022; Zhang et al., 2018) in open dialogue chat domain for generating text based on input text and given context, there is little work on how to model this for human conversation where the external knowledge comes from various interactions between the same 2 users. Usually the external knowledge in existing research is given as input while in our case the external knowledge is actually learned from conversation itself.\nE.g. if during a conversation between Alice and Bob, Bob learns that Alice loves pizza, and later on the conversation Alice asks for resturant recommendation, Bob could recommend pizza resturants.\nUsually in human interactions we tend to learn more about the other person during the course of multiple interactions. And modify our interactions accordingly based on the learned information. Can we do the same in a smart reply system is another good direction to approach."}, {"title": "Conclusion", "content": "With recent advancements in large language model, our work tries to find applications for the latest research in real world usage. We propose 3 novel on-device tasks to assist users which much more powerful experiences - visual question answering, automated form filling using previous on screen context and support for smart replies with linguistic code-switching. We then do initial experiments to propose solutions for each of the task. While the solutions do work on the limited amount of data we have - we observe 2 limitations, lack of large scale experimentation on commercial level, large amount of latency for proposed solutions. Lastly, we also discuss few possible directions for exploration of future work in the field."}]}