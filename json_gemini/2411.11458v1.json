{"title": "HistoEncoder: a digital pathology foundation model for prostate cancer", "authors": ["Joona Pohjonen", "Abderrahim-Oussama Batouche", "Antti Rannikko", "Kevin Sandeman", "Andrew Erickson", "Esa Pitk\u00e4nen", "Tuomas Mirtti"], "abstract": "Foundation models are trained on massive amounts of data to distinguish complex patterns and can be adapted to a wide range of downstream tasks with minimal computational resources. Here, we develop a foundation model for prostate cancer digital pathology called HistoEncoder by pre-training on 48 million prostate tissue tile images. We demonstrate that HistoEncoder features extracted from tile images with similar histological patterns map closely together in the feature space. HistoEncoder outperforms models pre-trained with natural images, even without fine-tuning or with 1000 times less training data. We describe two use cases that leverage the capabilities of HistoEncoder by fine-tuning the model with a limited amount of data and computational resources. First, we show how HistoEncoder can be used to automatically annotate large-scale datasets with high accuracy. Second, we combine histomics with commonly used clinical nomograms, significantly improving prostate cancer-specific death survival models. Foundation models such as HistoEncoder can allow organizations with limited resources to build effective clinical software tools without needing extensive datasets or significant amounts of computing.", "sections": [{"title": "1. Introduction", "content": "Neural network-based solutions [33] have achieved impressive results with tissue diagnostics, often surpassing human counterparts in consistency, speed and accuracy [2, 20, 61]. For instance, a multiple instance learning model by Esteva et al., trained and validated with clinical trial histological images and data, showed that prostate morphological features learnt by the model contain predictive information beyond conventional nomograms [23]. A subsequent AI biomarker, predicting the benefit of adding androgen deprivation therapy to radiation therapy, primarily includes conventional Gleason patterns and image features extracted with a neural network as key components of the model [51].\nDespite promising results, recent work has demonstrated that neural networks perform substantially worse on datasets not used during training [7, 16, 37, 43, 66]. A major driver of this performance reduction is dataset shift [47, 56], where neural networks fail to generalize to data from a new clinical setting that differs from the training data. Fine-tuning pre-trained neural networks for downstream tasks has been shown to improve model robustness and reduce uncertainty [29]. Many recent publications have confirmed this by leveraging neural networks pre-trained on natural images [11, 12, 18, 24, 34, 36, 39-41, 49, 53, 58]. Still, due to the considerable domain shift between histological and natural images, transfer learning from a neural network pre-trained with natural images offers little benefit to performance [48]. Thus, there is a need for neural networks pre-trained with histological images.\nRecent advances in self-supervised learning [9, 28, 44, 68, 70] have resulted in the emergence of foundation models [3, 8, 25, 26]. Foundation models are trained on large-scale datasets, leveraging unlabelled samples via self-supervised learning [35, 52], and can be then easily adapted for downstream tasks, even without any additional training [9, 44].\nHere, we train foundation models on 48 million tile images from thousands of histological slide images with prostate tissue (Fig. 1). These foundation models outperform models pre-trained with natural images by a large margin. Additionally, we describe two workflows leveraging foundation models. The first workflow describes a method for automatically annotating large-scale tissue image datasets, which we evaluate by annotating the largest publicly available histological dataset [5] with high accuracy. The second workflow integrates annotated significant histology features with commonly used clinical prognostic nomograms to improve the prediction of prostate cancer-"}, {"title": "2. Materials and methods", "content": ""}, {"title": "2.1. HistoEncoder models", "content": "In this study, we train cross-covariance image transformers (XCiT) [21] with a self-supervised learning method DINO [9, 44], which leverages discriminative signals between groups of images to learn useful features using self-distillation. Instead of focusing on maximizing the fine-tuning performance on a downstream task [28, 68], the encoder model training aims to maximize the representativeness of extracted features [44]. We selected XCiT due to linear complexity in the number of tokens, allowing efficient processing of high-resolution images [21], which are commonly encountered in digital pathology. Two XCiT backbones are trained on prostate tissue samples, prostate-s and prostate-m, based on the small and medium-sized XCiT model variants XCiT-S12 and XCiT-M24, respectively.\nMany recent models for clinical digital pathology image analysis have been pre-trained with natural images, often using supervised training methods [11, 12, 18, 24, 34, 36, 39-41, 49, 53, 58]. Although it is common to use models pre-trained with supervised training methods, this does not always produce useful embedded features, when compared to models trained with self-supervised methods (see [9] and Supplementary Tab. 2, 3, and 4). Thus, to make comparisons between models trained with histological images and natural images more fair, all comparisons are against XCiT-S12 and XCiT-M24 models trained on natural images using the self-supervised method DINO [9, 17]. These models are denoted as natural-s and natural-m, respectively."}, {"title": "2.2. Workflows leveraging encoder models", "content": "We describe two workflows that leverage the image features from our encoder models. The first is for automatic pre-annotation of large imaging datasets, and the second is for combining histomics data with other data modalities. Command line interfaces and a Python module for running these workflows can be found from https://github.com/\njopo666/HistoEncoder."}, {"title": "2.2.1. Automatic annotation of large-scale slide image datasets", "content": "First, all slide images in the dataset are cut into small tile images, for example with HistoPrep [46]. Second, features for all tile images are extracted with either prostate-s or prostate-m encoder models. Third, all extracted features for the tile images are clustered into n clusters with mini-batch K-means. Given that HistoEncoder produces similar embedded features for tile images with similar histological patterns, clusters can be labelled based on visual inspection of the prevailing histological pattern. After automatically pre-annotating all tile images in a dataset with HistoEncoder, a pathologist can visually inspect a subset of the tile images in each cluster. Now, if a given cluster contains only a single histological pattern, such as stroma or epithelium, all tile images assigned to this cluster can be labelled based on the prevailing pattern. If a cluster contains too much variability, the tile images in this cluster can be further clustered and labelled to produce clusters with less variability. The process can then be repeated until enough tile images have been labelled for the task at hand."}, {"title": "2.2.2. Combining histomics with other data modalities", "content": "Here, we present a workflow to integrate HistoEncoder image features with other data modalities. When additional data modalities, like spatial transcriptomics data [54] or tissue microarray (TMA) spot labels, provide information for smaller tissue regions, feature vectors can be directly generated for these regions using the prostate-s or prostate-m encoder models.\nIn contrast, if the data modalities to be integrated contain information for larger tissue regions or whole or multiple slide images, the slide images must be cut into smaller tile images that are compatible with HistoEncoder. A common example is patient-level clinical data, where data from each patient pertains to all tile images from the same patient. In this case, we will first automatically annotate and cluster all tile images following the workflow in Sec. 2.2.1. Each cluster contains tile images with similar histological patterns. Second, we will calculate, for each patient, the proportion of tile images assigned to each cluster (\"cluster fractions\"). Cluster fractions represent a patient-specific summary of histological patterns observed in the slide images.\nTo give an example, in a dataset of tissue biopsy slide images and associated clinical data, we can derive patient-level cluster fractions by analyzing all tile images extracted from each biopsy slide. If a specific cluster predominantly contains tile images with high-grade cancer, the fraction of this cluster for a given patient would represent the proportion of high-grade cancer observed in that patient's biopsy slides."}, {"title": "2.3. Datasets", "content": "All datasets leveraged in this study are summarized in Tab. 1. All slide images are processed with HistoPrep [46]."}, {"title": "2.3.1. Training dataset", "content": "Training data, designated as HelsinkiProstate, for the prostate tissue encoders, consists of patients who have undergone prostate biopsies or radical prostatectomy in Helsinki University Hospital between 2013 and 2021. In total, there are 1,307 patients with 5,642 and 5,584 tissue slides of needle biopsies, and tissue sections from radical prostatectomy (Tab. 1). To create the HelsinkiProstate dataset, each slide is cut into 640 \u00d7 640 pixel tile images at magnifications 20x, 10x, and 5x with 20% overlap between neighbouring tile images. This produces several hundred million tile images, which are then preprocessed with HistoPrep [46] to remove tiles containing non-tissue areas such as pen markings or other artefacts. Once irrelevant tile images have been filtered out, all remaining tile images are run through a prostate cancer classifier model from [47]. To create a balanced training dataset, all 16 million tile images with a prediction score \u0177 > 0.2, and 32 million randomly sampled tile images with score \u0177 \u2264 0.2 were selected to comprise the HelsinkiProstate dataset."}, {"title": "2.3.2. Evaluation datasets", "content": "HelsinkiTMA dataset contains 1,769 TMA spots from 432 prostate cancer patients who underwent radical prostatectomy between 1983 and 1998 at the Helsinki University Hospital. All slide images are cut into 512 \u00d7 512 with 25% overlap between neighbouring tiles. The patients have a median follow-up time of 19.0 years. All 432 patients have Gleason grade information available, and 238 have complete clinical data for calculating CAPRA-S [14] score and Memorial Sloan Kettering Cancer Center 15-year survival probability [1], denoted as MSKCC-S. Kaplan-Meier survival curves [30] for Gleason grade groups and both clinical nomograms are presented in Supplementary Fig. 7.\nHelsinki30 and Helsinki60 datasets [47] contain whole slide images (normal size and whole mounts) from 30 and 60 patients who have undergone radical prostatectomy at the Helsinki University Hospital between the years 2014 and 2021. All slide images in both datasets have been annotated by pathologists, and classified as cancerous or benign. These datasets are also part of the training (HelsinkiProstate) dataset (Tab. 1).\nSeveral publicly available datasets are used in this study. The test set of the PESO dataset [4, 22] contains 137 tile images of 2, 500 \u00d7 2, 500 pixels from 37 different slide images annotated as either cancerous or benign. PANDA development dataset [5] contains 10,616 prostate biopsy slides from 2,113 patients from Radboud University Medical Center and Karolinska Institute, which we denote in this study as Radboud and Karolinska."}, {"title": "2.4. Fine-tuning the encoder models", "content": "To fine-tune the encoders for downstream tasks, the partial fine-tuning setup from [28] is used, with the following modifications. RandAugment [15] is replaced with StrongAugment [47] with 2, 3 or 4 operations with 0.5, 0.3 and 0.2 probability, respectively. Each image is flipped vertically and/or horizontally and a perspective operation is applied with a scale from 0 to 0.1 with a probability of 0.5. For partial fine-tuning [28], only the last 0.5, 2, 4 or 8 XCiT-S12 model blocks are fine-tuned, where 0.5 denotes fine-tuning the last fully connected layer of the last model block. There are 1.182, 3.550, 7.120 and 14.260 million fine-tuned parameters for 0.5, 2, 4, and 8 model blocks, and 25.868 million for the full XCiT-S12 model. For fine-tuning the encoders without training, a K-nearest neighbour (KNN) classifier is used with k = 20. The KNN-classifier is fit with the features extracted from the training dataset, and label predictions correspond to the proportion of nearest neighbours with a positive label. Each experiment is repeated five times, and the mean and standard deviation of the area under the receiver operating curve (AUROC) are reported.\nFor the PESO dataset, randomly resized crops with scale [0.01, 0.2] are randomly sampled from the 2,500x2,500 pixel region of interest images during fine-tuning, and an epoch is defined as 51,200 training samples. For Karolinska and Radboud datasets, training images are randomly sampled from 384x384 pixel tile images with 20% overlaps, and an epoch is defined as 262,144 training samples, or the maximum amount of tile images included in model training. When limiting the number of training images from the PESO dataset, region of interest images from 1, 2, 4, 8, 16 or 32 histological slides are used for training. When limiting the number of training data in the Karolinska and Radboud datasets, only 4,096, 8,192, 16,384, 32,768, 65,536, 131,072, 262,144 or 524,288 randomly selected tile images, or all 972,800 and 782,336 images in the Karolinska and Radboud datasets, are used for training. In both training data limitation experiments, the last two encoder blocks are fine-tuned.\nFor augmentations and transformations, strong-augment (0.1.0) and albumentations (1.3.0) [6] Python packages were used."}, {"title": "2.4.1. Prostate cancer-specific death survival models", "content": "To create survival models using HistoEncoder features, we first cluster the HelsinkiTMA dataset and obtain histological cluster memberships for each patient as described in Sec. 2.2.2 with the prostate-m encoder and number of clusters set to 32.\nPenalized Cox proportional hazards models are then used to predict prostate cancer-specific death, with a penalizer of 0.001 and an L1-ratio of 0.5. From the 32 patient-level clusters, six clusters are selected based on a parameter importance analysis. All models are then trained on the patient Gleason grade, CAPRA-S or MSKCC-S, with or without HistoEncoder cluster features. Each model is trained 1,000 times, where 25% of the samples are set aside as a test set using stratified random splits. Model performance is evaluated with a concordance score, time-dependent AUC score between 1 and 23 years, and a net benefit [62] curve for predicting prostate cancer-specific death at 15 years."}, {"title": "3. Results", "content": ""}, {"title": "3.1. Fine-tuning a prostate cancer classifier", "content": "In this section, we report the performance of both prostate-s and natural-s encoder models in classifying prostate cancer in tissue images. The models are fine-tuned using either the PESO, or Karolinska and Radboud datasets. Both encoder models are fine-tuned while limiting either the number of fine-tuned parameters or the amount of training data.\nWhile limiting the number of fine-tuned parameters (Fig. 2a and Supplementary Fig. 6a), prostate-s based models significantly outperform natural-s based models in all evaluation datasets, regardless how many parameters are fine-tuned. Prostate-s based models also saturate quickly and only require minimal fine-tuning to achieve the best performance. Prostate-s based models seem to overfit to the training datasets (Supplementary Fig. 6a), and would likely benefit from higher regularisation.\nWhile limiting the number of training images (Fig. 2b and Supplementary Fig. 6b), prostate-s based models significantly outperform natural-s based models in all evaluation datasets. Prostate-s based models achieve comparable or better performance with only 16 regions of interest crops from four distinct histological slides, when compared to natural-s based models trained on the full PESO dataset (Fig. 2b). In particular, prostate-s based models achieve significantly better performance with only 1,024 tile images when compared to natural-s based models trained with all 972,800 and 782,336 tile images in the Karolinska and Radboud datasets (Supplementary Fig. 6b), respectively. Prostate-s based models also saturate quickly with diminishing returns after 8,192 tile images.\nRemarkably, a KNN-classifier fitted with features extracted from prostate-s outperforms a fully fine-tuned natural-s model on all evaluation datasets (Fig. 2a and Supplementary Fig. 6).Prostate-s outperform models trained in a supervised manner with ImageNet data (Supplementary Tab. 2,3, and 4)."}, {"title": "3.2. Identifying histological patterns as HistoEncoder feature clusters", "content": "Tissue image features derived from the prostate-s model create clusters that clearly separate benign from cancerous epithelium (Fig. 3). This representation also allows differentiating cancer grades (Gleason). Conversely, features extracted with the natural-s model show a more diffuse pattern, making it harder to distinguish tissue types.\nTo quantify whether similar features are extracted for tile images with similar histological patterns, the prostate-m encoder is used to automatically label the Karolinska and Radboud datasets with the annotation workflow (Sec. 2.2.1). After assigning each tile image to a cluster, the proportion of different labels in each feature cluster can be assessed. In the Karolinska dataset, 45.6% of all tile images, 53.3% of cancerous epithelium, and 42.4% of benign epithelium and stroma are contained in clusters with a label purity above 90% (Fig. 4). In the Radboud dataset, a clear majority of images belong to high-purity clusters (68.8% all tiles, 74.7% cancerous epithelium, and 63.9% benign epithelium and stroma). In total, 2.1 million 384 x 384 tile images in the PANDA dataset could be classified into cancerous and benign tissue with over 90% accuracy after visually inspecting only 256 tile image clusters.\nAnnotations in the Radboud dataset are more granular than those in the Karolinska dataset, distinguishing between stroma and benign epithelium. In the Radboud dataset, 68.0% of all tile images, 65.1% of stroma, and 70.5% of epithelium are contained in clusters with a label purity above 90% (Fig. 4b). For the harder task of classifying epithelium tissue as cancerous or benign, 42.9% of all tile images, 51.4% of cancerous, and 30.5% of benign epithelium are contained in clusters with a label purity above 90%. Varying the number of annotated clusters, we observe that as few as eight clusters are enough to find clusters with label purity over 90% (Supplementary Fig. 8)."}, {"title": "3.3. Predicing prostate cancer-specific mortality", "content": "We next combine information extracted with HistoEncoder from histological slide images with clinical data to build multimodal survival models predicting prostate cancer-specific death. The prostate-m encoder is used to collect patient-level feature cluster frequencies (n = 32 clusters; Sec. 2.2.2) for the tissue microarray spots in the HelsinkiTMA dataset. These cluster frequencies represent the histological pattern distribution for each patient and can be simply concatenated with clinical data. Next, six feature clusters are selected and Cox proportional hazards models are fitted (see Sec. 2.4.1). A visual assessment of the top six feature clusters suggests more atypia in the cellular morphologies in clusters associated with worse prognoses (Supplementary Fig. 9).Certain subtypes and growth patterns, such as cribriform gland architecture and mucinous pattern are recurrent in the clusters with the highest HRs. Extracellular matrix formation, individual cell clusters and lymphocyte composition seem to vary even between the top six clusters.\nIn 1,000 random stratified splits, survival models augmented with the patient-level cluster frequencies achieve higher concordance scores than the baseline models in 84.9%, 89.2%, and 67.4% of the splits with Gleason grade, CAPRA-S and MSKCC-S, respectively (Fig. 5, left). HistoEncoder-augmented models also achieve higher mean time-dependent AUC scores over a 1 to 23-year period (Fig. 5, center). Finally, the augmented models yield a higher net benefit predicting prostate cancer death at 15 years over a wide threshold probability (Fig. 5, right)."}, {"title": "4. Discussion", "content": "In this work, we introduce HistoEncoder, a foundation model for prostate cancer digital histopathology. Foundation models hold significant promise to contribute to precision cancer medicine. While pre-training a foundation model takes considerable computational resources, the model can then be fine-tuned to a multitude of specific tasks [59, 64, 67]. Importantly, fine-tuning often requires orders of magnitude less data and resources than pre-training [31, 32, 64].\nWe pre-trained HistoEncoder with 48 million tile images extracted from prostate tissue images and showed how the model could distinguish malignant and benign tissues, Gleason grades, as well as stromal and epithelial tissues without any annotated information on tissue types being available during training. Previously, foundation models for digital histopathology have been typically pre-trained with natural images [27, 38, 57]. In this study, we demonstrated how pre-training with domain-specific images yielded substantially better performance than using natural images for prostate histopathology tasks. In line with our findings, recent foundation models trained on tissue images have had excellent performance in digital histopathology downstream tasks such as cancer detection and survival prediction [63, 65]. These models have however not challenged existing cancer-specific clinical prediction models to the extent that we address prostate cancer.\nHere, we demonstrated extending and applying HistoEncoder in two clinically relevant tasks. First, we created a classifier which achieved high performance in predicting the presence of prostate cancer. We observed significantly better prediction accuracy, and compute and data efficiency with HistoEncoder models trained with tissue images compared to identical models trained with natural images, also in unsupervised cancer grading task. Second, we applied HistoEncoder in the integration of tissue imaging and clinical data to compare with commonly used nomograms for prostate cancer-specific mortality prediction. This approach resulted in a model which is able to consistently and robustly outperform the commonly applied risk classification systems in clinical use (i.e., Gleason grading, CAPRA-S and MSKCC-S systems). Notably, only a handful of annotated cases and a personal computer were sufficient to fine-tune the HistoEncoder models for this purpose.\nPreviously, Pinckaers et al. developed a CNN model trained on biochemical recurrence (BCR) outcomes in a case-control setting [45]. A biomarker derived from this ImageNet-pretrained ResNet50-D model demonstrated significant predictivity in a multivariable model including preoperative PSA, ISUP Gleason grade, pathological stage and surgical margin status. In an external validation cohort, however, the conventional ISUP Gleason grade outperformed the biomarker. This highlights the challenge of developing generalizable models for real-world clinical applications. Despite extensive data augmentation, models pre-trained with natural images may struggle to learn features that would consistently surpass Gleason's grading across diverse patient cohorts.\nIn contrast, our results suggest that HistoEncoder learns visualisable and comprehensible features (Supplementary Fig. 9) beyond Gleason-grade patterns that are useful in clinical tasks such as predicting prostate cancer mortality. There is a plethora of previous work showing that explainable features beyond conventional histopathological classification systems are prognostic or predictive of cancer-patient outcome [10, 19, 60]. However, reports visualizing comprehensible AI-derived features for a human expert and clinically meaningful multimodal approaches are scarce. Here we showed that HistoEncoder was able to learn such image features without expert guidance or labeled data.\nOne of the weaknesses of our study is the lack of an external validation cohort in the survival analysis, and therefore it remains to be validated whether these features would be present also in other cohorts. Future efforts should hence evaluate whether HistoEncoder is able to extract coherent, predictive features across multiple cohorts, as this could yield a clinically applicable approach to stratify patients to subgroups based on survival probability beyond Gleason grades. Ultimately, complex machine learning models need to provide additional clinical value beyond current classification systems such as Gleason scoring, preferably with explainable features, in order to be adopted in everyday clinical practice."}, {"title": "4.1. Extending HistoEncoder", "content": "We provide HistoEncoder as an easy-to-use Python package containing both the models pre-trained with prostate tissue images, as well as a standalone software tool to extract features and cluster tissue tile images. HistoEncoder can thus be extended to specific tasks involving any histopathological images.\nAs proof of concept, we were able to address cancer detection and mortality prediction with HistoEncoder using limited data and computing resources from a single laptop computer. This exemplifies the utility of such foundation models in lowering the barrier to creating clinical software tools. The prostate-s and prostate-m encoder models were able to compress information from large tissue areas into a feature vector, which was subsequently combined with other data modalities. If the other data modalities, such as spatial transcriptomics data [55] or TMA spot labels, contain information for small tissue regions, feature vectors can be directly extracted for these regions with HistoEncoder models.\nTaken together, our results demonstrate how HistoEncoder can provide clinically relevant insights from tissue images. We also highlight the importance of training data representing domain-specific data. Foundation models such as HistoEncoder allow computational methods to be quickly developed for precision cancer medicine tasks with small amounts of domain-specific data. In this study, we did not evaluate whether HistoEncoder would be useful as a foundation for models involving other cancers than prostate cancer, or in a pan-cancer setting. An exciting direction would be to explore the utility of foundation models in the analysis of multiple clinically relevant modalities and as part of multimodal predictor and interpreter models [42, 50, 69]. It is crucial to evaluate HistoEncoder against tissue-agnostic foundation models pre-trained specifically on histopathology images [13, 63, 65]. Future multimodal models are likely to allow leveraging tissue imaging on a large scale in conjunction with other clinical and molecular profiling data, leading to improved tools for cancer diagnosis, prognosis and treatment."}]}