{"title": "KAAE: Numerical Reasoning for Knowledge Graphs via Knowledge-aware Attributes Learning", "authors": ["Ming Yin", "Qiang Zhou", "Zongsheng Cao", "Mei Li"], "abstract": "Numerical reasoning is pivotal in various artificial intelligence applications, such as natural language processing and recommender systems, where it involves using entities, relations, and attribute values (e.g., weight, length) to infer new factual relations (e.g., the Nile is longer than the Amazon). However, existing approaches encounter two critical challenges in modeling: (1) semantic relevance-the challenge of insufficiently capturing the necessary contextual interactions among entities, relations, and numerical attributes, often resulting in suboptimal inference; and (2) semantic ambiguity-the difficulty in accurately distinguishing ordinal relationships during numerical reasoning, which compromises the generation of high-quality samples and limits the effectiveness of contrastive learning. To address these challenges, we propose the novel Knowledge-Aware Attributes Embedding model (KAAE) for knowledge graph embeddings in numerical reasoning. Specifically, to overcome the challenge of semantic relevance, we introduce a Mixture-of-Experts-Knowledge-Aware (MoEKA) Encoder, designed to integrate the semantics of entities, relations, and numerical attributes into a joint semantic space. To tackle semantic ambiguity, we implement a new ordinal knowledge contrastive learning (OKCL) strategy that generates high-quality ordinal samples from the original data with the aid of ordinal relations, capturing fine-grained semantic nuances essential for accurate numerical reasoning. Experiments on three public benchmark datasets demonstrate the superior performance of KAAE across various attribute value distributions.", "sections": [{"title": "1 INTRODUCTION", "content": "Knowledge Graphs (KGs) serve as structured representations of real-world facts and knowledge, playing a pivotal role in powering a wide array of downstream applications, such as recommender systems (RS) [8, 16, 22], natural language processing (NLP) [15, 23, 28] and multimodal tasks[21, 25, 39]. Among them, numerical reasoning within KGs emerges as a crucial capability, aimed at identifying and inferring missing numerical relations or hierarchical orders among entities.\nExisting methods typically integrate additional attributes of entities into traditional knowledge graph embedding (KGE) models, leveraging entities with rich attribute information for numerical reasoning tasks. Others embed numerical entities directly into a vector space or use Graph Neural Networks (GNNs) to enhance KGE performance. However, these approaches often face critical limitations:\nC1 Semantic relevance: Most current models fail to capture the intricate interactions among entities, relations, and numerical attributes sufficiently. This deficiency results in suboptimal inference outcomes, as these models do not account for how the significance of numeric attributes changes depending on the relational context. For example, in addressing the query (LeBron James, Taller_Than, ?), the height attribute is"}, {"title": "2 RELATED WORK", "content": "Knowledge Graph Embedding(KGE). KGE amis to capture the latent representations of entities and relations in KGs. TransE[6] and its extended models [17, 40, 45], focus on treating relation as a \"translation\" from the head entity to the tail entity. DistMult[12] represent each relation with a diagonal matrix, ComplEx [33] extends DistMult to complex space. ConvE[11] extracts deep features of head entity and relation based on 2D convolution. ConE[2], MuRP[3], GIE[9] and AttH[10] embed KGs into hyperbolic spaces to model hierarchical relations. Further, models [26, 27, 34] of GNNs have been proposed to model higher-order connectivity in knowledge graphs. However, these models do not consider numerical values, making it difficult to accomplish numerical reasoning tasks.\nKnowledge Graph Embedding with Numerical Attributes. To improve the performance of KGE, several KGE models have been proposed to incorporate auxiliary information about the entity such as literals and numerical attributes (e.g., age, weight, and born_year). For example, KBLRN [14] accomplish the KGE task based on numerical values, considering numerical differences between different entities. MT-KGNN[31] and TransEA[41] are models for multi-task learning to predict both numeric attribute values and entity/relation embeddings. LiteralE [20] combines the entity embedding with numeric attributes based on a learnable gating function. Deterministic stand-alone value representation methods including DICE[30], NEKG[13] and NRN[1] are also used to predict numerical attributes in KGs. Moreover, RAKGE[19] learns the representation for attribution with relations. However, these methods do not consider the joint information among entities, attributes, and relations. On the contrary, our KAAE model can exploit joint complex interactions among entities, relations and numerical attributes to fully mine the semantic information useful for numerical reasoning tasks.\nContrastive Learning. Contrastive learning (CL) is an effective self-supervised learning method by pulling semantically close neighbors together while pushing non-neighbors away, and some models have attempted to utilize contrastive learning to improve representations of entities/graphs. SimGCL [43] adds uniform noise to the entity representation, which is an expansion-free CL method. HeCO[38] employs contrast learning in network schema and metapath, capturing both local and global features of entities. SLICE[37] makes use of mutual attraction between closed nodes to learn subgraph representations. RAKGE[19] interacts with training samples with head entities to generate samples. However, none of the above methods can guarantee effectiveness in numerical reasoning tasks. To improve contrast learning, we optimize the sampling process with ordinal relations in KGs and generate high-quality ordinal positive/negative samples based on the technique of similarity sampling, thus achieving significant performance improvements."}, {"title": "3 METHODOLOGY", "content": "In this section, we present a new KG-based numerical reasoning framework called KAAE from the semantic joint interaction perspective. Specifically, there are two main components: Mixture-of-Experts-Knowledge-Aware Encoder (MoEKA-Encoder) and ordinal knowledge contrastive learning (OKCL) strategy. The overall framework of KAAE is illustrated in Figure 2."}, {"title": "3.1 Knowledge-Aware Learning", "content": "To tackle the semantic relevance of KGs, we propose a MoE-knowledge-aware encoder consisting of four parts: the numeric value embedding layer, a mixture of knowledge experts, the knowledge perceptual attention layer, and the knowledge aggregation layer.\nNumeric Value Embedding Learning. To learn the numeric values, for observable scalars, we use a learnable embedding matrix to map them to a vector space. For missing scalars, we use learnable special missing value embeddings to preserve the magnitude information and prevent the task from being threatened, instead of using fixed values such as zero: $o_m = (W_m + w_m \\times X_{im}) v_m$, where $X_{im}$ is the corresponding numeric value of the entity i. $v_m \\in \\mathbb{R}^{d_{att}}$ represents the embedding vector of the numeric attribute field m, $d_{att}$ stands for the embedding dimension of attribute. $w_m \\in \\mathbb{R}^{d_{att}}$ and $W_m \\in \\mathbb{R}^{d_{att} \\times d_{att}}$ are linear transformations that learn the context information between each numeric value and attribute. The operation symbol represents point-wise multiplication. We denote $o_m \\in \\mathbb{R}^{d_{att}}$ as the m-th field attribute embedding of the entity i. In this way, we can learn a more robust representation of both existing/missing numerical attributes.\nMixture of Knowledge Experts. To better learn entity embeddings from different perspectives with relation contexts and numerical attributes, we first employ a module called Mixture of Knowledge Experts to build expert networks, where each perspective corresponds to an expert network. First, the entity $i \\in \\mathcal{E}$ has a raw feature $\\bar{e}_i$. We then learn the multi-perspective embeddings $H^1_i, H^2_i, ..., H^k_i$ for the entity i by establishing K knowledge experts denoted as $W_1, W_2, ..., W_K$. This process can be represented as $H^k_i = W_k(\\bar{e}_i)$. Next, we design a semantic-guided gated fusion network (SGN) to facilitate the fusion of inter-perspective entity embeddings with relation guidance.\n$e_i = \\bigoplus_{k=1}^K F_k(H^k_i, r)$         (1)\nwhere $e_i \\in \\mathbb{R}^{d_{emb}}$ is the output entity embedding of entity i for relation r, $F_k$ is the weight for each expert calculated by the GFN:\n$F_k (H^k_i, r) = \\frac{exp((U(\\tilde{H^k_i}) + \\psi_k)/\\rho(\\epsilon_r))}{\\sum_{j=1}^K exp((U(\\tilde{H^k_i}) + \\psi_j)/\\rho(\\epsilon_r))}$,         (2)\nwhere $k \\sim \\mathcal{N}(0, U'(\\tilde{H^k_i}))$, U and U' are two projection layers, and $\\psi_k$ is tunable Gaussian noise used to balance the weights for each expert and enhance the model's robustness. Additionally, we"}, {"title": "3.2 Ordinal Knowledge Contrastive Learning", "content": "Learning ordinal relations is essential for numerical reasoning tasks. Specifically, for numerical reasoning, data consists of continuous scalar values, resulting in numerical values that are extremely sensitive to minimal differences near boundary values, thus affecting the accuracy of ordinal embedding. Unlike the previous work mainly focusing on generating valid, positive, and negative samples, we turn to a more difficult and significant task that generates high-quality ordinal samples. For example, consider the head entity with a weight attribution of 80kg and the relation is_heavier_than. The ideal learning range consists of generating all possible positive tail entities weighing less than 80kg. It is particularly crucial to distinguish boundary values(e.g., tails between 79 and 81kg) more efficiently.\nOrdinal Relation Learning. Given N objects and the ordinal relationship triples $S = \\{(a, b, c) | a, b, c \\in \\mathcal{E}, a \\neq b \\neq c\\}$, where $[N] = \\{1, ..., N\\}$ and object a is more similar to object b than it is to object c. Given some distance function $d(., .)$, we aim to learn the representations of objects, denoted as $\\{x_1, x_2, ..., x_N\\}$, such that\n$d(x_a, x_b) < d(x_a, x_c), \\forall (a, b, c) \\in S$.         (8)\nKnowledge Samples Generator. On the analysis above, to distinguish precise semantics in numerical reasoning, we generate the preliminary positive/negative ($\\mathcal{E}^+$ and $\\mathcal{E}^-$) samples based on the available head entities and relations, the set of positive/negative samples is as follows:\n$\\mathcal{E}^+ = \\{e^{att}_{joint,i} | i \\in P[h, r]\\}, \\mathcal{E}^- = \\{e^{att}_{joint,j} | j \\in N[h, r]\\},         (9)$\nwhere P[h, r] represents the set of positive tails containing entities whose weight is less than 80kg (e.g., 65, 70, and 75kg). N[h, r] represents the set of positive tails containing entities whose weight is more than 80kg (e.g., 85, 90, and 95kg).\nOrdinal Samples Extractor. Since each attribute field (e.g., year, age, weight) has its unique distribution, we select k number of samples with the highest cosine-similarity to $e^{att}_{joint,i}$ from all the generated positive/negative samples with the assistance of Eq.(8). Note that this step is essential and it can generate the samples obeying ordinal embedding. Then we generate high-quality ordinal samples as follows:\n$e^{mix, +}_{joint, h} = \\alpha \\cdot \\sum_{p \\in Topk(\\mathcal{E}^+)} e^{att}_{joint,p} + (1-\\alpha) e^{att}_{joint,h},$         (10)\n$e^{mix, -}_{joint, h} = \\beta \\cdot \\sum_{n \\in Topk(\\mathcal{E}^-)} e^{att}_{joint,n} + (1-\\beta) e^{att}_{joint,h},$         (11)\nwhere $\\alpha, \\beta$ are blending coefficients that are sampled from the uniform distribution [0, 1]. $Topk(\\mathcal{E}^+)$ and $Topk(\\mathcal{E}^-)$ are sampling sets with the highest cosine similarity to $e^{att}_{joint,i}$ in $\\mathcal{E}^+$ and $\\mathcal{E}^-$, respectively. In this sense, to satisfy ordinal relation, we have:\n$d(e^{att}_{joint,h}, e^{att}_{joint,p}) < d(e^{att}_{joint,h}, e^{att}_{joint,n})$"}, {"title": "3.3 Training", "content": "Score Function. A classical assumption of typical KGE methods is to compute the distances between head entities, relations, and tail entities. However, in numerical reasoning tasks, these methods suffer from serious drawbacks because they do not effectively address the order embedding problem posed by relational patterns in complex tasks. For example, given the head entity (weight=80kg) and its relation (is_heavier_than), and using the TransE score function, the two tail entities $e_1$ (weight=70kg) and $e_2$ (weight=60kg) are mapped to the same location, failing to express a numerical ordering relation such as 80>70>60, which impedes numerical reasoning. To address this issue, we design a rationality score function specifically for the numerical reasoning task as follows:\n$score(e^{att}_{joint,h}, e_r, e^{att}_{joint,i}) = \\varepsilon - ||e^{att}_{joint,h} + e_r - e^{att}_{joint,i}||_{1/2} + \\Delta ||max(0, W_re^{att}_{joint,h} - W_re^{att}_{joint,i})||^2,$         (13)\nwhere $\\Delta$ represents the weight in the numerical reasoning score function. The first term is derived from TransE (Eq. (17)), while the second term modifies the Order-embedding approach (Eq. (18)). $W_r \\in \\mathbb{R}^{d_{emb} \\times d_{emb}}$ is a projection matrix critical for mapping entities to a specific relational space, indicating that depending on the type of relation, each entity may assume a different order.\nLoss Function. In this paper, we use the binary cross-entropy loss that presented in ConvE[11] and LiteralE[20] so that we can convert the score of each triple obtained based on Eq.(13) to a probability by using a sigmoid function.Let $T = \\mathcal{G} \\cup \\mathcal{G}^-$ denote the training dataset, where $\\mathcal{G}$ denotes the set of positive knowledge triples, $\\mathcal{G}^-$ denotes the set of negative knowledge triples $\\{(h, r, t')|h, t' \\in \\mathcal{E}, r \\in \\mathcal{R}, (h, r, t') \\notin \\mathcal{G}\\}$. The binary-cross entropy loss is defined as follows:\n$L_{BCE} = -\\frac{1}{|\\mathcal{T}|} \\sum_{l \\in \\mathcal{T}} (y \\log(p_l) + (1 -y) \\log(1 - p_l)),        (14)$\nwhere $y \\in \\{0, 1\\}$ is the truth label, and $p_l \\in [0,1]$ is the probability of each triple (h, r, t) = l \\in T, which is formulated as Eq.(13). Combining Eq. (12) and Eq. (14), the final loss can be summarized as follows:\n$L_{total} = L_{BCE} + \\lambda L_{CL},        (15)$"}, {"title": "4 EXPERIMENTS", "content": "4.1 Experimental Settings\nDatasets. We use three real-world KG datasets to evaluate the proposed model on a numerical reasoning task. Specifically, US-Cities\u00b9 is a large-scale city dataset containing basic information about several cities in the United States, including more than 70 attribute fields (e.g., household income, household education level, dimension, etc). Spotify is a knowledge graph provided by Spotify for developers\u00b2. Credit is a knowledge graph constructed from default events in Taiwan [42]. Each credit card holder corresponds to an entity with numerical information about the credit card (e.g., total_delay, total_bill) as the numerical attribute. Additionally, it is worth noting that all three of the above datasets are modified to simulate numerical reasoning tasks under real conditions. Close to 20% of the numerical values are masked to zero (missing values) while using as few triples as possible for the training set. The remaining triples are used for evaluation. The statistics of the dataset can be found in Appendix C.\nEvaluation Metrics. We used the improved learning methods based on dropout, batch normalization, and linear transformation proposed alongside LTE [46] for evaluation. Mean Reciprocal"}, {"title": "4.2 Experimental Results", "content": "Performance Comparison. To address these challenges, we introduce a new model termed KAAE (Knowledge-Aware Attributes Embedding), crafted for numerical reasoning via knowledge graphs. Specifically, to tackle semantic relevance, we utilize the Mixture-of-Experts (MoE) framework [5] to integrate semantic-guided knowledge experts, constructing expert networks for entities across various relational contexts. Additionally, we implement a joint interaction framework that leverages appropriate contexts and captures the intricate connections between entity-relation pairs and attributes within a unified vector space. Employing this knowledge-aware mechanism, our model discerns the significance and relevance of each attribute, effectively synthesizing the latent semantics"}, {"title": "4.3 Experimental Analysis", "content": "Ablation Study. We conduct an ablation experiment to evaluate the performance of different variants of KAAE. Initially, we set $\\delta_e$ in Eq.(3) to 0 to disable the entity-aware module. This modification allows the model to perceive numerical attributes solely through a relation-aware approach, without considering the embeddings of entities associated with the attributes. We also test the performance of KAAE without a full understanding of the entity semantics through the MoE module. Furthermore, we train the model without incorporating the top-k ordinal sampling methodology (Eqs. (10-11)) into the contrastive learning framework, which is essential for generating high-quality training samples and enhancing the model's ability to discern discrete numerical boundaries. The results, as detailed in Table 3, indicate that the omission of any single module results in a decline in model performance. This finding emphasizes the critical role of making the most of entity-related information in numerical reasoning tasks and the significance of high-quality ordinal samples in contrastive learning."}, {"title": "3.4 Theoretical Analysis", "content": "To clarify how our model differs from these models, we compare our model with some popular models in Table1.\nTime Complexity Analysis. KAAE consists of three components, the MoEKA encoder, a process of contrastive learning, and the score function. In terms of MoEKA Encoder, the time complexity of processing every triple is O(|M|). The time complexity of the numerical value embedding layer is O(|M|) because we transform each attribute field into an embedding vector. In the joint perceptual attention layer, the query is the target relation and the number of keys and values corresponds to the number of attribute fields, so the time complexity remains O(|M|). The gating layer of the MoEKA Encoder is not involved in the calculation of time complexity. During the process of contrastive learning, for every given triple(h,r,t), the time complexity is proportional to the number of positive samples |P[h, r]| and the number of negative samples |N[h, r]|. The time complexity is O(|M| \u00b7 rel_num), where rel_num denotes the number of relations per entity. The TransE score function involves only addition and subtraction operations, so it does not change the time complexity. Therefore, the overall time complexity of KAAE is O(T(|M|+|M|\u00b7rel_num)) \u2248 O(|T|\u00b7rel_num), where T denotes the number of positive and negative triples used for training.\nThe Expressiveness of Our Model. We conduct the expressiveness of numerical reasoning from the perspective of a one-hop reasoning task. Specifically, we aim to theoretically analyze the process of contrastive learning in KAAE and establish theoretical guarantees for the downstream performance of the learned representations. In the numerical reasoning, we denote the set of node representations as V, and define the mean representations from one-hop neighborhoods of node v as $z_v = \\frac{\\sum_{u \\in N(v)} u}{|N(v)|}$, where u represents the projected node representations of positive samples of node v, $z_v$ describes the one-hop neighborhood pattern of node v. In the world, nodes belonging to the same semantic class tend to have similar neighborhood patterns, so $z_v$ can be viewed sampled from $Z|Y \\sim \\mathcal{N}(z_y, I)$ where Y is the latent semantic class indicating the one-hop pattern of node v. We demonstrate that minimizing KAAE's objective in Equation (12) with an exponential moving average is equivalent to maximizing mutual information between representation V and the one-hop pattern Y, which explains the"}, {"title": "5 CONCLUSION", "content": "In this paper, we introduce a new KGE model called KAAE. Specifically, to address the issue of semantic relevance, KAAE incorporates a joint interaction process that simultaneously captures the connections among entities, relations, and attributes within a unified framework. To improve semantic precision, we introduce a novel contrastive learning approach that enhances the diversity of training samples by optimizing ordinal samples based on cosine similarity and an entity-blending technique. Our KAAE model effectively enhances numerical reasoning in KGs. Extensive experiments on standard benchmarks such as US-Cities, Spotify, and Credit demonstrate KAAE's effectiveness."}, {"title": "A PRELIMINARIES", "content": "A.1 Knowledge Graph Embedding Methods\nOur model is based on the existing KGE method to model and evaluate the plausibility of the triples. Considering the continuity and scheduling problem of the numeric values, in this paper, we use TransE[6] and Order-embedding[36]. $e_h$, $e_r$, and $e_t$, denote the embedding of head entities, tail entities, and relations respectively.The reasonableness score of the triple $(e_h, e_r, e_t)$ is obtained as follows.\nTransE. A translation-based method that was designed to optimize the formula $e_h + e_r \\approx e_t$.\n$score(e_h, e_r, e_t) = \\varepsilon - ||e_h + e_r - e_t||_{1/2},        (17)$\nwhere $\\varepsilon$ is a hyperparameter and $|\\cdot|$ can be L1 or L2 distance.\nOrder-embedding. Encoding the hierarchical and structural information contained in the numerical attributes of entities into the model with certain sequences to improve the model's ability to process the data and get a better understanding of the semantics and structure of the knowledge. For numerical reasoning tasks, the top-ranked attributes usually have larger values(e.g.,80kg>70kg>60kg). To improve the efficiency and accuracy of the model, we propose the idea based on the score function of KGE as follows.\n$score(e_h, e_t) = \\varepsilon - ||max(0, e_t - e_h)||_2$         (18)\nwhere $\\varepsilon$ is a hyperparameter."}, {"title": "B THE DETAILS OF PROOFS", "content": "B.1 The Loss of Contrastive Learning\nWe provide details of the derivations of KAAE in Eq. (12) and show that minimizing KAAE is approximately to minimize the combination of prediction and uniformity losses.\n$L_{CL} = -\\frac{1}{|V|} \\sum_{v \\in V} \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)}$ $\\log \\frac{\\exp(p^T u / \\tau)}{\\exp(p^T u / \\tau) + \\sum_{v' \\in V} \\exp(v^T v' / \\tau)}$\n$=-\\frac{1}{|V|} \\sum_{v \\in V} \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)}$ $ \\frac{p^T u}{\\tau}$\n$ + \\log \\bigg( \\exp(\\frac{p^T u}{\\tau}) + \\sum_{v' \\in V} \\exp(\\frac{v^T v'}{\\tau}) \\bigg)$         (19)\n$\\approx -\\frac{1}{|V|} \\sum_{v \\in V} \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)}$ $ \\frac{p^T u}{\\tau}$ + \\log \\bigg( \\sum_{v' \\in V} \\exp(\\frac{v^T v'}{\\tau}) \\bigg)$\n$\\approx -\\frac{1}{|V|} \\sum_{v \\in V} \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)}$ $ \\frac{p^T u}{\\tau}$ + \\log \\bigg( \\sum_{v' \\in V} \\exp(\\frac{v^T v'}{\\tau}) \\bigg)$\n$\\approx -\\frac{1}{|V|} \\sum_{v \\in V} \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)}$ $ \\frac{p^T u}{\\tau}$ + \\log \\bigg( \\frac{1}{|V|}  \\sum_{v'} \\exp(\\frac{v^T v'}{\\tau}) \\bigg) - \\log|V|$         (20)\n$= -\\frac{1}{|V|} \\sum_{v \\in V} \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)}$ $ \\frac{p^T u}{\\tau}$ + \\frac{1}{|V|}  \\sum_{v'} \\log \\exp(\\frac{v^T v'}{\\tau}) - \\log|V|$\n$\\approx -\\frac{1}{|V|} \\sum_{v \\in V} \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)}$ $ \\frac{p^T u}{\\tau} + \\frac{1}{|V|}  \\sum_{v' \\in V} \\frac{v'}{\\tau} + \\frac{1}{|V|} \\sum_{u \\in \\mathcal{N}(v)} ||p - u||_2^2$\n$=\\frac{1}{|V|} \\sum_{v \\in V} \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)} ||p - u||_2^2 - \\frac{1}{|V|} \\sum_{v \\in V} \\sum_{v' \\in \\mathcal{V}} ||v - v'||_2,        (21)$\nwhere the symbol $\\propto$ indicates equality up to a multiplicative and/or additive constant. Here, we utilize Jensen's inequality in Equation (20). Equation (21) holds because p and u are both normalized. Given the above, we can conclude that minimizing KAAE objective is approximately to minimize the combination of the prediction and uniformity losses."}, {"title": "B.2 Proof of Theorem", "content": "Minimizing KAAE's objective in Equation (12) with an exponential moving average is equivalent to maximizing mutual information between representation V and the one-hop pattern Y:\n$\\mathcal{L}_{CL} \\geq \\mathcal{H}(V|Y) - \\mathcal{H}(V) = -I(V;Y),         (22)$"}, {"title": "C MORE DETAILS OF EXPERIMENTAL SETTINGS", "content": "The following subsections describe the data statistics, hyperparameters, experimental setup, and additional experimental results.\nC.1 Dataset Statistics\nTo evaluate the performance of KAAE in this paper, we used the US-Cities, Spotify, and Credit datasets, and detailed information about the datasets is described in Table 7 below.\nBaseline. We classified various existing representation learning methods into six groups: Euclidean KGE including TransE[6], ConvE [11],"}, {"title": "D LEARNING PROCEDURE", "content": "The overall learning process of KAAE is shown in Algorithm 1. At the beginning, we initialize the loss. We then use the KA Encoder to obtain the header embedding $e^{att}_{joint,h}$ for each triple(h, r, t) in the training set (described in line 5). If r is a numerical comparison relation, we obtain the set $\\mathcal{E}^+$, $\\mathcal{E}^-$ of comparative positive and negative samples based on $e^{att}_{joint,h}$ (described in lines 7-8). Lines 9-10 show the process of generating high-quality blended positive/negative samples via Top-K sampling. Then, the contrast learning loss is accounted for in the total loss (line 11). Finally, the cross-entropy loss is added according to Eq(14), and finish the updating process."}, {"title": "E ADDITIONAL EXPERIMENTAL RESULTS", "content": "E.1 Feasibility Test\nTo test the ability of KAAE to learn values based on the magnitude of attribute numerical values, we conducted a feasibility experiment. We convert the values in the Spotify dataset to binary values to distinguish the presence or absence of numeric attributes. We assign a value of 1 to data that exists and a value of 0 to data that does not exist. The result is shown in Table 9. Our proposed model KAAE outperforms the competitors LiteralE [20] and RAKGE[19]. LiteralE did not show significant changes under the two experimental conditions. Both KAAE and RAKGE can compare the magnitude of the values, and KAAE performed better."}, {"title": "E.3 Loss Experiment", "content": "To verify the effectiveness of our proposed two loss functions, we compare the performance of KAAE trained with different loss functions. The results in Table 10 show that both loss functions are reasonable.\n$Loss1 = -\\log\\sigma(score(e^{att}_{joint,h}, e_r, e^{mix}_{joint,i}))$         (27)\n$Loss2 = -\\frac{1}{|G|} \\sum_{l \\in G} \\log \\frac{\\exp(p^Tu / \\tau)}{\\exp(p^Tu / \\tau) + \\sum_{v \\neq mix} \\exp(v^Tv' / \\tau)},        (28)$"}]}