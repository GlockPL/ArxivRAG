{"title": "MvKeTR: Chest CT Report Generation with Multi-View Perception and Knowledge Enhancement", "authors": ["Xiwei Deng", "Xianchun He", "Yudan Zhou", "Shuhui Cai", "Congbo Cai", "Zhong Chen"], "abstract": "Abstract\u2014CT report generation (CTRG) aims to auto-\nmatically generate diagnostic reports for 3D volumes, re-\nlieving clinicians' workload and improving patient care.\nDespite clinical value, existing works fail to effectively in-\ncorporate diagnostic information from multiple anatomical\nviews and lack related clinical expertise essential for ac-\ncurate and reliable diagnosis. To resolve these limitations,\nwe propose a novel Multi-view perception Knowledge-\nenhanced TansfoRmer (MvKeTR) to mimic the diagnostic\nworkflow of clinicians. Just as radiologists first examine\nCT scans from multiple planes, a Multi-View Perception\nAggregator (MVPA) with view-aware attention effectively\nsynthesizes diagnostic information from multiple anatom-\nical views. Then, inspired by how radiologists further refer\nto relevant clinical records to guide diagnostic decision-\nmaking, a Cross-Modal Knowledge Enhancer (CMKE) re-\ntrieves the most similar reports based on the query volume\nto incorporate domain knowledge into the diagnosis pro-\ncedure. Furthermore, instead of traditional MLPs, we em-\nploy Kolmogorov-Arnold Networks (KANs) with learnable\nnonlinear activation functions as the fundamental building\nblocks of both modules to better capture intricate diagnos-\ntic patterns in CT interpretation. Extensive experiments on\nthe public CTRG-Chest-548K dataset demonstrate that our\nmethod outpaces prior state-of-the-art models across all\nmetrics.", "sections": [{"title": "I. INTRODUCTION", "content": "In clinical practice, medical imaging plays an indispensable\nrole, serving as a cornerstone for disease diagnosis, report\nwriting, and subsequent decision-making. After acquiring a\npatient's radiology images, physicians examine all anatomical\nstructures and regions of concern, then employ related expert\nknowledge to write a hand-crafted, clinically coherent report\nthat documents the observations [1]. As evidenced in Fig. 1,\nsuch a radiology report typically comprises a findings section\nthat describes thorough medical observations, followed by an\nimpression summarizing the most significant observations. The\nprocess of report writing is time-intensive and error-prone for\nclinicians [2]\u2013[4]. Hence, the automation of report generation\nholds substantial value in lightening the burden of physicians\nand improving the quality of reports.\nDespite recent advancements, there remain several con-\nstraints in previous studies that have not been fully addressed:\n(i) Ineffective incorporation of diagnostic information from\nmultiple anatomical views. In comparison with 2D imag-\ning [5], [6], 3D medical imaging (e.g., chest CT) provides\na more holistic view of anatomical detail and spatial informa-\ntion, preserving the volumetric nature of the human body [7].\nFurthermore, unlike a single view of 3D volumes [8]\u2013[10],\nthe multi-view nature of CT scans enables physicians to ex-\namine the patient's condition from different anatomical views\naxial, sagittal, and coronal- allowing for a more accurate\ndiagnosis of many diseases. As a common example, pulmonary\nnodules, especially smaller ones, may be easily obscured by\nadjacent structures or misdiagnosed when viewed from a single\nperspective [11]. Axial views might reveal the presence of a\nnodule, but coronal and sagittal views are crucial for accurately\nassessing its characteristics and relationships to surrounding\ntissues [12], thereby enhancing differential diagnosis [13].\n(ii) Absence of pertinent medical knowledge crucial for pre-\ncise and dependable diagnosis. Relying merely on radiology\nimages for report writing may be sub-optimal, overlooking\nthe critical medical expertise [9], [14]. Experienced clinicians\nusually consult analogous case reports to extricate insights\ninto diagnostic patterns and imaging features, guiding their\ndecision-making when faced with complex or ambiguous cases\nduring the diagnostic process. This knowledge-driven ap-\nproach is critical for interpreting imaging findings, identifying\nsubtle abnormalities, and writing clinically accurate reports.\nMotivated by the aforementioned observations, we develop\na novel Multi-view perception Knowledge-enhanced Trans-"}, {"title": "II. RELATED WORK", "content": "Image captioning [16], [17] aims to describe an image\nwith a brief sentence. In image captioning, researchers mainly\nused rule-based methods to create image descriptions. Early\napproaches to image captioning predominantly adopted the\nencoder-decoder architecture [18]\u2013[20]. The emergence of\nTransformer [21] has substantially propelled progress in image\ncaptioning field. Xu et al. [22] created an attention-based\nmodel trained via a dual approach deterministic back-\npropagation and stochastic maximization of a variational lower\nbound. Li et al. [23] proposed GLA that generates relevant sen-\ntences by fusing object-level local representations and image-\nlevel global representations. Lu et al. [24] employed an adap-\ntive attention model with a visual sentinel, enabling dynamic\ndetermination of whether and where to focus on the input\nimages during sequential word generation. Cornia et al. [25]\ncompletely exploited multi-modal contexts and multi-level\ninformation using a meshed transformer with memory. Liu et\nal. [26] reframed image captioning as a sequence-to-sequence\nprediction problem and proposed a caption transformer model.\nThis transformer takes serialized input images and instills\nglobal contextual awareness across encoder layers from the\noutset while entirely eschewing convolutions. The techniques\nmentioned above have predominantly been explored within the\n2D image domain, with 3D image captioning methodologies\nremaining relatively sparse thus far."}, {"title": "B. Medical Report Generation", "content": "The generation task of radiology report can be regarded as\nthe derivative task of image captioning. Due to the encoder-\ndecoder architecture's notable success in image captioning,\nnumerous studies have utilized this method to produce medical\nimage reports [27]\u2013[30]. For example, Xue et al. [31] pro-\nposed MRMA, which circularly combines CNN and LSTM\nand constantly uses old sentences to generate new and co-\nherent reports. Because transformer can capture the global\nand long dependencies of the entire input sequence, many\nresearchers use it to improve the CNN-RNN structures, im-\nproving the accuracy of report generation and processing effi-\nciency. Nooralahzadeh et al. [32] introduced M2TR, which is a\nprogressive (from image to text to text) generative architecture\nthat first extracts global feature-level context representations,\nthen utilizes a large language model to generate comprehen-\nsive reports. Chen et al. [5] designed R2GenCMN, which\nemploys a shared memory matrix to capture the alignment\nbetween image and text, facilitating cross-modal interaction\nand generation through memory querying and retrieval. Yang\net al. [33] developed M2KT, which leverages a learnable med-\nical knowledge base and multi-modal alignment mechanism\nto comprehensively learn visual features, enabling automated\ngeneration of radiology reports. Yi et al. [34] implemented\nTSGET, which incorporates two global enhancement layers\nin the transformer. The first layer captures global visual\ncontext, while the second layer refines global and regional\nfeatures to obtain more comprehensive visual information.\nYi et al. [35] presented UDT, which obtains disease tag\nfeatures via clustering and then fuses these with the attended\nvisual features to assist report generation. Wang et al. [36]\nconstructed CAMANet, which designed a class activation map\nguided attention network that explicitly facilitates alignment\nand enhances the acquisition of disease-pertinent feature rep-\nresentations by harnessing aggregate class activation maps.\nThese methods are limited to 2D image domains and have\nnot been extended to 3D applications, researchers are now\nshifting their focus towards 3D report generation. Tang et\nal. [8] introduced SL-DG, a dual-module framework for CT\nscan report generation, consisting of a self-attention-based\nscan localizer (SL) for salient lesion feature extraction and\na dynamic generator (DG) for report recommendation and\nsynthesis. Hamamci et al. [9] established CT2rep, a framework\nthat generates 3D medical reports using a novel auto-regressive"}, {"title": "III. METHODOLOGY", "content": "In this section, we describe our proposed MvKeTR model in\ndetail. Table I illustrates the key notations and their respective\ndescriptions adopted in this study."}, {"title": "A. Overview of the Proposed Approach", "content": "The generation of radiology reports can be considered\nan image-to-text problem, for which we follow a sequence-\nto-sequence paradigm. In doing so, unlike previous ap-\nproaches [8], [10] that typically process a single-view im-\nage, our network handles the input 3D CT volume $X\u2208$\n$IR^{(224)\u00d7224x224}$ by transforming it into three source sequences\nof CT patches: $Z^{v}$ = {$z_1, z_2, ...,z_{Np}$},$z_i\u2208$ $R^{(8)\u00d78\u00d78}$, where\n$v \u2208 {a, c, s}$ represents the axial, coronal, and sagittal views\nrespectively, $Z^{v}$ are embedded CT tokens extracted by 3D\nvisual extractor for each view, and D is the dimension of the\nembedding. We regard the corresponding report as the target\nsequence $Y = {Y_1, Y_2, ..., Y_L }$, where $y_\u03b9 \u2208 V$ are the predicted\ntokens, L the length of predicted tokens and V the vocabulary\nof all possible tokens. Thus, the aforementioned process of\nreport generation can be formalized as follows:\n$log p(Y|X) = \\sum_{l=1}^{T}log p (Y_l|y_1,..., y_{l-1}, X)$                                                                     (1)\nSubsequently, the model is trained to maximize p(Y|X) by\nthe negative conditional log-likelihood of Y given X:\n$\\gamma^* = arg \\max_\\gamma  \\sum_{l=1}^{T}log p (y_l|y_1, ..., y_{l-1}, X; \\gamma)$                                                                                                                   (2)\nwhere $\\gamma$ is the parameters of the model. The overall archi-\ntecture of our proposed framework is illustrated in Fig. 2,\nwhere the contributions of each component are detailed in the\nfollowing subsections."}, {"title": "B. 3D Visual Extractor", "content": "Following the previous work [9], CT-ViT is employed to\nextract embedded CT tokens $Z^{v} \u2208$ $[R^{8\u00d78\u00d78\u00d7512}$, by initially\nextracting $(28) \u00d7 28 \u00d7 28$ non-overlapping patches from\nthe given 3D CT volume $X^{v}$, where $v \u2208 {a,c,s}$. Then,\neach patch is embedded in a D-dimensional space, reshaped,\nand linearly transformed to an intermediate tensor $T_i\u2208$\n$R^{B\u00d7N_t\u00d7P_h\u00d7P_w\u00d7D}$ Here, $P_t$ represents the temporal patch\nsize, B the batch size, $N_t$ the temporal patch count, h and\nw correspond to the height and width of the CT slices, and\nD the dimension of the embedding space. $P_h$ and $P_w$ denote\nthe spatial patch sizes of height and width. Subsequently, $T_i$\nis processed and reshaped to the final outcome tensor $T_o\u2208$\n$R^{(P_t\u00b7P_h\u00b7P_w)\u00d7(B\u00b7N_t)\u00d7D}$ consecutively by the spatial and causal\ntransformer models. The 3D visual extractor is composed of\nthree independent CT-ViTs, which process the axial, coronal,\nand sagittal view CT volume, respectively. Fig. 3 illustrates\nthe pipeline of visual feature extraction through CT-ViT.\nThe whole procedure can be formally formulated as:\n$Z^{ve} = \\{z_1, z_2, ..., z_{Np} \\}= \u03a6_{ve} (X^a)$                                                                                                                                   (3)\n$Z^{ve} = \\{Z_1, Z_2, ..., Z_{Np} \\}= \u03a6_{ve} (X^c)$                                                                                                                                    (4)\n$Z^{ve} = \\{z_1, z_2, ..., z_{Np} \\}= \u03a6_{ve} (X^s)$                                                                                                                                     (5)\nwhere $N_p$ denotes the number of extracted CT patches, $\u03a6_{ve}(\u00b7)$,\n$P_{ve}(\u00b7)$, and $P_{ve}(\u00b7)$ denote the axial, coronal, and sagittal CT-\nViT, respectively."}, {"title": "C. KAN as Nonlinear Learner", "content": "Multi-layer perceptrons (MLPs) [37], [38] serve as basic\nblocks of contemporary deep learning models and prevail in\nnumerous tasks (e.g., radiology report generation). However,\nthese models often struggle with learning intricate nonlinear\npatterns in high-dimensional data, particularly in biomedical\napplications [39], where complicated interactions between\nfeatures are common.\nUnlike traditional MLPs that employ fixed activation func-\ntions at their nodes, KANs introduce learnable activation\nfunctions at the edges [15]. This simple modification enables\nKANs to achieve similar or superior accuracy in function-\nfitting tasks, even with fewer parameters than larger MLPs.\nGiven an input x, a KAN network is a stack of multiple KAN\nlayers(as illustrated in Fig. 4), which can be characterized as:\nKAN(x) = ($\u03a6_{k-1}\u25cb \u03a6_{k-2}\u25cb\u00b7\u00b7\u00b7 \u25cb \u03a6_1\u25cb \u03a6_0$) x                                                                                                     (6)"}, {"title": "D. Multi-View Perception Aggregator", "content": "The learnable activation functions & of the kth KAN layer can\nbe formalized as\n$\u03a6_k = {\u03c6_{i,j}}, i = 1,2,\u2026\u2026, N_{in}, j = 1,2\u2026\u2026, N_{out}$                                                                                                                     (7)\nwhere $\u03c6_{i,j}$ represents functions with trainable parameters,\nand $N_{in}$ and $N_{out}$ denote the input and output dimensions,\nrespectively. Then, the calculation of the KAN network from\nthe kth layer to the $(k + 1)^{th}$ layer can be represented as:\n$X_{k+1} = \u03a6_kX_k$                                                                                                                                                                                                  (8)\nHere, $\u03a6_k$ is defined as a matrix form:\n$\\begin{bmatrix} \u03a6_{k,1,1}(\u00b7) & \u03a6_{k,1,2}(\u00b7) & ... & \u03a6_{k,1,N_k}(\u00b7) \\\\ \u03a6_{k,2,1}(\u00b7) & \u03a6_{k,2,2}(\u00b7) & ... & \u03a6_{k,2,N_k}(\u00b7) \\\\ : & : & & : \\\\ \u03a6_{k,N_{k+1},1}(\u00b7) & \u03a6_{k,N_{k+1},2}(\u00b7) & ... & \u03a6_{k,N_{k+1},N_k}(\u00b7)  \\end{bmatrix}$                                                                                                                                                    (9)\nInspired by KANs' promising potential, we explore using\nKANs as an alternative to traditional MLPs in the design of a\nmulti-view perception aggregator and cross-modal knowledge\nenhancer."}, {"title": "E. Cross-Modal Knowledge Enhancer", "content": "Analogous to clinicians' diagnostic practice of consulting\nrelevant medical records, the cross-modal knowledge enhancer\nis developed to integrate medical expertise into report gener-\nation. The entire procedure can be elaborated upon below.\n1) Volume-to-Report Retrieval: As depicted in Fig. 6, we\nharness a well-trained CT-CLIP model on the CT-RATE\ndataset [40] to perform volume-to-report retrieval, which con-\nsists of an image encoder for CT volumes and a text encoder\nfor reports. Given an input axial CT volume $X^a$ and a \u0421\u0422\nreport bank $R = {R_1, R_2, ..., R_{N_r}}$ where $R_i$ represents the\ni-th radiology report, $N_r$ is the number of reports, we aim\nto retrieve the top-k most relevant report embeddings $Z_{top-k}$\nfrom R. Here, to maintain consistency with CT-CLIP's self-\nsupervised pre-training, we utilize axial CT volume as query\nimages for report retrieval.\nThe CT-CLIP image encoder $E_{image}$ maps the input volume\n$X^a$ to a D-dimensional embedding $v \u2208 R^D$:\n$v = E_{image}(X^a)$                                                                                                                                                                                                   (14)\nSimilarly, each report $R_i$ is encoded into a report embedding\n$r_i \u2208 R^D$ by the CT-CLIP text encoder $E_{text}$:\n$r_i = E_{text}(R_i)$                                                                                                                                                                                                (15)\nAfter encoding both the input CT volume and reports into\nthe shared embedding space, we apply L2 normalization to\nstandardize the embeddings and compute the cosine similarity\nbetween them to retrieve the most relevant report embeddings:\n$sim(v, r_i) = \\frac {v^T r_i}{||V|| ||ri||}$                                                                                                                                                                                                                                                                                                                                 (16)\n$Z_{top-k} = arg \\max_{r_i\u2208R'} top-k sim(v, r_i)$                                                                                                                                                        (17)"}, {"title": "F. Report Generator", "content": "To effectively integrate visual\nfeatures with retrieved medical knowledge, we introduce a\nknowledge enhancement mechanism. This mechanism em-\nploys a cross-attention module that enables the model to\nadaptively focus on relevant information from both the axial\nembedded CT tokens $Z^a$ and the retrieved report embeddings\n$Z_{top-k}$. Specifically, given the query Q, key K, value V\nmatrices, the cross-attention can be computed as follows:\n$CA_a = CrossAttention(Q, K, V)$                                                                                                                                                                                                                                                          (18)\n$= Softmax (\\frac {Q K^T}{\\sqrt{d_k}})V$\nwhere $Q = Z^QW_q, K = Z_{top-k}W_k, V = Z_{top-k}W_u, W_q$,\n$W_k$, and $W_u$ are learnable projection matrices. The output\nis further processed through residual connection and layer\nnormalization:\n$AN_a = Norm(CA_a + Z^a)$                                                                                                                                                                                                                                                                                                                               (19)\nThen, we process the enhanced features through a KAN layer\nfollowed by another residual connection and layer normaliza-\ntion:\n$F_{ke} = Norm(KANLayer(AN_a) + AN_a)$                                                                                                                                                                                                                                                                            (20)\nTo efficiently produce reports, a significant body of recent\nworks [5], [8]\u2013[10], [32]\u2013[36] adopt the encoder-decoder\narchitecture, which is built upon standard transformer. We\nfollow the R2GenCMN proposed by Chen et al. [5], which\nenhances the alignment between visual and textual modali-\nties by introducing cross-modal memory networks(CMN). In\ndetail, this network employs a learnable memory matrix and\nrevise memory vectors by the attention mechanism during\nthe multi-threaded querying and responding procedure, so as\nto better align the visual and textual representations. Given\na source sequence $F_s$ = {$f_1, f_2, f_3,..., f_s$}(concatenated\nfeatures from the MVPA and CMKE modules for an input 3D\nCT volume), a target sequence $Y = {Y_1, Y_2, Y_3, ..., Y_T }$, and\na memory matrix $M = {m_1, m_2, m_3, ..., m_I }$, the memory\nresponses of $F_s$ and $Y$ can be obtained by:\n$\\gamma_{Fs} = \\frac {F_s .m_i}{\\sqrt{d}}$                                                                                                                                                                                                                                                                                                                                                                   (21)\n$\\gamma_{Yt} = \\frac {Yt .m_i}{\\sqrt{d}}$                                                                                                                                                                                                                                                                                                                                                                                                                                                    (22)\nwhere I denotes the number of memory vectors, $m_i \u2208$\n$R^d$ the memory vector at row j with d indicating its dimen-\nsion. As illustrated in Fig. 7, the report generator of our model\nintegrates a three-layer transformer with the above-mentioned\ncross-modal memory mechanism. The features encoded by\nMVPA and CMKE modules are concatenated and fed into\nthis module, where the encoder processes only multi-modal\nfeatures while the decoder takes only textual features as input.\nThis process can be formulated as:\n$Z_S = R_e(\\gamma_{f1}, \\gamma_{f2},...,\\gamma_{fs})$                                                                                                                                                                                                                                                                                                                                                          (23)\n$Y_t = R_d(Z_S, y_1, y_2,...,\\gamma_{y(t-1)})$                                                                                                                                                                                                                                                                                                                                                           (24)\nwhere $R_d()$ and $R_e()$ denote the decoder and encoder\nof the report generator, respectively, {$\\gamma_{f1}, \\gamma_{f2},...,\\gamma_{fs}$} and\n{\\gamma_{y1},\\gamma_{y2},...,\\gamma_{y(t-1)}} represent the memory responses for\nmulti-modal features and textual features from previously\ngenerated tokens, respectively, $Z_S$ denotes the intermediate\nencoded states, $y_t$ the predicted output at the current time step\nt. The complete report is produced by:\n$Y_t = Softmax (Linear (y_t))$                                                                                                                                                                                                                                                                                                               (25)\nwhere Linear(.) and Softmax(.) denote the Linear and Soft-\nmax layer of the module, respectively, and $Y_t$ represents the\npredicted token at time step t."}, {"title": "IV. EXPERIMENTS", "content": "To validate the superiority of our proposed model, extensive\nexperiments are carried out on the public chest CT report\ndataset CTRG-Chest-548K [8]\u00b9"}, {"title": "D. Quantitative Analysis", "content": "To showcase the validity of the proposed MvKeTR, we\ncompare it with the recent state-of-the-art methods, including\nCNN-RNN(i.e., MRMA [31]), CNN-Transformer(i.e., Vanilla\nTransformer [21], M2TR [32], R2GenCMN [5], TSGET [34],"}, {"title": "E. Qualitative Analysis", "content": "In addition to quantitative analysis, we also conduct qual-\nitative analysis on test cases by comparing their ground\ntruths with the reports generated from our proposed method,\nMvKeTR, and its baseline, R2GenCMN, upon which our\nmethod is built. Fig. 9 shows two examples from CTRG-\nChest-548K and their corresponding reports where green and\nred highlights denote correct and incorrect content, respec-\ntively. In both cases, MvKeTR generates reports that resemble\nradiologists'observations in terms of clinical accuracy and\nprofessional terminology. To better illustrate the performance\ndifference between MvKeTR and R2GenCMN, a detailed\nanalysis of these two cases is presented below.\nIn the first case(top row of Fig.9), MvKeTR accurately\ndetects key abnormalities such as increased lung trans-\nparency and patchy shadows in the left lower lobe. Although\nboth methods make some errors in describing pleural effu-\nsion/thickening, MvKeTR provides a more comprehensive and\naccurate description similar to the ground truth report. For\nthe second case (bottom row of Fig.9), MvKeTR successfully\nspots the presence of multiple nodules in both lungs, which\nis a critical diagnostic finding that R2GenCMN completely\noverlooks. These two cases suggest that MvKeTR not only\nprovides more accurate and comprehensive reports but also\nshows better capability in detecting clinically significant ab-\nnormalities, which is essential for reliable medical diagnosis."}, {"title": "F. Ablation Study", "content": "Furthermore, we perform a body of ablation experiments to\ninvestigate the contributions of each component. The following\nbaselines are used:"}, {"title": "G. Discussion", "content": "Comparison of different 3D visual extractors: We conduct\nexperiments to assess the impact of various 3D visual ex-\ntractors, including CNN-based methods: U-Net [48], and CT-\nNet [47] as well as recent Vit-based methods: 3D ViT [46],\nand CT-ViT [44]. As demonstrated in Table IV, we find that\nCT-ViT(see Fig. 3) is the most suitable 3D visual extractor\nfor our proposed MvKeTR, which leverages the advantages\nof two types of transformers: spatial transformers and causal\ntransformers. Specifically, the spatial transformers excel at\ncapturing local spatial features and relationships within CT\nslices, which is crucial for understanding anatomical structures\nand lesion locations. Meanwhile, the causal transformers are\nadept at modeling long-range dependencies across different\nslices or depth levels of the 3D CT volume, ensuring com-\nprehensive feature extraction. This combination effectively\npreserves spatial and volumetric information throughout the\n3D visual feature extraction pipeline. Moreover, the ViT-based\nmethods show relatively higher performance compared to the\nCNN-based counterparts, suggesting that the self-attention\nmechanism is more effective in capturing global dependencies\nand long-range interactions within visual features, which is\ncrucial for CT report generation.\n2) Effect of varying top-k: The top-k is a key hyperparameter\nof CMKE module, which determines the number of most rel-\nevant reports to be retrieved for the current report generation.\nAs revealed in Fig. 10, we observe that increasing the top-k\nat first improves the BLEU-1 score, reaching a peak at top-\nk=16, beyond which the performance deteriorates. This trend\nsuggests that while retrieving more relevant reports provides\nricher reference information, excessive retrieval beyond a cer-\ntain threshold may introduce noise without additional benefits.\nHence, the top-k is set to 16, balancing performance gains and\ncomputational efficiency."}, {"title": "V. CONCLUSION", "content": "In this paper, we present MvKeTR, a novel framework that\nintegrates multi-view perception and knowledge enhancement\nfor high-quality radiology report generation from 3D CT vol-\nlumes. First, multi-view features are extracted by a 3D visual\nextractor, which comprises three separate CT-ViT networks.\nThe multi-view perception aggregator is then utilized to syn-\nthesize visual features from three anatomical views. Following\nthis, the cross-modal knowledge enhancer is responsible for\nincorporating clinical knowledge from the most relevant cases\ninto the diagnostic process. Finally, these features encoded by\nthe two aforementioned modules are concatenated and fed into\nthe report generator to produce the final report. Experimental\nresults on the CTRG-Chest-548K dataset demonstrate the\nsuperiority of our method over prior state-of-the-art studies.\nNotwithstanding our contributions in automatic radiology\nreport generation, there are still limitations. In future work, we\nwill focus on validating the generalizability and effectiveness\nof our proposed model through a real-world pilot study across\nlocal hospitals and extending the framework to other 3D\nmedical imaging modalities (e.g., MRI) and different body\nparts."}]}