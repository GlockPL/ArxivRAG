{"title": "Graph Counterfactual Explainable AI via Latent Space Traversal", "authors": ["Andreas Abildtrup Hansen", "Paraskevas Pegios", "Anna Calissano", "Aasa Feragen"], "abstract": "Explaining the predictions of a deep neural network\nis a nontrivial task, yet high-quality explanations for\npredictions are often a prerequisite for practitioners\nto trust these models. Counterfactual explanations\naim to explain predictions by finding the \"nearest\"\nin-distribution alternative input whose prediction\nchanges in a pre-specified way. However, it remains\nan open question how to define this nearest alter-\nnative input, whose solution depends on both the\ndomain (e.g. images, graphs, tabular data, etc.) and\nthe specific application considered. For graphs, this\nproblem is complicated i) by their discrete nature,\nas opposed to the continuous nature of state-of-the-\nart graph classifiers; and ii) by the node permu-\ntation group acting on the graphs. We propose a\nmethod to generate counterfactual explanations for\nany differentiable black-box graph classifier, utiliz-\ning a case-specific permutation equivariant graph\nvariational autoencoder. We generate counterfactual\nexplanations in a continuous fashion by traversing\nthe latent space of the autoencoder across the clas-\nsification boundary of the classifier, allowing for\nseamless integration of discrete graph structure and\ncontinuous graph attributes. We empirically vali-\ndate the approach on three graph datasets, showing\nthat our model is consistently high-performing and\nmore robust than the baselines.", "sections": [{"title": "1 Introduction", "content": "Sets of graphs arise in different applications,\ne.g. brain connectivity [1-3], brain arterial net-\nworks [4], anatomical trees [5, 6], mobility networks\n[7], and chemistry, and graph classifiers play im-\nportant roles in our daily infrastructure, healthcare\nquality, and national security. Their explainability\nis crucial to ensure both safety and trust when using\nAI for high-stakes applications.\nHowever, explainable AI (XAI) for graph predic-\ntors remains challenging due to the misalignment\nbetween the discrete graph structure and the con-\ntinuous nature of state-of-the-art graph- and XAI\nmodels. Moreover, the action of the node permuta-\ntion group challenges the interpretability of latent\nfeature embeddings [8]. In this paper, we generate\ncounterfactual explanations for graph classifiers by\nutilizing a permutation equivariant graph variational\nautoencoder to build a semantic latent graph repre-\nsentation. Guided by the classifier, we traverse this\nlatent space to obtain graphs that are semantically\nsimilar to the input graph but whose prediction has\nbeen altered in a predetermined way. This allows us\nto answer questions such as \"How do we most easily\nalter a given chemical molecule to improve a specific\nchemical property?\" or \"How should a given so-\ncial network change to reduce its fraud risk score?\".\nWe evaluate our approach on three commonly used\ndatasets of molecular graphs.\n1.1 Background\nCounterfactual explanations for graph classifiers\ntake as input a graph and aim to return a mini-\nmally altered version of the graph whose class label\nhas been altered. In other words, counterfactual\nexplanations answer the question: \"What is the\neasiest way to change the graph G to alter its classi-\nfication\". For these explanations to be maximally\ninterpretable, in the sense that the factual and coun-\nterfactual graph need to be aligned, the pipeline\nshould be permutation equivariant: If the ordering\nof the input graph nodes is changed, we would like\nthe output graph nodes to be altered accordingly. In\nthis way, any natural alignment between the input\nand output graphs is preserved.\nCounterfactual Explanations highlight essen-\ntial, yet in-sample, changes in input features that af-\nfect the outcome of a predictive model, offering valu-\nable insights into the model's decisions. Several ap-\nproaches exist for tabular [9, 10] and image data [11],\nas well as graph data [12, 13]. Early works introduce\nand address key aspects such as sparsity [14, 15],\nactionability [16, 17], diversity [18, 19] and causal-\nity [20, 21]. Recent research focuses on maintaining\ncounterfactuals close to the data manifold, which is\nusually approximated with generative models such\nas autoencoders [22-24], flows [25, 26], and diffu-\nsion models [27-29]. In the context of graphs, both\nmodel-level [30, 31] and instance-level [32, 33] coun-\nterfactual explanations have been proposed. Gener-\nating counterfactual explanations based on a VAE"}, {"title": "2 Method", "content": "We design XAI counterfactual graphs using a varia-\ntional autoencoder (VAE) to generate in-distribution\ncounterfactual graphs. The generative procedure is\nbuilt on the assumption that a meaningful latent\ngraph representation has been obtained. This repre-\nsentation space can then be traversed by using the\nloss of the graph classifier to steer the new graphs\ntoward a class of interest.\nBelow, we describe the individual components\nused to build our counterfactual generators, before\njoining them together in an equivariant framework\nfor counterfactual graph explanation. As graph\nrepresentations are affected by node ordering, our\npipeline is designed to be equivariant with respect\nto node permutations by using a PEGVAE, and we\nconsider classifiers that are permutation invariant.\nFig. 1 shows the complete counterfactual pipeline.\n2.1 Graph Representation\nIn the following, a graph $G = (B, V, A, E) \\in \\mathcal{G}$ is\nrepresented using the following dense matrices:\n$\\bullet$ $B \\in \\{0, 1\\}^{n \\times 2}$ is a boolean matrix indicating\nthe existence of nodes.\n$\\bullet$ $V \\in \\mathbb{R}^{n \\times d_v}$ is a real-valued matrix that con-\ntains the node attributes.\n$\\bullet$ $A \\in \\{0, 1\\}^{n \\times n}$ is a Boolean matrix indicating\nthe existence of edges, the adjacency matrix.\n$\\bullet$ $E \\in \\mathbb{R}^{n \\times n \\times d_e}$ is a real-valued matrix that con-\ntains the edge features if they exist.\nNote that in this representation, n is chosen to be\nthe same for all graphs, and graphs containing less\nthan n nodes are zero-padded. This ensures that\nbatching of graphs can be done during model train-\ning even if a dense graph representation is employed.\nFurthermore, while our experiments only consider\ncategorical node and edge attributes, the framework\nextends to continuous attributes.\n2.2 Invariance and Equivariance to\nPermutation\nHaving defined the graph representation, we define\nhow the permutation group $S_n$ of order n acts on a\ngraph $G\\in \\mathcal{G}$. Specifically for a permutation $\\sigma\\in S_n$\nwe denote the permutation matrix associated with\nthis element as $P_\\sigma$. In this case, we will simply\ndefine the group action as\n$\\sigma \\cdot G := (P_\\sigma B, P_\\sigma V, P_\\sigma A P_\\sigma^\\top, P_\\sigma E P_\\sigma^\\top)$. (1)\nThat is, the rows and columns of A and E are\npermuted, whereas only the rows of B and V are\npermuted.\nIt is widely acknowledged that predictive graph\nmodels should incorporate the notions of invariance\nand equivariance [41]. In the case of classification,\na graph classifier should not depend on the node\nordering. The graph classification model $C: \\mathcal{G} \\rightarrow$\n$\\{0, 1\\}$ considered in this paper is therefore designed\nto be invariant, that is, $C(\\sigma \\cdot G) = C(G)$ for any\n$G\\in \\mathcal{G}$ and $\\sigma\\in S_n$.\nLikewise, we design the autoencoder to be equivari-\nant to permutation. Thus, the encoder $F: \\mathcal{G} \\rightarrow \\mathbb{R}^n$\nand decoder $D: \\mathbb{R}^n \\rightarrow \\mathcal{G}$ should have the property,\nthat for any $G\\in \\mathcal{G}$:\n$D(F(\\sigma \\cdot G)) = \\sigma \\cdot \\widehat{G}$ for all $\\sigma\\in S_n$. (2)\nWhere $\\widehat{G}$ denotes the reconstruction of G. For graph\nautoencoders, equivariance is crucial: We need input\nand output graphs to be aligned when computing the\nloss during training. This also applies to assessing\ncounterfactual graph explanations.\nSeveral ways of creating equivariant layers can be\nconsidered, but for the networks in this paper, we\nconsider a specific basic building block consisting"}, {"title": "2.3 PEGVAE", "content": "We design a PEGVAE similar to Hansen et al.\n[8]. We employ a standard Gaussian prior on the\nlatent space variable, i.e. $p(z) = \\mathcal{N}(z \\vert 0,I)$, and\nlet $q_{\\phi}(z \\vert G)$ denote the approximate posterior also\nbeing Gaussian, and where $\\phi$ refers to the parameters\nof the encoder as is custom when dealing with VAEs.\nSpecifically, we factorize the likelihood $p_{\\theta}(G \\vert z)$ as:\n$p_{\\theta}(G \\vert z) = p_{\\theta_B}(B \\vert z) p_{\\theta_V}(V \\vert z, B)$\n$p_{\\theta_A}(A \\vert z, B, V)$\n$p_{\\theta_E}(E \\vert z, B, V, A)$, (3)\nwhere $\\theta = \\{\\theta_B, \\theta_V, \\theta_A, \\theta_E\\}$ refers to the parameters\nof the decoder. Thus, when sampling a new graph\nfrom $p(G)$, we first sample the prior $p(z)$, then $p(B \\vert$\nz), etc., until a graph has been obtained. This\nfactorization makes it easier to sample valid graphs\nthan if $p(G \\vert z)$ were modeled directly: We can\nconstrain edges to be sampled between nodes that\nexist, and assign classes only to nodes and edges\nthat exist. We now minimize the negative evidence\nlower bound (ELBO), which is given by, i.e.:\n$-ELBO = \\mathbb{E}_{z \\sim q_{\\phi}(z | G)} [-\\log p_{\\theta}(G \\vert z)]$\n$+ KL(q_{\\phi}(z \\vert G) || p(z)),$ (4)\nwhere KL($\\cdot || \\cdot$) denotes the Kullback-Leibler\ndivergence. Here, the negative log-likelihood works\nas a reconstruction loss, and the KL-loss works as\nregularization. Since the prior is Gaussian with\nmean zero, and thus has a symmetric density, the\nnode ordering will not affect the size of this term. In\npractice, we tune the KL-loss with a hyperparameter\n$\\beta \\in [0, 1]$ [43] by optimizing,\n$\\mathbb{E}_{z \\sim q_{\\phi}(z | G)} [-\\log p_{\\theta}(G \\vert z)]+\\beta \\cdot KL(q_{\\phi}(z | G) || p(z)).$\nThe exact architecture of the permutation equiv-\nariant VAE as well as any hyperparameters used\nduring training can be found in Appendix A.1.2."}, {"title": "2.4 Classifier Design", "content": "As the PEGVAE is permutation equivariant, the\ngraph classifier $C : \\mathcal{G} \\rightarrow \\{0,1\\}$ is designed to be per-\nmutation invariant as the prediction should not be\naffected by node ordering. Following Bronstein et al.\n[41], this feature is obtained by composing a number\nof equivariant layers followed by an invariant global\npooling layer. The output of this global pooling\nlayer can be viewed as a graph embedding, which is\npassed to a fully connected neural network to obtain\nprobabilistic class predictions. The architecture of\nthe classifier, as well as the hyperparameters used\nduring training, can be found in Appendix A.1.1."}, {"title": "2.5 Generating Counterfactuals via\nLatent Space Traversal", "content": "Given a factual graph $G^F$ we generate counterfactual\nexplanations for the class prediction. First, a latent\nencoding $z^F$ of $G^F$ is found by evaluating $F(G^F)$."}, {"title": "3 Experiments", "content": "3.1 Data\nWe evaluate our method using three molecular\ngraph datasets: NCI1 [45], Mutagenicity [46-48] and\nAIDS [47, 49], where graph nodes represent atoms\nand edges represent bonds. Each dataset poses a\nbinary classification task: Whether the molecule\nis active against HIV or not (AIDS), whether it is\nmutagenic or not (Mutagenicity), and whether the\nmolecule is anticancer (NCI1). We follow the graph\npre-processing and filtering of Huang et al. [50], in-\ncluding only molecules with nodes occurring with a\nfrequency larger than 50 in the dataset to avoid im-\nbalance. Furthermore, as our architecture relies on a\ndense graph representation, we only consider graphs\nwith fewer than 50 nodes (Mutagenicity, NCI1) and\n30 nodes (AIDS). Each dataset is divided into train-\ning, validation, and test sets, with 10% allocated to\ntest and validation. Additional information can be\nfound in Tab. A.1 in Appendix A.\n3.2 Evaluation Metrics\nThe counterfactuals quality of the counterfactuals\nare evaluated as a trade-off between identity preser-\nvation, i.e, the degree to which the generated coun-\nterfactual graph resembles the factual graph, and\nvalidity, i.e, whether the generated counterfactual is\nindeed a valid candidate for the desired class.\nIdentity Preservation. Three identity preserva-\ntion measures are used. Graph Edit Distance (GED),\nmeasures the graph distance between factual and"}, {"title": "3.3 Baselines", "content": "We include several baseline counterfactual methods.\nFirst, counterfactual explanations can be generated\nnaively by decoding random samples from p(z). We\nrefer to this method as Random. That is, we first\nsample from the prior p(z) and subsequently from\n$p_{\\theta}(G \\vert z)$. This serves as a na\u00efve baseline, as nothing\npromotes the generation of a counterfactual.\nThe Graph of NN from Training counterfac-\ntual uses a factual latent code $z^F$ and a desired label\n$y_D$ to pick the closest latent code of graphs in the\ntraining dataset with the desired label. We then let"}, {"title": "3.4 Results", "content": "Tab. 1 shows a variety of identity preservation and\nvalidity metrics for counterfactual quality, computed\non the test set. A good counterfactual has high va-\nlidity, while still maintaining a high degree of iden-\ntity preservation of the original input. Note that\nthe classifier-guided counterfactuals consistently out-\nperform the baselines measured via flip-ratio, only\noccasionally surpassed by the Decoded Mean of k-\nNN on AIDS. Unsurprisingly, the best-performing\nmethod in terms of identity preservation is often just\nthe Graph of NN from Training method. However,\nwe also see, that this method consistently performs\npoorly in terms of validity, underscoring the fact\nthat a trade-off is present, and the evaluation of the\nmethod based on the table alone is not sufficient.\nFig. 2 depicts the trade-off between identity preser-\nvation (columns) and validity (rows) for each dataset.\nThe curve is constructed by computing the aver-\nage validity score from all samples with an identity\npreservation score below a certain threshold, given\nby the x-axis. Note that we only compute the av-\nerage after having made 10 observations with the\nvalidity below the threshold given by the x-axis be-\nfore we compute an average to remove noise. In the\ncase of the Flip-Ratio this can be interpreted as an\nestimate for the probability of the counterfactual\ngraph having successfully flipped the class of its\nfactual graph, given that the level of identity preser-\nvation is below a certain threshold. We observe that\nthe classifier-guided counterfactuals achieve trade-\noffs competitive with all baselines, and outperform\non NCI1 and Mutagenicity. One should note, that to\nmake the interpretation of the plots the same (high\nvalues being desirable, and low values being undesire-\nable) we consider the negative cosine similarity. The\nhistograms of Fig. 2 show the distribution of identity\npreservation scores of the generated counterfactuals\nfor each score."}, {"title": "4 Discussion", "content": "Classifier Guided CF is robust on all datasets.\nOn AIDS, the Decoded Mean of k-NN and CGCF\nachieve good validity scores, as well as good trade-\noffs between identity preservation and validity. How-\never, only the CGCF proves to be robust across all\ndatasets considered here. It is worth noting that this\nmay be because the AIDS dataset contains graphs\nwith fewer nodes (30) than Mutagenicity (50) and\nNCI1 (50). As the dimensionality of our VAE latent\nspace matches the number of nodes, this could ex-\nplain the decrease in performance. Also note that\neven though the Graph of NN from Training method\nis guaranteed to generate a counterfactual of the de-\nsired class, the validity for this method is in general\nvery low, as it will often generate a graph that will\nbe misclassified by the classifier. See Appendix A.3\nfor an illustration of this phenomenon.\nGenerating counterfactual explanations based\non latent space exploration removes the need\nfor defining graph distance explicitly. When\nproducing counterfactual explanations, we generally\nwant to find a counterfactual graph\n$G^{CF} = \\arg \\min_G dist(G, G^F)$\nsuch that $C(G^F) \\neq C(G) = y_D$ for some notion of\ngraph distance d and some desired label $y_D$. This\nensures that the counterfactual graph is similar to\nthe original (factual) graph. However, the choice\nof graph distance will always be open and likely\ndepends heavily on the application at hand. When\ncounterfactual graphs are generated based on latent\nspace traversal, we do not have to define an explicit\nmetric on the graph space. Our method relies on\nthe distance between latent graph representations,\nmaking it widely usable across different cases.\nThe method enables the construction of an\narbitrary number of possible explanations.\nOur method is probabilistic and models the like-\nlihood $p_{\\theta}(G \\vert z)$. Thus, we can easily sample an\nunlimited amount of counterfactual explanations\ngiven some factual graphs.\nLimitations. Generating counterfactuals based\non a pre-trained VAE means that the quality of the\nmethod relies heavily on the quality of the generative\nmodel. For instance, if a latent counterfactual is\nobtained on the classification boundary in the latent\nspace far away from any latent training point, then\nthe sampled counterfactual may not be reasonable.\nIn this work, we focus on exploring instance-level\nexplanations. Extending our method to model-level\nexplanations by exploring general patterns that arise\nwhen moving toward the counterfactual class will\nbe a subject of future work."}, {"title": "5 Conclusion", "content": "We proposed to generate counterfactual explanations\nfor graph classification using the latent representa-\ntion space of a permutation-equivariant VAE. We\nfind it a promising direction of research to utilize a\npreexisting generative model, such as a VAE, to gen-\nerate valid counterfactual explanations for graphs\nwithout having to define explicit graph metrics. As\nshown by our experiments, the classifier-guided coun-\nterfactual explanations provide a robust trade-off\nbetween identity preservation and validity."}, {"title": "A.1 Evaluation of auxiliary models.", "content": "The success of our suggested method for generating\ncounterfactual explanations is highly dependent on\nthe quality of the models considered. The classifier\nand the VAE considered in this work are both de-\nsigned to be permutation invariant and equivariant\nrespectively.\nA.1.1 Classifier\nThe classifier used for all datasets consists of four\nequivariant modules (as described in Section 2.2)\nproducing node embedding. These are then fol-\nlows be an invariant max-pooling operation ensur-\ning the model as a whole is invariant. The embed-\ndings obtained after this operation are the graph-\nembeddings. Each module has 20 channels. These\ngraph-embeddings are then passed through a fully\nconnected neural network with 200 neurons. Note,\nthat the first module only considers the B and V\nmatrices, and the output is appended to the A and\nE matrices for subsequent processing. The classifier\nwas trained for 100 epochs using a learning rate\nof 0.001, and a batch-size of 64. In Tab. A.2 the\nAUROC computed on the test-set is reported for all\nclassifiers.\nA.1.2 Permutation equivariant VAE\nThe permutation equivariant VAE is designed using\nmodules similar to the ones used in the classifier.\nThe encoder consists of 4 equivariant modules (as\ndescribed in Section 2.2) each of which deals with\n20 channels. Again, the first module only considers\nthe B and V matrices. The last layer is divided"}, {"title": "A.2 Hyperparameter Selection for\nGeneration of Counterfactuals", "content": "For the Classifier Guided CF method optimization\nwas done using an Adam optimizer with a learning\nrate of 0.05 for 1 000 iterations and X = 1 used\nfor regularization. The hyperparameter - used for\nthe Gumbel-Softmax was also set to 1. This pa-\nrameter determines how close an approximation the\nGumbel-Softmax is to the desired discrete, categor-\nical distribution as opposed to a uniform distribu-\ntion assigning equal probability mass to all classes.\nFor the Decoded Mean of k-NN method, we choose\nk = 10."}, {"title": "A.3 Illustration of Nearest Neighbor\nGraph Counterfactual with Low\nValidity", "content": "In figure A.1 we illustrate intuitively how producing\ncounterfactual explanations based on the Graph of\nNN from Training method can produce low validity\nscores even though, the counterfactuals produced\nare guaranteed to be from the opposite class. In\nthe depicted examples graphs are considered divided\ninto two classes: Triangles and squares, and these\ntwo classes are almost perfectly separated by the\nclassification boundary, with only one square being\nclassified as a triangle. However, since the repre-\nsentation which we have obtained through the VAE\nto a large degree clusters latent codes of the same\ntype, this single misclassified graph will be chosen"}]}