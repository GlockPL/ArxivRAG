{"title": "Identifying Implicit Social Biases in Vision-Language Models", "authors": ["Kimia Hamidieh", "Haoran Zhang", "Walter Gerych", "Thomas Hartvigsen", "Marzyeh Ghassemi"], "abstract": "Vision-language models, like CLIP (Contrastive Language Image Pretraining), are becoming increasingly popular for a wide range of multimodal retrieval tasks. However, prior work has shown that large language and deep vision models can learn historical biases contained in their training sets, leading to perpetuation of stereotypes and potential downstream harm. In this work, we conduct a systematic analysis of the social biases that are present in CLIP, with a focus on the interaction between image and text modalities. We first propose a taxonomy of social biases called So-B-IT, which contains 374 words categorized across ten types of bias. Each type can lead to societal harm if associated with a particular demographic group. Using this taxonomy, we examine images retrieved by CLIP from a facial image dataset using each word as part of a prompt. We find that CLIP frequently displays undesirable associations between harmful words and specific demographic groups, such as retrieving mostly pictures of Middle Eastern men when asked to retrieve images of a \"terrorist\". Finally, we conduct an analysis of the source of such biases, by showing that the same harmful stereotypes are also present in a large image-text dataset used to train CLIP models for examples of biases that we find. Our findings highlight the importance of evaluating and addressing bias in vision-language models, and suggest the need for transparency and fairness-aware curation of large pre-training datasets.", "sections": [{"title": "Introduction", "content": "Machine learning has seen rapid advances in Vision-Language (VL) models that learn to jointly represent image and language data in a shared embedding space (Radford et al. 2021; Jia et al. 2021). Recent advances on a range of multi-modal tasks are exemplified by the VL model CLIP (Radford et al. 2021), leading to state-of-the-art performance on several zero-shot retrieval tasks (Xu et al. 2021) as well as being integrated into various VL models such as LLaVA (Liu et al. 2024) and BLIP (Li et al. 2022a), which combine the frozen vision encoder with language models for enhanced multi-modal understanding and alignment, Stable Diffusion, which leverages CLIP embeddings for refined text-to-image generation (Rombach et al. 2022) and various other VL models.\nThese successes have spurred several VL models in end-user applications, such as facial recognition systems where CLIP enhances zero-shot face recognition (Zhao and Patras 2023), and multimedia event extraction, as well as event detection in images and captions (Li et al. 2022b; Lu et al. 2024). However, recent works show that large pre-trained models that operate over vision (May et al. 2019; Park et al. 2021), language (Bender et al. 2021; Guo and Caliskan 2020; Zhang et al. 2020a) or both learn social biases from training data (Barocas, Hardt, and Narayanan 2017; Corbett-Davies and Goel 2018), which risks perpetuating bias into downstream retrieval and generation tasks (Silva, Tambwekar, and Gombolay 2021; Luccioni et al. 2023; Weidinger et al. 2021). In VL models specifically, terms related to race have been found to be associated more with people of color (Agarwal et al. 2021), and women are generally under-represented in image retrieval tasks (Wang, Liu, and Wang 2021). However, these existing works focus only on very specific forms of bias while probing disparities using a small set of curated words (Bhargava and Forsyth 2019).\nGiven that more extensive and intersectional forms of bias may exist in VL models, there is a need to expand these"}, {"title": "Related Work", "content": "Vision-Language Models. Recently, Vision-Language (VL) models have shown great potential for learning general visual representations and enabling prompting for zero-shot transfer to a range of downstream classification tasks (Radford et al. 2021; Jia et al. 2021; Zhang et al. 2020b). In this work, we focus our experiments on CLIP-based models (Radford et al. 2021). CLIP models utilizes an image encoder and a text encoder to match vector representations for images and text in a multi-modal embedding space. The training objective for CLIP is to maximize the cosine similarity between an image and its corresponding natural language caption, while minimizing the similarity between the image and all other captions in the batch, a training technique known as contrastive learning (Chen et al. 2020; Mnih and Kavukcuoglu 2013). By learning a meaningful joint representation between text and images, CLIP achieves strong zero-shot performances on vision benchmarks while also benefiting downstream VL tasks (Shen et al. 2021). The original CLIP model, OACLIP (Radford et al. 2021), was trained on a large scale image-text pair dataset. According to its creators, this dataset consists of approximately 300 million images and their associated text descriptions, but the source of this data was not specified.\nBias in Vision-Language Models. A number of prior works have focused on harmful biases of CLIP. Agarwal et al. (2021) conducted a preliminary study on racial and gender bias in the CLIP model showing that CLIP associates a \"white\" text label with the white racial label less than associating in the individuals belonging to the other racial groups with their group. Dehouche (2021) show that CLIP has a gender bias when prompted with gender neutral text. Wolfe, Banaji, and Caliskan (2022) show that multiracial people are more likely to be assigned a racial or"}, {"title": "Creating a Taxonomy of Social Biases in Vision-Language Models", "content": "We propose a taxonomy of VL model biases called SO-B-IT (Social Bias Implications Taxonomy), which categorizes 374 words into nine types of bias as show in Table 4 in the Appendix. We define bias as a disproportionate association between a word or concept and a specific demographic group in comparison to others (Operario and Fiske 2001; Levinson and Young 2009), and especially focus on gender and racial identities.\nIn the first step, we consider different categories of biases. Our first step in creating the taxonomy involved a review of existing literature in biases. There are many papers that propose different types of biases in AI models, but we selected those that are either actionable or have higher sentiment associated with them, or they exhibit allocative harms (Nadeem, Bethke, and Reddy 2020; Steed and Caliskan 2021). Allocative harms involve making assumptions about people that can lead to unfair resource distribution, whereas representational harms involve the misrepresentation of people that can perpetuate stereotypes (Barocas et al. 2017). Our taxonomy focuses on representational harms that could turn into allocative harms. By identifying these biases, we aim to mitigate potential negative impacts in real-world applications. While our taxonomy is not exhaustive, it is designed to be easily extendable. Below, we discuss the different types of categories included in our taxonomy.\nAlgorithmic Governance Areas\nTo examine potential biases in VL models, we refer to the top AI use cases by policy areas proposed by Engstrom et al. (2020), that are specifically related applications that can harm marginalized groups by using images or videos of faces. So-B-IT contains potentially-biased words from the following categories.\nCriminal Justice. Machine learning models have been deployed in criminal justice for tasks including recidivism prediction (Berk 2017; Tolan et al. 2019), predictive policing (Shapiro 2017), and criminal risk assessment (Berk, Berk, and Drougas 2019). These models have also been shown to have disparate perform across demographic groups. For example, models used to predict recividism risk have been shown to exhibit a higher false positive rate for Black inmates (Wadsworth, Vera, and Piech 2018). We probe the relations learned by CLIP and concepts associated with historical biases (Alexander 2020) such as \u201ccriminal\u201d, \u201cdelinquent\u201d, and \u201cterrorist\u201d.\nEducation and wealth. Discrimination based on education level is common, and automated inference can lead to real harm (Brown and Tannock 2009). For instance, in education-based hiring (Tannock 2008), candidacy can be overlooked for those with less education (Van Noord et al. 2019). Moreover, given recent use of ML to predict student dropout in university admission decisions, detecting educational bias in VL models is increasingly important (Liu et al. 2022).\nHealth. There is a long history of bias and discrimination in healthcare (Govender and Penn-Kekana 2008). Such bias can worsen outcomes for people struggling with mental health (Thornicroft, Rose, and Kassam 2007) and for the aging population (Kydd and Fleming 2015), especially for racial minorities (Peek et al. 2011). We check for health-based biases using words like \"disabled,\" \"mentally ill,\" and \"addicted\".\nOccupation. Different occupations are unfairly associated with different groups of people. For example, many recent works have studied associations between gender and occupation (Singh et al. 2020). A well-established"}, {"title": "Stereotypical Markers", "content": "In addition to algorithmic governance areas, we also consider categories that are not directly related to known applications. However, these categories may be used spuriously as a proxy for a particular gender or racial demographic group. Probing VL model biases in these categories can help prevent the misrepresentation or under-representation of certain groups, which can have serious consequences for individuals' lives and opportunities. For instance, a biased model that associates specific physical traits or behaviors with a particular gender or racial group may result in unfair or discriminatory hiring practices in the employment sector. Similarly, a model that perpetuates harmful stereotypes about a specific group may contribute to the over-criminalization of that group in the criminal justice system (Alexander 2020). For instance, in recommendation-based models such as TikTok's algorithm, Karizat et al. (2021) have shown how participants alter their behavior and thus their algorithmic profile to resist the suppression of marginalized social identities via individual and collective action. Therefore, we include words from the following categories in So-B-IT.\nAppearance. Our self-worth is often tied to our perceived physical appearance (Patrick, Neighbors, and Knee 2004). For example, comparing oneself to cultural beauty standards can be detrimental, especially for members of minority groups (Mahajan 2007). To investigate appearance-related biases in CLIP, we look for disproportionate associations between racial and gender identities and the set of Appearance words in Table 4. We focus on subjective descriptors of cultural attractiveness like \"beautiful\u201d and \u201cchubby\" but also include words that may correlate with appearance like \u201cold\u201d and \u201ctall\".\nBehavior. Bias can stem from assumptions about others' behavior. For example, incorrect assumptions about behavior can occur in interracial interactions, often to the detriment of minority populations (Dovidio et al. 2002). We study such behavior bias using mostly adjectives like \"aggressive\u201d or \u201ccalm,\u201d which describe interactions with the world.\nPortrayal in media. How people are depicted can reinforce historical biases. For example, recent media coverage of Russia's war against Ukraine compares Ukraine to the Middle East, perpetuating harmful \u201cwar-torn", "third-world\" and": "avage"}, {"title": "Vision-Language Model Bias Identification Pipeline: Exploring Different Types of Biases Across Demographic Groups", "content": "We propose a simple framework for evaluating potential biases of VL models in facial recognition tasks. We focus on harmful associations present in the model, specifically based on retrieved images of people from different demographic groups as defined by the intersection of race and gender.\nSetup for Vision-Language Bias Identification Pipeline\nTo identify biases in VL models, we employ a word-association approach that focuses on identifying biases based on a given adjective or word's association with individuals from a certain demographic group. Specifically, we measure the similarity between the VL model's encoding of the word and its encoding of images of human faces from the FairFace dataset belonging to each demographic group.\nData and Model FairFace (K\u00e4rkk\u00e4inen and Joo 2019) is a face image dataset that is balanced in terms of race and gender. It includes 108,501 images from seven different racial groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino/Hispanic. The images were collected from the YFCC-100M Flickr dataset and labeled with information about race, gender, and age groups. In order to capture social biases in the face images, we use the taxonomy of social biases as described previously and shown in the taxonomy table in the appendix.\nCaption generation To generate the captions, we design templates for four categories of words: adjectives, profession or political nouns, object, and activities. Then, for each word in our taxonomy, we use the caption a photo of a/an [adjective] person for adjectives, a photo of a/an [noun] for nouns, and a photo of a person who is [gerund verb for activity]. We then calculate the similarity of the CLIP model's response to all images in the training set of the FairFace dataset for each category of prompts. We obtain the similarity scores using the cosine similarity between the prompt embedding and the image embedding in CLIP's representation space.\nMeasuring Image-Caption Association for Demographic Groups\nIn the next step, we want to measure how descriptive a caption is for a certain demographic group in comparison to the rest of the groups. As both caption and image representations lie within a joint representation space, we use cosine similarity $d(c, x)$ to measure the similarity between caption $c$ and image $x$. To measure the level of association between captions and demographic groups, we employ a method that is inspired by the Word Embedding Association Test (WEAT) (Caliskan, Bryson, and Narayanan 2017) measure in natural language processing. Specifically, we select a target demographic group $G$, such as a particular race or gender, and compute the average cosine similarity between a given caption $c$ and the image representations of"}, {"title": "Identifying Bias with Caption-Association Image Retrieval", "content": "For each category of bias, given the similarities of captions corresponding to words in the taxonomy of the bias type, we retrieve the top-k samples with the highest similarity scores for each caption. We use k=100 for our experiments. For each prompt, we focus on the demographic composition of the top-k samples by computing the distribution of the race and gender of people in the retrieved images. Since the FairFace dataset has an equal number of samples across gender and racial groups, we do not need to normalize the proportions. Thus, if the distribution of the demographic group is uniform across the top-k images, we infer that the VL model exhibits no social bias for this particular word. Conversely, if the proportion of a certain demographic group in the top-k samples is significantly higher or lower than expected, we infer the presence of bias. We repeat this process for each prompt and analyze the results to identify prevalent categories of biases in the VL model.\nAuditing Demographic Biases in Vision-Language Models\nOur taxonomy So-B-IT can be applied to audit any VL model. Here, we use it to audit the following four CLIP models:"}, {"title": "Debiased Models Are Still Biased", "content": "We now perform an intersectional audit of CLIP debiasing. Recent methods have been proposed to debias VL models with respect to protected attributes such as race and gender (Berg et al. 2022; Zhu et al. 2023; Seth, Hemani, and Agarwal 2023). However, whether debiasing for one attribute im-"}, {"title": "Ethical Considerations", "content": "Pre-training data and transparency One of the key factors that can influence the representations learned by a VL model like CLIP is the data it is trained on. The dataset that OAICLIP (Radford et al. 2019) was trained on was not released, though there are some speculations about the sources of the data (Nguyen et al. 2022). This lack of transparency makes it difficult to decode a model's biases and limitations. Given the potential biases and discrimination identified in our experiments, it is important to consider the data used to train the model and how it may have influenced the representations learned by CLIP. For example, the recent publicly available LAION dataset (Schuhmann et al. 2022) has been found to contain both images and textual representations of rape, pornography (Birhane, Prabhu, and Kahembwe 2021), and child sexual abuse material (Thiel 2023). Given our finding of the correlation between gender stereotypes in LAION-400m and their presence in the CLIP model trained on such data, it is likely that other problematic correlations could have been learned by the model as well. Thus, it is critical to consider the issue of bias through a data-centric perspective (Oala et al. 2023). Manual curation of a pre-training dataset without undesirable stereotypes may be required to obtain a model truly free of such biases (Jernite et al. 2022; Gadre et al. 2023; Birhane et al. 2023).\nAnother potential concern is data colonialism in the training data of VL systems and other foundation models. The use of data from marginalized or colonized populations without proper consent or compensation can perpetuate existing power imbalances and contribute to the exploitation of these groups.\nRegulation and Auditing Given the potential biases and discrimination identified in our experiments, regulating and auditing VL models is crucial for fairness and equality. Bias audits, impact assessments, and algorithmic accountability frameworks (Metcalf et al. 2021; Raji et al. 2020) can help evaluate performance, transparency, and fairness. Importantly, VL evaluations must be intersectional. Our analysis shows that considering bias for single attributes is insufficient: OpenCLIP associates Homemakers specifically with Indian women, for instance. Moreover, bias mitigation for one attribute can increase bias for others. Future debiasing approaches should use an intersectional evaluation framework like So-B-IT to measure effectiveness accurately."}, {"title": "Limitations", "content": "We recognize several limitations with our study. First, we make use of the FairFace dataset, which has several flaws. In particular, all race, gender and age attribute labels were obtained from Amazon Mechanical Turks, and so is already the product of human biases and stereotyping. In addition, the assumption of binary gender and the consideration of only seven racial groups is not representative of the full range of identities present in society, and one may also identify with a different gender or race over time. Other facial image datasets such as CelebA (Liu et al. 2015) or UTKFace (Zhang, Song, and Qi 2017) suffer from similar flaws, and conducting similar analyses on additional datasets"}, {"title": "Identifying Strong vs. Weak word associations", "content": "Out of the several hundred words we have included in our taxonomy, we hypothesize that some captions may be better descriptors of human faces than others, regardless of demographic attributes. These captions will thus have higher association overall to a facial image dataset. For instance, we expect the caption \"a photo of an actor\" to be more descriptive than \"a photo of an apple\" given a set of face images. In this experiment, we seek to quantify this relationship, which will allow us to elucidate the terms that CLIP associates with humans in general.\nIn this section, we propose a metric which assesses the level of relevance between a caption and an entire dataset. As \"strong\" and \"weak\" are relative terms, we define the strength of an association for a particular caption c relative to a large corpus of baseline captions S. Here, we choose S to be the top 10,000 most frequently occurring words in English (Research 2021).\nTo start, we embed each s\u2208 S with the CLIP text encoder, and compute the mean cosine similarity for the top-k retrieved images from FairFace. This gives us a distribution of similarity scores over the baseline corpus. Then, given a particular caption of interest c, we compute its similarity score in the same way, and then pass this value through the cumulative distribution function for the baseline similarity distribution, to obtain a metric between 0 and 1. Here, a score of 75% means means that c is a more relevant descriptor of the dataset than 75% of the words in S."}, {"title": "Taxonomy of Social Biases", "content": "The full set of words in So-B-IT is specified in table 4 as below."}]}