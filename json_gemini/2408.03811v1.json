{"title": "GENERATIVE LANGUAGE MODELS WITH RETRIEVAL AUGMENTED GENERATION FOR AUTOMATED SHORT ANSWER SCORING", "authors": ["Zifan Wang", "Christopher Ormerod"], "abstract": "Automated Short Answer Scoring (ASAS) is a critical component in educational assessment. While traditional ASAS systems relied on rule-based algorithms or complex deep learning methods, recent advancements in Generative Language Models (GLMs) offer new opportunities for improvement. This study explores the application of GLMs to ASAS, leveraging their off-the-shelf capabilities and performance in various domains. We propose a novel pipeline that combines vector databases, transformer-based encoders, and GLMs to enhance short answer scoring accuracy. Our approach stores training responses in a vector database, retrieves semantically similar responses during inference, and employs a GLM to analyze these responses and determine appropriate scores. We further optimize the system through fine-tuned retrieval processes and prompt engineering. Evaluation on the SemEval 2013 dataset demonstrates a significant improvement on the SCIENTSBANK 3-way and 2-way tasks compared to existing methods, highlighting the potential of GLMs in advancing ASAS technology.", "sections": [{"title": "1 Introduction", "content": "An effective assessment program employs various question formats, each designed to evaluate specific standards. Standards involving comprehension and knowledge are often best assessed using short answer questions, requiring students to construct their own responses. Automated Short Answer Scoring (ASAS) uses natural language processing (NLP) and statistical models to evaluate student responses to open-ended questions. The earliest statistical models were frequency and rule-based model [1]. As methods in NLP have advanced, the models applied to ASAS have become more sophisticated. These methods include clustering methods [2], a mixture of recurrent and feed-forward neural networks [3], language models [4], and ensembles of networks [5].\nCreating an Automated Short Answer Scoring (ASAS) system is primarily costly due to the need for high-quality, manually scored responses for training. A significant challenge in ASAS development is reducing the required number of training samples while maintaining scoring quality. Recent advancements in generative language models (GLMs) have led to promising developments in zero-shot, one-shot, and few-shot learning techniques [6]. Applying these approaches could significantly reduce the number of training responses needed for a high-quality scoring pipeline. Our research aims to leverage these methods, utilizing advanced GLMs like ChatGPT [7] and Claude [8], to maximize scoring quality with minimal training samples.\nIn this study, we focus on the SemEval-2013 joint student response dataset [9]. This dataset has been subjected to many traditional approaches [10, 11, 12, 13, 14, 15, 16, 17]. Pretrained encoder-based language models like BERT [18] and ELECTRA [19] have set impressive benchmarks [4]."}, {"title": "2 Background", "content": "For this study, we utilized the dataset from the SemEval-2013 task 7 on joint student responses [9]. This dataset contains two distinct corpora; the BEETLE dataset and SciEntsBank dataset. The BEETLE dataset consists of 3000 student responses to a set of 56 questions relating to electricity and electrons. The SciEntsBank consists of approximately 10,000 answers to a range of 197 questions relating to 15 different scientific domains. Most responses were 1 or 2 sentences long and assessed specific comprehension skills relating to the material provided. This serves as an interesting use case for the method we propose because traditional ASAS systems require a single model per question, however, each corpus contains relatively few answers per question.\nThe dataset underwent manual categorization using three distinct classification methods: a 5-category system, a 3-category system, and a 2-category system. In the 5-category classification, each student's answer was assigned to one of the following groups:\n\u2022 Correct: A paraphrase of the reference answer.\n\u2022 Partially correct incomplete: Contains some but not all information from the reference answer.\n\u2022 Contradictory: The student answer and reference answer contradict each other.\n\u2022 Irrelevant: Provides information on the topic but not relevant to the answer.\n\u2022 Non-domain: Does not provide domain content. e.g., \"I don't know\"\nThe 3-way classification was derived from the 5-way scheme by grouping Irrelevant, and Non-domain responses into a single Incorrect category. The 2-way simplifies the 5-way scheme by categorizing all responses except those labeled Correct as Incorrect.\nBoth the BEETLE dataset and the SciEntsBank dataset have two distinct test sets according to the conditions of the test:\n\u2022 Unseen answers (UA): A portion of student responses for each question in the training dataset was randomly selected and set aside. This reserved subset forms a separate evaluation set, allowing assessment of the system's performance on familiar questions (those present in the training data) but with previously unseen student answers.\n\u2022 Unseen questions (UQ): An evaluation set was created to measure the system's effectiveness on new questions within the same subject areas as the training data. This set consists of all student responses to a randomly chosen subset of questions from each dataset, which were entirely withheld from the training process.\nIn addition to these two test sets, the SciEntsBank dataset contains a third test set:\n\u2022 Unseen Domain (UD): An evaluation set composed of responses covering domains absent from the training data, designed to assess the system's performance across unfamiliar domains."}, {"title": "2.1 Dataset", "content": "For this study, we utilized the dataset from the SemEval-2013 task 7 on joint student responses [9]. This dataset contains two distinct corpora; the BEETLE dataset and SciEntsBank dataset. The BEETLE dataset consists of 3000 student responses to a set of 56 questions relating to electricity and electrons. The SciEntsBank consists of approximately 10,000 answers to a range of 197 questions relating to 15 different scientific domains. Most responses were 1 or 2 sentences long and assessed specific comprehension skills relating to the material provided. This serves as an interesting use case for the method we propose because traditional ASAS systems require a single model per question, however, each corpus contains relatively few answers per question.\nThe dataset underwent manual categorization using three distinct classification methods: a 5-category system, a 3-category system, and a 2-category system. In the 5-category classification, each student's answer was assigned to one of the following groups:\n\u2022 Correct: A paraphrase of the reference answer.\n\u2022 Partially correct incomplete: Contains some but not all information from the reference answer.\n\u2022 Contradictory: The student answer and reference answer contradict each other.\n\u2022 Irrelevant: Provides information on the topic but not relevant to the answer.\n\u2022 Non-domain: Does not provide domain content. e.g., \"I don't know\"\nThe 3-way classification was derived from the 5-way scheme by grouping Irrelevant, and Non-domain responses into a single Incorrect category. The 2-way simplifies the 5-way scheme by categorizing all responses except those labeled Correct as Incorrect.\nBoth the BEETLE dataset and the SciEntsBank dataset have two distinct test sets according to the conditions of the test:\n\u2022 Unseen answers (UA): A portion of student responses for each question in the training dataset was randomly selected and set aside. This reserved subset forms a separate evaluation set, allowing assessment of the system's performance on familiar questions (those present in the training data) but with previously unseen student answers.\n\u2022 Unseen questions (UQ): An evaluation set was created to measure the system's effectiveness on new questions within the same subject areas as the training data. This set consists of all student responses to a randomly chosen subset of questions from each dataset, which were entirely withheld from the training process.\nIn addition to these two test sets, the SciEntsBank dataset contains a third test set:\n\u2022 Unseen Domain (UD): An evaluation set composed of responses covering domains absent from the training data, designed to assess the system's performance across unfamiliar domains."}, {"title": "2.2 Information Retrieval", "content": "An Information Retrieval (IR) pipeline is a system that provides relevant information (i.e., documents) from a database based on user queries. The key to this approach is to map a document to a vector that encodes information relevant to queries, called its representation, in some vector space. The idea is that semantically similar documents are mapped to similar vectors. More formally, if d\u2081 and d\u2082 are documents, and $ is an embedding that maps the set of documents to Rn, then the semantic similarity between the documents, S(d1, d2), is modeled by the cosine similarity of the embeddings, given by\n$S(d1, d2) = \\frac{\\phi(d1). \\phi(d2)}{||\\phi(d1) || ||\\phi(d2) ||}\u02d9$ \nConsider a set of documents, D, consisting of elements d\u2081, ..., dn. We can represent their normalized embeddings as rows in a matrix A. If we can convert a query into a vector x, then finding the k most relevant documents to that query in D becomes equivalent to identifying the k largest values in the vector resulting from the matrix multiplication Ax.\nIn our case, D is a set of student responses In the training set, our query is a response, d, in the test set. Since we expect d to be of the same form as the elements of D, it makes sense that x = $(d). In settings like question-answering systems, where the queries are of a different form to the documents being retrieved, the two embeddings can be different [26]. Examples of more traditional methods of determining the embedding, 4, have included term frequency-based approaches like tf-idf [27] and recurrent approaches [28]. These approaches have been superseded by transformer-based approaches [26]."}, {"title": "2.3 Generative Language Models", "content": "A Generative Language Model (GLM) is a type of neural network trained on massive amounts of text data based on deep learning techniques. The most ubiquitous examples in the literature have become transformer-based models [29]. The most general architecture consists of an encoder, which maps text to a vector space, and a decoder that maps that vector space back to text. The advantage these networks have is that we can improve the performance on downstream tasks by pertaining these models on large corpora of data [18, 30]. There are three classes of such models; encoder networks such as BERT [18], decoder networks such as GPT [30], and encoder-decoder networks such as T5 [31].\nThe embedding we use as part of our IR pipeline is a pretrained encoder network trained for semantic similarity [26] derived from a truncation of the BERT model [18]. We fine-tune this model for our own use case.\nThe second type of model we use is a decoder network. The fundamental task of decoder GLMs is to predict the next token from a sequence of prior tokens [30]. Modern GLMs generally have an order of magnitude more parameters and are trained in several stages. The first stage is to pretrain the model on a large corpus of data. The model is then fine-tuned to perform instructions [30]. Lastly, the model is subjected to reinforcement learning [32]. The remarkable thing about these models is that they are often capable of performing tasks not necessarily similar to tasks in their training data [8, 7]."}, {"title": "2.4 Retreival Augmented Generation", "content": "Even though GLMs possess extensive knowledge and demonstrate excellent performance on various tasks, including auto-scoring, the vanilla auto-scoring approach\u2014providing a GLM with answers, questions, and rubrics, and asking it to give feedback-presents challenges. Grading answers solely based on questions and rubrics is difficult, even for GLMs, let alone that rubrics are not always readily available.\nStudies indicate that providing additional context and examples can enhance their performance. Therefore, we incorporate documents retrieved by our IR system as examples in the prompt for the GLM to reference. This method enables GLMs to learn from the examples and improve their grading capabilities, with or without a rubric, by implicitly conveying the logic of grading through the examples."}, {"title": "2.5 Prompt Optimization", "content": "Recent studies have demonstrated that carefully crafted prompts can significantly enhance performance across various tasks. Effective prompts typically employ several key techniques:\n\u2022 Structured format: Organize the prompt to clearly present descriptions, requirements, examples, procedures, and scoring criteria.\n\u2022 Role assignment: Explicitly define the role the Generative Language Model should assume.\n\u2022 XML tag utilization: Employ XML tags to delineate input and output fields, enhancing clarity and structure.\n\u2022 Task-specific techniques: Incorporate methods like chain-of-thought reasoning when appropriate, though this may not be suitable for all applications, such as autoscoring.\nWhile we initially drafted our prompts incorporating these techniques, we recognized the need for further optimization. To address this, we explored two advanced tools: DSPy [33, 34] and the Claude Prompt Generator. These tools offer distinct approaches to prompt refinement, each with its own strengths and limitations."}, {"title": "2.5.1 DSPy", "content": "DSPy, described as \"a framework for algorithmically optimizing LM prompts and weights\" [33], provides a systematic approach to prompt optimization. Our focus was primarily on its prompt optimization capabilities. The core concept behind DSPy involves the interaction between two GLMs:\n\u2022 A low-temperature task GLM: Responsible for evaluating prompt performance with high consistency.\n\u2022 A high-temperature prompt critic GLM: Tasked with improving prompts based on optimization history, leveraging its high temperature setting to generate creative alternatives.\nThe optimization process in DSPy follows these steps:\n1. Initialization: Input a dataset, evaluation metric, and draft prompt (including role setting and input/output field descriptions).\n2. Iterative optimization: Conduct D optimization steps.\n3. Candidate generation: For each step, generate B prompt candidates.\n4. Evaluation: Assess all candidates on the entire dataset.\n5. Ranking and selection: Rank all candidates (including previous ones) by performance, retaining only the top B candidates.\nThis iterative optimization process enables DSPy to explore a wide range of potential prompts, ultimately yielding an optimized version through a process of guided randomization and performance-based selection."}, {"title": "2.5.2 Claude Prompt Generator", "content": "The Claude Prompt Generator [22], an integrated tool within the Claude Console, offers a different approach to prompt optimization. This tool leverages the Claude3.5 Sonnet model to refine user-provided prompts based on a comprehensive, expert-calibrated meta-prompt. The meta-prompt incorporates multiple examples and guidelines, allowing the model to draw upon its extensive knowledge and understanding to enhance the initial prompt."}, {"title": "2.5.3 Comparative Analysis of DSPy and Claude Prompt Generator", "content": "Both tools offer unique advantages and face certain limitations in the context of prompt optimization.\nDSPy:\n\u2022 Strengths:\nAutomated search capability: Systematically explores a wide range of prompt variations.\nPerformance-based selection: Retains and builds upon the most effective prompts.\nPotential for discovering novel prompt structures: The high-temperature critic GLM can generate creative alternatives.\n\u2022 Limitations:\nRigid template structure: Relies on a fixed template with predefined sections, potentially limiting flexibility.\nScope of optimization: The prompt critic GLM can only modify the system prompt and output field names, leaving other structural elements unchanged.\nPotential outdated practices: The fixed template may not incorporate the latest advancements in prompt engineering.\nClaude Prompt Generator:\n\u2022 Strengths:\nUp-to-date meta-prompt: Incorporates recent best practices and well-crafted examples.\nEfficiency: Provides refined prompts quickly without requiring multiple iterations.\n\u2022 Limitations:\nLack of iterative refinement: Does not incorporate performance feedback or allow for multiple optimization cycles.\nComplexity of evaluation: Assessing the performance of generated prompts is challenging and not directly integrated into the tool.\nIn our experimentation, we found that the Claude Prompt Generator often produced superior results compared to DSPy, likely due to its more current meta-prompt and holistic approach to prompt refinement. However, the lack of iterative optimization in the Claude Prompt Generator presents a potential area for future improvement, particularly in scenarios where prompt performance can be quantitatively measured."}, {"title": "3 Method", "content": "The proposed autoscoring system comprises two main phases: an offline optimization phase and an online running phase. The offline phase focuses on preparing datasets, building the Information Retrieval (IR) system, and generating optimal prompts, while the online phase utilizes these components for real-time autoscoring."}, {"title": "3.1 Offline Optimization Phase", "content": "The offline optimization phase consists of three critical steps: training the IR system, constructing the IR vector database, and optimizing GLM prompt templates."}, {"title": "3.1.1 Training the IR System", "content": "Dataset Preparation: A comprehensive dataset for autoscoring typically includes questions, rubrics, reference answers, student responses, and corresponding judgments. However, due to practical constraints such as cost and policy restrictions, we prioritize the essential components: student responses and their associated judgments. The dataset is divided into training and test sets, with the latter held out during the training process to ensure unbiased evaluation.\nWe follow 4 steps to prepare a training set for every dataset to train the Information Retrieval (IR) system:\n1. Division by Question: We first divide the training set into sub-training sets, each corresponding to a specific question.\n2. Answer Pair Formation: Within each sub-training set, we iterate through every student answer, pairing it with all other answers to form answer pairs. This process is done without repetition to avoid redundancy.\n3. Labeling Strategies: We implement and compare two strategies for labeling answer pairs:\n\u2022 Strict Labeling: Pairs are labeled 1 only if both answers are from either the Correct or Incorrect categories. Pairs are labeled 0 if the answers are from different categories or from the same category but not Correct or Incorrect.\n\u2022 General Labeling: Pairs are labeled 1 if both answers are from the same category, regardless of which category it is.\n4. Balancing the Training Set: Both labeling strategies result in an imbalanced training set, with a higher proportion of 0-labeled pairs. To mitigate this imbalance and improve training efficacy, we create a balanced training set by retaining all pairs labeled 1 while randomly sampling an equal number of pairs labeled 0. This approach ensures that we include as many positively labeled pairs as possible while maintaining a balanced distribution between positive and negative examples, which is crucial for effective model training.\nFor special training losses, such as triplet loss, we construct triplet training sets. Each triplet consists of (anchor answer, positive answer, negative answer). We build these sets by: 1) Iterating through each answer in the sub-training sets, designating it as the anchor answer, 2) Matching the anchor with an answer from the same category as the positive answer, 3) Matching the anchor with an answer from a different category as the negative answer. This process is done iteratively and without repetition to ensure comprehensive coverage of the answer space.\nFine-tune IR Backbone. We fine-tune a pre-trained Sentence Transformer model, 'all-MiniLM-L6-v2', as the backbone embedding model in our Information Retrieval (IR) system, for a good balance between performance and computational efficiency.\nTo explore factors contributing to performance differences, we implement and compare two training strategies:\n\u2022 Question-specific Training: This approach involves training a separate IR backbone for each question. Each backbone is optimized specifically for its target question using the corresponding sub-training set. During the online running phase, the IR pipeline dynamically selects the appropriate backbone based on the question being evaluated. This strategy allows for highly specialized models but may require more computational resources and storage.\n\u2022 Global Training: In this approach, we train a single global IR backbone for all questions in a dataset. The global training set is created by combining all question-specific sub-training sets. The training performance is optimized based on the overall loss across all questions. This strategy results in a more generalized model that can be applied to any question, potentially sacrificing some question-specific performance for improved efficiency and scalability."}, {"title": "3.1.2 Building the IR Vector Database", "content": "Following the fine-tuning process, we use the embedding model to convert all student responses in the training set into high-dimensional vectors, which are then used to construct a vector database. This database is indexed by these vectors, with each entry containing a JSON object that stores the original, unembedded student response and its corresponding judgment as metadata. When available, the metadata may also include the question, reference answer, and rubric."}, {"title": "3.1.3 Optimizing GLM Prompt Templates", "content": "To generate effective autoscoring prompt templates, we utilize the Claude Prompt Generator. This advanced tool transforms our initial prompt drafts into well-structured task prompts optimized for autoscoring. Our initial attempt included specified input fields, judgment criteria, and format requirements. The resulting output is a clear, step-by-step prompt encapsulated in HTML tags, optimized to guide the GLM in producing consistent and accurate autoscoring results. The full prompt template is available in Appendix A for reference."}, {"title": "3.2 Online Running Phase", "content": "During the online running phase, our system processes each test response through a sequence of three steps: 1) retrieve K similar responses, 2) compose GLM prompt with retrievals, 3) call task GLM and parse the result. The workflow is shown in 2."}, {"title": "3.2.1 Top-k Retrieval", "content": "When presented with a test response, the system first projects the response into a vector using the fine-tuned embedding model. Then, it computes cosine similarities between this vector and all vectors in the database. Subsequently, the system re-ranks all vectors based on these similarities in descending order and returns the top k entries, where k is a hyper-parameter that varies on the fly. This approach ensures that the most relevant historical responses are retrieved for each new test response."}, {"title": "3.2.2 GLM Prompt Composition", "content": "The system composes the actual GLM task prompts by integrating the retrieved entries into the optimized prompt templates. While the core content of the prompts remains consistent across all tests, minor adjustments may be made to accommodate specific experimental requirements. Any modifications typically involve manually removing content that may not be applicable to certain experiments, but these adjustments are minor and do not significantly alter the prompt's overall structure or intent."}, {"title": "3.2.3 GLM Autoscoring", "content": "The final step involves sending the composed task prompts to the GLM for autoscoring. The GLM processes the prompt and generates a judgment for the student response. To ensure consistency and facilitate further analysis, the system uses regular expressions to clean and parse GLM results."}, {"title": "4 Experiments", "content": "Many works use conventional F1 scores for evaluation. We also use them for comparison to ensure consistency with existing literature and to provide a comprehensive assessment of our model's performance.\n\u2022 Accuracy (ACC): This metric represents the proportion of correctly graded answers across all classes. It is calculated as:\n$ACC = \\frac{Number \\ of \\ correct \\ predictions}{Total \\ number \\ of \\ predictions}$\nSince one test sample can only be classified into one category in our task, the Accuracy is equivalent to F1 score.\n\u2022 Macro Average F1 score (M-F1): This metric independently calculates precision, recall, and F1 scores for each label, then takes the arithmetic mean of these scores across all categories. The formula is:\n$M-F1 = \\frac{1}{n} \\sum_{i=1}^{n} ACC_i$\nwhere n is the number of categories and ACC is the Accuracy for the i-th class. This approach treats all categories equally, regardless of their size, making it particularly useful for evaluating performance on minority categories.\n\u2022 Weighted Average F1 score (W-F1): This metric calculates the F1 score for each class, then combines them by considering the proportion of test samples in each class. The formula is:\n$W-F1 = \\sum_{i=1}^{n} w_i ACC_i,$\nwhere wi is the proportion of samples in the i-th class. This approach ensures that larger classes have a greater influence on the final score, reflecting their prevalence in the dataset. It provides a balance between the micro and macro averages, accounting for both class imbalance and overall performance."}, {"title": "4.1 Metrics", "content": "Many works use conventional F1 scores for evaluation. We also use them for comparison to ensure consistency with existing literature and to provide a comprehensive assessment of our model's performance.\n\u2022 Accuracy (ACC): This metric represents the proportion of correctly graded answers across all classes. It is calculated as:\n$ACC = \\frac{Number \\ of \\ correct \\ predictions}{Total \\ number \\ of \\ predictions}$\nSince one test sample can only be classified into one category in our task, the Accuracy is equivalent to F1 score.\n\u2022 Macro Average F1 score (M-F1): This metric independently calculates precision, recall, and F1 scores for each label, then takes the arithmetic mean of these scores across all categories. The formula is:\n$M-F1 = \\frac{1}{n} \\sum_{i=1}^{n} ACC_i$\nwhere n is the number of categories and ACC is the Accuracy for the i-th class. This approach treats all categories equally, regardless of their size, making it particularly useful for evaluating performance on minority categories.\n\u2022 Weighted Average F1 score (W-F1): This metric calculates the F1 score for each class, then combines them by considering the proportion of test samples in each class. The formula is:\n$W-F1 = \\sum_{i=1}^{n} w_i ACC_i,$\nwhere wi is the proportion of samples in the i-th class. This approach ensures that larger classes have a greater influence on the final score, reflecting their prevalence in the dataset. It provides a balance between the micro and macro averages, accounting for both class imbalance and overall performance."}, {"title": "4.2 Overall Settings", "content": "In all experiments, the proposed approach utilized a question-specific training strategy with Cosine Sentence Loss on training sets with a general labeling strategy, unless otherwise specified. All results are averaged from three runs to ensure consistency."}, {"title": "4.3 Main Results", "content": "In this section, we present the main results of our experiments across all datasets. We focus on three specific datasets: SCIENTSBANK 3-way, SCIENTSBANK 2-way, and BEETLE 5-way. It's worth noting that the SCIENTSBANK 5-way dataset is no longer publicly accessible, and therefore, it has been excluded from this study. GLM abbreviations: Haiku: Claude 3 Haiku, Sonnet: Claude 3.5 Sonnet. We compare our proposed approach with several previous state-of-the-art results. State-of-the-art results are referred from [35]."}, {"title": "4.3.1 SCIENTSBANK 3-way", "content": "This experiment presents the results of the SCIENTSBANK 3-way classification task across three scenarios. The results are shown in 2. The Proposed Approach outperforms all other models across all scenarios and metrics. Performance generally decreases from unseen answer to unseen question and unseen domain. This limitation arises because the unseen question and unseen domain test sets contain questions that are not present in the training sets. As a result, the proposed approach cannot provide relevant examples for the RAG GLM. Specifically, by using Claude 3.5 Sonnet, our approach increases 9.04%, 29.44%, and 22.08% on macro F1 score to the best previous results on unseen answer, unseen question, and unseen domain, respectively."}, {"title": "4.3.2 SCIENTSBANK 2-way", "content": "As shown in 3, the proposed approach also outperforms other results for the SCIENTSBANK 2-way classification task. As the 2-way task is easier than the 3-way task, our approach's performance increase is less. Specifically, by using Claude 3.5 Sonnet, our approach increases 3.69%, 7.09%, and 6.09% on macro F1 score to the best previous results on unseen answer, unseen question, and unseen domain, respectively."}, {"title": "4.3.3 Beetle 5-way", "content": "We were unable to find any reliable benchmark results for the Beetle 5-way task, so we present only our own findings in Table 4. It's important to note that this is a 5-way classification task, where random guessing would yield only 20% accuracy. In light of this, our proposed approach demonstrates a significant improvement, increasing the accuracy in unseen answer scenario by 267.9% to reach 73.58% when using Claude 3.5 Sonnet. It is interesting to find that Claude 3.5 Sonnet has a worse performance than Claude 3 Haiku in unseen question scenario."}, {"title": "4.4 Ablation Study", "content": "This section presents a series of ablation studies designed to investigate the individual contributions of key components in our proposed model. All experiments are conducted on the SCIENTSBANK 3-way task using the Claude 3 Haiku model as a trade-off between performance and cost. Through these experiments, we aim to provide insights into the relative importance of different architectural choices and training strategies."}, {"title": "4.4.1 Effectiveness of input fields", "content": "Table 5 shows the results of an ablation study examining the effectiveness of different input field combinations for the SCIENTSBANK 3-way task across all three scenarios. The fields Answer, Reference, and Question refer to the student's answer, reference answer, and question, respectively. The proposed approach uses all three input fields and performs best across all scenarios and metrics. In addition, the improvement is most pronounced in the unseen question and unseen domain scenarios. In detail, our approach increases 7.73%, 173.39%, and 107.24% on macro F1 score on unseen answer, unseen question, and unseen domain, respectively."}, {"title": "4.4.2 Effectiveness of IR systems", "content": "The second ablation study investigates the efficacy of various IR pipelines. The results are presented in Table 6. \"Pretrained IR\" refers to the use of a pre-trained Sentence Transformer model, specifically \u2018all-MiniLM-L6-v2,' without any fine-tuning for the IR pipeline. \"Global IR\" denotes the IR pipeline trained using a global training strategy, while \"Question-specific IR\" indicates an IR pipeline trained using a question-specific approach. The proposed approach used the question-specific training strategy with general labeling."}, {"title": "4.4.3 Effectiveness of IR loss function", "content": "Table 7 presents the results of an ablation study comparing the performance of different loss functions when fine-tuning the IR pipeline. The findings indicate that the Cosine Sentence Loss demonstrated superior performance compared to the other two loss functions. Specifically, it improved the macro-F1 score by 5.92% and 2.74% relative to the triplet loss and cosine similarity, respectively."}, {"title": "4.4.4 Effectiveness of prompts", "content": "Table 8 presents the results of an ablation study comparing the performance of different prompts. In particular, we compare DSPy-style prompt templates with Claude Prompt Generator-generated prompt templates. Using the Claude Prompt Generator-generated prompts, the macro-F1 score is increased by 13.14%, 3.15%, and 10.87% on unseen answer, unseen question, and unseen domain compared to DSPy-style prompts respectively."}, {"title": "4.4.5 Effectiveness of RAG", "content": "In previous experiments, all models exhibited lower F1 scores on the unseen question and unseen domain scenarios due to domain shift. To address this, we designed an experiment to explore how RAG improves autoscoring performance. The results are presented in Table 9.\nWe extracted a certain fraction of the test set to build a vector database, enabling the IR pipeline to provide examples for RAG GLM autoscoring. It's important to note that the IR model was not further fine-tuned on this extracted fraction; the data was solely used for RAG purposes."}, {"title": "5 Discussion", "content": "There are many researchers who have pursued the use of GLMs for AES and ASAS [36, 37, 38]. Large proprietary GLMs present challenges for researchers due to inaccessible model weights and hardware requirements for fine-tuning. Our work showcases how RAG can effectively leverage extensive datasets without these limitations. While this approach might seem applicable to AES, we believe it is not suitable. The qualitative nature of holistic rubrics in essay evaluation contrasts with the semantic-based criteria used in short answer scoring; we believe that this might make RAG less appropriate for AES tasks.\nWe emphasize a key insight: RAG systems frequently rely on pretrained semantic similarity models that power these IR pipelines. Since these IR components carry out the bulk of the critical work in RAG, their accuracy can significantly impact the overall results. Our research demonstrates the substantial benefits of customizing the semantic similarity model that forms the foundation of these IR pipelines. Our ablation studies also highlight how best to fine-tune these models; using Cross-Entropy loss and question-specific similarity models.\nThis pipeline can be enhanced by incorporating a follow up prompt aimed at providing feedback. In this setup, the language model could analyze the reference answer or training responses to identify key information elements necessary for a comprehensive answer. However, as with many applications of GLMs in educational settings, such a process would require rigorous oversight and validation before it could be considered for use in formative assessment programs."}, {"title": "A Prompt Templates", "content": "A.1 SCIENTSBANK 3-way"}, {"title": "A.1 SCIENTSBANK 3-way", "content": "A.1.1 Unseen Answer"}, {"title": "A.1.1 Unseen Answer", "content": "This template is generated by the Claude Prompt Generator.\nYou are an expert grader tasked with evaluating answers to a given question. Your goal is to carefully analyze the provided information and make a judgment on the correctness of a new answer based on specific criteria. Follow these steps:\n1. Read the following question carefully:\n<question>\n{{QUESTION}}\n</question>\n2. Now, consider the reference answer, which is the gold standard for correctness:\n<reference_answer>\n{{REFERENCE_ANSWER}}\n</reference_answer>\n3. Review the following examples of similar answers and their corresponding judgments from a golden grader:\n<examples>\n{{EXAMPLES}}\n</examples>\n4. Now, examine the new answer that needs to be evaluated:\n<new_answer>\n{{NEW_ANSWER}}\n</new_answer>\n5. Analyze the new answer by comparing it to the reference answer and the examples provided. Consider the following criteria:\na. Is the new answer a complete paraphrase of the reference answer that correctly answers the question?\nb. Is the new answer partially correct, irrelevant to the question, or out of the domain?\nc. Does the new answer explicitly contradict the reference answer?\n6. Based on your analysis, choose one of the following judgments:\n\"correct\": if the new answer is a complete paraphrase of the reference answer that answers the question correctly.\n\"incorrect\": if the new answer is partially correct, irrelevant to the question, or out of the domain.\n\"contradictory\": if the new answer explicitly contradicts the reference answer.\n7. Provide your judgment using the following format:\n<judgment>\n[Insert your chosen judgment here: correct, incorrect, or contradictory]\n</judgment>\nImportant notes:\n- Do not provide any explanations or additional text apart from the judgment itself.\nEnsure that your judgment aligns with the most similar answer in the examples provided.\nFocus solely on the content and meaning of the answers, not on minor differences in wording or style."}]}