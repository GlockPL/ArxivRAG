{"title": "GENERATIVE LANGUAGE MODELS WITH RETRIEVAL AUGMENTED GENERATION FOR AUTOMATED SHORT ANSWER SCORING", "authors": ["Zifan Wang", "Christopher Ormerod"], "abstract": "Automated Short Answer Scoring (ASAS) is a critical component in educational assessment. While traditional ASAS systems relied on rule-based algorithms or complex deep learning methods, recent advancements in Generative Language Models (GLMs) offer new opportunities for improvement. This study explores the application of GLMs to ASAS, leveraging their off-the-shelf capabilities and performance in various domains. We propose a novel pipeline that combines vector databases, transformer-based encoders, and GLMs to enhance short answer scoring accuracy. Our approach stores training responses in a vector database, retrieves semantically similar responses during inference, and employs a GLM to analyze these responses and determine appropriate scores. We further optimize the system through fine-tuned retrieval processes and prompt engineering. Evaluation on the SemEval 2013 dataset demonstrates a significant improvement on the SCIENTSBANK 3-way and 2-way tasks compared to existing methods, highlighting the potential of GLMs in advancing ASAS technology.", "sections": [{"title": "1 Introduction", "content": "An effective assessment program employs various question formats, each designed to evaluate specific standards. Standards involving comprehension and knowledge are often best assessed using short answer questions, requiring students to construct their own responses. Automated Short Answer Scoring (ASAS) uses natural language processing (NLP) and statistical models to evaluate student responses to open-ended questions. The earliest statistical models were frequency and rule-based model [1]. As methods in NLP have advanced, the models applied to ASAS have become more sophisticated. These methods include clustering methods [2], a mixture of recurrent and feed-forward neural networks [3], language models [4], and ensembles of networks [5].\nCreating an Automated Short Answer Scoring (ASAS) system is primarily costly due to the need for high-quality, manually scored responses for training. A significant challenge in ASAS development is reducing the required number of training samples while maintaining scoring quality. Recent advancements in generative language models (GLMs) have led to promising developments in zero-shot, one-shot, and few-shot learning techniques [6]. Applying these approaches could significantly reduce the number of training responses needed for a high-quality scoring pipeline. Our research aims to leverage these methods, utilizing advanced GLMs like ChatGPT [7] and Claude [8], to maximize scoring quality with minimal training samples.\nIn this study, we focus on the SemEval-2013 joint student response dataset [9]. This dataset has been subjected to many traditional approaches [10, 11, 12, 13, 14, 15, 16, 17]. Pretrained encoder-based language models like BERT [18] and ELECTRA [19] have set impressive benchmarks [4]."}, {"title": "2 Background", "content": "For this study, we utilized the dataset from the SemEval-2013 task 7 on joint student responses [9]. This dataset contains two distinct corpora; the BEETLE dataset and SciEntsBank dataset. The BEETLE dataset consists of 3000 student responses to a set of 56 questions relating to electricity and electrons. The SciEntsBank consists of approximately 10,000 answers to a range of 197 questions relating to 15 different scientific domains. Most responses were 1 or 2 sentences long and assessed specific comprehension skills relating to the material provided. This serves as an interesting use case for the method we propose because traditional ASAS systems require a single model per question, however, each corpus contains relatively few answers per question.\nThe dataset underwent manual categorization using three distinct classification methods: a 5-category system, a 3-category system, and a 2-category system. In the 5-category classification, each student's answer was assigned to one of the following groups:\n\u2022 Correct: A paraphrase of the reference answer.\n\u2022 Partially correct incomplete: Contains some but not all information from the reference answer.\n\u2022 Contradictory: The student answer and reference answer contradict each other.\n\u2022 Irrelevant: Provides information on the topic but not relevant to the answer.\n\u2022 Non-domain: Does not provide domain content. e.g., \"I don't know\"\nThe 3-way classification was derived from the 5-way scheme by grouping Irrelevant, and Non-domain responses into a single Incorrect category. The 2-way simplifies the 5-way scheme by categorizing all responses except those labeled Correct as Incorrect.\nBoth the BEETLE dataset and the SciEntsBank dataset have two distinct test sets according to the conditions of the test:\n\u2022 Unseen answers (UA): A portion of student responses for each question in the training dataset was randomly selected and set aside. This reserved subset forms a separate evaluation set, allowing assessment of the system's performance on familiar questions (those present in the training data) but with previously unseen student answers.\n\u2022 Unseen questions (UQ): An evaluation set was created to measure the system's effectiveness on new questions within the same subject areas as the training data. This set consists of all student responses to a randomly chosen subset of questions from each dataset, which were entirely withheld from the training process.\nIn addition to these two test sets, the SciEntsBank dataset contains a third test set:\n\u2022 Unseen Domain (UD): An evaluation set composed of responses covering domains absent from the training data, designed to assess the system's performance across unfamiliar domains."}, {"title": "2.2 Information Retrieval", "content": "An Information Retrieval (IR) pipeline is a system that provides relevant information (i.e., documents) from a database based on user queries. The key to this approach is to map a document to a vector that encodes information relevant to queries, called its representation, in some vector space. The idea is that semantically similar documents are mapped to similar vectors. More formally, if $d_1$ and $d_2$ are documents, and $ is an embedding that maps the set of documents to $R^n$, then the semantic similarity between the documents, $S(d_1, d_2)$, is modeled by the cosine similarity of the embeddings, given by\n$S(d_1, d_2) = \\frac{\\phi(d_1) \\cdot \\phi(d_2)}{||\\phi(d_1) || ||\\phi(d_2) ||}\u02d9$\nConsider a set of documents, $D$, consisting of elements $d_1, ..., d_n$. We can represent their normalized embeddings as rows in a matrix $A$. If we can convert a query into a vector $x$, then finding the $k$ most relevant documents to that query in $D$ becomes equivalent to identifying the $k$ largest values in the vector resulting from the matrix multiplication $Ax$.\nIn our case, $D$ is a set of student responses In the training set, our query is a response, $d$, in the test set. Since we expect $d$ to be of the same form as the elements of $D$, it makes sense that $x = \\phi(d)$. In settings like question-answering systems, where the queries are of a different form to the documents being retrieved, the two embeddings can be different [26]. Examples of more traditional methods of determining the embedding, $\\phi$, have included term frequency-based approaches like tf-idf [27] and recurrent approaches [28]. These approaches have been superseded by transformer-based approaches [26]."}, {"title": "2.3 Generative Language Models", "content": "A Generative Language Model (GLM) is a type of neural network trained on massive amounts of text data based on deep learning techniques. The most ubiquitous examples in the literature have become transformer-based models [29]. The most general architecture consists of an encoder, which maps text to a vector space, and a decoder that maps that vector space back to text. The advantage these networks have is that we can improve the performance on downstream tasks by pertaining these models on large corpora of data [18, 30]. There are three classes of such models; encoder networks such as BERT [18], decoder networks such as GPT [30], and encoder-decoder networks such as T5 [31].\nThe embedding we use as part of our IR pipeline is a pretrained encoder network trained for semantic similarity [26] derived from a truncation of the BERT model [18]. We fine-tune this model for our own use case.\nThe second type of model we use is a decoder network. The fundamental task of decoder GLMs is to predict the next token from a sequence of prior tokens [30]. Modern GLMs generally have an order of magnitude more parameters and are trained in several stages. The first stage is to pretrain the model on a large corpus of data. The model is then fine-tuned to perform instructions [30]. Lastly, the model is subjected to reinforcement learning [32]. The remarkable thing about these models is that they are often capable of performing tasks not necessarily similar to tasks in their training data [8, 7]."}, {"title": "2.4 Retreival Augmented Generation", "content": "Even though GLMs possess extensive knowledge and demonstrate excellent performance on various tasks, including auto-scoring, the vanilla auto-scoring approach\u2014providing a GLM with answers, questions, and rubrics, and asking it to give feedback-presents challenges. Grading answers solely based on questions and rubrics is difficult, even for GLMs, let alone that rubrics are not always readily available.\nStudies indicate that providing additional context and examples can enhance their performance. Therefore, we incorporate documents retrieved by our IR system as examples in the prompt for the GLM to reference. This method enables GLMs to learn from the examples and improve their grading capabilities, with or without a rubric, by implicitly conveying the logic of grading through the examples."}, {"title": "2.5 Prompt Optimization", "content": "Recent studies have demonstrated that carefully crafted prompts can significantly enhance performance across various tasks. Effective prompts typically employ several key techniques:\n\u2022 Structured format: Organize the prompt to clearly present descriptions, requirements, examples, procedures, and scoring criteria.\n\u2022 Role assignment: Explicitly define the role the Generative Language Model should assume.\n\u2022 XML tag utilization: Employ XML tags to delineate input and output fields, enhancing clarity and structure.\n\u2022 Task-specific techniques: Incorporate methods like chain-of-thought reasoning when appropriate, though this may not be suitable for all applications, such as autoscoring.\nWhile we initially drafted our prompts incorporating these techniques, we recognized the need for further optimization. To address this, we explored two advanced tools: DSPy [33, 34] and the Claude Prompt Generator. These tools offer distinct approaches to prompt refinement, each with its own strengths and limitations."}, {"title": "2.5.1 DSPy", "content": "DSPy, described as \"a framework for algorithmically optimizing LM prompts and weights\" [33], provides a systematic approach to prompt optimization. Our focus was primarily on its prompt optimization capabilities. The core concept behind DSPy involves the interaction between two GLMs:"}, {"title": "3 Method", "content": "The proposed autoscoring system comprises two main phases: an offline optimization phase and an online running phase. The offline phase focuses on preparing datasets, building the Information Retrieval (IR) system, and generating optimal prompts, while the online phase utilizes these components for real-time autoscoring."}, {"title": "3.1 Offline Optimization Phase", "content": "The offline optimization phase consists of three critical steps: training the IR system, constructing the IR vector database, and optimizing GLM prompt templates."}, {"title": "3.1.1 Training the IR System", "content": "Dataset Preparation: A comprehensive dataset for autoscoring typically includes questions, rubrics, reference answers, student responses, and corresponding judgments. However, due to practical constraints such as cost and policy restrictions, we prioritize the essential components: student responses and their associated judgments. The dataset is divided into training and test sets, with the latter held out during the training process to ensure unbiased evaluation.\nWe follow 4 steps to prepare a training set for every dataset to train the Information Retrieval (IR) system:\n1. Division by Question: We first divide the training set into sub-training sets, each corresponding to a specific question.\n2. Answer Pair Formation: Within each sub-training set, we iterate through every student answer, pairing it with all other answers to form answer pairs. This process is done without repetition to avoid redundancy.\n3. Labeling Strategies: We implement and compare two strategies for labeling answer pairs:\n\u2022 Strict Labeling: Pairs are labeled 1 only if both answers are from either the Correct or Incorrect categories. Pairs are labeled 0 if the answers are from different categories or from the same category but not Correct or Incorrect.\n\u2022 General Labeling: Pairs are labeled 1 if both answers are from the same category, regardless of which category it is.\n4. Balancing the Training Set: Both labeling strategies result in an imbalanced training set, with a higher proportion of 0-labeled pairs. To mitigate this imbalance and improve training efficacy, we create a balanced training set by retaining all pairs labeled 1 while randomly sampling an equal number of pairs labeled 0. This approach ensures that we include as many positively labeled pairs as possible while maintaining a balanced distribution between positive and negative examples, which is crucial for effective model training.\nFor special training losses, such as triplet loss, we construct triplet training sets. Each triplet consists of (anchor answer, positive answer, negative answer). We build these sets by: 1) Iterating through each answer in the sub-training sets, designating it as the anchor answer, 2) Matching the anchor with an answer from the same category as the positive answer, 3) Matching the anchor with an answer from a different category as the negative answer. This process is done iteratively and without repetition to ensure comprehensive coverage of the answer space.\nFine-tune IR Backbone. We fine-tune a pre-trained Sentence Transformer model, 'all-MiniLM-L6-v2', as the backbone embedding model in our Information Retrieval (IR) system, for a good balance between performance and computational efficiency.\nTo explore factors contributing to performance differences, we implement and compare two training strategies:\n\u2022 Question-specific Training: This approach involves training a separate IR backbone for each question. Each backbone is optimized specifically for its target question using the corresponding sub-training set. During the online running phase, the IR pipeline dynamically selects the appropriate backbone based on the question being evaluated. This strategy allows for highly specialized models but may require more computational resources and storage.\n\u2022 Global Training: In this approach, we train a single global IR backbone for all questions in a dataset. The global training set is created by combining all question-specific sub-training sets. The training performance is optimized based on the overall loss across all questions. This strategy results in a more generalized model that can be applied to any question, potentially sacrificing some question-specific performance for improved efficiency and scalability."}, {"title": "3.1.2 Building the IR Vector Database", "content": "Following the fine-tuning process, we use the embedding model to convert all student responses in the training set into high-dimensional vectors, which are then used to construct a vector database. This database is indexed by these vectors, with each entry containing a JSON object that stores the original, unembedded student response and its corresponding judgment as metadata. When available, the metadata may also include the question, reference answer, and rubric."}, {"title": "3.1.3 Optimizing GLM Prompt Templates", "content": "To generate effective autoscoring prompt templates, we utilize the Claude Prompt Generator. This advanced tool transforms our initial prompt drafts into well-structured task prompts optimized for autoscoring. Our initial attempt included specified input fields, judgment criteria, and format requirements. The resulting output is a clear, step-by-step prompt encapsulated in HTML tags, optimized to guide the GLM in producing consistent and accurate autoscoring results. The full prompt template is available in Appendix A for reference."}, {"title": "3.2 Online Running Phase", "content": "During the online running phase, our system processes each test response through a sequence of three steps: 1) retrieve K similar responses, 2) compose GLM prompt with retrievals, 3) call task GLM and parse the result."}, {"title": "3.2.1 Top-k Retrieval", "content": "When presented with a test response, the system first projects the response into a vector using the fine-tuned embedding model. Then, it computes cosine similarities between this vector and all vectors in the database. Subsequently, the system re-ranks all vectors based on these similarities in descending order and returns the top k entries, where k is a hyper-parameter that varies on the fly. This approach ensures that the most relevant historical responses are retrieved for each new test response."}, {"title": "3.2.2 GLM Prompt Composition", "content": "The system composes the actual GLM task prompts by integrating the retrieved entries into the optimized prompt templates. While the core content of the prompts remains consistent across all tests, minor adjustments may be made to accommodate specific experimental requirements. Any modifications typically involve manually removing content that may not be applicable to certain experiments, but these adjustments are minor and do not significantly alter the prompt's overall structure or intent."}, {"title": "3.2.3 GLM Autoscoring", "content": "The final step involves sending the composed task prompts to the GLM for autoscoring. The GLM processes the prompt and generates a judgment for the student response. To ensure consistency and facilitate further analysis, the system uses regular expressions to clean and parse GLM results."}, {"title": "4 Experiments", "content": "Many works use conventional F1 scores for evaluation. We also use them for comparison to ensure consistency with existing literature and to provide a comprehensive assessment of our model's performance.\n\u2022 Accuracy (ACC): This metric represents the proportion of correctly graded answers across all classes. It is calculated as:\n$ACC = \\frac{Number of correct predictions}{Total number of predictions}$\nSince one test sample can only be classified into one category in our task, the Accuracy is equivalent to F1 score.\n\u2022 Macro Average F1 score (M-F1): This metric independently calculates precision, recall, and F1 scores for each label, then takes the arithmetic mean of these scores across all categories. The formula is:\n$M-F1 = \\frac{\\sum_{i=1}^{n} ACC_i}{n}$\nwhere n is the number of categories and ACC is the Accuracy for the i-th class. This approach treats all categories equally, regardless of their size, making it particularly useful for evaluating performance on minority categories.\n\u2022 Weighted Average F1 score (W-F1): This metric calculates the F1 score for each class, then combines them by considering the proportion of test samples in each class. The formula is:\n$W-F1 = \\sum_{i=1}^{n} W_i ACC_i,$\nwhere $w_i$ is the proportion of samples in the i-th class. This approach ensures that larger classes have a greater influence on the final score, reflecting their prevalence in the dataset. It provides a balance between the micro and macro averages, accounting for both class imbalance and overall performance."}, {"title": "4.2 Overall Settings", "content": "In all experiments, the proposed approach utilized a question-specific training strategy with Cosine Sentence Loss on training sets with a general labeling strategy, unless otherwise specified. All results are averaged from three runs to ensure consistency."}, {"title": "5 Discussion", "content": "There are many researchers who have pursued the use of GLMs for AES and ASAS [36, 37, 38]. Large proprietary GLMs present challenges for researchers due to inaccessible model weights and hardware requirements for fine-tuning. Our work showcases how RAG can effectively leverage extensive datasets without these limitations. While this approach might seem applicable to AES, we believe it is not suitable. The qualitative nature of holistic rubrics in essay evaluation contrasts with the semantic-based criteria used in short answer scoring; we believe that this might make RAG less appropriate for AES tasks.\nWe emphasize a key insight: RAG systems frequently rely on pretrained semantic similarity models that power these IR pipelines. Since these IR components carry out the bulk of the critical work in RAG, their accuracy can significantly impact the overall results. Our research demonstrates the substantial benefits of customizing the semantic similarity model that forms the foundation of these IR pipelines. Our ablation studies also highlight how best to fine-tune these models; using Cross-Entropy loss and question-specific similarity models.\nThis pipeline can be enhanced by incorporating a follow up prompt aimed at providing feedback. In this setup, the language model could analyze the reference answer or training responses to identify key information elements necessary for a comprehensive answer. However, as with many applications of GLMs in educational settings, such a process would require rigorous oversight and validation before it could be considered for use in formative assessment programs."}]}