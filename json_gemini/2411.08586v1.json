{"title": "Optimizing Automatic Summarization of Long Clinical Records\nUsing Dynamic Context Extension\nTesting and Evaluation of the NBCE Method", "authors": ["Guoqing ZHANG", "Keita FUKUYAMA", "Kazumasa KISHIMOTO", "Tomohiro KURODA"], "abstract": "Summarizing patient clinical notes is vital for reducing documentation burdens. Current manual\nsummarization makes medical staff struggle. We propose an automatic method using LLMs, but long inputs\ncause LLMs to lose context, reducing output quality especially in small size model. We used a 7B model, open-\ncalm-7b, enhanced with Native Bayes Context Extend and a redesigned decoding mechanism to reference one\nsentence at a time, keeping inputs within context windows, 2048 tokens. Our improved model achieved near-\nparity with Google's over 175B Gemini on ROUGE-L metrics with 200 samples, indicating strong performance\nusing less resources, enhancing automated EMR summarization feasibility.", "sections": [{"title": "1. Introduction", "content": "In clinical practice, summarizing patient\nclinical records, commonly known as patient\nsummaries, is valuable for quickly understanding\npatient information. However, since a significant\nportion of healthcare professionals' working hours\nis devoted to creating these summaries, there is a\nneed to improve the efficiency and automation of\nthis process. With the recent advancements in\nlarge language models (LLMs), it has become\nincreasingly possible to automate complex tasks\ninvolving natural language. Text summarization\nusing LLMs has shown practical performance in\nsummarizing academic papers and news content,\nraising expectations for its application in\ngenerating summaries within healthcare settings.\nHowever, when implementing automated\nsummarization of clinical records using LLMs, the\nlimitations of the context window become a\nchallenge. While individual clinical entries may\nnot be particularly lengthy, the accumulated\nrecords over extended patient treatment periods\ncan eventually exceed the maximum token limit\nthat each LLM can handle, significantly\nsurpassing the context window capacity. According\nto research by Rewon Child, the memory and\ncomputational requirements of the core formula of\nTransformers, Attention, increase proportionally\nto the square of the sequence length [1]. As the\ncontext window length increases, the amount of\ninput information the model must process also\ngrows, leading to an increase in the number of\nmodel parameters and computational complexity,\nthereby enlarging the model size. Specifically, in\nLLMs, the Attention mechanism needs to compute\nthe relationships between every word and all other\nwords within the input sequence. The\ncomputational load for this process increases\nexponentially with the length of the context\nwindow. Consequently, as the number of model\nparameters grows, so does the need for storage and\ncomputational resources, such as GPU memory.\nWhen inputs exceed the context window set during\ntraining, the combination of long-term Attention\ndeficiency in the Attention algorithm leads to\ncatastrophic forgetting when LLMs process long\ncontexts, resulting in a decline in output quality\n[2][3][4].\nIn the case of recent cloud-based LLMs with over\n200B parameters, the maximum token capacity\nhas also grown significantly, enabling the\nsummarization of long texts, 8,192 tokens.\nHowever, when creating summaries of long clinical\ntexts in healthcare settings, the nature of the data\nimposes certain restrictions on the use of cloud-\nbased LLMs. On the other hand, the use of LLMs\non-premises is limited by GPU memory size, and it\nis not easy to introduce expensive hardware\ncapable of handling models exceeding 100B\nparameters. By solving this issue with relatively\nsmaller on-premises LLMs, ranging from 7B to\n30B in scale, driven by smaller GPUs within\nelectronic medical record (EMR) networks, it is\nexpected that a summarization system that can be\npromptly deployed in real-world clinical settings\ncan be realized."}, {"title": "2. Related Works", "content": "ClinicalBERT is known for its research on using\nlanguage models to analyze clinical data [5]. This\nresearch focuses on analyzing time-series data\nwithin hospital information systems to predict the\nlikelihood of patient readmission after discharge at\na specific point in time, but it does not produce\nText-to-Text outputs. Following the introduction of\nChatGPT, research on the automatic\nsummarization of clinical records using large\nlanguage models has been reported by Dave Van\nVeen [6]. Using the Open-i radiology report dataset,\nhe evaluated current mainstream open-source and\nclosed-source models using methods like BLEU\nand ROUGE-L, conducting multiple evaluations\nwith different prompts and hyperparameters. The\nbest result reported was a ROUGE-L (F1) score of\n35.5%.\nIn terms of summarizing Japanese medical texts,\nthere are reports utilizing case report articles, but\nthese mostly use relatively short and well-\nstructured texts [7]. In contrast, clinical records in\nhealthcare settings include significantly longer\ntexts compared to radiology reports or case studies.\nNakagawa et al. reported a system that extracts\nnecessary elements by selecting them after Named\nEntity Recognition (NER) and then generates text.\nHowever, due to the selection process post-NER,\nthe summary generation itself is not fully\nautomated [8].\nRegarding research on extending the context\nwindow, two main approaches are highlighted:\nParallel Context Window (PCW) [9] and Naive\nBayes-based Context Extension (NBCE) [10]. Su\nJianlin's paper compares NBCE and PCW,\nshowing that NBCE achieves superior results\nacross multiple benchmarks."}, {"title": "3. Proposal", "content": "We designed a new method for automatically\ngenerating summaries of clinical records using\nLLMs (Large Language Models), drawing\ninspiration from Dr. Su Jianlin's NBCE (Naive\nBayes-based Context Extension). His research\nincorporates context-referencing information (such\nas news or literature) relevant to user queries. By\nusing a parallel input method, his approach\ncircumvents the context window limitations of\nLLMs, allowing the model to reference a broader\nrange of contextual information when responding\nto user queries. The decoding layer of the LLM was\nmodified to decode input clinical records in parallel,\nwhile dynamically switching context references\nbased on changes in information entropy, thus\nshortening the length of the context used during\nprediction.\nThe study included experiments comparing our\nmodel with Google Gemini, using the same\nprompts and datasets, and reported the ROUGE-L"}, {"title": "4. Method", "content": "We enumerated all combinations and added\nprompt information to create a single training\ninstance, fine-tuning the Open-Calm-7B model,\nmaxing input length is 2048 tokens.\nTo address the issue of losing context due to long\ninputs, we applied Dr. Su Jianlin's NBCE method,\nwhich significantly extends the context window\nlimit by splitting long context and processing them\nindividually as inputs to the LLM. Tokens from\neach output were selected to form the final\nresponse. This approach involved modifying the\nLLM's decoding layer to decode reference texts in\nparallel along with prompts. Using information\nentropy as an indicator, the decoder selects output\ntokens dynamically.\nIn terms of implementation, when the model\nreceives all clinical records for a single patient, an\nalgorithm samples the data set with a sampling\nrate of 0.15 based on the distribution of token\nlengths, and then performs parallel decoding on\nthe sampled dataset.\nDuring decoding, Shannon's information entropy\nwas used. When generating the next token, our\nalgorithm independently computes the Shannon\ninformation entropy of all preceding inputs and\nselects the preceding input with the lowest entropy\nas the basis for the conditional probability\ndistribution of the next token to fetch the most\nadoptive reference content for next token\npredicting, miniest entropy strategy. This process\nis repeated for each generated token until the\npredetermined maximum number of tokens is\nreached (Figure 2: Structure of NBCE decoding\nprocessing. The medical record entries required for\nsummary generation (such as S, O, current medical history,\netc. on the left side) will be input for each individual\nmedical record. The token will be estimated for each input).\nShannon Entropy = -$\\sum_{i=1}^{n}p(token_i) log(p(token_i))$\nThrough our investigation of actual clinical\nrecords, we observed that the length of clinical\nnotes exhibits a high variance. This variability\nmay obscure the patterns between clinical notes\nand their corresponding summaries, making it\nchallenging for large language models to learn\neffectively. However, manually denoising the\ndataset is time-consuming and costly. Therefore,\nwe propose a method to denoise the training set as\nefficiently as possible."}, {"title": "5. Experiment", "content": "6.1 Dataset and Preprocessing\nFor data collection, we sourced patient records\nfrom the electronic medical records database of\nKyoto University Hospital, covering nearly a\ndecade from 2013 to 2023. This dataset includes\napproximately 500,000 samples from around 5,000\npatients. We calculated the cosine similarity for all\nsamples, sorted them in descending order, and\nselected the top 160,000 records as the training set.\nFrom this set, we randomly sampled 2,000 records\nas the validation dataset and 200 records as the\ntest dataset.\n6.2 Evaluation Metrics\nThe generated summary was evaluated using\nthe ROUGE-L (Recall-Oriented Understudy for\nGisting Evaluation-Longest Common Subsequence)\nscore, with the summary created by physicians as\nthe ground truth data. ROUGE-L is one of the\nmetrics used to assess the similarity between the\ngenerated text and the reference text. It is\nprimarily applied in the evaluation of automatic\nsummarization, measuring text similarity based\non the Longest Common Subsequence (LCS). The\ncalculation of ROUGE-L is represented by the\nfollowing formula:\nROUGE L=$\\frac{LCS(generated, Human Write)}{Length of Human Write}$\nLongest Common Subsequence (LCS): ROUGE-\nL is based on the LCS between the generated text\nand the reference text. The LCS refers to the\nlongest subsequence that appears in both\nsequences without rearranging the order of\ncharacters.\nPrecision, Recall, F1 Score: ROUGE-L calculates\nprecision (using the model output as the reference),\nrecall (using the physician-written summary as\nthe reference), and the F1 score (the harmonic\nmean of precision and recall).\nApplications: ROUGE-L is used to evaluate the\nquality of text in tasks such as text summarization,\nmachine translation, and dialogue generation. It is\nparticularly useful for assessing the similarity of\nlonger texts, as it can capture relatively long\nmatching sequences.\nOverall, while ROUGE-L has certain advantages\nin evaluating the fluency and coherence of\ngenerated text, it is known not to fully reflect\nsemantic equivalence between texts."}, {"title": "6.3 Baseline", "content": "We use Google's Gemini as baseline model which\nsize is over 175B and maximum acceptable input\nis 8,192 tokens, do this Comparison experiment.\nThe model we use is one of its 25ths, and the\nmaximum acceptable input is 2048, which is one\nquarter of it, after tuning on 160,000 samples."}, {"title": "6.4. Result", "content": "Here are the ROUGE-L scores evaluated for 228\nsummaries across Human Writing, our model\n(Figure 3), and Google Gemini (Figure 4), average\nscore in Table 2."}, {"title": "8. Discussion", "content": "In this study, we proposed an optimization\nstrategy that enables a small language model with\n7 billion parameters to achieve performance\ncomparable to a commercial model with 175 billion\nparameters on medical clinical summarization\ntasks. This breakthrough offers significant\npractical value, especially in healthcare settings,\nby substantially reducing costs while enhancing\ndata security.\n1. Cost Efficiency and Scalability of Small Models.\nCompared to large language models, small models\n(7B) offer significant advantages in terms of\ndeployment costs. Large models typically require\nexpensive computational resources, such as high-\nperformance GPU clusters, as well as substantial\nmemory and storage capacity, which can limit their\ndeployment in practical applications. Our\napproach enables the 7B model to deliver high-\nquality medical summarization output even in\nenvironments with limited hardware resources.\nThis cost-effective nature makes small models\nmore suitable for large-scale deployment in\nhospitals, not only reducing hardware expenses\nbut also lowering ongoing operational costs\nassociated with cloud computing. As hospitals\nincreasingly demand intelligent healthcare\nsolutions, the affordability of these models can\nfacilitate wider adoption, thereby enhancing the\noverall intelligence of healthcare systems.\n2. Enhancing Data Security and Privacy through\nLocal Deployment. Data security and patient\nprivacy are critical considerations in medical\napplications. Traditional cloud-based Al models\noften require patient data to be uploaded to remote\nservers, which poses risks of data breaches and\ncyberattacks. Our proposed small model can be\nfully deployed within a hospital's local network,\neffectively eliminating the risk of data leakage\nduring transmission. By processing data entirely\non-premises, sensitive patient information is\nbetter protected, ensuring compliance with\nstringent data privacy regulations (e.g., GDPR,\nHIPAA). This localized data processing approach\noffers healthcare institutions a secure way to\nleverage AI technologies while safeguarding\npatient confidentiality.\n3. Reducing Communication Latency to Improve\nClinical Decision-Making. In many clinical\nscenarios that require rapid response, such as\nemergency rooms or operating theaters, the speed\nof model inference directly impacts the timeliness\nof medical decisions. Cloud-based large models\noften suffer from network latency due to the need\nfor remote communication, which can lead to\ndelays in critical situations. In contrast, our small\nmodel, deployed locally, virtually eliminates\ncommunication latency, significantly enhancing\nresponse times. This low-latency characteristic\nmakes local models particularly suitable for real-\ntime decision-making in healthcare settings,\nfurther improving their clinical utility.\n4. Performance Comparison with Large Models\nand Limitations. While our small model\ndemonstrates strong performance in medical\nsummarization tasks, it still has certain\nlimitations compared to commercial models with\n175 billion parameters. Large models inherently\nexcel in handling complex semantic understanding\nand reasoning tasks, potentially offering more\naccurate summaries in highly intricate medical\ncases. However, through fine-tuning and task-\nspecific optimizations, our small model achieves\nhigh performance in most common medical\nsummarization tasks. Thus, it provides a practical\nand effective alternative for resource-constrained\nhospitals, balancing cost and performance."}]}