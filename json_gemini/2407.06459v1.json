{"title": "How Much Progress Did I Make? An Unexplored Human Feedback Signal for Teaching Robots", "authors": ["Hang Yu", "Qidi Fang", "Shijie Fang", "Reuben M. Aronson", "Elaine Schaertl Short"], "abstract": "Enhancing the expressiveness of human teaching is vital for both improving robots' learning from humans and the human-teaching-robot experience. In this work, we characterize and test a little-used teaching signal: progress, designed to represent the completion percentage of a task. We conducted two online studies with 76 crowd-sourced participants and one public space study with 40 non-expert participants to validate the capability of this progress signal. We find that progress indicates whether the task is successfully performed, reflects the degree of task completion, identifies unproductive but harmless behaviors, and is likely to be more consistent across participants. Furthermore, our results show that giving progress does not require extra workload and time. An additional contribution of our work is a dataset of 40 non-expert demonstrations from the public space study through an ice cream topping-adding task, which we observe to be multi-policy and sub-optimal, with sub-optimality not only from teleoperation errors but also from exploratory actions and attempts.", "sections": [{"title": "I. INTRODUCTION", "content": "Robots have already firmly become part of our daily lives, making it crucial to learn from users, especially non-expert users. Learning from Demonstration (LfD) enables robots to learn new skills by observing expert policies [1], [2] while Learning from Human Feedback (LfHF) allows robots to adapt to human preferences or correct wrong behaviors by learning or shaping a policy [3], [4], [5]. More recent work has further shown that using human feedback and demonstrations together can make learning even more effective by reducing the data needs for human feedback [6] and loosening the requirements of demonstrations to be near-optimal [7]. However, while interest in learning fully or partially from humans is high, there is relatively little research on what the most effective forms of human feedback are, especially in combination with human demonstrations.\nHuman feedback and human demonstrations can be com-plementary due to the difference in human knowledge they carry. Demonstrations carry relatively dense and global in-formation including policies and goals, and tend to be less accurate [8]. Human feedback carries relatively sparse and local information such as the correctness or a rating of a robot's action, and giving high-quality feedback can be much easier than giving high-quality demonstrations [9]. Perfect demonstrations are hard to obtain while purely learning from human feedback requires many human labels, often obtained at significant time and expense. To address these challenges and improve the quality of learning, human demonstrations can be combined with human feedback: demonstrations can be used to train an initial policy to improve the sample efficiency of feedback [6], and feedback can be used to refine the policy learned from demonstrations [8].\nIn previous work using human feedback alongside demon-strations, the forms of human feedback used are directly adapted from LfHF [6], [8], which might not be effective for evaluating a demonstration. The quality of a demon-stration or a partial demonstration is typically assessed by comparing it to another [8], [7], [1]. This approach of comparing or ranking demonstrations is extremely hard for naive users, especially when the trajectory is only a partial demonstration [10]. It also neglects the objective quality of a demonstration or a partial demonstration itself: a pair of demonstrations might both be good or both bad, making preferences difficult to provide. This is especially true for non-expert demonstrations: non-expert demonstrations can be noisy, multi-policy, and yet still succeed (we show this in subsection V-C). While feedback like binary evaluation and scalar feedback is capable of assessing the quality of a demonstration, comparative information is unavailable for binary feedback and is unreliable for scalar feedback [11].\nIn this work, we characterize a novel type of human feedback for robot learning: progress, which is used to capture the completion of a task. We show that progress can indicate the extent of task completion, determine if a task is completed, and be robust to unproductive behaviors."}, {"title": "II. BACKGROUND", "content": "By leveraging human knowledge, interactive machine learning allows learning agents to adapt to the needs of individual users and improve sample efficiency [9]. Human knowledge can take a variety of forms, such as semantic representation [12], numerical feedback [5], eye gaze [13], gestures [14], facial expressions [3], and demonstrations [15]. In this work, we focus on human feedback and human demonstrations. Separately, each of these approaches has significant limitations: on the one hand, inferring a policy from human feedback requires a large number of interactions [5], [16], [17], and on the other, error-free demonstrations are rare and expensive to obtain in the real world [15]. To address this, many approaches in Interactive Machine Learning seek to combine human feedback and human demonstration to compensate for the limitations of each.\nLearning from Human Feedback Learning from Human Feedback has emerged as a promising technology for robots or machine-learning agents to learn from humans via in-teractions [18]. LfHF, in general, refers to methods that have three components: feedback collection, policy or reward shaping, and policy optimization. Human feedback can be in a variety of forms, such as verbal [19], [20], numerical [4], [5], [21], and implicit [3], [22]. Three representative works of learning from explicit human feedback are Policy Shaping [4], TAMER [5], and Preference-based policy learning [23]. Human feedback has also been applied to modern Large Language Models (LLMs) [24] to further improve the perfor-mance of trained models by having humans rate the outputs with binary critiques. Despite a large body of work that has been done, human feedback is mostly used as reward signals.\nLearning from Human Demonstrations In robotics, Learn-ing from Demonstrations (LfD) is a method that facilitates robots to learn new skills by imitating humans [25]. The use of LfD offers several advantages, including eliminating the need for expert programming [26], high data efficiency [27], safety for learning [28], and guaranteed performance [29]. The research interest in teaching robots via demon-strations has steadily advanced. LfD methods are capable of producing optimal behaviors with clean demonstrations and sufficient error-free demonstrations [30]. However, due to the optimal assumption on the demonstrations, LfD methods like generative adversarial imitation learning [31] or behavior cloning [32] failed to acquire optimal policy for many robot tasks since even human experts would make mistakes while providing demonstrations [33]. Weighting [34], [33], [2] or ranking [1], [35] demonstrations are considered to be robust methods of learning from noisy demonstrations. However, learning from multiple users and learning from imperfect demonstrations are still challenging problems [15]. One previous work used a technique they refer to as \"reward sketching\"; in practice the annotators were instructed to provide progress [36]. While their work demonstrates the potential of progress for guiding learning, it did not closely investigate the properties of progress or take full advantage of it as a teaching signal, instead using large numbers of these \"reward sketching\" annotations as a loose approximation for a dense reward function.\nUsing Human Demonstrations and Human Feedback Re-cent work has demonstrated that combining human feedback and human demonstrations could overcome many disadvan-tages of using one of them solely, including safety [37], sample efficiency [6], and accuracy [8]. Specifically, work from [6] used demonstrations to train an initial model for efficiently collecting preferences from users. Work from [8] built on [6] and used a model-based method to reduce data from humans. Using human rankings, work from [7] has achieved super-human demonstration performance. Although prior work has achieved great success in consolidating human feedback with human demonstrations, the source of human feedback and human demonstrations are likely from experts or pre-trained agents [38], and the human feedback they used could be more informative. Our work differs from prior work by focusing on non-experts. We conducted our studies with crowd-sourced workers and random passersby. We showed that progress is informative and consistent when human demonstrations are multi-policy and non-optimal."}, {"title": "III. PROGRESS", "content": "Our goal is to improve learning from human feedback with a new teaching signal: progress. In this section, we first define progress, and then introduce our hypothesis.\nWe hypothesize that progress provides complementary information to demonstrations beyond rewards. We charac-terize progress as the accumulative task completion rate over a task based on the current observation, ranging from totally incomplete to complete fully. Our intuition is that: A teaching signal would be more robust to sub-optimal demonstrations and more consistent among users if human teachers could"}, {"title": "A. Progress", "content": "have objective references while providing the teaching signal. For progress specifically, users can use start states and finish states as references. In this work, we use the progress signal as a range from 0 to 100. A progress value of 0 indicates that the task has not yet begun, while a value of 100 signifies task completion. In between, we expect that for any given task t, current state s, any action $a_i$ and $a_j$, and any previous state $s_i$ and $s_j$:\n$$prog_t(s_i, a_i, s) = prog_t(s_j, a_j, s) \\forall i, j$$"}, {"title": "B. Hypotheses", "content": "Ideally, progress is independent of the path taken to the state, irrespective of the sequence of preceding states. However, human feedback is known to be noisy and can only be considered consistent if we view it as an approximate value [11]. Thus, we collect progress by presenting users with a trajectory instead of a single state to increase reliability.\nWe expect that people naturally estimate task completion in their daily lives, so progress should be not hard to give. We also expect that participants would use progress to describe the completion degree of a task. We hypothesize:\nH1. Giving progress does not require extra workload and extra time compared to giving scalar feedback.\nH2. Progress describes the completion rate of a task.\nH3. Progress could correctly indicate if the task is complete even if the robot has made mistakes.\nH4. Progress is more consistent than scalar feedback when demonstrations are non-optimal."}, {"title": "IV. PROGRESS WITH NON-SELF-PROVIDED DEMONSTRATIONS", "content": "We first crowd-sourced users to provide progress with pre-recorded demonstrations for our online studies. This allows us to explore the effectiveness of utilizing progress alone, and examine its applicability across a range of intricate scenarios. We conducted two online studies: Online study I uses three simple tasks to evaluate the workload of giving progress, while online study II involves a long-horizon task comprising six sub-tasks and five scenarios to assess the utility of progress."}, {"title": "A. Online study setups", "content": "First, we validated the workload of providing progress and verified that progress contains unique information relative to scalar feedback in a range of 0 to 100. For each study, we recruited two groups of participants from an online platform. One group of participants was only asked to give progress and the other group of participants was only asked to give scalar feedback. Participants in two groups watched the same demonstrations in the same order.\n1) Online study I: We recruited two groups of 20 partic-ipants from Amazon Mechanical Turk to provide progress annotations to a robot performing three related tasks. The three tasks we used are reaching, pouring, and spinning, shown in Figure 2, which are sub-tasks of the task we used in the online study II. For each task, participants gave 10"}, {"title": "B. Quantitative analysis", "content": "To analyze the data, we used t-tests [40] and Bayesian statistics with the schemes present in [41]. For t-test results, we use Shapiro-Wilk tests to determine if the data is from a normal distribution. If the data is from a normal distribution, we use a standard independent samples t-test. Otherwise, we apply Kruskal-Wallis H Tests and Wilcoxon Rank-Sum Tests to our results. For Bayesian statistics, a Bayes Factor (BF) is used. We interpret BF lower than 3 as \u201cno evidence\" for the alternative hypothesis, between 3 to 10 as \"moderate evidence\", and 30 or above as \"strong evidence\"."}, {"title": "C. Giving progress is not time-consuming and not hard", "content": "We used the NASA TLX form to measure the workload, and we recorded the average task completion time. The results are shown in Figure 3. We did not find any significant difference between giving scalar feedback and progress in all the dimensions of the NASA TLX results using t-tests and BF (p > 0.45 and 0.3 < BF < 1 for all dimensions)."}, {"title": "D. Progress is used to signify the completion rate of a task", "content": "The results of online study II are shown in Figure 4. Each line represents the average progress or scalar feedback of 18 participants under a certain scenario. While scalar feedback and progress can both describe the quality of a demonstration, progress and scalar feedback focus on different information.\nProgress describes the degree to what extent a task has been completed. Progress was low at the beginning, increased as the task was completed more, stayed the same or became lower while the robot was not acting towards finishing the task (see Failure before spinning, Unaware after missing the stirrer, and Corrected before re-picking the stirrer). A higher value of progress indicates more successful task completion. For instance, in the Unaware scenario, the progress at the final step is greater than in the Failure scenario but lower than in the Perfect scenario, due to 5 out of 6 sub-tasks being completed in the Unaware, as opposed to 1 out of 6 in the Failure. We calculate Pearson corrections between the progress function from participants and an accumulative progress function created by true labels (+1 for completing part of the task, 0 for not completing anything, and -1 for backtracking) for each scenario. The correlations are 0.983 for Perfect scenario (p < 0.001), 0.982 for Imperfect scenario (p < 0.001), 0.974 for Unaware scenario (p < 0.001), 0.977 for Corrected scenario (p < 0.001), and 0.895 for Failure scenario (p < 0.001)."}, {"title": "E. Progress indicates if a task is complete", "content": "We showed average progress and average scalar feedback at the last step for all participants and statistical analysis results between each scenario and the Perfect scenario in Table I. We found that progress could correctly indicate if the task is complete, while the indication of task completion was not captured by scalar feedback. The task has been completed in three scenarios, Perfect, Imperfect, and Corrected. The average progress of three completed scenarios at the last step is about 90, and there was no evidence showing that there is any difference between the other two and the Perfect case (p = 0.714, BF = 0.34 for Imperfect, and p = 0.620, BF = 0.33 for Corrected). For the Unaware and Failure scenarios, there is strong evidence indicating that the task was not completed (avg = 61.3, p < 0.001, BF = 98.75 for Unaware, and avg = 23.4, p < 0.001, BF > 1000 for Failure)."}, {"title": "F. Progress is robust and more consistent to sub-optimality", "content": "In the Imperfect scenario, when the robot dropped the cup onto the table, the progress remained at a similar level and did not affect the eventual progress. The conclusion holds the same in the Corrected scenario. The progress only changed slightly when the robot missed the stirrer and started spinning unproductively, and progress at the end is similar to Perfect. Scalar feedback, on the other hand, changed dramatically in all these cases. Participants used scalar feed-back to indicate if a demonstration was clean and good, but the possibility that the errors were \"harmless explorations\" or \"fixable mistakes\" is not captured by scalar feedback. Moreover, for every scenario other than Perfect, progress has a lower average standard deviation in each scenario compared to scalar feedback. The average standard deviation for Imperfect: 16.1 for progress and 22.5 for scalar feedback (p = 0.018, BF = 3.19), Unaware: 16.8 for progress and 21.1 for scalar feedback (p = 0.060, BF = 1.41), Corrected 16.1 for progress and 20.3 for scalar feedback (p = 0.058, BF = 1.39), and Failure: 15.6 for progress and 28.3 for scalar feedback (p < 0.001, BF > 1000)."}, {"title": "V. PROGRESS WITH SELF-PROVIDED DEMONSTRATIONS FROM NON-EXPERTS", "content": "While progress can be effective in being used along with expert demonstrations, using progress to annotate non-expert demonstrations may still be challenging. We expected most hypotheses will hold. We conducted a public space study to validate the applicability of progress with self-provided demonstrations from non-experts. We recruited 40 participants to provide demonstrations, progress, and scalar feedback in an ice cream topping-adding task. Participants were recruited from the atrium of a university building and the overall participation time was about 15 minutes. This work is approved by the Institutional Review Board and all data collected was anonymous.\nWe released all data we collected from our public space study as a dataset, along with the example scripts that read the data from files."}, {"title": "A. Experiment Setup", "content": "The study was settled in the lobby of a university building. Each participant was asked to first give one demonstration and then watch a replay of the demonstration. The task we asked participants to demonstrate is an ice cream topping-adding task. The goal for participants is to pick up a topping from a shelf and pour the topping into an ice cream via teleoperating a robot arm. The shelf is located on the right side of the workspace, and there are four toppings available which are located at four locations. The participants controlled the arm by using an Xbox controller. The arm was a Kinova Gen 3 Lite arm with six DoF. The setup and the workspace are shown in Figure 1. We recorded the demonstrations in 5 HZ. During the replay, the arm would stop every 10% of the frames, and we would ask participants for one progress and one scalar feedback for the replayed partial trajectory."}, {"title": "B. Experiment Procedure", "content": "We recruited participants by asking people who walked by our setup. Of the 40 participants, 22 participants were male, 14 participants were female, and 4 participants preferred not to say. If the participants agreed to join the study, we first asked them if they were familiar with robots, and excluded them if they said yes. We then asked them to fill out a consent form. Then we introduced them to the ice cream topping adding task, and how to use an Xbox controller to teleoperate the arm. Participants had up to 3 minutes to practice the"}, {"title": "C. Non-Expert Demonstrations Are Multi-Policy and Noisy", "content": "We collected 40 demonstrations from 40 participants. All trajectories are visualized in Figure 5. We also visualized the locations of the objects used in the experiment. The blue trajectories are successful demonstrations and the orange trajectories are failed demonstrations. We found that non-expert demonstrations are noisy and contain a variety of policies, even though 34 out of 40 are ultimately successful. This suggests that policies can be both successful and sub-optimal, which supports our intuition: assessing the quality of demonstrations by comparing is likely to lose information if there are many \"good enough\u201d policies. We also observed that the noise in the demonstrations is not only teleoperating errors but also explorations. For instance, we observed that 14 participants slightly shook the topping jar to test if the gripper firmly held the jar when picking the jar, and 17 participants poured a few toppings out first to see if the jar was right above the ice cream when pouring the toppings. This highlights the importance of detecting unproductive behaviors, and suggests that noisy demonstrations from partially trained agents or perfect demonstrations with injected noise are inappropriate approximations of noisy human demonstrations: human demonstrations are \"noisy\" in a meaningful way. For the six failed demonstrations,"}, {"title": "D. Progress indicates task completions and is more consistent across participants than scalar feedback", "content": "We collected progress and scalar feedback from 34 partic-ipants (two participants' progress and scalar feedback were excluded since all progress and scalar feedback they provided were 100). The results are shown in Figure 6. We plot the average progress and scalar feedback for all successful demonstrations, and progress and scalar individually for all failure demonstrations. We are not able to determine if progress from participants correctly describes the task completion rate since we do not have ground truth labels, but average progress for successful demonstrations did start from low and increased as the demonstrations were reproduced which is a strong indication that progress correlates with task completion rates.\nWe successfully identified all failed demonstrations by only looking at progress at the last step. We use 90 as the divide value, which is the average progress at the last step for successful demonstrations in our online study. If progress at"}, {"title": "E. Progress allows the awareness of backtracking", "content": "As mentioned in subsection V-C, participants would do explorations while demonstrating difficult parts of the task. We also observed that some participants went back a few steps to adjust the grip pose or the pouring position. For instance, one participant failed to pour the toppings into the ice cream because the grasp point was not optimal. Then the participant decided to reset the arm to the initial position, demonstrated the entire task from the beginning again, and succeeded. We plot the trajectory for that demonstration along with progress and scalar feedback, shown in Figure 7. As indicated by progress, the arm was reset to the initial position between step 4 and step 5, and the task was successfully completed afterward."}, {"title": "VI. DISCUSSION", "content": "In this work, we investigated and closely defined an under-explored teaching signal, progress, and conducted three different studies to show the usefulness of progress across a"}, {"title": "Prevent Reward Hacking", "content": "variety of scenarios. We showed that giving progress is not hard and progress carries information beyond scalar feedback and other teaching signals in prior work. We expect that progress will be effective in many applications other than just using along with demonstrations.\nProgress could be a powerful signal to indicate reward hacking. Reward hacking is a phenomenon in which a learning agent learns to achieve high rewards by performing unintended actions instead of finishing the task. For instance, a cleaning robot gets a +1 reward every time it cleans a room. The robot, instead of cleaning one room and going to the next room, repetitively ejects dirt in a room, then cleans it, and thus achieves high rewards. Using progress, we can easily identify that the robot is not advancing towards task completion."}, {"title": "Inverse Reinforcement Learning", "content": "We observed a simi-larity between progress functions and reward functions and expect that a progress function might be a representation of a reward function. We trained a reward function using demonstrations we collected in the public space study us-ing Adversarial Inverse Reinforcement Learning [42], and calculated the rewards for each demonstration. We then calculated the Pearson correlation between the rewards of the demonstration and the progress, as well as the correlation between the rewards and the scalar feedback from each participant in Figure 8. We found that progress is strongly correlated with the learned reward function (avgr = 0.83) and the average correlation is significantly higher (p < 0.001, BF > 1000) than scalar feedback (avgr = 0.19)."}, {"title": "Data Filtering and Ranking", "content": "Progress could also be used to rank demonstrations. For example, a demonstration with a progress of 60 at the end should be ranked lower than a demonstration with a progress of 90. Moreover, progress can be applied as a data filter especially when the demonstrations are sub-optimal.\nA key area for future work is to compare the model per-formance between the model trained using progress and the model trained using other types of human feedback. Training a reliable model from limited non-expert demonstrations and non-expert annotations is challenging but future work could expand our data with more demonstrations along with more types of human teaching signals such as preference."}, {"title": "VII. CONCLUSION", "content": "In conclusion, we defined progress in detail and found that progress could be used to describe completion degrees of a task, indicate if a task is complete, and be more consistent across users, without requiring extra workload or time com-pared to giving scalar feedback. We collected 40 non-expert demonstrations along with progress and scalar feedback, and released them as a dataset. We found that non-expert demonstrations are multi-policy and mostly successful, while noisy in a meaningful way. Our work suggests that progress is information-rich and is worth more attention to develop new methods to effectively leverage the novel information from progress."}]}