{"title": "RAVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models", "authors": ["Maya Varma", "Jean-Benoit Delbrouck", "Zhihong Chen", "Akshay Chaudhari", "Curtis Langlotz"], "abstract": "Fine-tuned vision-language models (VLMs) often capture spurious correlations between image features and textual attributes, resulting in degraded zero-shot performance at test time. Existing approaches for addressing spurious correlations (i) primarily operate at the global image-level rather than intervening directly on fine-grained image features and (ii) are predominantly designed for unimodal settings. In this work, we present RAVL, which takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features rather than operating at the global image level. Given a fine-tuned VLM, RAVL first discovers spurious correlations by leveraging a region-level clustering approach to identify precise image features contributing to zero-shot classification errors. Then, RAVL mitigates the identified spurious correlation with a novel region-aware loss function that enables the VLM to focus on relevant regions and ignore spurious relationships during fine-tuning. We evaluate RAVL on 654 VLMs with various model architectures, data domains, and learned spurious correlations. Our results show that RAVL accurately discovers (191% improvement over the closest baseline) and mitigates (8.2% improvement on worst-group image classification accuracy) spurious correlations. Qualitative evaluations on general-domain and medical-domain VLMs confirm our findings.", "sections": [{"title": "1 Introduction", "content": "Contrastive vision-language models (VLMs) (e.g., CLIP [36] and ALIGN [24]) are a powerful class of models that jointly learn relationships between images and text. VLMs are generally pretrained on web-scale datasets with millions of image-text pairs and have been shown to exhibit impressive capabilities on a wide range of downstream tasks. In particular, VLMs have the ability to perform tasks in a zero-shot manner without utilizing explicit task-specific training data; this is accomplished by modeling downstream tasks (e.g., image classification, text-to-image retrieval) as image-text matching tasks [36].\nHowever, pretrained VLMs can exhibit poor zero-shot performance when compared to state-of-the-art task-specific models, particularly on challenging or out-of-domain downstream tasks [36, 7, 17, 19]. As a result, pretrained VLMs are often fine-tuned on domain-specific vision-language datasets in order\nto improve zero-shot performance on tasks of interest. For instance, recent works have fine-tuned the CLIP VLM [36] on vision-language datasets consisting of (i) chest X-rays and paired physician reports [45], (ii) pathology data and paired text [17, 19], and (iii) product images and paired captions from online fashion retailers [7].\nDomain-specific vision-language datasets used to fine-tune VLMs may be small in size, preventing VLMs from gaining the robustness benefits that come with training on diverse, web-scale data [6, 14]. As a result, fine-tuned VLMs may capture spurious correlations between image features and textual attributes [56]. For instance, consider a VLM fine-tuned on an animal image-text dataset where the presence of butterflies is closely correlated with the presence of flowers (Figure 1). Consequently, the VLM may learn to incorrectly associate the image features corresponding to flower with the textual attribute butterfly. At test time, the VLM is likely to exhibit degraded zero-shot classification performance on (i) images of butterflies without flowers and (ii) images of other animals with flowers.\nImproving robustness of fine-tuned VLMs to spurious correlations is challenging for the following two reasons. First, existing automated approaches primarily discover and mitigate spurious correlations at the global image level rather than intervening directly on fine-grained image features. Such approaches discover spurious correlations by identifying coherent groups of misclassified images in an automated fashion [13, 43, 22, 42]; then, the identified spurious correlation can be mitigated during training using data augmentation or robust optimization [43, 39, 22, 56]. However, recent works have suggested that such global image-level strategies (i) discover spurious correlations that align poorly with human-interpretable attributes [25] and (ii) may not effectively enable models to ignore spurious correlations during training [15, 18]. Second, existing approaches for discovering and mitigating spurious correlations are predominantly designed to improve robustness of unimodal image classification models [39, 43] or pretrained VLMs [60, 49]. These settings differ substantially from the fine-tuned VLM setting, which presents several unique challenges such as the absence of class and subgroup labels in the training set and the inclusion of free-form text.\nIn this work, we address these challenges by introducing Region-aware Vision-Language learning (RAVL), an approach for improving the robustness of fine-tuned VLMs to spurious correlations. RAVL takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features, rather than operating at the global image level. Our contributions are:\n\u2022 First, given a fine-tuned VLM, RAVL discovers learned spurious correlations between image features and textual attributes. Using a labeled classification dataset, we decompose images into candidate regions, utilize the VLM embedding space to group visually-similar regions into feature clusters, and quantitatively evaluate the effects of each feature on zero-shot classification errors.\n\u2022 Second, given a ranked list of image features that the VLM has learned to spuriously correlate with one or more textual attributes, RAVL mitigates the identified spurious correlations. Our key insight is that region-level information can be leveraged during VLM fine-tuning in order to improve model robustness. To this end, we introduce a novel region-aware loss function"}, {"title": "2 Preliminaries", "content": "In this section, we formally describe our problem setting. Datasets used for fine-tuning VLMs can be expressed as $D_F = \\{(I_i, T_i)\\}_{i=1}^m$, where $I_i$ represents image inputs and $T_i$ represents paired free-form text. We do not assume access to any class or subgroup labels.\nThe performance of fine-tuned VLMs can be characterized with zero-shot classification tasks. In line with prior work [13, 22, 56], we assume that the zero-shot classification dataset includes a validation split $D_V = \\{(I_i, y_i)\\}_{i=1}^n$ with images $I_i$ and known ground-truth class labels $y_i \\in Y$, where $Y$ denotes the set of all possible class labels. At evaluation time, classification performance is computed by encoding class labels in Y as text and matching images to the closest class label using embedding similarity. We do not assume access to any subgroup labels.\nFine-tuned VLMs may learn spurious correlations between image features and textual attributes. Let $e_a$ represent the image features corresponding to a visual concept $a$ (e.g., flowers in Figure 1) and $y \\in Y$ represent a class label (e.g., \u201cbutterfly\u201d in Figure 1) such that $e_a$ and $y$ share no causal relationship. Then, a fine-tuned VLM that has learned a spurious correlation will be unable to disentangle $e_a$ and $y$ at evaluation time. This will manifest in low zero-shot classification performance on the following two subgroups of data: (i) images from class label $y$ without the feature $e_a$ and (ii) images from other class labels $Y \\setminus \\{y\\}$ with the feature $e_a$.\nHowever, since neither the fine-tuning dataset $D_F$ nor the evaluation dataset $D_V$ include subgroup labels corresponding to visual concepts $a$, discovering and mitigating such spurious correlations poses a challenge. For instance, in Figure 1, there are no annotations for flowers in datasets $D_F$ and $D_V$, making it challenging to identify and address the learned spurious correlation between image features corresponding to flowers and the textual attribute corresponding to \"butterfly\".\nIn the following sections, we will discuss our automated approach RAVL, which aims to address this challenge by employing fine-grained region-level information to discover (Section 3) and mitigate (Section 4) spurious correlations in fine-tuned vision-language models."}, {"title": "3 Discovering Spurious Correlations in Fine-Tuned Vision-Language Models", "content": "In this section, we present the first stage of RAVL, which aims to discover learned spurious correlations in VLMs. In Section 3.1, we discuss our region-aware approach for discovering fine-grained spurious correlations. Then, in order to quantitatively evaluate the efficacy of spurious feature discovery methods, we introduce a large-scale evaluation framework in Section 3.2. Finally, in Section 3.3, we use our evaluation framework to demonstrate that RAVL outperforms prior approaches in discovering fine-grained spurious correlations between image features and textual attributes."}, {"title": "3.1 Our Approach: Discovering Spurious Correlations", "content": "The first stage of RAVL aims to identify spurious correlations between image features and textual attributes learned by a fine-tuned VLM M. In contrast to prior works that have incorporated humans in the loop in order to identify spurious correlations [56, 30], RAVL is a fully automated approach. Additionally, whereas previous automated methods for discovering spurious correlations focus predominantly on identifying groups of images with high error rates [22, 13], our approach identifies specific image features that model M has learned to spuriously correlate with a textual attribute. Our goal is to discover precise spurious correlations that can be easily interpreted by humans.\nAs discussed in Section 2, a model M that has learned a spurious correlation between an image feature $e_a$ and a textual attribute $y$ will demonstrate low zero-shot performance on (i) images in $D_V$ with label $y$ without the feature $e_a$ and (ii) images in $D_V$ with other labels $Y \\setminus \\{y\\}$ with the feature $e_a$. The key challenge lies in identifying such relationships when no annotations are provided for visual concepts $a$. RAVL addresses this challenge by (1) obtaining candidate image features in $D_V$, (2) identifying the candidate image features that, when present in an image, directly contribute to classification errors, and (3) ranking the identified image features by degree of learned spurious correlations.\nObtaining candidate image features. RAVL first utilizes the zero-shot classification dataset $D_V$ to identify candidate image features. To this end, we use the fine-tuned VLM M to extract an image embedding for each image $I_i$ in $D_V$ and a text embedding for each class $y \\in Y$. Zero-shot classification is performed using the computed embeddings; this results in a softmax-normalized image score distribution vector $s_{I_i} \\in R^{|Y|}$, where $|Y|$ represents the number of classes. Then, we decompose each image $I_i$ in $D_V$ into a set of candidate regions $R_i$. There are a variety of ways in which an image can be decomposed into regions, such as dividing images into equal-sized segments (e.g., quadrants) or using region proposal networks (RPNs) [38]. Ideally, regions should capture key features in the image; however, we emphasize that RAVL does not require ground-truth region-level annotations. We then apply RoIAlign [16, 63] to the image encoder of M to extract embeddings for each region. Zero-shot classification is performed using the computed region embeddings, resulting in a softmax-normalized region score distribution matrix $S_{R_i} \\in R^{|R_i| \\times |Y|}$.\nGiven region-level embeddings for all candidate regions in $D_V$, we next aim to identify coherent groups of image features that occur consistently throughout the dataset (e.g., features corresponding to \"flower\" or \u201cbutterfly\u201d in Figure 1). To this end, we cluster the computed region-level embeddings using the K-Medoids algorithm with cosine distance. The optimal number of clusters is selected in an automated fashion using Silhouette distance. The resulting clusters (denoted as $C$) capture key image features in $D_V$. For feature cluster $c \\in C$, let $e_c$ denotes the set of features in cluster $c$.\nIdentifying candidate image features that directly contribute to classification errors. We now seek to identify features that, when present in an image, are directly responsible for prediction errors.\nLet $R_c$ represent the set of regions assigned to cluster $c$ and let $I_c$ represent the set of images associated with the regions in cluster $c$. We identify labels for images in $I_c$; we designate this label set as $V_c$. For each class label $y \\in V_c$, we identify all images in $I_c$ with label $y$, and we designate zero-shot classification accuracy on this subset of $n_{in}$ images as $p_{in}^c$. Then, we identify all images in $D_V$ with label $y$ that do not have a region included in cluster $c$, and we designate zero-shot classification accuracy on this subset of $n_{out}$ images as $p_{out}^c$.\nWe now introduce the cluster influence score, which evaluates the extent to which features $e_c$ contribute to mispredicted image classification labels. We restrict our evaluation to only include mispredicted images in $I_c$ with ground-truth labels $y$ such that $p_{in}^c < p_{out}^c$; we will refer to this subset as $I_{err} \\subset I_c$. For each image $I_i \\in I_{err}$, we extract (i) the image score distribution vector $s_{I_i}$ and (ii) the region score distribution matrix $S_{R_i}$. We use $s_{I_i}$ to identify the predicted image class $\\hat{y}$, and we then identify the region $r_{max}$ in $R_i$ with the highest score for class $\\hat{y}$.\nDefinition 1 (Cluster Influence Score). For cluster $c$ and label $y$, the cluster influence score is the proportion of images $I_i \\in I_{err}$ with label $y$ where the identified highest-scoring region $r_{max}$ is part of cluster $c$ (i.e., $r_{max} \\in R_c$):\n$H_y^c = \\frac{1}{|\\{I_i \\in I_{err}|y_i = y\\}|} \\sum_{I_i \\in I_{err}; y_i=y} 1[r_{max} \\in R_c]$ (1)"}, {"title": "3.2 Experimental Setup: Designing a Large-Scale Evaluation Framework", "content": "We now discuss our approach for evaluating RAVL. Evaluating the accuracy of predicted spurious correlations is challenging because the ground-truth spurious correlations learned by a model M are typically unknown. Previous works on VLM robustness evaluate discovered spurious correlations with qualitative experiments, human-in-the-loop evaluations, or small-scale datasets [56]. Our aim in this section is to introduce a large-scale experimental setup where the ground-truth spurious correlations learned by VLMs are known and annotated in advance; this can then enable us to determine whether the features discovered by RAVL in Section 3.1 accurately align with the ground-truth. Our evaluation framework is motivated by prior work [26, 13]; however, in contrast to existing approaches, we introduce evaluation settings that are designed (i) for evaluating robustness approaches at the fine-grained region level rather than the global image-level, and (ii) for evaluating VLMs rather than unimodal models.\nDesigning Controlled Evaluations: Our evaluation framework artificially induces spurious correlations in the VLM fine-tuning data; then, given the known pre-defined spurious correlation and a VLM that learned the desired spurious correlation, we can quantitatively evaluate the extent to which RAVL discovers the correlation.\nWe create a set of evaluation settings using data from two domains: (1) synthetic data (MNIST [11] and FashionMNIST [51]) and (2) real-world data (COCO [27]). Each evaluation setting consists of the following components:\n1. Predefined spurious correlation: We define a spurious image feature and textual attribute pair $(e_{eval}, a_{eval})$. For MNIST and FashionMNIST, $e_{eval}$ represents a red rectangle; $a_{eval}$ is generated from the set of class labels {zero, one, two, three, four five, six, seven, eight, nine} for MNIST and {t-shirt, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, ankle boot} for FashionMNIST. For COCO, we sample $e_{eval}$ and $a_{eval}$ from the list of annotated attributes.\n2. Fine-tuning dataset: We construct a vision-language fine-tuning dataset $D_{eval} = \\{(I_i, T_i)\\}_{i=1}^l$ with images $I_i$ and text $T_i$. Dataset $D_{eval}$ is sampled from the training sets of MNIST, FashionMNIST, or COCO such that the presence of image feature $e_{eval}$ is closely correlated with the presence of text attribute $a_{eval}$ as measured by Cramer's V [57]."}, {"title": "3.3 Results: RaVL Effectively Discovers Spurious Correlations", "content": "Comparisons to Prior Approaches: Given an evaluation setting with a predefined spurious correlation $(e_{eval}, a_{eval})$, a fine-tuned VLM M, and an evaluation dataset $D_{eval}$, our goal is to determine the extent to which RAVL can discover the correlation between $e_{eval}$ and $a_{eval}$.\nTo this end, we use the labeled zero-shot classification dataset $D_{eval}$, which includes ground-truth region bounding boxes and associated region labels. We provide the ground-truth bounding boxes as input to RAVL, which returns a single top-ranked cluster of regions likely to include spurious features. We rank regions within the cluster based on similarity to the cluster medoid, and we utilize the provided region-level labels in $D_{eval}$ to evaluate the proportion of top-K regions that contain the desired spurious feature $e_{eval}$. In line with prior work [13], we report performance with Precision@K metrics. We note that given an identified spurious feature $e_{eval}$, the correlated textual attribute $a_{eval}$ can be detected by identifying the class label in $D_{eval}$ where the absence of feature $e_{eval}$ leads to degraded performance.\nThere are few existing approaches for performing automated detection of fine-grained spurious features learned by VLMs. Here, we compare RAVL with four previously-developed methods: Distilling Failures [22], George [43], Domino [13], and Spurious-Aware Detection [56]. Distilling Failures, George, and Domino are state-of-the-art approaches for automatic identification of model failures resulting from spurious correlations; although these methods operate at the global image level and are designed for unimodal settings, we adapt these approaches for our setting by utilizing regions and zero-shot classification scores as input. Spurious-Aware Detection operates at the fine-grained region level by computing class-based performance gaps resulting from the presence or absence of particular features. To enable a fair comparison with RAVL, we provide the same set of regions and associated embeddings as input to all baselines. We also compare RAVL with a random baseline, where the ranked list of regions is shuffled randomly.\nTable 1 summarizes mean Precision@ 10 metrics across all 654 evaluation settings. Results demonstrate that RAVL consistently outperforms prior approaches in discovering spurious correlations between image features and textual attributes, contributing to a 191% improvement over the closest baseline. In Table 1, we evaluate the effects of learned spurious correlation strength by varying the error threshold $T_{eval}$ from 10 to 40 and reporting performance for the subset of valid evaluation settings. Results show that RAVL is particularly effective when VLM M learns a strong spurious correlation; as learned correlation strength increases, performance of RAVL increases by 47% whereas most baselines degrade in performance. We also observe that Domino, George, and Distilling Failures often achieve performance near or below the random baseline across our evaluation settings; this suggests that methods designed for detecting errors resulting from spurious correlations at the global image-level cannot be easily adapted for fine-grained region-level discovery. Figure 2 demonstrates that our findings hold for both synthetic and real-world data.\nAblations: Our ablation study evaluates the role of the cluster influence score $H_c$ and the cluster performance gap metric $G_c$ (Section 3.1) in enabling accurate discovery of spurious correlations between image features and textual attributes. We compare the following three metrics for ranking clusters: (1) an unweighted cluster performance gap metric where $w_y$ is set to 1, (2) the cluster performance gap with $w_y$ computed as in Section 3.1, and (3) a combination of the cluster performance gap and cluster influence metric as used in RAVL. As shown in Table 2, the metrics utilized by RAVL consistently demonstrate the best performance across various learned correlation strengths ($T_{eval}$). Our results suggest the utility of both the performance gap metric and the influence score in identifying fine-grained spurious correlations.\nEvaluations in the Wild: In addition to our controlled evaluations, we evaluate the ability of RAVL to surface spurious correlations learned by 12 off-the-shelf VLMs [12, 36, 20]; this presents a realistic and uncontrolled evaluation setting. We consider two zero-shot classification tasks $D_V$: (1) a 397-class scene classification task on SUN397 [52] and (2) binary classification of cardiomegaly in chest X-rays from ObjectCXR [23]. We use the cluster performance gap metric $G_c$, introduced in Section 3.1, to quantify the degree of the learned spurious correlation."}, {"title": "4 Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models", "content": "In this section, we present the second stage of RAVL, which aims to mitigate learned spurious correlations in VLMs. In Section 4.1, we discuss our methodology for mitigating fine-grained spurious correlations with a novel region-aware loss function. In Section 4.2, we use the evaluation framework previously introduced in Section 3.2 to demonstrate that RAVL substantially outperforms prior approaches in mitigating spurious correlations between image features and textual attributes."}, {"title": "4.1 Our Approach: Mitigating Spurious Correlations", "content": "As described in Section 3, Stage 1 of RAVL discovers image features that VLM M has learned to spuriously correlate with textual attributes. We next aim to mitigate the spurious correlation. Motivated by prior work on fine-grained VLMs [58, 46], our key insight is that utilizing region-level information during VLM training can enable models to focus on relevant image-text relationships and ignore spurious correlations.\nSince dataset DF exclusively consists of images and text, ground-truth subgroup and class labels are not available. As a result, we first assign plausible (i) region-level subgroup labels and (ii) image-level class labels to the vision-language fine-tuning dataset DF. To assign subgroup labels, we decompose each image Ii in dataset DF into a set of candidate regions Ri. We then fit the trained K-Medoids clustering model from Section 3.1 on Ri and identify all spurious regions associated with the top ranked cluster. We represent the identified spurious regions as R and remaining non-spurious regions as R such that R \u222a R = Ri. In order to assign plausible class labels, we parse the paired text Ti associated with each image to identify samples that reference the class labels included in the zero-shot classification label set Y; we refer to the assigned class label for image Ii as \u0177i.\nWe now introduce a novel region-aware contrastive loss function for training VLM Mnew. For batch B, we define R as the set of all spurious regions in the batch: R = UI\u2081\u2208B R. For image I\u00bf \u2208 B, the first loss component LR encourages high embedding similarity between non-spurious regions R and assigned class label \u0177i when compared to other class labels.\n$L'_R = -log \\frac{\\sigma_m (R_I, \\hat{Y}_i)}{\\sum_{\\hat{y}_j \\in B} \\sigma_m (R_I, \\hat{y}_j) + P(R_I^B)}$ (3)\nHere, for region embedding function f and text embedding function g, $\u03c3_m (A,b) = exp(max_{a \\in A} ((f(a), g(b)) /\u03c4))$ with temperature \u03c4. The term $P(R_I^B)$ is a penalty that enforces embedding-level dissimilarity between spurious regions and correlated class labels.\nThe second loss component L\u2081 encourages high embedding similarity between non-spurious regions R and assigned class label \u0177\u2081 when compared to other regions. We define $\u03c3(a,b) = exp((f(a), g(b)) /\u03c4)$ with temperature \u03c4.\n$L_A = -log \\frac{\\sigma_m (R_I^', \\hat{Y}_i)}{\\sigma_m (R_I^', \\hat{Y}_i) + \\sum_{j=1, \\hat{y}_j \\neq y} \u03c3_m (R_I^', \\hat{Y}_i) + \\sum_{r_j \\in R} \u03c3(r_j, \\hat{y}_i)}$ (4)\nThe final loss is expressed as $L = \u03bbL_{CL} + (1 \u2212 \u03bb) (L'_R + L_A)$. Here, \u039b is a hyperparameter and LCL takes the form of the original loss function used for training M; in our experiments, LCL is the CLIP objective [36]. Extended formulations of our loss function are provided in Appendix C."}, {"title": "4.2 Results: RaVL Effectively Mitigates Spurious Correlations", "content": "Comparisons to Prior Approaches: We use the evaluation framework previously introduced in Section 3.2 to compare RAVL with prior approaches. There are few existing approaches for mitigating spurious correlations in the setting of fine-tuned VLMs. Here, we compare RAVL with standard VLM fine-tuning, upsampled VLM fine-tuning, ERM, GDRO [39], and Spurious-Aware Mitigation [56]. Since ERM and GDRO are traditionally used in unimodal classification settings, we adapt these approaches for our setting by adding a contrastive vision-language objective and using zero-shot classification scores during fine-tuning; we refer to these approaches as VL-ERM and VL-GDRO respectively.\nTable 3 summarizes mean zero-shot classification results across our real-world evaluation settings. Since performance of mitigation methods is dependent on the accuracy of the discovered spurious correlations in Stage 1, Table 3 displays results for two evaluation categories: (i) the 192 settings where RAVL Stage 1 Precision@10 is greater than 0.6, and (ii) the 106 settings where RAVL Stage 1 Precision@10 is greater than 0.8. In line with prior works on robustness [39, 56], we report image overall performance and image worst-group performance. Additionally, in order to evaluate the extent to which the VLM understands fine-grained features, we introduce two new metrics: region overall performance and region worst-group performance. Region-level accuracies are computed by performing zero-shot classification with region embeddings and comparing predicted labels to the ground-truth region-level labels provided in the zero-shot classification dataset."}, {"title": "5 Conclusion", "content": "In this work, we introduced RAVL, a fine-grained region-aware approach for addressing spurious correlations in VLMs. We demonstrate through large-scale, controlled experiments as well as in-the-wild evaluations that RAVL can discover (191% improvement in identified correlations) and mitigate (8.2% improvement on worst-group performance) spurious correlations in VLMs. We hope that our work can help (i) diagnose and correct critical failure modes in VLMs prior to deployment and (ii) drive progress towards the development of fine-grained approaches for model robustness."}, {"title": "A Related Work", "content": "Machine learning models often learn spurious correlations (also known as shortcuts) between image features and class labels. For instance, models have been shown to rely on the presence of chest tubes rather than disease features when identifying collapsed lungs in chest X-rays [34]; surgical skin markings when detecting melanoma from skin lesions [48]; and environmental features when performing object recognition tasks [3]. Models that learn spurious correlations will generalize poorly to real-world settings. Our work builds on several recent research directions for (i) discovering and (ii) mitigating spurious correlations.\nDiscovering Spurious Correlations. In the unimodal setting, prior works have developed automated methods for identifying systematic errors resulting from learned spurious correlations in vision models. Using a labeled validation set, these approaches utilize clustering algorithms [13, 43] or lightweight models [42, 22, 31] to identify subgroups of images with high error rates; for instance, in the example in Figure 1, images containing butterflies without flowers may be identified as one such subgroup. Given a set of images in the identified subgroups, a user can then identify the common features and rectify the data or model. However, recent work has suggested that it is often challenging for humans to interpret identified subgroups and accurately determine the shared features resulting in model failure [25]. Additionally, such methods often focus solely on identifying images with high error rates (e.g. butterflies without flowers) rather than identifying the specific class of features contributing to the error (e.g. flowers). A related line of work has aimed to identify spurious features using human supervision [41] or external concept banks [50].\nIn the vision-language setting, Yang et al. use an external off-the-shelf object detector to annotate features [56]. Then, for each feature, the difference in zero-shot classification accuracy between images containing the feature and those without the feature is measured; high performance gaps are used to signal spurious features. However, the efficacy of this approach is reliant on the quality of the object detector and a human-in-the-loop is used to verify results; also, as we show in this work, performance gaps alone are not always sufficient for discovering spurious features.\nMitigating Spurious Correlations. There is a line of work aiming to mitigate spurious correlations in the context of deep learning [61, 39, 32, 9, 28, 33, 21]. These works explore strategies like data augmentation [55, 59, 57, 22, 50] and instance upsampling [39, 43]. While these approaches have been explored widely in unimodal tasks [29, 53], mitigating spurious correlations in vision-language settings has not been extensively studied. Some previous works have studied this problem within the context of pretrained VLMs [60, 49, 1]; however, their setting differs markedly from the fine-tuned"}, {"title": "B Extended Details on Evaluation Settings", "content": "We create 654 evaluation settings using data from two domains: (1) synthetic data (MNIST [11] and FashionMNIST [51]) and (2) real-world data (COCO [27]). Below, we provide implementation details for the four components included in each evaluation setting:\n1. Predefined spurious correlation: We define a spurious image feature and textual attribute pair (eeval, aeval). For MNIST and FashionMNIST, eeval represents a red rectangle; aeval is generated from the set {zero, one, two, three, four five, six, seven, eight, nine} for MNIST and {t-shirt, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, ankle boot} for FashionMNIST. For COCO, we sample eeval and aeval from the list of annotated attributes.\n2. Fine-tuning dataset: Vision-language fine-tuning datasets $D_{eval}$ are sampled from the training sets of MNIST, FashionMNIST, and COCO such that the presence of feature eeval is correlated with the presence of text attribute aeval as measured by Cramer's V. For MNIST and FashionMNIST, we synthetically generate text captions by randomly sampling from the following pre-defined prompt templates: THE IMAGE SHOWS A [CLASS LABEL], THE DIGIT APPEARS TO BE [CLASS LABEL], THERE IS AN IMAGE SHOWING A [CLASS LABEL], and THE NUMBER IS A [CLASS LABEL]. In order to reflect real-world settings where spurious features (e.g. skin markings in dermoscopic images [48]) may not be annotated in text, text captions in our synthetic settings solely refer to class labels and do not describe the spurious feature. For COCO, we use the provided text captions.\n3. Fine-tuned VLM: We fine-tune each model M on dataset $D_{eval}$ using a single NVIDIA A100 GPU with an initial learning rate of 5e-5. We use a batch size of 128 and train for 100 epochs with early stopping. We set the loss temperature as \u03c4 = 0.07. In line with prior works that explore the benefits of locked image-text training [2, 46], we freeze the text encoder and only learn weights for the image encoder.\n4. Evaluation dataset: We construct zero-shot classification datasets $D_{eval}$ from the test sets of MNIST, FashionMNIST, and COCO. For MNIST and FashionMNIST, we generate region bounding boxes using equally-sized quadrants. For COCO, we use the ground-truth bounding boxes and associated labels. Evaluation datasets are sampled to ensure that a correlation between aeval and eeval does not exist. For MNIST, we perform prompt ensembling for zero-shot classification using the following prompts: A PHOTO OF THE NUMBER [CLASS LABEL]; THE DIGIT [CLASS LABEL]; AN IMAGE OF A [CLASS LABEL]; [CLASS LABEL]. For FashionMNIST, we use the following prompts: A PHOTO OF A [CLASS LABEL]; THE [CLASS LABEL]; AN IMAGE OF A [CLASS LABEL]; [CLASS LABEL]. For COCO, we use the following prompts: THERE IS A [CLASS LABEL]; A PHOTO OF THE [CLASS LABEL]; A PHOTO OF A [CLASS LABEL]; [CLASS LABEL]."}, {"title": "C Extended Details on RAVL Mitigation", "content": "In this section, we extend Section 4.1 by providing additional descriptions of our region-aware loss function.\nFor batch B, we define $R_I^B$ as the set of all spurious regions in the batch: $R_I^B = \\cup_{I_i \\in B} R_I$. For image I in batch B, the first component of our region-aware loss function $L'_R$ is designed to maximize embedding similarity between non-spurious regions $R_I^'$ and assigned class label \u0177i; simultaneously, $L'_R$ will minimize embedding similarity between non-spurious regions $R_I^'$ and other class labels in the batch. We formulate $L'_R$ as follows:\n$L'_R = -log \\frac{\\sigma_m (R_I^', \\hat{Y}_i)}{\\sum_{\\hat{y}_j \\in B} \\sigma_m (R_I^', \\hat{y}_j) + P(R_I^B)}$ (5)\nwhere $P(R_I^B)$ is a penalty term that encourages dissimilarity between spurious features and correlated class labels as expressed below. Including this term in the denominator of $L'_R$ is meant to pull embeddings of spurious regions away from correlated class labels.\n$P(R_I^B) = \\sum_{r_j \\in R_I^B} max \u03c3(r_j, Y_k)$ (6)\nThe formula for L\u2081 includes two similarity functions: \u03c3 and $\u03c3_m$. We define \u03c3 and $\u03c3_m$ as follows. Let f represent a region embedding function (associated with the image encoder of VLM M) and"}, {"title": "D Extended Evaluations", "content": "In this section, we extend the results provided in Section 3.3 with additional evaluations of Stage 1 of RAVL. Our goal is to evaluate the ability of RAVL to discover fine-grained spurious correlations between image features and textual attributes."}, {"title": "D.1.1 Extended Comparisons to Prior Approaches", "content": "We implement RAVL according"}]}