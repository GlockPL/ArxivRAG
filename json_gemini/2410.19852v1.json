{"title": "Survival of the Fittest: Evolutionary Adaptation of Policies for Environmental Shifts", "authors": ["Sheryl Paula", "Jyotirmoy V. Deshmukh"], "abstract": "Reinforcement learning (RL) has been successfully applied to solve the problem of finding obstacle-free paths for autonomous agents operating in stochastic and uncertain environments. However, when the underlying stochastic dynamics of the environment experiences drastic distribution shifts, the optimal policy obtained in the trained environment may be sub-optimal or may entirely fail in helping find goal-reaching paths for the agent. Approaches like domain randomization and robust RL can provide robust policies, but typically assume minor (bounded) distribution shifts. For substantial distribution shifts, retraining (either with a warm-start policy or from scratch) is an alternative approach. In this paper, we develop a novel approach called Evolutionary Robust Policy Optimization (ERPO), an adaptive re-training algorithm inspired by evolutionary game theory (EGT). ERPO learns an optimal policy for the shifted environment iteratively using a temperature parameter that controls the trade off between exploration and adherence to the old optimal policy. The policy update itself is an instantiation of the replicator dynamics used in EGT. We show that under fairly common sparsity assumptions on rewards in such environments, ERPO converges to the optimal policy in the shifted environment. We empirically demonstrate that for several popular RL and deep RL algorithms (PPO, A3C, DQN) in many scenarios and popular environments. This includes scenarios where the RL algorithms are allowed to train from scratch in the new environment, when they are retrained on the new environment, or when they are used in conjunction with domain randomization. ERPO shows faster policy adaptation, higher average rewards, and reduced computational costs in policy adaptation.", "sections": [{"title": "1 Introduction", "content": "A significant challenge for autonomous robotic agents used in automated warehouses, autonomous driving, and multi-UAV missions is the problem of identifying the optimal motion policy, i.e., for each state in the environment, deciding the action that the agent should execute. There are several computationally efficient approaches for planning the agent's actions in deterministic and stochastic environments, especially when a model of the environment is available. However, such models may not be available for agents deployed in highly uncertain and dynamic environments. Model-free reinforcement learning (RL) algorithms have been highly effective at learning optimal policies when the environment dynamics are unknown.\nTraditional RL methods suffer from their lack of generalizability when exposed to new, unanticipated changes in the environment. Typically, these RL approaches demonstrate only moderate resistance to noise and exhibit poor performance when deployed in environments significantly different from those encountered during training. The lack of robust adaptation capabilities in these algorithms is a critical drawback, especially in applications where reliability and consistency across varied operational conditions are paramount.\nRelated Work: In response to these challenges, several techniques have been developed to enhance the robustness of RL algorithms including domain randomization and distributionally robust reinforcement learning. Domain randomization trains models across a wide range of simulated variations, thereby improving the algorithm's immunity to noise and its performance under environmental changes. However, this method generally does not perform well if the changes to the environment are substantial.\nAdversarial RL and robust reinforcement learning techniques, including approaches like Monotonic Robust Policy Optimization (MRPO), specifically aim to optimize the algorithm's performance in the worst-case scenarios. These approaches involve training under conditions that include adversarial disturbances or significant noise, thereby preparing the model to handle extreme situations. Although these methods significantly enhance the model's resilience, they sometimes fail to provide optimal solutions in less challenging or more typical scenarios indicating a trade-off between general robustness and peak performance\nCurrent approaches in Robust RL focus on enabling model adaptation to bridge the gap between simulation and real-world applications. Simulation models are generally simplistic and fail to consider environmental variables such as resistance, friction, and various other minor disturbances, so they cannot be directly deployed in risk-averse applications.\nThere is also theoretical work in developing versions of DQN such as DQN-Uncertain Robust Bellman Equation that focuses on developing Robust Markov Decision Processes (RMDPs) with a Bayesian approach. Moreover, control theory-inspired approaches train models on subsets of underperforming trajectories. These methods focus on developing policies that exhibit greater durability and resilience in adverse conditions, often by considering worst-case performance guarantees as essential benchmarks for reliability, but once again, they suffer from being sub-optimal in many cases.\nApproaches in transfer learning with deep reinforcement learning (Deep RL) focuses on leveraging knowledge from previously learned tasks to accelerate learning in new, but related, environments. While this approach promises to improve adaptability and reduce training time, its shortcomings include difficulties in identifying which parts of knowledge are transferable and the tendency to overfit to source tasks.\nSo we see that despite these advancements, there remains a substantial gap in effectively adapting pre-trained models to environments that undergo significant and sudden changes. Traditional RL methods are often ill-equipped to handle situations such as alterations in factory floor layouts, unexpected blockages in warehouse paths, or disruptions in road networks due to natural disasters or construction. These scenarios can drastically alter the dynamics of the environment, rendering previous optimal actions ineffective or suboptimal, and thereby demanding either complete retraining of the models or significant adjustments to their parameters and training protocols.\nContributions. To overcome prevalent limitations in traditional reinforcement learning, we introduce a novel method that synergizes RL-based planning with principles from evolutionary game theory. We generate batches of trajectories in a simulated perturbed environment and strategically explore this environment by employing an weighted version of the policy optimal in the original setting, enhancing adaptability. Subsequently, we refine the policy by prioritizing state-action pairs that demonstrate high fitness or returns, drawing on the concept of replicator dynamics . This evolutionary game theory concept has been successfully applied in analyzing both normative and descriptive behaviors among agents . Unlike traditional methods, our approach incrementally modifies the policy with new batches of data without relying on gradient calculations, ensuring convergence to optimality with theoretical guarantees.\nWe evaluate our algorithm 'Evolutionary Robust Policy Optimization' (ERPO) based on this policy update against leading deep reinforcement learning methods, including Proximal Policy Optimization (PPO), PPO with Domain Randomization (PPO-DR), Deep Q-Network (DQN), and Advantage Actor Critic (A2C), both trained from scratch and retrained from a baseline model trained on the original environment environment (denoted as PPO-B, DQN-B, and A2C-B respectively). Our method demonstrates superior performance in various standard gym environments typical in RL research. Focused on discrete state and action spaces, we have applied our model to complex versions of environments such as FrozenLake, Taxi, CliffWalking, Minigrid: DistributionShift, and a challenging Minigrid setup featuring walls and lava (Walls&Lava). Our findings reveal that our algorithm not only reduces computation times but also decreases the number of training episodes required to reach performance levels comparable to or better than those achieved by the aforementioned mainstream methodologies."}, {"title": "2 Preliminaries", "content": "We model the system consisting of an autonomous agent interacting with its environment as a Markov Decision Process (MDP) defined as the tuple: (S, A, R, \u0394, \u03b3). At each time-step t, we assume that the agent is in some state $s_t \\in S$, executes an action $a_t \\in A$, transitioning to the next state $s_{t+1} \\in S$, and receiving a reward $r_t \\in R$ with a discount factor $\\gamma \\in (0, 1]$. The transition dynamics \u0394 is a probability of observing the next state $s_{t+1}$ and getting the reward $r_t$, given that the agent is in state $s_t$ and takes the action $a_t$.\nIn this paper, we consider finite time-horizon (denoted as T) problems under a stochastic policy \u03c0, a probability distribution over actions given states, such that the action $a_t$ is sampled from the distribution $\u03c0(a | s = s_t)$ at any time t. We define F \u2286 S as the set of goal states in which the agent's task is considered to be achieved. Now we formalize the sparse reward setting: if $r(s_t, a_t, s_{t+1})$ is the reward received after taking action a in state s.\n$r(s_t, a_t, s_{t+1}) \\gg r(s_t, a_t, s_{t+1})$.\nwhere $s_{t+1} \\in F, s_{t+1} \\notin F$.\nA trajectory \u03c4 of the agent induced by policy \u03c0 is defined as a (T+1)-length sequence of state-action pairs:\n$\u03c4 = \\{(s_0, a_0), (s_1, a_1),..., (s_{T-1}, a_{t-1}), s_T\\}$,\nwhere, $\\forall t < T: a_t \\sim \u03c0(a | s = s_t), (s_{t+1}) \\sim (s_{t+1} | s_t, a_t)$.\nWe also donate a trajectory \u03c4 where actions have been sampled using the policy \u03c0 as $\u03c4 \\sim \u03c0$. Given a trajectory, the total discounted reward of a trajectory is:\n$G^\u03c0(\u03c4) = \\sum_{t=0}^{T} r_t$\nWe define the state-value and action-value functions as follows:\n$v^\u03c0(s) = E_{\u03c4 \\sim \u03c0}[G^\u03c0(\u03c4)|s_0 = s]$\nand\n$q^\u03c0(s, a) = E_{\u03c4 \\sim \u03c0}[G^\u03c0(\u03c4)|s_0 = s, a_0 = a]$\nLet \u03b7(\u03c0) be the expected discounted return for an agent under the policy \u03c0 across all trajectories\n$\\eta(\\pi)=E_{\\tau \\sim \\pi}[\\sum_{t=0}^{T-1} \\gamma^t r_{t+1}]$\nThe optimal policy $\u03c0^*$ for the MDP can then be defined as:\n$\\pi^* = arg \\max_{\\pi} \\eta(\\pi)$"}, {"title": "2.1 Reinforcement Learning", "content": "We model the system consisting of an autonomous agent interacting with its environment as a Markov Decision Process (MDP) defined as the tuple: (S, A, R, \u0394, \u03b3). At each time-step t, we assume that the agent is in some state st \u2208 S, executes an action at \u2208 A, transitioning to the next state st+1 \u2208 S, and receiving a reward rt \u2208 R with a discount factor \u03b3\u2208 (0, 1]. The transition dynamics A is a probability of observing the next state st+1 and getting the reward rt, given that the agent is in state st and takes the action at.\nIn this paper, we consider finite time-horizon (denoted as T) problems under a stochastic policy \u3160, a probability distribution over actions given states, such that the action at is sampled from the distribution \u03c0(\u03b1 | s = st) at any time t. We define FC S as the set of goal states in which the agent's task is considered to be achieved. Now we formalize the sparse reward setting: if r(st, at, St+1) is the reward received after taking action at in state st.\nr(st, at, St+1) \u226br(St, at, St+1).\nwhere st+1 \u2208 F, St+1 & F.\nA trajectory \nof the agent induced by policy is defined as a (T+1)-length sequence of state-action pairs:\nT = {(so, ao), (81,a1),..., (ST-1, at-1), ST},\n(1)\nwhere, Vt <T: at ~ \u03c0(a | s = St), (St+1) ~ (St+1 | St, At).\nWe also donate a trajectory 7 where actions have been sampled using the policy \u03c0as \u03c4 ~ \u03c0. Given a trajectory, the total discounted reward of a trajectory is:\nT\nG* (1) = \u2211rt\nt=0\nWe define the state-value and action-value functions as follows:\n\u03c5\" (s) = \u0395\u03c4\u03bf\u03c0 [G\" (T)|80 = 8]\nand\nq\" (s, a) = \u0395\u03c4~\u3160 [G\u2122 (T)|So = s, ao\na]\nLet \u03b7(\u03c0) be the expected discounted return for an agent under the policy across all trajectories\n\u03b7(\u03c0) = \u0395\u03c4\u03c5\u03c0\nT-1\nt=0\nVrt+1\nThe optimal policy \u03c0* for the MDP can then be defined as:\n\u03c0* arg max \u03b7(\u03c0),\n\u03c0"}, {"title": "2.2 Evolutionary Game Theory", "content": "EGT originated as the application of game-theoretic concepts to biological settings. This concept stems from the understanding that frequency-dependent fitness introduces a strategic dimension to the process of evolution [2]. It models Darwinian competition and can be a dynamic alternative to traditional game theory that also obviates the need for assumptions of rationality from the participating members. It has been applied to modeling complex adaptive systems where strategies evolve over time as those yielding higher payoffs become more prevalent, akin to the survival of the fittest in natural selection. We aim to leverage the principles of EGT [28], [25] to build an approach for our distribution shift problem."}, {"title": "2.2.1 Replicator Dynamics Equation:", "content": "The key concept within EGT pertinent to us is that of replicator dynamics, which describes how the frequency of strategies (or policies in RL) changes over time based on their relative performance. The classic replicator equation in evolutionary game theory describes how the proportion of a population adopting a certain strategy evolves over time. Mathematically, it is expressed as [18]:\nxj (i + 1) = xj (i)fj(i)f(i)\n(2)\nwhere xj (i) represents the proportion of the population using strategy j at time i, f; (i) is the fitness of strategy j, and f(i) is the average fitness of all strategies at time i. The equation indicates that the growth rate of a strategy's proportion is proportional to how much its fitness exceeds the average fitness, leading to an increase in the frequency of strategies that perform better than average.\nProblem Definition: We assume that we have an optimal policy for the original environment dynamics A, referred to as \u03c0\u03bb.\nSuppose that we have a new environment with dynamics Anew obtained by significantly perturbing the distribution representing A,\u00b9then the problem we wish top solve is to learn a new policy \u03c0new such that\n\\arg \\max_\\pi \\eta_{\\Delta_{new}}(\\pi).\n(3)"}, {"title": "3 Solution Approach", "content": ""}, {"title": "3.1 Translation of the Replicator Equation", "content": "Representation of populations and the fitness equivalent: We represent the probability distribution over actions in a given state as a population, where each type of population corresponds to a possible action. For a system comprising n states with m possible actions in each state, we effectively have n distinct populations of m types each. This results in m\" different types of individuals in the aggregated state population.\nThe fitness function measures the reproductive success of strategies based on payoffs from interactions, similar to utility in classical game theory, or how in RL, the expected return measures the long-term benefits of actions based on received rewards. Both serve as optimization criteria: strategies or policies are chosen to maximize these cumulative success measures to guide them towards optimal behavior. Therefore, in our model, the fitness for a state f(s) corresponds to the expected return from that state, equivalent to the value function v(s), and the fitness for a state-action pair f(s, a) corresponds to the expected return from taking action a in state s, equivalent to the action-value function q(s, a).\nUnder the assumption of sparse rewards-where significant rewards are received only upon reaching specific states or goals-f(s) is defined as E[f(Ts)], the expected return across all trajectories through state s. Likewise, f(s, a) is defined as E[f(T(s,a))], the expected return across trajectories involving the state-action pair (s, a).\nq(s, a) = f(s, a) \u2248 E[f(T(s,a))], (4)\nv(s) = f(s) \u2248 E[f(T(s))], (5)\nPolicy Update Mechanism: The replicator equation can be adapted to update the probability of selecting certain actions based on their relative performance compared to the average. The adaptation of the replicator equation is as follows:\n\u03c0\u00b2+1(s, \u03b1) =\n\u03c0\u00b2 (s, a) f (s, a)\n\u03a3\u03b1'\u03b5\u03b1\u03c0\u03ad(s, a')f(s,a')\n(6)\nwhere \u03c0\u00b2(s, a) is the probability of action a in state s, in the rith iteration, and \u03c0\u00b2+\u00b9 represents the policy int he i + 1th iteration.\nLemma 1. Policy update in Eq. (6) encodes the replicator dynamics in Eq. (2).\nThe proof of the above lemma follows from the observation that for a state s, and a specific action ai, the replicator equation 2 in our setting would look like2:\nXs,aj (i + 1) = Xs,aj (i)f(s, aj)\u03a3\u03b1\u03b5\u03b1 f(s, a) x(s,a) (i)(7)\nBy our representation of policies as populations, we see that \u03c0\u00b2 (8, aj) is equivalent to xs,a; (i), and so Eq. ( 6) follows from Eq. (7). The policy update in Eq. (6) is just a simultaneous application of parallel replicator equations to all states (being updated in a given iteration).\nThis rule essentially captures the essence of the replicator dynamic by adjusting the probability of action a in state s proportionally to its performance relative to the average performance of all actions in that state. The normalizing factor in the denominator ensures that the updated policy remains a valid probability distribution, aligning with the principle of the replicator dynamic where strategy frequencies within a population must sum to one.3\nTheorem 2. The algorithm (ERPO) that employs the policy update specified in Eq. (6), ensures that the value of each state monotonically improves with each iteration, converging to an optimal policy under assumptions of sparse rewards.\nProof. We note that our algorithm employs a batched Monte Carlo-style sampling approach, collecting multiple trajectories in each batch. We assume that each batch is sufficiently large to ensure that the estimated values of the v and q functions closely approximate their true values, so that Eq. (5) and Eq. (4) hold. We also assume that each state is visited at least once in each batch.\nWe define the action-value function and value function under policy \u03c0\u00b2, accounting for transition probabilities:\nq*(s, a) = \u2211\u2206(s, a, s') (r(s, a, s') + yv\u00b2 (s')),(8)\ns'ES\nv (s) = \u2211 \u03c0\u00b2 (s, a)q\u02bb(s, a).\n\u03b1\u0395\u0391\nWe now partition the set of actions A into Ah and At such that:\nAh = {an \u2208 A | q\u00b2(s, an) \u2265 v\u00b2(s)}"}, {"title": "3.2 Evolutionary Robust Policy Optimization (ERPO)", "content": "Our algorithm uses batch-based updates: We initialize our training policy to be a weighted combination of the old optimal policy \u03c0*,\nand our new policy new which is initially random. We sample trajectories as part of a batch, and in doing so we make sampling assumptions as mentioned earlier, and the state-action pairs in these trajectories are updated according to the update rule. The return for the i + 1th iteration is set as the return under the training policy, and the training policy is updated, to take into account the update to new. The weight assigned to the old policy is decremented with each iteration. This process is repeated until our termination condition (n\u00b2+1 \u2013 \u03b7\u00b2 > \u03b4) is met. The termination condition checks if the expected return across all states is changing over our batch runs, and when the difference in the expected returns across consecutive batches is minimal, we say that the algorithm has converged."}, {"title": "4 Experiments", "content": "Benchmarks: In our empirical analysis, we benchmark our approach against a selection of established reinforcement learning algorithms. The comparison is conducted under two different scenarios: (1) each RL algorithm is allowed to train on the modified environment from scratch, (2) we obtain pre-trained corresponding to the optimal policy, and then train them over the modified environment. Each baseline scenario is described in detail below:\n\u2022 PPO [26]: Standard Proximal Policy Optimization, re-trained from scratch in the new environment.\n\u2022 DQN [15]: Deep Q-Network, also re-trained from scratch in the new environment.\n\u2022 A2C [16]: Advantage Actor-Critic, re-trained from scratch in the new environment."}, {"title": "5 Results", "content": "We present the results of each environment for ERPO and the other baseline algorithms. We note that the performance of ERPO does not vary much with increasing levels of difficulty, even when the new environment is drastically different and much more difficult to navigate than the base environment, while the other algorithms suffer.\nComparison with models trained from scratch and Domain Randomization: ERPO significantly outperforms the other algorithms in terms of timesteps required for convergence. PPO-DR and A2C are the closest competitors, yet they still require up to an order of magnitude more timesteps than ERPO in the Walls&Lava environment.\nIn the CliffWalking environment, IDA* consistently achieves rewards of approximately 2K across all levels, though this success requires the starting node to be positioned near the cliff's edge. In contrast, A* struggles, with rewards ranging from -200 to -300.\nWhile vanilla heuristic search methods may underperform in stochastic environments, methods like stochastic A* might be more suitable. Additionally, extending these methods to continuous spaces via function approximators is challenging and requires an admissible heuristic. A key difference is that ERPO and other learning-based methods generate policies that can generalize to any start location within the grid, whereas heuristic-based approaches may need to restart the search when encountering previously unexplored states.\nPreliminary Results on custom Environments: In addition to the results on standard gym environments, we also conducted experiments in custom gym environments with a similar reach-avoid mission. The agents in these environments are assigned randomly to one of many pre-determined start locations, and must reach one of the goal locations whilst avoiding obstacles. One sample result for a 100x100 grid world with 20% new obstacles is as follows: We leave the extension to larger custom grids with varying levels of obstacle density for future work.\nAnalysis: Our observations indicate a distinct advantage of ERPO over traditional reinforcement learning algorithms even when trained over the pre-trained models, and domain randomization methods. While PPO, PPO-DR, and A2C utilize batch-wise updates, and DQN depends on episode-wise updates, these algorithms generally treat each step within a batch or episode as equally significant for the purpose of policy updates. This approach can dilute the impact of particularly successful or unsuccessful trajectories on the overall learning process. In contrast, ERPO places emphasis on trajectories that significantly deviate from the norm either by outperforming or underperforming compared to the rest of the batch and prioritizes learning from those that are the most informative. This selective update mechanism ensures that ERPO rapidly identifies and leverages the most effective strategies. As a result of this approach, the fittest trajectories become increasingly predominant in the batch over just a few training episodes. Thus, ERPO leads to a faster and more efficient convergence towards optimal policies."}, {"title": "6 Discussion", "content": "Limitations and Future Work: Our set up is limited to discrete state-action spaces. We are working on an extension that works with continuous spaces. This will be carried out with function approximation using radial basis functions that also update the policies of states within a certain distance of the state we are updating. Additionally, because we normalize the probability distribution across actions of a given state, a continuous model would work instead along with a probability density function that can be updated using Dirac delta functions. Our set up is also limited to single agent models (unless extended with independent learning). We are working on extensions that can combine other game-theoretic solution concepts for cooperative multi-agent learning.\nConclusion: This paper presents a new approach to incrementally adapt the optimal policy of an autonomous agent in an environment that experiences large distirbution shifts in the environment dynamics. Our algorithm uses principles from evolutionary game theory (EGT) to adapt the policy and our policy update can be viewed as a version of replicator dynamics used in EGT. We provide theoretical convergence guarantees for our algorithm and empirically demonstrate that it outperforms several popular RL algorithms, both when the algorithms are warm-started with the old optimal policy, and when they are re-trained from scratch."}]}