{"title": "Synthetic Multimodal Question Generation", "authors": ["Ian Wu", "Sravan Jayanthi", "Vijay Viswanathan", "Simon Rosenberg", "Sina Pakazad", "Tongshuang Wu", "Graham Neubig"], "abstract": "Multimodal Retrieval Augmented Generation (MMRAG) is a powerful approach to question-answering over multimodal documents. A key challenge with evaluating MMRAG is the paucity of high-quality datasets matching the question styles and modalities of interest. In light of this, we propose SMMQG, a synthetic data generation framework. SMMQG leverages interplay between a retriever, large language model (LLM) and large multimodal model (LMM) to generate question and answer pairs directly from multimodal documents, with the questions conforming to specified styles and modalities. We use SMMQG to generate an MMRAG dataset of 1024 questions over Wikipedia documents and evaluate state-of-the-art models using it, revealing insights into model performance that are attainable only through style- and modality-specific evaluation data. Next, we measure the quality of data produced by SMMQG via a human study. We find that the quality of our synthetic data is on par with the quality of the crowdsourced benchmark MMQA and that downstream evaluation results using both datasets strongly concur.", "sections": [{"title": "1 Introduction", "content": "Following the increased adoption of Retrieval Augmented Generation (RAG) (Lewis et al., 2021) for text-based question-answering (QA), there has been much interest in extending RAG to the multimodal setting (Chen et al., 2022; Chang et al., 2022; Lin and Byrne, 2022; Yasunaga et al., 2023). In multimodal RAG (MMRAG), QA is performed by a large language model (LLM) or large multimodal model (LMM) grounded in sources that span modalities such as text, tables and images. As with RAG, MMRAG has the potential to increase answer quality and transparency when compared with closed-book QA, where models directly answer questions without access to external knowledge.\nA major challenge encountered when implementing MMRAG systems is evaluation, which is typically done using fixed benchmark datasets. These datasets consist of (source(s), question, answer) tuples that enable separate evaluation of the retriever and the QA model. Some representative datasets include MMQA (Talmor et al., 2021), MMMU (Yue et al., 2023) and WebQA (Chang et al., 2022).\nThe problem with this approach is that we cannot tailor the evaluation questions to our desired specifications. We identify two aspects of the evaluation questions we may wish to control. The first aspect is the question style. The style of a question determines the reasoning abilities required to answer it, and certain models perform better on certain question styles than others. Examples of question styles include mathematical (Hendrycks et al., 2021; Yu et al., 2023), multi-hop (Yang et al., 2018) and extractive (Rajpurkar et al., 2016) question styles. The modality of a question refers to the modality or modalities (e.g. text, table, image) of the sources required to answer it, and both retrieval (Wei et al., 2023) and QA (Jia et al., 2021; Chen et al., 2020) performance depends on the modalities of their inputs.\nGiven the sensitivity of MMRAG performance to question styles and modalities, we want our evaluation questions to match the styles and modalities of the questions our system is likely to encounter. Such a benchmark may not exist, making it difficult to perform the comprehensive evaluations needed to reveal important model deficiencies.\nTo address this issue, we introduce a method for Synthetic Multimodal Question Generation we call SMMQG. SMMQG is a synthetic data generation framework that leverages interplay between a retriever, an LLM and an LMM with in-context learning (Brown et al., 2020) to generate multimodal questions and answers based directly on input documents. Crucially, SMMQG enables fine-grained control over the styles and modalities of questions, and is capable of producing both unimodal and cross-modal questions. Using SMMQG, we create a dataset of 1024 questions and answers of various styles and modalities from Wikipedia documents. We then use this dataset to unearth style- and modality-specific insights into the MMRAG performance of various models.\nOne concern with synthetic data generation is that the resulting data is of low quality. To assuage this concern, we conduct a human study to measure the quality of our dataset. We find that our dataset's quality is on par with or better than that of popular crowdsourced benchmark MMQA (Talmor et al., 2021) when measured along five different metrics. We also show that downstream evaluation results obtained using our SMMQG dataset display strong concurrence (Liu et al., 2023b) with those obtained using MMQA, demonstrating that our synthetic dataset can be used in place of MMQA for model selection."}, {"title": "2 Problem Setting", "content": "We define the multimodal sources S to include the text passages, tables, and images extracted from one or more specified documents. Multimodal sources are parsed from a document as a pre-processing step.\u00b9 Text sources consist of chunked text passages and table sources consist of pipe-separated strings that are prepended with titles. Image sources consist of an image, its caption and an image verbalisation. The image verbalisation is a text description of the image generated using an image-captioning model or LMM. Verbalisations are useful because they allow text-based models to search and reason over images, as seen in Liu et al. (2023c). See Appendix A for examples of sources and Appendix B for details of our image verbalisation strategy."}, {"title": "2.2 Formulation", "content": "SMMQG takes three inputs. These are (1) multimodal sources S, which serve to provide the context for question generation (2) question style v, which is a description of the question style along with examples, and (3) modality requirements M. This is a 3-tuple of integers M = (mtext, Mtable, Mimage). M is used to indicate the modalities of the generated questions: M = (2,1,0), for example, indicates that our generated question should be a cross-modal text-table question with two text and one table sources.\nSMMQG jointly produces three outputs. These are (1) the synthetic question q, with style dependent on v (2) a long-form answer a to the question and (3) references to the question sources Z, where zi \u2208 S. q is only answerable using information derived from every source in Z. The modalities of the question sources must match M: if M = (1,1,0), for example, then |Z| = 2 and z1 and z2 must be text and table modalities in some order."}, {"title": "3 Method", "content": "SMMQG is composed of five steps, as illustrated in Figure 2. The output of the first three steps is a set of candidate sources \u017d, where \u017ei \u2208 S. These consist of semantically-related sources connected by an entity. The fourth step is responsible for generating q and a and for choosing Z \u2286 \u017d. Thematic unity is maintained by the fact that all candidate sources are semantically related, which enables the generation of meaningful multi-source and potentially cross-modal questions. The fifth step is responsible for validating q and a.\nStep 1: Sample Seed Source The goal of this step is to locate a seed source sseed \u2208 S. The most straightforward way to select sseed is to choose one by uniformly sampling over S. However, we find that many sources are outliers that are unrelated to other sources and do not reflect the main topics found in the documents. Building coherent multi-source questions from such sources is difficult, as there are often no related candidate sources to choose from.\nTo correct for this, we introduce weights wi. The probability of sampling si as sseed is\nPsi =\nexp(\u2212\u03b2\u03c9i)\n\u2211j exp(\u2212\u03b2\u03c9j)\nwhere \u03b2 is a temperature parameter, which we set to 0.1 based on manual inspection of resulting outputs. The weights wi are the average cosine-distance of the kseed-nearest neighbours of si in embedding space. Given some embedding model E,\nwi =\n1\nkseed\n\u2211\ndist(E(si), E(sj))\nsj\u2208kseednn(si)\nWe use the E5-Large embedding model (Wang et al., 2024) in our experiments, with kseed = 5.\nStep 2: Extract Entity In this step, we use GPT-4-Turbo (OpenAI, 2024) to extract a prominent entity (e.g. \"tennis\", \"Japan\", \"machine learning\") from the seed source via three-shot prompting. For image sources, we provide the LLM with the image verbalisation and the image caption, which we find captures image entities sufficiently well. We also find that using a high temperature of 1.0 improves the diversity of extracted entities, which in turn improves the diversity of generated questions. See Appendix C for our entity extraction prompt.\nStep 3: Retrieve Candidate Sources In this step, we retrieve candidate sources \u017d using the E5-Large retriever, with the extracted entity from Step 2 as the query. The candidate sources are therefore all semantically related through the entity.\nWe also define an integer kmodality. For every modality i \u2208 {text, table, image}, we retrieve the top mikmodality candidate sources of that modality. For example, given M = (1,2,0) we retrieve kmodality text and 2kmodality table sources respectively. We set kmodality = 2, as we find that providing a choice of candidate sources improves the quality of generated questions. For retrieving images, we rely on image verbalisations. We find that this text-only retrieval approach improves over the use of multimodal retrieval with CLIP (Radford et al., 2021) embeddings, likely due to the prevalence of long text and table sources.\nStep 4: Question Generation We now pass \u017d to the question-generation model, which is an LLM or LMM, depending on whether there are image candidate sources present. We use GPT-4-Turbo (OpenAI, 2024) as both in our experiments. In addition to \u017d, the question-generation model is also given the task instruction, the question style and its description v, the modality requirements M, and three style-specific few-shot examples. The task instruction asks the model to choose question sources Z from \u017d, adhering to M, and to then generate q and a using Z, following v. The model is also asked to produce references to Z by outputting their source IDs. We instruct the model to refuse requests if it does not believe that a coherent question adhering to v and M can be generated. See Appendix C for our question-generation prompts.\nStep 5: Question Verification We perform three checks and reject samples that fail any of these. Firstly, we cross-reference the modalities of the chosen question sources with M and reject samples where the source modalities do not match the requirements. For example, if M = (0,1,1), we check that the modalities of Z are exactly 1 table and 1 image. Secondly, we pass q to an LLM and prompt it to verify that the question adheres to the specified question style. Lastly, we pass q, a and Z to an LLM or LMM and prompt it to verify that (a) a is correct given q and Z and (b) every source in Z is required to answer q. We perform both the second and third steps of QA verification with a single call to GPT-4-Turbo. See Appendix D for our QA verification prompts."}, {"title": "3.2 Multi-hop QA Generation", "content": "Although Step 4 of SMMQG can be used to generate multi-hop questions, we propose an alternative version that produces higher quality multi-hop questions more consistently. We start by generating two intermediate questions and their respective answers and question sources: for the first intermediate question, we prompt the model to generate a question about the entity extracted in Step 2, given some subset of \u017d. For the second question, we prompt the model to generate a question where the answer is the same entity, given the remaining \u017d. When we generate cross-modal multi-hop questions, \u017d are split by modality. When we generate unimodal multi-hop questions, \u017d are split randomly.\nNext, we prompt the model to combine the intermediate questions and answers to form a multi-hop question and answer. The multi-hop question sources are the union of the question sources chosen in the intermediate steps. See Appendix E for examples of the question combination prompts and a diagram illustrating multi-hop question generation."}, {"title": "4 Experiments", "content": "We use SMMQG to build a QA dataset over Wikipedia documents. We use the text passages, tables and images gathered by Talmor et al. (2021) for MMQA as our sources. These sources include approximately 57,000 captioned images, 232,000 text passages and 12,000 tables, and were scraped from the 2020-01-01 English Wikipedia dump.\nWe preprocess the dataset by removing text passages with lengths of less than 200 characters, as we find that short passages rarely contain enough information to generate good questions. We generate a total of 1024 QA samples across five different question styles covering all pairwise modality combinations. These are summarised in Table 1, and further examples are detailed in Appendix A. We choose these five question styles because (1) they test for a diverse set of reasoning abilities (2) they are well-represented across the QA literature (Khashabi et al., 2018; Rajpurkar et al., 2016; Yang et al., 2018; Dua et al., 2019). We plan on releasing our SMMQG-generated dataset following publication of this work."}, {"title": "4.2 Retriever and QA Model Evaluation", "content": "We use our dataset to evaluate the performance of three retrievers and eight LLM + LMM combinations. For retrieval, we evaluate BM25, E5-Large (Wang et al., 2024) (both text-based) and OpenCLIP (Cherti et al., 2022) \u2013 an open-source implementation of CLIP offering improved performance - using recall@5 and recall@10 as our metric. For the text-based retrievers, we rely on captions and verbalisations for retrieving over images.\nWe evaluate the LLM + LMM combinations by conditioning answer generation on the SMMQG questions and question sources and then scoring the predictions against the answers using GPT-4-Turbo as a judge, which has been shown to produce judgements that correlate strongly with human judgements (Zheng et al., 2023; Kim et al., 2024). The judge is asked to score the predicted answer on a three-point scale given the SMMQG-generated answer and the question sources. For QA, we use the LMM when the question sources contain at least one image; otherwise we use the LLM. An example of the judge prompt is provided in Appendix F.\nWe assess the open-source model combinations Vicuna-7b-v1.5 + LLaVA-v1.5-7b, its 13b counterpart (Zheng et al., 2023; Liu et al., 2023a), and Qwen-Chat + Qwen-Chat-VL (Bai et al., 2023a,b). The LLaVA-v1.5 models are LMMs finetuned from Vicuna-v1.5 (Peng et al., 2023), and both Qwen models are 7B parameter models finetuned from Qwen-LM (Bai et al., 2023a). We also assess the proprietary multimodal models GPT-4-Turbo (OpenAI, 2024), Gemini Pro 1.0 (Google, 2024) and the Claude 3 family (Anthropic, 2024), which are trained to process text-only inputs in addition to multimodal inputs and so can act as both the LLM and LMM. See Appendix G for further details.\nRetriever evaluation results are shown in the top subtables of Tables 2 and 3. We find that the text-based E5 retriever outperforms BM25 and OpenCLIP across all question styles, and is especially strong on abstractive styles (numerical, compare contrast, multi-hop). We also find that OpenCLIP significantly outperforms text-based retrieval for pure image retrieval questions, but underperforms text-based retrieval for all other modalities.\nThe QA evaluation results are shown in the bottom subtables of Tables 2 and 3. To start, we see that GPT-4-Turbo outperforms all other models across all question styles and modalities. This strength may be attributable to (1) GPT-4-Turbo being genuinely strong (2) GPT-4-Turbo being the evaluation data generation model, which filters out many questions that the model is unable to answer (3) GPT-4-Turbo being itself used as the judge, as LLM judges favour their own outputs (Koo et al., 2023).\nAmongst the other models, Claude 3 Opus generally performs best, although further analysis yields novel style- and modality-specific insights. Firstly, Gemini Pro 1.0 performs similarly to Claude 3 Opus on extractive question styles (information extraction, compound), but is weaker on abstractive styles, where it is comparable to the smaller Claude 3 models. Secondly, the gap between open-source and proprietary models is small for extractive question styles but large for abstractive styles, with open-source models being especially poor on numerical and compare contrast questions. Comparing open-source models, we find that Qwen-Chat + Qwen-VL-Chat performs better on numerical and multi-hop questions than Vicuna-13b + LLaVA-13b despite being significantly smaller.\nOn the modality front, we learn that Gemini Pro 1.0 performs strongly for questions containing image sources, and is stronger than even Claude 3 Opus for unimodal image questions, although it suffers from weaker text and table reasoning abilities. Claude 3 Haiku, meanwhile, is surprisingly poor at table QA (comparable to open-source models), but makes up for this with superior image reasoning capabilities.\nIn summary, we demonstrate that SMMQG can generate question style and modality-specific evaluation datasets. Such datasets reveal important insights into the style- and modality specific strengths and weaknesses of retrievers and QA models that would otherwise remain hidden."}, {"title": "5 Assessing Data Quality", "content": "In the following sections, we assess the quality of our SMMQG dataset and use MMQA (Talmor et al., 2021) as a reference to better understand the significance of our results. MMQA is a crowdsourced benchmark constructed over the same documents as ours. It contains both uni- and cross-modal questions, and provides long-form answers. We use MMQA as our reference dataset for three reasons. Firstly, the modalities present in MMQA overlap exactly with our modalities of interest, unlike datasets such as WebQA (Chang et al., 2022) and OK-VQA (Marino et al., 2019), which contain only text and images. Secondly, MMQA is built on source documents spanning a diverse set of domains (Wikipedia), unlike domain-specific datasets such as BioASQ (Krithara et al., 2023) (biomedical) and MMMU (Yue et al., 2023) (exams and textbooks). Using it as a reference is therefore likely to yield more generalisable results. Lastly, the single modality and compose questions from MMQA are stylistically very similar to the info extraction and multi-hop questions from our SMMQG dataset; the presence of overlapping question styles enables direct comparison between datasets."}, {"title": "5.2 Human Study", "content": "We conduct a human study to directly evaluate the quality of our SMMQG dataset. We randomly draw 300 samples from our dataset, along with 60 single modality and 60 compose samples from the train set of MMQA, ensuring even distribution over modalities. We combine the samples and ask crowdworkers to rate them along five metrics:\n\u2022 Question Fluency (5-likert): How fluent is the question?\n\u2022 Question Style Faithfulness (Yes/No): Is the question faithful to the question style?\n\u2022 Source Relevance (Yes/No): Are all the sources relevant to answering the question?\n\u2022 Answerability (Yes/No): Is the question answerable using only information in the sources?\n\u2022 Answer Correctness (Yes/No): Is the answer correct given the sources?\nWe compute the average rating for each metric for each dataset and report the results in Table 4. See Appendix I for further details on our human study methodology and for style-specific results. We employ the Mann-Whitney U test (for Question Fluency) and Fisher's exact test (for the remaining metrics) to determine statistical significance. Our findings are as follows:\nSMMQG achieves high question style faithfulness. This finding suggests that SMMQG can reliably produce questions with styles based on user specifications.\nSMMQG questions are more fluent than MMQA questions across comparable styles. Any results we obtain better reflect the true reasoning capabilities of the model, as there is less interference caused by poor phrasing. However, we do not assess the ability of the model to handle poorly-phrased questions, which may reflect real user queries (Kwiatkowski et al., 2019).\nSMMQG question sources are highly relevant. One source of error in SMMQG arises when the question-generation model selects question sources that the generated question is not based on. Our human study results address this concern.\nSMMQG questions are highly likely answerable given the question sources. Another potential source of error arises when the question-generation model generates questions that are not answerable given its own selected question sources. Our study shows that SMMQG questions are actually statistically significantly more likely answerable than MMQA questions.\nSMMQG answers are highly likely to be correct. A high level of answer correctness reduces the noise associated with incorrect labels leading to incorrect assessments of the predicted answers, and our SMMQG answers are statistically significantly more likely to be correct."}, {"title": "5.3 Measuring Concurrence", "content": "We compute the concurrence (Liu et al., 2023b) between our SMMQG dataset and MMQA. The motivation for this is as follows: if we assume that MMQA is a useful evaluation dataset, and if our SMMQG dataset discriminates between models in the same way as MMQA, then we can reasonably conclude that our SMMQG dataset is also a useful evaluation dataset (Viswanathan et al., 2023).\nWe randomly draw 150 information extraction and 150 multi-hop samples from our SMMQG dataset, evenly distributed over modalities. We also randomly draw 150 single modality and 150 compose samples from MMQA, again distributed evenly over modalities. We then run evaluation using the methodology described in Section 4.2 (see Appendix H for these results) and calculate concurrence by computing Kendall's tau on the ranked lists of these results. We report our findings in Table 5.\nWe find that MMQA and SMMQG strongly concur (\u03c4 > 0.8, Liu et al. (2023b)) for both retrieval and QA. This implies that our SMMQG dataset can be used in-place of MMQA to discriminate between models (at least for the two common question styles), further validating its quality."}, {"title": "6 Related Work", "content": "Multimodal QA Benchmarks Many existing benchmarks rely on human annotators to handcraft questions and answers over fixed sets of documents. MMQA (Talmor et al., 2021) curate question-answer pairs by combining compositional question templates with crowdsourcing. This limits the complexity and diversity of generated questions (Chen et al., 2022). MMMU (Yue et al., 2023), which was constructed from college-level education material, was costly to curate, requiring the input of over 50 individuals. Other human-crafted multimodal QA benchmarks include WebQA (Chang et al., 2022), BioASQ (Krithara et al., 2023), ScienceQA (Lu et al., 2022), InfoSeek (Chen et al., 2023) and OK-VQA (Marino et al., 2019)\nSynthetic QA Generation There exists a large body of work leveraging synthetic data to train and evaluate text-only QA systems. Puri et al. (2020) generate extractive QA data and use this to train a BERT-based (Devlin et al., 2019) model for QA. Shakeri et al. (2020) generate QA pairs using BART (Lewis et al., 2019) and use this for domain adaptation. Pan et al. (2020) propose a multi-step process to generate multi-hop questions, while Es et al. (2023) create a synthetic evaluation dataset for text-based RAG evaluation. Synthetic question generation has also been used to evaluate the quality of text summaries (Durmus et al., 2020; Wang et al., 2020). Note that, unlike SMMQG, none of the works discussed above allow for fine-grained control of question styles.\nSynthetic Multimodal QA Generation Synthetic data generation has been studied in the context of visual QA (VQA). MultiQG-TI (Wang and Baraniuk, 2023) utilize an image-to-text model and OCR to create text descriptions of images that are combined with text passages and then passed to an LLM for QA generation. Patel et al. (2020) build a diverse synthetic QA dataset from images and their associated metadata using an image-to-text model. These datasets concern QA over images and possibly image-text pairs only, and do not address text-only or table modalities. Furthermore, they do not enable control over question styles."}, {"title": "7 Conclusion", "content": "We introduce SMMQG, a framework for generating synthetic multimodal questions and answers grounded in multimodal documents that adhere to user-specified question styles and modalities. We demonstrate that an SMMQG-generated evaluation dataset is able to reveal novel style- and modality-specific insights into the performance of state-of-the-art retrievers, LLMs and LMMs. Through a human study and by measuring dataset concurrence, we also show that the quality of data generated by SMMQG is on-par with the quality of data from crowdsourced benchmark MMQA and that our dataset can be used in place of MMQA for model selection. We hope that this work will enable automatic, large-scale and tailored evaluation of MMRAG systems, thereby facilitating their adoption in practical applications."}, {"title": "Limitations", "content": "In our work, we evaluate SMMQG on Wikipedia documents and use GPT-4-Turbo to generate questions belonging to five question styles. SMMQG performance may differ when these variables are altered. The impact of new question styles and documents on SMMQG performance depends largely on how well GPT-4-Turbo is able to reason over these new question styles and understand these documents. This highlights a limitation of our work: we presuppose the existence of a model capable of reasoning over our question style and understanding our document of choice. Nonetheless, when such a model does exist, SMMQG enables us to generate synthetic data that can be used to evaluate other, possibly weaker models. Another related limitation of our work is that we only assess the viability of E5-Large and GPT-4-Turbo as the SMMQG retriever and question-generation model, and it is possible that new challenges arise when other models are used. Another limitation is that we limit our study to use of SMMQG data for evaluation, even though it is possible to use it for training. We encourage further research in this direction, but we cannot yet claim that SMMQG data is appropriate for model training. Finally, our work relies on the existence of a high-quality set of unpaired data sources (in our experiments, this consists of images, text passages, and tables). This may not be an appropriate assumption in all situations; we did not test the ability of our dataset generation method to generalize to noisy data sources."}, {"title": "Potential Risks", "content": "LLMs and LMMs are known to generate false, harmful and biased material. As SMMQG leverages LLMs and LMMs, it may potentially generate false, harmful and biased questions and answers. We also note that we have only explored the use of SMMQG for assessing the QA performance of MMRAG, and not its alignment to human values and preferences. MMRAG systems may excel on SMMQG-generated datasets but nonetheless be misaligned."}]}