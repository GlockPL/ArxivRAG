{"title": "Spot Risks Before Speaking! Unraveling Safety Attention Heads in Large Vision-Language Models", "authors": ["Ziwei Zheng", "Junyao Zhao", "Le Yang", "Lijun He", "Fan Li"], "abstract": "With the integration of an additional modality, large vision-language models (LVLMs) exhibit greater vulnerability to safety risks (e.g., jailbreaking) compared to their language-only predecessors. Although recent studies have devoted considerable effort to the post-hoc alignment of LVLMs, the inner safety mechanisms remain largely unexplored. In this paper, we discover that internal activations of LVLMs during the first token generation can effectively identify malicious prompts across different attacks. This inherent safety perception is governed by sparse attention heads, which we term \u201csafety heads.\u201d Further analysis reveals that these heads act as specialized shields against malicious prompts; ablating them leads to higher attack success rates, while the model's utility remains unaffected. By locating these safety heads and concatenating their activations, we construct a straightforward but powerful malicious prompt detector that integrates seamlessly into the generation process with minimal extra inference overhead. Despite its simple structure of a logistic regression model, the detector surprisingly exhibits strong zero-shot generalization capabilities. Experiments across various prompt-based attacks confirm the effectiveness of leveraging safety heads to protect LVLMs.", "sections": [{"title": "1. Introduction", "content": "The fast development of large language models (LLMs) [11, 14, 36, 37] have driven rapid progress in large vision-Language models (LVLMs), such as GPT-4 [1], MiniGPT4 [8] and LLaVA [22]. These LVLMs have demonstrated remarkable abilities and achieved promising results across various applications by integrating vision encoders and fine-tuning on multimodal instruction-following datasets. However, recent studies have found that LVLMs exhibit more vulnerability to safety risks compared to its LLM backbone.\nThis alignment degradation is even more crucial regarding safety-related queries [23], raising critical concerns about their reliability and safety in real-world applications.\nSeveral existing works have explored the vulnerability of LVLMs. By transforming harmful content into images [16, 23, 26] or creating adversarial images [34, 46], the model can be easily jailbroken, leading to harmful responses. To improve model safety, a line of work has made successful attempts by training LVLMs with carefully curated datasets [47]. However, these tuning-based methods are annotation-intensive and computationally expensive. Inspired by findings in LLM fields that internal representations of the model can reveal human-interpretable concepts [27, 29, 40], we aim to provide a deeper understanding of the inner safety mechanisms of LVLMs and explore the potential of constructing a tuning-free method to enhance the model safety in a simpler yet more effective way.\nIn this study, we observe that the internal activations of certain attention heads in LVLMs can reliably distinguish malicious prompts across various attacks. This suggests that the model itself can detect malicious intent even before"}, {"title": "2. Related Work", "content": "Vulnerability of LVLMs. By combining the strengths of visual perception with large language models (LLMs), large viosion-language models (LVLMs) [3, 5, 9, 12, 17, 42] inherit the advanced reasoning capabilities of LLMs and perform exceptionally well in dialogues involving visual components. However, despite their impressive abilities, state-of-the-art LVLMs have shown increasing susceptibility to malicious prompt attacks [24], including vision only [23] or cross-modal [26] inputs. Recent studies in this area can be categorized into two main approaches concerning the introduction of malicious content. The first approach [16, 23, 26] transforms harmful content into images using text-to-image tools, effectively circumventing the safety mechanisms of LVLMs. For example, [16] demonstrates that embedding malicious queries within images through typography can effectively bypass the defense mechanisms of LVLMs. The second approach [13, 25, 31, 34, 38, 46] focuses on using gradient-based techniques to craft adversarial images that trigger harmful responses. Adversarial images in discriminative tasks are crafted with subtle changes, like minor perturbations or patches, using the model's gradients to deceive classifiers while staying unnoticed by humans [4, 7, 15, 33].\nSafeguarding LVLMs. To enhance the safety of LVLMs, a simple approach is to align them with specially designed red-teaming datasets [10, 21, 47]. However, training-based methods require a significant amount of high-quality data and sufficient computational resources and may only cover some possible types of attacks. Another strategy aims at securing LVLMs during the inference time; for instance, Wu et al. [41] suggests using manually crafted prompts to define acceptable and unacceptable behaviors but fails to generalize to unseen tasks. Recently, researchers [18] propose to leverage the safety perceptions of LLMs and transform the visual input to textual descriptions for further processing, Wang et al. [39] aim to learn extra prompts as a warning to address safety concerns. Unlike them, SAHs focuses on the intrinsic mechanism of LVLMs towards safety perceptions. The localized safety representations can be well generalized to other attacks with limited effort."}, {"title": "3. The Existence of Safety Attention Heads", "content": null}, {"title": "3.1. Preliminary and Setups", "content": "LVLM and Multi-head Attention. We consider an LVLM \\(M_{\\theta}\\) parameterized by \u03b8, with a general architecture consisting of a vision encoder, a LLM text decoder and a cross-modal projection module. Given an input image v and a text query q, v is first transformed into visual embeddings through the encoder and projection module, and then together with the query q as the input of LLM to generate response y autoregressively. Formally, we have:\n\\(y_t \\sim P_{\\theta}(\\cdot|v, q, y_{<t}) \\propto exp \\{f_{\\theta}(\\cdot|v, q, y_{<t})\\},\\) (1)\nwhere \\(y_t\\) is the tth token, \\(y_{<t}\\) denotes the token sequence generated up to time step t, and \\(f_{\\theta}\\) is the logit distribution produced by \\(M_{\\theta}\\). In each layer l, the Multi-head Attention (MHA) consists of H separate linear operations:\n\\(x_{l+1} = x_l + \\sum_{h=1}^{H} O_h a_l^h, a_l^h = Att_h(x_l),\\) (2)\nwhere \\(Att_h\\) is an operator offers token-wise communications and \\(O_h \\in \\mathbb{R}^{D_H \\times D}\\) aggregates head-wise activations."}, {"title": "3.2. Findings", "content": "We explore the role of attention heads in LVLM safety with linear probes. Our key findings are summarized as follows.\nFinding 1: Activations from attention heads can linearly separate malicious prompts from benign ones.\nWe conduct our experiments using MM-SafetyBench, leveraging internal activations from various attention layers within the LLM components of LVLMs"}, {"title": "4. SAHs: An Efficient and Generalizable LVLM Defender", "content": "Previous studies have shown that certain attention heads in LVLMs are strongly associated with distinguishing malicious prompts from benign ones. In this section, we take a further look at these \"safety heads\" and explore their potential application in safeguarding LVLMs.\nFigure 7 demonstrates an overview of SAHs, which mainly consists of three steps: 1) locating \"safety heads\" with few-shot linear probes; 2) Training a binary classifier based on the activations from \"safety heads\" as a malicious prompt detector; 3) Plug the detector into the first token forward pass as an almost free defender for LVLMs.\nThe amount of training data for linear probes plays an important role in identifying \u201ctrue\u201d safety heads, as we have"}, {"title": "Algorithm 1 Pipeline of SAHs", "content": "Step 1: Locating Safety Heads\nInput: Probing dataset Dtrain, Dval, LVLM M.\nOutput: Top-k safety heads Sk.\nfor Nshot in {1,2,..., |Dtrain|} do\n1. Collect activations for each attention heads al.\n2. Fit linear probes gl (\u00b7) on altrain\n3. Evaluate gl (\u00b7) on alval and get its accuracy.\n4. Select top-k probes with the highest accuracy and calculate their mean accuracy Acck.\nif Acck > \u03b8th then\nBreak, get Sk = {(l,h)}k.\nend if\nend for\nStep 2: Training Malicious Prompt Detector\nInput: Safety heads Sk, Train set Dtrain, LVLM M.\nOutput: Detector GM(\u00b7).\n1. Collect activations for attention heads in Sk.\n2. Concatenate activations and get Atrain \u2208 RNxkD.\n3. Train GM(\u00b7) on Atrain\nStep 3: Inference-time Defender\n1. Compute defense rate pG = GM(Atest) during the first token generation process.\n2. Add corresponding indicating prompt and start a regular generation.\ndiscussed before. In order to locate attention heads that produce the most discriminative pattern for classifying malicious and benign prompts, we start to fitting our probes with only 1 data pair as shown in Step1 of Algorithm 1. The data pair is randomly selected from the training set Dtrain, and the probe gl for each attention head at each layer is"}, {"title": "5. Experiment", "content": null}, {"title": "5.1. Setups", "content": "Datasets. MM-SafetyBench [23] is the widely-adopted prompt-based attack dataset as introduced in Section 3.1. Most of the malicious content is in the images, while the texts are usually benign. It generates harmful images in three different ways: Stable Diffusion (SD), Typography (TYPO), or their combination (SD+TYPO). VLGuard [47] is a large-scale vision-language safety dataset comprising 3,000 images with safe and harmful queries. Malicious information in VLGuard appears in both vision and text modalities, spanning five scenarios like Bad ads, privacy alerts, and hateful memes. VLSafe [10] offers 1,110 malicious image-text pairs in its examine split, and the malicious intent is clearly represented in the text queries only. Besides the benign samples in the above datasets, we also use a popular LVLM benchmark MM-Vet [43] to examine the \"over-defensiveness\u201d of the proposed method.\nLVLMs. We evaluate our method and other counterparts"}, {"title": "5.2. Main Results", "content": "Evaluation of Safety. Table 1 provides the evaluation results on MM-SafetyBench [23] and VLGuard [47]. Compared with other tuning-free methods, the proposed SAHs effectively defenses malicious prompts against jailbreak attacks on both datasets with remarkably high performance. Although AdaShield [39] can defend most of the malicious prompts with a low Attack success rate (ASR), it also rejects a great range of benign requests with a low Pass rate (PR), e.g., 36.73% on MM-SafetyBench with Qwen-VL-Chat. However, since our defender is based on the detector, which distinguishes two classes clearly, SAHs can achieve low ASR while maintaining high PR consistently. The comparison with supervised fine-tuning is provided in Figure 8, SAHs only need 10% of the training data to achieve even better performance than SFT, highlighting its data efficiency. Besides, SAHs is tuning-free and operates in inference time, significant computational cost can thus be saved.\nEvaluation of Utility. Table 2 demonstrates the results on the popular LVLM benchmark MM-vet [43]. Integrating the proposed SAHs into the original model has minimal impact on its general utility, demonstrating a low misclassification rate and negligible \u201cover-defensiveness.\u201d\nZero-shot Generalization Capability. Besides the remarkable performance of trained on in-domain data pairs, SAHs also shows strong zero-shot generalization capabilities. From the results of Figure 10, simply transferring the safety heads as well as the trained detector to another dataset can also achieve impressive performance. Specifically, the accuracy on VLGuard of zero-shot detector transferred from MM-SafetyBench reaches 88.6%, which is only around 10% lower than testing with its own detector. The striking results further validate the effectiveness of safety heads, as they are able to capture the most distinctive safety patterns and transfer to other datasets with no effort.\nInference Speed. Figure 9 reports the practical inference speed on an NVIDIA RTX 4090 GPU. Since SAHs is built on the detector, we also report comparisons of the detector-only version with other detector-based methods. ECSO [18] relies heavily on inference-time decisions involving multi-"}, {"title": "5.3. Analysis and Ablation Study", "content": "Scaling Safety Heads. The safety heads localized by few-shot probes capture the most distinctive safety representations and can build a well-generalized detector. To obtain a deep understanding of these safety heads, we scale up the number of heads to construct detectors in Figure 11a. With the increase of more safety heads, although the in-domain accuracy of MM-SafetyBench consistently grows, the zero-"}, {"title": "Multi-class Classification", "content": "Besides the binary classification of malicious and benign prompts, we also explore the potential of multi-classification with representations derived from safety heads. As MM-SafetyBench provides 13 sub-scenarios (e.g., illegal activity, physical harm), we provide the accuracy with 14 classes (benign prompts as one class) in Table 3. The classifier is trained with data sampled equally from each class. Due to limited data in some scenarios, we report the performance up to obtaining 30% of the whole dataset, and a clear increase can be witnessed.\nChoice of Classification Method. From the results in Table 4 we can see that the choice of classification method is not limited, and a simple Logistic Regression (LR) can already achieve promising performance with great efficiency.\nSource of Activations. We vary the choices of activations from different sources in Table 5. Since the difference between malicious and benign prompts of MM-SafetyBench only comes from images, the activations from the vision encoder can also be adopted for classification. However, the ViT's activations can not separate data pairs from VLGuard, which attacks happened both on visual and textual prompts. The results indicate that focus on activations from the language model is more robust for various attack types. The mean token averaged by all context tokens and the token from the last generation step t\u22121 are also examined, and the results further confirm the effectiveness of our choice.\nTransfer to Adversarial Attacks. We also conduct experiments on adversarial attacks to validate the transferability of the proposed SAHs. The adversarial attacks pose significant differences from malicious prompts since the harmfulness is hidden in unnoticeable noises in input images. The ADV-64 [32] and ImgJP [28] datasets are evaluated in this section. First, we repeat the head ablation as described in Section 3.2 and find that removing 64 attention heads leads to a significant increase in ASR while with little drop in MM-Vet, indicating the existence of safety heads for adversarial attacks. Next, we report the performance of the corresponding defender on both datasets. Equipping the SAHs achieves remarkably low ASR on ADV-64 and ImgJP, further verifying the transferability of safety heads."}, {"title": "6. Conclusion", "content": "This work reveals the role of attention heads in the safety of LVLMs, showing that several heads act as specialized shields to protect the models. By eliciting these \"safety heads\" through few-shot linear probes, we construct a simple yet effective detector capable of distinguishing malicious prompts. Built on this detector, we propose a LVLM defender SAHs that leverages the generation process with minimal extra inference cost, while demonstrating remarkable performance and zero-shot generalization capabilities. We hope our findings shed light on the discovery of internal representations of LVLMs in the safety field."}, {"title": "A. Datasets", "content": null}, {"title": "A.1. Safety Datasets", "content": "MM-SafetyBench [23] introduces jailbreaking attacks targeting LVLMs across 13 scenarios using malicious text prompts and images. The original dataset provides 1,680 unsafe questions designed for attacks. For each question, three different types of images are generated, categorized as follows: (1) SD: Images generated by Stable Diffusion (SD) [?], conditioned on malicious keywords; (2) TYPO: Images that contain malicious keywords embedded as textual content; (3) SD+TYPO: Images first generated by Stable Diffusion and then subtitled with malicious keywords by TYPO. \nA limitation of the dataset is that it only includes jailbreaking attacks. This means that a model that refuses to answer any questions would achieve perfect performance on the dataset, despite being impractical in real-world applications. To address this issue, [45] adopts the data generation pipeline from MM-SafetyBench [23], first generating safe questions by prompting GPT-4 and then converting these questions into image-question pairs. For the 9 safe categories-daily activity, economics, physical, legal, politics, finance, health, sex, and government-they generate 200 questions per category, each paired with a corresponding image. As a result, the safe dataset provides a total of 1,800 image-question pairs.\nIn our study, we use the SD+TYPO split, which usually shows the highest attack success rates. We use all 1680 unsafe data from MM-SafetyBench [23] as the benign data, while randomly selecting 1680 safe data from [45] as the malicious inputs. This results in a total of 1,680 positive-negative data pairs. Following AdaShield [39], we partition the MM-SafetyBench [23] dataset into two subsets: training & validation, and testing, with proportions of 15% and 85%. \nVLGuard [47] is proposed to ensure the safety alignment of LVLMs. Its training set contains 2,000 images, of which 977 are harmful and the remaining 1,023 are benign. Each benign image is paired with both a safe query-response pair and an unsafe query-response pair, while each harmful image is accompanied by a single query-instruction explaining the unsafe nature of the image. Note that the queries and responses in this dataset are generated by GPT-4. In total, the training set contains approximately 3,000 query-response pairs."}, {"title": "A.2. Utility Datasets", "content": "MM-Vet [43] defines six fundamental vision-language capabilities: recognition, OCR, knowledge, language generation, spatial awareness, and mathematical reasoning. These capabilities are integrated to tackle a variety of complex multimodal tasks. Unlike MME, MM-Vet requires the LVLM to provide open-ended answers, offering greater flexibility but introducing additional challenges in evaluation. To address this complexity, MM-Vet employs GPT-4 with few-shot evaluation prompts to score model predictions on a scale from 0 to 1. The overall utility score for the dataset is calculated as the sum of all prediction scores divided by the total number of questions, scaled by a factor of 100 to standardize the range to [0, 100].\nMME [?] evaluates the perception (MME-P) and cognition (MME-C) capabilities of LVLMs across 14 sub-tasks comprising a total of 2,374 questions. Each instruction consists of a question followed by the prompt, \u201cPlease answer yes or no.\u201d For every test image, two instructions are manually designed: the ground-truth answer to the first question is \"yes,\u201d while the answer to the second question is \u201cno.\u201d"}, {"title": "B. Experiment Details", "content": "Models For our experiments, we utilize the official model weights sourced from HuggingFace and GitHub repositories. The raw LVLMs include: LLaVA-v1.5-7B\u00b9, MiniGPT4-llama2-7B\u00b2 , and Qwen-VL-Chat\u00b3. To ensure reproducibility, we use greedy decoding for all inference tasks and set the maximum generation length to 512 tokens. All experiments are conducted on NVIDIA RTX 4090 GPUs.\nEvaluation Metrics In this paper, We utilize the keyword to calculate attack success rate (ASR) for evaluating the performance of all defense methods. This metric classifies jailbreak responses by detecting whether the predefined keywords exist in the responses from MLLMs. Such keywords include phrases like \u201cI am sorry", "over-defensiveness": "roblem while prioritizing the pass rate of normal requests, we design indicating prompts for regeneration rather than directly rejecting the request after detection. These indicating"}, {"title": "C. Additional Experiment", "content": "Utility Evaluation on MME. Besides the evaluation of utility on MM-Vet, we also conduct additional experiments on another popular LVLM benchmark, MME. As demonstrated in Table 11, similar results can be witnessed that the proposed SAHs brings minimal effects on the original LVLMs.\nMulti-classification Results on MM-SafetyBench. The detector proposed in SAHs can not only distinguish malicious prompts from benign ones but is also capable of multi-class detection. Table 12 provides detailed results for each sub-class in MM-SafetyBench, and a variation can be witnessed. In multi-class settings, benign prompts are also easy to separate from others, as well as some scenarios like Financial Advice and Health Consultation. The detection for scenarios like Malware Generation and Physical Harm seems to be more difficult than others."}]}