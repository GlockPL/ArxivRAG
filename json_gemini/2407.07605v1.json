{"title": "Early Explorations of Lightweight Models for Wound Segmentation on Mobile Devices", "authors": ["Vanessa Borst", "Timo Dittus", "Konstantin M\u00fcller", "Samuel Kounev"], "abstract": "The aging population poses numerous challenges to health-care, including the increase in chronic wounds in the elderly. The current approach to wound assessment by therapists based on photographic documentation is subjective, highlighting the need for computer-aided wound recognition from smartphone photos. This offers objective and convenient therapy monitoring, while being accessible to patients from their home at any time. However, despite research in mobile image segmentation, there is a lack of focus on mobile wound segmentation. To address this gap, we conduct initial research on three lightweight architectures to investigate their suitability for smartphone-based wound segmentation. Using public datasets and UNet as a baseline, our results are promising, with both ENet and TopFormer, as well as the larger UNeXt variant, showing comparable performance to UNet. Furthermore, we deploy the models into a smartphone app for visual assessment of live segmentation, where results demonstrate the effectiveness of TopFormer in distinguishing wounds from wound-coloured objects. While our study highlights the potential of transformer models for mobile wound segmentation, future work should aim to further improve the mask contours\u00b9.", "sections": [{"title": "1 Introduction", "content": "Chronic wounds, such as diabetic foot ulcers, affect a significant proportion of the world's population and present a major challenge to healthcare systems worldwide. In the United States alone, it is estimated that more than 6 million people suffer from chronic wounds each year [23]. In addition to diabetes, age is a significant risk factor, with a disproportionate impact on the elderly. This is evidenced by a prevalence of 7.8% in German nursing home residents [24] compared to 1% in the German population [9]. As the world's population continues to age and the incidence of conditions such as diabetes increases, the prevalence of chronic wounds is expected to soar in the coming years. This increase poses a significant challenge to healthcare providers in providing appropriate care to those affected. From the patient's perspective, the need for frequent visits to specialised wound care facilities to manage chronic wounds can be a significant burden. Access to such clinics may be limited, particularly in remote areas, and"}, {"title": "2 Approach", "content": "mobility issues exacerbate the challenge. There is an urgent need for innovative solutions to alleviate these burdens and improve access to wound care services. Telemedicine and automation offer promising ways to address these chal-lenges by improving efficiency and accessibility. Automated wound size moni-toring has the potential to eliminate the need for patients to visit clinics solely for monitoring purposes, thereby reducing the associated logistical challenges and relieving the burden on healthcare professionals. Furthermore, automated approaches provide timely and accurate information on wound status, facili-tating proactive intervention. Mobile semantic segmentation emerges as a key technology in realizing these advancements, providing real-time feedback and segmentation masks directly to users, thereby circumventing latency issues as-sociated with server-based processing. In addition, deploying models directly on smartphones ensures enhanced privacy and security by keeping sensitive medical data local. In addition, it enables offline functionality, which is crucial for appli-cations in remote areas with limited network coverage. Despite the advantages of mobile segmentation, there are also significant challenges associated with this endeavour. To be effective, such solutions must include small, lightweight models that can be seamlessly deployed on smartphones. At the same time, such models must be robust in the face of challenging conditions such as different back-grounds, varying lighting conditions, and different camera settings, as mobile wound monitoring is intended to take place in the patient's home environment. However, despite the necessity for compact yet resilient models, the inves-tigation into mobile wound segmentation remains limited to date. While sev-eral methods have been proposed for wound segmentation utilizing feature-engineering-based machine learning [27,35,14], deep learning [20,16,4,8], or even both [31,15], they often lack emphasis on lightweight architectures and mobile suitability. Some publications suggest that the analyzed architecture(s) could be used on mobile devices due to its efficiency, but such indications often rely solely on metrics such as parameter count as mobile performance indicator, lack-ing real-world deployment [33,20]. Of the few approaches deployed to a mobile device in practice, many apply traditional image processing rather than deep learning [25,30,7], while others evaluate only single architectures [19], or require manual annotation outlines from users [1,2]. To the best of our knowledge, a sys-tematic comparison of different neural network-based wound segmentation tech-niques on mobile devices is still pending. In contrast, the number of proposed architectures for mobile segmentation beyond the wound context has increased significantly. To address this discrepancy, this work conducts initial investiga-tions into the applicability of different architectures from non-wound domains for mobile wound segmentation, including a visual comparison in real-world usage."}, {"title": "2.1 Task Definition and Requirements", "content": "This study aims to assess the efficacy of lightweight models for mobile wound segmentation by systematically comparing approaches from beyond the wound"}, {"title": "2.2 Model Selection", "content": "The number of proposed models for segmentation on mobile devices has increased significantly beyond wound segmentation. Notable advances include CNN-based methods, such as encoder-decoder architectures with lightweight backbones such as MobileNets [11,10] combined with different decoders, ENet [21], UNeXt [29], or Fast-SCNN [22], which have proven successful in mobile environments. In addi-tion, vision transformer-based models, such as MobileFormer [3], MobileViT [17], Seaformer [32], or TopFormer [38], have been explored increasingly. From this spectrum, our model selection aims to cover different network types (CNN, vision transformer) and application domains (general-purpose, (bio)medicine), while setting reasonable limits on training effort. Therefore, we limit the inclusion of architectures to four, prioritizing the assessment of different-sized variants of selected methods where feasible. Specifically, we selected TopFormer [38], a representative (hybrid) vision transformer among recent general-purpose state-of-the-art (SOTA) methods, and UNeXt [29], a compact CNN-based SOTA technique for medical segmentation. The selection of these two SOTA mod-els was based on the following criteria: (i) competitive performance on respec-tive benchmark datasets (ii) publication in prestigious journals or conferences (iii) efficient resource utilization, as indicated by reported parameter counts and GFLOPs (iv) code availability on GitHub. Additionally, ENet [21], a widely ref-erenced general-purpose CNN for real-time semantic segmentation from 2016, was included due to its remarkable efficiency in terms of trainable parameters and promising outcomes in medical segmentation studies [37,28], given its size. Lastly, to establish a baseline for wound segmentation, we integrated UNet [26], a prominent model in biomedical segmentation."}, {"title": "3 Experimental Setup", "content": "3.1 Dataset\nWe perform experiments on a combination of two prominent datasets: the Foot Ulcer Segmentation Challenge 2021 (FuSeg) [34] and the Diabetic Foot Ulcer Challenge 2022 (DFUC 2022) [13,36] datasets. FuSeg encompasses 1,210 foot ulcer images, each with a resolution of 512 \u00d7 512 pixels, sourced from 889 pa-tients' medical visits spanning from October 2019 to April 2021. Meanwhile, the DFUC 2022 dataset comprises 4,000 foot ulcer images with a resolution of 640 \u00d7 480 pixels, captured approximately 30 to 40 cm away from the ulcer. Our custom dataset, Combined Foot Ulcers (CFU), integrates FuSeg and DFUC 2022, com-prising images with mixed resolutions of either 512 \u00d7 512 or 640 \u00d7 480 pixels. Due to the privacy of test set annotations (200 out of 1,210 for FuSeg and 2,000 out of 4,000 for DFUC 2022), only publicly accessible training and validation images are utilized for CFU. To prevent data leakage, duplicate images within FuSeg and DFUC 2022 were identified and removed. A similarity analysis, em-ploying the perceptual hashing algorithm [18], was conducted. Image sets with identical raw bytes or perceptual hash values exhibiting a Hamming distance of 11 or less were deemed duplicates. For illustrative purposes, a number of ex-amples are provided in Figure 2. Eliminating duplicate image-annotation pairs, we obtained a final dataset with 2,887 samples after discarding 123 pairs. Next, these samples were randomly partitioned into training (60%), validation (20%), and test (20%) set."}, {"title": "3.2 Evaluation Protocol", "content": "To assess model performance, we employed established segmentation metrics, such as Dice score (DSC), Intersection over Union (IoU), precision (Prc), and recall (Rec), utilizing the micro-averaging method while excluding the back-ground class. Our analysis covered two training modalities: training models from scratch and utilizing pre-training on either the ImageNet [6] or Cityscapes [5] dataset. To facilitate this, we downloaded pre-trained weights where available: ENet (Cityscapes), UNet (ImageNet), and TopFormer (ImageNet). Since pre-trained weights were unavailable for UNeXt, we trained both variants ourselves for 100 epochs on Cityscapes, using the best checkpoints as initial weights."}, {"title": "3.3 Implementation Details", "content": "We implemented this approach using Python 3.10.9 and PyTorch 2.0.1 on a server with two partitioned Nvidia A100 GPUs, reserving a large-sized instance with 40 GB VRAM for our experiments. All models were trained end-to-end using the AdamW optimizer with an initial learning rate of 0.0001, a batch size of four, and the binary cross entropy (BCE) loss. We used a ReduceLROn-Plateau scheduler conditioned on the Dice score, with a minimum learning rate of 0.000001. All models were trained for 200 epochs from scratch and for at least"}, {"title": "4 Evaluation", "content": "4.1 Results\nWe compare the selected models in Table 2, where we report the micro-averaged metric scores of each variant for both training modalities. Our findings indicate that, without pretraining, UNeXt-B exhibits the most favorable segmentation performance, balancing parameter count and segmentation quality. Notably, all lightweight models, except UNeXt-S, achieve results within a similar size range as the larger UNet, with the base variants of both UNeXt and Topformer, and ENet even slightly surpassing UNet. Pretraining considerably enhances performance across all models in terms of IoU and DSC metrics. Specifically, all lightweight networks demonstrate comparable performance, with IoU ranging from 69% to 72% and DSC from 82% to 84%, except for UNeXt-S, which performs worse."}, {"title": "4.2 Real-World Deployment", "content": "To assess the models' real-world capabilities, we developed a Flutter prototype. For seamless integration within the app, all PyTorch models except UNet were"}, {"title": "5 Conclusion and Future Work", "content": "In conclusion, our preliminary investigations into lightweight neural networks demonstrate their potential for mobile wound segmentation. The analyzed archi-tectures, especially TopFormer, yield promising results in real-world application scenarios, although some refinements are still necessary, such as clearer edge de-tection. Future work will include extending the comparison to further promising architectures such as MBSNet [12], MobileFormer [3], or Seaformer [32]. Fur-thermore, retraining the networks on an expanded dataset with more images, including those featuring objects with wound-similar colors, is expected to en-hance the robustness and generalization ability of the models, thereby improving the accessibility and effectiveness of future wound care."}, {"title": "A Appendix", "content": null}]}