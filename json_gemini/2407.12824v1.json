{"title": "Whispering Experts: Neural Interventions for Toxicity Mitigation in Language Models", "authors": ["Xavier Suau", "Pieter Delobelle", "Katherine Metcalf", "Armand Joulin", "Nicholas Apostoloff", "Luca Zappella", "Pau Rodr\u00edguez"], "abstract": "An important issue with Large Language Models (LLMs) is their undesired ability to generate toxic language. In this work, we show that the neurons responsible for toxicity can be determined by their power to discriminate toxic sentences, and that toxic language can be mitigated by reducing their activation levels proportionally to this power. We propose AUROC adaptation (AURA), an intervention that can be applied to any pre-trained LLM to mitigate toxicity. As the intervention is proportional to the ability of each neuron to discriminate toxic content, it is free of any model-dependent hyperparameters. We show that AURA can achieve up to 2.2\u00d7 reduction in toxicity with only a 0.72 perplexity increase. We also show that AURA is effective with models of different scale (from 1.5B to 40B parameters), and its effectiveness in mitigating toxic language, while preserving common-sense zero-shot abilities, holds across all scales. AURA can be combined with pre-prompting strategies, boosting its average mitigation potential from 1.28\u00d7 to 2.35\u00d7. Moreover, AURA can counteract adversarial pre-prompts that maliciously elicit toxic content, making it an effective method for deploying safer and less toxic models.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have increased their effectiveness in solving diverse tasks, spanning from text completion to storytelling and zero-shot common sense reasoning (Raffel et al., 2020; Brown et al., 2020; Zhang et al., 2022b; Touvron et al., 2023). Consequently, LLMs have gained popularity and are commonly used, even by non-ML experts. These models are pre-trained with simple tasks, such as predicting masked or the next tokens, on vast corpora gathered from diverse sources, with distinct content, style, and tone. However, the broadness of pre-training data can be a source of conflict with downstream tasks.\nMisalignment between pre-training and downstream tasks can result in undesired behaviors, such as generating harmful language, or perpetuating human biases embedded in the training data (Taylor et al., 2016; Brown et al., 2020). In this paper we focus on one of these undesired behaviors: the generation of harmful (toxic) language. Mitigating toxic language is a critical step towards the deployment of safe LLMs (Wallace et al., 2019; Gehman et al., 2020).\nA common solution to misalignment, including mitigating the generation of toxic language, is to fine-tune the weights of the network on data aligned with a desired behavior (Ouyang et al., 2022; Keskar et al., 2019; Korbak et al., 2023). In addition to the cost of gathering aligned data, this intervention requires an extra training phase, increasing the computational cost, and potentially harming other abilities of the network as a side-effect. Less involved alternatives add some pre-processing in the form of pre-prompting (Brown et al., 2020; Rae et al., 2021), or post-processing to detect undesired generations (Dathathri et al., 2019). These approaches are more flexible and easy to deploy, but they can be jail-broken (Perez & Ribeiro, 2022), and may degrade downstream performance and increase perplexity (Zhang et al., 2022a; Wolf et al., 2023).\nIn this study, we investigate intervention mechanisms that suppress the activations of toxicity-inducing neurons to reduce toxic content generation. We base our work on the discovery of expert neurons in neural networks, which are neurons that are responsible for encoding particular concepts (Radford et al., 2017). Suau et al. (2022) showed that adjusting the value of these neurons during generation induces the presence of the respective concept in the generated text with minimal impact on perplexity. While Suau et al. (2022) reported results on inducing concepts, they did not report results on concept suppression. However, they noted that zeroing the activations of expert neurons did not effectively suppress the respective concepts.\nWe revisit the idea of zeroing experts to mitigate toxic language, finding it mildly effective if the number of experts is carefully selected but causing a dramatic perplexity increase if too many are used. This sensitivity to the number of interventions makes it impractical since the optimal number of experts to intervene upon differs for each model.\nWe extend this study by introducing new strategies that are less sensitive to the number of intervened experts. Specifically, strategies that intervene softly on expert neurons to have less impact on model perplexity than zeroing activations. These soft interventions allow experts to pass some signal rather than completely muting them. We find that an effective soft intervention strategy is to dampen the contribution of expert neurons proportionally to their level of expertise. The proposed intervention only depends on each neuron's expertise, is free of model-dependent hyperparameters, straightforward to implement, and our findings indicate it is highly effective for toxicity mitigation. Importantly, it preserves the model's perplexities and performance on other tasks, such as zero-shot common sense. We coin this method AURA (AUROC Adaptation)."}, {"title": "2. Revisiting self-conditioning LLMs", "content": "Our work uses the presence of expert neurons in LLMs. Suau et al. (2022) showed that expert neurons can be used to induce presence of certain concepts in the generated"}, {"title": "3. Whispering Experts", "content": "In this section we first show that Detzero can mitigate toxicity but it is sensitive to the number of experts k intervened upon. Then, we show that a more effective approach is to dampen experts' activation by a constant factor \u03b1, rather than muting them as in Detzero. Finally, we propose a dynamic conditioning method that is effective at toxicity mitigation without additional hyperparameters. We provide a side-by-side algorithmic comparison of these three strategies for serving detoxified LLMs in Appendix A.\nThe following analysis is based on two metrics: a toxicity and a perplexity score. Toxicity is measured on the"}, {"title": "3.1. AURA", "content": "We propose to scale down the output of each expert neuron proportionally to the neuron's expertise. With this simple-yet-effective intervention, strong experts are almost muted, while non-expert neurons remain unaffected.\nThe use of AUROC to measure expertise allows us to select as experts those neurons whose expertise is above chance, \\(Q_{\\text{AUROC}>0.5}\\). Thus, adapting the dampening to the neuron's expertise simultaneously removes the need to find \u03b1 and k. This intervention has the same benefits shown with DAMP while removing the problem of fine-grained hyperparameter search. The intervention, which we name AURA, is defined as:\n\\[\\text{AURA}(z_m, a_m) = a_m z_m \\,\\forall m \\in Q_{\\text{AUROC}>0.5}.\\tag{2}\\]\nThe response of expert m is damped by a factor \\(a_m\\) designed to be proportional to the expertise of that neuron. We implement \\(a_m\\) as the Gini coefficient per neuron, which re-scales the AUROC so that 0 corresponds to a random classifier and 1 to a perfect classifier:\n\\[a_m = 1 - \\text{Gini}(z_m, y_c),\\tag{3}\\]\nwith \\(\\text{Gini}(z_m, y_c) = 2(\\text{AUROC}(z_m, y_c) - 0.5)\\). Since \\(a_m = 1\\) for a random toxicity classifier and \\(a_m = 0\\) for a perfect classifier, AURA keeps the original activation for all neurons with \\(\\text{AUROC} < 0.5\\). For experts with \\(\\text{AUROC} > 0.5\\), AURA scales down their activation values linearly. In Appendix D we show the range of \\(a_m\\) found for some of the models analyzed.\nServing Toxicity Mitigated LLMs. AURA can be efficiently implemented as a permanent modification of the weights and biases of the LLM. Let a layer output (before the non-linearity) be \\(z = Wx + b\\), then a dampening by \\(a_m\\) of the m-th neuron amounts to multiplying the m-th row of \\(W\\) and of \\(b\\) by \\(a_m\\). This intervention allows the suppression of toxic content in pre-trained LLMs that can then be deployed with no fine tuning or modification to the inference procedure."}, {"title": "4. Experimental Results", "content": "In this section we provide a summary of the experimental results that show the toxicity mitigation power of our method across a variety of models. For that, we use a set of LLMs, ranging from 1.5B to 40B parameters; as well as several benchmarks and baseline models.\nBenchmarks. We consider several hate speech and toxicity benchmarks throughout this paper, as well as common-sense reasoning benchmarks to assess general language modelling quality. We describe the toxicity and hate speech benchmarks in this section and refer the reader to Appendix E for the common-sense reasoning benchmarks:"}, {"title": "4.1. LLMs with AURA show less toxicity", "content": "In this section we evaluate how toxicity decreases when dampening toxic experts using AURA compared to other methods, on various models."}, {"title": "4.2. Interaction with Pre-prompting", "content": "With the rise of instruction-tuned models (Ouyang et al., 2022; Chung et al., 2022) prepending prompts (pre-prompts) has become an effective strategy to condition LLMs. Pre-prompts can induce a desired behaviour (eg., (Bai et al., 2022b)). However, malicious pre-prompts can also induce undesirable behavior (i.e., toxicity). Given the importance of prompting in today's use of LLMs, we evaluate how AURA interacts with favorable and adversarial pre-prompts. We take inspiration from Bai et al. (2022b) to construct the pre-prompts. The full evaluation including the pre-prompts used and generated examples can be found in Appendix H."}, {"title": "4.3. The Effect of AURA on Common-Sense Reasoning", "content": "In \u00a7 4.1 we show that AURA mitigates toxicity with minimal impact on non-toxic content, as indicated by \\(\\text{PPL}_{\\text{WIK}}\\). In this section we further evaluate how AURA affects higher-level abilities of LLMs, by measuring the difference in performance (with respect to the non-intervened model) on five common-sense reasoning tasks available in the Eleuther"}, {"title": "4.4. AURA Shifts Toxic Data Modes to OOD", "content": "We have introduced \\(\\text{PPL}_{\\text{WIK}}\\) in \u00a7 4.1, computed using the model post-intervention on a non-toxic data mode (Wikipedia). We expect \\(\\text{PPL}_{\\text{WIK}}\\) to remain unchanged as we intervene, indicating that the model after the intervention perceives a non-toxic mode as the original model.\nIn addition to \\(\\text{PPL}_{\\text{WIK}}\\), we measure how a model diverges from the nominal behavior on specific toxic data modes. To that end, we compute the following perplexities: \\(\\text{PPL}_{\\text{TX}}\\), \\(\\text{PPL}_{\\text{STX}}\\), \\(\\text{PPL}_{\\text{IDH}}\\), \\(\\text{PPL}_{\\text{THR}}\\), \\(\\text{PPL}_{\\text{INS}}\\) and \\(\\text{PPL}_{\\text{OBS}}\\) on the Toxic, Severe Toxic, Identity Hate, Threat, Insult and Obscene data modes of Jigsaw respectively. We expect these perplexities to increase as we strengthen the intervention, indicating that after the intervention the model perceives toxic data modes as out of distribution (OOD)."}, {"title": "4.5. Ablation Study", "content": "The two main design choices that make AURA hyperparameter-free are: (1) the number of experts intervened-on is automatically set by choosing those with \\(\\text{AUROC} > 0.5\\), and (2) the use of an intervention proportional to each neuron's level of expertise. In Table 2 we show that these result in a good trade-off in perplexity and toxicity mitigation, for MPT-7B.\nFor the choice of the number of experts to condition (k), we perform a sweep over k and compare the best k with only conditioning those experts with \\(\\text{AUROC} > 0.5\\). We found that the set of experts |\\(Q_{\\text{AUROC}>0.5}\\)| is much larger than the best k, and causes a catastrophic increase in perplexity when using constant interventions. AURA is robust to the choice of k since the dampening factor is proportional to each expert's AUROC. This results in AURA being able to condition more experts and further reduce toxicity without a drastic increase in perplexity.\nFor the intervention method, we compare AURA with set-"}, {"title": "5. Related Work", "content": "We give a brief overview of the relevant literature on measuring and reducing toxicity and biases in LMs and on controlling the behavior of a network with internal interventions.\nMeasuring toxicity and social biases. Generating text with LLMs can lead to toxic and biased content (Nadeem et al., 2020; Delobelle et al., 2022), and most recent advances in language modeling come with an investigation of these issues (Radford et al., 2018; Radford et al.; Zhang et al., 2022b; Touvron et al., 2023). These investigations rely on standardized benchmarks that were either designed for sentence encoders (May et al., 2019; Zhao et al., 2019; Basta et al., 2019; Kurita et al., 2019) or generation with a language model (Nangia et al., 2020; Nadeem et al., 2020; Sheng et al., 2019; Gehman et al., 2020; Welbl et al., 2021; Ju et al., 2022). However, defining and thus measuring these issues is complex (Jacobs & Wallach, 2021) and studies have highlighted the danger of taking results from these benchmarks (Blodgett et al., 2021), or worse, using them as a form of guarantee of safety (Delobelle et al., 2022).\nReducing toxicity and social biases. Some works reduce toxic generation by modifying the pre-training data (Keskar et al., 2019; Korbak et al., 2023), but most of the literature focuses on controlling the generation of pre-trained networks (Xu et al., 2020). The dominant approach is to finetune the network into a safer version, using either supervised examples or reinforcement learning with human feedback (Adolphs et al., 2022; Bai et al., 2022a; Zeldes et al., 2020; Ziegler et al., 2019; Chung et al., 2022; Ouyang et al., 2022). Finetuning produces a single language model \u2013 eg., a chatbot like ChatGPT or Claude \u2013 and hence, can only fit a single set of safety guidelines. It is thus not adapted to the case where we have different guidelines for different communities. Alternatives closer to our work, add a safety component on top of a fixed network by either filtering its output (Dathathri et al., 2019; Xu et al., 2020; Krause et al., 2020; Yang & Klein, 2021) or pre-prompting its generation (Li & Liang, 2021; Liu et al., 2022b). These approaches are more flexible, i.e., they can fit any community standards without modifying the network. Our work follows the same principles and complements existing work by modifying internal mechanisms instead of external quantities.\nExpert neurons. The seminal work of Radford et al. (2017) shows the existence of sentiment neurons in language models. These neurons can be manipulated to induce a positive or negative sentiment in the output. Suau et al. (2022) generalize expert neurons to arbitrary concepts by measuring their response to positive and negative examples. This approach modifies the behavior of the network while perturbing only a fraction of its neurons, reducing the impact on the perplexity than post-processing approaches, such as FUDGE (Yang & Klein, 2021) and PPLM-BoW (Dathathri et al., 2019)."}, {"title": "6. Limitations and Future Work", "content": "While our work focuses on the mitigation of toxic language in LLMs, we have not tested AURA to reduce the presence of other concepts. However, since the formulation of AURA is valid for any concept representable by a set of sentences, a similar behavior as the one observed for toxicity is expected. Note that the effectiveness of our mitigation approach is both contingent on the inclusion of relevant examples in the dataset used to rank experts, and on model's ability to capture the concept (presence of experts).\nAs demonstrated, it is possible to modify the weights of an LLM using AURA, and serve a toxicity suppressed version of the model. This amounts to performing a static intervention, however, we have not explored applying a dynamic intervention, for example when only specific behaviors or concepts are identified. We speculate that this would preserve the original model abilities even further.\nAs in Suau et al. (2022), we only consider linear layers outside attention blocks. A summary of the number of neurons considered is shown in Appendix I. A more thorough exploration could further improve our results. One such improvement could lead to more robustness to the architectural differences of Mistral-7B or Llama-v2."}, {"title": "7. Conclusion", "content": "We investigate intervention mechanisms to alleviate the issue of toxic language generation in pre-trained LLMs. We find that zeroing or dampening the activations of expert neurons are effective strategies but very sensitive to the choice of hyperparameters. Motivated by these findings, we introduce AURA, a new intervention that is hyperparameter-free: it dampens the response of LLM neurons proportionally to their ability to generate toxic language. In experiments we show that AURA achieves significant toxicity reductions (up to 2.2\u00d7) while having a minimal impact on perplexity and common-sense reasoning, and no impact on the computational cost of the LLM. Importantly, we show that AURA significantly amplifies the impact of positive pre-prompting and counteracts the negative impact of adversarial pre-prompting with respect to toxicity generation. We believe our work constitutes an important step towards the safe deployment of LLMs."}, {"title": "A. Algorithms", "content": "In this section we provide pseudo-code for the algorithms to compute neuron expertise (Algorithm 1), as well as to implement Detzero (Algorithm 2), DAMP (Algorithm 3) and AURA (Algorithm 4).\nLet \\(l(m)\\) be the linear layer of neuron m and \\(r(m)\\) be the position of neuron m in \\(l(m)\\). And let \\(W^{l(m)}\\) and \\(b^{l(m)}\\) be the weights matrix and biases vector of the linear layer \\(l(m)\\).\nIn the algorithms below we show in color those parameters that will require a search for each model."}, {"title": "B. Pareto Fronts of Toxicity vs.  PPLWIK for Different Models", "content": "We show in Figure 5 the effect of sweeping k in Detzero and DAMP (for the best \u03b1 found in Figure 6), complementing the analysis shown in Figure 2. As explained in \u00a7 3.1, Detzero initially reduces toxicity for low values of k, but soon starts increasing toxicity and perplexity with increasing k. Indeed, perplexity increases to prohibitive values for k close to |\\(Q_{\\text{AUROC}>0.5}\\)| (number of experts used in AURA) as also shown in Table 2.\nMistral-7B shows a different behavior, where Detzero is able to achieve a good reduction in toxicity at lower perplexity than AURA. Nevertheless, the increase in PPL incurred by AURA is below +3 points, and it is widely applicable to all models. On the other hand, Detzero is much less effective for all the other models, and requires an extra sweep of the parameter k. Similarly, while DAMP offers better trade-offs than Detzero, it requires to optimize both k and \u03b1, while AURA achieves very similar results, without the need of searching for any parameter.\nIn Figure 6 we show the Pareto fronts for the different models as we sweep \u03b1 between 0 and 1, in 0.1 intervals. We recall that \u03b1 = 1 means no intervention, while \u03b1 = 0 means setting expert neurons to 0 (as in Detzero). We see how \u03b1 = 0.5 (bold cross) provides a good trade-off between toxicity mitigation (x-axis) and an increase in perplexity (y-axis)."}, {"title": "C. Comparison between AP and AUROC for Detzero", "content": "In this work, rather than using the AP curve to identify experts, as in (Suau et al., 2022), we use the area under the ROC curve, which has the advantage of always being 0.5 for a random classifier, regardless of the class imbalance. To demonstrate that this is a suitable metric to replace the AP curve, we compare the ranking of expert neurons intervened-on with Detzero by AP and AUROC in Figure 7. We observe similar behavior when changing the sorting metric, showing that AUROC is also a suitable ranking metric."}, {"title": "D. AURA am dampening factor across models", "content": "To show the overall neuron toxicity expertise and to provide an intuition about which kind of factor \u03b1 AURA uses, we plot the dampening factors of the neurons under consideration with AUROC > 0.5. We can see that the minimum dampening factor range roughly between 0.2 to 0.3 while the maximum is 1, as expected since the majority of the neurons are not experts, hence their signal is not dampened.\nA lower dampening factor indicates a higher expertise. We see that GPT2-XL is the model with the lowest maximum expertise and also the one with the overall less number of experts as shown by the area above the curve (although this is not surprising given that it is also a smaller model).\nAmong the 7B parameters models (MPT-7B, Falcon-7B and Mistral), Mistral is the one with the highest maximum expertise but also the one with the lowest number of experts (as the curve increases more quickly than that of Falcon-7B and MPT-7B). Falcon-7B is the model, within this group, with the larger area above the curve (indicating high expertise but also high number of experts).\nInterestingly, the larger models (MPT-30B and Falcon-40B) do not show the highest expertise but as expected they have the largest number of experts."}, {"title": "E. Full results on zero-shot common sense reasoning", "content": "We evaluate the effect of AURA on the following five commonsense reasoning datasets:\n\u2022 PiQA (Bisk et al., 2020): Physical Interaction Question Answering, evaluates machine reasoning about physical interactions and dynamics through cause-and-effect scenarios. Tasks are formualted as multiple choice question answering: given a question q and two possible solutions s1, s2, a model or a human must choose the most appropriate solution, of which only one is correct.\n\u2022 SiQA (Sap et al., 2019): Social IQa (Commonsense Reasoning about Social Interactions), assesses a system's contextual reasoning ability by understanding and answering questions in specific social situations. Social IQa contains over 37K QA pairs for evaluating models' abilities to reason about the social implications of everyday events and situations.\n\u2022 TriviaQA (Joshi et al., 2017): Tests a model's general knowledge and reasoning skills with questions spanning diverse topics, evaluating its grasp of varied information. TriviaQA is a comprehensive reading comprehension dataset comprising more than 650K triples of question-answer-evidence. It encompasses 95K question-answer pairs contributed by trivia enthusiasts. The dataset also features independently collected evidence documents, with an average of six documents per question, offering robust distant supervision to ensure high-quality answers to the questions.\n\u2022 TruthfulQA (Lin et al., 2022): Evaluates a machine's accuracy in providing truthful responses, emphasizing the avoidance of generating misleading or incorrect answers. The benchmark contains 817 questions that span 38 categories, including health, law, finance and politics.\n\u2022 Hellaswag (Zellers et al., 2019): a dataset for grounded commonsense inference, features 70k multiple-choice questions from activitynet or wikihow domains. Each question involves grounded situations, presenting four answer choices about the potential next events in the scene."}, {"title": "F. RealToxicityPrompt Experimental Details", "content": "We use the setup of RealToxicityPrompts (Gehman et al., 2020) to evaluate toxic completions. Specifically, we generate 25 completions per prompt and generate maximum 20 tokens. For computational reasons, we evaluate 5000 randomly sampled prompts our of the entire dataset of 99k prompts, similar to Liu et al. (2021) where 1000 prompts were evaluated.\nTo generate the completions to the prompts, we use the \u2018generate' function from the Hugging Face transformers library, which automatically sets several hyperparameters (beams = 1, top-50 multinomial sampling, temperature = 1) based on the model's configuration."}, {"title": "G. Comparison of Toxicity Models", "content": "For reproducible comparisons between models, we changed the toxicity evaluation from RealToxcitityPrompts. This was originally done by Perspective API, which offers an endpoint to classify text as toxic or not. However, since the Perspective API does not support model pinning, there is no guarantee that the underlying classification models are the same in the future or even during this research."}, {"title": "H. Full results for Pre-Prompting", "content": "We use several pre-prompts to induce Falcon-7B-instruct to generate either toxic or non-toxic language. With these pre-prompts, we evalute how (1) the LLM behaves naturally and (2) how AURA is able to mitigate toxic behavior. The results are summarized in Table 5. We observe a strong reduction in toxicity when using non-toxic pre-prompts combined with AURA, showing how our method enhances the effect of collaborative pre-prompts (top). More interestingly, we observe a reduction in toxicity of up to 2.51\u00d7 in the presence of toxic pre-prompts (bottom). On average, AURA is able to reduce toxicity by 2x with respect to pre-prompting in presence of toxic pre-prompts, and by 1.86\u00d7 in the presence of non-toxic pre-prompts (top). In terms of total reduction with respect to the original model, AURA achieves an average 1.24\u00d7 for toxic pre-prompts, meaning that on average AURA is able to return the model to a toxicity state lower than the original model. On the other hand, AURA achieves an average 2.35\u00d7 reduction for non-toxic pre-prompts."}, {"title": "I. Number of Expert Neurons Intervened", "content": "In \u00a7 4.1 we report the toxicity mitigation at the optimal number of expert neurons k. This value is chosen to be the one that results in the lowest toxicity with an increase of PPLWIK smaller than 2 points. In Figure 9 we report the actual values found per model, as well as the total number of neurons considered in the expert identification phase. In Table 7 we list the number of layers are explored in this work."}, {"title": "J. Full results on Perplexities", "content": "Table 8: Impact of dampening toxic neurons on perplexity for toxic and non-toxic content. Evaluations of the perplexity of different models with and without AURA intervention. We evaluate on the WIK neutral corpus (to the left of the dotted line) and on different toxic datasets (to the right of the dotted line). We observe that the perplexity remains low and unchanged for neutral corpora and strongly increases for the toxic ones, indicating that toxic data has shifted to OOD."}, {"title": "K. Human Evaluation", "content": "Several works have shown that Perspective API has a high false alarm rate (Hosseini et al., 2017), and it is very sensitive to the presence of profanity terms (Chen, 2022), and to identity terms (Nozza et al., 2022).\nSince our toxicity scores are highly correlated to those from Perspective API (see Appendix G), we run a human evaluation to confirm whether AURA poses a real advantage for reducing toxicity in LLMs. We prompt each of the 7 models considered in Table 1 with 50 toxic and 50 non-toxic prompts randomly sampled from RTP and generate continuations with and without AURA. Each pair of continuations is then evaluated by 5 randomly selected annotators from a pool of 108. The annotators decide whether one continuation is equally or more toxic than the other, and whether one continuation is equally or more coherent with the prompt (see Figure 10)."}]}