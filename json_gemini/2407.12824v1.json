{"title": "Whispering Experts: Neural Interventions for Toxicity Mitigation in Language Models", "authors": ["Xavier Suau", "Pieter Delobelle", "Katherine Metcalf", "Armand Joulin", "Nicholas Apostoloff", "Luca Zappella", "Pau Rodr\u00edguez"], "abstract": "An important issue with Large Language Models (LLMs) is their undesired ability to generate toxic language. In this work, we show that the neurons responsible for toxicity can be determined by their power to discriminate toxic sentences, and that toxic language can be mitigated by reducing their activation levels proportionally to this power. We propose AUROC adaptation (AURA), an intervention that can be applied to any pre-trained LLM to mitigate toxicity. As the intervention is proportional to the ability of each neuron to discriminate toxic content, it is free of any model-dependent hyperparameters. We show that AURA can achieve up to 2.2\u00d7 reduction in toxicity with only a 0.72 perplexity increase. We also show that AURA is effective with models of different scale (from 1.5B to 40B parameters), and its effectiveness in mitigating toxic language, while preserving common-sense zero-shot abilities, holds across all scales. AURA can be combined with pre-prompting strategies, boosting its average mitigation potential from 1.28\u00d7 to 2.35\u00d7. Moreover, AURA can counteract adversarial pre-prompts that maliciously elicit toxic content, making it an effective method for deploying safer and less toxic models.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have increased their effectiveness in solving diverse tasks, spanning from text completion to storytelling and zero-shot common sense reasoning (Raffel et al., 2020; Brown et al., 2020; Zhang et al., 2022b; Touvron et al., 2023). Consequently, LLMs have gained popularity and are commonly used, even by non-ML experts. These models are pre-trained with simple tasks, such as predicting masked or the next tokens, on vast corpora gathered from diverse sources, with distinct content, style, and tone. However, the broadness of pre-training data can be a source of conflict with downstream tasks.\nMisalignment between pre-training and downstream tasks can result in undesired behaviors, such as generating harmful language, or perpetuating human biases embedded in the training data (Taylor et al., 2016; Brown et al., 2020). In this paper we focus on one of these undesired behaviors: the generation of harmful (toxic) language. Mitigating toxic language is a critical step towards the deployment of safe LLMs (Wallace et al., 2019; Gehman et al., 2020).\nA common solution to misalignment, including mitigating the generation of toxic language, is to fine-tune the weights of the network on data aligned with a desired behavior (Ouyang et al., 2022; Keskar et al., 2019; Korbak et al., 2023). In addition to the cost of gathering aligned data, this intervention requires an extra training phase, increasing the computational cost, and potentially harming other abilities of the network as a side-effect. Less involved alternatives add some pre-processing in the form of pre-prompting (Brown et al., 2020; Rae et al., 2021), or post-processing to detect undesired generations (Dathathri et al., 2019). These approaches are more flexible and easy to deploy, but they can be jail-broken (Perez & Ribeiro, 2022), and may degrade downstream performance and increase perplexity (Zhang et al., 2022a; Wolf et al., 2023).\nIn this study, we investigate intervention mechanisms that suppress the activations of toxicity-inducing neurons to reduce toxic content generation. We base our work on the discovery of expert neurons in neural networks, which are neurons that are responsible for encoding particular concepts (Radford et al., 2017). Suau et al. (2022) showed that adjusting the value of these neurons during generation induces the presence of the respective concept in the generated text with minimal impact on perplexity. While Suau et al. (2022) reported results on inducing concepts, they did not report results on concept suppression. However, they noted that zeroing the activations of expert neurons did not effectively suppress the respective concepts.\nWe revisit the idea of zeroing experts to mitigate toxic language, finding it mildly effective if the number of experts is carefully selected but causing a dramatic perplexity increase if too many are used. This sensitivity to the number of interventions makes it impractical since the optimal number of experts to intervene upon differs for each model.\nWe extend this study by introducing new strategies that are less sensitive to the number of intervened experts. Specifically, strategies that intervene softly on expert neurons to have less impact on model perplexity than zeroing activations. These soft interventions allow experts to pass some signal rather than completely muting them. We find that an effective soft intervention strategy is to dampen the contribution of expert neurons proportionally to their level of expertise. The proposed intervention only depends on each neuron's expertise, is free of model-dependent hyperparameters, straightforward to implement, and our findings indicate it is highly effective for toxicity mitigation. Importantly, it preserves the model's perplexities and performance on other tasks, such as zero-shot common sense. We coin this method AURA (AUROC Adaptation)."}, {"title": "2. Revisiting self-conditioning LLMs", "content": "Our work uses the presence of expert neurons in LLMs. Suau et al. (2022) showed that expert neurons can be used to induce presence of certain concepts in the generated text. We expand on this work to probe whether intervening on these neurons can also be used to mitigate the generation of given concepts, specifically toxic language. In this section we review the original algorithm, which is composed of two steps: identification of the experts, and intervention.\nIdentification of experts. Expert neurons are identified by considering each neuron m in the LLM as a potential classifier to detect the presence of a specific concept in a given prompt. Experts are evaluated by leveraging a dataset of N pairs \\{x_i, y_i\\}_{i=1}^N that defines a concept, where x is the i-th sentence and y_i = 1 if the sentence contains the concept c, y_i = 0 otherwise.\nEach neuron is analyzed in isolation, its maximum response (before the non-linearity) over each sentence in the dataset is used as a binary predictor for the the presence of concept c. Formally, z_m^i = \\max(\\z_m^i,t), where z_m,t^i is the response of neuron m to the t-th token of sentence i. All z_m^i values are computed using the dataset of N pairs and the expertise of the neuron for concept c is measured by the area under the Precision-Recall curve, AP(z_m, y_c), where to simplify the notation z_m and y_c are the vectorial representations of z_m and y over all N sentences. The set Q_k that contains the indices of the k neurons with highest AP(z_m, y_c) is the set of expert neurons for concept c.\nIntervention in (Suau et al., 2022). The intervention on Q_k used to induce the presence of concept c consists of replacing the output of each expert neuron with a fixed value y_c^{exp} = \\mathbb{E}_{y_c=1} [z_m], which is the mean maximum activation of that neuron in presence of concept c. We can summarize the intervention as:\n\\text{Det}(z_m, \\gamma_{det}) = \\gamma_{det} \\forall m \\in Q_k.\nIn (Suau et al., 2022) the authors mentioned that a similar intervention with \\gamma_{det} = 0 on Q_k was not successful in removing concepts from generated output. However, since no evaluation was presented, we quantify this intervention and refer to it as Detzero."}, {"title": "3. Whispering Experts", "content": "In this section we first show that Detzero can mitigate toxicity but it is sensitive to the number of experts k intervened upon. Then, we show that a more effective approach is to dampen experts' activation by a constant factor \u03b1, rather than muting them as in Detzero. Finally, we propose a dynamic conditioning method that is effective at toxicity mitigation without additional hyperparameters. We provide a side-by-side algorithmic comparison of these three strategies for serving detoxified LLMs in Appendix A.\nThe following analysis is based on two metrics: a toxicity and a perplexity score. Toxicity is measured on the RealToxicity Prompts (Gehman et al., 2020) dataset, while perplexity is computed on a fixed Wikipedia (Wikimedia) dataset. These metrics are explained in detail in \u00a7 4. However, it is helpful to remember that an ideal intervention should reduce the toxicity score while preserving perplexity (the lower the perplexity the better). Finally, while these initial analysis are presented on the MPT-7B model, we show in Appendix B that the conclusions hold for different models.\nIn this work, rather than using the AP curve to identify experts, as in (Suau et al., 2022), we use the area under the ROC curve, which is more interpretable and it behaves comparably to AP as we observe in Appendix C. The AUROC has the advantage of always being 0.5 for a random classifier, regardless of the class imbalance in y_c, which is not the case for AP.\nDetzero. We begin by analyzing the effectiveness of Detzero using an increasing number of experts k. We observe in Figure 2 (bottom) that for small values of k the toxicity can be reduced. However, when a larger portion of the model is muted the method typically fails catastrophically in toxicity and perplexity. From this, we conclude that the neurons selected as experts are indeed playing a role in the generation of toxic language. However, setting their activations to zero (effectively pruning part of the model) for a large set of neurons degrades the model abilities.\nDAMP. Our hypothesis is that a fixed intervention breaks the LLM inference dynamics after a certain k, thus limiting the effectiveness of Detzero. One way to make the intervention less destructive is to dampen the activations of experts by a factor \u03b1 as follows: DAMP(z_m, \\alpha) = \\alpha z_m \\forall m \\in Q_k (with 0 < \\alpha < 1). We conjecture that this intervention better preserves the dynamics of the LLM by allowing contextual signals to continue to pass through the network, and in turn allowing one to intervene on a larger set of experts and achieve a stronger mitigation. We assess various toxicity vs perplexity pareto-front curves for different values of k (as in Figure 2), and note that with DAMP we can achieve a better toxicity mitigation compared to Detzero while preserving perplexity when using up to k \u2248 4000 experts for a value of \u03b1 = 0.5. For more than 2000 experts, Detzero not only increases perplexity but also starts increasing toxicity. In Figure 2 (top), we show the effect of \u03b1 in DAMP, concluding that we can find a good combination of k and \u03b1 for which toxicity can be reduced by up to 2.3\u00d7 while the perplexity increases only by 0.92. Additionally, as shown in Figure 2 (bottom) in gray, intervening on a random set of neurons simply degrades perplexity while leaving toxicity almost unchanged. This confirms that the experts selected are toxicity-generating neurons and are a good set to intervene upon to mitigate toxicity.\nSummarizing, DAMP improves over Detzero but it does so at the cost of now two model-dependent hyperparameters to tune, k and \u03b1. Motivated by these results we propose in \u00a7 3.1 a hyperparameter-free intervention that uses the potential of the dampening strategy."}, {"title": "3.1. AURA", "content": "We propose to scale down the output of each expert neuron proportionally to the neuron's expertise. With this simple-yet-effective intervention, strong experts are almost muted, while non-expert neurons remain unaffected.\nThe use of AUROC to measure expertise allows us to select as experts those neurons whose expertise is above chance, Q_{\\text{AUROC}>0.5}. Thus, adapting the dampening to the neuron's expertise simultaneously removes the need to find \u03b1 and k. This intervention has the same benefits shown with DAMP while removing the problem of fine-grained hyperparameter search. The intervention, which we name AURA, is defined as:\n\\text{AURA}(z_m, a_m) = a_m z_m \\forall m \\in Q_{\\text{AUROC}>0.5}.\nThe response of expert m is damped by a factor a_m designed to be proportional to the expertise of that neuron. We implement a_m as the Gini coefficient per neuron, which re-scales the AUROC so that 0 corresponds to a random classifier and 1 to a perfect classifier:\na_m = 1 - \\text{Gini}(z_m, y_c),\nwith \\text{Gini}(z_m, y_c) = 2(\\text{AUROC}(z_m, y_c) \u2013 0.5). Since a_m = 1 for a random toxicity classifier and a_m = 0 for a perfect classifier, AURA keeps the original activation for all neurons with AUROC < 0.5. For experts with AUROC > 0.5, AURA scales down their activation values linearly. In Appendix D we show the range of a_m found for some of the models analyzed.\nServing Toxicity Mitigated LLMs. AURA can be efficiently implemented as a permanent modification of the weights and biases of the LLM. Let a layer output (before the non-linearity) be z = Wx + b, then a dampening by a_m of the m-th neuron amounts to multiplying the m-th row of W and of b by a_m. This intervention allows the suppression of toxic content in pre-trained LLMs that can then be deployed with no fine tuning or modification to the inference procedure."}, {"title": "4. Experimental Results", "content": "In this section we provide a summary of the experimental results that show the toxicity mitigation power of our method across a variety of models. For that, we use a set of LLMs, ranging from 1.5B to 40B parameters; as well as several benchmarks and baseline models.\nWe consider several hate speech and toxicity benchmarks throughout this paper, as well as common-sense reasoning benchmarks to assess general language modelling quality. We describe the toxicity and hate speech benchmarks in this section and refer the reader to Appendix E for the common-sense reasoning benchmarks:\n\u25b7 AURA reduces toxicity with minimal impact on perplexity. Overall, AURA achieves the greatest toxicity reduction on both benchmarks, especially on RTP. This relative improvement is encouraging since HONEST is composed of simple generated toxic and non-toxic sentences, while RTP contains more challenging prompts. On GPT2-XL, AURA achieves a 1.3\u00d7 reduction of toxicity on RTP with 0.96 lower PPLWIK, while DExperts achieves a 1.2\u00d7 reduction of toxicity on RTP with 1.48 increase in PPLWIK. Note that DExperts requires more memory since it is composed of the LLM, an expert, and a counter-expert LLM (which also incurs additional computational cost). Detzero can reach only 1.1\u00d7 toxicity reduction and CTRL is unable to reduce toxicity while preserving PPLWIK.\nInterestingly, all methods are more effective at reducing toxicity for non-toxic prompts. Note that Gehman et al. (2020) found non-toxic prompts were still able to increase toxicity at the output of the LLM. Thus, one should not take them as completely non-toxic. In this regime, AURA achieves up to 3.3\u00d7 mitigation with Falcon-7B. We confirm the effectiveness of AURA with a human evaluation in Appendix K, where annotators found AURA's continuations ~ 2\u00d7 less toxic than the vanilla model on average.\nWe observe that Detzero achieves better toxicity mitigation for Mistral and Llama-v2. However, AURA is consistent across models, does not require specific hyperparameter search and does not reduce model abilities (eg., Detzero reduces 0-shot performance for Llama-v2 by 4 points, see \u00a7 4.3). An important difference between Mistral and the other LLMs is the use of an updated transformer architecture with SwiGLU (Touvron et al., 2023). Exploring how architecture differences interact with expert interventions is a promising direction for further investigation."}, {"title": "4.2. Interaction with Pre-prompting", "content": "With the rise of instruction-tuned models (Ouyang et al., 2022; Chung et al., 2022) prepending prompts (pre-prompts) has become an effective strategy to condition LLMs. Pre-prompts can induce a desired behaviour (eg., (Bai et al., 2022b)). However, malicious pre-prompts can also induce undesirable behavior (i.e., toxicity). Given the importance of prompting in today's use of LLMs, we evaluate how AURA interacts with favorable and adversarial pre-prompts. We take inspiration from Bai et al. (2022b) to construct the pre-prompts. The full evaluation including the pre-prompts used and generated examples can be found in Appendix H.\n\u25b7 AURA significantly augments the positive impact of pre-prompting. In Figure 3 we report toxicity mitigation on Falcon-7B-i when prompting with favorable pre-prompts. We observe a strong reduction in toxicity when using non-toxic pre-prompts combined with AURA, showing how our method enhances the effect of collaborative pre-prompts. AURA achieves an average toxicity reduction of 2.35\u00d7 with respect to the original model, with a maximum of 2.94x. We also observe that pre-prompting alone achieves an average reduction of only 1.28\u00d7, showing the importance of AURA in the mitigation. Note that the original model (circles) has a PPLWIK = 12.2 while the model intervened with AURA (stars) has PPLWIK = 13.1, indicating that the intervention does not negatively affect the performance of the model on non-toxic content.\n\u25b7 AURA is robust to adversarial instruction pre-prompts. In Figure 3 we show pre-prompts that elicit toxic language in red. We observe a strong reduction in toxicity of up to 2.51x in the presence of toxic pre-prompts. On average, AURA is able to reduce toxicity by 2\u00d7 with respect to pre-prompting in presence of toxic pre-prompts. Note that toxic pre-prompts induce significant toxicity with an average increase of 1.58\u00d7. We note that, for most of the adversarial pre-prompts, AURA is able to return the model to a toxicity state lower than the original model (left of the vertical dashed line), showing an average reduction of 1.24x with respect to the original model.\nWe also observe that AURA cannot reduce toxicity for some very specific toxic pre-prompts. By inspecting them, we observe that such pre-prompts ask the LLM to be mostly unethical and foolish, which are concepts not necessarily captured by the \"toxicity\" sentences from the Jigsaw dataset that we used to identify expert neurons.\nOverall, AURA is robust to the pre-prompts evaluated and effective at reducing toxicity in instruction-tuned scenarios."}, {"title": "4.3. The Effect of AURA on Common-Sense Reasoning", "content": "In \u00a7 4.1 we show that AURA mitigates toxicity with minimal impact on non-toxic content, as indicated by PPLWIK. In this section we further evaluate how AURA affects higher-level abilities of LLMs, by measuring the difference in performance (with respect to the non-intervened model) on five common-sense reasoning tasks available in the Eleuther benchmark harness (Gao et al., 2023).\n\u25b7 AURA preserves 0-shot reasoning ability."}, {"title": "4.4. AURA Shifts Toxic Data Modes to OOD", "content": "We have introduced PPLWIK in \u00a7 4.1, computed using the model post-intervention on a non-toxic data mode (Wikipedia). We expect PPLWIK to remain unchanged as we intervene, indicating that the model after the intervention perceives a non-toxic mode as the original model.\nIn addition to PPLWIK, we measure how a model diverges from the nominal behavior on specific toxic data modes. To that end, we compute the following perplexities: PPLTX, PPLSTX, PPLIDH, PPLTHR, PPLINS and PPLOBS on the Toxic, Severe Toxic, Identity Hate, Threat, Insult and Obscene data modes of Jigsaw respectively. We expect these perplexities to increase as we strengthen the intervention, indicating that after the intervention the model perceives toxic data modes as out of distribution (OOD).\n\u25b7 AURA maintains non-toxic data modes and shifts toxic ones to OOD. Figure 4 summarizes the results for the non-intervened model and the increase in perplexity incurred when intervening with AURA. We group the perplexities as non-toxic (PPLWIK) and toxic (PPLTX, PPLSTX, PPLIDH, PPLTHR, PPLINS and PPLOBS). Indeed, we observe a minimal increase of 0.59 in perplexity for non-toxic data modes (left panel). This result shows how AURA preserves the likelihood of non-toxic data measured as a property of the intervened model (through PPLWIK), see full results in Table 8 in Appendix J). On the right panel of Figure 4, we show perplexities corresponding to toxic data modes, which are expected to increase after the intervention on the LLM. Note that these perplexities are already high for the non-intervened model, indicating their lower likelihood. However, AURA drastically increases the perplexities of toxic modes by a median increase of 193.46, showing that our method reduces the likelihood of toxic data modes."}, {"title": "4.5. Ablation Study", "content": "The two main design choices that make AURA hyperparameter-free are: (1) the number of experts intervened-on is automatically set by choosing those with AUROC > 0.5, and (2) the use of an intervention proportional to each neuron's level of expertise. In Table 2 we show that these result in a good trade-off in perplexity and toxicity mitigation, for MPT-7B.\nFor the choice of the number of experts to condition (k), we perform a sweep over k and compare the best k with only conditioning those experts with AUROC > 0.5. We found that the set of experts |QAUROC>0.5| is much larger than the best k, and causes a catastrophic increase in perplexity when using constant interventions. AURA is robust to the choice of k since the dampening factor is proportional to each expert's AUROC. This results in AURA being able to condition more experts and further reduce toxicity without a drastic increase in perplexity.\nFor the intervention method, we compare AURA with setting the experts to zero (Detzero) or dampening all experts equally by the best factor \u03b1 found through a sweep. Interestingly, finding the optimal \u03b1 and k yields similar results to AURA, with the downside of requiring an expensive sweep over two parameters. More details about the search of k, \u03b1 are given in Appendix B and Figure 2."}, {"title": "5. Related Work", "content": "We give a brief overview of the relevant literature on measuring and reducing toxicity and biases in LMs and on controlling the behavior of a network with internal interventions.\nMeasuring toxicity and social biases. Generating text with LLMs can lead to toxic and biased content (Nadeem et al., 2020; Delobelle et al., 2022), and most recent advances in language modeling come with an investigation of these issues (Radford et al., 2018; Radford et al.; Zhang et al., 2022b; Touvron et al., 2023). These investigations rely on standardized benchmarks that were either designed for sentence encoders (May et al., 2019; Zhao et al., 2019; Basta et al., 2019; Kurita et al., 2019) or generation with a language model (Nangia et al., 2020; Nadeem et al., 2020; Sheng et al., 2019; Gehman et al., 2020; Welbl et al., 2021; Ju et al., 2022). However, defining and thus measuring these issues is complex (Jacobs & Wallach, 2021) and studies have highlighted the danger of taking results from these benchmarks (Blodgett et al., 2021), or worse, using them as a form of guarantee of safety (Delobelle et al., 2022).\nReducing toxicity and social biases. Some works reduce toxic generation by modifying the pre-training data (Keskar et al., 2019; Korbak et al., 2023), but most of the literature focuses on controlling the generation of pre-trained networks (Xu et al., 2020). The dominant approach is to finetune the network into a safer version, using either supervised examples or reinforcement learning with human feedback (Adolphs et al., 2022; Bai et al., 2022a; Zeldes et al., 2020; Ziegler et al., 2019; Chung et al., 2022; Ouyang et al., 2022). Finetuning produces a single language model eg., a chatbot like ChatGPT or Claude \u2013 and hence, can only fit a single set of safety guidelines. It is thus not adapted to the case where we have different guidelines for different communities. Alternatives closer to our work, add a safety component on top of a fixed network by either filtering its output (Dathathri et al., 2019; Xu et al., 2020; Krause et al., 2020; Yang & Klein, 2021) or pre-prompting its generation (Li & Liang, 2021; Liu et al., 2022b). These approaches are more flexible, i.e., they can fit any community standards without modifying the network. Our work follows the same principles and complements existing work by modifying internal mechanisms instead of external quantities.\nExpert neurons. The seminal work of Radford et al. (2017) shows the existence of sentiment neurons in language models. These neurons can be manipulated to induce a positive or negative sentiment in the output. Suau et al. (2022) generalize expert neurons to arbitrary concepts by measuring their response to positive and negative examples. This approach modifies the behavior of the network while perturbing only a fraction of its neurons, reducing the impact on the perplexity than post-processing approaches, such as FUDGE (Yang & Klein, 2021) and PPLM-BoW (Dathathri et al., 2019)."}, {"title": "6. Limitations and Future Work", "content": "While our work focuses on the mitigation of toxic language in LLMs, we have not tested AURA to reduce the presence of other concepts. However, since the formulation of AURA is valid for any concept representable by a set of sentences, a similar behavior as the one observed for toxicity is expected. Note that the effectiveness of our mitigation approach is both contingent on the inclusion of relevant examples in the dataset used to rank experts, and on model's ability to capture the concept (presence of experts).\nAs demonstrated, it is possible to modify the weights of an LLM using AURA, and serve a toxicity suppressed version of the model. This amounts to performing a static intervention, however, we have not explored applying a dynamic intervention, for example when only specific behaviors or concepts are identified. We speculate that this would preserve the original model abilities even further.\nAs in Suau et al. (2022), we only consider linear layers outside attention blocks. A summary of the number of neurons considered is shown in Appendix I. A more thorough exploration could further improve our results. One such improvement could lead to more robustness to the architectural differences of Mistral-7B or Llama-v2."}, {"title": "7. Conclusion", "content": "We investigate intervention mechanisms to alleviate the issue of toxic language generation in pre-trained LLMs. We find that zeroing or dampening the activations of expert neurons are effective strategies but very sensitive to the choice of hyperparameters. Motivated by these findings, we introduce AURA, a new intervention that is hyperparameter-free: it dampens the response of LLM neurons proportionally to their ability to generate toxic language. In experiments we show that AURA achieves significant toxicity reductions (up to 2.2\u00d7) while having a minimal impact on perplexity and common-sense reasoning, and no impact on the computational cost of the LLM. Importantly, we show that AURA significantly amplifies the impact of positive pre-prompting and counteracts the negative impact of adversarial pre-prompting with respect to toxicity generation. We believe our work constitutes an important step towards the safe deployment of LLMs."}, {"title": "A. Algorithms", "content": "In this section we provide pseudo-code for the algorithms to compute neuron expertise (Algorithm 1), as well as to implement\nDetzero (Algorithm 2), DAMP (Algorithm 3) and AURA (Algorithm 4).\nServe LLM"}, {"title": "B. Pareto Fronts of Toxicity vs. PPLWIK for Different Models", "content": "We show in Figure 5 the effect of sweeping k in Detzero and DAMP (for the best \u03b1 found in Figure 6), complementing the analysis shown in Figure 2. As explained in \u00a7 3.1, Detzero initially reduces toxicity for low values of k, but soon starts increasing toxicity and perplexity with increasing k. Indeed, perplexity increases to prohibitive values for k close to |QAUROC>0.5| (number of experts used in AURA) as also shown in Table 2.\nMistral-7B shows a different behavior, where Detzero is able to achieve a good reduction in toxicity at lower perplexity than AURA. Nevertheless, the increase in PPL incurred by AURA is below +3 points, and it is widely applicable to all models. On the other hand, Detzero is much less effective for all the other models, and requires an extra sweep of the parameter k. Similarly, while DAMP offers better trade-offs than Detzero, it requires to optimize both k and \u03b1, while AURA achieves very similar results, without the need of searching for any parameter.\nIn Figure 6 we show the Pareto fronts for the different models as we sweep \u03b1 between 0 and 1, in 0.1 intervals. We recall that \u03b1 = 1 means no intervention, while \u03b1 = 0 means setting expert neurons to 0 (as in Detzero). We see how \u03b1 = 0.5 (bold cross) provides a good trade-off between toxicity mitigation (x-axis) and an increase in perplexity (y-axis)."}, {"title": "C. Comparison between AP and AUROC for Detzero", "content": "In this work, rather than using the AP curve to identify experts, as in (Suau et al., 2022), we use the area under the ROC curve, which has the advantage of always being 0.5 for a random classifier, regardless of the class imbalance. To demonstrate that this is a suitable metric to replace the AP curve, we compare the ranking of expert neurons intervened-on with Detzero by AP and AUROC in Figure 7. We observe similar behavior when changing the sorting metric, showing that AUROC is also a suitable ranking metric."}, {"title": "D. AURA \u03b1m dampening factor across models", "content": "To show the overall neuron toxicity expertise and to provide an intuition about which kind of factor \u03b1 AURA uses, we plot the dampening factors of the neurons under consideration with AUROC > 0.5. We can see that the minimum dampening factor range roughly between 0.2 to 0.3 while the maximum is 1, as expected since the majority of the neurons are not experts, hence their signal is not dampened.\nA lower dampening factor indicates a higher expertise. We see that GPT2-XL is the model with the lowest maximum expertise and also the one with the overall less number of experts as shown by the area above the curve (although this is not surprising given that it is also a smaller model).\nAmong the 7B parameters models (MPT-7B, Falcon-7B and Mistral), Mistral is the one with the highest maximum expertise but also the one with the lowest number of experts (as the curve increases more quickly than that of Falcon-7B and MPT-7B). Falcon-7B is the model, within this group, with the larger area above the curve (indicating high expertise but also high number of experts).\nInterestingly, the larger models (MPT-30B and Falcon-40B) do not show the highest expertise but as expected they have the largest number of experts."}, {"title": "E. Full results on zero-shot common sense reasoning", "content": "We evaluate the effect of AURA on the following five commonsense reasoning datasets."}, {"title": "F. RealToxicityPrompt Experimental Details", "content": "We use the setup of RealToxicityPrompts (Gehman et al., 2020) to evaluate toxic completions. Specifically, we generate 25 completions per prompt and generate maximum 20 tokens. For computational reasons, we evaluate 5000 randomly sampled prompts our of the entire dataset of 99k prompts, similar to Liu et al. (2021) where 1000 prompts were evaluated.\nTo generate the completions to the prompts, we use the \u2018generate' function from the Hugging Face transformers library, which automatically sets several hyperparameters (beams = 1, top-50 multinomial sampling, temperature = 1) based on the model's configuration.\nWe evaluate using the same metric for toxicity as RealToxicityPrompts: the probability of generating a toxic continuation at least once over 25 generations. Unlike RealToxicityPrompts, we determine if a continuation is biased using a classifier (see Appendix G) instead of the Perspective API for increased reproducibility, as the Perspective API can change their underlying model without notice."}, {"title": "G. Comparison of Toxicity Models", "content": "For reproducible comparisons between models, we changed the toxicity evaluation from RealToxcitityPrompts. This was originally done by Perspective API, which offers an endpoint to classify text as toxic or not. However, since the Perspective API does not support model pinning, there is no guarantee that the underlying classification models are the same in the future or even during this research. To determine which publicly available model is a suitable replacement for the Perspective API, we calculate the Inter-Annotator Agreement (IAA) between the Perspective API and the models listed in"}, {"title": "H. Full results for Pre-Prompting", "content": "We use several pre-prompts to induce Falcon-7B-instruct to generate either toxic or non-toxic language. With these pre-prompts, we evalute how (1) the LLM behaves naturally and (2) how AURA is able to mitigate toxic behavior. The results are summarized in Table 5. We observe a strong reduction in toxicity when using non-toxic pre-prompts combined with AURA, showing how our method enhances the effect of collaborative pre-prompts (top). More interestingly, we observe a reduction in toxicity of up to 2.51\u00d7 in the presence of toxic pre-prompts (bottom). On average, AURA is able to reduce toxicity by 2x with respect to pre-prompting in presence of toxic pre-prompts, and by 1.86\u00d7 in the presence of non-toxic pre-prompts (top). In terms of total reduction with respect to the original model, AURA achieves an average 1.24\u00d7 for toxic pre-prompts, meaning that on average AURA is able to return the model to a toxicity state lower than the original model. On the other hand, AURA achieves an average 2"}]}