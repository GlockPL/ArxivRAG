{"title": "Protecting Users From Themselves:\nSafeguarding Contextual Privacy in Interactions with Conversational\nAgents", "authors": ["Ivoline C. Ngong", "Swanand Kadhe", "Hao Wang", "Keerthiram Murugesan", "Justin D. Weisz", "Amit Dhurandhar", "Karthikeyan Natesan Ramamurthy"], "abstract": "Conversational agents are increasingly woven\ninto individuals' personal lives, yet users of-\nten underestimate the privacy risks involved.\nThe moment users share information with these\nagents (e.g., LLMs), their private information\nbecomes vulnerable to exposure. In this pa-\nper, we characterize the notion of contextual\nprivacy for user interactions with LLMs. It\naims to minimize privacy risks by ensuring\nthat users (sender) disclose only information\nthat is both relevant and necessary for achiev-\ning their intended goals when interacting with\nLLMs (untrusted receivers). Through a forma-\ntive design user study, we observe how even\n\"privacy-conscious\u201d users inadvertently reveal\nsensitive information through indirect disclo-\nsures. Based on insights from this study, we\npropose a locally-deployable framework that\noperates between users and LLMs, and identi-\nfies and reformulates out-of-context informa-\ntion in user prompts. Our evaluation using ex-\namples from ShareGPT shows that lightweight\nmodels can effectively implement this frame-\nwork, achieving strong gains in contextual pri-\nvacy while preserving the user's intended in-\nteraction goals through different approaches to\nclassify information relevant to the intended\ngoals.", "sections": [{"title": "1 Introduction", "content": "LLM-based Conversational Agents (LCAs) such as\nchatbots, can offer valuable services to individual\nusers in specialized systems\nsuch as customer service platforms and medical\nassistants, but present unique privacy challenges\nthat fundamentally differ from human-human inter-\nactions. For example, they can memorize and potentially misuse\ninformation. They are vulner-\nable to data breaches or unauthorized sharing with\nthird parties, and user-provided data\nmay be incorporated into future model training, po-\ntentially resulting in unintended information leaks\nduring deployment. In this paper, we focus on a critical but understud-\nied aspect in user-LCA interactions: helping users\nmake informed decisions about what information\nthey share with these untrusted agents in the first\nplace. This is particularly important because once\ninformation is shared with an LCA, users lose con-\ntrol over how it might be used or disseminated.\nFigure 1 provides an overview of our proposed\nmethodology to achieve this."}, {"title": "Motivation", "content": "As LCAs become more adept at han-\ndling complex tasks and users remain uninformed\nabout privacy risks, they develop increasing trust in\nboth the technology and their own ability to protect\nthemselves. Indeed, it has been shown that\nusers are increasingly disclosing personal and sen-\nsitive information to LCAs. In our own forma-\ntive user study (Section 3), we found that even\nexpert participants are unaware of how indirect dis-\nclosures could reveal sensitive details in specific\ncontexts. They expressed a desire for a real-time\nsystem that could highlight privacy risks and assist\nin revising information before sharing it with con-\nversational agents. Similarly, our analysis of the\nreal-world ShareGPT dataset (Chiang et al., 2023),\nreveals that users often share information beyond\nwhat their context requires, inadvertently exposing\nsensitive details that were unnecessary for their in-\ntended goals (see examples in Table 1, details in\nSection 3).\nThis motivates our main objective:\nDevelop a framework that operates between users\nand conversational agents to detect and manage\ncontextually inappropriate sensitive information\nduring interactions."}, {"title": "Contextual Privacy", "content": "To enable the development\nof such a framework, we define the notion of con-\ntextual privacy in user-LCA interactions, drawing\nideas from the Contextual Integrity (CI) theory\n. Contextual integrity\ndefines privacy not merely as hiding personal infor-\nmation, but as maintaining appropriate information\nflows within specific contexts. Drawing on the\nfundamental CI parameters, we define contextual\nprivacy by characterizing User\u2192LCA information\nflows (Section 2). Our contextual privacy notion\nrequires that user prompts include only information\nthat is contextually appropriate, relevant, and nec-\nessary to achieve the user's intended goals when\ninteracting with LCAs, going beyond approaches\nthat simply protect sensitive information. For instance, when a user\nis querying an LCA of a bank to locate tax forms,\nsharing SSN would adhere to contextual privacy,\nas it may be necessary for the task. On the other\nhand, if a user seeks advice on managing personal\nfinances, sharing the names of family members\nwould violate contextual privacy."}, {"title": "Proposed Framework", "content": "We design a framework\nthat can protect users during their interactions with\nLCAs. By analyzing user inputs, detecting poten-\ntially sensitive irrelevant content, and guiding users\nto reformulate prompts based on contextual rel-\nevance, our framework empowers users to make\nmore informed, privacy-conscious decisions in real\ntime. Rather than enforcing rigid privacy rules,\nthe system helps users understand the privacy im-\nplications of their choices while preserving their\nintended interaction goals.\nOur main contributions include:\n\u2022 We formulate the definition of contextual privacy\nfor the specific case of User LCA information\nflows, where users act as senders and LCAs as\nuntrusted receivers;\n\u2022 We apply our contextual privacy definition to\nanalyze real-word conversation from ShareGPT\n(Chiang et al., 2023) and demonstrate how users\nunintentionally violate contextual privacy in in-\nteractions with LCAs;\n\u2022 We develop a privacy safeguarding framework\nthat acts as an intermediary between the user\nand LCA, and helps users identify and reformu-\nlate out-of-context information in their prompts\nwhile maintaining their intended goals;\n\u2022 We design novel metrics to measure the con-\ntextual privacy and utility performance of our\nframework;\n\u2022 We show that our privacy safeguarding frame-\nwork can be implemented using a small LLM\nthat can be locally deployed at the user side.\nWe consider three state-or-the-art models for\nimplementation, and compare their privacy and\nutility performances. Our experiments shows\nthat lightweight models can effectively imple-\nment this framework, achieving both strong pri-\nvacy protection and utility through different ap-\nproaches to classify information relevant to the\nintended goals.\nWe fully contextualize our contributions with\nregards to existing literature in Appendix A."}, {"title": "2 Threat Models and Privacy Definition", "content": "Threat Model. We consider a scenario where users\ninteract with large, remote, and untrusted LCAs\nthrough APIs. These can be web-based or hosted\non cloud-based services or private networks and\nmay be either general-purpose or domain-specific.\nUsers often share personal, financial, or medical"}, {"title": "3 A Framework for Safeguarding\nContextual Privacy", "content": "Our goal is to develop a framework that acts as an\nintermediary between the user and LCA, and en-\nables the user to detect whether their prompt incurs\nany contextual privacy violations, and judiciously\nreformulate the prompt to ensure contextual pri-\nvacy. We first conduct a formative design study to\nguide our framework design.\nUser Study to Guide Our Framework Design:\nWe conducted a Wizard-of-Oz formative user study\nto explore users' expectation of privacy when inter-\nacting with LCAs and to gather technical require-"}, {"title": "Proposed Framework", "content": "We propose a framework\nthat acts as an intermediary between the user and\nthe conversation agent and enables the user to de-\ntect out-of-context sensitive information in the user\nprompt and judiciously reformulate the prompt to\nensure contextual privacy. The key components"}, {"title": "4 Implementation and Evaluation", "content": ""}, {"title": "4.1 Contextual Privacy Evaluation of\nReal-World Queries", "content": "Before implementing and evaluating our frame-\nwork, we first perform initial privacy analysis by\nevaluating an open-source version of the ShareGPT\ndataset to understand the\nprevalence of contextual privacy violations. To\ninstantiate our formal privacy definition, we used\nLlama-3.1-405B-Instruct (Team, 2024) as judge,\nwith a prompt designed to identify violations of\ncontextual integrity (Appendix D.1). From over\n90,000 conversations, we retain 11,305 single-turn\nconversations within a reasonable length range (25-\n2,500 words). For each conversation, the judge\nmodel assessed the context, sensitive information,\nand their necessity for task completion. This anal-\nysis identified approximately 8,000 conversations\ncontaining potential contextual integrity violations.\nTo manage inference costs, we focused on cases"}, {"title": "4.2 Implementation Details", "content": "Models. We implement our framework using a\nmodel that is significantly smaller than typical\nchat agents like ChatGPT, enabling users to de-\nploy the model locally via Ollama\u00b9 without relying\non external APIs. In our experiments, we evaluate\nthree models with different characteristics: Mixtral-\n8x7B-Instruct-v0.1\u00b2 , Llama-\n3.1-8B-Instruct\u00b3 (Team, 2024), and DeepSeek-R1-\nDistill-Llama-8B\u2074 (focused on reasoning) (Team,\n2025). We refer to these models as Mixtral, Llama\nand Deepseek in short going forward. The local\ndeployment of models ensures no further privacy\nleakage due to the framework. Although our eval-\nuation focuses on three LLMs, our approach is\nmodel-agnostic and can be applied to other archi-\ntectures. For assessment of privacy and utility, we\nuse Llama-3.1-405B-Instruct (Team, 2024) as an\nimpartial judge, which was hosted in a secure cloud\ninfrastructure.\nExperiment Setup. As discribed in the previous\nsection, our framework processes user prompts in\nthree stages: (a) context identification, (b) sensi-\ntive information classification, and (c) reformula-\ntion. The locally deployed model first determines\nthe context of the conversation, identifying its do-\nmain and task (Appendix C) using the prompts\nin Appendix Appendix D.2 and Appendix D.3 re-\nspectively. It then detects sensitive information,\ncategorizing it as either essential (required for task\ncompletion) or non-essential (privacy-sensitive and\nremovable). Finally, if non-essential sensitive in-\nformation is present, the model reformulates the\nprompt to improve privacy while preserving intent.\nWe implement two approaches for sensitive in-\nformation classification: dynamic classification"}, {"title": "4.3 Evaluation and Results", "content": "We evaluate our framework by measuring two key\nmetrics: privacy gain and utility. Privacy gain\nquantifies how effectively sensitive information is\nremoved during reformulation, while utility mea-\nsures how well the reformulated prompt maintains\nthe original prompt's intent. We compute these\nmetrics using two complementary methods: an au-\ntomated BERTScore-based comparison of sensitive\nattributes, and an LLM-based assessment that ag-\ngregates multiple evaluation aspects."}, {"title": "4.3.1 Evaluation via Attribute-based Metrics", "content": "Metrics. We measure privacy gain by comput-\ning semantic similarity between non-essential at-\ntributes between original and reformulated prompts,\nwhere similarity is computed using BERTScore\n. Specifically, we first run\nthe judge model on reformulated prompts to ob-\ntain non-essential sensitive attributes $P_{non-ess}^{re}$$m\non-ess$, us-\ning a prompt designed to identify contextual pri-\nvacy violations (Appendix D.1). We have non-\nessential sensitive attributes for original prompts\nfrom Section 4.1. Given sets of strings\n$P_{non-ess}^{orig}$ and $P_{non-ess}^{re}$, privacy gain is computed as\n$BERTScore (P_{non-ess}^{orig}, P_{non-ess}^{re})$, with a score of\n1.0 assigned when either set is empty. A higher\nprivacy gain indicates better removal of sensi-\ntive information. For utility, we measure seman-\ntic similarity between essential attributes using\n$BERTScore (P_{ess}^{orig}, P_{ess}^{re})$, where a score closer\nto 1.0 indicates better preservation of task-critical\ninformation. Since BERTScore works on text pairs,\nwe match each original attribute to its closest refor-\nmulated one and compute utility as the fraction of\nmatched attributes above a similarity threshold of\n0.5.\nResults. Table 3 shows that under dynamic clas-\nsification, all three models achieve strong pri-\nvacy scores (0.85-0.88) with comparable utility\n(~ 0.57), suggesting that the ability to identify\ncontext-specific sensitive information is robust\nacross different model architectures.\nThe structured classification approach shows\ngreater variation between models. While Llama\nachieves high scores in both privacy (0.873) and\nutility (0.606), structured classification generally\nyields slightly lower privacy scores but more vari-\nable utility. This suggests a natural trade-off: prede-"}, {"title": "4.3.2 LLM-as-a-Judge Assessment", "content": "Setup. We use Llama-3.1-405B-Instruct as a\njudge to provide a complementary evaluation of\nprivacy and utility across 100 randomly selected\nqueries per model (6\u00d7100 total). Given the high\ncomputational cost of LLM-based inference, this\ntargeted sampling allows us to validate key trends\nobserved in the attribute-based evaluation while\nminimizing overhead. Privacy gain is computed\nby asking the judge to evaluate privacy leakage,\ncoverage, and retention, while utility is computed\nby measuring query relevance, response validity,\nand cross-relevance. These binary evaluations are\naveraged to produce final privacy gains and utility\nscores. See Appendix D.7 for detailed prompts and\nevaluation criteria.\nResults. The LLM-based assessment shows gen-\nerally higher utility scores (0.82-0.86) across all\nmodels compared to BERTScore-based evaluation,\nwhile maintaining similar privacy levels (0.80-\n0.86). This difference can be attributed to how\nattributes are detected and compared\u2014BERTScore\nevaluates exact semantic matches between at-\ntributes, while the LLM judge takes a more holistic\nview of information preservation. For instance,\nwhen essential information is restructured (e.g.,\n\"my friend Mark\" split into separate attributes),\nBERTScore may indicate lower utility despite se-\nmantic equivalence.\nThe LLM evaluation confirms the effectiveness\nof both classification approaches, with dynamic\nclassification showing slightly more consistent per-\nformance across models. Llama maintains its\nstrong performance under both approaches (privacy\ngain: ~ 0.85, utility score: ~ 0.86), reinforcing its\nreliability for privacy-preserving reformulation."}, {"title": "4.3.3 Example Reformulations and Trade-offs", "content": "Setup. Table 5 presents a set of diverse example\nreformulations illustrating our framework's abil-\nity to balance privacy and utility across different\nscenarios. These examples highlight both ideal\ncases\u2014where reformulation effectively preserves\nboth privacy and utility\u2014and more challenging\nones where trade-offs are unavoidable.\nResults. Our framework successfully removes\npersonal identifiers while preserving task rele-\nvance, as seen in the third example (privacy gain =\n0.5, utility score = 0.83). In creative requests like\nthe Valentine's poem (second example), removing\npersonal details reduces privacy risks but slightly\nimpacts personalization utility = 0.5).\nSome contexts resist reformulation. The last\nconversation (privacy gain = 0.0, utility score =\n0.0) highlights cases where the entire prompt is\ninherently sensitive, requiring alternative privacy\nmeasures beyond text transformation. This is not\na failure of our approach but an indicator of when\nreformulation alone is insufficient.\nThese examples reinforce that privacy-\npreserving reformulation is a trade-off, not a\none-size-fits-all solution."}, {"title": "5 Discussion and Conclusion", "content": "Drawing ideas from the contextual integrity the-\nory, we defined the notion of contextual privacy\nfor users interacting with LLM-based conversation\nagents. We proposed a framework, grounded in\nour contextual privacy formulation, that acts as an\nintermediary between the user and the agent, and\ncarefully reformulates user prompts to preserve\ncontextual privacy while preserving the utility.\nThis work serves as an initial step in exploring\nprivacy protection in user interactions with conver-\nsational agents. There are several directions that"}, {"title": "Limitations", "content": "Contextual integrity is a relatively new and fluid\nnotion of privacy. Ours is also one of the very early\nworks exploring this space from the standpoint of\nLLM-based conversational agents. Naturally, this\nleads to a number of challenges, some of which\nare beyond the scope of the work and should be\naddressed in the future. Like we discussed before,\nestablishing privacy norms and principles in CI it-\nself is complex and dependent on societal contexts,\nwhich is why we restrict ourselves to a practical and\nuseful variation of the idea. However, developing\ntemplates for implementing CI under various soci-\netal contexts deserves significant attention from the\nresearch community in the future.\nOur framework addresses critical privacy con-\ncerns in LLM interactions, potentially shaping fu-\nture norms around data sharing in conversational\nAI. By enhancing user awareness and control over\nsensitive information, it promotes more ethical AI\ndeployments, safeguarding user privacy in diverse\napplications such as healthcare, legal, and personal\nassistance. However, there are ethical challenges,\nsuch as ensuring fairness across cultural contexts\nand preventing over-reliance on automated privacy\ndetection."}, {"title": "B User Study to Guide System Design", "content": "To explore users' perceptions of privacy with LCAs\nand gather technical requirements for our frame-\nwork, we conducted a Wizard-of-Oz formative user\nstudy with six participants from our institution who\nwere generally familiar with LLMs.\nThe study involved a 30-minute semi-structured\ninterview where participants were presented with\nthree mid-fidelity UX mockups, each designed to\ndemonstrate different ways private and sensitive\ninformation could be detected and remediated (see\nAppendix B.1). These mockups, featuring syn-\nthetic examples inspired by real-world patterns in\nthe ShareGPT dataset, were created to expose par-\nticipants to targeted privacy risks, such as uninten-"}, {"title": "C Domains and Tasks", "content": ""}, {"title": "D Prompts", "content": ""}, {"title": "D.1 Prompt Template for Detection of Contextual Privacy Violations", "content": "The prompt that was used for detecting the primary context, essential and non-essential information in the\nuser's prompt is:"}, {"title": "D.2 Prompt Template for Intent Detection", "content": ""}, {"title": "D.3 Prompt Template for Task Detection", "content": ""}, {"title": "D.4 Dynamic Prompt Template for Sensitive Information Detection", "content": ""}, {"title": "D.5 Structured Prompt Template for Sensitive Information Detection", "content": ""}, {"title": "D.6 Prompt Template For Reformulation", "content": ""}, {"title": "D.7 LLM-as-a-Judge Evaluation Prompt Template", "content": ""}, {"title": "E User Prompts Before And After Reformulation", "content": ""}, {"title": "F User Prompts Before And After Reformulation", "content": ""}]}