{"title": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities", "authors": ["Fabrizio Davide", "Pietro Torre", "Andrea Gaggioli"], "abstract": "We tasked 16 state-of-the-art large language models (LLMs) with estimating the likelihood of Artificial General Intelligence (AGI) emerging by 2030. To assess the quality of these forecasts, we implemented an automated peer review process (LLM-PR). The LLMs' estimates varied widely, ranging from 3% (Reka-Core) to 47.6% (GPT-40), with a median of 12.5%. These estimates closely align with a recent expert survey that projected a 10% likelihood of AGI by 2027, underscoring the relevance of LLMs in forecasting complex, speculative scenarios. The LLM-PR process demonstrated strong reliability, evidenced by a high Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable consistency in scoring across the models. Among the models, Pplx-70b-online emerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A cross-comparison with external benchmarks, such as LMSYS Chatbot Arena, revealed that LLM rankings remained consistent across different evaluation methods, suggesting that existing benchmarks may not encapsulate some of the skills relevant for AGI prediction. We further explored the use of weighting schemes based on external benchmarks, optimizing the alignment of LLMs' predictions with human expert forecasts. This analysis led to the development of a new, 'AGI benchmark' designed to highlight performance differences in AGI-related tasks. Our findings offer insights into LLMs' capabilities in speculative, interdisciplinary forecasting tasks and emphasize the growing need for innovative evaluation frameworks for assessing AI performance in complex, uncertain real-world scenarios.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) are a type of artificial intelligence system trained on vast amounts of text data to understand and generate human-like text. These models, which include systems like GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers), have demonstrated remarkable capabilities in tasks ranging from natural language understanding and generation to complex problem-solving and analysis. As these models continue to evolve, becoming increasingly sophisticated and multifaceted, the need for comprehensive evaluation methods has become paramount.\nTraditional evaluation methods for LLMs often rely on task-specific benchmarks designed to assess performance in narrowly defined domains. Standardized tests in areas such as question answering, text summarization, or sentiment analysis provide important insights into specific functionalities. However, these tests often operate within confined parameters that may not reflect the open-ended, multifaceted nature of real-world cognitive challenges.\nThe limitations of traditional benchmarks become particularly apparent when attempting to evaluate LLMs' performance on tasks that require the integration of knowledge across multiple domains, abstract reasoning, and metacognitive abilities. Real-world problems often demand a synthesis of diverse information, the ability to reason under uncertainty, and the capacity for self- evaluation-facets that existing evaluation frameworks may not fully capture.\nTo address these limitations, we introduce an assessment methodology that combines two key tasks:\n1. An Artificial General Intelligence (AGI) forecasting task: We tasked LLMs with predicting the probability of AGI occurring by 2030. We chose this task because it presents an open-ended challenge requiring the integration of knowledge across multiple domains such as computer science, cognitive science, philosophy, and futurism.\n2. A LLM peer review (LLM-PR) task: This approach involves LLMs evaluating each other's forecasts, including their own, based on a set of predefined criteria. This method builds upon and extends previous work on"}, {"title": "2. Background", "content": null}, {"title": "2.1 Evolving challenges in LLMs evaluation", "content": "LLMs such as GPT, BERT, and their successors, have revolutionized natural language processing by demonstrating unprecedented capabilities in generating, understanding, and interacting with human language across a wide range of contexts. These models are trained on massive datasets and leverage sophisticated architectures to mimic human-like text generation and comprehension. As these models have rapidly advanced in capabilities, traditional evaluation methods that rely on narrow, task-specific benchmarks have become increasingly inadequate for assessing their full spectrum of abilities (McIntosh et al., 2024). The current landscape of LLM evaluation is fragmented, with a proliferation of benchmarks that lack standardization and may not accurately reflect real-world application scenarios (Tikhonov & Yamshchikov, 2023). This creates challenges in comprehensively and fairly comparing different LLMs, especially as they approach or potentially surpass human-level performance on many tasks. Moreover, the rapid pace of LLM development has outstripped the evolution of evaluation methodologies, leading to a situation where benchmarks quickly become obsolete or fail to capture the capabilities of the latest models (McIntosh et al., 2024). Furthermore, as Tikhonov and Yamshchikov (2023) point out, since LLMs increasingly mimic human-like behaviors, traditional evaluation proxies such as the Turing test have become less reliable, emphasizing the need for more flexible, holistic, and interdisciplinary approaches to LLM evaluation that can keep pace with rapid advancements in the field and provide meaningful insights into these models' true capabilities and limitations. Such approaches should not only assess technical performance but also consider ethical implications, robustness, and the ability to generalize across diverse tasks and domains (McIntosh et al., 2024). To contribute to this open challenge, we designed two tasks: one focused on forecasting the emergence of AGI, requiring models to integrate interdisciplinary knowledge and address uncertainty and temporal complexity, thereby testing their capabilities beyond traditional benchmarks. Additionally, we implemented the LLM Peer Review task, where LLMs evaluate their own forecasts and those of other models based on a structured set of criteria. This dual-task approach allows us to assess both the predictive accuracy and the evaluative consistency of the models, providing a comprehensive evaluation of their capabilities in the context of AGI forecasting."}, {"title": "2.2 AGI forecasting task", "content": "AGI refers to Al systems capable of performing any intellectual task that humans can, with comparable or superior proficiency across a wide range of domains (Goertzel & Pennachin, 2006). Also termed Human- Level Machine Intelligence (HLMI) or Human-Level AI (HLAI) (e.g., Besold and Schmid 2016), AGI surpasses narrow AI, which excels at specific, predefined tasks but lacks the adaptability and generalization capabilities of human intelligence. The potential impact of AGI on society is profound and multifaceted. In science and technology, AGI could accelerate research and innovation, potentially leading to breakthroughs in areas such as medicine, clean energy, and space exploration. In economics, AGI could dramatically increase productivity and economic growth, potentially reshaping labor markets and economic structures (Hanson, 2016). However, the development of AGI also raises significant ethical and existential concerns, including the potential for rapid, uncontrolled self-improvement leading to an intelligence explosion, as well as issues of Al alignment and control (Bostrom, 2014). Given AGI far-reaching implications, forecasting its development has become a subject of significant interest and debate. Several notable studies have attempted to gauge expert opinion on AGI timelines. Baum et al. (2011) surveyed participants at an AGI conference, finding that a majority expected human- level AGI to be achieved by 2050. The study revealed a dichotomy between \"AGI optimists\" and \"AGI pessimists,\" with optimists generally expecting AGI within a few decades and pessimists projecting much longer timelines or expressing skepticism about AGI's feasibility. M\u00fcller and Bostrom (2016) conducted a global survey of Al experts, finding a wide range of opinions but a general trend towards expecting AGI within the 21st century. Their study also explored experts' views on the potential consequences of AGI development, including both positive and negative outcomes. Grace et al. (2018) surveyed a broader group of machine learning researchers, revealing a median estimate of 45 years until the achievement of high-level machine intelligence. This study also explored researchers' beliefs about the potential impacts of AGI, including economic, social, and existential risks. Zhang et al. (2022) carried out a comprehensive survey of Al and machine learning (ML) researchers regarding their views on Al advancements indicates that, on average, the respondents estimated a 50% probability of achieving human-level machine intelligence by 2060. More recently, a survey of 2,778 AI researchers provided a median forecast with a 50% probability that AI systems would achieve significant milestones by 2028 and that unaided machines would surpass human performance in all tasks by 2047. The same study estimated a 10% aggregate probability that AGI could be achieved by 2027 (Grace et al., 2024).\nRecent works have examined the use of LLMs in specialized forecasting tasks. For instance, Chang et al. (2024) and Gruver et al. (2024) have shown that LLMs can predict future values in time series data with performance comparable to traditional statistical methods. Similarly, Schoenegger et al. (2023) and Halawi et al. (2024) have explored LLMs' ability to forecast real-world events, demonstrating that in some scenarios, LLMs can match or even surpass human crowd performance. However, to the best of our knowledge, no study has yet explored the use of LLMs in forecasting AGI development. AGI forecasting requires the integration of knowledge from various fields, including computer science, cognitive science, neuroscience, and philosophy, allowing to test LLMs' ability to synthesize information across diverse domains. It involves understanding and extrapolating technological trends over extended periods, challenging LLMs' capacity for long-term and temporal reasoning. Also, the task inherently involves dealing with high levels of uncertainty, testing LLMs' ability to reason probabilistically and qualify their predictions. Crucially, unlike many traditional benchmark tasks, there's no definitive \"correct\" answer in AGI forecasting."}, {"title": "2.3 LLMs Peer Review task", "content": "Recent research has explored various approaches to leveraging LLMs for self-evaluation. These methods aim to provide scalable, cost-effective alternatives to human evaluation while maintaining high levels of accuracy and insight. Liu et al. (2023) developed G-EVAL, a framework that uses LLMs to assess the quality of generated texts through a form-filling paradigm. The process involves providing the LLM with a task introduction and evaluation criteria, after which the LLM generates a chain-of-thought (CoT) detailing the evaluation steps. The LLM then uses this CoT to evaluate the text outputs in a structured manner. G-EVAL's approach allows for more fine-grained and explainable evaluations, as the LLM not only provides scores but also rationales for its judgments. The authors found that G- EVAL, particularly when using GPT-4, achieved higher correlations with human judgments compared to previous methods, especially for open-ended tasks like dialogue generation. GPTScore (Fu et al., 2023) leverages the capabilities of LMS to assess the quality of generated text. This approach employs models like GPT- 3 to assign higher probabilities to high-quality content through multidimensional evaluation prompted by multiple queries.\nDubois et al. (2024) introduced AlpacaEval, a benchmark specifically designed for evaluating instruction-"}, {"title": "3. AGI forecasting Task", "content": null}, {"title": "3.1 LLMs", "content": "We selected 16 leading LLMs for this study, as listed in Table 1. The selection was based on the LMSYS Chatbot Arena ranking, an open and recognized platform for evaluating LLMs. LMSYS Chatbot Arena uses a crowdsourced evaluation method that has collected over"}, {"title": "3.2 AGI Forecasting task: procedure", "content": "Each LLM was presented with a detailed forecasting prompt asking them to estimate the likelihood of AGI occurring by late 2030. The prompt (see 11.1.1) included:\na definition of AGI:\n\"Artificial General Intelligence (AGI), also known as Strong AI or Full AI, refers to a type of artificial intelligence that can understand, learn, and apply intelligence across a wide range of tasks at a level comparable to human beings.\""}, {"title": "4. Analysis of LLMs forecasts", "content": "To analyze the forecasts generated by the 16 LLMs, we performed a qualitative analysis of the text to capture key themes and patterns in the LLMs' reasoning. First, the codes for analysis were defined and applied to each LLM response. Following, a thematic analysis was conducted to identify overarching themes and patterns across the LLM responses. The analysis was performed using the software MAXQDA 2020."}, {"title": "4.1 Qualitative analysis of LLMs forecasts", "content": "We first categorized the LLMs forecasts based on the probability assigned to AGI development by late 2030. The distribution of predictions shows that the majority of models (13 out of 16, or 81.2%) forecast a probability lower than 30% for an AGI event by 2030, with only 3 models (18.7%) being optimistic with predictions above 30%. Among the optimistic models, pplx-70b-online is the most confident with a 47% probability, closely followed by gpt-40-2024-05-13 at 45%, and Yi-Large-preview at 38%. In the moderate range, 6 models (37.5% of the total) predict a probability between 10% and 30%, with estimates in this group varying from 12% to 15%. The pessimistic category, which includes most models (7 out of 16, or 43.7%), forecasts a probability below 10%, with estimates ranging from 3% to 8%. The overall trend leans towards more conservative predictions, with most models anticipating a low probability of an AGI event by 2030. However, there is a notable variation in estimates, spanning from 3% to 47%, indicating a high degree of uncertainty or disagreement among the models.\nMost LLMs employed historical comparisons to contextualize AGI development by drawing parallels with previous technological milestones, to offer a reference point for understanding the complexities and uncertainties associated with developing AGI. For example, the development of the Internet was referenced by four LLMs as an example of a transformative technology that emerged over a few decades. Three LLMs cited the progress of narrow AI and machine learning to show the current state and trajectory of AI development. The advent of personal computers and smartphones were also mentioned by two LLMs as examples of technologies that significantly changed society. Other notable comparisons included the development of nuclear energy, cited by four LLMs, the invention of the microprocessor, the development of commercial flight, and the sequencing of the human genome. Some LLMs also referenced specific Al milestones to highlight the field's progression. For example, Mixtral-8x22b-Instruct-v0.1 mentioned Deep Blue (1997), Watson (2011), AlphaGo (2016), and GPT- 3 (2020) as indicators of accelerating progress in AI.\nAll LLMs discussed factors that could accelerate or hinder AGI development, to highlight the complexity of predicting AGI's timeline and the multitude of factors that can influence its progress. Among the most frequently cited inciting events, advances in machine learning algorithms and breakthroughs in computational power. Advances in hardware were also noted, as well as the developments in cognitive neuroscience. Significant investments in Al research were cited as crucial factors that can accelerate progress towards AGI. Increased funding and resources dedicated to Al research was mentioned as a factor that can lead to more research initiatives, talent acquisition, and resource availability. On the blocking side, ethical and regulatory constraints were the most frequently mentioned, underscoring the potential impact of safety, fairness, and societal concerns in slowing down or halting AGI development. Limitations in current research methodologies were pointed out by several LLMs, indicating that current approaches in Al research might not be sufficient to achieve AGI, requiring new paradigms and innovative solutions. Unforeseen technical stumbling blocks were acknowledged by 8 LLMs, highlighting the unpredictable nature of scientific and technological challenges as factors that could introduce significant delays in achieving AGI. Several LLMs cited specific trends and trajectories to define the context for predicting AGI development. For example, 4 LLMs referenced Moore's Law (which predicts the doubling of transistors on integrated circuits approximately every two years) to describe the context of technological evolution eventually leading to AGI.\nIn describing the forecasting approach, the majority of LLMs (10/16) recognized the development of AGI as a complex and ambitious goal with significant uncertainties and potential roadblocks. Frequently cited factors contributing to the uncertainty include the difficulty in predicting the pace of technological progress, potential regulatory or societal barriers, and the need for fundamental advances in our understanding of human intelligence and cognition. For instance, one LLM noted, \"Previous forecasts for AGI have varied widely, with some suggesting feasibility as early as 2030 and others predicting much later dates or even questioning the possibility altogether\" (Reka-Core-20240501). Another stated, \"Given the inherent uncertainty in predicting such a complex phenomenon, I must stress that this estimation is based on the assumption that the current base rate remains constant over time, with no major inciting or blocking events radically shifting the overall progress of AGI\" (Phi-3-Medium-4k-Instruct). These examples underscore the cautious approach taken by LLMs in their predictions, highlighting the significant challenges and uncertainties involved in developing AGI. Furthermore, about half of LLMs (7/16) consider recent analysis and predictions for AGI (such as Ray Kurzweil's prediction of 2029 for AGI), varying predictions underscore the complexity and transformative potential of AGI. For example, one LLM stated, \"Recent predictions for AGI have ranged from approximately 2030-2045 with base rates around 5-10%. Notable past forecasts such as Ray Kurzweil's predictions which suggest a timeline of 2029 for AGI can provide an insightful context\" (Phi-3- Medium-4k-Instruct). Another LLM mentioned, \"Examining expert predictions, there is a wide range of opinions reflecting the topic's inherent uncertainty\" (Mistral-Large-2402).\nSome LLMs stressed the importance of interdisciplinary considerations in predicting AGI development. For example, Yi-Large-preview emphasized the multifaceted nature of AGI development, involving advances in machine learning, hardware capabilities, energy efficiency, and interdisciplinary collaborations. GLM-4- 0520 highlighted the need for technological breakthroughs, algorithmic innovations, and new conceptual frameworks, with fields like neuroscience, psychology, and philosophy influencing AGI's trajectory."}, {"title": "4.2 Comparison of LLM-based and human experts AGI forecasts", "content": "To compare the predictions generated by LLMs with human expert forecasts, we used the results from the survey \"Thousands of AI Authors on the Future of AI\" by Grace et al., (2024). The survey, conducted in 2023, involved 2,778 researchers who had published in six top- tier Al venues, which provides a fair representation of expertise within the AI research community. The survey defined High-Level Machine Intelligence (HLMI) and asked participants to predict when it would be feasible, assuming continued scientific progress. Of the total participants, 1,714 answered the HLMI question. The survey employed both fixed-year and fixed-probability question formats to reduce potential framing biases. Each participant provided three year-probability pairs, which were used to fit a gamma distribution. These individual distributions were then aggregated by calculating the mean across all participants. This approach yielded a median probability equal to 10% of achieving high-level machine intelligence (HLMI) by 2027. The alignment in AGI probability estimates between LLM predictions of AGI by 2030 and those of human experts of AGI by 2027 (respectively, 12.25% vs. 10%) confirms previous observations that LLMs are not only capable of performing forecasting tasks but also able to produce results comparable to human predictions (Schoenegger et al., 2023; Halawi et al., 2024). However, in the context of this study, the reference to human expert forecasts is not intended to assess how closely LLM performance in the forecasting task matches or diverges from human performance, as would be the case in \u201cstandard\u201d benchmarking tasks (e.g., coding challenges, math problems, real-world science questions). In fact, such a comparison would not even be appropriate, since the prompt given to the LLMs did not coincide with the instructions provided to the experts in the Grace study. Instead, the reference to human experts serves to provide context and a useful point of comparison for understanding the reasoning and justifications produced by the LLMs."}, {"title": "5. LLM Peer-review task", "content": null}, {"title": "5.1 Peer-evaluation procedure", "content": "To further evaluate the forecasters' output, we considered using a panel of human experts, such as futurologists and professional forecasters. However, we opted to have the LLMs evaluate themselves. This approach not only allowed us to assess the models' predictive abilities but also provided insight into how they evaluate one another and their self-assessment skills. Crucially, this approach enables a comparative analysis of the LLMs' ability to both generate and assess forecasts, highlighting strengths and weaknesses in different aspects of their reasoning."}, {"title": "5.2 Scoring model", "content": "We use a single-point scoring model, where each rater evaluates the quality of a forecast independently, without direct comparison to other forecasts (Verga et al., 2024). The evaluation prompt (see 11.1.2) provides clear instructions on how the grading should be conducted, defining the characteristics of a good or poor response. Thus, ratings are based solely on the rater's judgment of what constitutes a high-quality forecast. Therefore, the j- th rater independently scores the i-th forecast, after the k- th criterion, with a score $S_{ij}^{(C_k)} \\in \\{1,2 ... 5\\}$. Those individual scores are then pooled together forming the matrix $S^{(c)} = [S(C_1)|S(C_2)| ...S(C_9)] \\in \\mathbb{R}^{16\\times153}$. The final score $s_i$ of the i-th forecaster after the panel voting needs a counting function f to be computed: $s_i = f(s_{ij}^{(C_k)})$ , k = (1..9), where f is weighting and counting."}, {"title": "5.3 Results of the peer review", "content": "Table 5 presents the scores assigned by the raters (listed horizontally) to the forecasters (listed vertically), averaged across the criteria, averages over the LLMs ensemble and the standard deviation of each rater's scores. On average, DBRX-Instruct-Preview (X15) was the most generous, assigning an average score of 4.8. In contrast, Gemini-1.5-Pro-API-0514 (X3) was the most critical, with an average score of 2.7. Further, Gemini 1.5 Pro exhibited the highest coefficient of variation in given scores, indicating significant differences in its evaluations. The standard deviation of the scores assumes the maximum (3,8) for the rater Command-R+ and the minimum (0,10) for the rater Gemma-2.\nFig. 1 reports the studentized residuals of the scores (i.e. the dimensionless ratio resulting from the division of a residual by the sample estimate of its standard deviation, as the mean and standard deviation are estimated per rater on the LLM ensemble). The distribution of the studentized residuals is consistent between the raters, as the ICC analysis in section 6.1 will explain in depth, meaning that all the raters contribute significantly to the final scoring and ranking.\nTable 6 presents the evaluation scores assigned to each LLM for its AGI forecasts for each of the nine criteria, after averaging across all raters. This is computed as: $S_i^{(C_K)} = \\frac{1}{16}\\sum_j^{16} S_{ij}^{(C_K)} \\in \\mathbb{R}^{16\\times9}$.Overall, the scores are high, with a grand mean score of 4.207. Scores range from a low of 3.500 (Gemini-1.5-pro-api on criterion 5) to a high of 4.938 (pplx-70b-online on criterion 3). \"Rich context for the AGI event\" (Criterion 3) received the highest average score of 4.52, indicating that according to LLM raters, most forecasts excelled in providing comprehensive contextual information. In contrast, \"Reporting on relevant past events and other pertinent forecasts\" (Criterion 5) had the lowest average score of 3.98, suggesting it was a common area of weakness. The standard deviation of the criterium score assumes the maximum (0,26) with Criterium 6 and the minimum (0,19) with Criterium 1.\nFig. 2 presents the studentized residuals of the same scores as in Table 6. The distribution of the studentized residuals is consistent between the criteria, as the ICC analysis in section 6.1 will show, meaning that all the criteria contribute significantly to the final scoring and ranking.\nFig. 3 displays the rankings determined by each rater, plus the final ranking of forecasters, based on the uniform weighting of the raters scores, shown on the far right. Let us focus of the Top3 forecasters and their ranks along the raters: pplx-70b is ranked as a Top3 by 9 (over 16) raters, and often falls to the lowest ranks (11, 12, 13, 15); Qwen2 is ranked Top3 by 4 raters, keeping the 9-th rank with three raters, even resulting 9 and 11; Llama-3-70b is ranked Top3 by 3 raters, keeping the 6-th rank with three raters, and resulting 9 and 10. This means that the raters have a significant diversity of evaluations, and the weight that we give to each of them can affect the final ranking.\nFig. 4 shows that pplx-70b is ranked as a Top3 by 8 criteria over 9 with just one drop to 10; Qwen2 is ranked Top3 only by 5 criteria, Llama-3-70b is ranked Top3 only by 3 criteria."}, {"title": "6. Analysis of LLM peer review: agreement and consistency", "content": null}, {"title": "6.1 Inter-rater consistency analysis", "content": "We assessed the consistency of peer review evaluations across different raters using the Intraclass Correlation Coefficient (ICC), a statistical measure that evaluates the level of consistency and agreement among raters. The ICC is a ratio of covariance to total variance, accounting for various sources of variance in the score matrix S, including the selection of forecasters and raters. Given the systematic variation in scores between raters, we employed a two-way random model to accurately represent the data. The ICC values were calculated following the definitions provided by McGraw (1996):\n- ICC(C,n) estimates the squared correlation of average scores and universe scores, representing the degree of consistency for scores that are averages of n independent ratings on randomly selected forecasts.\n- ICC(A,n) estimates the squared correlation of average scores and universe scores, including variance between raters, representing the degree of absolute agreement for scores that are averages based on n independent raters on randomly selected forecasts.\nIn our analysis, consistency is contrasted with absolute agreement when measuring correlation. In consistency, the total score variance (i.e., differences in the overall scores given by raters) is used as the denominator (McGraw, 1996). If two raters' scores can be aligned by applying an additive transformation (for instance, subtracting each rater's mean score from their individual"}, {"title": "6.2 Alternative ranking methods", "content": "In the previous section, we analyzed the consistency of peer review scores using standard ranking methods based on uniform weighting, if all raters contribute equally to the final rankings. However, given that raters can vary in their expertise and accuracy across different tasks, it is worth exploring whether alternative ranking methods- especially those incorporating external benchmarks\u2014 could yield different results. In this section, we investigate the impact of applying weights to the raters' scores based on their performance in external evaluations, such as the LLM becnhmarks. By adjusting the peer review scores using these benchmarks, we aim to assess whether the relative rankings of the forecasters change and whether the models' performance in forecasting AGI events can be linked to their broader capabilities, as measured by external evaluations. For this purpose, we selected three diverse benchmarks: LMSYS Chatbot Arena, MixEval, and AlpacaEval (updated as of July 17, 2024) whose benchmark values are in Table 9. This selection provides a diversified benchmark"}, {"title": "6.3 Analysis of LLM self-evaluation", "content": "After exploring the impact of different ranking methods in Section 6.2, we now investigate how LLMs assess their own performance (i.e., Self-Evaluation) compared to how they are evaluated by others (i.e., Hetero- Evaluation). By examining the accuracy of self- evaluations, we aim to understand whether certain models tend to overestimate or underestimate their performance. In Table 11, we compare the self- evaluations of each model with the average evaluations they received from others. DeepSeek Coder V2 Instruct and Mistral Large 2402R showed a better balance, with their self-assigned scores closely aligning with the scores given by other LLMs. In contrast, DBRX-Instruct- Preview and Mixtral-8x22b-Instruct-v0.1 displayed significant self-preference, assigning themselves scores that were 17% higher than those from others. Gemini- 1.5-pro-api-0514, the most critical in its evaluations of forecasts, demonstrated marked self-underestimation, giving itself a score 40% lower than the average it received from others. Further we define a LLM's Self- Evaluation Index (SEI) as the ratio between its self- assessment score (SES) and the average score it received from other LLMs (hetero-evaluation score - HES):\n$SEI_i = \\frac{SES_i}{HES_i} = \\frac{S_{ii}}{\\sum_{j=1, j\\neq i}^{16} S_{ij}/15}$       (3)\nThe SEI indices are presented in Table 11. Values above 1.0 indicate self-overestimation and values below 1.0 indicate self-underestimation (relative to peer evaluations). SEI values range from 0.599 (gemini-1.5- pro-api-0514) to 1.184 (DBRX-Instruct-Preview), with DeepSeek-Coder-V2-Instruct achieving a perfect balance at SEI of 1.0."}, {"title": "7. Comparing LLMs to Human Experts on AGI", "content": null}, {"title": "7.1 Comparing LLMs forecast to the human expert likelihood estimation", "content": "We now shift focus to comparing AGI likelihood estimates from LLMs with those of human Al experts, as reported in Grace et al. (2024). The aggregate expert estimate of AGI likelihood by 2027 is 10%. After an adjustment caused by our term equal to 2030, we consider this as a reference value to assess how closely LLM predictions align with human judgment. Additionally, we explore whether benchmark weighting can enhance this alignment, offering insights into the relationship between LLM performance and the reliability of AGI forecasting. It's important to clarify that here, \"reliability\" does not refer to absolute accuracy, as AGI prediction is inherently uncertain, even for human experts. In other words, the aim of our assessment method is to understand how LLMs handle uncertain predictions and evaluate them (i.e., the process), rather than how closely they match an objectively correct answer (i.e., the outcome).\nFirst, we computed the simulated scores that human experts would have assigned to the forecasters based solely on their predictions of AGI likelihood (the scoring formula is reported in appendix). The computation starts from a similarity measure and reduces it to the standard 1-5 Likert scale we used in the previous discussion. Table 13 (in the fourth column) reports the simulated scores. Notably, Mixtral-8x22b-Instruct-v0.1, GLM-4-0520 012 1207 PP, and Gemini-1.5-pro-api-0514 show the closest alignment with the adjusted human estimates. This comparative framework allows us to assess the degree of concordance between LLM-generated forecasts and those of human experts, providing an additional external validation metric for evaluating the LLM performance in the complex task of AGI prediction. It is evident that 13 LLMs out of 16 receive scores over 4, showing a good evaluation from the human expert panel."}, {"title": "7.2 Evaluation performance", "content": "Figure 5, in the fifth column, shows the expert ranking, that is based on the expert simulated scores and compares it to the other rankings shown in Figure 5 (which were based on benchmark weights). Crucially, the expert ranking shows a dramatic reshuffling of the LLMs' positions. In fact, 4 out of 5 LLMs (80%) in the top group have now changed ranking. Only one LLM, Claude-3-5- sonnet-20240620, remains in the top 5 in both the previous rankings and this new human-aligned ranking. This indicates a significant difference between how LLMs align with human experts in evaluation of AGI. A more quantitative perspective is provided by the Kendall normalized distances between the five rankings shown in Figure 5. The Arena and Uniform benchmarks have the smallest distance between the raters (1,6%), indicating that using the Arena benchmark yields similar results to applying equal weighting to the raters. However, both rankings show a substantial distance from the Expert ranking (approximately 0.7). This suggests that the Arena benchmark is unlikely to help the panel align its evaluation with expert judgment. A similar pattern emerges with the MixEval and Alpaca benchmarks. In conclusion, within our evaluation framework, performance on standard AI benchmarks does not correlate strongly with AGI predictions that match expert opinions. This underscores the uniqueness of the AGI prediction task and suggests that different skills or capabilities might be required for this specific type of forecasting compared to those measured by standard AI benchmarks."}, {"title": "7.3 Confidence weight of raters", "content": "According to Ning et al. (2024) we considered evaluating wj in equation (1) as a confidence weight for the j-th rater. As the peer-review process works in an unsupervised way", "state": "n$\\\\underset{w}{argmin}\\\\hspace{"}]}