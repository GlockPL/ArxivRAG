{"title": "Trustworthy and Responsible AI for Human-Centric Autonomous Decision-Making Systems", "authors": ["Farzaneh Dehghani", "Mahsa Dibaji", "Fahim Anzum", "Lily Dey", "Alican Basdemir", "Sayeh Bayat", "Jean-Christophe Boucher", "Steve Drew", "Sarah Elaine Eaton", "Richard Frayne", "Gouri Ginde", "Ashley Harris", "Yani Ioannou", "Catherine Lebel", "John Lysack", "Leslie Salgado Arzuaga", "Emma Stanley", "Roberto Souza", "Ronnie Souza", "Lana Wells", "Tyler Williamson", "Matthias Wilms", "Zaman Wahid", "Mark Ungrin", "Marina Gavrilova", "Mariana Bento"], "abstract": "Artificial Intelligence (AI) has paved the way for revolutionary decision-making processes, which if harnessed appropriately, can contribute to advancements in various sectors, from healthcare to economics. However, its black box nature presents significant ethical challenges related to bias and transparency. AI applications are hugely impacted by biases, presenting inconsistent and unreliable findings, leading to significant costs and consequences, highlighting and perpetuating inequalities and unequal access to resources. Hence, developing safe, reliable, ethical, and Trustworthy AI systems is essential.\nOur team of researchers working with Trustworthy and Responsible AI, part of the Transdisciplinary Scholarship Initiative within the University of Calgary, conducts research on Trustworthy and Responsible AI, including fairness, bias mitigation, reproducibility, generalization, interpretability, and authenticity. In this paper, we review and discuss the intricacies of AI biases, definitions, methods of detection and mitigation, and metrics for evaluating bias. We also discuss open challenges with regard to the trustworthiness and widespread application of AI across diverse domains of human-centric decision making, as well as guidelines to foster Responsible and Trustworthy AI models.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) represents the frontier of computer science, enabling machines to emulate human intelligence and perform tasks that were once exclusive to human capabilities (Briganti and Le Moine 2020). This rapid progression in AI, driven by Machine Learning (ML) and Deep Learning (DL) innovations, has catalyzed breakthroughs across various industries, including business, communication, healthcare, and education, among others. Utilizing state-of-the-art computational resources, the AI models are trained on extensive datasets and can be used for decision-making on unseen data. Recent advancements in AI algorithms and feature engineering techniques have played a pivotal role in transforming various human-centric fields, notably, healthcare (Esteva et al 2019), image and text generation (Epstein et al 2023), biometrics and cybersecurity (Gavrilova et al 2022), online social media opinion mining (Anzum and Gavrilova 2023), autonomous driving vehicles (Ma et al 2020), and beyond.\nDespite the impressive capabilities exhibited by recent AI-based systems, a significant challenge lies in their inherent black box nature. Due to the lack of explainability and interpretability of AI models, establishing trust among end users has become critical (von Eschenbach 2021). Therefore, to ensure trustworthiness in AI-empowered systems, it is imperative not only to improve the model's accuracy but also to incorporate explainability and interpretability into the model's architecture and decision-making process. Interpretability refers to the ability to explain or provide meaning in a way humans can understand, while explainability involves providing details or reasons to facilitate or clarify understanding of AI systems. An interpretable AI system is transparent in providing details about the relationships between the inputs and outputs, whereas an Explainable AI system provides explanation and reasons on the results and the decision made by AI tools (Arrieta et al 2020). While explainability refers to the act of providing meaning, interpretability is related to understanding, which means acknowledge the user agency and positionality in the process (Goisauf and Cano Abad\u00eda 2022).\nAI algorithms have raised issues related to bias, fairness, privacy, and safety, especially when human data is being used, avoiding perpetuating stereotypes and reinforcing inequalities (Anzum et al 2022). Therefore, to develop an AI system that can be trusted by its end users and the communities it impacts, these factors should be considered fundamental. Research shows that explainability can ensure trustworthiness in AI-based models' decisions by visualizing the factors affecting the result, leading to fair and ethical analysis of the model (Schoenherr et al 2023).\nPreserving privacy when using human data is one of the most significant challenges in Trustworthy AI (Stahl and Wright 2018). This can convince users that a model respects the privacy of data owners and does not reveal any information related to data. As for safety, not all AI-empowered systems with high performance can be considered safe, especially for human-subject involvement (Rasheed et al 2022).\nHere, the key research question is \"what are the core concepts, challenges, and guidelines that underlie the principles of Trustworthy and Responsible AI within academic and industry contexts?\". To answer this question, our paper makes the following contributions:\n1. Determining and discussing several foundational principles in the landscape of AI including Trustworthy AI, Responsible AI, Explainable AI, and Fairness in AI.\n2. Exploring various principles regarding AI Governance, Regulatory Compliance, reproducibility, reliability, and communication.\n3. Presenting a comprehensive overview of bias in terms of source and types, various techniques to determine and mitigate bias, and fairness evaluation metrics.\n4. Identifying the potential research gaps, challenges, and opportunities in the context of Trustworthy and Responsible AI by synthesizing and standardizing data from various applied human-centric domains, and providing our perspective on this context as a transdisciplinary team of researchers from the University of Calgary.\n5. Discussing open challenges and limitations in the application of AI across diverse domains and proposing comprehensive guidelines to develop Trustworthy, fair, and reliable AI models."}, {"title": "2 Trustworthy and Responsible AI Definition", "content": "Despite extensive research on Trustworthy AI, there is a lack in definition and standardization, particularly across various disciplines where AI is applied. Therefore, our team, consisting of researchers from diverse fields such as communication, media and film, philosophy, social-behavioral sciences, public policy, political science, foreign policy, ethics, education, radiology, pediatrics, health informatics, veterinary medicine, biomedical research, computer science, and electrical and software engineering, provides a comprehensive perspective on Trustworthy and Responsible \u0391\u0399.\nIn the landscape of intelligent human-centric systems, several foundational principles stand out as guides for research and development. Prominent among these is Trustworthy AI. Trustworthy decision-making is defined as the ability of an intelligent computer system to perform a real-time task repeatedly, reliably, and dependably in complex real-world conditions (Lyu et al 2021). This principle emphasizes that AI should be perceived as reliable by all its users, from individual consumers to entire organizations and broader society. Trustworthiness in AI is related to compliance with laws or robust system performance, also ensuring that AI adheres ethical guidelines (D\u00edaz-Rodr\u00edguez et al 2023). A key component of trustworthiness is transparency (Li et al 2023). The decisions made by AI systems, the utilized data, and the processes governing them should be clear and interpretable to all stakeholders involved. Such clarity ensures that AI does not remain an enigmatic \"black box\" and becomes an entity whose actions and decisions are understandable and, more importantly, accountable.\nFundamental principles of Trustworthy AI are beneficence, non-maleficence, autonomy, justice, and explicability. Beneficence refers to the development and deployment of AI systems that promote human and environmental well-being while respecting human rights. Non-maleficence indicates the development and deployment of AI systems that are harmless to humans. In autonomy, the aim is to trade-off decisions made by human against decisions made by machine to ensure integrity and reliability of AI systems. Justice aims to ensure that AI systems are based on ethical principles. Explicability refers to Explainable and Responsible AI (Thiebes et al 2021).\nComplementary to trustworthiness is the notion of Responsible AI. While they share similarities, Responsible AI has a more tactical orientation (Arrieta et al 2020). The Responsible AI framework acknowledges not only AI's transformative potential, but mainly its socio-technical character (Dignum 2023). Issues like biases, unforeseen outcomes, and a lack of clarity about an AI system's operations can foster distrust and limit the technology's widespread. Therefore, Responsible AI addresses these concerns by using AI in ways that minimize potential harm to individuals, society, and the environment. This involves proactive bias reduction, enhancing system interpretability and explainability, and ensuring fairness, accountability, and privacy are always at the forefront (Cheng et al 2021).\nAnother significant notion in the AI landscape is Explainable AI (XAI). Fundamentally, from a computer science perspective, explainability is primarily about AI model transparency and comprehensible decision-making (Confalonieri et al 2021). However, this technical view is expanded from the broader academic community, especially social science scholars. They advocate for a broader conception of explainability, one that goes beyond mere technical intricacies (Wang et al 2019). This broader viewpoint underscores the importance of post-hoc interpretable explanations, which provide clarity on AI decisions (Gianchandani et al 2023). It also highlights the need for including diverse stakeholders, from AI specialists to potential end-users, right from AI's developmental stages (Clement et al 2023). This inclusive approach aims to make AI accessible, understandable, and trusted resource for all. To ensure model explainability and transparency, self-explainable models, being able to visually explain their decisions via attribution maps and counterfactuals, can be developed (Wilms et al 2022). However, Explainable AI models do not release details on the decision-making process of models, which makes them unreliable and misleading. Thus, developing inherently interpretable models that are able to provide their own explanations is paramount (Rudin 2019).\nLastly, Fairness in AI is a principle that acts as a safeguard against discriminatory practices (Madaio et al 2022). An AI system, regardless of its complexities or application, should be devoid of unfair biases. It should not favor or marginalize any group based on social, demographic, or behavioral attributes. Thus, fairness is not just passively avoiding discrimination; it is an active commitment. Even if input data carries biases, the AI system's outputs should remain impartial, ensuring equitable algorithmic decisions for all (Dibaji et al 2023). An experience developed by Fujitso in japan showed the challenge of assessing fairness in different contexts and scenarios, due to the cultural and societal differences. Researchers demonstrated the benefits of embracing socio-technical solutions to mitigate bias in the system outcomes (Research and Development 2023).\nAnother key concern relates to personal privacy (Stahl and Wright 2018). If not properly regulated and without proper consent, AI may pose threats to personal freedom and human rights. Moreover, given Al's ability to process massive data volumes, preserving and respecting the richness of diverse cultures becomes imperative.\nThe advent of generative AI (Muller et al 2022) introduces a new set of challenges. Such AI models, trained on extensive online data sets, can produce content that can blur the lines between reality and fiction (Jo 2023), exacerbating disinformation. The old digital belief, \"seeing is believing\", is rapidly being rewritten. Companies, researchers, and developers have a collective responsibility to implement stringent safeguards, transparently disclose the origins of AI-generated content, ensuring that the digital ecosystem remains a source of reliable information.\nShifting focus into specific domains, healthcare presents its own set of unique challenges. Preserving patient data sanctity is paramount by removing individual patient, known as anonymization. While AI models in healthcare revolutionize diagnostics, treatments, and patient care, they require vast amounts of diverse data for effective training (Tom et al 2020). However, the sharing of such data between institutions often hits a wall due to stringent privacy concerns. Federated learning, where data remains in its original location, is a possible solution to this challenge.\nThese challenges require the AI deployment respecting the individual, society, and the environment. Such an approach ensures that as we advance into the future, we proceed with caution, responsibility, and respect for ethical considerations."}, {"title": "3 Governance for Human-Centric Intelligence Systems", "content": "AI Governance emerges as a multifaceted discipline, crucial for ensuring that AI systems operate within a well-defined and ethical framework during the whole AI-life cycle. Governance is not just about overseeing AI models; it extends to the data use for model development. Effective data governance mandates that data acquisition, storage, and utilization presents ethical and integrity standards (Janssen et al 2020). As AI systems grow in complexity and reach, mechanisms like federated learning gain prominence, advocating for data sharing without sacrificing privacy. This approach encompasses self-regulatory practices, including ethics and impact assessments at early design stages, and is complemented by practices that focus on data reuse and ethics, establishing guidelines on data re-use.\nComplementing governance is the pivotal area of Data Security. AI is only as good as the data it is fed. But this data, often sensitive and personal, needs to be shielded from unauthorized access and potential breaches. AI systems should not only prioritize users' privacy rights but should stand as Data Security fortresses. Ethical data acquisition is the starting point, ensuring that every piece of data is obtained with informed consent. Developers and operators of AI systems bear the responsibility of ethically managing this data throughout its lifecycle (Haakman et al 2021). Their accountability extends beyond just Data Security, encompassing the outcomes that AI models produce. Regardless of intentions, developers and operators must take responsibility for the repercussions-be they beneficial or detrimental that arise from AI's actions.\nThe narrative of governance and security is also intrinsically tied to Regulatory Compliance. As AI continues to influence every facet of our lives, the need to translate external AI regulations into actionable policies intensifies. However, Regulatory Compliance is a dynamic process. Ensuring that AI systems adhere to regulations requires meticulous design, rigorous testing, and ongoing evaluation. Beyond technical compliance, there exists an urgent need for a comprehensive legal and regulatory framework-one that not only ensures AI systems function within the legal boundaries but also upholds the rights of individuals. This framework embodies the principles of accountability and responsibility, ensuring AI's evolution harmonizes with societal norms and values.\nIn our exploration of AI, reproducibility, reliability, and communication principles emerge as foundational pillars. These elements converge to shape the broader narrative of Trustworthy AI systems. Reproducibility represents the scientific rigor essential for AI (Gundersen and Kjensmo 2018). Reproducible methods and generalizable results becomes paramount as AI's influence expands across diverse sectors, from healthcare to finance. The challenge is highlighed with AI, where complex algorithms can obfuscate the path from primary evidence to knowledge. There is a growing temptation to rely on secondary sources or prevailing consensus. However, the true essence of scientific inquiry lies in the ability to trace information back to its primary sources assessing its reliability. Introducing an \"AI black box\", potentially limits individual researchers' ability to challenge and rectify errors.\nReliability in AI emphasizes consistent performance and resilience against potential adversarial threats. AI systems should consistently deliver reliable results across varied scenarios. In essence, reliability ensures that AI remains a trustworthy tool. The significance of communication in AI cannot be overstated. It is not just about conveying information but fostering genuine understanding between AI systems and their users. As these systems grow in complexity, the duty falls upon developers, researchers, and communicators to demystify AI processes. Transparency and explainability are paramount, not just as technical necessities but as essential communication tools. This perspective on communication underscores the importance of transparency, user engagement, and fostering AI literacy. It is pivotal to understand not only how AI operates but also its origin, applications, and the logic behind its decisions."}, {"title": "4 Biases", "content": "AI bias refers to systematic prejudiced results produced by algorithms, which can lead to incorrect predictions. If harnessed appropriately, AI can satisfy expectations of accurate and fair decision-making. However, AI-empowered systems are vulnerable to biases that make their decisions unfair. Thus, focusing on ethical scrutiny of these systems is on the increase (Aquino et al 2023).\nDiscrimination refers to a biased treatment of a certain group of people according to their age, gender, skin color, race, culture, economic conditions, etc (Calmon et al 2017). While discrimination and bias are considered sources of unfairness, the former results from human prejudice and stereotyping and the latter is due to data collection, sampling, and measurement. Most AI systems are data-driven, thus, data plays a significant role in the functionality of these systems. Machine learning algorithms used for decision-making inherit any bias or discrimination in the training data. As a result, existing bias in data can produce biased outcomes that are fed into real-world systems and can affect the end user's decisions (Anzum et al 2022). In some cases, the algorithmic choice of design can also lead to biased outcomes, even if the data is not biased itself (Mehrabi et al 2021).\nIt is necessary to take some important discrimination related to social, political, and economic aspects into account. For example, unequal treatment of patients and access to the healthcare system as a result of political disagreement and economic inequalities is considered discrimination. People with low socioeconomic status and racial minorities are less likely to receive adequate medical care. Thus, the missing data information from these groups affects the fairness of AI systems and consequently, the healthcare system (Lekadir et al 2021). Social determinants of health refer to a specific group of social and economic factors within the broader determinants of health. These relate to an individual's place in society, such as income, education, or employment. Experiences of discrimination, racism, and historical trauma are important social determinants of health for certain groups such as Indigenous Peoples, LGBTQ, and Black Canadians. Minority populations faced with prolonged marginalization should receive more attention to ensure compensation for discrimination they experienced (Owens and Walker 2020).\nThe heart-failure risk score that categorizes black patients as being in need of less care and cancer detection algorithms that perform poorly for people of color are two examples of racial discrimination and inequitable health practices for people of color (Owens and Walker 2020). To ensure unbiased medical practices toward patients of color, justice should be a fundamental principle in clinical and research ethics. This can be achieved by systematic education of health providers and applying anti-racist standards for the development and analysis of research (Owens and Walker 2020).\nThe discussion on the socio-material character of data and how it reflects discrimination and historical inequalities is a critical conversation in social studies of science and technology. As we have seen in examples of AI generators trained with data over-represented for a determined population, it amplifies existing inequalities. The example of the images of doctors and nurses generated by MidJourney 1 is one of the most well-known cases. A critical question is how the propagation of images that reflect longstanding inequalities and discrimination reinforces users' bias concerning a determined issue.\nThere are three categories of biases, namely, data to algorithm, algorithm to user, and user to data that could cause a user feedback loop (Mehrabi et al 2021). Firstly, 'data to algorithm' bias occurs when the data used to train AI systems is not representative or contains inherent prejudices. This can lead to AI algorithms that perpetuate and even amplify these biases in their outcomes. Secondly, there is a potential for 'algorithm to user' bias, where the decisions or perceptions of users are influenced by the biased outputs of AI algorithms. This interaction can subtly shape user behavior and judgments. Lastly, 'user to data' bias reflects the impact of human biases on user generated data, which, when used to train AI systems, can introduce or reinforce biases.\nData to Algorithm In ML algorithms, biases in data can be responsible for biased algorithmic outcomes. Methods of choosing, utilizing, and measuring certain features of a target group cause measurement bias. Cognitive bias, a subgroup of measurement bias, refers to the human brain's tendency to simplify information processing according to personal experience and preferences (Mehrabi et al 2021). For example, if the database is generated by developers, engineers, or medical professionals in isolation, or by a group of people with a certain ethnicity or social background, chances are the database is unintentionally affected by their own biases (Ricci Lara et al 2022). Omitting some variables from the model can also lead to a bias called omitted variable bias. Biases due to missing data information are usually more common within a specific population (prevalence of a specific population). If the data used to train the AI model does not adequately represent the diversity of human behavior, the system may be biased toward certain behaviors or groups. For instance, if the training data mainly consists of people from a certain geographical region, the model might not perform well or may misinterpret behaviors from people in other regions (Anzum et al 2022).\nLack of diversity and proper representation of the target population in the training database is a type of data-driven bias (Anzum et al 2022). Differences between demographics of the target population and the database, such as age, gender, ethnicity, etc., can lead to aggregation bias, ensuring that the model is not well-suited for all groups of the population (Anzum et al 2022). For example, morbidity differences vary among diabetes patients with different ethnicities and genders. However, few available datasets contain clinical information and records such as age, race/ethnicity, gender, etc. Selection bias occurs if the sample selected for a study is not representative of the entire population (Yu and Eng 2020). For example, if a study on social behavior only includes participants from a certain socio-economic background, it might not accurately reflect the behaviors of those from different backgrounds. Selection bias can take many different forms: Coverage bias, wherein data is not selected in a representative fashion; Non-response bias (or participation bias), wherein data is unrepresentative due to participation gaps in the data-collection process (Berg 2005); Sampling bias, wherein proper randomization is not used during data collection (Jeong et al 2018).\nData heterogeneity and target class imbalance can influence fairness and model performance in AI systems. Class imbalance refers to the unequal distribution of classes in the training dataset. Data heterogeneity can occur due to different equipment (vendor, model, etc.) and data acquisition protocols and the underlying distribution of subjects of a certain ethnicity, gender, or age (Dinsdale et al 2021). Collecting data is a sensitive issue as it is susceptible to several biases (Acosta et al 2022). Machine learning models are vulnerable to intrinsic characteristics of the data used and/or the algorithm employed. Any biases in data leads to biased outcome and spurious correlations that a specific model could exploit (Ricci Lara et al 2022). Combining data from multiple sources or using multi-modal data, such as imaging data and electronic health records (EHR), can exacerbate the problem of bias (Acosta et al 2022).\nOne significant solution to mitigate bias in the design, validation, and deployment of AI systems is to ensure diversity during data collection. This can be satisfied by enhancing transparency of datasets and providing information about patient demographics and baseline characteristics (Vokinger et al 2021). Data in healthcare typically includes patient demographics such as age, sex or gender, skin tone or race/ethnicity, and comorbidities. However some information might be absent due to privacy concern (Ricci Lara et al 2022). Having a global perspective in the design of AI tools and applying meticulous approach to analyze the performance of these models based on population subgroups, including age, sex, ethnicity, geography, and sociodemographic status is of paramount importance (Goisauf and Cano Abad\u00eda 2022).\nData labeling is a crucial aspect of the socio-technical system in which artifacts are designed and utilized. Most current AI models require human intervention for manually labeling data, a process that is often costly, complex, and time-consuming, especially when dealing with large volumes of data. One potential solution is to use semi-supervised or unsupervised learning algorithms. These methods allow the model to assign labels to unlabeled data, with human input sought only when the model's confidence is low. This approach can streamline the labeling process (Yakimovich et al 2021). However, challenges remain, particularly regarding the potential for bias introduced by human involvement. People responsible for labeling are subject to organizational constraints and biased behavior, which may influence their decisions (Baker and Hawn 2021). To mitigate this, clear, standardized methods for data collection and storage are necessary. Additionally, ensuring the fairness of AI systems involves considering standardized metadata and key variables like age, sex/gender, ethnicity, and geography during the data collection and preparation stages. These measures are essential for the integrity and effectiveness of AI models (Trocin et al 2021).\nAlgorithm to User Algorithms affect user's behavior, thus, any bias in algorithms can lead to bias in the behavior of users. Algorithms have their own implicit biases even disregarding biases in data. Algorithmic bias refers to biases in the algorithms used to interpret data (Blanzeisky and Cunningham 2021). If the algorithm is not properly designed or trained, it may incorrectly categorize or interpret certain behaviors, leading to unfair outcomes. For example, using DL models can augment original data bias due to model design (architecture, loss function, optimizer, etc.). Changes in population, cultural values, and societal knowledge can lead to the emergent bias. By using inappropriate benchmarks for the evaluation of an AI system, bias can arise during the evaluation process. For example, using benchmarks biased toward skin color or gender in facial recognition applications can create evaluation bias. Popularity bias occurs because popular objects appear more in public, thus they tend to be exposed more (Mehrabi et al 2021).\nUser to Data Many of databases are generated by human, therefore, any inherent biases in the behavior of users might introduce bias into data sources. Further, due to the impact of algorithms on the behavior of users, algorithmic biases can result in data bias. In cases where statistics, demographics, representatives, and user characteristics in end users are different from those of the original target population, population bias occurs. Demographics include age, sex, gender, race, diverse genotypes, ethnicity/genetics, and appearance. Self-selection bias, which is a subtype of the selection bias, occurs when subjects of research select themselves. Behavioral bias refers to the different behavior of users across different platforms or datasets. The differences in behaviors over time is a temporal bias (Mehrabi et al 2021).\nHere, other sorts of bias, including visible minority bias, research bias, and group attribution bias are introduced. Exclusion of patients with rare diseases and disabilities, such as rare genetic variance, from training data can affect the performance of A\u0399 systems. This is due to sampling bias and lack of generalizability. Neglecting patients with rare diseases or disabilities from research on the application of AI in disease diagnosis can have significant negative implications for patients and AI developers and affect the trustworthiness of AI in clinical settings (Hasani et al 2022).\nRacial minority groups suffer from increased risk of diagnostic error, resulting from unequal access to healthcare and disparities in the quality or delivery of diagnostic systems. Moreover, in the clinical context, algorithmic bias in AI systems has worsened the existing inequity in healthcare and as a result, under-represented and marginalized groups are disadvantaged (Aquino et al 2023). The lack of research and available data on marginalized (under-served) communities such as intersex, transgenders, and LGBTQ2S+ have raised bias for this minority population. Few studies and lack of enough data from them, due to narrow and binary background assumptions regrading sex and gender, has worsened healthcare inequity. As another example, effects of sex and gender dimensions on health and diseases have been overlooked by AI developers. Indeed, gender/sex imbalanced datasets contribute to bias in model performance and disease diagnosis. Thus, it is recommended to not only include under-served communities in research to mitigate bias but also to introduce desirable bias to counteract the effects of undesirable bias and discrimination (Goisauf and Cano Abad\u00eda 2022).\nAnother example in this context is the application of biometric systems trained on datasets mainly from one culture. The aim of biometric systems is to establish or verify demographic attributes, such as age, race, and gender.\nOne of the most significant topics in this context is research biases. In low-income countries, lack of research funding prevents conducting research on many health problems which raises bias against some ethnicities and increases health inequity. Implicit bias occurs when assumptions are made based on one's own mental models and personal experiences that do not necessarily apply more generally. A common form of implicit bias is confirmation bias, where model builders and researchers consciously or unconsciously seek data or interpret results in ways that affirm preexisting beliefs. In some cases, a model builder may actually keep training a model until it produces a result that aligns with their original hypothesis; this is called the experimenter's bias. Confirmation bias leads to biased conclusions and increases bias in research. By hiring a multidisciplinary team of experts and stakeholders, including AI developers, medical professionals, patients, and social scientists, and applying their perspectives to the design, implementation, and testing of AI algorithms, one can improve fairness in research.\nOne very important component relevant to the research bias and confirmation bias, particularly in the health space, is how to decide what is \"authoritative\" - the basis for interpreting the often conflicting and always noisy information in the literature. For example, there is a tendency to rely on the Evidence-Based Medicine (EBM) structure and evidence ranking criteria when assessing the medical literature. The EBM structure considers applying contemporary scientific evidences in the decision-making and treatment planning of each individual patient (Bluhm 2005). EBM is often presented in very definite terms, but ultimately it is a useful heuristic to assist in decision-making by non-specialists (Solomon 2011). EBM has some limitations due to bias in the reporting of clinical trials, as a result of subject selection (Rawlins 2008), model performance, and analysis of results, as well as journals' reluctance to publish negative results (Sheridan and Julian 2016). It is quite likely that AI researchers collaborating with clinical experts, particularly if no specialists with basic research training are involved, might wind up adopting EBM approaches without consideration of the assumptions embedded in them. Any system trained on the basis of this approach risks setting in stone of biases, in ways that may be completely opaque to the end user (Greenhalgh et al 2014).\nAlthough research retraction should be managed and resolved immediately, the process takes years. This will propose challenges for EBM and AI, especially in medicine. Indeed, even when a paper is retracted, flawed data used for training and validation will exist on the Internet for years, affecting workflows, analysis, and model performance of AI systems. Notwithstanding the significance of online data for advancement in AI-based systems in various fields, certainty of the quality, reliability, and annotation of data should be taken seriously to settle the issue of retraction in research (Holzinger et al 2022).\nGroup attribution bias is a tendency to generalize what is true of individuals to an entire group to which they belong. Two key manifestations of this bias are In-group bias and Out-group homogeneity bias. In-group bias refers to a preference for members of a group to which they also belong, or for characteristics that they also share. Out-group homogeneity bias refers to a tendency to stereotype individual members of a group to which they do not belong or to see their characteristics as more uniform (B\u00f6hm et al 2020).\nThe traditional philosophical literature on AI focuses so much on the possibility of AI as a genuine form of consciousness and moral agency. This is why a lot of philosophers have been polarized about the nature of artificial consciousness. However, there has been a shift in the literature in the last 10-15 years. These days, philosophers of AI work on more practical and actionable issues about the ethical, social, and political issues of AI."}, {"title": "4.1 Strategies to Detect Biases", "content": "Although most effort (particularly at the bench) is focused on prospectively avoiding biases (some studies are done on tissue from both male and female donors", "concepts": "fairness through awareness and fairness through unawareness. Fairness through awareness indicates that individuals with similar attributes, including sensitive attributes, should be treated and classified similarly. In this method, a distance metric, defining similarity between individuals, is considered as the source of awareness. In fairness through unawareness, fairness can be obtained by eliminating sensitive attributes during the decision-making process. However, this technique may represent indirect bias when there is a correlation between the sensitive attribute and the remaining ones used for predicting the outcomes. Rationality-based fairness comprises statistical-based fairness, such as demographic parity and equal odds, and causality-based fairness, such as counterfactual fairness (Wang et al 2022).\nDetecting data bias and algorithmic bias is a sensitive issue as it can affect the trustworthiness of AI systems. There exist different methods to detect bias in data and algorithms. Disparate Impact Analysis is a quantitative method used to detect bias in AI systems (Zafar et al 2017). It measures how the system's outcomes differ across different demographic groups. For instance, if an AI system makes accurate predictions for one group but not for others, there may be a bias in the system. Bias Audit is a comprehensive method for the evaluation of the AI system using various metrics. It can involve reviewing the data used to train the model, examining the algorithm's decision-making processes, and testing the system's outcomes with different input data. Counterfactual Analysis involves changing the features of the data that represent protected characteristics (like race, gender, etc.) and assessing changes in the AI system's output. Significant changes could be an indication of bias. (Mothilal et al 2020).\nAgarwal et al. (Agarwal et al 2018) proposed a test generation technique that detects all combinations of protected and non-protected attributes in the system that can cause bias. Srivastava and Rossi (Srivastava and Rossi 2018) proposed a two-step bias detection approach, wherein a 3-level scale bias rating is developed to identify whether an AI system is unbiased, data-sensitive biased, or biased. To detect statistical and causal discrimination at the individual level, Black et al. (Black et al 2020) proposed a fairness testing approach called the Flip Test.\nResearchers have developed fairness toolkits in order to ease bias detection and mitigation. Salerio et al. (Saleiro et al 2018) proposed a bias detection toolkit to identify and correct data bias before training the model. There exist some toolkits to address bias detection and bias mitigation as well. Bantilan et al. (Bantilan 2018) proposed a toolkit named Themis-ML to detect and mitigate bias. This technique provides fairness metrics, such as mean difference, for bias detection, and relabeling and additive counterfactually fair estimator for bias mitigation. Bellamy et al. (Bellamy Rachel et al 2019) introduced an extensible toolkit called AI Fairness 360 to detect, understand, and mitigate algorithmic bias. The AI Fairness 360 is a comprehensive toolkit that brings together a thorough set of bias metrics for bias detection, bias algorithms, and a unique extensible metric explanation facility to provide end users with the meaning of bias detection results.\nResearchers focusing on Trustworthy AI have offered several methods to address bias and make AI systems fair. Bias mitigation techniques are categorized in three main classes, namely, Pre-processing, In-processing, and Post-processing (Mehrabi et al 2021). The Pre-processing method deals with modifying the training data, in case it is allowed by the algorithm, to remove any bias and discrimination, assuring the data is unbiased, mitigating over- or under-representation of any specific population. In In-processing techniques, by changing and modifying ML algorithms during the training process, bias can be removed. Post-processing algorithms can be applied to the model predictions to remove bias (Mehrabi et al 2021).\nSeveral Pre-processing algorithms have been proposed by researchers. Feldman et al. (Feldman et al 2015) proposed a technique that hides protected attributes from the training dataset while preserving the data properties. Calmon et al. (Calmon et al 2017) proposed a probabilistic framework based on an optimization formula to transform data in order to reduce algorithmic discrimination. Samadi et al. (Samadi et al 2018) proposed a linear dimensionality reduction technique to reduce bias in the training data. Kamiran and Calders (Kamiran and Calders 2012) proposed three data Pre-processing techniques to remove bias from the training data, namely, Massaging technique"}]}