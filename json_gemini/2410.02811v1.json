{"title": "SAC-KG: Exploiting Large Language Models as Skilled Automatic Constructors for Domain Knowledge Graphs", "authors": ["Hanzhu Chen", "Xu Shen", "Qitan Lv", "Jie Wang", "Xiaoqi Ni", "Jieping Ye"], "abstract": "Knowledge graphs (KGs) play a pivotal role in knowledge-intensive tasks across specialized domains, where the acquisition of precise and dependable knowledge is crucial. However, existing KG construction methods heavily rely on human intervention to attain qualified KGs, which severely hinders the practical applicability in real-world scenarios. To address this challenge, we propose a general KG construction framework, named SAC-KG, to exploit large language models (LLMs) as Skilled Automatic Constructors for domain Knowledge Graph. SAC-KG effectively involves LLMs as domain experts to generate specialized and precise multi-level KGs. Specifically, SAC-KG consists of three components: Generator, Verifier, and Pruner. For a given entity, Generator produces its relations and tails from raw domain corpora, to construct a specialized single-level KG. Verifier and Pruner then work together to ensure precision by correcting generation errors and determining whether newly produced tails require further iteration for the next-level KG. Experiments demonstrate that SAC-KG automatically constructs a domain KG at the scale of over one million nodes and achieves a precision of 89.32%, leading to a superior performance with over 20% increase in precision rate compared to existing state-of-the-art methods for the KG construction task.", "sections": [{"title": "Introduction", "content": "Knowledge graphs (KGs) are a collection of factual triples, which represent human knowledge in a structured way, i.e., (head entity, relation, tail entity). In recent years, KGs have been successfully applied in various domains, including medical science (Santos et al., 2022), biology (Zhang et al., 2022), and social networks (Qiu et al., 2018). However, constructing domain KG requires extensive expert knowledge and human intervention, which severely restricts the practical implementation of domain KG construction.\nTo address this challenge, extensive research efforts have been devoted to the KG construction task (Angeli et al., 2015; Etzioni et al., 2008a). Canonical KG construction methods mainly focus on learning logical rules based on semantic patterns. Rule-based methods extract subject-predicate-object triples by utilizing lexical and semantic role labels (Zhan and Zhao, 2020). Recently, some large language models (LLMs)-based methods have emerged as a new trend and achieved superior performances than rule-based methods (Wang et al., 2021; Han et al., 2023). LLM-based methods extract triples from raw corpora by harnessing the prior knowledge stored within the LLM. Extensive works demonstrate that LLM-based methods are more creative (Swanson et al., 2021) and more human-understandable (Chefer et al., 2021).\nAlbeit with multiple benefits of the LLM-based methods, they confront two significant challenges that severely hinder their performance and deployment. First, there is contextual noise in input. Existing LLM-based methods extract triples directly from the raw context. The raw context includes a substantial amount of domain-irrelevant information, which may potentially distract the LLM and consequently degrade its performance (Shi et al., 2023; Kumar et al., 2021). Second, there is knowledge hallucination in output. Knowledge hallucination is that the LLM may generate content that is nonsensical or unfaithful to the provided source content (Zhang et al., 2023; Ji et al., 2023).\nRegarding the domain KG construction, the LLM might generate certain irrelevant or incorrect triples due to the contextual noise and knowledge hallucination. Moreover, these incorrect triples may further propagate their errors to the next iteration,"}, {"title": "Related Works", "content": "Open Information Extraction. Open information extraction (OIE) facilitates domain-independent discovery of relational facts from large corpora (Zhou et al., 2022). TextRunner (Etzioni et al., 2008b) is the first OIE model, which merges tuples with identical entities and normalizes relationships based on predefined rules. Following TextRunner, Stanford OIE (Angeli et al., 2015), a popular method for extracting general knowledge from texts, proposes an effective and novel approach to open information extraction, utilizing a classifier to extract self-contained clauses and natural logic inference to determine specific arguments. OIE6 proposes (Kolluru et al., 2020) a novel iterative grid labeling architecture to further improve the extraction quality. More recently, some methods employ LLMs to generate triples directly from input context (Wang et al., 2020; Cohen et al., 2023). Deepex (Wang et al., 2021) leverages the attention matrix of a finetuned pretrained language model to extract triples. PIVE (Han et al., 2023) prompts the LLM and complements additional triples iteratively. However, existing LLM-based methods suffer from both contextual noise and knowledge hallucination to generate high-qualified triples.\nIn-context Learning. In-context learning (ICL), where LLMs make predictions only based on contexts with a few examples, has become a new paradigm for natural language processing (Liu et al., 2021). With the scaling of both model size and training corpora size (Brown et al., 2020), LLMs demonstrate the ability of learning from a few prompts that contain some training examples (Dong et al., 2022a; Kojima et al., 2022). Different from supervised learning requiring a training stage that uses backward gradients to update model parameters, ICL does not need parameter update and directly performs predictions. ICL aims to learn from analogy, which directly LLMs to make predictions (Dong et al., 2022b) with these examples. By concatenating both context and prompt, LLMs learn patterns hidden in examples and perform well on downstream tasks (Kojima et al., 2022)."}, {"title": "Method", "content": "We develop a general framework, named SAC-KG, to exploit LLMs (see Appendix A for related works) as skilled automatic constructors for domain KGs. Given domain corpora, the overall task is to extract triples with automation, precision, and controllability. SAC-KG organically integrates generator, verifier, and pruner in a unified framework to perform KG construction. An overview of SAC-KG is shown in Figure 2."}, {"title": "Generator", "content": "For a specified entity, which is typically a domain name or a randomly selected set of nouns within that domain, Generator employs a domain corpora retriever to retrieve the most relevant context from raw domain corpora and an open knowledge retriever to retrieve the most relevant triples from an open-source encyclopedic KG, DBpedia (Xu et al., 2017). Generator adopts in-context learning, rendering it parameter-free, unsupervised, and fully automatic. Retrievers also contribute to ensuring the quality of generated triples."}, {"title": "Domain corpora Retriever", "content": "LLMs are frequently constrained by substantial knowledge hallucinations, where the contents produced by LLMs often diverge from factual knowledge (Dziri et al., 2022; Shuster et al., 2021). The hallucinations may potentially impact the reliability and practical applications of the constructed domain KG. To facilitate accurate knowledge augmentation for the LLM, we propose a domain corpora retriever. For a given entity, it initially segments the domain corpora into sentences and then ranks the relevant sentences based on the frequency of occurrence of that entity. Then, these sentences are concatenated into a text list. Finally, we rank them in descending order of relevance to the given entity, and concatenate them into a fixed-length text as input to the LLM."}, {"title": "Open KG Retriever", "content": "When the input consists solely of domain-specific corpora and straightforward instructions, the output generated by large language models is often challenging to control and may even exhibit incorrect triple formats. To address this issue, we propose an open KG retriever, which adopts the in-context learning (Shin et al., 2022) and retrieves the most related triples associated with the entity from DBpedia (Xu et al., 2017) as examples. These examples encourage the model to generate content in the correct format, which enhances the controllability. We present our retrieval strategy as follows:\n(i) For entities presented in the open-source KG, we provide related triples wherein the entity serves as the head entity, offering up to 10 cases as examples.\n(ii) For entities not presented in the open-source KG, we tokenize them and retrieve the most related set of triples. For instance, given the entity \"micropropagation\", if it is not found within the open-source KG, it will be tokenized into two subentities, \u201cmicro\u201d and \"propagation\", to perform a subsequent retrieval from the open KG again.\n(iii) For entities that remain unmatched even after tokenization, we randomly select ten triples in the KG as prompts.\nWe then concatenate the related context, the triple prompts, and corresponding instructions as input to the LLM and obtain the extracted triples as output for the generator."}, {"title": "Verifier", "content": "While the generator contributes to enhancing the output quality of the LLM, errors in generated triples still exist. To further enhance the quality of the final generated domain KG, we introduce verifier, which is responsible for identifying and filtering out erroneous triples generated by the LLM. Verifier is rule-based and parameter-free, enabling efficient execute error detection and correction. Specifically, the verifier consists of two steps: error detection step and error correction step.\nFor error correction, we use existing criteria mined from open KGs within RuleHub (Ahmadi et al., 2020) to identify errors and output error types. The workflow is as follows.\n(i) Quantity check. If the number of triples is less than the threshold (default is 3), it will be categorized as \u201cQuantity insufficient\u201d.\n(ii) Format Check. If the triple does not conform to the example format, it will be categorized as \"Format error\". If head entity does not match the predefined entity, it will be categorized as \"Head entity error\". If head entity and tail entity are identical, it will be categorized as \"Contradiction between head and tail\".\n(iii) Conflict Check. Verifier conducts comprehensive conflict detection for each triple in RuleHub (Ahmadi et al., 2020). For instance, ensuring that a person's birth time precedes their time of death and a person's age is not a negative number.\nWe sequentially conduct quantity, format, and conflict check for generated triples and output information about the error types.\nFor error correction, we first determine the error type using the error detection step and offer corresponding prompts. Then, we reprompt the LLM to regenerate a corrected output. For instance, if the error type is \"format error\", we prompt the model with: \"Please generate it again strictly according"}, {"title": "Pruner", "content": "After passing through the verifier, we obtain all the correct triples for this level, and then proceed to generate the next-level triples.\nHowever, not all triples need the next-level generation. For instance, the triple \"(rice, optimal growth temperature, 20-25 degrees Celsius)\" is a correct triple, while its tail entity \"20-25 degrees Celsius\" does not need to be further generated as the head entity for the next-level triple generation.\nTherefore, to enhance the controllability of the constructed KG, we propose pruner, a T5 binary classifier model finetuned on an open-source KG, DBpedia. Its input consists of the tail entities from each correct triple. Its output is \"growing\" or \u201cpruned\u201d, indicating whether the entity should proceed to generate the next-level KG or cease further generation. Specifically, we input the text of entities to T5 and it generates \u201cgrowing\u201d or \u201cpruned\u201d as output. To train the pruner, we gather training data from DBpedia and select a subset of head entities to represent the \"growing\" category. We also gather an equivalent subset of tail entities, excluding those that overlap with the head entity list, to constitute the \"pruned\" category. We then use these entities text as input and the corresponding labels \u201cgrowing\" or \"pruned\" as output targets during fine-tuning.\nFinally, leveraging domain corpora, we can produce a single-level KG for the input entity, which will subsequently be incorporated into a new level of generated KG. Hence, SAC-KG generates multiple triples with the entity and proceed to iterate, creating a KG subtree rooted in head entities of the generated triples. This process resembles the incremental growth of a tree layer by layer, akin to retrieving and accessing domain knowledge from shallow to deep. Furthermore, SAC-KG is an unsupervised approach that can be applied to any domain with significant volumes of unstructured text corpora, without the need for labeled data."}, {"title": "Experiments", "content": "We design experiments to evaluate the effectiveness of the proposed SAC-KG and provide more insights of the constructed domain KG. With this desiderata, we divide the experiments into five parts:\n(i) To evaluate the effectiveness of SAC-KG, we compare SAC-KG with existing state-of-the-art methods for domain KG construction task.\n(ii) To offer a more comprehensive evaluation of constructed KG, we conduct agreement evaluation between GPT4 and humans.\n(iii) To provide more insight into SAC-KG, we conduct the ablation study of each component.\n(iv) To analyze the constructed KG, we conduct case study of the constructed domain KG.\n(v) To further demonstrate the effectiveness of SAC-KG, we evaluate SAC-KG on existing"}, {"title": "Datasets and Experiment Setup", "content": "We initially collect raw textual data from specialized books, web pages, and genealogical data to the rice domain. In total, we collect 70 specialized books, 1522 web pages, and 24000 genealogical records related to rice (see Appendix H for details). These domain corpora exhibit varying degrees of structural diversity and different levels of textual quality, which can also effectively emulate the conditions encountered in the majority of original corpora within other domains.\nWe retrieve domain entities as root node from the open-source KG and obtain their domain texts from domain corpora. We retrieve up to 500 tokens of domain text for each node. We then compare the extraction of triples based on the same input text by different baselines. We assess performance by using the following metrics.\nPrecision: To assess precision, we conduct evaluations through both manual and automatic manners, with the latter being more scalable in nature. Following Vicuna (Zheng et al., 2023), we employ GPT-4 (OpenAI, 2023) as an automatic judge. Specifically, we take extracted triples with their corresponding text as input to the GPT-4 for assessing the correctness of each triple.\nRecall: Estimating recall is infeasible due to the inability to access the ground truth triples for each domain text. Therefore, we report the average count of verified triples for each domain text. That is, we report recall without providing the denominator. We refer to this metric by the number of recalls. As in (Vo and Bagheri, 2016; Kolluru et al., 2020), this metric aligns with the real-world scenarios, where it is impractical to obtain the entire set of accurate facts. Consequently, the convention is to report only the count of generated facts. This metric serves as an indicator of the effective extraction and utilization of domain corpora.\nDomain Specificity: We aim to generate triples that are correct, domain-related, and distinct from those triples in the open-source encyclopedic KG. Specifically, inspired by the survey (Wang et al., 2023), we aim to ensure the construction of a large-scale domain KG with higher domain expertise. To this end, we introduce a domain-specific metric that quantifies the proportion of generated triples meeting three criteria: correctness, domain-related, and not presented in the open-source encyclopedic KG. This metric is computed as |set of generated domain-related and correct triples - set of triples in the open-source KG| / |set of generated triples|, where \"-\" denotes the set difference operation, and \"||\" represents the cardinality of a set. The primary objective of domain specificity is encouraging the LLM to extract knowledge not solely reliant on the open-source KG but also capable of summarizing and condensing domain knowledge from the domain corpora.\nFor the parameter set up of generator, we set temperature value of the LLM as 0.1 and a maximum sequence length of 2000 tokens. For pruner, we use the low-rank adaptation (Hu et al., 2021) to efficiently finetune a T5 (Roberts et al., 2019) model. We train the model with 2 epochs and use batch size of 64. We set the learning rate as 0.001. More details can be found in Appendix E."}, {"title": "Main Results", "content": "We employ four baseline models for our study, namely OpenIE 6 (Kolluru et al., 2020), Stanford OIE (Angeli et al., 2015), DeepEx (Wang et al., 2021), and PIVE (Han et al., 2023). OpenIE 6 and Stanford OIE represent state-of-the-art methods for rule-based triple extraction, with the Stanford OIE method being based on the updated version released in September 2023. DeepEx is a representative approach that combines Bert (Devlin et al., 2018) with rule-based techniques for triple extraction, while PIVE directly utilizes ChatGPT (OpenAI, 2020) to construct KGs.\nTo comprehensively evaluate the performance of our approach, we utilize multiple LLMs as backbones. We mainly use ChatGPT (OpenAI, 2020), which is widely recognized for its superior performance and serves as the foundation for many research endeavors. Furthermore, to demonstrate the effectiveness of our method on models with smaller parameter sizes, we also select Qwen 7B, Llama2 7B, and Llama2 13B (Bai et al., 2023; Touvron et al., 2023) as our backbones.\nAs shown in Table 1, SAC-KG outperforms previous methods in KG construction consistently. Rule-based approaches like OIE6 and Stanford OIE, which extract triples through lexical and semantic role labels, exhibit poor performance for precision and domain specificity metrics. We also observe that rule-based approaches tend to extract uninformative triples, leading to a falsely inflated recall rate (see Section 4.5 for details). DeepEx and PIVE, which utilise LMs as backbones, show some improvement but still also perform suboptimally. When utilizing the ChatGPT as the backbone, we achieve an precision rate of 89.32% and domain specificity of 81.25%. This further demonstrates the effectiveness of our approach in the direct construction of KGs from open domain corpora."}, {"title": "Agreement Evaluation", "content": "We use GPT-4 for automatic and efficient evaluation. To demonstrate the validity of this approach, we conduct a human evaluation. Specifically, we engage 20 volunteers, comprising 5 PhDs, 7 PhD candidates, and 8 master students with KGs and rice backgrounds. We make evaluation questionnaires, each with 100 \u201ctext-triple list\" pairs (input texts from the model and corresponding output triple lists). Volunteers assess triple correctness and error reasons (e.g., text inconsistency, formatting, or others) when marking a triple as incorrect.\nWe provide key statistical indicators for more trustworthy results. We use human evaluation results as the ground truth for precision, recall, F1 score, and average precision. As shown in Table 4, the results indicate a close alignment between GPT4 evaluation and human evaluation. With a precision value of 0.906, GPT4 identifies most positive samples. The recall value of 0.951 shows that it can capture most true positives. The F1 score of 0.928 also shows the solidity of GPT4 evaluation. Moreover, Cohen's Kappa coefficient above 0.6 suggests a medium to high consistency between GPT4 evaluation and human evaluation. Overall, these statistics demonstrate the effectiveness and dependability of GPT4 evaluation. More details of GPT and human evaluation are in Appendix C."}, {"title": "Ablation Study", "content": "To further investigate the contribution of each component within SAC-KG to the KG construction, we conduct a series of ablation experiments on the entire framework. We compute these metrics in each iteration to obtain a fine-grained result. Specifically, we denote SAC-KG without the open KG retriever as SAC-KGw/o prompt,SAC-KG without the domain corpora retriever as SAC-KGw/o text, SAC-KG without the verifier as SAC-KGw/o verifier, and SAC-KG without the pruner as SAC-KGw/o pruner, respectively."}, {"title": "Case Study", "content": "We conduct a case study on the KGs constructed by SAC-KG and the baselines. Specifically, we select rice varieties and rice experts as two cases to analyze the distinctions between different constructed KGs. More cases are in Appendix F. As illustrated in Table 3, each iteration of SAC-KG demonstrates favorable results in terms of precision and domain specificity. While in rice expert case, rule-based approaches achieve higher recall rates but exhibit suboptimal precision and domain specificity. This may be attributed to the heightened sensitivity to personal name entities of rule-based methods (Kolluru et al., 2020). However, their precision and domain specificity do not demonstrate satisfactory performance. On the contrary, SAC-KG exhibits higher precision and domain specificity in this case, albeit with lower recall rates.\nWe visualize three single-level KGs in the rice expert case to gain a further insight. As Figure 3 shows, the rule-based approaches (OIE6) tend to generate redundant triples simply through lexical and syntactical analysis. These triples often contain limited specific information. PIVE extracts more informative triples, while it is still affected by irrelevant textual noise and extracted incorrect triples such as \"(Gurdev Singh Khush, achievement, exponential population growth)\u201d. SAC-KG, while extracting a reduced number of triples, produces triples that possess a higher degree of human interpretability and domain information. Therefore, improving recall rates in specific cases of SAC-KG to increase the utilization of domain corpora information will be a focus of our future research."}, {"title": "Results on OIE benchmarks", "content": "To further demonstrate the effectiveness and generality of our SAC-KG, we conduct experiments on open-source benchmarks for traditional OIE tasks. Following the setting and the evaluation method of DeepEx (Wang et al., 2021), we evaluate the OIE2016 (Stanovsky and Dagan, 2016), NYT, WEB (Mesquita et al., 2013), and PENN (Radford et al., 2021) datasets and use traditional AUC and F1 score as metrics. Details of the datasets are summarized in Appendix G.\nAs shown in Table 5, SAC-KG also outperforms existing state-of-the-art methods across traditional OIE benchmarks, which demonstrate its effectiveness and generalization. Specifically, SAC-KG outperforms rule-based methods (OpenIE 6 and Stanford OIE) by a large margin. And compared with LLM-based methods (DeepEx and PIVE) SAC-KG also attain the optimal results consistently, which demonstrates the effectiveness and robustness of SAC-KG. These results also show the effectiveness of SAC-KG in the traditional OIE task."}, {"title": "Conclusion", "content": "In this paper, we propose a novel automatic domain KG construction framework named SAC-KG, which effectively constructs KG directly from domain corpora. SAC-KG incorporates LLMs as domain experts and iteratively employs an entity-induced tree search algorithm for the construction of a multi-level KG. Specifically, we propose Generator, Verifier, and Pruner to form a general KG construction framework with automation, precision, and controllability. SAC-KG constructs a domain KG at the scale of over a million nodes with an precision of 89.32%, achieving over 20% increase in precision metric. This superior performance of SAC-KG over existing state-of-the-art methods demonstrates effectiveness of our SAC-KG."}, {"title": "Limitations", "content": "While SAC-KG can construct domain-specific KGs, it cannot inject or update the domain knowledge into LLMs. Exploring low-cost methods to inject domain knowledge into LLMs for the creation of a domain-specific LLMs will be the focus of our future work. We will also focus on employing this approach as a means to explicitly interpret the learned knowledge of LLMs."}, {"title": "More Related Works", "content": "Language Models. Language models including GPT (Radford et al.), BERT (Devlin et al., 2018), ROBERTa (Liu et al., 2019), and Megatron-LM (Shoeybi et al., 2019) have led to a learning paradigm shift in natural language processing (NLP). Models are first pre-trained on extensive volumes of unlabeled text corpora with language modeling objectives, and then fine-tuned on downstream tasks. Recently, large language models (LLMs) including ChatGPT (OpenAI, 2020) and PaLM (Chowdhery et al., 2022) have shown great performance in both few-shot and even zero-shot scenarios (Brown et al., 2020). To further enhance the interpretability of these LLMs, some research endeavors explain LLMs through attribution analysis (Wang et al., 2022; Hanna et al., 2023; Gurnee et al., 2023). Another line of work aims to retrieve the knowledge explicitly from LLMs as the basis for interpreting them, including the reasoning task (Shi et al., 2023) and the QA task (Hao et al., 2023; Dhingra et al., 2020; Guu et al., 2020)."}, {"title": "Details of Error Types and Prompts", "content": "Our approach involves performing error correction based on the types of errors output by the error detection module. To further enhance inference efficiency, if the number of categorized triples exceeds a predefined threshold (the default being 3), the verifier will amalgamate the corresponding prompts with Table 6 with the original input to the LLMs for regeneration. Conversely, if the number of categorized triples is below this threshold, the verifier will directly eliminate the marked triples, obviating the need for regeneration."}, {"title": "More Details of Agreement Evaluation", "content": "We conduct a human evaluation by engaging several doctoral candidates with expertise in rice research. Specifically, we involve the validation of each generated fact by manual examination of highly reliable web sources such as Wikipedia to verify the precision of each fact. Moreover, we provide initial correct/incorrect triple examples for guidance. We set a 5-second minimum evaluation time per triple. For a 10-triple pair, volunteers need spend 50 seconds before the next evaluation. We also highlight entities and relations in text when they appear in triples.\nFollowing Vicuna(Zheng et al., 2023), we report agreement evaluation to demonstrate the rationality of our automatic evaluation. In Table 7, we compare the results of different evaluation methods for the same generated KG, where \u2018GPT-4\u2019 denotes the automatic evaluation in our main text, 'Author' denotes the results of evaluation by the authors, 'Humen' denotes the results of evaluation by domain experts, and \u2018Humen-M' denotes the majority judgment of humen. Moreover, we report the agreement between two types of judges on GPT-4, Author, Human, and Human-M in Table 8. The agreement between two types of judges as the probability of each type agreeing on questions, i.e., whether a given triple is correct or incorrect.\nWe engage volunteers with background related to KGs and rice, because they possess basic domain knowledge, which enables them to more accurately assess the quality of the generated domain knowledge graphs. Moreover, these volunteers come from a variety of universities and research institutions to enhance objectivity in evaluation."}, {"title": "Visualization of Ablation Study", "content": "We further visualize the first three-level generated KG of each ablated version of SAC-KG. As Figure 5 shows, the full version of SAC-KG exhibits the overall best result, and the number of error triples in each level do not exhibit significant differences. This phenomenon reveals that error propagation is not notable in the iterative generation of the domain KG. On the contrary, SAC-KGw/o text and SAC-KGw/o pruner exhibit error propagation, which leads to a significant increase of error triples generated in the third layer. SAC-KGw/o prompt and SAC-KGw/o verifier only extract fewer triples, which means the LLM suffers from summarizing knowledge in domain corpora without examples and error correction process. These results further affirm that each component within the framework contributes significantly to the construction."}, {"title": "Details of Experiment Setup", "content": "We provide parameter settings for the mentioned large language models. For all large language models, We set the temperature hyperparameter that controls the output stability of LLMs as 0.1. The lower the temperature setting, the more stable the model output is. We set the max input length as 500 tokens, which represents the maximum token length input to the model is 500 tokens per response. We set the max length of retrieved text as 2000, which represents the maximum length of"}, {"title": "More Case Study Results", "content": "We visualize the rice experts case results (mentioned in Section 4.5) of Stanford OIE and Deepex in Figure 4. SAK-KG also performs better and produces more precise and domain-aware triples.\nWe also present more case study results in Table 10. We also observe that in rice disease, rice pest and rice variety cases, SAC-KG also outperforms all mentioned baselines in all three metrics by a large margin. This also demonstrates the effectiveness of our SAC-KG.\nWe further visualize the three case study results in Figure 6. Three single-level KGs generated by SAC-KG in these three cases are also precise and human understandable, which demonstrates the effectiveness of our method."}, {"title": "More discussions on SAC-KG", "content": "I.1 Could the random return of a set of triples in cases where no relevant triples are retrieved potentially detrimentally affect performance or accuracy?\nA1: It does not detrimentally impact performance. In experiments, we observe that the more relevant the triple prompts, the stronger the prompting effect. In instances where relevant triples cannot be retrieved, randomly returning a set of triples or returning a fixed set (both perform equally) still enhances performance compared to not returning any triples.\nI.2 If there are still incorrect triples after passing through the verifier, does the verifier fail to detect them?\nA2: To enhance reasoning efficiency, the verifier presently employs the rule-based techniques for format and conflict assessments, resorting to re-prompting the LLM solely upon the detection of errors. Furthermore, extensive empirical findings reveal that the utilization of domain text and triple prompts diminish the likelihood of encountering factual inaccuracies (e.g., erroneous triples such as \"United States, Capital, New York\u201d), with the majority of errors being concentrated in formatting or conflicts. Nevertheless, it is worth noting that future research could explore the possibility of enabling the model to autonomously perform verification, which would substantially increase the computational cost of reasoning.\nI.3 Are all the triples provided as input to the pruner correct?\nA3: Not necessarily all of them are correct. The preceding verifier strives to ensure the correctness of triples while maintaining high efficiency. However, the accuracy of triples does not significantly impact the subsequent generation process, as the quality of the tail entity primarily dictates the quality of the subsequent generation, irrespective of triple correctness. Consequently, the objective of the pruner is to eliminate low-quality tail entities.\nI.4 Why is the pruner trained with DBpedia able to yield favorable results in the domain of rice?\nA4: Indeed, this is intuitive because determining whether an entity can function as a head entity does not necessitate an extensive domain-specific knowledge. It primarily involves learning the pattern distinctions between head and tail entities. Additionally, DBpedia encompasses fundamental nouns pertinent to the domain of rice. For instance, in the case of tail entities such as \u201cglutinous rice\u201d, \u201crice blast disease\u201d, \u201c33 acres\u201d, \"3-4 days of concentrated irrigation\u201d, and \u201clack of nitrogen fertilizer\", it is relatively straightforward to discern that those entities marked in bold can be utilized as head entities, implying their association with meaningful individual objects.\n1.5 Does the open-source KG incorporate domain entities?\nA5: As previously stated, open-source KGs like DBpedia encompass fundamental nouns related to the domain of rice. They encompass basic rice varieties, rice-related diseases and pests, as well as planting methods. Nevertheless, more specialized terminology may not be encompassed. Consequently, one approach is to identify common domain entities from the open-source knowledge graph and employ them as a foundation for constructing the KG. Subsequently, SAC-KG can be employed to guide the incremental extraction of \"specialized knowledge from the corpus, layer by layer\".\"\n    }"}]}