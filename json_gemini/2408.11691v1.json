{"title": "Physics-informed Discovery of State Variables in Second-Order and Hamiltonian Systems", "authors": ["F\u00e9lix Chavelli", "Zi-Yu Khoo", "Dawen Wu", "Jonathan Sze Choong Low", "St\u00e9phane Bressan"], "abstract": "The modeling of dynamical systems is a pervasive concern for not only describing but also predicting and controlling natural phenomena and engineered systems. Current data-driven approaches often assume prior knowledge of the relevant state variables or result in over-parameterized state spaces. Boyuan Chen and his co-authors proposed a neural network model that estimates the degrees of freedom and attempts to discover the state variables of a dynamical system. Despite its innovative approach, this baseline model lacks a connection to the physical principles governing the systems it analyzes, leading to unreliable state variables. This research proposes a method that leverages the physical characteristics of second-order Hamiltonian systems to constrain the baseline model. The proposed model outperforms the baseline model in identifying a minimal set of non-redundant and interpretable state variables.", "sections": [{"title": "1 Introduction", "content": "Dynamical systems, systems that change over time, pervade the natural and engineered world, embodying the complex interactions and evolution observed across a multitude of fields, from physics and biology to economics and engineering. Their universality and applicability in modeling real-world phenomena underscore the imperative for their study, motivating works that offer insights into system behavior, prediction, and control [17,7,18,16,6]. However, most data-driven methods for modeling dynamical systems still rely on the assumption that the relevant state variables are already known [5,6] or use more parameters than necessary to represent possible configurations of the state variables [18,7].\nRecent work by Chen et al. [5] proposed a neural network model for dynamical system analysis that identifies the relevant state variables. The model"}, {"title": "2 Background", "content": "Dynamical systems are defined by a set of time-dependent equations that characterize the evolution of a system's state over time [8], and are represented as\n$$x = f(x,t)$$\nwhere x is a vector of the state variables of length equivalent to the number of degrees of freedom of the dynamical system, X is the time derivative of x with respect to time t, and f is a function that defines the system dynamics [8].\nAutonomous second-order dynamical systems are a class of dynamical systems characterized by an even number of state variables where half of them describe the position (7), and the other half represent the associated momentum (p) [8]. Hamiltonian dynamical systems are a class of dynamical systems whose"}, {"title": "3 Methodology", "content": "Chen et al.'s baseline model is a nested autoencoder [5] (Figure 1, A and B). In the first step, two consecutive video frames from a dynamical system are input to the outermost autoencoder. A compact representation of the input is\n$$ \\frac{\\partial H}{\\partial \rho}  =  \\dot{r}  ; \\dot{p}  =  - \\frac{\\partial H}{\\partial r}$$\nwhere H represents the total energy or Hamiltonian of the dynamical system [14,15], and is a conserved quantity. Hamiltonian neural networks [3,11,10] are physics-informed machine learning models that incorporate learning biases based on Hamilton's equations within neural networks. They regress the Hamiltonian of a dynamical system directly from its state variables using Hamilton's equations and enforce the invariance of the total energy of the dynamical system.\nAn autoencoder is a neural network comprising an encoder function, which constructs an encoding of the input, and a decoder function, which produces a reconstruction of the input [9]. Sandwiched between them is a hidden layer that describes a code, or latent variables, used to represent the input [9]. Autoencoders are designed to be unable to learn to copy perfectly, usually by limiting the number of latent variables [9].\nVariational Autoencoders (VAEs) extend the autoencoder framework by in- troducing a probabilistic approach to encoding inputs [12]. VAEs modify the loss function of an autoencoder by adding a Kullback-Leibler (KL) divergence term in the loss function,\n$$LVAE = -\\beta \\cdot E_{q(z|x)} [log p(x|z)] + KL(q(z|x)||p(z))$$\nwhere x is the input and z its latent representation, \u2014Eq(z|x) [logp(x|z)] is the reconstruction loss in a variational context, q(z|x) denotes the encoder's distribution and p(z) denotes a prior distribution p(z). \u1e9e adjusts the respective weights of the two terms. It can be placed on either the reconstruction or KL divergence term. The KL divergence term penalizes deviations of the learned distribution in the latent space from a chosen prior Gaussian distribution.\nThe KL divergence term enforces independence among the latent variables by pushing the encoded distributions to resemble the prior [1]. Therefore vari- ables that do not contribute significantly to reducing the reconstruction loss become redundant and converge to the non-informative zero prior [1]. This process inherently minimizes the size of the latent space to the number of degrees of freedom of the dynamical system, by eliminating excess variables. Furthermore, the inclusion of the KL divergence encourages the VAE to find disentangled, semantically meaningful, statistically independent, and causal factors of variation in data [12]. The result is a more interpretable and minimal representation, which facilitates understanding of the underlying structure of the data."}, {"title": "3.1 Observational bias", "content": "The Physics-Informed AutoEncoder (PI-AE) builds on Chen et al.'s baseline. It incorporates an observational bias by enforcing the system's second-order constraint to the latent space of the innermost autoencoder. Following the nomenclature of second-order dynamical systems, pairs of latent variables of the autoencoder are constrained, such that the first represents the position, and the second, the momentum of the dynamical system."}, {"title": "3.2 Learning bias", "content": "The Physics-Informed Variational AutoEncoder (PI-VAE) builds on the PI-AE and incorporates a learning bias regarding the time-continuity of the dynamical system and the independence of the state variables. The constraint is incorpo- rated by a variational autoencoder. The KL divergence term in the loss function enforces latent sparsity, as the learned distribution in the latent space has a Standard Multivariate Normal Distribution prior."}, {"title": "3.3 Inductive bias", "content": "The Hamiltonian Physics-Informed Variational AutoEncoder (HPI-VAE) builds on the PI-VAE and incorporates an inductive bias. It modifies the PI-VAE ar- chitecture to include a Hamiltonian neural network which takes as input the latent variables. This model has three terms in its loss function. They are the reconstruction loss, the KL divergence, and Hamilton's equations."}, {"title": "4 Experiments", "content": "The models are experimentally compared on the task of estimating the number of degrees of freedom of five dynamical systems. The latent variables of the model correspond to the state variables of the dynamical system. We comment on the interpretability of the identified state variables for some dynamical systems.\nThe data comprises video frames of the systems which are either real, filmed with a camera, or numerically simulated from Chen et al. They comprise 100,000 frames per dynamical system, comprising 100 frames from 1,000 random trajec- tories. These datasets are divided into training, validation, and test sets, consti- tuting approximately 80%, 10%, and 10% of the data, respectively."}, {"title": "5 Conclusion", "content": "In conclusion, through the integration of physics-informed machine learning with variational autoencoders, we have allied physical knowledge and data-driven ma- chine learning to enhance the interpretability and simplicity of modeling complex systems. Our approach marks an improvement over traditional methods by parsi- moniously identifying the degrees of freedom of dynamical systems. Furthermore, the latent variables are a minimal, non-redundant representation of dynamics that faithfully captures the system's physical characteristics. This advancement holds promise for a wide range of applications, from fundamental physics to engineering. We anticipate that the methodologies and insights gleaned from this work will help catalyze further research, fostering the development of more sophisticated, physics-informed models capable of tackling the complexities in- herent in the natural and engineered world.\nFuture work should be directed toward the interpretation of latent variables. This can involve extending the proposed models to increasingly complex sys- tems, using other physics constraints, or encompassing different modified loss functions. Different state variables have varying contributions to their respec- tive dynamical systems, being able to differentiate the penalization of errors for different latent variables at large and small spatial or temporal scales will be beneficial to identifying more interpretable state variables."}]}