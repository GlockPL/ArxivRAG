{"title": "Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective", "authors": ["Zhaotian Weng", "Zijun Gao", "Jerone Andrews", "Jieyu Zhao"], "abstract": "Vision-language models (VLMs) pre-trained on extensive datasets can inadvertently learn biases by correlating gender information with specific objects or scenarios. Current methods, which focus on modifying inputs and monitoring changes in the model's output probability scores, often struggle to comprehensively understand bias from the perspective of model components. We propose a framework that incorporates causal mediation analysis to measure and map the pathways of bias generation and propagation within VLMs. This approach allows us to identify the direct effects of interventions on model bias and the indirect effects of interventions on bias mediated through different model components. Our results show that image features are the primary contributors to bias, with significantly higher impacts than text features, specifically accounting for 32.57% and 12.63% of the bias in the MSCOCO and PASCAL-SENTENCE datasets, respectively. Notably, the image encoder's contribution surpasses that of the text encoder and the deep fusion encoder. Further experimentation confirms that contributions from both language and vision modalities are aligned and non-conflicting. Consequently, focusing on blurring gender representations within the image encoder which contributes most to the model bias, reduces bias efficiently by 22.03% and 9.04% in the MSCOCO and PASCAL-SENTENCE datasets, respectively, with minimal performance loss or increased computational demands.", "sections": [{"title": "Introduction", "content": "Vision-language models have shown promising results in tasks such as classification (Li et al., 2023; Jia et al., 2021; Radford et al., 2021), image search (Sun et al., 2023; Radford et al., 2021; Li et al., 2023), and object detection (Kuo et al., 2023; Li et al., 2022) by training on large-scale image-text pairs to understand the correspondences between cross-modal image features and language features. Models trained on extensive datasets exhibit excellent zero-shot capabilities (Radford et al., 2021; Yu et al., 2022; Li et al., 2022; Zhang et al., 2022a) but also risk discovering and exploiting societal biases present in the underlying image-text pair corpora, potentially introducing bias that leads to social unfairness (Zhao et al., 2017). The revelation, measurement, and understanding of biases within models (Zhou et al., 2022; Zhang et al., 2022b; Lee et al., 2023; Vig et al., 2020) has sparked widespread research interest and are crucial for bias mitigation (Zhang et al., 2022b; Seth et al., 2023; Dehdashtian et al., 2023). However, most contemporary methods, derived from language models, lack standardized metrics for evaluating bias and primarily assess the correlation between the outputs of classifiers and external attributes (Zhang et al., 2022b). Barrett et al. (2019) noted that interpretations based on classifier outputs can be factually inaccurate and not generalizable. While these methods can highlight the impacts of certain contributions on model outputs, they (1) fail to comprehend the generation and flow of bias within the model and (2) do not understand the causal roles of model components in the generation and propagation of bias. Consequently, they are not able to provide clear guidance on how to effectively mitigate bias at the model level.\nIn this work, we propose a standardized framework to measure bias in VLMs, providing a comprehensive understanding of how bias flows within the entire model structure. Specifically, we use the GLIP model (Li et al., 2022) as a case study, focusing on gender bias in the task of object detection, which is a predominant and challenging problem in computer vision. We conduct the analysis on both the MS-COCO (Lin et al., 2014) and PASCAL-SENTENCE (Rashtchian et al., 2010) datasets. We observe that GLIP model exhibits unbalanced inference capabilities on different genders, with certain indoor object categories like pets more likely to be associated with females and outdoor objects like vehicles with males. To holistically understand how the bias flows in the model, we adapt causal mediation analysis (Vig et al., 2020) to VLMs, providing a finer-grained study of contributions from different model components. We find that, among the different model components (text module, image module, and fusion module that combines them), the image module contributes the most to the model's bias over twice as much as the text module. In the MSCOCO and PASCAL-SENTENCE datasets, image features accounted for 32.57% and 12.63% of the bias generated, respectively, compared to approximately 15.48% and 5.64% by text features. Also, the interaction and updating process between image and text features during the deep fusion process significantly impacts bias production, accounting for about 57% of the contributions in the image and text encoders. Furthermore, by integrating interventions across different modules, we discovered that their contributions to bias are aligned and do not conflict, allowing us to prioritize bias mitigation efforts within the image encoder, which is the most substantial contributor to bias. Based on the results, we propose to effectively mitigate the bias in VLMs: reducing biases from the image module can successfully reduce bias by 22.03% on the MSCOCO dataset and 9.04% on the PASCAL-SENTENCE dataset, compared to a reduction of 7.08% and 1.18% in the text module. In summary, the contributions of our work are:\n\u2022 We provide a comprehensive evaluation of the bias in VLMs, with an understanding of the contribution from each model module, which is missing in the literature.\n\u2022 We analyze the correlation between the biases from different modules and discover that the bias in different modules are aligned and do not conflict with each other.\n\u2022 We propose an effective bias mitigation strategy to reduce the bias from the module that contributes most to the model bias when facing a limited budget."}, {"title": "Related Work", "content": "In recent years, vision-language models (VLMs) have experienced rapid advancements. The latest developments in VLMs often employ a dual-stream architecture that separately encodes text and images (Kim et al., 2021), and these are then merged and aligned to facilitate cross-modal understanding of visual and linguistic features (Radford et al., 2021). Furthermore, some studies treat the joint training of images and text as a phrase localization process, aiming to better align and integrate visual and linguistic features (Li et al., 2022). Typically, these models are trained on image-text pairs from datasets such as MSCOCO (Lin et al., 2014), VQA (Antol et al., 2015), OpenImages (Kuznetsova et al., 2020), and Flickr30k Entities (Plummer et al., 2015), achieving impressive results in various downstream tasks including image classification (Radford et al., 2021), image generation (Radford et al., 2021), visual question answering (Li et al., 2018; Antol et al., 2015), and image captioning (Lu et al., 2019; Alayrac et al., 2022).\nAlongside their development, the societal biases exhibited by VLMs have also attracted significant attention. These models often reflect societal stereotypes and may even amplify such biases (Zhou et al., 2022). Most contemporary research addressing bias in VLMs has borrowed methodologies from language model studies. For instance, Srinivasan and Bisk (2021) utilized a language masking model to explore gender biases by using templates containing a specific entity and analyzing the probabilities of masked entities (Kurita et al., 2019). Some researchers have examined biases through the comparison of factual and counterfactual inputs, with Zhang et al. (2022b) investigating biases by examining predicted probabilities from both factual and counterfactual inputs, and Howard et al. (2024) using the Perspective API to score predictions derived from such inputs to study model biases.\nHowever, existing evaluation methods primarily observe changes in the probability scores of model outputs following interventions on input samples. This approach limits our understanding of the underlying causes of bias generation and propagation within model components (Barrett et al., 2019). Therefore, we propose a standardized framework for evaluating bias in vision-language tasks and introduce causal mediation analysis (Robins and Greenland, 1992; Pearl, 2022; Vig et al., 2020) within the context of vision-language models. This methodology helps us understand the pathways of bias generation and propagation from the input level to model components."}, {"title": "Bias Measurement and Understanding in VLM", "content": "In this section, we propose a bias evaluation metric to assess the bias of VLM in the object detection task. By applying causal mediation analysis, we quantify the contribution on bias from various components within the model pipeline which helps us trace the origins and propagation of bias throughout the model pipeline. Additionally, we investigate the interactions between different modalities to understand how they collectively influence model bias which will be used as guidance for bias mitigation later."}, {"title": "Bias Evaluation Metrics", "content": "In the literature, there have been various methodologies proposed to measure bias, including notable contributions from Zhao et al. (2017), Wang and Russakovsky (2021) and Zhao et al. (2023). These studies often assess bias amplification by comparing statistics between the training dataset and model outputs, where the models are trained and tested on similarly distributed datasets. In contemporary settings, most VLMs undergo training on extensive collections of image and text corpora. In real-world applications, users may fine-tune a model on a dataset specific to a downstream task. The combination of fine-tuning data and pre-training data can introduce noise, complicating the statistics of previously mentioned bias evaluations. Additionally, many pre-training datasets used for large-scale models are either difficult to access or require significant computational resources for analysis, making existing evaluations challenging to deploy in modern settings.\nNotably, recent advancements in VLM have demonstrated impressive zero-shot performance, enabling models to make accurate predictions on benchmark datasets without any fine-tuning (Radford et al., 2021; Yu et al., 2022; Li et al., 2022; Zhang et al., 2022a). In our study, we explore a zero-shot scenario where VLMs are directly tasked with making predictions on a benchmark dataset without any fine-tuning.\nDrawing inspiration from observations in Zhao et al. (2017), where females typically correlate more closely with indoor objects than males, we introduce the definition of BIASVL which captures model's underlying correlations between sensitive attributes (e.g., genders) and objects:\n$BIAS_{VL} := \\sum_{object} |C(object, male) - C(object, female)|$                                                                                                                                                  (1)\nwhere C(x, y) measures the correlation between x and y. In our case, we use a false positive rate (FPR) to describe the correlation, which measures how often one specific gender y can trigger a model to incorrectly predict one object x in the image."}, {"title": "Causal Mediation Analysis Method", "content": "Causal mediation analysis measures how a treatment effect influences an outcome either directly or indirectly through a mediator variable (Robins and Greenland, 1992; Vig et al., 2020; Robins, 2003; Pearl, 2022). An illustrative example is shown in Figure 1, where athletes engage in strength training to improve athletic performance. After training, they need muscle relaxation to alleviate soreness, which also impacts performance. Thus, strength training can have a direct effect on athletic performance through its intended mechanisms and an indirect effect through muscle relaxation.\nIn our study, the treatment consists of interventions on the input module, while the mediator could be any model component or finer-grained layer or neuron we are interested in and the outcome is the change in gender bias in the model's prediction results. Therefore, we define three types of intervention: a) replace-gender, which replaces the gender word man or woman to a gender-neutral word person in the text of the input module; b) mask-gender, where pixels corresponding to a person in the image module are masked, thus removing gender information from the input images; and c) null, which leaves the original text and image modules unchanged."}, {"title": "Experimental Setup of Bias Measurement and Understanding", "content": "For the object detection task, we employed the GLIP model pre-trained on the 0365, GoldG, CC3M, and SBU datasets (Li et al., 2022). The model consists of an image module, a text module, and a deep-fusion module that updates and aligns image features and text features. For object detection, the GLIP model makes predictions based on the given image and a text input, which is a list of possible categories separated by commas.\nOur experiments were conducted on the MSCOCO and PASCAL-SENTENCE datasets. For MSCOCO, we follow the setting in Zhao et al. (2017), where we only consider 66 objects that appear with man or woman more than 100 times in the training data. For the PASCAL-SENTENCE dataset, which includes 20 categories but lacks gender labels, we annotated gender based on the five captions associated with each image. An image is labeled as male if any caption mentions \u201cmale, males, man, men, boy, boys\" and as female if any caption mentions \u201cfemale, females, woman, women, girl, girls\". Images that do not include any person or mention both genders were excluded.\nInitially, we implement replace-gender and mask-gender interventions on the inputs respectively without any alterations to the model components. By monitoring the changes in the values of BIASVL, the individual impacts of image and text inputs on gender bias within the input module were assessed. Subsequently, we conducted a detailed causal mediation analysis on the text encoder and image encoder, respectively, by choosing the attention head within a specific layer and those in all preceding layers as mediators, conducting experiments from shallow to deep layers. This analysis aimed to identify whether the text encoder or image encoder contributes more significantly to gender bias and to determine which layers in the model are principally responsible for bias generation. It also sought to understand how bias flows and accumulates across different layers within the encoders. Then, we selected a combination of attention layers from both the image encoder and text encoder as mediators to observe changes in bias and compare these results with previous findings, exploring whether different modalities reinforce bias or conflict in the direction of bias.\nIn the deep fusion encoder, where image and text features dynamically interact and are updated, we implement replace-gender and mask-gender interventions in the input module to control the state of image and text features within the deep fusion module. We also select the attention heads within a specific layer and all preceding layers' attention heads as the mediator for conducting causal mediation analysis. By observing changes in the values of BIASVL, we explore how image and text features individually affect the deep fusion process and subsequently influence bias generation."}, {"title": "Results", "content": "We present the results of BIASVL in Table 1, for the MSCOCO dataset, without any intervention on the inputs, the BIASVL measured was 1.434. To highlight the significance of this bias, we randomly divided subsets composed of male images into two equal parts, achieving an BIASVL of 0.278. Similarly, dividing female image subsets randomly resulted in an BIASVL of 0.359. Both results are significantly lower than 1.434, and comparable results were observed with the PASCAL-SENTENCE dataset, as detailed in Table 1. The results in the random division demonstrate that a model with balanced inference capabilities across a dataset would yield minimal BIASVL values when divided into equal subsets (i.e., the gender stays the same). However, when model predictions are influenced by attributes such as gender, splitting the dataset based on such attributes leads to higher BIASVL values.\nWe also provide detailed statistics of False Positive Rate (FPR) scores for various objects in the PASCAL-SENTENCE dataset, presented in Figure 3. Our statistics reveal that a significant portion of indoor objects, such as furniture and pets, exhibit higher FPRs in images of females than in those of males. Conversely, outdoor objects, such as vehicles, tend to have higher misclassification rates in images of males. These findings suggest that the model more closely associates females with indoor objects. The FPR scores for different objects on the MSCOCO dataset are included in the appendix."}, {"title": "Bias Understanding with Causal Mediation Analysis", "content": "We conduct the causal mediation analysis on different modules to study their effect on the model bias. We find that the image module influences the model bias more than the text module and the fusion module. In addition, we show that the bias in the image and text modules are aligned \u2013 they are showing similar gender bias tendencies rather than conflicting ones.\nApplying the mask-gender intervention to the input image module reduced the BIASVL to 0.967 for the MSCOCO dataset and to 0.664 for the PASCAL-SENTENCE dataset, representing reductions of approximately 32.57% and 12.63%, respectively. We employed the attention heads in the image encoder as the mediator to examine both the indirect effects of this model component and the direct effects of the mask-gender on predictions. Figure 4a and Figure 4e illustrate that employing more attention heads as mediators leads to greater reductions in indirect effect, with diminishing reductions in direct effect. This supports an intuition that removing gender information from more layers in the image encoder weakens the model's dependency on latent correlations between gender in images and specific objects, thus mitigating gender bias in predictions. Furthermore, while interventions at the input level significantly impact final predictions, targeting the image encoder alone achieves about 53% of the mask-gender effect.\nImplementing a replace-gender intervention on the input text module reduced the BIASVL to 1.212 for the MSCOCO dataset and to 0.720 for the PASCAL-SENTENCE dataset, reductions of approximately 15.48% and 5.64%, respectively. We chose the attention heads within the text encoder as the mediator in this case. As shown in Figure 4b and Figure 4f, similar to the image encoder insights, removing gender information from multiple layers in the text encoder substantially decreases the model's reliance on latent correlations between gender in text and specific objects, thereby reducing prediction biases. The replace-gender intervention led to a smaller reduction in bias compared to mask-gender, emphasizing the more substantial role of images in generating gender bias relative to text. This outcome is likely influenced by the simplistic structure of the input text used in our study, which adheres to the format described in original GLIP experiments (Li et al., 2022), separating each category with a period, resulting in less complex text features than image features. Language models typically capture basic features such as syntactic structures at shallow layers and more complex semantic information at deeper layers, correlating with the significant changes in BIASVL observed at the sixth layer.\nTo further validate whether image features contribute more to bias creation than text features, we utilized the attention heads in the deep fusion encoder as the mediator, adjusting the attention heads' parameters in the states of either mask-gender intervention or replace-gender intervention. The results displayed in Figure 4d and Figure 4c show that for the MSCOCO dataset, the indirect effects from mask-gender and replace-gender through the deep fusion encoder are up to 0.260 and 0.189, respectively, reducing the BIASVL by approximately 18.13% and 13.18%. For the PASCAL-Sentence dataset, the reductions are 10.80% and 0.53%, respectively (Figure 4h and Figure 4g). These findings reaffirm our conclusion that image features play a more substantial role in bias generation than text features. They also suggest that even though the deep fusion module does not extract features directly from images and text, the interactive updating process between text and image features significantly influences bias generation, accounting for approximately 55.70% of the effect observed with the encoder alone.\nMulti-modal models consist of various interacting modules, each of which can learn distinct biases. However, the current literature does not thoroughly investigate whether these biases are aligned or disparate across different modules. In this section, we conduct an empirical analysis in VLMs to address this question. We simultaneously intervene in both the vision and language modalities. We apply replace-gender and mask-gender interventions to the input module and select a consistent proportion of attention heads in both the image encoder and text encoder as mediators. This setup allows us to observe changes in BIASVL and compare these with the changes induced by interventions in single modalities. Figure 5a and Figure 5b demonstrate that combined interventions on both images and text achieve greater bias reduction than interventions on either alone. However, the total reduction is not merely additive; the overall bias reduction is less than the sum of the individual contributions."}, {"title": "Bias Mitigation Method", "content": "Based on our experimental results, image features contribute most significantly to gender bias and the image encoder has a more pronounced impact on bias compared to the text encoder and deep-fusion encoder. Therefore, our intuition is that focusing on reducing gender representation in the image encoder will effectively reduce bias, especially when facing a computation budget. We use the bias mitigation achieved from the text encoder as a baseline, then focus on reducing bias from the image encoder and compare the results with the baseline.\nFor the text encoder, we aim to blur the gender representation in text features. We modify the structure of the text encoder to first identify gender-related terms (man, woman, men, women, male, female, males, females) in the input text. A new sentence is generated by replacing these gendered terms with their corresponding anti-gender terms (i.e., man to woman, male to female). The text encoder's output features are the average of the original sentence's text features and the anti-gender sentence's text features. Since the only difference between the two sentences is the gendered terms, this approach effectively blurs gender representation within the text encoder. We then let model to make predictions and observe the reduction in BIASVL.\nSimilarly, for the image encoder, we aim to blur gender representation in image features. To achieve this, we incorporate MTCNN (Zhang et al., 2016) as a face detector and MobileNet (Sandler et al., 2018) as a gender classifier into the existing image encoder framework. Both networks are lightweight, allowing their integration without significantly increasing the computational load during inference. When an image is input into the image encoder, the MTCNN (Zhang et al., 2016) network first identifies potential faces and outlines them with bounding boxes. MobileNet (Sandler et al., 2018) then classifies the gender of the faces within these boxes. We have prepared a male face image and a female face image in advance. Depending on the gender predicted by MobileNet (Sandler et al., 2018), we replace the face in the bounding box with the corresponding pre-prepared anti-gender face image. The final image features output by the image encoder are an average of the original image features and the features of the newly introduced anti-gender face. This method effectively blurs the original gender representation in the image. Then we let the model to make predictions and observe the reduction in BIASVL."}, {"title": "Experimental Setup of Bias Mitigation", "content": "We utilized the GLIP model, pre-trained on the O365, GoldG, CC3M, and SBU datasets (Li et al., 2022). In our setup, we incorporated an MTCNN (Zhang et al., 2016) pre-trained on the Wider Face and CelebA datasets as a face detector within the image encoder. Additionally, we integrated a MobileNet (Sandler et al., 2018) pre-trained on ImageNet to serve as a gender classifier.\nWe evaluated the effectiveness of bias mitigation on the MSCOCO and PASCAL-SENTENCE datasets. To assess the model's object detection performance, we compared it with the original GLIP (Li et al., 2022) on the MSCOCO and PASCAL-SENTENCE datasets using the AP (Average Precision) metric for zero-shot object detection."}, {"title": "Results", "content": "As indicated in Table 2, blurring gender representations in the image encoder demonstrated significant bias mitigation on both the MSCOCO and PASCAL-SENTENCE datasets. The experimental findings suggest that obscuring gender information in the image encoder is more effective at reducing model bias compared to similar interventions in the text encoder. Our results show that by blurring gender representations in the image features within the image encoder, we effectively reduced model bias by 22.03% and 9.04% on the MSCOCO and PASCAL-SENTENCE datasets, respectively, with minimal impact on model performance."}, {"title": "Conclusion", "content": "Vision-language models (VLMs) trained on large-scale image-text pair corpora are at risk of learning social biases from their training data. In this paper, we introduced a standardized framework incorporating causal mediation analysis to measure and understand the pathways through which model bias is generated and propagated within VLMs. We discovered that image features contribute significantly more to model bias than text features, and the contributions from the image encoder substantially exceed those from the text encoder and deep fusion encoder. Furthermore, the contributions to bias from different language modalities reinforce each other. Subsequently, by focusing on the components that contribute most to bias, we efficiently reduced model bias.\nOur work provides a framework for measuring, understanding, and mitigating model bias, which, although utilized here within the realm of object detection, can be extended to a wide range of VLM tasks. However, our framework is primarily applicable to white-box models, as it requires interventions at the internal components of the model. A promising direction for future work would involve expanding our framework to encompass additional modalities such as audio or video. This expansion could further enhance our understanding of multimodal interactions and their impact on bias, as well as deepen insights into how different sensory inputs contribute to, or mitigate, biases in AI systems."}, {"title": "Limitations", "content": "Our work provides a framework for measuring, understanding, and mitigating model bias in vision-language models (VLMs), with broad applicability across various VLM tasks. However, our approach primarily applies to white-box models, as it requires interventions within the model's internal components. Consequently, this limitation implies that our methods might not be directly applicable to scenarios where model internals are inaccessible or when dealing with black-box systems."}, {"title": "Appendix", "content": ""}]}