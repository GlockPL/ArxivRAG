{"title": "LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by Long-Short-Term Prompting", "authors": ["Haoxin Liu", "Zhiyuan Zhao", "Jindong Wang", "Harshavardhan Kamarthi", "B. Aditya Prakash"], "abstract": "Time-series forecasting (TSF) finds broad applications in real-world scenarios. Prompting off-the-shelf Large Language Models (LLMs) demonstrates strong zero-shot TSF capabilities while preserving computational efficiency. However, existing prompting methods oversimplify TSF as language next-token predictions, overlooking its dynamic nature and lack of integration with state-of-the-art prompt strategies such as Chain-of-Thought. Thus, we propose LSTPrompt, a novel approach for prompting LLMs in zero-shot TSF tasks. LSTPrompt decomposes TSF into short-term and long-term forecasting sub-tasks, tailoring prompts to each. LSTPrompt guides LLMs to regularly reassess forecasting mechanisms to enhance adaptability. Extensive evaluations demonstrate consistently better performance of LSTPrompt than existing prompting methods, and competitive results compared to foundation TSF models.", "sections": [{"title": "1 Introduction", "content": "Time-series (TS) data are ubiquitous across various domains, including public health (Adhikari et al., 2019), finance (Deb et al., 2017), and energy (Tay and Cao, 2001). Time-series forecasting (TSF), a crucial task in TS data analysis, aims to predict future events or trends based on historical data. Recent advancements in large Pre-Trained Models (PTMs), a.k.a. foundation models, and Large Language Models (LLMs) have demonstrated their effectiveness for TSF tasks. This is achieved either by training TS foundation models from scratch (Yeh et al., 2023; Kamarthi and Prakash, 2023; Garza and Mergenthaler-Canseco, 2023; Das et al., 2023) or adapting LLMs to TS data as natural language modalities (Jin et al., 2023; Chang et al., 2023; Xue and Salim, 2023; Gruver et al., 2023). These methods leverage powerful generalization capabilities of PTMs or LLMs, proving effectiveness in zero-shot TSF tasks with promising applications without the need for domain-specific training data.\nDesigning proper prompting techniques for zero-shot TSF tasks offers notable advantages, which avoids training models from scratch or fine-tuning LLMs for computational efficiency while maintaining forecasting accuracy. Existing approaches (Xue and Salim, 2023; Gruver et al., 2023) prompt LLMs for zero-shot TSF tasks by aligning TS data with natural language sequences and prompting LLMs to perform TSF as sequence completion tasks. However, these methods overlook the dynamic nature of TS data and the intricate forecasting mechanisms inherent in TSF tasks, such as modeling temporal dependencies, which cannot be adequately modeled by simple sequence completion tasks.\nTo address the limitation, we introduce LSTPrompt, a novel prompt strategy of LLMs for TSF tasks by providing specific TSF-oriented guidelines. Our contributions are summarized as follows:\n\u2022 We propose Long-Short-Term Prompt (LST-Prompt), which decomposes TSF into short-term and long-term forecasting subtasks. Each subtask guides LLMs with distinct forecasting rules and mechanisms, forming a Chain-of-Thought reasoning path for predictions.\n\u2022 We introduce TimeBreath to LSTPrompt, an innovative component that encourages LLMs to regularly revisit forecasting mechanisms, enabling leveraging different forecasting mechanisms for different time periods.\n\u2022 We evaluate LSTPrompt on multiple benchmark and concurrent datasets, demonstrating its effectiveness for zero-shot TSF tasks. We show its generalization ability to outperform non-zero-shot methods in specific scenarios."}, {"title": "2 Methodology", "content": "2.1 Problem Formulation and Motivation\nZero-shot TSF aims to predict future TS ${y}_{t+1}^{t+H}$ with a horizon window size $H$ based on a reference TS ${y}_{t-L}^{t}$, with lookback window size $L$, without prior exposure or training on the target series. Solving zero-shot TSF tasks with LLMs requires aligning TS data with natural language modalities to leverage remarkable generalization abilities and generate predictions based on the provided context.\nOne approach to align TS data with LLMs is to present TS data as text. Existing zero-shot TSF prompt strategies (Xue and Salim, 2023; Gruver et al., 2023) represent TS data as strings of numerical digits and treat TSF tasks as text-based next-token predictions. However, these strategies overlook the need for sophisticated forecasting mechanisms inherent in dynamic TS data. Without explicit instructions, existing strategies may yield inaccurate predictions with high uncertainty.\nTo address this, we propose LSTPrompt, tailored for zero-shot TSF tasks through prompting LLMs informatively. LSTPrompt comprises two components: (1) TimeDecomp, decomposing TSF tasks into subtasks for systematic reasoning, and (2) TimeBreath, facilitating periodic breaks to adapt forecasting strategies within the horizon window. We detail each module in the subsequent sections.\n2.2 TimeDecomp\nRather than directly prompting complex questions to LLMs, recent studies advocate decomposing inquiries into simpler, sequential steps (Wei et al., 2022; Kojima et al., 2022). This approach aids LLMs in constructing a coherent reasoning path. However, applying such chain-of-thought or step-by-step strategies to TSF tasks remains unexplored.\nTo address this, we introduce TimeDecomp, which breaks down TSF tasks into short-term and long-term forecasting subtasks. This is motivated by different forecasting mechanisms for short/long-term forecasting. Particularly, TimeDecomp prompts LLMs to partition horizon time steps into short-term and long-term accordingly. Then, it guides LLMs through each subtask, directing them to focus on specific aspects: short-term forecasting emphasizes trend changes and dynamic patterns, while long-term forecasting highlights statistical properties and periodic patterns. TimeDecomp's chain-of-thought process follows step-by-step cues: it prompts tasks with specific datasets, decomposes tasks into short-term and long-term sub-tasks, and guides LLMs to incorporate appropriate forecasting mechanisms and domain knowledge.\n2.3 TimeBreath\nIn addition to chain-of-thought prompting, recent studies emphasize the importance of incentivizing LLMs to follow step-by-step reasoning, especially when having numerous subtasks (Zhou et al., 2022b; Yang et al., 2023). To facilitate this, Yang et al. propose a strategy that introduces \"Take a deep breath\" before initiating step-by-step tasks.\nTSF tasks involve varying reasoning across different time steps and overly lengthy forecasting horizons can overwhelm LLMs' reasoning abilities. Inspired by the \"deep breath\" design, we introduce TimeBreath, which prompts LLMs to take \"rhythmic breaths\" during sequential reasoning for TSF. In the TSF task with $H$ time steps horizon, Time-Breath guides LLMs to rhythmically breathe every $k$ steps, where $k$ is a hyperparameter determining the breath frequency. The intuition of TimeBreath is to encourage LLMs to reassess forecasting mechanisms regularly, particularly for distant time steps that may require different reasonings. By taking breaks, TimeBreath helps LLMs avoid prior irrelevant inferences and fosters adaptive forecasting mechanisms to current forecasts.\nIn practice, the choice of $k$ significantly impacts LLMs' performance in zero-shot TSF tasks, as demonstrated in the sensitivity analysis provided in Appendix C. A straightforward approach is to align the frequency of breaks with the upper time scale. For example, setting $k = 5$ prompts weekly breaks for daily stock forecasting, while $k = 4$ encourages monthly breaks for weekly Influenza forecasting.\n2.4 LSTPrompt\nWe introduce LSTPrompt, which integrates TimeDecomp and TimeBreath to create the comprehensive prompt strategy. The prompt is straightforward: LSTPrompt first guides LLMs through the chain-of-thought steps outlined by TimeDecomp, then instructs them to take regular breaks using TimeBreath. A LSTPrompt demo is shown by Figure 1. We provide a detailed prompting example in Appendix B. LSTPrompt is designed for any TS datasets for zero-shot TSF tasks. It can be easily tailored to different scenarios by adjusting a single hyperparameter, $k$, as previously discussed."}, {"title": "3 Experiments", "content": "3.1 Benchmark Evaluation\nTo benchmark the performance of LSTPrompt, we use three common TSF benchmarks: Darts (Herzen et al., 2022), Monash (Godahewa et al., 2021), and Informer datasets (Zhou et al., 2021). While these datasets can potentially be used for training LLMs, evaluating LSTPrompt on these datasets allows fair comparisons within aligned settings, which strictly follows the established setup for zero-shot TSF tasks (Gruver et al., 2023) and are detailed in Appendix C. We use the SOTA prompting method LLMTime (Gruver et al., 2023) and a recent PTM TimesFM (Das et al., 2023) as zero-shot baselines. The results are shown in Table 1. We showcase visualized results in Appendix C.\nThe results highlight two main benefits of LST-Prompt: First, LSTPrompt achieves the best performance on 8 out of 12 benchmark datasets and the second-best performance on the remaining 4 among zero-shot methods. Notably, LSTPrompt always outperforms the SOTA prompt method LLMTime, while may slightly lag behind TimesFM, which is expected since TimesFM is a TSF-specific PTMs. Second, LSTPrompt can outperform best supervised results under certain scenarios. For instance, LSTPrompt achieves a 74.6% lower MAE compared to the best supervised result on the MilkProduction dataset. This improvement relies on the strong generalization ability of LLMs, which helps mitigate overfitting for supervised models.\n3.2 Concurrent Dataset Evaluation\nTo evaluate the true zero-shot ability of LSTPrompt, we conduct experiments over three concurrent datasets from different domains: influenza-like illness (ILI), Stock, and Weather (Detailed in Appendix C). These datasets ensure that the test data are after June 2023, while most LLMs are trained only up to 2022 (Achiam et al., 2023). Employing these datasets ensures the zero-shot property, even for GPT4. The experiment setup follows Benchmark Evaluations. We omit PromptCast (Xue and Salim, 2023), exclude TimesFM, and include another foundation time-series model, LPTM (Kamarthi and Prakash, 2023), for zero-shot baselines with explanations in Appendix C. We include supervised TSF models, including Informer (Zhou et al., 2021), Autoformer (Wu et al., 2021), FEDformer (Zhou et al., 2022a), and PatchTST (Nie et al., 2022), to show performance disparities between zero-shot methods and supervised models on TSF tasks. The results are shown in Table 2.\nThe results demonstrate that LSTPrompt consistently outperforms zero-shot baselines on all evaluations. Notably, LSTPrompt consistently outperforms best supervised results on Stock and Weather datasets. This is attributed to heavy distribution drifts on these datasets, which largely degrade the supervised models' performances. In contrast, benefiting from strong generalization abilities of LLMs and zero-shot properties, zero-shot methods mitigate the impacts of distribution drifts and achieve better performance than supervised models.\n3.3 Ablation Study\nTo understand the significance of various components of LSTPrompt, we conduct two ablation studies: (1) Analyzing the impact of employing different LLMs; (2) Analyzing the effects of TimeDecomp and TimeBreath. We conduct experiments with combinations of different LLMs and various ablated versions of LSTPrompt on the Stock dataset, with results visualized in Figure 2.\nPrompting Different LLMs. In prior experiments, we presented forecasting results based on the most suitable LLMs (e.g., GPT3.5-Turbo-Instruct for LLMTime and GPT4 for LSTPrompt). However, performance differences can arise among zero-shot TSF methods, including LSTPrompt, when evaluated across different LLMs. Thus, we investigate and interpret the potential impacts of utilizing GPT3.5-Turbo, GPT3.5-Turbo-Instruct, and GPT4 with LSTPrompt. The results indicate LSTPrompt coupled with GPT4.0 outperforms instances with GPT3.5-Turbo and GPT3.5 Turbo-Instruct. This finding aligns with expectation, as LSTPrompt prompts LLMs to follow the reasoning path through distinct short-term and long-term forecasting subtasks, each requiring different reasoning mechanisms, while GPT4 is known for its reasoning abilities compared to the remaining two.\nModule Effectiveness. To understand the significance of TimeDecomp and TimeBreath, we analyze performance discrepancies over three ablated versions of LSTPrompt: (1) Base, using standard prompts; (2) LSTPrompt\\TD, excluding TimeDecomp from LSTPrompt; (3) LSTPrompt\\TB, excluding TimeBreath from LSTPrompt. We include the state-of-the-art Chain-of-Thought method (Yang et al., 2023) (referred to as 'CoT') to highlight performance differences with the SOTA prompt strategy for general tasks.\nThe results demonstrate the effectiveness of both TimeDecomp and TimeBreath. Incorporating TimeDecomp and TimeBreath reduces the average NMAE by 26.8% and 34.1%, respectively, compared to Base prompts. Employing both modules enhances average performance by 46.7% than Base prompts. Moreover, the sole utilization of either TimeDecomp or TimeBreath demonstrates certain advantages in forecasting accuracy over the best CoT method, highlighting the necessity of designing tailored prompts for TSF tasks."}, {"title": "4 Conclusion", "content": "In this paper, we introduce LSTPrompt, a novel prompt paradigm for zero-shot TSF tasks through prompting LLMs. LSTPrompt enables LLMs to achieve accurate zero-shot TSF tasks through two innovative modules: TimeDecomp, which decomposes zero-shot TSF tasks into a series of chain-of-thought subtasks, and TimeBreath, which en-"}, {"title": "A Additional Related Works", "content": "Time-Series Forecasting. Traditional time series methods approach forecasting from a statistical standpoint, treating it as standard regression problems with time-varying parameters (Nadaraya, 1964; Williams and Rasmussen, 1995; Zhang, 2003). Recent advancements in deep learning have led to significant breakthroughs in this field, exemplified by deep models like LSTNet and N-BEATS (Lai et al., 2018; Oreshkin et al., 2019). Many state-of-the-art deep learning methods, such as Informer, Autoformer, PatchTST, and CA-Mul (Zhou et al., 2021; Wu et al., 2021; Nie et al., 2022; Kamarthi et al., 2022), build upon the success of self-attention mechanisms, popularized by transformer-based architectures (Vaswani et al., 2017). These transformer-based models excel at capturing long-range dependencies, surpassing the capabilities of traditional Recurrent Neural Network (RNN) models, owing to their effective use of self-attention mechanisms.\nLarge Language Models. The augmentation of language model parameters and training data size has been shown to enhance generalization ability (Brown et al., 2020). Consequently, researchers have developed Large Language Models (LLMs) like GPT (Brown et al., 2020; Achiam et al., 2023) and Llama (Touvron et al., 2023). These models excel at identifying patterns in prompts and extrapolating them through next-token prediction, achieving remarkable success in few-shot or zero-shot generalization and in-context learning. Beyond natural language tasks, LLMs exhibit effectiveness in transfer learning across diverse modalities, including images (Lu et al., 2021), audio (Ghosal et al., 2023), tabular data (Hegselmann et al., 2023), and time-series data (Zhou et al., 2023). These accomplishments underscore the importance of aligning modalities appropriately to enable LLMs to comprehend tokenized patterns across different domains beyond traditional language processing tasks.\nLarge Models for Time-Series Forecasting. In addition to the success of large models in language tasks, researchers in the field of time-series forecasting (TSF) have pursued the development of large models from two main perspectives: First, they train Pre-Trained Time-Series Models from scratch (Garza and Mergenthaler-Canseco, 2023; Das et al., 2023; Kamarthi and Prakash, 2023; Yeh et al., 2023), utilizing extensive time-series datasets and tailoring them specifically for TSF tasks. Alternatively, researchers harness the generalization capabilities of Large Language Models (LLMs) by aligning time-series data with language modalities through techniques such as reprogramming (Jin et al., 2023; Chang et al., 2023; Zhou et al., 2023) or prompting (Gruver et al., 2023; Xue and Salim, 2023). To better understand the similarities and dif-"}, {"title": "B Prompt Details", "content": "Below, we introduce a template prompt for LST-Prompt, designed to be adaptable to various time-series datasets for zero-shot time-series forecasting tasks. The template is outlined as follows:\n1 f\"Please continue the following input sequence by addressing the task of forecasting {dataname}. You should break down the task into short-term and long-term predictions, following a three-step plan. First, adaptively and reasonably identify the ranges for short-term and long-term predictions. Then, design distinct and correct forecasting mechanisms for both short-term and long-term prediction tasks. For short-term predictions, focus on trends and the last few steps of the input sequence. For long-term predictions, emphasize cyclical patterns and statistical properties of the entire input sequence. You may further optimize the forecasting mechanisms based on your observations and domain knowledge. Finally, correctly implement the forecasting mechanisms, completing predictions one-time step at a time. 2 Remember to take a deep breath after every breath_steps} time steps of prediction. The input sequence is as follows:\\n\""}, {"title": "C Additional Experiment Details", "content": "Experiment Setup. Following the established setups in LLMTime and with the consideration of evaluating costs, we limit our focus to univariate time series forecasting tasks. However, LST-Prompt can readily extend to the multivariate forecasting domain by employing multiple univariate forecasting techniques (Gruver et al., 2023; Lim and Zohren, 2021). We strictly followed LLMTime's data-splitting method for benchmark datasets, where the test set comprises the last 20% of each time series.\nIn addition to well-known benchmark datasets such as Darts, Monash, and ETT, our zero-shot evaluations encompass three concurrent datasets: ILI, Stock, and Weather. This selection ensures that the test data have never been exposed to LLMs training. All these datasets are publicly accessible. We use data after June 2023 for testing, thereby guaranteeing that GPT-3.5 and GPT-4 models have not been trained on these sets. Further details on these datasets are provided below:\n\u2022 ILI\u00b2: The ILI dataset provides the reported influenza-like illness patients with age divisions. The dataset covers from 2002 to 2023. The forecasting target is the weekly number of ILI patients.\n\u2022 Stock\u00b3: The Stock dataset provides daily historical data of Alphabet Inc. (GOOG). The Stock dataset set has 7 columns, including the stock's opening price, closing price, highest price of the day, etc. The dataset covers from 2013 to 2024 (Jan). The forecasting target is the daily opening price.\n\u2022 Weather4: The Weather dataset provides historical weather record of Chicago. This dataset set has 10 columns, including date, temperature, precipitation, humidity, wind speed, and atmospheric pressure. The dataset covers from 2021 to 2023. The forecasting target is the daily average temperature.\nBaseline. In supervised baselines, we adopt various models depending on the benchmark's offi-\nFor zero-shot baselines, we categorize methods into pre-trained Time-Series Foundation Models (PTMs) and prompting methods. In benchmark evaluations, we utilize TimesFM (Das et al., 2023) for PTMs, as it asserts not being trained on these datasets, while LPTM (Kamarthi and Prakash, 2023) does. Conversely, for concurrent dataset evaluations, we employ LPTM, as it is open-source compared to TimesFM. Reprogramming methods are omitted, such as TimeLLM (Jin et al., 2023) and LLM4TS (Chang et al., 2023), due to their inapplicability to our zero-shot setting.\nFor prompting methods, we compare LLM-Time (Gruver et al., 2023) with our proposed LST-Prompt. Promptcast (Xue and Salim, 2023) is omitted, as LLMTime consistently outperforms it, and LSTPrompt demonstrates uniformly better performance across all evaluations than LLMTime.\nEvaluation Metric. Following the established setups, we evaluate the Mean Absolute Error (MAE) on Darts and Monash datasets between predictions and raw target sequences. For ETT, ILI, Stock, and Weather datasets, we evaluate the MAE based on the normalized predictions and target sequences according to the mean and variance of the training data. The formulation of MAE = $\\frac{1}{n} \\sum_{i=1}^n |Y_t - \\hat{Y_t}|$.\nHyperparameter Sensitivity Study. As previously mentioned in Section 2, we conduct experiments using LSTPrompt on the Stock dataset with varying values of breath frequency k. The results are shown in Fiugure 3. Note that k = 0 denotes LSTPrompt without employing TimeBreath.\nThe results suggest that setting k = 5, enabling LSTPrompt to breathe weekly in forecasting the stock prices, achieves the best performance compared to other breath frequencies. This optimal frequency aligns with the Stock dataset's structure, which includes daily stock prices for 5 weekdays. Intuitively, setting k = 5 encourages LSTPrompt to reassess its reasoning and forecasting strategy on a weekly basis, fitting well with the inherent weekly cycles in stock data. By appropriately adjusting the breath frequency in TimeBreath, LSTPrompt can dynamically infer patterns while effectively adapting to the data's periodic nature, leading to more accurate forecasts."}, {"title": "D Limitation Discussion", "content": "While LSTPrompt has demonstrated effectiveness in zero-shot TSF tasks by employing simple prompts for LLMs, its limitations should be acknowledged from two perspectives. First, the interpretability of LSTPrompt may be compromised. The evaluation of LSTPrompt heavily relies on existing LLMs, the mechanisms and response behaviors of which are currently challenging to interpret. Consequently, LSTPrompt may suffer from reduced interpretability due to our limited understanding of LLMs. Second, incorporating additional instructions in the prompts, such as the names and properties of time-series datasets, could potentially introduce information leaks that are exploited by the LLMs. We advocate for further research within the safe AI community to investigate the trustworthiness of LLMs, ensuring that LST-Prompt can be deployed without concerns regarding information leakage issues."}]}