{"title": "Learning Generalizable Policy for Obstacle-Aware Autonomous Drone Racing", "authors": ["Yueqian Liu"], "abstract": "Autonomous drone racing has gained attention for its potential to push the boundaries of drone navigation technologies. While much of the existing research focuses on racing in obstacle-free environments, few studies have addressed the complexities of obstacle-aware racing, and approaches presented in these studies often suffer from overfitting, with learned policies generalizing poorly to new environments. This work addresses the challenge of developing a generalizable obstacle-aware drone racing policy using deep reinforcement learning. We propose applying domain randomization on racing tracks and obstacle configurations before every rollout, combined with parallel experience collection in randomized environments to achieve the goal. The proposed randomization strategy is shown to be effective through simulated experiments where drones reach speeds of up to 70 km/h, racing in unseen cluttered environments. This study serves as a stepping stone toward learning robust policies for obstacle-aware drone racing and general-purpose drone navigation in cluttered environments. Code is available at https://github.com/ErcBunny/IsaacGymEnvs.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous drone navigation has emerged as a critical area of research, driven by the growing demand for drones in industries such as delivery, inspection, and emergency response. Drone racing, with its emphasis on minimum-time navigation, has become a benchmark task for testing advanced autonomous systems aiming to navigate at high speeds while avoiding obstacles in partially or fully unknown environments. Drone racing originally began as a competitive sport where human pilots control agile drones via radio to fly through a racing track as fast as possible while avoiding potentially present obstacles. This requires precision, quick reflexes, and expert navigation skills. In autonomous drone racing, human pilots are replaced by algorithms and artificial intelligence (AI). This introduces the challenge for algorithms and AI of matching human-level performance and adaptability.\nThere have been several global autonomous drone racing events, including the 2016-2019 IROS Autonomous Drone Racing (ADR) competitions [1], [2], the 2019 AlphaPilot Challenge [3], [4], the 2019 NeurIPS Game of Drones [5], and the 2022-2023 DJI RMUA UAV Challenges [6], [7]. The tracks in early competitions, such as the IROS ADR, AlphaPilot, and Game of Drones, are situated in less cluttered spaces, allowing drones to complete the tracks without considering obstacle avoidance. However, for tracks in cluttered environments, such as those in the more recent DJI RMUA Challenges, the absence of obstacle awareness could cause crashing. Additionally, in human-piloted drone racing, such as the Drone Racing League competitions, and in drone racing video games, there are plenty of tracks that require obstacle avoidance.\nAlthough autonomous drone racing has received significant attention, much of the research has been limited to obstacle-free scenarios [8]. Obstacle-free scenarios do not reflect the complexities encountered in real-world tasks where obstacle avoidance is necessary. Recognizing this, researchers have been exploring ways to integrate drone racing with obstacle avoidance through various approaches. Path-planning and optimization-based approaches can achieve short lap times [9] and strike a balance between lap times and computational efficiency [6], [10], [11], but rely on carefully designed algorithms and may experience performance degradation when model mismatches occur. Current learning-based approaches [12], [13] leverage reinforcement learning (RL) and imitation learning (IL) to train neural policies capable of making low-latency decisions that result in aggressive and collision-free trajectories. However, these policies do not generalize well to new racing tracks or different obstacle configurations.\nThis paper aims to enhance the generalization ability of learned policies. Specifically, the goal is to develop a single policy capable of navigating a quadcopter through various racing tracks with obstacles, without requiring additional tuning after training. Drawing inspiration from works on learning drone navigation in cluttered environments [14], [15] and generalizable obstacle-free drone racing [16], which all involve training the policy in multiple randomized environments, we propose applying the same strategy, domain randomization [17] over racing tracks, to expose the agent to a diverse set of environments. This allows the policy to learn the underlying navigation \"skills\u201d while not relying on unique observations associated with one or a few training environments. Simulated experiments verify that the resulting policy can indeed generalize to unseen racing tracks while avoiding obstacles in unseen sizes and shapes. In summary, the main contributions of this study are:\n\u2022 We verify the effectiveness of applying domain randomization to encourage the learning of generalizable skills.\n\u2022 We present the first generalizable neural policy for the obstacle-aware drone racing task, where the policy directly maps observations to low-level commands.\n\u2022 We open-source tools and reusable modules to facilitate research and development in both obstacle-free and obstacle-aware drone racing."}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. Obstacle-Free Autonomous Drone Racing", "content": "Optimization-based methods have been widely applied to the task of drone racing. For static obstacle-free racing tracks, time-optimal trajectories passing through gates' center waypoints can be generated using optimization with Complementary Progress Constraint (CPC) [18]. However, this approach is computationally expensive and struggles to adapt to changing track layouts. To reduce computational overhead, Model Predictive Contouring Control (MPCC) is introduced [19]. MPCC has been further extended to include an online reference path generation module to adapt to dynamic tracks and handle external disturbances [20]. A more recent study [21] demonstrates that lap times can be further reduced by exploiting the spatial potential of the gates.\nReinforcement Learning has also emerged as a promising approach for autonomous drone racing. Near-time-optimal agile flight can be achieved through state-based RL [16]. Although the learned policy results in slightly longer lap times than the CPC method, it handles variations in gate poses and generalizes to unseen tracks. Furthermore, state-based policies can serve as teacher policies within the IL framework, enabling the training of purely vision-based student policies [22]. In a follow-up study [23], the student policy is further fine-tuned using RL. These two studies show the feasibility of high-speed agile flight using AI with the same input-output modalities as human pilots.\nReinforcement learning based approaches offer several advantages over optimization-based methods. These include improved lap times and higher success rates during real-world flights, where unmodeled effects and disturbances are non-negligible [24]. However, deploying policies in real-world scenarios is challenging, requiring closing the sim-to-real gap and careful system engineering. The Swift system [25] demonstrates that, by bridging the gap via fine-tuning using data-driven residual models, AI systems powered by RL policies can achieve performance on par with human champions."}, {"title": "B. Obstacle-Aware Autonomous Drone Racing", "content": "For the task of obstacle-Aware autonomous drone racing, leveraging optimization and planning, several methods have proven effective. The teach-and-repeat framework is widely used in autonomous robot missions, and has been applied to drone racing [10]. This framework enables the drone to fly through the track while avoiding previously unseen and dynamic obstacles. In static environments, Fast-Racing [11] provides a polynomial trajectory baseline for obstacle-aware autonomous drone racing. The winning solution of the 2022 DJI RMUA UAV Challenge [6] also follows a polynomial-based trajectory but incorporates an additional online re-planning module to avoid dynamic obstacles and pass through moving gates. Moreover, Penick et al. [9] offer a sampling-based baseline aimed at finding time-optimal trajectories in cluttered environments, though this method struggles to scale with increasing environment complexity.\nWhile RL-based methods have shown great promise in obstacle-free autonomous drone racing, achieving better lap times, disturbance rejection, and less compute latency, their application to obstacle-aware racing remains relatively sparse. To the best of our knowledge, only two studies [12], [13] have addressed this challenging task. Furthermore, they all focus on completing a single predefined racing track in minimum time, not considering generalizing to unseen scenarios. Realizing this gap, we aim to explore methods that enhance policy generalization in obstacle-aware drone racing."}, {"title": "C. Vision-Involved Navigation via Deep RL", "content": "In the aforementioned works addressing obstacle-aware drone racing with RL, the policies fail to generalize to environments different from the ones they were trained on. To overcome this limitation, research on learning general-purpose navigation offers valuable insights and guidance.\nNear-perfect discrete-action indoor navigation for ground robots has been demonstrated with DD-PPO [26], in which training occurs across multiple indoor scenes to enhance generalization. The agent utilizes a policy network comprising a Convolutional Neural Network (CNN) as the encoder and a Long Short-Term Memory (LSTM) network. At first, this network is optimized as a whole using RL, which is inefficient. Follow-up works [27], [28] show that using auxiliary tasks, such as predicting depth, inverse dynamics, and remaining distance to target, results in quicker policy convergence and better overall performance.\nBesides using auxiliary tasks, modular learning is another approach to achieving efficient learning for vision-involved navigation tasks. Hoeller et al. [29] propose a modular learning framework for training a quadruped robot to navigate in cluttered dynamic environments. Here network modules are learned separately: once an upstream module is learned, it is frozen while downstream modules are optimized. MAVRL [14] and Kulkarni et al. [15] also adopt a similar framework for drone navigation in clutter. All these methods utilize randomization of the training environments to promote generalization, which directly inspires our core idea for learning a generalizable policy in obstacle-aware drone racing."}, {"title": "III. METHODOLOGY", "content": ""}, {"title": "A. Drone Model and Waypoints", "content": "The drone is modeled as a rigid body with mass $m$ and moment of inertia $J$. With the body frame attached to the center of gravity, the equations of dynamics can be written as:\n\n$p_W = v_W$\n$\\dot{q}_{WB} = \\frac{1}{2} q_{WB} \\otimes [0 \\quad \\omega_B]^T$\n$\\dot{v}_W = g_W + R_{WB}(f_a + f_d)$\n$J\\dot{\\omega}_B = J^{-1}(\\tau_a + \\tau_d - \\omega_B \\times J \\omega_B)$\n$\\Omega = k_r(\\Omega_s - \\Omega)$ (1)\n\nwhere $W, B$ denote the world frame and the drone body frame, and $p_W, q_{WB}, v_W, \\omega_B, g_W$ represent drone position, attitude quaternion, linear velocity, angular velocity, and gravitational acceleration, respectively. Rotation matrix of drone attitude is $R_{WB}$. Actuator wrench, or \u201cforce and torque\", $(f_a, \\tau_a)$ and aerodynamic drag wrench $(f_d, \\tau_d)$ make up the total wrench acting on the rigid body. Rotor spinning dynamics is considered as a first-order lag model: the derivative of rotor angular velocities in rounds per second (RPM) $\\dot{\\Omega}$ is the product of the rotor constant $k_r$ and the difference between steady-state RPM $\\Omega_s$ and current RPM $\\Omega$.\nFrom rotor angular velocities and accelerations we can calculate the actuator wrenches:\n\n$f_a = \\sum_i f_p(\\Omega_i)$\n$\\tau_a = \\sum_i \\tau_p(\\Omega_i) + r_i \\times f_p(i) + S_i r_i$ (2)\n\nwhere $f_p(\\Omega_i)$ and $\\tau_p(\\Omega_i)$ represent the thrust force and torque generated by the $i$-th spinning propeller, respectively. They are calculated using polynomial models derived from the motor manufacturer's data. Here, $r_i$ is the displacement of the $i$-th rotor relative to the drone's body frame origin, $J_r$ denotes the moment of inertia of the rotor, which includes the propeller and the spinning parts of the motor, and $S_i$ indicates the rotational direction of the $i$-th rotor.\nThe aerodynamic drag depends on the linear velocity, $v_B = R_{WB} v_W$, and the angular velocity, $\\omega_B$, in the body frame:\n\n$f_d = A_t(\\rho v_W \\|v_W\\|)$\n$\\tau_d = A_r(C_0 \\omega_B + C_1 v_W + C_2 \\omega_B^3 + C_3 v_W \\omega_B)$ (3)\n\nwhere $\\rho$ is the air density, $A_t$ and $A_r$ are the effective areas responsible for generating translational and rotational drag, respectively. Coefficients $C_i$ for $i$ from 0 to 3 are adjustable coefficients in the polynomial drag model.\nThe steady-state angular velocity is determined by the motor commands $u_m$ using a polynomial model fitted to the motor manufacturer's data. Simulating how human operators control racing drones, we employ an angular velocity controller to translate control commands $a \\in [-1,1]^4$ to motor commands $u_m \\in [0,1]^4$ of the drone. Vector $a$ contains 3 channels for body rates and 1 channel for the throttle, or collective thrust level. The angular velocity controller is derived from Betaflight [30] and is responsible for mapping control commands to desired angular velocity, running closed-loop control, and allocating the control to motor commands.\""}, {"title": "B. Task Formulation", "content": "We formulate obstacle-aware autonomous drone racing as a partially observable Markov decision process (POMDP) [31]. POMDP models the agent-environment interaction and internal dynamics of the environment. The agent is an AI system that decides on the action to take based on observations from the environment. The environment is everything else that takes the agent's actions, updates environment states, computes rewards, and finally outputs the observations, closing the interaction loop. Actions are taken based on the policy $\\pi$ that maps observations to actions, with the transition model of states, a trajectory $T$ can be produced. The goal is to find the policy that maximizes the expectation of total discounted reward:\n$\\max_{\\pi} E \\sum_{t=0}^{\\infty} \\gamma^t r(t)$ (4)\nwhere $\\gamma \\in [0, 1)$ stands for the discount factor, and $r(t)$ denotes the reward as a function of time $t$.\n1) States: States include every piece of necessary information to define the environment configuration. This may include drone rigid body states, camera transform, internal states of the actuator model and controllers, gate poses and sizes, obstacle shapes, and poses, etc.\n2) Action: As discussed in the previous drone model section, we use the control commands denoted by $a$ as the action to simulate human operators' control commands to a radio-controlled racing drone.\n3) Transition: The transition of states of our environment is deterministic and only updates states associated with the drone model. Action $a$ is turned into actuator wrenches through the angular velocity controller and Equation (2). Together with the drag wrenches in Equation (3), drone states $p_W, q_{WB}, v_W,$ and $\\omega_B$ can be updated using Equation (1) with an integrator or a physics engine. States of obstacles and gates are fixed within an episode of the process.\n4) Reward Function: The reward function is a weighted sum of reward terms including the progress reward $r_{prog}$, perception reward $r_{prec}$, command reward $r_{cmd}$, collision reward $r_{col}$, guidance reward $r_{guid}$, waypoint passing reward $r_{wp}$, timeout reward $r_{time}$, and finally the linear velocity reward $r_{vel}$. By representing weights collectively as a vector $\\Lambda$, we can write the reward as:\n$r = [r_{prog} \\quad r_{prec} \\quad r_{cmd} \\quad r_{col} \\quad r_{guid} \\quad r_{wp} \\quad r_{time} \\quad r_{vel}] \\Lambda^T.$ (5)\nTerms $r_{prog}, r_{prec}, r_{cmd}$, and $r_{col}$ are formulated as in Swift [25]. The guidance reward is extended based on the safety reward seen in [16]. The remaining ones are additional terms proposed in this study.\nTo calculate the guidance reward, we first need to transform the drone position from the world frame to the target waypoint frame. Let $p_g = [x \\quad y \\quad z]^T$ denote drone position in the waypoint frame, the guidance reward is $r_{guid} = -f_2(x) \\cdot g(x, y, z)$, with $f(x) = \\max(1 - \\text{sgn}(x) x/k_0, 0)$ and $g(x, y, z)$ expanded to:\n$g(x, y, z) = \\begin{cases} k_1 \\exp(\\frac{-y^2 + z^2}{\\nu^2}), & x > 0 \\\\ (1 - \\exp(\\frac{-y^2 + z^2}{\\nu^2})), & x \\le 0 \\end{cases}$ (6)\nwhere $\\nu$ is further expanded to:\n$\\nu = \\frac{k_2}{(1 + f^2(x))} \\frac{y^2 + z^2}{(z/h_{wp})^2 + (y/w_{wp})^2}$ (7)\nif $y^2 + z^2 \\neq 0$. Otherwise $\\nu = k_2 (1 + f^2(x))$. Here $k_i$ for $i$ from 0 to 2 are scalar parameters, and $k_2$ is different for cases $x > 0$ and $x < 0$, i.e. for different sides of the waypoint. The formulation adapts the original \"safety reward\u201d to rectangular waypoints and additionally penalizes the behavior of approaching the gate from the wrong side.\nThe waypoint passing reward $r_{wp}$ and the timeout reward $r_{time}$ are sparse and only become non-zero at specific steps: $r_{wp}$ turns to positive if the drone has just passed through a waypoint, and $r_{time}$ turns to negative if the drone has not crossed the final waypoint within a time limit.\nFinally, we use the linear velocity reward $r_{vel}$ to encourage forward flight, which is beneficial for both making progress and obstacle avoidance with a limited camera field of view. It penalizes lateral and backwards velocity in the body frame $v_B = [v_x \\quad v_y \\quad v_z]^T$ using negative parameters $k_3$ and $k_4$:\n$r_{vel} = k_3 v_x^2 + k_4 (\\min(v_x, 0))^2$. (8)\n5) Observations: We assume all observations are noise-free and deterministic. At time $t$, the observations include: the depth image $d_t \\in [0,1]^{270 \\times 480}$, drone states $s_t \\in [-1,1]^{18}$, the last action $a_{t-1} \\in [-1,1]^4$, and waypoint information of the next two target waypoints $w_t \\in [-1,1]^{34}$.\nThe depth image is produced by a depth camera using the pinhole model. We set resolutions to 270\u00d7480 and its horizontal FOV to 90 degrees. The transform from the ground-truth depth $d_{gt}$ to the observed depth $d$ is:\n$d = \\min (d_{gt} / d_{max}, 1)$, (9)\nwhere $d_{max}$ denotes the maximum sensing range.\nThe drone states vector is defined as:\n$s = [ \\frac{(p_W - p_{W0})}{p_{max}} \\quad x_B \\quad y_B \\quad z_B \\quad \\frac{v_W}{v_{max}} \\quad \\frac{\\omega_B}{\\omega_{max}} ]^T$ (10)\nwhere $p_{W0}$ is the initial drone position; $x_B, y_B,$ and $z_B$ are column vectors of the rotation matrix $R_{WB}$. Manually adjustable parameters for maximum sensing ranges for the position, linear velocity, and angular velocity are denoted by $p_{max}, v_{max},$ and $\\omega_{max}$, respectively. This vector is further clamped to [-1,1] before returned.\nWe include information about two future waypoints based on the result of the gate observation experiment in [16]: including information about two future gates can improve success rate and lap times. The information vector of one waypoint, indexed $i$, is defined as:\n$w_i = [s_c \\quad \\min(\\frac{l^T}{l_{max}}, 1) \\quad corners ]$, (11)\nwith $s_c$ being the cosine similarity between vector $p_{wp_i} - p_{WB}$ and the x-axis of waypoint $i$, $l$ being the vector containing lengths of vectors from the origin of the drone body frame to 4 waypoint corners, $l_{max}$ being the maximum allowed value of these lengths, and $corners$ denoting concatenated unit vectors from the drone to the corners. The dimension of $w_i$ adds up to 17, so the dimension of $w_t$, containing $w_0$ and $w_1$, is 34.\n6) Policy: We use a neural network to represent the policy. Denoting the parameters of the neural network by $\\theta$, we can express the policy function as:\n$a_t = \\pi_{\\theta}(d_t, s_t, w_t, a_{t-1})$. (12)\nThe neural network consists of an image encoder module that encodes an image $d_t$ to a 64-dimensional latent vector $z_t$ and a multi-layer perceptron (MLP) module that maps the concatenated 120-dimensional vector $[z_t \\quad s_t \\quad w_t \\quad a_{t-1}]^T$ to action $a_t$.\nWe employ a pre-trained Deep Collision Encoder (DCE) [32] as the image encoder and freeze its weights during"}, {"title": "C. Policy Training", "content": "Given that our policy is represented by a neural network, our goal boils down to finding the optimal parameters $\\theta$ that maximizes the expectation of accumulated rewards in Equation (4). We use the Proximal Policy Optimization (PPO) [33] reinforcement learning algorithm for this purpose. Additionally, we explore the technique of domain randomization to enable generalization to unseen environments, hoping that with enough variability encountered during training, an unseen environment would appear as just another variation, where the policy has required knowledge to finish the track. To achieve this, we have designed a waypoint generator, a random obstacle manager, and multiple training strategies.\n1) Waypoint Generator and Obstacle Manager: We consider a racing track the combination of waypoints and obstacles for the obstacle-aware drone racing task. We use the waypoint generator to generate random waypoints and use the obstacle manager to put obstacles at places that effectively block the flight paths between waypoints.\nWe use 5 values $(\\psi_{wp_{ij}}, \\theta_{wp_{ij}}, r_{wp_{ij}}, \\phi_{wp_{ij}}, \\Upsilon_{wp_{ij}})$ to parameterize relative pose between waypoint $i$ and $j$, as shown in Figure 3(a). Given the pose of waypoint $i$ as position vector and rotation matrix $(p_{wp_i}, R_{wp_i})$, the pose of waypoint $j$ can be calculated using:\n$p_{wp_j} = r_{wp_{ij}} R_y(\\Upsilon_{wp_i}) R_z(\\theta_{wp_{ij}}) R_{wp_i} [1 \\quad 0 \\quad 0]^T + p_{wp_i}$\n$R_{wp_j} = R_y(\\psi_{wp_{ij}}) R_x(\\theta_{wp_{ij}}) R_y(\\phi_{wp_i}) R_z(\\theta_{wp_{ij}}) R_{wp_i}$ (13)\nAlthough there are only 5 degrees of freedom, this parameterization allows for enough room for randomization and intuitive adjustments of the track's difficulties.\nWaypoints are generated procedurally. Firstly, the Initial waypoint's roll, pitch, and yaw angles are sampled uniformly within defined bounds, and the position is set to an arbitrary value. Secondly, for subsequent waypoints, we sample the relative pose parameters uniformly within defined bounds and calculate their poses till the final waypoint using Equation (13). Thirdly, parameters $(w_{wp}, h_{wp}, g_{wp})$ are also sampled uniformly within defined ranges for all waypoints. Lastly, we offset all waypoints' positions to fit the track within environment boundaries.\nWe observe that uniformly distributing obstacles in $\\mathbb{R}^3$, as seen in [15], is not suitable for significantly larger environments. Uniformly distributing obstacles requires an excessive number of obstacles in the environments, which increases computational overhead and hurts simulation performance. Our obstacle manager allows for challenging the agent on obstacle avoidance using a small number of obstacles, by strategically sampling obstacle poses based on the generated waypoints. The manager supports uniformly distributing tree-like obstacles along line segments connecting waypoint centers, placing wall-like cuboids between waypoints, and finally making obstacles of various shapes orbit waypoints. By anchoring obstacles to the racing track, we achieve efficient obstacle management. Furthermore, the difficulty level can controlled by specifying the number of obstacles in each group and parameters defining the shapes of the obstacles.\n2) Training on Track Segments: Since full-length tracks are the combination of segments of shorter lengths, we believe that training on short track segments will allow generalization to full-length tracks, while reducing computational overhead for the waypoint generator, obstacle manager, and the physics engine. Plus, with shorter track lengths, we can fit full episodes into shorter rollout horizons, which increases policy update frequency and potentially reduces the wall-clock time required for the policy to converge. We generate waypoints and manage obstacles for short track segments containing only 4 way-points. The task is to fly from the initial position near waypoint 0, pass through waypoint 1, and finally finish the episode by passing through waypoint 2. Waypoint 3 is generated to keep the dimensions of the waypoint information vector $w_t$ consistent.\n3) Environment Randomization: Combining the waypoint generator and obstacle manager, we can create an infinite amount of random environments for training. Aiming to provide the agent with diverse experience, our implementation of the waypoint generator and the obstacle manager supports vectorized environments, that is, for a single round of data collection (rollout), multiple different environments are randomly created using these tools. By incorporating experience collected in different environments into a single rollout dataset, we avoid overfitting the policy to a single environment from the ground up. For a small number of parallel environments, e.g. a few hundred, creating only a single set of random environments and using them for all rollouts is not enough, as this makes the policy learn an average strategy that maximizes the mean total reward for this specific set of environments. To solve this problem, we generate a new set of random environments for every single rollout, which further diversifies the total experience dataset.\n4) Random Agent Initialization: The camera transform in the drone body frame and drone states are randomly initialized upon agent resets to make the policy more robust. This strategy can also be seen as an application of domain randomization. Specifically, we randomize the camera position in the yz-plane of the drone body frame, and the camera tilt angle. For drone states, we initialize the position within the obstacle-free zone of the initial waypoint to avoid spawning the drone into obstacles, other states such as linear and angular velocities, as well as the initial actions, are uniformly sampled within defined ranges. The initial attitude is either the same as the initial waypoint's attitude or the resulting attitude of firstly aligning the body x-axis with vector $p_{wp_1} - p_{wp_0}$, and secondly rotating about the vector for a random angle."}, {"title": "D. Implementation Details", "content": "We implement vectorized environments based on the Isaac Gym Simulator [34], which supports parallel physics simulation on GPUs and offers relatively high image rendering speed. To work with Isaac Gym, our code is highly optimized using PyTorch-based vectorized operations. The physics simulation frequency and angular velocity control frequency are set to 250 Hz, while the camera rendering frequency and policy control frequency are set to 25 Hz, that is, one environment step corresponds to 10 closed-loop physics steps. With this setting, we achieve about 3,000 total environment steps per second with camera sensors enabled. This speed is recorded on a consumer-grade desktop PC equipped with an Intel i9-13900K CPU and an Nvidia RTX 4090 GPU running the Ubuntu 22.04 operating system.\nWe code the training loop based on the actor-critic PPO agent in RL-Games [36]. Our domain randomization happens during environment resets, so we modify the original training loop to include calling environment reset before running rollout in every training iteration. In total, we train the policy for 1,000 iterations, which corresponds to collecting experiences in 512,000 different environments for about 520 million environment steps. It takes about 50 wall-clock hours to finish all iterations."}, {"title": "IV. EXPERIMENTS", "content": ""}, {"title": "A. Demonstrating Generalizable Obstacle-Aware Racing", "content": "We first demonstrate the policy's ability to generalize to unseen waypoint placement and the number of waypoints. Four full-length tracks are designed for this experiment: \u201cKebab", "Circle\", \u201cTurns\", and \u201cWavy Eight\". The \\\"Kebab\\\" features 5 waypoints roughly in a row, representing scenarios encountered during training. The \\\"Circle\\\" consists of 9 waypoints uniformly distributed on a circle with a radius of 15 meters. The \\\"Turns\\\" has 11 waypoints positioned on a big letter \\\"S\\\". Then in the \\\"Wavy Eight\\\", waypoints form a figure of eight, with their $z$ coordinates different. Since the drone's position is part of the observation, all waypoints in the test tracks are positioned within the same environment boundaries as those used for training.\nDrones are randomly initialized as in training, but we set larger ranges for initial attitude, body-frame velocities, and commands, to further \u201cstress": "he policy. Under this setup, we roll out 100 episodes for each track, log the trajectories, and calculate the success rate and the average linear speed. On track \"Kebab\", the most similar to the training scenarios, the policy achieves the highest success rate. As the track gets more twists and includes more sharp turns that are not present in the training set, the success rate drops. Despite the performance drop, this experiment confirms that the policy generalizes to full-length racing tracks and completely unseen relative waypoint poses."}, {"title": "B. Benchmarking Policy by Varying Scene Complexity", "content": "To further evaluate the capability of our policy, we assess its performance on a series of randomized tracks with varying levels of complexity. Level 1 represents the difficulty level of training environments: in each environment there are 12 wall-like obstacles and 4 tree-like obstacles, relative waypoint pose parameters $(\\psi_{wp_{ij}}, \\theta_{wp_{ij}}, r_{wp_{ij}}, \\phi_{wp_{ij}}, \\Upsilon_{wp_{ij}})$ are set between (-0.3, -0.3,6,0,0) and (0.3, 0.3, 18, 3.14, 0.2). Level 2 doubles the amount of obstacles and leaves waypoint parameters unchanged. Level 3 includes 60 additional obstacles orbiting waypoint 0 and waypoint 1. Finally, level 4 sets the bounds of the waypoint parameters to (-1,-0.4,6,0,0) and (1,0.4, 18, 3.14,0.3) on top of other settings in level 3.\nWe generate 100 random tracks per difficulty level and roll out 10 episodes per track, resulting in a total of 1000 episodes per level. Screenshots of environments in Isaac Gym, sample environments, and resulting trajectories are illustrated in Figure 9. For each trajectory, we log its termination mode, safety margin, mean and maximum values of average commands of all motors, linear speed, and angular speed. Then we can calculate the success rates and plot the data distributions of other metrics over all trajectories for all difficulty levels,\nThe success rate starts at 0.9 for level 1, consistent with the training results shown in Figure 5, but decreases as track difficulty increases, reaching around 0.4 by level 4. A similar downward trend is observed for the safety margin, indicating that the drone comes closer to obstacles on harder tracks. In terms of control effort, the maximum values of motor commands remain consistent across difficulty levels, suggesting that the policy pushes the drone to its control limits whenever possible. The mean values of commands show a clear upward trend, implying that navigating more complex tracks requires increasingly aggressive control inputs. This is also reflected in the angular speed, where both mean and maximum values increase with difficulty, as the drone must rotate more quickly to handle tighter turns and avoid obstacles. Finally, both the mean and maximum values of linear speed show a slight decrease as difficulty increases, due to the drone slowing down in response to more complex track layouts and more cluttered spaces.\nThese results show that the policy generalizes to unseen tracks and achieves a high success rate in similar-to-training environments. As the test set deviates from the training set, the policy can adapt to increased difficulty with higher control effort and more careful velocity management and still maintain a certain level of success rate."}, {"title": "C. Towards Robust Obstacle-Aware Racing Policies", "content": "We additionally evaluate the policy on four hard tracks, all characterized by shorter waypoint distances, and a higher density of obstacles. These characteristics require the drone to do sharper turns. As a result, the policy struggles to navigate, with most trajectories being unsuccessful, lowering success rates to below 0.01. This performance decline suggests that the current policy, trained in simple environments, lacks the robustness required to navigate more complex and constrained tracks. This limitation could be due to over-simplified environments in the training set. The training tracks have fewer obstacles, and obstacles are all distributed in a simple way, which may make the policy overfit to this specific track design. As a result, the policy struggles in new, more challenging scenarios that require advanced obstacle avoidance and tighter maneuvering.\nTo improve robustness, the core problem would be how to randomly create a set of training environments that better represent the complexity of real-world tracks. Once this problem is answered, we can train the policy with domain randomization to finally obtain a robust policy for obstacle-aware drone racing."}, {"title": "V. CONCLUSION", "content": "This work presents an approach for training a generalizable obstacle-aware drone racing policy using domain randomization and deep reinforcement learning. The policy is trained on randomized short track segments, and evaluated on both hand-crafted, full-length tracks and randomized short segments across varying difficulty levels. Experiment results demonstrate that the policy generalizes well to unseen tracks and adapts to increased difficulty levels, achieving high success rates in environments closely resembling the training set. However, the policy encounters challenges in more complex, cluttered environments, where obstacle density and tighter waypoint spacing result in significant performance drops of the policy. This highlights the importance of further research into the method for generating more diverse and challenging training environments. Future work may also explore using advanced training strategies such as curriculum learning or adaptive difficulty scaling to better prepare policies for real-world drone racing challenges."}]}