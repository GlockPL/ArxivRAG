{"title": "Neural Control and Certificate Repair via Runtime Monitoring", "authors": ["Emily Yu", "\u0110or\u0111e \u017dikeli\u0107", "Thomas A. Henzinger"], "abstract": "Learning-based methods provide a promising approach to solving highly non-linear control tasks that are often challenging for classical control methods. To ensure the satisfaction of a safety property, learning-based methods jointly learn a control policy together with a certificate function for the property. Popular examples include barrier functions for safety and Lyapunov functions for asymptotic stability. While there has been significant progress on learning-based control with certificate functions in the white-box setting, where the correctness of the certificate function can be formally verified, there has been little work on ensuring their reliability in the black-box setting where the system dynamics are unknown. In this work, we consider the problems of certifying and repairing neural network control policies and certificate functions in the black-box setting. We propose a novel framework that utilizes runtime monitoring to detect system behaviors that violate the property of interest under some initially trained neural network policy and certificate. These violating behaviors are used to extract new training data, that is used to re-train the neural network policy and the certificate function and to ultimately repair them. We demonstrate the effectiveness of our approach empirically by using it to repair and to boost the safety rate of neural network policies learned by a state-of-the-art method for learning-based control on two autonomous system control tasks.", "sections": [{"title": "Introduction", "content": "The rapid advance of machine learning has sparked interest in using it to solve hard problems across various application domains, and autonomous robotic systems control is no exception. Learning-based control methods obtain data through repeated interaction with an unknown environment, which is then used to learn a control policy. A popular example is (deep) reinforcement learning algorithms, which learn neural network policies (Mnih et al. 2015; Sutton and Barto 2018). However, the complex and often uninterpretable nature of learned policies poses a significant barrier to their safe and trustworthy deployment in safety-critical applications such as self-driving cars or healthcare devices (Amodei et al. 2016; Garc\u00eda and Fern\u00e1ndez 2015).\nIn control theory, a classical method to certify the correctness of a control policy concerning some property of interest is to compute a certificate function for that property (Dawson, Gao, and Fan 2023). A certificate function is a mathematical object which proves that the system under the control policy indeed satisfies the property. Common examples of certificate functions include Lyapunov functions for stability (Khalil 2002) and barrier functions for safety set invariance (Prajna and Jadbabaie 2004). While certificate functions are a classical and well-established concept in control theory, earlier methods for their computation are based either on polynomial optimization or rely on manually provided certificates, which makes them restricted to polynomial systems and typically scalable to low-dimensional environments (Ahmadi and Majumdar 2016; Dawson, Gao, and Fan 2023; Srinivasan et al. 2021).\nRecent years have seen significant progress in overcoming these limitations, by utilizing machine learning to compute certificates for non-polynomial environments with neural network control policies (Abate et al. 2021b; Chang, Roohi, and Gao 2019; Dawson et al. 2021; Qin, Sun, and Fan 2022; Qin et al. 2021; Richards, Berkenkamp, and Krause 2018). The key idea behind these methods is to jointly learn a policy together with a certificate function, with both parametrized as neural networks. This is achieved by defining a loss function that encodes all defining properties of the certificate function. The learning algorithm then incurs loss whenever some of the defining certificate conditions are violated at any system state explored by the learning algorithm. Hence, the training process effectively guides the learning algorithm towards a control policy that admits a correct certificate function. A survey of the existing approaches and recent advances can be found in (Dawson, Gao, and Fan 2023).\nWhile the above approach guides the learning algorithm towards a correct certificate, it does not guarantee that the output control policy or the certificate function is correct. To address this issue, a number of recent works have considered the white-box setting which assumes complete knowledge of the environment dynamics and formally verifies the correctness of the certificate function. Notable examples include methods for learning and verifying neural Lyapunov functions (Abate et al. 2021b; Chang, Roohi, and Gao 2019; Richards, Berkenkamp, and Krause 2018) and neural barrier functions (Abate et al. 2021a; Zhang et al. 2023; Zhao et al. 2020). More recently, to incorporate environment uncertainty, several works have gone a step beyond and considered neural certificates in possibly stochastic environments (Abate et al. 2023; Ansaripour et al. 2023; Chatterjee et al. 2023; Lechner et al. 2022; Mathiesen, Calvert, and Laurenti 2023; Mazouz et al. 2022; Zhi et al. 2024; Zikelic et al. 2023a,b), assuming complete knowledge of the uncertainty model. While there are several methods that learn control policies together with certificates in the black-box setting, e.g. (Dawson et al. 2021; Qin, Sun, and Fan 2022; Qin et al. 2021), to the best of our knowledge no prior method considers the problem of analyzing and certifying their correctness in the black-box setting, without assuming any knowledge on the dynamics. In this setting, one cannot guarantee the correctness of certificate functions at every system state as we do not have access to the system model at each state. However, we may still use more lightweight techniques to evaluate the control policy and the certificate and to repair them in cases when they are found to be incorrect.\nOur contributions. In this work, we propose a method for analyzing and repairing neural network policies and certificates in the black-box setting. While guaranteeing correctness in the black-box setting is not possible without further assumptions on the dynamics, our method is based on theoretical foundations of provably correct control as it uses both neural policies and certificates of their correctness in order to achieve successful repair. More specifically, our method uses runtime monitoring to detect system behaviors that violate the property of interest or the certificate defining conditions. These behaviors are used to extract additional training data, which is then used by the training algorithm to re-train and ultimately repair the neural policy and the certificate. Our method consists of two modules - called the monitor and the learner, which are composed in a loop as shown in Fig. 1. The loop is executed until the monitor can no longer find property or certificate condition violating behaviors, upon which our algorithm concludes that the neural policy and the certificate have been repaired.\nWhile the idea of using runtime monitors to identify incorrect behaviors is natural, it introduces several subtle challenges whose overcoming turns out to be highly non-trivial. The first and obvious approach to runtime monitoring of a control policy in isolation is to simply test it, flag runs that violate the property, and add visited states to the training set. However, such a simple monitor suffers from two significant limitations. First, it can only be used to monitor violations of properties that can be observed from finite trajectories such as safety, but not of properties that are defined by the limiting behavior such as stability. Second, the simple monitor detects property violations only after they happen, meaning that it cannot identify states and actions that do not yet violate the property but inevitably lead to property violation. In contrast, in practice we are often able to predict in advance when some unsafe scenario may occur. For instance, if we drive a car in the direction of a wall at a high speed, then even before hitting the wall we can predict that a safety violation would occur unless we change the course of action.\nTo overcome these limitations, we advocate the runtime monitoring of neural network policies together with certificate functions and propose two new monitoring algorithms. We are not aware of any prior method that utilizes certificate functions for the runtime monitoring of control policies. The first algorithm, called Certificate Policy Monitor (CertPM), issues a warning whenever either the property or some of the defining conditions of the certificate are violated. The second algorithm, called Predictive Policy Monitor (PredPM), goes a step further by estimating the remaining time until the property or some certificate condition may be violated, and issues a warning if this time is below some tolerable threshold. Experimental results demonstrate the ability of CertPM and PredPM to detect the property or certificate condition violating behaviors. Furthermore, we show that CertPM and PredPM allow our method to repair and significantly improve neural policies and certificates computed by a state-of-the-art learning-based control theory method.\nOur contributions can be summarized as follows:\n1.  Runtime monitoring of policies and certificates. We design two novel algorithms for the runtime monitoring of control policies and certificates in the black-box setting.\n2.  Monitoring-based policy and certificate repair. By extracting additional training data from warnings issued by our monitors, we design a novel method for automated repair of neural network control policies and certificates.\n3.  Empirical evaluation. Our prototype successfully repairs neural network control policies and certificates computed by a state-of-the-art learning-based control method."}, {"title": "Preliminaries", "content": "We consider a (deterministic, continuous-time) dynamical system \u03a3 = (X, U, f), where X \u2286 R^n is the state space, U \u2286 R^m is the control action space and f : X \u00d7 U \u2192 X is the system dynamics, assumed to be Lipschitz continuous. The dynamics of the system are defined by x(t) = f(x(t), u(t)), where t \u2208 R>0 is time, x : R>o \u2192 X is the state trajectory, and u : R>0 \u2192 U is the control input trajectory. The control input trajectory is determined by a control policy \u03c0 : X \u2192 U. We use X0 \u2264 X to denote the set of initial states of the system. For each initial state xo \u2208 Xo, the dynamical system under policy gives rise to a unique trajectory x(t) with x(0) = 10. Control tasks are concerned with computing a control policy \u03c0 : X \u2192 U such that, under the control input u(t) = \u03c0(x(t)), the state trajectory induced by x(t) = f(x(t), \u03c0(x(t)) satisfies a desired property for every initial state in Xo. In this work, we consider two of the most common families of control properties:\n1.  Safety. Given a set of unsafe states Xu \u2286 X, the dynamical system satisfies the safety property under policy \u03c0 with respect to Xu, if it never reaches Xu, i.e., if for each initial state in Xo we have x(t) \u2209 Xu for all t \u2265 0.\n2.  Stability. Given a set of goal states Xg \u2286 X, the dynamical system satisfies the (asymptotic) stability property under policy \u03c0 with respect to Xg, if it asympotically converges to Xg, i.e., if limt\u2192\u221e infxg\u2208Xg ||x(t) \u2212 xg|| = 0 for each initial state in Xo.\nWe also consider the stability-while-avoid property, which is a logical conjunction of stability and safety properties. To ensure that the dynamical system satisfies the property of interest, classical control methods compute a certificate function for the property. In this work, we consider barrier functions (Ames, Grizzle, and Tabuada 2014) for proving safety and Lyapunov functions (Khalil 2002) for proving stability. The stability-while-avoid property is then proved by computing both certificate functions.\nProposition 1 (Barrier functions) Suppose that there exists a continuously differentiable function B: X \u2192 R for the dynamical system \u2211 under a policy n with respect to the unsafe set Xu, that satisfies the following conditions:\n1.  Initial condition. B(x) \u2265 0 for all x \u2208 X0.\n2.  Safety condition. B(x) < 0 for all x \u2208 Xu.\n3.  Non-decreasing condition. LfB(x) + B(x) \u2265 0 for all x \u2208 {x | B(x) > 0}, where LfB = \\frac{\\partial B}{\\partial x} f(x, u) is the Lie derivative of B with respect to f.\nThen, \u2211 satisfies the safety property under \u3160 with respect to Xu, and we call B a barrier function.\nProposition 2 (Lyapunov functions) Suppose that there exists a continuously differentiable function V : X \u2192 R for the dynamical system \u2211 under policy \u3160 with respect to the goal set Xg, that satisfies the following conditions:\n1.  Zero upon goal. V(xg) = 0 for all xg \u2208 Xg.\n2.  Strict positivity away from goal. V(x) > 0 for all x \u2208 X\\Xg.\n3.  Decreasing condition. L\u0192V < 0 for all x \u2208 X\\Xg.\nThen, \u2211 satisfies the stability property under \u3160 with respect to Xg, and we call V a Lyapunov function.\nAssumptions. We consider the black-box setting, meaning that the state space X, the control action space U as well as the goal set Xg and/or the unsafe set Xu (depending on the property of interest) are known. However, the system dynamics function f is unknown and we only assume access to an engine which allows us to execute the dynamics function.\nProblem statement. Consider a dynamical system \u2211 defined as above. Suppose we are given one of the above properties, specified by the unsafe set Xu and/or the goal set Xg. Moreover, suppose that we are given a control policy \u03c0 and a certificate function B for the property.\n1.  Policy certification and repair. Determine whether the system \u2211 under policy \u03c0 satisfies the property. If not, repair the policy \u3160 such that the property is satisfied.\n2.  Certificate certification and repair. Determine whether B is a good certificate function which shows that the dynamical system \u2211 under policy \u03c0 satisfies the property. If not, repair the certificate function B such that it becomes a correct certificate function."}, {"title": "Runtime Monitoring Policies and Certificates", "content": "We now present our algorithms for runtime monitoring of a policy by monitoring it together with a certificate function. These algorithms present the first step in our solution to the two problems defined above. The second step, namely policy and certificate repair, will follow in the next section. The runtime monitoring algorithms apply to general policies and certificate functions, not necessarily being neural networks.\nMotivation for certificate monitoring. Given a dynamical system \u03a3 = (X,U, f) and a property of interest, a monitor is a function M : X+ \u2192 {0,1} that maps each finite sequence of system states to a verdict on whether the sequence violates the property. This means that the monitor can only detect violations with respect to properties whose violations can be observed from finite state trajectories. This includes safety properties where violations can be observed upon reaching the unsafe set, but not infinite-time horizon properties like stability which requires the state trajectory to asymptotically converge to the goal set in the limit. In contrast, monitoring both the control policy and the certificate function allows the monitor to issue a verdict on either (1) property violation, or (2) certificate violation, i.e. violation of one of the defining conditions in Proposition 1 for barrier functions or Proposition 2 for Lyapunov functions. By Propositions 1 and 2, we know that the barrier function or the Lyapunov function being correct provides a formal proof of the safety or the stability property. Hence, in order to show that there are no property violations and that the property of interest is satisfied, it suffices to show that there are no certificate violations which can be achieved by monitoring both the control policy and the certificate function.\nCertificate Policy Monitor\nWe call our first monitor the certificate policy monitor (CertPM). For each finite sequence of observed states X0, X1,..., Xn, the monitor MCertPM issues a verdict on whether a property or a certificate violation has been observed. If we are considering a safety property, the property violation verdict is issued whenever In \u2208 Xu, and the certificate violation verdict is issued whenever one of the three defining conditions in Proposition 1 is violated at xn. If we are considering a stability property, the property violation verdict cannot be issued, however, the certificate violation verdict is issued whenever one of the defining conditions in Proposition 2 is violated at xn. In the interest of space, in what follows we define the monitor MCertPM for a safety property specified by the unsafe set of states Xu. The definition of MCertPM for a stability property is similar, see the Appendix. Let \u03c0 be a policy and B be a barrier function:\n\u2022 The safety violation verdict is issued if xn \u2208 Xu. We set MCertPM(x0,x1,...,xn) = 1.\n\u2022 The certificate violation verdict for the Initial condition in Proposition 1 is issued if xn \u2208 Xo is an initial state but B(xn) < 0. We set MCertPM(x0, X1,...,xn) = 1.\n\u2022 The certificate violation verdict for the Safety condition in Proposition 1 is issued if B(xn) < 0. We set MCertPM(x0,x1,...,xn) = 1.\n\u2022 Checking the Non-decreasing condition in Proposition 1 is more challenging since the dynamical system evolves over continuous-time and the non-decreasing condition involves a Lie derivative. To address this challenge, we approximate the non-decreasing condition by considering the subsequent observed state Xn+1 and approximating the Lie derivative via\nLfB(xn) = \\frac{B(Xn+1) \u2013 B(xn)}{tn+1-tn}\nThis requires the monitor to wait for at least one new observation before issuing the verdict. The approximation satisfies |LfB(x) - LfB(x)| \u2264 \u20ac for all x \u2208 X where \u20ac = \u2206t(CBLf+CfLB)Cf, with L\u00df and Lf being the Lipschitz constants of B and f bounded by constants CB, Cf \u2208 R>0 (Nejati et al. 2023). This suggests we can achieve good precision by using sufficiently small time intervals for monitoring. The certificate violation verdict for the Non-decreasing condition in Proposition 1 is issued if B(xn) \u2265 0 but LfB(xn) + B(xn) < 0. We set MCertPM (x0, X1,...,xn) = 1.\n\u2022 Otherwise, no violation verdict is issued. We set MCertPM (x0, X1,...,xn) = 0.\nPredictive Policy Monitor\nCertPM checks if the property or one of the certificate defining conditions is violated at the states observed so far. Our second monitor, which we call the predictive policy monitor (PredPM), considers the case of safety properties and estimates the remaining time before the property or the certificate violation may occur in the future. PredPM then issues a verdict based on whether any of the estimated remaining times are below some pre-defined thresholds. Thus, our second monitor aims to predict future behavior and issue property and certificate violation verdicts before they happen.\nSince PredPM is restricted to safety properties, let Xu be the set of unsafe states. Let \u03c0be a policy and B be a barrier function. For each finite sequence of observed states X0, X1,..., Xxn, PredPM computes three quantitative assessments [v\u03c5, \u03c5\u03c2, \u03c5\u03c0], where:\n1.  vu is an estimate of the time until Xu is reached and hence the safety property is violated (or, if xn \u2208 Xu already, vu is an estimate of the time until the safe part X\\Xu is reached).\n2.  vs is an estimate of the time until the set {x \u2208 X | B(x) < 0} may be reached and hence the Safety condition in Proposition 1 may be violated;\n3.  UN is an estimate of the time until the Non-dec. condition in Proposition 1 may be violated.\nNote that these three values are estimates on the remaining time until some set S C X is reached, where S = Xu (or S = X\\Xu if xn \u2208 Xu) for computing vu, S = {x \u2208 X | B(x) < 0} for computing vs, and S = {x \u2208 X | LfB(x)+B(x) <0} for computing VN. PredPM computes each of the three values by solving the following problem:\nmin T s.t. \\frac{dv}{dt} = v(t), \\frac{da}{dt} = a(t), |a(t)| \u2264 \u0430\u0442\u0430\u0445, \\forall t \u2208 [0,T]; x(T) \u2208 S, x(0) = x\u03b7, \u03c5(0) = \u03c5\u03bf.\nHere, v(t) is the velocity and a(t) is the acceleration of the trajectory x(t), with amax being the maximum allowed acceleration. Intuitively, solving this optimization problem results in the acceleration that the controller should use at each time such that the set S is reached in the shortest time possible. To make the computation physically more realistic, we assume a physical bound amax on the maximum acceleration that the controller can achieve at any time. To compute an approximate solution to this problem and hence estimate \u03bd\u03c5, \u03c5\u03c2 and \u03c5\u03bd, PredPM discretizes the time [0, T] and for each discrete time point it uses stochastic gradient descent to select the next acceleration within the range [0, amax].\nOnce PredPM computes \u03bd\u03c5, vs, vn, it checks if any of the values exceed the thresholds \u03be\u03c5, \u03be\u03c2, \u03be\u03bd that are assumed to be provided by the user. The monitor issues the verdict MPredPM(x0, X1, . . ., xn) = 1 if either v\u03c5 > \u03be\u03c5 or \u03c5\u03c2 > \u03be\u03c2 or \u03c5\u03c0 > \u03be\u03bd. Otherwise, it issues the verdict 0. Having quantitative assessments allows the user to specify how fault-tolerant the monitor should be. For highly safety-critical systems, one can set the thresholds to be positive such that the monitor signals a warning in advance, i.e., when it predicts a dangerous situation and before it happens."}, {"title": "Neural Policy and Certificate Repair", "content": "Our monitors in the previous section are able to flag finite state trajectories that may lead to property or certificate violations. We now show how the verdicts issued by our monitors can be used to extract additional training data that describe the property or certificate violations. Hence, if the control policy and the certificate are both learned as neural networks, we show how they can be retrained on this new data and repaired. In the interest of space, in this section, we consider the case of safety properties where certificates are barrier functions. Suppose that Xu is a set of unsafe states in a dynamical system \u03a3 = (X,U, f). The pseudocode of our monitoring-based repair algorithm is shown in Algorithm 1. Our method can be easily modified to allow Lyapunov function repair and we provide this extension in the Appendix.\nLearning-based control with certificates. Before showing how to extract new training data and use it for policy and certificate repair, we first provide an overview of the general framework for jointly learning neural network policies and barrier functions employed by existing learning-based control methods (Zhao et al. 2020). Two neural networks are learned simultaneously, by minimizing a loss function which captures each of the defining conditions of barrier functions in Proposition 1. That way, the learning process is guided towards learning a neural control policy that admits a barrier function and hence satisfies the safety property. The loss function contains one loss term for each defining condition:\nL(0,v) = LInit(0,1) + LSafe (\u03b8, \u03bd) + LNon-dec(\u03b8, \u03bd), (1)\nwhere \u03b8 and v are vectors of parameters of neural networks \u03c0\u03b8 and B, respectively, and\nLInit(0, 1) = \\frac{1}{DInit}\u2211_{xED Init} max(-B(x), 0);\nLSafe(0, 1) = \\frac{1}{DSafe}\u2211_{xEDSafe} max(B,(x), 0);\nLNon-dec(0, 1) = \\frac{1}{DNon-dec}\u03a3_{xEDNon-dec}max (-LB, (x) - B,(x),0).\nIn words, Dinit, DSafe, DNon-dec X are the training sets of system states used for each loss term that incurs loss whenever the defining condition in Proposition 1 is violated. The term Lfo B\u2081\u2084(x) in LNon-dec is approximated by executing the system dynamics from state x for a small time interval. Since we are interested in repair and not the initialization of \u03c0\u03b8 and B, we omit the details on how this training data is collected and refer the reader to (Qin, Sun, and Fan 2022).\nMonitoring-based neural policy and certificate repair. We now show how the verdicts of our monitors are used to obtain new training data that describes the property or certificate violations, and how this new training data is used for the policy and certificate repair. Algorithm 1 takes as input the neural network control policy \u03c0 and neural network barrier function B, trained as above. It also takes as input a finite set of time points to = 0 < t\u2081 < < t at which the monitor observes new states, as well as the number D of state trajectories that it monitors. In addition, if the PredPM monitor is to be used, the algorithm also takes as input the three thresholds \u03be\u03c5, \u03be\u03c2, and \u03be\u03bd.\nAlgorithm 1 first initializes the monitor M by constructing either the CertPM or the PredPM (line 1), the set of new training data DNew-data which is initially empty (line 2) and a set XoXo of D initial states obtained via sampling from the initial set Xo (line 3). Then, for each initial state x \u2208 X, it executes the dynamical system to obtain a state trajectory x(t) from x(0) = xo (line 5). For each time point tn, a new state In is observed (line 7) and the monitor verdict M(x0, X1,...,xn) is computed (line 8). If M(x0, X1,...,xn) = 1, i.e. if the monitor issues a property or certificate violation verdict, the new state xn is added to the new training dataset DNew-data (line 8). Once the new data is collected, it is used to initialize the new training datasets Drepair = DNew-data \u2229 Xo, Drepair = DNew-data \u2229 Xu and Drepair = DNew-data \u2229 {x | B(x) \u2265 0} (line 9). Finally, the neural network policy and the neural network barrier function B are repaired by being retrained on the loss function in eq. (1) with the new training datasets (line 11). If we are only interested in repairing the barrier function B for a fixed control policy \u03c0, only the network B is repaired while keeping the parameters of \u03c0 fixed (line 13)."}, {"title": "Experimental Evaluation", "content": "We implemented a prototype of our method in Python 3.6 as an extension of the SABLAS codebase (Qin, Sun, and Fan 2022). SABLAS is a state-of-the-art method and tool for learning-based control of neural network control policies with certificate functions. In order to evaluate our method, we consider the benchmarks from the SABLAS codebase, together with policies and certificate functions learned by the SABLAS method for these benchmarks. We then apply our method to these control policies and certificate functions in order to experimentally evaluate the ability of our method to detect incorrect behaviors and to repair them. Our goal is to answer the following three research questions (RQs):\n\u2022 RQ1: Is our method able to detect violating behavior and repair neural network policies and certificate functions? This RQ pertains to Problem 1.\n\u2022 RQ2: Is our method able to detect incorrect behavior and repair neural network certificate functions? This RQ pertains to Problem 2 in the Preliminaries Section.\n\u2022 RQ3: How strong is predictive power of PredPM monitor, that is, can it predict safety violations ahead of time?\nBenchmarks. We consider two benchmarks that are available in the SABLAS codebase (Qin, Sun, and Fan 2022), originally taken from (Fossen 2000; Qin et al. 2021). The first benchmark concerns a parcel delivery drone flying in a city (called the active drone), among 1024 other drones that are obstacles to be avoided. Only the active drone is controlled by the learned policy, whereas other drones move according to pre-defined routes. The state space of this environment is 8-dimensional and states are defined via 8 variables x = [x, y, z, Ux, Vy, Uz, Ox, \u03b8y]: three coordinate variables in the 3D space, three velocity variables, and two variables for row and pitch angles. The actions produced by the policy correspond to angular accelerations of \u03b8\u03b1, \u03b8y, as well as the vertical thrust. The drone is completing a delivery task at a set destination, hence the property of interest in this task is a stability-while-avoid property.\nThe second benchmark ShipEnv concerns a ship moving in a river among 32 other ships. The state space of this environment is 6-dimensional and states are defined via 6 variables [x, y, \u03b8, u, v, w]. The first two variables specify the 2D coordinates of the ship, @ is the heading angle, u, v are the velocities in each direction, and w is the angular velocity of the heading angle. The property of interest in this task is also a stability-while-avoid property with obstacles being collisions with other ships.\nExperimental setup. We consider the black-box setting, hence the dynamics are unknown to the monitor and repair algorithm. For each monitored execution, N = 2000 and N = 1200 states are observed for ShipEnv and DroneEnv, respectively, spaced out at time intervals of At = 0.1s. In each environment, the states of the eight nearest obstacles (i.e. drones or ships) are given as observations to the policy, as well as the certificate functions. In our implementation of the PredPM monitor, we employ Adam (Kingma and Ba 2014) for approximating the assessments [v\u03c5, \u03c5\u03c2,UN].\nResults: RQ1. We observed that the control policy learned by SABLAS for DroneEnv does not satisfy the stability-while-avoid property on all runs its safety rate is 93.99% whereas it leads to collision with other drones 6.01% of the time, initialized with 104 state samples. We applied our repair method to the control policy and the barrier function learned by SABLAS with D = 1000. To evaluate the im-"}, {"title": "Summary of results", "content": "Our experimental results empirically justify the following claims: (i) Our method is able to successfully repair neural network control policies and certificate functions. Using either CertPM or PredPM for repair leads to significant improvements over the initial policy; (ii) Our method is able to successfully repair neural network certificate functions in the setting where the control policies are fixed; (iii) Using PredPM allows predicting safety property violations before they happen, hence showing potential for practical safety deployment even in the runtime setting.\nPractical considerations and limitations. To conclude, we also discuss two practical aspects that one should take into account before the deployment of our method: (i) As highlighted in the Introduction, our method provides no guarantees on the correctness of repaired policies. This means that, in principle, one could end up with a policy whose performance is suboptimal compared to the initial policy. However, we did not observe a single case of such a behavior in our experiments. (ii) It was observed by (Zikelic et al. 2022) that methods for jointly learning policies and certificates rely on a good policy initialization. Hence, our repair method is also best suited for cases when the policy is well initialized by some off-the-shelf method (e.g. with SR at least 90%)."}, {"title": "Conclusion", "content": "In this work, we propose a method for determining the correctness of neural network control policies and certificate functions and for repairing them by utilizing runtime monitoring. Our method applies to the black-box setting and does not assume knowledge of the system dynamics. We present two novel monitoring algorithms, CertPM and PredPM. Our experiments demonstrate the advantage of monitoring policies together with certificate functions and are able to repair neural policies and certificates learned by a state-of-the-art learning-based control method. Interesting directions of future work would be to consider the repair problem for stochastic systems and multi-agent systems. Another interesting direction would be to explore the possibility of deploying predictive monitors towards enhancing the safety of learned controllers upon deployment, i.e., at runtime."}, {"title": "Runtime Monitoring of Lyapunov Functions", "content": "In this section, we present how to construct MCertPM in the case of a stability property and a Lyapunov function certificate. Consider a stability property defined by the goal set Xg. Let \u03c0 be a policy and V be a Lyapunov function. Consider a finite sequence of observed states x0,x1,..., Xn. MCertPM checks if all the defining conditions in Proposition 2 are satisfied at xn:\n\u2022 The certificate violation verdict for the Zero upon goal condition in Proposition 2 is issued whenever Xn \u2208 Xg is a goal state but V(xn) \u2260 0. We set MCertPM(x0,x1,...,xn) = 1.\n\u2022 The certificate violation verdict for the Strict positivity condition in Proposition 2 is issued whenever whenever V(xn) < 0. We set MCertPM(x0, X1,...,xn).\n\u2022 The certificate violation verdict for the Decreasing condition in Proposition 2 is issued as follows. Similarly as for barrier functions, we approximate the Decreasing condition by considering the subsequent observed state Xn+1 and approximating the Lie derivative via\nLFV(xn) = \\frac{V(xn+1) - V(xn)}{tn+1-tn}\nThe certificate violation verdict for the Decreasing condition in Proposition 2 is issued whenever xn Xg, V(xn) > 0 and LfV(xn) \u2265 0. We set MCertPM(x0,x1,...,xn).\n\u2022 Finally, if none of the three conditions are violated, no violation verdict is issued. We set MCertPM(x0,x1,...,xn) = 0."}, {"title": "Neural Lyapunov Function Repair", "content": "Consider a stability property and suppose that X, is a set of goal states in a dynamical system \u2211 = (X,U, f). The pseudocode of our monitoring-based repair algorithm is shown in Algorithm 2.\nLearning-based control with Lyapunov functions. Before showing how to extract new training data and use it for policy and Lyapunov function repair, we first provide an overview of the general framework for jointly learning neural network policies and Lyapunov functions. Two neural networks are learned simultaneously, by minimizing a loss function which captures each of the defining conditions of Lyapunov functions in Proposition 2. That way, the learning process is guided towards learning a neural control policy that admits a Lyapunov function and hence satisfies the stability property. The loss function contains one loss term for the Zero upon goal and the Decreasing conditions in Proposition 2 (Strict positivity condition is enforced by applying a non-negative activation function on the Lyapunov function output):\nL(0,v) = LG(0,\u03bd) + Lp(\u03b8, \u03bd), (2)\nwhere \u03b8 and v are vectors of parameters of neural networks \u03c0\u03b8 and V, respectively, and\nLG(\u03b8, \u03bd) = \\frac{1}{DG} \u03a3V(x)\u00b2;\nLD(0, v) = \\frac{1}{|DD|}\u2211_{XEDD} [max (LoV, (x),0) + max ( - L\u2081(x),0)].\nIn words, DG, DD \u2264 X are the training sets of system states used for each loss term and each loss term incurs loss whenever the defining condition in Proposition 2 is violated. DG is the data set collected for goal states, and DD is the set for sampled data points."}]}