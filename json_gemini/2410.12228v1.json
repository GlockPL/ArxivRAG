{"title": "Triple Modality Fusion: Aligning Visual, Textual, and Graph Data with Large Language Models for Multi-Behavior Recommendations", "authors": ["Luyi Ma", "Xiaohan Li", "Zezhong Fan", "Jianpeng Xu", "Jason Cho", "Praveen Kanumala", "Kaushiki Nag", "Sushant Kumar", "Kannan Achan"], "abstract": "Integrating diverse data modalities is crucial for enhancing the performance of personalized recommendation systems. Traditional models, which often rely on singular data sources, lack the depth needed to accurately capture the multifaceted nature of item features and user behaviors. This paper introduces a novel framework for multi-behavior recommendations, leveraging the fusion of triple-modality, which is visual, textual, and graph data through alignment with large language models (LLMs). By incorporating visual information, we capture contextual and aesthetic item characteristics; textual data provides insights into user interests and item features in detail; and graph data elucidates relationships within the item-behavior heterogeneous graphs. Our proposed model called Triple Modality Fusion (TMF) utilizes the power of LLMs to align and integrate these three modalities, achieving a comprehensive representation of user behaviors. The LLM models the user's interactions including behaviors and item features in natural languages. Initially, the LLM is warmed up using only natural language-based prompts. We then devise the modality fusion module based on cross-attention and self-attention mechanisms to integrate different modalities from other models into the same embedding space and incorporate them into an LLM. Extensive experiments demonstrate the effectiveness of our approach in improving recommendation accuracy. Further ablation studies validate the effectiveness of our model design and benefits of the TMF.", "sections": [{"title": "1 INTRODUCTION", "content": "In the rapid development of personalized recommendation systems, the ability to leverage diverse data modalities is pivotal for delivering accurate and relevant recommendations to users [17, 55]. Traditional recommendation models often rely on singular data sources, which limits their capability to fully comprehend the intricate and multifaceted nature of user behaviors and item features. This paper addresses this limitation by introducing a novel framework for multi-behavior recommendations that leverages the fusion of three distinct modalities: visual, textual and graph data based on the power of Large Language Models (LLMs).\nRecently, inspired by the great success of Large Language Models (LLMs) [2, 4] and Multi-Modal Language Models [42], exploring the potential of LLMs and VLMs in recommendation systems are attracting attention [1, 5, 13, 14, 20, 31, 32, 41], especially driven by extensive world knowledge and reasoning capabilities of LLMs [2].\nAt the core is to reshape sequential recommendation as the language modeling task - that is, convert users' behavioral sequence into the textual input prompt. For example, as illustrated in Figure 1, \"The user views grey athletic T-shirt, adds to cart water bottle, what's the next to purchase?\" is the prompt we input to the LLM, where"}, {"title": "2 RELATED WORKS", "content": null}, {"title": "2.1 Multi-Behavior Recommendation", "content": "To enhance the capability of recommendation systems to capture user preferences more effectively, various approaches have been developed that incorporate multi-behavior information about users and items. Multi-behavior recommendation systems (MBRS), aiming to bolster the recommendation of a target behavior by leveraging auxiliary behavioral data, can be generally classified into two categories. 1) For sequence-based MBRS, they model behavior sequences as inputs into a sequential model. Yuan et al. [53] applies the transformer model [44] to model heterogeneous item-level multi-behavior dependencies. The study in [46] explores the interconnections among behavior sequences using a temporal graph transformer. 2) For graph-based MBRS, they utilize Graph Neural Networks (GNNs) [27, 30, 33] and Knowledge Graphs (KGs) [28, 34] to model behaviors. the MBGCN model [22] applies a graph convolutional network to learn the similarities in behaviors and understand user interests. KMCLR [48] applied contrastive learning on a KG to fully use multi-behavior information. Furthermore, MB-GMN [47] incorporates graph meta-learning into the multi-behavior pattern modeling. MBHT [50] utilizes hypergraph to learn both short-term and long-term cross-type behavior dependencies.\nOur proposed Triple Modality Fusion (TMF) improves the performance of sequence-based MBRS with the power of LLMs, which are effective in recommendation systems [5, 13]. Traditional recommendation models often rely on singular data modality [24, 29]. As graph modality has been demonstrated useful in MBRS, we incorporate the embeddings learned from a graph-based model to LLMs with an MLP adaptor. Moreover, as item images are also important features [7, 11, 17], we also align the visual representations of items with LLMs with the help of Vision Transformer [9]."}, {"title": "2.2 LLM-based Recommendation Systems", "content": "Recently, Large Language Models (LLMs) have demonstrated their advantages in diverse applications [3, 10]. Research on LLM-based recommendation systems falls into three distinct categories: representation learning, in-context learning, and generative recommendation. In representation learning of LLM-based recommendation systems, the primary use of LLMs is to enhance user and item representations by utilizing contextual data during the recommendation process [19, 26, 36, 51, 54]. In-context learning of LLM-based recommendation systems have concentrated on probing the potential of LLMs such as ChatGPT 1 to directly generate recommendations without the need for training or fine-tuning [6, 12, 20, 32, 40]. However, ChatGPT cannot generate recommendations with accuracy competitive with that of traditional recommendation methods for all tasks based on the research from [32].\nThis paper focuses on the last category: LLM-based generative recommendation. This novel approach transitions from the"}, {"title": "3 PRELIMINARY", "content": "In this section, we introduce the preliminary information about our proposed TMF."}, {"title": "3.1 Problem Definition", "content": "In multi-behavior recommendation systems (MBRS), we distinguish between target behaviors, the types of interactions we seek to predict, and auxiliary behaviors, which supplement the recommendation process. Auxiliary behaviors provide context by revealing different user preferences to enhance the performance of the recommendation on target behavior types. For instance, on many e-commerce platforms, purchasing behaviors are often targeted for prediction because of their direct correlation with the Gross Merchandise Volume (GMV), representing the total sales value of merchandise [10, 45]. We define our approach to the multi-behavior recommendation problem as follows:\n\u2022 Input: The images and descriptions of items; The interaction history of a given user u consisting of n interacted items and their associated behaviors $H_u = (i_1, b_1), (i_2, b_2), ..., (i_n, b_n)$, where i represent item and b means behavior.\n\u2022 Output: Text tokens representing the recommended item."}, {"title": "3.2 Visual and Textual Modalities", "content": "In modern recommendation systems, the item images and descriptions are important modalities for the users to understand the details of items [17, 26, 52]. Given an item i, we define its visual embedding $v_{image}$ and textual embedding $v_{text}$ by Eq. 1 and 2 by the pre-trained image encoder $F_{visual}(.)$ and the text encoder $F_{text}(.)$ respectively, where $image_i$ is the image of the item i, and $text_i$ represents the textual information of the item i. Specifically, we concatenate textual information for the item i, including title, brand, categories, and description, to build $text_i$. In most cases, text and $image_i$ are highly correlated to ensure the fidelity of item information.\n$v_{image} = F_{visual}(image_i), v_{image} \\in \\mathbb{R}^{1 \\times d_o}$ (1)\n$v_{text} = F_{text}(text_i), v_{text} \\in \\mathbb{R}^{1 \\times d_t}$ (2)"}, {"title": "3.3 Graph Modality", "content": "In addition to the visual and textual information, we also model the relationships between items and behaviors as the third modality to elucidate the complicated item-behavior relationship. Typically, the graph modality of item-behavior relationships over multi-behaviors"}, {"title": "3.4 LLM-based Recommender", "content": "LLM-based Recommender. An LLM-based recommender for MBRS could be classified into three steps: (1) representation of user history, (2) prompt formatting, and (3) LLM inference for the next-item recommendation. The history of a given user u consists of the past n interacted items $H_u = (i_1, b_1), (i_2, b_2), ..., (i_n, b_n)$, where $i_n$ is the most-recently interacted items via behavior $b_n$. To represent $H_u$ in natural language, a common way is to convert items and the associated behaviors into their names and concatenate them into a long sentence $text_u$. In the second part, the user history $text_u$ will be integrated into a prompt that an LLM can use to understand the context and instructions. Finally, an LLM processes the prompt and predicts the next item, i.e., the n+1-th item given $H_u$. To better align MBRS with LLMs, motivated by [31], we introduce special tokens after each item's name and behavior's name and initialize the embeddings of special tokens by projecting the item and behavior embeddings from graph-based MBRS model to the token space.\nInstruction Tuning. Instruction tuning has become a crucial technique to enhance the performance of Large Language Models (LLMs) on task-specific instructions. It significantly improves the ability of LLMs to follow human task-specific feedback [35]. This approach involves reorganizing data to define textual instructions and their corresponding responses clearly. By pairing task descriptions with their associated responses in a natural language format, a more comprehensive instructional context is created. Following this, the LLMs can be fine-tuned using the autoregressive objective.\nParameter Efficient Fine-Tuning with LoRA. Fine-tuning all the parameters of an LLM is a time-intensive process. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) [16, 25] focuses on optimizing a smaller subset of parameters, greatly reducing the computational demands while still maintaining strong performance. LORA [21], a common PEFT algorithm, keeps the LLM's weights"}, {"title": "4 METHODOLOGY", "content": "In this section, we explain the modality fusion module to integrate visual, textual and graph data into an LLM. Then we introduce the prompt designs of the LLM and discuss the instruction tuning details."}, {"title": "4.1 Modality Fusion Module", "content": "To unleash the power of different modalities of items in MBRS, we employ pre-trained models to generate embeddings of items for each modality and introduce two mechanisms for modality fusion (Figure 2). In this three-level design, we first consider a self-attention mechanism on top of the user sequence with embeddings of all item modalities to learn better query embeddings for cross-modality attentions. For the second and third levels, we consider the output of the previous layer as a query and treat the image and the text as key and value (vice versa), respectively."}, {"title": "4.1.1 All-Modality Self-attention (AMSA)", "content": "The motivation behind the All-Modality Self-attention (AMSA) is that a proper query for cross-modality attention could be determined by both the modalities of an item and other items in the same user behavior sequence. We concatenate embeddings of different modalities of the item i generated by their associated encoders to get a single representation of all modalities,\n$v_{i}^{M} = CONCAT(v_{image}, v_{text}, v_{graph})$, (5)\nwhere $v_{i}^{M} \\in \\mathbb{R}^{1 \\times d}, d = \\sum d_m;$ . Given a user behavior sequence, $H_u$, we can represent the sequence by stacking the item representation, $v^{M} \\in \\mathbb{R}^{|H_u| \\times d}$. Next, we use self-attention to address the dynamic importance of modalities during fusion (Eq. 6). In this design, the self-attention weight matrix $W_{att} \\in \\mathbb{R}^{|H_u| \\times |H_u|}$ can capture the dynamic importance of items in the user sequence.\n$v_{amsa} = softmax(\\frac{v_{i}^{M}v_{i}^{M^T}}{\\sqrt{d}})v_{i}^{M}$, (6)"}, {"title": "4.1.2 Cross-modality Attention (CMA)", "content": "To further leverage the rich context in the image embedding and the text embedding, we build a cross-modality attention mechanism for modality fusion after AMSA. We consider the output of AMSA as the query to extract useful information from the image and the text modality. As illustrated in Figure 2, we project the output of AMSA, $v_{amsa}$ into the same dimension of the image embedding and the text embedding, and apply the output embedding from AMSA as the query, and the image and text embeddings serve as the key and the value, respectively. For simplicity, we reuse $v_{amsa}$ for the query embedding after projection. We summarize the basic cross-modality"}, {"title": "4.2 Prompt Formatting", "content": "In an LLM-based recommender, the user-item interaction sequence determines the context of user preference in the prompt. Integrating different modalities of items and behaviors into a prompt for LLM-based recommenders requires a unique prompt design. Given an item and its associated behavior (i, b) \u2208 $H_u$ for a user u, we represent them by their names, ($name_i$, $name_b$). Instead of direct insertion of ($name_i$, $name_b$) into a prompt, we design a hybrid prompt and introduce extra modality tokens for items and behaviors after their names, respectively.\nTypically, we define the item i's modality token as '[MT;]', and its associated behavior b's modality token as '[MT]'. These extra tokens are not seen during the pre-training steps of LLMs. To warm up the embeddings of the behavior token \u2018[MT]' and the item ID token '[MT;]', we develop two adaptor modules that project behavior embeddings and the output of the item modality fusion module (section 4.1) to the LLM token embedding space, respectively. We use a two-layer MLP to complete the projection. Figure 1 shows the high-level design of our hybrid prompt. The language tokens are from the pre-trained LLM, while the item ID token '[MT;]' and the behavior token '[MT]' are also included with embedding warm-up from adaptors.\nInstruction of the recommendation task is also critical for a successful LLM inference. We follow the prompt design in LLaRA [31] and include the candidate set into the prompt beside the task definition and the user-item interaction sequence. The recommendation task is to predict the name of the next item under a certain target user behavior, given the candidate item set."}, {"title": "4.3 Instruction Tuning with LoRA", "content": "To properly align the knowledge in the pre-trained LLMs and the modality information from the user-item interactions, we conduct the instruction tuning with LoRA [21] for a pre-trained LLM over the hybrid prompt. However, because the LLMs are predominantly pre-trained on textual data, the tuning task complexity could increase if we include more modalities in the prompts. We consider the curriculum tuning strategy [31] to determine the order of various tuning tasks."}, {"title": "Task Complexity", "content": "The task complexity is related to the heterogeneity of modalities in the prompts. We design learning tasks with three levels of complexity. The easy task has text-only prompts without introducing the behavior tokens and the item ID tokens (Figure 3(a)). The medium task considers both text-only prompts and the behavior tokens (Figure 3(b)). Finally, the hard task includes the item ID tokens on top of the medium task (Figure 3(c)).\nLet $\\Theta_{LLM}$ denote the LoRA parameters for LLM fine-tuning, and $Y_f, Y_i, Y_b$ be the learnable parameters of the modality fusion module, the adaptor for item ID tokens and the adaptor for behavior tokens, respectively. We list the loss function for each task for optimization in Eq. 9-11, where (Hu, Yu) is the input behavior sequence and the target interaction pair. We only keep the trainable parameters for simplicity."}, {"title": "Scheduler Formulation and Training", "content": "To transfer the learning from the easy task to more challenging tasks, we introduce two thresholds on the number of training iterations, $T_1 < T$, to control the transition from easy to medium, where T is the total number of training iterations. Typically, we define $p_m(t)$ as the probability of picking the medium task to learn from only the easy and the medium task in Eq. 12, where t represents the iterations. As \u03c4 increases, the learning system should give a higher probability of assigning the medium task, and 1 \u2212 $p_m(t)$ is the probability of assigning the easy task.\nOnce $t \\geq T_1$, we start introducing the hard task. We define the probability of choosing the hard task from on the medium and the hard task $p_h(\\tau|{M, H})$ definition in Eq. 13, and 1 - $p_h(\\tau|{M, H})$ is the probability of assigning the medium task in this case.\nThe final loss for optimization could be formulated under different learning stages Eq. 14, where I(a,b) is an indicator of whether a random variable from the uniform distribution has the probability value between a and b."}, {"title": "LLM-based Generation Process", "content": "As LLMs parse the hybrid prompt with user history data, the inference process predicts the subsequent tokens as the name of the predicted item. The overall generation process of the TMF framework for multi-behavior recommendation is explained in Figure 1. The behavior tokens and"}, {"title": "5 EXPERIMENTS", "content": null}, {"title": "5.1 Datasets and Setup", "content": "Datasets. We consider real-world customer behaviors from Walmart e-commerce platform and sample data from three categories: (1) Electronics, (2) Pets, and (3) Sports. We consider view, add to cart, and purchase to represent customer behaviors on items. We drop the customer behavior sequence without a purchase behavior and convert data from each category to a dataset. Table 1 summarizes the statistics of our datasets. We also report the '#Item/#User Ratio' to indicate the user behavior complexity for each dataset: the higher ratio means users interact with more diverse set of items and show more complicated shopping intents in this dataset.\nImplementation Details. We select Llama2-7B [43] as the backbone model in the LLM-based Recommender. To be compatible with the curriculum learning design, the instruction format for training and testing is randomly sampled from several prompts. The pre-trained encoders for item images and textual information are all from CLIP [37]. The MHBT model [50] generates the item embeddings and behavior embeddings learned from the item-behavior graph. For all LLM-related methods, each experiment is trained for a maximum of 3 epochs, with a batch size of 64 on one A100 GPU. We follow the setting in [31] for learning rate warm-up and grid search. We use Adam Optimizer to optimize the adapter parameters and the LoRA parameters of LLM layers.\nEvaluation Metrics. For each user and the associated behavior sequence, we randomly select 10 non-interacted items to construct the candidate set, and the ground truth is included in the candidate set. TMF and other baseline models are evaluated on identifying the ground truth item out of the candidate set. Their performance"}, {"title": "5.2 Comparison Experiments", "content": "Baselines. We mainly consider three categories of baseline models:\nTraditional Sequential Recommender System:\nGRU4Rec [18], SASRec [23] and Bert4Rec [39] apply Recurrent Neural Network (RNN), attention mechanism and Bert [8] to model user behavioral sequences.\nGraph-based Multi-behavior Recommender Systems:\nMBGCN [22], MBRec [46] and MBHT [50] utilize is a multi-behavior graph neural network and hypergraph enhanced transformer to learn the cross-type behavior interaction patterns.\nLLM-based recommender system:\nLLaRA [31] uses a hybrid prompting method with textual item features that integrates ID-based item embeddings learned by SASRec.\nLlama-2 [43] is an open-source LLM released by Meta. In our experiments, the proposed TMF with Llama2 7B backbone can be reduced to Llama-2 if no modalities from items and behaviors are considered. We use the reduced Llama-2 with the same prompts with TMF as a baseline and fine-tune it on our datasets.\nExperiment Results. We summarized the experiment results in Table 2. We highlight the best scores in bold and the second-best scores with underline. From Table 2, we have the following observations:"}, {"title": "5.3 Ablation Study", "content": "To better understand the effectiveness of different modules in our model, we conduct ablation studies starting from the basic TMF (Llama-2 7B) model, which is only the Llama-2 backbone of the TMF. By adding the behavior tokens, Item ID tokens (AMSA) and CMA layers one by one into the basic TMF, we have three more ablation models and the last one is the complete TMF. The results of the ablation studies are presented in Table 3. From this table, we find:"}, {"title": "5.4 Human Evaluation", "content": "Our TMF framework has been deployed in a real-world recommendation production to generate candidate sets to ranking models. To understand the improvement of the TMF in real-world production, we leverage human raters to evaluate recommendations from TMF and the best non-LLM-based sequential recommender model MBHT under the multi-behavior setting. We sample 100 user behavior sequences from each dataset and ask 15 human raters to score the recommendation from TMF and MBHT, respectively. Raters must consider the given behavior sequences as the context to evaluate their likelihood of purchasing the recommendation. The score ranges from 1 to 5, 1 for totally irrelevant purchases and 5 for tightly"}, {"title": "5.5 Case Studies on Synthetic Data", "content": "We conducted the case study on the Sports and Electronics datasets to illustrate the recommendation generated by the TMF. We synthesize two user behavior sequences under Electronics and Sports, respectively, and use TMF to predict the next purchase. The prediction targets are also curated carefully by human to reflect the underlying relevance within each modality between the prediction and the user behavior sequence. Typically, we following these principles to simulate the real shopping behaviors:\nModal-specific Preference: Item images and textual descriptions reflect (explicitly or implicitly) the user's preference on the visual"}, {"title": "6 CONCLUSION", "content": "In this paper, we introduce the Triple Modality Fusion (TMF) framework to enhance multi-behavior recommendation systems (MBRS) by integrating visual, textual, and graph modalities with the power of large language models (LLMs). The inclusion of diverse data types allows the model to capture a multifaceted understanding of user behaviors and item characteristics, offering a more accurate and contextually relevant recommendation results. We also propose the modality fusion module based on self-attention and cross-attention mechanisms. Our extensive experiments demonstrate the TMF framework's superior performance in improving recommendation accuracy. Additionally, ablation studies affirm the critical role of all modalities and the effectiveness of the cross-attention mechanism used in modality fusion."}]}