{"title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented Instructions", "authors": ["Wanlong Liu", "Junying Chen", "Ke Ji", "Li Zhou", "Wenyu Chen", "Benyou Wang"], "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a key paradigm for enhancing large language models (LLMs) by incorporating external knowledge. However, current RAG methods face two limitations: (1) they only cover limited RAG scenarios. (2) They suffer from limited task diversity due to the lack of a general RAG dataset. To address these limitations, we propose RAG-Instruct, a general method for synthesizing diverse and high-quality RAG instruction data based on any source corpus. Our approach leverages (1) five RAG paradigms, which encompass diverse query-document relationships, and (2) instruction simulation, which enhances instruction diversity and quality by utilizing the strengths of existing instruction datasets. Using this method, we construct a 40K instruction dataset from Wikipedia, comprehensively covering diverse RAG scenarios and tasks. Experiments demonstrate that RAG-Instruct effectively enhances LLMs' RAG capabilities, achieving strong zero-shot performance and significantly outperforming various RAG baselines across a diverse set of tasks. RAG-Instruct is publicly available at https://github.com/FreedomIntelligence/RAG-Instruct.", "sections": [{"title": "1 Introduction", "content": "Retrieval-Augmented Generation (RAG) (Guu et al., 2020; Asai et al., 2024b) enhances large language models (LLMs) by integrating external knowledge through document retrieval, effectively reducing hallucinations and improving performance across diverse tasks (Asai et al., 2023; Jin et al., 2024; Lu et al., 2022; Liu et al., 2024a). Since retrievers are not perfect, and considerable research has shown that noisy retrieval can adversely impact LLM performance (Petroni et al.,"}, {"title": "2 Preliminary Study", "content": "Since retrievers are not perfect, the helpfulness of retrieved documents to the query varies in real-world scenarios. This raises the question: Can existing RAG methods handle complex and various RAG scenarios?\nTo investigate this, we first define five RAG scenarios based on query-document relationships, which we believe cover the majority of RAG use cases: Single-Doc Answer (helpful), Single-Doc Support (midhelp), Useless Doc (helpless), Multi-Doc Answer (helpful), and Multi-Doc Support (midhelp). Detailed definitions for each scenario are provided in \u00a7 3.1.\nNext, we evaluate the performance of existing RAG methods across these five scenarios. Using GPT-40 (Achiam et al., 2023), we categorize questions from two question answering (QA) datasets, Single-hop QA (TriviaQA) and Multi-hop QA (HotPotQA (Yang et al., 2018)), into relevant subsets based on the defined RAG scenarios.\nDetailed prompts for categorization are provided in the Appendix B.1. Then we choose some robust RAG methods, including Self-RAG (Asai et al., 2024a), RQ-RAG (Chan et al., 2024), ChatQA-1.5 and ChatQA-2.0 (Liu et al., 2024b) as baselines"}, {"title": "3 Method", "content": "This section outlines the RAG-Instruct process, focusing on constructing diverse and high-quality synthetic RAG datasets. The detailed architecture is illustrated in Figure 1."}, {"title": "3.1 RAG-Instruct", "content": "Synthesizing RAG Instructions. Recent proprietary models like GPT-40 (Achiam et al., 2023) have demonstrated remarkable capabilities, and many works (Zheng et al., 2023b; Xu et al., 2023; Chen et al., 2023a) based on synthetic datasets have achieved notable success. Therefore, we use GPT-40 to synthesize RAG instructions by leveraging source documents D* to create context-rich instructions. Specifically, GPT-40 synthesizes an instruction q* based on D*, followed by a response a* referencing D*, which can be formalized as:\n(q*, a*) = LLM(D*). (1)\nInspired by work (Zhang et al., 2024), we introduce documents D unrelated to q*, which serve as additional noise to enhance the robustness. Then our target RAG instruction is as follows:\nD*,D\u00ae,q* \u2192a*.\nHowever, RAG instructions generated this way lack diversity in both RAG scenarios and tasks. To address this, we define five RAG paradigms and introduce Instruction Simulation."}, {"title": "3.2 Dataset Construction", "content": "We construct RAG-Instruct using Wikipedia corpus. For each synthesis, we sample an RAG paradigm r, a simulated instruction q', and retrieved source documents D* to generate (q*,a*) using GPT-40. To incorporate unrelated documents D\u00af, we randomly sample documents retrieved based on q* and ranked beyond the top 200 as D\u00af. Additionally, for cases where |D* | \u2265 2, we ensure that the number of source documents is fewer than 5. Subsequently, D*,D\u00ae,q* \u2192 a* is set as the training objective to form RAG-Instruct. In total, we build a dataset of 40K instructions, with the distributions of RAG paradigms and simulated instructions illustrated in Figure 3. More dataset construction details are shown in Appendix A.1."}, {"title": "4 Experiments", "content": "4.1 Experimental Settings\nEvaluation Tasks. We conduct evaluations of our RAG-Instruct and various baselines across 10 tasks in four major categories: (1) Open-Ended Tasks, including WebQA (WQA) (Berant et al., 2013), PopQA (PQA) (Mallen et al., 2023), and TriviaQA-unfiltered (TQA) (Joshi et al., 2017), where models answer open-domain factual questions with accuracy as the metric. (2) Closed-Set Tasks, including OpenbookQA (OBQA) (Mihaylov et al., 2018), PubHealth (Pub) (Zhang et al., 2023) and ARC-Challenge (ARC) (Clark et al., 2018), involving multiple-choice QA with Extract Match (EM) as the metric. (3) Multi-Hop Tasks, including 2WikiMultiHopQA (2WIKI) (Ho et al., 2020), HotpotQA (HotQ) (Yang et al., 2018), and Musique (MSQ) (Trivedi et al., 2022), requiring multi-hop reasoning with accuracy as the metric. (4) Domain-Specific Tasks, CFQA (Chen et al., 2022) in the financial domain and PubMedQA (Jin et al., 2019) in the medical domain, with EM as the metric. We perform zero-shot evaluations throughout these experiments, providing task instructions without few-shot demonstrations. Reasoning details and prompts are provided in Appendix A.2.\nBaselines. We compare our method against a diverse set of baselines, grouped into two main categories: (1) Closed-Source LLMs without RAG, including GPT-40 and GPT-40-mini. We test them using OpenAI's official APIs. (2) Open-source model baselines with RAG, including Llama2-7b (Touvron et al., 2023), Llama3-8b (Dubey et al., 2024). Additionally, we also compare with competitive open-source instruction-tuned LMs, such as Llama3-8b-Instruct, Llama3-70B-Instruct, Llama2-"}, {"title": "4.2 RAG Capability Gains", "content": "Comparison against closed-source LLMs. As shown in Table 4, compared to powerful proprietary models like GPT-40 and GPT-40-mini, our RAG-Instruct, trained on base 8B models, matches or even outperforms them on several tasks, including open-ended tasks (PQA and TQA), multi-hop tasks (HotQA and MSQ), and domain-specific tasks (PubMedQA). This demonstrates that our RAG-Instruct significantly enhances the model's RAG capabilities.\nComparison against RAG-specific models. As shown in Table 4, RAG-specific models such as Self-RAG, and RQ-RAG show significant improvements over the base models on open-ended and closed-set tasks. However, they underperform com-"}, {"title": "4.3 Impact of Instruction Simulation", "content": "To investigate the impact of Instruction Simulation, we design a comparative experiment. We randomly sample a subset Ds containing 20,000 entries from our RAG-Instruct dataset and create another subset D' without using Instruction Simulation. To ensure a fair comparison, Ds and D share the same source documents D* and include all five RAG scenario paradigms. We then train two models on Llama3-8B using Ds and D with identical hyperparameters. As shown in Table 5, removing the Instruction Simulation process results in performance declines across all tasks. The drop is smaller for open-ended tasks but significantly larger for closed-set, multi-hop, and domain-specific tasks. We observe that without Instruction Simulation, GPT-40 tends to generate overly simple and uniform questions, resembling open-ended ones, leading to minimal impact on closed-set evaluation. However, the diverse formats of closed-set, multi-hop, and domain-specific tasks, such as multiple-choice and multi-hop reasoning, pose challenges that the model struggles to handle. This highlights the critical role of Instruction Simulation in enabling the model to adapt to a wide variety of tasks.\nAdditionally, we provide specific cases, as shown in Figure 4, demonstrating that Instruction Simulation generates questions that closely resemble exemplar questions, significantly enhancing diversity compared to those produced without it. Given the high quality and diversity of the synthesized dataset, Instruction Simulation ensures both attributes effectively."}, {"title": "4.4 Role of RAG Paradigms", "content": "To evaluate the role of RAG paradigms, we design an ablation experiment to verify the effectiveness of the five RAG scenarios in RAG-Instruct. Specifically, we remove the data corresponding to each paradigm from RAG-Instruct one at a time and train models on Llama3-8B using identical training hyperparameters, respectively.\nAs shown in Table 6, when a single RAG paradigm (e.g. ro) is removed from RAG-Instruct, we observe a noticeable performance drop in evaluation benchmarks corresponding to that specific RAG scenario. This indicates that each RAG paradigm plays a critical role in enhancing the model's RAG capabilities across different scenarios. Furthermore, we observe that removing multi-document paradigms (r2 and r4) leads to a significant decline in multi-hop performance. Notably,"}, {"title": "4.5 Further Analysis", "content": "Performance in non-retrieval scenarios. Since our RAG-Instruct is built on the Wikipedia corpus, the performance improvements on evaluation benchmarks may stem from knowledge injection during the supervised fine-tuning stage. To investigate whether our approach genuinely enhances the model's RAG capabilities, we compare the performance in both retrieval and non-retrieval scenarios (based on the Llama3-8B model trained on RAG-Instruct). As shown in Table 5, performance in non-retrieval scenarios is significantly lower across all benchmarks compared to retrieval scenarios, demonstrating that RAG-Instruct effectively enhances the model's capabilities in RAG scenarios.\nDifferent retrieval source. To further explore the generalization of our method, we investigate the impact of using different retrieval sources. Specifically, we further evaluate our method on four single-"}, {"title": "5 Related Work", "content": "Retrieval-augmented generation (RAG) is a widely adopted approach for supplementing the parametric knowledge of large language models (LLMs) with external information sources. Due to the imperfections of retrievers, the retrieved information often fails to align well with the LLM's needs, which can negatively impact LLM performance (Petroni et al., 2020; Shi et al., 2023; Maekawa et al., 2024).\nTo enhance LLM-based RAG capabilities, some studies focus on aligning retrievers with LLM needs (Shi et al., 2024; Lin et al., 2023) through multi-step retrieval processes (Trivedi et al., 2023; Jiang et al., 2023; Jeong et al., 2024; Shao et al., 2023; Yu et al., 2023; Asai et al., 2024a; Wei et al., 2024) and query reformulation (Ma et al., 2023; Jeong et al., 2024). On the other hand, several studies focus on enhancing the RAG capabilities of LLMs by improving their robustness in noisy retrieval contexts. Research such as (Chan et al., 2024; Zhang et al., 2024; Liu et al., 2024b; Yoran et al., 2024) trains models with additional irrelevant or noisy documents to better handle such scenarios. However, these approaches consider only a limited range of RAG scenarios. Furthermore, the lack of a general RAG dataset forces many works, such as RAFT (Zhang et al., 2024), to fine-tune models on task-specific datasets, leading to poor task generalization. This highlights the need for a dataset that covers diverse RAG scenarios and tasks."}, {"title": "6 Conclusion", "content": "This work introduces RAG-Instruct, a method for synthesizing diverse and high-quality RAG instruction data from any source corpus. It incorporates five RAG paradigms to capture diverse query-document relationships and uses instruction simulation to enhance data quality and diversity by leveraging existing datasets. Using this approach, we construct a 40K instruction dataset from Wikipedia, covering diverse RAG scenarios and tasks. For future work, we plan to expand the instructions in RAG-Instruct to incorporate chain-of-thought (CoT) characteristics, enabling models to perform planned retrieval based on the query."}, {"title": "Limitations", "content": "Granularity of RAG Paradigms While RAG-Instruct introduces five distinct RAG query paradigms to handle various query-document relationships, this relationship is of a coarse granularity. Specifically, the current set of paradigms focuses on broad categories but does not explore more granular or specialized paradigms that could better capture nuanced retrieval tasks. For instance, for multi-hop queries, the number of hops could be specified, and relevance might have more granular options. Expanding the range of RAG paradigms to cover finer distinctions could enhance the model's ability to handle complex, diverse, and edge-case retrieval situations, thereby improving its robustness and performance.\nReliance on Synthetic Data Our approach relies on synthetic data generation, which inherently carries the risk of introducing errors or biases, even when using powerful large language models like GPT-4. While the use of large-scale instruction datasets such as SlimOrca and Evol Instruct improves the diversity and quality of the generated data, it is still possible for GPT-4 to produce flawed or inconsistent RAG instructions that may negatively impact downstream tasks. As synthetic data generation becomes more prevalent, ensuring the accuracy and reliability of such data remains an ongoing challenge, especially in high-stakes domains where the correctness of information is critical."}, {"title": "A Experimental Details", "content": "A.1 More Details of Training\nDataset Construction. Our RAG-Instruct corpus is built using Wikipedia. Following the approach (Karpukhin et al., 2020), each document is a disjoint text block of up to 100 words extracted from a Wikipedia article. Following work (Shi et al., 2023), we generate Wikipedia document embeddings.\nFor exemplar data, we select datasets such as ShareGPT (Wang et al., 2023a), Alpaca (hin Cheung and Lam, 2023), WizardLM-70K (Xu et al., 2023), Lmsys-chat-1M (Zheng et al., 2023a), and SlimOrca (Mitra et al., 2023). First, we remove overly short, overly long, and low-quality data from these datasets. Then, we randomly sample 120K questions from the filtered data. Since RAG is most effective in knowledge-intensive task scenarios (Maekawa et al., 2024; Shi et al., 2023), we use GPT-40 to further filter for knowledge-intensive instructions from these synthetic datasets. The specific prompt used is shown in Figure 5.\nTraining Details. We train our models using 8 Nvidia A800 GPUs, each with 80GB of memory. All models are trained for 3 epochs with a total batch size of 128, a peak learning rate of 5e-6, 3% warmup steps, and linear weight decay. The maximum token length is set to 4096 for all models. We leverage DeepSpeed Stage 3 (Rajbhandari et al., 2020) for multi-GPU distributed training with BFloat16 precision enabled. FlashAttention (Dao et al., 2022) is employed to improve efficiency during long-context training.\nA.2 More Details of Inference\nWe conduct evaluations of our RAG-Instruct and various baselines across a wide range of downstream tasks, covering 11 tasks in four major categories. Throughout these experiments, we perform zero-shot evaluations, providing task instructions without few-shot demonstrations. For RAG-specific models, we follow the original papers' weights and prompts for inference. For our model and other baselines, reasoning details and prompts are provided in Table 8."}, {"title": "B Detailed Prompts in our Experiments", "content": "B.1 Prompts for dividing the datasets into five RAG scenarios.\nTo explore the performance of RAG methods across five different scenarios, we use GPT-4o to categorize questions from two QA datasets: Single-hop QA (TriviaQA) and Multi-hop QA (HotPotQA), into relevant subsets based on the defined RAG scenarios. The prompts used for categorization are shown in Figure 6 (Single-hop QA) and Figure 7 (Multi-hop QA). The final data volume for each subset is shown in Table 9.\nB.2 Prompts for synthesizing data for five RAG scenarios.\nWe construct five RAG paradigms as described in Figure 8, Figure 9, Figure 10, Figure 11, and Fig-"}, {"title": "C Additionally Experiments", "content": "C.1 Experiments on Different Retrieval Source\nTo further explore the generalization of our method, we investigate the impact of using different retrieval sources. Specifically, we further evaluate our method on four single-hop QA tasks, including ARC, PQA, TQA, and OBQA, utilizing Duck-DuckGo, Wikipedia, and Bing Search as retrieval"}]}