{"title": "Deep Learning and Machine Learning: Advancing Big Data Analytics and Management", "authors": ["Keyu Chen", "Ziqian Bi", "Qian Niu", "Junyu Liu", "Benji Peng", "Sen Zhang", "Ming Liu", "Ming Li", "Jiawei Xu", "Xuanhe Pan", "Pohsun Feng", "Jinlang Wang"], "abstract": null, "sections": [{"title": "What is TensorFlow (TF) for Deep Learning", "content": "TensorFlow (TF) is an open-source platform developed by Google for machine learning and deep learning applications. It provides a flexible architecture for building machine learning models, especially neural networks, making it an essential tool for both beginners and experienced practitioners in deep learning. TensorFlow allows developers to perform computations efficiently on both CPUs and GPUs, supporting high-performance machine learning applications [1].\nAt its core, TensorFlow enables the creation of computational graphs, which represent mathematical operations in the form of a directed graph. These graphs make it easier to visualize and optimize the performance of deep learning models. TensorFlow can handle large datasets and perform complex operations with its optimized execution engine."}, {"title": "TensorFlow Architecture for Deep Learning", "content": "TensorFlow's architecture is designed to provide flexibility and scalability. The core components of its architecture include:\n1. TensorFlow Core: The foundation of TensorFlow, providing low-level API functions for handling tensors and operations. It allows full control over model design and execution.\n2. Tensors: The fundamental data structure in TensorFlow, representing n-dimensional arrays. Tensors can be scalars (0-D), vectors (1-D), matrices (2-D), or higher-dimensional objects.\n3. Graph: TensorFlow uses computational graphs to represent operations. Nodes in the graph represent operations (like addition or multiplication), and the edges between them represent tensors being passed as inputs and outputs.\n4. Session: To execute a computational graph, TensorFlow uses sessions. A session manages the resources and execution of operations within the graph.\n5. Eager Execution: While TensorFlow traditionally relied on constructing computational graphs and then executing them, TensorFlow now supports eager execution. This mode allows operations to be executed immediately, simplifying debugging and interaction."}, {"title": "Key Features of TensorFlow for Pretrained Models", "content": "TensorFlow provides several key features that make it a powerful tool for working with pretrained models:\n\u2022 Model Zoo: TensorFlow offers access to a large collection of pretrained models in the TensorFlow Model Garden, ranging from image classification, object detection, to natural language processing.\n\u2022 TensorFlow Hub: TensorFlow Hub is a repository of reusable machine learning modules that can be easily integrated into new models. These modules include pretrained models and can be fine-tuned for specific tasks.\n\u2022 Transfer Learning: TensorFlow supports transfer learning, a technique where you can take a pretrained model and adapt it to a new, related task by retraining only certain layers. This reduces the need for large amounts of labeled data and computational resources.\n\u2022 TensorFlow Serving: TensorFlow Serving is designed for serving machine learning models in production environments. It provides a flexible and efficient system to serve trained models in real-time.\n\u2022 Model Optimization: TensorFlow has built-in tools for optimizing pretrained models, such as quantization and pruning, which can reduce the model size and improve performance without sacrificing accuracy."}, {"title": "TensorFlow in Pretrained Model Workflows", "content": "TensorFlow plays a critical role in workflows that utilize pretrained models, enabling developers to streamline their processes. Here are the steps typically involved in using TensorFlow with pretrained models:\n1. Model Selection: Begin by selecting a pretrained model from TensorFlow Hub or the TensorFlow Model Garden. These models have been trained on large datasets and can be fine-tuned for specific tasks.\n2. Loading the Model: TensorFlow makes it easy to load pretrained models using its high-level API.\nFor example, in TensorFlow Hub, you can load a model using the following Python code:\n3. Preprocessing Data: Pretrained models often require specific input formats, so it's important to preprocess your data accordingly. TensorFlow provides many tools to help with this, such as the tf. image module for image data manipulation."}, {"title": "Fine-tuning the Model", "content": "You can modify the pretrained model by adding new layers or freezing some of the existing layers. The following code snippet demonstrates how to add new layers to a pretrained model in TensorFlow:\n5. Training the Model: Once your model is set up, you can train it using TensorFlow's high-level Keras API. The training process typically involves specifying the optimizer, loss function, and evaluation metrics.\n6. Model Evaluation: After training, the model can be evaluated on a test dataset to measure its performance.\n7. Deployment: Once the model is trained and evaluated, it can be deployed using TensorFlow Serving or exported as a SavedModel format for future use."}, {"title": "What is a Pretrained Model", "content": "A pretrained model is a machine learning model that has been previously trained on a large dataset, typically using a task that is related to the problem you are trying to solve. This training helps the model to learn general patterns, which can then be fine-tuned or adapted to new, specific tasks. Instead of training a model from scratch, you can leverage the knowledge that the pretrained model has already gained and apply it to your own problem, saving both time and computational resources.\nPretrained models are especially popular in deep learning, particularly in areas like computer vision and natural language processing (NLP), where large datasets and substantial computational power are required to train complex models like convolutional neural networks (CNNs) or transformers."}, {"title": "Definition of Pretrained Models", "content": "A pretrained model is a machine learning model that has been previously trained on a large dataset, typically using a task that is related to the problem you are trying to solve. This training helps the model to learn general patterns, which can then be fine-tuned or adapted to new, specific tasks. Instead of training a model from scratch, you can leverage the knowledge that the pretrained model has already gained and apply it to your own problem, saving both time and computational resources.\nPretrained models are especially popular in deep learning, particularly in areas like computer vision and natural language processing (NLP), where large datasets and substantial computational power are required to train complex models like convolutional neural networks (CNNs) or transformers."}, {"title": "Advantages of Pretrained Models", "content": "Pretrained models offer several advantages, especially for beginners and those with limited computational resources. Some of the key benefits include:\n\u2022 Faster development: Since the model has already learned many of the basic patterns, you don't need to spend as much time training it from scratch. You can fine-tune the model on your specific task, which is generally much faster.\n\u2022 Better performance with limited data: In many cases, you might not have enough data to train a deep learning model from scratch. Pretrained models are helpful because they have already been trained on large datasets and can generalize well even with smaller datasets.\n\u2022 Reduced computational cost: Training deep learning models from scratch can be very computationally expensive. Pretrained models allow you to leverage the power of complex models without the need for high-end hardware or long training times.\n\u2022 Access to state-of-the-art techniques: Many pretrained models are based on cutting-edge research and have been fine-tuned to achieve high accuracy in a variety of tasks. By using these models, you can implement state-of-the-art solutions without needing deep expertise in model design or training."}, {"title": "Common Use Cases of Pretrained Models", "content": "Pretrained models are used in a wide range of applications across different fields. Some common use cases include:\n\u2022 Image classification: Pretrained models like ResNet, VGG, or MobileNet are commonly used for classifying images into different categories. These models have been trained on large image datasets like ImageNet.\n\u2022 Object detection: Models such as YOLO (You Only Look Once) or Faster R-CNN are used for identifying and locating objects within an image.\n\u2022 Natural Language Processing (NLP): Pretrained models such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pretrained Transformer) are used for tasks like text classification, sentiment analysis, and language translation.\n\u2022 Transfer learning: Pretrained models are often used in transfer learning, where the knowledge gained from one task (e.g., image classification) is transferred to a different but related task (e.g., object detection).\n\u2022 Feature extraction: Pretrained models are also used as feature extractors, where the learned features of the model are used as inputs for other models or algorithms."}, {"title": "Pretrained Models in TensorFlow", "content": "TensorFlow provides easy access to a wide range of pretrained models, which can be used for tasks such as image classification, object detection, and text analysis. You can load these models from the TensorFlow Hub, a repository of pretrained models that are ready to use."}, {"title": "How to Use Pretrained Models", "content": "Here's an example of how you can load and use a pretrained model for image classification in TensorFlow:\nIn this example, we load a pretrained MobileNetV2 model from TensorFlow Hub and use it to classify an image. The image is first preprocessed by resizing it to the required input size and normalizing the pixel values. Finally, the model makes a prediction, and we output the predicted class.\nBy using TensorFlow Hub and pretrained models, you can quickly get started with complex machine learning tasks, even if you're a beginner."}, {"title": "How to Use Pretrained Models", "content": "Pretrained models are models that have already been trained on a large dataset, often for a similar task. Instead of training a new model from scratch, we can leverage these pretrained models for tasks such as image classification, object detection, natural language processing, etc. This approach saves both time and computational resources.\nIn this section, we will cover three main methods for utilizing pretrained models: Transfer Learning, Linear Probe, and Fine-Tuning."}, {"title": "Transfer Learning", "content": "Transfer learning is a machine learning technique where a model trained on one task is reused on a different, but related task. It allows us to leverage the knowledge a model has acquired from a large dataset to apply it to a smaller dataset or a new task. This reduces the need for large amounts of data and decreases the training time."}, {"title": "Feature Extraction", "content": "Feature extraction refers to using the pretrained model to extract useful features from the input data, and then using these features in a new model. Typically, only the top layers of the pretrained model (the feature extraction layers) are used, while a new classifier is trained on top of them."}, {"title": "Using Pretrained Weights", "content": "In many cases, pretrained models come with weights that have been learned on large datasets such as ImageNet. These weights can be used directly in new models to enhance performance. Here's an example of how to load pretrained weights."}, {"title": "Linear Probe", "content": "A Linear Probe is a lightweight approach to transfer learning where only the final classifier layer is trained, while all other layers are frozen. This allows for very fast training and serves as a good baseline to determine whether transfer learning is a viable approach for your task."}, {"title": "Training Only the Classifier Layer", "content": "In this case, all of the layers except for the final classifier are kept frozen during training. The classifier layer is initialized randomly and trained on your specific dataset."}, {"title": "Advantages and Disadvantages of Linear Probing", "content": "The main advantage of using a linear probe is its simplicity and speed. Since we only train the final layer, the training process is fast and computationally inexpensive. However, it may not be as powerful as fine-tuning the entire model, especially when the pretrained model is not closely related to the target task.\nLinear Probe: Advantages and Disadvantages\n\u2022 Advantages\nFaster Training\nLess Computationally Expensive\n\u2022 Disadvantages\nMay Not Generalize Well to New Tasks\nLess Accurate on Complex Datasets"}, {"title": "Fine-Tuning", "content": "Fine-tuning is a more advanced transfer learning technique where you unfreeze some or all of the layers in the pretrained model and retrain them on the new dataset. This allows the model to adapt more fully to the new task, improving accuracy, especially if the pretrained model's dataset is not very similar to the new one."}, {"title": "Fine-Tuning the Entire Model", "content": "Fine-tuning the entire model involves unfreezing all layers of the pretrained model and training the entire model on the new data. This allows the weights in all layers to adjust to the specifics of the new task."}, {"title": "Fine-Tuning Specific Layers", "content": "Sometimes it's beneficial to fine-tune only certain layers of the model, especially deeper layers. Early layers often capture general features (such as edges in images) that are useful across a variety of tasks, while deeper layers capture more task-specific features."}, {"title": "When to Use Fine-Tuning", "content": "Fine-tuning is most useful when:\n\u2022 Your new dataset is large and somewhat different from the dataset used to pretrain the model.\n\u2022 You need the model to be highly specialized for your new task.\n\u2022 You have the computational resources and time to fine-tune multiple layers."}, {"title": "Dataset", "content": "In this experiment, we use two classic datasets: CIFAR-10 and ImageNet."}, {"title": "CIFAR-10", "content": "CIFAR-10 is a widely used small image dataset containing 10 classes, with 6,000 images per class, resulting in a total of 60,000 images. The images are of size 32x32 pixels and are colored. The dataset includes categories such as airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. Due to its small size and low image resolution, CIFAR-10 is well-suited for rapid experimentation and prototyping. In this experiment, we use CIFAR-10 as an example to demonstrate the process of training and testing models [10]."}, {"title": "ImageNet", "content": "ImageNet is a large-scale image dataset that spans over 1,000 classes and contains millions of high-resolution images. It plays a central role in the field of computer vision and is widely used for tasks such as image classification, object detection, and feature extraction. Models pre-trained on ImageNet have a broad understanding of the world due to the diversity of the images, making them useful for transfer learning and feature extraction tasks [4]."}, {"title": "Why CIFAR-10 and ImageNet?", "content": "We chose CIFAR-10 as the demonstration dataset primarily because it is small and manageable. This makes it ideal for quick iteration and verifying the feasibility of algorithms in resource-constrained environments. For beginners and researchers, CIFAR-10 serves as a very practical platform for experimentation [10].\nOn the other hand, ImageNet is used due to its vast scale and diverse range of images, making it a common choice for pre-training models. Models pre-trained on ImageNet provide rich, generalizable features that significantly enhance downstream tasks. In this experiment, we utilize ImageNet pre-trained models to extract image features, reducing training cost and improving model generalization [4]."}, {"title": "Comparison Between Linear Probe and Fine-tuning", "content": "In this section, we will compare two approaches for adapting deep learning models to new data: Linear Probe and Fine-tuning. We will use the CIFAR-10 dataset and a pre-trained ResNet-152 model to illustrate the differences between these methods through visualization techniques such as PCA, t-SNE, and UMAP."}, {"title": "Introduction to Dimensionality Reduction", "content": "When working with high-dimensional data, visualizing patterns and relationships can be challenging. Dimensionality reduction techniques help simplify data by reducing the number of features while preserving important information.\nCommon methods include:\n\u2022 Principal Component Analysis (PCA): Projects data onto directions that maximize variance.\n\u2022 t-Distributed Stochastic Neighbor Embedding (t-SNE): Converts similarities between data points into probabilities to preserve local structures."}, {"title": "Linear Probe on 20% of CIFAR-10 Features Extracted by ResNet-152", "content": "In the Linear Probe approach, we use a pre-trained model to extract features from the dataset without updating the model's weights. We then analyze these features to understand how well the pre-trained model represents our data [2]."}, {"title": "Installing Required Libraries", "content": "Before running the code, install the necessary libraries:\npip install tensorflow matplotlib scikit-learn umap-learn"}, {"title": "Python Code for Feature Extraction and Visualization", "content": "The following code performs feature extraction using a pre-trained ResNet-152 model in TensorFlow and visualizes the features using PCA, t-SNE, and UMAP [6, 1, 12, 23, 13]."}, {"title": "Understanding the Code", "content": "In the code above:\n\u2022 We load and preprocess the CIFAR-10 dataset to match the input size of ResNet-152 (224x224 pixels) and apply the necessary preprocessing function.\n\u2022 The dataset is split into 20% for feature extraction (Linear Probe) and 80% for fine-tuning.\n\u2022 We load the pre-trained ResNet-152 model without the top classification layer to use it as a feature extractor.\n\u2022 Features are extracted by passing the preprocessed images through the base model.\n\u2022 The visualize function reduces the features to two dimensions using PCA, t-SNE, or UMAP and saves the plots with the specified filenames."}, {"title": "PCA Visualization", "content": "PCA reduces the dimensionality of the data by projecting it onto directions (principal components) that maximize variance [12]."}, {"title": "t-SNE Visualization", "content": "t-SNE is a non-linear technique that preserves local structures and is particularly good at visualizing clusters [23]."}, {"title": "UMAP Visualization", "content": "UMAP aims to preserve both local and global structures of the data [13]."}, {"title": "Fine-tuning ResNet-152 on the Remaining 80% of CIFAR-10 Data", "content": "Fine-tuning involves updating the weights of a pre-trained model on a new dataset, allowing it to adapt to the specific features of the new data and potentially improve performance."}, {"title": "Python Code for Fine-tuning and Visualization", "content": "The following code fine-tunes the ResNet-152 model on the remaining 80% of the CIFAR-10 data and visualizes the updated features."}, {"title": "Understanding the Code", "content": "In this code:\n\u2022 We prepare the 80% dataset for fine-tuning by preprocessing the images and converting labels to categorical format.\n\u2022 We load the pre-trained ResNet-152 model and add new classification layers suitable for CIFAR-10.\n\u2022 The entire model is compiled and fine-tuned on the 80% dataset for a specified number of epochs.\n\u2022 After training, we define a new model (feature_extractor) to extract features from the fine-tuned model.\n\u2022 The extracted features are then visualized using PCA, t-SNE, and UMAP, and the plots are saved with matching filenames."}, {"title": "VGG (2014)", "content": "The VGG network, introduced in 2014 by the Visual Geometry Group at Oxford University, is a highly influential deep convolutional neural network (CNN). The key feature of VGG is its use of small 3x3 convolutional filters and the depth of the network. The deeper the network, the better its capacity to learn complex patterns from images. In this section, we will discuss the structure of the VGG family (VGG-11, VGG-13, VGG-16, and VGG-19), explain their components, and provide code examples for VGG-16 [18]."}, {"title": "Comparison of VGG Architectures", "content": "Below is a comparison of the main components in each VGG architecture. The table compares the number of convolutional layers, max-pooling layers, and fully connected layers (classification head) in each model. This table follows the format from the original VGG paper."}, {"title": "Explanation of the Components", "content": "Conv3-64: The first block consists of 3x3 convolutional filters applied to the input image (or feature map). The number \"64\" refers to the number of feature maps (or channels) output by the layer. Deeper models (VGG-13, VGG-16, VGG-19) have more convolutional layers to capture finer details in the data.\nConv3-128: In the second block, the convolutional filters continue to extract features from the previous layer's output, increasing the number of channels to 128, allowing the network to capture more complex patterns.\nConv3-256: The third block increases the number of feature maps to 256, further refining the information captured from the image. Models like VGG-16 and VGG-19 use additional layers here to extract more detailed information.\nConv3-512: The fourth and fifth blocks increase the number of feature maps to 512, capturing very high-level abstract features from the input. Deeper models like VGG-16 and VGG-19 have more layers here, making them more powerful at recognizing complex patterns in images.\nMaxPooling: Max-pooling layers are inserted after each set of convolutional layers. These layers reduce the spatial size of the feature maps, which helps in reducing the computational cost and the number of parameters, while retaining important spatial features.\nClassification Head: All VGG models have an identical classification head. This consists of three fully connected layers: - First fully connected layer: 4096 units - Second fully connected layer: 4096 units - Final fully connected layer: 1000 units with softmax activation for classification into 1000 categories."}, {"title": "Design Philosophy of VGG", "content": "The VGG architecture is built with simplicity in mind. Instead of using large convolutional filters, VGG opts for small 3x3 filters, which allows the network to increase depth (number of layers) while keeping the computational complexity manageable. This depth gives the network greater capacity to learn more intricate patterns in the data."}, {"title": "TensorFlow Code for VGG-16", "content": "Now that we have explained the components, let's implement the VGG-16 model using TensorFlow:"}, {"title": "Key Insights for Beginners", "content": "Why 3x3 filters? Small filters like 3x3 allow VGG to increase depth, capturing more complex patterns while keeping computational costs under control.\nWhy use MaxPooling? Pooling reduces the spatial size of the data progressively, preventing the model from becoming too large and reducing the chances of overfitting.\nFully Connected Layers: The fully connected layers at the end are responsible for interpreting the high-level features and making the final classification."}, {"title": "VGG16", "content": "The VGG16 model is widely used in image classification tasks. Here, we will use the VGG16 model pre-trained on the ImageNet dataset and apply transfer learning to the CIFAR-10 dataset using two approaches: Linear Probe and Fine-tuning. The CIFAR-10 dataset contains images of size 32x32 pixels, so we will resize them to 224x224 pixels to match the input size expected by VGG16 [18].\nLinear Probe In the Linear Probe approach, we freeze the pre-trained VGG16 model's convolutional layers and train only the classification layers on CIFAR-10."}, {"title": "VGG (2014)", "content": "In this code, we load the CIFAR-10 dataset, resize the images to 224x224 pixels, and normalize the pixel values. The VGG16 model is loaded without its top classification layers, and we freeze all of its convolutional layers. A custom classification head is added, consisting of a Flatten layer and two Dense layers. The model is trained for 10 epochs, and the test accuracy after training is printed.\nFine-tuning Fine-tuning involves unfreezing some or all of the pre-trained layers and training them along with the classification layers. This allows the model to better adapt the pre-trained features to the new dataset."}, {"title": "VGG19", "content": "VGG19 is a deeper version of VGG16, with more convolutional layers. Here, we apply both the Linear Probe and Fine-tuning methods to VGG19, similar to what we did with VGG16 [18]."}, {"title": "INCEPTION (2015)", "content": "The Inception network, also known as GoogLeNet, was introduced in 2015 by Szegedy et al. in the paper \"Going Deeper with Convolutions.\" Inception's key contribution was to allow for increased depth and width of neural networks without excessively increasing computational complexity. This was achieved by introducing the Inception module, a structure that applies multiple convolution filters of different sizes in parallel, allowing the network to capture features at various scales [20].\nInception was highly successful, winning the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) in 2014, and laid the foundation for later versions such as Inception-v3 and Inception-v4.\nThe Inception Module The core idea of the Inception module is to apply multiple types of operations to the same input and concatenate the outputs. These operations include: - 1 \u00d7 1 convolutions, which are used for dimensionality reduction. - 3 \u00d7 3 and 5 \u00d7 5 convolutions, which capture spatial features at different scales. - 3\u00d73 max pooling, which reduces spatial dimensions and adds more robust features.\nThe benefit of the Inception module is that it allows the network to look at the input in different ways (i.e., at different resolutions) without having to decide in advance what size of convolution filter to use. This multi-scale approach improves the model's ability to capture diverse types of patterns in the input data."}, {"title": "Mathematical Explanation of the Inception Module", "content": "For a given input $x$, the output of an Inception module consists of the concatenation of four parallel paths: 1. 1 \u00d7 1 convolution: This reduces the depth (number of channels) of the input before applying larger convolutions. 2. 3 \u00d7 3 convolution: This captures medium-scale features. 3. 5 \u00d7 5 convolution: This captures larger-scale features. 4. 3 \u00d7 3 max pooling: This reduces spatial size and captures prominent features.\nLet $f_{1\u00d71}(x)$, $f_{3\u00d73}(x)$, and $f_{5\u00d75}(x)$ represent the outputs of the 1 \u00d7 1, 3 x 3, and 5 \u00d7 5 convolutions, and $f_{pool}(x)$ the output of the max pooling layer. The overall output $y$ of the Inception module is the concatenation of these outputs:\n$y = concat(f_{1\u00d71}(x), f_{3\u00d73}(x), f_{5\u00d75}(x), f_{pool}(x))$\nThe concatenation step allows the network to combine information from different scales."}, {"title": "Inception Module Diagram", "content": "Below is an updated diagram illustrating the structure of the Inception module, which includes parallel convolution and pooling operations:"}, {"title": "Comparison of Inception Variants", "content": "The Inception architecture has evolved through several versions (v1, v2, v3, v4). Each version introduces modifications to improve accuracy and efficiency. Below is a table comparing the key components of Inception-v1, Inception-v3, and Inception-v4."}, {"title": "Explanation of the Components", "content": "1x1 Convolutions: Used primarily for dimensionality reduction, allowing the larger 3 \u00d7 3 and 5 \u00d7 5 convolutions to be applied efficiently without drastically increasing the number of parameters.\n3x3 Convolutions: Captures medium-sized spatial features. In later versions like Inception-v3 and Inception-v4, the 3 \u00d7 3 convolutions are factorized into two consecutive 3 \u00d7 3 convolutions for better computational efficiency.\n5x5 Convolutions: Used in Inception-v1 to capture larger-scale spatial features. In later versions, this is replaced by two consecutive 3 \u00d7 3 convolutions, which are more computationally efficient but capture the same effective receptive field.\nMaxPooling and AveragePooling: Pooling layers help reduce the spatial size of the feature maps, making the network more computationally efficient and robust to spatial variance in the input.\nAuxiliary Classifiers: These are intermediate classifiers added during training to help with gradient propagation and to regularize the network. They are not used during inference but improve training stability."}, {"title": "Design Philosophy of Inception", "content": "The design philosophy behind Inception is to handle computational complexity by allowing the network to process information at multiple scales simultaneously. The use of 1 x 1 convolutions helps reduce the dimensionality of the input before applying larger convolution filters, thereby reducing the overall number of parameters. This efficiency allowed Inception to build deeper networks with greater accuracy without a corresponding increase in computational cost."}, {"title": "TensorFlow Code for Inception Module", "content": "Below is the implementation of an Inception module using TensorFlow tf.keras."}, {"title": "Key Insights for Beginners", "content": "Why use multiple convolution filters in parallel? The Inception module applies multiple convolution filters of different sizes (e.g., 1 \u00d7 1, 3 \u00d7 3, and 5 \u00d7 5) in parallel. This allows the network to capture features at different spatial scales, improving its ability to recognize patterns in the data.\nWhat is dimensionality reduction with 1\u00d71 convolutions? 1\u00d71 convolutions are used to reduce the number of input channels before applying larger filters, which significantly reduces the computational complexity of the network."}, {"title": "InceptionV3", "content": "InceptionV3 is a convolutional neural network architecture widely used for image classification. It improves upon earlier Inception models by using factorized convolutions, which reduce computational cost. In this section, we will explore how to apply Linear Probe and Fine-tuning on InceptionV3 using the CIFAR-10 dataset [20].\nLinear Probe In the Linear Probe approach, we freeze the convolutional layers of the pre-trained InceptionV3 model and train only the classification layers."}, {"title": "Inception (2015)", "content": "In this approach, we unfreeze the last four layers of VGG19 and fine-tune the model for another 10 epochs. By using a lower learning rate, we carefully update the pre-trained weights. The test accuracy after fine-tuning is printed to show how this method improves performance compared to the linear probe approach."}, {"title": "InceptionResNet", "content": "Inception ResNet combines the strengths of both Inception and ResNet architectures, using residual connections to improve training stability and performance. Below, we explore InceptionResNetV2 with both Linear Probe and Fine-tuning approaches [19].\nInceptionResNetV2\nInceptionResNetV2 is an architecture that combines the inception modules with residual connections. Here, we apply both Linear Probe and Fine-tuning on the CIFAR-10 dataset.\nLinear Probe In the Linear Probe approach, we freeze the convolutional layers of the pre-trained Inception ResNetV2 model and train only the classification layers."}, {"title": "ResNet (2015)", "content": "ResNet, or Residual Networks, were introduced in 2015 by Kaiming He and colleagues to address the vanishing gradient problem that occurs in very deep networks. Deep networks are more powerful in learning complex patterns from data, but as depth increases, training becomes more difficult due to problems like vanishing gradients. ResNet introduces the concept of residual learning, which enables networks to be trained with very deep architectures, such as 50, 101, and even 152 layers, without degradation in performance."}, {"title": "Why Use Residual Learning?", "content": "The main challenge in training deep networks is the vanishing gradient problem, which occurs when the gradients of the loss function with respect to the network parameters become extremely small during backpropagation. This problem is exacerbated in very deep networks, where information needs to propagate through many layers. As a result, early layers in the network receive minimal updates, leading to slow learning and poor performance.\nThe issue arises due to the chain rule of differentiation, also known as chain-based backpropaga- tion. In a deep network, each layer's gradients are calculated as the product of the gradients from the layers before it. For very deep networks, this can lead to exponentially small gradients if the derivative values are less than 1, causing earlier layers to stop learning effectively.\nMathematically, the gradient of the loss $L$ with respect to the weights $W_n$ of layer $n$ is given by:\n$\\frac{\u2202L}{\u2202W_n} = \\frac{\u2202L}{\u2202h_{n+1}} \\cdot \\frac{\u2202h_{n+1}}{\u2202h_n} \\cdot \\frac{\u2202h_n}{\u2202W_n}$\nAs the network depth increases, $\\frac{\u2202h_{n+1}}{\u2202h_n}$ can become very small, leading to vanishing gradients."}, {"title": "How Does ResNet Solve This?", "content": "ResNet addresses this problem by introducing skip connections, or shortcut connections, that allow the gradient to bypass certain layers, helping to preserve the gradient signal during backpropagation. This is achieved by learning a residual function [6]:\n$y = F(x, {Wi}) + x$\nHere, $x$ is the input to a residual block, $F(x, {Wi})$ is the residual mapping to be learned, and $y$ is the output of the block. The key idea is that the network only needs to learn the residual function $F(x, {Wi})$, making it easier to optimize deep networks. This allows deeper networks to be trained without the risk of vanishing gradients."}, {"title": "Mathematical Explanation of the Residual Block", "content": "In a standard deep neural network", "becomes": "n$y = F(x) + x$\nThis skip connection ensures that if $F(x)$ is close to zero, the model can still learn the identity"}]}