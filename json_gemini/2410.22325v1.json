{"title": "ROBOTS PRE-TRAIN ROBOTS: MANIPULATION-CENTRIC ROBOTIC REPRESENTATION FROM LARGE-SCALE ROBOT DATASET", "authors": ["Guangqi Jiang", "Yifei Sun", "Tao Huang", "Huanyu Li", "Yongyuan Liang", "Huazhe Xu"], "abstract": "The pre-training of visual representations has enhanced the efficiency of robot learning. Due to the lack of large-scale in-domain robotic datasets, prior works utilize in-the-wild human videos to pre-train robotic visual representation. Despite their promising results, representations from human videos are inevitably subject to distribution shifts and lack the dynamics information crucial for task completion. We first evaluate various pre-trained representations in terms of their correlation to the downstream robotic manipulation tasks (i.e., manipulation centricity). Interestingly, we find that the \u201cmanipulation centricity\" is a strong indicator of success rates when applied to downstream tasks. Drawing from these findings, we propose Manipulation Centric Representation (MCR), a foundation representation learning framework capturing both visual features and the dynamics information such as actions and proprioceptions of manipulation tasks to improve manipulation centricity. Specifically, we pre-train a visual encoder on the DROID (Khazatsky et al., 2024) robotic dataset and leverage motion-relevant data such as robot proprioceptive states and actions. We introduce a novel contrastive loss that aligns visual observations with the robot's proprioceptive state-action dynamics, combined with an action prediction loss and a time contrastive loss during pre-training. Empirical results across four simulation domains with 20 robotic manipulation tasks demonstrate that MCR outperforms the strongest baseline by 14.8%. Additionally, MCR significantly boosts the success rate in three real-world manipulation tasks by 76.9%. Project website: robots-pretrain-robots.github.io.", "sections": [{"title": "1 INTRODUCTION", "content": "Grounding robots with generalizable visual representations, termed robotic representation, is crucial for real-world visuomotor control. Pre-training robotic representations on extensive in-domain data offers a promising strategy for robotics, drawing from the success of large-scale pre-training in computer vision (He et al., 2022) and natural language processing (Devlin et al., 2019). However, due to the scarcity of robotics data and high collection costs, recent studies have utilized everyday human operation videos (Grauman et al., 2021; Goyal et al., 2017) to pre-train visual representations for robotic manipulation, positing that knowledge behind human manipulation can inform representations in question. Various levels of human manipulation knowledge \u2013 such as task semantics (Nair et al., 2022), pixel-level comprehension (Xiao et al., 2022; Majumdar et al., 2023), and physical interaction (Jia et al., 2024; Srirama et al., 2024)- have demonstrated their effectiveness in benefiting robotic representation. A fundamental question naturally arises: (Q1) What specific features captured from human data significantly contribute to improving robotic manipulation?\nTo investigate this question, we conduct a comprehensive evaluation of prior representations concerning their policy learning performance across various downstream simulation domains. Interestingly, we observe a correlation between a representation's downstream performance and its ability to capture manipulation-relevant regions including robot end-effectors and task-relevant objects. To examine this correlation more formally, we introduce a metric, 'manipulation centricity', and propose an evaluation protocol to quantify it across different representations. The core of this protocol is to measure the similarity between ground truth manipulation regions and the focus of the representation. Our results indicate a strong correlation between downstream performance and manipulation centricity, as illustrated in Figure 2. These findings provide valuable insights into our first question: (A1) Manipulation centricity emerges as the key factor contributing to enhanced robotic manipulation performance.\nHowever, human videos not only introduce inherent distribution shifts due to the human-robot embodiment gap, but also lack the dynamics information essential for successful task execution. This naturally leads to the second question: (Q2) Is there a better dataset choice than human dataset to learn manipulation centricity in robotic representation? With the recent emergence of large-scale robot datasets (Collaboration et al., 2024; Walke et al., 2023), we hypothesize that the smaller domain gap presented by these datasets may naturally be more suitable for learning manipulation centricity. To prove this hypothesis, we re-train the prior method with a representative robot dataset, DROID (Khazatsky et al., 2024), and indeed observe significant improvements in both performance and manipulation centricity. This answers our second question: (A2) Large-scale robot datasets can be a better choice than human datasets for learning manipulation centricity.\nRecognizing that robotic datasets provide more relevant information about robot embodiment trajectories, this raises the third question: (Q3) How to learn manipulation centricity better with large-scale robot datasets? Our starting point is the dynamics labels, including robot proprioceptive states and actions, in the robot dataset but absent in the human dataset. We consider that these dynamics labels contain the core knowledge behind accomplishing a manipulation task, which has not been explicitly utilized in previous pre-trained robotic representations from human data. To this end, we introduce a new method, Manipulation Centric Representation (MCR), designed to leverage dynamics labels from robot dataset to improve manipulation centricity of robotic representation."}, {"title": "2 EXPERIMENTAL SETUP: EVALUATING ROBOTIC REPRESENTATIONS", "content": "This section outlines the experimental setup used to evaluate the effectiveness of pre-trained visual representations for robotic manipulation. In line with prior work (Nair et al., 2022; Xiao et al., 2022), we freeze the pre-trained encoders and utilize Imitation Learning (IL; Argall et al. (2009)) for downstream policy learning. Accordingly, the quality of the visual representations is assessed based on IL performance averaged across various downstream tasks.\nEvaluation protocol. To assess a visual encoder \\(F_\\theta\\), which maps an RGB image \\(I \\in \\mathbb{R}^{H \\times W \\times D}\\) to a continuous feature vector \\(z = F_\\theta(I)\\), we introduce a policy network \\(\\pi_\\theta\\) built atop the frozen encoder \\(F_\\theta\\). This policy network takes as input the feature vector z and the robot's proprioceptive state s, outputting an action distribution \\(\\mathbf{a} \\sim \\pi_\\theta(\\cdot|z, s)\\). To train the policy, a task-specific demonstration dataset \\(D_{IL} = \\{\\tau_1, \\tau_2, ..., \\tau_n\\}\\) is collected, where each demonstration \\(\\tau_i\\) is a trajectory consisting of a sequence of expert-level observation-action pairs \\(\\tau_i = \\{(I_t, S_t, a_t)\\}_{t=1}^T\\). Then, the policy is optimized via the Behavior Cloning (BC; Bain & Sammut (1995)) algorithm, where the optimization objective is to minimize the error between the predicted action a and the optimal action a from demonstration data. For a fair comparison, we employ the same BC algorithm for any used pre-trained encoder in each task. During evaluation, the policy is executed in a closed-loop manner within the environment with online feedback to test its success rate on the target manipulation task. We evaluate at least 20 episodes every fixed number of training epochs, selecting the highest success rate. The mean and standard error of success rates across three seeds are reported.\nSimulation environments and datasets. We select a total of 20 tasks across 4 simulation environments to evaluate representation quality. These tasks encompass a range of end-effectors, including grippers and dexterous hands, along with diverse robot arms and varying levels of manipulation complexity. Task visualizations are shown in Figure 3, with additional details provided in Appendix A.1."}, {"title": "3 INVESTIGATION: MANIPULATION CENTRICITY OF REPRESENTATIONS", "content": "Prior work utilizes in-the-wild human videos to assist robotic representation learning, yet the domain gap between human and robotic tasks may influence the representation quality. To investigate this problem, we take an initial step by visualizing these representations in simulated tasks. Interestingly, we observe that a representation's downstream task performance appears to correlate with its ability to capture manipulation-relevant regions. To investigate this correlation more formally, we introduce a metric, 'manipulation centricity', and propose an evaluation protocol to measure it across different methods. Our results indicate a strong correlation between manipulation centricity and downstream task performance, which strongly guides our method design in the next section. We introduce the whole pipeline in the following parts and present more details in Appendix B.\nFeature visualization & motivation. We use Gradient-weighted Class Activation Mapping (Grad-CAM; Selvaraju et al. (2017)), a widely-adopted network visualization technique in computer vision, to analyze the features of existing representations. Grad-CAM highlights the regions of an input image that are most influential in the model's decision-making process. In our context, this technique reveals how the network interprets images within manipulation tasks, as shown in Table 1. Specifically, in the Square task, we observe that the MVP model tends to focus on irrelevant regions, such as the table, rather than the manipulator or objects, while in the Pick Place Wall task, R3M exhibits a similar pattern. Both cases correspond to poor downstream performance. In contrast, representations that emphasize the robot's end-effectors and objects are associated with better downstream performance. This motivates our investigation into whether representation quality is linked to its ability to capture these 'manipulation-relevant' regions, a property we define as manipulation centricity."}, {"title": "Measuring manipulation centricity", "content": "To quantify manipulation centricity, we compute the similarity between the regions highlighted by Grad-CAM and the ground truth regions corresponding to the end-effector and task-relevant objects. The ground truth is generated using the powerful video segmentation model SAM 2 (Ravi et al., 2024), with manual annotation of key points within the target regions. This approach enables efficient annotation across an entire video of task execution. We apply this annotation method to create segmentation masks for demonstration data \\(D_{IL}\\) across all simulated tasks, forming our evaluation dataset. As illustrated in Figure 4, manipulation centricity is then measured by averaging the Jaccard Index, a widely-used similarity metric in image segmentation, between the binarized Grad-CAM outputs and the ground truth segmentation masks over the entire evaluation dataset. All the ground truth annotations are available in Appendix B.\nKey findings. The aggregate results, presented in Figure 2, reveal a strong correlation between manipulation centricity and downstream task performance, with a Pearson correlation coefficient of R = 0.93. We also evaluate this correlation within individual simulation domains, where the evaluation dataset consists of demonstrations specific to each domain. The positive correlation remains consistent across all domains, albeit with varying strengths. In summary, these results suggest that manipulation centricity measured on our entire evaluation dataset is a reliable indicator of the effectiveness of robotic representations."}, {"title": "4 MCR: LEARNING MANIPULATION-CENTRIC REPRESENTATION", "content": "Drawing from the conclusions in Section 3, our focus shifts to improving the manipulation centricity of robotic representations in this section. This is achieved by two attempts in our proposed method, Manipulation Centric Representation (MCR). First, we find that re-training existing models with large-scale robot data (i.e., DROID (Khazatsky et al., 2024)) instead of human data can significantly improve manipulation centricity. This indicates the value of utilizing robot-specific datasets, as discussed in Section 4.1. Second, we introduce novel pre-training objectives that leverage the robot's state-action dynamics-information provided in robot data, which is inherently absent in human datasets, to further enhance manipulation centricity, detailed in Section 4.2."}, {"title": "4.1 LARGE-SCALE ROBOT DATASET", "content": "Dataset processing. In recent years, several large-scale robot datasets have been introduced (Brohan et al., 2023; Walke et al., 2023; Collaboration et al., 2024; Khazatsky et al., 2024). Among these, we select the DROID dataset for our method because of its extensive scene diversity and a large volume of data. The dataset is collected using the Franka robot arm and Robotiq 2F-85 gripper via teleoperation, comprising a total of 76k trajectories. Each trajectory includes RGB images from two external Zed 2 cameras, robot proprioceptive states, and actions consisting of delta 6D poses and 1-DoF gripper actions. To ensure data quality, we filter out trajectories with fewer than 40 timesteps to ensure adequate temporal information in each trajectory. Additionally, trajectories containing incomplete or single-word language instructions are removed, as these may indicate lower-quality interactions. After processing, we retain 36k trajectories for pre-training.\nA motivating discovery. Many previous methods rely on human-object interaction datasets to learn manipulation concepts from human behavior (Nair et al., 2022; Xiao et al., 2022; Srirama et al., 2024). However, the domain gap between human hands and robotic manipulators may inherently affect representation quality. We re-train the R3M model using the robot-specific DROID dataset, yielding a variant we call R3M-DROID (equivalent to R3M*). As expected, Grad-CAM visualizations in Table 1 show that R3M-DROID better captures manipulation-relevant regions, and quantitative results downstream performance, as shown in Figure 2, confirms this improvement. This suggests that robot-specific data inherently benefits representation learning by narrowing the domain gap between training and deployment environments."}, {"title": "4.2 TRAINING MCR", "content": "Unlike human datasets, robot datasets provide access to manipulator dynamics, including proprioceptive states and actions. While previous approaches do not fully leverage this dynamics information, we hypothesize that incorporating it will enhance manipulation centricity. To achieve this, we design two new training objectives that explicitly leverage robot dynamics. Additionally, we adopt a semantic learning loss from prior work (Nair et al., 2022) to retain semantic information in the representations. These objectives and their integration into our training process are detailed below.\nDynamics alignment. Our first insight is that each image observation corresponds to an underlying proprioceptive robot state at every timestep. We aim to learn this correspondence, termed dynamics alignment, through contrastive learning. Formally, we define a state-action dynamic chunk of length l at timestep t as \\(d_t = [S_{t-l}, a_{t-l}, S_{t-l+1}, ..., S_{t+l}]\\). The positive sample for \\(d_t\\) is its corresponding RGB image \\(I_t\\) at the same timestep, while the negative samples are drawn from a different timestep k within the same trajectory. During training, we randomly choose t and k. The encoder \\(F_\\theta\\) is trained to discard irrelevant details from high-dimensional images and retain the essential information for manipulation. We employ the InfoNCE loss (van den Oord et al., 2019) for contrastive learning, introducing an MLP projector H to map the dynamics chunk \\(d_t\\) to the same dimension as the image feature vector \\(z = F_\\theta(I)\\). The objective function is illustrated in Figure 5 and formalized as:\n\\(L_{dyn} = - \\sum_{b \\in B} \\log \\frac{e^{S(z, H(d_t)_b)}}{e^{S(z, H(d_t)_b)} + e^{S(z, H(d_k)_b)}},\\)\nwhere S represents the negative L2 distance, and b denotes a sample from the batch B.\nAction prediction. We also integrate a behavior cloning (BC)-like actor into our pre-training framework, based on the idea that robotic representations should be predictive of expert-level behaviors in the dataset. The actor is implemented as a shallow MLP head that maps the image feature vector \\(Z_t\\) to the predicted robot actions \\(\\hat{a}_t\\). We use mean squared error as the objective for action prediction:\n\\(L_{act} = \\sum_{b \\in B} MSE(\\hat{a}_t, a_t).\\)\nTemporal contrast. We also wish the representation to encode temporal information, which has shown importance for manipulation tasks (Zhao et al., 2023). To this end, we adopt the time-contrastive learning objective from Nair et al. (2022), which encourages temporally close frames in a video to be closer in the embedding space than those that are temporally distant or from different videos. For this, we sample a frame triplet \\((I_u, I_v, I_w)\\) where u < v < w, and compute the following loss:\n\\(L_{tcl} = - \\sum_{BEB} \\log \\frac{e^{s(z_u, z_v)}}{e^{s(z_u, z_v)} + e^{s(z_u, z_{w})} + e^{s(z_u, z^{'})}},\\)\nwhere \\(z^{'}\\) is a negative sample from a different video within the batch B.\nOveall objective & implementations. We train MCR with a combination of introduced objectives:\n\\(L_{MCR} = L_{dyn} + L_{act} + L_{tcl}.\\)\nWe do not introduce additional hyperparameters for weighting these objectives, as \\(L_{MCR}\\) already yields strong empirical performance. Our default encoder backbone is ResNet-50, and we use Adam (Kingma, 2015) as the optimizer. The encoder is trained for 500k steps to ensure convergence, and we select the last checkpoint as the final model. The whole training process is used 50 hours with a single NVIDIA 3090. Further training details are provided in Appendix A."}, {"title": "5 EVALUATION AND ANALYSIS OF MCR", "content": "Our experiments are conducted in simulation and the real world to answer the following questions:\n(1) Does MCR learn manipulation centricity and outperform baselines in simulation? (Section 5.1)\n(2) Can the conclusions from (1) generalize to real-world manipulation tasks? (Section 5.2)\n(3) Which design choices of MCR matter for learning manipulation centricity? (Section 5.3)\n(4) What benefits does large-scale robot data bring to robotic representation learning? (Section 5.4)\n(5) How many training computational resources does MCR require? (Section 5.5)"}, {"title": "5.1 SIMULATION RESULTS", "content": "MCR does improve manipulation centricity. We begin by presenting visualizations of Grad-CAM results for MCR in Table 1 (see all results in Table 8). Qualitatively, our representation excels at capturing manipulation-relevant regions of each task. For instance, in the Robomimic Square task, MCR focuses on the gripper, square tool, and the target area-key elements of the task. In contrast, other methods like VC-1 and MVP either fail to highlight these areas, or, like R3M and HRP, only capture them partially. This improved localization of task-relevant features is further confirmed quantitatively in Figure 2, where MCR significantly enhances manipulation centricity across all domains. Notably, MCR even highlights the optimal path the end-effector should follow, suggesting that our representation also learns essential information related to task execution.\nMCR outperforms baselines on simulation tasks. Thanks to its improved manipulation centricity, MCR delivers substantial downstream performance gains compared to the strongest baseline methods across all selected domains, as visualized in Figure 6. This holds even in the DexArt domain, which involves a dexterous hand as the end-effector\u2014a setup different from the gripper used in the DROID pre-training dataset. Moreover, in the MetaWorld domain, where prior baselines struggle and perform no better than the Learning-from-Scratch (LfS) method, a strong baseline with data augmentation implemented by Hansen et al. (2023), MCR maintains a significant advantage. We attribute the poor performance of baselines in MetaWorld to the limited number of demonstrations, which makes it difficult for policies to leverage the pre-trained representations. In contrast, MCR reduces the policy's learning burden by providing manipulation-centric features that better fit the task. In summary, our experiments suggest that pre-training with large-scale robot data, along with optimizing for manipulation centricity, yields representations that significantly improve performance in simulated manipulation tasks."}, {"title": "5.2 REAL ROBOT RESULTS", "content": "Experimental setup. As visualized in Figure 7, our real-world experiments involve a UR5e arm equipped with a Robotiq 2F-85 gripper and a RealSense D435i camera for RGB image capture. We designed three tabletop manipulation tasks with different objects and manipulation skills:\n\u2022 Lift. The robot grips the sandbag on the plate and lifts it up in the sky.\n\u2022 Sweep. The robot grasps the broom to sweep the trash from the table into the dustpan.\n\u2022 Rearrange. The robot picks up the on-table pot and places it at the designated spot on the stove.\nThe demonstrations used for BC-based policy training are collected by keyboards with the human operator, with 30 collected for Lift and 40 for more difficult tasks Rearrange and Sweep. The pre-trained representations are frozen during policy training, inheriting the simulation setup. For a fair comparison, we evaluate each method with the same sets of start-up conditions, which are unseen in demonstrations, in each task. More experimental details can be found in Appendix A.5.\nMCR significantly surpasses baselines on real robot tasks. The evaluation results, shown in Table 2, demonstrate that MCR consistently outperforms the baseline methods across all tasks. In the Lift and Rearrange tasks, baseline methods often fail to grasp the object accurately, particularly when object positions were unseen in the demonstrations. In the Sweep task, their performance deteriorates further due to poor accuracy in sweeping, with the trash frequently missing the dustpan and rolling off in unintended directions. In contrast, the policies trained with our representation exhibit robust and generalizable handling of these complex tasks."}, {"title": "5.3 ABLATION STUDIES", "content": "The results presented in Table 4 indicate the effect of key design choices in our approach, offering insights and guidance when employing our manipulation-centric representations. All experiments are conducted on three challenging tasks: Can from Robomimic, Stick Pull from MetaWorld, and Laptop from DexArt, covering all robot arms and end-effectors in prior simulation experiments.\nAll objectives improve manipulation centricity. We begin by evaluating the effect of each training objective outlined in Equation (4) through an ablation study. Our results reveal that all objectives are essential to achieving strong downstream performance. In particular, the dynamics alignment and action prediction losses have the most significant effect. We attribute this to their ability to effectively utilize dynamics-relevant information in the robot dataset, which enhances manipulation centricity. Additionally, the temporal contrastive loss also plays a crucial role by capturing the temporal dependencies in the video sequences. Collectively, this analysis supports our key claim: learning manipulation-centric representations is beneficial for robotic manipulation, and our proposed objectives substantially improve this capability.\nMedium dynamic chunk length works best. Recall that dynamic chunks define the temporal horizon of robot state-action pairs utilized for modeling robot dynamics, as specified in Equation (1)."}, {"title": "5.4 ANALYSIS ON ROBOT DATASET", "content": "Besides the studies on methodology design, we also reveal more insights into the utilization of robot dataset in learning our robotic representations.\nLarger dataset, better performance. We begin by examining how the size of the robot dataset affects pre-training outcomes. Specifically, we reduced the data in each DROID scene from 100% to 25% and assessed the downstream performance, as shown in Figure 9. Our findings indicate that larger datasets contribute to improved representation quality. This seemingly contrasts with previous research (Dasari et al., 2023), which suggested that merely increasing dataset size may not yield benefits. We remark that the key differences here are (1) our use of robot data rather than human data, and (2) the incorporation of additional robot dynamics information. These factors enhance the scalability and effectiveness of MCR when applied to larger robot datasets. While some studies have attempted to combine human and robot data (Dasari et al., 2023; Majumdar et al., 2023), little improvement was observed, likely due to the insufficient utilization of robot dynamics. In summary, our method effectively scales with dataset size by leveraging dynamics information.\nGreater benefits for tasks with less embodiment gap. Next, we investigate which downstream tasks benefit more from pre-training with robot data. Specifically, we examine the embodiment gap associated with the end-effector. The simulation tasks are accordingly categorized into gripper-based and dexterous hand-based tasks. As illustrated in Figure 10, both R3M-DROID and MCR outperform R3M on gripper-based tasks in terms of manipulation centricity and downstream success rate, supporting our earlier conclusions. However, performance drops in hand-based tasks, where R3M-DROID even underperforms R3M. This is likely due to the fact that all data in DROID was collected using a gripper, which presents an embodiment gap compared to dexterous hands. To alleviate this issue, we suggest two attempts in the future: (1) leveraging dynamics from robot data better to mitigate the embodiment gap, and (2) incorporating more end-effectors, such as dexterous hands, into robot datasets to enhance our manipulation-centric representations.\nFeature analysis. To gain a more intuitive understanding of the impact of the robot dataset, we employ t-SNE (Van der Maaten & Hinton, 2008) to process and visualize feature embeddings generated by the pre-trained encoder. The visualization results are presented in Figure 11. In the simulation domain, R3M struggles to cluster images within individual tasks. However, this issue is partially alleviated by using a robot dataset, as R3M-DROID exhibits improved clustering ability. Nonetheless, due to the significant domain gap between DROID and simulation environments, it still encounters difficulties in distinguishing many tasks. In contrast, our method demonstrates markedly superior clustering capabilities, indicating the importance of incorporating real-robot dynamics for effective robotic representation. Additionally, we observe that all three methods exhibit good clustering performance in real-world tasks, likely attributed to the more distinct visual changes between tasks."}, {"title": "5.5 TRAINING EFFICIENCY", "content": "Pre-training representations typically requires extensive computational resources. Methods like VC-1 and MVP have prolonged training times due to their use of MAE (Feichtenhofer et al., 2022). Even the most computationally efficient baseline, R3M, requires 120 hours on a single NVIDIA V100. Our method, however, achieves state-of-the-art performance while requiring less computation. As shown in Table 5, our approach has a shorter training time than R3M, achieving an good balance between performance and computational efficiency."}, {"title": "6 RELATED WORKS", "content": "Pretrained robotic representations. The development of pre-trained visual representations (PVRs) has significantly improved the efficiency of downstream policy learning in robotics. Notable ap- proaches include a variant of MoCo-v2 (Parisi et al., 2022) that integrates multi-layer features, MVP (Xiao et al., 2022) and VC-1 (Majumdar et al., 2023) utilizing Masked Autoencoders (He et al., 2022), and R3M (Nair et al., 2022), which employs a time-contrastive objective and video- language alignment. Other works like Zheng et al. (2024) use temporal action contrastive learning, while MPI (Jia et al., 2024) focuses on predicting transition frames based on language instructions. HRP (Srirama et al., 2024) extracts affordances from large-scale human videos for enhanced gener- alization, and Theia (Shang et al., 2024) distills diverse vision models for robot learning. VIP (Ma et al., 2023) generates dense reward functions for robotic tasks. Similar to our method, RPT (Ra- dosavovic et al., 2023) employs trajectory labels for representation training. In contrast, our work introduces the concept of manipulation centricity, leveraging large-scale robotic data to capture manipulation-specific dynamics, resulting in improved performance on downstream tasks.\nLearning from large-scale robotic data. Recent advancements in robotics increasingly utilize large-scale datasets to enhance the capabilities of robotic systems. Collaboration et al. (2024) in- troduces the Open X-Embodiment dataset, the largest robotic dataset comprising extensive data from diverse robot embodiments across various tasks. The RT-X model (Collaboration et al., 2024), trained on this diverse dataset, shows promising results in cross-robot skill transfer as a generalist. Team et al. (2024) introduces Octo, a transformer-based diffusion policy also trained on Open X- Embodiment, supporting flexible task and observation definitions. Additionally, OpenVLA (Kim et al., 2024) is developed as a vision-language model enabling direct mapping from image inputs to continuous robot actions. Unlike these approaches, our work focuses on extracting dynamics and interaction information from robotic datasets to create specialized visual representations for policy learning, offering an efficient alternative to generalist policies.\nDynamics-aware representation. Representation learning is crucial for extracting key features from image inputs in imitation learning and reinforcement learning (Xu et al., 2023; Ji et al., 2024; Yuan et al., 2022). CURL (Laskin et al., 2020) uses InfoNCE (van den Oord et al., 2019) to max- imize agreement between augmented observations, while CPC (Henaff, 2020) and ATC (Stooke et al., 2021) incorporate temporal dynamics into contrastive loss. DRIML (Mazoure et al., 2020) proposes a policy-dependent auxiliary objective, and KOROL (Chen et al., 2024) trains feature ex- tractors on task-specific RGBD images during Koopman operator dynamics learning. TACO (Zheng"}, {"title": "7 CONCLUSIONS AND DISCUSSIONS", "content": "Our work introduces the concept of manipulation centricity in visual representations for robotic manipulation, revealing its crucial role in downstream task performance. By leveraging large-scale robot data and dynamics-aware learning objectives, we develop a method that significantly enhances the extraction of manipulation-centric features. This approach not only advances the state-of-the-art in robotic manipulation across diverse tasks but also provides a new lens through which to un- derstand and evaluate representation learning in robotics. Our findings highlight the importance of aligning representation learning with the specific demands of robotic control, potentially shifting the paradigm of how we approach feature extraction for embodied agents. Looking forward, this work opens avenues for exploring multi-modal integration, such as using language instructions to learn task-aware features and further leveraging trajectory data to capture spatial-temporal robotic dynam- ics, promising to further narrow the gap between pre-trained generalized representation models and real-world robotic manipulation."}, {"title": "A MORE EXPERIMENTAL DETAILS", "content": "We select a diverse set of tasks from various robotic manipulation benchmarks for evaluation. Specifically, we include three tasks from Robomimic (Mandlekar et al., 2021), three tasks from RoboCasa (Nasiriany et al., 2024), ten tasks from MetaWorld (Yu et al., 2019), and four dexterous tasks from DexArt (Bao et al., 2023). Detailed descriptions of each task are provided below:\n\u2022 Can (Robomimic, \\(A \\in \\mathbb{R}^{7}\\)): the task is to manipulate the can using the robot's arm to perform various actions such as picking it up, moving it to a different location, and orienting it in a specific way.\n\u2022 Lift (Robomimic, \\(A \\in \\mathbb{R}^{7}\\)): the task is to grasp a specified item and then raise it to a desired height.\n\u2022 Square (Robomimic, \\(A \\in \\mathbb{R}^{7}\\)): the task is to pick up a square-shaped nut and place it onto a rod successfully.\n\u2022 Close Drawer (RoboCasa, \\(A \\in \\mathbb{R}^{7}\\)): the task is to accurately close a drawer.\n\u2022 Coffee Button Press (RoboCasa, \\(A \\in \\mathbb{R}^{7}\\)): the task is to press the button on the coffee machine to start the coffee brewing process.\n\u2022 Open Single Door (RoboCasa, \\(A \\in \\mathbb{R}^{7}\\)): the task is to open a door that is singularly paneled, such as a cabinet or microwave door, which is already closed.\n\u2022 Assembly (MetaWorld, \\(A \\in \\mathbb{R}^{4}\\)): the task is to grasp a nut and position it on a dowel using the gripper.\n\u2022 Bin Picking (MetaWorld, \\(A \\in \\mathbb{R}^{4}\\)): the task is to transfer a disc from one bin container to another.\n\u2022 Button Press (MetaWorld, \\(A \\in \\mathbb{R}^{4}\\)): the task is to press a button using a robotic arm to activate a device.\n\u2022 Disassemble (MetaWorld, \\(A \\in \\mathbb{R}^{4}\\)): the task is to remove a nut from a peg by picking it up.\n\u2022 Drawer Open (MetaWorld, \\(A \\in \\mathbb{R}^{4}\\)): the task is to accurately open a drawer.\n\u2022 Hammer (MetaWorld, \\(A \\in \\mathbb{R}^{4}\\)): the task is to drive a screw into the wall using a hammer.\n\u2022 Pick Place Wall (MetaWorld, \\(A \\in \\mathbb{R}^{4}\\)): the task is to grab a puck, go around a wall and put the puck in the designated spot.\n\u2022 Shelf Place (MetaWorld, \\(A \\in \\mathbb{R}^{4}\\)): the task is to grab a puck and set it on a shelf.\n\u2022 Stick Pull (MetaWorld, \\(A \\in \\mathbb{R}^{4}\\)): the task is to use a stick to pull a box by holding onto the stick.\n\u2022 Stick Push (MetaWorld, \\(A \\in \\mathbb{R}^{4}\\)): the task is to hold a stick to push a box with it.\n\u2022 Bucket (DexArt, \\(A \\in \\mathbb{R}^{22}\\)): the task is to elevate a bucket..\n\u2022 Faucet (DexArt, \\(A \\in \\mathbb{R}^{22}\\)): the task is to activate a faucet using a rotating joint.\n\u2022 Laptop (DexArt, \\(A \\in \\mathbb{R}^{22}\\)): the task is to grasp the center of the display and then lift the laptop cover.\n\u2022 Toilet (DexArt, \\(A \\in \\mathbb{R}^{22}\\)): the task is to initiate the process of lifting a bigger toilet seat."}, {"title": "A.2 MORE GRAD-CAM VISUALIZATIONS", "content": "The Grad-CAM visualizations for each task are presented in Table 8, which provides a comprehensive comparison with other baseline methods. Notably, our approach is shown to effectively facilitate the capture of key manipulation-centric features, thereby enhancing the model's ability to focus on the most relevant aspects of the task."}, {"title": "A.3 PRE-TRAINING HYPERPARAMETERS", "content": "We show our hyperparameters during the pre-training stage in Table 6. Downstream policy learning settings are introduced in Section A.5."}, {"title": "A.4 PRE-TRAINING IMPLEMENTATION", "content": "Our codebase is built upon the implementation of R3M. Similarly", "below": "nactor_trunk =\n# self.outdim is the output dimension of ResNet, for example, ResNet"}]}