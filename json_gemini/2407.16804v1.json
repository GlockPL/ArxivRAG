{"title": "Multimodal Machine Learning in Mental Health: A Survey of Data, Algorithms, and Challenges", "authors": ["Zahraa Al Sahili", "Ioannis Patras", "Matthew Purver"], "abstract": "The application of machine learning (ML) in detecting, diagnosing, and treating mental health disorders is garnering increasing attention. Traditionally, research has focused on single modalities, such as text from clinical notes, audio from speech samples, or video of interaction patterns. Recently, multimodal ML, which combines information from multiple modalities, has demonstrated significant promise in offering novel insights into human behavior patterns and recognizing mental health symptoms and risk factors. Despite its potential, multimodal ML in mental health remains an emerging field, facing several complex challenges before practical applications can be effectively developed. This survey provides a comprehensive overview of the data availability and current state-of-the-art multimodal ML applications for mental health. It discusses key challenges that must be addressed to advance the field. The insights from this survey aim to deepen the understanding of the potential and limitations of multimodal ML in mental health, guiding future research and development in this evolving domain.", "sections": [{"title": "INTRODUCTION", "content": "Mental health disorders represent a significant global health concern, with approximately 1 in 8 people worldwide affected by conditions such as anxiety and depression, accounting for an estimated 970 million individuals in 2019 alone [1]. Despite the prevalence of these disorders, there is a substantial treatment gap, particularly in low- and middle-income countries where up to 85% of individuals with mental health conditions receive no treatment [2]. This gap underscores the urgent need for innovative solutions to enhance mental health care access and efficacy.\nIn recent years, digital health interventions have emerged as a promising avenue to address this need. The global digital mental health market, valued at $5.1 billion in 2020, is projected to expand at a compound annual growth rate of 21.0% from 2021 to 2028 [3]. Among these interventions, multimodal machine learning\u2014an approach that integrates diverse data types such as text, audio, and video\u2014has shown considerable promise. Research indicates that combining indicates t multiple data types (e.g., audio, video, text) leads to better accuracy and robustness in detecting and assessing mental health conditions compared to using a single data type [4]. For example, integrating facial expressions, speech, and physiological signals can enhance the detection of emotions and mental states, offering more comprehensive insights for interventions speech and facial recognition data can significantly improve the accuracy of detecting mental health conditions, such as depression, compared to single data modality approaches [4].\nThis survey provides a comprehensive overview of multimodal machine learning in mental health research. We review the various types of multimodal data commonly used, including text, audio, video, and physiological data. We present an overview of available datasets and the specific mental health conditions they address, such as depression, stress, bipolar disorder, and PTSD. Additionally, we discuss state-of-the-art machine learning techniques employed to analyze these data, encompassing RNN/CNN-based, transformer-based, and graph neural network-based algorithms.\nFurthermore, we explore the challenges and opportunities in multimodal machine learning for mental health, including issues related to data availability, privacy, bias, benchmarking, complexity and evaluation. Despite these challenges, the potential benefits of multimodal machine learning for mental health research are vast, promising significant improvements in diagnosis and treatment, ultimately enhancing the lives of millions."}, {"title": "DATA TYPES AND DATASETS", "content": "In multimodal machine learning for mental health applications, a variety of data types are commonly used, including text, audio, video, and physiological data."}, {"title": "Data Types", "content": "Text Data: such as social media posts, clinical notes, and survey responses, can provide insight into an individual's thoughts and emotions. Text data can be analyzed using natural language processing (NLP) techniques, such as sentiment analysis and emotion detection, to extract information about an individual's mental state.\nAudio Data: such as speech and nonverbal cues, can provide information on an individual's tone of voice and emotional state. Audio data can be analyzed using techniques such as speech recognition, emotion recognition, and speaker identification to extract information about an individual's mental state.\nVideo Data: such as facial expressions and body language, can provide information on an individual's nonverbal cues and emotional state. Video data can be analyzed using techniques such as facial expression recognition, body language analysis, and gaze tracking to extract information about an individual's mental state.\nPhysiological Data: such as heart rate, skin conductance, and electroencephalography (EEG), can provide objective measures of an individual's physiological responses to different stimuli. Physiological data can be analyzed using techniques such as signal processing and pattern recognition to extract information about an individual's mental state.\nTo analyze these data, it is important to have large, diverse, and representative datasets. However, collecting such data can be challenging becuase of patients privacy restrictions."}, {"title": "Datasets", "content": "Multimodal mental health datasets have become increasingly valuable for researchers aiming to investigate the underlying mechanisms and treatment options for various mental health disorders. These datasets comprise a range of modalities, such as video, audio, text, and physiological signals, offering a comprehensive understanding of the complex nature of mental health disorders. In this section, we present an overview of the available multimodal mental health datasets that are currently available [5-30], highlighting the types of data included in each dataset as well as their applications as shown Table 1."}, {"title": "ALGORITHMS", "content": "A variety of machine learning algorithms have been applied to multimodal data for mental health applications.Historically, Support Vector Machines (SVMs) and Random Forests have been used to classify multimodal data, such as text and physiological data. In addition, SVMs were particularly useful for high-dimensional data and have been applied to classify mental health conditions, such as depression and PTSD. Moreover, Random Forest algorithm was used for a wide range of multimodal mental health data as classifying depression and anxiety through aggregating multiple decision trees to increase the accuracy of the classifier. Recent research has focused on advanced deep learning techniques, particularly CNN/RNN-based models, Transformer models, and Graph Convolutional Networks (GCNs)."}, {"title": "CNN/RNN-Based Models", "content": "Multimodal algorithms leveraging Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) have shown significant promise in integrating information from various modalities-such as text, image, audio, and video for diverse tasks. CNNs are adept at extracting high-level visual features from image and video data, while RNNs, particularly Long Short-Term Memory (LSTM) networks, are proficient in modeling temporal dependencies within sequential data like text and speech.In these multimodal frameworks, CNNs process visual inputs, extracting salient features, and RNNs handle sequential data, capturing contextual dependencies. Outputs from both networks are typically combined in a fusion layer to make final predictions, improving the performance over traditional methods.\nFor instance, Man et al. demonstrated that a multimodal pipeline combining ResNet architectures for imaging with ALMO, FastText, and BOW for text significantly outperformed six different unimodal approaches in detecting depressive symptoms from social media data [31]. In addition, Gui et al. developed a decentralized execution approach using COMMA policy gradients, which employed GRU for text and VGG-Net for image features. Their model used policy gradient agents to decide feature selection, resulting in enhanced depression detection performance [32]. Similarly, Rohanian et al. proposed an LSTM-based word-level multimodal system that incrementally fused modalities and introduced fusion gates to control the contributions of audio and visual inputs, effectively mitigating noise [33].To add, Zhang et al. introduced a trilateral bimodal encoding model (MEN) incorporating attentional decision fusion (ADF) and a hybrid fusion approach for depression detection [34]. Their model combined early intra-modality and late inter-modality fusion, achieving superior performance on the AVEC2013 and AVEC2014 datasets, with notable improvements in MAE and RMSE scores [34]. Moreover, Pradhan et al. utilized a hybrid network combining Complex dual Tree with Fast Lifting Wavelet Transform (CTFL-WT) for pre-processing and Channel Attentive SqueezeNet (CASN) for feature extraction to predict depression risk [35]. Their model showed improved classification performance on the WESAD dataset by using Adaptive Arithmetic Optimization Algorithm (AAOA) and a Hybrid DenseNet with LSTM (DLSTM) [35].\nDas et al. proposed an audio-based depression detection method, using neural networks to classify audio spectrogram features [36]. Their model, combining MFCC and spectrogram features with a novel CNN architecture, achieved over 90% accuracy on the DAIC-WOZ and MODMA datasets, outperforming state-of-the-art methods [36].In addition, Zhang et al. developed the Audio, Video, and Text Fusion-Three Branch Network (AVTF-TBN), which integrated auditory, visual, and textual cues, achieving high performance on depression risk analysis tasks [37].Moreover, Lilhore et al. introduced a hybrid model for predicting Postpartum Depression Disorder (PPDD), combining Improved Bi-directional Long Short-Term Memory (IBi-LSTM) with Transfer Learning (TL) for CNN-text and CNN-audio architectures [38]. Their model demonstrated superior precision, recall, accuracy, and F1-score on a UCI dataset, highlighting its potential for early PPDD risk assessment [38]. Furthermore, Beniwal et al. explored the relationship between social media use and depression, developing a multimodal dataset comprising text, emoticons, and images [39]. Their hybrid BERT-CNN model achieved a remarkable 99% accuracy in categorizing users as depressive or non-depressive, showcasing the effectiveness of combining text and image data for mental health detection [39].In a similar manner, Aina et al. introduced a pipeline for analyzing facial expressions using AffectNet and 2013 Facial Emotion Recognition (FER) datasets [40]. Their hybrid architecture, utilizing the YOLOv8 algorithm and an ensemble of CNNs and Visual Transformer (ViT) models, achieved an 81% accuracy in predicting mental disorders. Techniques like Gradient-weighted Class Activation Mapping (Grad-CAM) and saliency maps were integrated to enhance interpretability, aiding healthcare professionals in understanding model predictions [40]."}, {"title": "Multimodal Transformer Models", "content": "Transformer models have significantly advanced multimodal data integration, enabling comprehensive analysis across text, image, and audio data. Yoon et al. proposed a method utilizing unimodal transformers for video and audio processing, followed by a multimodal transformer and a depression detection layer, demonstrating the approach on their Dvlog dataset [10]. Similarly, Bucur et al. presented a versatile, time-enhanced multimodal transformer architecture for identifying depression from social media posts [41]. Their model used pre-trained embeddings for images and text, integrated time2vec positional embeddings to account for time differences between posts, and incorporated EmoBERTa and CLIP embeddings, achieving robust performance even with unstructured datasets. Additionally, Yang et al. introduced the Multimodal Purification Fusion (MMPF) model for automatic depression detection (ADD) [41]. MMPF incorporated clinical constraints into the learning process through a prior constraint gating (PCG) strategy and employed text and audio encoders to extract features. The model used a multimodal purification refinement process and multiperspective contrastive learning (MCL) strategy, achieving promising results on the DAIC-WOZ dataset [42].\nThe Topic ATtentive transformer-based ADD (TOAT) model by Guo et al. addressed challenges in organizing and processing varied audio and textual data lengths, and the scarcity of training samples due to privacy concerns [43]. TOAT utilized a topic attention module and a two-branch architecture with late fusion to independently encode audio and textual data, achieving state-of-the-art performance on the DAIC-WOZ dataset [43]. In addition, Tao et al. proposed the DepMSTAT framework, designed to detect depression using acoustic and visual features from social media data [44]. This framework included modules for data preprocessing, token generation, and a Spatial-Temporal Attentional Transformer (STAT) for capturing spatial and temporal correlations. DepMSTAT demonstrated high accuracy (71.53%) on the D-Vlog depression database, outperforming existing models [44].\nOn the other side, Yoa et al. explored the interconnectedness of stress detection and emotion recognition using textual and acoustic features [45]. Their MUSER architecture, a transformer-based model, employed a speed-based dynamic sampling approach for multi-task learning, demonstrating effective stress detection on the MuSE dataset and external validation with the OMG-Emotion dataset [45].Moreover, Lee et al. addressed mood disorder detection by integrating brain structural MRI scans with DNA whole-exome sequencing data [46]. Their fusion model, combining Vision Transformer (ViT) and XGBoost, significantly improved the area under the curve (AUC) and accuracy, demonstrating the potential of multimodal integration for refining mood disorder diagnostics [46].Furthermore, Malhotra et al. emphasized the application of explainable AI (XAI) techniques to enhance the interpretability of Transformer-based models in mental healthcare diagnostics [47]. By employing SHAP and LIME, they conducted extensive experiments with large language models (LLMs) fine-tuned for detecting depressive and suicidal behavior from online content [47]. Their study highlighted the importance of XAI in understanding the psychological states and behaviors of individuals.Additionally, Moon et al. addressed the limitations of controlled laboratory environments by introducing the Extended D-vlog dataset, which includes real-life YouTube vlogs [48]. They developed a virtual agent to provide Cognitive Behavioral Therapy (CBT)-based responses, achieving impressive performance in real-world depression detection and intervention with the TVLT model on the Multimodal Extended D-vlog Dataset [48]."}, {"title": "Graph Convolutional Networks (GCNs)", "content": "Graph Convolutional Networks (GCNs) have been applied to multimodal data to effectively capture complex and relational information. Zheng et al. proposed a graph attention model that integrates multimodal knowledge for depression detection [49]. Their approach, leveraging audio, video, and text modalities, demonstrated superior performance on the DIAC-WOZ dataset by learning effective embeddings and incorporating medical knowledge [49]. In addition, Xia et al. (2024) addressed the few-shot learning problem in depression detection with a multimodal GNN-based model [50]. By integrating a GNN for recursive aggregation and transformation of node representations, their approach effectively combined audio, text, and video features in a Bi-LSTM fusion network [50]. This method achieved an accuracy of 0.861 on the DAIC-WOZ dataset, significantly outperforming baseline models. Also, Cha et al. highlighted the importance of early depression detection and introduced the Multimodal Object-Oriented Graph Attention Model (MOGAM) [51]. This model, applicable to various data types including text, images, and videos, employed a cross-attention mechanism to aggregate multimodal features [51]. MOGAM achieved an accuracy of 0.871 and an F1-score of 0.888, proving its scalability and effectiveness on social media datasets [51].In addition, Shen et al. introduced a graph-based multimodal fusion approach for automatic depression assessment (ADA) [52]. Their model used undirected edges to link temporally continuous features within each modality and across different modalities, ensuring seamless global information propagation. This approach achieved a root mean square error (RMSE) of 4.80 and a CCC value of 0.563 on the E-DAIC dataset [52].Furthermore, Noman et al. proposed a graph deep learning framework for classifying brain networks in major depressive disorder (MDD) [53]. Their graph autoencoder (GAE) architecture embedded the topological structure and node content of fMRI networks into low-dimensional representations, significantly outperforming state-of-the-art methods in brain connectome classification [53].\nOn the other side, Pann et al. introduced MAMF-GCN, a multi-scale adaptive multi-channel fusion deep graph convolutional network designed to predict mental disorders [54]. This model extracts both specific and shared features from imaging data, phenotypic information, and other sources, using an attention mechanism to learn adaptive weights for feature fusion, thereby enhancing diagnostic accuracy for brain disorders [54]. Moreover, Hu et al. developed MMGCN, a multimodal fusion framework employing deep graph convolution networks for emotion recognition in conversations [55]. Their model utilized a modality encoder and a multimodal spectral-based GCN, achieving state-of-the-art performance on the IEMOCAP and MELD datasets [55]. Similarly, Joshi et al. proposed COGMEN, a contextualized GNN-based multimodal emotion recognition model that combines a recurrent GNN with a graph transformer network [56].In addition, Cabral et al. proposed a novel multi-label emotion graph representation for classifying mental health based on social media posts [57]. They constructed a word-document graph tensor to describe emotion-based contextual representation, demonstrating significant performance improvements across three public datasets. Moreover, Wang et al. introduced the Multimodal Transformer Graph Convolution Attention Isomorphism Network (MTGCAIN) for diagnosing intellectual disability (ID) [58]. MTGCAIN leveraged graph convolution attention to extract brain connectivity features and used multiple brain network atlases for comprehensive information exchange. This approach achieved an accuracy of 81.29% and an AUC of 0.8760 [58]. Likewise, Liu et al. developed a deep GCN based on variable multi-graph and multimodal data (VMM-DGCN) for autism spectrum disorder (ASD) diagnosis [59]. Their algorithm, evaluated on the ABIDE I dataset, achieved a 91.62% accuracy and a 95.74% AUC, outperforming other diagnostic algorithms by effectively capturing multi-scale feature representations and incorporating non-imaging information into feature representations[59].\nThe potential for multimodal transformers and graph neural networks in mental health research is substantial. GNNs excel in capturing complex relationships inherent in mental health conditions, while transformers are effective in pretraining and transfer learning, beneficial in data-scarce scenarios. Future research could explore graph transformers and spatio-temporal graph neural networks, as well as the integration of GNN-based fusion within multimodal transformers."}, {"title": "OPEN CHALLENGES", "content": "This section discusses the open challenges in the era of multimodal machine learning for mental health including the need to develop robust and explainable multimodal machine learning models for mental health applications that are ethically sound and socially responsible."}, {"title": "Data availability", "content": "One of the primary challenges in multimodal machine learning for mental health applications is the need for extensive and diverse data collection. Effective training and validation of machine learning models require access to large, representative datasets. However, collecting such data is challenging due to ethical considerations, including data privacy and informed consent, and logistical issues related to the inclusion of marginalized communities. This lack of diverse data can lead to biased models that do not accurately represent all segments of the population, thereby impacting the fairness and accuracy of mental health assessments."}, {"title": "Benchmarks", "content": "Establishing benchmarks in multimodal mental health research is another significant challenge. Standardized and reliable methods for evaluating the performance of various models and techniques are essential for the advancement of the field. There is a need for a common framework that accurately reflects real-world scenarios and accounts for the complexity of mental health conditions. Effective benchmarks will facilitate the comparison of different approaches and ultimately lead to improved mental health diagnostics and treatments."}, {"title": "Explainability", "content": "Explainability is a critical challenge in the development of multimodal machine learning models for mental health. Mental health assessments are inherently subjective and complex, making it difficult to create models that are both robust and interpretable. For patients and healthcare providers to trust the outcomes and recommendations of these models, clear and transparent explanations of the decision-making processes are essential. Additionally, explainability can help identify and mitigate biases in the data and models, thereby enhancing the accuracy and fairness of mental health diagnoses and treatments."}, {"title": "Bias and Fairness", "content": "Bias and fairness are crucial considerations in the development of multimodal models for mental health. These models can perpetuate and amplify existing biases in the data and healthcare system, leading to unfair or inaccurate diagnoses and treatments for certain populations, particularly those from underrepresented groups. It is essential to identify and mitigate any biases present in the data and models to ensure that these tools are reliable and equitable for all patients. This involves developing robust methods for evaluating and addressing bias and using diverse and representative training data."}, {"title": "Privacy", "content": "Privacy is a major concern in the development of multimodal models for mental health, as these models typically require access to sensitive and personal information, such as medical records, mental health assessments, and audio and video recordings. Ensuring the privacy of this information is crucial to building trust with patients and healthcare providers and avoiding potential harm. This necessitates the development of secure, privacy-preserving techniques for collecting, storing, and utilizing sensitive data in the creation and deployment of multimodal algorithms."}, {"title": "Evaluation", "content": "Robust evaluation of multimodal machine learning models for mental health applications is an ongoing challenge. Traditional evaluation metrics, such as accuracy and precision, may not adequately capture the complexity and subjectivity of mental health assessments. Therefore, there is a need to develop new evaluation methods that consider the nuanced nature of mental health conditions and the impact of multimodal data integration."}, {"title": "Complexity and Multifactorial Nature", "content": "The inherent complexity and multifactorial nature of mental health disorders present significant challenges in model development. Mental health conditions are influenced by a wide range of factors, including biological, psychological, and social elements, making it difficult to create models that accurately capture these complexities. Additionally, some disorders, such as schizophrenia and bipolar disorder, require further research to understand their underlying mechanisms before machine learning techniques can be effectively applied.\nOverall, addressing these challenges requires further research to develop robust, explainable, and ethically sound multimodal machine learning models for mental health applications. Ensuring that these models are socially responsible and capable of providing accurate and fair assessments across diverse populations is essential for advancing the field and improving mental health outcomes."}, {"title": "CONCLUSION", "content": "The integration of artificial intelligence (AI) and multimodal machine learning (ML) in mental health research presents significant potential for improving the identification, diagnosis, and treatment of mental health disorders. Our survey demonstrates that multimodal ML can enhance the accuracy and robustness of mental health assessments by integrating diverse data sources such as video, audio, text, and physiological signals. This comprehensive approach offers a more complete understanding of an individual's mental health, leading to more effective treatments.\nHowever, several challenges need to be addressed to fully leverage the benefits of multimodal ML in mental health. These include the need for extensive and diverse data collection, ethical considerations regarding data privacy, and the importance of addressing bias and fairness to ensure equitable model outcomes,added to the development of standardized benchmarks and evaluation methods.\nDespite these challenges, the potential benefits of multimodal ML in mental health research are immense. Continued research and collaboration are essential to overcome these obstacles, leading to significant advancements in mental health diagnostics and treatments. By addressing these issues, we can ultimately improve the lives of millions of individuals worldwide, paving the way for a future where mental health care is more accurate, effective, and accessible."}]}