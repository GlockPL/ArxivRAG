{"title": "Multilingual Transfer and Domain Adaptation for Low-Resource Languages of Spain", "authors": ["Yuanchang Luo", "Zhanglin Wu", "Daimeng Wei", "Hengchao Shang", "Zongyao Li", "Jiaxin Guo", "Zhiqiang Rao", "Shaojun Li", "Jinlong Yang", "Yuhao Xie", "Jiawei Zheng", "Bin Wei", "Hao Yang"], "abstract": "This article introduces the submission status of the Translation into Low-Resource Languages of Spain task at (WMT 2024) by Huawei Translation Service Center (HW-TSC). We participated in three translation tasks: spanish to aragonese (es\u2192arg), spanish to aranese (es\u2192arn), and spanish to asturian (es\u2192ast). For these three translation tasks, we use training strategies such as multilingual transfer, regularized dropout, forward translation and back translation, labse denoising, transduction ensemble learning and other strategies to neural machine translation (NMT) model based on training deep transformer-big architecture. By using these enhancement strategies, our submission achieved a competitive result in the final evaluation.", "sections": [{"title": "1 Introduction", "content": "Neural machine translation (MT) (Lyu et al., 2019; Bahdanau et al., 2014; Gehring et al., 2017) allows translation systems to be trained end-to-end without having to deal with issues like word alignment, translation rules, and complex decoding algorithms that characterize statistical machine translation systems (SMT) (Koehn et al., 2007). Although neural machine translation has developed rapidly in recent years, it relies heavily on big data - large-scale, high-quality bilingual corpora. Due to the cost and scarcity of real corpora, synthetic data plays an important role in improving translation quality. Existing methods for synthesizing data in NMT focus on leveraging monolingual data during training. Among them, forward translation (Abdulmumin et al., 2021), back translation (Abdulmumin et al., 2021) and data diversity (Nguyen et al., 2020) have been widely used to generate synthetic bilingual corpora. Such synthetic data can be used to improve the performance of NMT models(Wu et al., 2023b). (Wei et al., 2023) also considers the style of the training data and exploits it to improve performance. Although synthetic data is efficient, synthetic data inevitably contains noise and erroneous translations. Denoising can prevent the training of NMT models from being interfered by noisy synthetic data by introducing high-quality real data as guidance. Another direction to improve the performance of NMT models is to use more efficient training strategies. For example, by mixing similar language data together to train a multi-language pre-training model (Li et al., 2022), due to the shared vocabulary, encoding layer and decoding layer parameters and language similarity, languages with less data can benefit from languages with more data. Regularized dropout (Wu et al., 2021) allows the NMT model to more effectively utilize limited data during the training process. Transduction ensemble learning (Wang et al., 2020) can aggregate the translation capabilities of multiple models into one model.\nFor the Translation into Low-Resource Languages of Spain task of WMT 2024, we participated in the es\u2192arg, es\u2192arn and es\u2192ast language pair. We use training strategies such as multi-language pre-training models (Li et al., 2022), regularized dropout (Wu et al., 2021), forward translation (Abdulmumin et al., 2021), back translation (Abdulmumin et al., 2021), Labse denoise (Feng et al., 2020) and transduction ensemble learning (Wang et al., 2020) to train neural machine translation (NMT) models based on deep Transformer architecture.\nNext, this article will expand on the details of our translation system in different translation tasks. The structure of the remaining sections is as follows: Section 2 introduces the data scale and data preprocessing process; Section 3 describes the overview of the NMT system; Section 4 gives the parameter settings, data processing results and experimental results; Section 6 gives System conclusions were drawn."}, {"title": "2 Dataset", "content": "2.1 Data Size\nIn accordance with the requirements of the WMT 2024 outline, on the Translation into Low-Resource Languages of Spain machine translation task, we used the officially provided data to train the NMT system from scratch. Table 1 shows the training data size for each language pair of the bilingual machine translation task. These language pairs include Spanish to Aragonese (es\u2192arg), Spanish to Arabic (es\u2192arn) and Spanish to Asturian (es\u2192ast).\n2.2 Data Pre-processing\nThe data pre-processing process is as follows:\n\u2022 Remove duplicate sentences or sentence pairs.\n\u2022 Remove invisible characters and xml escape characters.\n\u2022 Convert full-width symbols to half-width symbols.\n\u2022 Use fast_align (Dyer et al., 2013) to filter poorly aligned sentence pairs.\n\u2022 Filter out sentences with more than 80 tokens in bilingual data.\n\u2022 Remove sentences with duplicate tokens.\n\u2022 When performing subword segmentation, joint sentencepiece (Kudo and Richardson, 2018) is used for es\u2192arg, es\u2192arn and es\u2192ast translation tasks."}, {"title": "3 NMT System", "content": "3.1 System Overview\nTransformer is the state-of-the-art model structure in recent MT evaluations. There are two parts of research to improve this kind: the first part uses wide networks (eg: Transformer-Big (Vaswani, 2017)), and the other part uses deeper language representations (eg: Deep Transformer (Wang et al., 2019))."}, {"title": "3.2 Multilingual Transfer", "content": "Recent researches have shown that multilingual models outperform their bilingual counterparts, particularly when the number of languages in the system is limited and those languages are related (Li et al., 2022). This is mainly due to the capability of the model to learn interlingual knowledge (shared semantic representation between languages). Transfer learning using pre-trained multilingual model has shown very promising results for low resource tasks. In this task, we first select a multilingual system as the base system, then fine-tune the system with low resource language pairs.\nSpecifically, we add the \"<arg>\" tag to the Spanish side of the es\u2192arg bilingual data, the \"<arn>\" tag to the Spanish side of the es\u2192arn bilingual data, and the \"<ast>\" tag to the Spanish side of the es\u2192ast bilingual data, and sample them. Mix shuf to train a one-to-many pre-training model; sample the es\u2192arg, es\u2192arn and es\u2192ast original bilingual data and then mix shuf to train a many-to-one pre-training model. Then, the one-to-many pre-training model and the many-to-one pre-training model are trained by using the original bilingual data, and three translation models from Spanish to Aragonese, Arabic, and Asturian and three translation models from Aragonese, Arabic, and Asturian to Spanish are obtained."}, {"title": "3.3 Regularization Dropout", "content": "Dropout (Srivastava et al., 2014) is a widely used technique for regularizing deep neural network training, which is crucial to prevent over-fitting and improve the generalization ability of deep models. Dropout performs implicit ensemble by simply dropping a certain proportion of hidden units from the neural network during training, which may cause an unnegligible inconsistency between training and inference. Regularized Dropout (R-Drop) (Wu et al., 2021) is a simple yet more effective alternative to regularize the training inconsistency induced by dropout. Concretely, in each mini-batch training, each data sample goes through the forward pass twice, and each pass is processed by a different sub model by randomly dropping out some hidden units. R-Drop forces the two distributions for the same data sample outputted by the two sub models to be consistent with each other, through minimizing the bidirectional Kullback-Leibler (KL) divergence (Van Erven and Harremos, 2014) between the two distributions. In this way, the inconsistency between the training and inference stage can be alleviated."}, {"title": "3.4 Forward translation and Back translation", "content": "Forward translation, also known as self-training (Abdulmumin et al., 2021), is one of the most commonly used data augmentation methods. FT has proven effective for improving NMT performance by augmenting model training with synthetic parallel data. Generally, FT is performed in three steps: (1) randomly sample a subset from the large-scale source monolingual data; (2) use a \u201cteacher\u201d NMT model to translate the subset data into the target language to construct the synthetic parallel data; (3) combine the synthetic and authentic parallel data to train a \"student\" NMT model.\nApertium is a free/open-source rule-based architecture for MT that consists of a pipeline of modules performing part-of-speech disambiguation and tagging, lexical transfer, lexical selection, chunk-level or recursive structural transfer, and morphological generation. To make our model better, we use Apertium as a \"teacher\" model to produce pseudo-corpus.\nBack translation (BT) (Abdulmumin et al., 2021) refers to translating the target monolingual data into the source language, and then using the synthetic data to increase the training data size. This method has been proven effective to improve the NMT model performance.\nWe use the machine translation model obtained by Multilingual Transfer to produce back translation synthetic parallel data, and mix it with forward translation synthetic parallel data and authentic parallel data for training, which can achieve better results than FT or BT."}, {"title": "3.5 Labse Denoising", "content": "Due to the low quality of our bilingual data, we use LaBSE (Feng et al., 2020) to calculate the semantic similarity of each bilingual sentence pair and exclude bilingual sentence pairs with similarity scores below 0.7 from our training corpus. Use these clean data to better train the model."}, {"title": "3.6 Transductive Ensemble Learning", "content": "Ensemble learning (Garmash and Monz, 2016), which aggregates multiple diverse models for inference, is a common practice to improve the accuracy of machine learning tasks. However, it has been observed that the conventional ensemble methods only bring marginal improvement for NMT when individual models are strong or there are a large number of individual models. Transductive Ensemble Learning (TEL) (Wang et al., 2020) study how to effectively aggregate multiple NMT models under the transductive setting where the source sentences of the test set are known. TEL uses dev sets finetune a strong model, which boosts strong individual models with significant improvement and benefits a lot from more individual models."}, {"title": "4 Experiment", "content": "4.1 Setup\nWe use the open-source fairseq (Ott et al., 2019) to train NMT models, and then use SacreBLEU (Post, 2018) and Chrf++ to measure system performance. The main parameters are as follows: each model is trained using 8 V100 GPUs, batch size is 4096, parameter update frequency is 1, and learning rate is 5e-4. The number of warmup steps is 4000, and model is saved every 1000 steps. The architecture we used is described in section 3.1. We adopt dropout, and the rate varies across different training phases. R-Drop (Srivastava et al., 2014) is used in model training, and we set A to 5.\n4.2 Data processing\nDue to the poor quality of bilingual data in low-resource languages, after the rule cleaning mentioned in section 2.2 and the labse model cleaning mentioned in section 3.2, the amount of data is smaller, and the data amount of es\u2192arg, es\u2192arn and es\u2192ast is quite different. When training one-to-many and many-to-one pre-training models, if the amount of bilingual data for a certain language direction is too small, the translation quality will be extremely poor. Therefore, Following (Conneau and Lample, 2019; Liu et al., 2020) we re-balance the training set by upsampling data from each language l with a ratio:\n$\\lambda_l = \\frac{1}{\\rho_l^{1/T}} \\frac{\\rho_l^{1/T}}{\\sum_{i=1}^L \\rho_i^{1/T}}$ with $\\rho_l = \\frac{n_l}{\\sum_{i=1}^L n_i}$\nwhere, T is the temperature parameter and we set T to 2. ni is the number of utterances for language 1 in the training set. The data amount changes as shown in the following table 3.\n4.3 Results\nTables 2 shows the evaluation results of es\u2192arg, es\u2192arn and es\u2192ast NMT systems on the brand new FLORES+ dev sets and devtest sets, the results of dev test sets are obtained through OCELOT submission. We use Multilingual Transfer and R-Drop to build a strong baseline, then use FT and BT for data enhancement, and use Labse denoising for more efficient training, and finally use Transductive Ensemble Learning to ensemble multiple models ability.\nAs can be seen from the table above, after FT & BT and Labse denoising, the translation quality from Spanish to three directions has been improved to varying degrees. This shows that for low-resource scenarios, these two strategies can expand the amount of data and improve the quality of the data. Enhance the translation quality of machine translation models. Among them, the improvement of both strategies in the es\u2192arg direction is higher than that of the other two directions, and the bilingual data of es\u2192arg is also the least. This shows that FT & BT's strategy of expanding the amount of data and labse denoising's strategy of improving data quality are both in situations where the amount of bilingual data is small, The effect is more obvious.\nIn addition, after Transductive Ensemble Learning, the BLEU value of FLORES+ devtest sets has been greatly improved compared to the FLORES+ dev sets test set. Although it is not the same test set, the BLEU value has improved across latitudes, which shows that The fields of dev sets and devtest sets are very consistent, and Transductive Ensemble Learning, a strategy that utilizes dev sets, can maximize the translation effect of the model on the"}, {"title": "5 Conclusion", "content": "This paper presents HW-TSC's submission to the Translation into Low-Resource Languages of Spain task of WMT 2024. For both translation tasks, we use a series of training strategies to train NMT models based on the deep Transformer-big architecture. By using these enhancement strategies, our submission achieves a competitive result in the final evaluation. For example, #607 in the spanish to aragonese constrained submissions, #608 in the spanish to aranese constrained submissions, and #606 in the spanish to asturian constrained submissions."}]}