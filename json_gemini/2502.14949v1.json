{"title": "KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding", "authors": ["Ahmed Heakl", "Abdullah Sohail", "Mukul Ranjan", "Rania Hossam", "Ghazi Ahmed", "Mohamed El-Geish", "Omar Maher", "Zhiqiang Shen", "Fahad Khan", "Salman Khan"], "abstract": "With the growing adoption of Retrieval-Augmented Generation (RAG) in document processing, robust text recognition has become increasingly critical for knowledge extraction. While OCR (Optical Character Recognition) for English and other languages benefits from large datasets and well-established benchmarks, Arabic OCR faces unique challenges due to its cursive script, right-to-left text flow, and complex typographic and calligraphic features. We present KITAB-Bench, a comprehensive Arabic OCR benchmark that fills the gaps in current evaluation systems. Our benchmark comprises 8,809 samples across 9 major domains and 36 sub-domains, encompassing diverse document types including handwritten text, structured tables, and specialized coverage of 21 chart types for business intelligence. Our findings show that modern vision-language models (such as GPT-4, Gemini, and Qwen) outperform traditional OCR approaches (like EasyOCR, PaddleOCR, and Surya) by an average of 60% in Character Error Rate (CER). Furthermore, we highlight significant limitations of current Arabic OCR models, particularly in PDF-to-Markdown conversion, where the best model Gemini-2.0-Flash achieves only 65% accuracy. This underscores the challenges in accurately recognizing Arabic text, including issues with complex fonts, numeral recognition errors, word elongation, and table structure detection. This work establishes a rigorous evaluation framework that can drive improvements in Arabic document analysis methods and bridge the performance gap with English OCR technologies.", "sections": [{"title": "Introduction", "content": "With the upsurge in adoption of Retrieval-Augmented Generation (RAG) based systems for document processing, the quality of document ingestion pipelines has become increasingly critical.\nOptical Character Recognition (OCR) plays a crucial role in this pipeline, enabling the conversion of physical documents into machine-readable text and databases for enabling effective knowledge retrieval. Although significant progress has been made in the multilingual OCR , with comprehensive datasets like PubLayNet , DocBank , M6Doc , and DocLayNet , Arabic OCR continues to lag behind. This gap is largely due to the unique challenges of the Arabic script, including its cursive nature, complex typography, and right-to-left text orientation.\nExisting Arabic OCR datasets (Table 1), like KHATT and IFN/ENIT  focus mainly on handwritten text, whereas APTI ."}, {"title": "Related Work", "content": "The development of robust Optical Character Recognition (OCR) systems has been extensively studied across document layout analysis , table detection and document understanding . While English OCR benefits from rich"}, {"title": "KITAB-Bench", "content": "Our methodology offers a novel approach to benchmarking Arabic OCR systems via a comprehensive data collection strategy and a systematic evaluation framework. We gather curated samples from existing Arabic document datasets, manually collected and annotated PDFs, and employ a five-phase LLM-assisted human-in-the-loop pipeline (Figure 4) to generate diverse supplementary content. Our evaluation framework spans nine specialized tasks, enabling thorough assessment of OCR performance across various document processing challenges and providing a robust benchmark for Arabic document understanding tasks."}, {"title": "PDF Data Collection", "content": "We curated 33 diverse PDFs from online sources in academia, medicine, law, and literature. To ensure challenging cases, we selected documents featuring richly formatted tables with extensive color usage, merged cells, Arabic numerals, historical texts, watermarks, and handwritten annotations. Each PDF averaged three pages, and we then manually annotated them. This dataset comprehensively captures real-world complexities, making it a valuable benchmark for PDF-to-Markdown conversion."}, {"title": "LLM-Assisted Data Generation Pipeline", "content": "To generate data for charts, diagrams and tables, we implemented a five-phase LLM-assisted generation pipeline with human validation at critical stages, as illustrated in Figure 4. In Phase I (Topic Generation), our system employs an LLM to generate diverse topic names across multiple domains. This phase incorporates various personas (academic, legal, medical, technical) to ensure broad coverage of document types. Phase II (Data Generation) transforms the validated topics into structured raw data. The LLM generates content following Arabic linguistic and formatting conventions across various domains. In Phase III (Code Generation), the system converts the validated raw data into plotting code, with special attention to Arabic text rendering requirements and RTL content management. Phase IV (Image Rendering) utilizes specialized rendering engines (Mermaid, Plotly, Vegalite, HTML) to create visual representations while maintaining Arabic text integrity.\nThe final phase (Human Evaluation) implements rigorous quality control through expert validation. Evaluators filter charts, tables and diagrams based on detected anomalies and ensure adherence to Arabic-specific document conventions. This phase is crucial for maintaining the high quality of our benchmark dataset."}, {"title": "Dataset Statistics", "content": "Our benchmark dataset comprises over 8,809 samples across 9 major domains and 36 sub-domains, representing a comprehensive collection of Arabic document types for OCR evaluation. As detailed in Table 8, the dataset combines carefully curated samples from established datasets, manually annotation PDFs, and synthetically generated content created through our LLM-assisted pipeline (Figure 4). The Image-to-Text portion (3,760 samples) includes data from historical documents (History Ar), handwritten text collections (Khatt , ADAB , Muharaf , and scene text , while layout detection comprises 2,100 samples from BCE-Arabic-v1  and DocLayNet .\nFor layout analysis, we incorporated 1,700 samples from BCE-Arabic-v1 dataset, 400 samples from DocLayNet dataset  focusing on financial, academic, legal, and patent documents. The line detection and recognition tasks contains 378 samples each from self-developed dataset. We further enriched the dataset with 500 samples from PATS-A01 benchmark to ensure diverse representation. For handwritten text recognition, we assembled a comprehensive collection of 1,000"}, {"title": "Experiments", "content": "Our experimental evaluation comprehensively assesses the capabilities of current OCR systems and state-of-the-art vision-language models (VLMs) across different Arabic and multilingual document understanding tasks. Figure 2 illustrates the nine distinct tasks in our evaluation framework.\nWe evaluate three categories of systems: VLMs, traditional OCR systems, and specialized document processing tools. For VLMs, we include both closed-source models like gpt-4o-2024-08-06, gpt-4o-mini-2024-07-18 , and gemini-2.0-flash , as well as open-source alternatives such as Qwen2-VL-7B, Qwen2.5-VL-7B , and the AIN-7B . Traditional OCR approaches in our evaluation include Surya , Tesseract , EasyOCR , and PaddleOCR . For specialized document processing tasks, we employ systems like Docling  and Marker . Layout detection capabilities are evaluated using methods implemented in Surya-layout, Yolo-doclaynet from MinerU , and RT-DETR based method in Docling ."}, {"title": "Evaluation Frameworks and Metrics", "content": "Our evaluation framework comprises nine specialized tasks designed to assess different aspects of Arabic OCR systems, as demonstrated in Figure 2. Each task addresses specific challenges in Arabic document processing. For this reason, we employ task-specific metrics to evaluate different aspects of document understanding.\nPDF-to-Markdown: It evaluates the conversion of Arabic PDFs to structured markdown while preserving the text and table structure. Since both table and text structure are important, for evaluating PDF to Markdown conversion quality, we propose MARS (Markdown Recognition Score), which combines chrF  with Tree-Edit-Distance-based Similarity (TEDS) :\nMARS = a \\cdot chrF3 + (1 \u2212 a) \\cdot TEDS(T_a, T_b)  (1)\nwhere a (0 < a < 1) is the weight. \\(T_a\\) represent"}, {"title": "Experimental Setup", "content": "We implement our evaluation pipeline with careful consideration of hyperparameters for different metrics. All experiments use NVIDIA A100 GPUs. For VLMs, we use their official implementations or API endpoints. Traditional OCR systems are evaluated using pre-trained models provided by the frameworks. For PDF-to-Markdown evaluation metric MARS 1, we choose a = 0.5 and a = 0.5 and \u03b2 = 0.2 for Diagram-to-JSON evaluation metric CODM. We average the results over multiple runs, with performance comparisons shown in different tables [3, 6, 5, 7, and 4]."}, {"title": "Results and Discussion", "content": "In this section, we present a comprehensive evaluation of different models across different tasks of our framework. The results provide a clear distinction between the performance of closed-source models, open-source models, and framework-based solutions, revealing both their strengths and limitations. We observe very clear performance gap between closed and open-source solutions. While closed-source models like Gemini-2.0-Flash consistently outperform other models almost all the tasks."}, {"title": "Charts, Diagrams, and VQA", "content": "Table [7] presents model performance across different chart and diagram understanding tasks, evaluated using SCRM and CharTeX (for charts), and VQA-based accuracy metrics. Among closed-source models, Gemini-2.0 achieves the highest performance on chart understanding metrics, scor-ing 71.4% on SCRM and 56.28% on CharTeX. The performance gap between Gemini-2.0 and GPT-4o is particularly pronounced in CharTeX evaluation (10.33%) compared to SCRM (2.8%). Open-source models shows a significant limitation in complex chart understanding. While their SCRM scores remain competitive, both Qwen variants score below 23% on CharTeX evaluation. The visual question-answering results reveal an important exception to the general closed-source advantage. AIN achieves 87% on PATDVQA, surpassing Gemini-2.0 by 11.5%. AIN also shows competitive performance on MTVQA (31.50%), which is similar to GPT-4o and 4% better than GPT-4o-mini. This shows that open-source models can be competitive with closed-source alternatives."}, {"title": "Layout and Lines: Document Structure", "content": "Our evaluation of document structure understanding reveals distinct performance patterns across layout detection and line processing tasks. In layout detection (Table 4), RT-DETR achieves superior overall performance with mAP@0.5 scores of 0.750 and 0.758 on BCE (arabic only) and DocLayNet (english) datset respectively. However, Surya  demonstrates higher precision (0.782 on DocLayNet, 0.751 on BCE), despite lower recall rates. This trade-off suggests that different architectures optimize for different aspects of layout detection.\nThe line processing results (Table 3) highlight a clear contrast between detection and recognition capabilities. While Surya excels in detection with a mAP@0.50 of 79.67%, EasyOCR demonstrates superior recognition performance (WER: 0.53, CER:"}, {"title": "Tables, OCR, and PDF-to-Markdown", "content": "Across table extraction tasks (Table 5), closed-source models maintain a clear advantage, with GPT-40 achieving 85.76% TEDS and 66.36% Jaccard scores. Among open-source models, AIN (75.94% TEDS) significantly outperforms Qwen variants, while specialized frameworks like Surya achieve competitive results (70.42% Jaccard) through targeted pipelines. In OCR evaluation (Table 6), Gemini-2.0-Flash leads with the lowest error rates (CER: 0.13, WER: 0.32). Notably, AIN matches this performance level (WER: 0.28), while traditional OCR frameworks like EasyOCR and Tesseract show moderate performance (CER: 0.58, 0.54). The significant performance drop in Paddle (CER: 0.79) and Surya (CER: 4.95) highlights the challenges in developing robust OCR systems.\nEnd-to-end document processing (Table 5) reveals the largest gaps between approaches. Closed-source models maintain consistent performance (GPT-40: 65.12% MARS, Gemini-2.0: 65.65% MARS), while open-source models show substantial degradation (Qwen2-VL-7B: 21.42% MARS). Framework approaches achieve better stability, with Tesseract and EasyOCR scoring above 50% MARS, suggesting that specialized pipelines can partially bridge the gap with larger models in complete document processing tasks.\nOur comprehensive evaluation demonstrates that while closed-source models maintain superior performance over open-source models across most tasks, specialized frameworks like Surya, RT-DETR Layout, and EasyOCR achieve competitive performance in targeted scenarios like table extraction, layout detection, and text recognition respectively. However, this framework advantage significantly diminishes in end-to-end pdf-to-markdown tasks where the integration capabilities of large models prove crucial, as evidenced by the performance gaps between commercial VLMs and traditional systems like EasyOCR, Surya and Tesseract in End-to-End PDF task (Table 5)."}, {"title": "Conclusion", "content": "We introduce a comprehensive benchmark for Arabic OCR that fills the gap in standardized evaluation frameworks for Arabic document processing. Our dataset of 8,809 samples across nine major domains is the most diverse collection assembled for OCR evaluation, incorporating handwritten, scanned, synthetic, and scene text, as well as complex tables, charts, and end-to-end pdf-to-markdown. This framework extends beyond simple text recognition to include structural document analysis and enables systematic assessment of OCR performance across various fonts, styles, and layouts."}, {"title": "Limitations and Future Directions", "content": "Despite its contributions, this benchmark has limitations. While it covers diverse Arabic document types, it lacks full representation of historical manuscripts and low-resource dialects. Future work should expand to include these, along with scanned records from government, academic, and financial institutions.\nAnother key limitation is in table and chart recognition, where OCR models struggle with structure preservation, header detection, and merged cell parsing. Though our benchmark introduces challenges in these areas, further refinements are needed for robust multimodal OCR capable of jointly processing text, tables, and figures. Future advancements should focus on dataset expansion, novel evaluation metrics, deep learning refinements, and cross-lingual OCR innovations to enhance Arabic VLMs."}]}