{"title": "ULTRASOUND IMAGE SYNTHESIS USING GENERATIVE \u0391\u0399 FOR LUNG CONSOLIDATION DETECTION", "authors": ["Yu-Cheng Chou", "Gary Y. Li", "Li Chen", "Mohsen Zahiri", "Naveen Balaraju", "Shubham Patil", "Bryson Hicks", "Nikolai Schnittke", "David O. Kessler", "Jeffrey Shupp", "Maria Parker", "Cristiana Baloescu", "Christopher Moore", "Cynthia Gregory", "Kenton Gregory", "Balasundar Raju", "Jochen Kruecker", "Alvin Chen"], "abstract": "Developing reliable healthcare AI models requires training with representative and diverse data. In imbalanced datasets, model performance tends to plateau on the more prevalent classes while remaining low on less common cases. To overcome this limitation, we propose DiffUltra, the first generative AI technique capable of synthesizing realistic Lung Ultrasound (LUS) images with extensive lesion variability.\nSpecifically, we condition the generative AI by the introduced Lesion-anatomy Bank, which captures the lesion's structural and positional properties from real patient data to guide the image synthesis. We demonstrate that DiffUltra improves consolidation detection by 5.6% in AP compared to the models trained solely on real patient data. More importantly, DiffUltra increases data diversity and prevalence of rare cases, leading to a 25% AP improvement in detecting rare instances such as large lung consolidations, which make up only 10% of the dataset.", "sections": [{"title": "1. INTRODUCTION", "content": "Recent advancements in generative models have significantly improved the data synthesis for assisting AI training [1, 2, 3, 4]. Typically, the generative models adhere to the mask-and-paste generation paradigm, synthesizing only the lesion area guided by pixel-level annotation of target lesion (e.g., segmentation mask) and then pasting the synthesized lesion onto a healthy background [5]. However, since the segmentation masks do not capture the internal structure and texture of the target lesion, the synthetic lesions often exhibit uniform structures, making them easily distinguishable from the surrounding tissue by the differences in boundary intensity. [6, 7].\nMoreover, the mask-and-paste generation paradigm becomes impractical when segmentation mask of a lesion is not available, rendering obvious boundary artifacts between the synthetic lesion and its background (Figure 1). Therefore, in this paper, we aim to move beyond the mask-and-paste generation paradigm and investigate a new approach that can synthesize structurally and positionally realistic lesions.\nWe hypothesize that the uniform structure of synthetic lesions stems from insufficient guidance, such as the conditions [6, 8] used in conditional diffusion models [9, 10], and poor modeling of the lesion's internal structure [1, 5, 11]. For LUS images in particular, we further hypothesize that lesion location plays a critical role in synthesizing realistic images."}, {"title": "2. DiffUltra", "content": "DiffUltra aims to generate realistic lesions that blend seamlessly into healthy LUS images. To place lesions in anatomically appropriate locations, we use a lesion-anatomy bank, capturing the lesion's relative position to surrounding structures. The lesion-anatomy bank is constructed by creating a joint conditional probability mass function (PMF) and a lesion bank of foregrounds, both derived from real patient data. For realistic texture, we condition the generative model with a detailed structural representation of the lesion, enabling the synthesis of whole LUS images where lesions integrate naturally with the background anatomy (Figure 3-(a))."}, {"title": "2.1. Lesion-anatomy Bank", "content": ""}, {"title": "2.1.1. Determining appropriate lesion position for synthesis", "content": "To ensure synthesized lesions are placed appropriately relative to their surrounding anatomical structures (e.g., the pleural line) in LUS images, we model the lesion's relative position to its surrounding anatomical structures using a conditional PMF - P(\u0394\u03a7, \u0394\u03a5 | X, Y), built from real patient data. Here, X and Y represent the coordinates of a key anatomical structure's center, while AX and AY denote the relative distance between the key anatomical structure and the lesion. This conditional PMF allows us to determine the position of the synthesized lesion by sampling from P(\u0394\u03a7, \u0394\u03a5 | X = x,Y = y), where x and y are derived from a healthy image during synthesis.Figure 3-(a) visualizes one of these conditional PMF.\nTo construct the conditional PMF, we first compute the joint PMF P(AX = \u2206x, \u0394\u03a5 = \u2206y, X = x,Y = y) using real patient data annotated at the bounding box level. For each lesion, the distance to its nearest key anatomical structure is calculated as:\n$(\\Delta x_i, \\Delta y_i) = (x' - x_i, Y' - y_i)$,\n$(\\Delta X, \\Delta y) = \\min \\sqrt{\\Delta x^2 + \\Delta y^2}$    (1)\nwhere (x', y') and (xi, Yi) are the bounding box centers of the lesion and its surrounding anatomical structure i, respectively. This approach allows precise modeling of relative positions, even when multiple key anatomical structures are present in the image. For each scanning zone and orientation, the joint PMF is built by counting occurrences in a 4D grid (e.g., a 10\u00d710\u00d710\u00d710 grid for a given coordinate system). Next, we obtain P(X, Y) by marginalizing out \u2206X and \u2206\u03a5 from the joint PMF. The final conditional PMF P(\u0394\u03a7, \u0394\u03a5 | X, Y) is then obtained by dividing the joint PMF by P(X, Y)."}, {"title": "2.1.2. Selecting appropriate lesion for synthesis", "content": "After determining the position of the lesion to be synthesized in the healthy image (by sampling P(\u0394\u03a7, \u0394\u03a5 | X = x, Y = y)), the next step is to select a lesion with the appropriate size and texture that fits the sampled relative position (\u0394x, \u0394y). To achieve this, we propose a lesion bank that stores lesion foregrounds (regions inside the lesion's bounding box) extracted from real patient data, indexed by (\u2206x, \u2206y, x, y). During inference, a lesion foreground is randomly selected from the bank for the target position. Texture and size information are extracted from the selected foreground using Otsu' segmentation [13] (as shown in Figure 3-(b)) and used as conditions for the generative model as shown in Figure 2 (\u00a72.3). This process is represented mathematically as:\nP(L | AX = \u2206x, \u0394\u03a5 = \u2206y, X = x, Y = y),   (2)\nwhere L is the lesion foreground index. Since lesion foregrounds are unique, this conditional PMF is uniform, allowing for random sampling to retrieve a variety of lesion foregrounds for image synthesis."}, {"title": "2.2. Lesion Structural Representation", "content": "A simple method to add synthetic lesions to healthy images involves pasting a sampled lesion onto a chosen location. However, this method lacks texture variation, as it merely replicates the original lesion. To enhance variability, we use a generative model conditioned on a detailed structural representation of the lesion-its \"skeleton\" without texture. This approach allows the generative model to introduce texture variation during synthesis. To extract the lesion skeleton S, as illustrated in Figure 3-(b), we applied Otsu' seg- mentation [13] to the lesion foreground capture fine-grained structural details."}, {"title": "2.3. Conditional Diffusion Model", "content": "Unlike Medfusion [14], which uses covariables like age and sex, we condition the model on structural representations and latent features. Following [14], we use a stable diffusion model [9] conditioned by structural representations and latent features from a pretrained autoencoder as shown in Figure 2. With the autoencoder Dec(Enc(\u00b7)) and diffusion model D(\u00b7), we generate lesions in healthy LUS images \u00ce as:\n$\\hat{I} = Dec(D(f, S))$,   (3)\nwhere S is the lesion skeleton in 2.2 and f is the latent feature, obtained by:\n$f = Enc(\\hat{I}_{masked}), \\hat{I}_{masked} = \\begin{cases}    \\hat{I}(x, y), & \\text{if } (x, y) \\notin F, \\\\    10, 0, & \\text{if } (x, y) \\in F,  \\end{cases}$   (4)\nwith \u00cemasked representing the masked healthy image by the entire bounding box area of the lesion foreground F, sampled from P(L | AX = \u2206x, \u0394\u03a5 = \u2206y, X = x, Y = y). The lesion's center is determined by:\n(x', y') = (x + \u2206x, y + \u2206y). (5)"}, {"title": "3. EXPERIMENTS", "content": ""}, {"title": "3.1. Experimental Setting", "content": "Implementation Details. To reduce the input dimensions for the stable diffusion model, we trained an autoencoder (AE) to downsample LUS frames from 512 \u00d7 512 \u00d7 1 to 64 \u00d7 64 \u00d7 8 latent features. The AE was trained on all frames, and the best checkpoint was selected based on the lowest validation Mean Squred Error (MSE) loss. The diffusion model was then trained in this latent space. The diffusion models were trained for 50 epochs on 4 A100 GPUs with a batch size of 8, and the best checkpoint was chosen based on the lowest MSE loss in the foreground. During synthesis, we randomly sampled healthy images that have pleural line boxes to generate lesion-present images. Following [6], we replaced the"}, {"title": "3.2. Experimental Results", "content": "Synthetic Data Improves Downstream Tasks To evaluate the performance gain provided by DiffUltra, we compare a detection model trained on both real and synthetic data with one trained only on real data. Results in Table 1 show that the model trained on both real and synthetic data outperforms the baseline in lesion-level AP and video-level AUROC (+5.6% and +1.4%). Furthermore, DiffUltra outperforms DiffTumor [6] that generates lesions using the mask-and-paste paradigm, highlighting the effectiveness of our method in synthesizing complete LUS images.\nSynthetic Data Alleviates Class Imbalance To evaluate DiffUltra's impact on improving performance in the low prevalence cases, we conducted a sub-analysis on video-level classification (VLC) of consolidations across four severity levels. To ensure a fair comparison, we matched the VLC specificity of the baseline Yolo-v5 model (89.1%) with that of DiffUltra (88.8%) by adjusting the threshold. As shown in Table 2, DiffUltra significantly enhances VLC sensitivity for severity level-1 and level-4 consolidations (+22.3% and +25%), highlighting its effectiveness in handling rare cases, which represent only 1.5% and 10.7% of the testing set, respectively."}, {"title": "3.3. Ablation Study", "content": "Excluding structural representation. Conditioning the generative model on a binary mask significantly reduces performance compared to using structural representations (11.7% vs. 18.3%, Table 3). This results are in line with our hypothesis about the need for more detailed structural representation as diffusion condition.\nExcluding positional guidance. We show that randomly placing lesions creates unrealistic relations with surrounding anatomy, reducing detection performance (14.8% vs. 18.3%, Table 3). This results are in line with our hypothesis about need for realistic lesion location during synthesizing.\nRepeating rare cases. We also tested whether simply repeating rare cases would improve performance by balancing the data. However, as shown in Table 4, repeating rare cases did not enhance performance, indicating that balancing alone, without new information, is insufficient for improvement.\nSearching Optimal Amount of Synthetic Data We optimized the amount of synthetic data generated through experimentation. By replacing only healthy LUS images with lesion-present ones, the overall training data volume stayed constant while increasing the positive-to-negative (P: N) ratio. Experimental results are shown in Table 4."}, {"title": "4. CONCLUSION", "content": "We present DiffUltra, a method for synthesizing Lung Ultrasound images with flexible, clinically accurate lesions. Using a Lesion-Anatomy Bank, DiffUltra captures structural and positional relationships, generating realistic anatomy-lesion"}]}