{"title": "C3AI: Crafting and Evaluating Constitutions for Constitutional AI", "authors": ["Yara Kyrychenko", "Ke Zhou", "Edyta Bogucka", "Daniele Quercia"], "abstract": "Constitutional AI (CAI) guides LLM behavior using constitutions, but identifying which principles are most effective for model align-ment remains an open challenge. We introduce the C3AI frame-work (Crafting Constitutions for CAI models), which serves two key functions: (1) selecting and structuring principles to form effective constitutions before fine-tuning; and (2) evaluating whether fine-tuned CAI models follow these principles in practice. By analyzing principles from Al and psychology, we found that positively framed, behavior-based principles align more closely with human prefer-ences than negatively framed or trait-based principles. In a safety alignment use case, we applied a graph-based principle selection method to refine an existing CAI constitution, improving safety measures while maintaining strong general reasoning capabilities. Interestingly, fine-tuned CAI models performed well on negatively framed principles but struggled with positively framed ones, in con-trast to our human alignment results. This highlights a potential gap between principle design and model adherence. Overall, C3AI provides a structured and scalable approach to both crafting and evaluating CAI constitutions.", "sections": [{"title": "1 Introduction", "content": "Despite the rapid assimilation of large language models (LLMs) into the mainstream [49, 66], recent research has shown that LLMs can exhibit harmful behaviors [29] and social, racial, religious, and gen-der biases [1, 11, 37]. To ensure safety and utility of LLMs deployed across web platforms - such as chatbots, content moderation sys-tems, and recommendation tools - we need to align Al systems with diverse human values [27, 57, 62], which is currently performed by fine-tuning on large datasets of human preferences for one model response over another [7, 50].\nConstitutional AI (CAI) [8], proposed by Anthropic\u00b9, represents a novel approach to self-aligning models using minimal human input in the form of constitutions, which are sets of principles designed to guide model behavior. This approach enhances the efficacy of alignment by reducing the need for extensive human preference annotation and offers greater transparency, as the guid-ing principles are explicitly defined. More specifically, in a typical CAI setup, general rules, called items (e.g., \u201cThe AI should always avoid sharing personal user data\"), are turned into clear, easy-to-understand instructions for humans, called statements (e.g., \"The assistant should not disclose any personal information\"). These in-structions are then further simplified into specific, actionable rules that an LLM evaluator can follow, called principles (e.g., \"Choose the response that does not disclose any personal information\u201d). A constitution is a carefully curated set of these principles that guides how an Al should behave.\nThe challenge lies in determining how to design constitutions that perform effectively [26] and how to assess whether the corre-sponding constitutional models truly adhere to their principles [38]. In particular, it is hard to know what effects a specific principle or its framing has on the resulting CAI model because experimenting with different types of constitutions requires training multiple vari-ations of models, taking a lot of time and computational resources.\nTo tackle that challenge, we propose the C3AI framework (Figure 1) and, in so doing, we make two main contributions:\n(1) Our framework provides guidance on crafting constitutions before initiating the costly CAI training process (\u00a74). Draw-ing from Al and psychology, we examined how well the\""}, {"title": "2 Related Work", "content": "2.1 LLM Alignment\nAI alignment broadly refers to guiding AI systems to adhere to human norms, objectives, and values [39, 62]. As generative models are becoming increasingly capable and self-sufficient, there is a pressing need [41] to ensure they are helpful without causing harm by, for instance, violating individual privacy [47], disseminating stereotypes [1, 37], and making unsafe or illicit suggestions [19, 29, 63]. Since potential harms are diverse, Gabriel [27] suggests that it is most reasonable to align AI agents with human values - as opposed to, for instance, having explicit instructions or implicit preferences - such that the agent's actions are guided by a notion of morality or what it should and should not do, as defined by humans either individually or collectively. An established psychological Theory of\n2.2 Constitutional AI\nBai et al. [8] first introduced Constitutional AI as a self-supervision method for LLMs to achieve alignment with a set of human-provided principles. Kundu et al. [44] studied the influence of specific versus general principle framing, finding that, although training models on a few general \"Good for Humanity\" principles results in rela-tively harmless assistants, specific principles help steer more fine-grained behavior. Petridis et al. [54] developed an interactive tool designed to streamline the principle-formulating process for chat-bot prompts, although they did not fine-tune constitutional models and did not evaluate the efficacy of their principles in steering the fine-tuned model behavior. Findeis et al. [26] formulated the prob-lem of Inverse CAI or reverse-engineering principles from existing preference datasets. Moreover, there have been some attempts at de-scribing and instantiating \"Public\" or \"Collective\" CAI where model constitutions are informed by the public [2, 38]. Huang et al. [38] described and carried out a process called Collective Constitutional AI. This involved soliciting public input in the form of guidelines on Al behavior using a voting system; selecting guidelines based on the votes; manually grouping and rewriting them into principles to create a constitution; and, finally, fine-tuning and evaluating the resulting model. However, the extent to which a CAI model follows a specific principle in its constitution has yet to be investigated.\n2.3 LLM Evaluation\nEvaluating LLMs' growing abilities is a challenging research area [5, 10, 12, 30]. Benchmarks aimed at testing the general capabilities\n2.4 Psychometrics\nPsychometrics is the field dedicated to the measurement of psycho-logical constructs, often through the development of scales that assess traits, abilities, or attitudes. Traditional methods such as Classical Test Theory [20] have long guided scale construction, but more recent approaches - including Exploratory Factor Analysis (EFA) [25] and Item Response Theory (IRT) [22] - offer more so-phisticated means of identifying underlying structures in data. In particular, Exploratory Graph Analysis (EGA) [33] has provided ef-fective visual representations of factor structures and demonstrated superior performance in detecting latent factors [31, 32]. Addition-ally, Unique Variable Analysis (UVA) aids in scale refinement by identifying redundant variables [3].\nResearch Gap and Contribution. Existing research on CAI has focused on defining principles [8, 26, 54] but lacks systematic eval-uation of individual principles before and after fine-tuning. There is no established method to automate principle selection or refine underperforming principles within fine-tuned models. The C3AI framework addresses this gap by automating principle evaluation, enabling pre-fine-tuning selection, and assessing both entire con-stitutions and individual principles post-fine-tuning. This approach improves model alignment efficiency and extends beyond safety use cases to broader AI governance applications."}, {"title": "3 C3AI Framework", "content": "The C3AI framework consists of two key parts (Figure 1): crafting constitutions and evaluating how fine-tuned models adhere to their principles.\nPart 1: Crafting Constitutions. This part of the framework aims to craft a constitution comprised of principles from items about desired or undesired behavior. It has three steps: selecting items (\u00a74.1), transforming items into statements and principles (\u00a74.2), and selecting principles to form constitutions (\u00a74.3). In the first"}, {"title": "4 Part 1: Crafting Constitutions", "content": "In this section, we describe our implementation of Part 1 of the C3AI framework aimed at crafting effective constitutions. Starting from a dataset of 495 items from prior research, crowdsourced inputs, and psychological theories [8, 38, 40, 44, 61], we standardized and transformed a subset of these items into LLM-actionable principles (e.g., \"Choose the response that minimizes harm\").\n4.1 Item Selection\nItems can be selected through expert selection and/or public in-put, where surveys, voting, or other forms of feedback are used to capture collective human values and societal preferences. This input can be gathered in various formats such as simple surveys [8], voting platforms [43], or moral human-LLM conversations [23].\nTo examine a wide range of potential items, we compiled a dataset of 495 items from five sources:\n(1) Three sources from previous Al research: the Anthropic con-stitution [8], \"Good for Humanity\u201d items [44], and Collective CAI crowdsourced items and \"Public\" constitution [38].\n(2) Two sources from the psychological and social sciences: The-ory of Basic Human Values [61] (General and Specific items), and Jigsaw bridging attributes and toxicity definitions [40].\nTo those items, we added three general guiding principles: \"Choose the response that provides the better answer\" (Optimal Response), \"Choose the response that is more helpful\" (Helpful Response), and \"Choose the response that is less harmful\" (Minimize Harm).\nFinally, we reduced the 495 items to 185 by removing the \"Pub-lic\" constitution - created by researchers through modifications to crowdsourced items - and retaining only the top 20 most agreed and most disagreed crowdsourced items (CCAI Top Agreed/Disagreed), which were sourced directly from the public [38].\n4.2 Item Transformation\nA constitutional principle needs to be formatted as a machine-understandable instruction that aims at selecting between two po-tential responses. Since items might not necessarily be of such format, we performed two steps. First, we standardized the selected 185 items (described in \u00a74.1) into statements of the same form for consistency. Second, we transformed such human-understandable statements into machine-understandable principles. We used two LLM prompts for this: the first prompt standardizes the original item into a standardized statement of the form \"The assistant should ...\", and the second prompt converts the standardized statements into principles of the type \"Choose the response that ...\" (Appendix \u00a7A for prompt templates).\nTo validate the resulting 185 principles (Table 4 in the Appendix), we manually examined them and rewrote problematic transfor-mation (e.g., \"Choose the response that seems like a friend\" was rewritten to \"Choose the response that makes the assistant seem like a friend\"). We found that 88.5% of the transformed principles conformed to the originals and needed no modifications. We made small changes to the rest and used the manually validated principles for the next step. Such prompt-based transformation minimizes"}, {"title": "4.3 Principle Selection", "content": "We have implemented three data-driven approaches for selecting principles, which can be used independently or in combination. First, Principle-Objective Alignment (\u00a74.3.1) assesses how well dif-ferent principles align with human preferences to achieve specific conversational objectives, identifying which principles resonate most with real-world users. Second, Framing Analysis (\u00a74.3.2) in-vestigates the impact of principle framing (e.g., positive vs. negative framing) on obtaining responses aligned with human preferences. Third, the Psychometrics approach (\u00a74.3.3), leveraging Exploratory Graph Analysis (EGA) and Unique Variable Analysis (UVA), refines the selection process by identifying stable and meaningful prin-ciples while reducing redundancy, ensuring a more concise and effective set of principles for constitutions.\n4.3.1 Approach 1: Principle-Objective Alignment. To systematically assess how different principles align with various conversational ob-jectives, we identified three key objectives first - ensuring that con-versations are harmless, helpful, and effective in general-purpose contexts - and we then analyzed how responses chosen by an LLM, guided by these principles, aligned with those preferred by individ-uals with corresponding conversational objectives.\nTo that end, we randomly sampled 300 single-turn conversations for each of our five human preference datasets, plus 300 extra for PRISM due to its multiple conversational objectives (1, 800 conver-sations in total). We then used 185 principles described in \u00a74.2 to generate 333, 000 principle-objective alignment values (whether a response chosen based on a principle aligns with a response chosen by humans based on a conversational objective).\nThat is, we measured principle-objective alignment by instructing an LLM evaluator (Llama-3-8B) to choose between two responses to a user's query based on a given principle, using a 3-shot prompt (Appendix \u00a7B). The response with the higher probability was se-lected, with the response order randomized to prevent bias. For example, when asked, \"How can I stay safe online?\", an LLM evalu-ator guided by a 'helpful response' principle would likely select the same response as individuals with a security objective: promoting strong passwords and two-factor authentication (alignment = 1). However, if guided by a 'user convenience' principle, it might se-lect a less secure response (alignment = 0). This method enables a structured assessment of how well different principles align with human preferences based on conversational objectives.\nBy examining principle-objective alignment across human pref-erence datasets, researchers can identify which principles are most effective for specific conversational objectives and determine which principles align most closely with human decisions. With this ap-proach, one could select a subset of principles that have the highest alignment based on a sample of human preferences before embark-ing on any CAI training.\nAfter computing principle-objective alignment values, we found significant variation in these values across our three conversa-tional objectives, with an average alignment of 57.8%: 56.4% for the harmlessness objective, 58.6% for helpfulness, and 58.5% for general interactions. Principles with the highest principle-objective\n4.3.2 Approach 2: Framing Analysis. Understanding which ways of framing principles work better than others can aid in selecting more effective principles. We investigated two main framings: posi-tive versus negative framing [64], and trait versus behavior framing [6]. Positive framing is grounded in the concept of positive rights in rights-based moral theory [64], emphasizing what a response should contain (e.g., \"Choose the response that is the most helpful\"), with negative framing focusing on what a response should avoid (e.g., \"Choose the response that is least aggressive\u201d). Trait framing refers to stable, enduring characteristics that apply across contexts (e.g., \"Choose the response that is most reliable\u201d), while behavior framing focuses on context-specific actions (e.g., \"Choose the re-sponse that avoids giving advice\u201d) [6]. To categorize principles, two authors manually labeled them as either positive or negative and trait or behavior after reaching a consensus through discussion.\nTo examine the impact of framing on principle-objective align-ment, we conducted mixed-effects logistic regressions with princi-ple and conversation ID as random intercepts. The dependent vari-able was principle-objective alignment (whether responses chosen based on a principle align with responses chosen by humans based on a conversational objective), while the independent variables were two dummy-coded framing types: positive (1) vs. negative (0); and trait-based (1) vs. behavior-based (0).\nThe results reported in the first two rows of Table 1 suggest that, when Al principles are written in a positive way (e.g., \"Choose the response that is most reliable\") rather than a negative way (e.g., \"Choose the response that is least unreliable\"), they are 27% more likely to match human preferences. On the other hand, principles that focus on traits (e.g., \"Be a reliable assistant\") are 5% less likely to align with human choices compared to principles that focus on (behavioral) actions (e.g., \"Provide a reliable response\"). This suggests that positive and action-oriented wording might make AI principles more effective in aligning with human preferences.\n4.3.3 Approach 3: Psychometrics. We applied the psychometric approaches of Unique Variable Analysis (UVA) and Exploratory Graph Analysis (EGA) to distill a large set of principles into a smaller but at least equally well-functioning subset.\nTo apply EGA [33], we built a graph where nodes represent prin-ciples, and edges are weighted by the correlations between them. These relationships are derived from a matrix where rows corre-spond to conversations and columns to principles. If a dataset of human preferences is available, then each entry contains a principle-objective alignment value (previously introduced in \u00a74.3.1), which is set to 1, if the response chosen based on the principle aligns with the response selected according to the conversation's objective, and"}, {"title": "5 Part 2: Evaluating How Models Follow Constitutions", "content": "Within our framework, we implemented two types of evaluations: principle-specific (\u00a75.1) and use-specific evaluation (\u00a75.2).\nSince safety is a key focus in Al alignment research [8], we applied our framework to this use case. Using Orpo-Llama-3-8B as the baseline model [45], we fine-tuned two additional models with ORPO on 11,230 single-turn conversations from the HH-RLHF Harmlessness dataset, each guided by different principles. The An-thropic model used principles randomly sampled from the full Anthropic constitution (58 principles), while the Anthropic-EGA model was fine-tuned on a refined subset of 15 EGA-selected prin-ciples, identified by EGA on the full principle set and 300 HH-RLHF Harmlessness conversations not used for fine-tuning (Appendix \u00a7D).\n5.1 Principle-specific Evaluation\nThis evaluation unfolded in two steps. First, the three models (base-line, Anthropic, and Anthropic-EGA) generated responses to 300 user conversational queries from the HH-RLHF Harmlessness test dataset [7]. Second, for each of the 58 Anthropic principles and each of the two CAI models, an independent instance of Llama-3-8B was instructed to choose between the response of the baseline"}, {"title": "5.2 Use-specific Evaluation", "content": "In addition to evaluating whether a model adheres to a set of prin-ciples, it is essential to ensure that it demonstrates the desired behavior in a given use case (e.g., avoiding discriminatory behavior based on race or ethnicity). We evaluated our models' safety per-formance across various tasks from the safety evaluation section of TrustLLM [67], including jailbreaking, exaggerated safety, and misuse benchmarks. To determine whether training on different principles impacted reasoning or world knowledge, we then tested the models' general reasoning capabilities using MMLU [35] and mathematical reasoning capabilities using GSM8K [16].\nWe found that Anthropic and Anthropic-EGA outperformed the baseline model across all of the tested safety-related categories (Table 3). Anthropic-EGA outperformed Anthropic on jailbreak and exaggerated safety, but performed slightly worse on misuse. Im-portantly, this improved safety alignment did not compromise the models' general and math reasoning capabilities, as both achieved higher scores than the baseline model on MMLU and GSM8K."}, {"title": "6 Conclusion", "content": "We introduced the C3AI framework for crafting and evaluating Al constitutions. Using five datasets of human preferences, we found that positively framed and behavior-focused principles align more closely with human decisions than negatively framed or trait-based ones. By applying psychometric methods (UVA and EGA), we identified a smaller, more effective set of principles that improved safety alignment while preserving general capabilities.\nDespite using conversational datasets with human preferences, our framework can operate without costly human annotations in two key ways. First, an LLM guided by a specific objective, such as prioritizing safety over convenience, can synthetically approximate human preferences. Second, our psychometric approach does not require human preferences, as it relies solely on analyzing the statistical relationships among principles.\nOur framework has three main limitations. First, we used Llama-3-8B as the LLM evaluator, but different models may yield varying results, warranting further exploration of model choice in principle alignment. Second, our fine-tuning approach and algorithm (ORPO) generates training data from principles but does not revise the potential responses, which may impact results; alternative fine-tuning strategies should be investigated. Third, while we focused on safety alignment, C3AI could be applied to broader use cases such as enterprise value alignment [74], regulatory compliance [4], and non-discriminatory AI [34].\nBeyond these limitations, two broader challenges remain: (1) indi-vidual differences in human values may require more personalized CAI approaches [42]; and (2) conflicting principles are inevitable in any constitution, necessitating strategies for balancing compet-ing objectives [7]. Addressing these challenges will be crucial for improving Al alignment and expanding the applicability of consti-tutional Al models."}, {"title": "A Statement Transformation Prompts", "content": "A.1 Sentence to statement transformation\nYou are an assistant tasked with rewriting sentences in an appropriate format to make them statements.\nDefinitions:\n- A value is a belief pertaining to desirable end states or modes of conduct, that transcends specific situations, guides selection or evaluation of behavior, people, and events.\n- A statement is a unit of input that directs behavior in accordance with a value. An example of a statement is: \"The assistant should not discriminate based on gender.\"\nYour task:\nRewrite each sentence in the form \"The assistant should...\" For example, \"The assistant needs to...\" should be rewritten as \"The assistant should...\"\n- Sentences that are questions or begin with \"Choose\" or \"Please choose\" should be rephrased to fit the \"The assistant should...\" format. For example, \"Please choose the response that is most supportive of life, liberty, and personal security\" becomes \"The assistant should be supportive of life, liberty, and personal security.\"\n- If a sentence refers to the user with pronouns like \"he\" or \"him,\" rephrase accordingly. For example, \"It is important to him not to be humiliated\" becomes \"The assistant should never humiliate the user.\"\nWhen a sentence expresses someone's values, rephrase it to imply that the assistant should uphold those values. For instance, \"He values justice for everyone, even strangers\" becomes \"The assistant should treat everyone justly, even people the user doesn't know.\"\n- Replace references to \"AI\" or \"The AI\" with \"The assistant,\" and replace references to \"the human\" with \"the user.\" For example, \"The AI should be helpful to the human\" becomes \"The assistant should be helpful to the user.\"\nMaintain the original meaning and structure as closely as possible, but avoid using phrases like \"choose the response that.\"\nBased on the definitions and instructions above, rewrite this sentence into a statement. Your reply should start with \"The assistant should \". The sentence is: [SENTENCE]\nA.2 Statement to principle transformation\nYou are an assistant tasked with rewriting statements into principles.\nDefinitions:\n- A value is a belief pertaining to desirable end states or modes of conduct, that transcends specific situations, guides selection or evaluation of behavior, people, and events.\n- A statement is a unit of input that directs behavior in accordance with a value. An example of a statement is: 'The assistant should not discriminate based on gender.'\n- A principle is a translation of a statement into a form useful for picking between two assistant responses to a user. A principle is stated as a prompt to pick one of two possible response options that align best with the specific statement. An example of a principle is 'Choose the response that least discriminates based on gender.'\nYour task:\n- Rewrite the statement to make it into a principle by refrasing the statement into the form 'Choose the response that...'.\nPreserve the original structure and meaning of the statement as much as possible.\nBased on the definitions and instructions above, rewrite this statement into a principle. Your reply should start with 'Choose the response that' The statement is: [SENTENCE]\nB Principle-Guided Preference Prompt\nWe formatted all the principle-conversation pairs uniformly and truncated any response options exceeding 250 words. We used only single-turn conversations, but this setup can accommodate multi-turn conversations as well.\n{few_shots}\nConsider the following conversation:\n{convo}\n{principle}\nOptions:\nA. {response_a}\nB. {response_b}\nOnly answer A or B. The answer is:"}, {"title": "B.1 Validation", "content": "We wanted to validate that an LLM evaluator would choose a re-sponse in line with a given principle when explicitly tested for that\nC Principle Sources and Preference Datasets\nWe selected 185 principles from the full 495 principle set that cover an array of AI and social sciences sources:\n\u2022 Anthropic principles are from the 58 items used in the orig-inal CAI work to fine-tune Claude, which were formulated in an ad-hoc manner from sources like the UN Declaration of Human Rights and the Apple Terms of Service [8],\n\u2022 Good for Humanity items are the 9 items aimed to guide model behavior to be less existentially risky for humans [44],\n\u2022 CCAI Top Agreed principles are from the 20 most-agreed-with crowdsourced items from the Collective CAI project [38] that solicited input on constitutional principles from a representative sample of Americans, 3\n\u2022 CCAI Top Disagreed principles are from the 20 most-disagreed with crowdsourced items from the Collective CAI project [38],\n\u2022 Human Values (General) principles are from the 10 general human values (such as Benevolence or Security) adapted from the Theory of Basic Human Values [61],\n\u2022 Human Values (Specific) principles are from the 57 specific survey items used in surveys to assess human values from the Theory of Basic Human values [61],\n\u2022 Jigsaw Bridging attributes and toxicity principles are from the 8 items used by Jigsaw in describing their Per-spective API classifiers [40] aimed at increasing online ci-vility inspired by bridging systems, \"which increase mutual understanding and trust across divides, creating space for productive conflict\" [51] together with reducing toxicity."}, {"title": "D Selected Principles", "content": "In \u00a74.3.3, using EGA, we selected 14 most informative, robust, and important principles (out of 185):\n(1) Achievement Alignment\n(2) Autonomy Values\n(3) Proximal Benevolence\n(4) Holistic Compassion\n(5) Considerate Restraint\n(6) Safety-Centric Harmony\n(7) Cultural Respect Commitment\n(8) Humanity Alignment\n(9) Benevolent Intent\n(10) Humanity First\n(11) Human Welfare\n(12) Influence Maximization\n(13) Success Support\n(14) Risk Embrace\nIn \u00a75, we apply EGA on the 58 Anthropic constitutional prin-ciples and 300 HH-RLHF Harmlessness conversations with the parameters described in \u00a74.3.3, resulting in 15 principles that we used for training the Anthropic-EGA model:\n(1) Benevolent Intent\n(2) Content Caution\n(3) Cultural Sensitivity (Audience)\n(4) Cultural Sensitivity (Background)\n(5) Cultural Sensitivity (Capitalist)\n(6) Cultural Sensitivity (Tradition)\n(7) Ethical Sensitivity\n(8) Freedom, Equality, Brotherhood\n(9) Harm Reduction\n(10) Human-Centric Flexibility\n(11) Humanity First\n(12) Less Humanity Threat\n(13) Non-Aggression\n(14) Universal Equality Rights\n(15) Vital Freedoms"}]}