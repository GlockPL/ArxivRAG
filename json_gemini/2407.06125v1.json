{"title": "Depression Detection and Analysis using Large Language Models on Textual and Audio-Visual Modalities", "authors": ["Avinash Anand", "Chayan Tank", "Sarthak Pol", "Vinayak Katoch", "Shaina Mehta", "Rajiv Ratn Shah"], "abstract": "Clinical depression, also known as Major depressive disorder (MDD), is a prevalent psychological disorder affecting the general population worldwide. Depression has proven to be a significant public health issue, profoundly affecting the psychological well-being of individuals. If it remains undiagnosed, depression can lead to severe health issues, which can manifest physically and even lead to suicide. Generally, Diagnosing depression or any other mental disorder involves conducting semi-structured interviews alongside supplementary questionnaires, including variants of the Patient Health Questionnaire (PHQ) by Clinicians and mental health professionals. This approach places significant reliance on the experience and judgment of trained physicians, making the diagnosis susceptible to personal biases. Given that the underlying mechanisms causing depression are still being actively researched, physicians often face challenges in diagnosing and treating the condition, particularly in its early stages of clinical presentation. Recently, significant strides have been made in Artificial neural computing to solve problems involving text, image, and speech in various domains. Our analysis has aimed to leverage these state-of-the-art (SOTA) models in our experiments to achieve optimal outcomes leveraging multiple modalities. The experiments were performed on the Extended Distress Analysis Interview Corpus Wizard of Oz dataset (E-DAIC) corpus presented in the Audio/Visual Emotion Challenge (AVEC) 2019 Challenge. The proposed solutions demonstrate better results achieved by Proprietary and Open-source Large Language Models (LLMs), which achieved a Root Mean Square Error (RMSE) score of 3.98 on Textual Modality, beating the AVEC 2019 challenge baseline results and current SOTA regression analysis architectures. Additionally, the proposed solution achieved an accuracy of 71.43% in the classification task. The paper also includes a novel audio-visual multi-modal network that predicts PHQ-8 scores with an RMSE of 6.51. The paper also discussed the potential limitations of the dataset and how they can be overcome.", "sections": [{"title": "I. INTRODUCTION", "content": "DEPRESSION is a widespread and debilitating psychiatric disorder experienced by people around the world [1]. It is marked by a continuous low mood and a diminished interest or pleasure in everyday activities. The effects of this disorder go well beyond emotional suffering, affecting all areas of a person's life, including their personal health and societal productivity. The prevalence of depression is alarmingly high, with estimates suggesting that it affects over 3.8% of the global population [2]. This statistic translates to about 280 million people grappling with this condition, spanning various demographics and geographies. Depression manifests uniquely across various age groups and genders, with around 5% of adults affected by the condition-comprising 4% of men and 6% of women. This indicates that women are more prone to depression than men. Among older adults, particularly those over 60 years old, the prevalence increases to approximately 5.7%. These numbers underline the significant impact of depression on public health and the critical need for effective management strategies. Depression can be triggered by a range of factors, both biological and environmental. Individuals who have undergone traumatic experiences, such as abuse or severe loss, are particularly susceptible to developing depression. Additionally, certain biological factors, including genetics and changes in brain chemistry, play crucial roles in the onset and progression of the disorder. This complex interplay of factors makes the detection and treatment of depression a challenging endeavour. [1]\nOne of the primary challenges in managing depression effectively is its diagnosis. Traditional methods for diagnosing depression primarily rely on subjective assessments, such as patient-reported questionnaires like the PHQ. These tools depend on the individuals' ability to report their feelings and behaviours accurately, a process often complicated by the nature of depression itself, which can distort self-perception and awareness. The subjective nature of these diagnostic tools can lead to considerable variability in diagnosing depression, with significant implications for treatment. Misdiagnosis or delayed diagnosis can prevent individuals from receiving the appropriate care and potentially aggravate the severity of the condition. Moreover, the lack of straightforward, objective diagnostic tests means that clinicians must rely on regular screenings to monitor the severity of depression in patients, a process that is not only time-consuming but also fraught with the potential for inconsistency.\nGiven these challenges, there is increasing interest in af- fective computing, especially in utilising behavioural clues to help detect and assess depression. Quantifiable data from be- havioural markers like facial expressions, speech patterns, and changes in physical activity levels can be analysed objectively. These markers can offer insights into an individual's emotional state, potentially helping clinicians detect depression more reliably and at an earlier stage. This emerging approach is part of a broader trend towards multi-modal frameworks in psychiatric assessment, where data from different sources are integrated to form a more complete and precise picture of a patient's mental health. By combining traditional psycholog- ical assessments with cutting-edge technology in behaviour analysis, researchers hope to develop more robust methods for not only diagnosing depression but also assessing its severity. In conclusion, Depression continues to be a significant global health issue due to its high prevalence, profound impact on quality of life, and the complexities involved in its diagnosis and treatment. Developing new tools that offer more objective and reliable assessments of depression is essential. This paper introduces a multi-modal architecture and analysis designed to utilise behavioural clues for more effective depression detection, marking a significant advancement in mental health diagnostics."}, {"title": "II. LITERATURE REVIEW", "content": "The AVEC workshop was started in 2011 and ended in 2019. Initially, the challenge focuses on multi-modal ap- proaches for sentiment analysis [3]. Later, specific challenges related to mental health diagnosis using Artificial Intelligence were introduced in the Depression Detection task introduced in the AVEC 2013 challenge [4]. In AVEC 2016 [5], and 2017 [6] and AVEC 2019 challenge [7], Distress Analysis Interview Corpus (DAIC) [8] [9], and E-DAIC dataset was used for depression detection and analysis respectively. The baseline system of depression detection and analysis of these AVEC challenges and their results for their multi-modal frameworks combining audio-video on the test set is given in TableI.\nA. AVEC 2019 Challenge\nFrom AVEC 2019 challenge [7] published papers, some of the exciting approaches we found based on machine learning and deep learning-based techniques were mentioned in [10], [11], [12], [13], and [14]. Firstly, Zhang et al. [10], who segregated the voices of Ellie and the patient from the audio recordings by using the timestamps given in the transcripts and extracted Extended Geneva Minimalistic Acoustic Param- eter Set (eGeMAPS) features using DigiVoice featurisation pipeline and based features using Collaborative Voice Analysis Repository (COVAREP) toolkit and applied Logistic Regres- sion with L1 regularisation and Random Forest Algorithm with hyperparameter tuning. They achieved the RMSE and Mean Absolute Error (MAE) of 6.78 and 5.77 on audio modality, which is lower than the baseline results of the AVEC 2019 challenge [7]. Another research is proposed by Fan et al. [11], created a multi-scale temporal dilated Convolutional Neural Network (CNN) for depression severity analysis on audio and textual features of the E-DAIC dataset and achieved the RMSE of 5.07 and 5.91, MAE of 4.06 and 4.39 and Con- cordance Correlation Coefficient (CCC) of 0.466 and 0.430 on validation and test data, respectively. Makiuchi et al. [12] applied a multi-modal fusion approach which combines audio and textual features for depression assessment by processing audio features from a pre-trained VGG-16 network through a Gated Convolutional Neural Network (GCNN) followed by a Long Short Term Memory (LSTM) layer and textual features obtained from Bidirectional Encoder Representations from Transformers (BERT) are processed using a CNN followed by an LSTM layer. Then, all the features were fused using a fully connected layer and yielded a CCC score of 0.696 and 0.403 on the development and test sets, respectively.\nYin et al. [13] suggested a multi-modal architecture for depression assessment using a hierarchical recurrent neural network which incorporates two hierarchies of Bidirectional Long Short-Term Memory (Bi-LSTM) memories for multi- modal fusion of data and applied the adaptive sample weight- ing mechanism to training data. On the validation and test sets, they achieved RMSE and CCC values of 4.94 and 0.402 and 5.50 and 0.442. Finally, Ray et al. [14] introduced a multi-level attention network consisting of only Bi-LSTM and attention mechanism for the depression severity analysis task. They achieved the RMSE, MAE and CCC of 4.37, 4.02 and 0.67 on textual modality, winning the AVEC 2019 challenge [7]. However, the models she proposed based on audio, video, and multi-modality beat the baseline results of the challenge.\nB. Post AVEC 2019 Challenge\nAfter the AVEC challenge, several researchers have also proposed various machine learning and deep learning-based architectures capable of handling single as well as multi- modality on several datasets, mainly focusing on AVEC 2016, 2017 and 2019 challenges [5], [6], and [7] datasets. Firstly, Zhang et al. [15] introduced a multi-modal framework for depression assessment by processing audio and video features using Multi-modal Deep Denoising Auto-Encoder (DDAE) and encoding them into fisher vectors. Relevant features are selected using the tree-based model. They used paragraph vector (doc-to-vec) models to extract textual-level features. Finally, the Multi-task Deep Neural Network is trained on these features plus the ResNet features and achieves a CCC of 0.560 in textual features, Mean Squared Error (MSE) of 20.06 and F1 Score of 91.7 per cent on multi-modal settings, accuracy of 89.3 per cent on audio-visual modalities, on the development set of EDAIC dataset [7]. Another research is proposed by Jo et al. [16] who introduced a four-stream depression detection model that uses an ensemble of Bi-LSTM and CNN to analyse audio and textual elements. They trained the model using the DAIC [8] [9], and E-DAIC [7] datasets and obtained F1 scores of 97 per cent and 99 per cent, Precision of 97 per cent and 100 per cent, and Recall of 97 per cent and 98 per cent, respectively. Sun et al. [17] proposed the multi- modal framework for depression analysis known as Cube MLP, which comprises three separate MLP units, each with two affine transformations, and each unit performs separate operations that are sequential mixing, modality mixing, and channel mixing on the data. They achieved an MAE and CCC of 4.37 and 0.583, respectively, on the E-DAIC dataset [7]. Yuan et al. [18] extracted textual features using sentence- level vector (sent2vec) encoders, the Universal Sentence En- coder, and Bi-LSTM and extracted audio and visual features using a combination of the PCA and Midmax algorithms. Then, they applied the Multi-modal Multi-order Factor Fusion (MMFF) algorithm on all the features, achieving RMSE, MAE and CCC of 4.91, 3.98, and 0.676, respectively, on a test set of the E-DAIC dataset [7]. Saggu et al. [19] proposed Depress- Net as a novel multi-modal machine learning framework for depression detection, which uses a Bi-LSTM layer network with an attention mechanism. This approach yields an RMSE and CCC of 4.32 and 5.36, and 0.662 and 0.457 development and test set of the E-DAIC dataset [7] respectively. Wang et al. [20] processed head pose and Action Units (AUs) as features and audio-based features from the COVAREP toolkit using CNN and LSTM in series and converted them into a feature matrix. The textual features are extracted using Sentence BERT (s-BERT) and passed through the network consisting of CNN and Bi-LSTM layers. Then, all the features are passed through Multi-modal Multi-Utterance-Bimodal At- tention Networks, followed by an attention mechanism, two LSTM and two dense layers. They trained their network on E-DAIC [7] and DAIC [8] [9] dataset and achieved the RMSE and MAE of 4.03 and 3.05, respectively, on the development set.\nSun et al. [21] proposed a multi-modal architecture called 'Tensorformer', which allows all the modalities to exchange all the relevant information simultaneously. They trained the network on the E-DAIC [7] dataset and achieved RMSE and CCC scores of 4.31 and 0.491, respectively, on the test set, beating the SOTA model of AVEC 2019 DDS challenge winner [14] in terms of RMSE only. Mao et al. [22] categorised the severity of depression based on the PHQ-8 Score into five categories that are healthy, mild, moderate, moderately severe, and severe. They used COVAREP features of the audio and Bi-LSTM and time-distributed CNN, and for textual modality, they used global vectors for word representation (GloVe) embeddings fed into the Bi-LSTM network. Finally, the fusion of modalities is performed using self-attention, dense layers, and majority voting. They achieved the F1 score of 95.80 per cent on patient-level depression detection on the DAIC dataset [8] and [9]. Teng et al. [23] proposed a study Integrating AVEC 2019 [7] and CMU-MOSEI [?] datasets. Their research adopts a multi-modal, multi-task learning approach to enhance depression detection accuracy. By leveraging emotional data, their method significantly boosts precision, achieving a CCC and MAE of 0.466 and 5.21 on a test set of the E-DAIC dataset.\nLi et al. [24] proposed the Flexible Parallel Transformer (FPT) model, which integrates video and audio descriptors. Tested on the E-DAIC dataset, the FPT model achieved an RMSE of 4.80, an MAE of 4.58, and a depression classifica- tion accuracy of 0.79 at a PHQ-8 threshold of 10, outperform- ing fewer complex models and proving the effectiveness of multi-modal approaches. G\u00f3mez et al. [25] proposed a multi- level temporal model analogous to multi-modal transformer architecture for depression detection by processing audio- visual embeddings, facial and body landmarks, and feeding into the transformer-based encoder. This approach achieved the precision, recall and F1-scores of 74 per cent, 84 per cent and 78 per cent, respectively, on DAIC [8] [9] dataset and 59 per cent, 58 per cent and 56 per cent, respectively, on E-DAIC dataset respectively. Steijn et al. [26] converted the audio into text using the Google ASR toolkit and extracted textual features using s-BERT and Linguistic Inquiry and Word Count (LIWC) toolkit and handcrafted features such as emotion, speech rate, etc., from the audio files. They also used the Ker- nel Extreme Learning Machine (KELM) algorithm for single multi-task regression and single-task classification purposes and the K-Fold cross-validation technique. They achieved RMSE and CCC scores of 6.06 and 0.62 when performing a summation of multi-task regression symptom predictions and RMSE and CCC of 5.37 and 0.53 using the second stage RF approach, which uses single-task classification symptom predictions on E-DAIC dataset [7].\nC. Large Language Models Based Approaches\nIn recent years, several researchers have started using Large Language Models for depression assessment, as done in [27], [28] and [29]. Firstly, Sadeghi et al. [27] proposed the text- based architecture for depression severity analysis on the E- DAIC dataset [7] by obtaining the textual transcripts from the OpenAI's Whisper-large model, extracted its features using OpenAI's GPT 3.5 Turbo model, fine-tuned the DepROBERTa model and extracted its features and trained the Support Vector Regression (SVR) model. They achieved the MAE of 3.56 and 4.26 and RMSE of 5.27 and 5.36 on the development and test set, respectively. Danner et al. [28] in which they combined the training set of DAIC [8] [9], and E-DAIC [7] datasets and train the BERT model for the depression detection task. They evaluated the model using the test set of DAIC [8] [9] dataset and combination of test set both DAIC [8] [9] and E-DAIC [7] datasets and achieved precision scores of 63 per cent and 83 per cent, recall scores of 66 per cent and 82 per cent and F1 scores of 64 per cent and 82 per cent, respectively. They also performed a direct evaluation of the GPT 3.5 model and ChatGPT 4 model using a test set of DAIC [8] [9] and achieved a precision score of 78 per cent, and 70 per cent, recall score of 79 per cent and 60 per cent and F1 score of 78 per cent and 61 per cent respectively. Finally, Hadzic et al. [29] performed a direct evaluation of Chat-GPT 3.5 and GPT 4 using a test set of DAIC [8] [9] and achieved precision, recall and F1 scores of 81 per cent, 70 per cent, and 71 per cent respectively for two-class classification. They also trained the BERT model on a training set of DAIC [8] [9] and E-DAIC [7] datasets.\nFurthermore, to demonstrate the capabilities of large lan- guage models, A. Anand et al. [30] finetuned an LLM through instructional calibration on a dataset tailored to High School Physics and utilizing retrieval augmentation. Their finetuned retrieval-augmented model, SciPhy-RAG, demonstrates signif- icant improvements over Vicuna-7b.\nAlso, in their other papers [31] and [32] A. Anand et al. propose MM-PhyQA, a dataset featuring high school-level multimodal physics problems and 'Mathquest' a maths dataset derived from 11th and 12th standard NCERT Mathematics textbooks respectively. These papers show their study to evaluate the performance of LLMs on these datasets, and their approach highlights the potential for enhancing LLMs' capability in textual corpora with specialized datasets and techniques.\nMoreover, using the same dataset MM-PhyQA, [33] pro- posed an LLM-based chatbot to answer multimodal physics multiple-choice questions (MCQs). They demonstrated that their Reinforcement Learning from Human Feedback (RLHF) and Image Captioning techniques improve the quality and accuracy of the model's responses compared to supervised fine-tuned LLMs.\nIn conclusion, the existing approaches in the literature have primarily been centred around the constraints of the AVEC 2019 challenge, and the integration of recent SOTA language models (LLMs) into their architectures needs to be addressed. This omission represents a significant gap, as modern LLMs have demonstrated remarkable natural language understanding, generation, and contextual reasoning capabili- ties. When effectively incorporated, these models can enhance the generalizability and efficiency of various applications, from sentiment analysis to complex predictive tasks; our research aims to bridge this divide by integrating SOTA LLMs into our proposed framework. By leveraging models such as GPT, Llama models and their successors, we aim to create a more robust, adaptable, and efficient architecture capable of handling complex linguistic nuances with minimal domain- specific adjustments."}, {"title": "III. DATASET", "content": "E-DAIC [7] is an extension of the DAIC-WOZ dataset and includes semi-clinical interviews intended to assist in diag- nosing psychological distress conditions, including depression, anxiety, and post-traumatic stress disorder. This dataset was also utilised in the AVEC 2019. It comprises 275 interviews conducted by an animated virtual interviewer named Ellie and by an AI-based agent, of which 163, 56, and 56 are included in the training, validation, and test sets, respectively. The training and validation sets consist of interviews taken by Ellie and AI- based agents, whereas the test set consists of only AI-based agents. Each interview record consists of the following files:\n\u2022\tAudio files in the form of .wav format.\n\u2022\tTranscripts of the interview in the form of .csv files.\n\u2022\tVisual features obtained from the OpenFace toolkit [34].\n\u2022\tThese features include head pose features, eye gaze features, position coordinates, head rotation coordinates, and 17 Facial Action Units (FAU or AU) stored in row and column format, making it 35 features corresponding to 17 AU. These features are also in the form of .csv files.\n\u2022\tThe Mel-Frequency Cepstral Coefficients (MFCCs) fea- tures extracted from the openSMILE toolkit [35]. These contain 13 MFCCs features with 13 delta and 13 double delta features combined to make 39 columns representing acoustic Low-Level Descriptors (LLD). These are in .csv file format.\n\u2022\tThe eGeMaPS [36] features extracted from the openS- MILE toolkit [35]. The eGeMaPS features contain 88 measures representing the LLD information of the audio file given in the dataset. These are also in .csv file format.\n\u2022\tThe Bag of Video Words (BoVW) obtained from the visual features mentioned above are processed and sum- marised over a block of 4 seconds for each step of 1 second.\n\u2022\tThe Bag of Audio Words (BoAW), derived from the above-mentioned audio features, is processed and sum- marised over 4-second blocks with a step duration of 1 second.\n\u2022\tMetadata files in the form .csv format consists of PHQ-8 Score, PHQ-8 Binary Score, Session ID etc. and the gender of the patient whose information is given in the table II where the PHQ Binary 0 Represents Healthy patient and one as Depressed patient and the figures 2, 3, 4 and 5."}, {"title": "IV. EVALUATION METRICS", "content": "For evaluating our experiments, we have used different evaluation metrics such as RMSE is a standard metric used to quantify the error of a model in predicting numerical data. It is used for Evaluating Regression experiments and is defined as:\n$RMSE = \\sqrt{\\frac{1}{M} \\sum_{i=1}^{M} (a_i - \\hat{a_i})^2}$ (1)\nwhere $a_i$ is the predicted value, $\\hat{a_i}$ is the actual value, and M is the number of observations.\nIn parallel, another metric we used for regression analysis is MAE, which measures the average magnitude of errors in a set of predictions without considering their direction. It is defined as:\n$MAE = \\frac{1}{M} \\sum_{i=1}^{M} |a_i - \\hat{a_i}|$ (2)\nwhere $a_i$ is the actual value, $\\hat{a_i}$ is the predicted value, and M is the number of observations.\nFinally, we also evaluated our regression results using the Concordance Correlation Coefficient (CCC) [37], which quantifies the agreement between two variables, yielding a value between -1 and 1, with 1 indicating perfect agreement. It is defined as:\n$CCC = \\frac{2\\rho \\sigma_a \\sigma_{\\hat{a}}}{\\sigma_a^2 + \\sigma_{\\hat{a}}^2 + (\\mu_a - \\mu_{\\hat{a}})^2}$ (3)\nwhere $\\rho$ represents the Pearson correlation coefficient be- tween the predicted and actual values, $\\sigma_a$ and $\\sigma_{\\hat{a}}$ are the standard deviations of the predicted and actual values, and $\\mu_a$ and $\\mu_{\\hat{a}}$ are the means of the predicted and actual values.\nFor the classification experiments regarding the textual modality, we used accuracy, F1-scores, precision, and recall as evaluation metrics to calculate the results' performance and compare them easily with other approaches.\nThese metrics have been kept the same as the AVEC 2019 challenge metrics so that we can compare our results with them."}, {"title": "V. PROPOSED METHODOLOGY", "content": "The Methodologies adopted to analyse the various modali- ties, such as audio, video, and textual, are based on regression analysis across multiple models and algorithms. For the regres- sion task, we used the PHQ-8 scores, which ranged from 0 to 24; each of the eight questions in the PHQ-8 questions was scored from 0 to 3 to determine the severity of the depression. Each score from 0-3 measures how many days the subjects have been experiencing the questionnaire problems given in the past two weeks, with 0 being \"Not any day\" and 3 being \"Every day\". Finally, the scores from these eight questions are summed to calculate the PHQ-8 score. [38]. For the textual analysis, the methodology adopted has been based on two Supervised learning tasks: classification and regression. For classification experiments, the Subjects were divided into three classes: Healthy (PHQ-8 from 0-8), Mild (PHQ-8 from 9-15), and Depressed (PHQ-8 from 16-24).\nA. Audio Data\nUpon analysis of various audio features to understand patient conditions better, we used eGeMaps features. For a comprehensive understanding, we have calculated the mean (\u03bc) and standard deviation (\u03c3) of these features for patients divided into three classes: Healthy, Mild, and Depressive. These statistical measures help illustrate the differences and similarities in audio characteristics across the groups. The mean values indicate each feature's central tendency or average level within a group, while the standard deviation provides information about the variability or dispersion of the features around the mean. Table III presents the detailed results, showcasing \u03bc and \u03c3 for each audio feature across the classes divided based on PHQ-8 scores: PHQ-8 scores from 0-8 as Healthy, 9-15 as Mild, and scores 16-24 as depressed. This data was essential for identifying patterns and trends that can distinguish between healthy individuals and those with varying degrees of depression. By examining these metrics, we understood how certain audio features correlate with different levels of depression, ultimately aiding in more accurate and early diagnosis.\nAs the trends Observed in III. Loudness Reflects the perceived intensity of the sound. Variations in loudness can indicate different emotional states. The loudness in the depression class is lower than the loudness in the non- depression class. Hammarberg Index Measures the ratio of high-frequency to low-frequency energy. It helps in assessing the quality of voice. As shown, the Hammarberg Index of depression is lower than that of the non-depression. Spectral Flux represents the rate of change in the power spectrum. It helps identify changes in speech patterns. The Spectral Flux of the depression is lower than that of the non-depression. Jitter Indicates frequency variation between cycles of the vocal waveform. High jitter values can be associated with voice pathologies. The mean Jitter of the depression is lower than that of the non-depression. Shimmer Represents amplitude variation between cycles. Like jitter, higher shimmer values can indicate vocal issues. The mean Shimmer is lower in depressed individuals than in non-depressed individuals.\nFor Modelling tasks, we have performed the following experiments:\n1) BiLSTM: To preprocess the audio features such that they can be used for experimentation, we clipped the first two columns ('name' and 'frame-time') from MFCC features so that only features remain; then, we normalised the MFCC values using standard scalar so that all samples are scaled. Also, to make all the samples consistent for training, we set a padding threshold of 80,000 rows. We found that MFCC features performed better than eGeMaps, so we only used MFCCs in this experiment. Although the eGeMaps did contribute to a deeper understanding of audio features as discussed in III. According to the majority of approaches in the AVEC challenge 2019, BiLSTM-based networks were ubiquitous; this is due to the sequential nature of features given, which are on a frame basis for every milli second; in fact, the SOTA approach was also BiLSTM-based, so we first tried a BiLSTM-based architecture to perform regression on MFCC-feature data given in the dataset. This architecture employs an attention-based BiLSTM model. The model consists of two LSTM layers with 200 hidden units, which connect to a fully connected layer, taking 39 MFCC features as input and producing a single output for regression.\n2) Whisper: Whisper [39] is an Automatic speech recogni- tion (ASR) and Speech translation model released by OpenAI. Its remarkable speech recognition, translation, and transcribing capabilities have been leveraged for our interview audio files. We have fine-tuned the Whisper-Base model (74M param- eters) applied to patients' raw interview audio files from the dataset and performed a downstream Regression task. Whisper is an encoder-decoder architecture-based model in which the encoder module transforms the input audio files into encodings, which then are given to the decoder that converts them into text tokens. So we have used the encoder module to encode the audio files, and then these 512-dimensional encodings are followed by regression layers composed of a series of fully connected layers (4098, 2048, 1024, 512, 1) with dropouts. Each layer reduces the dimensionality through linear transformations and ReLU activation, culminating in a single neuron output for performing regression. We have performed the training on a 50 GB RTX A6000 GPU for all our tasks.\nB. Visual Data\nFor the video features, we used the Pose, Gaze, and AU pairs, represented by 49 features, containing six poses,"}, {"title": "VI. RESULTS AND DISCUSSION", "content": "A. Regression Analysis\n1) Results on Textual Modality: The results from different models on the textual modality in terms of evaluation metrics on the test set are displayed in Table VI. The textual experiments are performed on the test set in contrast to the audio and video experiments performed on the validation set. This has made it easy to compare most audio-visual and multimodal results on the validation set. The approaches on textual modalities such as in [28] have been performed in the test set, and hence, we have also adopted the evaluation on the test set. DeepRoBERTa tokeniser + RoBERTa model achieved the RMSE and MAE scores of 6.047 and 4.885, respectively; GPT 3.5 achieved the RMSE, MAE, and CCC scores of 5.896, 4.589 and 0.474, respectively, and LLAMA 8B Instruct model achieved the RMSE, MAE and CCC scores of 6.293, 4.893 and 0.494. On the other hand, GPT 4 achieved the RMSE, MAE and CCC scores of 3.975, 3.161 and 0.781, respectively, beating the current SOTA results for textual regression [14]. The knowledge of PHQ questionnaires helps the models accurately predict the subject's state as they try to identify the PHQ-8 answers and scores from the transcript text and aggregate them to determine the PHQ-8 score. The knowledge of class division defined in two-shot prompts also helps predict the correct score and label.\n2) Results on Audio and Visual Modalities: The results obtained from different audio and visual modalities models on the validation set in terms of RMSE, MAE and CCC are shown in Table VII. The whisper model on audio features achieves an RMSE score of 5.7 on the validation set, whereas the Bi-LSTM model on visual features achieves an RMSE of 6.45. On merging both audio and visual features, we achieved an RMSE score of 6.72 on the validation set when they were trained on the Whisper and BiLSTM model, which is 6.51 and an RMSE score of 5.39 on the validation set when trained on the BiLSTM model.\n3) Comparative Analysis: Table VIII compares RMSE and CCC scores of various state-of-the-art architectures with models trained or prompt engineered on various modalities (mainly textual modality) when inferred on a test set. From Table VIII, it is evident that the DepROBERTa tokeniser and ROBERTa model outperformed the baseline models cited by the competition [7] and the models proposed by Steijn et al. (using multi-task regression symptom predictions) [26], Makiuchi et al. [12], and Zhang et al. [10] in terms of RMSE scores. Additionally, it performed better than the model proposed by Teng et al. [23] and Zhang et al. [10] regarding MAE.\nThe LLAMA 8B instruct model exceeded the competition's baseline models [7] and the model by Zhang et al. [10] regarding RMSE score. It also outperforms models by Teng et al. [23] and Zhang et al. [10] in MAE and performed better than competition's baseline model [7] as well as models proposed by Fan et al. [11], Yin et al. [13], Teng et al. [23], Makiuchi et al. [12], and Saggu et al. [19] in terms of CCC score. The GPT 3.5 model, using two-shot prompting, also surpassed the competition's baseline models [7] and the models by Fan et al. [11], Steijn et al. (with multi-task regression symptom predictions) [26], Makiuchi et al. [12] and Zhang et al. [10] in RMSE scores while performing better than Teng et al.'s model [23], Sun et al. [21] and Zhang et al. [6] in MAE. Additionally, it performed better than the competition's baseline model [7] as well as models proposed by Fan et al. [11], Yin et al. [13], Teng et al. [23], Makiuchi et al. [12], and Saggu et al. [19] in terms of CCC score.\nMoreover, the GPT 4 model outperformed all current state- of-the-art models and the competition's baseline in RMSE, MAE, and CCC scores on the test set, establishing new SOTA performance. In audio and visual modalities, the models performed better than competition baseline models [7] but not much better than other SOTA approaches regarding RMSE scores on the validation set.\nB. Classification Analysis\nThis section discusses the classification results of all the models in textual modality. Although we tried performing classification on audio video experiments by modifying the architectures by replacing the last regressor layer with a classification layer and cross-entropy loss, the results were unsatisfactory; the confusion matrices showed all the samples classified as the healthy class. Therefore, the results are computed for the test set based solely on the textual modality, including metrics such as accuracy, F1 scores, precision, and recall scores. The results are in Table IX and the corresponding confusion matrices are in figures 7 8 9:"}, {"title": "VII. CONCLUSION AND FUTURE SCOPE", "content": "In our experimentations, we have demonstrated multiple approaches on different modalities and achieved a SOTA result on regression analysis of two-shot prompting from the GPT -4 model. These Experiments show the superior capabilities of LLMs for text-based tasks such as regression and classification over other proposed architectures.\nA significant challenge during our experimentations was the limited number of samples in the dataset. The increased number of samples might result in better results by fine- tuning LLMs for textual modality. Also, raw video files and audio availability might result in more comprehensive behavioural clues for analysis and detection. The method of Data augmentation to increase the number of samples might not be ethical, given the nature and sensitivity of the data and the patient's privacy. Hence, reliable and valid data annotated by clinical experts must be created. As seen in this paper, the Textual modality, due to LLMs performance, was leading among all other modalities. For future scope, multimodal LLM-based"}]}