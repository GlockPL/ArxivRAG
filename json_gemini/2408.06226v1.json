{"title": "A Large-Scale Study of Model Integration in ML-Enabled Software Systems", "authors": ["Yorick Sens", "Henriette Knopp", "Sven Peldszus", "Thorsten Berger"], "abstract": "The rise of machine learning (ML) and its em- bedding in systems has drastically changed the engineering of software-intensive systems. Traditionally, software engineering focuses on manually created artifacts such as source code and the process of creating them, as well as best practices for integrating them, i.e., software architectures. In contrast, the development of ML artifacts, i.e. ML models, comes from data science and focuses on the ML models and their training data. However, to deliver value to end users, these ML models must be embedded in traditional software, often forming complex topologies. In fact, ML-enabled software can easily incorporate many different ML models. While the challenges and practices of building ML-enabled systems have been studied to some extent, beyond isolated examples, little is known about the characteristics of real-world ML-enabled systems. Properly embedding ML models in systems so that they can be easily maintained or reused is far from trivial. We need to improve our empirical understanding of such systems, which we address by presenting the first large-scale study of real ML-enabled software systems, covering over 2,928 open source systems on GitHub. We classified and analyzed them to determine their characteristics, as well as their practices for reusing ML models and related code, and the architecture of these systems. Our findings provide practitioners and researchers with insight into practices for embedding and integrating ML models, bringing data science and software engineering closer together.", "sections": [{"title": "I. INTRODUCTION", "content": "Many recent breakthroughs in machine learning (ML) have given rise to ML-enabled software that was not realizable before. Consider cyber-physical systems, such as autonomous vehicles [1], [2] or unmanned aerial vehicles [3], but also software in finance [4] or healthcare [5]. All benefit from advances in ML model architectures and training algorithms. Nevertheless, developers still struggle to develop systems that integrate ML models with traditional software components. In fact, industrial surveys report that 78% of ML projects stall [6], while others say as many as 85% fail [7]. Many fac- tors can cause such failures [8], [9] [10], including data quality, waterfall-like development processes, limited experience and education, but also lack of proper tools [11], [8], [12], [13]. In particular, the integration of ML models into software systems pose new challenges, requiring boilerplate code and additional infrastructure to execute the models [12]. As ML models are typically probabilistic models, their behavior can be undefined for certain inputs, or they can produce unwanted results violating domain restrictions, which often requires to manually implement safeguards, especially in safety-critical systems [14]. Consider perception systems in autonomous driving as an extreme example, which can boast up to 28 different ML models that interact with each other [15]. Furthermore, existing tools and processes, e.g., version control systems and dependency management, are often perceived as inadequate by practitioners [16], [17]. In particular, reusing pre-trained ML models has become common in developing ML-enabled systems, as it allows for larger models trained on more data while reducing costs [18], [19], but does not fit with current software engineering practices. However, a detailed overview on reuse practices is missing. Plenty of research [20], [21], [17], [14], [22], [23], [24], [25] has aimed to characterize the current state of practice of the development of ML-enabled software and describe potential problems. Among the most frequently named prob- lems are data quality, integration of ML models with tradi- tional software components, tool support for managing ML- related artifacts, and quality assurance, particularly concerning dependability and safety [26]. Current research in software engineering seeks to address these deficiencies by proposing novel workflow patterns and providing improved tool support supporting ML experiments [27], [21], [20]. Advances in this area so far have been limited, which can be attributed to a lack of understanding of the current practices and concrete prob- lems of actually developing ML-enabled software systems. In other words, while we have focused on ML engineering, we still know very little about how ML models are actually embedded and integrated into ML-enabled systems. We present an empirical study of 2,928 ML-enabled soft- ware repositories on GitHub, characterizing the use of ML models in software systems. We are especially interested in the reuse of ML models across software systems, as well as the integration of ML models. We define three research questions: RQ1: What are characteristics of ML-enabled systems? To understand ML-enabled software systems, we need to know what role ML plays in them and what characterizes them. For instance, what is the proportion of ML code in relation to non-ML source code, what types of software systems employ ML (e.g., libraries or end-user oriented applications), and how are they doing so? Additionally, quality assurance practices play an essential role and may vary [14]. RQ2: How are ML models reused across software systems? As models grow larger and more general, using pre-trained ML models becomes more common [28], [29]. While many forms of software reuse have been analyzed, i.e, by copying code [30], [31], and often impede maintainability, so far with-"}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "ML-enabled systems are software systems containing ML- based functionality, ranging from utility functions to core application logic. The functionality can involve making predictions, preprocessing data or, since recently, creating content via generative AI [33]. ML-enabled systems employ at least one ML model processing data fed into it at runtime. Engineering ML-Enabled Systems. Introducing ML models gives rise to novel engineering challenges for development processes, architectures, and testing, among others. These arise from the fundamentally different nature of ML models compared to traditional software [8]. ML models are prob- abilistic, essentially constituting unreliable functions, while traditional software is more deterministic. This introduces uncertainties and risks throughout the lifecycle of a project. The development process for ML models is data-driven and highly iterative. The most challenging aspects of development are related to management and quality of the data required for training and validation [11], [21]. Concrete problems include, for example, additional dependencies between source code, ML models and external data, feedback loops, quality and management of data, and experiment management [20]. Properly integrating ML models with traditional code is also challenging. Models are typically encapsulated in software modules containing the ML model (binary) made available to execution via an ML framework together with glue and integration code. Integration challenges have been illustrated in the literature as anti-patterns, including excessive use of glue code, complicated data-processing pipelines, dead experimental code paths, and lack of abstraction [12]. Another challenge lies in the different toolchains for ML models and software. Improving the empirical understanding of ML-enabled systems will help in building better and well-integrated tools. There also is a lack of tools for managing (e.g., versioning) the many different artifacts (called ML assets in the remainder) for ML. ML assets are model implementa- tions, code surrounding ML training and experiments, model binaries and other files related to the development or execution of ML models. Traditional software engineering relies on well- established version-control and collaborative development systems, such as Git. However, these systems are not suited for additional assets required for ML, such as datasets, hyperparameters, data transformations, and models [17], as well as for managing ML pipelines and tracking the many different experiments needed to optimize model performance. ML Frameworks and Libraries. The three most popular ML libraries [34] are PyTorch [35], TensorFlow [36], and Scikit- Learn [37]. Many others are built upon them. While Tensor- Flow and PyTorch offer deep learning (DL), Scikit-Learn of- fers ML techniques, but mostly using traditional ML, ranging from supervised methods (e.g., linear regression and percep- tron) to unsupervised methods (e.g., clustering and PCA). TensorFlow and PyTorch are frameworks allowing developers to implement their ML models modularly. In both, models are abstracted by a class \u201cModule,\u201d which represents either complete models or their building blocks (e.g., layers). This provides a common interface for generic operations, such as forward and backward pass. A similar class called \"BaseEstimator\" exists in Scikit-Learn. A wide range of basic building blocks, such as linear layers or convolution layers is also provided in PyTorch and TensorFlow. They also provide loss functions, optimizers, and other utility functions for ML. Model Reuse. Developing ML models is still challenging and requires expertise in data science and machine learning. To integrate ML into software, developers can also reuse estab- lished models, reducing effort and data needs while improving model performance and robustness [29][28]. Two approaches to model reuse exist: (i) in transfer learning [38] the model is trained on a generic dataset and is then fine-tuned to solve a more specific problem; (ii) pretrained models (PTMs) are fully trained and usually reused without any fine-tuning. PTMs can be loaded directly by copying the model or accessing it via an API, typically via so-called model hubs [39]. However, while model reuse helps engineering ML-enabled systems, it is still one of the main challenges [21]. While studies on reuse exist, for instance, on how models could be differentiated to identify plagiarism [40], or on reuse practices of models from specific libraries or platforms [41], there is still a lack of hard empirical data on reuse in real ML-enabled systems. Our work expands these studies. We aim to determine the state of ML model reuse on GitHub, which mainly focuses on common practices among open-source projects."}, {"title": "III. METHODOLOGY", "content": "Figure 1 shows our methodology of mining, classifying, and analyzing repositories quantitatively and qualitatively.\nA. Subject Selection\nWe relied on GitHub's dependency graph to obtain an initial set of repositories that use one of the three most popular [34] ML libraries (IC\u2081) and that are implemented in the currently most popular programming language for ML projects (IC2). Inclusion Criteria. We included repositories when the follow- ing two criteria applied.\nIC\u2081 the repository depends on TensorFlow [36], PyTorch [35], or scikit-learn [37]. This criteria aimed to identify ML- enabled systems. These three popular libraries support developing (especially training) ML models, but are also needed to execute the models in a software system. IC2 The main programming language is Python. This criteria aimed to enhance relevance our analysis not least since Python is the currently most popular programming language for ML-enabled systems\u2014and the comparability of the subject systems by focusing only on one language. Applying IC1 yielded 255,096 repositories dependent on Py- Torch, 233,779 on TensorFlow, and 465,980 on Scikit-Learn, and IC2 in 405,091 Python repositories. We then screened this initial dataset and further filtered the repositories to obtain a high quality set of ML-enabled software systems of a certain size and maturity. For instance, we excluded tutorials and small toy projects. Exclusion Criteria. We filtered out repositories when at least one of the following criteria applied.\nEC1 The repository is not original, i.e., forked or duplicated from another one. Forks would distort the results, as popular projects with many forks would be counted multiple times and disproportionately affect the results, without providing additional insights.\nEC2 The repository has fewer than 100 stars. Sufficient popularity ensures to avoid practically irrelevant projects. EC3 The codebase contains fewer than 5,000 lines of Python code. This criterion aimed at ensuring a certain size of the code, excluding tutorials and toy projects. The thresholds for (EC2) and (EC3) were chosen based on previous studies [44], [45], [46], [47], [48] and manual explo- ration of a sample. We set them relatively low to exclude sim- ple toy projects, but still obtain a large, representative dataset. We excluded forks (EC\u2081) in two steps: First, we excluded projects that were forked directly from GitHub, which can be accessed through GitHub's REST API. Second, since we found that the dataset still contained a significant number of duplicates we identified them based on identical README files and kept only the repository that was created first. This selection resulted in a final dataset of 2,928 repositories, containing systems with medium to high maturity.\nB. RQ1: System Characteristics\nSystem Classification. Software systems can be of different types, such as applications, libraries, and frameworks [51], [46]. To obtain an initial overview of the dataset and to identify differences in the use of ML, as well as the implications for engineering practices, we first determined what types of projects are included in the dataset and how they relate to ML. The classification of project types is inspired by Idowu et al. [51], who studied characteristics of GitHub repositories with ML models. Our labels describing how ML is used in relation to the systems functionalities is inspired by other studies [44], [52], [13]. In our study we systematically refined existing label definitions for ML resulting in the labels above."}, {"title": "C. RQ2: ML Model and Code Reuse", "content": "To analyze the reuse of ML models across different software systems, recall that it can be achieved in two ways: (i) Use of pre-trained models involves obtaining an already trained ML model from an external source; or (ii) reuse of only the architecture of an ML model, by copying the related implementation in source code. We considered both ways. Identify the Usage of Pretrained Models in Code. A common source for pre-trained model are so-called model hubs. Via these, researchers and practitioners can distribute their own models or find models that solve a specific task. Jiang et al. [39] identified 8 such model hubs when studying the content of their provided artifacts and related security risks: PyTorch Hub [55], Tensorflow Hub [36], Hugging Face [56], Model Zoo [57], ONNX Model Zoo [58], Model Hub [59], NVIDIA NGC [60], and MATLAB Model Hub [61]. We disregard MATLAB model hub from our analysis, because it is intended only for the MATLAB programming language, while we focus on Python. We use the same list to identify model reuse in our dataset. A full list of the model hubs with their detailed APIs for model loading can be found in our online appendix [32]. From the ASTs of the software systems we identified whether and where their APIs are used. Extracting Duplicated ML Assets. Many techniques for code duplicate detection exist [62], [31]. Upon screening meta-studies [63], [64], we decided to use JPlag [65], which supports Python and is ranked as one of the best tools. In a pairwise comparison a match is found when a tokens in a sequence for two files match. We used a thresh of 100 tokens to avoid false positives. Based on these results, we identified the most prominent sources of duplicated model implementations by sorting the subject system by the amount of duplicates shared with all others systems. The metric used is defined as sum over the number of duplicated tokens for all other systems in the dataset. Then, starting from the projects with the highest amount of duplicated code, we manually inspected these systems to determine if they are an original or what the original project was. To this end, we analyzed the copied source files and searched for hints on their origin. Usually, the copyright information was left in place crediting the source of the code. This analysis was repeated until we encountered no more original systems for the duplicated ones. We report the number of code duplicates from each original system in our dataset. The original systems were not always contained in the dataset, either because they did not match the filtering criteria, or because they were not stored on GitHub. In these cases, we use another system, that only contains a com- plete duplicate from this system and no other, as a placeholder to report the number of duplicates from the original one."}, {"title": "D. RQ3: ML Integration Architectures", "content": "Integration of ML Models. To identify patterns of how ML models are integrated, we manually and qualitatively analyzed a sample of our subject systems that we had classified as type \"Application\" and which uses ML to implement business logic (\"Business Focused Applied\") (cf. Sec. III-B). For each system we first identified the models, as described in Sec. III-B. Using the model classes as starting points, we analyzed the surround- ing source code to gather information about the integration of the ML models. We sought to answer the following questions: ML Functionalities and Tasks: What are the ML models used for? What functionality is implemented using ML? Which ML task realizes the functionality? Number and Type of Models: How many ML models are used in the system? What type of ML do the models belong to? What ML technology is used for the models? Integration of Models: How do multiple models interact? (Parallel, alternative, sequential, fusion/forking) Origin of the Models: Is the model custom-built or reused from third-party? If reused, is it pre-trained or fine-tuned? Storing & Loading of Models: Where are the ML models stored and how are they loaded into the system? Are there stored locally or are they loaded from a model hub (remote)? Model In- & Output: What is the input and output of the ML models? What kind of data is used and which data types? Pre- & Postprocessing: What pre-and postprocessing steps are implemented for the ML models? By following method calls, we investigated where models are instantiated. We disregarded instantiations that are tests or demonstrations, or that occur in files no longer in use, but kept by developers. For the instantiations that occur in actual production-code, we then proceeded to analyze the surrounding source code to determine the input and output of the model. If necessary, we followed the method calls further to clarify the data format of the input and output, and possible pre-and postprocessing steps. After checking each instantiation for each model, we summarized the results, obtaining the total number of models that are actually used. We time-boxed our manual analysis to two hours for each project, ensuring the efficiency of the process. If we could not gather enough information in that time, e.g., due to insufficient documentation or poorly structured source code, we reported the respective question as unknown."}, {"title": "E. Threats to Validity", "content": "Internal Validity: Our automated analysis relies on the Python AST, in which we noticed smaller parser errors for individual files of some systems, however, the number of occurrences is negligible, not impacting our results overall. The use of wrappers in ML-enabled systems impacts the automated analysis because they hide the real number of ML functions, e.g., when analyzing how many and where ML models are loaded. As a result, our results only show a lower bound on the number of ML functions. Here, our manual analysis shows more accurate results, but is limited to fewer subject systems. The sometimes generic names of the APIs used to load ML models, may lead to false positives, addressed by only considering API calls in the context of a model hub and not being ambiguous about the object they load. External Validity: Considering only three ML libraries (PyTorch, TensorFlow, and Scikit-Learn) for selecting repositories may limit the generality of the derived dataset, but these libraries are used for the vast majority of ML- enabled systems [34]. While we focus on Python, PyTorch also provides a C++ API (although it is primarily a Python library) and TensorFlow supports a range of programming languages, including Java, JavaScript, and C++. This enhances our external validity, as most of our results likely apply to non-Python systems as well."}, {"title": "IV. SYSTEM CHARACTERISTICS (RQ1)", "content": "System Sizes, Relevance, and Popularity. To obtain an impression of relevance and popularity of the systems analyzed, we used metrics from GitHub, such as the number of stars and commits. The average is at 1,487 stars (median 327) and 829 commits (median 187) (see Fig. 2), which indicates medium to high popularity. To estimate system sizes, we derived code properties: number of source files, classes, functions, and lines of code (LOC). The results show that our subjects span a wide range of sizes, with on average 263 source files (median 119), 34,000 LOC (median 12,000), 252 classes (median 113), and 1,171 functions (median 715). Outliers are The Algorithms (181k stars), a collection of ML algorithms for learning purposes and the Azure SDK for Python with the largest codebase with 10 million LOC. System Types and Relation to ML. To understand how the systems relate to ML, we manually classified a random sample of 160 systems. We derived labels for system types and how ML is used. Our appendix [32] has detailed label descriptions, examples, and labeling guidelines. We differentiated between the following four types of systems. Applications are standalone, executable systems that provide end-user-oriented functionality through a user interface (e.g., GUI or command line). They do not require users to have programming skills. Application solve problems and provides solutions to end-users, implementing business logic to do so. Libraries provide functionality intended to be used in other programs via code APIs. Except for the provided functionality they contain no application logic and cannot run standalone. Frameworks provide general application logic, but are designed to be extended by concrete functionality orchestrated by the framework. To this end, frameworks take control of the code that uses the framework, according to the principle of dependency inversion [66]. Similar to libraries,"}, {"title": "V. REUSE OF ML MODELS AND CODE (RQ2)", "content": "We have identified two different ways ML models are reused. ML models can be distributed through ML model hubs or model registries. These model hubs serve as libraries for ML technology and provide users with pre-trained ML models. We also observed that a common practice is to reuse ML models by cloning their implementations from other repositories. Use of Pre-Trained Models from Model Hubs. We find that 1,209 of the 2,928 systems (41%) load at least one pre-trained model (PTM) from a model hub. Of those systems, which load at least one PTM from a hub, the average number of respective model hub API calls is 29. Figure 9 shows the results. A single API call loads either a models component a full functional model. Model components are assembled into a single usable ML model, which is particularly common for encoder, decoder, and transformer models, which we will see in our manual analysis (RQ3, Sec. VI). However, how many components form a single model varies. In addition, systems may use different PTMs or load a model in multiple places, such as in productive, test, and demonstration code. To better understand why multiple models are loaded in the same system, we analyzed where the API calls are located and what their purpose is, i.e., demo or test code, as determined by file names and directory paths. We find that 8.3% of these files, which contain API calls related to loading ML models from a model hub, are located in test directories, and 9.8% are located in directories named \"demo\" or \"experiment.\" The remaining 81.9% are located in directories that fit neither of these criteria, so most likely contain production code. This indicates that systems loading PTMs either load multiple models as part of the production code or build models out of model components loaded individually. Both is the case, as we will see for RQ3 (Sec. VI). The identified load calls in test directories are distributed over 263 systems (22%), those in demo directories over 254 systems (21%), and those in other directories over 1,129 systems (93%). in each of the three categories we also looked at the combinations. The majority of systems (809) load ML models for purposes that are not related to testing or demonstration purposes. It is safe to assume that these systems load the models only to enable functionalities. This result was to be expected. The total number of systems that do not use testing (955) is not surprising but nevertheless shows that pre-trained models are considered as out-of-the-box solutions which do not require additional quality assurance. It is notable to show that there are some systems that only load models for testing or demonstration purposes (80). This can be explained by the ML Tools we identified in RQ1 (Sec. IV). While MLOps or experimentation management tools to not use ML to realize functionalities they still might include some to test their system or provide examples. Reuse of ML Code. To identify how model implementations are reused across our subject systems we investigated for each project, how much of the source code related to the implementation of ML models is duplicated from other systems in the dataset. We found this type of reuse 1,690 of our subject systems (58%). These contain, on average, 18 files which are at least partially duplicated. For context, the average number of files implementing ML models is 34, so for half of the systems in the dataset, half of the model implementations are at least partially copied. In summary, reuse through code duplication is not predom- inant in a single ML technology, but distributed across all areas, as can be seen by the many different ML technologies (transformers, stable diffusion, and others) present in Table II."}, {"title": "VI. ML INTEGRATION ARCHITECTURES (RQ3)", "content": "To understand practices of integrating ML models into applica- tions (i.e., where ML supports end-user-oriented functionality), we manually inspected systems we classified (Sec. IV) as Applications and as using ML as part of their business logic (Business Focused Applied), amounting to 26 of the 160 clas- sified systems. We now report results for each of the questions defined in Sec. III-D. All projects are listed with their details in our online appendix [32]. In the remainder, we also refer to individual projects (typeset in sans-serif) for illustration. ML Functionalities and Tasks. The functionality provided to end users, implemented using ML models, varies widely. Common functionalities include chatbots (5 systems), video processing (5 systems), image processing (6 systems), and data analysis (4 systems), and robotic navigation (3 systems). The three other systems use ML to transcribe music, infer metada from scanned documents, and play games. On a technical level, the underlying ML tasks performed are generative AI (11 systems), classification (6 systems), decision making (4 systems), segmentation (4 systems), dimensionality reduction (3 systems), and clustering (1 system), with 2 sys- tems not fitting into established task categories. For example, tournesol uses linear regression to visualize correlations. Four systems combine tasks to provide functionality; for instance, Video-ChatGPT is a chatbot with video processing capabilities that allows user to query the content of videos. Number and Type of ML Models. With 23 out of 26 systems, the majority employs multiple ML models to implement the functionality (see Fig. 10). The highest number 18 in one system (h2ogpt) and systems with 7 or more models are frequent, but most systems utilize below 5 ML models. Three systems allow to select arbitrary models from model hubs, making the number of models practically infinite. For example, gentopia allows the user to configure a chatbot by selecting a model from Hugging Face. The most common model type is Deep Neural Network (DNN, i.e., DL), found in 21 systems. Classical ML (ML that is not DL) was found 5 times. Within the DNN models, we observed multiple variations, including 8 Transformers, 6 Convolutional Neural Networks (CNN), 3 Fully Connected Neural Networks (FCNN), and one Long Short Term Memory (LSTM) [70]. CNNs are used exclusively for image processing, whereas Transformers and LSTMs are mainly intended for natural language processing, although we find 3 Transformers that are used for image generation (e.g., ControlLoRA). ML can be further divided into Supervised Learning (SL), Unsupervised Learning (UL), and Reinforcement Learning (RL) [71]. UL was found only in the form of classical ML, such as Principal Component Analysis (PCA) (3 systems) and Clustering (1 system). Two systems use supervised classical ML. The DNNs are mostly used for SL, but 3 RL systems were also found. While classical ML is mostly used for auxiliary functions, e.g., Corpus Tools uses PCA to support data visu- alization, the use of CNNs covers all observed functionalities. Interaction Between ML Models. In the 23 systems with multiple models we observed that interactions in various ways. Inspired by Peng et al. [15], we categorized and analyzed the interactions as follows: Alternative models is the most common pattern (15 systems), where multiple models are offered for the same task, either automatically by the pro- gram or based on user input. For example, home-robot uses different RL agents for different tasks (e.g., navigating or manipulating objects). Independent models used for unrelated functionalities are found in 7 systems. Larger systems in particular provide the user with multiple functions, some of which require ML. For example, VIAME offers image processing tools (e.g., for segmentation or classification) to be used independently by the user. Sequential integration of ML models was observed in 7 systems. They are used for complex tasks where no single model provides the desired functionality. The output of one model is the input of another. A common case are perception systems where objects are detected in a first step and classified in a second step; e.g., home-robot uses several models to process sensor input and uses the results (e.g., detected obstacles) as input to an RL agent that performs navigation. Sometimes, additional data is added to the second (or third, etc.) model. For example, Video-ChatGPT uses a Transformer model to summarize videos in text, then feeding the results into an LLM along with text prompts as extra input. Joining integration of ML models is the most complex pattern we found. Results of two or more models are combined, either by another model or code logic. We observed the first case in Vlog, which has a complex pipeline of 6 models to transform videos into text descriptions. The second case we observed in eynollah where two different segmentation models are applied to segment an input image into different classes. Our systems often used multiple patterns. For instance, VI- AME offers multiple independent ML functionalities, but has alternative models for some of them. Vlog uses an integration of ML models that combine patterns: processing starts with a feature extractor model, followed by a segmentor (sequential) to divide a video into chunks, which are then processed by two other models, one to generate subtitles and the other to perform segmentation and classification. In parallel, the audio track is translated into text by another ML model. The results of the two captioners and the audio translator are then fed into an LLM (joining) assembling a text description of the video. Origin of the Models. The models are either custom implemented, copied models that are fine-tuned, or pre-trained models. 11 systems use pre-trained models directly out-of-the-box, while 7 fine-tune them, and 11 systems use completely custom-implemented models, where all training material is also stored in the same repository. DL models are usually pre-trained or fine-tuned, whereas classical ML models always custom-trained. A reason may be that the more complex a model becomes, the more training data and effort is required, so pre-trained models relieve developers of end-user-oriented systems of this burden. Still, we found 6 systems that use custom trained DL models, e.g., ControlLoRA extends a pre-trained image generation model in a custom DL model, actually fine-tuning it. Storing & Loading of Models. We observed local and remote storage equally often, 15 times each, with 7 systems supporting both. For example, Vlog uses pre-trained models from different sources, some stored locally in the repository, some downloaded from Hugging Face. Since unsupervised models do not need to be stored at all, 3 systems have no storage. Custom-implemented models are always stored locally, while pre-trained models are usually stored remotely. While storing pre-trained models remotely (usually at a model hub) makes it easier to integrate new versions of the model, we have also found systems that store models distributed through model hubs locally, e.g. stable-diffusion-webui-depthmap-script or VideoTo3dPoseAndBvh. Possi- ble reasons for this practice could be that the version of the model is not changed unexpectedly, but also complicates main- taining distributed models as there are many copies circulating. We noticed that loading ML models can get complicated, often addressed by wrappers encapsulating the assembly of a model to make it reusable. For example, falldetec- tion_openpifpaf implements factories specifically for loading and assembling an ML model from a base and head model. Our analysis also shows that loading alternative models is a configuration problem. In falldetection_openpifpaf load- time configuration is implemented in extensive IF-ELSE state- ments; a total of 19 IF statements are used to configure the base model alone. In addition, for each base model, additional model components were added in later steps. A similar IF pattern is found in h2ogpt. This indicates that ML-enabled sys- tems rely on being highly configurable, often times, intention- ally providing alternative models for specific user expectations. Model In- & Output. The types of input data we found were text (10 systems), images (10 systems), and video (5 systems). Less frequent are audio (sheetsage) or an environment state for RL agents, given in the form of structured numeric data (deep_learning_and_the_game_of_go or sarl_star). The outputs are more diverse, commonly text (8 systems), images (5 systems), labels (4 systems), actions (4 systems), bounding boxes (2 systems), and video (2 systems). While we observed mostly frequently used data formats, special application scenarios sometimes require special formats, e.g., Video To3dPoseAndBvh extracts animations from video and saves in the BVH format. Structured numeric data is"}, {"title": "VII. RELATED WORK", "content": "Various aspects of building ML-enabled systems have been studied before. Jiang et al. [72] study reuse on a dataset of pre-trained models as well as related GitHub repositories, and extracted some summary statistics. In contrast to our contributions their data set is limited to two model hubs. While we compare ML and non-ML code within systems, Gonzales et al. [44] mined applied AI and ML projects to compare them to a reference data set of non-ML projects concerning general statistics, such as programming languages. Simmons et al. [73] analyze issues in open-source machine learning projects, finding that ML-related issues take longer to resolve than non-ML issues. Studies on ML architecture exists, but are not systematic to our knowledge. Architectural patterns, mostly found in gray literature, are suggestions or guidelines for practitioners, i.e, a conceptual AI system architecture of Gorton et al. [23]. Like we, Nahar et al. [46] combine repos- itory mining with a manual analysis of a subset, finding that only a fraction of subject systems using multiple ML models contain interacting models, but staying on a more abstract level than we do. Peng et al. [15] examine the architecture of a single ML-enabled system, an autonomous driving system, and identify patterns of interaction between ML models and code. We complemented their findings with a broader perspective, covering 26 ML-enabled systems. Houerbi et al. [74] studied CI pipelines in open source ML projects and identified significant issues. In fact, in our dataset on 1,091 of 2,928 repositories (37%) use continuous integration (883 use GitHub actions, 192 Travis, and 16 Jenkins), confirming their observations about adoption. Openja et al. [75] investigate testing practices by manually analyzing test files from 11 open-source ML projects, highlighting used test types and tested ML-related functionalities. While data management is beyond the scope of our study, Munappy et al. [76] investigate challenges in data management for deep learning models, identifying 20 challenges and possible solutions. Biswas et al. [47] characterize data-science pipelines, finding that the different pipelines in practice have varying characteristics and are often more complicated than the theoretical models. While we took a static view on ML-enabled systems, others analyze them concerning their evolution, focusing on how ML-related artifacts evolve in relation to source code [77], and the types of changes contributed by forks of ML repositories [13]."}, {"title": "VIII. CONCLUSION", "content": "We presented a large scale study on model integration in ML-enabled software. We quantitatively analyzed 2,928 ML- enabled systems, investigated their characteristics in relation to ML or system type (on a manually classified sample of 160 systems), and a qualitatively analyzed of 26 applications in depth with respect to their model integration architectures."}]}