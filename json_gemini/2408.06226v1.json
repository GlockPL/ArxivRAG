{"title": "A Large-Scale Study of Model Integration in ML-Enabled Software Systems", "authors": ["Yorick Sens", "Henriette Knopp", "Sven Peldszus", "Thorsten Berger"], "abstract": "The rise of machine learning (ML) and its embedding in systems has drastically changed the engineering of software-intensive systems. Traditionally, software engineering focuses on manually created artifacts such as source code and the process of creating them, as well as best practices for integrating them, i.e., software architectures. In contrast, the development of ML artifacts, i.e. ML models, comes from data science and focuses on the ML models and their training data. However, to deliver value to end users, these ML models must be embedded in traditional software, often forming complex topologies. In fact, ML-enabled software can easily incorporate many different ML models. While the challenges and practices of building ML-enabled systems have been studied to some extent, beyond isolated examples, little is known about the characteristics of real-world ML-enabled systems. Properly embedding ML models in systems so that they can be easily maintained or reused is far from trivial. We need to improve our empirical understanding of such systems, which we address by presenting the first large-scale study of real ML-enabled software systems, covering over 2,928 open source systems on GitHub. We classified and analyzed them to determine their characteristics, as well as their practices for reusing ML models and related code, and the architecture of these systems. Our findings provide practitioners and researchers with insight into practices for embedding and integrating ML models, bringing data science and software engineering closer together.", "sections": [{"title": "I. INTRODUCTION", "content": "Many recent breakthroughs in machine learning (ML) have given rise to ML-enabled software that was not realizable before. Consider cyber-physical systems, such as autonomous vehicles [1], [2] or unmanned aerial vehicles [3], but also software in finance [4] or healthcare [5]. All benefit from advances in ML model architectures and training algorithms. Nevertheless, developers still struggle to develop systems that integrate ML models with traditional software components. In fact, industrial surveys report that 78% of ML projects stall [6], while others say as many as 85% fail [7]. Many factors can cause such failures [8], [9] [10], including data quality, waterfall-like development processes, limited experience and education, but also lack of proper tools [11], [8], [12], [13]. In particular, the integration of ML models into software systems pose new challenges, requiring boilerplate code and additional infrastructure to execute the models [12]. As ML models are typically probabilistic models, their behavior can be undefined for certain inputs, or they can produce unwanted results violating domain restrictions, which often requires to manually implement safeguards, especially in safety-critical systems [14]. Consider perception systems in autonomous driving as an extreme example, which can boast up to 28 different ML models that interact with each other [15]. Furthermore, existing tools and processes, e.g., version control systems and dependency management, are often perceived as inadequate by practitioners [16], [17]. In particular, reusing pre-trained ML models has become common in developing ML-enabled systems, as it allows for larger models trained on more data while reducing costs [18], [19], but does not fit with current software engineering practices. However, a detailed overview on reuse practices is missing. Plenty of research [20], [21], [17], [14], [22], [23], [24], [25] has aimed to characterize the current state of practice of the development of ML-enabled software and describe potential problems. Among the most frequently named prob-lems are data quality, integration of ML models with tradi-tional software components, tool support for managing ML-related artifacts, and quality assurance, particularly concerning dependability and safety [26]. Current research in software engineering seeks to address these deficiencies by proposing novel workflow patterns and providing improved tool support supporting ML experiments [27], [21], [20]. Advances in this area so far have been limited, which can be attributed to a lack of understanding of the current practices and concrete prob-lems of actually developing ML-enabled software systems. In other words, while we have focused on ML engineering, we still know very little about how ML models are actually embedded and integrated into ML-enabled systems. We present an empirical study of 2,928 ML-enabled soft-ware repositories on GitHub, characterizing the use of ML models in software systems. We are especially interested in the reuse of ML models across software systems, as well as the integration of ML models. We define three research questions: RQ1: What are characteristics of ML-enabled systems? To understand ML-enabled software systems, we need to know what role ML plays in them and what characterizes them. For instance, what is the proportion of ML code in relation to non-ML source code, what types of software systems employ ML (e.g., libraries or end-user oriented applications), and how are they doing so? Additionally, quality assurance practices play an essential role and may vary [14]. RQ2: How are ML models reused across software systems? As models grow larger and more general, using pre-trained ML models becomes more common [28], [29]. While many forms of software reuse have been analyzed, i.e, by copying code [30], [31], and often impede maintainability, so far with-"}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "ML-enabled systems are software systems containing ML-based functionality, ranging from utility functions to core application logic. The functionality can involve making predictions, preprocessing data or, since recently, creating content via generative AI [33]. ML-enabled systems employ at least one ML model processing data fed into it at runtime. Engineering ML-Enabled Systems. Introducing ML models gives rise to novel engineering challenges for development processes, architectures, and testing, among others. These arise from the fundamentally different nature of ML models compared to traditional software [8]. ML models are prob-abilistic, essentially constituting unreliable functions, while traditional software is more deterministic. This introduces uncertainties and risks throughout the lifecycle of a project. The development process for ML models is data-driven and highly iterative. The most challenging aspects of development are related to management and quality of the data required for training and validation [11], [21]. Concrete problems include, for example, additional dependencies between source code, ML models and external data, feedback loops, quality and management of data, and experiment management [20]. Properly integrating ML models with traditional code is also challenging. Models are typically encapsulated in software modules containing the ML model (binary) made available to execution via an ML framework together with glue and integration code. Integration challenges have been illustrated in the literature as anti-patterns, including excessive use of glue code, complicated data-processing pipelines, dead experimental code paths, and lack of abstraction [12]. Another challenge lies in the different toolchains for ML models and software. Improving the empirical understanding of ML-enabled systems will help in building better and well-integrated tools. There also is a lack of tools for managing (e.g., versioning) the many different artifacts (called ML assets in the remainder) for ML. ML assets are model implementa-tions, code surrounding ML training and experiments, model binaries and other files related to the development or execution of ML models. Traditional software engineering relies on well-established version-control and collaborative development systems, such as Git. However, these systems are not suited for additional assets required for ML, such as datasets, hyperparameters, data transformations, and models [17], as well as for managing ML pipelines and tracking the many different experiments needed to optimize model performance. ML Frameworks and Libraries. The three most popular ML libraries [34] are PyTorch [35], TensorFlow [36], and Scikit-Learn [37]. Many others are built upon them. While Tensor-Flow and PyTorch offer deep learning (DL), Scikit-Learn of-fers ML techniques, but mostly using traditional ML, ranging from supervised methods (e.g., linear regression and percep-tron) to unsupervised methods (e.g., clustering and PCA). TensorFlow and PyTorch are frameworks allowing developers to implement their ML models modularly. In both, models are abstracted by a class \u201cModule,\u201d which represents either complete models or their building blocks (e.g., layers). This provides a common interface for generic operations, such as forward and backward pass. A similar class called \"BaseEstimator\" exists in Scikit-Learn. A wide range of basic building blocks, such as linear layers or convolution layers is also provided in PyTorch and TensorFlow. They also provide loss functions, optimizers, and other utility functions for ML. Model Reuse. Developing ML models is still challenging and requires expertise in data science and machine learning. To integrate ML into software, developers can also reuse estab-lished models, reducing effort and data needs while improving model performance and robustness [29][28]. Two approaches to model reuse exist: (i) in transfer learning [38] the model is trained on a generic dataset and is then fine-tuned to solve a more specific problem; (ii) pretrained models (PTMs) are fully trained and usually reused without any fine-tuning. PTMs can be loaded directly by copying the model or accessing it via an API, typically via so-called model hubs [39]. However, while model reuse helps engineering ML-enabled systems, it is still one of the main challenges [21]. While studies on reuse exist, for instance, on how models could be differentiated to identify plagiarism [40], or on reuse practices of models from specific libraries or platforms [41], there is still a lack of hard empirical data on reuse in real ML-enabled systems. Our work expands these studies. We aim to determine the state of ML model reuse on GitHub, which mainly focuses on common practices among open-source projects."}, {"title": "III. METHODOLOGY", "content": "Figure 1 shows our methodology of mining, classifying, and analyzing repositories quantitatively and qualitatively.\nA. Subject Selection\nWe relied on GitHub's dependency graph to obtain an initial set of repositories that use one of the three most popular [34] ML libraries (IC\u2081) and that are implemented in the currently most popular programming language for ML projects (IC\u2082). Inclusion Criteria. We included repositories when the follow-ing two criteria applied.\nIC\u2081 the repository depends on TensorFlow [36], PyTorch [35], or scikit-learn [37]. This criteria aimed to identify ML-enabled systems. These three popular libraries support developing (especially training) ML models, but are also needed to execute the models in a software system. IC2 The main programming language is Python. This criteria aimed to enhance relevance our analysis not least since Python is the currently most popular programming language for ML-enabled systems\u2014and the comparability of the subject systems by focusing only on one language.\nApplying IC1 yielded 255,096 repositories dependent on Py-Torch, 233,779 on TensorFlow, and 465,980 on Scikit-Learn, and IC2 in 405,091 Python repositories. We then screened this initial dataset and further filtered the repositories to obtain a high quality set of ML-enabled software systems of a certain size and maturity. For instance, we excluded tutorials and small toy projects. Exclusion Criteria. We filtered out repositories when at least one of the following criteria applied. EC1 The repository is not original, i.e., forked or duplicated from another one. Forks would distort the results, as popular projects with many forks would be counted multiple times and disproportionately affect the results, without providing additional insights. EC2 The repository has fewer than 100 stars. Sufficient popularity ensures to avoid practically irrelevant projects. EC3 The codebase contains fewer than 5,000 lines of Python code. This criterion aimed at ensuring a certain size of the code, excluding tutorials and toy projects.\nThe thresholds for (EC2) and (EC3) were chosen based on previous studies [44], [45], [46], [47], [48] and manual explo-ration of a sample. We set them relatively low to exclude sim-ple toy projects, but still obtain a large, representative dataset. We excluded forks (EC\u2081) in two steps: First, we excluded projects that were forked directly from GitHub, which can be accessed through GitHub's REST API. Second, since we found that the dataset still contained a significant number of duplicates we identified them based on identical README files and kept only the repository that was created first. This selection resulted in a final dataset of 2,928 repositories, containing systems with medium to high maturity.\nB. RQ1: System Characteristics\nSystem Classification. Software systems can be of different types, such as applications, libraries, and frameworks [51], [46]. To obtain an initial overview of the dataset and to identify differences in the use of ML, as well as the implications for engineering practices, we first determined what types of projects are included in the dataset and how they relate to ML. The classification of project types is inspired by Idowu et al. [51], who studied characteristics of GitHub repositories with ML models. Our labels describing how ML is used in relation to the systems functionalities is inspired by other studies [44], [52], [13]. In our study we systematically refined existing label definitions for ML resulting in the labels above."}, {"title": "V. REUSE OF ML MODELS AND CODE (RQ2)", "content": "We have identified two different ways ML models are reused. ML models can be distributed through ML model hubs or model registries. These model hubs serve as libraries for ML technology and provide users with pre-trained ML models. We also observed that a common practice is to reuse ML models by cloning their implementations from other repositories. Use of Pre-Trained Models from Model Hubs. We find that 1,209 of the 2,928 systems (41%) load at least one pre-trained model (PTM) from a model hub. Of those systems, which load at least one PTM from a hub, the average number of respective model hub API calls is 29. Figure 9 shows the results. A single API call loads either a models component a full functional model. Model components are assembled into a single usable ML model, which is particularly common for encoder, decoder, and transformer models, which we will see in our manual analysis (RQ3, Sec. VI). However, how many components form a single model varies. In addition, systems may use different PTMs or load a model in multiple places, such as in productive, test, and demonstration code. To better understand why multiple models are loaded in the same system, we analyzed where the API calls are located and what their purpose is, i.e., demo or test code, as determined by file names and directory paths. We find that 8.3% of these files, which contain API calls related to loading ML models from a model hub, are located in test directories, and 9.8% are located in directories named \"demo\" or \"experiment.\" The remaining 81.9% are located in directories that fit neither of these criteria, so most likely contain production code. This indicates that systems loading PTMs either load multiple models as part of the production code or build models out of model components loaded individually. Both is the case, as we will see for RQ3 (Sec. VI). The identified load calls in test directories are distributed over 263 systems (22%), those in demo directories over 254 systems (21%), and those in other directories over 1,129 systems (93%). in each of the three categories we also looked at the combinations. The majority of systems (809) load ML models for purposes that are not related to testing or demonstration purposes. It is safe to assume that these systems load the models only to enable functionalities. This result was to be expected. The total number of systems that do not use testing (955) is not surprising but nevertheless shows that pre-trained models are considered as out-of-the-box solutions which do not require additional quality assurance. It is notable to show that there are some systems that only load models for testing or demonstration purposes (80). This can be explained by the ML Tools we identified in RQ1 (Sec. IV). While MLOps or experimentation management tools to not use ML to realize functionalities they still might include some to test their system or provide examples. Reuse of ML Code. To identify how model implementations are reused across our subject systems we investigated for each project, how much of the source code related to the implementation of ML models is duplicated from other systems in the dataset. We found this type of reuse 1,690 of our subject systems (58%). These contain, on average, 18 files which are at least partially duplicated. For context, the average number of files implementing ML models is 34, so for half of the systems in the dataset, half of the model implementations are at least partially copied. Table II shows the 10 projects from which is copied the most. The results show that a few repositories exist that provide the basis for many copies, and that many of these are managed by the same companies or organizations, most notably Microsoft, OpenMMLab, and Hugging Face. The amount of copied model code was surprising. It indi-cates that copying model code for reuse is a common practice, particularly copying the models from ML companies with ex-tensive resources. However, reusing code by cloning is known to cause problems with the maintainability of systems [69]. In summary, reuse through code duplication is not predom-inant in a single ML technology, but distributed across all areas, as can be seen by the many different ML technologies (transformers, stable diffusion, and others) present in Table II. Summary RQ2: Model Reuse Reuse of ML models is prevalent among the systems. 1,209 systems load pre-trained models and 1,690 systems copy ML implementations directly. The origins of the copied ML implementations are a few repositories that contain ML technology provided by well-known companies."}, {"title": "VI. ML INTEGRATION ARCHITECTURES (RQ3)", "content": "To understand practices of integrating ML models into applica-tions (i.e., where ML supports end-user-oriented functionality), we manually inspected systems we classified (Sec. IV) as Applications and as using ML as part of their business logic (Business Focused Applied), amounting to 26 of the 160 clas-sified systems. We now report results for each of the questions defined in Sec. III-D. All projects are listed with their details in our online appendix [32]. In the remainder, we also refer to individual projects (typeset in sans-serif) for illustration. ML Functionalities and Tasks. The functionality provided to end users, implemented using ML models, varies widely. Common functionalities include chatbots (5 systems), video processing (5 systems), image processing (6 systems), and data analysis (4 systems), and robotic navigation (3 systems). The three other systems use ML to transcribe music, infer metada from scanned documents, and play games. On a technical level, the underlying ML tasks performed are generative AI (11 systems), classification (6 systems), decision making (4 systems), segmentation (4 systems), dimensionality reduction (3 systems), and clustering (1 system), with 2 sys-tems not fitting into established task categories. For example, tournesol uses linear regression to visualize correlations. Four systems combine tasks to provide functionality; for instance, Video-ChatGPT is a chatbot with video processing capabilities that allows user to query the content of videos. Number and Type of ML Models. With 23 out of 26 systems, the majority employs multiple ML models to implement the functionality (see Fig. 10). The highest number 18 in one system (h2ogpt) and systems with 7 or more models are frequent, but most systems utilize below 5 ML models. Three systems allow to select arbitrary models from model hubs, making the number of models practically infinite. For example, gentopia allows the user to configure a chatbot by selecting a model from Hugging Face. The most common model type is Deep Neural Network (DNN, i.e., DL), found in 21 systems. Classical ML (ML that is not DL) was found 5 times. Within the DNN models, we observed multiple variations, including 8 Transformers, 6 Convolutional Neural Networks (CNN), 3 Fully Connected Neural Networks (FCNN), and one Long Short Term Memory (LSTM) [70]. CNNs are used exclusively for image processing, whereas Transformers and LSTMs are mainly intended for natural language processing, although we find 3 Transformers that are used for image generation (e.g., ControlLoRA). ML can be further divided into Supervised Learning (SL), Unsupervised Learning (UL), and Reinforcement Learning (RL) [71]. UL was found only in the form of classical ML, such as Principal Component Analysis (PCA) (3 systems) and Clustering (1 system). Two systems use supervised classical ML. The DNNs are mostly used for SL, but 3 RL systems were also found. While classical ML is mostly used for auxiliary functions, e.g., CorpusTools uses PCA to support data visu-alization, the use of CNNs covers all observed functionalities. Interaction Between ML Models. In the 23 systems with multiple models we observed that interactions in various ways. Inspired by Peng et al. [15], we categorized and analyzed the interactions as follows: Alternative models is the most common pattern (15 systems), where multiple models are offered for the same task, either automatically by the pro-gram or based on user input. For example, home-robot uses different RL agents for different tasks (e.g., navigating or manipulating objects). Independent models used for unrelated functionalities are found in 7 systems. Larger systems in particular provide the user with multiple functions, some of which require ML. For example, VIAME offers image processing tools (e.g., for segmentation or classification) to be used independently by the user. Sequential integration of ML models was observed in 7 systems. They are used for complex tasks where no single model provides the desired functionality. The output of one model is the input of another. A common case are perception systems where objects are detected in a first step and classified in a second step; e.g., home-robot uses several models to process sensor input and uses the results (e.g., detected obstacles) as input to an RL agent that performs navigation. Sometimes, additional data is added to the second (or third, etc.) model. For example, Video-ChatGPT uses a Transformer model to summarize videos in text, then feeding the results into an LLM along with text prompts as extra input. Joining integration of ML models is the most complex pattern we found. Results of two or more models are combined, either by another model or code logic. We observed the first case in Vlog, which has a complex pipeline of 6 models to transform videos into text descriptions. The second case we observed in eynollah where two different segmentation models are applied to segment an input image into different classes. Our systems often used multiple patterns. For instance, VI-AME offers multiple independent ML functionalities, but has alternative models for some of them. Vlog uses an integration of ML models that combine patterns: processing starts with a feature extractor model, followed by a segmentor (sequential) to divide a video into chunks, which are then processed by two other models, one to generate subtitles and the other to perform segmentation and classification. In parallel, the audio track is translated into text by another ML model. The results of the two captioners and the audio translator are then fed into an LLM (joining) assembling a text description of the video. Origin of the Models. The models are either custom implemented, copied models that are fine-tuned, or pre-trained models. 11 systems use pre-trained models directly out-of-the-box, while 7 fine-tune them, and 11 systems use completely custom-implemented models, where all training material is also stored in the same repository. DL models are usually pre-trained or fine-tuned, whereas classical ML models always custom-trained. A reason may be that the more complex a model becomes, the more training data and effort is required, so pre-trained models relieve developers of end-user-oriented systems of this burden. Still, we found 6 systems that use custom trained DL models, e.g., ControlLoRA extends a pre-trained image generation model in a custom DL model, actually fine-tuning it. Storing & Loading of Models. We observed local and remote storage equally often, 15 times each, with 7 systems supporting both. For example, Vlog uses pre-trained models from different sources, some stored locally in the repository, some downloaded from Hugging Face. Since unsupervised models do not need to be stored at all, 3 systems have no storage. Custom-implemented models are always stored locally, while pre-trained models are usually stored remotely. While storing pre-trained models remotely (usually at a model hub) makes it easier to integrate new versions of the model, we have also found systems that store models distributed through model hubs locally, e.g. stable-diffusion-webui-depthmap-script or VideoTo3dPoseAndBvh. Possi-ble reasons for this practice could be that the version of the model is not changed unexpectedly, but also complicates main-taining distributed models as there are many copies circulating. We noticed that loading ML models can get complicated, often addressed by wrappers encapsulating the assembly of a model to make it reusable. For example, falldetec-tion_openpifpaf implements factories specifically for loading and assembling an ML model from a base and head model. Our analysis also shows that loading alternative models is a configuration problem. In falldetection_openpifpaf load-time configuration is implemented in extensive IF-ELSE state-ments; a total of 19 IF statements are used to configure the base model alone. In addition, for each base model, additional model components were added in later steps. A similar IF pattern is found in h2ogpt. This indicates that ML-enabled sys-tems rely on being highly configurable, often times, intention-ally providing alternative models for specific user expectations. Model In- & Output. The types of input data we found were text (10 systems), images (10 systems), and video (5 systems). Less frequent are audio (sheetsage) or an environment state for RL agents, given in the form of structured numeric data (deep_learning_and_the_game_of_go or sarl_star). The outputs are more diverse, commonly text (8 systems), images (5 systems), labels (4 systems), actions (4 systems), bounding boxes (2 systems), and video (2 systems). While we observed mostly frequently used data formats, special application scenarios sometimes require special formats, e.g., Video To3dPoseAndBvh extracts animations from video and saves in the BVH format. Structured numeric data is outputted by the model of Sunnypilot containing inferred information about the environment, like the lane lines or the distance to the lead vehicle. The most common combination of input and output is text to text (in 4 systems, all chatbots). Other common combinations are text to image or image to text. clip-glass is an example of both, providing image captioning and image generation from text. Pre- & Postprocessing. Almost all systems contain some pre- and post processing steps, ranging from generic operations (e.g., text tokenization) to more complex content manipulations, which are highly dependent on the system. We clearly identified preprocessing code in 15 systems; 4 systems contained no preprocessing; while the remaining 7 were in-conclusive. In particular, more complex operations were often scattered throughout the code and merged into application logic. Common preprocessing steps include normalization and feature extraction. In the case of text processing, such as chatbots, the input data is usually vectorized. Postprocessing was found in 8 systems, 9 contain none, 9 were inconclusive. Again, the majority are simple data conver-sions, such as projecting tokens back to text. More complex examples include checking the validity of actions chosen by an RL agent (deep_learning_and_the_game_of_go). Particularly interesting is tiatoolbox, which provides users deep configuration options, including pre- and post-processing methods. It provides default functions, but can also accept user-written functions added to the model as callbacks. For DreamArtist-stable-diffusion, we find that it implements some minor security functionalities that ensure input integrity. Summary RQ3: Model Integration Most systems use supervised DL with pre-trained model, less often deep reinforcement learning and classical un-supervised learning. Most process text or images, but other types are also found. Using multiple models is very common. Although most models are either used as alterna-tives or independent, some systems apply multiple models sequentially. Pre-and postprocessing steps are difficult to comprehend, often scattered throughout the systems. There are no common practices for integrating ML models."}, {"title": "VII. RELATED WORK", "content": "Various aspects of building ML-enabled systems have been studied before. Jiang et al. [72] study reuse on a dataset of pre-trained models as well as related GitHub repositories, and extracted some summary statistics. In contrast to our contributions their data set is limited to two model hubs. While we compare ML and non-ML code within systems, Gonzales et al. [44] mined applied AI and ML projects to compare them to a reference data set of non-ML projects concerning general statistics, such as programming languages. Simmons et al. [73] analyze issues in open-source machine learning projects, finding that ML-related issues take longer to resolve than non-ML issues. Studies on ML architecture exists, but are not systematic to our knowledge. Architectural patterns, mostly found in gray literature, are suggestions or guidelines for practitioners, i.e, a conceptual AI system architecture of Gorton et al. [23]. Like we, Nahar et al. [46] combine repos-itory mining with a manual analysis of a subset, finding that only a fraction of subject systems using multiple ML models contain interacting models, but staying on a more abstract level than we do. Peng et al. [15] examine the architecture of a single ML-enabled system, an autonomous driving system, and identify patterns of interaction between ML models and code. We complemented their findings with a broader perspective, covering 26 ML-enabled systems. Houerbi et al. [74] studied CI pipelines in open source ML projects and identified significant issues. In fact, in our dataset on 1,091 of 2,928 repositories (37%) use continuous integration (883 use GitHub actions, 192 Travis, and 16 Jenkins), confirming their observations about adoption. Openja et al. [75] investigate testing practices by manually analyzing test files from 11 open-source ML projects, highlighting used test types and tested ML-related functionalities. While data management is beyond the scope of our study, Munappy et al. [76] investigate challenges in data management for deep learning models, identifying 20 challenges and possible solutions. Biswas et al. [47] characterize data-science pipelines, finding that the different pipelines in practice have varying characteristics and are often more complicated than the theoretical models. While we took a static view on ML-enabled systems, others analyze them concerning their evolution, focusing on how ML-related artifacts evolve in relation to source code [77], and the types of changes contributed by forks of ML repositories [13]."}, {"title": "VIII. CONCLUSION", "content": "We presented a large scale study on model integration in ML-enabled software. We quantitatively analyzed 2,928 ML-enabled systems, investigated their characteristics in relation to ML or system type (on a manually classified sample of 160 systems), and a qualitatively analyzed of 26 applications in depth with respect to their model integration architectures. Among the systems, ML is currently used mostly to develop new technical applications, such as classifiers that rather address conceptual problems (e.g., detect moving people in video streams), but is not yet widely applied in actual applications targeting end users. While more than half of our subject systems were libraries that provide ML functionalities, only 18% of systems are applications that use ML to provide end-user-oriented functionality (RQ1, Sec. IV). Nevertheless, ML is already part of many repositories, and related artifacts, such as ML models, need to be created and maintained. Interestingly, ML model implementations are often copied between repositories, while also many pre-trained models from model hubs are used (RQ2, Sec. V). While such pre-trained models represent systematic reuse, the presence of code duplication among our systems still indicates poor dependency management. We even found many entirely copied repositories to integrate ML models into new systems (RQ3, Sec. VI). In such cases, improvements to the original model code might not reach the copy. We found employing multiple ML models either for providing one or multiple functionalities widely adopted. However, we could not find established architectural patterns in the systems, but rather ad hoc solutions being implemented. In the end, the architecture and how ML models are integrated often depends on the domain of the system. However, despite the lack of well-researched integration patterns [15], we observed different kinds of interations of ML models. Most commonly, multiple ML models are used as alternatives to each other, but there are also cases where they are used sequentially, in parallel, or outputs are joined (RQ3, Sec. VI)."}]}