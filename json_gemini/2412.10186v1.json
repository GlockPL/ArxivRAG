{"title": "BICERT: A BILINEAR MIXED INTEGER PROGRAMMING FORMULATION FOR PRECISE CERTIFIED BOUNDS AGAINST DATA POISONING ATTACKS", "authors": ["Tobias Lorenz", "Marta Kwiatkowska", "Mario Fritz"], "abstract": "Data poisoning attacks pose one of the biggest threats to modern AI systems, necessitating robust defenses. While extensive efforts have been made to develop empirical defenses, attackers continue to evolve, creating sophisticated methods to circumvent these measures. To address this, we must move beyond empirical defenses and establish provable certification methods that guarantee robustness. This paper introduces a novel certification approach, BiCert, using Bilinear Mixed Integer Programming (BMIP) to compute sound deterministic bounds that provide such provable robustness. Using BMIP, we compute the reachable set of parameters that could result from training with potentially manipulated data. A key element to make this computation feasible is to relax the reachable parameter set to a convex set between training iterations. At test time, this parameter set allows us to predict all possible outcomes, guaranteeing robustness. BiCert is more precise than previous methods, which rely solely on interval and polyhedral bounds. Crucially, our approach overcomes the fundamental limitation of prior approaches where parameter bounds could only grow, often uncontrollably. We show that BiCert's tighter bounds eliminate a key source of divergence issues, resulting in more stable training and higher certified accuracy.", "sections": [{"title": "Introduction", "content": "Data poisoning attacks are one of the most significant threats to the integrity of machine learning models today. Attackers can potentially exert far-reaching influence by poisoning ubiquitous foundation models that are widely used, or targeting systems in critical applications, such as finance, healthcare, or autonomous decision-making. These attacks aim to inject malicious data into the training process, leading the model to make incorrect or harmful predictions during deployment. The recent trend of training foundation models on massive datasets scraped from all available sources has exacerbated this threat, as data curation on this scale is nearly impossible [Wan et al., 2023, Carlini et al., 2024]. Government agencies across the US and Europe have recognized poisoning as one of the fundamental threats to AI systems and highlight the need for robust defenses in their respective reports [ENISA, 2020, Vassilev et al., 2024] and legislation [European Union, 2024].\nIdentifying this threat, many empirical defenses have been developed to mitigate the risks [Cin\u00e0 et al., 2023]. These approaches typically use a combination of data filtering, robust training, and model inspection to detect or prevent the influence of poisoning attacks. However, their empirical nature means they can detect the presence of specific attacks but cannot guarantee their absence. As a result, more sophisticated attacks have been developed to circumvent these defenses [Suciu et al., 2018]. A recent line of work explores rigorous provable guarantees that bound the influence that poisoning attacks can have on a model's predictions [Lorenz et al., 2024, Sosnin et al., 2024]. These methods are based on test-time certifiers [Gowal et al., 2018, Zhang et al., 2018] and use interval and polyhedral constraints to compute upper and lower real-valued bounds for each parameter during training. These parameter intervals bound the influence that data perturbations can have on the trained model. These works are crucial, as they are the first to provide sound deterministic bounds on the model parameters and, consequently, an adversary's influence on the training."}, {"title": "Related Work", "content": "There are two major lines of work on certified defenses against training-time attacks: ensemble-based methods and bound-based methods.\nEnsemble-Based Methods. Ensemble-based methods are typically based on either bagging or randomized smoothing. Wang et al. [2020], Rosenfeld et al. [2020], and Weber et al. [2023] extend Randomized Smoothing [Cohen et al., 2019] to training-time attacks. While Wang et al. [2020] and Weber et al. [2023] compute probabilistic guarantees against lo and 12-norm adversaries respectively, Cohen et al. [2019] provide these guarantees against label flipping attacks.\nA similar line of work provides probabilistic guarantees against training-time attacks using bagging. Jia et al. [2021] find that bagging's data sub-sampling shows intrinsic robustness to poisoning. Wang et al. [2022] enhance the robustness guarantees with advanced sampling strategies, while Levine and Feizi [2021] introduce a deterministic bagging method. Zhang et al. [2022] adapt this approach for backdoor attacks with triggers. Recent studies also explore different threat models, including temporal aspects Wang and Feizi [2023] and dynamic attacks Bose et al. [2024].\nBound-Based Methods. In contrast to these ensemble-based sampling methods, Lorenz et al. [2024] and Sosnin et al. [2024] propose to compute sound deterministic bounds of the model's parameters during training. Both methods share the same underlying principle. They define a polytope of allowed perturbations in input space and propagate it through the forward and backward passes during training. By over-approximating the reachable set with polytopes along the way, they compute sound worst-case bounds for the model's gradients. Using these bounds, the model parameters can be updated with sound upper and lower bounds, guaranteeing that all possible parameters resulting from the data perturbations lie within these bounds. Lorenz et al. [2024] use intervals to represent these polytopes and extend the approach to also include test-time perturbations. Sosnin et al. [2024] use a combination of interval and linear bounds and additionally limit the number of data points that can be perturbed. Both approaches' biggest limitations are the significant over-approximations caused by using polytopes to represent the reachable sets, which can lead to loose bounds and divergence issues."}, {"title": "Certified Training using Bilinear Mixed Integer Programming", "content": "The goal of our method is to bound the error that bounded perturbations to the training data can introduce on the final machine learning model. However, precisely computing these bounds is infeasible even for small model sizes, as it has been shown that this is a 5-hard problem [Marro and Lombardi, 2023]. The challenge therefore lies in finding a feasible over-approximation, which (i) ensures sound bounds, (ii) makes as few over-approximations as possible, and (iii) remains computationally feasible.\nPrevious works [Lorenz et al., 2024, Sosnin et al., 2024] compute sound bounds using Interval Bound Propagation (IBP). However, their scalability is limited due to significant over-approximations of interval bounds. While over-approximations are undesirable in any certification task, they are especially costly during training. Lorenz et al. [2024] show that over-approximations can cause the training to diverge, making it prudent to minimize them.\nThere are two leading causes of divergence when training with interval bounds: (i) IBP does not preserve the relationship between input and output variables and therefore over-approximates even simple operations such as addition and subtraction. (ii) The size of bounds for parameters can only grow due to the interval subtraction in the parameter update step (Section 4). This motivates the main design decisions of our method using Bilinear Mixed Integer Programming."}, {"title": "Illustrative Example", "content": "Before we formally define BiCert, we illustrate it on a simplified example (Fig. 1). This illustration employs a model with only two inputs, 21 and 22. The first operation is a fully connected layer (without bias) with weights W11, W12, W21, and W22. This linear layer is followed by a ReLU non-linearity and a second fully connected layer with weights W5 and w6. We use (x1 = 1,x2 = 1) as an example input with target label t = 1. We set the perturbations \u20ac = 1, which leads to upper and lower bounds of [0, 2] for both inputs. In the following, we highlight all equations that belong to the optimization problem in teal. The full optimization problem with all constraints is in Appendix A.\nForward Pass. The first step is to encode the forward pass through the model as constraints of the optimization problem. We start with the input by defining 11 and 12 as variables of the optimization problem. By the definition of the threat model, their constraints are 0 < x1 < 2 and 0 < x2 \u2264 2.\nFor the fully connected layer, we encode the variables 13 and 24 with respect to x1 and 12. We need the parameters Wij as variables to do so, which are set to their initial values W11 = 1, W12 = 1, W21 = 1, and W22 = -1. One might be tempted to substitute these variables with their values to simplify the optimization problem. However, this would cause information loss, which would decrease the precision of the final solution. This effect will amplify in later rounds where Wij are no longer single values but intervals with upper and lower bounds. We add the constraints x3 = W11X1 + W21X2 and x4 = W12X1 + W22x2 to the optimization problem. Since we multiply two variables with each other, the problem becomes bilinear, which is one of the reasons we require a bilinear optimization problem.\nThe ReLU non-linearity can be directly encoded as a piecewise linear constraint: x5 = max(0, 23), or, equivalently, X5 = X3 if X3 > 0, and 15 = 0 otherwise. 26 is defined accordingly. ReLUs are the main reason why we require Mixed Integer Programming, as they allow us to encode piecewise linear constraints using binary decision variables. x7 is a linear combination of the two outputs, adding the constraint x7 = W5X5 + W6x6 with W5 = -1 and w6 = 1. At this stage, we can already see that BiCert's bounds are more precise. Computing the bounds for x7 using FullCert's IBP"}, {"title": "Loss", "content": "After the forward pass through the model, we encode the loss function as constraints to the optimization problem. We use the hinge loss L(x7, t) = max(0,1 - tx7) = max(0,1-27) in this example because it is a piecewise linear function and therefore can be exactly encoded, analogous to ReLUs. General losses can be supported by bounding the function with piecewise linear bounds."}, {"title": "Backward Pass", "content": "For the backward pass, we need to compute the loss gradient for each parameter using the chain rule. It starts with the last layer, which is $\\frac{\\partial L}{\\partial x_7}$ = -1 if x7 \u2264 1,0 otherwise. This is also a piecewise linear function and can be encoded as a constraint to the optimization problem.\nThe gradients for the linear layer x7 = W5X5 + W626 can be determined using the chain rule: $\\frac{\\partial L}{\\partial w_5} = \\frac{\\partial L}{\\partial x_7} x_5$ and $\\frac{\\partial L}{\\partial x_5} = w_5 \\frac{\\partial L}{\\partial x_7}$, with corresponding expressions for 26 and w6. Given that the outer gradient $\\frac{\\partial L}{\\partial x_7}$, as well as 25 and W5, are variables, this backward propagation leads to bilinear constraints. The derivatives of ReLU are piecewise linear, resulting in $\\frac{\\partial L}{\\partial x_3} = 0$ if x3 \u2264 0 and $\\frac{\\partial L}{\\partial x_3} = \\frac{\\partial L}{\\partial x_5}$ otherwise. The derivatives for the parameters w11, w12, w21, and w22 function analogously to w5 and w6."}, {"title": "Parameter Update", "content": "The last step is the parameter update. We also encode the new parameters as a constraint: w. Theoretically, we could directly continue with the next forward pass, using the new parameters w, resulting in an optimization problem that precisely encodes the entire training. However, this is computationally infeasible in practice. We therefore relax the constraints after each parameter update by solving the optimization problem for each parameter: w = min w, and w\u2081 = max wi, subject to the constraints that encode the forward and backward passes from above. w and w are real-valued constraints that guarantee w < w < w. This leads to valid bounds for all parameters in consecutive iterations, as mentioned above."}, {"title": "Formal Definition", "content": "Before we introduce the details of our method, we must first define the training-time certification problem and establish the soundness of the general approach. To certify the robustness of the model's training, we have to define a precondition for which the certificate should hold. For training-time certification, this is the formalization of the adversary's capabilities to perturb the training data. We define all valid dataset permutations as a family of datasets, similar to prior work (Eq. (2) in Lorenz et al. [2024] and Section 3.1 in Sosnin et al. [2024]):\nDefinition 1 (Family of Datasets) A family of datasets $D_p$ is the set of all datasets that can be produced by perturbing data points x from the original dataset D within an lp ball:\n$D_p := \\{ D' | \\forall (x', y') \\in D' \\exists (x,y) \\in D \\land ||x' - x||_p \\leq \\epsilon \\land y' = y \\}.$\n(1)\nFor continuous domains of x (e.g., Rd), the cardinality of $D_p$ is generally infinite.\nThe final certificate should guarantee that at test time, the model's predictions are independent of the training data perturbation. Given a gradient-based training algorithm A (e.g., SGD), dataset D, and model f with parameters 0, we can formalize training as 0 = A(D, 00), where 00 is the parameter initialization. This allows us to define the certificate as follows:\nDefinition 2 (Certificate) For test input x and its label y, the certificate guarantees that\n$f_{\\theta'}(x) = y \\forall D' \\in D_p \\land \\theta' = A(D',0_o)$.\n(2)\nThe equation holds if we can guarantee that the model predicts the same label for all perturbed datasets in D.\nSolving this problem is, while theoretically possible, typically infeasible in practice. The computational graph of training a model is deep, especially for larger models and datasets. We therefore divide the problem into two steps: (1) find sound bounds on the model's parameters after training, and (2) based on these parameter bounds, check whether the prediction is correct for all parameters within those bounds. This division has an additional bonus: because the training, which is the most costly part of the computation, is independent of the test sample x, we only have to compute the parameter bounds once and can then re-use them for multiple test samples."}, {"title": "BiCert: Bounds via Bilinear Mixed Integer Programming", "content": "The key innovation of our method BiCert is the approach to solving Eq. (5). Using Bilinear Mixed Integer Programming, we can compute an exact solution for each iteration, avoiding over-approximations. For each training iteration, we build an optimization problem over each model parameter, with the new, updated value as the optimization target. D, the current model parameters, the transformation functions of each layer, the loss, the gradients of the backward pass, and the parameter update are encoded as constraints. This leads to 2 * m optimization problems for a model with m parameters of the form\nmin/max 0+1, j = 1,..., m\nsubject to Input Constraints\nParameter Constraints\nLayer Constraints\nLoss Constraints\nGradient Constraints\nParameter Update Constraints.\n(6)\nThe objectives are the parameters, which we maximize and minimize independently to compute their upper and lower bounds. The constraints are the same for all parameters and only have to be constructed once. We present these constraints for fully-connected models with ReLU activation below.\nInput Constraints. The first set of constraints encodes the allowed perturbations, in this case, the l\u221e norm with radius e, where are the auxiliary variables encoding the n input features:\n$\\underline{o}^{(0)}_k < x_k+\\epsilon$, $k = 1,..., n$\n$\\underline{o}^{(0)}_k \\geq x_k - \\epsilon$, $k = 1,...,n$.\n(7)\nParameter Constraints. Parameters have bounds starting from the second iteration, as discussed above. We encode them as:\n$\\underline{\\theta}_j\\leq \\theta, j = 1,...,m$\n$\\overline{\\theta}_j\\geq \\theta, j = 1,...,m$.\n(8)"}, {"title": "Linear Layers", "content": "Linear layer constraints are straightforward, as they are linear combinations of the layer's inputs , and the layer's weights wa \u2208 \u03b8\u2081\u22121 and biases bl) \u0395\u03b8\u22121:\n$\\underline{o}^{(l)}_u = \\sum_{v \\in \\underline{o}^{(l-1)}} w^{(l)}_{uv}\\underline{o}^{(l-1)} + b^{(l)}, u = 1,..., |\\underline{o}^{(l)}|$.\n(9)\nThis results in bilinear constraints, as the layer's parameters are multiplied by the inputs, both of which are variables of the optimization problem."}, {"title": "ReLU Layers", "content": "ReLUs are encoded as piecewise-linear constraints, e.g., via Big-M or SOS:\n$\\underline{o}^{(l)} = \\{\\begin{array}{ll}0 & \\text { if } \\underline{o}^{(l-1)} < 0\\\\ \\underline{o}^{(l-1)}, & \\text { otherwise }\\end{array} u = 1,..., |\\underline{o}^{(0)}|$.\n(10)"}, {"title": "Loss Function", "content": "We use the margin loss because it is piecewise linear, and we can therefore encode it exactly. For other loss functions, we can use (piecewise) linear relaxations. With the last-layer output o(L), the ground-truth label y, and auxiliary variable J, we define the constraint as Eq. (11):\nJ = max (0,1 \u2013 yo(L))\n(11)\n$\\frac{\\partial J}{\\partial o^{(L)}}=\\begin{cases} -y & \\text{if $yo^{(L)} < 1$}\\\\0 & \\text{otherwise}\\end{cases}$\n(12)"}, {"title": "Loss Gradients", "content": "The derivative of the margin loss is also piecewise linear (Eq. (12))."}, {"title": "ReLU Gradients", "content": "The local gradient of the ReLU function is also piecewise linear (Eq. (13)). Multiplication with the upstream gradient results in a piecewise bilinear constraint (Eq. (14)).\n$\\frac{\\partial \\underline{o}^{(l)}}{\\partial \\underline{o}^{(l-1)}}=\\begin{cases}0 & \\text{if $x^{(l-1)} < 0$}\\\\ \\frac{\\partial J}{\\partial o^{(l)}} & \\text{otherwise}\\end{cases}$\n(13)\n$\\frac{\\partial J}{\\partial x^{(l-1)}}=\\frac{\\partial \\underline{o}^{(l)}}{\\partial x^{(l)}}*\\frac{\\partial J}{\\partial \\underline{o}^{(l)}}$\n(14)"}, {"title": "Linear Gradients", "content": "All partial derivatives for linear layers are bilinear:\n$\\frac{\\partial J}{\\partial \\underline{o}^{(l-1)}}=\\sum_v \\frac{\\partial J}{\\partial \\underline{o}^{(l)}}\\frac{w^{(l)}_{uv}}{\\partial \\underline{o}^{(l-1)}}$     (15a)\n$\\frac{\\partial J}{\\partial w^{(l)}}=\\frac{\\partial J}{\\partial \\underline{o}^{(l)}}\\underline{o}^{(l-1)}$          (15b)\n$\\frac{\\partial J}{\\partial b^{(l)}}=\\frac{\\partial J}{\\partial \\underline{o}^{(l)}}$                     (15c)"}, {"title": "Parameter Updates", "content": "The last set of constraints is the parameter updates. It is essential to include this step before relaxation because the old parameters are contained in both subtraction operands. Solving this precisely is a key advantage compared to prior work (Section 4).\n$\\theta^{i+1}_j = \\theta^i_j -\\lambda \\frac{\\partial J}{\\partial \\theta^i_j}$, $j = 1,..., m$.\n(16)"}, {"title": "Algorithmic Implementation", "content": "We implement this optimization procedure according to algorithm 1, effectively unrolling the training procedure. For each iteration of the training algorithm (line 2), we initialize an optimization problem (line 3) and add the current parameter bounds as constraints (line 4). For each data point x, we add the input constraints (line 6), layer constraints (line 8), loss constraints (line 9), derivative constraints (lines 10-12), and parameter update constraints (line 13). We then solve the optimization problem twice for each parameter, once for the upper and once for the lower bound (lines 14-15). The algorithm returns the final parameter bounds.\nOnce the model is trained, we can use the final parameter bounds for prediction (algorithm 2). The principle is the same as encoding only the forward pass from training (lines 1-5). For classification, we can then compare the logit and check whether one is always greater than all others (lines 6-12). If so, we return the corresponding class (line 12). Otherwise, we cannot guarantee a prediction, and the algorithm has to abstain (line 13). The post-condition (lines 6-13) can be adjusted, e.g., for regression tasks."}, {"title": "Theoretical Comparison to Prior Work", "content": "As discussed before, the main limitation of prior work is the significant over-approximations introduced by relaxing bounds to interval/polyhedral constraints after each layer. While these techniques have shown an acceptable trade-off"}, {"title": "Experiments", "content": "We perform several experiments to evaluate our method and to validate our theoretical findings. To this end, we compare BiCert to prior work and analyze its runtime behavior."}, {"title": "Implementation Details", "content": "We use Gurobi [Gurobi Optimization, LLC, 2024] as a solver for this work. It offers several benefits, including support for piecewise linear and bilinear constraints and a Python interface for integrating deep learning frameworks. The library also supports reusing the same optimization model for multiple optimizations, including warm starts that allow for more efficient solutions. To represent the parameter bounds, we use the BoundFlow library by Lorenz et al. [2024]. Our implementation also supports mini-batch processing, similarly to typical deep learning frameworks. Additional implementation and training details, including model architectures, hyperparameters, and hardware configuration, are listed in Appendix B."}, {"title": "Certified Accuracy Across Perturbation sizes", "content": "To validate the theoretical analysis of our method, we evaluate BiCert experimentally and compare it to FullCert [Lorenz et al., 2024] and Sosnin et al. [2024]. We evaluate it on the Two-Moons dataset for classification, a widely used dataset with two classes of points configured in interleaving half circles."}, {"title": "Runtime and Complexity Analysis", "content": "Of course, there is no free lunch when it comes to optimization. In our case, the higher precision and, therefore, certified accuracy, comes at the cost of an increased computational complexity. We investigate this impact by measuring the time it takes to train our models. Table 2 shows the average runtime for each perturbation size for the first 10 epochs. The general trend is an increased complexity for later epochs and an increased complexity for larger perturbation radii."}, {"title": "Discussion", "content": "BiCert addresses the fundamental limitations of prior certified training approaches, specifically the over-reliance on convex over-approximations that lead to unstable training and diverging parameter bounds. By introducing Bilinear Mixed Integer Programming (BMIP) for certified training, we provide tighter, more precise bounds at each training iteration. Our method ensures that the parameter bounds can shrink, effectively mitigating the divergence issues faced by previous methods.\nThe primary benefit of this approach lies in its improved precision, which leads to significantly higher certified accuracy, especially for larger perturbations. As demonstrated in our experiments, BiCert consistently outperforms existing methods in terms of the certified accuracy of the model. Additionally, the reduced variance in our results highlights the robustness and reliability of our approach.\nThis increased precision comes at the cost of a higher computational complexity. Solving piecewise bilinear optimization problems for each parameter update is more computationally expensive than the interval-based methods used in prior work. This trade-off between computational cost and certification precision is crucial when deploying certified defenses in practical settings. While the added cost may be prohibitive for large-scale models, our method provides a valuable framework for improving the precision and reliability of certification in smaller models or scenarios where robustness is critical.\nLooking forward, there are opportunities to explore hybrid approaches that combine the precision of BMIP with the computational efficiency of interval-based methods, potentially achieving a more scalable solution. Intelligently"}, {"title": "Conclusion", "content": "This work addresses fundamental limitations in state-of-the-art certifiers that compute bounds for training-time attacks on neural networks. In particular, their over-reliance on convex over-approximations limits their applicability to larger perturbation sizes, makes training unstable, and can cause models to diverge even for ideal, convex settings. Using Bilinear Mixed Integer Programming (BMIP), we can compute exact bounds for each training iteration. This approach significantly improves training stability and certified accuracy of the resulting models, outperforming the state-of-the-art. We believe that this work lays the foundation for effective certified defenses against training-time attacks."}, {"title": "Full Example", "content": "In Section 3.1 we present an example model to illustrate our method. We list the full optimization problem for the first training iteration here.\nmin/max wi, i\u2208 {11,12,21, 22, 5, 6}\nsubject to 0 \u2264 x1 < 2\n0 \u2264 x2 \u2264 2\n1 < W11 \u2264 1\n1 < W12 \u2264 1\n1 < W21 \u2264 1\n-1 < W22 < -1\n-1 < W5 \u2264 -1\n1 \u2264 w6 \u2264 1\nX3 = W11X1 + W21X2\nX4 = W12X1 + W22X2\nX5\nX6\nX7 = W5X5+ W6X6\n$\n{\\begin{cases}x3 & \\text { if } x_3 > 0\\\\0 & \\text { if } x_3 = 0\\end{cases}$\n{\\begin{cases}x4 & \\text { if } x_4 > 0\\\\0 & \\text { if } x_4 = 0\\end{cases}$\nL =\n$\\frac{\\partial L}{\\partial o^{(L)}}=\\begin{cases} -y & \\text{if $yo^{(L)} < 1$}\\\\0 & \\text{otherwise}\\end{cases}$\n{\\begin{cases}1-X7 & \\text { if } 1-X7 > 0\\\\0 & \\text { otherwise\\end{cases}$\n{\\begin{cases}-1 & \\text { if } 1 - x_7 > 0\\\\0 & \\text { otherwise\\end{cases}$wi = Wi - \u03bb$\\frac{\\partial J}{\\partial w_i}$"}, {"title": "Training and Implementation Details", "content": "We train our models using a combination of the BoundFlow library [Lorenz et al., 2024] and the Gurobi optimizer [Gurobi Optimization, LLC, 2024]. Since PyTorch [Paszke et al., 2019] does not support bound-based training, we only use its basic tensor representations.\nAll computations are performed on a compute cluster, which mainly consists of AMD Rome 7742 CPUs with 128 cores and 2.25 GHz. Each task is allocated up to 32 cores. No GPUs are used since Gurobi does not use them for solving.\nUnless indicated otherwise, we use fully connected networks with ReLU activations, two layers, and 20 neurons per layer. For binary classification problems, we use margin loss, i.e., J = max(0, 1 \u2013 y \u00b7 f(x)), because it is piecewise linear and can therefore be encoded exactly. It produces similar results to Binary Cross-Entropy loss for regular training without perturbations.\nWe train models until convergence using a held-out validation set, typically after 5 to 10 epochs on Two-Moons. We use a default batch size of 100 and a constant learning rate of 0.1. We sub-sample the training set with 100 points per iteration. All reported numbers are computed using a held-out test set that was not used during training or hyper-parameter configuration."}]}