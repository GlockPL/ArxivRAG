{"title": "A Unified Differentiable Boolean Operator with Fuzzy Logic", "authors": ["HSUEH-TI DEREK LIU", "MANEESH AGRAWALA", "CEM YUKSEL", "TIM OMERNICK", "VINITH MISRA", "STEFANO CORAZZA", "MORGAN MCGUIRE", "VICTOR ZORDAN"], "abstract": "This paper presents a unified differentiable boolean operator for implicit solid shape modeling using Constructive Solid Geometry (CSG). Traditional CSG relies on min, max operators to perform boolean operations on implicit shapes. But because these boolean operators are discontinuous and discrete in the choice of operations, this makes optimization over the CSG representation challenging. Drawing inspiration from fuzzy logic, we present a unified boolean operator that outputs a continuous function and is differentiable with respect to operator types. This enables optimization of both the primitives and the boolean operations employed in CSG with continuous optimization techniques, such as gradient descent. We further demonstrate that such a continuous boolean operator allows the modeling of both sharp mechanical objects and smooth organic shapes with the same framework. Our proposed boolean operator opens up new possibilities for future research toward fully continuous CSG optimization.", "sections": [{"title": "1 INTRODUCTION", "content": "Boolean operations are a central ingredient in Constructive Solid Geometry (CSG) \u2013 a modeling paradigm that represents a complex shape using a collection of primitive shapes which are combined together via boolean operations (INTERSECTION, UNION, and DIFFERENCE). CSG provides a precise, hierarchical representation of solid shapes and is widely used in computer graphics.\nThe importance of CSG has motivated researchers to investigate the inverse problem; constructing a CSG tree for a given 3D model from a collection of parameterized primitive shapes. A common approach is to treat this as an optimization problem that involves choosing the structure of the CSG tree; the type of boolean operation to perform at each internal node in the tree, as well as the parameters and type (e.g., sphere, cube, cylinder) of the leaf node primitive shapes. The optimization is difficult because it contains a mixture of discrete (type of boolean operation, number and type of primitive shapes) and continuous (parameters of primitives e.g., radius, width, etc.) variables. Moreover, the degrees of freedom grow exponentially with the complexity of the CSG tree, making the optimization landscape very challenging to navigate.\nPrevious attempts either tackle the inverse optimization directly with evolutionary algorithms [Friedrich et al. 2019], or relax some of the discrete variables into continuous variables to reduce the discrete search space. For instance, one of the discrete decisions is"}, {"title": "2 RELATED WORK", "content": "Our contribution uses fuzzy logic to design a unified differentiable boolean operator with applications in gradient-based inverse CSG optimization. While researchers have formulated inverse CSG as a program synthesis [Du et al. 2018; Sharma et al. 2018; Wu et al. 2021], combinatorial optimization [Wu et al. 2018], or a global optimization [Friedrich et al. 2019; Hamza and Saitou 2004] problem, we reformulate it as differentiable gradient descent optimization problem with respect to boolean and primitive parameters. Here, we consider how our work relates to differentiable CSG optimization and to other boolean operators used in geometric modeling."}, {"title": "2.1 Gradient-Based CSG Optimization", "content": "Optimizing CSG trees requires determining the structure of the tree, the boolean operations, and the primitive parameters. In order to deploy continuous optimization techniques (such as gradient descent), existing solutions rely on pre-defining all the discrete variables (the tree structure and the boolean operations), and then only optimizing the primitive parameters. The simplest pre-defined \"CSG tree\" is a union of many parts, including convex shapes [Chen et al. 2020; Deng et al. 2020] and neural implicit functions [Deng et al. 2022]. Some works have also explored more complicated pre-defined tree structures with a mixture of INTERSECTION, UNION, DIFFERENCE operators [Ren et al. 2021; Yu et al. 2023, 2022]. Instead of pre-determining the boolean operators, Kania et al. [2020] proposed a brute force approach to all possible boolean combinations. However this approach suffers from scalability issues as the number of combinations grows exponentially with respect to the depth of the CSG tree.\nOur contribution complements these techniques by introducing a unified boolean operator which enables continuous optimization on the choice of boolean operations. This avoids the need for brute force or pre-determining boolean operations, leading to better reconstruction (Fig. 1)."}, {"title": "2.2 Boolean Operators in Geometric Modeling", "content": "The importance of boolean operations has stimulated research topics in differentiable boolean operators in geometric modeling. Despite having the same name, the term \u201cdifferentiable boolean\" can refer to (1) boolean operators that output a differentiable function, and (2) unified boolean operators that can be differentiated with respect to the type of operations.\nThe most common usage of \"differentiable boolean\" refers to boolean operations that produce differentiable functions, also known as soft blending. Traditionally, min and max operators are used to produce INTERSECTION and UNION between two implicit functions. But the caveat is that their gradients are ill-defined at locations when the input functions have the same value. This motivated Ricci [1973] to introduce a soft blending operator using a variant of p-norms $||x||_p = (\\Sigma_i |x_i|^P)^{1/p}$ to produce smooth and differentiable outputs."}, {"title": "3 BACKGROUND", "content": "The concept of fuzzy logic [Zadeh 1965] has applications in a wide variety of problem domains [Dzitac et al. 2017]. In computer graphics, fuzzy logic has been used in image processing [Chac\u00f3n M 2006], color compositing [Smith 1995], and spline interpolation [Li and Tian 2008]. Here we summarize the core ideas of fuzzy logic."}, {"title": "3.1 Fuzzy Set", "content": "A fuzzy set X = (P, fx) is a tuple of the universe of elements P = {p} and a membership function fx : P \u2192 [0, 1] such that fx (p) = 0 implies that element p is not a member of X, fx (p) = 1 implies p is a full member of X and 0 < fx (p) < 1 implies p is a partial member of X. Fuzzy sets are a generalization of the classic \"crisp\" set, whose membership function only outputs 0 or 1. For instance, suppose we define a fuzzy set H = (P, fH) of the temperatures one considers hot. The universe of elements P are all possible temperature values. One might consider some temperature values, such as 40 degrees Celsius, as full members of H so that fH (40\u00b0C) = 1. But 25 degrees Celsius, might only be a partial member fH (25\u00b0H) = 0.3. Fuzzy sets model this notion of partial membership."}, {"title": "3.2 Fuzzy Logic", "content": "Fuzzy logic develops boolean operations on fuzzy sets. Given two fuzzy sets X = (P, fx) and Y = (P, fy), a boolean operation is defined on the membership function. For instance, INTERSECTION \u2229, UNION U, and COMPLEMENT \u00ac between fuzzy sets are defined as\nXOY = (P, fxny), XUY = (P, fxuy), X = (P, f\u00acx). (1)\nA core question in fuzzy logic research is how to define these boolean membership functions fx\u2229y, fxUY, f\u00acx. A very common approach is to define them using the min, max operators [G\u00f6del 1932] as\n$f_{X \\cap Y} = min(f_X(p), f_Y(p)) = min(x, y),$ (2)\n$f_{X \\cup Y} = max(f_X(p), f_Y(p)) = max(x, y),$ (3)\n$f_{\\neg X} = 1 - f_X(p) = 1 \u2212 x.$ (4)\nTo simplify notation, here and for the rest of the paper, we use the lowercase letter x to refer to fx (p), the membership function of X applied to a generic element p \u2208 P. Similarly, y refers to fy (p).\nFuzzy Intersection. Fuzzy logic researchers have explored other definitions of fx\u2229y [Klir and Yuan 1995]. Suppose fx\u2229y = T(x, y). They define T as a valid intersection function when the following axioms hold:\nT(x, 1) = x (boundary condition)\nT(x, y) \u2264 T(x, z), if y \u2264 z (monotonicity)\nT(x, y) = T (y, x) (commutativity)\nT(x, T(y, z)) = T(T(x, y), z) (associativity)\nwhere x = fx (p), y = fy (p), z = fz(p) \u2208 [0, 1] are fuzzy mem- bership values for generic element p. Any function that satisfies these axioms is called a t-norm T [Menger 1942]. These axioms ensure that the behavior of the fuzzy intersection operator con- verges to the classic intersection (AND) operator when membership values are binary. Some popular t-norms include G\u00f6del's [1932] minimum where T(x,y) = min(x, y), product T(x, y) = xy, \u0141ukasiewicz T (x, y) = max(0, x + y \u2212 1), and Yager [1980] T(x, y) = max(1 \u2013 ((1 \u2013 x)P + (1 \u2212 b)P)1/P, 0)."}, {"title": "Fuzzy Union", "content": "Similarly, suppose fxuy = 1(x, y). In fuzzy logic, 1 is a valid UNION function if:\n1(x, 0) = x (boundary condition)\n1(x, y) \u2264 1(x, z), if y \u2264 z (monotonicity)\n1(x, y) = (y, x) (commutativity)\n\u22a5(x, \u22a5(y, z)) = 1(1(x, y), z) (associativity)\nThe functions that satisfy these axioms are known as t-conorms 1. Common t-conorms include G\u00f6del's [1932] maximum 1(x, y) = max(x, y), probabilistic sum \u22a5(x, y) = x + y \u2212 x y, bounded sum 1(x, y) = min(x + y, 1), and Yager [1980] 1(x,y) = min((xP + yP)1/P, 1)."}, {"title": "Fuzzy Complement", "content": "The set of axioms for defining a valid COMPLE- MENT function f\u00acx = C(x) are\nC(0) = 1, C(1) = 0 (boundary condition)\nif x \u2264 y, then C (x) < C(y) (monotonicity)\nValid complement functions include cosine $C(x) = \\frac{1+cos(\\pi x)/2}{2}$, Sugeno C(x) = $\\frac{1-x}{1+\\lambda x}$ with \u03bb\u2208 (-1,0\u221e), and Yager [1980] $C(x) = (1 \u2212 x^{\\lambda})^{1/\\lambda}$. The widely used complement C (x) = 1 \u2212 x, is the Yager complement with \u03bb = 1."}, {"title": "Fuzzy Difference", "content": "In fuzzy logic, the DIFFERENCE Operator \\ is usu- ally derived from the De Mor- gan's laws (see inset), which state the relationship between the UNION, INTERSECTION, and COMPLEMENT operators,\n\u00ac(XUY) = \u00acX \u2229 \u00acY, (5)\nand the relationship between DIFFERENCE and the other operators\nX \\ Y = X \u2229 \u00acY. (6)\nFor the De Morgan's law to hold, one has to jointly define the INTERSECTION, UNION, and COMPLEMENT Operators so that they satisfy Eq. 5. Then a valid DIFFERENCE operator \\ can be derived from INTERSECTION and COMPLEMENT using Eq. 6 as\n$f_{X \\backslash Y} = T(x, C(y)).$ (7)"}, {"title": "4 A UNIFIED DIFFERENTIABLE BOOLEAN OPERATOR", "content": "To apply fuzzy logic to CSG modeling, we interpret a solid shape, represented by a soft occupancy function, as a fuzzy set X = {P, fx}. Here, P = {p} denotes the universe of points p\u2208 Rd and the mem- bership function fx: P\u2192 [0, 1] is the soft occupancy function representing the probability of a point lying inside the shape. Then we can directly apply the fuzzy boolean operations presented in Sec. 3. However, we must choose INTERSECTION, UNION, and COMPLEMENT appropriate to our task. We first present our choice for each of these functions (Sec. 4.1) and then describe how to combine them into a unified boolean operator (Sec. 4.2)."}, {"title": "4.1 Product Fuzzy Logic", "content": "Motivated by our goal of continuous optimization, we would like each of our individual fuzzy boolean operations INTERSECTION T, UNION and COMPLEMENT C to be differentiable and have non- vanishing (i.e. non-zero) gradients with respect to their inputs. Van- ishing gradients can result in plateaus in the energy landscape making gradient-based optimization difficult.\nBoolean operators as defined by the product fuzzy logic meet these criteria. Specifically they are defined as\n$f_{X \\cap Y} = T(x, y) = xy$ (8)\n$f_{X \\cup Y} = \\perp(x, y) = x + y \u2013 xy$ (9)\n$f_{\\neg X} = C(x) = 1-x$ (10)\nwhere X and Y are two solid shapes and x = fx (p), y = fy (p) \u2208 [0, 1] are their soft occupancy values at a generic point p. These definitions satisfy the axioms of valid boolean operators (see Sec. 3). They correspond to valid t-norm T, t-conorm 1, and complement C functions, respectively, in fuzzy logic. They also satisfy De Morgan's law Eq. 5, allowing us to compute DIFFERENCE as\n$f_{X \\backslash X} = x-xy, \\ \\ f_{Y \\backslash X} = y - xy$ (11)\nThe product fuzzy logic boolean functions are differentiable with respect to their inputs x and y. Other fuzzy logic functions, such as G\u00f6del's [1932] min/max, t-norm/t-conorm, are not differ- entiable at singularities. In addi- tion, the product fuzzy logic func- tions are also much less prone to vanishing gradients compared to many other fuzzy logic function definitions [van Krieken et al. 2022]. More formally vanishing gra- dients occur when the partial derivatives \u2202/\u2202x, \u2202/\u2202y equal zero (or become very small). In the inset, we illustrate a 1D example where occupancy values x are strictly larger than or equal to y. Defining UNION with the G\u00f6del's max operator results in a zero gradient for y, as \u2202/\u2202y = 0. In contrast, using the UNION defined in Eq. 8 still possesses non-zero gradients for both x, y."}, {"title": "4.2 Unifying Boolean Operations", "content": "To create a unified fuzzy boolean operator that is differentiable with respect to the type of boolean operation (INTERSECTION, UNION, DIFFERENCE), our approach is to interpolate their respective mem- bership functions using a set of interpolation control parameters c. Our goal is to design an interpolation scheme that is continuous and monotonic in the parameters c so that the interpolation function avoids unnecessary local minima.\nA naive solution is to use bilinear interpolation between the four boolean operations fx\u2229Y, fxUY, fx\\Y, fy\\x. While such interpola- tion can look smooth, bilinear interpolation exhibits non-monotonic changes and creates local minima in the interpolated occupancy (see Fig. 5). This is because bilinear interpolation implicitly forces the"}, {"title": "define our unified boolean operator function Be as barycentric in- terpolation within it as", "content": "$B_c(x, y) = (c_1 + c_2)x + (c_1 + c_3)y + (c_0 - c_1 - c_2 - c_3)xy$ (12)\nwhere c = {co, C1, C2, C3} are parameters that control the type of boolean operations and they satisfy the properties of barycentric coordinates\n0 \u2264 ci\u2264 1 and CO + C1+C2 + C3 = 1. (13)\nWhen the parameter c is a one-hot vector, i.e. the barycentric coor- dinates for the vertices of a tetrahedron, it exactly reproduces the product logic operators\n$B_{1,0,0,0} (x, y) = xy = f_{X \\cap Y}$ (14)\n$B_{0,1,0,0} (x, y) = x + y - xy = f_{X \\cup Y}$ (15)\n$B_{0,0,1,0} (x, y) = x - xy = f_{X \\backslash Y}$ (16)\n$B_{0,0,0,1} (x, y) = y - xy = f_{Y \\backslash X}$ (17)\nFrom Eq. 12, we can immediately observe that our unified opera- tor is continuously differentiable with respect to both the inputs \u2202Bc/\u2202x, \u2202Bc/\u2202y and the control pa- rameters \u2202Bc/\u2202c\u2081 by design. More- over, our operator Be provides monotonic interpolation between the individual boolean operations at the vertices because interpola- tion along the edge of a tetrahedron is equivalent to a 1D convex combination (Fig. 6). Empirically, using barycentric interpolation leads to a smaller error compared to using bilinear interpolation (see inset)."}, {"title": "5 RESULTS", "content": "Building on top of fuzzy logic, we first demonstrate our choice of individual operators from Eq. 8 leads to a natural generalization"}, {"title": "5.1 Fuzzy CSG System", "content": "Using fuzzy boolean operators in CSG gives the ability to model both mechanical objects with crisp edges and smooth organic shapes with the same framework. Specifically, if the underlying implicit shapes are crisp binary occupancy functions, our method produces the same sharp results as the traditional CSG. If the input shapes are soft occupancy functions, our method outputs smooth shapes based on the \"softness\" present in the input shape.\nThis capability allows us to obtain visually indistinguishable re- sults compared to the popular smoothed min/max [Quilez 2013] operations on the signed distance function (Fig. 7). Moreover, our approach is free from artifacts caused by discrepancy between the input and the output (see Fig. 8). This is because our method is closed: both the input and the output are guaranteed to be soft occupancy functions. This is different from the previous method by Quilez [2013] such that their outputs are not signed distance functions [Marschner et al. 2023], even though the input is.\nAs the smoothness is controlled at the primitive level, we can eas- ily have adaptive smoothness across the shape by simply changing the softness of each primitive occupancy (see Fig. 9). Specifically, we consider primitive shapes represented as the signed distance function s, and we convert it to occupancy with the sigmoid func- tion sigmoid(ts) with different softnesses by adjusting the positive temperature parameter t."}, {"title": "5.2 Single Shape Inverse CSG with Gradient Descent", "content": "Our approach enables us to sim- ply use gradient descent to op- timize a CSG tree that outputs a given shape (see Fig. 10), even for smooth organic objects (see inset). Our method starts with a full binary CSG tree (each boolean node has exactly two children, each primitive node is a leaf node) with randomly initialized (uni- fied) boolean operators Be and primitive parameters (see Fig. 1). Given a ground truth occupancy function, we minimize the mean square error between the output occupancy from the CSG tree and the ground truth with the ADAM optimizer [Kingma and Ba 2015]. To enforce our unified boolean operators converge to one of the boolean operations, we use a temperetured softmax function (see App. A for implementation details). At the end of the optimization,"}, {"title": "5.3 CSG Generative Models", "content": "We demonstrate how our method can improve existing methods for fitting a shape dataset and generating CSG trees. Specifically, we focus on improving a hypernetwork approach proposed by Ren et al. [2021]. In short, given a point cloud, they propose to train a hyper- network conditioned on the point cloud to output the parameters of their proposed CSG tree structure with pre-determined boolean operations. To demonstrate improvements over their method, we conduct the experiment on the same dataset, loss function, and hypernetwork, but we replace their CSG tree structure with our fuzzy boolean CSG tree. With such change, we demonstrate notice- able improvement over both qualitative Fig. 13 and quantitative Tab. 1 evaluations. The baseline [Ren et al. 2021] is based on their implementation and their pre-trained model weights."}, {"title": "6 LIMITATIONS & FUTURE WORK", "content": "We introduce a unified differentiable boolean operator for solid shapes with soft occupancy. Our approach enables optimization of both the primitives and the boolean operations with continuous optimization techniques, such as gradient descent. As a preliminary investigation, the efforts described here open a cast of new directions for future work as well as room for improvement. We describe next a set of the limitations and opportunities for additional next steps and research directions.\nOptimizing Tree Structures. Although we have enabled differenti- ation through boolean and primitive nodes, currently, the structure of our tree is held fixed during optimization. Despite fitting the shape well, our approach often leads to complicated CSG trees as a result, even after pruning. We believe future research in optimizing among tree structures and identifying when to grow/prune/rotate the tree nodes would be beneficial to reduce tree complexity."}, {"title": "Tree Properties", "content": "The ability to optimize the tree structure could unlock optimizing the tree to have certain properties, such as com- pactness or editability. A well-known challenge of inverse CSG is that a shape can be constructed by an infinite number of different CSG trees. We suffer from the same issue that our approach only finds one of the trees, but there is no guarantee that the tree we obtain is, for instance, the most compact option."}, {"title": "Extended Fuzzy CSG System", "content": "In our work, we explore how fuzzy logic may be applied to CSG modeling. We evaluate the fuzzy coun- terparts of existing CSG operations (UNION, INTERSECTION, DIFFER- ENCE), but there are more fuzzy logic operators that do not exist in CSG traditionally. For instance, the fuzzy aggregation operator [Klir and Yuan 1995] can be perceived as a generalization of UNION or INTERSECTION on a collection of primitive shapes, instead of two. Adding such operators could enable new possibilities in tree struc- ture optimization by, for instance, selecting which primitives to use when performing boolean operations."}, {"title": "Hardware Acceleration", "content": "Our current fuzzy CSG system is based on an un-optimized implementation of fuzzy logic operators. How- ever, as shown in several other fields, fuzzy logic operators can be greatly accelerated with parallel hardware implementations (e.g., [Ontiveros-Robles et al. 2016]). A hardware-accelerated version of our CSG system based on fuzzy logic could accelerate our method to run in real time."}, {"title": "CSG Generative Models", "content": "Making CSG systems differentiable could be beneficial for future exploration on (black boxed) neural symbolic generative models [Ritchie et al. 2023] that output (white boxed) CSG tree parameters. As this is orthogonal to our contributions, we simply evaluate our method based on the off-the-shelf architecture in Sec. 5.3. Future work on better neural network architectures for tree generation would be beneficial to empower CSG generation."}, {"title": "7", "content": "A IMPLEMENTATION DETAILS FOR INVERSE CSG\nInitialization. Our method starts with a randomly initialized full binary CSG tree that consists of our fuzzy boolean nodes Eq. 12 and primitive shape represented as soft occupancy functions. We initialize the parameters of the boolean and primitive nodes with a uniform distribution between -0.5 and 0.5. As the required tree complexity is unknown, we initialize a \"big\" CSG tree (e.g., 1024 primitive shapes) to reduce the chance of having an insufficient number of primitives.\nPrimitive Choices. In terms of the choice of primitives, except the one in Fig. 11, we use quadric surfaces q\nq(x, y, z) = qox\u00b2 + q\u0131y\u00b2 + q2z\u00b2 (18)\n+ q3xy + q4yz + q5zx + q6x + q7y + q8z + 99 (19)\nin all our experiments partly due to its popularity in industry [Samuel et al. 1976]. More crucially, we believe using a less expressive primi- tive (compared to MLPs) give us a clearer signal on the performance of our proposed boolean operator. This is because an expressive primitive family, such as a big neural network, is able to fit a shape even without using any boolean operations. Then we convert the quadric function into a soft occupancy function with the sigmoid function\n\u03c3(x, y, z) = sigmoid(s \u00d7 q(x, y, z)) (20)\nwhere s is a trainable \"sharpness\" parameter to uniformly scale the quadric function to make it sharper or smoother. This allows the model to change the sharpness of quadric surface without changing the shape. Empirically, we notice a better convergence rate with a trainable sharpness.\nBoolean Parameterization. The side effect of having a unified boolean operator in is the possibility of not converging the one of the boolean operations. We alleviate this issue by parameterizing c with \u010d\u2208 R4 as\nc = softmax(sin(wc) t) (21)\nwhere t \u2208 R is the temperature. We leverage the softmax function to ensure the resulting c is always a valid barycentric coordinate. We set the temperature t to a high value (e.g., t = 10\u00b3) to encourage c to be numerically close to a one-hot vector for most parameter choices of \u010d. The sin(w) (with w = 10) function is to ensure boolean operator type can still be changed easily in the later stage of the optimization. Without it, changing c will require many iterations when \u010d has a large magnitude because each gradient update only updates \u010d a little. We observe that this parameterization of c converges to a one-hot vector in all our experiments, even though we only softly encourage most parameter choices of \u010d to be one-hot vectors. We suspect this is because any in-between operations will have occupancy values away from 0 or 1, whereas the target shape has binary occupancy values, converging to in-between operations can still occur when imperfect fitting happens.\nOptimization. We define the loss function as the mean square error between the output occupancy from the CSG tree and the ground truth occupancy, evaluated on some sampled 3D points. We sample the points with approximately 40% on the surface, 40% near the surface, and 20% randomly in the volume. We regenerate these sampled point every couple iterations (e.g., 10) to make sure we sample most areas in the volume. We use the ADAM optimizer [Kingma and Ba 2015] with learning rate 1e-3 to train our model.\nPruning. After training, we prune redundant primitive/boolean nodes with post-processing. To determine redundant nodes, we follow the definition proposed by Tilove [1984] to characterize each boolean or primitive node as either W-redundant or \u00d8-redundant. Intuitively, given a boolean node and its two child subtrees, if a subtree can be replaced with a full (soft occupancy with all 1s) or an empty (soft occupancy with all 0s) function without changing the output after the boolean operation, then this node is redundant and can be removed. We generalize such a redundancy definition to fuzzy boolean operations by setting a small threshold (e.g., mean squared soft occupancy error 10-3) to determine whether the difference after replacing a subtree with full/empty function is small enough. With the notion of redundancy, we visit each node in the CSG tree in post-order and delete the node (including its children) if it is classified as a redundant node. We demonstrate the effectiveness of such a simple pruning strategy to greatly reduce the complexity of the optimized CSG tree in Fig. 10.\nLinear Time CSG Forward Pass. During training, the full binary tree structure can be implemented in parallel for each layer by lever- aging the fact that the number of node is pre-determined. However, after pruning, the tree structure becomes irregular. Running the forward pass after pruning requires graph traversal from the leaf primitive nodes to the boolean nodes and all the way to the root boolean node. To facilitate efficient inference, we employ the linear time traversal algorithm proposed by Grasberger et al. [2016] to speed up the forward pass. Their key idea is to traverse the CSG tree in post-order and push/pop intermediate results from a stack. This traversal has a continuous memory storage of all the nodes and only requires reading each node once."}]}