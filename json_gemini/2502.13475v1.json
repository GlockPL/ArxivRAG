{"title": "LLM SHOULD THINK AND ACTION AS A HUMAN", "authors": ["Haun Leung", "ZiNan Wang"], "abstract": "It is popular lately to train large language models to be used as chat assistants, but in the conversation\nbetween the user and the chat assistant, there are prompts, such as some commands or requests,\nrequire multi-turns between the chat assistant and the user. However, there are a number of issues\nwith the multi-turns conversation: The response of the chat assistant is prone to errors and cannot help\nusers achieve their goals, and as the number of conversation turns increases, the probability of errors\nwill also increase; It is difficult for chat assistant to generate responses with different processes based\non actual needs for the same command or request; Chat assistant require the use of tools to interactive\nwith environment, but the current approach is not elegant and efficient enough, and due to the limited\ncontext window, the number of tool calls that can be supported is limited. The main reason for these\nissues is that large language models do not have the thinking ability as a human, lack the reasoning\nability and planning ability, and lack the ability to execute plans. To solve these issues, we propose a\nthinking method based on a built-in chain of thought: In the multi-turns conversation, for each user\nprompt, the large language model thinks based on elements such as chat history, thinking context,\naction calls, memory and knowledge, makes detailed reasoning and planning, and actions according\nto the plan. We also explored how the large language model enhances thinking ability through this\nthinking method: Collect training datasets according to the thinking method and fine tune the large\nlanguage model through supervised learning; Train a consistency reward model and use it as a reward\nfunction to fine tune the large language model using reinforcement learning, and the reinforced large\nlanguage model outputs according to this way of thinking. Our experimental results show that the\nreasoning ability and planning ability of the large language model are enhanced, and the issues in the\nmulti-turns conversation are solved.", "sections": [{"title": "1 Introduction", "content": "Recently, people have trained large language models as chat assistants, such as ChatGPT and Llama-3-chat [Touvron\net al., 2023, Dubey et al., 2024], but in the conversation between the user and the chat assistant, there are prompts, such\nas some commands or requests, require multi-turns between the chat assistant and the user. 1 However, there are a\nnumber of issues with the multi-turns conversation.\n(1) The response of the chat assistant is prone to errors and cannot help users achieve their goals, and as the\nnumber of conversation turns increases, the probability of errors will also increase. For example, when it comes to\nthe user command 'Help me buy a birthday cake', large language models find it difficult to reason and plan, especially\nwhen executing the plan. Due to the involvement of multi-turns conversations, incorrect answers are likely to occur in\nthe subsequent steps. If we consider the exceptions in tool calls during conversations and the user prompts noise in\nsubsequent conversations, the responses of the large language model are more prone to errors.\n(2) It is difficult for chat assistant to generate responses with different processes based on actual needs for the\nsame command or request. For example, for the user command \"Help me buy a birthday cake\", sometimes we would\nlike the response process to be \"First, ask the user about the cake flavor, size, and budget, then use the ordering tool to\nplace an order based on these parameters, and finally inform the user of the ordering result\". But sometimes we hope\nthat the response process is to \"first recommend a few cakes to the user, including details and pictures, and ask the"}, {"title": "2 Related work", "content": "Tool calls: Function calls or tool calls of OepnAI ChatGPT. Langchain's prompt engineering enables the large language\nmodels to use tool calls. In [Li et al., 2024] propose a novel approach FnCTOD for solving DST with LLMs through\nfunction calling, this method improves zero-shot DST, allowing adaptation to diverse domains without extensive data\ncollection or model tuning. Our job is to innovate the use of action calls, a more elegant and efficient tool calls.\nAI Agent: Langchain's prompt engineering enables the large language models to solve problems. Andrew Ng\nhas proposed design patterns specifically for building AI agents - Reflection, Tool Use, Planning, and Multi-Agent\nCollaboration. The focus of these studies is on AI agents, which use prompt engineering to prompt models to solve\nproblems. In these jobs, AI agents are the active party, while the large language models are the passive party. Our focus\nis on the model, which is the active party, not the agent. The agent is only a tool provider to us, and how to use the tool\nand which tools to use are determined by the model. In other words, the agent is optional to us."}, {"title": "3 Method and experiment details", "content": ""}, {"title": "3.1 Overview", "content": "First, we propose a thinking method based on built-in chain of thought. Secondly, we collect an action tasks dataset\nbased on this thinking method. Then, we use this action tasks dataset to fine tune the base model using supervise\nlearning to obtain an initial strategy. Finally, we start with this initial strategy and conduct the reinforcement learning\ntraining process. The process then consists of three steps that can be repeated iteratively."}, {"title": "Step 1: Collect samples from policies.", "content": "For each action tasks dataset sample, we sample responses from two sources\nincluding the current policy and initial policy. We then pair the action tasks dataset responses and sampling responses,\nand have a human evaluator evaluate the consistency to obtain consistency labels. We have obtained a consistency\ndataset."}, {"title": "Step 2: Train a consistency reward model.", "content": "We use the consistency dataset to train a reward model to predict the\nlogarithmic probability of consistency."}, {"title": "Step 3: Optimize the policy according to the consistency reward model.", "content": "We treat the logit output of the consistency\nreward model as a reward that we optimize using reinforcement learning.\nFinally, we explore conducting large-scale reinforcement learning on all tasks. We provide a more detailed description\nin the following section."}, {"title": "3.2 Thinking method based on built-in chain of thought", "content": ""}, {"title": "3.2.1 Thinking method", "content": "Why innovate thinking method? When handling with action tasks, large language models need to think carefully,\nmakes detailed reasoning and planning, and actions according to the plan. But is the planning correct? Can all the\nplanning steps be completed? How to interact with the environment? How to handle exceptions when using tool calls?\nDuring the execution of action tasks, How to do when a user initiates a new task? How to do when the user inputs noise\nthat interferes with the correct process? If these complex questions are not handled well, the large language model\nwill answer incorrectly. We needed a clear approach to what to do, so we propose a thinking method based on built-in\nchain of thought. The thinking method mainly consists of five elements, two processing logics and one planning step\nexecution. The thinking process of the thinking method connects all these features together, shown in Figure 2."}, {"title": "The thinking process of the thinking method.", "content": "Based on the thinking elements: In multi-turns conversations, for each user prompt, the model first thinks based on\nfour elements: chat history, global thinking context (Section 3.2.3), built-in action calls (Section 3.2.2), memory and\nknowledge.\nThe last plan step matches: If the user prompt is the last plan step corresponding to the last unfinished plan, then the\nmodel collects the useful information in the user prompt, and then proceeds to the current or next step of the plan, and if\nnecessary, uses the action calls (Section 3.2.2) to interact with the environment, and deduces the answer based on the\naction calls result. Instead, the model thinks about how to answer based on the four elements.\nThink about how to answer: If the model feels that it can not answer the prompt based on the four elements it already\nhas, it further obtains the local thinking context (Section 3.2.3) related to the prompt. If the local thinking context\ndoes not exist, then the final answer is \"can not help\" or something like that; If exist, the next step will be moved to\ndetermine whether planning is needed. If the model feels that it can answer the prompt, it will move on to the next step\nto determine whether planning is needed.\nIs planning needed? If the answer does not require a plan, then reason about it; Instead, reason and plan.\nReasoning processing logic: The model will conduct rigorous reasoning based on five elements, and if necessary, use\naction calls to interacte with environment and infer answer based on the results of action calls.\nReasoning and planning processing logic: The model will conduct rigorous reasoning and planning based on five\nelements, make a plan that includes multiple steps and begin executing the first step of the plan, and if necessary, use\naction calls to interacte with environment and infer answer based on the results of action calls."}, {"title": "Built-in chain of thought.", "content": "Besides the answer, the thinking process of the thinking method is encapsulated in special\ntokens \"\u00abthink\u00bb\" and \"\u00ab/think\u00bb\", which is often referred to as built-in chain of thought. This is similar to OpenAI's\nGPT-01-preview and Deepseek-R1."}, {"title": "The elements of thinking method and their priorities.", "content": "There are five thinking elements: Chat History, Global thinking\ncontext, Built-in action calls, Local Thinking Context, Memory and Knowledge. Chat history refers to the history of all\nuser prompts and model responses prior to the current user prompt. Memory and knowledge refers to the memory and\nknowledge that the model has, which is usually reflected in the training data. The ability of reasoning and planning\nwhen thinking is influenced by memory and knowledge. Global thinking context, Local thinking context, and Built-in\naction calls are described in the following sections. The priorities of thinking elements are as follows:\nChat History > Global thinking context > Built-in action calls > Local Thinking Context > Memory and Knowledge\nWhen thinking, the model prioritizes high priority elements over low priority elements. For example, if there is an\naction call in the global thinking context that has the same functionality as a built-in action call, we will prioritize using\nthe action call in the global thinking context."}, {"title": "Details to pay attention to when thinking.", "content": "Action call exception: When using action calls, various exceptions may be generated and returned. It is necessary to\nhandle these exceptions well and try to ensure that the plan continues.\nUser initiated task interruption: In the current task of model processing, the user may actively interrupt the plan that is\ncurrently being executed.\nUser prompt noise: In the current task of model processing, users may input prompts that interfere with the process, and\nthe model needs to think and respond.\nTask nesting: In the current task of model processing, users may input prompts to perform new tasks, such as action\ntasks or other tasks."}, {"title": "3.2.2 Action calls and Built-in Action calls", "content": "Action calls are used when the model interacts with the environment, similar in function or tool calls. But action calls\nare more elegant and efficient than function calls or tool calls.\nAction calls definition. Action call definition uses JSON structure, with attributes including: name, description,\nparameters, exception.\n\u2022 name: The name of the action call, which is used by the model when thinking.\n\u2022 description: A detailed description of the action call, including its purpose and usage scenarios.\n\u2022 parameters: The description of the parameters passed in when using action calls, including but not limited to\nunits, enumeration values, optional, etc.\n\u2022 exception: The description of possible exceptions that may occur when using action calls.\nFor example, the action call definition for real-time weather query is as follows:\n{ \"name\":\"Check the real-time weather forecast.\", \"description\":\"Use to check the real-time weather forecast.\", \"parameters\":[\n\"location\":\"The location to be queried. This's optional.\", \"time\":\"The time to be queried. This's optional.\"], \"exception\":\"If\nthe location does not exist, an exception will be returned.\" }\nAction calls usage. Models use action calls when thinking. Format for using action call:"}, {"title": "3.2.3 Global thinking context and Local thinking context", "content": "Global thinking context. Global thinking context consists of two parts: Thinking background and guideline; Action\ncalls definition. The thinking background is the data and information that the model depends on when thinking. The\nthinking guideline is a set of guiding rules for model thinking, typically used to guide how to create a plan. The action\ncalls definition defines a list of action calls that the model can use to interact with the environment during thinking.\nGlobal thinking context is placed in the system context. The role of global thinking context is similar to system context,\nguiding the model on how to do it and what to use to do it. As is well known, chat assistant such as ChatGPT typically\nplace function calls and tool calls in the system context. The disadvantages of system context include: it can occupy the\ncontext window and limit the output length; Meanwhile, excessively long system context can lead to slower inference\nspeed; Sometimes some input prompts do not require the use of any tools to call. To address these pain points, we have\ninnovated local thinking contexts.\nLocal thinking context. Local thinking context consists of two parts: Thinking background and guideline; Action calls\ndefinition. The explanation of the thinking background and guidelines, and action calls definitions is the same as the\nglobal thinking context.\nFor each user input prompt, the model first considers four elements: chat history, global thinking context, built-in action\ncalls, memory and knowledge. If the model feels that these four elements cannot help answer the prompt, it will further"}, {"title": "3.2.4 Security", "content": "If there are illegal, violent, or pornographic contents in the thinking context, the model should refuse to execute it when\nthinking. Also If there are such contents in the result of the action calls, the model should ignore them and not display\nto the user."}, {"title": "3.3 Action tasks dataset collection", "content": "We collected an action tasks dataset D containing thousands of samples, based on the thinking method. The collection\nways include human labeler and prompt engineering based on high-performance language models.\nSample format: x1, Y1, X2, Y2, ...., Xn, Y'n. Xi is user prompt or the results of action calls, y is label response. Example\nsee Figure 4. Each sample also includes reference responses, that is in addition to label responses, the two responses are\nconsistent in content and logic (see Section 3.5.1)."}, {"title": "Sample Distribution:", "content": "A variety of distributions should be taken into account when constructing the sample, and it is\nnot limited to the following situations.\nAction call exception: When using action calls, various exceptions may be generated and returned. It is necessary to\nhandle these exceptions well and try to ensure that the plan continues.\nUser initiated task interruption: In the current task of model processing, the user may actively interrupt the plan that is\ncurrently being executed.\nUser prompt noise: In the current task of model processing, users may input prompts that interfere with the process, and\nthe model needs to think and respond.\nTask nesting: In the current task of model processing, users may input prompts to perform new tasks, such as action\ntasks or other tasks.\nApplication scenarios: food delivery, shopping, McDonald style ordering, device control, customer service ans so on."}, {"title": "3.4 Supervised Learning fine-tuning", "content": "We use this action tasks dataset to fine tune the base model with supervised learning, and obtain an initial policy."}, {"title": "3.5 Reinforcement Learning fine-tuning", "content": ""}, {"title": "3.5.1 Collect samples from policies", "content": "Samples collection. For each sample of action tasks dataset D, we sample responses from two sources including the\ncurrent policy and initial policy. For sample (x1, y1, x2, Y2, \u2026\u2026\u2026\u2026, Xn, Yn) ~ D, xi is user prompt or the results of action\ncalls, y is label response. We use (x1, x2, ..., In) to sample the policy \u03c0 and get the output responses (Y1, Y2, \u2026\u2026\u2026, Yn).\nThat is:\nY1 ~ \u03c0(x1), Y2 ~ \u03c0(X1, Y1, X2), ...., Yn ~ \u03c0(X1, Y1, X2, Y2, ..., Xn)\nConsistency judgment. We then pair the label responses of action tasks dataset sample and the sampling responses\nof policy: [(y1, Y1), (Y2, Y2), \u2026\u2026\u2026\u2026, (Yn, Yn)]. And have a human evaluator evaluate the consistency, that is whether all\nsentences pairs are consistent, and obtain consistency label t equal to 0 or 1. Finally We have obtained a consistency\ndataset D': [(y1, Y1), (Y2, Y2), ...., (Y'n, Yn)], t ~ D'.\nThe consistency of sentences pair. If the content and logic of the two sentences is basically as the same, we say that\nit is consistent. But it is important to note that the planning steps that result from reasoning and planning cannot be\nshuffled, and if the order is not the same, we say that it is inconsistent."}, {"title": "3.5.2 Train a consistency reward model", "content": "From the obtained consistency dataset D', we train a reward model to predict the logarithmic probability of consistency.\nFor performance reasons, we train the consistency reward model based on a high-performance small language model,\nsuch as Llama-3-8B.\nImplementation steps. We remove the de-embedding layer of the Transformer decoder [Vaswani et al., 2017] and add\na prediction header with an output dimension of 2. The logarithmic probability of consistency is given by the prediction\nheader output. For each sample of consistency dataset D': [(y1, Y1), (\u0423\u2082, Y2), ...., (Y'n, Yn)],t ~ D', We packed all\nthe (yi', yi) pairs in the sample into one prompt x, then train the reward model to predict consistency. The loss of\nconsistency reward model is:\nloss(re) = E(x,t)~D' [cross_entropy_error(re(x), t)] (1)"}, {"title": "3.5.3 Optimize the policy according to the consistency reward model", "content": "We use the trained consistency reward model to train a policy, and reinforce the thinking ability of policy. We initialize\nour policy as a model fine tuned using supervised learning on the action task dataset. For each sample of action tasks\ndataset: (x1, y1, x2, Y2, \u2026\u2026\u2026\u2026, Xn, Yn) ~ D, xi is user prompt or the results of action calls, y is label response. We use\n(X1, X2, ..., xn) to sample the policy \u03c0\u03b8 and get the output responses (Y1, Y2, \u2026\u2026\u2026\u2026, Yn) as follows:\nY1 ~ \u03c0\u03bf(X1), Y2 ~ \u03c0\u04e9 (X1,Y1, X2), ...., Yn ~ \u03c0\u03b8 (X1,Y1, X2,Y2, ..., Xn)\nWe then optimize the policy by treating the output of the reward function as a reward for the policy output responses.\nReward function. The reward function is mainly composed of two types of rewards: format reward and consistency\nreward.\nformat reward: Check whether the special tokens in the policy output responses (Y1, Y2, ..., Yn) comply with the rules.\nThese special tokens includes \"\u00abthink\u00bb\", \"\u00ab/think\u00bb\", \"\u00abaction\u00bb\" and \"\u00ab/action\u00bb\".\nconsistency reward: Given by the consistency reward model. We encapsulate [(y1, y1), (Y2, Y2), \u2026\u2026\u2026, (Yn, Yn)] as prompt\nx, and put it into the consistency reward model to predict consistency."}, {"title": "3.6 Reinforcement Learning for all Tasks", "content": "In addition to action tasks, we can further integrate reasoning tasks and other tasks into the thinking method process.\nWe have only analyzed possible implementation steps here, and the actual work will be left to the future.\nAction tasks. Further collect more action tasks samples, in addition to human labeler and prompt engineering generation,\ntrained policy can also be used to generate samples, but format filtering and manual screening are required. The training\nprocess involves supervised fine-tuning and RL fine-tuning on the base model, following the same logic as described\nabove. Reward function is composed of two types of rewards: format reward and consistency reward.\nReasoning tasks. Similar to the approach of Deepseek-R1, reinforcement training is conducted for reasoning tasks\nwith accurate answers. How to integrate it into the thinking method process of this paper? Equivalent to only one-turn\nof conversation. Reward function is composed of two types of rewards: format reward and rule-based reward.\nOther tasks. Other tasks besides action tasks and reasoning tasks with accurate answers, such as writing tasks,\nknowledge tasks, and so on. How to integrate it into the thinking method process of this paper? Equivalent to only\none-turn or multi-turns of conversation. Reward function is composed of two types of rewards: format reward and\nhuman preference reward."}, {"title": "4 Results", "content": ""}, {"title": "4.1 The thinking ability of our model has been enhanced.", "content": "We found that our model's reasoning and planning capabilities, as well as the ability to execute the plan, were enhanced,\nby comparing with the baseline model. We use the action tasks test dataset, give the baseline model the same prompts\nand global thinking context and action call results, and then judge the model's completion rate of the tasks through\nhuman judgment and consistency reward model. We found that the baseline model did not have a higher completion\nrate for tasks than our model."}, {"title": "4.2 Action calls are more efficient than tool calls.", "content": "Action calls are syntactically more elegant than tool calls. Tool calls need to add two new message types, while\naction calls don't need to add any one. Action calls is just a message built into the chain of thought [Wei et al., 2022].\nMoreover, the names of the action calls use the sentences that exist in human life, which are widely distributed in the\ntraining data. Action calls are also more efficient than tool calls. We use tool calls on the baseline model and action\ncalls on our model, and use action tasks test datasets to fine tune both models with supervised training. Note that these\ntool calls and action calls have the same function. We observed that action calls performed better with the same amount\nof training data."}, {"title": "4.3 The relationship between reward model performance and dataset size and parameter size.", "content": "It will cost time too much for the consistency reward model to determine the consistency if the model parameters are\ntoo large, which will lead to a longer RL training time. So it is necessary to study the relationship between reward\nmodel performance and parameter size, and we will use the smallest model. How large the data size of the model needs\nto achieve ideal performance is also one of our concerns."}, {"title": "5 Discussion", "content": ""}, {"title": "5.1 Limitations.", "content": "One limitation of our work is that it is difficult to collect thinking context and cover multiple scenarios, which can lead\nto low generalization ability of the model. Collecting output from policies and evaluating consistency through manual\nlabelers is also time-consuming and laborious, resulting in a smaller dataset for consistency action tasks. We need to\nthink about how to improve the reinforcement algorithm, conduct more effective policy sampling and reinforcement\ntraining."}, {"title": "5.2 Future works.", "content": "In future work, we will explore the use of thinking method for large-scale reinforcement learning on various tasks.\nIntegrate action tasks, reasoning tasks, and other tasks into the thinking method process and conduct large-scale\nreinforcement learning. Simultaneously consider how to collect more meaningful thinking contexts and cover a wider\nrange of applications."}]}