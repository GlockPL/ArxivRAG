{"title": "Data Overvaluation Attack and Truthful Data Valuation", "authors": ["Shuyuan Zheng", "Sudong Cai", "Chuan Xiao", "Yang Cao", "Jianbin Qin", "Masatoshi Yoshikawa", "Makoto Onizuka"], "abstract": "In collaborative machine learning, data valuation, i.e., evaluating the contribution of each client' data to the machine\nlearning model, has become a critical task for incentivizing and selecting positive data contributions. However, existing\nstudies often assume that clients engage in data valuation truthfully, overlooking the practical motivation for clients to\nexaggerate their contributions. To unlock this threat, this paper introduces the first data overvaluation attack, enabling\nstrategic clients to have their data significantly overvalued. Furthermore, we propose a truthful data valuation metric,\nnamed Truth-Shapley. Truth-Shapley is the unique metric that guarantees some promising axioms for data valuation\nwhile ensuring that clients' optimal strategy is to perform truthful data valuation. Our experiments demonstrate\nthe vulnerability of existing data valuation metrics to the data overvaluation attack and validate the robustness and\neffectiveness of Truth-Shapley.", "sections": [{"title": "1 Introduction", "content": "As data regulations become increasingly stringent, it is becoming more challenging for enterprises to collect sufficient\nhigh-quality data for machine learning (ML). To address this issue, collaborative ML (CML), such as federated learn-\ning (McMahan et al., 2017), has emerged as a promising solution that enables enterprises to train accurate ML models\nwithout directly sharing data. Given that enterprises' datasets are often highly heterogeneous, a critical task in CML\nis data valuation, that is, how to reasonably evaluate the contribution of different heterogeneous datasets to model per-\nformance improvement. Based on the datasets' data values, i.e., the outcome of data valuation, enterprises can select\nhigher-quality data to further enhance model performance and fairly allocate rewards among themselves, such as the\nrevenue made by deploying the model.\nIn the literature, marginal contribution-based valuation metrics, represented by the leave-one-out (LOO) (Cook, 1977)\nand the Shapley value (SV) (Shapley, 1953), have been widely adopted for data valuation in CML. These metrics evaluate\ndata value by measuring the impact of including or excluding a dataset on model performance. For example, the SV requires\niterating over all possible combinations of the datasets and computing the model utility improvement contributed by each\ndataset to each combination, leading to significant computational costs for repeated model retraining. Consequently,\nextensive research efforts (e.g., (Ghorbani & Zou, 2019; Jia et al., 2019b,a; Kwon et al., 2021)) have been devoted to\nimproving the computational efficiency of data valuation to enhance its practicality. However, existing studies overlook\na critical trust vulnerability: During model retraining, clients may misreport their datasets to untruthfully overvalue\nthem, thereby maximizing their gains in data selection and reward allocation.\u00b9 This gap motivates us to conduct the first\nexploration of data overvaluation and truthful data valuation.\nIn this paper, we propose a novel attack method targeting data valuation: the data overvaluation attack. This attack\nenables strategic clients to misreport their datasets to significantly inflate their data value, thereby gaining an unfair\nadvantage in subsequent data selection and reward allocation tasks. Notably, the attack works against all linear data\nvaluation metrics, which cover most of the state-of-the-arts (SOTAs) including the LOO and the SV. Our experimental\nresults demonstrate that the data overvaluation attack can increase an attacker's SV by up to 210% and even boost their\nLOO value by four orders of magnitude.\nNext, we explore how to ensure truthful data valuation. We theoretically characterize the subclass of linear data\nvaluation metrics that can resist the data overvaluation attack. This characterization is fundamental since most of\nmainstream data valuation metrics are linear, inlcuding the LOO, the SV, Beta Shapley (Kwon & Zou, 2022), and Banzhaf\nvalue Wang & Jia (2023). From this subclass, we identify a novel valuation metric, named Truth-Shapley. Similar to the\nSV, Truth-Shapley uniquely satisfies a set of promising axioms for valuation, thereby ensuring effective data selection and"}, {"title": "2 Preliminaries", "content": "We consider a CML scenario where N clients (i.e., data owners) N = {1, . . ., N} collaboratively train an ML model under\nthe coordination of a server (e.g., a model buyer or a broker in the data market). Each client i \u2208 N possesses a dataset Di\nthat includes Mi data blocks. A data block Dij \u2208 Di could be a subset of the samples in Di, a subset of the features in\nDi, or even a subset of the features from a subset of the samples in Di. We write CCN to denote a subset of clients and\nDc to denote the set of data blocks possessed by clients C, i.e., Dc = UiecDi = {Di,j | i \u2208 C, j\u2208 [Mi]}. Given all clients'\ndata blocks, the server utilizes a CML algorithm A to train an ML model A(DN) on the grand dataset DN.\nAfter model training, the server performs data valuation to evaluate each data block Di,j's block-level data value\n\u03a6i,j (DN, v), which reflects the contribution of Di,j to improving the utility v(DN) of the global model A(DN). We write\n\u03a6i,j(DN, v) as di,j for simplicity when there is no ambiguity. Then, each client i's client-level data value di(DN, v) or\nsimply or is the sum of their data blocks' data values, i.e., \u03a6i = \u2211j\u2208[\u039c\u2081] \u0424i,j. Consequently, the data valuation problem is\nto design a data valuation metric 6, defined as follows, to determine data values for all data blocks involved in the CML.\nData valuation facilitates the following two downstream tasks, which ensure fairness and incentivize clients to partici-\npate in CML:\n\u2022 Data selection: The server selects data blocks Di,j with high(er) block-level data values \u00fei,j to enhance the performance\nof CML next time (Cohen et al., 2005; Nagalapatti & Narayanam, 2021), which is critical when there exist clients who\ncontribute trivial data or outliers.\n\u2022 Reward allocation: The server allocates rewards Ri($1,..., \u03a6\u039d) to each client i based on their client-level data values \u03c6\u03af\u00b7\nThe rewards may be revenue obtained from commercializing the global model (i.e., monetary rewards (Nguyen et al.,\n2022)) or customized models with differing utility (i.e., model rewards (Sim et al., 2020)). Each client i's reward\nRi($1,...,n) increases with their own data value di and decreases with the sum of the other clients' data values\n\u03a6\u2212 = \u03a3\u03af'\u2208\u039d\\{i} i'. Therefore, we assume that each client i is selfish and rational, aiming to maximize di while\nminimizing i"}, {"title": "2.2 Shapley Value for Data Valuation", "content": "As a classic metric for contribution evaluation in cooperative game theory, the Shapley value (SV) has been widely adopted\nfor data valuation in CML. It calculates the average contribution of each participant to a coalition. In our scenario, as\neach data block can be regarded as a participant, the SV determines the data value os of each data block Di,j as follows.\n$$\u03a6_{i,j}^{SV}(D_N, v): = \\sum_{S\\subseteq D_N \\{D_{i,j}\\}} w^{SV}(S) (v(S+) \u2013 v(S))$$\nwhere $$w^{SV} (S) := \\frac{|S|!(|D_N|-|S|-1)!}{|D_N|!}$$ and $$S+ = S\\cup{D_{i,j}}$$. Specifically, the SV enumerates all possible subsets S of DN\nexcluding the data block Di,j. The term $$(v(S+) \u2013 v(S))$$ quantifies the utility improvement achieved by adding Di,j to\nsubset S, and computing $$v(S)$$ and $$v(S+)$$\nrequires model retraining. $$w^{SV} (S)$$ is a coefficient that weights the importance\nof S. The data value y thus is the weighted aggregation of all utility improvements attributed to Di,j.\nSV\nPi,j\nThe SV is considered an ideal solution to data valuation because it has been proven be to the unique valuation metric\nthat satisfies the following axioms (Shapley, 1953).\n\u2022 Linearity (LIN): The server can linearly combine the data values evaluated on any two utility metrics v\u2081 and v2, i.e.,\n$$\u03a6_{i,j} (D_N, v_1 + v_2) = \u03a6_{i,j} (D_N, v_1) + \u0424_{i,j} (D_N, v_2)$$.\n\u2022 Efficiency (EFF): The sum of all data blocks' data values equals the utility improved by the grand dataset DN, i.e.,\n$$\\sum_{i\\in N} \\sum_{j\\in [M_i]} \u03a6_{i,j} (D_N, v) = v(D_N)$$.\nSV\ni,j\n\u2022 Dummy actions (DUM): If a data block Di,j does not have any synergy with the other blocks, its data value o equals\nthe utility v(Di,j) of the model trained only on itself. That is, if for all S \u2286 DN \\ {Di,j}, we have $$v(S \\cup D_{i,j}) \u2013 v(S) =\nv(D_{i,j})$$, then $$\u03a6_{i,j} (D_N, v) = v(D_{i,j})$$.\n\u2022 Symmetry (SYM): If two data blocks have the same effect on the model utility, they should obtain the same block-\nlevel data values. In other words, for two data blocks $$D_{i_1,j_1}, D_{i_2,j_2} \\in D_N$$, if for any subset of data blocks S C\n$$D_N \\backslash {D_{i_1,j_1}, D_{i_2,j_2}}$$, we have $$v(S\\cup D_{i_1,j_1}) = v(S \\cup D_{i_2,j_2})$$, then we have $$\u03a6_{i_1,j_1} (D_N, v) = \u03a6_{i_2,j_2} (D_N, v)$$.\nTheorem 2.2 (Uniqueness of SV (Shapley, 1953)). The SV $$\u03a6^{SV}$$ is the unique data valuation metric that satisfies DUM,\nSYM, LIN, and EFF."}, {"title": "3 Data Overvaluation Attack", "content": "In this section, we first provide the data overvaluation attack against the SV and then generalize it for other metrics."}, {"title": "3.1 Data Overvaluation against the SV", "content": "Although the SV fairly allocates data values to honest clients, strategic clients can manipulate their SVs by misreporting\ndata subsets for model retraining. As shown in Equation (1), the SV Sy enumerates all subsets S of the grand dataset\nDN; for each subset S C DN, a model A(S) is retrained, and its utility v(S) is evaluated for calculating Sy. Let DS\ndenote the set of data blocks in S that belong to client i, DS; denote the others data blocks in S, i.e., $$D^{S}_{\\backslash i} = S/D^S_i$$,\nand N(S) denote the set of clients who have at least one data block in S. Then, for each subset S C DN with i\u2208 N(S),\nsince v(S) can also be expressed as $$v(D^S_i \\cup D^{S}_{\\backslash i})$$, client i can vary v(S) by misreporting DS, thereby manipulating their\nblock-level SVs . \nPi,j\nSimilarly, client i can also manipulate their client-level SV V by altering the model utility v(S). Specifically, the\nclient-level SV V = \u2211j\u2208 [M]\nSy\nPi,j\ncan be written in the following form:\n$$\\Phi^{SV}_{i}(D_N, v) = \\sum_{S\\subseteq D_N} \u03b2^{SV}(S) \u00b7 v(S),$$\nwhere\n$$\u03b2^{SV}(S) =$$\n$$\\begin{cases}\n\\frac{(|D^S_i|)(|D_N|-|D_i|-|S|+1)!}{|D_N|!} , & S \\subseteq D_N, S \\neq 0, \\\\\n\\frac{|D_i|}{|D_N|}, & S = D_N, \\\\\n-\\frac{|D_i|}{|D_N|}, & S = \\emptyset.\n\\end{cases}$$\nBecause $$\\frac{\\partial \u03a6^{SV}_{i}}{\\partial v(S)} = \u03b2^{SV}(S)$$, when $$\u03b2^{SV}(S) > 0$$, increasing v(S) can enhance V; when $$\u03b2^{SV}(S) < 0$$, decreasing v(S)\nimproves qV; when $$\u03b2^{SV}(S) = 0$$, changing v(S) reduces V has no effect on o\u0218V.\nAlgorithm. Based on the above analysis, we propose Algorithm 1 to implement data overvaluation against the SV,\nconsidering a strategic client i as an attacker. Let De denote the version of Ds reported by client i and DS, denote the\nversion of D\u00ba; reported by the other clients for evaluating the model utility v(S). That means, instead of truthfully using\ndataset Di, client i may untruthfully employ dataset D \u2260 D to increase the model utility from v(S) to v(5), where\n$$S = D^S_i \\cup D^{S}_{\\backslash i}$$. In Algorithm 1, to compute the SV, we iterate over every subset S C DN and evaluate its corresponding\nmodel utility (Lines 3-18). Note that the grand model's utility v(DN) is given as the algorithm's input, as the grand\nmodel A(DN) has already been trained before data valuation. When a subset S includes client i's data blocks, if \u1e9e\u0218V (S)\nis nonzero, client i has the incentive to manipulate v(S); thus, in this case, client i positively/negatively augments their"}, {"title": "3.2 Generalization", "content": "In addition to the SV, we further generalize the data overvaluation attack to manipulate all linear data valuation metrics.\nLemma 3.1. If a data valuation metric \u00f3 satisfies LIN, then for each client i, there exists Bi : 2DN \u2192 R such that\n$$\u03a6_{i}(D_N, v) = \\sum_{S\\subseteq D_N}\u03b2_{i}(S) \u00b7 v(S)$$.\nSpecifically, Lemma 3.1 indicates that any linear data value di can be expressed as a weighted sum of model utilities\n$${v(S)}_{S\\subseteq D_N}$$. Consequently, similar to the case of the SV, for each subset SC DN, and for each client i \u2208 N(S), when\nBi(S) is positive (negative), they can increase (decrease) v(S) to enhance their linear data value $i. However, unlike the\nSV, some data valuation metrics such as Beta Shapley and Banzhaf value do not satisfy EFF, meaning that an increase in\ndi does not necessarily lead to a decrease in -i. As a result, client i may not always receive a higher reward. Therefore,\nwhen Bi(S) is positive (negative), we should also ensure that $$\u03b2_{-i}(S) = \\sum_{i'\\in N(S)\\{i\\}} \u03b2_{i'}(S)$$ is non-positive to prevent \u0444\u2212i\nfrom increasing."}, {"title": "4 Truthful Data Valuation for CML", "content": "In this section, we first characterize the subclass of data valuation metrics that can prevent the data overvaluation attack\nand then select a special metric from this class, named Truth-Shapley (Truthful Shapley value)."}, {"title": "4.1 Characterization of Truthful Data Valuation", "content": "From Definition 3.2, we know that the issue of data overvaluation arises from strategic clients untruthfully reporting their\ndata subset Ds, which is highly analogous to the problem of untruthful bidding in auctions. Specifically, for each client i,\nif we regard their reported data \u0189,\u2200S C DN as their bid and the empirical data value 6\u2081 as their payoff, the problem of\npreventing data overvaluation can be viewed as ensuring a truthful auction.\nDefinition 4.1 (Bayesian Incentive Compatibility for Truthful Data Valuation). A data valuation metric & is Bayesian\nincentive compatible (BIC) if for any game (DN, v), for any client i, and for any reported data subsets {D\u0218 | S CDN, i \u2208\nN(S)}, we have\n$$E_{D^S\\sim \u03c3(\u00b7|S), \u2200S\\subseteq D_N} [\u03a6_i(D_N, v)] \\le$$\n$$E_{D^S\\sim \u03c3(\u00b7|S), \u2200S\\subseteq D_N} [\u03a6_i(D_N, v|\u2200S \\subseteq D_N, D^{S} = D^{S})],$$\n$$E_{D^S\\sim \u03c3(\u00b7|S), \u2200S\\subseteq D_N} [\u03a6_{-i}(D_N, v)] \\ge$$\n$$E_{D^S\\sim \u03c3(\u00b7|S), \u2200S\\subseteq D_N} [\u03a6_{-i}(D_N, v|\u2200S \\subseteq D_N, D^{S} = D^{S})],$$\nwhere \u03c3\u03af(\u00b7 | S) denotes the distribution of DS, estimated by client i in their belief.\nAccordingly, we draw on the concept of Bayesian incentive compatibility (BIC) from auction theory d'Aspremont & G\u00e9rard-Varet\n(1982), defined in Definition 4.1, to ensure truthful data valuation. Intuitively, BIC ensures that for each client i, truth-\nfully reporting their data D = D is the optimal strategy that not only maximizes their expected data value in Formula\n(2) but also minimizes the sum of the other clients' data values in Formula (4). Note that the expected data values in\nFormulas (2)-(5) are based on client i's prior beliefs {\u03c3\u03af(\u00b7 | S)}\nSCDN about the other clients' reported data. This implies\nthat truthful reporting is subjectively optimal based on their beliefs, rather than objectively optimal. For simplicity, we\nwill omit the conditional subscript of the expectation operator E where there is no ambiguity.\nAssumption 4.2 (Subjectively Optimal Data). For any data subset S C DN with D = Di, and for any D\u0218, we have\n$$E_{D^S\\sim \u03c3(\u00b7|S), \u2200S\\subseteq D_N} [v(D^S\\cup D^{S}_{\\backslash i})]\\ge$$\n$$E_{D^S\\sim \u03c3(\u00b7|S), \u2200S\\subseteq D_N} [v(D^S\\cup D^{S}_{\\backslash i})].$$"}, {"title": "4.2 Truth-Shapley", "content": "Next, we attempt to select a strong member from the subclass of linear and BIC data valuation metrics. Our idea is to\nsatisfy the four axioms enjoyed by the SV as much as possible, even though, according to Theorem 2.2, full compliance is\nimpossible. The first axiom we prioritize is EFF, as it ensures that the model utility v(DN) is fully attributed to all data\nblocks. Consequently, we propose Theorem 4.4, which characterizes linear, efficient, and BIC valuation metrics.\nTheorem 4.4 (Characterization 2). Consider a linear, efficient data valuation metric \u00f3i(DN, v) := \u2211s\u2286D\u2122 Bi(S) \u00b7 v(S)\nwhere Bi : 2DN \u2192 R. Under Assumption 4.2, $ satisfies BIC iff: $$\u03c6_i(D_N,v) = \\sum_{C\\subseteq N} \u03b2_{i}(D_C) \u00b7 v(D_C)$$ where $$\u03b2_{i}(D_C) \\ge 0$$ for\nall CCN with i \u2208 C.\nBased on Theorem 4.4, we know that under a data valuation metric satisfying LIN, EFF, and BIC, each client's\nclient-level data value i should be determined only by the utilities v(Dc) of all combinations Dc of client's full datasets\nD1,..., Dr. Accordingly, we propose Truth-Shapley (simply TSV) $TSV, which uses an SV-style approach to (1) compute\nthe client-level data value $7SV based on the clients' full datasets and then (2) divide SV among individual data blocks\nto derive TV,.\nTSV\n\u00b7 \u03a6, \u039c\u03b5\nSpecifically, let Dc = {Di}viec for all C\u2286 N and D\u2212\u00bf = DN \\ {Di}. Note that Dc is mathematically distinct from Dc,\nbut it holds the same physical meaning and thus corresponds to the same utility $$v(D_C) = v(D_C)$$. Then, we apply the\napproach of the SV to calculate the client-level TSV:\n$$\\Phi^{TSV}_{i}(D_N, v) := V^{TSV}(D_N, v)$$\n$$= \\sum_{C\\subseteq N\\{i\\}} w^{SV} (C|N) (v(D_C \\cup \\{D_i\\}) \u2013 v(D_C)),$$\nwhere $$w^{SV} (C | N) := \\frac{|C|!(|N|-|C|-1)!}{|N|!}$$. Next, we employ the SV again to calculate the block-level TSV:\n$$\\Phi^{TSV}_{i,j}(D_N, v) := \u03a6^{SV}(D_{i,j}, v^{\\Phi^{TSV}})$$\n$$= \\sum_{S\\subseteq D_i\\{D_{i,j}\\}} w^{SV} (S|D_i) (v^{\\Phi^{TSV}} (S \\cup \\{D_{i,j}\\})-v^{\\Phi^{TSV}} (S)),$$\nwhere $$w^{SV} (S | D_i) := \\frac{|S|!(|D_i|-|S|-1)!}{|D_i|!}$$ and $$v^{\\Phi^{TSV}}(S) := \u03a6^{TSV}(D_{-i} \\cup \\{D_i\\}, v)$$. Intuitively, the utility $$v^{\\Phi^{TSV}} (S)$$\nrepresents\nthe client-level TSV V when client i contributes dataset Ds. Consequently, the block-level TSV TSV measures the\nexpected marginal contribution of the data block Dij to improving the client-level TSV SV. Due to the use of the SV-\nstyle approach for defining both TSV and TSV, we ensure that Truth-Shapley is linear, efficient, and perfectly complies\nwith the characterization in Theorem 4.4. Therefore, we conclude that it satisfies BIC.\nTheorem 4.5. Truth-Shapley (TSV satisfies BIC.\nFrom Theorem 2.2, we know that Truth-Shapley cannot simultaneously satisfy DUM, SYM, LIN, and EFF. However,\nwe find that apart from LIN and EFF, Truth-Shapley satisfies the following axioms:"}, {"title": "5 Data Selection", "content": "For data selection, we evaluate data valuation metrics based on two selection criteria. In Table 2, we perform CML using\nthe top K data blocks ranked by their data values. In Table 3, we calculate the contribution rate of each data block,\ndefined as the ratio of its block-level data value to the sum of all data blocks' data values. Only data blocks with a\ncontribution rate no less than a predefined threshold are selected for CML.\nAs shown in Tables 2 and 3, in HFL, data overvaluation attacks against Truth-Shaply cannot affect the model accuracy\nat all, as it can fully resist such attacks. In contrast, under other data valuation metrics, since data overvaluation\nsignificantly alters both the absolute values of block-level data values and their relative rankings, the model accuracy\ndeclines significantly in most cases. Additionally, in VFL and HyFL, the situation is generally similar, except in the case\nof BSV, where data overvaluation does not succeed as shown in Table 1.\nIn summary, under data valuation metrics that do not satisfy BIC, the data overvaluation attack can significantly impact\ndata selection outcomes. This not only harms model accuracy but also leads to unfair opportunities for participating in\nCML, indirectly affecting clients' potential rewards. Notably, even in the absence of the attack, Truth-Shapley remains as\neffective as other metrics in selecting data blocks."}, {"title": "6 Discussion", "content": "Poisoning attacker. Assumption 4.2 is the core assumption of this paper, which implicitly assumes that the attacker's\ndataset Di is not a poisoning dataset. This assumption is based on the premise that client i aims to maximize their reward\nand thus will not poison the grand model A(DN), as doing so would reduce the reward derived from monetizing/utilizing\nA(DN). However, in certain scenarios, client i may pursue dual objectives: both attacking the grand model and conducting\ndata overvaluation. Addressing this dual-objective scenario requires further exploration.\nComputational efficiency. Similar to computing the SV, computing Truth-Shapley is time-consuming, as it requires\nO(2N+maxi Mi) times of model retraining. Since Truth-Shapley utilizes the SV-style approach to define both its client-level\ndata value and block-level data value, existing techniques for accelerating SV computation can be applied to computing\nthese two levels of data value. Also, designing more efficient acceleration methods specifically for Truth-Shapley is a\npromising direction for future research.\nExtension of data overvaluation attack. The data overvaluation attack proposed in Definition 3.2 allows client i\nto manipulate the utility v(S) of a data subset S C DN by misreporting client i's data blocks D. Similarly, client i can\nachieve the same objective by violating the training algorithm A. For example, client i can decrease v(S) by performing\na gradient ascent attack during model training. Truth-Shapley remains resistant to this extension of data overvaluation\nattack with a slight modification to Assumption 4.2: we assume that, in client i's belief, following algorithm A maximizes\nthe expected utility for any S C DN."}, {"title": "7 Related Work", "content": "Most of existing studies designed data valuation methods for CML based on two data valuation metrics: LOO (Cook,\n1977) and the SV (Shapley, 1953). Since computing these metrics usually requires evaluating the model utilities for\na large number of data subsets, substantial research efforts have been devoted to improving the efficiency of the com-\nputation. Their approaches include downsampling data subsets Ghorbani & Zou (2019); Jia et al. (2019b); Luo et al.\n(2024, 2022); Lin et al. (2022); Jia et al. (2019a); Kwon et al. (2021), designing training-free utility functions (Wang et al.,\n2024a; Pruthi et al., 2020; Koh & Liang, 2017), and approximating retrained models (Wu et al., 2022; Just et al., 2023;\nNohyun et al., 2022).\nAnother line of research focuses on enhancing the robustness and reliability of data valuation. Xu et al. (2021) designed\na new utility function that is more robust to clients' data replication behavior. Lin et al. (2024) provided a validation-free\nutility function for clients without a joinly-agreed validation dataset. Some studies (Schoch et al., 2022; Xu et al., 2024;\nXia et al., 2024) have designed utility functions that capture a model's predictive capability at a finer granularity than\nprediction accuracy. Tian et al. (2024) and Xia et al. (2023) proposed methods to accelerate recomputing data values\nin machine unlearning scenarios. Zheng et al. (2023) and Wang et al. (2024b) proposed methods to ensure privacy and\nsecurity in data valuation. Wang et al. (2023) introduced the Banzhaf value as a data valuation metric, which is robust to\nthe randomness of model retraining. Kwon et al. (2022) extended the SV to Beta Shapley, improving the detection of noisy\ndata points. Our work reveals a new vulnerability in data valuation, i.e., data overvaluation, and proposes Truth-Shapley\nto enhance robustness/reliability against data overvaluation."}, {"title": "8 Conclusion", "content": "This paper introduces the first data overvaluation attack in CML scenarios. We characterized the subclass of linear and BIC\ndata valuation metrics that can resist this attack and selected Truth-Shapley from the subclass as a promising solution to\ntruthful data valuation. Through both theoretical analysis and empirical experiments, we demonstrated the vulnerability\nof existing linear data valuation metrics to data overvaluation and the robustness and effectiveness of Truth-Shapley. In\naddition to the research opportunities discussed in Section 6, there remains substantial room for further exploration in\ndata overvaluation and truthful data valuation. Potential directions include developing new data overvaluation strategies,\ndesigning tailored algorithms for implementing data overvaluation in specific CML scenarios, and constructing defense\nmechanisms compatible with those vulnerable data valuation metrics."}, {"title": "Impact Statement", "content": "For the industry, this paper identifies a new attack method that poses a trust crisis for data valuation in CML. For the\nacademia, this paper opens up a new research direction: truthful data valuation for CML."}, {"title": "A Experimental Setup", "content": "As shown in Table 4, we perform data valuation in the following three CML scenarios.\n\u2022 Horizontal federated learning (HFL): In HFL, clients possess data blocks with the same feature space but different\nsample spaces. In this scenario, we use the Apartments for Rent dataset (apa, 2019), which contains rental data from\nvarious states. We assume three regional rental companies as clients and divide the dataset into 12 data blocks based\non the geographic regions of the apartments in the US, assigning these blocks to the clients. These clients utilize the\nFedAVG algorithm (McMahan et al., 2017) to perform HFL, collaboratively training a multilayer perceptron (MLP) to\npredict rental prices.\n\u2022 Vertical federated learning (VFL): In VFL, clients possess different features of the same samples. We use the Bank\nMarketing dataset (Moro et al., 2014) and partition its features into 12 data blocks based on their content, assigning\nthem to three finance-related companies as clients. These clients then perform VFL using the split learning algo-\nrithm Gupta & Raskar (2018) to train a SplitNN-based binary classification model for target customer detection.\n\u2022 Hybrid federated learning (HyFL): HyFL allows clients to have data with both different sample spaces and feature spaces.\nWe consider three medical institutions as clients, each with a non-overlapping patient group and distinct diagnostic tests.\nThey own 11 data blocks from the CDC medical dataset (CDC, 2015), where each data block contains patient data\nrelated to a specific diagnostic test at a particular institution. These institutions use the FedMD algorithm (Li & Wang,\n2019) to collaboratively train an MLP-based ensemble model for diabetes prediction."}, {"title": "B Proofs", "content": "Proof of Lemma 3.1. For every data subset S C DN, we define the basis game ds(T) = by\n$$\\delta_S (T) = \\begin{cases}\n1, & T = S, \\\\\n0, & T \\neq S.\n\\end{cases}$$\nThe set {ds | S \u2286 DN} is a natural basis for G(DN). Then, consider a linear data valuation metric $(DN, v). For any two\nutility functions v\u2081 and v2 and scalars w\u2081, W2 \u2208 R, we have:\n$$\u03a6_i(D_N, w_1v_1 + w_2v_2) = w_1\u03a6_i(D_N, v_1) + w_2\u03a6_i(D_N, v_2).$$"}]}