{"title": "A Survey on LoRA of Large Language Models", "authors": ["Yuren MAO", "Yuhang GE", "Yijiang FAN", "Wenyi XU", "Yu MI", "Zhonghao HU", "Yunjun GAO"], "abstract": "Low-Rank Adaptation (LoRA), which updates the dense neural network layers with plug-gable low-rank matrices, is one of the best per-formed parameter efficient fine-tuning paradigms. Furthermore, it has significant advantages in cross-task generalization and privacy-preserving. Hence, LORA has gained much attention recently, and the number of related literature demonstrates exponen-tial growth. It is necessary to conduct a compre-hensive overview of the current progress on LoRA. This survey categorizes and reviews the progress from the perspectives of (1) downstream adaptation improving variants that improve LoRA's perfor-mance on downstream tasks; (2) cross-task gener-alization methods that mix multiple LoRA plugins to achieve cross-task generalization; (3) efficiency-improving methods that boost the computation-efficiency of LoRA; (4) data privacy-preserving methods that use LoRA in federated learning; (5) application. Besides, this survey also discusses the future directions in this field.", "sections": [{"title": "1 Introduction", "content": "Rapidly increasing parameter scales of pre-training language models improves their generalization ability and brings emergent abilities. In the last few years, the parameter scales of pre-training lan-guages models have increased by thousands of times (e.g., from 330M parameter BERT [1] to 540B parameter PaLM [2]). These pre-training language models having large parameter scales are termed Large language models (LLMs). Never-theless, due to the knowledge boundaries of the LLMs, their abilities on some downstream tasks are still limited. To expand the knowledge bound-aries, it remains necessary to fine-tune LLMs on the downstream tasks.\nHowever, fine-tuning the full parameters of an LLM, namely full fine-tuning, is extremely compu-tationally expensive, for example, full fine-tuning of a LLaMA2-7B [3] model requires approxi-mately 60GB of memory, which exceeds the capacity of common consumer GPUs [4]. To reduce LLMs.\nthe computational cost, various parameter-efficient\nfine-tuning (PEFT) methods have been proposed\n[5]. They adapt LLMs to downstream tasks by\nonly fine-tuning a small number of (extra) model\nparameters. From the perspective of whether ex-\ntra parameters are involved, PEFT methods can\nbe divided into two categories: extra-parameter\nmethods and intra-parameter methods. The extra-\nparameter methods freeze all of the original param-\neters of an LLM and insert a set of learnable param-\neters to optimize the model input or model layers\nsuch as adapter tuning [6] and prompt tuning [7].\nBy contrast, intra-parameter methods freeze most\nof the original parameters of an LLM and only tune\na small number of parameters of the LLM such as\nBitFit [8], LISA [4] and LoRA [9].\nWhen we do not have access to modify the model\narchitecture, intra-parameter methods are desir-\nable. Among the intra-parameter methods, LoRA\nis the most widely used one, because it can achieve\na comparable or better downstream adaptation per-\nformance to the full fine-tuning on a range of\ndownstream tasks [9] and is easy to implement.\nBesides, there are many variants have been pro-\nposed to further improve the downstream adapta-\ntion ability of LoRA on more challenging down-\nstream tasks.\nLORA achieves parameter efficiency by updat-\ning the dense neural network layers of an LLM\nwith pluggable low-rank matrices. These matri-\nces (a.k.a, LoRA plugins) are independent of the\nLLM, which can be stored and reused in other re-\nlated downstream tasks. Furthermore, these LoRA\nplugins can be combined to achieve cross-task gen-\neralization, which can facilitate multi-task learn-\ning, domain adaptation, and continual learning for\nAs the LoRA plugins accumulate, the computation cost of managing LoRA plugins is increasing. Al-though LoRA is computation-efficient, the compu-tational cost of managing a larger number of LoRA plugins is unignorable. It is necessary to further improve the computation efficiency of LoRA. The improvement can come from reducing the compu-tation cost of single LoRA plugins and accelerat-ing the scalable serving of multiple plugins. It can boost the application of LoRA in real-world use cases, such as Generative-as-a-Service (GaaS) cloud products.\nIn some cases, the training data are privately owned by multiple clients and cannot be centralized. To adapt LLMs with the distributed training data, we can adopt federated learning to protect the data pri-vacy of each client. However, federated learning suffers expensive communication and computation costs. To reduce costs, LoRA is a natural choice. Its parameter-efficient nature helps to reduce the computation cost of each client and the commu-nication cost of sharing parameters across clients. Furthermore, the pluggable feature of LoRA can help preserve the parameter privacy of each client in federated learning. Therefore, LoRA has a great potential for privacy-preserving.\nIn this survey, we give a comprehensive overview of the current progress on LoRA for methods (1) improving downstream adaption performance of LoRA; (2) mixing LoRA plugins to achieve cross-task generalization; (3) boosting the computation-efficiency of LoRA; (4) adopting LoRA in feder-ated learning. Besides, the application of LoRA is briefly introduced. This taxonomy of LoRA-related methods is illustrated in Figure 1. This sur-vey is expected to give comprehensive background"}, {"title": "2 Low-Rank Adaptation (LoRA)", "content": "The Low-dimensional intrinsic dimensionality hy-pothesis [186] presents that over-parameterized models reside on a low intrinsic dimension, which demonstrates that we can achieve proper learn-ing performance by only updating parameters re-lated to the intrinsic rank. Based on this hypothe-sis, LoRA [9] proposes to update dense layers in a model with low-rank matrices. It can achieve both parameter- and computational- efficiency. In this section, we first introduce the details of LoRA and then introduce existing works that focus on the theoretical analysis of LoRA. Furthermore, we demonstrate LoRA's efficiency in practice. At last, this section presents that LoRA can be used in other use cases except fine-tuning."}, {"title": "2.1 LORA", "content": "Given a dense neural network layer parameterized by \\(W_o \\in \\mathbb{R}^{d\\times k}\\), to adapt it to a downstream task, we update it with \\(\\Delta W \\in \\mathbb{R}^{d\\times k}\\) and obtain an updated layer parameterized by \\(W = W_o + \\Delta W\\). For full fine-tuning, \\(\\Delta W\\) is computed based on gradients of all the d\u00d7k parameters for the layer, which is com-putationally expensive and requires a large amount of GPU memory for LLMs. To improve the com-putational efficiency, LoRA decomposes \\(\\Delta W\\) into two small matrices \\(B \\in \\mathbb{R}^{d\\times r}\\) and \\(A \\in \\mathbb{R}^{r\\times k}\\), i.e.,\n\n\\[W = W_o + \\alpha BA \\tag{1}\\]\nwhere \\(r < min\\{d,k\\}\\), B and A are initialized with a random Gaussian distribution and zero respec-tively, \\(\\alpha\\) represents the scaling factor that controls the strength of updates. The parameter number of LORA is \\(r\\times (d + k)\\), which is significantly less than \\(d \\times k\\). Figure 2 (a) and (b) compare the structures of full fine-tuning and LoRA.\nLoRA is highly parameter efficient for it up-dates only a small subset of model parameters, which reduces the memory and computational re-quirements for fine-tuning without increasing in-ference latency [187]. Furthermore, The parame-ter efficiency can be further improved by extend-ing from the low-rank matrix to low-rank ten-sor [188] or combining with the Kronecker de-composition [189, 190]. Except for parameter ef-ficiency, LoRA is also pluggable for the LoRA parameters that can be separated from the model after training. The pluggable character of LoRA enables it to be shared and reused by multiple users [191]. When we have LoRA plugins for multiple tasks, we can combine these plugins and expect a proper cross-task generalization perfor-mance [58]. Besides, the low-rank mechanism of LORA is compatible with other parameter-efficient methods, such as adapter [192, 193].\nIn practice, for a Transformer-based LLM, the dense layers typically consist of two types of weight matrices: the projection matrices in atten-tion modules and feed-forward neural (FFN) mod-ules. In the original study, LoRA is applied to the weight matrix of the attention layer. Subsequent work shows that using it in the FFN layers can fur-ther improve model performance [194]."}, {"title": "2.2 Theoretical Analysis", "content": "To understand why LoRA is effective and how LoRA can be more effective, several works have provided theoretical analyses from various aspects. To answer the question that why LoRA is effec-tive, Malladi et al. [10] analyze the fine-tuning dy-namics of LoRA from the kernel view and demon-strate that in the lazy regime, LoRA fine-tuning is nearly equivalent to full fine-tuning. Besides, Zeng et al. [14] provides a theoretical analysis of the LoRA's expressive power for both fully con-nected neural networks (FNNs) and Transformer networks (TFNs). They proved that for FNNs, LoRA can adapt any model \\(f\\) to accurately rep-resent any smaller target model \\(f'\\) if LoRA-rank \u2265 (width of \\(f\\)) \u00d7 \\(\\frac{depth~of~f'}{depth~of~f}\\), under a mild assump-tion. Moreover, they quantify the approximation error when the LoRA-rank falls below this thresh-old. Regarding TFNs, they showed that any model can be adapted to a target model of equivalent size using a rank-(embedding size) for LoRA. Additionally, Koubbi et al. [11] utilize the mathematical frame-work for Transformers established by [195-197] to investigate the how low-rank perturbations in at-tention parameters affect.\nAs to the question that how LoRA can be more effective, Jang et al. [12] analyze the fine-tuning of LoRA within the neural tangent kernel (NTK) [198] framework, showing that employing a rank\\(r\\geq \\sqrt{N}\\) in LoRA helps to avoid spurious local minima and facilitates the discovery of low-rank solutions that exhibit good generalization. Besides, Zhu et al. [13] observe that the project-down matrix A is utilized for extracting features from the input, while the project-up matrix B employs these features to create the desired output. Based on this observation, they demonstrate that freezing the project-down matrix A while tuning only the project-up matrix B leads to better generalization compared to tuning both matrices, in addition to achieving a 2\u00d7 reduction in parameters."}, {"title": "2.3 Efficiency in Practice", "content": "The computational efficiency of LoRA is signifi-cantly higher than that for full fine-tuning. Tak-ing fine-tuning the dense weight matrix of the first FFN layer in LLaMA2-7B as an example, full fine-tuning needs to fine-tune 11,008 \u00d7 4,096 = 45,088, 768 parameters while LoRA only needs to"}, {"title": "2.4 Beyond Fine-tuning", "content": "Besides fine-tuning, LoRA can be applied to other learning paradigms, such as pre-training [15, 17] and continual training [18]. For pre-training, ReLoRA [15] and MoRA [16] are proposed to use low-rank updates to train high-rank networks; moreover, LTE [17] is proposed to perform par-allel training of multiple low-rank heads across computing nodes to minimize the need for fre-quent synchronization, which facilitates the utiliza-tion of LoRA in pre-training. As for continual training, there are several methods have been pro-posed to address the catastrophic forgetting prob-lem. InfLoRA [18] addresses catastrophic forget-ting by reparameterizing pre-trained weights with a minimal set of parameters in a subspace. GS-LORA [19] uses group sparse regularization to au-tomatically select specific LoRA groups while ze-roing out others to mitigate catastrophic forgetting effects. I-LORA [20] leverages dual-memory expe-rience replay combined with LoRA parameter in-terpolation to combat catastrophic forgetting.\nFurthermore, LoRA can be used to overcome the limited context size for LLMs [3,21]. For instance, LongLoRA [3] successfully computaitional effi-ciently extends the context window of LLaMA2-7B [199] from 4k to 100k tokens by combining LoRA with shifted sparse attention. However, Lon-gLoRA does not match the efficiency of vanilla at-tention due to chaotic attention head structures and unnecessary information exchange between token"}, {"title": "3 Downstream Adaptation Improving", "content": "Although LoRA can achieve proper adaptation per-formance on some downstream tasks, there is still a performance gap between LoRA and full fine-tuning on many downstream tasks, such as math-ematical reasoning [200-202]. To fill this gap, many methods are proposed to further improve the downstream task adaption performance of LoRA. Typically, existing methods improve the down-stream adaptation performance from the follow-ing perspectives: (1) breaking the low-rank bot-tleneck, refer to Figure 2 (c); (2) adaptively allo-cating the ranks of different LoRA modules, refer to Figure 2 (d); (3) optimizing the learning proce-dure of LoRA; (4) combining with other learning paradigms. In this section, we introduce these four types of methods respectively."}, {"title": "3.1 Breaking the Low-rank Bottleneck", "content": "The low-rank updates enable LoRA to be param-eter efficient; however, it restricts LLMs' ability to memorize downstream knowledge and general-ization on downstream tasks [16, 201-204]. This low-rank limitation causes inferior performance of LoRA in knowledge- and skill-intensive domains comparing to full-fine tuning, such as code and math. Experimental study [202] demonstrates that the rank for full fine-tuning is significant (10-100 \u00d7) higher than that for LoRA, and increas-ing the rank of LoRA updation can narrow the performance gap between LoRA and full fine-tuning. To increase the rank of LoRA and improve its performance, several methods have been pro-posed [15,22,25,205], which typically increase the rank through (1) stacking LoRAs along learning it-erations; (2) updating as gradient compressors; (3) co-updating LLM and LoRA modules during fine-tuning."}, {"title": "3.1.1 Stacking LoRAs along Fine-tuning", "content": "Matrix rank is subadditive, i.e., rank(M\u2081 + M2) \u2264 rank(M\u2081) + rank(M2) for metrices M\u2081 and M2 that have the same size. Based on the subadditiv-ity, we can aggregate multiple LoRA modules to-gether to increase the rank and break the low-rank bottleneck. Following this idea, ReLoRA [15] proposes a merge-and-reinit procedure for LoRA, which periodically merges the LoRA modules to the LLM and then reinitializes the LoRA mod-ules during fine-tuning. It equals stacking mul-tiple LoRA modules along with fine-tuning and can increase the rank of the overall updates. Similarly, COLA [22] proposes another merge-and-reinit method based on Frank-Wolfe algo-rithm [206]. However, MELORA [23] points out that the merge-and-reinit procedure does not nec-essarily guarantee an increase in rank, because there can be overlap between the series of LORA modules along fine-tuning. To solve this problem, MELORA proposes to decompose the LoRA mod-ules into smaller mini LoRAs and then parallelly stack these mini LoRAs, whose effectiveness in in-creasing the rank is theoretically verified."}, {"title": "3.1.2 Updating as Gradient Compressor", "content": "The above methods break the low-rank bottle-neck in the parameter space. As a supplement, FLORA [24] finds that LoRA performs a fixed random projection to compress gradients and re-stricts the total weight matrix change to low-rank."}, {"title": "3.1.3 Co-updating LLM and LoRA", "content": "The above two kinds of methods focus on im-proving the representation ability of LoRA itself. Different from them, Delta-LoRA [25] proposes to jointly update the LLM and LoRA modules, which directly updates the high-rank LLM and can gain better representations capable than updat-ing LoRA independently. Delta-LoRA updates the LLM based on the difference between two LoRA modules of two consecutive iterations, which en-ables it to update the LLM without any extra mem-ory."}, {"title": "3.2 Dynamic Rank Allocation", "content": "For the rank of LoRA, higher is not always better. The abundant LoRA ranks may cause degeneration in both performance and efficiency. Furthermore, the importance of weights can vary across different layers of a Transformer model during fine-tuning, requiring different ranks for each layer. [26, 29, 31, 207]. Therefore, assigning the same rank to LORA modules of different layers is not the op-timal choice. It is better to adaptively allocate ranks to LoRA modules of different layers. Existing methods adaptively allocate ranks for LoRA modules from the perspectives of (1) singular value decomposition (SVD); (2) single-rank decomposi-tion (SRD); (3) rank sampling."}, {"title": "3.2.1 SVD-based Methods", "content": "Decomposing a matrix with singular value decom-position (SVD) and selectively truncating its sin-gular values is an effective way to control the rank of the matrix. Inspire by SVD, we can decompose the LoRA parameter matrix \\(BA\\) into an SVD form, i.e, \\(PAQ\\) where P and Q are orthogonal and \\(\\Lambda\\) is a non-negative diagonal matrix. By controlling the elements in \\(\\Lambda\\), we can control the rank of \\(BA\\) and allocate ranks for LoRA modules. Following this idea, several rank allocation methods approx-imate the SVD decomposition for \\(BA\\) and allocate the ranks by filtering the diagonal matrix. For in-stance, AdaLoRA [26] approximates the SVD de-composition by regularizing the orthogonality of P and Q. Then, it drops unimportant singular val-ues based on novel importance scoring methods. Similarly, SaLoRA [27] also introduces an orthog-onality regularization for P and Q; by contrast, it drops unimportant singular values based on the Lo norm. However, the above methods are not effi-cient enough for they start with a high rank and then reduce the rank iteratively, which brings a pre-defined budget [28]. To solve this problem, In-creLoRA [28] proposes to start from a single rank and then automatically increase the rank based on a heuristic importance score, where the orthogo-nality regularization is also involved while the ele-ments in \\(\\Lambda\\) is not required to be non-negative."}, {"title": "3.2.2 SRD-based Methods", "content": "However, the orthogonality regularization brings unignorable computational costs for LoRA and degenerates its efficiency. To address this prob-lem, several methods omit the orthogonality re-quirement of SVD and directly decompose \\(BA\\) into single-rank components. Then, they allo-cate the ranks by selecting the proper components. DORA (Dynamic Low-Rank Adaptation) [29] proposes to decompose the LoRA parameter ma-trix \\(BA\\) into single-rank components and prunes"}, {"title": "3.2.3 Rank Sampling-based Methods", "content": "In the SVD parameterization- and component-wise decomposition-based methods, we need to spend the extra computational costs to search proper ranks. To avoid the extra cost, DyLoRA [33] points out that we can allocate ranks directly by random sampling. In each training step, it samples a value \\(b\\) from a pre-defined discrete distribution and allo-cates \\(b\\) as the rank. Then, the matrices A and B are truncated to rank-\\(b\\). In the fine-tuning procedure, only the parameters on the \\(b\\)-th row of A and \\(b\\)-th column of B are tunable while other parameters are frozen. Besides, the distribution can be defined based on users' preferences."}, {"title": "3.3 Optimizing the Learning Procedure", "content": "In practice, LoRA converges more slowly than full fine-tuning. Moreover, it is also sensitive to hyperparameters and suffers from overfitting. These issues affect LoRA's efficiency and hinder its downstream adaption performance. To address these issues, researchers have developed several approaches to optimize the learning procedure of LoRA, which can be categorized into the follow-ing three types: (1) Initialization Improvement; (2) Gradient Update Optimization; (3) Overfitting Mit-igation."}, {"title": "3.3.1 Initialization Improvement", "content": "LORA usually initializes its parameter matrices A and B using Gaussian noise and zeros respectively. There are two simple schemes: Init[A], which sets matrix B to zero and randomly initializes matrix A, and Init[B], which does the reverse. Literature [34] compares these two schemes and concludes that Init[A] is better through theoretical analysis. It reveals that Init[A] allows using a larger learning rate without causing instability, making the learn-ing process more efficient. However, even with init[A], this random initialization method still re-sults in small initial gradients, leading to slower convergence. To solve this, PiSSA [35] initial-izes LoRA with the principal singular components of the pre-trained matrix. Since principal singular components represent the most significant direc-tions in the matrix, aligning the initial weights with these components can accelerate convergence and improve performance. In contrast, MiLoRA [36] initializes LoRA with the minor singular compo-nents. Given that random initialization of low-rank matrices can interfere with the important features learned in the pre-trained matrix, it reduces this interference to improve overall performance while adapting to new tasks."}, {"title": "3.3.2 Gradient Update Optimization", "content": "To further enhance the convergence and reliability of LoRA, several studies have proposed improve-ments from the perspective of gradient updates. [37] introduces a scaled gradient method based on Riemannian optimization, which incorporates an"}, {"title": "3.3.3 Overfitting Mitigation", "content": "Although LoRA effectively reduces the number of trainable parameters compared to full fine-tuning, some studies have shown that LoRA is also prone to overfitting [45], which contradicts previ-ous views. To address this issue, BiLoRA [43] adopts a bi-level optimization strategy. It alter-nately trains the singular vectors and singular val-ues of the low-rank increment matrix on differ-ent subsets of the training data. This approach avoids the simultaneous optimization of parame-ters at different levels on a single dataset, thus mit-igating overfitting. In addition, literature [44] ap-"}, {"title": "3.4 Combining with other Learning Paradigms", "content": "LORA is compatible with other learning paradigms, such as Bayesian Learning, In-context Learning and Active Learning. Combining LoRA with these learning paradigms can address several problems that hurt the downstream adap-tation performance. For example, combining with Bayesian Learning, Laplace-LoRA [46] can relieve the overconfidence phenomenon that happened in downstream adaptation. Combining with In-context Learning, PILLOW [47] aims to solve the low-resource dilemmas existing in some downstream tasks. Combining with Active Learning, STAR [48] can effectively improve the data efficiency."}, {"title": "4 Cross-task Generalization", "content": "LoRA's pluggable nature enables users to accu-mulate LoRA plugins for different tasks. For ex-ample, on Hugging Face 1), there are more than 300 LoRA plugins compatible with Flan-T5 for different tasks. These accumulated LoRA plug-ins can not only be utilized independently but also be mixed to achieve cross-task generalization [58]. Mixing multiple LoRA plugins together, namely LoRA mixture, has been widely applied in areas requiring cross-task generalization, such as multi-task learning, domain adaptation, and continual learning. Existing LoRA mixture methods can be categorized into (1) mixture with manually de-"}, {"title": "4.1 Mixture with Manually Designed Weights", "content": "Early LoRA mixture methods attempt to linearly combine different LoRA plugins with manually de-signed weights. Some research demonstrates that we can achieve proper cross-task generalization ability by simply averaging the plugins or their re-lated outputs [49-51]. Furthermore, several meth-ods have been proposed to further improve the performance of the LoRA mixture via adopting manually designed weights. For example, Con-trolPE [52], [53] and [54] set the weight factors as hyperparameters, and ControlPE uses hyperpa-rameter search to determine the optimal combina-tion of two LoRA plugins. Additionally, Token-level Adaptation [55] utilizes cosine similarity be-tween the input feature and the adapter dataset cen-ter as weight factors, while BYOM [56] applies ba-sic model fusion methods such as Task Arithmetic, Fisher-Merging, and RegMean.\nMixture with manually designed weights can quickly mix multiple LoRAs without extra train-ing, which demonstrates simplicity and computa-tional efficiency. However, it often fails to find the optimal weights, leading to unstable perfor-mance and limited generalization. Subsequently, researchers have explored using learning-based methods to achieve more precise and adaptive mix-tures."}, {"title": "4.2 Mixture with Learnt Weights", "content": "To learn the optimal mixture weights, several meth-ods have been proposed at task level, instance level and token level to meet different needs. Task-level methods focus on enhancing task transferability, which can be either gradient-based, such as [57], or gradient-free, as seen in LoRAHub [58]. Lo-RAHub employs a black-box algorithm named CMA-ES [209] to optimize weight factors for LoRA plugins, simplifying the training process. Later, ComPEFT [59] and L-LoRA [60] use Lo-RAHub to mix quantized LoRA plugins, further improving computational efficiency.\nCompared to task-level methods, instance-level and token-level methods can provide flexibility and precision for complex inputs. For multimodal instruction tuning, MixLoRA [61] dynamically"}, {"title": "4.3 Mixture of LoRA Experts", "content": "When the LoRA plugins are trainable, we can jointly learn the mixture weights and the LoRA plugins, which can further improve the perfor-mance of the LoRA mixture. To jointly learn the mixture weights and LoRA plugins, Mixture of LORA Experts (LoRA MoE) is a natural choice, where each LoRA plugin acts as an expert, while a router network typically assigns the mixture weights. LoRA MoE has been proven to be ef-fective in many tasks, such as continual learning [63, 64], vision-language tasks [65] and multi-task medical applications [66].\nExisting methods improve the performance of LORA MoE from the perspectives of initialization, task relationship management and efficiency. For initialization, Mixture-of-LoRAs [67] first trains multiple LoRAs separately as initialization and then optimizes the router and LoRAs jointly. MultiLoRA [68] proposes refining the initialization to reduce parameter dependency, which can yield more balanced unitary subspaces. As for task bal-ance, MLoRE [69] adds a low-rank convolution path in the MoE structure to capture global task relationships. MTLORA [70] adopts both task-agnostic and task-specific LoRA modules to ad-dress task conflicts. For efficiency, MoLA [71]"}, {"title": "5 Efficiency Improving", "content": "With the popularization of LLMs, the demand for training and running LoRA plugins increases rapidly. This increasing demand brings an unig-norable computational burden; thus, for LoRA, the smaller, the faster, the better. To meet this demand, existing methods improve the computational effi-ciency of LoRA from the perspectives of (1) pa-rameter reduction; (2) parameter quantization; (3) parallel LoRA computing frameworks."}, {"title": "5.1 Parameter Reduction", "content": "LoRA significantly reduces the number of tunable parameters for fine-tuning LLMs. However, it still requires expensive activation memory to update low-rank matrices. To further reduce the memory cost, existing methods reduce the number of tun-able parameters of LoRA via parameter freezing, parameter pruning, and parameter sharing."}, {"title": "5.1.1 Parameter Freezing", "content": "Parameter freezing methods reduce the number of tunable parameters for LoRA via freezing some of its parameters. They can be divided into two categories: intra-parameter methods and extra-parameter methods.\nThe intra-parameter methods tune a subset of parameters of LoRA while freezing the others. LORA-SP [77] randomly selects half of the LORA parameters to freeze during fine-tuning. LoRA-FA [78]freezes the down-projection weights and updates the up-projection weights in each layer of LORA. AFLORA [79] constructs a low-rank train-able path and gradually freezes parameters during training LoRA. Additionally, DropBP [80] accel-erates the training process by randomly dropping some LoRA gradient calculations during back-propagation.\nBy contrast, the extra-parameter methods intro-duce and tune a set of extra parameters while freezing the original parameters of LoRA. Most of them are proposed based on Singular Value De-composition(SVD). LoRA-XS [81] adds a small \\(r\\times r\\) weight matrix between frozen LoRA matri-ces, which are constructed using the SVD of the original weight matrix; then it tunes only the \\(r\\times r\\) weight matrices in fine-tuning. Similarly, BYOM-LoRA [56] adopts SVD to compress LoRA matri-ces for multi-task models."}, {"title": "5.1.2 Parameter Pruning", "content": "Parameter pruning methods aim to remove unim-portant LoRA parameters during training and in-ference. They prune parameters by either prun-ing LoRA independently or jointly pruning LoRA and the LLM. LoRA-drop [82] uses the output of LoRA at each layer to evaluate the importance of parameters and prune the unimportant parame-ters. By contrast, LoRAPrune [83] jointly pruning LORA matrices and the LLM parameters based on LoRA's gradients. Besides, we can also use LORA to support parameters pruning for LLMs [84,85]."}, {"title": "5.1.3 Parameter Sharing", "content": "Parameter-sharing methods reduce the number of parameters by sharing parameters across differ-ent layers or modules of LLMs. VeRA [86] and VB-LORA [87] are two representative parameter-sharing methods for LoRA. Specifically, VeRA proposes to share a pair of frozen random matri-ces across all layers and conduct layer-wise adapta-tion with \"scaling vectors\". By contrast, VB-LORA proposes a \u201cdivide-and-share\u201d paradigm, which di-vides LoRA's low-rank decomposition by a rank-one decomposition and achieves global sharing based on an admixture model."}, {"title": "5.2 Parameter Quantization", "content": "Quantization, which reduces the bit width of pa-rameters (e.g., from 32-bit floats to 4-bit integers), can be used to reduce the memory and computa-tional cost of LoRA. Existing quantization-aware LORA methods consist of post-training quantiza-tion (PTQ)-based methods and quantization-aware training (QAT)-based methods [92]."}, {"title": "5.2.1 PTQ-based methods", "content": "In PTQ-based methods, we first quantize an LLM and then fine-tune the quantized model, namely quantization and fine-tuning are sequentially con-ducted. QLORA [88] is the first PTQ-based quantization-aware LoRA method. In the fine-tuning stage, it first quantizes an LLM to 4 bits and"}, {"title": "5.2.2 QAT-based methods", "content": "In QAT-based methods, we jointly quantize and fine-tune an LLM, namely quantization and fine-tuning are simultaneously conducted. These meth-ods can alleviate the quantization discrepancies ob-served in PTQ-based methods. To address the quantization discrepancy of QLoRA, LoftQ [90] alternatively applies quantization and low-rank ap-"}, {"title": "5.3 Parallel LoRA Computing Frameworks", "content": "LoRA's parameter-efficient nature enables us to fine-tune or infer multiple plugins on a single GPU or a GPU cluster, which can save computational re-sources and improve the efficiency of LoRA. This section introduces the parallel fine-tuning and par-allel inference frameworks, respectively."}, {"title": "5.3.1 Parallel Fine-tuning", "content": "Parallelly fine-tuning multiple LoRA plugins on a single GPU can reduce GPU memory usage and improve computation efficiency. ASPEN [93] pro-poses a high-throughput parallel finetuning frame-work for LoRA, which consists of a BatchFu-sion approach and an adaptive job scheduling algo-rithm. Specifically, the BatchFusion approach sup-ports parallelly fine-tuning multiple LoRA plugins on a shared LLM by fusing multiple input batches"}, {"title": "5.3.2 Parallel Inference", "content": "Parallel inference framework for LoRA can not only improve the computational efficiency but also support the needs of multi-tenant service. Punica [94", "95": "further optimizes the par-allel inference framework by introducing a uni-fied paging mechanism and a new tensor paral-lelism strategy, which enables the service of thou-sands of concurrent LoRA plugins. Then, based on Punica and S-LoRA, CARASERVE [96"}]}