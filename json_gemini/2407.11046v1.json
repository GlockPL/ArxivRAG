{"title": "A Survey on LoRA of Large Language Models", "authors": ["Yuren MAO", "Yuhang GE", "Yijiang FAN", "Wenyi XU", "Yu MI", "Zhonghao HU", "Yunjun GAO"], "abstract": "Low-Rank Adaptation (LoRA), which updates the dense neural network layers with plug-gable low-rank matrices, is one of the best performed parameter efficient fine-tuning paradigms. Furthermore, it has significant advantages in cross-task generalization and privacy-preserving. Hence, LORA has gained much attention recently, and the number of related literature demonstrates exponential growth. It is necessary to conduct a comprehensive overview of the current progress on LoRA. This survey categorizes and reviews the progress from the perspectives of (1) downstream adaptation improving variants that improve LoRA's performance on downstream tasks; (2) cross-task generalization methods that mix multiple LoRA plugins to achieve cross-task generalization; (3) efficiency-improving methods that boost the computation-efficiency of LoRA; (4) data privacy-preserving methods that use LoRA in federated learning; (5) application. Besides, this survey also discusses the future directions in this field.", "sections": [{"title": "1 Introduction", "content": "Rapidly increasing parameter scales of pre-training language models improves their generalization ability and brings emergent abilities. In the last few years, the parameter scales of pre-training languages models have increased by thousands of times (e.g., from 330M parameter BERT [1] to 540B parameter PaLM [2]). These pre-training language models having large parameter scales are termed Large language models (LLMs). Nevertheless, due to the knowledge boundaries of the LLMs, their abilities on some downstream tasks are still limited. To expand the knowledge boundaries, it remains necessary to fine-tune LLMs on the downstream tasks.\nHowever, fine-tuning the full parameters of an LLM, namely full fine-tuning, is extremely computationally expensive, for example, full fine-tuning of a LLaMA2-7B [3] model requires approximately 60GB of memory, which exceeds the capacity of common consumer GPUs [4]. To reduce the computational cost, various parameter-efficient fine-tuning (PEFT) methods have been proposed [5]. They adapt LLMs to downstream tasks by only fine-tuning a small number of (extra) model parameters. From the perspective of whether extra parameters are involved, PEFT methods can be divided into two categories: extra-parameter methods and intra-parameter methods. The extra-parameter methods freeze all of the original parameters of an LLM and insert a set of learnable parameters to optimize the model input or model layers such as adapter tuning [6] and prompt tuning [7]. By contrast, intra-parameter methods freeze most of the original parameters of an LLM and only tune a small number of parameters of the LLM such as BitFit [8], LISA [4] and LoRA [9].\nWhen we do not have access to modify the model architecture, intra-parameter methods are desirable. Among the intra-parameter methods, LoRA is the most widely used one, because it can achieve a comparable or better downstream adaptation performance to the full fine-tuning on a range of downstream tasks [9] and is easy to implement. Besides, there are many variants have been proposed to further improve the downstream adaptation ability of LoRA on more challenging downstream tasks.\nLORA achieves parameter efficiency by updating the dense neural network layers of an LLM with pluggable low-rank matrices. These matrices (a.k.a, LoRA plugins) are independent of the LLM, which can be stored and reused in other related downstream tasks. Furthermore, these LoRA plugins can be combined to achieve cross-task generalization, which can facilitate multi-task learning, domain adaptation, and continual learning for LLMs.\nAs the LoRA plugins accumulate, the computation cost of managing LoRA plugins is increasing. Although LoRA is computation-efficient, the computational cost of managing a larger number of LoRA plugins is unignorable. It is necessary to further improve the computation efficiency of LoRA. The improvement can come from reducing the computation cost of single LoRA plugins and accelerating the scalable serving of multiple plugins. It can boost the application of LoRA in real-world use cases, such as Generative-as-a-Service (GaaS) cloud products.\nIn some cases, the training data are privately owned by multiple clients and cannot be centralized. To adapt LLMs with the distributed training data, we can adopt federated learning to protect the data privacy of each client. However, federated learning suffers expensive communication and computation costs. To reduce costs, LoRA is a natural choice. Its parameter-efficient nature helps to reduce the computation cost of each client and the communication cost of sharing parameters across clients. Furthermore, the pluggable feature of LoRA can help preserve the parameter privacy of each client in federated learning. Therefore, LoRA has a great potential for privacy-preserving.\nIn this survey, we give a comprehensive overview of the current progress on LoRA for methods (1) improving downstream adaption performance of LoRA; (2) mixing LoRA plugins to achieve cross-task generalization; (3) boosting the computation-efficiency of LoRA; (4) adopting LoRA in federated learning. Besides, the application of LoRA is briefly introduced. This taxonomy of LoRA-related methods is illustrated in Figure 1. This survey is expected to give comprehensive background"}, {"title": "2 Low-Rank Adaptation (LoRA)", "content": "The Low-dimensional intrinsic dimensionality hypothesis [186] presents that over-parameterized models reside on a low intrinsic dimension, which demonstrates that we can achieve proper learning performance by only updating parameters related to the intrinsic rank. Based on this hypothesis, LoRA [9] proposes to update dense layers in a model with low-rank matrices. It can achieve both parameter- and computational- efficiency. In this section, we first introduce the details of LoRA and then introduce existing works that focus on the theoretical analysis of LoRA. Furthermore, we demonstrate LoRA's efficiency in practice. At last, this section presents that LoRA can be used in other use cases except fine-tuning."}, {"title": "2.1 LORA", "content": "Given a dense neural network layer parameterized by $W_o \\in \\mathbb{R}^{d\\times k}$, to adapt it to a downstream task, we update it with $\\Delta W \\in \\mathbb{R}^{d\\times k}$ and obtain an updated layer parameterized by $W = W_o + \\Delta W$. For full fine-tuning, $\\Delta W$ is computed based on gradients of all the $d\\times k$ parameters for the layer, which is computationally expensive and requires a large amount of GPU memory for LLMs. To improve the computational efficiency, LoRA decomposes $\\Delta W$ into two small matrices $B \\in \\mathbb{R}^{d\\times r}$ and $A \\in \\mathbb{R}^{r\\times k}$, i.e.,\n$W = W_o + \\alpha BA$  (1)\nwhere $r < min{d,k}$, $B$ and $A$ are initialized with a random Gaussian distribution and zero respectively, $\\alpha$ represents the scaling factor that controls the strength of updates. The parameter number of LORA is $r\\times (d + k)$, which is significantly less than $d\\times k$. Figure 2 (a) and (b) compare the structures of full fine-tuning and LoRA.\nLoRA is highly parameter efficient for it updates only a small subset of model parameters, which reduces the memory and computational requirements for fine-tuning without increasing inference latency [187]. Furthermore, The parameter efficiency can be further improved by extending from the low-rank matrix to low-rank tensor [188] or combining with the Kronecker decomposition [189, 190]. Except for parameter efficiency, LoRA is also pluggable for the LoRA parameters that can be separated from the model after training. The pluggable character of LoRA enables it to be shared and reused by multiple users [191]. When we have LoRA plugins for multiple tasks, we can combine these plugins and expect a proper cross-task generalization performance [58]. Besides, the low-rank mechanism of LORA is compatible with other parameter-efficient methods, such as adapter [192, 193].\nIn practice, for a Transformer-based LLM, the dense layers typically consist of two types of weight matrices: the projection matrices in attention modules and feed-forward neural (FFN) modules. In the original study, LoRA is applied to the"}, {"title": "2.2 Theoretical Analysis", "content": "To understand why LoRA is effective and how LoRA can be more effective, several works have provided theoretical analyses from various aspects. To answer the question that why LoRA is effective, Malladi et al. [10] analyze the fine-tuning dynamics of LoRA from the kernel view and demonstrate that in the lazy regime, LoRA fine-tuning is nearly equivalent to full fine-tuning. Besides, Zeng et al. [14] provides a theoretical analysis of the LoRA's expressive power for both fully connected neural networks (FNNs) and Transformer networks (TFNs). They proved that for FNNs, LoRA can adapt any model $f$ to accurately represent any smaller target model $f'$ if LoRA-rank $\\ge (width\\ of\\ f) \\times \\frac{depth\\ of\\ f'}{depth\\ off}$, under a mild assumption. Moreover, they quantify the approximation error when the LoRA-rank falls below this threshold. Regarding TFNs, they showed that any model can be adapted to a target model of equivalent size using a rank-($embedding\\ size$) for LoRA. Additionally, Koubbi et al. [11] utilize the mathematical framework for Transformers established by [195-197] to investigate the how low-rank perturbations in attention parameters affect.\nAs to the question that how LoRA can be more effective, Jang et al. [12] analyze the fine-tuning of LoRA within the neural tangent kernel (NTK) [198] framework, showing that employing a rankr$\\ge \\sqrt{N}$ in LoRA helps to avoid spurious local minima and facilitates the discovery of low-rank solutions that exhibit good generalization. Besides, Zhu et al. [13] observe that the project-down matrix A is utilized for extracting features from the input, while the project-up matrix B employs these features to create the desired output. Based on this observation, they demonstrate that freezing the project-down matrix A while tuning only the project-up matrix B leads to better generalization compared to tuning both matrices, in addition to achieving a 2$\\times$ reduction in parameters."}, {"title": "2.3 Efficiency in Practice", "content": "The computational efficiency of LoRA is significantly higher than that for full fine-tuning. Taking fine-tuning the dense weight matrix of the first FFN layer in LLaMA2-7B as an example, full fine-tuning needs to fine-tune 11,008$\\times$4,096 = 45,088, 768 parameters while LoRA only needs to"}, {"title": "2.4 Beyond Fine-tuning", "content": "Besides fine-tuning, LoRA can be applied to other learning paradigms, such as pre-training [15, 17] and continual training [18]. For pre-training, ReLoRA [15] and MoRA [16] are proposed to use low-rank updates to train high-rank networks; moreover, LTE [17] is proposed to perform parallel training of multiple low-rank heads across computing nodes to minimize the need for frequent synchronization, which facilitates the utilization of LoRA in pre-training. As for continual training, there are several methods have been proposed to address the catastrophic forgetting problem. InfLoRA [18] addresses catastrophic forgetting by reparameterizing pre-trained weights with a minimal set of parameters in a subspace. GS-LORA [19] uses group sparse regularization to automatically select specific LoRA groups while zeroing out others to mitigate catastrophic forgetting effects. I-LORA [20] leverages dual-memory experience replay combined with LoRA parameter interpolation to combat catastrophic forgetting.\nFurthermore, LoRA can be used to overcome the limited context size for LLMs [3,21]. For instance, LongLoRA [3] successfully computaitional efficiently extends the context window of LLaMA2-7B [199] from 4k to 100k tokens by combining LoRA with shifted sparse attention. However, LongLoRA does not match the efficiency of vanilla attention due to chaotic attention head structures and unnecessary information exchange between token"}, {"title": "3 Downstream Adaptation Improving", "content": "Although LoRA can achieve proper adaptation performance on some downstream tasks, there is still a performance gap between LoRA and full fine-tuning on many downstream tasks, such as mathematical reasoning [200-202]. To fill this gap, many methods are proposed to further improve the downstream task adaption performance of LoRA. Typically, existing methods improve the downstream adaptation performance from the following perspectives: (1) breaking the low-rank bottleneck, refer to Figure 2 (c); (2) adaptively allocating the ranks of different LoRA modules, refer to Figure 2 (d); (3) optimizing the learning procedure of LoRA; (4) combining with other learning paradigms. In this section, we introduce these four types of methods respectively."}, {"title": "3.1 Breaking the Low-rank Bottleneck", "content": "The low-rank updates enable LoRA to be parameter efficient; however, it restricts LLMs' ability to memorize downstream knowledge and generalization on downstream tasks [16, 201-204]. This low-rank limitation causes inferior performance of LoRA in knowledge- and skill-intensive domains comparing to full-fine tuning, such as code and math. Experimental study [202] demonstrates that the rank for full fine-tuning is significant (10-100$\\times$) higher than that for LoRA, and increasing the rank of LoRA updation can narrow the performance gap between LoRA and full fine-tuning. To increase the rank of LoRA and improve"}, {"title": "3.1.1 Stacking LoRAs along Fine-tuning", "content": "Matrix rank is subadditive, i.e., $rank(M_1 + M_2) \\le rank(M_1) + rank(M_2)$ for metrices $M_1$ and $M_2$ that have the same size. Based on the subadditivity, we can aggregate multiple LoRA modules together to increase the rank and break the low-rank bottleneck. Following this idea, ReLoRA [15] proposes a merge-and-reinit procedure for LoRA, which periodically merges the LoRA modules to the LLM and then reinitializes the LoRA modules during fine-tuning. It equals stacking multiple LoRA modules along with fine-tuning and can increase the rank of the overall updates. Similarly, COLA [22] proposes another merge-and-reinit method based on Frank-Wolfe algorithm [206]. However, MELORA [23] points out that the merge-and-reinit procedure does not necessarily guarantee an increase in rank, because there can be overlap between the series of LORA modules along fine-tuning. To solve this problem, MELORA proposes to decompose the LoRA modules into smaller mini LoRAs and then parallelly stack these mini LoRAs, whose effectiveness in increasing the rank is theoretically verified."}, {"title": "3.1.2 Updating as Gradient Compressor", "content": "The above methods break the low-rank bottleneck in the parameter space. As a supplement, FLORA [24] finds that LoRA performs a fixed random projection to compress gradients and restricts the total weight matrix change to low-rank."}, {"title": "3.1.3 Co-updating LLM and LoRA", "content": "The above two kinds of methods focus on improving the representation ability of LoRA itself. Different from them, Delta-LoRA [25] proposes to jointly update the LLM and LoRA modules, which directly updates the high-rank LLM and can gain better representations capable than updating LoRA independently. Delta-LoRA updates the LLM based on the difference between two LoRA modules of two consecutive iterations, which enables it to update the LLM without any extra memory."}, {"title": "3.2 Dynamic Rank Allocation", "content": "For the rank of LoRA, higher is not always better. The abundant LoRA ranks may cause degeneration in both performance and efficiency. Furthermore, the importance of weights can vary across different layers of a Transformer model during fine-tuning, requiring different ranks for each layer. [26, 29, 31, 207]. Therefore, assigning the same rank to LORA modules of different layers is not the optimal choice. It is better to adaptively allocate ranks to LoRA modules of different layers. Existing methods adaptively allocate ranks for LoRA modules from the perspectives of (1) singular value decomposition (SVD); (2) single-rank decomposition (SRD); (3) rank sampling."}, {"title": "3.2.1 SVD-based Methods", "content": "Decomposing a matrix with singular value decomposition (SVD) and selectively truncating its singular values is an effective way to control the rank of the matrix. Inspire by SVD, we can decompose the LoRA parameter matrix BA into an SVD form, i.e, PAQ where P and Q are orthogonal and A is a non-negative diagonal matrix. By controlling the elements in A, we can control the rank of BA and allocate ranks for LoRA modules. Following this idea, several rank allocation methods approximate the SVD decomposition for BA and allocate the ranks by filtering the diagonal matrix. For instance, AdaLoRA [26] approximates the SVD decomposition by regularizing the orthogonality of P and Q. Then, it drops unimportant singular values based on novel importance scoring methods. Similarly, SaLoRA [27] also introduces an orthogonality regularization for P and Q; by contrast, it drops unimportant singular values based on the Lo norm. However, the above methods are not efficient enough for they start with a high rank and then reduce the rank iteratively, which brings a predefined budget [28]. To solve this problem, IncreLoRA [28] proposes to start from a single rank and then automatically increase the rank based on a heuristic importance score, where the orthogonality regularization is also involved while the elements in A is not required to be non-negative."}, {"title": "3.2.2 SRD-based Methods", "content": "However, the orthogonality regularization brings unignorable computational costs for LoRA and degenerates its efficiency. To address this problem, several methods omit the orthogonality requirement of SVD and directly decompose BA into single-rank components. Then, they allocate the ranks by selecting the proper components. DORA (Dynamic Low-Rank Adaptation) [29] proposes to decompose the LoRA parameter matrix BA into single-rank components and prunes"}, {"title": "3.2.3 Rank Sampling-based Methods", "content": "In the SVD parameterization- and component-wise decomposition-based methods, we need to spend the extra computational costs to search proper ranks. To avoid the extra cost, DyLoRA [33] points out that we can allocate ranks directly by random sampling. In each training step, it samples a value b from a pre-defined discrete distribution and allocates b as the rank. Then, the matrices A and B are truncated to rank-b. In the fine-tuning procedure, only the parameters on the b-th row of A and b-th column of B are tunable while other parameters are frozen. Besides, the distribution can be defined based on users' preferences."}, {"title": "3.3 Optimizing the Learning Procedure", "content": "In practice, LoRA converges more slowly than full fine-tuning. Moreover, it is also sensitive to hyperparameters and suffers from overfitting. These issues affect LoRA's efficiency and hinder its downstream adaption performance. To address these issues, researchers have developed several approaches to optimize the learning procedure of"}, {"title": "3.3.1 Initialization Improvement", "content": "LORA usually initializes its parameter matrices A and B using Gaussian noise and zeros respectively. There are two simple schemes: Init[A], which sets matrix B to zero and randomly initializes matrix A, and Init[B], which does the reverse. Literature [34] compares these two schemes and concludes that Init[A] is better through theoretical analysis. It reveals that Init[A] allows using a larger learning rate without causing instability, making the learning process more efficient. However, even with init[A], this random initialization method still results in small initial gradients, leading to slower convergence. To solve this, PiSSA [35] initializes LoRA with the principal singular components of the pre-trained matrix. Since principal singular components represent the most significant directions in the matrix, aligning the initial weights with these components can accelerate convergence and improve performance. In contrast, MiLoRA [36] initializes LoRA with the minor singular components. Given that random initialization of low-rank matrices can interfere with the important features learned in the pre-trained matrix, it reduces this interference to improve overall performance while adapting to new tasks."}, {"title": "3.3.2 Gradient Update Optimization", "content": "To further enhance the convergence and reliability of LoRA, several studies have proposed improvements from the perspective of gradient updates. [37] introduces a scaled gradient method based on Riemannian optimization, which incorporates an"}, {"title": "3.3.3 Overfitting Mitigation", "content": "Although LoRA effectively reduces the number of trainable parameters compared to full fine-tuning, some studies have shown that LoRA is also prone to overfitting [45], which contradicts previous views. To address this issue, BiLoRA [43] adopts a bi-level optimization strategy. It alternately trains the singular vectors and singular values of the low-rank increment matrix on different subsets of the training data. This approach avoids the simultaneous optimization of parameters at different levels on a single dataset, thus mitigating overfitting. In addition, literature [44] applies"}, {"title": "3.4 Combining with other Learning Paradigms", "content": "LORA is compatible with other learning paradigms, such as Bayesian Learning, In-context Learning and Active Learning. Combining LoRA with these learning paradigms can address several problems that hurt the downstream adaptation performance. For example, combining with Bayesian Learning, Laplace-LoRA [46] can relieve the overconfidence phenomenon that happened in downstream adaptation. Combining with In-context Learning, PILLOW [47] aims to solve the low-resource dilemmas existing in some downstream tasks. Combining with Active Learning, STAR [48] can effectively improve the data efficiency."}, {"title": "4 Cross-task Generalization", "content": "LoRA's pluggable nature enables users to accumulate LoRA plugins for different tasks. For example, on Hugging Face 1), there are more than 300 LoRA plugins compatible with Flan-T5 for different tasks. These accumulated LoRA plugins can not only be utilized independently but also be mixed to achieve cross-task generalization [58]. Mixing multiple LoRA plugins together, namely LoRA mixture, has been widely applied in areas requiring cross-task generalization, such as multi-task learning, domain adaptation, and continual learning. Existing LoRA mixture methods can be categorized into (1) mixture with manually designed weights; (2) mixture with learnt weights; (3)"}, {"title": "4.1 Mixture with Manually Designed Weights", "content": "Early LoRA mixture methods attempt to linearly combine different LoRA plugins with manually designed weights. Some research demonstrates that we can achieve proper cross-task generalization ability by simply averaging the plugins or their related outputs [49-51]. Furthermore, several methods have been proposed to further improve the performance of the LoRA mixture via adopting manually designed weights. For example, ControlPE [52], [53] and [54] set the weight factors as hyperparameters, and ControlPE uses hyperparameter search to determine the optimal combination of two LoRA plugins. Additionally, Token-level Adaptation [55] utilizes cosine similarity between the input feature and the adapter dataset center as weight factors, while BYOM [56] applies basic model fusion methods such as Task Arithmetic, Fisher-Merging, and RegMean.\nMixture with manually designed weights can quickly mix multiple LoRAs without extra train-"}, {"title": "4.2 Mixture with Learnt Weights", "content": "To learn the optimal mixture weights, several methods have been proposed at task level, instance level and token level to meet different needs. Task-level methods focus on enhancing task transferability, which can be either gradient-based, such as [57], or gradient-free, as seen in LoRAHub [58]. LoRAHub employs a black-box algorithm named CMA-ES [209] to optimize weight factors for LoRA plugins, simplifying the training process. Later, ComPEFT [59] and L-LoRA [60] use Lo-RAHub to mix quantized LoRA plugins, further improving computational efficiency.\nCompared to task-level methods, instance-level and token-level methods can provide flexibility and precision for complex inputs. For multimodal instruction tuning, MixLoRA [61] dynamically"}, {"title": "4.3 Mixture of LoRA Experts", "content": "When the LoRA plugins are trainable, we can jointly learn the mixture weights and the LoRA plugins, which can further improve the performance of the LoRA mixture. To jointly learn the mixture weights and LoRA plugins, Mixture of LORA Experts (LoRA MoE) is a natural choice, where each LoRA plugin acts as an expert, while a router network typically assigns the mixture weights. LoRA MoE has been proven to be effective in many tasks, such as continual learning [63, 64], vision-language tasks [65] and multi-task medical applications [66].\nExisting methods improve the performance of LORA MoE from the perspectives of initialization, task relationship management and efficiency. For initialization, Mixture-of-LoRAs [67] first trains multiple LoRAs separately as initialization and then optimizes the router and LoRAs jointly. MultiLoRA [68] proposes refining the initialization to reduce parameter dependency, which can yield more balanced unitary subspaces. As for task balance, MLoRE [69] adds a low-rank convolution path in the MoE structure to capture global task relationships. MTLORA [70] adopts both task-agnostic and task-specific LoRA modules to address task conflicts. For efficiency, MoLA [71]"}, {"title": "5 Efficiency Improving", "content": "With the popularization of LLMs, the demand for training and running LoRA plugins increases rapidly. This increasing demand brings an unignorable computational burden; thus, for LoRA, the smaller, the faster, the better. To meet this demand, existing methods improve the computational efficiency of LoRA from the perspectives of (1) parameter reduction; (2) parameter quantization; (3) parallel LoRA computing frameworks."}, {"title": "5.1 Parameter Reduction", "content": "LoRA significantly reduces the number of tunable parameters for fine-tuning LLMs. However, it still requires expensive activation memory to update low-rank matrices. To further reduce the memory cost, existing methods reduce the number of tunable parameters of LoRA via parameter freezing, parameter pruning, and parameter sharing."}, {"title": "5.1.1 Parameter Freezing", "content": "Parameter freezing methods reduce the number of tunable parameters for LoRA via freezing some of its parameters. They can be divided into two categories: intra-parameter methods and extra-parameter methods.\nThe intra-parameter methods tune a subset of parameters of LoRA while freezing the others. LORA-SP [77] randomly selects half of the LORA parameters to freeze during fine-tuning. LoRA-FA [78]freezes the down-projection weights and updates the up-projection weights in each layer of LORA. AFLORA [79] constructs a low-rank trainable path and gradually freezes parameters during training LoRA. Additionally, DropBP [80] accelerates the training process by randomly dropping some LoRA gradient calculations during back-propagation.\nBy contrast, the extra-parameter methods introduce and tune a set of extra parameters while freezing the original parameters of LoRA. Most of them are proposed based on Singular Value Decomposition(SVD). LoRA-XS [81] adds a small r$\\times$r weight matrix between frozen LoRA matrices, which are constructed using the SVD of the original weight matrix; then it tunes only the r$\\times$ r weight matrices in fine-tuning. Similarly, BYOM-LoRA [56] adopts SVD to compress LoRA matrices for multi-task models."}, {"title": "5.1.2 Parameter Pruning", "content": "Parameter pruning methods aim to remove unimportant LoRA parameters during training and inference. They prune parameters by either pruning LoRA independently or jointly pruning LoRA and the LLM. LoRA-drop [82] uses the output"}, {"title": "5.1.3 Parameter Sharing", "content": "Parameter-sharing methods reduce the number of parameters by sharing parameters across different layers or modules of LLMs. VeRA [86] and VB-LORA [87] are two representative parameter-sharing methods for LoRA. Specifically, VeRA proposes to share a pair of frozen random matrices across all layers and conduct layer-wise adaptation with \"scaling vectors\". By contrast, VB-LORA proposes a \u201cdivide-and-share\u201d paradigm, which divides LoRA's low-rank decomposition by a rank-one decomposition and achieves global sharing based on an admixture model."}, {"title": "5.2 Parameter Quantization", "content": "Quantization, which reduces the bit width of parameters (e.g., from 32-bit floats to 4-bit integers), can be used to reduce the memory and computational cost of LoRA. Existing quantization-aware LORA methods consist of post-training quantization (PTQ)-based methods and quantization-aware training (QAT)-based methods [92]."}, {"title": "5.2.1 PTQ-based methods", "content": "In PTQ-based methods, we first quantize an LLM and then fine-tune the quantized model, namely quantization and fine-tuning are sequentially conducted. QLORA [88] is the first PTQ-based quantization-aware LoRA method. In the fine-tuning stage, it first quantizes an LLM to 4 bits and"}, {"title": "5.2.2 QAT-based methods", "content": "In QAT-based methods, we jointly quantize and fine-tune an LLM, namely quantization and fine-tuning are simultaneously conducted. These methods can alleviate the quantization discrepancies observed in PTQ-based methods. To address the quantization discrepancy of QLoRA, LoftQ [90] alternatively applies quantization and low-rank ap-"}, {"title": "5.3 Parallel LoRA Computing Frameworks", "content": "LoRA's parameter-efficient nature enables us to fine-tune or infer multiple plugins on a single GPU or a GPU cluster, which can save computational resources and improve the efficiency of LoRA. This section introduces the parallel fine-tuning and parallel inference frameworks, respectively."}, {"title": "5.3.1 Parallel Fine-tuning", "content": "Parallelly fine-tuning multiple LoRA plugins on a single GPU can reduce GPU memory usage and improve computation efficiency. ASPEN [93] proposes a high-throughput parallel finetuning framework for LoRA, which consists of a BatchFusion approach and an adaptive job scheduling algorithm. Specifically, the BatchFusion approach sup-ports parallelly fine-tuning multiple LoRA plugins on a shared LLM by fusing multiple input batches"}, {"title": "5.3.2 Parallel Inference", "content": "Parallel inference framework for LoRA can not only improve the computational efficiency but also support the needs of multi-tenant service. Punica [94] uses a new CUDA kernel design to batch GPU operations for different LoRA plugins. Based on Punica, S-LoRA [95] further optimizes the parallel inference framework by introducing a unified paging mechanism and a new tensor parallelism strategy, which enables the service of thousands of concurrent LoRA plugins. Then, based on Punica and S-LoRA, CARASERVE [96] reduces the cold-start overhead and further improves the service efficiency and SLO (service-level objective) attainment rates by CPU-GPU cooperation and rank-aware scheduling."}, {"title": "6 LORA for Federated Learning", "content": "When adapting LLMs to vertical domains such as medicine and finance, the available training data can be privately owned by multiple clients. In this scenario, the training data is not centralized, and we have to fine-tune LLMs while keeping the data localized, namely federated learning. In federated learning, the clients typically compute weight updates locally and then share these updates with others to globally update the LLM. It brings both communication and computation costs for the clients. Fortunately, LoRA is parameter efficient and pluggable, which can reduce communication costs and lower computational resource requirements. LoRA can enhance the overall efficiency and scalability of federated learning."}, {"title": "6.1 Data Heterogeneity", "content": "Data heterogeneity refers to differences in data distribution across clients. In federated learning, different clients usually have different data distributions. The inconsistency in data distribution affects the overall performance of the model. Research reveals that in federated learning, as user data becomes more diverse, the performance gap between LORA and full fine-tuning widens [97]. To address this issue, researchers have proposed several improvement methods.\nSLORA [97] introduces a data-driven initialization method for LoRA. It first performs sparse feder-ated fine-tuning before applying LoRA and then performs SVD to decompose the accumulated gradient updates into low-rank matrices for LoRA initialization. The goal is to enable the LoRA modules to better adapt to the data distribution of each client, thereby integrating these heterogeneous data characteristics into the global model more effectively. FeDeRA [98] uses a simpler initialization method. It directly applies SVD to pre-trained weights to initialize LoRA. Retaining the principal components of the pre-trained weights aligns the direction and magnitude of weight updates across different clients to handle data heterogeneity. Additionally, FFA-LORA [99] freezes one low-rank"}, {"title": "6.2 Device Heterogeneity", "content": "Device heterogeneity refers to the differences in hardware capabilities, and network connectivity among clients participating in federated learning. Traditional federated learning methods often encounter the \u201cbuckets effect\u201d, implying that the system's overall performance is limited by the capability of the least powerful client. Specifically, these methods use the smallest LoRA rank to accommodate all clients, which prevents many resource-rich clients from fully utilizing their potential.\nTo address this issue, a dynamic parameter allocation strategy can be adopted. FedMS [100] dynamically adjusts the number of activated LoRA matrices based on the real-time computational resources of clients. FlexLoRA [101] uses a dynamic parameter allocation strategy. It adjusts the LORA rank and redistributes the SVD components of the global LoRA weights based on resource constraints. Similarly, HETLORA [102] assigns different ranks for different clients. However, it performs weighted aggregation according to the sparsity of the updates from different clients, balancing"}, {"title": "6.3 Model Heterogeneity", "content": "Model heterogeneity indicates differences in model structures among clients. In traditional federated learning, clients use local models with the same architecture, allowing their parameters to be aggregated into a global model on the server. However, in practice, clients may prefer unique local model architectures due to personal needs and often do not want to disclose model details. Thus, it is necessary to transfer knowledge between heterogeneous models without sharing private data or revealing local model structures [210].\nPrevious work has used knowledge distillation, model ensembling, and mutual learning to address model heterogeneity. However, these methods have limitations, such as reliance on public datasets, additional communication costs and poor local model performance. To avoid these limitations, pFedLoRA [103] uses LoRA as a carrier of both global and local knowledge. It adopts an iterative training strategy to facilitate knowledge transfer and integration, enabling knowledge sharing among heterogeneous models across different clients."}, {"title": "6.4 Parameter Privacy", "content": "In federated learning, protecting client-specific parameters is crucial because ensuring the privacy of these parameters also indirectly safeguards client data privacy. As a modular approach to adjusting personalized parameters, LoRA can be effectively integrated into federated learning systems to achieve parameter privacy protection.\nLiterature [104] proposes a secure distributed language model training framework based on model slicing. They deploy LoRA in a Trusted Execution Environment (TEE) and use OTP encryption to transmit features between the GPU and TEE, protecting model parameter privacy. PrivateLoRA [105] introduces a distributed system based on LoRA. It adds a square matrix M between low-rank matrices A and B. The non-trainable matrices A and B, along with most of the pre-trained weights, are deployed on the global server to enhance computation. Meanwhile, the trainable matrix M is stored on the client as personalized parameters, thus ensuring parameter privacy protection."}, {"title": "7 Applications of LoRA", "content": "In the rapidly evolving field of deep learning, LORA has become widely used due to its unique advantages. Researchers utilize LoRA to fine-tune pre-trained models for various downstream tasks, reducing computational resource requirements while enhancing performance. LORA's strong adaptability and efficiency have significantly improved various applications. In this section, we will introduce LoRA's applications in the following scenarios: (1) language tasks; (2) vision tasks; (3) multimodal tasks."}, {"title": "7.1 Language Tasks", "content": "Recently, the rapid development of pre-trained language models, especially LLMs, is revolutionizing the approach to language tasks due to their outstanding performance. However, these pre-trained models are trained on a large amount of general data and still require further fine-tuning on task-specific data to adapt to downstream tasks. Therefore, it is natural to use LoRA to fine-tune these pre-trained language models, as it reduces computational resource requirements. We mainly focus on some representative downstream tasks, which include traditional NLP tasks, code tasks, model alignment and vertical domain tasks."}, {"title": "7.1.1 Traditional NLP Tasks", "content": "Given the strong instruction-following and contextual understanding abilities of LLMs, some researches apply LoRA to fine-tune these models for traditional NLP tasks. For example, LoRA is widely adopted in LLaMA for various tasks, such as emotion recognition [106], text classification [107] and role recognition [108]. AutoRE [109] applies QLORA to three document-level relation extraction tasks, achieving great performance on different LLMs. Some studies [110-112] leverage LoRA from different perspectives to enhance the model's capability in machine translation tasks. Additionally, LoRA can also improve the performance of models like BERT and T5 for text understanding tasks [113, 114]."}, {"title": "7.1.2 Code Tasks", "content": "Some researchs apply LoRA to improve model performance in various code-related tasks. For example, BERT-style models fine-tuned with LoRA are suitable for code-change-related tasks, specifically"}, {"title": "7.1.3 Model Alignment Tasks", "content": "Model alignment tasks focus on adjusting a machine learning model to align with human values and intentions, often using techniques like Reinforcement Learning from Human Feedback (RLHF). To reduce memory requirements of RLHF, some studies use LoRA to fine-tune the reward model and policy model [121-123]. Furthermore, other works improve reward models by integrating multiple LoRA adapters. For example, DMOERM [124] combines MoE with LoRA, routing model inputs to multiple LoRA experts while another work [125] proposes a LoRA-based ensemble method as well. The integration can also benefit the quantification of uncertainty in reward models [126]. Besides, literature [127] applies Laplace-LoRA [128] to train Bayesian reward models, which mitigates reward overoptimization in best-of-n sampling."}, {"title": "7.1.4 Vertical Domain Tasks", "content": "LLMs often perform suboptimally in vertical domains, requiring fine-tuning with domain-specific expertise. Some works apply LoRA to improve the performance of LLMs on domain-specific tasks. For example, some studies fine-tune LLMs on"}, {"title": "7.2 Vision Tasks", "content": "In vision tasks, LoRA is primarily applied to image generation and image segmentation, significantly improving training efficiency and optimizing model performance."}, {"title": "7.2.1 Image Generation", "content": "Image generation tasks hold significant importance in the field of computer vision. In recent years, diffusion model have demonstrated exceptional performance in image generation tasks. LoRA is widely used in diffusion models to address various image generation tasks while reducing computational resources. Some works use LoRA to fine-tune diffusion models for image style transfer [141-145], while others apply it to text-to-image generation [146\u2013150].\nFurthermore, researchers have designed several LoRA-based methods to improve image generation quality. For instance, Smooth Diffusion [151] uses LoRA to achieve smoothness in the latent space, leading to better performance in various image generation and editing tasks. ResAdapter [152] employs LoRA to learn resolution priors, adjusting the receptive fields of con-"}, {"title": "7.3 Multimodal Tasks", "content": "Multimodal Large Language Models (MLLMs) aim to integrate text with various modalities such as audio, image and video, which enable cross-modal understanding and reasoning through a unified embedding space. The success of LoRA in both NLP and vision tasks has sparked considerable interest in applying them to MLLMs.\nIn MLLMs, LoRA can not only improve training efficiency but also facilitate effective modality alignment. In audio-text tasks, SALM [179] comprises LoRA layers, a frozen text-based LLM, an audio encoder and a modality adapter to handle speech inputs and corresponding task instructions. For image-text tasks, InternLM-XComposer2 [180] achieves modality alignment by applying LoRA to image tokens, mPLUG-Owl [181] freezes the visual module while jointly fine-tuning LoRA and abstractor of the text module, and CoLLaVO [182] employs QLoRA to preserve object-level image understanding. In the realm of video-text tasks, VSP-LLM [183] fine-tunes the text module with QLoRA for visual speech processing, MolCA [184] uses LoRA to understand 2D molecular graphs and text, while TPLLM [185] employs LoRA for efficient traffic prediction by integrating sequence and spatial features. These applications demonstrate the versatility and power of LORA in MLLMs tasks."}, {"title": "8 Conclusion and Future Direction", "content": "In this survey, the recent progress of LoRA have been systematically reviewed from the perspective of downstream adaptation improving, cross-task generalization, efficiency improving, federated learning and applications. From this review,"}, {"title": "8.1 LORA for GaaS", "content": "In Generative-as-a-Service (GaaS), cloud-based platforms provide users with generative artificial intelligence (AGI) services. GaaS enables users enjoy AGI without deploying local computational resources. For the users' needs are diverse, it is necessary to provides various functions for GaaS. To implement the various functions, we can construct a LoRA plugin for each function. The pramameter efficiency and plugability of LoRA can facilitate efficient functions' construction and execution. Besides, the services on GaaS platforms can change rapidly alonging time. To follow the changes, we can train new LoRA plugins that initialized by combination of previous plugins. The cross-task generalization ability of LoRA can facilitate fast adaption to service updations."}, {"title": "8.2 LORA for Continued Pre-training", "content": "In continued pre-training, a foundation model is continuely trained with unlabeled user data to adapt the model to specific domains. Typically, the self-supervised training objective is same with that for pre-training, and the learning rate is much smaller than than for pre-training. Continued pre-training is a important stage for constructing vertical domain LLMs. However, it is highly computational expensive, which impedes the development of vertical domain LLMs, especailly for the organiza-"}, {"title": "8.3 LORA for Autonomous Agents", "content": "In LLM-based autonomous agents, the agents are assigned with specific roles. Based the roles and environment, agents make actions to response users' or other agents' request. The actions can be made based on self-knowledge or tools that designed for domain-specific tasks. The request and the actions are stored in memory to support the future requests.\nIn the current agents, the roles are typically assigned by prompts; however, prompt may cannot give a comprehensive discription of the role when the role is complex and the number of related data is large. Assiging roles with LoRA plugins training from data related to the roles can be a better choice. Furthermore, the tools for agent can be LoRA plugins. Besides, the memory usually augments the agents with retrieval augmented generation (RAG); however, due to the input token limitation and the short-comings of in-context learning, the RAG-based support may be less effective. By contrast, we can use LoRA-based continual learning to construct memory plugins, which can solve the problem of RAG. Therefore, LoRA-driven agents are worth to explore."}]}