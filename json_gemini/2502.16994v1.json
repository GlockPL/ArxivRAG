{"title": "FADE: Why Bad Descriptions Happen to Good Features", "authors": ["Bruno Puri", "Aakriti Jain", "Elena Golimblevskaia", "Patrick Kahardipraja", "Thomas Wiegand", "Wojciech Samek", "Sebastian Lapuschkin"], "abstract": "Recent advances in mechanistic interpretability have highlighted the potential of automating interpretability pipelines in analyzing the latent representations within LLMs. While they may enhance our understanding of internal mechanisms, the field lacks standardized evaluation methods for assessing the validity of discovered features. We attempt to bridge this gap by introducing FADE: Feature Alignment to Description Evaluation, a scalable model-agnostic framework for evaluating feature-description alignment. FADE evaluates alignment across four key metrics Clarity, Responsiveness, Purity, and Faithfulness and systematically quantifies the causes for the misalignment of feature and their description. We apply FADE to analyze existing open-source feature descriptions, and assess key components of automated interpretability pipelines, aiming to enhance the quality of descriptions. Our findings highlight fundamental challenges in generating feature descriptions, particularly for SAEs as compared to MLP neurons, providing insights into the limitations and future directions of automated interpretability.", "sections": [{"title": "1 Introduction", "content": "Understanding the latent features of machine learning models and aligning their descriptions with human-comprehensible concepts remains a crucial challenge in AI interpretability research. Recent advances have made significant strides in this direction, by introducing automated interpretability methods (Bills et al., 2023; Bykov et al., 2024; Choi et al., 2024), that leverage larger language-capable models to describe the latent representations of smaller models (Bykov et al., 2023; Templeton et al., 2024; Dreyer et al., 2025). This facilitates inspection of ML models, enabling a deeper understanding of models' behaviour. Consequently, this enhances our ability to identify or mitigate harmful responses and biases, thus improving model transparency and interpretability (Lee et al., 2024; Gandikota et al., 2024). A key insight from these investigations is the highly polysemantic nature of individual neurons they rarely correspond to single, clear concepts. This discovery has led to the development and adoption of sparse autoencoders (SAEs) (McGrath et al., 2024; Bricken et al., 2023; Rajamanoharan et al., 2024), which are intended to decompose polysemantic representations by separating neuron activations into more interpretable components. While SAEs offer a promising approach for feature decomposition, their reliability remains an open question. Recent research reveals significant variability in the way SAEs capture the underlying learned features (Heap et al., 2025; Paulo and Belrose, 2025), thus highlighting the need for a holistic framework for the evaluation of feature-description alignment. To the best of our knowledge, there is an absence of widely accepted quantitative metrics for evaluating the quality and effectiveness of open-vocabulary feature descriptions. Different methodologies rely on custom evaluation criteria which makes it challenging to conduct meaningful, generalizable comparisons across techniques. Additionally, existing evaluation approaches typically optimize for a single metric (Bills et al., 2023; Choi et al., 2024) which may not capture the full complexity of a feature's behavior and leaves open questions about whether the model truly encodes the hypothesized concept rather than simply correlating with the measured feature. With our work, we contribute as follows:\n[1] We present a robust automated evaluation framework designed for broad applicability across model architectures and their SAE implementations. FADE combines four metrics that allow quantitative analysis of different aspects"}, {"title": "2 Related Work", "content": "Evaluating the alignment between features and their descriptions has become increasingly important with the rise of automated interpretability approaches. While manually inspecting highly activating examples remains a common method to validate interpretations and demonstrate automated interpretability techniques (Bills et al., 2023; Templeton et al., 2024), more scalable tools are needed for thorough quantitative evaluation. Many automated or semi-automated approaches have been proposed, generally falling into activation-centric and output-centric methods.\nActivation-centric methods focus on measuring how well a feature's activations correspond to its assigned description.\nOne prominent approach is a simulation-based scoring, where an LLM predicts feature activations based on the description and input data, and the correlation between predicted and real activations of a feature is measured (Bills et al., 2023; Bricken et al., 2023; Choi et al., 2024). While elegant, this approach can be computationally expensive and tends to favor broad, high-level explanations.\nA related and conceptually more straightforward way to measure how well the description explains a feature's behavior is to try to directly generate synthetic samples using the description and compare the resulting activations between concept and non-concept samples (Huang et al., 2023; Kopf et al., 2024; Gur-Arieh et al., 2025; Shaham et al., 2025). However, generated datasets are typically small (on the order of 5-20 samples (Huang et al., 2023; Gur-Arieh et al., 2025)) and often constrained to rigid syntactic structures or focus only on the occurence of particular tokens, making them less effective for evaluating abstract or open-ended language concepts (Huang et al., 2023; Foote et al., 2023).\nAnother strategy is rating individual samples from a natural dataset for how strongly they express a concept and compare those ratings to the feature's activations (Huang et al., 2023; Paulo et al., 2024; Templeton et al., 2024).\nA common limitation of activation-centric methods is that they primarily evaluate positively correlated activations while ignoring negatively encoded neurons, effectively ignoring negatively encoded features (Huang et al., 2023; Kopf et al., 2024).\nOutput-centric methods instead assess how feature activations influence model behavior. Some approaches measure the general decrease in performance of the model after ablating the feature (Bills et al., 2023; Makelov et al., 2024), while others use steering-based interventions, where an increase in generated outputs containing the concept is used as a proxy for feature alignment (Paulo et al., 2024;"}, {"title": "3 Evaluating Feature Explanations", "content": "Our primary objective is to establish a comprehensive framework that automatically evaluates feature descriptions across a variety of feature types without human intervention. Our framework encompasses four distinct metrics: Clarity, Responsiveness, Purity, and Faithfulness, which we consider necessary and sufficient for assessing the alignment between a feature and its description. In our opinion, such a comprehensive evaluation framework is necessary to ensure that features encode the ascribed concept in a robust way. As feature descriptions are often generated by optimizing for a single metric, such as maximizing the activations of specific neurons, they do not necessarily generalize well to other quantifiable aspects, such as faithfulness (Bills et al., 2023; Choi et al., 2024).\nWe base our approach on four key assumptions. First, we adopt a \u2460 Binary Concept Expression model, whereby a concept is either present in a text sequence or absent. Second, we assume \u2461 Concept Sparsity, i.e. that a given concept appears only rarely in natural datasets, though a sufficiently large dataset will contain some representative examples. Third, we assume \u2462 Feature Reactivity, meaning, when a feature encodes a concept, its activations are significantly stronger on samples that express the concept. This will be valid especially for SAEs, since by construction, for most samples, their activations are zero. This is a strong assumption, as it also implies that a feature should activate strongly only for a single concept. Note, however, that this does not require strict monosemanticity (Bricken et al., 2023). In our framework a feature might encode multiple, even entirely unrelated topics, as long as its feature description fully describes all of them. Unlike traditional monosemanticity, which assumes features should directly align with a single human-interpretable category, our framework evaluates interpretability based on whether the feature description accurately reflects the feature's truly encoded concept, rather than enforcing human-aligned conceptual boundaries. Assumption and \u2462 allow us to interpret the activations of a feature as output of a \"classifier\" of the encoded concept, which can then be easily evaluated. For our metrics, we expect a feature to encode the concept linearly in its activations. Finally, we assume \u2463 Causality - a feature is expected to causally influence the model's output so that modifying its activation will lead to predictable changes in the generation of concept-related content. These four assumptions will not always hold but are necessary simplifications for now."}, {"title": "3.1 Evaluation Framework Components", "content": "Our evaluation framework consists of three main components: A subject LLM, that contains the features we want to evaluate, a natural dataset, that ideally should be close to the LLM training data distribution and is sufficiently large to contain all the concepts, of which the descriptions we want to evaluate, and an evaluating LLM, an open- or closed-source LLM that is used for automating the evaluation process. The evaluating LLM is used for \"human-like\" inference tasks, such as rating the strength of concept expression in samples and creating synthetic concept data."}, {"title": "3.2 Evaluation Metrics", "content": "Clarity evaluates whether a feature's description is precise enough to generate strongly activating samples. We assess this by prompting the evaluating LLM to generate synthetic samples based on the feature description (see prompts in Appendix D.2.1). Unlike Gur-Arieh et al. (2025), which generates non-concept sequences artificially, we sample them uniformly from the natural dataset to avoid unnatural biases (i.e. by asking the evaluating LLM not to think about pink elephants). If a feature is well explained by its description, the synthetic concept samples should elicit significantly stronger activations than non-concept samples. We quantify this separability using the absolute Gini coefficient\n$G_{abs}(A_c, A_n) = 2 \\cdot \\frac{\\sum_{a_c \\in A_c} \\sum_{a_n \\in A_n} 1 [a_c > a_n]}{|| A_c ||_0 || A_n ||_0} - 1$ (1)\nwhere Ac and An are the sets of concept and non-concept activations, respectively. Since this metric focuses on linear separability rather than precision, it remains robust even when concept samples occasionally appear within the natural dataset. A low clarity score indicates that either the description is not precise enough to be useful, or might simply be unfitting for the feature, resulting in similar activations for both concept and non-concept samples. For example, in Figure 2, feature (d) responds to \"having something under one's belt,\" yet is inaccurately described as \u201cbelt\u201d. Conversely, a high clarity score confirms that we can effectively generate samples that elicit strong activations in the feature, although it does not guarantee that the feature is monosemantic or causally involved.\nResponsiveness evaluates the difference in activations between concept and non-concept samples. We select samples from the natural dataset based on their activation levels, drawing both from the highest activations and from lower percentiles (details in Appendix D.1). Following an approach similar to Templeton et al. (2024), we prompt the evaluating LLM to rate each sample on a three-point scale to indicate how strongly the concept is present (0 = not expressed, 1 = partially expressed, 2 = clearly expressed). By discarding the ambiguous (partially expressed) cases, we effectively binarize samples into concept and non-concept categories. We compute the responsiveness score again using the absolute Gini coefficient. A low responsiveness score indicates that activations of concept-samples are similarly strong as non-concept samples, while a high score indicates that, in natural data, samples with strong activations reliably contain the concept.\nPurity is computed using the same set of rated natural samples as responsiveness, but with a different focus: it evaluates whether the strong activations are exclusive to the target concept. In contrast to (Huang et al., 2023), who measure recall and precision for a single threshold, we measure the purity using the Average Precision (AP)\n$AP(A_c, A_n) = \\sum (r_j - r_{j-1}) \\cdot p_j$ (2)\nwhere $r_j$ is the recall and $p_j$ is the precision computed at threshold j, for each possible threshold, based on $A_c$ and $A_n$. The AP penalizes instances where non-concept samples also trigger high activations. A purity score near one thus indicates that the feature's activations are highly specific to the concept, whereas a score near zero suggests that"}, {"title": "Faithfulness", "content": "top activations occur for other unrelated concepts as well. This is, for example, the case in polysemanticity, where a feature responds to multiple unrelated concepts.\nFaithfulness addresses the causal relationship between a feature and the model's output. In other words, it tests whether direct manipulation of the feature's activations can steer the model's output toward generating more concept-related content. To evaluate faithfulness, we take random samples from the natural dataset and have the subject LLM generate continuations while applying different modifications to the feature's activation. For neurons, we multiply the raw activations by a range of factors, including negative values, so that we do not impose a directional bias on how the concept is encoded. For SAE features, of which the activations are more sparse, we first determine the maximum activation observed in the natural dataset (Templeton et al., 2024) and then scale this value by the different modification factors. After generating the modified continuations, the evaluating LLM rates how strongly the concept appears in each output. We quantify the strength of this causal influence by measuring the largest increase we were able to steer the model in producing concept-related outputs\n$Faithfulness(R) = \\frac{max(max(R) - R_0, 0)}{1 - R_0}$ (3)\nwhere R is a vector capturing the proportion of concept-related outputs for each modification factor, and Ro denotes the base case in which the feature is \"zeroed out\u201d (i.e., multiplied by zero). A faithfulness score of zero implies that manipulating the feature does not increase the occurrence of concept-related outputs, while a score of one indicates that for some modification factor the concept is produced in every continuation."}, {"title": "4 Experiments", "content": "In this section, we apply FADE to assess the quality of descriptions generated for various state-of-the-art feature descriptions (Choi et al., 2024; Lieberum et al., 2024). Our goal is to demonstrate that the proposed framework provides a robust, multidimensional measure of feature to feature description alignment.\nExperimental Setup As a natural dataset for the evaluations we use samples drawn from the test partition of the Pile dataset (Gao et al., 2020), preprocessed as shown in Appendix B. As evaluating LLM we use the OpenAI model gpt-4o-mini-2024-07-18 unless stated otherwise. Prompts for the evaluating LLM as well as details on the hyperparameters can be found in Appendix D.2. We run the experiments on 103 randomly chosen features from a single layer of a model: layer 20 for Gemma-2-2b (Riviere et al., 2024), layer 20 of Gemma Scope SAEs (Lieberum et al., 2024), and layer 19 of Llama-3.1-8B-Instruct (Dubey et al., 2024) (see Appendix C.1 for details). The evaluation results have a high variance, which is caused by both the inherent difficulty of interpreting some features as well as the quality of the ascribed feature descriptions. Therefore the mean values are demonstrated only if they aid the analysis of metrics distributions. For all of the presented tables we demonstrate the full distributions as kernel-density estimations with bandwidth adjustment factor 0.3 in Appendix E.1.\nAutomated interpretability approach Feature descriptions, which we refer to as MaxAct*, are generated based on samples of the train partition of the Pile dataset, that demonstrate maximum activation on the feature, similarly to methods utilized in (Bills et al., 2023; Paulo et al., 2024; Rajamanoharan et al., 2024), that we refer to as MaxAct. The minor differences between MaxAct and MaxAct* are prompts, optimized on qualitative analysis provided via FADE, and preprocessing steps of the dataset. The automated interpretability pipeline is described in Appendix C.1."}, {"title": "4.1 Depth and Reliability of Evaluations", "content": "Limitations of single-metric approaches We compare FADE with simulated-activation-based metrics (Bills et al., 2023; Templeton et al., 2024; Choi et al., 2024), that, while computationally efficient, fail to fully capture the feature-to-description alignment, potentially overlooking critical issues like polysemanticity. To illustrate this, we analyze feature descriptions of Llama-3.1-8B-Instruct generated in (Choi et al., 2024). As shown in Figure 3, despite high simulated-activation scores, FADE identifies many features with low purity. Moreover, comparing the average results across all subsampled features with the top 10% of features based on the simulated-activation metric, we find only a marginal gain in clarity and responsiveness, while the purity worsens. Via MaxAct*, we gen-"}, {"title": "5 Conclusion", "content": "In this work, we presented FADE, a new automated evaluation framework designed to rigorously evaluate the alignment between features and their open-vocabulary feature descriptions. By combining four complementary metrics Clarity, Responsiveness, Purity, and Faithfulness, our approach gives a comprehensive assessment of how a feature reacts to instances of the described concept,"}, {"title": "Limitations", "content": "an evaluation of the description itself as well as the feature's causal role in the model's outputs. Through extensive experiments across different feature types, layers, and description generation mechanisms, we demonstrated that methods relying on a single metric (e.g., simulation-based approaches) often give incomplete or misleading feature descriptions. Our framework can be used to highlight both the strengths and weaknesses of existing methods, while it also helps in debugging and improving these methods. We highlighted multiple results for improving the quality of feature explanations, such as using larger, more capable LLMs for the explainer and including more examples in the prompt. We hope that the open-source implementation of \u25c6FADE will drive further research in automated interpretability and help make language models more transparent and safe to use.\nLimitations\nDespite presenting a comprehensive and robust evaluation framework, our work has certain limitations that we want to highlight here: One key limitation is the potential biases in the LLMs used for both rating and synthetic data generation. These biases can affect the evaluation process and perpetuate biases, especially in automated interpretability. For instance, an LLM might recognize a feature encoding a concept in English as directly representing that concept, whereas the same feature in another language might be classified with the additional specification of the language. This discrepancy could lead to unintended biases when steering models based on these interpretations. Similar issues may arise from biases present in the pre-training datasets used in our evaluation procedure. Another limitation is related to the steering behavior in the faithfulness pipeline. Our current implementation does not explicitly verify whether the generated sequences under modification remain grammatically correct and semantically meaningful. However, minor modifications to the prompt could potentially address this issue in the future. Finally, our faithfulness measure is not well-suited for handling inhibitory neurons. A neuron may causally inhibit the presence of a concept in a model's output, but our metric, by design, does not effectively capture decreases in the appearance of sparse concepts. This limitation arises both from the definition of our faithfulness metric and the inherent challenges in measuring such suppression effects for already sparse concepts."}, {"title": "Acknowledgements", "content": "We sincerely thank Melina Zeeb for her valuable assistance in creating the graphics and logo for FADE. This work was supported by the Federal Ministry of Education and Research (BMBF) as grant BIFOLD (01IS18025A, 01IS180371I); the European Union's Horizon Europe research and innovation programme (EU Horizon Europe) as grants [ACHILLES (101189689), TEMA (101093003)]; and the German Research Foundation (DFG) as research unit DeSBi [KI-FOR 5363] (459422098)."}, {"title": "A Extended related work", "content": "A common approach to automatic interpretability includes selecting data samples that strongly activate a given neuron and using these samples, along with their activations as input to a larger LLM, that serves as an explainer model and generates feature descriptions (Bills et al., 2023; Bricken et al., 2023; Choi et al., 2024; Paulo et al., 2024; Rajamanoharan et al., 2024). Previous research has investigated various factors influencing this method, including prompt engineering, the number of data samples used, and the size of the explainer model (see Appendix A).\nBuilding on this approach, (Choi et al., 2024) advanced the method by fine-tuning Llama-3.1-8B-Instruct on the most accurate feature descriptions, as determined by their simulated-activation metric. This fine-tuning aimed to improve the performance and accuracy of description generation, ultimately outperforming GPT-40 mini.\nAn output-centric approach was introduced by (Gur-Arieh et al., 2025) in an attempt to address another key challenge-the feature descriptions generated via the data samples that activate the feature the most, often fail to reflect its influence on the model's output. The study demonstrates that a combined approach, integrating both activation-based and output-based data, results in more accurate feature descriptions and improves performance in causality evaluations.\nSeveral studies perform their experiments exclusively on SAEs (McGrath et al., 2024; Paulo et al., 2024; Rajamanoharan et al., 2024; Templeton et al., 2024), while others focus on MLP neurons (Bills et al., 2023; Choi et al., 2024). Although (Templeton et al., 2024) compares the interpretability of SAEs to that of neurons and concludes that features in SAEs are significantly more interpretable, these findings heavily rely on qualitative analyses.\nPrevious research consistently shows that open-source models are effective for generating explanations, with advanced models producing better descriptions. For instance, Bills et al. (2023) report that GPT-4 achieves the highest scores, whereas Claude 3.5 Sonnet performs best for Paulo et al. (2024). The number of data samples used to generate feature description also varies across studies. Bills et al. (2023) use the top five most activating samples, while Choi et al. (2024) select 10-20 of the most activating samples to generate multiple"}, {"title": "B Data Preprocessing", "content": "descriptions for evaluation. In contrast, Paulo et al. (2024) use 40 samples and suggests that randomly sampling from a broader set of activating leads to descriptions that cover a more diverse set of activating examples, whereas using only top activating examples often yields more concise descriptions which fails to capture the entire description.\nThe datasets used in these studies also differs. Paulo et al. (2024) utilize the RedPajama 10M (Computer, 2023) dataset, Choi et al. (2024) use the full LMSYSChat1M (Zheng et al., 2023) and 10B token subset of FineWeb (Penedo et al., 2024), and Bills et al. (2023) on WebText(Radford et al., 2019) and the data used to train GPT-2 (Radford et al., 2019). Additionally, different delimiter conventions are observed: Choi et al. (2024) use delimiters, Paulo et al. (2024) use \u00ab \u00bb, and Bills et al. (2023) use numerical markers.\nB Data Preprocessing\nFor our work, we used an uncopyrighted version of the Pile dataset, with all copyrighted content removed, available on Hugging Face (Gao et al., 2020) (https://huggingface.co/datasets/monology/pile-uncopyrighted). This version contains over 345.7 GB of training data from various sources. From this dataset, we extracted approximately 6 GB while preserving the relative proportions of the original data sources. The extracted portion from the training partition was used to collect the most activated samples. For evaluations, we utilized the test partition from the same dataset, applying identical preprocessing steps as those used for the training data.\nOur preprocessing involved several steps to ensure a balanced and informative dataset. First, we used the NLTK (Bird et al., 2009) sentence tok-"}, {"title": "C Automated Interpretability Pipeline", "content": "enizer to break large text chunks into individual sentences. We then filtered out sentences in the bottom and top fifth percentiles based on length, as these were typically out-of-distribution cases consisting of single words or characters or a few outliers. This step helped achieve a more balanced distribution. Additionally, we removed sentences containing only numbers or special characters with no meaningful content. Finally, duplicate sentences were deleted.\nC Automated Interpretability Pipeline\nC.1 Implementation of Automated Interpretability\nOur experiments are based on models and feature descriptions presented in Table 11. We generate our feature descriptions as follows. In the implementation of the autointerpretability pipeline, we're closely following (Bills et al., 2023; Paulo et al., 2024; Rajamanoharan et al., 2024) and others: we pass the natural dataset, used for generating descriptions (see Appendix B) through the model, and via forward hooks we access the activations of each feature and each layer. This part can also be easily parallelized. With Gemma Scope SAEs, a wrapper class is implemented, that is built into the model as another module. It can be extended easily to other SAEs.\nAfter passing the whole dataset through the model, for each feature we take top 103 data samples based on the maximum activation of tokens. We only consider the maximum activating token of a data sample when we do the sorting. Later, we uniformly subsample the necessary number of data samples, i.e. 5, 15, ..., 50, that are further passed to an explainer LLM.\nThe reason why we subsample from top 103 is to avoid outliers in terms of activation. Top 103 still represents top 0.001% of the dataset. More complex sampling strategies can bring better performance, as described in Appendix A, but their implementation and evaluation is left out for the future work.\nExperiments are performed on the following models: Gemma-2-2b layer 20 (Riviere et al., 2024), Llama-3.1-8B-Instruct layer 19 (Dubey et al., 2024), and SAEs Gemma Scope 16K and 65K layer 20 (Lieberum et al., 2024). We also generate descriptions using baseline methods, namely TF-IDF and unembedding matrix projection. Term Frequency-Inverse Document Frequency (TF-IDF) is a widely used technique in NLP for measuring the importance of a word in a document relative to a corpus. It balances word frequency with how uniquely the word appears across documents, assigning higher scores to informative words while down-weighting common ones. We generate these values using 15 maximally activating samples. On the other hand, the unembedding matrix (WU) (Joseph Bloom, 2024) in transformer models maps the residual stream activations to vocabulary logits, determining word probabilities in the output. By analyzing projections onto this unembedding matrix, we gain insight into how learned features influence token predictions. To generate SAE unembedding descriptions, we generate logit weight distribution across the vocabulary, and then use the top ten words with the highest probabilities as feature description.\n$Logit \\ weight \\ distribution W_U * W_{dec}[feature]$ here, Wu is the unembedding matrix of a transformer model and Wdec are the decoder weights of sparse auto encoders.\nThis reveals which words are most associated with a given feature, enabling interpretability of sparse autoencoder (SAE) features, as well as MLP neurons. Both these methods are cheap baselines to compare with different auto-interpretability generated descriptions."}, {"title": "C.2 Prompts Engineering", "content": "Our feature description pipeline consists of several key components. The Subject Model for which the descriptions are getting generated, while the Explainer Model is a larger model used to generate descriptions. The System Prompt provides task-specific instructions to the LLM, detailing what to focus on in the provided samples and how to format the output. We append 5, 10, or 50 sentences along with a user_message_ending at the end, which helps reinforce the expected output structure. Before normalizing activations, we first average activations for tokens belonging to the same word. These values are then normalized between 0 and 10. For delimiters, we use single curly brackets"}, {"title": "D FADE Evaluation Framework", "content": "Our framework is designed to work with a wide variety of subject models, including most Hugging-Face Transformers (Wolf et al., 2020) as well as any feature implemented as a named module in PyTorch. Moreover, our framework is extensible: interpretability tools such as SAEs or various supervised interpretability techniques can be integrated with minimal effort, provided they implement some basic steering functions. In addition, we offer an interface for a diverse set of evaluating LLMs, whether open-weight or proprietary, with support for platforms such as vllm, Ollama, OpenAI, and Azure APIs. Our implentation can be found at https://github.com/brunibrun/FADE.\nD.1 Implementation and Computational Efficiency\nTo compute the purity and responsiveness measures, we base our sampling from the natural dataset on the activations of the subject LLM. For each sequence, we calculate the maximum absolute activation value across all tokens. Using these values, we sample sequences by selecting a user-configurable percentage of those with the strongest activations and for the remainder, drawing an equal number of samples from each of the following percentile ranges: [0%, 50%[, [50%, 75%[, [75%, 95%[, and [95%, 100%].\nComputational efficiency is a key consideration in our design, as evaluating every neuron in an LLM can be prohibitively expensive. The cost of evaluations is dynamically adjustable based on several factors, including the number of samples generated and rated, the evaluating LLM used, and the natural dataset selection.\nOur method allows users to control the cost by setting the number of synthetic samples (denoted n) relative to the full size of the natural dataset (N). By pre-computing activations from the natural dataset in parallel, we effectively reduce the per-run complexity from O(N * M) for M neurons to O(n*M). Given that n is typically in the hundreds while N is in the millions, this strategy yields big efficiency gain.\nAdditionally, we only execute the computationally intensive faithfulness evaluation when both clarity and responsiveness exceed a user-configurable threshold. This conditional execution ensures that unnecessary computations are avoided for features that do not meet our interpretability criteria."}, {"title": "D.2 Details on the Experiment Setup", "content": "In our experiments we send 15 requests to the evaluation LLM for generating synthetic samples. We remove duplicates and use these as concept-samples. We use the whole evaluation dataset as control samples. For rating we draw 500 samples from the natural dataset, where we take 50 from the top activated and 450 from the lower percentiles, according to the sampling strategy outlined above in"}, {"title": "E Extended Results", "content": "Appendix D.1. If we obtain fewer than 15 concept-samples in this first rating we again sample 500 new samples with the same sampling approach. We rate 15 samples at once, and if one of the calls fails, for example due to formatting errors of the evaluating LLM, we retry the failed samples once. For the Faithfulness experiments we use the modification factors [-50, -10, -1, 0, 1, 10,50]. We draw 50 samples from the natural dataset and let the subject LLM continue them for 30 tokens. We then rate only these continuations for concept strength by the evaluating LLM, again retrying once, if the rating fails. We only execute the Faithfulness experiment, if both the Clarity and Responsiveness of a feature is larger or equal to 0.5.\nLicenses Gemma-2-2b is released under a custom Gemma Terms of Use. Gemma Scope SAES are released under Creative Commons Attribution 4.0 International. Llama3.1-8B-Instruct is released under a custom Llama 3.1 Community License. Transluce feature descriptions, Pile Uncopyrighted dataset and LangChain are released under MIT License. vLLM is released under Apache 2.0 License.\nD.2.1 Prompts for the Evaluating LLM\nGenerating Synthetic Data Prompt\nYou are tasked with building a database of sequences that best represent a specific concept. To create this, you will generate sequences that vary in style, tone, context, length, and structure, while maintaining a clear connection to the concept.\nThe concept does not need to be explicitly stated in each sequence, but each should relate meaningfully to it. Be creative and explore different ways to express the concept.\nHere are examples of how different concepts might be expressed:\nConcept: \"German language\" Sequences might include German phrases, or sentences.\nConcept: \"Start of a Java Function\" Sequences might include Java code snippets defining a function.\nConcept: \"Irony\" Sequences might include ironic statements or expressions.\nProvide your sequences as strings in a Python List format.\nExample: [\"This is a first example sequence.\", \" Second example sequence but it is much longer also there are somy typos in it. wjo told you that can type?\"]\nOutput only the Python List object, without any additional comments, symbols, or extraneous content.\nRating Natural Data Prompt\nYou are tasked with building a database of sequences that best represent a specific concept.\nTo create this, you will review a dataset of varying sequences and rate each one according to how much the concept is expressed."}, {"title": "E.1 Quantitative Analysis", "content": "For each sequence", "scale": "n0: The concept is not expressed.\n1: The concept is vaguely or partially expressed.\n2: The concept is clearly and unambiguously present.\nUse conservative ratings. If uncertain", "Output": {"14\"": 0, "15\"": 2, "20\"": 1, "27\"": 0}}, "nOutput only the dictionary no additional text, comments, or symbols.\"\nD.2.2 Associated Cost\nThe activation generation for the subject models for the results in section 4.1 was run locally on a cluster with NVIDIA A100 GPUs with 40GB of VRAM. Similar to section C.3 we assume a price of $1/hr. Since activations can be cached in parallel for the whole model, only a single pass over the evaluation dataset was needed per model. We estimate a needed time of 24 hours on one GPU for the activation generation, resulting in a cost of 24$ per model. During the evaluations, only the activations of synthetic samples need to be computed. Due to a suboptimal configuration of the evaluation and subject LLM in our experiments we assume an average time of one minute per neuron evaluation, leading to an average cost of 0.024$ per evaluated feature. The estimated cost for the evaluating LLM consists of the cost for the generation of synthetic samples as well as the cost for rating natural data. In the configuration used for the"]}