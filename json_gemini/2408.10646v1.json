{"title": "Beneath the Surface of Consistency: Exploring Cross-lingual Knowledge Representation Sharing in LLMs", "authors": ["Maxim Ifergan", "Leshem Choshen", "Roee Aharoni", "Idan Szpektor", "Omri Abend"], "abstract": "The veracity of a factoid is largely independent of the language it is written in. However, language models are inconsistent in their ability to answer the same factual question across languages. This raises questions about how LLMS represent a given fact across languages. We explore multilingual factual knowledge through two aspects: the model's ability to answer a query consistently across languages, and the ability to \"store\" answers in a shared representation for several languages. We propose a methodology to measure the extent of representation sharing across languages by repurposing knowledge editing methods. We examine LLMs with various multilingual configurations using a new multilingual dataset. We reveal that high consistency does not necessarily imply shared representation, particularly for languages with different scripts. Moreover, we find that script similarity is a dominant factor in representation sharing. Finally, we observe that if LLMs could fully share knowledge across languages, their accuracy in their best-performing language could benefit an increase of up to 150% on average. These findings highlight the need for improved multilingual knowledge representation in LLMs and suggest a path for the development of more robust and consistent multilingual LLMs.", "sections": [{"title": "Introduction", "content": "Pretrained large language models (LLMs) have demonstrated a remarkable capacity to encode and retrieve factual knowledge (Petroni et al., 2019; Chang et al., 2024) across diverse languages (Kassner et al., 2021; Jiang et al., 2020). However, substantial variation in model performance across languages with a strong bias toward high-resource languages (Kassner et al., 2021; Jiang et al., 2020; Fierro and S\u00f8gaard, 2022; Jiang et al., 2022; Qi et al., 2023) highlights the issue of cross-lingual knowledge (in-)consistency.\nThis inconsistency raises questions about how LLMs represent factual knowledge in different languages. On one end of the range of possibilities, models may store a set of distinct knowledge copies for each language. On the other end, models may store a single, shared representation of the factual knowledge and \u201cdecode\u201d it into surface forms in different languages. Thus, a shared representation manifests in consistency across languages, but consistent behavior can also occur without it.\nWhile consistency can be readily measured through agreement on identical queries across languages, measuring the extent to which knowledge representation is shared across languages requires more than just evaluating black box outputs. To quantify representation sharing, we propose editing factual knowledge in one language and examining the effects on other languages. For this purpose, we employ three knowledge editing techniques: two locate-and-edit methods, ROME (Meng et al., 2022a) and MEMIT (Meng et al., 2022b), and a finetuning-based method (Gangadhar and Stratos, 2024). These methods are designed to surgically modify the relevant components of the model responsible for storing factual knowledge and only them, as illustrated in Fig 1.\nAs a test bed for our experiments, we compiled CLIKE, a multilingual \u201cfill-in-the-blank\" factual knowledge probing dataset with 35k samples, designed for evaluation and editing across paraphrases in 13 languages with 7 scripts. We experiment with diverse 7B-parameter LLMs supporting different sets of languages in different setups: monolingual, bilingual, multilingual, and language-extended models. Our analysis reveals a significant disparity in factual knowledge retrieval across languages. We assessed that on average these models answer correctly in at least one language 150% more facts than their best-performing language and triple the number of facts averaged across all 13 languages.\nWe assess for the first time the extent of cross-lingual knowledge representation sharing. We find that languages within the same script family exhibit the highest degree of representation sharing. This trend is consistent across all the models we studied, regardless of their level of multilingualism. Moreover, we find that high agreement in answers across certain language pairs does not entail shared internal representations, especially with language pairs that do not share the same script. This mismatch is particularly evident when comparing lower-resource languages to any language with a different script, which shows high consistency but limited representation sharing.\nWe expect that shedding light on these mechanisms will support the development of better multilingual models, with more efficient representation of factual knowledge. This will in turn lead to a more balanced knowledge across different languages, ultimately enhancing LLM performance across languages."}, {"title": "Methodology", "content": "In our analysis, we would like to measure two main aspects:"}, {"title": "Cross-lingual Knowledge Consistency (CKC)", "content": "The extent to which a model shows consistency in answering factual questions when asked in different languages."}, {"title": "Cross-lingual Knowledge Representation Sharing (CKR)", "content": "The degree to which the model uses a common inner representation for the same fact across different languages."}, {"title": "Measuring CKC", "content": "For simplicity, we say an LLM knows a fact in a specific language, modeled as a question and answer pair, if it can correctly answer it through a query written in this language. We start by defining a model's Knowledge Base (KB) for a specific language, as a set of facts an LLM 'knows' in that language. Formally, for a given LLM M and a dataset $D = \\{f_i\\}_{i\\in[N]}$ of facts. Where $f_i := (q_l^i, a_l^i)$ is a question-answer pair written in the language $l \\in L$:\n$KB_l := \\{f_i \\in D | M(q_l^i) = a_l^i\\}.$\nTo capture the pairwise relationship of knowing a fact in language $l_1$ to know it in $l_2$, we define $C(l_1,l_2)$ as the conditional probability\n$P(f_i \\in KB_{l_2} | f_i \\in KB_{l_1}) = \\frac{|KB_{l_1} \\cap KB_{l_2}|}{|KB_{l_1}|}.$\nWe continue by defining the Number of Consistent Languages (NCL) a fact f known in as:\n$NCL(f) = |\\{l \\in L : f \\in KB_l\\}|$\nWith this aggregation, we can then compute the overall CKC of a model, as the average number of languages in which the LLM knows a fact:\n$E[NCL] = \\frac{1}{N} \\sum_{f \\in D} NCL(f)$"}, {"title": "Measuring CKR", "content": "Measuring the extent of shared knowledge representation across languages in LLMs cannot be done by merely evaluating model outputs. The same correct answer to a factual query across multiple languages could be generated from distinct, language-specific representations within the model, rather than a unified, language-agnostic abstraction. This requires a more sophisticated approach.\nTo measure this, we use an editing method E that modifies the model's parameters to provide a wrong answer for a query in a given language. We then examine the impact of such a change on the same fact query in other languages. Let $M^E$ denote the updated model applying E to the fact f in the language l to the target answer $t_l^f$. The model's KB for a specific language l' after the modification in language l is defined as the collection of facts for which the incorrect target answer, edited in language l, also propagates to language l', which can be formally expressed as:\n$KB_{l'}^E := \\{f_i \\in KB_{l'} : M^E(q_{l'}^i) = t_{l'}^i\\}.$\nWe can then estimate the amount of pairwise CKR between a language l1 to l2 by defining SR(l1,l2) as the conditional probability\n$P(f \\in KB_{l_2}^E | f \\in KB_{l_1}) = \\frac{|KB_{l_1}^E \\cap KB_{l_2}^E|}{|KB_{l_1}^E|}.$\nWe further define the Number of Transferred Languages (NTL) for a given fact edited in the language l as\n$NTL(f') = |\\{l' : f \\in KB_{l'}^E\\}|$\nWith this aggregation, we can then compute overall CKR, as the average number of languages in which the LLM represents a fact as\n$E[NTL] = \\frac{1}{|L|} \\sum_{l \\in L} \\frac{1}{|KB_l|} \\sum_{f \\in KB_l} NTL(f')$"}, {"title": "Experimental Setup", "content": null}, {"title": "Data", "content": "Dataset. We present CLIKE (Cross-LIngual Knowledge Editing), a dataset for evaluating and editing factual knowledge of pretrained LMs across languages and paraphrased expressions. CLIKE contains approximately 35k facts spanning 13 languages: English (en), French (fr), Italian (it), Spanish (es), Russian (ru), Ukrainian (uk), Bulgarian (bg), Hindi (hi), Bengali (bn), Chinese (zh), Japanese (ja), Hebrew (he), and Arabic (ar). Each fact is modeled as a language-independent (subject, relation, object) triplet and each relation has 3 paraphrased natural language templates for every language. Each template forms a sentence that conveys a fact and ends with the object, which we omit and expect the model to fill.\nFor example, the triplet (Bach, BirthCity, Leipzig) will be converted to 'Bach was born in the city of' and 'The birth city of Bach is', and 'The birthplace of Bach is the city of' expecting the pretrained LM to complete the prompt with 'Leipzig' correctly using its initial pretraining task without altering the model with a finetuning intervention.\nFact Collection. Following a similar approach to Petroni et al. (2019); ?); Kassner et al. (2021); Wei et al. (2024), fact triplets were collected from Wikidata Query Service. We manually crafted and published 14 SPARQL relation queries. Each query extracts wikidata entries for subjects and objects satisfying the query relation with their labels in all available languages. We then filtered all triplets with labels containing less than 8 of the examined languages to balance the languages in the dataset. Appendix B includes the languages and relation distributions.\nDataset Construction. We used \"Gemini Advanced\" and \"Claude Opus\" to generate the templates of each relation in all languages. For each relation, we generated 3 paraphrases adjusted to grammar rules such as the gender of the subject. The prompts for these templates were executed on the models' official websites (Gemini, Claude). Subsequently, professional translators or native speakers refined the templates and sampled generated fill-in-the-blank queries across all languages, following instructions detailed in Appendix A. For the knowledge editing task, we generated false but plausible objects for each fact by randomly sampling from other facts within the same relation category. This approach provided consistent incorrect alternatives across all languages for each query."}, {"title": "Models", "content": "We examine a range of LLMs with 7B parameters and decoder-only architectures. We focus on base pretrained language models to capture the knowledge acquired during the pretraining process, prior to any finetuning. BLOOM-7B (Scao et al., 2022) serves as our multilingual model. Qwen-7B (Bai et al., 2023) represents a bilingual Chinese-English model with a low tokenization compression rate multilingual vocabulary. We include two monolingual English models: Llama-2-7B (Touvron et al., 2023) and Mistral-7B-v0.1 (Jiang et al., 2023). Additionally, we examine two language-extended models, Chinese-llama-2-7B, and Hebrew-Mistral-7B, based on Llama-2-7B and Mistral-7B-v0.1 with additional pretraining in English and their expanded language (EL) and an expand EL tokenizer vocabulary. These models represent a diverse set of multilingual configurations, enabling a extensive analysis of cross-lingual knowledge representation."}, {"title": "Knowledge Editing Methods", "content": "We employ three knowledge editing methods: Finetuning (FT) (Gangadhar and Stratos, 2024), ROME (Meng et al., 2022a), and MEMIT (Meng et al., 2022b). The ROME and MEMIT editing methods leverage causal mediation analysis (Vig et al., 2020a,b) to identify the LM layer that has causally contributed to factual knowledge recall, suggesting the middle MLP layers act as key-value associative memory. ROME then computes a closed-form rank-one update to the layer's weights, inserting a new fact while minimizing disruption to existing knowledge stored in the weights. Similarly, MEMIT identifies a range of MLP layers that jointly contribute to the model's factual associations. Then it iteratively updates the weights of each MLP layer, distributing the changes across the MLP layers.\nBoth ROME and MEMIT use interpretability techniques to precisely locate and surgically modify the relevant components of the model responsible for storing factual knowledge. This approach allows for direct control over the model's memorized information while preserving its overall capabilities, providing a framework for isolating changes in actual knowledge without altering other components.\nFinetuning, our baseline approach, involves updating the weights of all middle layers in the model without the MLP restrictions imposed by ROME and MEMIT. For each fact to be edited, we finetuned the model on a single example consisting of the edition prompt paired with its new target answer. It incorporates new factual knowledge that resembles standard language model training practices.\nWe use the EasyEdit code library (Wang et al., 2023b) to perform all language model knowledge edits. Default parameters are employed for all models except BLOOM. Since BLOOM lacks a pre-existing implementation, we optimized and published custom hyperparameters for the editing methods."}, {"title": "Metrics and Evaluation", "content": "We employed the Exact Match (EM) metric to evaluate all answers to queries across our experiments. To provide context for the pretrained LLM, we used 3-demonstrations fewshot concatenated facts for both evaluation and editing tasks, maintaining the same examples and order. All answer generation was performed using greedy decoding to ensure deterministic outputs.\nModel performance and CKC in a given language were assessed as follows. The overall accuracy for a language was computed as the percentage of facts correctly answered in at least one paraphrased form. C(l1, 12), was measured by computing the mean score in language 12 across all paraphrases for facts known in language l1. Similarly, within-language consistency, C(l,l), was computed using the same approach, evaluating the model's consistency across paraphrases within a single language.\nFor the knowledge editing experiments, we randomly selected 500 known facts in each language to modify. We assessed the effectiveness of these edits using three standard metrics: Reliability, Generalization, and Locality. Reliability measures the accuracy of the model on the edited prompt itself. Generalization, denoted with SR(11, 12), evaluates the mean score across all paraphrases of the edited fact in all languages, including the language in which the edit was made (11). This quantifies how well the edit transfers across both languages and paraphrases variations of the fact. For Locality testing, we randomly sampled known facts for each language and evaluated the model's mean accuracy to answer these unrelated queries correctly. This ensured that the edits did not negatively impact other knowledge in different languages."}, {"title": "Results", "content": "Before presenting our main findings, we first validate the methodology's performance. We found a strong correlation (0.87) between the results obtained from different knowledge editing methods. This consistency across various editing techniques suggests that our findings are robust and not reliant on a specific method. Given this consistency, we primarily present results using MEMIT, with other methods' results available in Appendix C. Additionally, all knowledge editing methods maintained high locality scores (averaging above 70%), indicating that edits were specific and preserved broader model knowledge. Furthermore, all models exhibit some variation in performance across paraphrases even within the same language, aligning with findings from Mizrahi et al. (2024) and further justifying our approach of assessing knowledge using multiple paraphrases."}, {"title": "The Issue of Knowledge Variability", "content": "Large language models (LLMs) exhibit significant variability in their factual knowledge retrieval across different languages, as illustrated in Fig. 2. Our analysis of four 7B-parameter LLMs reveals a striking disparity: while models demonstrate knowledge of 42.5% of the facts on average in at least one language, their best-performing language achieves only 27.6% accuracy, and their average performance across all 13 languages in the CLIKE dataset is merely 11.8%.\nIf models could share knowledge across all languages, the best-performing language could potentially increase its accuracy by up to 53%. Moreover, models could then potentially more than triple their current cross-lingual average accuracy. This observation motivates our subsequent investigations into CKR, as we seek to understand and potentially leverage these untapped reservoirs of knowledge."}, {"title": "Consistency Does Not Imply Representation Sharing", "content": "We decouple CKC and CKR between languages, examining both general measures across languages (Fig. 3) and pairwise language relationships addressing their specific identities (Fig. 4). Our analysis reveals that high CKC does not necessarily imply high CKR, and in some cases, we observe inverse patterns.\nAt the general level, we observe for all models that $E[NCL]$ is consistently higher than $E[NTL]$, indicating that models tend to exhibit CKC across more languages than they share representations between. Interestingly, while $E[NCL]$ values show considerable variation across models, $E[NTL]$ values are more uniform. The persistent gap between $E[NCL]$ and $E[NTL]$ across all models highlights that consistent answers do not necessarily translate to shared internal representations. Moreover, we find that models with a lower proportion of facts known in only one language (NCL = 1) tend to have a higher proportion of facts represented in only one language (NTL = 1) as shown in Fig. 3.\nAt the pairwise language level, we find differences between CKC and CKR patterns. For instance, most models (except Qwen) exhibit a high degree of CKC among low-resource languages with different scripts (Chinese, Japanese, Hebrew, Arabic). However, when examining CKR, we find limited evidence of shared encoding between these languages. Conversely, we observe a higher degree of shared representation among Cyrillic languages compared to the shared representation between Cyrillic and Latin languages. This is despite the fact that CKC scores show an opposite trend, with higher CKC between Cyrillic and Latin languages than among Cyrillic languages themselves."}, {"title": "The Key Role of the Language Script", "content": "Our analysis provides quantifiable measures of CKR in LLMs. Following previous work (Qi et al., 2023; Beniwal et al., 2024), our study highlights the importance of the script of a language for multilingual knowledge. We observe that the pairwise SR measure is relatively consistent across models, despite their varying language support.\nWe find that languages within the same script family exhibit the highest degree of CKR across all models. As shown in Fig. 4, we observe a script-based grouping in both CKC and CKR likely highlighting a tokenization induced bias (Singh et al., 2019). Notably, we observe strong CKR between languages with Latin scripts (English, French, Italian, Spanish) and between languages with Cyrillic scripts (Russian, Ukrainian, Bulgarian). For Devanagari script languages (Hindi, Bengali) we observe relatively high CKR for models that perform well on these languages (Bloom, Mistral).\nWhile most CKR occurs among languages that use the same script, there is still some knowledge transfer between languages with different scripts. This cross-script transfer is particularly evident between Cyrillic and Latin script languages across various models. Additionally, in specific cases, such as with the BLOOM model, we observe a moderate degree of CKR between seemingly unrelated language pairs, e.g., 28% from Italian to Hindi.\nNotably, these relations between language scripts are sometimes asymmetrical. For example, knowledge in Cyrillic script languages implies a higher probability (approximately 40-60%) of knowing the same facts in Latin script languages. However, the reverse relation is weaker, with only about 10-20% probability of Cyrillic knowledge given Latin script knowledge. A similar asymmetrical relation appears across models suggesting a stronger transfer of knowledge from Cyrillic to Latin. We hypothesize that the dominance of Latin script languages, especially English, in the training data leads to more robust fact representations in Latin scripts, facilitating easier transfer from Cyrillic to Latin than vice versa."}, {"title": "Impact of Model Design Languages", "content": "How does a model's designed language support affect its CKR and CKC patterns? Although the patterns of CKR are relatively similar across models supporting different language sets, our analysis reveals some nuanced differences.\nThe multilingual BLOOM model demonstrates the highest pairwise average of language pairwise CKC (36%) and CKR (8.4%) across different script pairs. As shown in Fig. 4, BLOOM exhibits notable transfer between seemingly unrelated language pairs. These cross-script patterns validate BLOOM's design as a multilingual model, emphasizing its cross-lingual relationships rather than its overall low accuracy performance.\nWe find that the bilingual English-Chinese Qwen model is showing relatively high overall accuracy in Chinese. However, this Chinese knowledge remains largely distinct from English both in terms of language pairwise CKC and CKR Fig. 4. This pattern validates Qwen's design as a bilingual model, emphasizing its language-specific capabilities rather than cross-script knowledge sharing. Surprisingly, when examining the global shared representation, Qwen exhibits a higher number of 4-lingual representations sharing (NTL = 4) from uniquely represented facts. Although Qwen lacks cross-script knowledge sharing, it developed some degree of multilingual representation, particularly within script families.\nMonolingual English models (Mistral, LLaMA) exhibit a unique pattern. We discover an anomalous peak in the results for facts known and represented in exactly four languages (Fig. 3), corresponding primarily to the four Latin script languages in our dataset. This highlights the strong association between script similarity and knowledge sharing even in ostensibly monolingual models. Surprisingly, Mistral demonstrates the highest $E[NCL]$, $E[NTL]$ and average of language pairwise CKC (54.7%) and CKR (37.6%) within script families, despite being designed as a monolingual English model. This result highlights how Mistral's strong English foundation naturally extends to other Latin script languages, underscoring the impact of script similarity on cross-lingual knowledge representation even in monolingual models."}, {"title": "Language Extended LMs", "content": "How does additional pretraining on both English and an extended language (EL) impact cross-lingual CKC and shared representation in initially monolingual models? Analysis of chinese-llama-2-7b and he-mistral-7b reveals a similar trade-off: while gaining substantial knowledge in EL, models sacrifice much of their original English expertise. These extensions reshape cross-lingual knowledge distribution but fall short of fully bridging the gap between disparate writing systems.\nAs shown in Table 1, both models exhibit increased accuracy in the extended language (EL) coupled with decreased English accuracy. CKC measures paint a nuanced picture: models acquire extensive new knowledge in EL, largely unknown in English, yet this new EL knowledge covers more of English knowledge. Shared representation metrics underscore this asymmetry. Despite increased bidirectional knowledge transfer between English and EL, transfer remains stubbornly low. This suggests that even with targeted pretraining, models struggle to forge robust representations sharing across linguistically distant languages.\nFor analysis of how different relation types affect CKR, see Appendix D."}, {"title": "Related Work", "content": "Cross-lingual Knowledge Consistency. While monolingual knowledge consistency has been studied often in LMs (Elazar et al., 2021; Mizrahi et al., 2024), limited work has been done on cross-lingual knowledge consistency. ? proposed a cross-lingual consistency metric named RankC to measure similarity across multiple candidate answers, whether correct or incorrect. Our focus on correct answers allows a simpler assessment without being limited to pairwise language comparisons.\nCross-lingual Knowledge Representation Sharing. Previous studies explored this angle through different approaches. Some works studied parameter sharing across languages by analyzing neuron activation/deactivation when evaluating knowledge in different languages (Libovick\u1ef3 et al., 2020; Zhao et al., 2024b; Chen et al., 2024; Tang et al., 2024; Kojima et al., 2024). Enhancing language-independent neurons resulted in better multilingual abilities in a specific language without compromising others. Other works investigated the knowledge related to the training data and identified the language source of the acquired data (Choenni et al., 2023; Zhao et al., 2024a), providing evidence that knowledge from training data in one language can benefit the model in other languages. Another line of work analyzed how inputs in different languages affect the activation patterns, showing that semantically equivalent content in different languages tends to produce similar activation patterns (Singh et al., 2019; Libovick\u1ef3 et al., 2020; Chang et al., 2022).\nThese works pointed to a connection between knowledge in different languages. However, they do not yield an assessment of the amount of shared knowledge. While passive analysis can take as far as measuring the similarity between languages, active modification tools can also suggest a clear causal relation between the knowledge representation in different languages.\nMultilingual Knowledge Editing. Previous work on multilingual knowledge editing (Si et al., 2024; Xu et al., 2022; Wei et al., 2024; Wang et al., 2023a) primarily focused on comparing and improving editing methods' performance in multilingual settings. Our approach is different. We use these editing tools as analytical tools to understand representation sharing across languages and across models with different multilingual configurations."}, {"title": "Conclusion", "content": "This work investigated the relationship between cross-lingual knowledge consistency and representation sharing in LLMs. Our findings reveal that high consistency across languages does not necessarily imply shared internal representations, particularly for languages with different scripts. We introduced a novel methodology and dataset for quantifying these phenomena, providing a more nuanced understanding of how LLMs represent and retrieve factual knowledge. The significant disparity we observed in factual knowledge retrieval across languages, coupled with the potential for substantial performance improvements if knowledge could be fully shared, underscores the importance of developing more effective multilingual knowledge representations. We expect that our insights will guide the development of more efficient and equitable multilingual models, ultimately enhancing their performance across all languages."}, {"title": "Limitations", "content": "Our main limitation lies in the constraints imposed by our chosen editing methods and their focus on specific model components. By primarily targeting middle layers associated with factual knowledge storage, our analysis may have overlooked important cross-lingual interactions occurring elsewhere in the model architecture. Our reliance on specific editing techniques (ROME, MEMIT, and Finetuning) may not capture the full spectrum of knowledge representation and modification within the model. There might be multiple pathways to change the output in a specific language, potentially exhibiting different cross-lingual generalization patterns than those we observed.\nOur analysis focused exclusively on decoder-only language models with 7B parameters, limiting the generalizability of our findings across different architectures and sizes. Similarly, while our CLIKE dataset covers a diverse range of languages and relations, it may not fully represent the breadth of factual knowledge or linguistic phenomena. These constraints in both model selection and dataset composition could influence the observed patterns of cross-lingual representation."}, {"title": "Native Speaker Instruction", "content": "Dear <Native Speaker>,\nWe are reaching out to you for assistance in an important project that aims to improve the ability of Artificial Intelligence (AI) to understand and generate text in your native language. Your skills and knowledge as a native speaker are crucial to the success of this project.\nOur research team has created a collection of fill-in-the-blank sentences and templates in multiple languages, including yours. These sentences will be used to evaluate the knowledge and understanding of AI language models. To ensure the accuracy and effectiveness of our collection, we need your help in verifying the grammatical correctness of the sentences and templates we have created.\nAttached, you will find a list of approximately 60 simple sentences and sentence templates and templates that cover various relationships between subjects and objects in your native language. The task should take no more than 15-20 minutes to complete. Your task is to review each sentence and template and determine whether they are grammatically correct. If you find any grammatical errors, please provide a corrected version of the template. Additionally, if you wish, you may provide an optional explanation in English of what was wrong with the original template.\nExample:\nRelation: Birth City, Subject: Wolfgang Amadeus Mozart, and Object: Salzburg\nOriginal template:\n\"[subj] birthplace the city [obj]\" -> \"Wolfgang Amadeus Mozart birthplace the city Salzburg\"\nFixed template:\n\"[subj]'s birthplace is the city of [obj]\" -> \"Wolfgang Amadeus Mozart's birthplace is the city of Salzburg\"\nExplanation:\nThe original template is missing the verb \"is\" and the preposition \"of\" to form a grammatically correct sentence.\nWhen fixing the templates, please keep in mind the following guidelines:\nBe explicit about the relationship to avoid ambiguity. For example, given the information (Bach, Birth Year, 1685) and the template \"[subj] born in [obj]\", the AI might complete the prompt \"Bach was born in\" with the object \"Leipzig\" (his birth city) or \"31 March\" (his birth date) rather than the year \"1685\". Therefore, a good template should contain words that explicitly describe the relationship. The template \"Bach was born in the year [obj]\" will likely output \"1685\" since the word \"year\" appeared in the sentence.\nFor each relationship, we have provided three different prompt paraphrases that are supposed to be different from one another.\nThe subject must always appear before the object.\nThe last word of the prompt template should be the object.\nPlease note that the sentences do not necessarily need to sound natural or be well-written. Our primary focus is on ensuring that they are grammatically correct. However, if you have suggestions on how to make the sentences sound more natural without changing the core structure, feel free to include them in your feedback.\nYour contribution to this project is valuable and will help us create a reliable collection of sentences for advancing Al's ability to understand and generate text in your language. If you have any questions or concerns about the project or your role in it, please don't hesitate to reach out to us.\nThank you for your participation in this project. We look forward to receiving your feedback.\nBest regards,"}, {"title": "CLIKE Dataset Key statistics.", "content": null}]}