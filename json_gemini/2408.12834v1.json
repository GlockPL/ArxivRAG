{"title": "CLLMFS: A Contrastive Learning enhanced Large Language Model Framework for Few-Shot Named Entity Recognition", "authors": ["Yafeng Zhang", "Zilan Yu", "Yuang Huang", "Jing Tang"], "abstract": "Few-shot Named Entity Recognition (NER), the task of identifying named entities with only a limited amount of labeled data, has gained increasing significance in natu-ral language processing. While existing methodologies have shown some effectiveness, such as enriching label semantics through various prompting modes or employing metric learn-ing techniques, their performance exhibits limited robustness across diverse domains due to the lack of rich knowledge in their pre-trained models. To address this issue, we propose CLLMFS, a Contrastive Learning enhanced Large Language Model (LLM) Framework for Few-Shot Named Entity Recog-nition, achieving promising results with limited training data. Considering the impact of LLM's internal representations on downstream tasks, CLLMFS integrates Low-Rank Adaptation (LORA) and contrastive learning mechanisms specifically tai-lored for few-shot NER. By enhancing the model's internal rep-resentations, CLLMFS effectively improves both entity bound-ary awareness ability and entity recognition accuracy. Our method has achieved state-of-the-art performance improve-ments on F1-score ranging from 2.58% to 97.74% over exist-ing best-performing methods across several recognized bench-marks. Furthermore, through cross-domain NER experiments conducted on multiple datasets, we have further validated the robust generalization capability of our method. Our code will be released in the near future.", "sections": [{"title": "1 Introduction", "content": "Named Entity Recognition (NER) is pivotal for identifying and categorizing named entities within unstructured text across various domains, such as Location [23], Private Health In-formation [21] and Event [32]. However, developing accurate NER models demands substantial amounts of domain-specific annotated data, which are often scarce and costly to procure [14]. This has led to the demand for Few-Shot Named En-tity Recognition (FS-NER), which aims to learn from limited labeled examples to address entity tagging challenges under low-resource conditions [6].\nEarly FS-NER methods often employ neural networks with conventional supervised learning, which may lead to overfit-ting due to the large number of parameters to optimize [3]. To mitigate this, cross-domain NER approaches have been em-ployed, where models learn semantic features from base classes and adapt them to novel classes [16]. Despite this, these meth-ods may still exhibit suboptimal generalization in novel do-mains [9]. To address these limitations, contrastive learning has been introduced, utilizing Gaussian distributions to opti-mize the distributional distance between tokens in sentences [7].\nWith the rapid development of Large Language Models (LLMs), models like GPT-3 demonstrate few-shot capabilities through prompt-based construction, achieving satisfactory re-sults [30]. LLAMA 2 [25] emerges as a superior choice in low-resource settings due to its accessibility and adeptness across various natural language processing (NLP) tasks. However, deploying of LLMs such as ChatGLM, GPT-3, ChatGPT, GPT-4, LLAMA, and LLAMA 2 [10, 4, 17, 5, 24, 26] for FS-NER poses challenges, given their extensive parameter sizes and the need for substantial amounts of high-quality supervised fine-tuning (SFT) data, leading to high costs in training and data acquisi-tion. To address this, Parameter-Efficient Fine-Tuning (PEFT) techniques like Low-Rank Adaptation (LoRA) [13] have been proposed to enhance model performance on new tasks while minimizing fine-tuning parameters and computational com-plexity.\nIn this research, we present innovative method, CLLMFS, to tackle the NER task under low-resource conditions. Our approach leverages LLMs to effectively address the challenge of limited labeled data by exploiting their pre-trained knowl-edge. We fine-tune LLMs using supervised learning to adapt"}, {"title": "2 Related Works", "content": "2.1 Few-Shot NER\nFew-shot learning (FS-NER) enhances model performance with limited labeled data. Data-enhancement methods aug-ment small labeled datasets with additional data sources, but unreliable examples can affect precision [33]. Manner uses a Variational Autoencoder for an external memory module, but faces challenges in memory optimization and cross-domain generalization [11]. CONTaiNER employs contrastive learning to optimize token distribution, improving adaptability to new domains [7], but large source-target domain divergence can be problematic.\n2.2 Meta Learning\nMeta Learning offers new approaches for few-shot learning. Metric-based methods like Matching Networks [27] and Proto-typical Networks [19] calculate similarities to learn prototyp-ical representations for target classes. ProtoBERT uses a pre-computable BERT encoder for effective entity prediction [37]. ProML introduces multiple prompt schemas with weighted averages for enriched label semantics [6], achieving promising results across various settings. However, generated prototypes may lack precision, due to limited labeled data for various entity types in the support set.\n2.3 In-context learning\nLarge-scale pre-trained LLMs, like GPT-3 [4], have advanced in-context learning, applied in tasks like question answering and NER without additional training data [2]. Recent NLP re-search explores prompt-based methods for FS-NER, relying on prompts to predict labels. However, these methods primarily rely on prompts to predict labels using classification heads, rather than employing data-enhancement or metric learning techniques. Prompt-based NER uses language models to gener-ate entity predictions based on context and instructions. How-ever, these methods face limitations in prompt quality and design."}, {"title": "3 Methodology", "content": "The internal representations of language models play a piv-otal role in shaping the performance of downstream tasks. In this paper, we introduce a novel model, CLLMFS, based on large pre-trained language models. As depicted in Figure 1, our model undergoes supervised fine-tuning in source do-mains under the N-way K-shot scenario, enabling it to adapt to target domains effectively. Ultimately, our model integrates LORA and contrastive learning loss techniques, specifically customized for the NER task.\n3.1 Task definition\nGiven a sequence of n tokens {X1, X2, ..., Xn} and corresponding tag labels {Y1, Y2,..., Yn}, the primary objective of NER is to associate each token x\u2081 with its corresponding tag label yi. In Few-shot NER, a model undergoes training in a low-resource source domain with a tag-set denoted as \\{C\\}. Subsequently, it is tested in a target domain that employs a distinct tag-set, denoted as \\{C\\}, where i and j represent indices for different tags. Since \\{C\\} \u2229 \\{C\\} = \u00d8, the model faces the formidable challenge of generalizing to previously unseen test tags. In an N-way K-shot scenario, the source domain comprises N distinct entity types, denoted as |\\{C)\\}| = N. For each entity type, there are K examples in the support set. This setup means that the model is trained with K labeled examples for each of the N types, enabling it to learn and generalize from a limited number of examples.\n3.2 LLM for entity extraction\nOur method for entity extraction tasks is based on LLAMA 2, referred to as LLM. Figure 1 illustrates the pivotal compo-nents of our model. We utilize the 7-billion parameter model of LLAMA 2, balancing effectiveness and inference speed. Our approach achieves excellent results in entity extraction tasks with only a small amount of training samples.\nLLAMA 2's architecture closely resembles the standard Transformer Decoder, primarily consisting of 32 Transformer Blocks. Each block includes the following core components:\n\u2022 RMSNorm [36]: Normalizes the activation outputs of net-work layers, ensuring uniform scaling, accelerating training, and enhancing model stability.\n\u2022 SwiGLU [18]: Adds non-linearity to the model by trans-forming input values through the Swish activation function.\n\u2022 RoPE [22]: A novel positional encoding strategy that encodes positional information through rotation operations."}, {"title": "3.3 Model Training", "content": "3.3.1 Supervised fine-tuning with LoRA\nSupervised fine-tuning (SFT) refers to the process of adjusting a pre-trained LLM using labeled data to better adapt it to a spe-cific task. During SFT, weights of the model are adjusted based on the discrepancies with the true labels, aiming to enhance precision and task adaptation.\nEach sample in SFT typically consists of three parts: instruc-tion (i.e., prompt), input, and output. For instance, for the entity type \"Person\" (other entity types are provided in the Appendix A):\nConsidering the limited amount of the data generated by SFT, full model fine-tuning is not feasible and the issue of over-fitting is also serious. We adopt Low-Rank Adaptation (LoRA) [13] to address these problems. LoRA assumes that weight up-dates during the adaptation process also have a lower 'intrinsic rank'. For a pre-trained weight matrix Wo \u2208 Rdxk, we restrict its update through a low-rank decomposition as follows:\nWo + AW = Wo + BA\n(1)\nwhere B \u2208 Rdxr, A \u2208 Rrxk, and the rank r << min(d, k). Through-out training, Wo remains frozen and does not undergo gradient updates, while A and B include trainable parameters. We em-ploy a random Gaussian initialization for A and set B to zero, ensuring that AW = BA is zero at the start of training.\nIn other words, during the fine-tuning process, the model initializes with pre-trained parameters Wo and updates them to Wo + AW(0) by maximizing the conditional language model probability, where |0| << |Wo|:\n$\\max_{\\theta} \\sum_{(x,y)\\in Z} \\sum_{t=1}^{y} \\log(P_{Wo+AW(\\theta)}(Y_t|x, y < t))$\n(2)\nwhere Z denotes the training dataset comprising input se-quences x and their corresponding target sequences y, and |y| signifies the length of the target sequence y. PWo+AW(\u03b8)(y+x, y < t) represents the probability that the model predicts the t-th element yt of the target sequence, given the input sequence x and the first t elements of the target sequence y. LoRA fine-tune only need a subset of parameters, thereby avoiding issues such as excessive resource consumption caused by full fine-tuning.\nIn principle, LoRA can be applied to any subset of weight matrices in a neural network, thereby reducing the number of trainable parameters.\n3.3.2 Decoding strategies\nDecoding strategies play a pivotal role in text generation tasks. In our approach, we maintain a fixed temperature of 0.01 to control the diversity of generated text. Additionally, we employ the top-k sampling method, where only tokens ranking within the top probability threshold (top_p) are considered during the token sampling process. This strategy optimizes the greedy approach by sampling from the top-k tokens, allowing tokens with higher scores or probabilities beyond the top threshold to also have a chance of being selected.\nGiven the nature of our task, which involves information extraction, we implement a constrained generation approach. This ensures that the generated output is constrained to be a subset of the input, restricting the generated content within predefined boundaries. Furthermore, to prevent the model from endlessly generating content, we introduce a custom stop symbol, denoted as , marking the end of the gener-ated sequence. This mechanism effectively halts the genera-tion process after the last eight characters, ensuring controlled and targeted generation. Employing a combination of a low-temperature setting and constrained generation proves instru-mental in effectively handling few-shot scenarios.\nThe internal representations of language models have a sig-nificant impact on the performance of downstream tasks. In this paper, we employ contrastive learning loss in low-resource entity extraction tasks to enhance the boundary perception ability of the model and improve its effectiveness.\n3.4 Contrastive learning\nContrastive learning is a discriminative representation learn-ing method based on the principle of comparison, primarily used for unsupervised (self-supervised) representation learn-ing. The core idea of contrastive learning is to compare samples with positive examples (semantically similar) and negative examples (semantically dissimilar). By designing contrastive losses, it aims to bring representations of semantically similar positive examples closer while pushing representations of se-mantically dissimilar negative examples further apart. There-fore, the careful selection of positive and negative sample pairs for contrastive learning is crucial."}, {"title": "3.5 Enhancing Representation Uniformity with Adversarial Samples", "content": "The representations generated by contrastive learning are typ-ically regularized, causing them to concentrate within a hy-persphere. Alignment and uniformity refer to two essential characteristics of a good representation space: alignment en-sures that representations of semantically similar samples are close together, while uniformity ensures that representations of semantically dissimilar samples are evenly distributed across the hypersphere. Enhancing the uniformity of representation distributions can improve the performance of many tasks, such as recommendation systems.\nHowever, previous research has primarily relied on in-batch negative sampling or random negative sampling from the training data. This approach may introduce sampling bias, leading to the inclusion of inappropriate negative examples (such as false negatives or anisotropic representations) in con-trastive learning, potentially compromising the alignment and uniformity of the representation space.\nTo achieve a more uniformly distributed representation space, we focuses on the embedding space and directly in-troduces noise into the representations. Inspired by Yu et al. [34], we construct adversarial samples through imperceptible perturbations by adding uniformly distributed Gaussian ran-dom noise to positive embeddings of entities. While this ap-proach is simple, it can strengthen the positive samples to resist noise, leading to a significant enhancement in the model's ro-bustness against interference. Formally, given a token i and its embedding zi in the d-dimensional space, we can implement the following representation-level augmentation:\nz\u2081 = z\u00a1 + \u2206\n(4)"}, {"title": "3.6 Model Optimization", "content": "To train our model effectively for the low-resource entity ex-traction task, we employ a combined loss function comprising both cross-entropy loss and contrastive learning loss.\nThe primary objective of cross-entropy loss in Few-shot NER is to ensure that our model learns to correctly associate each token x; in the input sequence with its corresponding tag label yi. This involves minimizing the discrepancy between the pre-dicted tag probabilities and the ground truth labels across the entire sequence. Formally, the cross-entropy loss LCE is com-puted as follows:\nLCE = - \u03a3 \u03a3 yic log(\u0177ic)\nice(C)\n(5)\nwhere yi,c represents the ground truth label for token xi corre-sponding to tag c in the target domain, and \u0177i,c represents the predicted probability of token x\u2081 belonging to tag c.\nBy minimizing the cross-entropy loss, our model learns to accurately predict the tags associated with each token in the input sequence, thereby improving its performance in the NER task, especially when dealing with previously unseen tags in the target domain.\nIn addition to cross-entropy loss, we incorporate contrastive learning loss to further enhance the model's performance. The contrastive loss LCL encourages the model to effectively distin-guish between positive pairs (tokens associated with the same entity type) and negative pairs (tokens associated with differ-ent entity types).\nDue to the presence of 32 Transformer Blocks (i.e., 32 lay-ers of hidden states) in LLAMA 2, we determined the optimal layer for computing the contrastive loss through empirical test-ing. Configurations using the 10th, 25th, 26th, 27th, and 30th layers were evaluated, and the 26th layer consistently yielded the best performance. This layer selection closely aligns with the 8:2 golden ratio, providing a balance between the lower and higher layers in the model's architecture. Therefore, we compute the contrastive loss at the 26th layer to leverage this optimal configuration.\nFinally, we leverage a multi-task training strategy to jointly optimize the cross-entropy loss, and the contrastive learning loss. The overall loss function is:\nL = LCE + ALCL\n(6)\nwhere A serves as a hyperparameter to regulate the impact of contrastive learning and is set to 0.001. This choice is made considering that different losses calculate distinct gradients, with the aim of emphasizing the gradient of the main task.\nBy jointly optimizing cross-entropy loss and contrastive learning loss, our model learns to effectively classify entities while also capturing semantically meaningful representations, thus enhancing its overall performance in the low-resource en-tity extraction task."}, {"title": "4 Experiments", "content": "4.1 Dataset Description\nTo assess the effectiveness of our method, we utilize 5 datasets spanning various domains: WNUT'17 3, GUM 4, I2B2 5, OntoNotes 6, and Conll20037 which are publicly available and have been used in existing research [15, 28, 29, 31] to showcase diversity in terms of domain, scale, and sparsity.\n\u2022 WNUT'17 [8] is a collection of noisy user-generated text from social media platforms. This dataset contains annotations for 6 entity types, including 'corporation', 'creative-work', 'group', 'location', 'person', and 'product'.\n\u2022 GUM [35] stands as a versatile, open-source multilayer re-source, encompassing a spectrum of twelve text genres in-cluding narratives, interviews, news, instructions, and aca-demic writing. It covers 11 entity types such as time, object, quantity, organization and other entities.\n\u2022 I2B2 [20] is annotated for Protected Health Information (PHI) and disease Risk Factors, serves as a critical resource within the medical domain. We specially focus on 6 entity recognition of PHI, like 'Patient ID', 'Hospital Location', 'Visit Date', 'Patient Profession', and 'Profession Contact'.\n\u2022 OntoNotes [32] is a large-scale, multilingual corpus that is collected from news, conversational telephone speech, weblogs and broadcast. This paper focuses on 18 entity types, including 'Geopolitical Entity', 'Organization', 'Per-son', 'Location', 'Money', 'Facility', 'Date', 'Ordinal', 'Quan-tity', 'Time', 'Nationalities, Religious or Political Groups', 'Cardinal', 'Percent', 'Event', 'Work of Art', 'Language', 'Law', and 'Product'.\n\u2022 CoNLL'03 [23] is also a benchmark dataset that focuses on 4 types of entities: persons, locations, organizations, and miscellaneous entities that do not belong to the previous three categories.\nAll of the above datasets use the N-way and 5-shot setting for training. For a fair comparison on those datasets, we split long sentences in some datasets into multiple shorter sentences to accommodate the input token limit of LLM, thus facilitating the extraction of text information by CLLMFS. We conducted tests on the WNUT'17, GUM, I2B2, OntoNotes, and CoNLL\u203203 datasets, utilizing approximately 1,200, 800, 750, 10,000, and 1,100 instances, respectively.\n4.2 Experimental Settings\n4.2.1 Evaluation Metrics\nTo compare our model with previous state-of-the-art (SOTA) models, we evaluate its performance by computing the micro-F1 score across the target domain.\n\u2022 INTRA setting: In traditional NER datasets such as WNUT'17, GUM, I2B2, OntoNotes, and CoNLL'03, distinct tag-set distributions are present. To address this, we generate"}, {"title": "4.3 Performance Comparison", "content": "The performance comparison in Table 1 illustrates CLLMFS's superior effectiveness, achieving new state-of-the-art (SOTA) results. Across different datasets, CLLMFS shows substantial improvements over previous SOTA models, with average rela-tive gains of 62.54% and 86.28% in micro F1 under the INTRA and INTER settings, respectively.\nCLLMFS excels in various challenging scenarios, spanning both within-domain (INTRA) and cross-domain (INTER) NER tasks. Conventional baseline models, such as ProML, face dif-ficulties in adapting to unseen text domains like GUM due to limited prompt design and methodological constraints. CON-TaiNER, although effective in few-shot NER, struggles with"}, {"title": "4.4 Ablation Analysis", "content": "We conducted ablation experiments to systematically inves-tigate the impact of each constituent module on the perfor-mance of CLLMFS, which comprises three essential modules: Low-Rank Adaptation (LoRA), Contrastive Learning (CL), and Uniform Gaussian Random Noise (Noise). Due to the complex-ity of computations involved in the LLAMA 2 model without utilizing LoRA technology, the computational resources avail-able were insufficient to execute the model. Consequently, this experiment is excluded from consideration.\nAs depicted in Table 2, we observed a clear trend of per-formance improvement with the inclusion of each additional module, which demonstrates the beneficial impact of incorpo-rating Noise in conjunction with LoRA and CL, further bolster-ing the model's overall performance."}, {"title": "4.5 Influence of LoRA module selections", "content": "To enhance entity extraction tasks in low-resource settings, we systematically investigated the impact of LoRA module selections within the CLLMFS architecture. This architecture includes four weight matrices in the self-attention module (Wq, Wk, Wo, Wo), two in the MLP module (Win, Wout), and one for word token embeddings (Wwte). We applied LoRA to each weight matrix and explored the optimal configurations to max-imize performance.\nUsing 5-fold cross-validation, we ensured the robustness of our findings, averaging performance over five iterations. The results, summarized in Table 3, show that configurations in-volving LoRA on Wq, Wk, W\u028a, and Win consistently outperform others. However, adding LoRA to Wo, Wout and Wwte does not consistently improve performance, as indicated by varying F1-scores across different configurations.\nOverall, these findings underscore the importance of careful parameter tuning in optimizing the effectiveness of the LoRA module for few-shot NER tasks. The observed performance variations highlight the intricate interplay between different module parameters and their collective impact on model per-formance."}, {"title": "5 Discussion", "content": "Our proposed CLLMFS framework achieves promising per-formance in NER task. Different from previous few-shot NER methods, CLLMFS fine-tunes the model and leverages LLM's capabilities by constructing entity SFT data from limited data, enhancing generalization for few-shot NER tasks. Different from previous meta learning methods, CLLMFS leverages abundant semantic information in LLMs, achieving consistent performance across target domains, even with limited source domain samples. Different from previous in-context learning methods, CLLMFS integrates SFT data for fine-tuning and in-troduces contrastive learning for FS-NER, enhancing boundary awareness and entity recognition accuracy. Please refer to the Appendix B for some study cases."}, {"title": "6 Conclusion", "content": "In this paper, we first propose CLLMFS by enhancing the large language model with contrastive learning for few-shot NER. Our method leverages the inherent knowledge within LLM and utilizes LoRA for supervised fine-tuning. By inte-grating contrastive learning, CLLMFS enhances LLM's ability of boundary awareness and entity extraction accuracy. Our ap-proach has achieved state-of-the-art performance on multiple datasets with limited labeled data. The cross-domain exper-iment results confirm that the strong transfer capabilities of CLLMFS across different domains. In the future, we will con-centrate on named entity recognition and extend our current work to relation extraction."}]}