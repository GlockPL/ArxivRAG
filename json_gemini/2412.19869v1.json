{"title": "A Fully Hardware Implemented Accelerator Design in ReRAM Analog Computing without ADCS", "authors": ["Peng Dang", "Huawei Li", "Wei Wang"], "abstract": "Emerging ReRAM-based accelerators process neural networks via analog Computing-in-Memory (CiM) for ultra-high energy efficiency. However, significant overhead in peripheral circuits and complex nonlinear activation modes constrain system energy efficiency improvements. This work explores the hardware implementation of the Sigmoid and SoftMax activation functions of neural networks with stochastically binarized neurons by utilizing sampled noise signals from ReRAM devices to achieve a stochastic effect. We propose a complete ReRAM-based Analog Computing Accelerator (RACA) that accelerates neural network computation by leveraging stochastically binarized neurons in combination with ReRAM crossbars. The novel circuit design removes significant sources of energy/area efficiency degradation, i.e., the Digital-to-Analog and Analog-to-Digital Converters (DACs and ADCs) as well as the components to explicitly calculate the activation functions. Experimental results show that our proposed design outperforms traditional architectures across all overall performance metrics without compromising inference accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "With the rapid advancement of artificial intelligence (AI), Deep Neural Networks (DNNs) have been widely applied in fields such as image recognition and natural language processing [1]. However, traditional computing platforms are constrained by the \"memory wall\" bottleneck [2], facing significant energy consumption challenges when processing large-scale neural networks. To address this issue, researchers have proposed CiM, a computing paradigm that integrates computation units directly within non-volatile memory, allowing for in-situ computation without data movement and thereby significantly reducing the energy overhead associated with data transfer. Among various emerging memory technologies, ReRAM has become the preferred device for accelerating DNN computations due to its low power consumption, high-speed operations, high endurance, and strong compatibility with CMOS technology [3].\nIn traditional ReRAM accelerators, as shown in Fig. 1, the multiply-accumulate (MAC) operations in neural networks are performed on crossbar arrays, while nonlinear activations are executed in digital CMOS logic units [4], [5]. During a single computation cycle, the input vector is converted to analog voltages through DACs and applied to the crossbar array, where MAC operations are carried out based on Ohm's and Kirchhoff's laws [6], [7], [8]. The results are then read via ADCs, and nonlinear activations are applied in the digital domain. In this process, DACs and ADCs serve as signal conversion bridges between the digital and analog domains. However, data from previous prototype verification systems indicate that DACs and ADCs account for up to 72% of the total energy consumption and occupy as much as 81% of the area [9], with similar figures reported in other studies [10], [11]. This suggests that optimizing ADCs or rethinking the hardware architecture is crucial to reducing energy consumption."}, {"title": "II. BACKGROUND", "content": "The noise current in electronic devices is an internal random variation present widely in electronic components, including ReRAM. Theoretically, noise can stem from various sources, including thermal noise, flicker noise and others. Thermal noise is a major contributor, arising from the random motion of electrons within the device and is directly proportional to temperature. Thermal noise can be estimated using the Nyquist formula [13], which is expressed as:\n$\\\u0130RMS = \\sqrt{4kTGAf}$ (1)\nwhere $i_{RMS}$ is the root mean square (RMS) value of the noise current, K is the Boltzmann constant, T is the temperature, G is the conductance, and $\\Delta f$ is the bandwidth [13]. Please note that this is a simplified model and does not consider other potential sources of noise.\nSignal-to-Noise Ratio (SNR) is a metric used to measure the relative strength between the signal and noise, typically expressed in decibels (dB) [14]. The definition of SNR is the ratio of signal power to noise power, expressed as:\n$SNR = 10 \\log_{10} \\frac{P_{signal}}{P_{noise}}$ (2)\nwhere $P_{signal}$ is the signal power, and $P_{noise}$ is the noise power. In electronic circuits, noise current is typically related to noise power. Noise power can be expressed in terms of the RMS value of noise current and resistance, for example:\n$P_{noise} = i_{RMS}^2 R$ (3)\nwhere $i_{RMS}$ is the RMS value of noise current, and R is the resistance. This implies that the SNR depends on the signal power, the RMS value of the noise current, and the resistance. In practical circuit design, to enhance circuit performance, it is expected to reduce the noise current to improve the SNR."}, {"title": "B. Weight Mapping", "content": "A crossbar array of ReRAM allows for parallel scaling and computation of input signals (x) and weight parameters (W). Input signals are transmitted through the crossbar array's rows and columns of input lines. Each row corresponds to an input feature in the neural network, and each column corresponds to a neuron [15], [14], [16]. During accelerated neural network computation, a voltage sequence is used to implement input signals, and the conductance of the crossbar array simulates artificial synapses to represent weights (Eq. 4-6). In non-volatile memory like ReRAM, weight dynamic mapping can be achieved by adjusting the device's conductance [17], [18]. Efficient dot product calculations are realized by applying physical laws. Subsequently, the current at the output represents the accumulated scaled signals (Eq. 7).\n$\\begin{aligned}G_{ij} &= G_0 + \\frac{G_{\\max} - G_{\\min}}{W_{\\max} - W_{\\min}} W_{ij} \\\\\nG_{ref} &= \\frac{G_{\\max} W_{\\min} - W_{\\min} G_{\\max}}{W_{\\max} - W_{\\min}} \\\\\nV_j &= x_j V_r\n\\end{aligned}$ (4)\n$\\begin{aligned}G_{ij} &= W_{ij} G_0 + G_{ref}\n\\end{aligned}$ (5)\n$\\begin{aligned}I_j &= \\sum_i G_{ij} V_i\\end{aligned}$ (6)\n$\\begin{aligned}I_j &= \\sum_i W_{ij} x_i V_r\\end{aligned}$ (7)\nThese equations demonstrate the weight mapping process, where ReRAM crossbar arrays perform parallelized scaling of input information, incorporating weights to accumulate and output results."}, {"title": "III. METHODOLOGY", "content": "Stochastic Binary Neural Networks (SBNNs) use random thresholds to process neuron weights and activations, reducing computational and storage demands while maintaining model performance [14], [19]. In SBNNs, the activation of each neuron is stochastically binarized during each forward pass [20]. Binarization of neuron activation values using a stochastic threshold method, assuming x is the original activation of the neuron, and f (x) is the binarization function for activation, can be expressed as follows:\n$f(x) = \\begin{cases} 1, & x \\geq p \\\\ 0, & \\text{otherwise} \\end{cases}$ (8)\nwhere p is typically a stochastic threshold value between 0 and 1, determining the probability of activating binary values to 1 [21], [22]. As the activation for SBNNs involves discrete binarization operations, this provides a basis for reforming CiM architectures that traditionally rely on ADCs and DACs for nonlinear activations."}, {"title": "B. WTA Binary Stochastic SoftMax Neuron", "content": "In the hardware implementation of the classifier, we utilize the WTA [12] strategy for handling multi-class classification tasks. This approach prioritizes the category with the highest score (or probability) as the decisive classification result. The WTA mechanism ensures that only one neuron is activated, with its activation intensity being mapped to a probability distribution. Ultimately, these individual probabilities are then amalgamated into a cumulative probability distribution, signifying the likelihood predicted to each category. We choose the category with the highest cumulative probability as the final predicted result during the classification process."}, {"title": "C. RACA Architecture", "content": "The proposed RACA architecture consists of cascaded layers of Sigmoid neurons (as detailed in Section III-A) and SoftMax neurons (as explained in Section III-B), where each layer integrates ReRAM crossbar arrays and activation circuits. Similar to the original SBNN, a DAC is used at the input stage to preserve the integrity of input data features. However, with our implemented Sigmoid and SoftMax neuron activation circuits, DACs and ADCs can be removed from the hidden and output layers. It should be noted that the hardware for computing the cumulative probability distribution of the classifier is not provided here, but it can be easily implemented at the output end with a simple counter. This mixed-signal design, which operates without ADCs and DACs, is also compatible with mainstream digital devices in terms of signal domain. Moreover, the number of neural network layers and specifications supported by this architecture can be flexibly configured by the user to accommodate various computational tasks."}, {"title": "IV. EVALUATION", "content": "The proposed design's inference performance was tested using a fully trained fully connected neural network (FCNN) with a structure [784, 500, 300, 10] on the MNIST dataset. Binary stochastic Sigmoid neurons were used for the first two layers, and WTA binary stochastic SoftMax neurons were used for the final classification layer. For a specific neural network architecture, $N_{col}$, $G_0$ are immutable constants. In this case, proper SNR can be achieved to mimic the desired Sigmoid function by adjusting the $V_i$ and $\\Delta f$ parameters. In evaluation, we utilized Ag:Si devices fabricated with 32 nm process. To lower the SNR, the read voltage should be much smaller than the usual read voltage of ReRAM, resulting in lower energy consumption, in addition to the reduced complexity of the neuron circuits.\nTest Accuracy: Due to the stochastic nature of the inference, a single inference test meets an unavoidable decrease in recognition accuracy. However, repeating the stochastic inference and making a majority vote of the WTA SoftMax neurons could quickly improve the overall recognition accuracy, as shown in Fig.6(a) and (b). The findings also suggest that there is a wide range of values that can be utilized for our designed activation function, indicating improved robustness of the system. For the SoftMax neurons, the rest state threshold voltage is an important parameter. Small undermines the approximation in Eq. 1, while high decreases the activation probability of each neuron, which prolongs a single decision time. Our results demonstrate that with $V_{th0}$ set to 0.05V, the system's inference accuracy increases with the number of tests and achieves a final accuracy of 96.7%. When $V_{th0}$ is set to 0 V, an accuracy of 96% is achieved, although it falls slightly below the accuracy obtained with $V_{th0}$=0.05 V for fewer test iterations."}, {"title": "V. CONCLUSION", "content": "We propose a novel circuit design that leverages hardware noise signals to emulate the Sigmoid and SoftMax functions in neural network inference. These stochastically activated neurons are achieved by appropriately reducing the SNR of the crossbar array in artificial synapses. We circumvent the need for precisely sensing the output of the crossbar array and explicitly computing the activation functions, thereby realizing a highly simplified, efficient, and biologically inspired method for neural network inference."}]}