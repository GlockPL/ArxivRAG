{"title": "RusCode: Russian Cultural Code Benchmark for\nText-to-Image Generation", "authors": ["Viacheslav Vasilev", "Julia Agafonova", "Nikolai Gerasimenko", "Alexander Kapitanov", "Polina Mikhailova", "Evelina Mironova", "Denis Dimitrov"], "abstract": "Text-to-image generation models have gained\npopularity among users around the world. How-\never, many of these models exhibit a strong bias\ntoward English-speaking cultures, ignoring or\nmisrepresenting the unique characteristics of\nother language groups, countries, and nationali-\nties. The lack of cultural awareness can reduce\nthe generation quality and lead to undesirable\nconsequences such as unintentional insult, and\nthe spread of prejudice. In contrast to the field\nof natural language processing, cultural aware-\nness in computer vision has not been explored\nas extensively. In this paper, we strive to reduce\nthis gap. We propose a RusCode benchmark for\nevaluating the quality of text-to-image genera-\ntion containing elements of the Russian cultural\ncode. To do this, we form a list of 19 categories\nthat best represent the features of Russian vi-\nsual culture. Our final dataset consists of 1250\ntext prompts in Russian and their translations\ninto English. The prompts cover a wide range\nof topics, including complex concepts from art,\npopular culture, folk traditions, famous peo-\nple's names, natural objects, scientific achieve-\nments, etc. We present the results of a human\nevaluation of the side-by-side comparison of\nRussian visual concepts representations using\npopular generative models.", "sections": [{"title": "1 Introduction", "content": "In recent years, text-to-image (T2I) generation\nmodels have achieved a high level of photorealism\nand comprehension of complex textual prompts\n(Betker et al., 2023; Esser et al., 2024; Kastryulin\net al., 2024; Arkhipkin et al., 2024; Vladimir et al.,\n2024). This has significantly expanded the poten-\ntial for using them in various applications, such as\nadvertising, design, education, and art. As these\nmodels work with both visual and textual concepts,\ntheir operation is closely related to various aspects\nof human culture. The increasing popularity of\ngenerative systems available to users worldwide\nmeans that models need to understand text prompts\ncontaining specific elements from various cultures.\nHowever, as a general rule, these models are trained\nusing large open datasets or data collected from\nthe Internet. Due to the widespread influence of\nEnglish-speaking popular culture, there is a lack of\ncultural understanding of other geographical, na-\ntional, and social groups among generation models\n(Basu et al., 2023; Qadri et al., 2023; Mim et al.,\n2024). These restrictions will undoubtedly lead\nto incorrect generation results for specific cultural\nconcepts, a loss of user interest, and a limited ap-\nplicability of the model for real-world tasks. In\nthe worst-case scenario, this could lead to undesir-\nable social outcomes, such as unintentional insults\n(Ghosh et al., 2024), inciting hostility, spreading\nmisinformation, and perpetuating stereotypes and\nsocial biases (Naik and Nushi, 2023; Cho et al.,\n2023; Luccioni et al., 2024). This is one of the\nmain reasons for many concerns regarding the use\nof generative Artificial Intelligence (AI) in general\n(Weidinger et al., 2023; Bird et al., 2023).\nAs human culture is linked to language, simi-\nlar challenges have been considered previously in\nnatural language processing (NLP) (Hershcovich\net al., 2022; Yu et al., 2023; Cao et al., 2024a).\nHowever, the cultural awareness issue in visual\ngeneration tasks remains largely unexplored. We\nunderstand cultural awareness in the generation\nmodel as knowledge of the cultural code. We mean\nthe cultural code as a diverse set of concepts that\nmembers of a particular social group or national-\nity regularly encounter. These concepts are often\nan integral part of a person's cultural background\nand are widely accepted for communication within\nspecific communities (Corner, 1980). At the same\ntime, these concepts may be unfamiliar or even in-\ncomprehensible to other people, as they can contain\ncomplex, metaphorical, and fantastical elements.\nIn this paper, we present the RusCode bench-\nmark dataset for evaluating the quality of image\ngeneration based on textual descriptions that in-\nclude concepts from the Russian culture. We con-\nduct cultural analysis with the participation of ex-\nperts from various fields of the humanities, such\nas history, literature, sociology, psychology, and\nphilology. Based on these diverse perspectives, we\ncreate a list of 19 categories that cover various as-\npects of Russian culture. We aim to develop a sys-\ntem of concepts that will be easily understood by\nmost native Russian speakers. As a result, we con-\nstruct a dataset consisting of 1250 complex textual\ndescriptions in Russian and English, which reflect\nthe contextual use of many concepts from tradi-\ntional and modern Russian culture. When creating\nthe prompts, we took into account the opinions of\n13 people from various backgrounds, professions,\nand age groups. These descriptions include histori-\ncal, artistic, folkloric, natural, technical and other\nelements. We also associate a real reference image\nof a particular entity with each prompt. These im-\nages can be used to evaluate the generation quality\nfor the elements of Russian culture with the partici-\npation of people who are unfamiliar with Russian\nculture in detail. This allows one to use our dataset\nto evaluate multicultural image generation models.\nWe use the collected prompts to generate images\nusing popular models such as Stable Diffusion 3\n(Esser et al., 2024), DALL-E 3 (Betker et al., 2023),\nKandinsky 3.1 (Arkhipkin et al., 2024; Vladimir\net al., 2024), and YandexART 2 (Kastryulin et al.,\n2024). The results of a human evaluation of the\nside-by-side comparison of these models provide\ninsight into the current state of multicultural under-\nstanding in the modern state of T2I generation.\nThus, the contribution of our work is as follows:\n\u2022 We analyze the concept of the Russian visual\ncultural code and create a list of categories that\nrepresent the basic cultural background of a\nnative speaker within the context of Russian\nculture;\n\u2022 We present a RusCode benchmark dataset of\ntextual descriptions of Russian cultural con-\ncepts that can be used to assess the cultural\nawareness of text-to-image models\u00b9;\n\u2022 We report on the human evaluation results of\nthe side-by-side comparison of 4 popular text-\nto-image generation models using collected\nprompts."}, {"title": "2 Related Works", "content": "We define that a model has a high level of cultural\nawareness if it can generate semantically correct\nresults for text prompts that contain specific con-\ncepts related to a particular culture. Multicultural\nawareness involves understanding of linguistic and\nsemantic features (Wibowo et al., 2023), as well\nas correctly semantic matching of concepts from\ndifferent cultures (Cao et al., 2024b). Earlier, a\ntendency towards Western culture in generative\nmodels has been noted (Bhatia et al., 2024; Naik\nand Nushi, 2023; Berg et al., 2022). As language\nis a conduit of culture (Ventura et al., 2023), many\nNLP studies have focused on the issue of cultural\nawareness. This includes the task of adaptive trans-\nlation (Peskov et al., 2021), offensive language\ndetection (Zhou et al., 2023; Awal et al., 2024),\ndialog systems operation (Cao et al., 2024a) and\nother tasks (Hershcovich et al., 2022). The devel-\nopment of visual-language models (VLM) has led\nto a transfer of cultural awareness issues for the\nmultimodal architectures (Nayak et al., 2024). This\nproblem was considered in the context of the visual\nquestion answering (VQA) task (Becattini et al.,\n2023; Romero et al., 2024), image-text retrieval\nand grounding (Bhatia et al., 2024). With the ad-\ndition of a new modality, the problem of cultural\nawareness has become more acute. For example,\nthere are different levels of understanding of con-\ncepts from regional cultures among modern VLMs\n(Nayak et al., 2024). In text-to-image generation,\nquality metrics have long focused on the aesthet-\nics and photorealism of generated results, while\nignoring cultural awareness (Kannen et al., 2024)\nSeveral studies have identified significant gaps in\nthe level of multicultural awareness for the most\npopular T2I models. As far as we know, our work\nrepresents the first comprehensive approach to the\nissue of cultural awareness in relation to Russian\nculture in the T2I task."}, {"title": "2.2 Multicultural Benchmarks", "content": "Benchmarks for evaluating the multicultural and\nmultilingual abilities of generative models primar-\nily emerged in NLP tasks. Due to the fact that most\nexisting language models are designed primarily\nfor English, several studies have focused on as-\nsessing the linguistic and grammatical features of\nother languages (Cahyawijaya et al., 2021; Zhang\net al., 2022; Mukherjee et al., 2024; Kim et al.,"}, {"title": "2.3 Ethics and Social Biases in Generative AI", "content": "Insufficient cultural awareness of image genera-\ntion models can lead to the spread of social bi-\nases, misinformation, and offensive content (Naik\nand Nushi, 2023; Cho et al., 2023; Luccioni et al.,\n2024). A number of studies have focused on reduc-\ning the biases in generative models that are based\non factors such as race, skin color, gender, geogra-"}, {"title": "3 Cultural Code Analysis", "content": "The concept of cultural code is a complex idea\nthat draws upon various fields such as history, cul-\ntural studies, sociology, philosophy, semiotics, and\ncommunication theory. The cultural code of a par-\nticular community is formed by symbolic systems\nsuch as language, art, as well as traditions, along\nwith norms, values, social practices, and histori-\ncal background. Popular culture, visual media and\nother forms of information significantly contribute\nto shaping the cultural code (Corner, 1980). Gen-\neration models need to be trained on a deeper un-\nderstanding of cultural codes. This would improve\nthe visual quality of their outputs, enhance their\ninterpretative and communicative abilities, and en-\nable the creation of systems that are sensitive to\ncultural diversity. This in turn would reduce biases\nand foster a more ethical approach in the content\ngeneration.\nIn this study, we explore the Russian cultural\ncode. Drawing on existing research (Goloubkov,\n2013; Billington, 2010; Figes, 2002; Stites, 1992),\nwe highlight language, literature, art, religion, phi-\nlosophy, folklore, and history as central compo-\nnents of Russian cultural identity. Since language\nreflects cultural characteristics (Wierzbicka, 2002),\nit is essential for the model to accurately inter-\npret metaphors, proverbs, and figurative expres-\nsions that are common in everyday communication.\nFrom a visual standpoint, we also consider ele-\nments of contemporary popular culture, such as\nfilm and TV, as well as geographical places and\nnatural objects. To ensure that our data selection"}, {"title": "4 Dataset", "content": "After defining a list of main\ncategories and subcategories that represent the\nRussian cultural code (Section 3), we have cre-"}, {"title": "4.1 Prompts Creation", "content": "It was essential for us that\nthe prompts reflect the diverse experiences of peo-\nple from the Russian culture. We have assem-\nbled a team of 13 prompt-engineers, including na-\ntive speakers and professionals from various back-\ngrounds. It includes a doctor, a manager, a cook,\na pharmacist, a translator, an editor, a photojour-\nnalist, a psychologist, a car mechanic, a builder, a\nlogistics expert, a linguist, and a copywriter. The\nage of people ranged from 19 to 46 years old. All\nprompt-engineers had previous experience in cre-\nating textual description datasets for generation\nmodels. They were officially employed when they\ncompleted their task and were aware of the work's\nobjectives and the possible disclosure of their main\nareas of activity. Each team member has received\ninstructions and been made aware of the rules for\ndata collection, including ethical considerations\nand copyright laws."}, {"title": "Prompting", "content": "We did not expressly limit the au-\nthors in any way at the initial stage of creating\nprompts. We recommended that they rely on their\nown experience and imagination. They were also\nprovided with visual examples of how images with\nthe Russian cultural code should look, descriptions\nof which they should create. Prompt-engineers ac-\ntively used reference literature, books on painting\nand history, as well as open resources on the Inter-\nnet. In order to avoid potential inaccuracies and\nenrich the dataset with more specific concepts, they\ndid not utilize large language models. The final\nbreakdown of the number of collected prompts by\ncategory is presented in Figure 3."}, {"title": "4.2 Prompts Filtering and Post-processing", "content": "After the initial collection of prompts for each sub-\ncategory, they were selected and filtered by two ex-\nperienced and professional prompt-engineers, who\nalso contributed to the creation of a list of cate-\ngories and subcategories (Section 3). During the\nselection process, they were guided by their experi-\nence in creating high-quality and effective prompts,\nas well as by the idea of what prompts people use\nwhen they think about a particular entity and want\nto generate image with it. Additionally, profes-\nsional prompt-engineers have corrected and rewrit-\nten the descriptions based on the results of popular\nqueries in search engines related to Russian culture.\nThey checked the correctness of the descriptions to"}, {"title": "5 Evaluation", "content": "We used the RusCode\ndataset to generate images using four popular T2I\nmodels, such as Stable Diffusion 3 (Esser et al.,"}, {"title": "Human evaluation", "content": "We compared each of the\nfour models side-by-side with the other three, con-\nducting a human evaluation study. Each person was\nshown simultaneously the generations of two mod-\nels without specifying their names. The task was\nto choose the image that most accurately matches\nthe text description. A team of 48 people who\nwere not involved in the creation of the dataset\nparticipated in the evaluation of the generated con-\ntent. The age range of the participants was between\n18 and 54 years. The fields of study and profes-\nsions of the participants covered information sys-\ntems, anthropology, programming, law, economics,\nphilosophy, philology, linguistics, regional stud-\nies, political science, design, pedagogy, journalism,\necology, finance, sports, management, agriculture,\nand more. Each person viewed approximately 125\nimage pairs. The results of a general compari-\nson across all categories are presented in Figure\n5. The results of comparing models in individual\ncategories can be found in the Appendix B. As can\nbe seen, the Kandinsky 3.1 and YandexART 2 mod-\nels significantly outperform the Stable Diffusion\n3 and DALL-E 3 models. This indicates a lack of\nunderstanding of Russian culture among some of\nthe most popular generative models. The Appendix\nA contains the results of comparing the Kandinsky\n3.1 and Midjourney v6 (Midjourney, 2022) models.\nIt is important to note that sometimes models\nblocked generation for some prompts due to ex-\ncessive self-censorship. For example, the Yan-\ndexART 2 model blocked the prompts \"opera War\nand Peace, ball scene\u201d, \u201cMonument to the\nheroes of the Battle of Stalingrad on\nMamayev Kurgan\u201d, \u201cOstankino TV Tower at\nNight\", \"Mikhail Gorbachev in a hat\", etc.\nThese and other prompts from our dataset do not\ncontain any real offensive content. As a rule, au-\ntomatic censorship does not react correctly to the\nmention of anything related to historical military\ntopics or specific historical figures. When evaluat-\ning, we considered such censorship as a \"bad\" case.\nThe Midjourney v6 model allowed us to use only\n974 prompts out of 1250, so we did not include a\ncomparison with it in the main text.\""}, {"title": "CLIP Score", "content": "We used CLIP Score (Radford et al.,\n2021) to try to automatically assess the cultural\nawareness of the models on our dataset. The simi-\nlarity score between the embeddings of the English\nprompts and the corresponding image embeddings\nare presented in Table 2. As can be seen, the results\nfor all models are quite high and do not correlate\nwith the human evaluation results. This confirms\nthe inadequacy of using CLIP score for the cultural\nawareness assessment."}, {"title": "6 Discussion", "content": "We identified the features\nof errors that models encounter in trying to gener-\nate something from the Russian cultural code. In\nthe absence of appropriate training examples, the\nmodel, even using a sufficiently detailed textual de-\nscription, will not be able to correctly generate the\nnecessary entity. Nevertheless, for large popular\nmodels, we observe a distortion of the entity or a\ndisplay of an international concept rather than its\nreplacement by an entity from another culture."}, {"title": "Automatic metrics", "content": "As far as we know, there\nare currently no automatic metrics for assessing\nthe cultural awareness of image generation models.\nThe use of automatic metrics such as CLIP-score is\nnot suitable for this task, since the evaluator model\nitself has a low level of cultural awareness (Table 2).\nThis leads to the need to rely primarily on human\nevaluation, although we believe that our benchmark\ncan lead to the development of automated tests for\nthis task, for example, based on modern visual-\nlanguage models, finetuned for cultural specifics."}, {"title": "Causes and mitigation of the cultural awareness\ngap", "content": "We explain the advantage of the Kandin-\nsky 3.1 and YandexArt 2 models by the presence\nof training data in the domain of Russian culture.\nThe authors of Kandinsky 3.1 write about this ex-\nplicitly in their technical report (Arkhipkin et al.,\n2024), while it is not exactly known for YandexArt\n2. However, the fact that YandexArt 2 is primarily\nfocused on interacting with Russian users allows\nus to make such an argumentative assumption. Fol-\nlowing Kandinsky 3.1, we think that fine-tuning\nbased on specific culture data will significantly im-\nprove the cultural awarness of the model. We also\nnote that retrieval-augmented generation (RAG)\nmethods (Lewis et al., 2020) can be productive in\nthis direction."}, {"title": "7 Conclusion", "content": "In this paper, we proposed an open T2I benchmark\ndataset RusCode, which contains 1250 prompts in\nRussian and their corresponding translations into\nEnglish. The dataset will be published under the\nMIT license. As far as we know, this is the first\nstudy in which the Russian cultural code has been\nexamined in such a comprehensive and detailed\nmanner. Despite the complexity of analyzing the\nnational cultural code of any country, we have man-\naged to create a system of categories and subcate-\ngories that accurately reflects the basic understand-\ning of average users regarding prompts related to\nRussian visual concepts. The generation results\nof popular T2I models proof the existence of a\ncultural awareness issue, even though, in general,\nthese models have some knowledge of generalized\nconcepts. We strive to expand the use of our dataset.\nTo do this, we attach reference images to the textual\nprompts, which can be used to mark the correctness\nof the generated entity. In the future, we aim to\nattract new experts and significantly increase our\ndataset, both in terms of the number of categories\nand the number of prompts in each subcategory. In\nthe future, we also plan to expand this approach\nfor video generation task (Arkhipkin et al., 2023,\n2025)."}, {"title": "8 Limitations", "content": "The assessment\nof the visual generation quality of elements of the\nRussian cultural code, which can be obtained using\nour dataset, may still not give a complete under-\nstanding of the capabilities of the generation model.\nThis is directly related to the complexity and ambi-\nguity of the concept of the Russian cultural code,\nwhich we significantly narrow down by providing\na specific list of categories. This list could be sig-\nnificantly expanded, but we have focused on the\nmost common concepts among ordinary users. At\nthe same time, we do not exclude the fact that the\ndataset could contain data that requires more profes-\nsional knowledge and goes beyond basic erudition\nor, for example, the school curriculum."}, {"title": "Insufficient representation of individual subcat-\negories", "content": "Within each subcategory, we have pre-\nsented only 10 prompts to balance the data and\navoid giving preference to any particular topic. At\nthe same time, the importance of individual sub-\ncategories, such as \u201cProse\u201d within the category of"}, {"title": "9 Ethical Statement", "content": "We have avoided any poten-\ntially offensive or discriminatory concepts in our\ndataset. We do not include any racial prejudices\nor historical elements that might indicate a biased\nattitude towards any group in the concept of the\ncultural code. We strongly oppose nationalism and\nxenophobia. However, some elements of traditional\nculture might conflict with modern views of indi-\nviduals or social groups. It is important to treat\nthis with an understanding of their historical and\ncultural context."}, {"title": "Personal data", "content": "Our dataset contains names and\nportraits of well-known historical figures of Rus-\nsian culture. We would like to emphasize that we\nhave not violated their privacy, as all information\nand images used in our dataset were obtained from\nopen sources."}, {"title": "Usage", "content": "Our research aims to promote multicul-\nturalism and diversity in artificial intelligence. We\noppose using our data for any illegal purposes, in-\ncluding incitement of hostility, hatred, or the cre-\nation of technologies that misinform, create false,\nor politically biased materials."}, {"title": "Payment for prompt-engineers and evaluators", "content": "We properly paid the work of prompt-engineers\nwho participated in the collection of the dataset\n(Section 4.1), and the participants of the human\nevaluation study (Section 5). The average salary for\neach person exceeded the average salary in the city\nof his residence, according to publicly available\ngovernment statistics."}, {"title": "A Side-by-side evaluation for Kandinsky\n3.1 and Midjourney v6", "content": "According to the main results of the quality assess-\nment, the YandexART 2 and Kandinsky 3.1 models\nwere in the lead, but the YandexART 2 model of-\nten censored prompts and did not generate images.\nFor this reason, we chose the Kandinsky 3.1 and\nadditionally compared it with the Midjourney v6\nmodel (Midjourney, 2022). As can be seen from\nthe Figure 6, the models show competitive quality."}]}