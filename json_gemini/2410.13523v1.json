{"title": "CAN MEDICAL VISION-LANGUAGE PRE-TRAINING SUCCEED WITH PURELY SYNTHETIC DATA?", "authors": ["Che Liu", "Zhongwei Wan", "Haozhe Wang", "Yinda Chen", "Talha Qaiser", "Chen Jin", "Fariba Yousefi", "Nikolay Burlutskiy", "Rossella Arcucci"], "abstract": "Medical Vision-Language Pre-training (MedVLP) has made significant progress in enabling zero-shot tasks for medical image understanding. However, training MedVLP models typically requires large-scale datasets with paired, high-quality image-text data, which are scarce in the medical domain. Recent advancements in Large Language Models (LLMs) and diffusion models have made it possible to generate large-scale synthetic image-text pairs. This raises the question: Can MedVLP succeed using purely synthetic data? To address this, we use off-the-shelf generative models to create synthetic radiology reports and paired Chest X-ray (CXR) images, and propose an automated pipeline to build a diverse, high-quality synthetic dataset, enabling a rigorous study that isolates model and training settings, focusing entirely from the data perspective. Our results show that MedVLP models trained exclusively on synthetic data outperform those trained on real data by 3.8% in averaged AUC on zero-shot classification. Moreover, using a combination of synthetic and real data leads to a further improvement of 9.07%. Additionally, MedVLP models trained on synthetic or mixed data consistently outperform those trained on real data in zero-shot grounding, as well as in fine-tuned classification and segmentation tasks. Our analysis suggests MedVLP trained on well-designed synthetic data can outperform models trained on real datasets, which may be limited by low-quality samples and long-tailed distributions.", "sections": [{"title": "1 INTRODUCTION", "content": "In medical image analysis, learning representative features typically requires labor-intensive and costly image annotations (Ronneberger et al., 2015; Liu et al., 2023b). Medical Vision-Language Pre-training (MedVLP) addresses this challenge by aligning vision and language content using paired datasets of images and clinical reports, reducing the need for manual annotations (Radford et al., 2021; Zhang et al., 2020; Wu et al., 2023; Liu et al., 2023a). However, existing MedVLP models rely heavily on large-scale, high-quality paired data (Liu et al., 2023e), which is scarce in practice. Real-world datasets often contain noisy data, such as low-quality images and unpaired image-text samples, degrading model performance (Xie et al., 2024; Bannur et al., 2023). Recent advancements in Large Language Models (LLMs) and diffusion models enable the generation of large-scale synthetic image-text datasets, offering an alternative to traditional data collection. Although these techniques have shown promise in medical tasks, they are primarily used as auxiliary support for real data via augmentation (Chen et al., 2024a; Yao et al., 2021; Chen et al., 2022; Qin et al., 2023), and are often limited to single-modality settings. To the best of our knowledge, no studies have fully explored the potential of using synthetic multimodal data for MedVLP or considered training exclusively on synthetic data (Liu et al., 2023e).\nTo bridge this gap and showcase synthetic data's potential for MedVLP, our contributions are:"}, {"title": "2 RELATED WORK", "content": "Representation Learning with Synthetic Data. Synthetic data has been widely employed across various deep learning fields (Rossenbach et al., 2020; Varol et al., 2017; Jahanian et al., 2022; Zhou et al., 2023; Yang et al., 2020; Li et al., 2023). In visual representation learning, synthetic data has improved model performance in a range of tasks (Richter et al., 2016; Ros et al., 2016; Chen et al., 2019; Johnson-Roberson et al., 2017; Yuan et al., 2024; Shmelkov et al., 2018). Recent efforts have also focused on using synthetic data from text-to-image models to augment real-world data during training (Azizi et al., 2023; Sariyildiz et al., 2023; He et al., 2023). For example, (Yu et al., 2023) introduced a framework to generate synthetic images to diversify existing datasets. Notably, methods utilizing text-to-image generative models (Rombach et al., 2022) have demonstrated that synthetic images guided by real captions can effectively train self-supervised models, achieving performance comparable to that of real images (Tian et al., 2023b).\nFurther advancements like SynCLR (Tian et al., 2023a) have focused on visual representation learning using only synthetic images, generated with conditioning on various categories. Meanwhile, other"}, {"title": "3 METHODS", "content": "3.1 EXPLORING IMPERFECTIONS IN REAL DATA\nFor MedVLP, the most commonly used dataset is MIMIC-CXR (Johnson et al., 2019a;b), a collection of chest x-ray (CXR) images paired with their corresponding textual reports. after following the preprocessing steps outlined in previous works (Zhang et al., 2023; Wang et al., 2022; Huang et al., 2021), this dataset provides a total of 213,384 image-text pairs for pre-training. And all images must be frontal views according to the preprocessing steps outlined in (Huang et al., 2021)."}, {"title": "4 EXPERIMENTS CONFIGURATIONS", "content": "For pre-training, we apply the official configurations provided by ConVIRT (Zhang et al., 2020) and GLORIA (Huang et al., 2021) on the MIMIC-CXR dataset to our synthetic CXR image-text dataset, SynCXR.\n4.1 DOWNSTREAM TASK DATASETS AND CONFIGURATIONS\nFor downstream tasks, we evaluate the effectiveness of synthetic data for MedVLP across four tasks. Details on the datasets and implementation are provided in Appendix, Sec \u0410.\nZero-shot Medical Image Classification. Following the guidelines in (Phan et al., 2024b; Wu et al., 2023), we perform this task on seven datasets: CheXpert (Saporta et al., 2022), ChestXray-14 (Wang et al., 2017), PadChest-seen, PadChest-unseen, PadChest-rare (Bustos et al., 2020), RSNA (Shih et al., 2019), and SIIM (Steven G. Langer & George Shih, 2019), using the dataset splits from (Phan et al., 2024b). Evaluation metrics include AUC, F1, and ACC.\nZero-shot Medical Image Visual Grounding. In line with (Phan et al., 2024b), this task is conducted on the RSNA (Shih et al., 2019), SIIM (Steven G. Langer & George Shih, 2019), and Covid-19 Rural (Desai et al., 2020) datasets, using official splits and metrics. Grounding performance is evaluated with IoU, and Dice score.\nMedical Image Fine-tuned Classification. As described in (Phan et al., 2024b), we use the RSNA (Shih et al., 2019), SIIM (Steven G. Langer & George Shih, 2019), Covid-19 CXR-2 (Pavlova et al., 2022), and ChestXray-14 (Wang et al., 2017) datasets. During fine-tuning, all model parameters, including the pre-trained vision encoder and linear classifier, are updated. The AdamW optimizer is applied with a learning rate of 1 \u00d7 10\u22124, batch size of 64, and training runs for 50 epochs. Evaluation follows the AUC score protocol in (Huang et al., 2021; Wang et al., 2022; Zhou et al.).\nMedical Image Fine-tuned Segmentation. This task uses the RSNA (Shih et al., 2019), SIIM (Steven G. Langer & George Shih, 2019), and Covid-19 Rural (Desai et al., 2020) datasets, following preprocessing from (Wang et al., 2022; Huang et al., 2021). U-Net (Ronneberger et al., 2015) is used for fine-tuning, freezing the pre-trained vision encoder and updating only the decoder parameters."}, {"title": "5 ANALYSIS", "content": "Effect of Balanced Entity Sampling in Generating Synthetic Reports. We evaluate the impact of balanced sampling entities when generating synthetic reports using LLMs. For the synthetic dataset without balanced sampling, we adjust entity frequencies to match their distribution in MIMIC-CXR, leading to a long-tailed distribution. As shown in Tab 4a, for both MedVLP methods, the performance improves significantly when using synthetic datasets generated from balanced sampled entities. This demonstrates that balanced sampling of entities leads to a more representative dataset, benefiting MedVLP performance.\nEvaluating the Contribution of Synthetic Images and Reports. We aim to assess the individual impact of synthetic images and synthetic reports on MedVLP performance. As shown in Tab 4b, we generate two partially synthetic datasets by replacing either the image or the text with synthetic data, while keeping the other components real, to evaluate their respective contributions.\n\u2022 Real Image, Synthetic Report: In this setting, we use MedVersa (Zhou et al., 2024), a state of the art radiology report generation model, to generate synthetic reports for each real CXR image. We then train MedVLP models using these real image and synthetic report pairs.\n\u2022 Real Report, Synthetic Image: In this setting, we use RoentGen (Bluethgen et al., 2024), a text-to-image model, to generate synthetic CXR images for each real report. The \u2018IMPRESSION' section of each report serves as the prompt for generating synthetic CXR images. These synthetic image and real report pairs are used to train MedVLP models.\nAccording to Tab 4b, for both MedVLP methods, using real images with synthetic reports results in decreased performance, likely due to the persistent long-tailed distribution, as the synthetic reports are generated based on real images. However, using real reports with synthetic images slightly improves performance, as synthetic images can be curated using our image filtering procedure to ensure high quality, avoiding issues commonly found in real datasets. Using both synthetic images and synthetic reports achieves the highest performance, indicating that a well-curated synthetic dataset can significantly enhance MedVLP performance."}, {"title": "6 CONCLUSION", "content": "In this work, we tackle the question: Can MedVLP succeed using purely synthetic data? Our findings demonstrate that the answer is: Yes. To the best of our knowledge, this is the first study to comprehensively explore the potential of synthetic data for MedVLP models. We also identify key limitations in existing real-world datasets and introduce SynCXR\u2014a synthetic dataset of 200,000 image-text pairs generated without any manual quality checks. Our findings show that MedVLP models trained on purely synthetic data outperform those trained on real data. Moreover, combining synthetic and real data further boosts model performance, demonstrating the potential of synthetic data to overcome limitations in real-world datasets. We systematically analyze key factors in SynCXR and validate its effectiveness through extensive ablation studies. In summary, we show that MedVLP achieves strong performance using a purely synthetic image-text dataset and benefits significantly from a combination of real and synthetic data. We believe this work will inspire the community to fully leverage synthetic data and mitigate the challenges posed by noisy and limited real-world datasets."}, {"title": "3.  1 EXPLORING IMPERFECTIONS IN REAL DATA", "content": "<CXR Image>, Please check if the given\nimage is a chest X-ray scan. If it is a chest X-ray, return\n'YES'. Otherwise, return 'NO'."}, {"title": "\u2022 Detecting Non-Human CXR Images", "content": "<CXR Image>, Please verify if the\ngiven image is a human chest X-ray scan. If it is a chest\nX-ray, return 'YES'. Otherwise, return 'NO'."}, {"title": "\u2022 Detecting Wrong Views", "content": "<CXR Image>, Please check if the given\nimage is a frontal chest X-ray view. If it is a frontal\nview, return 'YES'. If it is a lateral view or any other\nview, return 'NO'."}, {"title": "\u2022 Assessing Image Quality", "content": "<CXR Image>, Please analyze the provided\nchest X-ray (CXR) image and respond with \u2018NO\u2032 if the image\nquality is poor, such as being blurry, containing artifacts,\nor having poor contrast. Respond with 'YES' if the image\nquality is acceptable."}, {"title": "\u2022 Detecting Artifacts and Overprocessing", "content": "<CXR Image>, Please analyze\nthe following chest X-ray image. Respond with 'YES' if the\nimage is clear, correctly oriented, and free of artifacts\nor imperfections that could affect its diagnostic quality.\nRespond with 'NO' if the image is blurry, incorrectly\noriented, contains artifacts, or has imperfections that make\nit unsuitable for further analysis."}, {"title": "\u2022 Checking High-Fidelity", "content": "<CXR Image>, Please check if the given\nimage is a high-fidelity human chest X-ray scan. If it is a\nhigh-fidelity chest X-ray, return \u2018YES\u2019. Otherwise, return\n'NO'."}, {"title": "For each generation, we sample", "content": "S\u2081 = {e1j, e2j,...,ekj} \ne(i)\u2208{ABNORMALITY, NON-ABNORMALITY, DISEASE, NON-DISEASE}"}, {"title": "For each generation, we sample", "content": "S\u2082 = {a1j, a2,..., amj}, \na(i) \u2208 ANATOMY"}, {"title": "We impose a maximum frequency threshold,", "content": "Tmax, for each entity e \u2208 E. If an entity e(i)in S reaches\nthis threshold, we resample e(i) while keeping the remaining entities in S unchanged:\nif f(e(i)) \u2265 Tmax, then resample ej(i)."}]}