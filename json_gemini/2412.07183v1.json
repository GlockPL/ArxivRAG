{"title": "Exploring What Why and How: A Multifaceted Benchmark for Causation Understanding of Video Anomaly", "authors": ["Hang Du", "Guoshun Nan", "Jiawen Qian", "Wangchenhui Wu", "Wendi Deng", "Hanqing Mu", "Zhenyan Chen", "Pengxuan Mao", "Xiaofeng Tao", "Jun Liu"], "abstract": "Recent advancements in video anomaly understanding (VAU) have opened the door to groundbreaking applications in various fields, such as traffic monitoring and industrial automation. While the current benchmarks in VAU predominantly emphasize the detection and localization of anomalies. Here, we endeavor to delve deeper into the practical aspects of VAU by addressing the essential questions: \"what anomaly occurred?\u201d, \u201cwhy did it happen?\u201d, and \u201chow severe is this abnormal event?\u201d. In pursuit of these answers, we introduce a comprehensive benchmark for Exploring the Causation of Video Anomalies (ECVA). Our benchmark is meticulously designed, with each video accompanied by detailed human annotations. Specifically, each instance of our ECVA involves three sets of human annotations to indicate \u201cwhat\u201d, \u201cwhy\u201d and \u201chow\u201d of an anomaly, including 1) anomaly type, start and end times, and event descriptions, 2) natural language explanations for the cause of an anomaly, and 3) free text reflecting the effect of the abnormality. Building upon this foundation, we propose a novel prompt-based methodology that serves as a baseline for tackling the intricate challenges posed by ECVA. We utilize \u201chard prompt\u201d to guide the model to focus on the critical parts related to video anomaly segments, and \u201c soft prompt\u201d to establish temporal and spatial relationships within these anomaly segments. Furthermore, we propose AnomEval, a specialized evaluation metric crafted to align closely with human judgment criteria for ECVA. This metric leverages the unique features of the ECVA dataset to provide a more comprehensive and reliable assessment of various video large language models. We demonstrate the efficacy of our approach through rigorous experimental analysis and delineate possible avenues for further investigation into the comprehension of video anomaly causation. Our code and dataset are available at https://github.com/Dulpy/ECVA.", "sections": [{"title": "I. INTRODUCTION", "content": "Anomalies represent occurrences or scenarios that deviate from the norm, defying expectations and straying from routine conditions [1]\u2013[4]. These events are typically characterized by their unique, sudden, or infrequent nature, often demanding special attention or intervention [5]. Recently proliferated video anomaly understanding (VAU) [6], [7] aims at automatically comprehending such abnormal events in videos, thereby facilitating various applications such as traffic surveillance, environmental monitoring, and industrial manu-facturing. Towards this direction, video anomaly detection and localization, which refer to identifying abnormal occurrences, and localizing temporally or spatially locate anomalous events in videos, have attracted enormous attention [8]\u2013[16]. Existing VAU benchmarks [17]\u2013[19] and approaches [20]\u2013[28] primarily focus on the aforementioned anomaly detection and localization tasks, while the underlying cause and the cor-responding effect of these occurrences, are still largely under-explored. These cues are crucial for perceiving the abnormality and making decisions based on human-interpretable expla-nations. Figure 1 demonstrates a scene of a traffic accident involving many vehicles. \u201cThe accident occurred because a white car parked by the roadside, and a dark gray car traveled at high speed to swerve and rear-end the black car next to it.\u201d Challenges of comprehending such a cause of the accident include: 1) capturing key cues in the long video: a model"}, {"title": "II. RELATED WORK", "content": "Anomaly Datasets: Existing VAU datasets primarily focus on anomaly detection [47]\u2013[49] and localization [50]\u2013[52], and can be broadly categorized into weakly-supervised ones [9], [39], and semi-supervised ones [2], [5], [40]. Weakly supervised datasets aim to enhance the model\u2019s generalizabil-ity, transferring the anomaly detection capabilities from the training set of normal videos to the test set of abnormal videos [42]. Semi-supervised datasets emphasize the time points or time periods of anomalous events based on frame level or video-piece level annotations [40], [41]. Early semi-supervised datasets are mainly designed to support anomaly detection (VAD) and classification tasks [5], [11], with a limited range of anomaly categories and short average video lengths. Recent VAD datasets [42], [44] leverage BMP or matrix formats to conduct anomaly localization tasks. Additionally, some datasets [2] simulate real-world anomaly events within vir-tual environments, enabling pixel-level annotation for a more precise characterization of anomalies. Our ECVA significantly differs from the existing datasets in these aspects. We manually annotate the \u201cWhat, Why and How\" of the anomaly, and leverage the importance curve to quantify the intensity of the anomaly. Methods: The recent strides in multimodal understanding have primarily been driven by the integration of image-based vision models with LLMs. The majority of video large language models (VLMs) first conduct uniform sampling of video content, followed by the transformation of frames into image tokens through a variety of image encoders such as CLIP [53] or DINO [54]. To accommodate the limited context length of large language models, VLMs employ different vision-language adapters (e.g. cross-attention [55], [56], Q-former [57], and Pooling operations) to compress these video tokens. However, this approach falls short when addressing the task of anomaly understanding in long videos within ECVA. To address the above issues, we propose a novel method called \"AnomShield\u201d. Specifically, we leverage \"hard prompt\" to conduct non-uniform sampling of videos to help models concentrate on the anomaly parts of the video. Then we develop an efficient mechanism to extract the spatial-temporal relationships within these video parts, which significantly enhances the model\u2019s ability to analyze anomaly video content. Evaluation Metrics: VAU evaluation metrics [58] include, reference-based ones such as ROUGE [37] and BLEURT [59], answer-based ones such as BLEU [36], Rankgen [60] and QAFactEval [61], and others such as Longformer [62], UniEval [63] and MoverScore [64]. They essentially evalu-ate the accuracy and completeness of the generated text by calculating the overlap between the generated text and the reference text in different ways. However, they still have shortcomings in capturing semantics. Recently, various GPT-based metrics [65]\u2013[67] have been developed. Compared to traditional methods, GPT-based approaches not only provide a more accurate assessment of generated answers but also consider various dimensions such as richness and detail of the generated answers. Nevertheless, due to the complexity of anomalies and instability of GPT, these evaluation metrics fail to adequately assess the results. We propose a novel evaluation metric \"AnomEval\" based on the unique characteristics of the ECVA dataset. AnomEval not only identifies VLMs\u2019 hallucinations but also provides a more comprehensive and robust assessment of the generated answers. Finally, this work is an extension version from [46]."}, {"title": "III. THE PROPOSED ECVA BENCHMARK", "content": "In this section, we first introduce our ECVA sub-tasks. Then we show how we collect and annotate data. We also provide a quantitative analysis of the benchmark. The overview of our ECVA is demonstrated in Figure 4.\nA. Task Definition\nWhat anomaly occurred: This task includes two ob-jectives: anomaly classification and anomaly description. Anomaly Classification includes all the anomaly classes present in the video, which are taken from our database of predefined anomaly classes as shown in Figure 4a. Each video has multiple anomaly classes at different levels, and this task will challenge the model's ability to detect anomaly classes at multiple levels of granularity. Anomaly Moment Description includes the timestamp in which the anomaly occurs and a detailed description of the anomalous event.\nWhy this anomaly happened: This task aims to describe the causal relationships within the video. Anomaly reasoning describes the reasons for the occurrence of anomalies in the video. This task requires the model to infer the cause of the anomaly based on the video content and describe it in natural language, which challenges the model's ability of video com-prehension and reasoning. Anomaly results primarily describe the impacts caused by anomalous events in the video. It mainly requests that the model combine external world-knowledge to handle details of anomalous events in the video.\nHow severe this anomaly: This task aims to reflect the changing trends in the severity of anomalies within the video. Thus, we propose a novel annotation approach called the importance curve. Details of our importance curve\u2019s pipeline can be found in Figure 3. The algorithm of the importance curve is illustrated in Algorithm 1. This approach has three advantages: 1) It provides an intuitive representation of the temporal variation in anomaly severity within the video. 2) It offers a more intuitive depiction of the inherent causal relationships among anomalous events in the video. 3) Such an approach enables us to unify various Video Temporal Grounding labels and tasks (e.g. Moment Retrieval, Highlight Detection, Video Summarization) under the same framework.\nB. Dataset Collection\nWe crawled data from prominent video platforms such as Bilibili and YouTube\u00b9. We discarded videos that encompass sensitive themes such as pornography and politics. Throughout"}, {"title": "IV. THE PROPOSED METHOD: ANOMALY SHIELD", "content": "In this section, we introduce a novel video large lan-guage model named Anomaly Shield (AnomShield), which is designed to address the two challenges presented by our dataset. To effectively capture crucial cues within long videos, we propose to leverage chain-of-thought prompting to guide VLMs to concentrate on pivotal clues in the video pertinent to the provided questions. By harnessing the exceptional logical reasoning capabilities of large language models (LLMs) to build a logic chain of the cause-effect, we design an efficient pretraining framework to facilitate video-language alignment.\nA. Chain-of-Thought Prompting Design\nTraditional VLMs adopt a uniform sampling strategy to capture the information of the video modality. However, it is evident that not all frames hold equal significance for understanding anomalies. Given the relatively long duration of videos within the ECVA and the limited number of video frames that the VLMs can process. Accurately selecting the anomaly-relevant parts of the video is crucial for the VLM to comprehend the abnormal events precisely. Our chain-of-thought prompting includes four steps: Sparse Sampling, Captioning, Retrieval, and Resampling.\nCoarse Sampling: We initially conduct uniform sampling to extract 32 frames from every piece of video and divide these 32 frames into 8 segments.\nCaptioning: We utilize the image understanding capabilities of our pre-trained VLM to generate captions for each segment.\nRetrieval: We utilize the GPT to meticulously compare the captions of each segment with the user\u2019s query. In this way, we are able to accurately pinpoint the positions of the several top key segments in the video that exhibit the highest degree of alignment with the user\u2019s query.\nResampling: In order to capture the nuances details associated with the anomaly events, we perform dense sampling on these key segments, averaging 8 frames per segment.\nB. Model Structure\nWe employ the chain-of-thought prompting technique to selectively sample key frames from the video. To capture the nuanced interactions across various levels of visual granularity, each frame is meticulously split into M patches. These patches are then processed through an image encoder to distill fine-grained features. Specifically, we use the CLIP-L [53] to extract patch-level features denoted as $P = \\{p_1, p_2, ..., p_m\\}$, where $p_m \u2208 R^{T\u00d7M\u00d7D}$ and D is the dimension of each patch-level feature. To retain the temporal and spatial position information, we add a trainable temporal position embedding $P_t \u2208 R^{T\u00d7D}$ and spatial position embedding $p_s \u2208 R^{M\u00d7T\u00d7D}$ for each patch.\n$F = P + P_t + P_s$ (1)\nFollowing the spatial-first strategy [73], [74], we sequence the features of each patch before feeding them into a connector module, which is denoted as $F_t$. This connector is designed to effectively extract dynamic spatio-temporal features between patches, and adeptly align the visual features with the word embedding space of Large Language Models (LLMs). To strike a balance between compute efficiency and the connec-tor\u2019s targets, we develop a method based on mamba [75] to"}, {"title": "V. EVALUATION METRIC", "content": "Currently, VLMs widely adopt the GPT-based approach to compare candidate answers with reference answers in terms of coherence, consistency, fluency, and correctness. However, this approach has significant limitations:\n\u2022 The multifactorial of causality in anomalous events. Due to the influence of analysis methods and the observer\u2019s experience, the causality of the same anomalous event can have multiple interpretations.\n\u2022 The instability of GPT. For the same sample, GPT may generate completely different evaluations.\n\u2022 Due to the hallucination issue within VLM, the model may provide accurate answers based on training data without truly understanding the content of the video [82].\nTo address the above issues, we propose a stable, compre-hensive, and accurate evaluation metric to assess the ability of VLMs to understand anomalous events. As shown in Figure 6, our evaluation matrix consists of three core components:\nBasic Reasoning: To address the multifactorial of causality in anomalous events, we employ GPT to evaluate the quality of candidate answers from two dimensions based on key entities within ECVA: First, we utilize GPT to assess the number of annotated key entities covered by the candidate answers, evaluating how many key entities in the video are covered by the model\u2019s responses. Second, we employ GPT to ascertain the rationality of the logic formed by these entities in the candidate answers, which is denoted by equation 5. K represents the set of key sentences for the i-th sample, x represents the candidate answer for the i-th sample, G denotes the scoring mechanism of GPT, and N represents the total number of samples.\n$S_1 = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{G(x_i \\cap K_i)}{|K_i|} * G(x_i)$ (11)\nConsistency: Given that GPT models exhibit a high level of stability when handling affirmative or negative questions [83], we leverage this advantage to measure the consistency level between the candidate answer and the reference answer. This approach significantly enhances the stability of AnomEval. As denoted in equation 6, $q_t$ denotes the t question for the i-th sample, $y_i$ denotes the ground truth for the i-th sample, and T represents the total number of questions.\n$S_2 = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\frac{1}{T} \\frac{G(\\text{``Yes''}|x_i, y_i, q_t)}{G(\\text{``Yes''}|x_i, y_i, q_t) + G(\\text{``No''}|x_i, y_i, q_t)}$ (12)\nHallucination: Since the VLM may provide relatively correct answers based on hallucinations [84]. To ensure that VLMs"}, {"title": "VI. EXPERIMENTS", "content": "A. Implementation Details\nIn our study, we employ Clip-Vit-L and Mistral as the visual encoder and Large Language Model (LLM). All training pro-cedures are conducted on 4 NVIDIA A800 GPUs. We pre-train AnomShield for three epochs with a batch size of 32, employ-ing the AdamW optimizer with a cosine schedule. Specifically, we pre-train our AnomShield with the initial phase requiring 16 hours, the second phase 10 hours, and the final phase 20 hours. During the third phase, we sample an average of sixteen frames per video and set the learning rate of the visual encoder to be 20% of that used for the LLM.\nB. Consistency evaluation of AnomEval\nOur AnomEval metric can better align with human\u2019s preference on causation understanding of video anomaly.\nC. Main Results"}, {"title": "VII. CONCLUSION", "content": "This paper presents ECVA, a novel benchmark for causation understanding of video anomaly. To the best of our knowledge,"}]}