{"title": "MATHCONSTRUCT: Challenging LLM Reasoning with Constructive Proofs", "authors": ["Mislav Balunovi\u0107", "Jasper Dekoninck", "Nikola Jovanovi\u0107", "Ivo Petrov", "Martin Vechev"], "abstract": "While Large Language Models (LLMs) demonstrate impressive performance in mathematics, existing math benchmarks come with significant limitations. Many focus on problems with fixed ground-truth answers, and are often saturated due to problem simplicity or the viability of guessing or memorization. Crucially, they capture only a narrow subset of relevant math problems. To address this research gap, we introduce MATHCONSTRUCT, a new benchmark of 126 challenging problems sourced from various math competitions, which targets constructive proofs, a widely encountered problem type requiring the construction of mathematical objects with specific properties. These proofs are particularly suitable for LLM evaluation, as solution correctness can be easily verified. Our automated verifiers also enable MATHCONSTRUCT to generate problem variations, used to evaluate robustness. State-of-the-art LLMs solve only 54% of MATHCONSTRUCT problems, highlighting its complexity and importance for LLM evaluation.", "sections": [{"title": "1. Introduction", "content": "Evaluating the mathematical reasoning abilities of Large Language Models (LLMs) requires high-quality public benchmarks that accurately measure progress. As shown in Fig. 1, existing benchmarks, such as MATH (Hendrycks et al., 2021), are becoming increasingly saturated as state-of-the-art models improve, highlighting the need for more challenging evaluation tasks. Many complex mathematical problems involve proofs, which are a fundamental component of advanced reasoning. However, current benchmarks primarily focus on problems where LLM outputs can be directly compared to ground truth answers, making them unsuitable for evaluating proofs. A promising alternative, formalized proof generation, requires LLMs to generate proofs that can be verified by automated theorem provers such as Lean (de Moura & Ullrich, 2021). Unfortunately, even models explicitly fine-tuned for this task struggle to perform well (Xin et al., 2024). Furthermore, this approach does not fully leverage the strong natural language reasoning capabilities of LLMs. This raises an important question: Is there a class of proof-based problems that are both challenging for LLMs and easy to verify for correctness?\nConstructive proofs One important class of proofs, commonly appearing in real-world applications and advanced mathematical competitions, involves constructive proofs. These proofs establish a mathematical result by explicitly constructing an object-such as a set, matrix, or graph-that satisfies specific constraints.\nFor instance, one of the most challenging problems from the 2022 International Mathematical Olympiad (IMO), shown in Fig. 2 (left), requires constructing an n \u00d7 n matrix that maximizes a particular quantity. More generally, disproving a conjecture often involves constructing a counterexample, as seen in Cantor's diagonal argument (Cantor, 1890). Similarly, proving a bound frequently requires constructing an object that achieves that bound, as in the proof of the Four Color Theorem (Appel & Haken, 1989)."}, {"title": "2. Related Work", "content": "This section reviews related work on mathematical benchmarking and constructive proofs in machine learning.\nEasier math benchmarks Most math benchmarks for LLMs focus on problems where the final answer is a numerical value or algebraic expression that can be compared with a fixed ground truth. Among these, early benchmarks such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) have been saturated by recent models (Jaech et al., 2024; DeepSeek-AI, 2025). More advanced problem sets, such as AIME 2024, are more difficult, yet state-of-the-art models still solve \u224887%."}, {"title": "3. MATHCONSTRUCT", "content": "Constructive proofs are a powerful tool for mathematicians, but turning these into a benchmark for evaluating the reasoning abilities of LLMs requires significant effort. In this section, we describe our approach for the creation of a reliable benchmark of difficult constructive proof problems, detailing the key steps: problem selection (Sec. 3.1), problem encoding (Sec. 3.2), and post-hoc problem review (Sec. 3.3). Further details on the development process are given in App. \u0410.\n3.1. Problem Selection\nAs a first step in the creation of MATHCONSTRUCT, our team consisting of students with significant experience with math competitions preselected a preliminary set of constructive proof problems. To ensure quality, the problems were exclusively sourced from reputable high-level mathematics competitions, including high-school olympiads, undergraduate contests, and national high-school competitions. In total, our team read around 3500 problems from archives of 20 different competitions, selecting 158 possible problems that met our relevance and quality criteria.\nProblem selection criteria Our criteria were as follows:\n\u2022 Difficult construction: For MATHCONSTRUCT to be future-proof and pose a challenge to current and future models, obtaining the required construction should involve non-trivial reasoning, and constitute the majority of the official solution to the competition problem.\n\u2022 Complex objects: To further ensure difficulty and reduce the probability of lucky guessing the answer, the required object should be non-trivial, i.e., the space of possible constructions should be large. In particular, we generally avoid problems where the result of the construction is a single integer."}, {"title": "3.2. Math Construction Problems", "content": "We now discuss the formalization of construction problems and their encoding into a format that can be used for programmatic evaluation.\nDefinition 3.1 (Construction Problem). A construction problem is a tuple $(P, \\theta, V_\\theta)$ where P is a symbolic problem statement in natural language, $\\theta$ are concrete parameters that replace symbolic variables in the problem statement, and $V_\\theta: O \\rightarrow \\{0, 1\\}$ is a verification function that takes an object and checks whether it satisfies the constraints of the problem. A valid solution or a constructive proof for this problem is any object o \u2208 O such that $V_\\theta(o) = 1$.\nEach problem is encoded as a single Python file, containing all of its components P, $\\theta$, and $V_\\theta$. Additionally, the file includes a variation generator function which generates a set of variations of the problem by plugging in different values for the parameters $\\theta$, and formatting instructions that instruct the model to output the solution in a specific format.\nWhen solving the problem, the model is given a concretized problem statement obtained by replacing the parameters in the problem statement P with concrete parameter values $\\theta$. Similar to human contestants, the model does not have access to the verification function $V_\\theta$.\nWe now further discuss each of the problem components.\nSymbolic problem statement While many problems are originally stated with concrete values, they can often be generalized to obtain a symbolic problem statement P. We do this for every problem where it is possible, replacing concrete values with symbolic parameters $\\theta$, allowing us to plug in different values for the parameter. For example, any positive integer can replace parameter n in the problem statement in Fig. 3."}, {"title": "3.3. Problem Review", "content": "We performed a review of MATHCONSTRUCT problems, in each stage discarding problems that did not meet our quality criteria and revising those that could be improved.\nManual quality checks First, each problem author was tasked with implementing a generic solution function that computes valid constructions for the problem. Additionally, extensive unit tests were implemented to check each problem's verification and solution functions, ensuring correct implementation. Next, each author was asked to ensure that their problem is solvable by a human using pen and paper, to ensure that we are testing reasoning, and not merely calculation skills. We remark that some of our problems may include more calculation than typical in human competitions (e.g., writing the complete n x n matrix instead of simply describing it)\u2014however, this step is trivial once the correct insight for the problem is found. Finally, in a peer review process, each problem was reviewed by at least one other team member, checking that the problem statement is sound and clear, that the verification function is feasible and returns informative feedback in case of errors, and that the problem is sufficiently challenging and of high quality.\nAutomated quality checks Complementing manual review, we also implemented automated checks. First, we verified that all problems are solvable by an LLM when explicitly given the solution. This verifies that the formatting instructions are unambiguous and that our parser and verifier are error-free. Second, we flagged for additional review all problems where our solution is longer than 4000 characters, as LLMs should not be significantly hampered by the difficulty of outputting large amounts of text. Finally, we implemented a code agent that flags problems that are solvable using a brute-force approach, as we want to ensure that each problem is only solvable via genuine reasoning."}, {"title": "4. Experimental Evaluation", "content": "We evaluate a diverse set of LLMs on MATHCONSTRUCT across various settings. We present our main results on reasoning models (Sec. 4.1), results with code agents (Sec. 4.2), error analysis of common failures (Sec. 4.3), effects of contamination (Sec. 4.4), and robustness of the models to variations (Sec. 4.5). For readability, we adopt shortened names for some models. You can find this and other details of the experimental setup in App. D.\n4.1. Main Results\nWe evaluate 14 state-of-the-art models on our benchmark and summarize the results in Table 2, which expands on Fig. 1. Each model is tasked with solving the problems in MATHCONSTRUCT while adhering to specific formatting guidelines for their responses (see App. F for details). To ensure correct parsing, models receive two rounds of feedback from the parser, allowing them to refine their answers.\nWe report two key metrics: average accuracy, which first computes accuracy over all variations of a problem and then averages these values across all problems, and robust accuracy, which considers a problem solved only if all its variations are answered correctly. The latter metric reflects a stricter evaluation, analogous to how a human who solves the general form of a problem can solve all instantiations. Additionally, we provide the total cost of running each model on the benchmark, measured in USD."}, {"title": "4.2. Alternative Evaluation Settings", "content": "We explore alternative evaluation settings on our benchmark and analyze their effects.\nCode agents We first evaluate the performance of models with access to a Python interpreter. Specifically, each model can execute Python code up to four times per problem to generate solutions, verify reasoning steps, or perform calculations. The output of each execution is fed back to the model, allowing it to iteratively refine its reasoning. Fig. 4 shows how accuracy and cost change when models are allowed to execute code. Compared to their base performance from Table 2, both accuracy and cost increase significantly. Most models roughly double their accuracy, at the expense of a fivefold increase in cost. Notably, 3.5-SONNET improves from 5% to 15.3% accuracy. FLASH still achieves the highest accuracy in this setting, reaching 17.5%.\nBrute-force solutions Some problems in our benchmark are susceptible to brute-force methods, as identified during our review process (Sec. 3.3). To evaluate this, we tested brute-force agents and adjusted problems that allowed trivial brute-force solutions. We considered two brute-force"}, {"title": "4.3. Error Analysis", "content": "By leveraging the detailed feedback given by our parser and verification methods (described in Sec. 3.2), we conducted a detailed error analysis of the models. Specifically, we categorized the errors into the following types: unparseable, where the model output could not be parsed, no solution, where the model does not provide a solution, reject, where the model rejects the question's premise and states there is no solution, format, where the output did not correctly follow the formatting instructions, and incorrect, where the solution does not satisfy the problem constraints."}, {"title": "4.4. Contamination Analysis", "content": "We further investigate the impact of data contamination on performance, which is particularly important since olympiad problems are commonly included in training datasets. Assessing contamination is crucial for verifying both the reliability of benchmark results. Given the modifications and variations we introduced to the problems, we expect minimal contamination. To confirm this, we follow Dekoninck et al. (2024) and compare model performance on the original benchmark against a rephrased version, where problem statements have been rewritten by GPT-40."}, {"title": "4.5. Effect of Problem Variations", "content": "Finally, to investigate the impact of problem variations on model performance, we test the robustness of FLASH-THINKING against a range of extreme variations. Specifically, we select 10 problems where the model performs reasonably well and evaluate its accuracy on 24 variations of each. These variations purposefully include both trivially small cases and impractically large ones-scenarios intentionally excluded from the default version of MATHCONSTRUCT. For reference, we also include 3.5-SONNET (BRUTE), the brute-forcing agent introduced in Sec. 4.2. As a proxy for variant difficulty, we define variant size as the string length of our (GOLD) solution."}, {"title": "5. Conclusion", "content": "In this work, we introduced MATHCONSTRUCT, a novel benchmark designed to evaluate the mathematical reasoning capabilities of LLMs through constructive proofs. Unlike existing benchmarks, MATHCONSTRUCT uniquely combines the intrinsic challenge of constructing valid mathematical objects with the ease of their correctness verification, creating a challenging set of tasks for LLM evaluation. Starting from 126 carefully curated problems, we generated 475 problems instances by systematically varying key parameters in the original problem statements. Our extensive evaluation of 14 leading LLMs on these tasks revealed that even the most advanced models struggle significantly with these tasks. Overall, we believe that by focusing on construction problems, MATHCONSTRUCT pushes the boundaries of mathematical reasoning benchmarks, offering a valuable resource for driving future advancements in the field."}]}