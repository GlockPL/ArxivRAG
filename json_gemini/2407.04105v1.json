{"title": "Can Pre-trained Language Models Understand Chinese Humor?", "authors": ["Yuyan Chen", "Zhixu Li", "Jiaqing Liang", "Yanghua Xiao", "Bang Liu", "Yunwen Chen"], "abstract": "Humor understanding is an important and challenging research in natural language processing. As the popularity of pre-trained language models (PLMs), some recent work makes preliminary attempts to adopt PLMs for humor recognition and generation. However, these simple attempts do not substantially answer the question: whether PLMs are capable of humor understanding? This paper is the first work that systematically investigates the humor understanding ability of PLMs. For this purpose, a comprehensive framework with three evaluation steps and four evaluation tasks is designed. We also construct a comprehensive Chinese humor dataset, which can fully meet all the data requirements of the pro- posed evaluation framework. Our empirical study on the Chinese humor dataset yields some valuable observations, which are of great guiding value for future optimization of PLMs in humor un- derstanding and generation.", "sections": [{"title": "1 INTRODUCTION", "content": "Humor is an advanced language art prevalently used in human languages. However, it is very challenging to let machines possess a sense of humor as humans, since it requires a deep understand- ing of semantics as well as cultural background. Nowadays, as the development of human-machine interaction systems and applica- tions, how to let machines have a sense of humor has become an increasingly important topic in Natural Language Processing (NLP). Its success or failure may potentially forecast whether a Babel of human-machine interaction could finally be built.\nDue to its importance, great efforts has been made on humor- relevant tasks in the NLP community, which mainly focuses on Humor Recognition and Humor Generation. Early work mainly relies on shallow linguistic features and templates to recognize or generate humors. For instance, Cattle and Ma [4] and Yang et al. [40] recognize humor with words associations and the latent semantic structures, while Aggarwal and Mamidi [1] and He et al. [21] generate poetic three liner jokes and puns through analyzing the structure and retrieve-and-edit approach, respectively. However, these methods rely on unaffordable human cost to design features or templates for different datasets, which can only recognize or generate a very limited range of humorous expressions.\nAs the popularity of pre-trained language models (PLMs), some recent work, such as Yu et al. [41] and Rodriguez et al. [32], make preliminary attempts to finetune PLMs for humor recognition and generation. Thanks to the powerful understanding and generation capabilities that PLMs have learned from massive amounts of data, they significantly reduce human cost and enable the recognition (or generation) on more types of humorous expressions. However, these simple endeavors do not substantially answer an important question: whether PLMs are capable of humor understanding?\nThis is a deep question worth exploring, which should be an- swered firstly when we utilize PLMs for various humor understand- ing and generation tasks. To answer this question, we would like"}, {"title": "2 EVALUATION FRAMEWORK", "content": "In this paper, we focus on evaluating the ability of PLMs in under- standing humor. Only when PLMs are capable of understanding humor can they generate more reasonable humorous texts, so we leave the investigation of the humor generation ability of PLMs for future work. As depicted in Fig. 1, we propose a comprehen- sive evaluation framework with three evaluation steps based on four evaluation tasks to investigate the capability of PLMs in un- derstanding humor. In the following, we introduce them in detail."}, {"title": "2.1 EVALUATION TASKS", "content": "We evaluate four representative humor-relevant tasks as follows:\nHumor Recognition. This task aims to distinguish humorous texts from humorless ones Taylor and Mazlack [38]. For each input text, the task outputs humorous or humorless as shown in Fig 2(a).\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHumor Type Classification. This task first appears in the CCL2019 competitions 1. Given a piece of humor text as input, it classifies hu- morous texts into several predefined humorous types and outputs harmonic, ambiguous or incongruous as shown in Fig 2(b).\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHumor Level Classification. This task works on judging the level of humor for input texts, also first proposed in the CCL2019 competitions, where the humor can be divided into five levels from the weakest to the strongest. Here we modify the five continual levels into three discrete levels. Given a piece of humor text, the task outputs its corresponding humor level (strong, medium or weak) as shown in Fig 2(c).\nPunchline Detection. According to humor theory [36, 39], this task determines whether there is a semantic incongruity between the previous context and its punchline (or laugh-point) ending, which originates but is slightly different from the research by Chen and Lee [5]. Specifically, the input of this task is a pair of texts: 1) the context of a humorous text before the punchline ending sen- tence and 2) its corresponding punchline ending sentence or a non- punchline normal ending sentence, and it will determine whether this ending sentence is a punchline one as shown in Fig 2(d).\nThe above four tasks are adopted for different evaluation pur- poses: Initially, humor recognition is the most basic task to examine a model's ability in discriminating humor. Further, humor type classification evaluates a model's ability in distinguishing between different types of humorous text, while humor level classification indicates whether the model is sensitive to different levels of humor. Last but not the least, punchline detection can reflect whether a model has a fine-grained understanding on a humorous text and thus judges whether the ending sentence is a punchline or normal one."}, {"title": "2.2 EVALUATION STEPS", "content": "To perform a thorough and insightful evaluation based on the above four humor-relevant tasks, three evaluation steps are designed for investigating the ability of PLMs in humor understanding:\nEvaluate Original PLMs. The first step is responsible for evaluat- ing the humor understanding ability of the original PLMs, which is expected to tell us: 1) Whether the original PLMs have humor understanding ability; and 2) What are the improvements and short- comings of PLMs after simple fine-tuning on the humor dataset?"}, {"title": "3 THE CHINESE HUMOR DATASET", "content": "To fully meet all data requirements of the evaluation framework, we construct a large-scale Chinese humor dataset, which consists of four sub-datasets as depicted in Table 1.\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHumor Recognition Sub-dataset. The humor recognition sub-dataset contains humorous texts mainly from released data1,2 and humorless text crawled from various platforms. On another hand, to construct negative examples, we mainly crawl short mono- logues or dialogues, such as hint fictions, fables, celebrity stories, or monologue-based diaries as the humorless texts. Such texts have similar language styles with the humorous ones. The lengths of the texts are ranged from 20 to 200 characters. Each humorless text is tagged by three recruited human volunteers to evaluate whether\nHumor Type Classification Sub-dataset. The humor type classification sub-dataset contains three types of humorous texts:\n\u2022 Harmonic humor: it means a word-pair has a similar pronun- ciation but different meanings in a piece of humorous text, such as \u201c\u5b63\u4ed8 (meaning: pay quarterly, Chinese pinyin: jifu)\u201d and\u201c\u7ee7\u7236 (meaning: stepfather, Chinese pinyin: jifu)\u201d, \u201c\u6708 \u4ed8 (meaning: pay monthly, Chinese pinyin: yuefu)\u201d and \u201c\u5cb3 \u7236(meaning: father-in-law, Chinese pinyin: yuefu)\" in the first row of Table 3;\n\u2022 Ambiguous humor: it means at least two definitions of a word are simultaneously used in a piece of humorous text. For example,\u201c\u5341\u5206\u201dcan mean \u201cvery\" and \"ten scores\" at the same time. in the second row of Table 3;\n\u2022 Incongruous humor: it means there is a semantic incongruity in a humorous text, which doesn't follow humans' expec- tation. For example, in the third row of Table 3,\u201c\u4f60\u770b\u8fd9 \u4e2a\u59d1\u5a18\u5c31\u5f88\u6709\u7d20\u8d28\u4e00\u76f4\u5f88\u51b7\u9759\u561b\u201dwhich means \u201cthe girl is calm without dispute\", and the response is\u201c\u6211\u4f4f30\u697c, \u8dd1\u4e0b\u6765\u7d2f\u4e86\u6b47\u4f1a\u513f\u518d\u9a82\u4f60\u201dwhich means \u201cthe girl keeps calm because she is too tired to quarrel\u201d. Such response has semantic incongruity which doesn't follow humans' normal expectation.\nWe first collect the data from the release data1,2 which already have labels. In order to guarantee the accuracy of the type labels, we proofread and then discard the wrong ones with the help of the recruited three human raters.\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHumor Level Classification Sub-dataset. The humor level classification sub-dataset classifies humorous texts into three levels of humor: weak, medium, and strong. We collect the data from the same places as the humor type classification sub-dataset, and also perform similar manual proofreading operations to guarantee the accuracy of the labels.\nPunchline Detection Sub-dataset. Based on the humorous text in humor recognition sub-dataset, we construct humorous and humorless context-ending pairs.\nThe humorous ending is extracted by dividing each humorous text into two parts: previous context and punchline. According to\""}, {"title": "4 METHODOLOGY", "content": "Fig 3 illustrates our framework for evaluating PLMs' ability in under- standing humor, which consists of three parts: evaluate original/fine- tuned PLMs, evaluate knowledge-enhanced PLMs, and interpret humor understanding in PLMs."}, {"title": "4.1 EVALUATE ORIGINAL/FINE-TUNED PLMS", "content": "In this module, we adopt several SOTA PLMs to model humor un- derstanding through four representative tasks: humor recognition, humor type classification, humor level classification, and punch- line detection (see Fig 3(a)). The first three are text classification tasks that take a piece of text as input and output text properties (i.e., humorous or humorless, humor types, and humor levels). The last task is performing text matching of a context-ending pair, and outputs the similarity of the pair to indicate whether the ending is a punchline or not. We calculate the similarity scores based on sentence-level embeddings, and the loss function we adopt is online contrastive loss \u00b3, which is proved better than contrastive loss [19] in our experiments. If the similarity of the pair is over 0.5, we regard the ending as a normal ending. Otherwise, we regard the ending as a punchline."}, {"title": "4.2 EVALUATE KNOWLEDGE-ENHANCED PLMS", "content": "In this module, we consider several types of external knowledge, such as general knowledge bases, commonsense bases, and linguis- tic knowledge. For each type of external knowledge as shown in Fig 3(b), we further use two ways of knowledge enhancement, i.e., implicit embedding and explicit fusion, to enhance PLMs.\nKnowledge Embedding Construction. We utilize open-sourced Tencent AI Lab Embedding Corpus 4 and ConceptNet 5 as knowl- edge embeddings from general knowledge bases and commonsense bases, respectively. For the linguistic knowledge, we learn a pinyin embedding for each character as the knowledge embedding to de- tect different semantic meanings for characters with the same or similar pronunciations. We first utilize the pypinyin 6 package to generate pinyin with one of four tones for each Chinese charac- ter in a given text. For polyphonic characters, we select the first pronunciation. Inspired by Sun et al. [37], we use special tokens to denote tones. The maximum length of input pinyin sequence is set as 8 and we use a special letter \"-\" for padding short pinyin sequences. We adopt a Convolution Neural Network model [23] to make pinyin embedding as shown below:\n\\(Embpinyin = Maxpool(CNN(pypinyin(Seqin)))\\) (1)\nFusion Layer. After constructing three types of knowledge embeddings, we normalize each of the embeddings by aggregating along the token dimension of each word or letter dimension of each pinyin (as each word is a sequence of tokens and each pinyin is a sequence of letters), respectively, with a fully connected layer to get normalized knowledge embedding(s) \\(Embk\\). Then we use two ways of knowledge enhancement for PLMs, which are Implicit embed- ding and Explicit fusion. Specifically, if we use BERT as the PLM, implicit embedding is to add word embedding \\(Emb word\\), segment embedding \\(Embseg\\), position embedding \\(Embpos\\) and normalized knowledge embedding \\(Embk\\) together as the input embeddings for PLMs. The output embeddings for PLMs are used to make new predictions. The process is shown as follows:\n\\(Embin = Emb word + Embseg + Embpos + Embk\\) (2)\n\\(Embout = PLM(Embin)\\) (3)\n\\(y = W * Embout + b\\) (4)"}, {"title": "4.3 INTERPRET HUMOR UNDERSTANDING", "content": "In this module, given an input text, we aim at figuring out which input words are critical for a PLM to make it correctly perform humor-relevant tasks and whether these words better interpret PLMs' humor understanding ability [26, 27]. We utilize gradient- based and perturbation-based techniques with the Captum pack- age 7 for this purpose (see Fig. 3(c).\nGradient-based approaches [35] compute saliency map based on the gradient of the input with respect to the output. We first take differentiable embeddings of tokens as the input for PLMs. Next, we aggregate the embeddings gradients with L2 normalization. Then we use Input X Gradient [34], which multiplies the gradient with the normalized embeddings, to improve the sharpness of the saliency scores, thus to compare them better. After that, we use a visualizer to present saliency maps for saliency scores to find clue words for PLMs' correct predictions on humor-relevant tasks.\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nPerturbation-based techniques perturb the input to find which input regions have a significant impact on the prediction. Given an input text, we randomly perturb it by adding or removing a few tokens and further check the new prediction of the PLMs. Based on perturbation-based techniques, we first investigate the faithfulness of the saliency maps to detect whether PLMs' correct prediction are not based on arbitrary choices. We replace the top-N (we set N = 3) most salient words with a mask token and then measure the drop of the PLMs' performance. Next, we investigate the stability of the saliency maps to detect whether insignificant words affect saliency maps. We add some random words at the end of the texts and then measure the correlation between the change of the prediction and the change of the saliency scores based on the Pearson correlation coefficient and Spearman correlation coefficient."}, {"title": "5 EXPERIMENTS", "content": "Following the proposed evaluation framework, we carry out ex- periments to investigate whether the pre-trained language mod- els (PLMs) have the ability of humor understanding (see Sec. 5.1), whether external knowledge can improve their humor understand- ing ability (see Sec. 5.2), and the interpretability of the detected"}, {"title": "5.1 RESULTS ON ORIGINAL/FINE-TUNED PLMS", "content": "The evaluation results on original and fine-tuned PLMs based on four humor-relevant tasks are shown in Table 4 (see the column \"zs\" and \"ft\"). From the results, we observe that the original PLMs have weak ability on humor understanding with the average value 54.98, 33.77, 33.32, and 49.96 in the zero-shot learning on the humor recognition, humor type classification, humor level classification, and punchline detection task, respectively. After fine-tuning on the corresponding sub-datasets, the performance is improved by 68.67%, 72.64%, 40.87%, and 91.89%, respectively. The accuracy on humor recognition and punchline detection are both over 90%. It suggests that PLMs have a certain degree of ability in humor recognition and punchline detection after fine-tuning on the humor dataset."}, {"title": "5.2 RESULTS ON KNOWLEDGE-ENHANCED PLMS", "content": "In this part, we only present the experimental results by injecting Chinese pinyin into PLMs in the way of explicit fusion as shown in Table 4. That is because we observe in our experiments that either injecting Chinese pinyin in the way of implicit embedding, or injecting another one or two types of knowledge embeddings in any ways do not improve PLMs' performance in all the four humor- relevant tasks. Due to space limitation, we omit these results and will make analysis later.\nSee the column \"K-ft\" in the above Tables, we find that PLMs perform better, improving by 70.78%, 77.47%, 44.07%, and 94.63% on the humor recognition, humor type classification, humor level classification, and punchline detection task respectively. This group of experiments demonstrates that external linguistic knowledge such as Chinese pinyin has a positive effect for PLMs in humor-relevant tasks, and injecting external knowledge by explicit fusion is more possible to maintain important information in the knowledge than by implicit embedding.\nHowever, for another one or two types of knowledge, which do not improve the performance of PLMs in all the four evaluation tasks, we give the possible reasons as follows: 1) The huge amount of data used for training PLMs may already contain most of the factual knowledge and commonsense knowledge, thus the existing knowledge bases or commonsense bases can not contribute more for PLMs in humor understanding. 2) Some humorous texts need complicated specific knowledge or inference paths to understand, which can not be provided by existing knowledge. For example, in the following harmonic humorous text\u201c...\u4e00\u6765\u95fb,\u4e8c\u6765\u95fb,\u4e09\u6765 \u95fb..\u201d, the Chinese phrase \u201c\u4e00\u6765\u95fb\u201dhas a similar pronunciation with the English word \"eleven\", where \u201ce\u201d and \u201cleven\u201d correspond the pronunciation of Chinese character\u201c\u4e00\u201d and \u201c\u6765\u95fb\u201d, respectively. Therefore,\u201c\u4e8c\u6765\u95fb(two-leven)\u201d and \u201c\u4e09\u6765\u95fb(three-leven)\", are analogous to\u201c\u4e00\u6765\u95fb(one-leven)\u201d,, which produce harmonic hu- mor. It's a much difficult inference process for PLMs to understand and make correct humor type classification.\nMoreover, when we inject other existing knowledge from knowl- edge bases and commonsense bases except Pinyin knowledge in the way of implicit embedding into PLMs, the performance for PLMs in the humor-relevant tasks do not have any improvement. Due to space limitation, we also omit these results in our paper and give some possible analysis as follows: 1) Fusing Knowledge from different sources will do harm to separate feature of each type of knowledge. It's difficult for PLMs to learn effective infor- mation from the fused knowledge. 2) Humor is a much tough issue. The existing knowledge is not powerful enough for PLMs in hu- mor understanding. Thus, besides linguistic knowledge, PLMs also need humor-relevant background knowledge for better performance in humor-relevant tasks and better humor understanding ability."}, {"title": "5.3 RESULTS ON INTERPRETABILITY ANALYSIS", "content": "To visualize the interpretability of PLMs' humor understanding abil- ity, we draw saliency maps for sentences to show the detected clue words. We first investigate the stability of the clue words detection results to verify whether these clue words are faithful to PLMs. We add some random characters such as\u201c\u6211(I)\u201d at the ending position of samples, and then measure the correlation between the change in the prediction and the change in the saliency scores. The p-values of Pearson correlation coefficient and Spearman correlation coeffi- cient are 0.0087 and 0.0010, respectively in the zero-shot learning, 0.0086 and 0.0060, respectively in the fine-tuning, and 0.0080 and 0.0064, respectively in the knowledge-enhanced fine-tuning. The changes are statistically different due to p-values all below 0.05, which suggests that saliency scores highly relate to predictions, thus the saliency scores are stable and the clue words detection results are faithful to PLMs, which can be trusted by humans for interpreting PLMs' humor understanding ability.\nFig 4 gives the saliency maps of several samples got from BERT- base humor-relevant tasks, and the faithfulness of the saliency map on four humor-relevant tasks is shown in Table 5. In the humor recognition task, we observe that the original PLMs only take some special tokens, such as [CLS], [UNK], or punctuations (see Fig 4 (a1)) as the clue words (which have deeper color). We mask the top three salient words except [SEP] of each instance, and the average performance has a slight drop from 54.98 to 53.61. In an- other three humor-relevant classification tasks, the results in the zero-shot learning as shown in Fig 4 (b1) are similar with those on the humor recognition task. These results prove that the original PLMs (without fine-tuning) can hardly understand humor, and their predictions on humor are nearly from arbitrary choices.\nAfter that, we fine-tune PLMs on four humor-relevant tasks, re- spectively. We observe that on the humor recognition sub-dataset, the saliency maps for fine-tune PLMs show that the models focus"}, {"title": "6 EVALUATION ON DOWNSTREAM TASKS", "content": "Similar to happiness, sadness, and anger, humor is one of the essential emotions of humans. Therefore, we carry out further evaluations to investigate whether PLMs which have been fine- tuned on the humor dataset can achieve better performance on the downstream tasks. We choose four Chinese sentiment classification datasets 8: 1) ChnSentiCorp-htl-all (Chn), which is a hotel review dataset with more than 5,000 positive reviews and more than 2,000 negative reviews; 2) Waimai-10k (Wai), which contains 4,000 posi- tive and 8,000 negative user reviews; 3) Online-shopping-10-cats (Shop), which has 30000 positive and 30000 negative user comments with online shopping; 4) weibo-senti-100k (Wei), which has about 50000 positive and 50000 negative comments.\nWe fine-tune BERT, which has been fine-tuned on our con- structed Chinese humor dataset, including all the four sub-datasets. The results are shown in Table 6. The baseline results are based on BERT, which are derived from their published research [17, 42, 43, 46].We observe that the performance increases a little after"}, {"title": "7 RELATED WORK", "content": "PLMs demonstrate terrific capabilities on various downstream tasks [6- 12]. We list two main related work as follows:\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHumor Datasets and Corpora. Some research dedicate them- selves to construct a large-scale humor datasets and corpora. For example, Engelthaler and Hills [15] design a humor dataset which provides researchers with a list of humor ratings with 4,997 English words. Chiruzzo et al. [13] present the development of a corpus of 30,000 Spanish tweets that were crowd-annotated with humor value and funniness score. Hossain et al. [22] introduce a new dataset called Humicroedit that design simple replacement edits to make English news headlines funny. Hasan et al. [20] introduce a mul- timodal English humor dataset to detect humorous expressions in TED talks. Different from the above research, we construct a Chinese humor dataset, which includes four sub-datasets, each of which can be used for one representative humor-relevant task.\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHUMOR RELEVANT TASKS\nHumor Recognition. Other research on humor focus on humor recognition which is to decide whether a given sentence expresses a certain degree of humor. For example, Mihalcea and Strapparava [28] report text classification techniques are a viable approach to recognize humorous one-liners. Barbieri and Saggion [2] design"}, {"title": "8 CONCLUSIONS AND FUTURE WORK", "content": "Humor understanding for PLMs is a challenging research in Natural Language Processing. In this work, we systematically investigate the humor understanding ability of PLMs with a designed compre- hensive framework with three evaluation steps and four evaluation tasks. We also construct a comprehensive Chinese humor dataset, and our empirical study on it yields some valuable observations : 1) While the original PLMs can hardly understand humor, they could gain a certain degree of humor understanding ability through fine-tuning. 2) Linguistic knowledge such as Chinese Pinyin has a positive effect for PLMs in humor-relevant tasks. 3) The existing knowledge bases and commonsense bases can not provide much required knowledge for humor understanding. As a future work, we will find ways to collect more cultural knowledge for the opti- mization of PLMs in humor understanding."}, {"title": "9 ACKNOWLEDGEMENT", "content": "This work was supported by Science and Technology Commission of Shanghai Municipality Grant (No. 22511105902), National Natu- ral Science Foundation of China (No.62072323), Shanghai Munici- pal Science and Technology Major Project (No.2021SHZDZX0103), Shanghai Science and Technology Innovation Action Plan (No. 22511104700), and National Natural Science Foundation of China (No. 62102095). Yanghua Xiao is also a member of Research Group of Computational and AI Communication at Institute for Global Communications and Integrated Media, Fudan University."}]}