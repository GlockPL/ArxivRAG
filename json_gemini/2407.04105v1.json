{"title": "Can Pre-trained Language Models Understand Chinese Humor?", "authors": ["Yuyan Chen", "Zhixu Li", "Jiaqing Liang", "Yanghua Xiao", "Bang Liu", "Yunwen Chen"], "abstract": "Humor understanding is an important and challenging research in natural language processing. As the popularity of pre-trained language models (PLMs), some recent work makes preliminary attempts to adopt PLMs for humor recognition and generation. However, these simple attempts do not substantially answer the question: whether PLMs are capable of humor understanding? This paper is the first work that systematically investigates the humor understanding ability of PLMs. For this purpose, a comprehensive framework with three evaluation steps and four evaluation tasks is designed. We also construct a comprehensive Chinese humor dataset, which can fully meet all the data requirements of the proposed evaluation framework. Our empirical study on the Chinese humor dataset yields some valuable observations, which are of great guiding value for future optimization of PLMs in humor understanding and generation.", "sections": [{"title": "1 INTRODUCTION", "content": "Humor is an advanced language art prevalently used in human languages. However, it is very challenging to let machines possess a sense of humor as humans, since it requires a deep understanding of semantics as well as cultural background. Nowadays, as the development of human-machine interaction systems and applications, how to let machines have a sense of humor has become an increasingly important topic in Natural Language Processing (NLP). Its success or failure may potentially forecast whether a Babel of human-machine interaction could finally be built.\nDue to its importance, great efforts has been made on humor-relevant tasks in the NLP community, which mainly focuses on Humor Recognition and Humor Generation. Early work mainly relies on shallow linguistic features and templates to recognize or generate humors. For instance, Cattle and Ma [4] and Yang et al. [40] recognize humor with words associations and the latent semantic structures, while Aggarwal and Mamidi [1] and He et al. [21] generate poetic three liner jokes and puns through analyzing the structure and retrieve-and-edit approach, respectively. However, these methods rely on unaffordable human cost to design features or templates for different datasets, which can only recognize or generate a very limited range of humorous expressions.\nAs the popularity of pre-trained language models (PLMs), some recent work, such as Yu et al. [41] and Rodriguez et al. [32], make preliminary attempts to finetune PLMs for humor recognition and generation. Thanks to the powerful understanding and generation capabilities that PLMs have learned from massive amounts of data, they significantly reduce human cost and enable the recognition (or generation) on more types of humorous expressions. However, these simple endeavors do not substantially answer an important question: whether PLMs are capable of humor understanding?\nThis is a deep question worth exploring, which should be answered firstly when we utilize PLMs for various humor understanding and generation tasks. To answer this question, we would like to investigate the humor understanding ability of PLMs in the following several aspects: i) Whether PLMs can understand humor before or after fine-tuning? ii) Whether existing external knowledge can help improve PLMs' humor understanding ability? iii) Whether PLMs can detect interpretable clue words that fit human intuitive understanding of humor? To this end, we need a well-designed evaluation framework and corresponding comprehensive dataset, both of which cannot be directly obtained from the existing humor-relevant tasks [1, 4, 21, 40].\nIn this paper, we first propose a three-step evaluation framework, each step of which is responsible for answering one of the above questions. Next, within this framework, we employ four representative humor-relevant tasks to conduct the systematically evaluation on PLMs, including humor recognition, humor type classification, humor level classification and punchline detection. Meanwhile, we construct a comprehensive Chinese humor dataset, which fully meets all data requirements of the four tasks and three steps evaluation framework. We choose to construct the Chinese humor dataset since Chinese humor is as worthy of study as English humor and more challenging. However, the existing Chinese humor datasets 1,2 are far less abundant than the English humor datasets, thus we want to fill in the gap for Chinese humor research.\nOur empirical study based on this Chinese humor dataset suggests that: 1) By fine-tuning on the constructed humor dataset, the humor understanding ability of PLMs has been greatly improved. 2) Some external knowledge, such as Chinese pinyin information, has a positive effect on improving the PLMs' performance on humor-related tasks. 3) Moreover, a portion of the detected clue words are considered being in line with human perception of humor, but there is much room for improvement in PLMs' ability to understand humor. To summarize, our contributions in this paper are threefold:\n\u2022 We are the first work to systematically evaluate the ability of PLMs in understanding Chinese humor. For this purpose, a comprehensive framework with three evaluation steps and four evaluation tasks is designed.\n\u2022 We construct a more comprehensive Chinese humor dataset compared with the prior research, which fully meets all the data requirements of the proposed evaluation framework.\n\u2022 Our empirical study justifies the positive effect of fine-tuning and external knowledge for PLMs on humor-relevant tasks, which has important guiding value for future optimization of PLMs in humor understanding and generation."}, {"title": "2 EVALUATION FRAMEWORK", "content": "In this paper, we focus on evaluating the ability of PLMs in understanding humor. Only when PLMs are capable of understanding humor can they generate more reasonable humorous texts, so we leave the investigation of the humor generation ability of PLMs for future work. As depicted in Fig. 1, we propose a comprehensive evaluation framework with three evaluation steps based on four evaluation tasks to investigate the capability of PLMs in understanding humor. In the following, we introduce them in detail."}, {"title": "2.1 EVALUATION TASKS", "content": "We evaluate four representative humor-relevant tasks as follows:\nHumor Recognition. This task aims to distinguish humorous texts from humorless ones Taylor and Mazlack [38]. For each input text, the task outputs humorous or humorless as shown in Fig 2(a).\nHumor Type Classification. This task first appears in the CCL2019 competitions 1. Given a piece of humor text as input, it classifies humorous texts into several predefined humorous types and outputs harmonic, ambiguous or incongruous as shown in Fig 2(b).\nHumor Level Classification. This task works on judging the level of humor for input texts, also first proposed in the CCL2019 competitions, where the humor can be divided into five levels from the weakest to the strongest. Here we modify the five continual levels into three discrete levels. Given a piece of humor text, the task outputs its corresponding humor level (strong, medium or weak) as shown in Fig 2(c).\nPunchline Detection. According to humor theory [36, 39], this task determines whether there is a semantic incongruity between the previous context and its punchline (or laugh-point) ending, which originates but is slightly different from the research by Chen and Lee [5]. Specifically, the input of this task is a pair of texts: 1) the context of a humorous text before the punchline ending sentence and 2) its corresponding punchline ending sentence or a non-punchline normal ending sentence, and it will determine whether this ending sentence is a punchline one as shown in Fig 2(d).\nThe above four tasks are adopted for different evaluation purposes: Initially, humor recognition is the most basic task to examine a model's ability in discriminating humor. Further, humor type classification evaluates a model's ability in distinguishing between different types of humorous text, while humor level classification indicates whether the model is sensitive to different levels of humor. Last but not the least, punchline detection can reflect whether a model has a fine-grained understanding on a humorous text and thus judges whether the ending sentence is a punchline or normal one."}, {"title": "2.2 EVALUATION STEPS", "content": "To perform a thorough and insightful evaluation based on the above four humor-relevant tasks, three evaluation steps are designed for investigating the ability of PLMs in humor understanding:\nEvaluate Original PLMs. The first step is responsible for evaluating the humor understanding ability of the original PLMs, which is expected to tell us: 1) Whether the original PLMs have humor understanding ability; and 2) What are the improvements and shortcomings of PLMs after simple fine-tuning on the humor dataset?"}, {"title": "3 THE CHINESE HUMOR DATASET", "content": "To fully meet all data requirements of the evaluation framework, we construct a large-scale Chinese humor dataset, which consists of four sub-datasets as depicted in Table 1.\nHumor Recognition Sub-dataset. The humor recognition sub-dataset contains humorous texts mainly from released data1,2 and humorless text crawled from various platforms. On another hand, to construct negative examples, we mainly crawl short monologues or dialogues, such as hint fictions, fables, celebrity stories, or monologue-based diaries as the humorless texts. Such texts have similar language styles with the humorous ones. The lengths of the texts are ranged from 20 to 200 characters. Each humorless text is tagged by three recruited human volunteers to evaluate whether this piece of text is actually humorless. Texts with controversial labels given by the three volunteers will be discarded.\nHumor Type Classification Sub-dataset. The humor type classification sub-dataset contains three types of humorous texts:\n\u2022 Harmonic humor: it means a word-pair has a similar pronunciation but different meanings in a piece of humorous text, such as \u201c\u5b63\u4ed8 (meaning: pay quarterly, Chinese pinyin: jifu)\u201d and\u201c\u7ee7\u7236 (meaning: stepfather, Chinese pinyin: jifu)\u201d, \u201c\u6708\u4ed8 (meaning: pay monthly, Chinese pinyin: yuefu)\u201d and \u201c\u5cb3\u7236(meaning: father-in-law, Chinese pinyin: yuefu)", "humor": "it means at least two definitions of a word are simultaneously used in a piece of humorous text. For example,\u201c\u5341\u5206\u201dcan mean \u201cvery\" and \"ten scores\" at the same time. in the second row of Table 3;\n\u2022 Incongruous humor: it means there is a semantic incongruity in a humorous text, which doesn't follow humans' expectation. For example, in the third row of Table 3,\u201c\u4f60\u770b\u8fd9\u4e2a\u59d1\u5a18\u5c31\u5f88\u6709\u7d20\u8d28\u4e00\u76f4\u5f88\u51b7\u9759\u561b\u201dwhich means \u201cthe girl is calm without dispute", "the girl keeps calm because she is too tired to quarrel": "Such response has semantic incongruity which doesn't follow humans' normal expectation.\nWe first collect the data from the release data1,2 which already have labels. In order to guarantee the accuracy of the type labels, we proofread and then discard the wrong ones with the help of the recruited three human raters.\nHumor Level Classification Sub-dataset. The humor level classification sub-dataset classifies humorous texts into three levels of humor: weak, medium, and strong. We collect the data from the same places as the humor type classification sub-dataset, and also perform similar manual proofreading operations to guarantee the accuracy of the labels.\nPunchline Detection Sub-dataset. Based on the humorous text in humor recognition sub-dataset, we construct humorous and humorless context-ending pairs.\nThe humorous ending is extracted by dividing each humorous text into two parts: previous context and punchline. According to the theories of humor [3, 16, 30, 36, 39], the reason why humor introduces laughter is that a piece of text presents an unexpected sentence which is incongruous with the previous context. Thus, we extract the unexpected sentence in a humorous text as the punchline with the help of human annotation. Specifically, we first enroll another three volunteers, and each of them is required to vote punchline sentences for all humorous texts. We then choose the sentence which has the highest vote as the final punchline sentence for each humorous text and discard the following content in this piece of text. If more than one sentences have equal high votes, we discard this piece of humorous text.\nThe normal ending is generated by a text generator. We input the previous context into a large language model such as CPM [44, 45], and generate a normal ending which has similar length with the punchline. We also carry out human evaluation and machine evaluation to guarantee the quality of the generated normal endings. The steps of quality evaluation are as follows: i) We first enroll another three volunteers and randomly select 3,000 normal endings. Each of the volunteers needs to give a rating for the overall 3,000 normal endings based on a scoresheet shown in Table 2. We calculate Inter-rater agreement of Krippendorff's Alpha (IRA) to ensure the confidence of human ratings. For the controversial ratings which have low agreements (<0.7) or a normal ending is rated below 0.85, we re-generate a new normal ending. ii) Next, we use simCSE [18] to calculate similarity scores to guarantee the generated normal endings have similar semantics with the corresponding punchlines and the previous contexts. The similarity score between the normal ending and the punchline ending is $s_1$, and the similarity score between context-punchline pair and context-normal-ending pair is $s_2$. The final similarity score s is the average of $s_1$ and $s_2$. For some normal endings whose s are below 0.85, we also re-generate new ones."}, {"title": "4 METHODOLOGY", "content": "Fig 3 illustrates our framework for evaluating PLMs' ability in understanding humor, which consists of three parts: evaluate original/fine-tuned PLMs, evaluate knowledge-enhanced PLMs, and interpret humor understanding in PLMs."}, {"title": "4.1 EVALUATE ORIGINAL/FINE-TUNED PLMS", "content": "In this module, we adopt several SOTA PLMs to model humor understanding through four representative tasks: humor recognition, humor type classification, humor level classification, and punchline detection (see Fig 3(a)). The first three are text classification tasks that take a piece of text as input and output text properties (i.e., humorous or humorless, humor types, and humor levels). The last task is performing text matching of a context-ending pair, and outputs the similarity of the pair to indicate whether the ending is a punchline or not. We calculate the similarity scores based on sentence-level embeddings, and the loss function we adopt is online contrastive loss \u00b3, which is proved better than contrastive loss [19] in our experiments. If the similarity of the pair is over 0.5, we regard the ending as a normal ending. Otherwise, we regard the ending as a punchline."}, {"title": "4.2 EVALUATE KNOWLEDGE-ENHANCED PLMS", "content": "In this module, we consider several types of external knowledge, such as general knowledge bases, commonsense bases, and linguistic knowledge. For each type of external knowledge as shown in Fig 3(b), we further use two ways of knowledge enhancement, i.e., implicit embedding and explicit fusion, to enhance PLMs.\nKnowledge Embedding Construction. We utilize open-sourced Tencent AI Lab Embedding Corpus 4 and ConceptNet 5 as knowledge embeddings from general knowledge bases and commonsense bases, respectively. For the linguistic knowledge, we learn a pinyin embedding for each character as the knowledge embedding to detect different semantic meanings for characters with the same or similar pronunciations. We first utilize the pypinyin 6 package to generate pinyin with one of four tones for each Chinese character in a given text. For polyphonic characters, we select the first pronunciation. Inspired by Sun et al. [37], we use special tokens to denote tones. The maximum length of input pinyin sequence is set as 8 and we use a special letter \"-\" for padding short pinyin sequences. We adopt a Convolution Neural Network model [23] to make pinyin embedding as shown below:\n$Embpinyin = Maxpool(CNN(pypinyin(Seqin)))$ (1)\nFusion Layer. After constructing three types of knowledge embeddings, we normalize each of the embeddings by aggregating along the token dimension of each word or letter dimension of each pinyin (as each word is a sequence of tokens and each pinyin is a sequence of letters), respectively, with a fully connected layer to get normalized knowledge embedding(s) $Embk$. Then we use two ways of knowledge enhancement for PLMs, which are Implicit embedding and Explicit fusion. Specifically, if we use BERT as the PLM, implicit embedding is to add word embedding $Embword$, segment embedding $Embseg$, position embedding $Embpos$ and normalized knowledge embedding $Embk$ together as the input embeddings for PLMs. The output embeddings for PLMs are used to make new predictions. The process is shown as follows:\n$Embin = Embword + Embseg + Embpos + Embk$ (2)\n$Embout = PLM(Embin)$ (3)\n$y = W * Embout + b$ (4)"}, {"title": "4.3 INTERPRET HUMOR UNDERSTANDING", "content": "In this module, given an input text, we aim at figuring out which input words are critical for a PLM to make it correctly perform humor-relevant tasks and whether these words better interpret PLMs' humor understanding ability [26, 27]. We utilize gradient-based and perturbation-based techniques with the Captum package 7 for this purpose (see Fig. 3(c)).\nGradient-based approaches [35] compute saliency map based on the gradient of the input with respect to the output. We first take differentiable embeddings of tokens as the input for PLMs. Next, we aggregate the embeddings gradients with L2 normalization. Then we use Input X Gradient [34], which multiplies the gradient with the normalized embeddings, to improve the sharpness of the saliency scores, thus to compare them better. After that, we use a visualizer to present saliency maps for saliency scores to find clue words for PLMs' correct predictions on humor-relevant tasks.\nPerturbation-based techniques perturb the input to find which input regions have a significant impact on the prediction. Given an input text, we randomly perturb it by adding or removing a few tokens and further check the new prediction of the PLMs. Based on perturbation-based techniques, we first investigate the faithfulness of the saliency maps to detect whether PLMs' correct prediction are not based on arbitrary choices. We replace the top-N (we set N = 3) most salient words with a mask token and then measure the drop of the PLMs' performance. Next, we investigate the stability of the saliency maps to detect whether insignificant words affect saliency maps. We add some random words at the end of the texts and then measure the correlation between the change of the prediction and the change of the saliency scores based on the Pearson correlation coefficient and Spearman correlation coefficient."}, {"title": "5 EXPERIMENTS", "content": "Following the proposed evaluation framework, we carry out experiments to investigate whether the pre-trained language models (PLMs) have the ability of humor understanding (see Sec. 5.1), whether external knowledge can improve their humor understanding ability (see Sec. 5.2), and the interpretability of the detected clue words which lead to PLMs' correct prediction on humor understanding tasks (see Sec. 5.3).\nExperiment Setup. Our experiments are carried on GeForce RTX 3090 GPU (on our machine) and TPU (on Google Colab) with Pytorch in Python. The sequence length is set to 200. We initialize the learning rate to 2e-5 and batch size from 4 to 32 according to the memory of the machine, and use early stopping with 20 epochs.\nBaselines, Datasets and Metrics. We adopt some representative PLMs, including base and large versions of BERT [14], ROBERTa [25], BART [24], T5 [31], CPT [33] for humor recognition, humor type classification, and humor level classification tasks. For the punchline detection task, we adopt base and large versions of simCSE-BERT [18] and simCSE-ROBERTa [18], which are proved to perform better in text matching. We divide each dataset into a training set and a dev set at a ratio of 7 to 3, and make down-sampling for all training and dev set to balance sample numbers in different classes. We use accuracy with percentage as the metric."}, {"title": "5.1 RESULTS ON ORIGINAL/FINE-TUNED PLMS", "content": "The evaluation results on original and fine-tuned PLMs based on four humor-relevant tasks are shown in Table 4 (see the column \"zs\" and \"ft\"). From the results, we observe that the original PLMs have weak ability on humor understanding with the average value 54.98, 33.77, 33.32, and 49.96 in the zero-shot learning on the humor recognition, humor type classification, humor level classification, and punchline detection task, respectively. After fine-tuning on the corresponding sub-datasets, the performance is improved by 68.67%, 72.64%, 40.87%, and 91.89%, respectively. The accuracy on humor recognition and punchline detection are both over 90%. It suggests that PLMs have a certain degree of ability in humor recognition and punchline detection after fine-tuning on the humor dataset."}, {"title": "5.2 RESULTS ON KNOWLEDGE-ENHANCED PLMS", "content": "In this part, we only present the experimental results by injecting Chinese pinyin into PLMs in the way of explicit fusion as shown in Table 4. That is because we observe in our experiments that either injecting Chinese pinyin in the way of implicit embedding, or injecting another one or two types of knowledge embeddings in any ways do not improve PLMs' performance in all the four humor-relevant tasks. Due to space limitation, we omit these results and will make analysis later.\nSee the column \"K-ft\" in the above Tables, we find that PLMs perform better, improving by 70.78%, 77.47%, 44.07%, and 94.63% on the humor recognition, humor type classification, humor level classification, and punchline detection task respectively. This group of experiments demonstrates that external linguistic knowledge such as Chinese pinyin has a positive effect for PLMs in humor-relevant tasks, and injecting external knowledge by explicit fusion is more possible to maintain important information in the knowledge than by implicit embedding.\nHowever, for another one or two types of knowledge, which do not improve the performance of PLMs in all the four evaluation tasks, we give the possible reasons as follows: 1) The huge amount of data used for training PLMs may already contain most of the factual knowledge and commonsense knowledge, thus the existing knowledge bases or commonsense bases can not contribute more for PLMs in humor understanding. 2) Some humorous texts need complicated specific knowledge or inference paths to understand, which can not be provided by existing knowledge. For example, in the following harmonic humorous text\u201c...\u4e00\u6765\u95fb,\u4e8c\u6765\u95fb,\u4e09\u6765\u95fb..\u201d, the Chinese phrase \u201c\u4e00\u6765\u95fb\u201dhas a similar pronunciation with the English word \"eleven\", where \u201ce\u201d and \u201cleven\u201d correspond the pronunciation of Chinese character\u201c\u4e00\u201d and \u201c\u6765\u95fb\u201d, respectively. Therefore,\u201c\u4e8c\u6765\u95fb(two-leven)\u201d and \u201c\u4e09\u6765\u95fb(three-leven)\", are analogous to\u201c\u4e00\u6765\u95fb(one-leven)\u201d,, which produce harmonic humor. It's a much difficult inference process for PLMs to understand and make correct humor type classification.\nMoreover, when we inject other existing knowledge from knowledge bases and commonsense bases except Pinyin knowledge in the way of implicit embedding into PLMs, the performance for PLMs in the humor-relevant tasks do not have any improvement. Due to space limitation, we also omit these results in our paper and give some possible analysis as follows: 1) Fusing Knowledge from different sources will do harm to separate feature of each type of knowledge. It's difficult for PLMs to learn effective information from the fused knowledge. 2) Humor is a much tough issue. The existing knowledge is not powerful enough for PLMs in humor understanding. Thus, besides linguistic knowledge, PLMs also need humor-relevant background knowledge for better performance in humor-relevant tasks and better humor understanding ability.\""}, {"title": "5.3 RESULTS ON INTERPRETABILITY ANALYSIS", "content": "To visualize the interpretability of PLMs' humor understanding ability, we draw saliency maps for sentences to show the detected clue words. We first investigate the stability of the clue words detection results to verify whether these clue words are faithful to PLMs. We add some random characters such as\u201c\u6211(I)\u201d at the ending position of samples, and then measure the correlation between the change in the prediction and the change in the saliency scores. The p-values of Pearson correlation coefficient and Spearman correlation coefficient are 0.0087 and 0.0010, respectively in the zero-shot learning, 0.0086 and 0.0060, respectively in the fine-tuning, and 0.0080 and 0.0064, respectively in the knowledge-enhanced fine-tuning. The changes are statistically different due to p-values all below 0.05, which suggests that saliency scores highly relate to predictions, thus the saliency scores are stable and the clue words detection results are faithful to PLMs, which can be trusted by humans for interpreting PLMs' humor understanding ability.\nFig 4 gives the saliency maps of several samples got from BERT-base humor-relevant tasks, and the faithfulness of the saliency map on four humor-relevant tasks is shown in Table 5. In the humor recognition task, we observe that the original PLMs only take some special tokens, such as [CLS], [UNK], or punctuations (see Fig 4 (a1)) as the clue words (which have deeper color). We mask the top three salient words except [SEP] of each instance, and the average performance has a slight drop from 54.98 to 53.61. In another three humor-relevant classification tasks, the results in the zero-shot learning as shown in Fig 4 (b1) are similar with those on the humor recognition task. These results prove that the original PLMs (without fine-tuning) can hardly understand humor, and their predictions on humor are nearly from arbitrary choices.\nAfter that, we fine-tune PLMs on four humor-relevant tasks, respectively. We observe that on the humor recognition sub-dataset, the saliency maps for fine-tune PLMs show that the models focus more on the sentiment words, such as\u201c\u56b7(shout)\u201d,\u201c\u9b54(demon)\u201d, when making correct predictions (see Fig 4 (a2)). We also mask the top three salient words of each instance, and the average performance has a dramatic drop from 92.73 to 53.58. When we fine-tune PLMs on humor type classification sub-dataset, PLMs focus more on the significant words, such as\u201c\u7a0e(meaning: taxes, pinyin: shui)\u201d,\u201c\u7761(meaning: sleep, pinyin: shui)", "examination)": "\u82f1(English)\u201d, in the incongruous humorous text when making correct predictions. We conjecture that PLMs find deep semantic correlations among these clue words, which help them make correct predictions. Moreover, when we fine-tune PLMs on the punchline detection sub-dataset, PLMs extract some significant words such as \u201c\u6821(meaning: school, pinyin: xiao)\u201d,\u201c\u5b5d(meaning: filial, pinyin: xiao)"}, {"title": "6 EVALUATION ON DOWNSTREAM TASKS", "content": "Similar to happiness, sadness, and anger, humor is one of the essential emotions of humans. Therefore, we carry out further evaluations to investigate whether PLMs which have been fine-tuned on the humor dataset can achieve better performance on the downstream tasks. We choose four Chinese sentiment classification datasets 8: 1) ChnSentiCorp-htl-all (Chn), which is a hotel review dataset with more than 5,000 positive reviews and more than 2,000 negative reviews; 2) Waimai-10k (Wai), which contains 4,000 positive and 8,000 negative user reviews; 3) Online-shopping-10-cats (Shop), which has 30000 positive and 30000 negative user comments with online shopping; 4) weibo-senti-100k (Wei), which has about 50000 positive and 50000 negative comments.\nWe fine-tune BERT, which has been fine-tuned on our constructed Chinese humor dataset, including all the four sub-datasets. The results are shown in Table 6. The baseline results are based on BERT, which are derived from their published research [17, 42, 43, 46].We observe that the performance increases a little after fine-tuning on the humor dataset (see the row \"ft\"). The results suggest that the sense of humor has common characteristics with other emotions, which thus humor-fine-tuned PLMs have a positive effect on other sentiment analysis tasks. We also find that there is no further improvement after injecting external knowledge in any above-mentioned ways (see the row \"K-ft\"). It suggests that Chinese pinyin is a unique and important characteristic for humor understanding, which is not very useful for other relevant downstream tasks."}, {"title": "7 RELATED WORK", "content": "PLMs demonstrate terrific capabilities on various downstream tasks [6-12]. We list two main related work as follows:\nHumor Datasets and Corpora. Some research dedicate themselves to construct a large-scale humor datasets and corpora. For example, Engelthaler and Hills [15] design a humor dataset which provides researchers with a list of humor ratings with 4,997 English words. Chiruzzo et al. [13] present the development of a corpus of 30,000 Spanish tweets that were crowd-annotated with humor value and funniness score. Hossain et al. [22] introduce a new dataset called Humicroedit that design simple replacement edits to make English news headlines funny. Hasan et al. [20] introduce a multimodal English humor dataset to detect humorous expressions in TED talks. Different from the above research, we construct a Chinese humor dataset, which includes four sub-datasets, each of which can be used for one representative humor-relevant task.\nHumor Recognition. Other research on humor focus on humor recognition which is to decide whether a given sentence expresses a certain degree of humor. For example, Mihalcea and Strapparava [28] report text classification techniques are a viable approach to recognize humorous one-liners. Barbieri and Saggion [2] design several linguistic features to automatically detect irony and humor in twitter. Cattle and Ma [4] adopt the minimum, maximum, and average Word2Vec similarity between ordered word pairs to recognize humor and extract humor anchor. Yang et al. [40] investigate the latent semantic structures behind humor in four aspects. Morales and Zhai [29] propose a generative language model and design some key component to identify humor in reviews. Chen and Lee [5] use semantic structural features and semantic distance features to predict audience's laughter in TED Talk Data based on convolutional Neural Network. However, most of them design linguistic features to recognize humor. They ignore the powerful learning abilities of pre-trained language models (PLMs). Different from them, we design a comprehensive evaluation framework to make a research on PLMs' humor understanding ability."}, {"title": "8 CONCLUSIONS AND FUTURE WORK", "content": "Humor understanding for PLMs is a challenging research in Natural Language Processing. In this work, we systematically investigate the humor understanding ability of PLMs with a designed comprehensive framework with three evaluation steps and four evaluation tasks. We also construct a comprehensive Chinese humor dataset, and our empirical study on it yields some valuable observations : 1) While the original PLMs can hardly understand humor, they could gain a certain degree of humor understanding ability through fine-tuning. 2) Linguistic knowledge such as Chinese Pinyin has a positive effect for PLMs in humor-relevant tasks. 3) The existing knowledge bases and commonsense bases can not provide much required knowledge for humor understanding. As a future work, we will find ways to collect more cultural knowledge for the optimization of PLMs in humor understanding."}, {"title": "9 ACKNOWLEDGEMENT", "content": "This work was supported by Science and Technology Commission of Shanghai Municipality Grant (No. 22511105902), National Natural Science Foundation of China (No.62072323), Shanghai Municipal Science and Technology Major Project (No.2021SHZDZX0103), Shanghai Science and Technology Innovation Action Plan (No. 22511104700), and National Natural Science Foundation of China (No. 62102095). Yanghua Xiao is also a member of Research Group of Computational and AI Communication at Institute for Global Communications and Integrated Media, Fudan University."}]}