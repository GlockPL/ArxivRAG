{"title": "A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding", "authors": ["Abdulfattah Safa", "G\u00f6zde G\u00fcl \u015eahin"], "abstract": "Dialogue State Tracking (DST) is crucial for understanding user needs and executing appropriate system actions in task-oriented dialogues. Majority of existing DST methods are designed to work within predefined ontologies and assume the availability of gold domain labels, struggling with adapting to new slots values. While Large Language Models (LLMs)-based systems show promising zero-shot DST performance, they either require extensive computational resources or they underperform existing fully-trained systems, limiting their practicality. To address these limitations, we propose a zero-shot, open-vocabulary system that integrates domain classification and DST in a single pipeline. Our approach includes reformulating DST as a question-answering task for less capable models and employing self-refining prompts for more adaptable ones. Our system does not rely on fixed slot values defined in the ontology allowing the system to adapt dynamically. We compare our approach with existing SOTA, and show that it provides up to 20% better Joint Goal Accuracy (JGA) over previous methods on datasets like MultiWOZ 2.1, with up to 90% fewer requests to the LLM API. The source code is provided for reproducibility", "sections": [{"title": "1 Introduction", "content": "Dialogue state tracking (DST) is a critical component of task-oriented dialogue systems, designed to extract and maintain users' goals throughout a conversation (Young et al., 2007). The challenge of DST lies in the infinite possibilities of user/agent conversations and the constant evolution of services, schemes, and APIs that dialogue systems interface with (Ren et al., 2018). While traditional approaches demonstrate reasonable performance within predefined ontologies (Mrk\u0161i\u0107 et al., 2017; Liu and Lane, 2017), current research is exploring various strategies for domain transfer. These strategies include adaptation to unseen domains (Li et al., 2021; Aksu et al., 2023), leveraging non-dialogue QA data to enhance generalization (Liu and Lane, 2017), and framing DST as a question-answering problem using natural language descriptions to enable zero-shot transfer (Lin et al., 2021b). However, these approaches still require training on seen domains and closely adhere to domain ontologies.\nA new generation of large language models (LLMs) such as GPT-4 (OpenAI, 2023), Llama 2 (Touvron et al., 2023) and Gemini 1.0 (Gemini, 2024) promise the ability to solve tasks without task-specific fine-tuning, relying instead on the extensive world knowledge acquired from training on vast amount of data. These LLMs have shown remarkable capabilities in in-context learning (ICL), where the model generates responses based on a natural language prompt and a few examples, achieving significant advancements over fine-tuned methods in few-shot scenarios. Researchers have begun to apply LLMs with ICL techniques to address the DST challenge (Heck et al., 2023; Pan et al., 2023; Feng et al., 2023), yet they have not surpassed state-of-the-art (SOTA) supervised methods, or lacked practicality in terms of number of queries to be executed for every single turn, and are highly dependent on the fixed ontology. Furthermore, the majority of these works (Heck et al., 2023; Feng et al., 2023) use gold domain labels, usually skipping the domain classification phase, which is nontrivial.\nTo address these challenges, we introduce a zero-shot, resource-efficient, and open-vocabulary pipeline system for task-oriented dialogue understanding. Our pipeline starts with domain classification, a crucial phase often overlooked in existing approaches, followed by two complementary approaches for DST. First, we propose DST-as-QA, which transforms DST into a multiple-"}, {"title": "2 Related Work", "content": "Dialogue State Tracking as a Question-Answering Problem The field has seen various approaches to addressing dialogue state tracking (DST) by framing it within a question-answering (QA) context. Gao et al. (2019) introduced slot-filling as sequential QA tasks, employing a Recurrent Neural Network (RNN) for generating responses. Following that trend, Tavares et al. (2023) fine-tuned a T5 model for the sequential QA tasks and performed zero-shot tests in unseen domains.\nSimilarly, Li et al. (2021) employed manually created questions and a GPT-2 decoder for generating slot values under a supervised framework, then tested for zero-shot applicability. Cho et al. (2023) used a retrieval model to find relevant QA pairs from previous dialogues, then finetuned a T5 model with these samples to adapt to unseen domains. To the best of our knowledge, there exists no in-context learning approach that formulates DST as a QA task. All the aforementioned approaches require a form of fine-tuning and domain adaptation technique. Furthermore, majority of the models extensively depend on existing ontologies for generating answers (Zhou and Small, 2019; Cho et al., 2023), and several others (Tavares et al., 2023; Li et al., 2021) struggle with the efficiency, since they need to generate tremendous amount of questions per turn (see \u00a76.2).\nLLMs Zero-Shot Dialogue State Tracking Pan et al. (2023) was the first to explore ChatGPT's zero-shot dialogue understanding capabilities using schema-based prompts, achieving notable success in basic slot-filling tasks but encountering issues with multi-turn dialogues. Then, Heck et al. (2023) assessed ChatGPT's performance across various slot types, employing custom prompts for different interaction lengths and slot types, slightly lagging behind state-of-the-art zero-shot models in handling complex slots. After that, Feng et al. (2023) attempted to track the slot values of the dialogue turns one by one, appending all possible slot values from schema and outperformed the current zero-shot models. Yet, this approach needs extensive number of queries for every turn, and unable to handle the dontcare slot values. Despite the remarkable results reported by these models, their dependence on predefined schemas and ontologies limits their practical utility in DST, particularly in environments where new entities, types, and services are continuously introduced.\nWhile both QA and DST-as-zero-shot approaches have shown promise, our research specifically addresses the challenge of identifying relevant slots to query efficiently, and generating possible answer options for the questions (open-vocabulary), instead of using ontology-dependent answer templates. Furthermore, we experiment with a realistic end-to-end pipeline that includes domain classification, unlike approaches that treat domains as \"given\" and use the gold annotation."}, {"title": "3 Methodology", "content": "An overview of our methodology is given in Fig. 1. Our pipeline begins by identifying the active domain for each turn. For domain classification, the active turn and all preceding turns in the dialogue history are passed to the model, which determines the domain using a specific prompt tailored to the language model (see \u00a7 3.2). Next, we extract the values of the selected domain slots using two approaches: question answering (see \u00a7 3.3) and self-refined prompting (see \u00a7 3.4).\n3.1 Self-Refined Prompt\nSelf-Refined prompt (SRP) approach (Madaan et al., 2024) iteratively refines the prompts harnessing the adaptive capabilities of language models (LMs). It has been shown to benefit many NLP tasks such as Code Optimization, Sentiment Reversal and mathematical reasoning that encouraged us to employ it both for domain classification and dialogue state tracking. Initially, a basic prompt template $P$ serves as either a simple task description outlining what the LM needs to accomplish or as a preliminary version of the prompt that will be iteratively improved. The LM generates an initial output based solely on $P$, without any additional context or task-specific information. Following this, the prompt itself includes instructions for the LM to analyze its output, identifying ambiguities, inaccuracies, or gaps in understanding. The SELF-REFINE approach is then employed, where the LM provides feedback on its own output, such as identifying specific errors or suggesting improvements. The model uses this feedback to refine $P$, generating a new version $P'$ that addresses the identified shortcomings, such as including more specific instructions or restructuring the format. This cycle of self-assessment, feedback, and refinement continues until the prompt $P$ evolves into a version $P'$ that meets a stopping criterion. The stopping criteria are reached either when the model begins to make only minor changes, suggesting that further refinements would not lead to notable improvements, or when the iteration limit is reached to ensure efficiency. The process ensures that the model optimizes its instructions and task execution autonomously, ultimately resulting in a prompt that consistently achieves better task performance across diverse contexts.\nIn this study, each LLM uses a set of custom self-refined prompts tailored to its specific characteristics and capabilities. The decision to employ model-specific prompts was driven by the recognition that different LLMs, due to their unique architectures and training data, may respond optimally to slightly different linguistic cues and task formulations (Zhu et al., 2024).\n3.2 Domain Classification\nUnderstanding user intent and their specific requests is crucial for dialogue state tracking (DST) and begins with identifying the target domains. This pivotal step is often overlooked in DST models, which either rely on a predefined set of domains from the dataset or attempt to indiscriminately track slots across all domains for each interaction. To develop a robust DST pipeline that accommodates multiple domains per turn, we classify the domains for each turn within the dialogue by incorporating dialogue history into the equations. Specifically, the input to the classifier is both the turn and the dialogue history. Let $D = {d_1,d_2,...,d_n}$ be the set of predefined domains and let $H_{t-1}$ represent the dialogue history up to turn $t$, with $U_t$ being the user's utterance at turn $t$. Then we define the classification function $f$ as $f(U_t, H_{t\u22121}) \u2192 {d_{i_1},d_{i_2},...,d_{i_k}}$, where ${d_{i_1}, d_{i_2},...,d_{i_k}} \u2286 D$ represents the set of domains classified for turn $t$. To approximate this function, we use a multi-label classification approach that can guide the model to infer the appropriate domains from the dialogue. The final prompt template is given in App. A.1.\nAs domain classification is the first step in the pipeline, its accuracy is critical for downstream tasks like slot tracking. Any misclassification can have a cascading impact. To mitigate this, we ensure that domain classification is independent for each dialogue turn. For instance, if a turn is classified as taxi-related, we don't carry over the \u201ctaxi\u201d to subsequent turns. To further improve the accuracy, we enforce strict guidelines in the prompts to ensure the classification remains grounded in the specific dialogue context and is not biased toward domains that might be inferred from specific phrases (the instruction part of the prompt). For example, given the utterance \u201cI want a taxi to go to the hotel.\", the domain should be strictly classified as \"taxi\" rather than \"hotel\" and \"taxi\". This is also to solve the classification of the closing turns (e.g., \"have a nice day\", \"anything else for today\"), which contain generic terms and hurt the performance as also showed in Hude\u010dek and Dusek (2023).\""}, {"title": "3.3 DST as Question Answering", "content": "Our DST-as-QA approach is given in Fig. 1(2.b), DST operates methodically at each user turn, indexed by $i$, incrementally updating the state as the dialogue progresses. First, we identify the set of entity types from the dataset README files (e.g., TIME, LOCATION etc...). Then, we extract named entities via zero-shot prompting given the user utterance $U_i$ resulting in a set $E_i = {e_1, e_2, ..., e_n}$, which includes all named entities identified during the turn. For instance, if a user says \"I'll need to arrive by 11:00 and it should be going to London Liverpool Street\", the named entities extracted could include \"11:00\" for the TIME entity and \"London Liverpool Street\" for the LOCATION entity. Next, these entities are matched by type to corresponding slots using a predefined matching function given in App. G, forming matched pairs $m_i = {e_i, {s_0..n}}$. Note that the mapping can be one-to-many. For instance, \u201c11:00\u201d is matched both to the leave-at and arrive-at (TIME), and \"London Liverpool Street\" is matched to the departure and destination slots (LOCATION). Then, for each slot $s_i$ that has been matched with an entity type (e.g., leave-at), a multiple-choice question with the options: found entity value (e.g., 11:00), and None. There are two exceptional cases: First case is detecting a dontcare slot. In that case, it is added to the options. The second case occurs when slots of the same type are captured in previous turns (e.g., booking a hotel and then a taxi to the booked hotel). Then, we add the values of these slots to the options to allow the model to handle cross-referencing issues (i.e., where a slot value in one domain depends on a slot value from another domain). We finally concatenate the question with the active turn $U_i$, and the dialogue history $H_{i\u22121}$ and the options and then prompt the model to select one of the options. The dialogue state at index $i$, denoted $D_i$, is updated with these selections, continually adapting with each user turn. This process ensures precise tracking and contextual updating of the dialogue, facilitating a dialogue state that dynamically adapts to user inputs and maintains contextual relevance throughout the interaction.\nHere, we create questions for only subsets of the slots that have extracted values of the same type, rather than for all the slots in the schema (Lee et al., 2021; Lin et al., 2021b; Li et al., 2021), which should significantly reduce computational costs. To illustrate, consider the MultiWOZ dataset, which includes 61 slots across 8 domains. If we were to generate questions for each slot in all 7372 turns of the test split, this would necessitate a total of 449,692 questions. However, by targeting only relevant subsets, we can dramatically decrease this number. Additionally, our approach does not rely on predetermined slot values from the schema or"}, {"title": "3.4 DST as Self-Refined Prompt", "content": "The general SRP approach is explained in Sec.3.1. Here, we explain the structure of the final revised version of the prompt App. A.4. It is divided into three main sections: task, schema, and regulations. The task section specifies actions for the language model, such as identifying updated or confirmed slots based on user input. The schema section provides a structured framework for the model by listing the slots to be tracked along with their descriptions. Finally, the regulations section defines the precise conditions and expected output formats, ensuring accurate updates of slot values.\nUnlike previous approaches (Hude\u010dek and Dusek, 2023), our method uses a consistent prompt template with adaptable slot names across all domains, making it flexible and efficient for supporting new slots with minimal modifications. Moreover, we instruct the model to track all slots simultaneously rather than one at a time, in contrast to (Feng et al., 2023), further improving efficiency. To illustrate, if we were to generate prompts for each slot individually across all 7372 turns in the MultiWOZ test split, similar to previous methods, this would result in 449,692 prompts (could be long prompt due to the task description length). Our approach, however, reduces this number significantly by consolidating all the domain slots into one comprehensive prompt per turn domain. It's worth noting that our schema avoids listing examples or potential slot values, focusing instead on precise task descriptions. This strategy improves the model's adaptability to varied dialogue contexts. Finally, unlike several previous approaches (Feng et al., 2023) that don't distinguish between \"None\" and dontcare slots, our method explicitly handles such cases, preventing misinterpretations and inaccuracies."}, {"title": "4 Experimental Setup", "content": "4.1 Datasets\nWe conduct experiments using test splits of two most common datasets for multi-domain task-oriented dialogue.\nSchema-Guided Dialogue (SGD) SGD (Rastogi et al., 2020) is the most challenging dataset, consisting of over 16,000 conversations between a human user and a virtual assistant. It encompasses 26 services across 16 domains, such as events, restaurants, and media. Notably, SGD introduces unseen domains in the test set, challenging the generalization ability of the model.\nMultiWOZ (Budzianowski et al., 2018) has had a significant impact on task-oriented dialogue research, serving as the first substantial public dataset available to researchers in this domain. The dataset includes over 10K conversations across eight domains, such as Train, Taxi, Bus, Hotel, Restaurant, Attraction, Police, and Hospital. Following the foundational MultiWOZ 2.0, the dataset underwent several significant annotation fixes and improvements (Eric et al., 2020; Zang et al., 2020; Han et al., 2020; Ye et al., 2022a). We chose to use versions 2.1 and 2.4 because 2.1 is the mostly widely in literature and 2.4 is the most stable version, and using both allows us to compare our results with previous work in a consistent manner.\n4.2 Evaluation\nFollowing the previous works (Hude\u010dek and Dusek, 2023; Ye et al., 2022b; Feng et al., 2023), we use accuracy for the domain classification task, and Joint Goal Accuracy (JGA) as main metric for the DST task. We also report Average Goal Accuracy (AGA) metric for the DST task in one experiment to compare the performance with the baseline model. JGA is the primary metric for DST evaluation and represents the ratio of dialogue turns for which the entire state is correctly predicted. AGA represents the average accuracy of the active slots in each turn. A slot becomes active if its value is mentioned in"}, {"title": "5 Experiments and Results", "content": "5.1 Domain Classification\nWe present the domain classification accuracy on MultiWOZ and SGD in Table 1. These results reveal that while all models are effective at domain classification tasks, Gemini demonstrates a slightly better performance in the MultiWOZ datasets. The improved accuracy on MultiWOZ 2.4 suggests that advancements in dataset quality and model improvements contribute to better overall performance. Both Llama3 and GPT-4 perform worse on SGD. We believe this is due to higher number of domains (e.g., 7 in MultiWOZ domains versus 16) and the considerable similarity between the SGD domains (see App. E). We observe that GPT-4-Turbo yields consistently high classification accuracy, showing robust performance across schemes.\n5.2 Dialogue State Tracking\nOur main results for the DST task are given Table 2, offering a comparative analysis of various language models on the MultiWOZ 2.1, 2.4 and"}, {"title": "6 Discussion & Analysis", "content": "6.1 Error Analysis\nTo get a better picture of the SRP approach performance with DST, we analyze the results of GPT-4-Turbo and Llama 3 per domain. Fig. 2 highlights that GPT-4-Turbo exhibits notable performance dips in the taxi domain. This performance issue could be related to the normalized leave-at and arrive-by time values in MultiWOZ 2.4 and the need for extra reasoning to handle cross-referencing slot values. Llama 3, while competent in certain areas, displays more variability. It performs well in the train domain but struggles notably in the attraction and hotel domain. More detailed analysis per slot can be found in App. F.\n6.2 Analysis of LLM Prompt Requests\nFinally, we evaluate the efficiency and scalability of our models by analyzing the number of LLM prompt requests required, a key metric for computational cost and deployment viability. Table 4 shows the average API calls per dialogue (see App. C for calculation details and the prompt length consideration) for MultiWOZ 2.4. The results show that our QA and SRP approaches need 96.35% and 97.08%"}, {"title": "7 Conclusion", "content": "In this work, we introduce a zero-shot, open-vocabulary Dialogue State Tracking (DST) system that integrates domain classification and DST in a single pipeline. By reformulating DST as a question-answering task and employing adaptable prompting techniques, our system dynamically adapts to new slot values without additional fine-tuning. We find that by selecting appropriate techniques-either QA-based or well-structured prompts tailored to the size of the language model-we can surpass SOTA models even without relying on any predefined values from ontologies. Although integrating the domain classification stage reduces the overall pipeline performance, it is essential for creating a practical system. Finally, we demonstrate the computational efficiency of our techniques by smartly selecting the slots to query and optimizing the prompts to track all input slots in one go, rather than querying the system for each slot individually."}, {"title": "Ethics Statement", "content": "The disclaimers for GPT-4-Turo and Gemini state that these models may produce inaccurate information and make mistakes. All models, code, and datasets were used in compliance with their respective licenses, terms of use, and intended purposes. We have provided the code and prompt templates developed for this work. The data we used and generated does not contain any information that names or uniquely identifies individual people, nor does it include offensive content."}, {"title": "Limitations", "content": "The performance of the Self-Refined Prompt (SRP) approach is dependent on the specific language model variant employed. Each model, has unique characteristics and capabilities that influence how well it can interpret and execute the prompts. Consequently, the SRP method requires careful tuning and adjustments for each model variant to achieve optimal performance. This process involves iteratively refining prompts. Additionally, the reliance on specific model variants means that updates or changes to these models by their developers could necessitate further adjustments to the SRP approach."}, {"title": "G Entities-Slots Mapping", "content": "Table 6 below shows the entity-type to slot map for MultiWOZ dataset."}]}