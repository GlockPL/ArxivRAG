{"title": "WiFlexFormer: Efficient WiFi-Based Person-Centric Sensing", "authors": ["Julian Strohmayer", "Matthias W\u00f6dlinger", "Martin Kampel"], "abstract": "We propose WiFlexFormer, a highly efficient Transformer-based architecture designed for WiFi Channel State Information (CSI)-based person-centric sensing. We benchmark WiFlexFormer against state-of-the-art vision and specialized architectures for processing radio frequency data and demonstrate that it achieves comparable Human Activity Recognition (HAR) performance while offering a significantly lower parameter count and faster inference times. With an inference time of just 10 ms on an Nvidia Jetson Orin Nano, WiFlexFormer is optimized for real-time inference. Additionally, its low parameter count contributes to improved cross-domain generalization, where it often outperforms larger models. Our comprehensive evaluation shows that WiFlexFormer is a potential solution for efficient, scalable WiFi-based sensing applications. The PyTorch implementation of WiFlexFormer is publicly available at: https://github.com/StrohmayerJ/WiFlexFormer.", "sections": [{"title": "1. Introduction", "content": "WiFi has emerged as a promising modality in person-centric sensing due to its advantages over optical approaches, including cost-effectiveness, unobtrusiveness, visual privacy protection, and the ability to perform long-range sensing through walls [4, 15]. Together, these characteristics enable efficient, contactless monitoring of human activities in confined indoor environments without the need for per-room sensor deployment, representing a significant economic advantage [18].\nChannel State Information (CSI) serves as the foundation for modern WiFi-based person-centric sensing. CSI is a metric obtained in the Orthogonal Frequency-Division Multiplexing (OFDM) scheme, which subdivides a WiFi channel into multiple sub-channels with different carrier frequencies (subcarriers) [7]. This subdivision allows for fast, parallel transmission of data, while CSI provides detailed information about how each subcarrier is affected by the environment, enabling the correction of environment-induced noise at the receiver on a per-subcarrier basis, and through correlating the distinctive patterns of amplitude attenuation and phase shifts in CSI caused by specific human movements, applications such as Human Activity Recognition (HAR) [12].\nWhile existing approaches to CSI-based HAR often rely on generic CNN-based vision architectures, they are not optimal due to their focus on local dependencies and shift-invariance [25]. To effectively leverage the unique properties of CSI, specialized architectures have been developed [2, 11, 25]. However, they tend to suffer from other problems such as overly complex designs and reliance on computationally expensive features, resulting in high inference times and limited practicality for real-time applications.\nContributions. To address these challenges, we make the following contributions: (I) We propose WiFlexFormer, a Transformer-based architecture that achieves similar HAR performance with significantly lower parameter count and inference time, making it highly efficient and well-suited for WiFi-based real-time person-centric sensing applications. (II) We conduct comprehensive evaluations of WiFlexFormer on publicly available WiFi datasets, assessing its HAR performance using amplitude and Doppler Frequency Shift (DFS) features and comparing it against existing state-of-the-art architectures. (III) We investigate the effectiveness of various subcarrier sub-sampling strategies to further optimize inference speed while maintaining model performance."}, {"title": "2. Related Work", "content": "Early works on learned methods for HAR using WiFi CSI data often relied on generic CNN-based vision architectures. Popular vision architectures such as ResNet [6], EfficientNet [21], and ShuffleNet [13] have been widely adopted for WiFi sensing tasks due to their proven effectiveness in feature extraction and their adaptability to various input types. For instance, the SignFi system [14] employed a 9-layer CNN for sign language recognition, while Widar3.0 [26] utilized a CNN-based architecture for cross-domain gesture recognition. Jiang et al. [9] proposed a CNN-based HAR framework that extracts features shared across different subjects and environments.\nTo better leverage the unique properties of CSI data, researchers have developed specialized architectures. SLNet [25] introduced a spectrogram learning neural network that uses complex-valued convolutional layers to extract features from DFS spectrograms. It relies on Principal Component Analysis (PCA) and Short-Time Fourier Transform (STFT) as preprocessing steps and introduces a submodule to remove spectral leakage due to STFT. While SLNet achieves impressive performance across multiple WiFi sensing tasks, its main drawback is the computationally expensive preprocessing step, which may limit its applicability in real-time or resource-constrained scenarios.\nLong Short-Term Memory (LSTM) networks have been utilized to capture temporal dependencies in CSI data [16]. Shi et al. [17] proposed an environment-robust device-free HAR system using CSI enhancement and one-shot learning to address the challenge of limited training data. Hybrid CNN-LSTM architectures have also been explored, such as in DeepSense [27], which introduced an autoencoder long-term recurrent convolutional network for device-free HAR, combining the spatial feature extraction capabilities of CNNs with the temporal modeling of LSTMs. Addressing the challenge of limited labeled data in new environments or for new users, RF-Net [2] introduced a meta-learning framework for one-shot human activity recognition using radio frequency (RF) signals such as WiFi. This approach aims to improve generalization across different domains and reduce the need for extensive data collection in each new setting.\nMore recently, Transformer-based architectures have been applied to WiFi sensing tasks, leveraging their ability to capture long-range dependencies and process sequential data efficiently. The Two-Stream Convolution Augmented Human Activity Transformer (THAT) architecture [11] takes a different approach by utilizing a two-stream structure to capture both time-over-channel and channel-over-time features. Contrary to our method it utilizes two transformer encoders in parallel whose outputs are concatenated and processed for the prediction. Other works, such as MetaFi [23] for WiFi-based pose estimation and WiTransformer [24] for gesture recognition, have further explored and adapted Transformer architectures for various WiFi sensing applications. While these specialized architectures have shown promising results, they often suffer from high complexity and computational costs, limiting their practicality for real-time applications. Additionally, many existing approaches rely on computationally expensive feature extraction methods, further increasing inference times and energy consumption.\nThe proposed WiFlexFormer addresses these limitations by using a lightweight Transformer-based architecture designed specifically for the efficient processing of WiFi CSI features. Unlike previous approaches that use generic vision architectures or overly complex specialized architectures, our method achieves comparable HAR performance with significantly lower parameter count and inference time. This makes it particularly well-suited for applications where low latency and energy efficiency are critical."}, {"title": "3. WiFlexFormer", "content": "The WiFlexFormer architecture, illustrated in Figure 1, comprises an initial stem module followed by a Transformer encoder. This design choice is motivated by the unique characteristics of WiFi CSI. The Transformer encoder has been shown to effectively capture long-range dependencies and global context [22], which is crucial for understanding the temporal and frequency patterns in CSI. The stem module provides initial feature extraction and dimensionality reduction, while the Transformer encoder enables the model to attend to relevant parts of the input sequence, regardless of their position.\nInput Features. WiFlexFormer expects generic real-valued input features in the shape [B, C, F, T], where B, C, F, and T correspond to the batch, channel, frequency, and time dimensions, respectively. This allows for the processing of common WiFi features such as CSI, DFS, amplitude, phase, and their derivations. To handle complex-valued inputs like CSI, we follow the approach presented in [25] by separating the real and imaginary parts and storing them in two real-valued channels. This results in an input tensor shape of [B, 2, F, T]. For unprocessed inputs such as CSI or amplitude, the dimensionality of F corresponds to the number of subcarriers, while for features resulting from STFT, such as DFS, F corresponds to the number of frequency bins.\nStem. The stem of WiFlexFormer is designed to handle various input features and perform initial feature extraction and dimensionality reduction. It consists of two main components: a 2D stem for multi-channel inputs and a 1D stem for further processing. For inputs with multiple channels (C > 1), such as DFS features where each subcarrier generates a 2D spectrogram, we first apply a 2D stem comprising two convolutional layers with kernel size (1, 3), the first maintaining the number of input channels and the second reducing it to 1, both using GELU activation functions.\nFollowing the 2D stem (or directly for single-channel inputs like amplitude features), we apply a 1D stem consisting of two blocks, each containing a 1D convolutional layer (kernel size 3, 32 filters), batch normalization, GELU activation, and dropout (rate 0.1). This 1D stem reduces the frequency dimension from its input size to a fixed dimension of 32.\nThis flexible architecture allows WiFlexFormer to handle various input types: amplitude features [B, 1, F, T] (F is the number of subcarriers) and DFS features [B, C, F,T] (C is the number of subcarriers, F is the number of frequency bins). The stem's design replaces heuristic preprocessing steps, providing an end-to-end learnable approach for feature extraction and noise reduction. The temporal receptive field of 5 in the 1D stem helps accumulate information from adjacent positions, enhancing the model's ability to handle noisy inputs.\nPositional Encoding and Class Token. To encode the temporal dimension, we apply Gaussian positional encoding to the stem's output, following the method described by Li et al. [11]. The resulting encoded features are combined with a class token before being input into a four-layer Transformer encoder. This token serves as an aggregator of global information, and its output will be used for the final classification. The use of a class token rather than direct feature aggregation, as in the work from Li et al. [11], prevents blurring temporal relationships.\nEncoder. Each layer of the encoder contains 16 attention heads and a feedforward dimension of 64. The final prediction is generated by processing the class token output through a linear classification head. The WiFlexFormer can be trained end-to-end and remains relatively lightweight, containing only \u2248 50k parameters (depending on the input shape). The proposed architecture is designed to work directly with CSI amplitude features and, therefore, does not rely on complicated or slow feature extraction methods.\nCross-Domain Generalization. WiFlexFormer incorporates several features that contribute to its potential for improved cross-domain generalization compared to other architectures. Its flexible input handling allows the processing of various features without extensive preprocessing, adapting to different data representations across domains. The combination of convolutional layers in the stem and the subsequent Transformer encoder allows for multi-scale feature learning, capturing generalizable patterns at various levels of abstraction. With only about 50k parameters, WiFlexFormer's lightweight design acts as a form of regularization, potentially preventing overfitting to domain-specific nuances."}, {"title": "4. Evaluation", "content": "The WiFi-based person-centric sensing capabilities of WiFlexFormer are evaluated on two publicly available datasets. These datasets encompass a variety of systems, transmitter-receiver configurations, and recording environments and include both micro- and macroscopic human activities. Additionally, our evaluation covers diverse scenarios, such as Line-of-Sight (LoS) and through-wall, i.e., Non-Line-of-Sight (NLoS), sensing. Given that cross-domain generalization remains a significant challenge in WiFi-based sensing [1], our evaluation is structured to measure performance in both in-domain and cross-domain contexts. To provide a comprehensive assessment, WiFlexFormer is compared against a range of state-of-the-art vision architectures, as well as architectures specifically designed for processing RF signals, such as WiFi. The evaluation metrics include both HAR performance and inference speed. Finally, to optimize inference speed, we explore various subcarrier sub-sampling strategies using amplitude and DFS features. DFS features are investigated alongside amplitude features since they are a popular choice among existing methods [2, 25] and due to their potential robustness to environmental changes. However, DFS features require more computational resources for extraction.\n4.1. Data\n3DO Dataset. The 3DO dataset is a WiFi through-wall HAR dataset specifically designed to evaluate model cross-domain generalization under varying static, dynamic, and temporal environmental conditions, making it distinct from existing datasets, which generally focus on multi-subject scenarios or same-room data collection. The dataset introduces a clear separation between environmental variations and participant-specific variations, focusing solely on environmental factors for isolated generalization assessment in a through-wall scenario. Unlike many other datasets that include multiple participants, 3DO uses a single participant to eliminate physiological variation, a well-known domain variation already covered by existing datasets. This approach allows for a focused evaluation of static and dynamic environmental changes in a through-wall scenario.\nThe dataset consists of recordings of three macroscopic activities: walking, sitting, and lying, performed in an office environment over three consecutive days. \nThe data collection uses the 2.4 GHz WiFi system by Strohmayer et al. [19], with an ESP32-S3-DevKitC-1U, ESP32-S3-WROOM-1U microcontroller, and ALFA Network APA-M25 antenna. WiFi packets are transmitted at 100 Hz and captured with synchronized video for activity labeling. Each day, five 5-minute sequences are recorded per activity. In walking, the participant moves freely within the area; in sitting, they alternate between two chairs with random head, arm, and leg movements; in lying, a fall detection scenario is simulated. In total, the dataset contains over 1.2 million labeled WiFi packets and is publicly available\u00b9.\nWidar3.0-G6 Dataset. The Widar3.0 dataset [26] is one of the most widely used person-centric WiFi datasets, featuring CSI recordings of 22 human hand gestures performed by 16 participants in three different indoor environments. Since not all 22 gestures are consistently performed in all three environments, a subset of Widar3.0, referred to as Widar3.0-G6 [8], is often utilized instead of the full dataset. This subset includes 6 gestures that are performed across all three environments by 15 users, resulting in a total of 11,250 hand gesture samples. The recording setup, shown in Figure 2c, consists of one 5.825 GHz WiFi transmitter (TX) and six receivers (RXn), each equipped with an Intel WiFi Link 5300 wireless NIC that has three antennas. The CSI of 90 subcarriers (3 antennas \u00d7 30 subcarriers) is collected at each receiver using the Linux CSI Tool [5], utilizing a packet sending rate of 1,000 Hz.\n4.2. Model Training\nArchitectures. We compare the performance of WiFlexFormer against a range of CNN-based vision architectures and specialized architectures for the processing of RF signals such as WiFi. The vision architectures include EfficientNetV2s [21], ResNet18 [6], and ShuffleNetV2x0.5 [13], while the RF signal processing architectures include RF-Net [2], SLNet [25], both of which rely on DFS features and THAT [11], which expects amplitude features as input.\nTraining Data. For model training, we utilize the 3DO dataset and the Widar3.0-G6 dataset, employing both CSI amplitude and DFS features. For the 3DO dataset, we perform a 3:1:1 split on day 1 data for training, validation, and testing to evaluate in-domain performance. Data from days 2 and 3 are reserved for testing cross-domain generalization under dynamic and static environmental variations, respectively. We use the CSI of the 52 Legacy Long Training Field (L-LTF) subcarriers, with samples extracted over a window of 351 WiFi packets, equivalent to 3.51 seconds at a 100 Hz packet sending rate, a duration determined empirically.\nThe Widar3.0-G6 dataset is employed to assess cross-receiver generalization. It is split into two subsets: one containing data from receivers RX1-3, used for training with an 8:2 training-validation split, and the other from receivers RX4-6, reserved for testing. To align with the single-link nature of the 3DO dataset, only the CSI from antenna 1 at each receiver is used, resulting in a selection of 30 subcarriers. Temporal sub-sampling is performed at 100 Hz, with the sampling window length set to 369 packets based on the longest sample length post-sub-sampling, while shorter samples are zero-padded to this length.\nBoth amplitude and DFS features are extracted from CSI data. DFS features are computed on a per-subcarrier basis using STFT with a Gaussian window and a segment and FFT length of 125 WiFi packets. A frequency band-pass filter from -60 Hz to 60 Hz, as proposed in [25], is applied, resulting in 121 frequency bins. The input shapes for amplitude features are [B, 1, 52, 351] for the 3DO dataset and [B, 1, 30, 369] for the Widar3.0-G6 dataset. For DFS features, the input shapes are [B, 52, 121, 351] and [B, 30, 121, 369] for the 3DO and Widar3.0-G6 datasets, respectively. Furthermore, for SLNet, real and imaginary parts of the complex-valued DFS features are stored separately in an additional dimension. The remaining models are fed with the absolute value of the computed DFS features.\nTraining Hyperparameters. For the activity recognition task, each architecture and feature configuration is trained from scratch in three independent runs with different random seeds, over 10 epochs. We use the AdamW optimizer with a learning rate of le-3 and a weight decay of le-3, optimizing for cross-entropy loss. To address class imbalances in the datasets, a balanced random sampler is employed. Training is conducted with a batch size of 32, and no data augmentation is applied, allowing us to evaluate the standalone generalization performance of each architecture. For each run, the best model, with respect to validation loss, is selected for evaluation on the test sets.\n4.3. Results\nWe evaluate model performance using standard metrics such as recall, precision, F1-score, and accuracy, computed on the test datasets for each model. To account for variability between runs, we report the mean and standard deviation of these metrics across three independent training runs, providing a more robust performance measure. Due to spatial constraints, only accuracy measurements are presented in the following; a detailed summary of all metrics is provided in the supplementary material.\nInference Time. To provide context for the HAR performance results, we first evaluate the inference times of all models, which reflect their parameter count and computational efficiency. We measure the inference time using amplitude and DFS features on an Nvidia Jetson Orin Nano single-board computer with 8 GB of VRAM. A batch size of 1 is used, resulting in input shapes of [1, 1, 52, 351] and [1, 52, 121, 351] for amplitude and DFS features, respectively. For each configuration, we perform 100 warm-up iterations preceding the measurement of the mean inference time over 1,000 iterations. The results, presented in Table 2, show that WiFlexFormer achieves the lowest inference times for both feature types, with a mean inference time of 9.26 ms for amplitude features and 11.06 ms for DFS features. The second-fastest model, ResNet18, achieves mean inference times of 9.67 ms for amplitude features and 12.01 ms for DFS features. In comparison, specialized models such as RF-Net (427.20 ms for DFS), SLNet (322.31 ms for DFS), and THAT (37.87 ms for amplitude) exhibit significantly higher inference times due to their larger parameter counts. These findings demonstrate that WiFlexFormer provides more efficient inference across both feature types, making it particularly suitable for real-time edge applications where low latency is essential.\nHAR Performance on 3DO. Table 3 presents the in- and cross-domain activity recognition performance for all models on the 3DO dataset using amplitude features. Day 1 represents in-domain performance, while days 2 and 3 reflect cross-domain performance under dynamic and static environmental variations, respectively.\nFor in-domain performance (day 1), all models perform similarly, with vision-based models such as ResNet18 and ShuffleNetV2x0.5 slightly outperforming specialized models. However, WiFlexFormer achieves a competitive accuracy of 98.41%, outperforming the specialized model THAT, while using only a fraction of the parameters (0.05 M vs. 7.96 M).\nIn the cross-domain evaluation on day 2, which introduces dynamic variations, WiFlexFormer demonstrates strong generalization capabilities, achieving 85.26% accuracy, second only to THAT at 88.03%. In contrast, vision-based models show a noticeable drop in accuracy, with ResNet18 reaching only 83.07%.\nOn day 3, which adds the challenge of static environmental variation, WiFlexFormer outperforms all other models with an accuracy of 86.98%, highlighting its robustness in challenging cross-domain scenarios. Notably, THAT experiences a significant drop in accuracy to 75.61%, while vision-based models like ResNet18 struggle to maintain performance, achieving only 78.70%.\nOverall, considering both in-domain and cross-domain performance, WiFlexFormer, using amplitude features, emerges as the best-performing model, offering superior generalization at a dramatically lower parameter count (0.05 M) compared to models like EfficientNetV2s (20.18 M) and ResNet18 (11.17 M), making it highly efficient for real-world WiFi-based HAR applications.\nTable 4 shows the in- and cross-domain HAR performance for all models on the 3DO dataset using DFS features. While WiFlexFormer is outperformed by larger vision models such as EfficientNetV2s and ShuffleNetV2x0.5, this is expected due to their high parameter count, which allows them to make better use of the dense DFS features across all subcarriers. In contrast, WiFlexFormer prioritizes strong feature compression in its design to remain computationally efficient, which limits its ability to leverage the full richness of DFS data. Despite this, WiFlexFormer delivers a competitive in-domain accuracy of 92.83%.\nIn cross-domain evaluations, especially on day 2, WiFlexFormer demonstrates reasonable generalization, achieving 79.91% accuracy, outperforming larger models such as ResNet18 and ShuffleNetV2x0.5, and coming close to EfficientNetV2s. On day 3, which introduces static environmental variations, WiFlexFormer continues to show stability, with an accuracy of 74.18%, higher than ResNet18 and comparable to other models, while SLNet and EfficientNetV2s exhibit a larger drop in performance.\nOne notable observation is the high run-to-run variance exhibited by the larger models, such as SLNet, across all days, indicating instability during training, especially when faced with out-of-distribution samples. In contrast, WiFlexFormer consistently shows lower variance, suggesting that its low parameter count may provide a natural regularization effect, leading to more stable training and performance across different domains.\nTable 5. Cross-receiver gesture recognition performance on the Widar3.0-G6 dataset using amplitude features. All models are trained on data from receivers 1-3 with an 8:2 training-validation split and tested on receivers 4-6. Amplitude features from all 30 subcarriers are used as input. Results are presented as mean and standard deviation across three independent runs with random initialization.\nInterestingly, none of the models, including the high-parameter models, were able to fully leverage DFS features for HAR, as the overall performance with DFS is notably lower than with amplitude features. This suggests that DFS features may not be optimal for this through-wall sensing scenario. We hypothesize that the noise induced by complex signal scattering and the highly noisy phase information involved in DFS computation likely contribute to the lower performance.\nFrom an efficiency perspective, this outcome is promising: amplitude features, which have a lower dimensionality and do not require computationally expensive preprocessing, outperform DFS features requiring computationally expensive preprocessing, such as STFT, across all models. For WiFlexFormer, this is particularly advantageous, as it achieves better HAR performance with simpler, faster-to-process amplitude features, making it an ideal solution for WiFi-based real-time sensing applications.\nHAR Performance on Widar3.0-G6. Table 5 presents the cross-receiver gesture recognition performance using amplitude features on the Widar3.0-G6 dataset. Overall, all models perform similarly, with EfficientNetV2s achieving the highest accuracy of 51.98%. Vision-based models generally outperform specialized models such as THAT and WiFlexFormer, but the performance differences remain small. For instance, WiFlexFormer trails EfficientNetV2s by only 2.6% accuracy, despite the latter having a 500x larger parameter count, demonstrating that WiFlexFormer offers competitive accuracy with a much smaller model.\nThe results using DFS features, as shown in Table 6, tell a similar story. EfficientNetV2s again achieves the highest accuracy with 51.34%, outperforming specialized models like RF-Net (48.11%) and SLNet (50.63%). WiFlexFormer delivers a competitive accuracy of 49.72%, trailing EfficientNetV2s by only 1.62%. Notably, WiFlexFormer outperforms RF-Net by 1.61% and comes close to SLNet, despite their significantly larger parameter counts.\nOverall, the performance across models on the Widar3.0-G6 dataset is quite close, and we observe no significant advantage in using DFS features over amplitude features in terms of cross-receiver generalization. Given that amplitude features require no computationally expensive preprocessing or parameter tuning, they remain a more efficient option for cross-domain generalization tasks.\nSubcarrier Selection. As an additional experiment, we evaluate the effectiveness of different subcarrier sub-sampling strategies using the 3DO dataset. While a straightforward way to process CSI-based features without information loss would be to pass features from all subcarriers to a model, this can result in high computational complexity and slow inference. This is particularly true for DFS features, which require STFT preprocessing on a per-subcarrier basis, incurring high computational costs. As demonstrated in [8], it is neither efficient nor necessary to consider all subcarriers, as inter-subcarrier information, while complementary, is highly correlated. Utilizing a smart subcarrier sub-sampling strategy that eliminates redundant subcarriers can reduce computational costs, shorten inference times, and result in similar model accuracy compared to an approach using all subcarriers. To this end, we evaluate the effectiveness of different subcarrier sub-sampling strategies, including random sampling from all subcarriers, band-restricted random sampling, uniform sampling, and projection-based sampling utilizing PCA for dimensionality reduction. The results of our evaluation for amplitude and DFS features are presented in Figures 3a and 3b, respectively. The presented accuracy measurements represent the mean and standard deviation across days 1-3, thus capturing both in-domain and cross-domain HAR performance.\nFor amplitude features, the highest accuracy is achieved using all subcarriers (None). Although uniform sampling of every 4th subcarrier (U4) and band-restricted sampling using eight bands with four subcarriers per band (B8-4) yield comparable results, the reduction in preprocessing and inference time for amplitude features is negligible, making sub-sampling unnecessary. For DFS features, the best accuracy is also obtained using all subcarriers, which is to be expected. However, subcarrier sub-sampling strategies, such as uniform sampling of every 2nd subcarrier (U2) or band-restricted sub-sampling with four bands and four subcarriers per band (B4-4), achieve similar accuracy while reducing the number of STFT computations to half and one-fourth, respectively. These strategies are a potential way to further reduce inference time, especially when using DFS features."}, {"title": "5. Limitations and Future Work", "content": "The lightweight design of WiFlexFormer offers advantages for real-time inference. With its low parameter count and an inference time of approximately 10 ms (Nvidia Jetson Orin Nano), it is well-suited for deployment on embedded devices, particularly in applications requiring high packet rates (e.g., 1-2 kHz for gesture recognition) [7]. However, its performance in real-world scenarios and across different edge hardware setups remains to be explored.\nMoreover, due to its small size and fast training capabilities, WiFlexFormer is well-suited for techniques like test-time training, allowing rapid adaptation to new WiFi domains [20]. This would be especially useful in dynamic environments like smart homes, enabling fine-tuning on-the-fly with minimal computational overhead and system downtime.\nWhile this work focused on comparing relative performance, WiFlexFormer's accuracy could be improved with techniques like ensemble models [3], which remain computationally feasible given its low parameter count. Pre-training and data augmentation, though not used here to avoid bias and variability, could further be leveraged to enhance absolute performance and generalization, especially in cross-domain scenarios [8].\nA limitation of this work is the evaluation's focus on two datasets: the 3DO dataset with three macroscopic activities and the Widar3.0-G6 dataset with six gestures. Future research should expand evaluation to diverse environments, hardware setups, and multi-person scenarios to better assess WiFlexFormer's generalization and applicability in various WiFi-based sensing domains."}, {"title": "6. Conclusion", "content": "In this work, we proposed WiFlexFormer, a highly efficient Transformer-based architecture designed for processing WiFi CSI features. Our comprehensive evaluation on publicly available WiFi datasets, assessing HAR performance using amplitude and DFS features, as well as inference time, demonstrates that WiFlexFormer compares favorably against state-of-the-art architectures. With only approximately 50k parameters and an inference time of around 10 ms (Nvidia Jetson Orin Nano), WiFlexFormer achieves a significant three orders of magnitude reduction in parameter count, making it particularly well-suited for on-device inference in real-time WiFi-based sensing applications at the edge. Additionally, we investigated the effectiveness of subcarrier sub-sampling strategies and identified uniform and band-restricted random sub-sampling as potential ways to further reduce computational complexity. The 3DO dataset used in our evaluation, along with a PyTorch implementation of WiFlexFormer, is made publicly available for further research and development."}]}