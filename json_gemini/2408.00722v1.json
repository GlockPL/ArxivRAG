{"title": "Pathway to Secure and Trustworthy 6G for LLMs: Attacks, Defense, and Opportunities", "authors": ["Sunder Ali Khowaja", "Parus Khuwaja", "Kapal Dev", "Hussam Al Hamadi", "Engin Zeydan"], "abstract": "Recently, large language models (LLMs) have been\ngaining a lot of interest due to their adaptability and extensibility\nin emerging applications, including communication networks. It\nis anticipated that 6G mobile edge computing networks will\nbe able to support LLMs as a service, as they provide ultra\nreliable low-latency communications and closed loop massive\nconnectivity. However, LLMs are vulnerable to data and model\nprivacy issues that affect the trustworthiness of LLMs to be\ndeployed for user-based services. In this paper, we explore the\nsecurity vulnerabilities associated with fine-tuning LLMs in 6G\nnetworks, in particular the membership inference attack. We\ndefine the characteristics of an attack network that can perform\na membership inference attack if the attacker has access to the\nfine-tuned model for the downstream task. We show that the\nmembership inference attacks are effective for any downstream\ntask, which can lead to a personal data breach when using\nLLM as a service. The experimental results show that the\nattack success rate of maximum 92% can be achieved on named\nentity recognition task. Based on the experimental analysis,\nwe discuss possible defense mechanisms and present possible\nresearch directions to make the LLMs more trustworthy in the\ncontext of 6G networks.", "sections": [{"title": "I. INTRODUCTION", "content": "The emergence of attention networks has been a stepping\nstone for transformer architectures, which also led to the\nintroduction of large language models (LLMs). More recently,\nLLMs are seen as the most significant advance in the field of\nartificial intelligence (AI) and a potential pathway to artificial\ngeneral intelligence (AGI) [1]. Every tech giant is in a race\nto advance in the field of LLMs by leveraging generative\nAI (GAI). Notable examples of LLMs from the tech giants\nare GPT-4 from OpenAI, LLaMA-3 from Meta and PALM\nfrom Google. However, there are also new players in this\nfield that surpass the performance of the LLMs mentioned\nabove. These include Mistral (in collaboration with NVIDIA),\nDCLM from Apple, xLAM from Salesforce, v2 chat from\nDeepseek, Groq, Claude, SmolLM and many more. These\nLLMs are trained on diverse and large amounts of datasets\nscraped or curated from the Internet. Some LLMs focus\non increasing model size, such as GPT, while others find\nnew ways to improve the generalization of LLMs through\ndata curation, model quantization, and innovative techniques.\nExamples of such LLMs are Claude, LLaMA, DCLM and\nGroq, which have recently outperformed GPT on various\nlanguage tasks. In continuation of the above-mentioned\nadvances in LLMs, several enterprises are leveraging pre-\ntrained encoders of LLMs to varying degrees to develop\ntheir own customized solutions for various applications and\nsectors, including healthcare, education, law and industrial\nautomation. In view of the rapid development of LLMs, it\ncan be assumed that LLMs will soon also be deployed on\nedge and handheld devices. Several studies have indicated\nthat the current iteration of 5G networks will not be able to\nsupport a plethora of services offered by LLMs. Therefore,\nresearchers are working intensively on the next iteration of\ncommunication systems, i.e. Sixth generation (6G), to meet\nthe above requirements [2]. Furthermore, as AI is an integral\npart of 6G systems, it is assumed that LLMs will be used\nintrinsically to optimize resources and performance while\nenabling human-centric customized services to users.\nThe standardization process towards 6G systems is already\nprogressing steadily. It is assumed that the evolution of the\ncommunication system will support distributed AI both for\nedge devices and within the mobile network [2]. Although\nmany researchers argue that the edge devices will not support\nthe use of LLMs, but with continuous breakthroughs in the\nfield of AI, support for edge devices can be extended through\ndistributed learning techniques such as federated learning (FL)\nand split learning (SL) [3]. In addition, quantization and\ntraining can be used to fine-tune an LLM on the edge devices.\nAs proposed in [4], an LLM with 65 billion parameters can\nbe fine-tuned with quantized low-rank adapters (QLoRA) on\na downstream task within a single day, achieving comparable\nperformance compared to other state-of-the-art (SOTA) LLMs.\nIt can be assumed that the convergence of quantized networks,\nLLMs and 6G Multi-Access Edge Computing (MEC) could\nresult in many innovative applications. Researchers have al-\nready begun to explore the mutual convergence of LLMs and\n6G MEC networks, calling them \"LLMs for networks\" and\n\"networks for LLMs\" respectively. We illustrate a network\nfor LLMs that corresponds to the vision of the Network\nfor AI (NetAI) with respect to a 6G communication system\nin Figure 1. The NetAI vision supports LLM deployments\nrelated to the MEC architecture [2]. Various services such\nas smart homes, healthcare, education, emergencies, mission-\ncritical applications and finance can be supported with the\nnetwork for LLMs.\nMost research today focuses on the integration of LLMs\nand communication networks, which would undoubtedly bring\nunprecedented advances and technological innovation. How-\never, one aspect of this technological progress is being over-\nlooked, namely the security aspect. With all the possibilities\nand potential of LLMs and the 6G ecosystem, we have to\nask ourselves, are LLMs trustworthy? Despite their ability\nto fine-tune to the downstream task, LLMs are deep neural\nnetworks that are vulnerable to privacy attacks, such as model\ninversion, model poisoning and membership leakage [3], [5].\nThe growing landscape of LLMs and their integration into\ncommunication systems therefore makes it necessary to ad-\ndress security concerns and the development of trustable AI\nencoders to safeguard the integrity of users and services in\n6G systems. To the best of our knowledge, the studies have\nnot explored the security vulnerabilities in network for LLMS\n(Net4LLMs), which subsequently leaves us defenseless against\nsuch attacks as we progress towards the Net4AI vision.\nTo address the above problem, in this paper, we propose to\naudit the trustworthiness of pre-trained AI encoders for mem-\nbership leakage attacks. The membership leakage attack is an\nattack in which the adversary tries to find out the distribution\nof the training data used to train the AI encoders. Considering\nthat the Net4LLMs will focus on fine-tuning the pre-trained\nAI encoders for service provisioning, the attacker will aim\nto determine if a data sample was used for the fine-tuning\nprocess. The fine-tuning process enables the replacement of\ntask-specific layer in the pre-trained AI encoder to meet\ntasks such as questions and answers, name entity recognition\nand classification [5], [6]. We evaluate the trustworthiness of\nAI encoders against the membership inference attacks. We\nassume that the adversary has particularly the knowledge of the\ndownstream task and the adversary is provided with the fine-\ntuned model, also known as black-box setting. We conduct\nexperiments to evaluate the trustworthiness of the AI pre-\ntrained encoders and to develop possible defenses to prevent\nthe membership leakage attacks in the context of Net4LLMs.\nThe specific contributions of this work are characterized as\nfollows:\n\u2022\n\u2022\n\u2022 This is the first study to investigate the trustworthiness of\npre-trained AI encoders for Net4LLMs.\n\u2022 Membership leakage attacks in the context of Net4LLMs\nare explored to assess trustworthiness.\n\u2022 Based on experimental analysis, defenses are proposed to\naudit trust in Net4LLMs.\n\u2022 At the end of the paper, open issues, challenges and\nfuture directions are also proposed to prevent potential\nadversarial attacks on LLMs."}, {"title": "II. RELATED WORKS", "content": "Recently, we have observed a plethora of advances in the\nfield of LLMs that would be a revolution in the field of\ncommunication networks, especially in the design and devel-\nopment of 6G networks. Some studies have already explored\nand demonstrated the significance of LLMs for potential 6G\napplications. For example, the study in [7] proposed Net-\nGPT, which enables personalized services to users through\ngenerative networks while handling comprehensive network\nintelligence and cloud collaboration in real time. Xu et al. [8]\nfocused on the data privacy in 6G communication systems\nusing LLMs. The study proposed to design LLM agents based\non the principle of split learning by distributing LLMs for\ndifferent roles across edge devices to make user interaction\nefficient and collaborative. Their results show that the split\nlearning setting was effective in improving the communication\nefficiency while offloading the tasks that are complex in nature\nto the servers for constructing global LLMs. The study in\n[9] emphasised that the newer LLMs must offer multimodal\nservices, i.e. they must handle image, text and audio data in\norder to offer automated services. Therefore, the deployment\nof LLM agents in the cloud could pose challenges in terms of\ndata privacy, high bandwidth costs and long response times.\nHowever, MEC based on 6G communication systems can\naddress the above problems in an effective way. Lin et al.\n[10]also proposed a split learning framework for the deploy-\nment of LLMs in 6G networks. However, their work focused\non the efficiency and effectiveness of LLMs in terms of\nparameter sharing, quantization, and efficient fine-tuning rather\nthan data or model security. Nguyen et al. [5] highlighted the\nadvantages of using LLMs in 6G networks while exploring\nthe security vulnerabilities from an adversarial point of view.\nHowever, the discussion of model security was very abstract\nand brief, focusing more on the attacks on services. The study\nalso suggested the use of blockchain technology to avoid the\nsecurity threats associated with LLMs and 6G networks. To\nthe best of our knowledge, none of the studies investigated a\nspecific model-based attack scenario related to pre-trained AI\nencoders and 6G networks."}, {"title": "III. TRAINING/FINE-TUNING LLMS IN 6G", "content": "The training strategies for fine-tuning LLMs in 6G networks\nare shown in Figure 2. The strategies are presented in accor-\ndance with the MEC framework. The cloud layer can leverage\nthe pre-trained AI encoders of LLMs for any of the training\nmodes, but the downstream task would mostly be generalized\nwhen passed to the edge and user layers. However, at the\nedge and user layer, the customization of the LLMs can be\ndone based on the context and resources. As can be seen from\nthe training modes, fine-tuning the pre-trained encoder and\nclassifier requires large amounts of computational resources.\nThe computational resources decrease significantly when mov-\ning to the edge and user layer. In this regard, the edge and\nuser layers can at most fine-tune/adapt the LLMs based on\ntheir application by keeping the pre-trained encoder frozen and\nusing the final layers to update the parameters. Alternatively,\nthey can simply freeze the entire LLM and extract only the\noutput embeddings that are used as representative features for\ntraining deep neural networks or shallow learning methods.\nAn example of this can be found in Figure 1 for an emergency\nservice application where LLM's pre-trained AI encoder can\nbe used and its parameters frozen while a limited amount of\nlabeled data is used to train the classifier or the final layers.\nThis would be beneficial for edge devices to fine-tune the\nnetwork locally to improve communication efficiency while\nutilizing computing resources efficiently. As proposed in [10],\nthe customization can be achieved through a split learning\nstrategy, where the cloud wants to fine-tune the LLM for a spe-\ncific task with respect to the communication system and splits\nthe network into smaller networks that are trained with devices"}, {"title": "A. Security Issues in LLMs and 6G", "content": "Several studies have now highlighted the security concerns\nrelated to the behavior, architecture and design of LLM. The\nsecurity concerns arise from the complexity of LLMs and\nthe challenges associated with their deployment and training\nprocess. In addition, backdoor attacks are possible in LLMs\nthat cannot be overcome with conventional security measures.\nThese backdoor attacks are applicable to LLMs that are fine-\ntuned in a supervised manner and trained with adversarial\nlearning or reinforcement learning. The different types of\nattacks in LLMs deployed within 6G networks are defined\nbelow.\n\u2022 Adversarial attacks: These attacks are carried out by\nmanipulating data to affect the performance of the model.\nAdversarial attacks can generally be divided into back-\ndoor attacks and poisoning attacks. In the former, a\ntrigger is hidden in the model to manipulate the inference\nbehavior, while in the latter, malicious examples are\ninjected into the training process to deceive the model.\n\u2022 Inversion attacks: Inversion attacks are performed to\nreconstruct the data or to extract certain information from\nthe model gradients. Inversion attacks include replicating\nthe model, extracting training data, gradient leakages,\nfeature space and stealing models.\n\u2022 Unfair exploitation and bias attack: This type of attack\nis related to the training data used to train or fine-tune\nthe LLMs. The attack disproportionately adds data with\na particular label to fine-tune or train the network so\nthat the inference perpetuates biases and unintentionally\nlearns to generate misinformation, social inequalities,\nreinforcement of stereotypes and discrimination in the\ngeneration of responses.\n\u2022 Instruction tuning attacks: These attacks aim to overload\nthe system's resources in order to carry out inadvertent\nactions. Examples of such attacks are Denial of Service\n(DoS), indirect prompt injection, jailbreaking and the\ndisclosure of guided prompts.\n\u2022 Zero-day attacks: These attacks are usually called sleeper\nagents because they are embedded with model weights\nwhen a particular defense method fails to eliminate them.\nThis type of attack is usually triggered by specific events\nor phrases. One example of such attacks is data theft.\n\u2022 Inference attacks: Last but not least, inference attacks\naim to extract sensitive information from the model,\nespecially in the context of the training data used to\nfine-tune a model. Examples of such attacks are attribute\ninference and membership inference attack. In this paper,\nwe focus on the membership inference attack as it can\nidentify specific data used to train or fine-tune the model.\nSuch information can be used to break the trust and\nconfidentiality of the AI model and be used against the\nuser. Other consequences of membership inference attack\ninclude breach of confidentiality, unauthorized access,\nidentity theft and violation of privacy."}, {"title": "IV. TRUSTABLE AI ENCODERS", "content": "In this section we provide the information about the threat\nmodel, the attack scenario, the datasets used for the attack and\nthe implementation details."}, {"title": "A. Threat Model", "content": "Before we define the threat model, we make some as-\nsumptions. We assume that the LLM is pre- trained on a\nlarge dataset capable of transforming the input (text) into\nembeddings. Using the pre-trained AI encoder, a downstream\ntask is fine-tuned by a customized dataset for a specific\napplication in 6G networks using optimization algorithms\nand a predefined loss function. The fine-tuned model is then\nable to transform the input into embeddings or classification\nprobabilities accordingly (which differ from the original, pre-\ntrained AI encoder). We define the attacker's purpose in this\nscenario as a dichotomous classification problem, where the\ngoal is to determine whether the input provided to the pre-\ntrained AI encoder is a member or non-member of the training\ndataset used for the subsequent task. In general, existing\nstudies assume two dimensions of an attacker's background\nknowledge when considering membership inference attacks.\nThe first dimension assumes a black box attack, which\nmeans that the attacker has no prior knowledge of the pre-\ntrained AI encoder architecture, but the attacker has access\nto the model that has been trained for the downstream task.\nThis is considered the most realistic scenario, as in 6G AI is\nused as a service and the models adapted for the downstream\ntask would be directly available to the public. The second\ndimension assumes that the attacker has access to a very small\nsubset of the member training data, which can be used to\ncreate an auxiliary dataset. The auxiliary dataset can then\nbe used to train the attack model. Studies have shown that\nsuch assumptions can be true if one infers the location and\nmakes an educated guess about the service used in a particular\narea [11]. With the large plethora of diverse data available on\nthe Internet, it is reasonable to assume that the attacker can\ngather meaningful data to create a shadow model for lodging\nmembership inference attack that corresponds to a real-world\nenvironment. Considering the two dimensions, we assume in\nthis study that the attacker has access to the downstream task\nmodel and has some knowledge of the application, which is\ntaken into account by the pre-trained AI encoder of the LLM."}, {"title": "B. Attack Scenario", "content": "It is known that the LLM's pre-trained AI encoder can be\nused for feature extraction, i.e. the LLM's task of transforming\nthe input into embedding vectors. The mapping of inputs\nto embeddings benefits the fine-tuning of LLMs or training\nwith deep neural networks for a specific task. However, when\nthe LLM is fine-tuned with the new data for a particular\ndownstream task, it tends to memorize the data during the\ntraining process. The memorization suggests that the member\ndata will have higher confidence values compared to the non-\nmember data. Therefore, it can be deduced that: (i) Pre-\ntrained AI encoders of LLMs behave differently to the member\nand non-member data. (ii) The behavior is propagated to the\nembedding vectors that are learned during fine-tuning for the\ndownstream tasks, so that the memorization of model will be\na part of the downstream model available to the attacker. We\nintend to use the above features to categorize whether the data\nis a member or a non-member of the pre-trained AI encoder.\nWe then apply the following steps to evaluate the effectiveness\nof the attack for a pre-trained AI encoder.\n\u2022 Our assumptions are that the attacker has some prior\nknowledge of the application for which the LLM is fine-\ntuned. Therefore, the attacker scraps the Internet or uses\npublicly available datasets to create an auxiliary dataset.\n\u2022 The attacker then prepares the auxiliary dataset for train-\ning by assigning pesudolabels to the data as members and\nnon-members. The attacker then feeds the pseudo-labeled\nauxiliary dataset into the downstream model. The training\nprocess is then performed to create an attack model that\nis capable of binary classification, i.e. categorizing the\ndata into members and non-members.\n\u2022 Once the attack model is trained, the attacker can enter\nthe candidate text into the attack model to determine\nwhether the candidate text is a member or a non-member."}, {"title": "C. Dataset", "content": "In this work, we use two state-of-the-art pre-trained lan-\nguage models RoBERTa [12] and ALBERT [13] for our\nexperiments. The two language models differ in their training\nschemes, loss functions and architectures. It should be noted\nthat we have not trained these language models from scratch,\nbut that we use the pre-trained language models for the attack\nscenario that are publicly available online\u00b9. According to the\nassumption considered for the attack scenario, the attacker\nhas access to the fine-tuned model for the downstream task.\nTherefore, we consider two publicly available datasets, i.e.,\nYelp Review/AG's News/SST [14] and CoNLL2003 [15]. The\nfirst data set is intended for the task of text classification, while\nthe second takes into account the task of Named Entity Recog-\nnition (NER). To perform the membership inference attack, we\nuse a small portion of the Yelp Review/AG's News/SST and\nCONLL2003 dataset, i.e., 0.15%, of each dataset to construct\nthe auxiliary dataset and label it as member data. We also\nconsider other third-party datasets such as AX, CoLA and\nIMDB for the auxiliary dataset and label them as non-member\ndata."}, {"title": "D. Implementation Details", "content": "To perform membership inference attack, we design a five-\nlayer multilayer perceptron as an attack model that uses the\noutput of the model fine-tuned to the downstream task as input.\nThe dimensions of the first layer vary depending on the model\nfine-tuned to a specific downstream task. Recall, precision and\nF1 score are used as evaluation metrics for the performance\nof the attack. The attack model is trained using the ADAM\noptimizer with a learning rate of 1e - 5. The model is trained\nfor 100 epochs. The auxiliary dataset was divided into two\nsets, i.e. a test dataset and a training dataset in a ratio of 1:5."}, {"title": "V. EXPERIMENTAL ANALYSIS", "content": "In Figure 3, we show the performance of the membership\ninference attack. It should be noted that Yelp Review/AG's\nNews/SST are classification tasks with 5/4/2 classes, respec-\ntively. The baseline, random guessing, refers to the value of\n0.5 for precision and recall. The attack performance shows\nthat the two pre-trained language models achieve a minimum\nF1 score of 0.77 in the SST task and a maximum F1 score of\n0.94 in the NER task. The results are significantly higher than\nrandom guessing, indicating that the membership leak exists\nin the pre-trained AI encoders.\nThe success rate of the attack raises serious concerns about\nthe trustworthiness of LLMs and pre-trained models in 6G\nnetworks. We repeat the above experiment with a reduced\nauxiliary dataset, i.e., we use Yelp Review/AG's News/SST\nfor training the attack model without considering CoNLL2003\nand vice versa to observe the results. The results for this\nexperiment are shown in Figure 4. It can be seen that the\nperformance is still above the random guess. Furthermore, the\nperformance degradation is about 0.12/0.096 for CONNL2003,\nabout 0.07/0.095 for Yelp Review, about 0.06/0.075 for AG's\nNews and about 0.105/0.074 for SST, using ALBERT and\nROBERTa, respectively. The attack performance still reaches\na maximum of 0.83 F1 score if the attacker only has access\nto the fine-tuned model for the downstream task, but does\nnot use the part of the same datasets. This behavior confirms\nour assumption that the pre-trained language models memorize\nthe data and behave differently with member and non-member\ndata.\nAnother interesting aspect was highlighted when we ex-\namined the attack performance while varying the number of\nclasses. Since the Yelp dataset has the highest number of\nclasses out of the datasets we selected, namely 5, we varied\nthe number of classes to observe the attack performance. The\nresults of this experiment are shown in Figure 5. It can be\nseen that the attack performance increases as the number of\nclasses increases. This is very interesting because it shows that\nthe membership inference attack can extract more information\nfrom data with a higher number of categories. It also shows\nwhy the attack performance on Yelp was better than on AG's\nNews and SST datasets, accordingly."}, {"title": "VI. ENABLING TRUST WITH PRE-TRAINED AI ENCODERS", "content": "Membership leakage and membership inference attacks\nhave been extensively studied in the context of computer\nvision and image modality. Defenses against such attacks\ntherefore include adversary regularization, differential privacy,\ndata augmentation, adding noise to images, intentional attacks,\nencryption techniques, and others [3], [11]. Some of the\nabove techniques are difficult to perform in textual modality,\nsuch as addition of noise and intentional attack initialization.\nSuch actions can also degrade the performance of LLMs in\n6G networks. Adding noise to either the data or confidence\nscores for classification can be used as a defense mechanism.\nHowever, studies suggest that such techniques degrade the\nperformance of the downstream task [5].\nBased on the observations gathered from our experiments, we\npropose two possible defenses. The first is to reduce the size\nof the dataset or reduce the number of epochs to fine-tune\nthe network. The intuition is that if the size of the dataset\nor the number of epochs for fine-tuning in the downstream\ntask is increased, the pre-trained AI encoder would tend to\nmemorize the downstream task data and thus make the mem-\nbership attack stronger. In this context, we suggest using either\nactive learning or curriculum learning, which can perform the\ntraining with less data or fewer epochs.\nThe second defense is based on the intuition \"confidence is\ndefined by trust\" (a quote from Patrick Mosher). In this case,\nhowever, we would look at confidence and trust from the\nperspective of AI. We propose to use a trust evaluation module\nat the edge layer that could evaluate the trust of the pre-trained\nAI encoder or a fine-tuned LLM with a predefined metric.\nOne of the examples of such a trust evaluation is as follows,\nassuming the fine-tuned LLM is trained for medical emergency\nservices using a 6G network.\n\u2022 The LLM is fine-tuned on less number of epochs and\na smaller amount of data. The responses do not ask\nfor age or personal information. Results in 88% for the\nperformance metric.\n\u2022 The LLM is fine-tuned on a large amount of data and high\nnumber of epochs. The responses do not ask for age or\npersonal information. Results in 89% for the performance\nmetric.\n\u2022 The LLM is fine-tuned on a large amount of data and\nhigh number of epochs. The responses asks for personal\ninformation. Yields 92% on performance metric.\nGiven the scenario described above, a trust module might favor\nthe first model as it is less vulnerable to membership leakage\nattack. The idea is simply to prioritize a fine-tuned LLM for\n6G services based on trustworthiness and confidence scores."}, {"title": "VII. OPEN ISSUES, CHALLENGES AND FUTURE\nDIRECTIONS", "content": "The integration of 6G and LLMs can be seen as\ntask-oriented communication services, where integration is\nachieved by utilizing resources from the communication in-\nfrastructure, the edge and mobile devices. In return, users\nreceive LLM agents that can perform certain actions, generate\ndata or call application programming interface (API) functions.\nAs already indicated, such integration can lead to security\nvulnerabilities, including the theft of personal information and\nmore. We have emphasised the importance of a trust module\nfor considered integration, but designing such a trust module\ncan present some challenges. These challenges include, but are\nnot limited to, the following.\n(i) Active Learning and Curriculum Learning ap-\nproaches: One of the ways to cope with membership leakage\nor inference attack is to use less amount of data and number\nof epochs for fine-tuning. In this regard, two approaches can\nbe opted for making this possible. The design of trust module\nneeds to favor the model with aforementioned characteristics,\nhowever, design of such methods can be a challenge that could\nhelp in resisting the attack while not compromising on the\nperformance.\n(ii) Multimodal LLMs: In this paper, we have focused\non the LLMs that only work with text data. In reality, users\nopt for multimodal LLMs that can generate text, images and\naudio. Each modality has its own security issues when it\ncomes to pre-trained AI encoders. However, it would be quite\na challenge to design a trust module for 6G and LLMs that is\nsuitable for different data modalities.\n(iii) User Privacy: Similar to the design of trust modules\nfor different data modalities, training processes, architectural\nmodifications, and encryption techniques must be used to\nimprove user privacy. Various attacks such as model inversion,\nmodel poisoning, gradient leakages, and adversarial attacks\ncan be used by attackers to disrupt the services and steal users'\nprivate information in 6G networks. Therefore, in addition\nto data modality, the trust module must also defend against\nvarious privacy attacks.\n(iv) Latency and Bandwidth Issues: We focused primarily\non the trustworthiness of the pre-trained Al encoders. How-\never, the fine-tuning of the LLMs and the deployment of the\ntrust module could burden the services in terms of latency and\nincreased bandwidth. In this context, the selection of suitable\nmodels for specific applications, scenarios and needs as well\nas optimization during deployment must also be researched.\n(v) Responsible AI: Finally, this study explores the trust\nmodule in the context of security and privacy. The trust\nmodule can be explored in the context of responsible use of\nAI so that hallucinations in the generation of data modality\ncould be controlled or restricted to prevent the spread of\nmisinformation, identity-related attacks and impersonation."}, {"title": "VIII. CONCLUSION", "content": "This article explores the use of LLM deployment in accor-\ndance with the 6G MEC framework. We evaluate the trustwor-\nthiness of fine-tuned models to be deployed as services in 6G\nnetworks. We discuss in detail about membership leakage and\ninference attacks with respect to the textual modality and show\nthat the attacks are quite effective in violating user privacy.\nWe propose possible defense mechanisms to cope with the\nmembership inference attacks and give several open issues,\nchallenges, and research directions for the development of\na generalization trust module for LLM deployment in the\n6G-MEC framework. We believe that this paper provides\nresearchers with the foundation for developing trustworthy\nLLMs for services in 6G networks."}]}