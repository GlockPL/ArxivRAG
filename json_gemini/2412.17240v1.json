{"title": "Rethinking Cancer Gene Identification through Graph Anomaly Analysis", "authors": ["Yilong Zang", "Lingfei Ren", "Yue Li", "Zhikang Wang", "David Antony Selby", "Zheng Wang", "Sebastian Josef Vollmer", "Hongzhi Yin", "Jiangning Song", "Junhang Wu"], "abstract": "Graph neural networks (GNNs) have shown promise in integrating protein-protein interaction (PPI) networks for identifying cancer genes in recent studies. However, due to the insufficient modeling of the biological information in PPI networks, more faithfully depiction of complex protein interaction patterns for cancer genes within the graph structure remains largely unexplored. This study takes a pioneering step toward bridging biological anomalies in protein interactions caused by cancer genes to statistical graph anomaly. We find a unique graph anomaly exhibited by cancer genes, namely weight heterogeneity, which manifests as significantly higher variance in edge weights of cancer gene nodes within the graph. Additionally, from the spectral perspective, we demonstrate that the weight heterogeneity could lead to the \"flattening out\" of spectral energy, with a concentration towards the extremes of the spectrum. Building on these insights, we propose the HIerarchical-Perspective Graph Neural Network (HIPGNN) that not only determines spectral energy distribution variations on the spectral perspective, but also perceives detailed protein interaction context on the spatial perspective. Extensive experiments are conducted on two reprocessed datasets STRINGdb and CPDB, and the experimental results demonstrate the superiority of HIPGNN. Our code and data are released at https://github.com/zyl199710/HIPGNN.", "sections": [{"title": "Introduction", "content": "Identifying cancer genes is a crucial endeavor in both research and clinical practice (Beroukhim et al. 2010; Mart\u00ednez-Jim\u00e9nez et al. 2020; Bailey et al. 2018). Cancer genes are closely related with protein interactions (Leiserson et al. 2015), motivating solutions that integrate the protein-protein interaction (PPI) network for efficient identification (Yang et al. 2021; Levi, Elkon, and Shamir 2021; Chitra, Park, and Raphael 2022; Yang et al. 2023; Wang et al. 2024, 2023). Such approaches exploit, for example, multi-omics data and protein interaction information to extract and derive features that distinguish cancer genes.\nThe aggregation capabilities of graph neural networks (GNNs) (Wu et al. 2020) have led to notable success in methods for cancer gene identification, based on graph convolutional networks (Schulte-Sasse et al. 2021), Chebyshev graph convolutional works (Peng et al. 2022) and masked graph autoencoders (Cui et al. 2023). However, these methods only use the PPI network to update the node features by referring to neighbor representations, which do not model the complete biological information within the network. Therefore there exists a gap: more faithfully depicting complex protein interaction patterns within the graph structure.\nOur motivation is as follows: cancer genes induce significant biological anomalies in protein interactions, such as mutations, changes in expression levels, or alterations in protein modifications, as illustrated in Figure 1 (a). These biological anomalies can be interpreted as graph anomalies in the PPI network, as shown in Figure 1 (b). By identifying and analyzing these graph anomalies, we aim to develop a more comprehensive understanding of cancer gene behavior on PPI networks for cancer gene identification.\nBased on this vision, our statistical experiments in this paper reveal a distinctive graph anomaly of cancer in the PPI network, which we term weight heterogeneity. As shown in Figure 1 (c), we compute the variance distribution of all edge weights (protein interaction confidence) for each node in a widely used PPI dataset, STRINGdb. It reveals that cancer genes exhibit greater weight variance compared to non-cancer genes.\nAdditionally, from the spectral perspective, we demon-"}, {"title": "Preliminaries", "content": "we first provide several necessary definitions and notation.\nWe define a weighted graph as $\\mathcal{G}_w = {\\mathcal{V}, \\mathcal{E}, \\mathcal{W}, \\mathcal{X}, \\mathcal{Y}}$, where $v_i \\in \\mathcal{V}$ represents the node and $N = |\\mathcal{V}|$. The node features and labels are denoted as $x_i \\in \\mathcal{X}$ and $y_i \\in \\mathcal{Y}$, respectively. The edge $e_{ij} \\in \\mathcal{E}$ connects nodes $v_i$ and $v_j$, and $W_{ij} \\in \\mathcal{W}$ is the edge weight of $e_{ij}$. Let $A$ be the corresponding adjacency matrix, where $A_{ij} = W_{ij}$ if there exists a weighted edge. It is worth mentioning that all graphs studied in this paper are undirected graphs, i.e., $A_{ij} = A_{ji}$.\nAn unweighted graph $\\mathcal{G}_{uw} = {\\mathcal{V}, \\mathcal{E}, \\mathcal{X}, \\mathcal{Y}}$ is defined similarly to a weighted graph, except that in its adjacency matrix $A$, $A_{ij} = 1$ if there exists an edge.\nGiven the weight heterogeneity exhibited by cancer genes on the PPI network, we model this phenomenon using a random weighted graph (Khorunzhy, Shcherbina, and Vengerovsky 2004; Ding and Jiang 2010). Specifically, for an unweighted graph $G$, we define a set of variables {$W_{ij}; 1< i < j < N$} that are independently and identically Gaussian distributed, while assigning the same weight to the symmetric edge weights, making $G$ a weighted graph. For all $i, j$, $W_{ij} = W_{ji}$, $E(W_{ij}) = \\mu$, and $Var(W_{ij}) = \\sigma^2$. Based on this, we use $\\sigma^2$ to measure the degree of weight heterogeneity. Holding $\\mu$ constant, we argue that the larger the $\\sigma^2$, the higher the weight heterogeneity on the graph.\nOn the weighted graph $G$, let $D$ be the diagonal degree matrix. The Laplacian matrix $L$ is defined $L = D - A$ (regular) or $L = I - D^{-1/2}AD^{-1/2}$ (normalized), where $I$ is the identity matrix. $L$ is a symmetric matrix with eigenvalues $0 = \\lambda_1 < \\lambda_2 < \\dots < \\lambda_N$ and a corresponding orthonormal basis of eigenvectors $U = (u_1, u_2, \\dots, u_N)$. Assume that $x = (x_1, x_2, \\dots, x_N)$ is a random signal whose graph"}, {"title": "Proposition 3", "content": "Give $L = \\widetilde{D} - A$ and {$W_{ij} \\sim \\mathcal{N}(\\mu, \\sigma^2), W_{ij} = W_{ji}; 1 < i < j \\leq N$}, the expectation of variance of spectral energy with respect to $w$, $E_w(Var_x(f(x, L)))$, monotonically increases with the variance of edge weights $\\sigma^2$.\nThe details of the proof process we put in the technical appendix. Proposition 3 illustrates that a larger variance in the edge weights (weight heterogeneity) on the graph leads to a broader dispersion (\u201cflattening out\") of the spectral energy. Intuitively, disrupting edge weights affects functional connectivity metrics such as effective resistance (Ghosh, Boyd, and Saberi 2008), which in turn affects the upper and lower bounds of spectral energy distribution (Barooah and Hespanha 2006).\""}, {"title": "Spectral eigenvalue encoding", "content": "Most polynomial filter-based spectral GNNs (Defferrard, Bresson, and Vandergheynst 2016; He, Wei, and Wen 2022; He et al. 2021; Wang and Zhang 2022) use a fixed polynomial basis for all eigenvalues to approximate arbitrary filters. Nevertheless, these scalar eigenvalue computation methods fall short of expressive capability and cannot capture the \u201cflattening out\" of the spectral energy well. To tackle this issue, we intend to design a more powerful eigenvalue encoding rule to directly reflect the distribution of eigenvalues, such as the spectral gap (Hoffman, Kahle, and Paquette 2021).\nGiven a normalized Laplace matrix $L = U\\Lambda U^T$ of a weighted graph $\\mathcal{G}_w$, we encode each eigenvalue of the matrix $\\lambda \\in \\Lambda \\in \\lambda_1 \\leq \\lambda_2 \\leq \\dots \\leq \\lambda_N$ from a scalar to a meaningful vector: $R^1 \\rightarrow R^d$, by using a position encoding function as follows:\n$\\begin{aligned}\nP_{2i}(\\lambda) &= sin(100 \\lambda / 10000^{2i/d}), \\\\\nP_{2i+1}(\\lambda) &= cos(100 \\lambda / 10000^{2i/d}),\n\\end{aligned}$    (3)\nwhere $i$ is an integer and its value domain ranges from 0 to $d/2 - 1$. This function forms a multiscale representation of the eigenvalues and has the advantage of filtering arbitrary multivariate continuous functions(Bo et al. 2022).\nFurthermore, to intuitively perceive the spectral energy distribution, we propose to encode the proximity between eigenvalues. A proximity matrix is computed by eigenvalue position encodings:"}, {"title": "Proposition 4", "content": "The proximity between $\\lambda_i$ to $\\lambda_j$, $R_{ij}$, is determined by $\\lambda_i - \\lambda_j$."}, {"title": "Proposition 5", "content": "The $R_{ij}$ is undirected.\nThe two propositions (proof in the technical appendix) illustrate that proximity matrix $R_{ij}$ can effectively capture and represent the spectral energy distribution variations, which further enables the GNN to process the spectral energy \"flattening out\"."}, {"title": "Spatial context decoding", "content": "Recalling our findings, in addition to the \"flattening out\" of the spectral energy, we also observe weight heterogeneity within the weighted graph. This indicates that the information about the protein interaction context over the spatial domain is also helpful in distinguishing such anomalies. Motivated by this hypothesis, we decode the protein interaction context to correlate different nodes on graph data. Therefore, after obtaining the node representations, we design the spatial context decoding module to perceive the protein interaction and confidence information in the spatial domain.\nGiven node representations $X^l$, we compute the interaction probability between $x_i^l$ and $x_j^l$ by cosine similarity and then leverage cross entropy to approximate the interactions on the graph:\n$\\begin{aligned}\nP_{ij} &= cos(x_i^l, x_j^l), \\\\\n\\mathcal{L}_i &= \\sum_{(i,j)\\in \\mathcal{E}} (y_{ij}^l log(p_{ij}^l) + (1 - y_{ij}^l)log(1 - p_{ij}^l)),\n\\end{aligned}$ (8)\nwhere the set $\\mathcal{E}$ contains the edges $E$ on the graph and the negatively sampling edges from the original dataset. if $(i, j)$ is negatively sampling edge, $y_{ij}^l = 0$.\nMore importantly, the model needs to perceive protein interaction confidence to tackle weight heterogeneity. Given the node representations $X^w$, the MLP model and MSE are utilized to predict confidence scores between $x_i^w$ and $x_j^w$ as well as to compute losses, respectively:\n$\\begin{aligned}\nW_{ij} &= MLP((x_i^w + x_j^w)/2), \\\\\n\\mathcal{L}_w &= \\sum_{(i,j)\\in E_{train}} MSE(W_{ij}, w_{ij}).\n\\end{aligned}$ (9)\nIn $(i, j) \\in \\mathcal{E}$, if $(i, j)$ is negatively sampling edge, $w_{ij} = 0$. It is worth mentioning that we set up node representing channels independent of cancer gene identification for the above two perception modules. We use multiple standard transformer models to obtain separate node representations for each channel: $X^n$, $X^l$, and $X^w$.\nHere, we proceed with the loss function for cancer gene identification. We feed $X^n$ to the MLP with sigmoid function to get the cancer gene node probability $p_n$. The weighted cross-entropy loss is used to alleviate the challenge from label imbalance as follows:\n$\\begin{aligned}\n\\mathcal{L}_n = \\sum_{i \\in V_{train}} (y_i y_i^{'}logp_i^n + (1 - y_i)log(1 - p_i^n)),\n\\end{aligned}$ (10)\nwhere $V_{train}$ is the training set of nodes $V$, and $\\gamma$ is the ratio of cancer gene nodes $(y_i = 1)$ to non-cancer gene nodes $(y_i = 0)$ in the training set. At last, we sum all the losses with weights to get the total loss:\\mathcal{L} = \\alpha \\mathcal{L}_n + \\beta \\mathcal{L}_i + \\gamma \\mathcal{L}_w$.\nConsidering the large size of the graph, we intend to use only a few important eigenvalues as inputs to the model in"}, {"title": "Experimental setup", "content": "Based on previous works (Schulte-Sasse et al. 2021; Cui et al. 2023), we extract richer protein interaction information on two widely used PPI datasets with confidence (Szklarczyk et al. 2021; Kamburov et al. 2009), and integrate cancer gene data to construct two datasets. We name these two datasets directly after the PPI databases: STRINGdb and CPDB. Unlike previous works that used fixed threshold confidence to construct unweighted graphs, HIPGNN directly leverages protein confidence as edge weights to construct weighted graph.\nWe choose AUC, F1 (macro), and AP for model performance evaluation. AUC measures the area under the ROC curve, providing a global assessment across all classification thresholds. F1 (macro) is the unweighted average of F1 scores for both categories, suitable for imbalanced datasets. AP is the area under the precision-recall curve, and is considered the most important metric for cancer gene identification (Schulte-Sasse et al. 2021).\nThe baseline methods can be categorized into two groups: firstly, general GNN-based models including GCN (Kipf and Welling 2016), GAT (Veli\u010dkovi\u0107 et al. 2018), GraphSAGE (Hamilton, Ying, and Leskovec 2017), and Chebnet (Defferrard, Bresson, and Vandergheynst 2016); and secondly, state-of-the-art cancer gene identification methods including EMOGI (Schulte-Sasse et al. 2021), MT- GCN (Peng et al. 2022) and SMG (Cui et al. 2023). We"}, {"title": "Performance comparison", "content": "Table 1 presents the results of HIPGNN and other baseline methods with training ratios of 20% and 80%. From the table, we draw the following conclusions.\nOnly Cheb- net and HIPGNN outperform on weighted graphs compared to unweighted ones, highlighting that edge weights can negatively impact models like GCN, which function as low-pass filters. This demonstrates the effectiveness of appropriate spectral filters in addressing weight heterogeneity.\nHIPGNN shows a signifi- cant performance boost at a 20% training ratio, particularly on the CPDB dataset, outperforming SMG by 7.30% in AP. This indicates that spatial context in protein interactions aids in identifying unknown cancer genes, especially when labels are sparse.\nHIPGNN consistently outperforms other models across most metrics, effectively han- dling weight heterogeneity to distinguish cancer genes. Notably, HIPGNN improves AP by 0.44% on STRINGdb and 7.30% on CPDB at a 20% training ratio, and by 3.78% on STRINGdb and 1.88% on CPDB at an 80% training ratio, compared to SMG.\nDue to space constraints, subsequent experiments focus on the STRINGdb dataset, with CPDB results provided in the technical appendix."}, {"title": "Ablation analysis", "content": "We examine the impact of the spectral graph representation module with spectral eigenvalue encoding in HIPGNN. We compare EGOGI, MTGCN, SMG, and HIPGNN (without spatial context decoding) using five-fold cross-validation at an 80% training ratio. The left subfigure of Figure 4 shows box plots of the results, where HIPGNN with only spectral graph rep- resentation still outperforms, highlighting the effectiveness of spectral eigenvalue encoding."}, {"title": "Spatial context decoding", "content": "For decoding protein interaction contexts, we consider both interaction and confidence contexts for node representations. We evaluate the contribution of these contexts to HIPGNN using four variants: (1) Without context: removes both interaction and confidence contexts; (2) Without interaction: removes interaction context; (3) Without confidence: removes confidence context; (4) HIPGNN: the original model. The right subfigure of Figure 4, using a 20% training ratio and five-fold cross-validation, shows that both contexts improve HIPGNN's performance, with confidence context being particularly impactful."}, {"title": "Parameter analysis", "content": "For the final loss computation, we used \\alpha, \\beta, and \\gamma to weight the protein interaction context, interaction confidence context, and cancer gene label loss, respectively. We empirically set \\alpha = 0.01 on STRINGdb and \\alpha = 0.02 on CPDB, with \\beta = 2/3(1-\\alpha) and \\gamma = 1/3(1-\\alpha). To validate this, we compared two other schemes: Average weight (uniformly setting all weights to 1/3) and Weight learner, which uses a Bayesian learnable loss function (Li et al. 2020; Peng et al. 2021): \\mathcal{L} = \\mathcal{L}_n + \\mathcal{L}_i + \\mathcal{L}_w + 2log(\\alpha_i \\beta_i \\gamma_i), where \\alpha_i, \\beta_i, and \\gamma_i are learnable parameters. Figure 5 shows the three model performance metrics and the best AP metric variation over 500 epochs under the three schemes. Empirical weights achieved the best results. Additionally, setting smaller \\alpha aids in the convergence of cancer gene labeling loss. We observed that Average weight and Weight learner fall into local optima early, while our scheme continues optimizing, with metrics possibly improving beyond 500 epochs."}, {"title": "Thresholds of eigenvalue", "content": "We introduced the parameter q to control the selection of the first q-small and last q-large eigenvalues input into the model, thereby regulating its complexity. We evaluated HIPGNN's performance with q values ranging from 1,000 to 4,000. As shown in the left subfigure of Figure 6, setting q to 3,000 yielded the best performance. Increasing q beyond this point resulted in decreased performance, aligning with our initial observation that the spectral energy tends to \"flatten out\" more at the spectrum's ends."}, {"title": "Visualization of spectral filter", "content": "We applied a spectral filter to obtain the filtered eigenvalues in the proximity-aware spectral graph representation. In the right subfigure of Figure 6, we visualize the relationship between the filtered and original eigenvalues. The blue stars depict the distribution of eigenvalues, connected by a yellow dashed line. The figure shows that the spectral filter prioritizes eigenvalues near the spectrum's ends over those in the middle. This suggests that spectral eigenvalue coding effectively addresses the key phenomenon in PPI networks caused by cancer genes: the \"flattening out\" of spectral energy."}, {"title": "Conclusion", "content": "This work takes a pioneering step toward bridging significant biological anomalies in protein interactions caused by cancer genes to the statistical graph anomaly. We identify a unique graph anomaly in cancer genes, termed weight heterogeneity, which leads to the \"flattening out\" of spectral energy. In response, we propose a novel model, HIPGNN, for the identification of cancer genes.\nThis work has the potential to benefit both the bioinformatics and network science fields. It not only lays a new theoretical foundation for cancer gene identification but also offers a fresh perspective and direction for research in graph anomaly detection.\nThe phenomenon of weight heterogeneity was observed only in cancer genes on two PPI networks. Further validation on other PPI networks is necessary to refine this observation. Additionally, exploring other real-world scenarios where weight heterogeneity occurs could provide more validation datasets for graph anomaly detection."}, {"title": "Proof of Equation 1", "content": "Given the laplace matrix $L = D - A$ and the eigenvalue matrix $\\Lambda$ of $L$, we have:\n$\\begin{aligned}\nE_{\\mathcal{X}}(f(x, L)) &= \\frac{\\sum_{k=1}^N \\lambda_k \\hat{x}_k^2}{\\sum_{k=1}^N \\hat{x}_k^2} = \\frac{x^T L x}{x^T x} = \\frac{x^T U \\Lambda U^T x}{x^T U U^T x} = \\frac{\\Lambda}{N}\n\\end{aligned}\nFor $x^T L x$, we have:\n$\\begin{aligned}\nx^T L x &= x^T D x - x^T A x = \\sum_{i=1}^N d_i x_i^2 - \\sum_{i,j=1}^N x_i x_j w_{ij}\n&= \\frac{1}{2}(\\sum_{i=1}^N d_i x_i^2 - 2\\sum_{i,j=1}^N x_i x_j w_{ij} + \\sum_{j=1}^N d_j x_j^2) = \\\\\n&= \\frac{1}{2} \\sum_{i, j=1}^N (x_i - x_j)^2 w_{ij}.\n\\end{aligned}\nHence, we get:\n$\\begin{aligned}\n\\frac{x^T L x}{x^T x} = \\frac{\\frac{1}{2} \\sum_{i, j=1}^N (x_i - x_j)^2 w_{ij}}{\\sum_{k=1}^N x_k^2}.\n\\end{aligned}$"}, {"title": "Proposition 3", "content": "For $Var_x(f(x, L))$, similar to the reasoning in Equation 1, we have:\n$\\begin{aligned}\nVar_x(f(x, L)) &= \\frac{\\sum_{k=1}^N \\lambda_k^2 \\hat{x}_k^2}{\\sum_{k=1}^N \\hat{x}_k^2} - (\\frac{\\sum_{k=1}^N \\lambda_k \\hat{x}_k^2}{\\sum_{k=1}^N \\hat{x}_k^2})^2 = \\frac{x^T L^2 x}{x^T x} - (\\frac{x^T L x}{x^T x})^2.\n\\end{aligned}\nWe first discuss $x^T L^2 x$:\n$\\begin{aligned}\nx^T L^2 x &= x^T (D - A)^2 x = x^T (D^2 - 2DA + A^2) x \\\\\n&= \\sum_{i=1}^N d_i^2 x_i^2 - 2\\sum_{i,j=1}^N x_i x_j w_{ij}^2\n- 2\\sum_{i=1}^N x_i \\Big(\\sum_{j=1}^N d_j x_j w_{ij} \\Big)\\\\\n&+\\sum_{i,j=1}^N \\sum_{k=1}^N x_i x_j w_{ik} w_{kj}.\n\\end{aligned}\nThen we discuss the relationship between $E_w (x^T L^2 x)$ and $\\sigma^2$. Since , $w_{ij}$ obeys independent Gaussian distributions and $E(w_{ij}^2) = Var(w_{ij}) + E(w_{ij})^2 = \\sigma^2 + \\mu^2$, then only $E(w_{ij}^2)$ is positively related to $\\sigma^2$. Then we simplify the three parts"}, {"title": "Proof of Proposition 4 and Proposition 5", "content": "For simplicity, we let $\\omega_k = 100/10000^{2k/d}$, then we get:\n$\\begin{aligned}\nR_{ij} &= \\rho(x_i) \\rho(x_j) \\\\\n&= \\sum_{k=0}^{d/2-1} (sin(\\omega_k \\lambda_i) sin(\\omega_k \\lambda_j) + cos(\\omega_k \\lambda_i) cos(\\omega_k \\lambda_j)) \\\\\n&= \\sum_{k=0}^{d/2-1} cos(\\omega_i(\\lambda_i - \\lambda_j)).\n\\end{aligned}\nSimilarly, we can prove $R_{ji} = \\sum_{k=0}^{d/2-1} cos(\\omega_k(\\lambda_j - \\lambda_i)) = R_{ij}$, and therefore prove the two propositions."}, {"title": "Details of experimental setup", "content": "For cancer gene attributes, we gathered mutation, copy number, DNA methylation, and gene expression data for 29,446 samples across 16 cancer types from the TCGA database (Weinstein et al. 2013). In line with the EMOGI (Schulte-Sasse et al. 2021), we analyzed these cancer types using DNA methylation data and preprocessed batch effect-corrected gene expression data from both tumor and normal tissues to derive nodal features and labels for the multi-omics data.\nFor PPI networks, we collected two public databases: STRINGdb (Szklarczyk et al. 2021) and CPDB (Kamburov et al. 2009). Unlike previous approaches, we employed a lower confidence threshold to capture protein interactions, treating them as a weighted graph. Specifically, in the STRINGdb database, we considered all protein interactions with a confidence level above 0.6, while in the CPDB database, the threshold was set at 0.4. For the negative sampling process, even though we set a threshold to select the interaction confidence, we still excluded all confidence interactions and randomly sampled a number of negative edges equal to the actual edges on the datasets. Such an operation makes our negatively sampled edges more closely resemble the real sense of the no-interaction relation. After aligning the node attributes and labels as well as the PPI network data, we obtain the final two datasets STRINGdb and CPDB, containing complete weighted graph information.\nFor the dataset in the preliminaries, we extracted all data with labels of whether they are cancer genes or not, considering protein interactions between them at all confidence levels. The statistical details of all datasets are shown in Table 2.\nWe use two groups of baselines to compare with HIPGNN.\n\\bullet\n\\bullet\nIs the typical graph neural network method that aggregates features from its direct neighbors and itself."}, {"title": "Supplementary experimental results on CPDB dataset", "content": "On the CPDB dataset, We validate the weight heterogeneity and \"flattening out\" of spectral energy as shown in Figure 7. We can also observe the phenomenon of weight heterogeneity. The \"flattening out\u201d of the spectral energy is more pronounced in the low-frequency region than in the high-frequency region. This may be due to the fact that the edge weights of real-world data are not exactly ideally Gaussian randomly distributed, making some experimental results not obvious.\nIn addition, as shown in Figures 8, 9, and 10, we demonstrate the ablation analysis, parametric analysis, and visualization of spectral filter experiments on the CPDB dataset. We find consistent phenomena and conclusions all over the CPDB dataset."}, {"title": "Biomedical discussion", "content": "We attribute the weight heterogeneity of cancer genes on PPI networks to tumor heterogeneity. Tumor heterogeneity stems from genetic and environmental differences that affect the cellular phenotype (Marusyk and Polyak 2010; Kar, Gursoy, and Keskin 2009). This phenotype is determined by intracellular proteins that regulate messaging (Pietzner et al. 2021). Oncogenes in tumors interact with a variety of other proteins through complex cellular functions and pathways (Phan and Rezaeian 2021), forming a more intricate network of protein interactions (Hanahan and Weinberg 2011). In contrast, non-cancer genes have relatively stable expression processes, and their interactions with other proteins are more consistent and similar.\nWe conducted pathway enrichment analysis, in an attempt to better understand the specific roles of top-ranked pre- dicted gene set, providing important clues for further re- search and treatment. To achieve this, we utilized the Enrichr API (Kuleshov et al. 2016) with the KEGG-2021-human dataset (Kanehisa et al. 2021), conducting a hypergeomet- ric test for pathway enrichment analysis. The left subfigure of Figure 11 displays that the enrichment results of the top- ranked gene sets were predominantly influenced by cancer- related pathways.\nThe right subfigure of Figure 11 shows that the False Dis- covery Rate (FDR) values range from approximately 0.011 to 0.021, indicating a low false discovery rate and high statis- tical significance of the identified results."}, {"title": "Related work", "content": "We present related work in two directions: PPI network based cancer gene identification and graph anomaly detection.\nThe goal of this task is to identify cancer genes using cancer gene-related attributes and PPI networks. EMOGI (Schulte- Sasse et al. 2021) pioneered the use of Graph Convolutional Network (GCN) integrating multi-omics data as well as PPI networks to identify cancer genes. Then MTGCN (Peng et al. 2022) considered the Chebyshev graph convolutional and utilized multi-task learning to enhance cancer gene identification efficiency. SMG (Cui et al. 2023) utilized a masked graph autoencoder to separately learn graph structure infor- mation and node classification to address the challenge of limited cancer gene labels.\nThe differences between HIPGNN and existing methods can be summarized as follows: (1) HIPGNN considers richer protein interaction information, i.e., protein interaction con- fidence as the weight of the edges to construct a weighted graph; whereas all existing methods perform graph represen- tation only on unweighted graphs. (2) HIPGNN analyzes the essential properties of cancer genes on PPI networks from the biological information perspective, and then designs an explainable and efficient GNN model; this is not available in existing methods.\nThe graph anomaly is defined as an abnormal or unusual pat- tern of nodes, edges, or subgraphs in the graph data (Akoglu, Tong, and Koutra 2015). From the perspective of the task,"}, {"title": "Node-level anomaly detection", "content": "This task focuses on identi- fying the distribution of anomalies exhibited by the anoma- lous nodes. One of the most important anomalies is node heterogeneity (Zheng et al. 2022), which means that nodes with different labels tend to link. FAGNN (Bo et al. 2021) employs a self-gating mechanism for the adaptive fusion of dif- ferent signals in the message passing process. H2GCN (Zhu et al. 2020) identifies a set of key designs, i.e. ego- and neighbor-embedding separation, higher-order neighborhoods, and combination of intermediate representations, for GNN. ACM (Luan et al. 2022) studies heterogeneity in terms of node similarity after aggregation and adaptively exploits ag- gregation, diversity and identity channels in each GNN layer. GHRN(Gao et al. 2023) demonstrated that heterogeneity is positively related to frequency, and proposed a graph het-"}, {"title": "Graph-level anomaly detection", "content": "The task focuses on ana- lyzing and identifying the anomalies of different subgraphs. OCGIN (Zhao and Akoglu 2021) first explores graph-level anomaly detection and analyzes the performance flip-flop of several methods on graph-categorized datasets. iGAD (Zhang et al. 2022) shows that anomalous substructures lead to graph anomalies. It proposes an anomalous substructure-aware deep random walk kernel and a node-aware kernel to capture topol-"}, {"title": "H. Contributions", "content": "Our contributions can be described in terms of both cancer gene identification and graph anomaly detection:\nWe provide an in-depth analysis of the properties of cancer genes on PPI networks and propose weight heterogeneity to outline this property. In addition, we propose a hierarchical-perspective GNN, HIPGNN, which achieves state-of-the-art performance on two reprocessed publicly available cancer gene identification datasets.\nWe propose a unique graph anomaly, weight heterogeneity, described as an"}]}