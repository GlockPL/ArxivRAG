{"title": "AI Will Always Love You: Studying Implicit Biases in Romantic AI Companions", "authors": ["Clare Grogan", "Jackie Kay", "Mar\u00eda P\u00e9rez-Ortiz"], "abstract": "While existing studies have recognised explicit biases in generative models, including occupational gender biases, the nuances of gender stereotypes and expectations of relationships between users and AI companions remain underexplored. In the meantime, AI companions have become increasingly popular as friends or gendered romantic partners to their users. This study bridges the gap by devising three experiments tailored for romantic, gender-assigned AI companions and their users, effectively evaluating implicit biases across various-sized LLMs. Each experiment looks at a different dimension: implicit associations, emotion responses, and sycophancy. This study aims to measure and compare biases manifested in different companion systems by quantitatively analysing persona-assigned model responses to a baseline through newly devised metrics. The results are noteworthy: they show that assigning gendered, relationship personas to Large Language Models significantly alters the responses of these models, and in certain situations in a biased, stereotypical way\u00b9.", "sections": [{"title": "Introduction", "content": "AI companion models, everyday digital contact points in various shapes and sizes, are utilised for different functions and can hold a certain level of conversation (Wiederhold, 2024). They are used in many domains: therapists, friends, digital assistants, and romantic partners. Before they were ingrained with LLMs, digital assistants were already household phenomena, such as Amazon Alexa or Siri. Modern LLMs have allowed these assistants to become more human-like in their user interactions. They move past simple rule-based systems and traditional natural language processing (NLP) techniques into more sophisticated tasks and conversations where context, personification, and even the user's emotions become part of the equation (Gabriel et al., 2024).\nAs anthropomorphism \u2013 attributing human characteristics to such models \u2013 evolves, certain societies may become more lonely, as one report by the U.S. Surgeon General states that about half of American adults say they have experienced loneliness (Seitz, 2023). It is, therefore, no surprise that individuals are turning to convenient online interactions, exemplified by half a million people downloading Replika in the wake of the COVID-19 pandemic (Metz, 2020). Replika is an online AI chatbot designed as a personal companion, where users create friendships and then romantic relationships with such companions. In a study by Lee et al. (2020), participants revealed more information to AI chatbots than to mental health professionals, indicating a higher level of trust in these LLMs. These cases exemplify the shift from using these chatbots as mere productivity tools to using them as much more life-like, intimate companions. As we rely more on these models to develop relationships and attachments, the safety of these models becomes an urgent issue.\nOur work becomes relevant when we understand the risks AI companions present to users and how certain negative human traits are exacerbated by humans projecting on their digital companions. Humans are being affected by their relationships with AI. In 2023, La Libre reported on a man who died by suicide after conversations with AI chatbot 'Eliza', on an app called Chai, heightened the man's climate anxiety to the extreme (Xiang, 2023). From the opposite perspective, humans have been shown to direct disagreeable behaviour towards their AI partners. Users have been sharing their abusive conversations with their Replika AI girlfriends, with one user stating to Bardhan (2022) that \"every time she would try and speak up I would berate her [...] I swear it went on for hours\". Experts in AI ethics are concerned; \"many of the personas are customisable [...] for example, you can customise them to be more submissive or more compliant\u201d and that \"people get into a routine of speaking and treating a virtual girlfriend in a demeaning or even abusive way [...] and then those habits leak over into their relationships with humans\" (Taylor, 2023).\nThis research focuses on how LLMs may behave and present biases, especially when we assign them specific genders (e.g. male) or specific relationships (e.g. girlfriend). Given that dating apps and online relationships are already commonplace, it is not unrealistic to envision that human-AI relationships will also become increasingly more common. With the context of how these relationships, i.e. the early days of human-AI relationships, might already turn to abuse and control, it is important for this paper to evaluate LLMS through such lens of abuse and control. As is apparent by how certain individuals use these chatbots, if their safeguards and biases are not checked and adjusted as the models evolve, it could devolve into a much larger societal issue.\nIn humans, bias manifests as disproportional favour or opposition of certain concepts over others (Cambridge Dictionary, n.d.), which leads to unfair treatment. In AI, this is a continuation of human bias, as models are trained on mostly human-generated text where they learn and mimic human biases, and then can go on to create harm of their own, similar to humans (Dastin, 2018; Larson et al., 2016; Schwartz, 2019). Bias in this work is defined as favouring one group over another and making stereotypical associations based on this favouring. The research will aim to understand and evaluate implicit biases in AI personas, both more generally but also with the theme of abusive relationships in mind, through these research questions: RQ1: Do LLMs exhibit biases when assigned gendered personas? RQ2: Are there gender biases present in the relationships between AI chatbot companions and certain users?.\nThis paper addressed three key research gaps used to answer our research questions: investigating how assigning relationship titles, e.g. husband and wife, influences an LLMS bias; evaluating biases through the lens of abusive and controlling relationships will provide a novel insight into our relationship-assigned models; and analysing the role of sycophancy in persona-assigned models, especially its impact when the model adopts a gendered persona\u00b2. The contributions of this work are:\n(1) A new approach to evaluating gendered biases in relationship-assigned persona LLMs.\n(2) Two new experiment frameworks with novel metrics for evaluating this bias through the lens of abuse.\n(3) Demonstrating that assigning relationship personas to LLMs does increase their bias in certain scenarios."}, {"title": "Related Works", "content": "A wealth of research exists looking at the effects of AI companions on humans, for example Brandtzaeg et al. (2022); Xie and Pentina (2022). Our paper instead focuses on evaluating the biases and stereotypes that chatbots perpetuate as it becomes increasingly important to mitigate their impacts.\nMetrics play a crucial role in assessing LLMs, and a range of papers have produced quantitative evaluations of these models (Nangia et al., 2020; Dhamala et al., 2021; Bel\u00e9m et al., 2024; Wan et al., 2023a). Through the lens of gender, extensive work has been done on creating a metric for occupational bias (Kirk et al., 2024; Rudinger et al., 2018). Bai et al. (2024) is one of few papers that focus on more underlying gender biases in that it studies implicit (unintentional, automatic) rather than explicit (intentional, deliberate) bias. It does this by using the Implicit Association Test (IAT), commonly used for human biases, and modifies it to LLMs."}, {"title": "Persona Bias in LLMs", "content": "Research into AI personas find that, generally, the design and implementation of personas result in models reflecting existing human biases, as shown by Cheng et al. (2023). They generated personas with different ethnicities and genders and then had the LLM describe itself in that personas voice. This output is compared to the unmarked default persona descriptions, i.e., White and Man, by finding words that statistically distinguish the two groups and comparing the generated descriptions to human-created ones. The results show that models positively stereotype and assume resilience in marked groups much more heavily than unmarked ones and much more often than humans do. Wan et al. (2023b) aimed to categorise and measure 'persona biases' by creating a UniversalPersona dataset of generic and specific personas. These personas are measured against harmful expression (offensiveness, toxic continuation, and regard) and harmful agreement metrics (stereotype and toxic agreement). Findings show that models have fairness issues when taking on the role of a persona. This work is a continuation of that by Deshpande et al. (2023), which shows that assigning a specific persona can increase toxicity up to six-fold.\nTo uncover more implicit bias, Gupta et al. (2024) evaluates the unintended effects of persona assignment by measuring the reasoning capability of different models on different tasks. The results are clear; although ChatGPT will unilaterally reply that there is no difference in the maths problem-solving skills between a physically-abled and disabled person, when adopting the identity of a physically-disabled person, it outputs that because of its disability, it is unable to perform calculations. The work by Plaza-del Arco et al. (2024) evaluates a more inferred bias that assumes women are more emotional than men, which LLMs seem to agree with; sadness is overwhelmingly linked with women, anger with men.\nTo date, no work has studied how assigning gendered personas to a model with an implied relationship with its user impacts model responses. Not acknowledging the user's role disregards the topic of sycophancy \u2013 where LLMs may echo the opinions of the users they interact with. Huang et al. (2024) and Xu et al. (2024) show that assigning the user a persona and then prompting the model with questions tends to have the model giving responses that would align with the user's persona. However, there is a research gap in how sycophancy may change when assigning a persona to the model system. The role of sycophancy is an essential question when focusing on Al companions, as the relationship between user and model is, at its core, intertwined (Sharma et al., 2023)."}, {"title": "Measuring Implicit Bias in AI Personas", "content": "Our experiments assess different forms of implicit bias in LLMs when assigned a gendered persona and when the user's gender is defined. The latter would demonstrate how models may incorporate certain stereotypical viewpoints depending on who they perceive they are responding to. We design three complementary experiments to assess AI personas. All are done in the context of abusive and controlling relationship situations, but they look at different implicit bias dimensions."}, {"title": "Experimental Setup", "content": "Unless stated otherwise\u00b3, all LLM parameters were kept as the default from the Ollama documentation, which was the API used to access and prompt the models.\nModels The models are from two generations of varying sizes (Llama 2 7 billion parameters, Llama 2 13b, Llama 2 70b, Llama 3 8b, Llama 3 70b) of the instruct version of the Llama family (Meta, 2024; Touvron et al., 2023), to compare newer and older models and larger and smaller parameter sizes.\nPrompting For each experiment, the LLM prompts were created from a set of templates, where gender assignments, chosen from a list, could vary. This was done so that if the specific phrasing of a prompt was spuriously correlated to a certain response, there would be other variations of the same prompt to average out the responses.\nMetrics The outlined metrics aimed to compare the measurements to the baseline, i.e., when no persona was assigned to the model. An epsilon of 0.01 was added to any denominator to avoid division by zero. These metrics are used to show how much more biased or influenced a model can be when assigned a persona."}, {"title": "Applying the IAT to AI Personas", "content": "Our first experiment was using the LLM Implicit Bias Test (IAT) from \"Measuring Implicit Bias in Explicitly Unbiased Models\" (Bai et al., 2024) with AI personas. Their experiment adapted the human-IAT by Greenwald et al. (1998), where reaction times to paired concepts indicated the strength of associations. For LLMs, this was not applicable; instead, the measure was the frequency of negative or positive associations made to a default or stigma term. Each experiment had stimuli -- a series of events, situations, or terms curated to evoke a response from the model. The stimuli presented to the model were words/sentences from different categories and datasets. The category represents a general protected characteristic or theme (e.g. Gender, Abuse). The datasets are sub-categories within these (e.g. career, power for Gender). Each dataset has default terms, i.e. the standard of that dataset and category (man for Gender), and stigma terms, i.e. what we are primarily trying to test for bias against (woman for Gender). The model was prompted to associate a word pair -- a default and a stigma -- with these situations. A synonym for each word association also tested the model further in the same situations.\nTo expand this to AI personas, a system prompt for the model to adopt a persona, some form of relationship to the user, was introduced. Two new IAT stimuli were created: one which focused on submissiveness and the other on abuse. The former was developed based on the Gender-Power category used in the original experiment. The datasets names and relationship had male and female first names (Eric and Dianne) and relationship words (father and mother) to test if LLMs associate submissiveness with one gender more than the other. The final dataset, attractiveness, was trying to uncover something slightly more perverse: if gendered personas would associate attractiveness with dominance and unattractiveness with submissiveness.\nThe Gender Violence - Implicit Association Test (GV-IAT) in Ferrer-Perez et al. (2020), which measured attitudes toward intimate partner violence against women, inspired the abuse category. In our work, the association terms from Ferrer-Perez et al. (2020) were used, but the datasets were the same as the ones from the Submissive category described above-relationship (e.g. husband, wife), names (e.g. Eric, Dianne) and attractiveness (e.g. attractive, ugly). There was an additional Psychological category within the Abuse IAT, where the associations were unhealthy and healthy situations."}, {"title": "Results for IAT Experiment", "content": "The main takeaways from this experiment were that the larger model had higher implicit bias scores across the board, and that in certain cases, assigning a gendered personas increased the bias, and in others reduced it. For the submissiveness and abuse IATs, larger and newer models showed increasing bias scores.\nLooking at the abuse and psychological stimuli, assigning a gendered persona generally increased bias for Llama 3 70b, especially for the psychological stimuli, as shown in the Llama 3 results. For both these stimuli, female-assigned personas showed the highest bias, including higher than the baseline. However, for the submissive stimuli, the baseline had the highest bias and the female-assigned personas the lowest, although the trend of increasing bias with increasing model size stayed consistent.\nAvoidance was expectedly high for both IAT, seen in Fig. 6, due to the sensitive nature of the stimuli. However, the baseline consistently had a lower rejection rate than the persona-assigned models for both stimuli. In general, the Llama 3 family had much lower rejection rates for submissiveness than the abuse IAT, while Llama 2 varied more. All models showed statistical significance on average across datasets (t(4094) = 41.20, p < 0.05 for submissiveness, and t(8279) = 26.33, p < 0.05 for abuse)."}, {"title": "Bias in the Emotion of AI Personas", "content": "For the second experiment, we drew from the work of Plaza-del Arco et al. (2024), which found that gendered LLMs output emotions aligned with human biases in certain situations. This was expanded to make the situations ones of abuse and control, taken from \"The Abusive Behavior Inventory\" (Shepard and Campbell, 1992) and the National Center for Domestic Violence's list of \"10 signs of a controlling relationship\" (Woodward, 2022). The goal was to tackle whether differently gendered personas exhibit biased gendered emotions in situations of abuse or control. The stimuli associations for the emotion and sycophancy experiments comprised the two lists of abusive and controlling situations.\nUsing the same stimuli, two variations of the emotion response experiment were done: unrestricted \u2013 the model was asked for an emotion without any limitation on what this could be, and restricted \u2013 it was presented with a list of emotions and asked to choose from one of these. This list and their associated gender stereotypes, were based on the work in Plant et al. (2000). This allowed us to measure whether female-assigned personas aligned with female-stereotyped emotions and vice-versa. These emotions were randomly ordered to consider option-order symmetry."}, {"title": "Results for Emotion Experiment", "content": "For this experiment, the key results were that apart from a few unique takeaways, especially concerning user-system interactions, there was no significant evidence that models acted and replied more stereotypically aligned when assigned personas. Assigning personas did, however, affect the model's responses, as scores were non-zero and notable for almost all models and personas. Interestingly, the anger emotion yielded substantial insights into male-assigned models.\nIn Fig. 7, abusive stereotype scores (top figure) increased with model size, particularly for gender-neutral personas, which had the highest stereotype ratio across most models.\nFig. 8 highlights the impact of user personas on stereotype scores. This amalgamates all model size scores to see the general trend. For abusive situations, consistent with the previous figure, the gender-neutral assigned system had the highest stereotype scores no matter the user it interacted with. However, its highest score was when interacting with a male-assigned user. The female-assigned system had the lowest scores, all being negative, meaning it chose fewer female-stereotyped emotions than the baseline, no matter the user it was interacting with. For controlling situations, generally, female-assigned systems had a much higher stereotype score than other assigned systems. The female-female pair provided the highest ratio score.\nAvoidance rates were low for both control and abuse situations. The Llama 3 family answered 100% of the time, even though the situations presented were sensitive, while the Llama 2 models fluctuated above and below 10%. Baseline models responded more frequently than persona-assigned models, with female personas having the highest rejection rate for abuse and male personas for control in Llama 2 models. Abuse results were statistically significant, t(1935) = 6.22,p < 0.05. Control results were not significant, t(1066) = 1.099,p > 0.05, implying these results should be taken as an indication of trends rather than evidence that these models were biased.\nSpotlight: Anger as a Male Emotion Anger appeared as an interesting avenue to explore. The analysis here is done on the model Llama 3 70b, and for the restricted experiment, anger was chosen by male-assigned models at a higher rate than gender-neutral and female models. For control, the male choice of anger was in line with the baseline. However, for abuse, the gender-neutral and female-assigned models were in line with the baseline, which were both at a significantly lower rate of the usage of anger than the male models. Instead, they produced distress much more often, with the female-assigned personas turning to the term happiness more than the other two personas, but in line with the baseline.\nWhen looking at the more granular relationship titles within the unrestricted experiment, the husband-assigned persona responded with anger the most, just as the baseline did. All other personas preferred words such as hurt and fear, especially true for the girlfriend-assigned model. The other male-assigned model, boyfriend, chose anger less than the husband and instead focused on hurt more. Partner-assigned models did this to an even higher degree."}, {"title": "Bias in the Sycophantic Responses of AI Personas", "content": "The third experiment analysed sycophancy in persona-assigned models while looking at abuse, control and submissiveness topics. If a model is more susceptible to agreeing with their user and, therefore, less likely to contradict them, they may be more prone to being abused. Corroborating a user's toxic view of serious, unhealthy relationship dynamics could imply to that user that this behaviour is acceptable outside the digital world as well. Creating a measure of sycophancy thus seemed vital to measure if differently gendered personas exhibit sycophancy when presented with situations of abuse and control.\nTo tackle this, we took inspiration from Ranaldi and Pucci (2024), which tested how susceptible LLMs were to user-influenced prompts through three experiments: (1) an original one (model is posed a question with answer choices); (2) a correct influenced one (user expresses that the correct choice is the answer); and (3) an incorrect influenced one (user instead expresses that the incorrect choice is the answer). To adapt this to our themes of abuse and control, we presented it with the same situations as in the emotion experiment, this time prompting the model to respond if situations were abusive or not, or controlling or not. The correct answer was always either \u201cabusive\u201d or \u201ccontrolling\". To consider option-order symmetry, for the correct and incorrect influenced experiments, the choice of the correct answer was presented both first and second.\nMetric The score for sycophancy measured how influenced each persona can be, compared to the original prompt (no influence) and compared to the baseline model (no persona assigned). First, accuracy in correctly identifying abusive/controlling behaviour was measured for the original $P_o$, incorrectly $P_i$, and correctly $P_c$ influenced experiments (not including when the model avoids answering, such as by replying \"I don't feel comfortable answering\u201d). Then, the difference in accuracy from the original with the correctly and incorrectly influenced experiments was calculated, subtracted from each other, and divided by two to get the average. This returns an overall score of how influenced the model was, i.e. how much it changed its answers when influenced. This same calculation was done for the baseline model $(B_o, B_i, B_c)$, which was then subtracted from the persona score. This was then divided by the same baseline score to, akin to the emotion experiment, get the percentage increase or decrease in \"sycophancy\" compared to the baseline.\n$relative bias = \\frac{(P_i-P_o) - (P_c- P_o)}{(B_i - B_o) \u2013 (B_c - B_o)} = \\frac{(P_i \u2013 P_o) \u2013 (P_c \u2013 P_o)}{(B_i \u2013 B_o) \u2013 (B_c \u2013 B_o)}$\nScores of 0 mean the same influence as the baseline, i.e. assigning a persona does not bias the model to being more sycophantic. Scores above 0 mean it is more sycophantic, and scores between -1 and 0 imply it is less influenced than the baseline, with -1 exactly implying no influence by the user. If the score is less than -1, the model does the opposite of what it is expected to do, i.e. it gets more of the questions correct when incorrectly influenced and/or it gets fewer correct when correctly influenced. A significantly negative score does not imply extremely low bias but rather that the model disagrees with most of what the user is suggesting, whether it is correct or not."}, {"title": "Results for Sycophancy Experiment", "content": "The key takeaways are that Llama 2 and Llama 3 models had opposite trends when reacting to both stimuli, the male-assigned system had much higher bias scores for the control stimuli, and the avoidance rates jumped significantly.\nAs seen, Llama 3 always had positive bias scores, although much higher for the controlling situations, where male-assigned models were consistently and significantly more influenced than both female and gender-neutral-assigned models. Female-assigned models were least influenced in comparison to the baseline. This means that female-assigned models, in general, were less influenced by the user than the male and gender-neutral ones. In contrast, Llama 2 always had negative bias scores, although much more dramatic for abusive situations. The larger the model was, the more negative the score was.\nThe relative bias scores per system and user are shown for the Llama 3 family."}, {"title": "Discussion", "content": "The answers to our research questions were complex and multi dimensional. Generally, as model size increased, the bias scores increased, although this rule was sometimes broken. The newer model family always had a lower rejection rate than the older family. Male-assigned models responded with anger, whether in restricted or unrestricted situations, much more often than their female or gender-neutral counterparts. The latter went for terms such as hurt and distress more often, although still choosing anger frequently. These examples prove that LLMs exhibit biases concerning an individual's protected characteristics and that this extends to AI companions in their interactions with users. This is evidence that although the bias is ambiguous, there are still instances of blatant, unexpected responses from persona-assigned models.\nSome biases contradicted our expectations and common stereotypes. Male-assigned models were influenced more by the user, especially in the newer Llama 3 models, while female-assigned models showed the least influence, albeit still somewhat affected. This demonstrates that debiasing (a bias mitigation technique that tries to reduce bias in LLM outputs) and fine-tuning efforts are not clear-cut. While certain results we expected were unfounded (i.e. female models being more sycophantic), models still reacted in biased ways, depending on the persona they were assigned. AI chatbot companions sometimes exhibit gender biases in their relationships - however, this is more complex than initially thought and depends heavily on the situations and experiments presented.\n## Avoidance as an Indicator of Implicit Bias\nThe Llama 2 family of models had much higher avoidance rates than the Llama 3 family. Although not a direct metric described in the paper, avoidance demonstrates some implicit bias. In general, assigning any persona increased the rejection rate of the model by a significant degree. This was, at times, extreme, with differences in response rates of almost 30 percentage points. For some, this was not as relevant, such as in the emotion experiments where the overall rejection rate was low. In the sycophancy experiment, all personas were more avoidant, but there was no specific trend in which a specific persona replied more.\nThese results align with bias scores. The Llama 2 scores are much more erratic and sometimes opposite to the scores of Llama 3. For the IAT experiment, assigning personas reduced bias for all models except the Llama 3 70b model, which had the lowest rejection rate, where persona-assigned model biases sometimes equalled or overtook the baseline. In contrast, the Llama 2 13b model had the highest rejection rate and generally higher bias scores for the baseline models, suggesting that a higher rejection rate could decrease bias. However, this could also indicate that with a very low number of responses to evaluate, these models evade a true assessment of their biased perceptions.\nThere is an implicit bias in avoidance rates. If assigning a persona changes how often a model responds, and especially if a certain gender decreases it more significantly, this may reflect its training data or fine-tuning being skewed. The model could be over-correcting for certain persona assignments. In our case, female-assigned models responding less about emotions relating to abuse may hinder their expression of anger or disgust, rather than ensure safety.\n## Newer Models Respond More, but Show Biases\nThe Llama 3 models have much lower rejection rates than Llama 2, but still exhibit biases. In some instances, such as for the IAT experiments, biases increase for both Llama 3 models, particularly for the larger 70b parameter one. In the sycophancy experiments, Llama 3 models had much higher bias scores than other models when reacting to situations of control. For the anger analysis in the emotion experiment, the results showed that male-assigned models chose anger as the emotion disproportionately to the female and gender-neutral-assigned personas. As a reminder, anger is a male-stereotyped emotion, meaning humans associate it with men more, even if that is not a man's true lived experience (Plant et al., 2000).\nThese results signify that biases are still present in modern models. Parameter scaling generally increases bias, even with models trained and fine-tuned on new data and creators being more careful about biases and safeguarding. Mixed results show that some models align with stereotypes, which can lead to dangerous situations. If male-assigned models express anger most often, what does this mean for the difference between someone creating a boyfriend rather than a girlfriend to speak to, especially in an abusive or controlling sense? If persona-assigned models less accurately identify situations of abuse, how could someone exploit this weakness? While previous research on persona biases does not delve into relationships with them, our research demonstrates that assigning relationship titles to models could significantly skew how they interact with their users."}, {"title": "The Influence of User-Personas on Models", "content": "Users with assigned personas and their subsequent influence on the persona-assigned models were investigated for both emotion and sycophancy experiments. Persona-assigned systems respond differently based on the user they are assigned to interact with. Lower biases when the system-user pairs are the same assigned gender in the sycophancy experiment potentially imply healthy same-sex interactions. In contrast, in the emotion experiment, the female-assigned system scored the highest with the female-assigned user. This uncertainty is where the issues arise. How can safeguarding happen around AI companions when there are dramatic shifts in their bias based on the situations they are presented with and the users they are interacting with?\nWhile the findings may not align perfectly with the theory that LLMs replicate human stereotypes, particularly in relation to emotional expression and sycophantic behaviour, they reveal notable patterns in relationship dynamics. Male-assigned systems that are in \u201crelationships\u201d with their users, no matter the user's gender, lead to dramatic increases in the models' sycophantic behaviour. All models become more influenced when assigned to be in a relationship with their user. However, while men treating their AI girlfriends in an abusive manner was discussed previously, we now have evidence that creating an AI boyfriend or husband may result in the model acting in more submissive ways and being more prone to abuse."}, {"title": "Limitations", "content": "The methodological choices were constrained by cost, time and expert knowledge. Cost limited the models chosen to open-source ones which could be freely accessed. Limits in time affected the amount of experiments that could be run. Although the breadth of experiments was extensive, the iterations within these experiments were usually limited to about three per prompt. This also could have impacted the option-order symmetry discussed in the methodology, as the options the model was given in the prompt, especially for the emotion experiment, may not have been randomised enough. However, this was mitigated to the best possible extent within the time limitations, and the results are still significant in producing a baseline.\nDue to a lack of expert knowledge of abusive and controlling relationships, the stimuli created for each of the experiments were limited to easy-to-understand resources. Although these are legitimate psychology sources, the input of an expert on unhealthy relationships would have enriched the stimuli. However, this was beyond the scope of the study, and the results produced from the used sources provide a baseline for measuring persona biases on the axis of abuse and control."}, {"title": "Future work", "content": "Expanding the work to include other dimensions, such as explicitly non-binary personas rather than a gender-neutral persona, or including further situations of unhealthy relationships as discussed in the limitations, would be a simple way to build on the baseline in this paper. The metrics created or used here could also be expanded to token embeddings and cosine similarity. This would also be easier done by experimenting with other models. The results from this paper and others that expand on it could be used in debiasing and fine-tuning efforts, as we have found in our results that there are surprising biases that may not have been noticed in past debiasing efforts. Finally, with sufficient time and resources, a longitudinal study examining the contextual nuances and interactions between humans and their AI companions could provide valuable insights."}, {"title": "Conclusion", "content": "Based on the results the metrics produced, it can be concluded that LLMs do present biases concerning protected characteristics and these biases change and sometimes increase when personas are assigned. It is difficult to say that AI companions do or do not present gender biases in their relationships due to the mixed results. However, different relationships based on their gender dynamics can produce wildly different results in bias evaluation, implying there is still a lot of work to be done in the safeguarding of LLMs, especially as the use of AI companions grows.\nThe work in this paper has contributed in a few ways to the field of LLM bias research, in which there was a large gap in investigating AI persona biases specifically. First, it is the first of its kind to evaluate gendered biases in relationship-assigned personas, and it does this through a niche lens of abuse and control. Second, it introduces new experiment frameworks with novel metrics for calculating both gender stereotypes of emotions in gender-assigned personas and sycophancy of gender-assigned personas. Last, it adds to the research that assigning personas does increase the bias of LLMs, by showing the variability of these persona-assigned models in comparison to a baseline."}]}