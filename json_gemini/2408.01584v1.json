{"title": "GPUDrive: Data-driven, multi-agent driving simulation at 1 million FPS", "authors": ["Saman Kazemkhani", "Aarav Pandya", "Brennan Shacklett", "Daphne Cornelisse", "Eugene Vinitsky"], "abstract": "Multi-agent learning algorithms have been successful at generating superhuman planning in a wide variety of games but have had little impact on the design of deployed multi-agent planners. A key bottleneck in applying these techniques to multi-agent planning is that they require billions of steps of experience. To enable the study of multi-agent planning at this scale, we present GPUDrive, a GPU-accelerated, multi-agent simulator built on top of the Madrona Game Engine that can generate over a million steps of experience per second. Observation, reward, and dynamics functions are written directly in C++, allowing users to define complex, heterogeneous agent behaviors that are lowered to high-performance CUDA. We show that using GPUDrive we are able to effectively train reinforcement learning agents over many scenes in the Waymo Motion dataset, yielding highly effective goal-reaching agents in minutes for individual scenes and generally capable agents in a few hours. We ship these trained agents as part of the code base at https://github.com/Emerge-Lab/gpudrive.", "sections": [{"title": "Introduction", "content": "Multi-agent learning has been impactful across a wide range of fully cooperative and zero-sum games [1, 2, 3, 4, 5, 6]. However, its impact on multi-agent planning for settings that mix humans and robots has been muted. In contrast to the ubiquity of multi-agent learning-based agents in zero-sum games, multi-agent planners for most practical robotic systems are not derived from the output of game-theoretically sound learning algorithms. While it is hard to characterize the space of deployed planners since many of them are proprietary, the majority likely use a mixture of collected data for the prediction of human motion and hand-tuned costs. These are then fed into a robust trajectory optimizer or may be based on imitation learning [7, 8]. This approach has been highly effective in scaling up real-world autonomy but can struggle with reasoning about long-term behavior, contingency planning, and interaction with humans in rare, complex scenarios.\n\nThe divergence in preferred technique between these two domains is partially the outcome of two distinct, challenging components of real-world multi-agent planning. First, unlike zero-sum games, it is necessary to play a human-compatible strategy that is difficult to identify without data. Second, generating the billions of samples needed for multi-agent learning algorithms is difficult with existing simulators. The former challenge is difficult for multi-agent learning since there is not a clear equilibrium concept that algorithms should be pursuing. The latter problem is a challenge for simulators since it is difficult to simulate embodied multi-agent environments at appropriately high rates."}, {"title": "Related work", "content": "Frameworks for batched simulators. There are various open-source frameworks available that support hardware-accelerated reinforcement learning environments. These environments are generally written directly in an acceleration framework such as Numpy [9], Jax [10], or Pytorch [11]. In terms of multi-agent accelerated environments, standard benchmarks include JaxMARL[12], Jumanji [13], and VMAS [14] which primarily feature fully cooperative or fully competitive tasks. Each benchmark requires the design of custom accelerated structures per environment. In contrast, GPUDrive focuses on a mixed motive setting and is built atop Madrona, an extensible ECS-based framework in C++, enabling GPU acceleration and parallelization across environments [15]. Madrona comes with vectorization of key components of embodied simulation such as collision checking and sensors such as LIDAR. GPUDrive can support hundreds of controllable agents in more than 100,000 distinct scenarios, offering a distinct generalization challenge and scale relative to existing benchmarks. Moreover, GPUDrive includes a large dataset of human demonstrations, enabling imitation learning, inverse RL, and combined IL-RL approaches.\n\nSimulators for autonomous driving research and development. Table 1 shows an overview of current simulators used in autonomous driving research. The purpose of GPUDrive is to facilitate the systematic study of behavioral, coordination, and control aspects of autonomous driving and multi-agent learning more broadly. As such, visual complexity is reduced, which differs from several existing simulators, which (partially) focus on perception challenges in driving [16, 17]. Driving simulators close to GPUDrive in terms of either features or speed include MetaDrive [18], nuPlan [19], Nocturne [20], and Waymax [21] which all utilize real-world data. Unlike MetaDrive and nuPlan, our simulator is GPU-accelerated. Like GPUDrive, Waymax is a JAX-based GPU-accelerated simulator that achieves high throughput through JIT compilation and efficient use of accelerators. With respect to Waymax, our simulator supports a wider range of possible sensor modalities (Section"}, {"title": "Simulation Design", "content": "Learning to safely navigate complex scenarios in a multi-agent setting requires generating many billions of environment samples. To feed sample-hungry learning algorithms, GPUDrive is built on top of Madrona [15], an Entity-Component-State system designed for high-throughput reinforcement learning environments. In the Madrona framework, multiple independent worlds (each containing an independent number of agents\u2020 ) are executed in parallel on accelerators via a shared engine.\n\nHowever, driving simulation offers a particular set of challenges that require several technical choices. First, road objects, such as road edges and lane lines are frequently represented as polylines (i.e. connected sets of points). These polylines can consist of hundreds of points as they are sampled at every 0.1 meters, leading to even small maps having upwards of tens of thousands of points. This can blow up the memory requirements of each world as well as lead to significant redundancy in agent observations. Second, the large numbers of agents and road objects can make collision checking a throughput bottleneck. Finally, there is immense variability in the number of agents and road objects in a particular scene. Each world allocates memory to data structures that track its state and accelerate simulation code. Though independent, each world incurs a memory footprint proportional to the maximum number of agents across all worlds. In this way, the performance of GPUDrive is sensitive to the variation in agent counts across all the worlds in a batch."}, {"title": "Simulation Engine", "content": "These challenges are partially resolved via the following mechanisms. First, a primary acceleration data structure leveraged by GPUDrive is a Bounding Volume Hierarchy (BVH). The BVH keeps track of all physics entities and is used to easily exclude candidate pairs for collisions. This allows us to then run a reduced-size collision check on potential collision candidate pairs. The use of a BVH avoids invoking a collision check that would otherwise always be quadratic in the number of agents in a world. Secondly, we observed that a lot of the lines in the geometry of the roads are straight. This allows us to omit many intermediate points while only suffering a minor hit in the quality of the curves. We apply a polyline decimation algorithm (Viswalingham-Whyatt Algorithm) [39] to approximate straight lines and filter out low-importance points in the polylines. With this modification, we can reduce the number of points by 10-15 times and significantly improve the step times while decreasing memory usage."}, {"title": "Simulator features", "content": "We provide an overview of some of the pertinent simulator features as well as sharp edges and limitations of the simulator as a guide to potential users."}, {"title": "Dataset.", "content": "GPUDrive represents its map as a series of polylines and does not require a connectivity map of the lanes. As such, it can be made compatible with most driving datasets given the pre-processing of the roads into the polyline format. Currently, GPUDrive supports the Waymo Open Motion Dataset (WOMD) [40] which is available under a non-commercial license. The WOMD consists of a set of over 100,000 multi-agent traffic scenarios, each of which contains the following key elements: 1) Road map - the layout and structure of a road, such as a highway or parking garage. 2) Expert human driving demonstrations. 3) Road objects, such as stop signs and crosswalks. Figure 1 depicts an example of an intersection traffic scenario as rendered in GPUDrive."}, {"title": "Sensor modalities.", "content": "GPUDrive supports a variety of observation spaces intended to enable heterogeneous types of agents. Fig. 1 depicts the three types of supported state spaces. The first mode is somewhat unphysical in which all agents and road objects within a fixed radius are observable to the agent. This mode is intended primarily for debugging and quick testing, enabling a user to minimize the amount of partial observability in the environment. The other two modes are based on a GPU-accelerated LIDAR scan, representing what an autonomous vehicle would be able to see and what a human would likely be able to see respectively. Both modes are based on casting LIDAR rays; to model human vision we simply restrict the LIDAR rays to emanate in a smaller, controllable-sized cone that can be rotated through an action corresponding to head rotation. Note that since all objects are represented as bounding boxes of fixed height, the LIDAR observations are over-conservative as humans while LIDAR scans in reality are usually able to see over the hoods of cars."}, {"title": "Agent dynamics.", "content": "Agents are stepped using a standard Ackermann bicycle model with actions corresponding to steering and acceleration. This model enables the dynamics of objects to be affected by their length, creating different dynamics for small cars vs. trucks. However, this model is not fully invertible which can make it challenging to use this model for imitation learning. To enable full invertibility for imitation learning, we also support the simplified bicycle model, taken from Waymax [21], which is a double-integrator in the position and velocity and updates its yaw as:\n\n$O_{t+1} = O + St(vtAt + \\frac{1}{2}at^2)$\n\nwhere @ is the yaw, s is the steering command, v is the velocity, and a is the acceleration at time t respectively. At is the timestep. This model is always invertible given an unbounded set of steering and acceleration actions but is independent of the vehicle length. See the appendix for full details on the models.\n\nNote that this model does not factor in the length of the car, causing both long and short objects to have identical dynamics. However, computing the expert actions and then using them to mimic the expert trajectory under this model leads to lower tracking error than the default bicycle model."}, {"title": "Rewards.", "content": "All agents are given a target goal to reach; this goal is selected by taking the last point observed in the vehicle's logged trajectory. A goal is reached when agents are within some configurable distance 8 of the goal. By default, agents in GPUDrive receive a reward of 1 for achieving their goal and otherwise receive a reward of 0. There are additional configurable collision penalties or other rewards based on agent-vehicle distances or agent-road distances though these are not used in this work."}, {"title": "Available driving simulation agents.", "content": "We use reinforcement learning to train a set of agents that reach their goals 95% of the time on a subset of 500 training scenes. While this number is far below the capability of human drivers, these agents are reactive in a distinct fashion from parametrized driver models in other simulators. In particular, many logged-data simulators construct reactivity by having the driver follow along its logged trajectory but decelerate if an agent passes in front of it. In contrast, these agents can maneuver and negotiate without remaining constrained to a logged trajectory. These trained agents are extremely aggressive about reaching their goals and can be used as an out-of-distribution test for proposed driving agents. The training procedure and more details can be found in Section 4.2."}, {"title": "Simulator sharp-edges.", "content": "We note the following limitations of the benchmark:\n\n\u2022 Absence of a map. The current version of the simulator does not have a well-defined notion of lanes or a higher-level road map which makes it challenging for algorithmic approaches that require maps. The absence of this feature also makes it challenging to define rewards such as \"stay lane-centered.\"\n\n\u2022 Convex objects only. Collision checking relies on the objects being represented as convex objects.\n\n\u2022 Unsolvable goals. Due to mislabelling in the Waymo dataset, some agent goals (roughly 2%) are unreachable. For these agents, we default them to simply replaying their logged trajectory and do not treat them as agents.\n\n\u2022 Variance in controllable agents per scenario. In the majority of scenes, there are approximately 8-10 agents and an average of 50 parked cars. Additionally, the dataset is gathered from the sensors of an autonomous vehicle, leading to some agents having their initial states recorded only after the first time-step of the simulator. These agents are not included, as incorporating them would necessitate \"teleporting\" them into the scene, potentially leading to unavoidable collisions with agents deviating from their logged trajectories."}, {"title": "Simulator performance", "content": "The following Sections describe the simulator speed. Section 4.1 first shows the raw simulator speed and peak goodput. Section 4.2 then investigates the impact on reinforcement learning workflows by evaluating the time it takes to train reinforcement learning agents through Independent PPO (IPPO) [41], a widely used multi-agent learning algorithm."}, {"title": "Simulation speed", "content": "Since scenarios contain a variable number of agents, we introduce a metric called Agent Steps Per Second (ASPS) to measure the sample throughput of the simulator. We define the ASPS as the total number of agents across all worlds in a batch that can be fully stepped in a second:\n\n$ASPS = \\frac{S}{\\Delta T} \\sum_{k=1}^{N} A_k$\n\nwhere $A_k$ is the number of agents in the $k^{th}$ world, S is the number of steps taken, and $\\Delta T$ is the number of seconds elapsed. Figure 2 examines the scaling of the simulator as the number of simulated worlds, which represents the amount of parallelism, increases. To measure performance, we sample random batches of scenarios of size equal to the number of worlds, so that every world is a unique scenario with K agents. On the left-hand side of Figure 2, we compare the performance of GPUDrive to the original Nocturne version [20] (CPU, no parallelism) and a CPU-accelerated version of Nocturne via Pufferlib (16 CPU cores) [42]. Empirically, the maximum achievable AFPS of Nocturne is 15,000 (blue dotted line) though we caution that additional speedups may be possible. In contrast, GPUDrive can reach over a million ASPS on a consumer-grade GPU at 512 worlds (average agents per scenario is 60). This performance also surpasses that of Waymax [21], a JAX-based simulator, where we could not run more than 32 environments in parallel due to Out of Memory (OOM) issues. Note that GPUDrive exhibits near-linear scaling of ASPS between 32 and 128 worlds on a datacenter-grade NVIDIA GPU and between 32 and 256 worlds on a consumer-grade GPU.\n\nTo ensure a fair comparison, Figure 2 (LHS) resembles the conditions used in [21], where all cars, bicyclists, and pedestrians are considered as valid experience-generating agents. However, by default, our system only considers something an agent if it is necessary to move to achieve the goal. This means that cars persistently parked throughout the episode are not considered agents, which aligns correctly with what should constitute an agent. This significantly alters the characteristics of the agent"}, {"title": "End-to-end speed and performance", "content": "The purpose of GPUDrive is to facilitate research and development in multi-agent algorithms by 1) reducing the completion time of experiments, and 2) enabling academic research labs to achieve scale on a limited computing budget. Ultimately, we are interested in the rate at which a machine learning researcher or practitioner can iterate on ideas using GPUDrive. This Section highlights what our simulator enables in this regard by studying the end-to-end process of learning policies in our simulator.\n\nFigure 3 contrasts the number of steps (experience) and the corresponding time required to solve 10 scenarios from the WOMD between Nocturne and GPUDrive. For benchmarking purposes, we say a scene is solved when 95% of agents across all 10 worlds can navigate to their designated target position without colliding or going off-road across all worlds. Ceteris paribus (Details in Appendix D), GPUDrive achieves a 25-40x training speedup, solving 10 scenarios in less than 15 minutes compared to approximately 10 hours in Nocturne."}, {"title": "Conclusion", "content": "In this work, we present GPUDrive, a GPU-accelerated, data-driven simulator. GPUDrive is intended to help generate the billions of samples that are likely needed to achieve effective reinforcement learning for training multi-agent driving planners. By building atop the Madrona Engine [15], we can scale GPUDrive to hundreds of worlds containing potentially hundreds of agents leading to throughput of millions of steps per second. This throughput occurs while synthesizing complex observations such as LIDAR. We show that this throughput has consequent implications for training reinforcement learning agents, leading to the ability to train agents to solve any particular scene in minutes and in seconds when amortized across many scenes. We release the simulator and integrated trained agents to enable further research.\n\nThis paper is a first step in scaling up reinforcement learning for multi-agent planning in safety-critical, mixed human-autonomous settings. However, several important challenges remain for future work. Firstly, we have not yet identified the optimal hyperparameter settings to effectively utilize the collected data, resulting in training being the bottleneck instead of data collection time. Secondly, while the simulator is fast, collecting data for reinforcement learning leads to frequent reset calls that significantly impact throughput. Lastly, training fully human-level drivers in the simulator to navigate without crashing in any scenario remains an ongoing challenge."}, {"title": "Reproducibility", "content": null}, {"title": "Code Reproducibility", "content": "All code required to reproduce the paper is open-sourced at https://github.com/Emerge-Lab/\ngpudrive under release number v0.1"}, {"title": "Computational Resources", "content": "All RL experiments in this paper were run on an NVIDIA RTX 8000 or A100. Total resources for the paper correspond to less than 24 GPU-days."}, {"title": "Vehicle Model", "content": "Our vehicles are driven by a kinematic bicycle model [43] which uses the center of gravity as reference point. The dynamics are as follows. Here (xt, Yt) stands for the coordinate of the vehicle's position at time t, Ot stands for the vehicle's heading at time t, vt stands for the vehicle's speed at time t, a stands for the vehicle's acceleration and 8 stands for the vehicle's steering angle. L is the distance from the front axle to the rear axle (in this case, just the length of the car) and lr is the distance from the center of gravity to the rear axle. Here we assume lr = 0.5L.\n\n$\u03cd = \u03b1$\n\n$v = clip(v_t + 0.5 v \u2206t, -V_{max}, U_{max})$\n\n$\u03b2 = tan^{-1}(\\frac{l_r tan(d)}{L})$\n\n$lr = tan^{-1}(0.5 tan(d))$\n\n$x = v cos(0 + \u03b2)$\n\n$\u1ef3 = v sin(0 + \u03b2)$\n\n$\u014d = \\frac{v cos(\u03b2) tan(d)}{L}$\n\nWe then step the dynamics as follows:\n\n$X_{t+1} = x_t + x At$\n\n$Y_{t+1} = Y_t + \u1ef3 At$\n\n$0_{t+1} = 0_t + 0 At$\n\n$U_{t+1} = clip(v_t + i \u2206t, -V_{max}, U_{max})$"}, {"title": "License Details and Accessibility", "content": "Our code is released under an MIT License. The Waymo Motion dataset is released under a Apache License 2.0. The code is available at https://github.com/Emerge-Lab/gpudrive and at release commit TODO."}, {"title": "Training details", "content": null}, {"title": "End-to-end performance", "content": "The Table below depicts the hyperparameters used to produce the results in Section 4.2.\n\nTable 2: Experiment hyperparameters used for comparing the training runs between Nocturne and GPUDrive in Figure 3. The environment configurations are aligned as closely as possible, using the same observations and field of view. The dataset includes the same 10 scenarios. It's important to note that the length of the GPUDrive rollout is approximately equal to the number of worlds multiplied by the rollout length and then multiplied by the number of controllable agents. We have set this value to be 92 \u00d7 50 \u2248 4600 to approximately match the rollout length in Nocturne."}]}