{"title": "Benchmarking Reinforcement Learning Methods\nfor Dexterous Robotic Manipulation with a Three-Fingered Gripper", "authors": ["Elizabeth Cutler", "Yuning Xing", "Tony Cui", "Brendan Zhou", "Koen van Rijnsoever", "Ben Hart", "David Valencia", "Lee Violet C. Ong", "Trevor Gee", "Minas Liarokapis", "Henry Williams"], "abstract": "Reinforcement Learning (RL) training is pre-\ndominantly conducted in cost-effective and con-\ntrolled simulation environments. However, the\ntransfer of these trained models to real-world\ntasks often presents unavoidable challenges.\nThis research explores the direct training of RL\nalgorithms in controlled yet realistic real-world\nsettings for the execution of dexterous manip-\nulation. The benchmarking results of three RL\nalgorithms trained on intricate in-hand manip-\nulation tasks within practical real-world con-\ntexts are presented. Our study not only demon-\nstrates the practicality of RL training in au-\nthentic real-world scenarios, facilitating direct\nreal-world applications, but also provides in-\nsights into the associated challenges and con-\nsiderations. Additionally, our experiences with\nthe employed experimental methods are shared,\nwith the aim of empowering and engaging fel-\nlow researchers and practitioners in this dy-\nnamic field of robotics.", "sections": [{"title": "1 Introduction", "content": "Dexterous robotic manipulation is the realm of robotics\nwhere multiple manipulators, typically robot fingers, col-\nlaborate to grip, handle, and manipulate objects within\nthe hand [Okamura et al., 2000]. In robotics, researchers\ntypically use simple robot grippers and hands for the exe-\ncution of complex industrial tasks like grabbing and mov-\ning components in structured environments [Tai et al.,\n2016]. However, the varied tasks found in human-centred\nspaces like homes require adaptable manipulators like\nmulti-fingered hands, which can execute a plethora of\neveryday actions such as moving objects, opening doors,\nand painting. Therefore, developing high-dimensional\nmulti-fingered robotic grippers for dexterous manipula-\ntion has become an increasingly intriguing and unsolved\nchallenge in robotics.\nControlling dexterous hands is a challenging task due\nto the required intricate coordination of sensor data, mo-\ntor control, adaptability to object variability, and pre-\ncise hand-object interactions [Ozawa and Tahara, 2017].\nImplementing robot dexterity requires a combination\nof advanced hardware components, sophisticated soft-\nware, and extensive development time to achieve suc-\ncess. Moreover, controlling a multiple-degree-of-freedom\nrobotic hand with classical or heuristic controls can be\nimpractical and complex due to the high level of control\nand fine-tuned motor skills required. Fortunately, Rein-\nforcement Learning provides a more robust and adaptive\nsolution for its ability to learn [Yu and Wang, 2022].\nRL holds the potential to automate complex control\ntasks by enabling agents to learn through trial and er-\nror. By defining a task and rewarding the agent ac-\ncordingly, over time, the agent can learn to coordinate\nits joints to accomplish the task. RL training is com-\nmonly executed in simulation environments. However,\nchallenges arise when transitioning these trained models\ninto real-world applications. Recently, there has been\na growing research interest in conducting RL training\nin controlled real-world environments as shown in [Yu\nand Wang, 2022]. Other examples include [Ahn et al.,\n2020][Valencia et al., 2023] and [Zhu et al., 2019].\nThe goal of this research paper is to add to the existing\nresearch into RL for robotic control and implementation\nof robotic dexterity by presenting:\n1.  Benchmarking results on a real-world, three-\nfingered gripper to learn control behaviours.\n2.  A general guide on conducting RL experiments in\nthe real world for dexterous robotic manipulation."}, {"title": "2 Background and Related Work", "content": "To accurately present the reinforcement learning bench-\nmark tasks that are assessed in this work on a real-world\nthree-finger gripper, it is critical to understand the cur-\nrent state of the art of dexterous manipulator design, the\nRL training in the real world, and the current state of\nthe art of RL for dexterous manipulation benchmarking."}, {"title": "2.1 Gripper Design", "content": "Simulated manipulators are a popular training solution\nas there are no physical components to purchase or de-\nvelopment time needed. Fatigue of actuators, cable man-\nagement, and component failures are factors that do not\nneed to be considered in the simulation. The ADROIT\nhand, used in [Fakoor et al., 2020] can complete the com-\nplex dexterous task of opening a door using all of the\ngripper's 24 degrees of freedom (DoF). However, simu-\nlated models are computationally expensive. For exam-\nple, training the simulated Shadow Hand involved 384\nworker machines, each with 16 CPU cores [OpenAI et\nal., 2019]. Even with such a large amount of comput-\ning power, the simulated robotic manipulators cannot\nbe equipped with complete and accurate physics models.\nThis leads to problems when transferring simulated con-\ntrol policies to the real world, known as the simulation-\nto-reality gap described in more detail in Section 2.2.\nThe design of dexterous robotic manipulators in the\nreal world is also challenging, due to the several design\nconsiderations that need be taken into account: weight,\ncost, durability, accuracy, reliability, ease of use etc. The\nuse of fully actuated designs with several motors is well\nknown to increase the cost, weight, and complexity of\nthe system, reducing the overall reliability. This has led\nto the development of underactuated dexterous robotic\nmanipulators - where there are fewer actuators than the\ndegrees of freedom - and the designs take advantage of\nstructural compliance that allows them to conform to\nthe object shapes and be more robust. In [Liarokapis\nand Dollar, 2016], the authors present an example of\nan underactuated end effector gripper that is used to\nlearn new control actions in a supervised manner. Such\nan end-effector is easy to control but has limited fidelity,\nconstraining the gripper to the execution of simple tasks.\n[Gupta et al., 2016] presents an extremely compliant\ngripper, more compliant than most, making the manip-\nlator less likely to be damaged. The gripper is pneu-\nmatically actuated through 7 air chambers, is difficult\nto control with standard methods, and requires the use\nof a complex motion capture system so as to record the\ngripper kinematics and produce accurate results. Hence,\nthe tasks that this manipulator can complete are limited\nby the corresponding control complexity."}, {"title": "2.2 Real world RL training", "content": "RL training is usually done in simulation as it pro-\nvides a safe, cost-effective, easy to use, and efficient data\ncollection solution. However, the challenges associated\nwith bridging the simulation-to-real gap as presented in\n[Zhao et al., 2020], pose a significant issue when applying\ntrained models in real world environments and scenarios.\nThis transition is difficult due to the assumptions used\nwhen creating a simulation that are unrealistic in real-\nworld scenarios. These are phenomena that are difficult\nto model, such as friction between fingertips and the\nobjects, uncontrolled contact slipping and rolling, and\ncomplex, non-uniform contact area compliance. Many\nattempts have been made to bridge this gap in the soft-\nware realm. NVIDIA researchers incorporate domain,\nphysics and non-physics randomisation into the simula-\ntion environment to better bridge this gap [Handa et al.,\n2023]. The challenges of real-world RL have been bench-\nmarked and analysed in [Dulac-Arnold et al., 2021]. This\npaper presents nine challenges RL must succeed at be-\nfore being deployed in real-world applications.\nTo prevent this problem, training directly in the real\nworld is being explored as an alternative. [Ahn et al.,\n2020] introduced two robots designed to facilitate on-\nhardware RL training for dexterous manipulation (The\n9 DoF robot D'Claw) and locomotion tasks (The 12 DoF\nrobot D'Kitty). They also included a series of continuous\ncontrol benchmark tasks tailored to each robot's capa-\nbilities. [Valencia et al., 2023] compared model-free and\nmodel-based RL approaches directly in a real-world con-\ntext. They accomplished this by training a low-cost four-\nDoF two-finger gripper. This experimental platform al-\nlowed the authors to evaluate the performance of RL\nalgorithms in a real-world setting. [Zhu et al., 2019] sim-\nilarly demonstrated the feasibility of applying model-free\nRL algorithms to robotic hands. The proposed approach\nallowed them to successfully address various challenging\ntasks that were difficult to replicate in simulation en-\nvironments. However, these are limited to low degree\nof freedom robotic grippers and the execution of simple\ntasks. This work increases the complexity of the gripper\nbeing employed and the tasks being solved."}, {"title": "2.3 Dexterous Manipulator RL\nBenchmarks", "content": "The work in [Haarnoja et al., 2018] presents Soft-Actor\nCritic (SAC), an off-policy actor-critic algorithm aiming\nto simultaneously maximise expected return and entropy\nwhile acting as randomly as possible. It is evaluated by\nlearning two challenging tasks directly in the real world,\nthe first to teach a quadruped robot to walk on different\nterrains, and the second involves turning a valve using a\n3-fingered robotic hand presented in [Zhu et al., 2019].\nThe results are impressive; however, the robot only ro-\ntates the valve to a fixed position. When only rotating\nthe valve to a fixed position, the learned policy is essen-\ntially what the agent has memorised and considers as a\nstrategy that will always work. If the initial position is\nchanged slightly, the agent will have difficulty using its\ntrained policy to complete the task.\nThe inclusion of offline data sets is studied in [Nair\net al., 2020] and presents the advantage-weighted actor-\ncritic (AWAC) method, enabling learning through prior\ndemonstration data and online experience. Similarly, it\nis evaluated on a range of dexterous manipulation tasks\nin the real world, such as the three-fingered gripper ro-\ntating a valve precisely 180 degrees. It is shown to\nhave a greater average return than all other prior work\nthey studied, using offline and online data across 20k\ntimesteps. However, the work is limited by setting a\nfixed value for rotation, and the gathering of prior knowl-\nedge is often difficult and impractical in the real world.\nA Model-Based Reinforcement Learning (MBRL) ap-\nproach is presented in [Nagabandi et al., 2020] using on-\nline planning with deep dynamics models (PDDM). The\ntask involves rotating two free-floating Baoding balls in\nthe palm with a 24-DoF shadow hand. The rotation re-\nquires significant dexterity because the robot must find\nprecise and coordinated manoeuvres to avoid dropping\nthe objects. They found that PDDM outperforms prior\nmodel-based and model-free methods, which failed to\nsucceed in learning. However, the authors use Model-\nPredictive control (MPC) [Lin et al., 2020] to select ac-\ntions instead of deep RL.\nCurrently, the design of grippers and dexterous manip-\nulators faces strong limitations. Many robotic grippers\nare under-actuated and compliant, lacking the fidelity\nrequired to complete complex enough dexterous tasks.\nFully actuated real-world grippers can provide the dex-\nterity required for the execution of complex tasks but\noften have to be simulated first to get good results com-\ning at a greater cost than the often expensive gripper\nitself. Many of the existing grippers are also hard to\nreplicate or purchase, with little to no open source ma-\nterial. Similar benchmarks are limited to rotating a valve\nto a fixed position, which may not demonstrate the full\npossibilities of the employed algorithm or gripper.\nIn this paper, we present the results of benchmarking\ntasks focusing on dexterous robotic manipulation, which\nare more complex than those previously presented, with\nseveral RL algorithms used with a three fingered grip-\nper in the real world without the need for simulation\nremoving the sim-to-real problem."}, {"title": "3 Gripper", "content": "This section provides a brief overview of the benchmark-\ning testbed, presenting the employed gripper and the\nsensorized object."}, {"title": "3.1 Hardware Design", "content": "Each robot finger is equipped with three degrees of free-\ndom, two for flexion extension and one for abduction /\nadduction. Dynamixel XL-320 servo motors have been\nchosen as the base element for developing and motoriz-\ning the robotic fingers. This is because they meet the\ngoal of making the fingers as modular and affordable\nas possible. For communication and power, XL-320s\nare able to be daisy-chained together, reducing the ca-\nble management complexity. These actuators make use\nof the Dynamixel SDK package for communication 1,\nwhich makes it easy to access various registers of infor-\nmation within the servo. The design was inspired by the\nROBEL D'Claw [Ahn et al., 2020] as its design seemed\nsuitable for the execution of complex dexterous manip-\nulation tasks. The plane of rotation of the base joint is\nperpendicular to the planes of rotation of the two up-\nper joints. The base of the fingertip is designed so as\nto mimic the rounded semi-spherical shape of a human\nfingertip and is aligned with the base joint. All three\nfingers are identical and almost entirely symmetrical, so\nif a gripper with more fingers needs to be developed, the\nonly part that has to be redesigned is the gripper base\nplate. The base plate of the gripper has several options\nfor how it is connected to its associated rig, making it\nusable for the execution of many complex tasks and for\nvarious applications.\nTo communicate from the programme to the servos and\nfrom the servos to the programme, a Robotis U2D2\nboard is used. The U2D2 board provides both TTL and\nRS-485 communication protocols, but as the servos use\nTTL, only TTL is connected. The U2D2 board is con-\nnected to the Robotis OpenCM 485 Expansion Board,\nwhich supplies 7.7V of power and extends communica-\ntion connections. Each finger is daisy chained indepen-\ndently, as the longer the daisy chain, the more suscepti-\nble it is to problems."}, {"title": "3.2 Valve Observation Method", "content": "To complete the chosen benchmarking tasks, a sensorized\nvalve is required. A 4-pronged valve similar to those\nfound on taps and in [Ahn et al., 2020] was designed and\n3d printed. The angular position of the valve is required\nfor the state vector, as described in section 4.3. To re-\nceive this position, the valve is connected to a Dynamixel\nXL330-M077-T servo motor that is equipped with a con-\ntactless absolute encoder (12-bit, 360\u00b0). This motor has\na low stall torque, provides 0-360 degree angular position\nvalues, and uses the Dynamixel SDK package\u00b9, which al-\nlows it to integrate into the code easily."}, {"title": "4 Experimental Methods", "content": "This work seeks to assess the efficiency of the model-free,\ndeep RL algorithms in executing dexterous manipulation\nwith a three-fingered gripper. To do so, the algorithms\nare trained and evaluated across three distinct rotational\ntasks. Each task is designed to mimic complex contact\npatterns and coordination demands reminiscent of every-\nday hand manipulations. To ensure the comparability\nof our results, we employ uniform sets of hyperparam-\neters, state, and action spaces, and reward structures\nthroughout our experimentation. This approach allows\nfor fair and meaningful conclusions about the RL algo-\nrithms' adaptability and effectiveness across the execu-\ntion of multiple tasks."}, {"title": "4.1 Tasks", "content": "A set of comprehensive evaluations have been conducted\nwith the gripper, particularly focusing on its proficiency\nand adaptability in performing distinct tasks. These\ntasks are important for assessing the functional capacity\nof the gripper but also for determining its potential ap-\nplications in real-world scenarios, especially where pre-\ncision and adaptability are required. Each manipulation\ntask assesses the algorithm's performance, encompassing\ntraining time, limitations, and real-world success rate.\nThree rotation tasks were defined, including a 90-degree\nrotation, a sequence of 90, 180, and 270 degrees, and ro-\ntations of 30 to 330 degrees. These tasks were designed\nto assess the algorithm's capacity to learn and generalise\nthe objective of rotating the sensorized valve.\nThe complexity of these tasks arises from the necessity\nfor coordinated finger movements. To push and rotate\nthe valve, the fingers must achieve a precise balance of\ntorque and pressure. Adding to the challenge, the valve\nstarts at a different angle between 0 and 360 degrees for\neach episode. This variability makes the training diffi-\ncult because it prevents the network from relying on a\nmemorised sequence of movements and compels genuine\nadaptation to each new task configuration.\nDexterous robotic manipulation in real-world scenar-\nios further intensifies these challenges. Unlike controlled\nenvironments, the real world presents unpredictable vari-\nables variations in hardware functionality, external\nforces, and subtle shifts in positioning or alignment. A\nslight change in these environments can lead to entire\ntask failures. Thus, we have generalised these tasks, ex-\ntending and surpassing previous benchmarking tasks."}, {"title": "90 degrees", "content": "This task aims to turn the valve to an angle 90 degrees\ngreater than the current angle. We employ the modulo\noperator to restrict the generated goal angle between 0\nand 360 degrees to ensure a continuous range of goals.\nThe successful completion of this task requires cooper-\native finger movements to push and rotate the valve,\npresenting an exploration challenge for the algorithms."}, {"title": "90, 180, 270 degrees", "content": "Like the 90-degree task, this task involves turning the\nvalve to a goal angle of either 90, 180, or 270 degrees\ngreater than the current angle. The additional com-\nplexity arises from the requirement to adapt to different\ndestination angles rather than relying on a fixed set of\nmovements that result in the same relative rotation."}, {"title": "30 - 330 degrees", "content": "The 30-330 degrees task represents the most challenging\nscenario. Here, the goal angle is randomly generated be-\ntween 30 and 330 degrees relative to the current angle.\nAccomplishing this task successfully necessitates the ac-\nquisition of significant dexterity by the robotic gripper\nthroughout the training process."}, {"title": "4.2\nConfigurations", "content": "RL has made significant strides in enabling agents to\nlearn from interactions within their environments. At\nthe forefront of this progress are algorithms such as TD3\n(Twin Delayed Deep Deterministic Policy Gradient) [Fu-\njimoto et al., 2018], DDPG (Deep Deterministic Policy\nGradient) [Lillicrap et al., 2015], and SAC [Haarnoja et\nal., 2018]. These algorithms are specifically designed to\nhandle challenges in continuous action spaces, making\nthem relevant for robotic applications.\nFor all the algorithms, the hyperparameters in Table\n1 were used to ensure consistent and unbiased training.\nMost of the parameters used are the default RL config-\nurations for TD3, SAC, and DDPG. From testing and\nevaluation, the results indicate that a G value in the\nrange of 7-8 yields the best training performance across\nall three algorithms. With a G value in this range, the al-\ngorithms demonstrated improved convergence rates and\nmore stable learning trajectories.\nExploration is a critical aspect that allows the model\nto try different actions to discover which ones yield the\nmost rewarding outcomes. A value of 1,000 exploration\nsteps was deliberately set to give the model sufficient\nopportunities to explore the action space without be-\ning overly biased by previous experiences and is flexible\nenough to converge to a sub-optimal strategy.\nWhen determining the optimal number of steps for\ntraining, both computational logic and hardware con-\nstraints play pivotal roles. In our case, we arrived at a\nstrategic decision to set the limit at 60,000 steps dur-\ning training. The rationale behind this wasn't merely\nbased on the theoretical or algorithmic side of things\nbut also stemmed from practical constraints posed by"}, {"title": "4.3 State and Action Space", "content": "The observation state-space representation for the three-\nfingered gripper completing valve rotation tasks is a 1D\nvector consisting of 19 values. These values are the angle\nof the nine servo joints, the four-valve tips' positions (X\nand Y), the current valve angle, and the goal angle, as\nshown in Figure 3. The four tip X-Y positions of the\nvalve are calculated using trigonometry; these positions\nare provided for extra information on where the valve\ncan be pushed.\nA continuous action space is used to output a nine-\nelement vector with the desired angle of each servo joint.\nThe angle values produced are constrained between the\nmaximum and minimum boundaries to prevent the ser-\nvos from moving to physically impossible positions."}, {"title": "4.4\nReward Function", "content": "A reward is given to the agent at each step, calculated\nusing the following equations.\n$R=\\begin{cases}\n-1, & \\text{if } |\\Delta\\theta| < \\epsilon \\\\\n+10, & \\text{elif } |\\Delta\\theta_A| < \\epsilon \\\\\n\\Delta\\theta/\\Delta\\theta_B, & \\text{otherwise}\n\\end{cases}$\nAs shown in Figure 4, $\\Delta\\theta_B$ and $\\Delta\\theta_A$ are the absolute\ndifference between $\\Theta_{goal}$ and $\\Theta_{before/after}$. e denotes the\nnoise tolerance threshold, where any change or difference\nbelow this limit is considered negligible. If the goal has\nbeen reached before the episode concludes, an additional\nreward of 10 is awarded.\nThis approach to calculating rewards places emphasis\non the agent's progress towards the goal, rather than pe-\nnalising it for being far away. We argue this encourages\nthe agent to focus on making progress and approaching\nthe goal in a positive way. This approach can help pre-\nvent the agent from getting stuck or discouraged if it\nstarts in a distant state, as it will receive rewards for\nany steps it takes in the right direction."}, {"title": "5 Results", "content": "Figure 5 presents a comparative evaluation of the learn-\ning processes employed by three model-free RL algo-\nrithms: DDPG, SAC, and TD3. The primary metric\nfor comparison is the average reward evaluation. Figure\n5 further delineates the performance of each algorithm\non each executed task.\nIn Figure 6, we present the success rates of three tasks,\neach trained on multiple iterations of dexterous manip-\nulation executed with the three-fingered gripper using\nthree distinguished algorithms: DDPG, SAC, and TD3.\nThe success rate metric serves as a vital tool to gauge\nthe effectiveness and efficiency of each algorithm when\napplied to physical hardware. Importantly, a success\nrate of 1 indicates that the algorithm, within a single\nepisode of 50 steps, successfully guided the gripper to its\nintended goal angle without error. This not only high-\nlights the algorithm's precision but also its responsive-\nness, especially when integrated with cost-effective, real-\nworld hardware. Comparing these success rates across\nvarious tasks and algorithms offers a deeper insight into\nthe advantages and potential challenges each algorithm\npresents in practical settings.\nFigure 7 showcases the evaluation process of the agent.\nThe agent was evaluated through an evaluation episode\nlasting 50 steps, every 10 training episodes. Notably,\nduring these evaluation phases, the agent remained static\nin its approach, refraining from learning or improving its\npolicy. The results represent the computed average re-\nward over each evaluation episode. From the displayed\noutcomes, it is evident that TD3 consistently outper-\nforms its counterparts. This observation highlights the\nrobustness and efficiency of the TD3 algorithm in the\nefficient execution of the considered tasks in the relevant\nenvironments. To demonstrate the gripper's capability\nin task execution, an evaluation loop is conducted us-\ning the final model of each trained agent, encompassing\n1000 steps. Performance assessment was based on suc-\ncess metrics as previously defined, with results shown\nin Table 2. Across all three tasks, a recurring pattern\nemerged, with TD3 consistently achieving a success rate\nexceeding 90%. DDPG and SAC closely trailed behind\nTD3 in performance.\nThe reason behind TD3's better performance is likely\nmultifaceted. At its core, TD3 employs a twin-delayed\ndeep deterministic policy gradient approach. This\nmethodology aims to tackle the challenges of value over-\nestimation. One key strategy TD3 employs to reduce\noverestimation is by using two critics and taking the\nminimum value of the two, which effectively addresses\nthe issue of over-optimistic value estimates. This is in\naddition to updating the policy less frequently than the\nQ-values and using noise regularisation. Furthermore,\nTD3 incorporates a policy smoothing technique to\ncombat the extrapolation error by adding noise to\nthe target policy during the critic update. These\narchitectural and algorithmic improvements, combined\nwith a more stable learning process, potentially render\nTD3 as the standout among the trio in the execution of\nthe dexterous robotic manipulation tasks and contexts\nshowcased in figures 6, 7 and 8.\nA video demonstrating the training process and per-\nformance of the final policy for different tasks can be"}, {"title": "6 Discussion", "content": "The evaluation of the DDPG, SAC, and TD3 methods,\nas illustrated through various metrics and visual repre-\nsentations, offers valuable insights into the relative learn-\nings of these model-free RL algorithms when piloted in\nthe execution of complex dexterous manipulation tasks\nwith physical hardware like the employed three-fingered\nrobotic gripper.\nTD3 is the best and emerges as the leader in terms\nof performance. This is particularly evident when con-\nsidering the learning curves, wherein TD3 consistently\nshowcases a more stable and rapid convergence towards\noptimal solutions. Such a performance underscores its\ninnate robustness and capability to efficiently navigate\nthe complexities associated with the execution of com-\nplex, dexterous real-world tasks. This efficiency may well\nbe attributed to the twin-delayed approach and judicious\nuse of two critics, minimising pitfalls such as value over-\nestimation, a notorious challenge in RL.\nAlthough TD3 proves to be the best, it's pivotal not\nto overshadow the potential of both the DDPG and the\nSAC RL methods. DDPG, despite its occasional vari-\nances, demonstrates a laudable ability to drive physi-\ncal robot hardware towards desired outcomes. On the"}, {"title": "7 Conclusions and Future Work", "content": "Benchmarking results indicate that TD3 consistently\noutperforms both DDPG and SAC across all tasks, un-\nderscoring its efficacy and resilience in addressing real-\nworld continuous tasks. Throughout the series of bench-\nmarking tasks presented, RL training in the real world\nhas revealed its practicality and advantages, effectively\nmitigating the simulation-to-real gap. Despite grap-\npling with hardware issues related to wear and tear, the\ntrained model has demonstrated robustness and adapt-\nability to diverse conditions. This paves the way for the\npotential of training RL models in executing complex\ntasks in controlled real-world environments.\nFuture work will focus on implementing velocity-based\ncontrol methods into the gripper, in contrast to the cur-\nrent method of position control. As part of this, we will\nintroduce tactile sensors into the fingertips of the three-\nfingered gripper that will be utilised to detect when the\ngripper touches the valve. With this addition, the ex-\npectation is to observe an improvement in the training\nof the three-fingered gripper as the sensors will provide\nan additional set of information that can be used as a\nreward. Furthermore, velocity-based control is expected\nto allow for more dynamic motion of the gripper, which\nmay potentially give way to higher success in executing\ncomplex, dexterous robotic manipulation tasks."}]}