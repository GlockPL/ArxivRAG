{"title": "Diff-CXR: Report-to-CXR generation through a disease-knowledge enhanced diffusion model", "authors": ["Peng Huang", "Bowen Guo", "Shuyu Liang", "Junhu Fu", "Yuanyuan Wang", "Yi Guo"], "abstract": "Abstract-Text-To-Image (TTI) generation is significant for controlled and diverse image generation with broad potential applications. Although current medical TTI methods have made some progress in report-to-Chest-Xray (CXR) generation, their generation performance may be limited due to the intrinsic characteristics of medical data. In this paper, we propose a novel disease-knowledge enhanced Diffusion-based TTI learning framework, named Diff-CXR, for medical report-to-CXR generation. First, to minimize the negative impacts of noisy data on generation, we devise a Latent Noise Filtering Strategy that gradually learns the general patterns of anomalies and removes them in the latent space. Then, an Adaptive Vision-Aware Textual Learning Strategy is designed to learn concise and important report embeddings in a domain-specific Vision-Language Model, providing textual guidance for Chest-Xray generation. Finally, by incorporating the general disease knowledge into the pretrained TTI model via a delicate control adapter, a disease-knowledge enhanced diffusion model is introduced to achieve realistic and precise report-to-CXR generation. Experimentally, our Diff-CXR outperforms previous SOTA medical TTI methods by 33.4% / 8.0% and 23.8% / 56.4% in the FID and mAUC score on MIMIC-CXR and IU-Xray, with the lowest computational complexity at 29.641 GFLOPs. Downstream experiments on three thorax disease classification benchmarks and one CXR-report generation benchmark demonstrate that Diff-CXR is effective in improving classical CXR analysis methods. Notably, models trained on the combination of 1% real data and synthetic data can achieve a competitive mAUC score compared to models trained on all data, presenting promising clinical applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Image generation involves utilizing computer algorithms to produce novel images, aiming to generate realistic or stylized visual content based on specified conditions. It has gained popularity across diverse domains, encompassing super-resolution, style transfer, image reconstruction and restoration, text-to-image generation (TTI) et al [1], [2], [3], [4], [5]. These achievements underscore its promising potential applications in fields such as art, medicine, and education.\nIn medical scene, dominant image generation methods focus on image-to-image generation, among which the most representative methods are Generative Adversarial Network (GAN) and its variants. As a generator endeavors to learn the distribution of real examples, GAN produces high-quality images to deceive the discriminator [6]. Various architectures and losses based on GAN have been designed to generate high-quality medical images of different modalities and anatomical regions [7], [8]. However, the diversity and controllability of generated samples are limited due to the challenge of specifying nuanced visual attributes solely within image-to-image generation process.\nText-to-image generation emerges and provides a more intuitive and detailed way to specify visual attributes through descriptive language. It exhibits refined semantic control, enabling the generation of more diverse samples to supplement downstream tasks. Additionally, it contributes to the efficient data transmission and storage, as well as a swift image-retrieval system.\nChest X-ray (CXR) is one of the most favored imaging modalities due to their rapid acquisition, non-invasive nature, and low-radiation exposure characteristics. Presently, several studies have made progress in medical TTI tasks, which generate CXRs from clinical reports. First, transformer-based autoregressive model has been successfully developed to generate images from medical reports with tailored architectures or training strategies [9], [10]. Their practical application and further research are impeded by excessive parameters and high inference costs. Diffusion models can strike a balance between image generation performance, and model parameters. Current research mainly focuses on adapting pre-trained Stable Diffusion (SD) [11] to the medical domain through fine-tuning strategies [12], [13]. However, the generation perf r.ormance and efficiency of both autoregressive-based and diffusion-based TTI model are still limited due to the intrinsic characteristics of medical images and reports, as illustrated in Fig. 1. Specifically, medical imaging datasets usually contain noisy samples [14], [15]. TTI model is prone to overfitting to these noisy patterns, thereby markedly compromising the authenticity and precision of the generated images. Next, to manage lengthy medical reports, domain-specific Vision-Language Model (VLM) often increases the maximum textual token length [16]. Textual embeddings with increased dimensions will lead to substantial climb in the computational complexities of the diffusion model. Third, medical reports typically detail diverse patient disease conditions and manifestations. Consequently, it is imperative to focus on the strategies for maintaining or emphasizing the disease representations during the diffusion process. In general, data curating, textual learning, and disease knowledge enhancement are essential to the report-to-CXR generation process."}, {"title": "B. Related Works", "content": "1) Text-to-image generation: Amidst the popularity of diffusion models, text-to-image generation has attained remarkable strides in natural domain [11], [18], [19], [20]. The most representative diffusion-based TTI method is Stable Diffusion [11], which scales up the latent diffusion model by increasing both the model size and data scale. It successfully achieves the trade-off between the computational complexity and the generation capability of diffusion models. Alternatively, U-ViT, a simpler and more general vision transformer architecture, is proposed, which treats time, image and text embeddings as tokens [19]. U-ViT achieves better generation performance in TTI task and has become the mainstream architecture.\nSeveral attempts have been made to perform TTI in medical domain in recent years, which can be divided into two groups, including transformer-based Autoregressive model and Diffusion-based model. Both UniXGen [9] and LLM-CXR [10] follow the typical Autoregressive TTI pipeline, which predicts image tokens based on report embeddings and maps the image tokens to the image-level outputs with a VQ-GAN [4] pre-trained in the medical domain. Although they can generate relatively realistic images from medical reports, extensive model parameters and high inference costs pose challenges for practical applications. Recently, two studies have carefully investigated the finetuning strategies to adapt Stable Diffusion to medical domain. [12] shows the feasibility of finetuning Stable Diffusion to generate medical images in a few-short manner. RoentGen further explores the fine-tuning strategies and demonstrates that finetuning both UNet denoising model and CLIP-based text encoder of Stable Diffusion obtains the best generation results [13]. RoentGen achieves relatively lower computational complexity, but the generation quality still lags behind autoregressive models. In this paper, we find that noisy images heavily impede the generation performance of diffusion-based TTI model and disease descriptions in medical reports are essential to the quality of medical TTI generation.\n2) Domain-specific VLM in medical domain: To provide textual guidance for image generation, domain-specific Vi-"}, {"title": "II. METHODOLOGY", "content": "An overview of our Diff-CXR is shown in Fig. 2. Firstly, the latent noise filtering strategy (LNFS) projects all images into the latent space where visual-relevant details are well preserved, and efficiently removes those data anomalies with their reports in such latent space, following a coarse-to-fine fashion. Then, within the curated dataset, the adaptive vision-aware textual learning strategy prompts the domain-specific language model to learn essential and succinct report embeddings, providing textual guidance for generation. Finally, disease-knowledge enhanced diffusion-based TTI model is trained in two stages, including vanilla diffusion process where only textual condition is injected, and disease-specific knowledge guidance to refine generation results gradually."}, {"title": "B. Latent noise filtering strategy", "content": "Inspired by the representative latent space where diffusion process is performed, here, all images are encoded by the pretrained autoencoder e, and LNFS is devised to gradually learn the distribution discrepancy between normal and noisy images in the latent space through three stages, including latent clustering, manifold modeling and explicit supervision. The framework of LNFS is illustrated in Fig. 3.\nPretrained autoencoder: In this module, the objective is to construct a latent space of input CXRs, where perceptually-related attributes are well preserved. It has been proven that the autoencoder component of SD can precisely reconstruct CXRs out-of-box [12], [13]. Consequently, we keep the autoencoder component frozen to derive the latent representations of input images. Assume that the training set {Xk, Yk}_1^{N} contains N (image, report) pairs. The encoder \u025b converts {k}_{k=1}^{N} into {z}_{k=1}^{N}, and the decoder D can reconstruct images from the latent space, giving x' = D(zk). Both data curating and diffusion process are performed in such latent space.\nLatent clustering: Considering that humans learn the ability to recognize anomalies based on thousands of non-anomalous samples they have known, the first stage of LNFS is K-means clustering process behaving as the preliminary step to identify the major part of normal images. Detailed in Fig. 3, latent representations are reshaped into a one-dimensional vector {k}_{k=1}^{N}, and then K-means clustering is directly applied to partition these vectors {Uk}_{k=1}^{m} into different clusters, which are denoted by different centroids {Ck}_{k=1}^{m}, where m is the numbers of centroids. All images are sorted based on euclidean distances between their latent representations Uk and corresponding centroids ck in the ascending order. By a predefined distance threshold, the typical normal data can be filtered out based on the assumption that anomalies typically manifest in locations distant from their nearest neighbors, whereas normal instances reside within densely clustered regions.\nManifold modeling: Given the typical normal data filtered from latent clustering, manifold modeling aims to understand the general distribution of non-anomalous data {zk}_{k=1}^{n} in the latent space and captures the underlying structure of normal data. In this stage, a U-shape Autoencoder, composed of an encoding network and a decoding network, is devised to reconstruct the normal instances in the latent space by Mean Square Error (MSE). Through reconstruction, the U-shape Autoencoder is forced to retain the signification information which is relevant to the normal data, so those anomalies that differ from the major normal instances should be poorly reconstructed. The reconstruction error is directly used as anomaly score to evaluate those suspicious data excluded from the clustering process, thus identifying those typical noisy data.\nExplicit supervision: Given the typical normal and noisy data, a latent noise discriminator is designed to explicitly model both noisy and normal samples. As shown in Fig. 3, the discriminator consists of a sequence of cascaded convolutional layers, normalization layers, and activation layers for hierarchical feature extraction, followed by an adaptive average pooling layer and a linear classifier for binary classification. The discriminator is supervised under the binary cross entropy loss, so the output score that determines whether the input is noisy pattern can also be employed as anomaly score.\nFinally, all noisy images are detected by the combination of the reconstruction error and discrimination score. The clean (CXR, report) pairs can be denoted as: {xk, Yk}_{k=1}^{n}, where n represents the number of entries in the pruned dataset."}, {"title": "C. Adaptive vision-aware textual learning strategy", "content": "Textual learning is conducted with clean (report, CXR) pairs {xk, Yk}_1^{n}. To obtain the informative and concise report embeddings, here we design an adaptive vision-aware textual learning strategy to sufficiently explore the report comprehension capabilities inherent in the large domain-specific VLM, ensuring that those visually relevant features can be well extracted and preserved.\nAs shown in Fig. 4a, for a long medical report, there are a certain number of image-unrelated tokens in the report, which are useless for our TTI generation. To capture concise image-related context, an effective information squeeze module is introduced to forcefully compress latent embeddings into a lower dimension via a contrastive objective. Detailed in Fig. 4b, the module consists of two cascaded blocks. Each block contains one-dimensional convolutional layer followed by the layer normalization and rectified linear unit (ReLU) activation functions. The information squeeze module is inserted at the end of the VLM text encoder, which outputs squeezed latent embeddings for textual guidance. To preserve those visual-concerned textual features during compression, the symmetric contrastive loss is introduced to apply vision supervision to report comprehension. Both image and text encoders of BiomedCLIP are employed and initialized with pretrained weights to map both modalities to specific latent space. Specifically, the contrastive objective jointly trains the image encoder and text encoder by maximizing the cosine similarity of the image and text embeddings of T real pairs in the batch while minimizing the cosine similarity of those false pairs in the batch. Notably, the contrastive objective optimizes for text-image retrieval, so those image-unrelated report parts will be weakened or ignored during the alignment process, which is well suited for our purpose to extract crucial text embeddings from medical reports.\nThe projected [CLS] tokens are used as the global representations, which can be denoted as (Gri,Gyi). xi, yi representes ith CXR and its corresponding report. InfoNCE loss is adopted as the contrastive objective, which can be detailed as follows:\n$L(G_{xi}, G_{yi}) = -log( \\frac{exp(G_{xi}^TG_{yi})}{\\sum_{k=1}^{T}exp(G_{xi}^TG_{yk})})$ (1)\n$L_{infoNCE} = \\frac{1}{T} \\sum_{i=1}^{T} [l(G_{xi}, G_{yi}) + l(G_{yi}, G_{xi})]$ (2)\nCombined with the symmetric contrastive loss, the information squeeze module can explicitly model the context relationship between different latent tokens and dynamically aggregate important token information for textual guidance."}, {"title": "D. Disease-knowledge enhanced TTI generation", "content": "To generate high-quality medical images from medical reports, we propose a novel disease-knowledge enhanced diffusion-based TTI model based on the clean data pruned by LNFS and textual guidance provided by AVA-TLS. The disease knowledge injection mechanism is designed to distill specific disease knowledge into the vanilla diffusion process, highlighting those disease-related parts in report embeddings and gradually refining the generation process. Next, we will introduce the basic formulation and the component details of our TTI model.\nBasic formulation: Latent diffusion models take the diffusion process in a more efficient and lower-dimensional latent space of powerful pretrained autoencoders. The noise injection process, termed as the forward process, models a fixed Markov chain of length T, which can be defined as:\nq(z_{1:T}|z_0) = \\prod_{t=1}^{T} q(z_t|z_{t-1}) (3)\n$q(z_t|z_{t-1}) = N(z_t| \\sqrt{1 - \\beta_t}z_{t-1}, \\beta_tI)$ (4)\nHere, zo is the latent representation of the clean sample. The noise depends on the time-varying variance $\u03b2_t \u2208 (0,1)$, where t \u2208 1, ...T. The learning task is to reverse this forward process by deducing 2t-1 from its corrupted version zt for t < T. The corresponding objective can be expressed as:\nFor our disease-knowledge enhanced diffusion model, conditioning information including the report embeddings and disease knowledge embeddings is further fed into vanilla diffusion process, the noise prediction object can be detailed as:\nmin_{\u03b8} E_{t,z_0,c,\\epsilon} = ||\\epsilon - \\epsilon_\u03b8(z_t, t, c)||^2 (5)\nwhere $\u03f5_\u03b8(z_t, t, c)$ denotes a sequence of denoising autoencoders parameterized with \u03b8, which shares equal weights to predict the noise \u03f5. c is the report condition or alternative condition information.\nVanilla denoising process: The denoising model receives the noisy input zt, diffusion timestep t and conditioned report embeddings as input to predict injected noise as timestep t. The U-ViT architecture is adopted as the backbone of denoising model because it is capable of handling inputs from different modalities without introducing extra modules and takes fewer parameters than Stable Diffusion, greatly facilitating the report-to-CXR generation process.\nDisease knowledge injection mechanism: Medical reports typically provide highly detailed descriptions of disease conditions. Hence, it is crucial to maintain or enhance disease representations during the generation process for the accuracy and authenticity of generated images.\nTo this end, DKIM is introduced to incorporate general and more detailed disease knowledge guidance into the pretrained TTI model, refining and highlighting disease representations in the diffusion process. Disease types are generally ascertainable or predictable through provided clinical presentations in reports. Here, CheXbert [25] is utilized to annotate data with disease categories. A simple sentence template \u201cThere is {disease type}.\u201d is employed to denote respective disease and \u201cThere is no finding.\u201d is utilized to denote the normal condition without any disease symptoms. When multiple diseases are present in a single image, different disease templates will be concatenated to represent the overall health condition. Given that AVA-TLS can forge meaningful connections between related sections of reports and their corresponding disease representations in images, the pretrained text encoder is introduced to extract disease knowledge embeddings based on predefined disease templates to gain finer control of the report-to-CXR generation process.\nAs shown in Fig. 5, a control adapter is proposed to gradually distill the disease knowledge into the diffusion process. Specifically, the weights of the vanilla denoising structure are fixed, and the encoder block, middle block, and decoder block are denoted as E, M, D, respectively. The encoding block and middle block are cloned to a trainable copy, designated as E' and M'. e(e'), m(m'), and d represent the output of certain encoder block, middle block or decoder block. Thereafter, the disease knowledge is incorporated from the control adapter during the decoding process. The input of ith block of the decoder block D can be expressed as:\n$m + zero(m'))+e_j+zero(e'_j) i = 1, i + j = 9$\n$d_{i-1}+e_j+zero(e'_j) 2 <= i <= 8, i + j = 9$ (6)\nwhere zero represents a zero linear year which weight and bias parameters are initialized to zero when the training starts. In this case, both zero(:) equals to zero when the training starts, and the input of ith block of the decoder block D can be re-expressed as:\n$m + e_j i = 1, i + j = 9$\n$d_{i-1}+e_j 2 <= i <= 8, i + j = 9$ (7)\nwhich is the same as the original denoising model, thus maintaining the generation capabilities of the pre-trained model. Therefore, zero-initialized layers effectively ensure that harmful noise will not influence the hidden states of network when the training starts.\nGenerally, the control adapter employs a zero linear layer to progressively modify the report representations on the context level based on given disease knowledge embeddings. Then, the trainable copy takes the modified report embeddings as input, and gradually distills the disease knowledge into the diffusion process to refine the generation results."}, {"title": "III. EXPERIMENTS", "content": "Experiments are conducted on two widely used report-to-CXR generation benchmarks, i.e., MIMIC-CXR [26] and IU-Xray [27].\nMIMIC-CXR is a large-scale (report, CXR) dataset, provided by the Beth Israel Deaconess Medical Center. The dataset includes 377,110 chest X-ray images and 227,835 reports. Each image is annotated with multiple classes of 14 diagnostic labels, and 11 common diseases are considered for benchmarking purposes. For a fair comparison, the official data splits are adopted.\nIU-Xray from Indiana University is a relatively small dataset, which contains 7,470 chest X-ray images and 3,955 radiology reports. IU-Xray comes from the different hospital and its reports are more concise and standardized. By CheXbert labeler [25], each image is also labeled with 14 diagnostic labels, and 11 diseases are utilized for evaluation. Only anterior-posterior (AP) / posterior-anterior (PA) scans are considered, and all images are resized to 256\u00d7256. Following the previous work [10], we use MIMIC-CXR to develop our Diff-CXR model, and then validate on the test set of MIMIC-CXR. The whole IU-Xray dataset is further employed to gauge the performance of Diff-CXR towards unseen text.\nTo further evaluate the effectiveness of our approach, we conduct a series of downstream task experiments. Specifically, we use the MIMIC-CXR, ChestX-ray14 [28], and CheXpert [29] datasets to validate our method on the multi-label thorax disease classification task.\nChestX-ray 14 contains 112,120 frontal-view X-rays from 30,805 patients with 14 disease labels. We follow the official data split which assigns 75,312 images for training and 25,596 images for testing. Images are resized from 1024\u00d71024 into 224x224.\nCheXpert is a large scale dataset consisting of 191,208 frontal-view chest X-rays. Within the dataset, images annotate 14 diseases. For evaluation, five common diseases, including atelectasis, cardiomegaly, consolidation, edema, and pleural effusion, are considered for benchmarking purposes. Images are resized to 224x224 pixels and the evaluation is conducted on the official validation set.\nMoreover, we apply the MIMIC-CXR dataset to a cross-modal CXR-report generation task, demonstrating the flexibility and versatility of our proposed method."}, {"title": "B. Evaluation measures", "content": "To gauge the quality of generated data, we utilize Fr\u00e9chet Inception Distance (FID), which quantifies the distribution discrepancy between generated data and real data. Lower FID score indicates better vision fidelity.\nTo assess vision-language alignment between generated images and input reports, we calculate the Mean Area Under the Curve (mAUC) score of generated data, against the original CheXbert labels from MIMIC-CXR. A pretrained thorax disease classification network, densnet121-resnet224-all [30], [31], is utilized to predict generated images. Higher mAUC score indicates better vision-language alignment between input reports and generated CXRs.\nFloating Point Operations (FLOPs) are employed to evaluate the computations complexity of different methods.\nIn downstream multi-label thorax disease classification tasks, mAUC on different classes is reported for comparison and higher mAUC score means superior classification performance. For CXR-report generation, we employ the widely-used natural language generation (NLG) metrics and adopt the standard evaluation protocol to calculate the captioning metrics: BLEU [32], METEOR [33] and ROUGE-L [34]. Higher BLEU, METEOR, and ROUGE-L scores indicate a greater similarity between generated reports and reference reports."}, {"title": "C. Experimental setting", "content": "All components of the proposed method are implemented with PyTorch and are trained with NVIDIA Graphics Device A100 80GB GPUs, separately. Implementation Details are listed in Table I.\nFor LNFS, to train the reconstruction network and the discriminator, the Adam optimizer was used to update parameters with a batch size 512. The learning rate was set to 3e-5 and was adaptively adjusted based on the CosineAnnealingLR.\nFor AVA-TLS, we adopt BiomedCLIP [16] as our baseline model. The AdamW optimizer was used in training with a batch size 128. The learning rate was set to le-5 and was also adaptively adjusted based on the CosineAnnealingLR.\nThe disease-knowledge guided TTI model is trained in two stages. First, vanilla diffusion process is performed. AdamW optimizer was employed with a batch size 512, using a learning rate of 2e-4, to update parameters. Second, the pretrained TTI model is kept frozen, disease knowledge is embedded, and only control adapter is trained for additional 200K steps. Training parameters are consistent with the first phase."}, {"title": "IV. RESULT", "content": "We separately ablate the LNFS, AVA-TLS, and DKIM to corroborate their contributions to report-to-CXR generation performance."}, {"title": "1) The necessity of the latent noise filtering strategy:", "content": "As illustrated in Fig. 6, we visualize multiple noisy images filtered by LNFS and provide several normal images as a reference. We can observe that these abnormal samples exhibit notable deviations from normal images, featuring low signal-to-noise ratio and shape distortion, et al. These deviations also result in the lack of the alignment with their respective medical reports.\nTo demonstrate the superiority of our coarse-to-fine anomaly detection method, we compare the filtered samples generated by manifold modeling and explicit supervision phases using TSNE. As shown in Fig 7, several noisy points near the decision boundary or nearby normal points, are misclassified as normal data during the manifold modeling stage. However, explicit supervision successfully identifies such noisy points and corrects the mistakes. Finally, 2000 images are identified as noisy data and excluded from training.\nFinally, we compare the performance of Diff-CXR with and without LNFS on MIMIC-CXR. As shown in Table II, the overall performance of Diff-CXR with LNFS is better than Diff-CXR without LNFS. The FID score decreases from 28.84 to 20.93 and the mAUC score increases from 0.644 to 0.670, proving the significant impact of noisy images on generation process."}, {"title": "2) The efficiency and precision improvements brought by the adaptive vision-aware textual learning strategy:", "content": "According to the quantitative evaluation results shown in Table II, AVA-TLS improves the mAUC score from 0.670 to 0.676, with a slight decrease in FID score. Furthermore, as illustrated in Table III, when token length is set as 77, AVA-TLS substantially reduces the computational complexity to 19.322 GFLOPs, which is nearly two-thirds of the original complexity, 29.729 GFLOPs. In general, AVA-TLS enables Diff-CXR to achieve a trade-off between the model complexity, vision-language alignment and authenticity of generated images successfully.\nNext, we compare the performance of AVA-TLS on different token length after compression. Obviously, when the token length decreases from 256 to 32, the complexity decreases from 29.729 GFLOPs to 16.705 GFLOPs. As shown in Table III, token 77 achieves the best mAUC score while token 32 achieves the best FID score. We assume that reduced tokens lead to smaller distribution discrepancies between different textual embeddings, so that model can easily reference corresponding images through fewer textual tokens. However, there may be some textual semantic information loss during the compression, further influencing the vision-language alignment of generated results. In following experiments, we adopt token 77 as our best configuration, because the consistency between generated images and input reports is considered the most crucial aspect in medical image generation tasks."}, {"title": "3) The image detail refinement benefiting from the disease-knowledge injection mechanism:", "content": "Firstly, to demonstrate that the feature embeddings obtained by the Vision Language Model (VLM) carry accurate disease information for a specific disease template sentence, we visualize CXRs generated from several disease template examples. As shown in Fig. 8, Diff-CXR can successfully synthesize CXRs conditioned on the disease template sentence encoded by the domain-specific VLM. In addition, with slight modifications on the template sentence, Diff-CXR can generate realistic CXRs with various lesions of different severity levels and locations. For example, Diff-CXR is capable of generating CXRs with pleural effusions in both left and right lungs, as well as varying degrees of edema, from mild to severe.\nWe perform relevant ablation experiments on the effectiveness of DKIM of our Diff-CXR, which are presented in Table II. As observed in the table, it is worth noting that DKIM decreases the FID value from 21.56 to 19.50 and increases the mAUC score from 0.676 to 0.678, demonstrating that DKIM successfully improves the realism and accuracy of generated CXRs."}, {"title": "B. Comparison with SOTA medical TTI methods", "content": "Prevalent medical TTI methods can be categorized into two types: autoregressive model and diffusion model. For autoregressive-based TTI methods, we choose UnixGen [9] and LLM-CXR [10] for comparison while for diffusion-based TTI methods, RoentGen [13] is employed.\nExperiments are conducted on MIMIC-CXR and IU-Xray to test both in-of-distribution and out-of-distribution performance. As shown in Table V, the AUC scores of images generated by Diff-CXR for nearly 11 common thorax diseases are consistently higher than those of LLM-CXR, indicating the best alignment between generated images and input reports. From Table IV, our Diff-CXR also surpasses LLM-CXR on the FID score by 9.770 and 9.829, respectively, proving the superiority of Diff-CXR in vision fidelity. As shown in Fig. 9, related disease representations are highlighted in images generated by Diff-CXR and the ground truth. Obviously, images generated by Diff-CXR share the similar disease imaging characteristics with the ground truth, underscoring the superiority of our method. Furthermore, our Diff-CXR demonstrates excellent generalizability towards unseen text with different distributions. Compared to LLM-CXR in IU-Xray, our Diff-CXR consistently delivers commendable results with an improvement of 56.4% and 23.8% in mAUC and FID score, respectively.\nAs indicated in Table IV, our Diff-CXR also achieves the lowest computation complexity 29.641 GFLOPs, nearly one-third that of RoentGen and one-ninth that of LLM-CXR, successfully demonstrating the efficiency of our method. In addition, the inference speed of our Diff-CXR is significantly enhanced due to the optimization in computational complexity, which is about 7.6 times and 4.0 times faster than that of LLM-CXR and RoentGen."}, {"title": "C. Experiments on downstream tasks", "content": "As shown in Fig. 8, synthetic images precisely reflect lesion types, positions and severity levels. Having such a powerful generative model enables us to perform data augmentation. Here, we synthesize novel images using reports which contains lesions coming from MIMIC-CXR and validate the efficacy of generated images on multi-label thorax disease classification tasks and a CXR-report generation task on MIMIC-CXR. In total, 95,787 images are generated for downstream tasks."}, {"title": "V. DISCUSSION", "content": "In this paper, we propose a unified learning framework for report-to-CXR generation through a disease knowledge-enhanced diffusion model. The performances are compared with the state-of-the-art methods on two widely used benchmarks and the results indicate that the proposed method outperforms the existing state-of-the-art methods. We will further discuss several important insights behind Diff-CXR."}, {"title": "A. Towards realistic medical TTI generation", "content": "Realistic images are essential for computer-aided diagnosis to ensure its reliability and effectiveness in clinical applications. However, in medical scenarios, diffusion-based TTI methods tend to overfit those noisy data, degrading the realism of generation results. As shown in Table II, Diff-CXR with LNFS outperforms Diff-CXR trained on the noisy dataset, yielding a remarkable 27.4% improvement in FID and a 4.0% increase in mAUC score. Here, we further delve into why those noisy samples seriously impede the Diffusion-based TTI generation performance. First, in the report-to-CXR generation setting, these noisy samples are treated as supervisory signals instead of the input signals. Once TTI model overfits these noisy patterns, the realism of generated CXRs will degrade seriously. Second, TTI task is data-hungry, while medical datasets with paired images and reports tend to be limited, which is far less than natural image-text datasets that can easily reach billion-scale. Thus, such noisy images result in fewer effective pairs of images and reports, leading to worse impacts on the generation task. Third, in the reverse diffusion process, the denoising model directly predicts the noise added onto the original input at arbitrary timestep t < T. In this case, the denoising model, which demonstrates strong learning capabilities, processes those noisy patterns for multiple times, further increasing the risk of overfitting."}, {"title": "B. Towards efficient and accurate medical TTI generation", "content": "The trade-off between efficiency and accuracy are crucial for the clinical implementation of medical TTI generation. For transformer-based autoregressive TTI methods, large parameter size and computational complexity limit their real applications. Diffusion models achieve comparative generation performance with a more reasonable model size. However, repeated evaluations on a noisy version of the input latent still make the training and inference of diffusion model expensive. With the token limits of medical VLM growing from 77 to 256, as depicted in Table III, the computation complexity of the denoising model increases from 19.322 GFLOPs to 29.727 GFLOPs. Therefore, in medical scenes, such training and inference costs are only expected to be higher. Fig. 4a points that there are several image-unrelated tokens in the specific report. Additionally, we also conduct the statistics of token length on MIMIC-CXR, which shows that the length of report tokens primarily falls within the range of 50 to 150. Hence, for major reports, large domain-specific VLM will introduce several image-unrelated tokens into the hidden space. As textual guidance, these report embeddings may hinder the efficiency of overall report-to-CXR generation. In this case, our Diff-CXR employs AVA-TLS to prompt the large pretrained medical VLM to learn concise and crucial report embeddings for conditioned image generation. As presented in Table II, AVA-TLS enhances the mAUC score of generated images from 0.670 to 0.676. This improvement demonstrates that the effectiveness of AVA-TLS in extracting CXR-related report information to guide the image generation process and strengthen the alignment between input reports and generated CXRs. In addition, Fig. 8 illustrates that TTI model can generate accurate CXRs based on the disease-knowledge embeddings encoded by AVA-TLS, further proving that AVA-TLS is capable of accurately capturing disease representations in clinical reports."}, {"title": "C. Towards high-quality medical TTI generation", "content": "For clinical application, medical field typically demands high-quality images, posing greater challenge for medical TTI task. Currently, SOTA medical TTI method mainly relies on tailored training strategies to enhance the quality of generated results. For example, LLM-CXR employs a two-stage fine-tuning strategy to gradually learn the intimate mapping between vision and language modalities. RoentGen extensively explores the fine-tuning strategies of Stable Diffusion to overcome the distribution shift between natural texts and medical reports. However, the quality of their generated images is still limited due to the intrinsic characteristics of medical images and reports. Here, we focus on the impact of disease knowledge on medical TTI. Medical reports mainly consist of detailed descriptions of patients' disease condition, so that the disease information is crucial for medical TTI. In this case, Diff-CXR designs DKIM to incorporate specific disease knowledge to optimize report representations and implement more refined control over the diffusion process, thus improving the synthesis quality. As shown in Table II, DKIM successfully improve the FID and mAUC score by 2.06 and 0.002, underscoring its effectiveness in enhancing the realism and accuracy of generated results. High-quality medical TTI generation can serve as a more powerful data augmentation method for more diverse downstream tasks. Table VI firstly illustrates that models trained on the combination of synthetic data and realistic data can achieve a remarkable improvement. Notably, with only 1% realistic data and our synthetic data, the mAUC score of maxVIT achieves a significant improvement, which is 0.004 lower than maxVIT trained solely on 1"}]}