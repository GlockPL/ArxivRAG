{"title": "Beyond Model Adaptation at Test Time: A Survey", "authors": ["Zehao Xiao", "Cees G. M. Snoek"], "abstract": "Machine learning algorithms have achieved remarkable success across various disciplines, use cases and applications, under the prevailing assumption that training and test samples are drawn from the same distribution. Consequently, these algorithms struggle and become brittle even when samples in the test distribution start to deviate from the ones observed during training. Domain adaptation and domain generalization have been studied extensively as approaches to address distribution shifts across test and train domains, but each has its limitations. Test-time adaptation, a recently emerging learning paradigm, combines the benefits of domain adaptation and domain generalization by training models only on source data and adapting them to target data during test-time inference. In this survey, we provide a comprehensive and systematic review on test-time adaptation, covering more than 400 recent papers. We structure our review by categorizing existing methods into five distinct categories based on what component of the method is adjusted for test-time adaptation: the model, the inference, the normalization, the sample, or the prompt, providing detailed analysis of each. We further discuss the various preparation and adaptation settings for methods within these categories, offering deeper insights into the effective deployment for the evaluation of distribution shifts and their real-world application in understanding images, video and 3D, as well as modalities beyond vision. We close the survey with an outlook on emerging research opportunities for test-time adaptation. A list of test-time adaptation literature is provided at https://github.com/zzzx1224/Beyond-model-adaptation-at-test-time-Papers.", "sections": [{"title": "1 INTRODUCTION", "content": "ACHINE learning has achieved a remarkable success in a wide variety of disciplines, use cases and applica-tions. For instance, AlexNet [159] demonstrated that training deep convolutional networks at scale outperforms traditional vision feature-engineering. LSTM [115] advanced sequential prediction by effectively handling long-term dependencies. Transformers [312] revolutionized natural language process-ing with context-aware understanding. AlphaGo [280] show-cased the ability of reinforcement learning to master strategic games. Despite these spectacular advances, many machine learning algorithms continue to assume that their training and test data distributions are similar. Naturally, such a strong assumption easily falters in real-world scenarios [275], [390]. In practice, complex and unpredictable discrepancies can arise between training and test data distributions, such as noisy sensory recordings during inference, an abrupt change in weather conditions, evolving user requirements, or entirely new objectives unforeseen at training time. This issue becomes even more pronounced as machine learning algorithms are being applied in more and more practical application-oriented contexts. When the test data distribu-tions start to differ from those encountered during training, an adaptation at test-time would mitigate machine learning failure. In this paper, we provide a comprehensive survey of test-time adaptation studies.\nThe notion of test-time adaptation was initially posed by Vladimir Vapnik, who stated: \"When solving a problem of interest, do not solve a more general problem as an intermediate step. Try to get the answer that you really need but not a more general one\u201d [308], [309]. A manifestation of this notion are local learning methods [23], [371] that train algorithms on the neighbors of each test sample before making their prediction. Alternatively, transductive learning [11], [46], [78], [140] was investigated that incorporates test data information into model training. With the advances of deep learning methods, test-time adaptation [293], [318] has recently emerged as a new learning method for adaptation at test time. As the name suggests, test-time adaptation achieves the adaptation procedure along with inference to reduce the negative impact of distribution shifts between the training and test data [319]. During training, the approaches develop general models solely based on the training data distributions, without having access to the target test data. At test time, they focus on adjusting the trained model parameters or test data representations to bridge the gap between the training and test data distributions, thereby enhancing the model's performance and robustness on specific test samples. By facilitating adaptation in conjunction with inference in an online manner, test-time adaptation is computationally efficient and well-suited for scenarios involving online or limited test data, which are so common in real-world use cases.\nThe machine learning landscape is witnessing a growing interest in test-time adaptation algorithms, attributed to their ability to handle unseen distribution shifts during test phases and the relatively flexible demands on target data. Figure 1 illustrates this trend, with research efforts on test-time adaptation emerging around 2020, with the pioneering works of Sun et al. [293] and Wang et al. [319], and expanding in depth and breadth each year. Two recent surveys also cover test-time adaptation. Liang et al. [183] provided a comprehensive overview of test-time adaptation together with source-free domain adaptation. Wang et al. [331] survey online test-time adaptation methods with their evaluations on Vision Transformers. Unlike previous surveys, which primarily focus on model adaptation techniques,"}, {"title": "2 BACKGROUND", "content": "We first provide the necessary background knowledge on test-time adaptation. In Section 2.1, we introduce the problem definition of distribution shifts and test-time adaptation, as well as the notation coventions we use throughout this survey. We then discuss the similarities and differences of test-time adaptation with related learning frameworks in Section 2.2.\n2.1 Problem definition\nNotations. We start from one or more source distributions N, defined during training as $S = \\{p(x_{s_i},y_{s_i})\\}_{i=1}^N$, and the test-time target distribution $T = p(x_t, y_t)$, both in the joint space $X \\times Y$. Here $X$ denotes the input (feature) space and $y$ denotes the label space, $(x_s, y_s)$ and $(x_t, y_t)$ denote the data-label pairs sampled from the source and target distributions, respectively. The joint distribution $p(x, y)$ of source and target data can be different. In the context of test-time adaptation, only the source distribution $S$ is accessible during training. At test time, only unlabeled data $x_t$ from the target distribution is available for adaptation and inference. The learned function or model is defined as $f: X \\rightarrow Y$ with parameters $\\theta$. We represent the source-specific model as $f_{\\theta_s}$ and the target model as $f_{\\theta_t}$.\nDistribution shifts. Test-time adaptation focuses on address-ing distribution shifts in machine learning algorithms. The fundamental problem is the incongruence between the target and source distributions, i.e.,\n$\\begin{equation}p(x_t, y_t) \\neq p(x_s, y_s).\\end{equation}$\nThe discrepancy indicates that a source-trained model $f_{\\theta_s}$ can falter when applied to data from the target distribution, leading to predictions that are not as precise and reliable as expected.\nBy decomposing the joint distribution $p(x, y)$ into $p(x,y) = p(x)p(y|x) = p(y)p(x|y)$, the distribution shifts can be further categorized into four common types, covariate shifts, label shifts, conditional shifts, and concept shifts [201], [338]. Covariate shifts ($p(x_t) \\neq p(x_s), p(y_t|x_t) = p(y_s|x_s)$) [271], [293] assume the distribution shifts occur only on the input space $p(x)$, while the labels given the input features remain the same. In contrast, label shifts ($p(y_t) \\neq p(y_s), p(x_t|y_t) = p(x_s|y_s)$) focus on the shifts in label space $p(y)$, by assuming the same label-conditioning data distribution. Concept shifts ($p(x_t) = p(x_s), p(y_t|x_t) \\neq p(y_s|x_s)$) denote differences in the conditional distributions with the same input distribution, such as noisy labels or different annotation methods [201]. Conditional shifts ($p(y_t) = p(y_s), p(x_t|y_t) \\neq p(x_s|y_s)$) assume that the label space remains the same, but the distribution of the input samples varies according to the labels, e.g., subpopulation problems [200], [270]. The large majority of test-time adaptation methods focus on covariate shifts [237], [271], [293], [319], with the distribution shifts coming from the discrepancies in the input data. Recently, approaches started to investigate label shifts [249], [291], [398] and some other distribution shifts [173], [338]. Additionally, some methods have investigated joint-shifts that combine both covariate and label shifts [249], [338].\nTest-time adaptation. Given labeled source distributions $S$ and unlabelled target distributions $T$, test-time adaptation aims to train a model $f_{\\theta}$ only on the source distributions $S$ and achieve adaptation with the source-trained model $f_{\\theta_s}$ and the target data $x_t$ to make predictions on $x_t$ after adaptation. The model parameters $\\theta_s$, target data $x_t$, and even prompts in transformer-based models can be adapted. Current test-time adaptation approaches adapt each or a combination of these components. Further note that test-time adaptation is achieved along with inference, in an online or batch-wise manner, without having access to large amounts of target data at each test step.\n2.2 Related problems\nTest-time adaptation draws inspiration from several related machine learning problems that attack distribution shifts,"}, {"title": "3 WHAT TO ADAPT", "content": "Given a source-trained model and new target data distribu-tions that appear at test time, the goal of test time adaptation is to fit the source-trained model to the unseen target data. In this section, we categorize the existing methods from the literature into five main categories, based on what they adapt: the model, the inference, the normalization, the sample, or the prompt. In the following sections, we present the details of each category along with an overarching equation and the corresponding methods. The adaptation component in each equation is highlighted in red.\n3.1 Model adaptation\nAn intuitive way of test-time adaptation is by adjusting the source-trained model parameters $\\theta_s$ according to the target data $x_t$. By doing so, obtaining the target-specific model parameters $\\theta_t$. Most model adaptation methods adjust their source-trained model by fine-tuning the model parameters through unsupervised losses on the target test data, see Figure 3.\nGiven an unsupervised loss $L(x_t; \\theta)$ on target data, the target-specific model $\\theta_t$ and the prediction $y_t$ is obtained by\n$\\begin{equation}\\theta_t = \\min_\\theta L(x_t; \\theta_s) = \\theta_s +\\eta \\nabla_\\theta L(x_t; \\theta_s), \\quad y_t = f_{\\theta_t}(x_t),\\end{equation}$\nwhere $\\nabla_\\theta L(x_t; \\theta_s)$ denotes the parameter gradient of un-supervised loss $L$ and $\\eta$ denotes the learning rate of backpropagation. The adaptation of $\\theta$ requires iterative optimization. We categorize the model adaptation methods further according to their unsupervised loss functions.\nAuxiliary self-supervision. Sun et al. [293] pioneered model adaptation by self-supervision. Their test-time training pro-cedure introduces an auxiliary self-supervised task with the loss function $L^a(x)$. The model parameters $\\Theta$ are split into the main-task parameters $\\Theta^m$ and the shared parameters $\\theta^s$. The auxiliary task also has its own task-specific parameters $\\theta^a$. The standard empirical risk minimization $\\min_{\\Theta} L_s (x_s, y_s; \\Theta)$ during source training is then changed to:\n$\\begin{equation}\\min_{\\theta^s,\\theta^m, \\theta^a} L^m (x_s, y_s; \\theta^s, \\theta^m) + L^a (x_s; \\theta^s, \\theta^a),\\end{equation}$\nwhere $L^m$ and $L^a$ denote the loss function of the main task and auxiliary task, respectively. After training, the source model consists of $\\theta = (\\theta^s, \\theta^m, \\theta^a)$. At test time, the main-task model $\\theta^m$ is fixed while the shared model $\\theta^s$ is fine-tuned by minimizing the auxiliary task loss on the target data $x_t$, which is formulated as:\n$\\begin{equation}\\theta^s = \\min_{\\theta^s} L^a (x_t; \\theta^s, \\theta^a).\\end{equation}$\nFinally, the model makes predictions on the target data using the target model $\\theta_t = (\\theta^s, \\theta^m)$. Since the model is optimized only by the auxiliary task during training, it is necessary to avoid model overfitting on the auxiliary task. Therefore, the training stage is altered to jointly optimize the model by the main task and the auxiliary task, expecting to make the adaptation of the auxiliary task compatible with the main task [202], [293].\nTo further enhance model adaptation by auxiliary self-supervision, several auxiliary tasks have been proposed.\nModel adaptation by auxiliary self-supervision requires careful design of the auxiliary tasks and their coupling with the main task to avoid overfitting. Often necessitating both main and auxiliary losses, leading to an alternating training stage with extra computational costs. As an alternative, entropy minimization of model predictions is proposed to achieve test-time model adaptation without any auxiliary task.\nEntropy minimization. Model adaptation by entropy min-"}, {"title": "6", "content": "imization does not introduce any change in the source-training procedure, but only adapts the source-trained model to the target data at test time. Wang et al. [319] propose Tent, which introduces the concept of fully test-time adaptation. It fine-tunes the source-trained model parameters on target distributions directly by minimizing the entropy of the model predictions.\n$\\begin{equation}\\theta_t = \\min_\\theta L(x_t, \\theta) = \\min_\\theta H(\\hat{y}_t),\\end{equation}$\nwhere the entropy $H(\\hat{y}_t)$ is conducted on the source-model predictions on the target sample $\\hat{y}_t = f_{\\theta_s}(x_t)$. Different from auxiliary self-supervised methods that split the main task parameters into shared parameters $\\theta^s$ and specific parameters $\\theta^m$ and only update $\\theta^s$ at test time, Tent can adjust all parameters $\\theta$ by test data. Hence, the loss function and update parameters directly relate to the main task.\nInspired by Tent, many methods enhance entropy mini-mization for better adaptation in specific use cases [38], [216], [237], [375]. MEMO [375] augments each target sample and achieves adaptation by minimizing the marginal entropy of the predictions for different augmentations. EATA [237] finds that adaptation on high entropy test samples can hurt performance. They propose adaptive entropy minimization to adapt low-entropy samples only. DeYO [168] introduces a probability difference to measure the difference between predictions before and after augmentation as an auxiliary metric for sample selection and weighting. SoTTA [91] removes the low-confidence inputs and large gradients for robust adaptation on noisy test data. Lee et al. [167] proposes Bayesian filtering to combine test and training during online model adaptation. CMF [166] utilizes a Kalman filter to strike a balance between model adaptation and information retention. Choi et al. [44] complement the entropy minimization with a mean entropy maximization, as well as an auxiliary classification task based on the nearest source prototype classifier. Lee et al. [170] select target samples before entropy minimization, where the samples are filtered out if the adapted confidence is lower than the original one. DomainAdaptor [372] introduces generalized entropy minimization, which includes a temperature scaling for the original entropy loss. AETTA [171] proposes prediction dis-agreement with dropout inferences for more robust accuracy estimation of test-time adaptation methods. STAMP [362] achieves adaptation over a stable memory bank with self-weighted entropy minimization. Bar et al. [18] matches the distribution of test entropy values to the source ones to adaptively update model parameters.\nEntropy minimization enables an unsupervised model adaptation that is highly related to the main task, without alternating the training stage and the need to design auxiliary self-supervised tasks. However, due to distribution shifts, the source-model prediction on target data can be incorrect. In this case, entropy minimization methods may lead to error accumulation during model adaptation.\nPseudo-labeling. Besides entropy minimization, pseudo-labeling [77], [165] is also widely used for model adaptation [264], [367]. Intuitively, pseudo-labeling is similar to entropy minimization, as both tend to optimize the source-trained model by maximizing the confidence of the model predic-tions on the target data. The key difference is that pseudo-"}, {"title": "5", "content": "labeling provides more explicit supervision and is easier to refine. Pseudo-labeling model adaptation is formulated as:\n$\\begin{equation}\\tilde{y}_t = \\text{argmax}(\\hat{y}_t = f_{\\theta_s} (x_t)), \\quad \\theta_t = \\min_\\theta L_{ce} (x_t, \\tilde{y}_t; \\theta),\\end{equation}$\nwhere $\\tilde{y}_t$ denotes the pseudo-labels obtained from the source model predictions $\\hat{y}_t$. $L_{ce}$ denotes the cross-entropy loss between model predictions and pseudo-labels. The pseudo-labels $\\tilde{y}_t$ can be either hard (i.e., one-hot) or soft (i.e., continuous). Due to distribution shifts between the source and target data, the pseudo-labels of the unseen target data can be inaccurate. Therefore, it is necessary to enhance and refine the pseudo-labels [184], [209], [324], [329], [360].\nBased on teacher and student networks, Rusak et al. [264] explore hard pseudo-labeling, soft pseudo-labeling, and entropy minimization. The work further proposes robust pseudo-labeling, which replaces the cross-entropy loss $L_{ce}$ in eq. (6) with the generalized cross-entropy loss to solve the problem of training stability and hyperparameter sensitivity in the common pseudo-labeling methods. Wang et al. [318] initialize their student model by contrastive learning from scratch on the target data and fine-tune it with the pseudo-labels generated by the teacher model. Differently, TeST [281] initializes their student model by the source-trained model and fine-tunes it with the pseudo-labels and entropy minimization. TeSLA [298] introduces flipped cross-entropy that uses the soft pseudo-labels from the teacher network together with an entropy maximization of the predictions.\nNaturally, the pseudo-labels can be refined with the in-formation of the target samples [34], [134], [324]. CTTA [324] generates pseudo-labels by a weighted-averaged teacher model together with the augmentation-averaged pseudo-labels. Chen et al. [34] propose Contrastive TTA with an online pseudo-label refinement that aggregates knowledge of the nearby target samples and fine-tunes the model with both pseudo-label cross-entropy and self-supervised contrastive learning. TAST [134] also generates pseudo-labels using the nearest neighbors from a set composed of previous test data. In addition to refining pseudo-labels with neighboring target samples, Litrico et al. [189] further reweight the loss based on the reliability of the pseudo-labels. Ambekar et al. [7] consider the uncertainty of the pseudo-labels by formulating adaptation as a probabilistic inference problem where pseudo-labels act as latent variables. Goyal et al. [95] find that different loss functions perform best for the classifier with different training losses. They propose conjugate pseudo-labels to specify a good adaptation loss at test time for any training loss function. PROGRAME [289] constructs a graph using prototypes and test samples to generate more reliable pseudo-labels. WATT [242] averages the updated model parameters in different optimization steps to facilitate test-time adaptation.\nSimilar to entropy minimization, pseudo-labeling meth-ods also suffer from error accumulation due to inaccurate pseudo-labels. Therefore, obtaining more accurate labels and dealing with noisy pseudo-labels is essential. Moreover, supervisions like entropy minimization and pseudo-labeling are mainly conducted on the output space. Therefore, the methods are often proposed for specific tasks like classifica-tion. Naturally, there are also methods that operate on the intermediate feature space."}, {"title": "6 Inference adaptation", "content": "Feature alignment. Common unsupervised objective func-tions on the feature space are feature alignment and consis-tency [76], [141], [176], [188], [223], [285], [287], [325]. The model optimized by the feature alignment or consistency objective functions is formulated as:\n$\\begin{equation}\\theta_t = \\min_\\theta L_{align} (z_t, z_a; \\theta),\\end{equation}$\nwhere $z_t$ and $z_a$ denote the feature representations of the test sample $x_t$ and auxiliary samples $x_a$. The auxiliary samples $x_a$ can be obtained from the neighboring target data [325], source data [285], and augmented data [76], [188], [231]. $L_{align}$ denotes the alignment or consistency objective functions, such as the MSE loss or KL divergence. CAFA [141] designs a class-aware feature alignment function to learn test data in a class-discriminative manner. Inspired by unsupervised domain adaptation methods, Su et al. [285] propose anchored clustering to align the source and target clusters and improve the clustering by pseudo-label filtering and iterative updating at test time. Fleuret et al. [76] introduce an augmentation consistency loss to enforce consistent predictions of the augmented target samples. Kang et al. [145] regularize the adaptation by matching the proposed proxies between training and test data. TIPI [231] introduces a trans-formation invariance regularizer as the objective function for test-time adaptation. Wang et al. [330] proposes distribution alignment to guide the test feature distribution back to the source for continual test-time adaptation. Compared with entropy minimization and pseudo-labeling that are specific for classification tasks, feature alignment can be used for a wider range of tasks beyond classification. However, it can also lead to suboptimal adaptation since the decoder of the model is difficult to adapt.\nDiscussion. Model adaptation methods achieve considerable improvements in out-of-distribution generalization with enough unlabeled test data and computational resources. However, the methods also have some challenges and weaknesses. Since the methods need incremental model updates, model adaptation at test time is computationally demanding [83], [341]. Moreover, the methods are known to be unstable in various test-time scenarios [237], [238], [324]. For example, model adaptation may fail or even harm the model robustness when experiencing continually changing distributions at test time [25], [324], or when experiencing a mixture of multiple distributions [341], or for very small batch sizes [238], [375]. Wu et al. [335] further find that"}, {"title": "2 Inference adaptation", "content": "model adaptation methods are vulnerable to malicious data. To address these problems, recent methods focus on new settings like continual test-time adaptation [25], [237], [324] and limited-data adaptation [238], [375], which we will further discuss in Section 5.\n3.2 Inference adaptation\nTo avoid the model adaptation requirements of having a large number of target samples and iterative compute, inference adaptation methods estimate model parameters with a small number of samples using just a feedforward pass at test time [63], [131], [341], as illustrated in Figure 4. For these methods the target-specific model parameters are obtained by:\n$\\begin{equation}\\theta_t = \\phi(x_t, \\theta_s), \\quad y_t = f_{\\theta_t}(x_t),\\end{equation}$\nwhere $\\phi$ denotes the model-inference module. The model inference procedure usually requires the information from the target data $x_t$ and source-trained model parameters $\\theta_s$. Since directly inferring the model parameters is difficult, it may require additional information from the target data. According to the amount of required test data, we divide them into batch-wise and sample-wise inference methods.\nBatch-wise inference. Batch-wise inference adaptation re-quires batches of target samples to update the model at test time. To infer their target-specific model, Dubey et al. [63] introduce a domain-specific function that generates domain embeddings through an MLP network. Differently, T3A [131] proposes a parameter-free inference module. The method utilizes averaged features to online adjust the classifiers according to pseudo-labels at test time. To avoid error accumulation, they filter the unreliable pseudo-labeled data with high entropy. AdaNPC [379] also achieves a non-parametric inference module that infers the classifier by storing the target information in an online updated memory at test time. The online update method is also utilized in continuous-time Bayesian neural network [124], which infers model parameters by a particle filter differential equation. Zhou et al. [397] introduce distribution normalization on multimodal features to improve similarity measurement between modalities at test time. Batch-wise inference adap-tation avoids the computational cost of backpropagation for obtaining target-specific models. However, it requires batches of target samples to be sampled from the same distribution, which may be difficult to obtain and assure in real-world applications."}, {"title": "3 Sample-wise inference", "content": "Sample-wise inference. To achieve inference adaptation with fewer target samples, Xiao et al. [341] propose to generate a sample-specific classifier for each target sample, where the inference module $\\phi$ is an MLP network trained by variational inference. DIGA [326] infers a prototypical classifier for each instance in a semantic segmentation task. Sun et al. [294] meta-learn the module $\\phi$ for inferring instance-specific model parameters. VoP [151] introduces variational inference in the inference module to achieve personalized test-time adaptation. They estimate the model parameters on the fly based on the personality of a small amount of personal data. RNA [355] trains a small inference network to predict parameters for modulating the original features. FedIns [72] obtains the instance-adaptive model by adaptively selecting the best-matched subsets of learned feature pools in federated learning. TDA [147] introduce dynamic caches that maintain few-shot test features and pseudo labels to enhance the model prediction at test time. BoostAdapter [377] leverages a light-weight key-value memory. DPE [369] infers prototypes from both textual and visual modalities from CLIP and optimizes prototypes using learnable residual parameters for each test sample. Sample-wise inference adaptation reduces the requirement for large amounts of data, making it much more data-efficient at test time. However, the sample-wise inference is more difficult due to the limited amount of information in one sample, which usually requires specifically designed training strategies like meta-learning [294], [341].\nDiscussion. Compared to model adaptation, inference adap-tation methods are more efficient since they achieve model generation and model updates by a single feedforward pass of the target data, without backpropagation. However, most inference adaptation methods require additional designs of the source-training stage, which may be not as convenient as model adaptation methods. Moreover, due to the large number of network model parameters, the model inference methods often can only focus on a subset of the parameters, e.g., the classifier layer [63], [131], [341]. As different distri-bution shifts are influenced by different network layers [173], it is difficult for these methods to achieve good adaptation."}, {"title": "3 Normalization adaptation", "content": "3 Normalization adaptation\nSince adjusting model parameters can be computationally expensive and training dependent, normalization adaptation methods focus on adjusting only the normalization statistics of the widely used batch normalization layers [130]. batch normalization is known to reduce the internal covariate shift by normalizing the input features of subsequent layers by $\\hat{x} = \\frac{x - \\mu_s}{\\sqrt{\\sigma_s^2 + \\epsilon}}$, where $\\mu_s$ and $\\sigma_s^2$ are the expectation and variance of the mini-batch of samples during source training. Since the target data $x_t$ and source statistics $\\mu_s, \\sigma_s$ are mismatched, the normalization adaptation methods estimate the target statistics $\\mu_t, \\sigma_t$ to normalize the target features while fixing the source-trained model parameters, as shown in Figure 5. The adaptation process is formulated as:\n$\\begin{equation}\\mu_t, \\sigma_t = h(x_t, \\mu_s, \\sigma_s), \\quad x' = \\frac{x_t - \\mu_t}{\\sqrt{\\sigma_t^2 + \\epsilon}},\\quad y_t = f_{\\theta_s} (x'),\\end{equation}$\nwhere $h(.)$ denotes the target statistics estimation module. $h(.)$ can be either parametric or non-parametric, and the inputs of the module are usually the target features $x_t$, sometimes together with the source statistics $\\mu_s, \\sigma_s$.\nSome model adaptation methods from Section 3.1 also change the batch normalization statistics to improve per-formance, e.g., Tent and some subsequent methods [286], [319], [349] estimate the target mini-batch statistics during fine-tuning the affine parameters of the batch normalization layers. In this section, we focus on methods that only adjust the normalization statistics, without altering their model parameters. We divide current methods into three types according to the target statistics estimation method: calculating target statistics directly, combining source and target statistics, and target statistics inference.\nTarget statistics. To make the target data fit source-trained model parameters, the target features $x_t$ at each layer of the deep neural network need to be normalized at test time, similarly to the source features during training. One of the most straightforward methods is to directly estimate the statistics of the target data in the batch normalization layers at test time [179], [228]. Nado et al. [228] propose prediction-time batch normalization to recompute the Batch Norm statistics, i.e., $\\mu_t$ and $\\sigma_t^2$ for each test batch. The method enhances calibration under covariate shifts. Kaku et al. [142] propose adaptive normalization that also utilizes the target batch normalization statistics to estimate the feature statistics of each test instance. In addition to recomputing batch normalization statistics, ARM [376] further changes the training stage by sampling each training batch from the same domain. This mimics the statistics computation at test time and boosts the performance by meta-learning. MedBN [245] replaces the means with medians when computing BN statistics to defend data poisoning attacks."}, {"title": "8", "content": "n is too small. The combined statistics is obtained by $\\mu = \\frac{M_s\\mu_s + M_t\\mu_t}{M_s+M_t}$, $\\sigma^2 = \\frac{M_s\\sigma_s^2 + M_t\\sigma_t^2}{M_s+M_t}$. Similarly, You et al. [359] propose \u03b1-BN to calibrate the batch normalization statistics as $\\mu = \\alpha \\mu_t +(1-\\alpha)\\mu_s$, $\\sigma = \\alpha \\sigma_t+(1- \\alpha)\\sigma_s$, where \u03b1 is also a hyperparameter. To extend the method to the case with only a single target sample, SITA [148] obtains $\\mu_t$ and $\\sigma_t$ of the aforementioned equations from various augmentations of the target sample. They achieve a hyperparameter-free combination algorithm by selecting prior hyperparameter values using the prediction entropy. TEMA [288] proposes an exponential moving average method to combine training and test statistics. TTN [186] converts the hyperparameter to the learnable parameter. The parameter \u03b1 is initialized by a gradient distance score and optimized during the proposed post-training phase. At test time, the batch normalization statistics are obtained by $\\mu = \\alpha \\mu_t + (1 - \\alpha)\\mu_s$, $\\bar{\\sigma}^2 = \\alpha \\sigma_t^2 + (1 - \\alpha)\\sigma_s^2 + \\alpha (1 - \\alpha)(\\mu_t - \\mu_s)^2$ with the learned $\\alpha \\in [0, 1]$.\nTo achieve a stable and robust test-time adaptation, some methods estimate their batch normalization statistics in an on-line manner at test time [123], [221], [349]. Like during train-ing, these methods estimate the statistics through a moving average, e.g., $\\bar{\\mu}_{k+1} = (1-\\lambda)\\mu_k+\\lambda\\mu_t$, $\\bar{\\sigma}_{k+1} = (1-\\lambda)\\sigma_k+\\lambda\\sigma_t$, where the superscripts represent the time steps and $\\mu_t$, $\\sigma_t$ denote the statistics estimated by the current test batch. GpreBN [349] further combines the moving-averaged target statistics with the moving-averaged source statistics obtained during training to stabilize the estimation. For stable and fast convergence, Mirza et al. [221] propose an adaptive moving average of normalization statistics that calculates specific statistics for each target sample. MemBN [146] introduces statistics memory queues to store the test batch statistics and accumulate the latest test batch. Combining the source and target statistics makes a trade-off between the target adap-tation and the preservation of source discriminative ability. It also relaxes the requirements of the amount of target data. However, the hyperparameters for the combination need to be set according to different cases, which is inconvenient and may hurt robustness in real-world applications."}, {"title": "3 Statistics inference", "content": "Replacing the source statistics directly with the target ones can alleviate the covariate shifts between source and target distributions with sufficient target samples. However, batch normalization always requires large batch sizes or moving averages of large numbers of samples for accurate estimation of the statistics. When the target samples are insufficient or from various distributions, the estimated target statistics will not be able to represent the target distribu-tion well, leading to unstable prediction and performance degradation. UnMix-TNS [299", "142": "replace the batch normalization with instance normalization [305", "126": "task-specific [26", "273": "information. Gong et al. [90"}, {"273": "."}]}