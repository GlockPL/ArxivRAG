{"title": "LLM-BIP: Structured Pruning for Large Language Models with Block-Wise Forward Importance Propagation", "authors": ["Haihang Wu"], "abstract": "Large language models (LLMs) have demonstrated remarkable performance across various language tasks, but their widespread deployment is impeded by their large size and high computational costs. Structural pruning is a prevailing technique used to introduce sparsity into pre-trained models and facilitate direct hardware acceleration during inference by removing redundant connections (structurally-grouped parameters), such as channels and attention heads. Existing structural pruning approaches often employ either global or layer-wise pruning criteria; however, they are hindered by in-effectiveness stemming from inaccurate evaluation of connection importance. Global pruning methods typically assess component importance using near-zero and unreliable gradients, while layer-wise pruning approaches encounter significant pruning error accumulation issues. To this end, we propose a more accurate pruning metric based on the block-wise importance score propagation, termed LLM-BIP. Specifically, LLM-BIP precisely evaluates connection importance by gauging its influence on the respective transformer block output, which can be efficiently approximated in a single forward pass through an upper bound derived from the assumption of Lipschitz continuity. We evaluate the proposed method using LLaMA-7B, Vicuna-7B, and LLaMA-13B across common zero-shot tasks. The results demonstrate that our approach achieves an average of 3.26% increase in accuracy for common reasoning tasks compared to previous best baselines. It also reduces perplexity by 14.09 and 68.76 on average for the WikiText2 dataset and PTB dataset, respectively.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have demonstrated exceptional performance across various language tasks such as question answering (Sakaguchi et al. 2019; Zellers et al. 2019; Clark et al. 2018) and language generation (Wei-Lin Chiang et al. 2023; Touvron et al. 2023). However, this notable performance is accompanied by significant computational requirements and model scale, leading to high memory costs and latency. To mitigate these challenges, techniques including knowledge distillation (Sun et al. 2019, 2020), quantization (Frantar et al. 2023; Dettmers et al. 2023) and network pruning (Zhuang et al. 2018; Xia et al. 2023) have been employed to compress LLMs. Among these methods, structural network pruning has emerged as a particularly effective method in compressing models and reducing latency by removing redundant structurally grouped parameters, such as attention heads and channels, from a pre-trained network, enabling direct inference speedup without necessitating specialized hardware support (Fang et al. 2023; Yang et al. 2023). Existing structural pruning approaches typically prune networks either globally (Ma, Fang, and Wang 2023; Zhang et al. 2023; Yu et al. 2018; Frantar and Alistarh 2023) or layer-wisely (Sun et al. 2024; Frantar and Alistarh 2023).\nGlobal structural pruning approaches (Figure 1) prune grouped parameters with minor impacts on the network's final output (Ma, Fang, and Wang 2023; Zhang et al. 2023; Tang et al. 2022). For some global pruning approaches, they evaluate the importance of parameters based on their gradients, and remove structures with small gradients (Ma, Fang, and Wang 2023; Zhang et al. 2023). However, the gradient criterion can be unreliable due to small scales in well-trained models (Cun et al. 1989; Frantar and Alistarh 2023), and also entails substantial computation and memory costs of these gradients particularly in LLMs. With the Lipschitz continuity assumptions on network components (e.g., activation function, linear projection layers), other global pruning approaches (Tang et al. 2022; Yu et al. 2018) assess neuron importance by propagating neuron importance scores from the final layer throughout the network, and prune structures with small importance scores. By pruning structures deemed unimportant to the network output, the network's function may be preserved effectively. However, Lipschitz continuity assumption is proved to be invalid for common dot-product self-attention module in LLM transformer blocks (Kim, Papamakarios, and Mnih 2021).\nConversely, layer-wise pruning approaches remove grouped parameters with minimal impacts on the current layer's output (Sun et al. 2024; Frantar and Alistarh 2023), typically requires no backpropagation. SparseGPT (Frantar and Alistarh 2023) utilizes optimal brain surgeon to identify and prune unimportant weights. The remaining weights are then updated to reconstruct the output of the current layer. Wanda (Sun et al. 2024) eliminates unimportant weights based on their magnitude and input feature norm of the current layer. By pruning parameters layer-wisely, the pruning efficiency is promising as only one forward pass is needed (Sun et al. 2024). However, layer-wise pruning can suffer from rapid error accumulation issues across layers (Xu et al. 2024).\nTo address these issues, we propose an effective and efficient block-wise importance score propagation strategy for pretrained LLMs, termed LLM-BIP. LLM-BIP aims to preserve the response of a transformer block as much as possible after pruning channels and attention heads in this block. With binary vectors representing pruning decisions, we formulate this objective as a mathematical optimization problem: searching binary vectors to minimize the l\u2081 distance between the output produced by the original network and a pruned network. As the searching cost is high due to the large searching space, we introduce a relaxation of this objective and derive an upper bound for this 11 distance. Remarkably, minimizing this upper bound yields a straightforward metric for evaluating channel importance. Leveraging this metric, we prune network channels within the Feed-Forward Network module, and attention heads of the self-attention module based on their importance scores aggregated from the importance scores of corresponding output channels. Notably, this also allows for the efficient pruning of the network within a single forward process. Our approach enhances pruning accuracy by circumventing the reliance on inaccurate gradients (Cun et al. 1989; Frantar and Alistarh 2023) and the invalid Lipschitz continuity assumption on self-attention modules (Kim, Papamakarios, and Mnih 2021), which are employed by global pruning methods. Additionally, our method evaluates channel importance on a broader scale, mitigating the rapid error accumulation issue associated with existing layer-wise pruning approaches (Sun et al. 2024; Frantar and Alistarh 2023).\nWe assess the efficacy of our proposed method on LLaMA-7B, Vicuna-7B, and LLaMA-13B using standard zero-shot datasets. Our experiments demonstrate that compared to Wanda (Sun et al. 2024) and LLM-Pruner (Ma, Fang, and Wang 2023), our method reduces perplexity by 14.09 and 68.76 on average for WikiText2 dataset and PTB dataset respectively. It also yields an average 3.26% increase in accuracy for common reasoning tasks.\nIn summary, our contributions are:\n\u2022 We propose a simple and effective block-wise structured pruning criterion, aimed at diminishing error accumulation and eradicating reliance on gradients. By deriving an upper bound to approximate the impact of the attention heads and FFN channels on the block output, we enable a single forward pass for efficient importance estimation.\n\u2022 Our approach outperforms existing structured pruning methods across various models, achieving an average perplexity reduction of 3.67 and 88.63 at 20% and 50% sparsities on WikiText2 and PTB datasets. We also observe an average accuracy improvement of 4.49% and 2.18% at 20% and 50% sparsities on seven common-sense reasoning datasets."}, {"title": "Related Work", "content": "Existing pruning methods can be categorized into unstructured pruning (Dong, Chen, and Pan 2017; Lee, Ajanthan, and Torr 2019; Singh and Alistarh 2020; Evci et al. 2020;"}, {"title": "Method", "content": "A transformer block $f(X)$ with input $X$ consists of a multi-head self-attention (MSA) module and a Feed-Forward Network (FFN) module:\n$h_i(X) = softmax(\\frac{QK^T}{\\sqrt{d}})V_i$ (1)\n$MSA(X) = Concat[h_1, ..., h_n]W^O$ (2)\n$X' = MSA(X) + X,$ (3)\n$FFN(X') = \\sigma(X'W^U)W^D,$ (4)\n$f(X) = FFN(X) + X',$ (5)\nwhere X and X' are the input to MSA and FFN modules respectively. $Q_i$, $K_i$, and $V_i$ denote the query, key, and value of the i-th attention head $h_i$. $W^U$, $W^D$ and $W^O$ are the weight matrices of FFN up projection layer, FFN down projection layer and MSA output projection layer, respectively. $\\sigma$ is an activation function. d is the embedding dimension of input tokens, and n is the number of attention heads in MSA.\nDependency-aware structured pruning. In structured pruning, connected components should be pruned together to maintain the model's integrity and functionality. For the FFN module, we prune channels in the FFN hidden layer based on their channel importance scores $s^f$, where s denotes a vector of importance scores and superscript F denotes channels in the FFN hidden layer. The connected weights in FFN up and gate projection layers are pruned accordingly. For the MSA module, we prune the attention heads together with the connected channels in query, key, value, and output projection layers. To obtain the importance score of one attention head, we sum up the importance scores $s^h$ of corresponding output channels from this head, where superscript H denotes output channels of attention heads. We then prune the attention heads with low attention head scores. In the subsequent section, we elaborate on the calculation of channel importance scores $s^h$ and $s^F$.\nOur goal is to minimize the impact of the pruned channels on the transformer block output. Mathematically, it aims to predict two binary masks $\\hat{s}^H$ (0 for pruning and 1 for keeping) and $\\hat{s}^F$ such that the reconstruction error $\\epsilon$ between the output $f(X)$ of the original unpruned transformer block and the output $f(X, \\hat{s}^H,\\hat{s}^F)$ produced by the pruned block is minimized under the constraint of target sparsity ratio r:\n$\\min \\epsilon = |f(X) - f(X, \\hat{s}^H,\\hat{s}^F)|,$\n$s.t. ||\\hat{s}^H||_1 = (1 - r) * N_H, \\hat{s}^H \\in {0, 1}^{N_H},$\n$||\\hat{s}^F||_1 = (1 - r) * N_F, \\hat{s}^F \\in {0, 1}^{N_F}$ (6)\nwhere $N_H$ and $N_F$ are the number of channels for attention heads and FFN hidden layer respectively. $||\\cdot||_1$ is the number of non-zero elements. The reconstruction error $\\epsilon$ is defined"}, {"title": "Pruning Metric", "content": "Directly optimizing Eq. (6) is still computationally expensive. This is because there are often several thousands of channels in LLMs, resulting in a large search space. Additionally, for each searched s, one forward pass through the transformer block is needed to evaluate the solution. To reduce computation cost, we derive an upper bound of this objective in Eq. (6) and minimize it with only one forward pass through the transformer block. From Eqs. (4), (5) and (6), we have:\n$|f(X) - f(X, s^F)|$\n$= |(FFN(X') + X') - (FFN(X', s^F) + X')|$\n$= |\\sigma(X'W^U)W^D - \\sigma(s^F \\odot X'W^U)W^D|$\n$\\leq |\\sigma(X'W^U) - \\sigma(s^F \\odot X'W^U)| \\cdot |W^D|$\n$\\leq C_o |X'W^U - s^F \\odot X'W^U | \\cdot |W^D|$\n$= C_o |1 - s^F | \\cdot |X'W^U| \\cdot |W^D|$\n$= C_o \\sum_{j=1}^{N_F} (1 - s_j^F) |X_j'W^U| \\cdot |W_j^D|$, (7)\nwhere $\\cdot$ is dot product and $\\odot$ is the element-wise product. $X_j'W^U$ and $W_j^D$ are the output value and output weights of j-th channel in the FFN hidden layer. In this derivation, the activation function $\\sigma$ is assumed to be Lipschitz continuous with the Lipschitz constant of $C_o$, and this holds for most activation functions (e.g., GeLU, ReLU, Swish) in LLMs. The bottom line in Eq. (7) is the upper bound for the objective in Eq. (6). To minimize this upper bound, $s^F$ should be set to one for the channels with large $|X_j'W^U| \\cdot |W_j^D|$ and zero for the channel with small values. In other words, channels with small $|X_j'W^U| \\cdot |W_j^D|$ should be pruned. This means that $|X_j'W^U| \\cdot |W_j^D|$ serves as the importance score for the j-th channel in the FFN hidden layer. Similarly, we can derive that $|X_j^H||W^O|(I + |W^U||W^D|)$ is the importance score for the j-th output channel of the MSA attention heads (as detailed in the supplementary materials). Based on the derivations, we define the channel importance of a transformer block as follows:"}, {"title": "Definition 1. (Block-wise importance scores)", "content": "Given a transformer block, the importance score $s^H$ for the j-th output channel of MSA attention heads and $s^F$ for the j-th channel of FFN hidden layer in this block are:\nMSA: $s_j^H = |X_j^H| \\cdot |W^O|(I + |W^U||W^D|),$ (8)\nFFN: $s_j^F = |X_j'W^U||W_j^D|,$ (9)\nwhere $X_j'$ and $W_j^D$ are the output value and output weights of the j-th channel in the FFN hidden layer. $X_j^H$ and $W_j^O$"}, {"title": "Experimental Settings", "content": "To demonstrate the effectiveness and versatility of the proposed method, we evaluate it on three open-source large language models with varied model sizes: LLaMA-1-7B, LLaMA-2-13B (Touvron et al. 2023), and Vicuna-7B (Wei-Lin Chiang et al. 2023).\nWe adopt LLaMA's evaluation protocol to conduct zero-shot task classification on common-sense reasoning datasets: BoolQ (Clark et al. 2019), PIQA (Bisk et al. 2020), HellaSwag (Zellers et al. 2019), WinoGrande (Sakaguchi et al. 2019), ARC-easy (Clark et al. 2018), ARC-challenge (Clark et al. 2018)"}, {"title": "Zero-shot Performance", "content": "Tables 1, 2 and 3 present the zero-shot performance of the pruned model. Evaluation on LLaMA-7B, Vicuna-7B and LLaMA-13B models demonstrates that with a 20% reduction in parameters without fine-tuning, the proposed method surpasses LLM-Pruner (the strongest baseline) by 1.40 on average on WikiText2 and PTB datasets. The robust performance of the proposed method is further confirmed by the common-sense reasoning tasks where the proposed method achieves 5.76% higher accuracy than LLM-Pruner, while maintaining 94.88% performance of the unpruned model.\nWith a higher sparsity level (50% parameters pruned), our method outperforms the baselines by a much more significant margin, achieving 117.93 lower perplexities on WikiText2 and PTB datasets and around 3.89% higher accuracy on common reasoning tasks compared to the best contender. The remarkable performance of our method at high sparsity levels should be attributed to the more accurate channel pruning employed by our approach.\nWe compare the fine-tuned performance of our pruned models with baselines. As depicted in Tables 1 and 2, we achieve similar or higher accuracies compared to the baselines after fine-tuning, highlighting the necessity of fine-tuning to enhance accuracy. Specifically, our fine-tuned pruned models yield approximately 2.44% higher accuracy on seven common reasoning tasks and reduce an average of 7.56 perplexities on WikiText2 and PTB datasets compared to LLM-pruner.\nNotably, fine-tuning offers no significant accuracy improvement for models pruned by our method on common reasoning tasks at the sparsity of 20% for both LLaMa-7B and Vicuna-7B. This suggests that our pruned models may already be near the optimal accuracy achievable through fine-tuning on common reasoning tasks. This is supported by a previous study (Zhang et al. 2023), where fine-tuning a 20% sparsity LLaMA-7B model on the larger LaMini-instruction dataset (Wu et al. 2024) with 2.58 million samples resulted in an accuracy of 62.70%, closely matching our pruned model's 62.57% accuracy without fine-tuning. However, fine-tuning remains necessary at higher sparsity levels (e.g., 50%) to improve performance, as our 50% sparsity LLaMA-7B model slightly underperforms LLM-pruner on common reasoning tasks after fine-tuning due to the smaller fine-tuning dataset (20k samples vs 50k samples) and fewer fine-tuning epochs (1 epoch vs 2 epochs).\nOur method outperforms the baselines due to a more appropriate \"pruning scope\". In contrast to global pruning approaches, our method avoids the invalid Lipschitz continuous assumption on the self-attention module (Kim, Papamakarios, and Mnih 2021) and the dependence on near-zero gradients for pruning decisions. Unlike the layerwise pruning approach, we minimize the pruning impact within the scope of a transformer block rather than a single layer, resulting in reduced error accumulation and improved efficacy."}, {"title": "Ablation Study", "content": "We compare our method with Wanda and LLM-pruner across various pruning ratios. Figure 2 illustrates that Wanda exhibits a rapid increase in perplexity at approximately 30% sparsity. Similarly, LLM-pruner demonstrates a significant perplexity increase when sparsity is increased from 20% to 50% on the LLaMA-7B and Vicuna-7B models. In contrast, our proposed method exhibits greater robustness to changes in sparsity.\nWe empirically compare our method with LLM-Pruner and Wanda pruning speed. Table 4 shows both Wanda and our method are 2 times faster than LLM-Pruner. This is because Wanda and our method only need one forward process to prune the channel while LLM-Pruner needs a forward and a backward processes to compute and store the gradients for pruning decisions.\nWe investigate the inference acceleration achieved by our proposed method. Table 5 demonstrates that, owing to structural pruning, our method reduces inference"}, {"title": "Conclusion", "content": "In this study, we have proposed LLM-BIP, a block-wise structured pruning approach for large language models. LLM-BIP aims to prune attention heads in the MSA and channels in the FFN while minimizing the impact on the corresponding transformer block output. To this end, we have derived a simple pruning metric that efficiently and effectively measures the importance of connections block-wisely in a single forward pass. We have evaluated the efficacy of LLM-BIP on LLAMA models and the Vicuna-7B model using common zero-shot datasets. Our results have demonstrated that LLM-BIP achieves similar accuracy to existing structured pruning approaches at low sparsity levels while significantly outperforming them at high sparsity levels."}, {"title": "Limitation", "content": "While the proposed LLM-BIP mitigates the error accumulation issue by employing a block-wise pruning scope, it remains a local pruning approach and may still encounter error accumulation."}]}