{"title": "Resource-Efficient Multiview Perception: Integrating Semantic Masking with Masked Autoencoders", "authors": ["Kosta Dakic", "Kanchana Thilakarathna", "Rodrigo N. Calheiros", "Teng Joon Lim"], "abstract": "Multiview systems have become a key technology in modern computer vision, offering advanced capabilities in scene understanding and analysis. However, these systems face critical challenges in bandwidth limitations and computational constraints, particularly for resource-limited camera nodes like drones. This paper presents a novel approach for communication-efficient distributed multiview detection and tracking using masked autoencoders (MAEs). We introduce a semantic-guided masking strategy that leverages pre-trained segmentation models and a tunable power function to prioritize informative image regions. This approach, combined with an MAE, reduces communication overhead while preserving essential visual information. We evaluate our method on both virtual and real-world multiview datasets, demonstrating comparable performance in terms of detection and tracking performance metrics compared to state-of-the-art techniques, even at high masking ratios. Our selective masking algorithm outperforms random masking, maintaining higher accuracy and precision as the masking ratio increases. Furthermore, our approach achieves a significant reduction in transmission data volume compared to baseline methods, thereby balancing multiview tracking performance with communication efficiency.", "sections": [{"title": "I. INTRODUCTION", "content": "Multiview systems play an increasingly important role in modern computer vision and pervasive computing, providing enhanced capabilities for comprehensive scene interpretation. These systems leverage the collaboration of multiple cameras to capture diverse perspectives of a scene, enabling a more thorough and robust interpretation of complex environments [1]. The applications of such systems span a wide spectrum, including but not limited to urban surveillance, disaster response coordination, intelligent traffic monitoring, and advanced sports analytics [2]. By integrating data from various viewpoints, multiview systems can be effective in addressing challenges such as occlusions, restricted fields of view, varying illumination conditions, objects with flexible or changing shapes, and scenarios involving extended periods of concealment [3].\nIn the context of pervasive computing, these multiview systems represent a significant step towards creating smart environments where computational power and sensing capabilities are seamlessly integrated into the physical world [4]. By distributing cameras throughout an environment, we create ubiquitous sensing networks that provide rich, contextual information about spaces and their occupants. However, the widespread deployment of these pervasive multiview systems exacerbates existing challenges and introduces new ones. As the number of cameras increases, the volume of visual data that needs to be transmitted and processed becomes excessive, straining network resources, especially in bandwidth-limited scenarios. Moreover, the real-time processing of high-dimensional video streams demands substantial computational power, which is particularly challenging for resource-limited camera nodes like drones or satellites [5].\nIn this context, the Masked Autoencoder [6] (MAE) is proving to be an excellent tool for the reconstruction of masked regions of an image [7]. MAEs leverage the power of transformers and the reconstruction capabilities of autoencoders to process images and reconstruct data from partial observations. This makes them well-suited for enhancing resource efficiency, adaptability, and robustness in various wireless communication tasks [8]. Masking and transmitting only a subset of the data patches at the camera node reduces the communication overhead, but the edge server can still reconstruct the complete visual data using the MAE network.\nMultiview target detection and tracking, a cornerstone application of multiview systems, involves the complex task of identifying and following objects or individuals across multiple camera views. This field has seen significant advancements in recent years, driven by the increasing demand for robust surveillance and monitoring solutions [9]. The challenge lies not only in accurately detecting targets from diverse angles but also in maintaining consistent object identities across different views, a problem known as cross-camera re-identification (Re-ID). Cutting-edge methods typically utilize a 2D to 3D projection, the camera views are aggregated, and the multiple targets are identified [10], [11]. Projecting camera views has revolutionized this domain, offering improved precision and accuracy in complex scene dynamics [12], [13]. For instance, methods leveraging convolutional neural networks (CNNs) have shown remarkable success in extracting discriminative features for both detection and re-identification tasks [14]. Furthermore, the integration of attention mechanisms and transformer architectures has enabled more effective modeling of spatial-temporal relationships in multiview scenarios [12].\nDespite these advancements, the trade-off between computational complexity and accuracy remains a critical consideration, especially for deployments with limited resources such as low-power camera nodes [15] and drone-based systems [16]. As the field progresses, there is a growing emphasis on developing more efficient algorithms that can leverage complementary information from multiple views while minimizing computational overhead and communication requirements. This intersection of multiview geometry, deep learning, and distributed computing presents both challenges and opportunities in the practical adoption of advanced multiview surveillance and monitoring applications.\nIn this paper, we propose a novel approach for efficient multiview detection and tracking using a semantic-guided masking strategy and MAE-based reconstruction. Our method comprises (i) Partially masking visual data from distributed cameras using a semantic-guided masking strategy to select and transmit only the most informative image patches. (ii) An edge server that reconstructs complete visual data for each camera view using the MAE encoder-decoder network. (iii) Multiview fusion of the reconstructed data to generate a comprehensive scene representation. (iv) CNN-based processing of the fused data for detection and tracking. This approach achieves efficient communication between cameras and the edge server while maintaining detection and tracking performance.\nThe main contributions of this work are as follows:\n\u2022 Development of a novel semantic-guided masking technique and MAE-based reconstruction for multiview systems. This technique leverages pre-trained segmentation models and a tunable power function to prioritize informative image regions, enhancing detection and tracking performance while reducing communication overhead.\n\u2022 Design of a distributed multiview system architecture that balances computational load between camera nodes and the central edge server. This system demonstrates the trade-offs between communication volume, computational complexity at camera nodes, and overall detection and tracking performance.\n\u2022 Comprehensive evaluation and quantification of the performance-communication trade-off in multiview detection and tracking scenarios. Our analysis compares random masking versus semantic-guided masking across various masking ratios, providing insights into the efficacy of our approach in bandwidth-constrained environments.\nThe rest of this paper is organized as follows; Sec. II covers the background literature, Sec. III covers the system model, Sec. IV shows the results, and Sec. V concludes the research work."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Recent advancements in object detection and tracking\nhave seen a shift from traditional methods to deep learning\napproaches. Notable developments include the enhancement of\nthe SORT algorithm [17], the introduction of transformer-based\nmodels like DETR [18] and its deformable variant [19], and\nend-to-end trainable models for simultaneous detection and\ntracking [20]. For a comprehensive review, see Luo et al. [21].\nMulti-camera tracking introduces additional challenges, par-\nticularly in maintaining object identities across views. Ristani\nand Tomasi [22] proposed using CNNs for robust feature\nlearning in Multi-Target Multi-Camera Tracking and Person\nRe-Identification. Addressing communication efficiency, Hu\net al. [23] proposed Where2comm, a collaborative perception\nframework that uses spatial confidence maps to guide pragmatic\ncompression and effective information aggregation. However,\nwhile Where2comm focuses on collaborative perception and\nbandwidth adaptation, it does not specifically address the\ncomputational constraints of camera nodes or the challenges of\nmultiview detection and tracking. For a comprehensive review\nof recent advancements in Multi-target Multi-camera detection\nand tracking, we refer readers to the survey paper by Amosa\net al. [24].\n1) Multiview: While multi-camera systems simply employ\nmultiple cameras to capture a scene from different angles,\nmultiview systems go a step further by integrating and\nprocessing data from these multiple perspectives to create a\nunified understanding of the environment. Multiview detection\nand tracking have seen significant advancements in recent\nyears, particularly in addressing challenges related to occlusion,\ncrowdedness, and computational efficiency. Fleuret et al. [25]\nlaid the groundwork with their probabilistic occupancy map\n(POM) approach, demonstrating robust multi-person detection\nin complex environments. Subsequent works have focused on\nleveraging bird's eye view (BEV) representations to improve\ndetection and tracking performance. Hou et al. [13] introduced\nMVDet, which projects features from multiple camera views\nonto a shared BEV representation, demonstrating the effec-\ntiveness of anchor-free representation and fully convolutional\nspatial aggregation. Their follow-up work, MVDeTr [26], incor-\nporated a shadow transformer to handle projection distortions\nmore effectively. Building on these concepts, Teepe et al. [14]\nproposed EarlyBird, which performs early fusion of camera\nviews in BEV space and introduces re-identification features\nfor appearance-based tracking. Their subsequent work [27]\nexplored various lifting algorithms for projecting multiview\nfeatures into BEV space, highlighting the importance of\ntemporal aggregation.\nWhile these approaches have improved multiview detection\nand tracking accuracy, they generally overlook the critical\naspects of communication and computational efficiency, par-\nticularly in distributed systems with resource-constrained\ncamera nodes. The CenterCoop framework by Zhou et al. [28]\nrepresents a step towards addressing communication efficiency\nin collaborative perception, but it does not fully consider the\ncomputational constraints of camera nodes. The OCMCTrack\nframework [29] comes closest to addressing the communication\nefficiency problem by separating single-camera processing from\ncross-camera association. However, it still requires significant\ncomputational resources at the camera for object detection,\nfeature extraction, and single-camera tracking. This approach,\nwhile reducing communication costs, could be detrimental\nto battery-operated devices and requires relatively powerful\ncomputational platforms at the camera node.\nOur proposed method uniquely addresses both commu-\nnication and computational efficiency challenges that are\nlargely overlooked in the existing literature. By employing\na masking technique at the camera node, we reduce both the\ncommunication overhead and the computational burden on\ncamera nodes. Unlike OCMCTrack, which performs complex\ncomputations at the camera node, our approach moves the\nbulk of the processing to the edge server, allowing for the\nuse of simpler, more energy-efficient camera nodes. This is\nparticularly crucial for battery-operated systems like drones or\nsatellite networks."}, {"title": "B. Semantic Communications", "content": "Semantic communication networks aim to convey the\nmeaning of messages rather than focusing solely on accurate\ndata transmission. This concept traces back to the seminal\n1948 work [30], which identified technical, semantic, and\neffectiveness levels in communication problems. Recent ad-\nvancements in machine learning and wireless communications\nhave made semantic communication increasingly feasible. A\ncomprehensive framework for next-generation semantic com-\nmunication networks [31] emphasizes minimalist, generalizable,\nand efficient semantic representations. This aligns with our\nproposed method, which uses a masking technique to compress\nvisual data from distributed cameras, transmitting only essential\ninformation for efficient target detection and tracking.\nResearch in semantic communication spans various applica-\ntions. An end-to-end deep learning system for text transmission,\nDeepSC [32], was later extended to speech signals with\nDeepSC-S [33]. Further advancements include multi-modal\nand multi-user scenarios [34] and efforts to reduce model\ncomplexity [35]. In image transmission, relevant to our work,\napproaches include joint source-channel coding [36], [37],\ndeep reinforcement learning for aerial image transmission [38],\nand MAE networks to combat semantic noise [39]. Federated\nlearning has also been applied for efficient distributed image\ntransmission in IoT devices [40].\nOur work builds upon the foundations of MAE-based\nimage transmission [41] but diverges from traditional se-\nmantic communication by introducing a semantically guided\napproach. We integrate semantic segmentation to identify\nregions of interest (ROI), enabling more efficient allocation\nof communication resources. While this aligns with recent\nAI-driven communication strategies for bandwidth-constrained\nscenarios [42], our method is specifically tailored for multiview\ndetection and tracking applications."}, {"title": "III. SYSTEM MODEL", "content": "In this section, we present the proposed framework for\nour communication-efficient multiview system. We introduce\nthe key components of the system, including the MAE and\nthe perspective transformation for aggregating the distributed\nimage sequences at the edge server.\nThe proposed communication-efficient multiview system\nconsists of multiple cameras (the camera nodes) that capture\nimage sequences from different viewpoints positioned to have\nan overlapping field of view. The system aims to efficiently\ntransmit the information present in the distributed image\nsequences to the edge server for target detection and tracking\ntasks.\nThe framework of the proposed system is illustrated in\nFig. 1. As an initial step, the images captured by the distributed\ncameras are resized to a lower resolution. This resizing serves\ntwo crucial purposes: firstly, it reduces the communication cost\nby decreasing the amount of data that needs to be transmitted;\nsecondly, it addresses the computational complexity issue, as\nthe processing time of MAEs increases quadratically with\nimage resolution [6]. After resizing, the distributed cameras\napply a masking process in preparation for the MAE e.g.,\nthey can perform computationally efficient random masking.\nAlternatively, we can employ semantically-guided masking\n(further explained in Sec. III-B) at the camera nodes to improve\nperformance at the cost of higher computational complexity\nas a semantic segmentation network needs to be used. At\nthe edge server, the MAE first encodes the received masked\nimages and then decodes the encoded latent data to output full\nimages, filling in the masked patches (see Sec. III-D). These\nreconstructed images are then aggregated using perspective\ntransformation (explained in Sec. III-E) to obtain a unified view\nof the scene in the BEV. The aggregated view is subsequently\nused for target detection and tracking tasks (see Sec. III-F)."}, {"title": "B. Semantic-Guided Masking", "content": "While the idea of utilizing semantic information in mask-\ning [43] has shown promise, our approach offers several key\ninnovations. Unlike previous methods that rely on custom\npart learning or gradual masking strategies, we leverage a\nstate-of-the-art pre-trained semantic segmentation model to\nobtain robust and accurate semantic information across diverse\nimage types. Our technique introduces a novel heatmap-based\npatch selection process, controlled by a tunable power function,\nwhich allows for fine-grained balancing of local and global\ninformation during the masking process.\nIn our proposed communication-efficient multiview system,\nwe employ a selective masking strategy to prioritize the\ntransmission of the most informative patches of the image\nto the edge server. This strategy is based on the premise that\nnot all patches in an image are equally important for the target\ndetection and tracking tasks. By selectively masking the less\ninformative patches and transmitting only the most relevant\nones, we can reduce the amount of data transmitted while\npreserving the essential semantic information.\nThe selective masking process begins by passing the input\nimage through a pre-trained semantic segmentation network\nto obtain the semantic masks of the targets. In the context\nof our research, we focus on pedestrian detection in a multi\nview dataset. Therefore, we utilize a pre-trained Detectron2\nnetwork [44] for semantic segmentation. Detectron2 is a well-\nestablished and highly accurate framework for object detection\nand segmentation tasks. By leveraging a pre-trained model, we\ncan avoid the need to train our semantic segmentation network\nfrom scratch, as semantic segmentation is a well-researched\nproblem and can be considered largely solved for our purposes.\nOnce the semantic masks of the pedestrians are obtained,\nwe proceed to create a heatmap of the image that highlights\nthe most \"active\" patches. To do this, we first divide the image\ninto patches of a fixed size (e.g., 20x20 pixels). For each patch,\nwe calculate its activity level by counting the number of pixels\nbelonging to the pedestrian masks within the patch itself and\nits eight neighboring patches. The sum of these pixel counts\nserves as a measure of the patch's activity level.\nAfter calculating the activity levels for all patches, we apply\na power function with a hyperparameter \u043a to the activity levels.\nThe power function is defined as $f(x) = x^\\kappa$, where x represents\nthe activity level of a patch. The purpose of the power function\nis to control the randomness of the patch selection process.\nBy adjusting the value of \u043a, we can influence the probability\ndistribution of the patches based on their activity levels. A lower\nvalue of $\\kappa$ increases the randomness, allowing patches with\nlower activity levels to have a higher chance of being selected,\nwhile a value of \u043a closer to 1 emphasizes the importance of\npatches with higher activity levels. Once the power function\nis applied, we normalize the modified activity levels to obtain\na probability distribution over the patches. We then randomly\nselect a subset of patches to be kept unmasked based on this\nprobability distribution. The number of patches to be kept\nunmasked is determined by a predefined masking ratio, which\nrepresents the percentage of patches to be transmitted.\nBy selectively masking the less informative patches, we can\nreduce the amount of data that needs to be transmitted to the\nedge server. This approach allows us to focus on transmitting\nthe most relevant information for the target detection and\ntracking tasks while minimizing the communication overhead.\nThe selective masking strategy helps to strike a balance between\ndata compression and the preservation of essential information."}, {"title": "Algorithm 1 Semantic-Guided Masking", "content": "Algorithm 1 performs semantic-guided masking by calculating patch activity levels based on semantic segmentation, applying a power function, normalizing to obtain a probability distribution, and randomly sampling patches to keep unmasked.\nThe activity level calculation for each patch i in step 4 is\nformulated as\n$A[i] = \\sum_{j\\in N(i)} \\sum_{x,y \\in j} M(x,y)$,\nwhere N(i) is the set of neighboring patches including i\nitself, and M(x, y) is the binary mask value at pixel (x, y).\nNormalization to obtain probability distribution from step 6 is\nshown as\n$P[i] = \\frac{A'[i]}{\\sum_{j} A'[j]}$\nRandom sampling of unmasked patches from step 8 is formu-\nlated as\n$U \\sim Multinomial(n, P)$,\nwhere $n = \\lfloor |P|\\cdot (1 - r) \\rfloor$ is the number of patches to keep\nunmasked."}, {"title": "C. Image Transmission Process", "content": "Our system begins by sending a seed for the masking pattern\nfrom each camera to the edge server. The number of patches\ndetermines this seed within an image, which in turn depends\non the original image's size and the patches' size.\nThe number of patches in the image, N, is given by\n$N = \\lfloor \\frac{W}{p} \\rfloor X \\lfloor \\frac{H}{p} \\rfloor$ ,\nwhere W and H are the width and height of the original image\nin pixels, p is the size of each square patch in pixels, and $\\lfloor . \\rfloor$\ndenotes the floor function.\nThe number of seeds required, S, is equal to the number of\nunmasked patches\n$S = \\lceil N \\times (1 - r) \\rceil$ ,\nwhere r is the masking ratio (percentage of patches to be\nmasked) and $\\lceil . \\rceil$ denotes the ceiling function. The idea after\nthe seed is sent is to sequentially send over only the RGB values\nof the unmasked patches, overall, reducing the communication\ncost."}, {"title": "D. MAE", "content": "The MAE is a key component of the proposed system. It\nis based on the concept of masked image modeling, where\na portion of the input image is masked, and the autoencoder\nis trained to reconstruct the original image from the masked\ninput.\nThe architecture of the MAE consists of an encoder and a\ndecoder. The encoder takes the masked image as input and\nextracts a compact feature representation. The decoder then\nreconstructs the original image from the feature representation.\nBy training the MAE to reconstruct the original image from\na masked input, it learns to capture the essential semantic\ninformation present in the image.\nThe masking strategy employed is designed to focus on\nthe regions of the image that are most relevant for the target\ndetection and tracking tasks. This is achieved by masking\nout the background regions of the image and preserving the\nforeground regions that are likely to contain the targets of\ninterest. This masking strategy helps to reduce the amount of\nirrelevant information transmitted to the edge server, thereby\nimproving the efficiency of the system.\nDuring the training phase, the MAE is trained on a large\ndataset of images to learn a generic feature representation. For\ntraining, a random masking strategy is employed to ensure\nthe model learns to reconstruct diverse image patterns. Once\ntrained, the full MAE (both encoder and decoder) is deployed\non the edge server. The camera nodes (distributed cameras)\nare responsible only for capturing images and applying the\nmasking strategy.\nIn operation, each camera captures an image sequence and\napplies masking and only these unmasked patches are then\ntransmitted to the edge server, reducing the data volume sent\nover the network. At the edge server, the received patches are\nfirst processed by the MAE encoder to extract features, and\nthen the MAE decoder reconstructs the full images by filling\nin the masked regions based on these features.\nThis architecture minimizes the computational burden on\nthe camera nodes, as they only perform image capture and\nmasking. The more computationally intensive tasks of encoding\nand reconstruction are centralized at the server, which typically\nhas more processing power. This approach allows for efficient\nuse of network bandwidth while maintaining the ability to\nreconstruct high-quality images for subsequent detection and\ntracking tasks."}, {"title": "E. Perspective Transformation", "content": "The perspective transformation occurs in the MVdet frame-\nwork. It is used to aggregate the feature representations after\nthe CNN decoder. The goal of the perspective transformation\nis to project the features from the different camera views onto\na common reference plane, typically the BEV. The perspective\ntransformation uses a homography matrix, which describes the\nmapping between the coordinate systems of the different camera\nviews and the common reference plane. The homography matrix\nis estimated using corresponding points between the camera\nviews, which can be obtained through camera calibration or\nfeature-matching techniques. Once the homography matrix\nis estimated, the feature representations from the different\ncamera views are projected onto the common reference plane\nusing the perspective transformation. This results in a unified\nrepresentation of the scene in the BEV, which facilitates the\ntarget detection and tracking tasks [13], [14].\nThe aggregated feature representation in the BEV provides\na comprehensive view of the scene, allowing for more accurate\nand robust target detection and tracking compared to using the\nindividual camera views separately. The perspective transforma-\ntion helps to mitigate the effects of occlusions and viewpoint\nvariations, as the targets of interest are likely to be visible in at\nleast one of the camera views. The perspective transformation\ncan be mathematically described using the pinhole camera\nmodel. The transformation between 3D world coordinates\n(x, y, z) and 2D image pixel coordinates (u, v) is given by\n$\\begin{bmatrix}\nsX\nsY\ns\n\\end{bmatrix} = K[R|t] \\begin{bmatrix}\nX\nY\nZ\n1\n\\end{bmatrix} =  \\begin{bmatrix}\np_{11} & p_{12} & p_{13} & p_{14}\np_{21} & p_{22} & p_{23} & p_{24}\np_{31} & p_{32} & p_{33} & p_{34}\n\\end{bmatrix} \\begin{bmatrix}\nX\nY\nZ\n1\n\\end{bmatrix}$ (6),\nwhere s is a scaling factor, $P = K[R|t]$ is the 3 \u00d7 4\nperspective transformation matrix, K is the intrinsic camera\nmatrix, and $[R|t]$ is the 3 \u00d7 4 extrinsic parameter matrix. For\nour BEV projection, we assume all points lie on the ground\nplane (z = 0). This simplifies the projection to\n$\\begin{bmatrix}\nsX\nsY\ns\n\\end{bmatrix} = P' \\begin{bmatrix}\nX\nY\n1\n\\end{bmatrix} =  \\begin{bmatrix}\np_{11} & p_{12} & p_{14}\np_{21} & p_{22} & p_{24}\np_{31} & p_{32} & p_{34}\n\\end{bmatrix} \\begin{bmatrix}\nX\nY\n1\n\\end{bmatrix}$ (7),\nwhere $P'$ is the 3 \u00d7 3 perspective transformation matrix\nwithout the third column of P. We apply this transformation\nto project features from all S cameras, with their respective\nprojection matrices $P'(s)$, onto a predefined ground plane grid\nof size $[H_g, W_g]$. Each grid position represents an area of\n10 cm x 10 cm. The resulting BEV feature has a size of\n$S \\times C_f \\times H_9 \\times W_g$, where $C_f$ is the number of feature channels.\nNote, it is important to distinguish between different types of\ntransformation matrices used in our framework. The perspective\nprojection matrix P is a 3 \u00d7 4 matrix that projects 3D world\ncoordinates to 2D image coordinates. The homography matrix\nis always a 3 \u00d7 3 matrix that maps points between two planes\n(e.g., from the image plane to the ground plane). Our simplified\nperspective projection matrix $P'$ is a 3 \u00d7 3 matrix derived from\nP by assuming z = 0. In the context of our BEV projection,\n$P'$ functions similarly to a homography matrix."}, {"title": "F. BEV Detection and Tracking", "content": "The decoder output bifurcates into BEV detection and re-\nidentification branches. The BEV detection branch employs\nfour prediction heads: center prediction (1-channel), offset\nprediction (2-channel), size prediction (3-channel), and rota-\ntion prediction (8-channel). These predictions undergo post-\nprocessing, including non-maximum suppression and top-K\nselection, yielding final BEV detections. Concurrently, the re-\nidentification branch extracts identity features from both BEV\nand image-level representations.\nThe tracking module integrates BEV detections with identity\nfeatures, employing a Joint Detection and Embedding (JDE)\ntracker. This tracker associates detections across frames using\nboth spatial and appearance information, enabling robust object\ntracking in the BEV domain despite occlusions and view\nchanges."}, {"title": "IV. PERFORMANCE EVALUATION", "content": "This section presents the performance evaluation of our\nproposed communication-efficient distributed multiview de-\ntection and tracking system using MAEs. We analyze the\nresults obtained from experiments conducted on the MultiviewX\nand Wildtrack datasets, focusing on detection and tracking\nmetrics, as well as communication efficiency. The simulation\nparameters used for the experiments are shown in Table I. We\ncompare the performance of our proposed system with the\nstate-of-the-art work presented in [14]. The performance of\nour proposed system is evaluated against the masking ratio\nr, defined as $r = N_{masked}/N_{total}$, where $N_{masked}$ is the number\nof masked patches and $N_{total}$ is the total number of patches\nin the image. A higher masking ratio indicates that fewer\npatches are transmitted, resulting in greater data compression\nbut potentially reduced image quality. This ratio quantifies\nthe trade-off between communication efficiency and system\nperformance across different compression levels."}, {"title": "A. MultiviewX Dataset", "content": "The MultiviewX dataset [13] presents a challenging multi-\nview pedestrian detection and tracking scenario. It comprises\nsynchronized video streams from six calibrated cameras,\ncapturing an outdoor scene with numerous pedestrians in a\nvirtual world generated by a video game engine. The dataset\nincludes 400 frames, with bounding box annotations provided\nin the ground plane. This dataset is particularly valuable for\nevaluating algorithms' ability to handle occlusions and varying\npedestrian densities in a multiview setup."}, {"title": "B. Wildtrack Dataset", "content": "Wildtrack [45] offers a real-world environment for multiview\npedestrian detection and tracking. It features seven synchro-\nnized and calibrated cameras recording an outdoor scene. The\ndataset consists of 400 frames, split evenly between training\nand testing sets. Ground truth annotations are provided as point\nannotations on the ground plane for each pedestrian. Wildtrack's\ndiversity in camera angles and pedestrian interactions makes\nit an excellent benchmark for assessing the robustness of\nmultiview tracking algorithms."}, {"title": "C. Evaluation Metrics", "content": "To comprehensively assess the performance of multiview\ndetection and tracking algorithms, we employ four key metrics,\ntwo for detection [46] and two for tracking performance [47]:\n\u2022 MODA (Multiple Object Detection Accuracy): This\nmetric evaluates the accuracy of detections by considering\nboth false positives and false negatives. It provides an\noverall measure of detection quality, penalizing both\nmissed detections and false alarms.\n\u2022 MODP (Multiple Object Detection Precision): MODP\nassesses the localization precision of correct detections.\nIt quantifies how well the detected bounding boxes align\nwith the ground truth, offering insight into the spatial\naccuracy of the detections.\n\u2022 MOTA (Multiple Object Tracking Accuracy): This\nmetric extends MODA to the tracking domain. It considers\nnot only false positives and false negatives but also identity\nswitches in tracking. MOTA provides a comprehensive\nview of a system's ability to track objects over time\nconsistently.\n\u2022 MOTP (Multiple Object Tracking Precision): Similar\nto MODP, MOTP measures the localization precision of\ntracks. It evaluates how closely the predicted trajectories\nmatch the ground truth paths, giving an indication of the\ntracking system's spatial accuracy over time.\nThese metrics collectively offer a multi-faceted evaluation of\nboth detection and tracking performance, enabling a thorough\nanalysis of multiview pedestrian tracking systems."}, {"title": "D. Detection and Tracking Performance", "content": "We first test the detection performance which is shown\nin Fig. 3. From this plot, we can see that the performance\nof our proposed method closely aligns with the state-of-\nthe-art work presented in [14] while maintaining a strong\nMODP performance of \u224880%, aligning with the state-of-the-art.\nUtilizing the semantically-guided masking algorithm yields an\neven better performance curve than the random masking MAE\nframework. This is expected since the semantically-guided\nmasking technique focuses on more informative parts of the\nimage. In addition, comparing the accuracy results with and\nwithout an MAE we can see that the accuracy performance\nincreases slightly. When using the MAE together with the\nsemantically guided masking algorithm displays even higher\nperformance than the state-of-the-art.\nFig. 4 shows the tracking results which demonstrates\nsimilar performance characteristics To the state-of-art also\nmaintaining a strong MOTP performance of \u224886%, Again the\naccuracy is closely aligned with the state-of-the-art work and\nutilizing the semantically-guided masking algorithm displays\nbetter performance results Compared to the random masking\nalgorithm. However, the accuracy performance does not seem\nto be affected too much with or without an MAE.\nFig. 5 and Fig. 6 illustrate the detection and tracking\nperformance, respectively, for the MultiviewX dataset. These\nresults exhibit similar trends to those observed in the Wildtrack\ndataset analysis but with some notable distinctions.\nAs shown in Fig. 5, the detection performance for the\nMultiviewX dataset follows a pattern comparable to that of the\nWildtrack dataset. However, a more pronounced difference\nis evident between the semantically-guided masking and\nrandom masking techniques. The semantically-guided masking\napproach demonstrates a higher performance across various\nmasking ratios.\nFig. 6, which depicts the tracking results, presents a similar\nnarrative. The performance characteristics align with those\nobserved in the detection results, with the semantically-guided\nmasking algorithm showcasing superior performance even at\nhigher masking ratios. The discrepancy between semantically-\nguided and random masking is more substantial compared to\nthe Wildtrack dataset results.\nWe hypothesize that the more substantial performance gap\nbetween semantically-guided and random masking in the\nMultiviewX dataset is attributed to the higher concentration of\npedestrian targets. The MultiviewX dataset contains an average\nof 40 pedestrians per frame, compared to approximately 20\nin the Wildtrack dataset [13], [45]. This increased density of\ntargets likely amplifies the benefits of the semantically-guided\nmasking technique.\nThe higher pedestrian count per frame in MultiviewX\nmeans that there are more informative regions in each image\nfor the semantically-guided masking algorithm to focus on.\nConsequently, the algorithm can more effectively prioritize\nareas with valuable information, leading to better preservation\nof critical features even at higher masking ratios. This results\nin a more robust performance compared to random masking,\nwhich does not account for the distribution of informative\nregions."}, {"title": "1) Comparison with State-of-the-Art Methods:", "content": "Table II\npresents a comprehensive comparison of our proposed method\nwith state-of-the-art approaches for multiview detection and\ntracking. Our method, operating at a 70% masking ratio,\ndemonstrates competitive performance across all metrics for\nboth the Wildtrack and MultiviewX datasets.\nWhat sets our approach apart is its ability to achieve these\ncompetitive results while reducing the communication cost.\nBy employing a 70% masking ratio, our method transmits\nonly 30% of the image data compared to other approaches.\nThis reduction in data transmission is further amplified by our\nimage-resizing strategy, which reduces the image dimensions\nby a factor of two in both height and width. Consequently,\nour method achieves a 13.33-fold reduction in data volume\n(0.3 x 0.25 = 0.075, or 1/13.33) compared to methods that\ntransmit full-resolution, unmasked images. Furthermore, to get\nthe communication volume over time we can multiply the data\nusage by two as both datasets operate at two frames per second."}, {"title": "E. Dropout", "content": "Fig. 7 demonstrates the effect of camera dropout on MODA\nperformance. As dropout percentage increases from 0% to 50%,\nMODA decreases for both pipeline configurations. Notably,\nthe performance gap between the semantically guided with\nthe MAE pipeline and the pipeline without MAE remains\nconsistent across all dropout rates.\nThis consistency suggests that the MAE component provides\na robust performance improvement, maintaining"}]}