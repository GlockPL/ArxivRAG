{"title": "MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices", "authors": ["Jianwen Jiang", "Gaojie Lin", "Zhengkun Rong", "Chao Liang", "Yongming Zhu", "Jiaqi Yang", "Tianyun Zhong"], "abstract": "Existing neural head avatars methods have achieved significant progress in the image quality and motion range of portrait animation. However, these methods neglect the computational overhead, and to the best of our knowledge, none is designed to run on mobile devices. This paper presents MobilePortrait, a lightweight one-shot neural head avatars method that reduces learning complexity by integrating external knowledge into both the motion modeling and image synthesis, enabling real-time inference on mobile devices. Specifically, we introduce a mixed representation of explicit and implicit keypoints for precise motion modeling and precomputed visual features for enhanced foreground and background synthesis. With these two key designs and using simple U-Nets as backbones, our method achieves state-of-the-art performance", "sections": [{"title": "1 Introduction", "content": "One-Shot Neural head avatar (NHA) is a technology that animates a single facial image according to a specified driving signal, which could be video or audio, to synthesize a portrait video. In recent years, significant improvements [4,9,17,29, 30,32-34,38] have been made in the quality of the generated image and the range of motion. However, existing NHA approaches concentrate on achieving realism and robustness in image synthesis with models that are increasingly complex, typically surpassing 100 GFLOPs [9,29,32,38], leading to the under-exploration of lightweight NHA. With the swift progress in large language models (LLMs) and the widespread use of smartphones, avatars on mobile devices are poised to become a crucial interface for AI interaction. This prospect has driven us to develop an efficient one-shot neural head avatar model optimized for performance on mobile platforms.\nInitially, we attempted to convert existing SOTA models into ones that could be deployed on mobile devices. However, we found that these models incorporated many complex modules in their structural design, such as memory modules, dynamic convolutions [9], attentions [9, 15], multiscale feature warping [9,29,38] or image-to-plane [32] methods. Reducing computational complexity is a challenging task, and the complexity of the models increased the difficulty and development workload of deploying them on mobile devices. Therefore, we start to reflect on the rationale underlying these methods and aim to construct a lightweight NHA model through the most essential and straightforward design.\nIn fact, Real3D [32] and MCNet [9] represent two distinct categories of motion modeling: explicit facial movement modeling and implicit global motion modeling. Explicit modeling [17,32,34] methods often involve predefined facial keypoints or 3D face representation to capture motion driven by facial movements. This results in undefined motion in regions beyond the face, necessitating a powerful motion network to extrapolate and fill in the motion for these areas based solely on facial movements. Implicit modeling methods [9,20,21,30,38] use an encoder to extract global and image-level motion from inputs without facial priors, representing motion with neural keypoints or latents, which requires a powerful motion network to define facial and background movements.\nFrom the Figure 1, we can observe that the explicit modeling method, Real3D, produces poor results in areas not defined by the 3DMM, such as inside the mouth and around the neck. Meanwhile, the implicit modeling method, MCNet, produces notable blurriness at the boundary between the person and the background, possibly due to the lack of an explicit facial region prior. This observation inspired us to develop a more holistic and efficient approach to motion modeling that combines face-specific knowledge with global motion representations, complementing each other.\nThe results shown in Figure 1 not only reveal issues with motion capturing but also indicate inadequate appearance synthesis capability of the model. Many recent advancements [1,11,19,23,27] in 3D-related fields have been driven by the integration of multiview designs into network architectures. The rationale is intuitive: a network exposed to more appearances can learn more effectively. This concept has led us to explore a parallel avenue: if facial knowledge can strengthen our motion network, might the incorporation of appearance knowledge similarly strengthen our synthesis network? The integration of appearance knowledge may redefine the synthesis network's task, transforming it from generating all content with a powerful network to efficiently completing content with the provided appearance knowledge, akin to shifting from a closed-book to an open-book exam. Importantly, this can be achieved with virtually no increase in computational load during runtime because appearance knowledge can be prepared in advance.\nBuilding on aforementioned observation and considerations, we meticulously design MobilePortrait, our lightweight one-shot neural head avatar method. First, we utilize lightweight U-Nets with conventional convolutional layers as the backbones for motion and synthesis networks, significantly reducing computational requirements compared to existing methods and is easily implementable on mobile devices. Second, to compensate for potential losses in motion accuracy due to reduced computations, we combine implicit global motion modeling with explicit facial motion modeling, introducing mixed keypoints to capture motion. We also design facial knowledge losses to ensure the incorporation of facial knowledge. Lastly, in the image synthesis phase, we incorporated appearance knowledge, utilizing pseudo multi-view features and pseudo backgrounds to enhance synthesis of foreground and background respectively. With these proposed designs, MobilePortrait achieves performance on par with or exceeding state-of-the-art methods with far less computational demand, as shown in Figure 1. Our contributions are succinctly outlined as follows:\nWe introduce MobilePortrait, which, to the best of our knowledge, is the first one-shot mobile neural head avatar method capable of real-time performance.\nWe streamline the task by leveraging external facial and appearance knowledge, merging explicit and implicit keypoints for comprehensive motion capture, and including features like pseudo multiview and background for improved synthesis. This approach allows MobilePortrait to efficiently create neural head avatars with lightweight U-Net [18] backbones.\nExtensive testing across various datasets confirms MobilePortrait's effectiveness. It achieves state-of-the-art performance while requiring significantly fewer FLOPs and parameters. Moreover, we have also verified that MobilePortrait can render at speeds of up to 100+ FPS on mobile devices and support both video and audio driving inputs."}, {"title": "2 Related Works", "content": "Neural head avatar generation can be categorized into video-driven and cross-modal driven approaches. video-driven neural head avatars methods mainly consists of two important parts: motion modeling and image synthesis. The former captures the motion between the source and driving images, while the latter generates the animated pixels. For motion modeling, some methods [9, 20, 21, 29, 30, 38] propose frameworks for decoupling appearance and motion representation in an unsupervised manner. For instance, [9, 20, 21, 38] involve learning to detect 2D implicit keypoints from images and further predict explicit warping flow. FaceV2V [29] expands the network architecture dimension and learns 3D implicit keypoints for motion modeling. LIA [30] constructs a latent motion space and represents motion as a linear displacement of the latent code. In contrast, other works [17,32,34] rely on explicit motion representation, such as pre-defined facial landmarks and blendshapes. For example, MetaPortrait [34] uses facial landmarks as input to predict a warp flow, while PIRenderer [17] and Real3D [32] employ the 3DMM model [22] to facilitate decoupling of the control of facial rotation, translation and expression. Although significant progress has been made, implicit and explicit modeling still remain largely independent of each other. For image synthesis, some methods [20,21,38] predict motion flow to directly warp the source image, utilizing more original pixel information, while other approaches [4,9,29,34] opt to warp features, offering greater flexibility for subsequent generative networks. StyleHeat [33] explores using pretrained Style-GAN [12] as a generator, achieving neural head avatars through latent edits on the powerful generator.\nCurrent cross-modal methods, mainly audio-driven, aim to generate motion signals from audio for natural talking head videos with accurate lip-sync and expressive facial animations. They typically produce driving signals as output and use separately trained video-driven models as video renderers. Sadtalker [36] uses a PoseVAE and ExpNet to generate head pose and expression as motion descriptors generated from audio and adopt FaceV2V [29] as renderer. Vivid-Talk [24] designs an Audio-To-Mesh module to predict 3DMM expression coefficients and lip-related vertex offsets based on an input audio signal and a reference image, while utilizing another mesh-to-video model as a renderer. Some recent works [8, 8, 14, 14, 35, 35] explore the use of diffusion models as audio-to-motion modules, employing VAEs [13] or existing video-driven models as renderers to enhance the accuracy and expressiveness of motion signals. EMO [26] designs a end-to-end diffusion model and can generate highly realistic portrait videos based on audio input. Although the inference process is end-to-end, the multi-stage training procedure for the network can, to some extent, correspond to the audio-to-motion and render modules. Image quality in audio-driven methods largely hinges on the rendering model or the module managing image quality. Given the driving signal's low transmission cost, it's suitable for server-side deployment. Thus, an efficient renderer is key for audio-driven neural head avatars on edge devices."}, {"title": "3 Method", "content": "This section first provides an overview of MobilePortrait's architecture, shown in Figure 2, comprising two primary modules: motion generation and image synthesis. Then in Section 3.2 we describe the hybrid motion modeling designed within the motion generation module, which utilizes both explicit and implicit facial keypoints. Next, we introduce techniques that enhance image synthesis through precomputed appearance knowledge in Section 3.3. Subsequently, in Section 3.4, we present the audio-to-motion module, which allows MobilePortrait to be driven by audio input. Finally, we outline the loss functions employed during training in Section 3.5."}, {"title": "3.1 Overview of MobilePortrait", "content": "As depicted in Figure 2, with video-driven animation as an example, MobilePortrait processes the source image S and each driving frame D from the driving video, generating target images frame by frame. Specifically, within the motion generation module, Keypoint Detectors initially produce a set of keypoints, which are our proposed mixed keypoints in the MobilePortrait, for both S and D, i.e. ${x_{s,i}, y_{s,i}\\}^{N_{mk}}_{i=1}$ and ${x_{d,i}, y_{d,i}\\}^{N_{mk}}_{i=1}$. The subsequent warping and generation process is similar to the previous works [9, 20, 21, 38]. Based on these keypoints, we follow TPS [38] to generate the initial transformations. Keypoints represented as heatmaps are input into the dense motion network and combined with initial transformations to generate the motion field, M, delineating the pixel displacement from S to D, or in other words, the optical flow. Based on the source image and the optical flow, a warp operation is performed to obtain the initial warped image, which is then multiplied by another output from the dense motion, the occlusion maps, to produce the final warped image $S_w$. Subsequently, the Image Synthesis module leverages $S_w$ and auxiliary appearance knowledge features derived from S to create the final target image through a synthesis network. For efficient computation and mobile deployment friendliness, we retained simple U-Nets without the additions from prior work [9,29,38], such as multi-scale feature warping, dynamic convolution, and attention modules, as backbones for both the dense motion network and the synthesis network."}, {"title": "3.2 Motion Generation with Facial Knowledge", "content": "Mixed Keypoint Representation. In the Motion Generation module, prior works such as FOMM [20], TPS [38], and MCNet [9] employ similar network design structures. They utilize a neural keypoint predictor, denoted as NK detector, to separately predict a pair of keypoints for S and D, and based on these keypoints, construct an initial collection of transformations. Dense motion network (DMN) then predicts local weights for this collection of transformations and occlusion maps for warped image. The optical flow field is obtained through a weighted summation of these elements. This process is similar to part (b) described in Figure 3.\nNeural keypoints enable the network to learn global motion information from the driving video, as well as facial movements. However, as the computational load of the dense motion network decreases, the network struggles to distinguish between the motion of the face and the background, leading to severe artifact-ing, akin to a \"liquefaction\" effect, or may even result in an inability to drive the synthesized video, as shown in the visualization results in Figure 4. To address this, we introduce a pretrained face keypoint detector to extract facial landmarks from S and D respectively. A mixed keypoint predictor, the merger shown in Figure 2, then merges the neural keypoints and the face keypoints to create mixed keypoints. As shown in the left part of Figure 3, once the mixed keypoints are calculated, we proceed to calculate the optical flow based on these keypoints, replacing the neural keypoints used in previous methods [9,20,21,38].\nOur experiments indicate that integrating implicit and explicit keypoints effectively reduces global liquefaction artifacts and enhances motion precision in the generated videos and also performs better than other methods of incorporating facial information. Additionally, inspired by MetaPortrait [34] and the ResNet [7] architecture, we add two extra output channels to the last layer of our dense motion network. This modification enables the network to produce a residual optical flow, enhancing the expressiveness of the generated optical flow.\nFace-Aware Motion Generation. In addition to incorporating facial priors into the keypoints representation, we enrich the input to the dense motion network with a foreground mask and facial landmarks mask from the source image. These only need to be computed once, preserving real-time inference capabilities. As shown in Figure 2, we further design a facial knowledge loss. Specifically, we add two predictors for these masks to the last feature layer of the DMN, which are trained with L1 losses to predict the foreground and landmarks mask for the driving image. These predictors, existing only during training, help the model to better understand portrait integrity, facilitating improved face-aware motion generation.\nWith these enhancements, our motion generation module leverages external facial knowledge to perform motion capture at both the face level and the video level, with virtually no increase in computational cost. This enables the model to generate a plausible optical flow M even when the computational load of dense motion network is reduced."}, {"title": "3.3 Image Synthesis with Appearance Knowledge", "content": "Image synthesis based on the warped source image capitalizes on the original pixel information, but as the warping itself doesn't create new pixel data, reliance on the warped source may lead to diminished synthesis quality when there are changes in pose angles, as shown in Figure 1. To compensate for the decrease in synthesis quality due to reduced complexity, we utilize the warped source image as input to U-Net based synthesis network and introduce precomputed visual features from source image to decrease the burden on Image Synthesis module.\nEnhanced Foreground Synthesis. We sample T frames uniformly from the driving video and, with the source image, generate T newly warped images using our motion generation module. As depicted in the top-right part of Figure 2, To ensure efficient feature extraction and fusion, we opt for the final downblocks of the U-Net, corresponding to the lowest spatial resolution. The early layers of the U-Net, up to the last downblock, are utilized for feature extraction from the newly warped image to obtain multiview features. An additional convolution layer merges multiview features with those of the current frame within the corresponding downblock. Apart from this, there are no further differences or additional computational burdens imposed on the synthesis network. These pseudo multiview image features offer appearance information from different poses to aid in enhancing the quality of synthesis and can be precomputed, thus not hindering inference efficiency.\nEnhanced Background Synthesis. We employ an offline inpainting model to fill in the source image after foreground removal, creating a complete background picture as shown in the top-right part of Figure 2. This inpainted background, along with a mask of the foreground, serves as extra inputs to the synthesis network. To ensure that the Image Synthesis module can effectively utilize this background information, we perform inpainting on the driving image during training, which has proven crucial in our experiments.\nWith these improvements, our image synthesis can rely on a simple yet efficient U-Net backbone while maintaining high-quality synthesis results during inference, with negligible additional computational cost."}, {"title": "3.4 Audio-Driven Functionality", "content": "In this section, we first introduce a baseline solution that enables MobilePortrait to support audio-driven functionality. To enable MobilePortrait to process audio-driven signals, we need to extract neural keypoints and facial keypoints from the audio input. Inspired by the audio-to-motion designed in SadTalker [36] and VividTalk [24], we train an audio-to-motion model that includes two modules: audio-to-mesh and mesh-to-neural keypoints. The former uses LSTM to convert audio signals into 3D Morphable Model (3DMM) coefficients to acquire facial meshes, whereas the latter employs a ResNet18 [7] to predict neural keypoints from images sketched with sampled mesh vertices and edges. Facial keypoints are directly extracted from the mesh. With this setup, we capture the necessary motion signals, including neural and facial keypoints, for driving MobilePortrait with audio input. Thanks to the trained mesh-to-neural keypoints module, MobilePortrait can also be driven by 3DMM. This not only facilitates expression editing via 3DMM but also enhances results in cross-identity scenarios when driven by 3DMM. It is important to note that we provide merely a baseline solution here, enabling audio-driven capability for MobilePortrait. MobilePortrait can accommodate more sophisticated designs [14,24,32,35] to achieve improved results."}, {"title": "3.5 Training Losses", "content": "Following previous works [9, 20, 38], We employ perceptual loss $L_{percep}$ and L1 loss $L_{L1}$ to optimize feature and pixel distances, keypoint distance loss $L_{kp}$ for facial keypoints accuracy, and equivariance loss [38] $L_{eq}$ for neural keypoint stability. Additionally, we add two proposed facial knowledge loss terms (shown in the top-left part of Figure 2), implemented in L1 loss, to make dense motion network to be aware of the landmark mask $L_{landmark}$ and foreground mask $L_{mask}$. The final loss can be written as follows:\n$L = L_{percep} + L_{L1} + L_{kp} + L_{eq} + L_{landmark} + L_{mask}$"}, {"title": "4 Experiments", "content": "Experimental Setup. To rigorously assess method effectiveness, we trained and tested our approach using various datasets. For training, we leveraged the VFHQ [31], VoxCeleb2 [3], and CelebvHQ [40] datasets, which together comprise 16,827 high-resolution portrait clips from VFHQ, 35,666 clips at 512\u00d7512 from CelebVHQ, and 150,480 clips at 256\u00d7256 from VoxCeleb2, collectively representing more than 21,000 distinct identities. To evaluate generalization, We construct a test set drawn from multiple datasets, including Talking Head 1K [29], CCv2 [6], and HDTF [37], from which we randomly sampled 38, 137, and 100 videos according to the dataset proportions, respectively. Videos were processed at 25 FPS, square-cropped based on face detection, and resized to 512px.\nImplementation Details. For facial keypoints FK, we adopt the 106 landmark protocol, and for neural keypoints NK, we select 50 points. The mixed keypoint predictor is realized by concatenating keypoints and processing them through a MLP. By fusing FK and NK, we obtain 50 mixed keypoints. A foreground segmenter [2] was employed for mask extraction, and LaMa [25] was used for background inpainting. The training processes of MobilePortrait are performed on 8 NVIDIA A100 GPUs, with a learning rate of 0.002 for 60 epochs.\nMetrics To comprehensively evaluate the efficacy of our method, we employed multiple metrics and assessed both same-id reenactment and cross-id reenactment. To evaluate the quality of the generated images, we used common image quality metrics [9,32] including reference-based indices like FID, SSIM, and PSNR, as well as identity preservation indicator CSIM. To measure the accuracy and stability of the synthesized motion, we evaluated average keypoint distance(AKD), head pose distance(HPD) and expression errors(AED). Additionally, referencing recent text-to-video evaluation metrics [10], we add a background consistency index (BCI).\nCompared methods. To validate the superiority and effectiveness of our method, we conducted comparative tests with recent top-performing methods, including latent-driven TPS, MCNet and FaceV2V, as well as approaches that use landmarks and 3DMM like Real3D, PIRender. For fair comparisons, we trained all methods on the same datasets as previously described, with the exception of Real3D. Due to its complex training requirements, we use the official release model trained on CelebVHQ. To account for this, we also included"}, {"title": "4.1 Comparisons with SOTA methods", "content": "In this section, we contrast MobilePortrait's video-driven performance with other techniques in Table 1, and will later include an audio-driven comparison. Given that audio-driven methods often use video-driven approaches for rendering, video-driven analysis serves as a reliable measure of synthesis quality. In same-id scenarios, the source image is sampled from the driving video, meaning there exists ground truth video for reference. In cross-id scenarios, the source image is not derived from the driving video; instead, we randomly select and sample a frame from another video in datasets, so there is no GT video for direct comparison, and we do not assess reference-based image quality metrics.\nIt can be discerned that MobilePortrait, despite employing a smaller computational load, achieves outcomes comparable to those with greater computational resources and excels in key metrics, leading in AKD and BCI and ranking second in FID, which assess motion and image quality. Furthermore, during cross-identity reenactment, the lead in HPD, AED and BCI metrics also demonstrates the effectiveness of MobilePortrait. While MobilePortrait does not achieve the best results in the CSIM, later visualization results show that it can yield satisfactory outcomes."}, {"title": "4.2 Comparisons among Different Computational Loads", "content": "Here, a comparative analysis of performance across various computational scales (FLOPs) is provided in right part of Table 2. By reducing the number of channels and layers, we obtain models of different sizes. MobilePortrait remarkably maintains satisfactory performance on key metrics such as FID and AKD, as well as cross-identity motion accuracy like HPD and AED, even when computational resources are limited to just 4 GFLOPs, marking a significant improvement over the baseline, which does not incorporate external facial and appearance knowledge as demonstrated in Figure 2. Moreover, visualization results in Figure 4 showcasing our approach's effectiveness. Concurrently, in the left part of Table 2 also details the computational resource consumption of MobilePortrait on mobile devices, underscoring its efficient viability on mobile platforms."}, {"title": "4.3 Ablation Studies", "content": "Motion Generation. we conduct ablation experiments to validate the effectiveness of proposed components. We assess key metrics for image quality and motion, such as FID and AKD, along with AED and HPD specifically in cross-identity scenarios, denoted as AED(C) and HPD(C). Table 3 presents comparisons among employing mixed keypoints, neural keypoints only and face keypoints only settings, where mixed keypoints demonstrate significant performance improvements. Additionally, excluding our proposed facial knowledge losses degrades results. We also explored alternative approaches to integrating NK and FK beyond the mixed keypoint predictor. For instance, as shown in (b) of Figure 3 and drawing inspiration from literatures [20, 21, 29], we perform fusion on the initial transformation. We transform FK into sparse motions to generate transformations, which, when concatenated with NK's transformations, yield a combined transformations. Alternatively, in (c) of the figure, both NK and FK are converted into heatmaps, which are then directly fed into a convolutional network to generate optical flow. However, these methods did not achieve better motion accuracy than mixed keypoints. Additionally, we experimented with removing the residual optical flow and observed that it indeed resulted in decreased motion accuracy, although it also introduced some perturbations to the FID. The experimental results demonstrate that integrating explicit and implicit information significantly improves the generated outcomes in terms of image quality and motion, while the fusion form of mixed keypoints is a simple and effective design.\nEnhanced Background Synthesis. In this section, we assess the effectiveness and usage of pseudo background in synthesis networks and list experimental results the left part of Table 4, where the setting employed in our method is marked in gray. We investigated four configurations, namely whether pseudo background should be input into the synthesis network (abbreviated as Inp. BG) and whether the model should synthesize the background (given the presence of pseudo backgrounds, we have the option to generate only the foreground and then composite it onto the background by additionally predicting an alpha channel, abbreviated as FG Comp.). We find that pseudo-background integration indeed enhances performance by transforming the task from full generation to knowledge-aided synthesis. Table 4 shows that separately generating and merging the foreground and background does not significantly improve performance. Our method, highlighted in row two, enhances synthesis by end-to-end training with pseudo backgrounds derived from driving images, enabling effective utilization of this knowledge in creating the final image. Real3D, which employs volume rendering, attempts to integrate the rendered head with the background using a split-and-merge-like strategy, but this can lead to inconsistent motion and visible discrepancies, as depicted in Figure 1 and Figure 5."}, {"title": "4.4 Experimental Results on More Application Scenarios", "content": "Comparisons among Audio-to-Motion. We compared MobilePortrait with some audio-driven methods. We used the 100 videos from HDTF for testing and measured lip synchronization using the Sync-D and Sync-C metrics generated by SyncNet [16] and evaluated background consistency using BSI. The results in Table 6 indicate that MobilePortrait achieves comparable performance to some audio-driven methods, outperforms one of them, and exhibits superior visual stability. It is noteworthy that MobilePortrait's primary focus is to achieve a"}, {"title": "5 Conclusion", "content": "In this work, we address the overlooked challenge of creating lightweight one-shot neural head avatars and introduce MobilePortrait, to the best of our knowledge, the first real-time solution for mobile devices. By employing a mixed representation of explicit and implicit keypoints, along with pseudo multiview and background, we enhance the network's motion generation and synthesis capabilities with external knowledge, enabling MobilePortrait to achieve neural head avatars with simple lightweight U-Nets. Extensive experiments confirm that MobilePortrait achieves state-of-the-art performance in synthesis quality and motion accuracy, and supports both video and audio driving inputs."}]}