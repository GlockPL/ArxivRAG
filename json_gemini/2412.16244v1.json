{"title": "Neural diversity is key to collective artificial learning", "authors": ["Matteo Bettini", "Ryan Kortvelesy", "Amanda Prorok"], "abstract": "Many of the world's most pressing issues, such as climate change and global peace, require complex collective problem-solving skills. Recent studies indicate that diversity in individuals' behaviors is key to developing such skills and increasing collective performance. Yet behavioral diversity in collective artificial learning is understudied, with today's machine learning paradigms commonly favoring homogeneous agent strategies over heterogeneous ones, mainly due to computational considerations. In this work, we employ novel diversity measurement and control paradigms to study the impact of behavioral heterogeneity in several facets of collective artificial learning. Through experiments in team play and other cooperative tasks, we show the emergence of unbiased behavioral roles that improve team outcomes; how neural diversity synergizes with morphological diversity; how diverse agents are more effective at finding cooperative solutions in sparse reward settings; and how behaviorally heterogeneous teams learn and retain latent skills to overcome repeated disruptions. Overall, our results indicate that, by controlling diversity, we can obtain non-trivial benefits over homogeneous training paradigms, demonstrating that diversity is a fundamental component of collective artificial learning, an insight thus far overlooked.", "sections": [{"title": "1 Introduction", "content": "Diversity of skills and behaviors is ubiquitous in real life and is widely believed to be key to the survival and thriving of natural ecosystems [1]. It enables collective intelligence [2, 3], a property that is not simply dependent on the maximum or total intelligence of team members. Collective intelligence in nature emerges from individuals learning in rich and heterogeneous environments, surrounded by diverse functional stimuli and interactions [4]. Increasing evidence, however, suggests that we may have underestimated the value of diversity in the learning of behaviors that solve complex cooperative tasks [5, 6]. In spite of this, current research in collective artificial learning has left diversity largely underexplored.\nThis issue is particularly evident in the context of Multi-Agent Reinforcement Learning (MARL), which is typically applied to synthesize team strategies in such collective learning problems. Traditional MARL algorithms constrain the strategies (i.e., policies) of agents to be identical [7-9]. This speeds-up learning by training a shared policy from all individuals' experiences, but results in the agents becoming behaviorally homogeneous. While some MARL methods encourage behavioral (neural) diversity [5, 6, 10, 11], they blindly promote it via additional learning objectives, lacking principled techniques to measure and control it. Despite these efforts, the benefit of diversity towards the resolution of complex cooperative tasks remains poorly understood. However, the recent development of principled techniques that enable the measurement and exact control of behavioral diversity in agent teams [12, 13] has enabled novel paradigms to study heterogeneity.\nIn this work, we employ these diversity measurement and control paradigms to study how and in what form behavioral diversity contributes to collective artificial learning. We decompose collective learning into three core challenges: (1) team play, the ability to cooperate with teammates in long-term decision-making; (2) exploration, the ability to discover the world and task objectives; (3) resilience, the ability to recover from unexpected disruptions. Our study spans several cooperative tasks including soccer, to evaluate team play, collective foraging, to evaluate exploration, and physically-coupled multi-agent navigation in the presence of dynamic disruptions, to evaluate resilience. We show that diversity is responsible for the emergence of complementary roles and better performing strategies in team play, that it boosts the learning process through more efficient exploration, and finally, that it enables agents to learn and retain latent skills to overcome repeated disruptions. Via this study, we aim to demonstrate that behavioral diversity should not be overlooked due to computational overheads, but should rather be considered a key enabler of collective learning."}, {"title": "1.1 Measuring diversity", "content": "We employ System Neural Diversity (SND) [12] to measure heterogeneity. Given a team of agents N = {1, . . ., i,..., n}, agent policies are functions \\( \\pi_i(o_i) \\) that produce a continuous action distribution given an observation \\( o_i \\). We measure the teams' diversity in two phases. First, we employ the Wasserstein statistical metric [14] to measure the diversity among agent pairs over a set O of observations: \\( d(\\pi_i, \\pi_j) = \\)"}, {"title": "1.2 Controlling diversity", "content": "To control diversity, we employ Diversity Control (DiCo) [13], which allows to constrain the diversity of a multi-agent system to a desired metric value SNDdes. This method works by representing each agent's policy \\( \\pi_i(o_i) \\) as the sum of a homogeneous parameter-shared component \\( \\pi_h(o_i) \\) and a per-agent heterogeneous deviation \\( \\pi_{h,i}(o_i) \\). The per-agents policy deviations are then rescaled with respect to the homogeneous component to obtain the desired diversity. The scaling factor is computed by dividing by their current diversity (SND := SND({\\(\\pi_{h,i}\\)\\}i\u2208N)) and multiplying by the desired one (SNDdes):"}, {"title": "2 Results", "content": "Team play, or cooperation, requires the collective ability to tackle a shared objective with mutual assistance of team members. As such, it provides an important benchmark for collective intelligence in both human and artificial systems, and has served as a milestone demonstrator for methodological progress [15-17]. To evaluate team play in collective learning, we train agents with a shared reward. We begin by studying the impact of neural diversity when agents (i.e., team players) are physically-identical (Sec. 2.1.2, Sec. 2.1.3, Sec. 2.1.4), for which the benefits of behavioral diversity have thus far been underexplored; we then focus on physically-different agents in Sec. 2.1.5."}, {"title": "2.1 Neural diversity is key to team play", "content": "Team play, or cooperation, requires the collective ability to tackle a shared objective with mutual assistance of team members. As such, it provides an important benchmark for collective intelligence in both human and artificial systems, and has served as a milestone demonstrator for methodological progress [15-17]. To evaluate team play in collective learning, we train agents with a shared reward.\u00b9. We begin by studying the impact of neural diversity when agents (i.e., team players) are physically-identical (Sec. 2.1.2, Sec. 2.1.3, Sec. 2.1.4), for which the benefits of behavioral diversity have thus far been underexplored; we then focus on physically-different agents in Sec. 2.1.5."}, {"title": "2.1.1 Soccer", "content": "We consider the game of Soccer as it requires both high-level and low-level decision making in a long-term collective task. Despite its game-like nature, Soccer represents one of the hardest team problems tackled by multi-agent learning researchers to date [17, 19]. Further, due to its approachable nature with communicable results, it is an apt choice for our study.\nSeveral existing works focus on soccer-inspired tasks. Winners of the RoboCup soccer challenge [20] leverage hierarchical or hybrid learning solutions [21, 22] and current MARL research typically considers either small teams or simplified scenarios, all using homogeneous agents [17, 23, 24]. Our experiments are thus a departure from common practice, and represent the first study of diversity in as large as five-player teams using end-to-end MARL.\nThe setup of our Soccer task is shown in Fig. 2a. Learning agents (blue) are trained against opponents (red) executing a heuristic that is designed and tuned by hand \u00b2. Agents receive sparse rewards for scoring a goal (positive) and for conceding a goal (negative). They also receive a smaller reward for moving the ball closer to the opponent's goal. We consider two setups: (1) non-kicking: where agents can only move holonomically with 2D continuous action forces and have to physically touch the ball to dribble, and (2) kicking: where agents have two additional actions for rotating and kicking the ball with a continuous force (given that the ball is within feasible range and distance). Agents have full observability over teammates, opponents, and the ball. The match terminates either when a goal is scored or when 500 timesteps have passed."}, {"title": "2.1.2 Two vs. two: diversity is fundamental to success", "content": "We begin by studying the simplest multi-agent soccer setting, consisting of a team of two players facing two opponents. We report reward (success rate) and learned diversity values for teams with various diversity targets (SNDdes), including homogeneous teams, where SNDdes = 0, as well as teams for which we there is no upper bound on diversity (i.e., it is left unconstrained), for both non-kicking (Fig. 2c) and kicking (Fig. 2d) agents. In the reward plot, a score of 1 means that learning agents score in 100% of the matches, while a score of -1 means that the opponents score in 100% of the matches (with 0 signifying a draw on average). The rewards and the opponent strength in the kicking setting are generally higher as the agents in this case have the ability to kick as an advantage over opponents.\nAt all diversity levels, both paradigms are able to learn to score without opponents in the first phase of learning. After the opponents are introduced, homogeneous teams exhibit lower performance compared to heterogeneous ones. This is due to the fact that they converge to locally-optimal policies; the most common resulting homogeneous strategy consists of all agents chasing after the ball (a known suboptimal strategy in soccer). Unconstrained diverse teams perform better, but converge to high effective diversity levels and do not attain the best performance. Team with controlled diversity, on the other hand, find the optimal trade-off between homogeneity (that can be used for sharing low-level skills such as dribbling) and heterogeneity (that allows diversity in higher-level strategies), as shown in Fig. 2ef and described below. For non-kicking agents (Fig. 2c), we see how too little (SNDdes = 0.1) or excessive (SNDdes = 0.5) controlled diversity can be detrimental, but the right level (SNDdes = 0.2) can lead to superior strategies against a challenging opponent. For kicking agents (Fig. 2d), where diversity is higher due to the larger action space, we observe how the right level of diversity (in this case, SNDdes = 0.75) leads to the quickest recovery from the addition of strong opponents, demonstrating that diversity is key to overcoming unforeseen challenges.\nDiverse agents learn to pass. Upon inspecting the cause of the higher performance by diverse teams, we observe that their policies resemble known human strategies. In Fig. 2ef, we report sample renderings with non-kicking (e) and kicking (f) agents. Fig. 2e resembles a crossing pass, where agent 1 heads directly to the opponents' goal, disinterested in the ball, while agent 2 dribbles past an opponent in the corner, eventually getting the ball across, where agent 1 receives it and scores. Fig. 2f"}, {"title": "2.1.3 Five vs. five: the emergence of behavioral roles", "content": "We extend the Soccer task to a five vs. five game, with the aim to study: (1) whether the insights gathered in the 2v2 setting still hold, and (2) whether the increased number of players exacerbates the impact of diversity in teams.\nWe introduce further diversity visualizations (Fig. 3a). In particular, the color of an agent now represents its average behavioral distance from its teammates, evaluated from the point of view of that agent's observation (i.e., by how much does its action differ to the other agents' actions, were they to be in its spot). Furthermore, for non-kicking agents, we plot the agent's action and the actions that its teammates would have taken in its stead.\nThe training pipeline and curriculum remain the same as in the 2v2 setting, while the agents' policy network is changed to incorporate scalability features. This model, shown in Fig. 3b, is composed of two main layers. In the first layer, a Deep Sets [25] architecture is used to process opponents' data in a permutation-invariant manner. This data, alongside ball and agent information, is then used as the node feature in a Graph Neural Network (GNN) that enables agents to communicate. The result of communication is an agent-specific context encoding. In the second layer, the context is given as input to the Diversity Control (DiCo) model.\nComparing the training results (Fig. 3cd) with the ones from the 2v2 (Fig. 2cd), we can confirm that the diversity benefits observed in the 2v2 extend to 5v5. In the non-kicking experiments, controlled diversity in teams obtains an average score of \u2248 1, as opposed to homogeneous teams, which obtain an average score of 0.75. In the kicking experiments, now harder due to the increased player density, diverse teams still achieve the best performance. Unconstrained diversity, however, suffers from low sample-efficiency, diverges to high effective diversity values, and is thus not able to leverage the sharing of commonly useful skills and fails to solve the task. Although, mechanistically, the difference between controlled vs. uncontrolled diver- sity during learning seems subtle, they are in fact completely separate paradigms, with substantially different impact on team performance.\nThe emergence of a goalkeeper. The diversity-inspection tools presented in Fig. 3a allow us to analyze the learned strategies, and to uncover the emergence of a goalkeeper role. In Fig. 3eg, we report renderings where this role has emerged from non- kicking (e) and kicking (g) agents. The pairwise agent behavioral distances (Fig. 3fh) for these strategies confirm that one agent becomes significantly different from the rest. Inspecting Fig. 3e, where agent 3 learns the emergent role, we observe that it learns to stay in its own goal area and wait for incoming opponents, intercepting all their attacks. The plotted actions show a strong bias to navigate to the goal, and in Fig. 3e3 we notice the agent's preferred left action when evaluated in all other agents'"}, {"title": "2.1.4 The final match: heterogeneous vs homogeneous teams", "content": "To further elucidate the benefits of diversity in team-play, we evaluate the best trained 5v5 models for heterogeneous and homogeneous teams in a match against each other."}, {"title": "2.1.5 The synergy of neural diversity and physical differences", "content": "Physical heterogeneity is commonplace in both natural and engineered collectives, yet its relation to behavioral diversity is not clear. To better understand this interplay, we design a variation of the 5v5 Soccer task (Fig. 5a), where three types of embodiment are used: Goalie (big and slow), Defender (average speed and size), and Attacker (small and fast). We train teams with different diversity targets, including homogeneous agents (SNDdes = 0), against the heuristic opponents with the strength curriculum shown in Fig. 5b. The results in Fig. 5c confirm that higher diversity in this task improves performance, better adapting to the increase of the opponents' strength. To explain these results, we plot behavioral differences (Fig. 5d), learned by heterogeneous agents (SNDdes = 0.2), as a function of fixed physical differences (speed), from the perspective of the Goalie. This shows that the learned behavioral distance between agents is proportional to their fixed physical difference, demonstrating that the diversity in the emergent roles aligns with the fixed physical embodiment.\nTo further show the benefits of this mind-body synergy, we evaluate heterogeneous (SNDdes = 0.2, Fig. 5e) and homogeneous (SNDdes = 0, Fig. 5g) agents, trained starting in a 1-2-2 formation, in a novel setting starting in a line formation with random agent order. Diverse agents are unaffected by this change of configuration, and continue to leverage their physical differences at the behavioral level: no matter where the agent is spawned, it maintains its role. Homogeneous agents, which were implicitly inferring their role from starting positions, are not able to adapt to this change, resulting in the same strategy being used for all agents, regardless of their embodiment. The behavioral matrices in Fig. 5fh confirm this, showing how heterogeneous agents are the only ones to learn behavioral differences that are proportional to physical differences. We highlight that: (1) the Goalie and Attacker are the furthest apart,"}, {"title": "2.2 Neural diversity boosts exploration", "content": "Exploration is key to learning from experience, with numerous methods proposing to improve it in multi-agent learning [11, 26-28]. Here we show how controlling diversity intrinsically boosts exploration, thus playing an key role in the overall learning process. To this end, we customize the Pac-Men task [5] (Fig. 6b) so that exploration during learning becomes crucial for performance. Four agents are spawned at the center of a four-way intersection with corridors of different lengths (down : left : up : right = 1 : 2:3:2). They are only able to observe a local area around them and are collectively and sparsely rewarded when consuming food at the end of the corridors. In this task, reward is proportional to exploration success as, to obtain the best performance, agents are required to venture in different directions, some taking longer paths than others.\nWe report results (Fig. 6a) for agents controlled to reach specified diversity ranges. Unlike previous experiments, we employ an inequality constraint on the diversity target, fixing only the minimum, while leaving the maximum unbounded. We also report results for homogeneous agents (SNDdes = 0) with and without the addition of a one-hot index to the their observations. The addition of this index enables homogeneous agents to condition upon it to learn multiple behaviors, avoiding the otherwise only possible policy of all going to the same food particle. However, such a paradigm still does not reach the performance and convergence speed of diversity-controlled heterogeneous agents. Agents controlled to reach higher diversity are able not only to achieve higher rewards and thus explore better, but also present a much faster convergence speed (as can be noticed by looking at the initial reward slope of SNDdes \u2265 2). The renderings in Fig. 6c further elucidate the correlation between diversity and exploration in the task, demonstrating that a minimum amount of diversity is necessary to achieve the full task (i.e., all four food sources are discovered)."}, {"title": "2.3 Neural diversity enables resilience", "content": "Diversity in natural systems has been shown to enable resilience to environmental disruptions [29]. In this section, we demonstrate that a similar paradigm emerges in artificial collectives that exhibit behavioral diversity.\nWe create the Dynamic Passage task (Fig. 6d). Two different-sized agents, physically linked by a joint, need to traverse horizontally two randomly-spawned gaps to reach their goal. While both gaps are initially big enough to fit either agent, the task undergoes a disruption, consisting in one of the gaps narrowing, such that it impedes passage to the lager agent. To overcome this disturbance, agents need to cooperate to find their fitting passageway.\nWe train unconstrained heterogeneous and homogeneous agents, reporting reward and diversity in Fig. 6e. There is no disturbance through iterations 0 to 100, and thus both paradigms learn to solve the task with resulting behavioral homogeneity (heterogeneous agents converge to low diversity values). At iteration 100, the gap narrows, causing both paradigms drop to a reward of 0.5, succeeding the task approximately 50% of the time. During iterations 100-700, heterogeneous agents increase their diversity and learn how to overcome the disturbance, effectively assigning the bigger agents"}, {"title": "3 Discussion", "content": "In this work, we studied the impact and benefits of neural diversity in collective artificial learning. To do this, we decomposed collective learning into three core challenges: cooperation (team play), exploration, and resilience. For each, we showed how heterogeneity provides non-trivial benefits over homogeneous training, demonstrating that diversity is core in all three aspects and, thus, in collective learning as a whole.\nThe insights gathered in this study constitute important progress in the overarching goal of understanding and developing collective machine intelligence. In particular, in Soccer, we have shown the emergence of unbiased behavioral roles in the form of passing and goalkeeping: known strategies already present in human teams that are now confirmed to naturally emerge and outperform spatially-concentrated homogeneous strategies. Furthermore, this emergence occurs and is beneficial even in the case of physically-identical agents, an insight that is hard to gather from human teams. In the case of physically-different players, we showed how they are able to learn diverse behavioral roles corresponding to their embodiment, and leverage this mind-body alignment for increased performance and resilience. We also demonstrated how constraining behavioral diversity acts as a natural exploration enabler, even in the absence of any explicit exploration objective or reward. Lastly, we saw how diverse agents collectively acquire and maintain latent skills that enable resilience, and leverage them to overcome repeated disruptions throughout learning.\nIn future work, we are interested in extending our study to a continual learning setting on real-world robot platforms, investigating the role of diversity in lifelong learning for physical agents."}, {"title": "4 Methods", "content": "Reinforcement Learning (RL) is a paradigm in which agents learn to interact with an environment to maximize a given reward signal. MARL is an extension of RL that involves multiple agents. Our MARL setup can be formalized as a partially observable Markov game."}, {"title": "4.1 Multi-agent reinforcement learning (MARL) preliminaries", "content": "Reinforcement Learning (RL) is a paradigm in which agents learn to interact with an environment to maximize a given reward signal. MARL is an extension of RL that involves multiple agents. Our MARL setup can be formalized as a partially observable Markov game.\nPartially Observable Markov Games. A Partially Observable Markov Game (POMG) [30] is defined as a tuple\n(N, S, {Oi}i\u2208N, {oi}ien, {Ai}ien, {Ri}ien, T, Y),"}, {"title": "4.2 Measuring diversity", "content": "We employ System Neural Diversity (SND) [12] to measure the diversity of policies \\( \\pi_i \\). This metric is briefly presented in the introduction and explained in depth in the respective paper.\nIn relation to this work, we highlight a few properties of this metric that are leveraged. In particular, we parameterize actions as multivariate normal distributions, independent in each action dimension. This allows to compute the Wasserstein distance W2 between agent pairs in a closed-form."}, {"title": "Definition 1 (Wasserstein metric for multivariate normal distributions)", "content": "Let \\( \\pi_1 = N(\\mu_1, \\Sigma_1) \\) and \\( \\pi_2 = N(\\mu_2, \\Sigma_2) \\) be two multivariate normal distributions on \\( R^m \\). Then, the 2-Wasserstein distance between 1 and 2 is computed as:\n\\( W_2(\\pi_1, \\pi_2) = \\sqrt{ ||\\mu_1 \u2013 \\mu_2||_2^2 + trace(\\Sigma_1 + \\Sigma_2 - 2(\\Sigma_2 \\Sigma_1 \\Sigma_2)^{\\frac{1}{2}} )} \\).\nWe consider policies with the form \\( \\pi_i(o) = N(\\mu_i(o), \\sigma_i(o)) \\), with \\( \\mu_i(o), \\sigma_i(o) \\in R^m \\), where oi is a standard deviation vector which uniquely defines a diagonal covariance matrix \\( \\Sigma_i(o) \\in R^{m\\times m} \\), \\( \\Sigma_i(o) = diag(\\sigma_i(o)^2) \\).\nWe further leverage the \"Invariance in the number of behaviorally equidistant agents\" property of SND, which implies that the value of the metric remains constant"}, {"title": "4.3 Controlling diversity", "content": "We employ Diversity Control (DiCo) [13] to control the diversity of policies \u03c0\u012f. This paradigm is briefly presented in the introduction and explained in depth in the respective paper.\nWe update the original implementation with additional features. We enable gradient propagation through the computation of the scaling factor to improve training stability. We extend the method to allow inequality diversity constraints (e.g., SNDdes \u2265 x). These are used for the exploration experiments in Fig. 6a-c. Inequality constraints are achieved by rescaling the policies only in case the current diversity falls outside of the desired range. For the Soccer 5vs5 tasks, we add shared encoder networks (i.e., Deep Sets, GNN) before the DiCo models to compute a context encoding, as shown in Fig. 3b. This improves training efficiency by allowing the diverse DiCo networks to condition of this context instead of raw observations and thus avoid extra computation that is better suited to homogeneous models."}, {"title": "4.4 Training details", "content": "Training is performed in the BenchMARL library [32] using the TorchRL framework [33]. Tasks are implemented and open-sourced in the Vectorized Multi-Agent Simulator (VMAS) [34]. All experiments are performed using the Proximal policy Optimization (PPO) training algorithm [35], either applied independently to all agents (IPPO [36], used in Pac-Men and Tag), or with a critic that takes all agents' data as input (MAPPO [37], used in Soccer and Dynamic passage). Experiment configurations leverage Hydra [38] to decouple YAML configuration files from the Python codebase. The attached code contains thorough instructions on the meaning of configuration parameters and reproducing instructions. Neural network models used for policies (actors) and value functions (critics) are Multi Layer Perceptrons (MLPs). In the case of 2vs2 Soccer and Dynamic passage, policies are able to access data from the other agent (e.g., via communication). In the case of Pac-Men and Tag, policies only have access to data from the ego agent."}, {"title": "4.4.1 5vs5 policy model", "content": "The policy model used in the Soccer 5vs5 tasks has a more complex architecture than simple MLPs to deal with the high (and possibly variable) number of agents and adversaries. The architecture, shown in Fig. 3b, has a first homogeneous encoding layer to process adversary data using a Deep Sets [25] model:\n\\( h_i = p_{\\theta} \\left( \\bigcirc_{j \\in Opp} \\varphi_{\\theta}(o_j) \\right), \\)"}, {"title": "Appendix A Computational resources used", "content": "For the realization of this work, several hours of compute resources have been used.\nIn particular, they have gone towards: experiment design, prototyping, and running final experiments results. Simulation and training are both run on GPUs, so no CPU compute has been used. Among the tasks, Soccer has required the majority of compute due to the long training times required deriving from its complexity. We estimate:\n\u2022 500 compute hours on an NVIDIA GeForce RTX 2080 Ti GPU\n\u2022 1500 compute hours on am NVIDIA L40S GPU\n\u2022 5000 compute hours on an NVIDIA A100-SXM-80GB GPU"}, {"title": "Appendix B Additional Soccer results", "content": "In this section, we analyze further strategies that emerge when constraining agents to be heterogeneous. The heterogeneous constraints indirectly improve the team's spatial coverage of the pitch, playing a similar role to that of a soccer coach. However, they do not prescribe any specific behavior and do not bias the strategies with human knowledge, making all the results shown purely emergent.\nIn the following, we analyze some of the emergent heterogeneous strategies in the two vs. two (Sec. B.1.1) and five vs. five (Sec. B.1.2) settings."}, {"title": "B.1 Further heterogeneous emergent strategies", "content": "In this section, we analyze further strategies that emerge when constraining agents to be heterogeneous. The heterogeneous constraints indirectly improve the team's spatial coverage of the pitch, playing a similar role to that of a soccer coach. However, they do not prescribe any specific behavior and do not bias the strategies with human knowledge, making all the results shown purely emergent.\nIn the following, we analyze some of the emergent heterogeneous strategies in the two vs. two (Sec. B.1.1) and five vs. five (Sec. B.1.2) settings."}, {"title": "B.1.1 Two vs. two", "content": "In Fig. B1 we show two further strategies that emerge when training diverse agents in the 2v2 setting.\nFig. Bla shows an \"opponent blocking\" strategy. Here, in the initial moments of the match, agent 2 focuses on physically blocking the opponent, while agent 1 goes for the ball undisturbed. This proves to be a successful strategy as it is able to buy precious time for agent 16. The agents then proceed to score thanks to a pass from agent 2 through the opponents.\nFig. Blb shows a \"last-second save\" strategy. Here, as soon as the agents loose control of the ball, agent 2 notices it and starts rushing back. By rushing on the side of the pitch, it is able to intercepts the opponent, which was otherwise proceeding undisturbed, and avoid a sure goal at the last second.\nIt is important to note that in both these scenarios only one agent chases the ball at a time, a positive characteristic that we hardly see emerging from homogeneous agents."}, {"title": "B.1.2 Five vs. five", "content": "In Fig. B2 we show two further strategies that emerge when training diverse agents in the 5vs5 setting."}, {"title": "B.2 Homogeneous strategies", "content": "Having analyzed the emergent strategies of heterogeneous agents, we now turn our attention to the strategies learned by homogeneous ones. In particular, we highlight how these strategies prove myopic and resemble the ones emerging in human teams that approach the sport without coaching.\nIn all task setups, homogeneous agents learn to converge and group around the ball. The is due to the fact that ball movement is the main source of reward. Collectively converging towards the reward source can appear as the best way to obtain reward, but shows its limitations in the long term. In fact, despite this strategy leading to success in certain cases (due to the overpowering physical superiority of a large group) it shows significant long-term limitations. Most importantly, spatial grouping prevents passing and proves brittle when opponents gain control of the ball and dribble past the group, not encountering any further line of defense.\nThis type of behavior also emerges in human teams, where players tend to collectively chase the ball in the first phases of learning, effectively not showing good spatial coverage of the pitch. In this analogy, our diversity constraint acts in a similar way to a football coach, incentivizing players to learn different strategies and, as a consequence, improve spatial coverage.\nIn the following, we analyze some of the emergent homogeneous strategies in the two vs. two (Sec. B.2.1) and five vs. five (Sec. B.2.2) settings and highlight their long-term inefficiencies."}, {"title": "B.2.1 Two vs. two", "content": "Fig. B3 reports examples of learned homogeneous strategies for non-kicking (Fig. B3a) and kicking (Fig. B3b) agents in the 2v2 soccer task. In both cases, the agents stay packed together to dribble the ball towards the opponents' goalpost. As soon as one of the opponents intercepts them and gains control of the ball, they try to fall back and start defending. However, being so close together, they are both too advanced, with no agent in a defensive position. Thus, the opponent is able to score undisturbed."}, {"title": "B.2.2 Five vs. five", "content": "Fig. B4 reports examples of learned homogeneous strategies for non-kicking (Fig. B4a) and kicking (Fig. B4b) agents in the 5v5 soccer task. Also in these cases, the agents learn to act similarly, despite being able to learn different behaviors based on the context. Their dense attack pattern can prove quite strong in advancing towards the goal, but as soon they loose control of the ball, they are not able to recover due to the absence of goalkeepers or agents playing defense."}, {"title": "Appendix C Task details", "content": "In this section, we provide a detailed description of he tasks analyzed in this work.\nExplaining in detail their actions, observations, rewards, and termination structure."}, {"title": "C.1 Soccer", "content": "The different setups of the soccer task are shown in Fig. 2a, Fig. 3a, and Fig. 5a. They vary in number of agents, starting locations, and embodiment type. Learning agents (blue) are trained against opponents (red) executing a hand-designed heuristic. Learning agents can be spawned either (1) at uniformly random positions in their"}, {"title": "C.1.1 Observation", "content": "Agents have full observability over teammates, opponents, and the ball. In particular, they observe: their own action force, position, velocity, relative position to the ball, relative velocity to the ball, relative position of the ball to the opponents' goalpost, ball velocity, ball acceleration, relative position and velocity to all opponents, and the opponents' absolute velocities and accelerations. Through communication they are able to access the observations of other teammates as well as compute relative position and velocity to all teammates. Shooting agents additionally observe their rotational heading."}, {"title": "C.1.2 Action", "content": "We consider two setups: (1) where agents can only move holonomically with 2D continuous action forces and have to physically touch the ball to dribble, and (2) where agents have two additional actions for rotating and kicking the ball with a continuous force (given that the ball is within feasible range and distance). Only the kicking action of the agent (including opponents) closest to the ball is allowed to take effect. Opponents are always using actions of type (1)."}, {"title": "C.1.3 Reward", "content": "Agents receive sparse reward of 100 for scoring a goal (positive) and for conceding a goal (negative).\nAgents receive a dense reward for moving the ball closer to the opponents' goalpost. Calling dball,goal \u2208 R the distance between the goalpost and the ball at time t, this reward is computed as:\n\\( 10(d_{ball,goal}^{t-1} - d_{ball,goal}^t), \\)\nincentivizing ball movement towards the goalpost independently of absolute positions.\nLastly, in the initial phase of learning, before the addition of the opponents, we add a small dense reward to incentivize exploration and interaction with the ball."}, {"title": "C.1.4 Termination", "content": "The match terminates when a goal is scored or after 500 steps. Each simulation step corresponds to 0.1 seconds."}, {"title": "C.1.5 Opponents", "content": "Opponents are always spawned at uniformly random positions in their half-pitch.\nThe soccer heuristic is a hand-crafted policy that defines the policy for a variable- sized team with tunable difficulty. It is constructed with a high-level planner and a low-level controller. The high-level outputs trajectories specified by an initial and terminal position and velocity. Then, the low-level controller generates a trajectory with a Hermite spline, and uses a closed-loop controller to track that trajectory. The high-level policy is split into a dribbling policy, when an agent has possession, and off- the-ball movement, when it does not. The in-possession policy generates a dribbling effect with a trajectory that ends in the same position as the ball with a terminal velocity pointing towards the goal. On the other hand, the out-of-possession policy samples random positions on the pitch and evaluates"}]}