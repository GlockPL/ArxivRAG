{"title": "ARXEVAL: EVALUATING THE RETRIEVAL AND GENERATION OF LANGUAGE MODELS FOR SCIENTIFIC PAPERS", "authors": ["Aarush Sinha", "Viraj Virk", "Dipshikha Chakraborty", "Sreeja P. St"], "abstract": "Language Models [LMs] are now playing an increasingly large role in information generation and synthesis; the representation of scientific knowledge in these systems needs to be highly accurate. A prime challenge is hallucination; that is, generating apparently plausible but actually false information, including invented citations and nonexistent research papers. This kind of inaccuracy is dangerous in all the domains that require high levels of factual correctness, such as academia and education. This work presents a pipeline for evaluating the frequency with which language models hallucinate in generating responses in the scientific literature. We propose ArxEval, an evaluation pipeline with two tasks using ArXiv as a repository: Jumbled Titles and Mixed Titles. Our evaluation includes fifteen widely used language models and provides comparative insights into their reliability in handling scientific literature.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have emerged as pivotal tools in information access and generation, particularly through their capabilities of producing factually accurate texts. As these models become increasingly integrated into various applications, ensuring the accuracy of their responses has become very important. The performance and reliability of LLMs in generating accurate information are significantly influenced by multiple factors, including training data quality, model architecture design, and post-training optimization processes [1], [2], [3].\nHowever, a significant challenge in the deployment of LLMs lies in their propensity to generate nonfactual responses, a phenomenon commonly referred to as hallucination. These hallucinations fundamentally undermine the reliability and faithfulness of LLMs, presenting substantial obstacles to their widespread adoption across various domains [4], [5]. The mitigation of hallucinations has consequently emerged as a critical area of research within the field. While various strategies have been proposed and implemented to reduce hallucinations, showing promising improvements in the faithfulness of LLMs for general-purpose tasks, domain-specific applications remain particularly challenging [6], [7], [8].\nIn this paper, we present a comprehensive study evaluating the extent of hallucination in LLMs under domain-specific prompting, with a particular focus on scientific literature. We develop and implement a systematic evaluation pipeline to assess fifteen prominent open-source LLMs: Qwen 2.5 [9], Gemma 2 [10], Llama 3 [11], Phi 3 [12], Orca 2 [13], Mistral v-0.3 [[14], Deepseek-llm [15], Olmo-2 [16], Mistral-Nemo [17], Eurus-2 [18], and Solar-Pro [19]. Our evaluation"}, {"title": "2 Related Work", "content": "2.1 Hallucinations in Large Language Models (LLMs)\nHallucinations in LLMs have been extensively studied and documented. While significant advancements have been made in improving their accuracy and reliability, LLMs have been shown to hallucinate even when tasked with generating responses based on known facts [21]. Such behavior suggests an inherent limitation in these models, reinforcing the hypothesis that hallucination may be an intrinsic characteristic [22].\n2.2 Hallucinations in Domain-Specific Settings\nDomain-specific hallucinations manifest when LLMs generate inaccurate or fabricated information in specialized fields. In domains like biomedicine, such hallucinations can have serious implications, potentially leading to incorrect medical advice or misinterpretation of research data. The fundamental challenge lies in maintaining factual accuracy while preserving the model's ability to generate coherent and contextually relevant responses.\nDomain-specific hallucinations primarily stem from two factors: deficiencies in training data and limitations in model architecture [23]. However, recent research presents an alternative viewpoint, suggesting that under certain conditions, hallucinations could be leveraged as a resource for novel problem-solving approaches [24].\n2.2.1 Detection and Evaluation Frameworks\n\u2022 DelucionQA [25]: A specialized dataset designed for detecting hallucinations in domain-specific question- answering tasks, providing evaluation metrics for retrieval-augmented LLMs.\n\u2022 DAHL [26]: A comprehensive benchmark for evaluating hallucinations in biomedical text generation, featuring atomic unit decomposition and the DAHL Score metric.\n2.3 Hallucinations in Multimodal Settings\nMultimodal hallucinations occur when models generate outputs inconsistent with visual or auditory inputs. This phenomenon is particularly critical in applications like video understanding, where temporal and spatial accuracy are essential [27].\n2.3.1 Evaluation Frameworks\n\u2022 VidHalluc [28]: A specialized benchmark for evaluating temporal hallucinations in video understanding, assessing multiple dimensions including action recognition and scene transitions.\n\u2022 MHaluBench [29]: A meta-evaluation framework for comprehensive multimodal hallucination detection across diverse categories."}, {"title": "2.4 Hallucinations in Natural Language Generation", "content": "In natural language generation tasks such as dialogue generation, abstractive summarization, and neural machine translation, hallucinations frequently manifest as plausible but factually incorrect outputs [30]. These inaccuracies significantly impact the reliability and trustworthiness of these models."}, {"title": "2.5 Hallucinations in Academic Reference Generation", "content": "Academic reference generation represents a critical challenge, with studies demonstrating that even state-of-the-art models frequently generate fabricated or inaccurate citations [31]. This limitation underscores the urgent need for continued research in hallucination mitigation, particularly in tasks where factual accuracy is paramount. In addressing these challenges, our work specifically focuses on Domain-Specific Biases by utilizing over 150 categories of papers from the ArXiv repository, providing a comprehensive evaluation across diverse academic domains."}, {"title": "3 Dataset", "content": "Here, we describe the datasets used for the Jumbled Title task and the Mixed Title task. A total of 176 categories were used to construct the dataset for both tasks. We used 3 titles from each category giving us 528 titles. Figure 1 shows the number of titles from each subject, Computer Science had the most number of categories at 65 having a total of 195 titles. Economics had the least at 3 categories and 9 titles.\nIn Table 3 we provide a further insights into the dataset for both the Jumbled Titles and Mixed Titles tasks. Using the Flesch reading ease score [32], we find that Jumbled Titles falls in the Very difficult to read. Best understood by university graduates range, while Mixed Titles is categorized as Extremely difficult to read. Best understood by university graduates. Using the Gunning fog index [33], we find Jumbled Titles scoring 17 falls into the College graduate level. Mixed Titles scores 19, which exceeds the standard Gunning fog categories but is most appropriately classified as College graduate level. These scores further validate the technical complexity of our dataset. The dataset exhibits diverse title lengths, with Jumbled Titles ranging from 2-24 words and Mixed Titles from 8-33 words. This broad distribution ensures robust evaluation across varying input complexities.\nThe Jumbled Title task includes 528 jumbled titles, distributed evenly among the 176 categories of the ArXiv dataset.\nThe Mixed Title task consists of 265 mixed titles, derived from the 176 categories in the ArXiv dataset."}, {"title": "4 Methodology", "content": "Our pipeline consists of two main tasks as seen in Figure 2. With our prompts, we aim to demonstrate and experiment with how users would typically interact with these models in real-world scenarios. This approach reflects the reality that average users, and even expert users, rarely invest time in fine-tuning their prompts with elaborate instructions during routine interactions.\n4.1 Jumbled Titles\nWe select 5 titles from each category available in the ArXiv dataset. Each title is scrambled within itself, creating a jumbled version of the original title. These jumbled titles serve as input to the LLMs, which are prompted using the following template:"}, {"title": "4.2 Mixed Titles", "content": "In the second part of the pipeline, we create mixed titles by combining two random paper titles from the dataset. These mixed titles are then used as input to the LLMs with the following prompt template:\nWhere [title] represents the mixed title.\nAlgorithm 2 shows how the Mixed Titles dataset is built. After the language models generate their responses, we perform a detailed evaluation of the provided DOIs using the Crossref API, DataCite API, UnPaywall API and OpenAlex API to ensure thorough search of all possible DOIs is done. The evaluation process involves the following two steps:\n1. Checking DOI Validity: For each model-generated DOI, we verify its existence through API requests to established academic databases. This validation step is crucial for assessing the model's ability to generate legitimate academic identifiers.\n2. Verifying Title Accuracy: For validated DOIs, we retrieve the official paper titles from the corresponding databases and compare them against the model-generated titles. This comparison enables us to evaluate the model's accuracy in associating DOIs with their correct academic works."}, {"title": "5 Results", "content": "To run the inference on the models we used 2\u00d7 T4 Tesla [16GB]. We made use of PyTorch [36] and Huggingface's Transformers [37]. It took us approximately 2.5 to 3 hrs on average to run the entire pipeline for each model. We also use 4-bit quantization using bitsandbytes [38] for faster and efficient inference.\nIn Table 4, we present the performance of models on the Jumbled Titles task. Mistral v0.3 [39] achieved the highest similarity scores, with 0.605 on cosine similarity, 0.542 on BERTScore and 0.607 on STS. This was followed by Qwen2.5 (7B) being the second best performing model overall. On average, BERTScore reduced the similarity between the texts by 1.068% compared to cosine similarity. The worst performing model on all tasks was Orca-2 (13B) with cosine similarity, BERTScore and STS at 0.476, 0.475, 0.477 respectively averaging 0.476."}, {"title": "6 Conclusion", "content": "This paper evaluates the extent of hallucination in state-of-the-art language models by designing two tasks: Jumbled Titles and Mixed Titles. In the Jumbled Titles task, the fifteen evaluated models achieved an average cosine similarity score of 0.544, 0.509 on BERTScore, and 0.545 on STS. Mistral-v0.3 was the best-performing model across all metrics averaging 0.585 on the Jumbled Titles task, outperforming models twice and thrice its size.\nFor the Mixed Titles task, while models generated DOIs for the mixed titles, they often cited non-existent papers or mismatched DOIs. These results underscore critical limitations in maintaining factual accuracy in domain-specific contexts. On average, valid DOIs were generated only 17.75% of the time. Every model also completely failed to generate the corresponding DOI for the title they generated, showing that models struggle with maintaining factual consistency. To further highlight Prompt Adherence, it is worth noting that none of the models generated the required number of two DOIs for each Mixed Title. This results in a discrepancy, as the expected output for each model was 530 DOIs (given 265 mixed titles), but none of the models met this requirement, as seen in Figure 4."}, {"title": "6.1 Model Size Performance", "content": "In Table 4 and Table 5, we observe a concerning trend where many of the larger models are significantly outperformed by their smaller counterparts in both tasks. For instance, the 7B Mistral-v0.3 outperforms models up to three times its size, while the Solar Preview(22B) demonstrates mediocre performance despite its substantially larger parameter count, even for the same models with Qwen2.5 where the 7B parameter model outperforms the 14B parameter variant. This pattern raises serious questions about the relationship between model size and task performance. The results starkly highlight that simply scaling up model parameters does not guarantee better performance in specialized tasks, particularly those requiring precise adherence to instructions and factual accuracy. This counterintuitive finding challenges the common assumption that larger language models inherently perform better, suggesting that architectural choices and training approaches might be more crucial than raw parameter count for achieving superior performance."}, {"title": "7 Limitations", "content": "There are several limitations to our work:\n1. Model Selection: Our evaluation focuses on smaller models due to computational constraints. Results may differ significantly with larger variants, which could exhibit different performance characteristics. Although our findings in Section 6.1 shows that might not always be the case, as we observed smaller models often outperforming their larger counterparts."}, {"title": "2. Model Quantization", "content": "We use 4-bit quantization for inference. While this may reduce performance, studies suggest the impact is minimal [40]. The trade-off between computational efficiency and potential performance impact was deemed acceptable for our experimental setup."}]}