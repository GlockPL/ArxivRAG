{"title": "Sim911: Towards Effective and Equitable 9-1-1 Dispatcher Training with an LLM-Enabled Simulation", "authors": ["Zirong Chen", "Elizabeth Chason", "Noah Mladenovski", "Erin Wilson", "Kristin Mullen", "Stephen Martini", "Meiyi Ma"], "abstract": "Emergency response services are vital for enhancing public safety by safeguarding the environment, property, and human lives. As frontline members of these services, 9-1-1 dispatchers have a direct impact on response times and the overall effectiveness of emergency operations. However, traditional dispatcher training methods, which rely on role-playing by experienced personnel, are labor-intensive, time-consuming, and often neglect the specific needs of underserved communities. To address these challenges, we introduce Sim911, the first training simulation for 9-1-1 dispatchers powered by Large Language Models (LLMs). Sim911 enhances training through three key technical innovations: (1) knowledge construction, which utilizes archived 9-1-1 call data to generate simulations that closely mirror real-world scenarios; (2) context-aware controlled generation, which employs dynamic prompts and vector bases to ensure that LLM behavior aligns with training objectives; and (3) validation with looped correction, which filters out low-quality responses and refines the system performance. Beyond its technical advancements, Sim911 delivers significant social impacts. Successfully deployed in the Metro Nashville Department of Emergency Communications (MNDEC), Sim911 has been integrated into multiple training sessions, saving time for dispatchers. By supporting a diverse range of incident types and caller tags, Sim911 provides more realistic and inclusive training experiences. In a conducted user study, 90.00% of participants found Sim911 to be as effective or even superior to traditional human-led training, making it a valuable tool for emergency communications centers nationwide, particularly those facing staffing challenges.", "sections": [{"title": "1 Introduction", "content": "Emergency response services are essential for public safety, managing around 240 million 911 calls annually, based on year-round stats from New York City (NYC-911 2022). However, there is a critical staffing shortfall, with a third of centers reporting more vacancies in 2023 compared to 2019, resulting in approximately 25,000 unfilled positions nationwide. This staffing crisis increases the workload on current staff (Chen et al. 2022, 2023), leading to dispatcher burnout and impacting emergency service quality (NICE 2023). As urban areas in the US grow, the strain on emergency response systems intensifies. Rapid urbanization and population growth demand effective solutions to adapt to and manage these increasing pressures (Ma et al. 2019).\nTraditional training environments prepare trainees for real-world dispatcher roles by employing role-playing scenarios where experienced dispatchers coach trainees through simulated calls. The need for experienced dispatchers to participate in training diverts essential personnel from actual emergency duties, causing inconsistencies in training quality and reducing the availability of skilled staff, particularly in underserved areas (Saxon et al. 2022; Afonso 2021). However, traditional training methods, which rely on role-playing by experienced personnel, are labor-intensive, time-consuming, and frequently overlook the specific needs of underserved communities.\nIn light of these demands, exploring innovative technological solutions is critical. Advancements in artificial intelligence, especially Large Language Models (LLMs), offer promising methods for enhancing training environments. Employing LLMs to simulate caller interactions can reduce reliance on human resources, improving training efficiency and consistency (Naveed et al. 2023; Wang et al. 2023; Carta et al. 2023). However, directly applying plain LLM agents is not ideal. In our preliminary investigations, we identified the following challenges: (1) Achieving consistent realistic simulations is difficult without detailed factual databases, despite meticulous prompt engineering across different LLMs. This lack of realism results in simulations that do not fit the local context, making the training less effective and potentially confusing for trainees. (2) LLMs excel in generating coherent content but tend to fabricate details, undermining authenticity. Simulations with fabricated geographic information lead dispatchers to make decisions based on incorrect data, compromising emergency response effectiveness. (3) The needs of vulnerable populations in metropolitan areas are often understudied during conventional training, leaving practitioners unprepared. This lack of inclusiveness results in biased training, inadequately preparing dispatchers for handling calls from vulnerable groups, and leading to disparities in emergency response. (4) The inherently complex nature of 9-1-1 calls presents significant challenges, even for human trainers. Human-led training may also fail to capture these complexities, as discussed in detail in Section 2.\nIn this paper, we introduce Sim911, the first system that leverages LLMs to simulate realistic 9-1-1 calls, specifically designed to enhance dispatcher training. Sim911 focuses on creating effective and equitable simulation experiences tailored to the local metro area. Sim911 comprises three key components: knowledge construction, context-aware controlled generation, and validation with looped feedback. Knowledge construction organizes real-world information into retrieval knowledge bases, while context-aware controlled generation fine-tunes the LLM's behavior through human-designed instructions. Validation with looped feedback ensures high-quality outputs by filtering out low-quality responses.\nWe summarize our technical innovations and contributions as follows: (1) Innovative Knowledge Construction from 9-1-1 Calls: Sim911 organizes real-world call data into detailed knowledge bases, allowing for the generation of contextually accurate and realistic training simulations, supporting 57 different incident types. (2) Context-Aware Controlled Generation: Sim911 strategically and dynamically uses advanced techniques, such as Chain-of-Thought (CoT) and Retrieval-Augmented Generation (RAG), to tailor LLM behavior during training sessions. (3) Validation with Looped Correction: Sim911 includes a unique validation process that filters out low-quality responses, ensuring high-quality and scenario-appropriate outputs. (4) Focus on Social Equity: Sim911 emphasizes training that addresses the needs of underserved and vulnerable communities, incorporating relevant data to better prepare dispatchers for diverse real-world scenarios. (5) Real-World Deployment and Evaluation: Successfully deployed in DEC, Sim911 has proven an effective tool in enhancing 9-1-1 dispatcher training from experimental results on real-world data.\nBeyond technical advancements, Sim911 delivers significant social impacts: (1) Sim911 has been successfully deployed in DEC's training programs, seamlessly integrated into 4 training classes across different service sites. (2) To the date of this paper, Sim911's system logs reveal a total active simulation time of 26.55 hours, effectively saving this time for MNDEC dispatchers. (3) Sim911 supports 57 different real-world incident types and covers 14 caller tags, such as \"unhoused\u201d and \u201cnon-English speaking,\" to enrich caller profiles. (4) In a user study conducted with DEC, 90.00% of participants found Sim911 to be as effective or even superior to traditional human-led training. Additionally, Sim911 received an average helpfulness score of 4.89 for its assistance in call-taking training. (5) Sim911 has the potential to assist emergency communications centers across the U.S. with limited staffing by enabling trainees to engage individually with the training program."}, {"title": "2 Motivating Study", "content": "We analyzed 11,841 real-world phone call recordings (from Nov. 2022 to May 2024) and manually reviewed 33 conventional training pieces (see examples in the Appendix, leading to the following observations.\nTraditional training is laborious and time-consuming. In traditional training setups, each trainee engages in call simulations, assuming three roles: the call-taker, the caller, and the instructor. The trainee, as the call-taker, manages calls directed by the instructor and an experienced dispatcher. Each trainee typically participates in 60 independent simulated calls, with each call requiring the involvement of three participants. Based on past recordings, the average call duration is 3.5 minutes; with an average of 12 trainees per session, the total time commitment for experienced dispatchers amounts to at least 84 working hours per session.\nReal-world 9-1-1 calls cover a wide spectrum of incident types and contextual scenarios. From our analysis of past phone call recordings, we identified over 200 distinct incident specifications. However, during initial training, each trainee is exposed to only 40 incident types and 15 call templates. Our review reveals that, on average, trainees cover only 48.00% of the incident types in the first 3-day program, and only 61.54% of special contexts or requests are adequately addressed. This limited exposure fails to prepare trainees for the variety of incidents they will encounter.\nCaller images are critical for call-taking training but rarely considered. Equity and inclusiveness are often overlooked in conventional role-playing simulations during dispatcher training. Even with guidance from experienced dispatchers, these simulations frequently struggle to empathetically and accurately capture the nuanced experiences of vulnerable groups. Among the 33 training scenarios we reviewed, only 4 focused on vulnerable populations (such as non-native English speakers, who may use different language patterns; and callers from lower-income housing areas, who might have limited access to personal vehicles) representing just 12.12%. However, government statistics, see Figure 1, indicate that the needs of various vulnerable groups are significantly reflected in real-world 9-1-1 calls. This discrepancy highlights the importance of incorporating diverse caller images into training, as different scenarios might require distinct call-taking skill sets to effectively handle real-life situations."}, {"title": "3 Methodology", "content": "This section first provides an overview of Sim911. Then we introduce the technical aspects of how Sim911 works in Sections Knowledge Construction, Context-aware Controlled Generation, and Validation with Looped Correction. Sim911 simulates calls by playing the role of 9-1-1 callers and interacting directly with the trainees. It comprises three main components, depicted in Figure 2: knowledge construction, context-aware controlled generation, and validation with looped correction. During the Knowledge Construction phase, 11,841 calls are analyzed to develop knowledge bases containing tags for incident specifics and caller images. This ensures that pertinent information is readily available during simulations. At runtime, Sim911 utilizes these tags to select the most suitable LLM backends, query the knowledge bases, and generate prompts in the context-aware controlled generation phase. This process tailors prompts to include incident details and caller profiles, ensuring that LLM responses align with simulation requirements. The Validation with Looped Correction component filters out low-quality responses in as the simulation goes."}, {"title": "3.1 Dynamic Knowledge Construction", "content": "As a first step, we build an in-depth review and sophisticated reconstruction of the existing dataset, see running examples in Appendix, which has three key data sources: Computer-Aided Dispatching(CAD) logs, archived 9-1-1 call recordings, and their corresponding transcriptions.\nDetailing Two Components in 9-1-1 Calls We integrate insights from dispatcher teams at MNDEC to identify two key components in 9-1-1 call handling: Incident Specifications and Caller Images. We use finely-grained tags for each data entry to create more accurate simulations. In our annotation work, we manually review each call and apply all relevant tags. A single call may be annotated with multiple tags to ensure comprehensive coverage.\nIncident Specifications (IS) capture critical details of incidents, including: (1) Incident Type: Categorizes the incident, from routine (e.g., illegal parking) to critical (e.g., severe medical emergencies). (2) Scenario Context: Adds situational context, such as environmental conditions (e.g., severe weather), potential threats (e.g., sightings of firearms), or specific events (e.g., large public gatherings). (3) Special Requests: Identifies specific instructions, like the need for specialized units (e.g., bomb squads) or coordination with other agencies (e.g., fire departments).\nCaller Images (CI) create a comprehensive caller profile, enhancing the LLM's understanding of the caller's perspective, especially for vulnerable groups. This includes: (1) General Tags: Profiles the caller by age (e.g., minor, adult) and emotion (e.g., \u201cneutral\u201d, \u201canxious\u201d). Dispatchers assign these tags based on conversation clues (e.g., \"My mom is mid-70s and living alone\") or voice analysis. These tags are less sensitive and linked to pre-customized LLM agents to avoid identification confusion (Wei, Haghtalab, and Steinhardt 2024). (2) Vulnerable Groups: These government-introduced tags include descriptors such as \"low-income housing area\" (if the call originates from a lower-income area, according to year-round statistics), \u201cmental health\u201d (if the caller exhibits potential mental health issues, such as bipolar disorder or depression; inferred from the conversation), \u201cnon-native speaker\u201d (if the caller uses limited English), and \"unhoused\" (if the caller indicates lack of stable, permanent housing, inferred from conversation clues). These tags are considered highly sensitive and remain hidden during runtime due to ethical concerns.\nSpecializing Knowledge Bases for Contextual Control We leverage the Retrieval Augmented Generation (RAG) approach, which enhances LLMs for tasks requiring deep knowledge by incorporating external databases as reference points during content generation. This methodology, as discussed by (Lewis et al. 2020), improves the LLMs' ability to provide accurate and relevant outputs. Here, we introduce the two major knowledge bases for runtime use.\nFactual Bases. First, we build a static base, which contains factual knowledge: (1) Validated Address List: a comprehensive list of real addresses within the local area; (2) Encoded Map with Connectivity Information: beyond simple address listings, this map provides detailed information about the connectivity between locations; (3) Tree-Structured Protocols: a collection of protocols for various types of emergency incidents, organized in a tree structure. These protocols detail the question sequence dispatchers should follow, ensuring Sim911's simulations adhere to procedural standards of emergency response.\nRetrievable Bases. This base includes data entries tagged according to Incident Specifications (IS) and Caller Images (CI). The retrievable base allows Sim911 to query and retrieve necessary data samples that enhance the simulation experience during runtime."}, {"title": "3.2 Context-aware Controlled Generation", "content": "Each simulation runtime begins with predefined tags (referred to as 'instructions') that detail the desired scenarios. These tags delineate Incident Specifications (IS) and Caller Images (CI), guiding the setup for each simulation. We select the most appropriate preset backend for each simulation based on less sensitive CI attributes (emotion, age) and gather data associated with these tags from knowledge bases. This ensures a well-informed simulation environment tailored to the given simulation instructions. Context-aware Controlled Generation employs advanced prompting techniques, including Chain-of-Thought (CoT), Retrieval-Augmented Generation (RAG), and Few-shot Prompting (FSP), to enhance LLM performance (Touvron et al. 2023; Kaplan et al. 2020; Wei et al. 2022). Unlike the direct and static application of these techniques, our approach dynamically adapts to the emergency response context, with a focus on both IS and CI. The context-aware controlled generation process consists of three major steps, illustrated in the running example in Figure 3: (1) Vector Base Incorporation: We statically mount the fact bases and dynamically retrieve all past call pieces associated with given tags in both IS (e.g., crash report, medical emergency, severe weather) and CI (e.g., non-English speaking, adult, unhoused). The LLM backend is granted access to both vector bases; (2) Detailed Task Explanation: We elaborate on the instruction through step-by-step explanations, setting the stage for how the simulation should proceed. This preparation allows the LLM to conceptualize the simulation's context and objectives before initiation; (3) Caller Image Deciphering: By including examples of past utterances and interactions associated with both similar tags, we provide the LLM with contextually relevant examples to draw from. This repository of past interactions enriches the LLM's understanding and ability to generate responses that are both consistent with the user's profile and grounded in real-world examples. See detailed algorithmic description in Appendix."}, {"title": "3.3 Validating with Looped Correction", "content": "During runtime, Sim911 employs an in-context validation process with a co-pilot design (Chen et al. 2024) to prevent negative examples and iteratively loops back to the LLM backend until a validated response is obtained. To avoid infinite loops and reduce latency, threshold of 3 is applied considering both latency and accuracy under deployment, see detailed discussion in Appendix. The In-Context Validation process includes four key checks: (1) Format check: This check ensures that the generated response adheres to the expected format. Any response that violates the format requirements is discarded to prevent system errors. (2) Alignment check: This step utilizes a BERT-based classifier (Devlin et al. 2019) to extract incident specifications from the response. The extracted specifications are then compared with the expected instructions, and any misalignment results in the response being discarded. (3) Factual check: A ROBERTa-based question-answering framework (Liu et al. 2019) is used to query key details, such as location information, by asking preset questions (e.g., \"What is the address?\"). If the extracted address does not exist in real life, the response is discarded. (4) Human-in-the-Loop check: This step allows users to provide immediate feedback on the generated response, supporting both written comments and scaled (1-5) ratings. Users can reject any response that does not meet their standards, and this feedback is systematically collected for further analysis. See details in Appendix."}, {"title": "4 Evaluation of Sim911", "content": "Sim911 introduces a pioneering AI-driven system to enhance call-taker training for emergency response scenarios. Due to its novelty, there is limited existing literature to guide its evaluation. To provide a comprehensive assessment, we not only report system-level performance but also conduct a study on the component-wise impacts using pre-configured runtimes. This approach allows us to evaluate Sim911 component by component without disrupting its ongoing deployment at DEC. Therefore, our evaluation of Sim911's performance, focusing on effectiveness and equity, consists of two components: (1) component-wise analysis using pre-configured runtimes, and (2) system-level assessment during real-world deployment. GPT-40 is tested to be the optimal LLM backend for Sim911 by the date of submission. Refer to the complete baseline comparison in Appendix. We fetched GPT-40 responses using OpenAI API and tested the workflow on a machine with a 2.50GHz CPU, 32GB RAM, and Nvidia RTX 3080Ti GPU.\nComponent-wise analysis with pre-configured runtimes: We begin by extracting instructions from 2,641 past calls in the MNDEC database, spanning 13 incident types (e.g., Motor Vehicle Accidents 11%, Lost and Stolen 10%, Aggressive Drivers 10%) from Nov. 2022 to May 2024, based on Incident Specifications (IS) and Caller Images (CI) provided by expert annotations. We then replicate the dialogue flows using rule-based scripts that mimic the questions typically asked by call-takers. These instructions and replicated dialogue flows are used to simulate calls with Sim911. Sim911 operates without access to highly granular details. For example, if a past call involved an abandoned vehicle with a specific license plate and tinted windows, Sim911 would only be informed that the simulation involves an abandoned vehicle, without further specifics like the license plate or tinted windows. To ensure a fair evaluation, we exclude these granular discrepancies from our analysis. Effectiveness and equity scores are obtained through call-wise comparisons under control experiments. We record average scores with standard deviations to ensure robust evaluation.\nSystem-level assessment during real-world deployment: During Sim911's deployment, we collected data from 3,416 system interactions and 3,409 user interactions across both complete (228) and incomplete simulations, each guided by its own set of instructions (IS and CI). These data are utilized in assessing Sim911's authenticity. Additionally, we conducted a user study in collaboration with MNDEC to evaluate Sim911 at a system level. This study involved trainees and personnel from DEC, including those from training management and quality assurance. The user study gathered scaled feedback (1-5) from MNDEC personnel on several key aspects, e.g., realism (\u2018How similar or vivid are the calls generated by Sim911 compared to real-world calls?'), authenticity ('Are Sim911's responses valid and true to real-life situations?'), equity (\u2018How well does Sim911 simulate the experiences of vulnerable callers?'), and helpfulness ('How helpful is Sim911 in assisting with call-taking training?'). Written comments were also collected to provide additional insights. See survey details in Appendix. To assess effectiveness and equity, we review system logs and user feedback, with further details discussed later."}, {"title": "4.1 Effectiveness of Sim911", "content": "We assess Sim911's effectiveness by following aspects: realism and authenticity.\nRealism: \"How closely do Sim911's simulations mirror real-world calls?\" We use the following metrics to evaluate Sim911's performance on pre-configured runtimes: Perplexity (a measure of distributional similarity commonly used in language model training; it assesses how reasonable the generated texts are compared to a reference set), METEOR (Banerjee and Lavie 2005) (text generation metric that balances precision and recall, considering word stems, synonyms, and word order to determine how closely a generated text mirrors a reference text), and TTR (Type-Token Ratio; measures lexical diversity by comparing the number of unique words to the total number of words in the text).\nAuthenticity: \"Does Sim911 provide accurate, true-to-life information without fabricating given instructions?\" For evaluation, we break authenticity down into two aspects: \"matter of facts\" and \"simulation alignment.\" For the first, we focus on the accuracy of the given location in a simulation, as recommended by MNDEC experts. We use the Google Maps API with Geocoding (Google Maps Platform 2024) to verify the geographic information provided in the simulation and report the locating success rate. To measure simulation alignment, we use the copilot's results to determine if the indicated Incident Specification (IS) aligns with the one provided in the simulation instructions. System-level performance during real-world deployment is assessed through quantitative analysis of system logs.\nFrom the statistics in Table 1, we observe the following key points. When all components are enabled, Sim911 achieves optimal results in both realism (PPL=11.07, METEOR=0.85) and authenticity (GMap=99.19%, SAR=98.42%). Disabling knowledge construction (KC) and the RAG sub-component of CaCG leads to significant drops in realism (PPL=31.22 and PPL=57.19, respectively). Similarly, turning off validation with looped correction (VLC) reduces both realism and authenticity, though the system remains moderately effective. When all components are disabled, the system's performance declines significantly, particularly in realism (PPL=61.99) and authenticity (SAR=81.63%). In conclusion, Sim911 demonstrates high effectiveness in terms of realism and authenticity in real-world deployment when all components are active. Disabling components harms Sim911's overall effectiveness."}, {"title": "4.2 Equity of Sim911", "content": "We assess Sim911's equity features by evaluating \"how effectively it provides simulation experiences for different caller groups\", represented by each supported tag in the caller image (CI). Recognizing that some tags are subjective and challenging to quantify, we adopt two general approaches to study these equity features. We employ fine-tuned BART (Lewis et al. 2019), a state-of-the-art model for zero-shot text classification, to evaluate Sim911-generated emergency call texts against a predefined set of image tags. For each generated call $x_i$, associated with ground truth tags $T(x_i)$, BART predicts the presence or absence of each tag $T_j$ using a binary classifier $C_j(x_i)$, which outputs 1 if $x_i$ is associated with $T_j$, and 0 otherwise. The predicted tags form a binary vector $\\uparrow(x_i) = \\{C_1(x_i), C_2(x_i),..., C_k(x_i)\\}$. Accuracy for each call is calculated by comparing $\\uparrow(x_i)$ with $T(x_i)$ using the formula $Acc(x_i) = \\sum_{j=1}^{k} I (C_j(x_i) = I(t_{ij} \\in T(x_i)))$. This classification is iteratively performed for each tag, and the overall accuracy is determined by averaging the individual accuracies across all generated calls as BART Score = $\\frac{1}{n} \\sum Acc(x_i)$. Second, we perform a textual similarity analysis based on syntax (Context-Free Grammar Parser), lexicon (TF-iDF), and sentiment (Loria 2018). We compare the generated outputs tagged as A with both the ground truth tagged as A and not-A. To quantify the strength of classification for each tag, we calculate the Margin Score (Similarity(A) \u2013 Similarity(\u00acA))/(Similarity(A) + Similarity(A)), where Similarity(.) is the overall syntactic similarity of the output to reference texts with a given tag.\nBesides these two approaches, we additionally introduce the following tag-specific evaluation methods: (1) NRCLex (Mohammad and Turney 2013) for unsupervised textual emotion detection, where we analyze the accuracy similarly to the BART Score. (2) Gunning Fog Index, a well-known method in linguistics of text readability analysis, is used to assess the readability of the text for non-native English speakers. Gunning Fog Index outputs a readability level and we analyze this score similarly to the Margin Score.\nFrom the statistics in Table 2, we derive the following findings. Sim911 achieves strong performance across all caller image tags when all components are enabled, including age groups (BART=83.11%, Margin=0.34), emotion ranges (BART=85.66%, Margin=0.36), and unhoused populations (BART=73.94%, Margin=0.21). Disabling the FSP sub-component of CaCG results in notable declines for age groups (BART=62.90%, Margin=0.09) and mental health tags (BART=64.55%, Margin=0.11). Turning off knowledge construction (KC) significantly reduces performance, especially for low-income housing (BART=51.13%, Margin=0.04) and mental health (BART=67.67%, Margin=0.17). Similarly, disabling validation with looped correction (VLC) leads to a decrease in metrics for mental health (BART=83.22%, Margin=0.21). When all components are disabled, the system's performance deteriorates significantly, particularly for the low-income housing tags (BART=44.44%, Margin=0.04) and non-native speakers (BART=69.13%, Margin=0.21). In conclusion, Sim911 delivers equitable and inclusive simulations in real-world deployment when all components are enabled. Disabling components negatively impacts Sim911's equity features."}, {"title": "4.3 Insights from User Study", "content": "We collected 10 anonymous feedback from trainees (x2), active call-takers/dispatchers (x2), and training officers (x6) at DEC. Surveys are contributed by MNDEC based on the availability. Responses included yes/no questions, written comments, and a scaled rating system: Not at all (1), Neutral (2), Somewhat (3), Very much (4), and Perfectly (5). We find following insights. See complete survey setup in Appendix.\nEffectiveness and Equity: Sim911 received scores of 4.50 for realism and 4.70 for authenticity. In terms of equity, it performed well across various caller image tags, with average scores as follows: Age Groups (4.25), Emotion Ranges (4.20), Unhoused (4.10), Mental Health (4.25), Non-Native Speakers (4.25), and Low-Income Housing (4.10). Additionally, Sim911 earned an average score of 4.89 for \"How effectively does Sim911 support call-taker training in real-life scenarios?\". One participant commented: \"I was surprised by how well it handled a call as a pregnant woman. I even managed to successfully deliver a baby on the phone!\" Another shared: \"When it played the role of a kid caller, it acted just like a real child- refusing to do anything until his mom arrived on the scene.\" These results emphasize Sim911's effectiveness in preparing call-takers by simulating diverse caller profiles and challenging real-life situations.\nComparison to Human-led Training: 9 out of 10 participants found Sim911 to be on par with or better than traditional human-led training. One participant remarked: \"Sim911 is a great starting point because it comes up more incident types than what we do right now. It's a valuable tool for enhancing our training.\" Another said: \"It's impressive how Sim911 can simulate different callers (images). Trainees can be exposed to rare but useful calls that we could not (simulate) in the past.\" These findings highlight that Sim911 not only complements human-led training but also enhances it by providing a broader range of incident types and scenarios that are difficult to replicate manually."}, {"title": "5 Related Work", "content": "Simulation-based training is a key component in various fields such as healthcare, aviation, and emergency services, where it provides a controlled environment for skill development without real-world risks (Suresh et al. 2023; Preiksaitis and Rose 2023; Daun et al. 2023). This method enhances critical thinking, decision-making, and practical skills by allowing repeated exposure to diverse and sometimes hazardous scenarios (Ibrahim et al. 2023; Flores, Ziakkas, and Dillman 2023; Rahman et al. 2023). Recent technological advancements, including Augmented Reality (AR), have begun to enhance traditional training setups, offering more immersive training experiences (Fitria 2023; Pfaff et al. 2020; Li et al. 2018; Ummenhofer et al. 2019), especially those for emergency responses (Parry et al. 2022; Mehta et al. 2022). Despite these innovations, most training simulations still rely heavily on human-scripted scenarios and instructor feedback, which can limit scalability and adaptability (Violato et al. 2023; Salvato et al. 2021; de Paula Ferreira, Armellini, and De Santa-Eulalia 2020). Large Language Models (LLMs) are emerging as a transformative tool for dialogue-focused simulations, able to generate dynamic and realistic interactions (Webb 2023; Thoppilan et al. 2022; Gong et al. 2023). But their integration into training programs must carefully address accuracy, ethical concerns, and potential biases to ensure effectiveness (Shanahan, McDonell, and Reynolds 2023; Shayegani et al. 2023; Yao et al. 2024b; Salewski et al. 2024). Refer to our extended relation work in Appendix."}, {"title": "6 Summary", "content": "In this paper, we introduce Sim911, the first AI-driven simulation environment designed to assist 9-1-1 dispatcher training under emergency response scenarios. Sim911 aims to enhance the preparedness of emergency dispatchers, contributing to the resilience and safety of urban populations. Evaluation results on pre-configured runtimes and real-world deployment show that Sim911 effectively delivers realistic, authentic, and equitable simulations, to assist dispatcher training with the integration of knowledge construction, context-aware controlled generation, and validation with looped correction.\nThis work can help emergency communications centers with limited staffing by allowing trainees to interact individually with the training program. Nearly 6,000 emergency communications centers could benefit from this training opportunity. The GenAI-enabled solution can be extended to other training spaces, such as teachers and medical students."}, {"title": "A Appendix", "content": "Discussion and Future Work\nWe plan to enhance the system in several aspects. (1) Knowledge Base Timeliness: Real-time updates are crucial for accuracy (Chen et al. 2022, 2023), but they can add overhead. Enhancing timeliness with text ranking or abstract representation will improve Sim911. (2) Hallucination: Despite using advanced LLM backends like GPT-40, the simulation sometimes generates inaccurate or fabricated information. Further research is needed to mitigate these hallucinations and enhance Sim911's credibility. (3) Systematic Review on Expert Feedback: Collecting and systematically reviewing expert feedback during deployment will provide valuable insights for improving Sim911 and other simulation tools. (4) Model Exploration: We will explore additional implementations of Sim911 by introducing and fine-tuning more LLMs, such as LlaMa (Touvron et al. 2023) and Falcon (Almazrouei et al. 2023)."}, {"title": "B Related Work (Extended)", "content": "Simulation-based training is recognized as a pivotal component in preparing individuals for various scenarios across different occupations (Suresh et al. 2023; Preiksaitis and Rose 2023; Daun et al. 2023). This training methodology offers a safe and controlled environment for learners to acquire and refine their skills without the real-world consequences of mistakes. Studies show that simulation-based training enhances decision-making, critical thinking, and practical skills, making it an indispensable tool in fields such as healthcare, aviation, and emergency response services, which require high-stakes decision-making (Ibrahim et al. 2023; Flores, Ziakkas, and Dillman 2023; Rahman et al. 2023). Simulation allows for repeated exposure to a wide range of scenarios, some of which may be infrequent or too dangerous to practice in real life, thereby ensuring that trainees are well-prepared for any situation.\nIn recent years, traditional training setups have begun to evolve, incorporating advanced technologies such as Augmented Reality (AR) and 3D Modeling to provide more immersive and interactive learning experiences (Fitria 2023; Li et al. 2018; Ummenhofer et al. 2019; Pfaff et al. 2020). These technological advancements aim to bridge the gap between simulation and real-life experience, offering more realistic and engaging training environments. However, despite these innovations, the core of most training simulations remains largely human-based, relying on scripted scenarios and instructor feedback to guide the learning process (Violato et al. 2023; Salvato et al. 2021; de Paula Ferreira, Armellini, and De Santa-Eulalia 2020). This dependence on human-generated content and feedback limits training programs' scalability and adaptability and increases training costs, especially in scenarios that demand rapid updates or customization to address emerging challenges.\nLarge Language Models (LLMs) demonstrate remarkable performance in generating human-like text, opening new avenues for their application in dialogue-focused simulations (Webb 2023; Thoppilan et al. 2022; Gong et al. 2023). With their ability to understand and generate natural language, LLMs offer more dynamic and responsive simulation experiences, adapting to the trainee's inputs in real time to create more personalized and unpredictable training scenarios (Shanahan, McDonell, and Reynolds 2023). This capability sets the stage for LLMs to revolutionize training simulations, particularly in domains like emergency call dispatching, where the ability to navigate complex dialogues under pressure is crucial. However"}]}