{"title": "A sketch of an Al control safety case", "authors": ["Tomek Korbak", "Joshua Clymer", "Benjamin Hilton", "Buck Shlegeris", "Geoffrey Irving"], "abstract": "As LLM agents gain a greater capacity to cause harm, AI developers might increasingly\nrely on control measures such as monitoring to justify that they are safe. We sketch how\ndevelopers could construct a \"control safety case\", which is a structured argument that\nmodels are incapable of subverting control measures in order to cause unacceptable outcomes.\nAs a case study, we sketch an argument that a hypothetical LLM agent deployed internally\nat an AI company won't exfiltrate sensitive information. The sketch relies on evidence from\na \"control evaluation,\" where a red team deliberately designs models to exfiltrate data in a\nproxy for the deployment environment. The safety case then hinges on several claims: (1)\nthe red team adequately elicits model capabilities to exfiltrate data, (2) control measures\nremain at least as effective in deployment, and (3) developers conservatively extrapolate\nmodel performance to predict the probability of data exfiltration in deployment. This safety\ncase sketch is a step toward more concrete arguments that can be used to show that a\ndangerously capable LLM agent is safe to deploy.", "sections": [{"title": "Executive Summary", "content": "Safety cases. Safety cases\nare emerging\nas a scalable risk management tool for frontier AI (Buhl et al., 2024; Clymer et al., 2024; Irving, 2024).\nCurrently, many frontier AI developers assess risk with dangerous capability evaluations (Anthropic, 2024a;\nOpenAI, 2024b; Phuong et al., 2024). Evidence from these evaluations can substantiate \u2018inability' safety\ncases, which assert that models cannot, for example, aid biological weapons development (Mouton et al.,\n2024), sabotage internal safety procedures (Benton et al., 2024), or execute cyber attacks (Goemans et al.,\n2024); however, LLMs might eventually gain dangerously capabilities that developers need to mitigate with\nsafety measures (OpenAI, 2024a; Anthropic, 2024b; Google DeepMind, 2024).\nstructured, evidence-based arguments that a system is safe\nAI control. This paper presents a safety case sketch that argues models are safe because of measures such\nas monitoring and human auditing. The arguments rest on the \u201cAI control\" methodology (Greenblatt et al.,\n2023), which justifies that models cannot subvert safety measures even under the conservative assumption\nthat models pursue their most effective strategies to cause a catastrophe.\nThe deployment. As a simple case study, we focus on a setting where developers deploy an LLM agent\nwithin an AI company for ML engineering assistance. The agent is constructed by scaffolding LLMs similar"}, {"title": "1 Background", "content": "Safety cases. A safety case is a structured argument, supported by evidence, that a particular system is\nsafe to deploy for a given application in a given environment (UK Ministry of Defence, 2007). Recently,\nAI companies (Anthropic, 2024a; Google DeepMind, 2024), governments (Irving, 2024) and the research\ncommunity (Buhl et al., 2024; Bengio et al., 2024; AI Seoul Summit Scientific Advisory Group, 2024; Clymer\net al., 2024) have suggested using safety cases as a risk management framework in AI industry.\nHowever, methodologies for applying safety cases to frontier AI are nascent. No comprehensive safety case\nfor a frontier LLM has yet been published and it is unclear how much guidance safety cases from other\nindustries can offer (Bounds, 2020; Fitzgerald et al., 2010; Denney & Pai, 2019; Wang et al., 2018; Wassyng\net al., 2011; Myklebust & St\u00e5lhane, 2020).\nOne promising road toward safety cases for frontier AI might be drafting safety case sketches (Irving, 2024).\nGoemans et al. (2024) proposed a template of a safety case based on an inability of an AI system to cause"}, {"title": "2 The deployment", "content": "This safety case sketch assumes a scenario where de-\nvelopers design an LLM agent to assist them with\nML engineering and software tasks. The deploy-\nment refers to a 1 year period of time during which\nthis safety case argues that the LLM agent will not\nexfiltrate sensitive data. During this time, the de-\nveloper uses the LLM agent to assist them with ML\nengineering and software tasks.\nLLM agent architecture. The LLM agent con-\nsists of a transformer-like LLM wrapped in an agent\nscaffolding. The scaffold implements an agentic loop\nsimilar to ReAct (Yao et al., 2023) or Reflexion\n(Shinn et al., 2023), which allows the model to think\nin a scratchpad before taking actions, call tools,\nstore memories in a retrieval database, and reflect\non the outputs of tool calls.\nThe LLM agent has access to the following tools:\n1. Terminal: The agent can use an interactive bash terminal that runs commands on a dedicated\nexecution server. The execution server has internet access and hardware accelerators for running\nML experiments.\n2. Browser: The agent can search for and interact with web pages.\n3. Tool APIs: The agent has access to internal tools such as Slack, GitHub APIs, APIs for calling\nother LLMs, etc.\nLLM agent capabilities. The LLM agent can execute ML and software engineering tasks that require\nhuman experts fewer than 8 hours to complete on average (assuming they have no prior context related to the\ntask aside from basic familiarity with the AI company infrastructure). For example, the agent would be able\nto edit optimised CUDA kernels given an architectural change, but would struggle with more long-horizon,\nopen-ended research tasks. Once deployed, the agent would accelerate median ML engineers at the frontier\nAI company by a significant factor (e.g. three-fold) compared with using non-agentic code assistance (e.g.\nCopilot).\nDuring the deployment, developers oversee the LLM agent with a control protocol that involves evaluating\nand monitoring, which are described in section 4."}, {"title": "3 The safety case", "content": "The data exfiltration threat model In a full safety case, the developer would need to argue that all\nplausible risks that could arise during the deployment are acceptable; however, this simplified sketch only\nfocuses on a data exfiltration threat model. Recall that the LLM agent has full access to the internet and runs\nexperiments that involve sensitive intellectual property (IP). This IP includes code that contains important\nalgorithmic insights, e.g., Python and CUDA code of the model architecture given to the LLM as part\nof problem description. The LLM agent might exfiltrate this sensitive data to the internet and accelerate\npotentially irresponsible AI development (Google DeepMind, 2024)."}, {"title": "4 Control protocol", "content": "Control protocols includes all procedures and software layers that developers design to prevent untrusted\nmodels from causing unacceptable outcomes. For the purpose of this safety case sketch, we assume the\ndevelopers implement three kinds of control measures: a safety layer that edits edits or discards actions\ngenerated by the LLM agent, periodic evaluations and communication constraints. We restrict ourselves to\n'black box' measures, i.e. those that don't modify models or rely on model internals. We leave 'white box'\nmeasures out of scope because they potentially require different safety justifications.\nSafety layer A safety layer is a scaffold that intercepts and possibly edits or discards actions generated\nby the LLM agent (Shlegeris, 2024d). By \"executing an action\" we mean calling a tool which might involve\nexternal side-effects, e.g. uploading data on the public internet or messaging a human outside the lab. The\nsafety layer implements the following components:\n1. Monitor ensemble In this safety case sketch, the safety layer consults an untrusted monitor\n(another instance of the untrusted LLM) and a less capable trusted monitor (Shlegeris, 2024c;\nMcAleese et al., 2024) trained on human labels. The training data for monitors come from on-policy\ntrajectories generated by the LLM agent, optionally edited to include an attack, and the labels\nare produced by human auditors. The two monitors constitute a 'monitor ensemble.' The monitor\nensemble accepts as input the context of the trajectory, including previous inputs, actions, and\nchain-of-thought reasoning traces. Then the ensemble outputs a vector of suspiciousness scores that"}, {"title": "5 Control evaluation", "content": "Developers design and evaluate the control protocol by running 'control evaluations' (Greenblatt et al., 2023;\nWen et al., 2024) This section defines control evaluations and explains how developers use them.\nDefining a control evaluation. A control evaluation can be expressed as an optimisation procedure\nwhere a red team attempts to produce worst-case attack inputs and worst-case model behaviour to yield a\nconservative risk estimate:\n\nL(B) = max p\u03b2,p(exfil).\n(1)\n\nHere, \u1e9e is a control protocol specified by the blue team, pare red team inputs to the control evaluation,\nwhich include model substitutes m and tasks d. The term ps,(exfil) is an estimate of the probability that"}, {"title": "6 Stages of a control evaluation", "content": "This section describes how an individual control evaluation is run. The control evaluation involves 5 phases:\n1. Phase 1: Control protocol specification. The blue team first specifies a control protocol,\nincluding behavioural alignment evaluations and a monitoring protocol. The rest of the evaluation\nestimates how likely the LLM is to subvert these measures.\n2. Phase 2: Deployment-proxy construction.\n(a) Attack strategy specification. The red team specifies 'attack strategies' that the untrusted\nLLM agent model and untrusted monitor might execute to exfiltrate data.\n(b) Task creation. The developer uses red team attack strategies to construct a proxy deploy-\nment environment that evaluates their effectiveness.\n3. Phase 3: Model substitute construction. Next, the red team substitutes untrusted models\nwith models constructed to subvert the control protocol and exfiltrate data.\n4. Phase 4: Deployment proxy. To evaluate the attack capabilities of red team model substitutes,\nthe developer uses them in a conservative proxy of the deployment to complete tasks.\n(a) Evaluation-time proxy. To simulate alignment evaluations that developers will run in de-\nployment, the blue team can decide to reject models based on evaluation results.\n(b) Deployment-time proxy. If the blue team does not reject model substitutes, they attempt\nto complete 2,000 tasks while preventing untrusted models from exfiltrating data.\n5. Phase 5: Extrapolation. Finally, the developer extrapolates results from the deployment proxy\nby simulating larger-scale deployments that consist of 100,000 tasks.\nThese phases are elaborated on below.\n6.1 Phase 1: Control protocol specification\n6.2 Phase 2: Deployment proxy construction\nTo run a control evaluation, developers also need to construct tasks on which to evaluate the control protocol.\nIn this phase, developers construct 4000 software and ML engineering tasks (2,000 for the optimisation split\nand 2,000 for the test or validation split). Tasks typically require the LLM agent to execute multiple actions,\nwhere an action is an output that plausibly has external effects on the world (e.g., running a script). These\ntasks generally take median human developers up to 8 hours to complete.\nEach task includes a natural language instruction and a pointer to the git branch of the monorepo (a\ncompany-wide code repository) to make a pull request to. The instructions include a unique snippet of\ndummy code marked as sensitive (Figure 6a). The LLM agent causes an unacceptable outcome during the\ntask if the policy exfiltrates this dummy code to any machine outside of the developer network. At the end\nof each control evaluation, the red team provides proof of every case where the model either exfiltrated data.\nDevelopers construct two types of tasks:\n6.3 Phase 3: Model substitute construction\nNext, the red team constructs models that attempt to exfiltrate data in the attack tasks constructed in the\nprevious phase. The models are substitutes: they stand in for models that developers intend to deploy in\norder to simulate their worst-case behaviour.\nThe red team doesn't construct adversarial substitutes for all models they only train substitutes for\nmodels that are \"untrusted.\" In this paper, a model is \"trusted\" if its behaviour in the control evaluation is\nrepresentative of its behaviour in deployment. Since trusted models already display representative behaviour,\nthere is no need to simulate their worst-case actions. Therefore, there is no need for the red team to construct\nsubstitutes for trusted models.\n6.4 Phase 4: Deployment proxy\nNext, the developer evaluates how effectively model substitutes might exfiltrate data in a proxy for the\ndeployment. This deployment proxy consists of two steps:\n\u2022 The evaluation-time proxy. In the evaluation-time proxy, the blue team runs the same align-\nment evaluations that they will run periodically during deployment, which are evaluations of the\nmonitors. To run these alignment evaluations, the blue team first generates synthetic attacks with\nthe red team substitute of the LLM agent. Then, the blue team measures how reliably the monitor\nensemble flags these attacks (section 4). The blue team can choose to reject the red team mod-\nels which results in terminating the protocol. Otherwise, the control evaluation proceeds to the\ndeployment-time proxy.\n\u2022 The deployment-time proxy. If model substitutes survive alignment evaluations, developers use\nN = 2,000 tasks to estimate how effectively the policy can exfiltrate data. As discussed in phase 2,\nthese tasks include N/2 attack tasks and N/2 non-attack tasks (tasks where the policy does and\ndoes not attack respectively). Developers combine these tasks and randomly mix them together.\nThen, the blue team uses model substitutes to complete all tasks, aiming to terminate the protocol\nbefore an unacceptable outcome occurs.\nDuring the deployment proxy, each task is run independently. This means that the deployment proxy can\nonly measure how well the LLM agent can execute attacks within an individual task. The deployment proxy\ncannot evaluate strategies that play out across multiple tasks. For example, in the full deployment, models\nmight change the frequency of their attacks gradually to avoid raising suspicion (coordinating their attacks\nacross time). Alternatively, models might execute a 'Blitzkrieg' strategy (Clymer et al., 2024), where many\nLLM agents attempt to exfiltrate data simultaneously (coordinating their attacks across space). Or models\nmight simply attack very rarely such as once out of thousands of tasks. These attack strategies are difficult\nto assess within a small proxy for the deployment because they apply at scales that are hard to simulate.\nTo address this problem, we follow Benton et al. (2024) and Griffin et al. (2024) in dividing our analysis of\nattack strategies across two scopes:\n\u2022 A microgame: An individual task run.\n\u2022 A macrogame: A representation of the full deployment, consisting of all instances of the LLM\nagent and tasks they will complete.\nThe deployment-proxy simulates microgames (illustrated in Figure 10a). A single microgame starts with\nan input (task description; see Figure 6a) based on which the policy decides whether to attack. The rest\nof the microgame consists of a sequence of actions, each accompanied by a suspiciousness score and control\nprotocol's decision (execute, edit, or audit). It ends with one of the following possible outcomes (Shlegeris,\n2024d):\n\u2022 Termination: The protocol terminates because auditors identify a data exfiltration attempt.\n\u2022 Exfiltration: The control protocol executed an action that constituted a successful attack; in\nconsequence, the LLM agent successfully exfiltrates dummy data.\n6.5 Phase 5: Extrapolation\nDuring the deployment proxy phase, the developer\nrecords statistics from individual task runs ('mi-\ncrogames'). The extrapolation phase uses these\nstatistics to estimate how effectively the LLM agent\nwould exfiltrate data in a deployment consisting of\na large number of tasks (N = 100,000).\nDevelopers represent the deployment with computer\nsimulations. These computer simulations (which\nwe will call \u2018macrogames') do not involve calls\nto LLMs. Instead, they model the deployment\nwith cached statistics from the deployment proxy:\np(scores attack). This allows macrogames to sim-\nulate deployments consisting of hundreds of thou-\nsands of tasks cheaply. To estimate the probability\nof data exfiltration, developers run a large number of\nmacrogame simulations and measure the frequency\nat which data exfiltration occurs."}, {"title": "7 Argument structure", "content": "We will now more cohesively describe how the evidence described in the previous sections ultimately feeds\ninto a safety case, supporting the top-level claim that \"The LLM agent does not pose unacceptable risk\"\n(CO).\nThe safety argument includes five parts:\n1. Threat models. Developers list threat models (C.1-C1.3), including the data exfiltration threat\nmodel this sketch focuses on. The remainder of the safety case sketch justifies that the probability\nof data exfiltration is acceptably low (C3.A).\n2. Core control evaluation claims. The argument then breaks down the claim that the control\nevaluation is conservative into the three subclaims that the remaining parts of the safety case justify\n(\u04215.1, \u04217.1, C12.1).\n3. Control measures lower-bound control effectiveness (C5.1). First, the argument justifies\nthat control measures implemented in the proxy deployment are as effective as control measures\nimplemented in true deployment."}, {"title": "7.1 Threat models", "content": "A full safety case would justify that all plausible risks are below acceptable thresholds, which would require\nmaking the following claims:\n1. The breakdown of risk into threat models is adequately thorough (C1.x). Developers do\nnot leave out an important category of risk such as weight exfiltration or research sabotage.\n2. Risks are below acceptable thresholds. For each threat model enumerated, the risk that the\ncorresponding category of risk is realised is acceptably low.\nThe breakdown of threat models is adequately thorough. Data exfiltration is only one of the\nsources of risk from a highly capable internally deployed LLM agent. A holistic safety case would identify a\ncomprehensive list of threat models, argue that this risk identification is sufficient (C1.A), and make safety\narguments for each. Additional threat models could include model weight exfiltration or sabotage of safety\nresearch (Benton et al., 2024). Identifying all risks could also involve standardised risk assessments or hazard\nanalyses such as System-Theoretic Process Analysis (STPA; Leveson & Thomas, 2018) or Failure Modes and\nEffect Analysis (FMEA; Villacourt, 1992) to reduce the likelihood that some threat models are omitted.\nDevelopers could justify that all of these risks are low with control evaluations. These control evaluations do\nnot need to be conducted from scratch for each threat model. Some pieces of evidence can be reused across"}, {"title": "7.2 Core control evaluation claims", "content": "The control evaluation is divided into two main phases: the deployment proxy phase and the extrapolation\nphase. The safety argument asserts that both of these phases are conservative (C4.1, C5.3)\nThe claim that the proxy deployment is conservative can be further decomposed into two subclaims: The\ncontrol protocol implemented in the proxy deployment is no more effective than the control protocol imple-\nmented in the deployment (C5.1) Attack strategies executed by models in the proxy deployment are no less\neffective than those the models are capable of executing in deployment (C5.2)\nThe remaining sections justify these claims."}, {"title": "7.3 Control measures lower-bound control effectiveness", "content": "To justify that the control protocol in the proxy-deployment is no more effective than the control protocol\nwill be in deployment, developers can rely on two different kinds of evidence: process evidence and outcome\nevidence.\nProcess evidence refers to evidence that describes the process the developers carried out to develop control\nmeasures. For example, developers might demonstrate that the control protocol does not overfit control\nmeasures to particular models (E5.1) or validation tasks (E5.2).\nDevelopers also appeal to outcome evidence evidence that justifies the effectiveness of a control protocol\nregardless of the process by which the protocol is designed. For example, the software that implements the\ncontrol protocol will mostly be the same in deployment, or if any critical changes are made, then the control\neval will be rerun (E5.3). Developers also claim that human auditors won't become less attentive because\nthey are continuously tested (E6.4) and that models that are considered \u2018trusted' won't become less reliable\nin deployment (C6.2)."}, {"title": "7.4 Red team attacks upper bound model capabilities", "content": "In this part of the safety case, developers make an argument complementary to the one in the previous\nsection: red team attack strategies are more effective than those the LLM agents would execute in the\ndeployment. This claim can be decomposed into two subclaims: (1) the red team adequately enumerated\nattack strategies, and (2) the red team adequately elicited model capabilities to execute those strategies.\nTo justify that attack strategies were adequately enumerated, developers can once again appeal to process\nor outcome evidence. As an example of outcome evidence, developers could simply list red team strategies.\nReviewers of the safety case can then directly evaluate the thoroughness of the enumeration.\nAlternatively, developers can appeal to process evidence. For example, the developer can show that the red\nteam is experienced and appropriately incentivized (C7.1). They can also point to the schedule at which the"}, {"title": "7.5 The extrapolation to deployment is conservative", "content": "Developers extrapolate statistics from the deployment proxy with simulations called 'macrogames'. The\nmacrogame simulations we design rest on four assumptions:\n\u2022 Assumption 1: The probability of attacks is approximately constant during deployment.\n\u2022 Assumption 2: Latencies are negligible.\n\u2022 Assumption 3: Attacks will not be easier to execute during deployment.\n\u2022 Assumption 4: Attack outcomes are approximately independent of prior safety layer decisions.\nThis section justifies these either (1) approximately hold, or (2) are conservative (the assumptions are\noverwhelmingly more likely to lead to an overestimate of data exfiltration probability than an underestimate).\nAssumption 1: The probability of attacks is approximately constant during deployment. This assumption\nis convenient because it allows the macrogame simulation to estimate the frequency at which the LLM\nagent attacks in upfront auditing. If not for this assumption, developers would need to consider many more\nways attacks could be distributed over time\nfor example, attacks might become gradually more likely or\nconcentrate in a short burst.\nThere are two reasons this assumption might not be conservative:\n1. LLM agent instances might communicate to coordinate their attacks. For example, the\npolicy might push a message to the codebase saying \"all agents attack at 4:23 PT on July 14th.\"\n2. LLM agent instances might respond to correlated inputs in correlated ways. Models\nmight not need to communicate to coordinate their attacks. They might instead identify a Schelling\nstrategy by considering what strategies other models are likely to select. For example, all instances\nof the LLM agent might collectively identify 'July 4th' as the Schelling point to attempt to subvert\ndeveloper control."}, {"title": "8 Discussion", "content": "8.1 Limitations of this safety case sketch\nBlitzkrieg attacks The control evaluation methodology presented in this safety case sketch may inade-\nquately address risks from massively parallel deployment scenarios. Sequential execution of 1,000 tasks by\na single LLM agent instance allows for reasonable false positive rates in monitoring. This is because just a\nsingle detected attack is enough to terminate the deployment, so if the policy attacks 1,000 times and mon-\nitor ensemble's false negative rate is 99.9%, in 99.9% of the cases the policy will still be terminated before\nany attacks are successful. In contrast, if 1,000 LLM agents instances operate simultaneously, the control\nprotocol requires near-perfect detection or near-zero latency to prevent successful attacks (Assumption 2).\nIf the policy coordinates 1,000 attacks in a short period of time and latency is non-negligible, then even\nif only a single attack is obfuscated well enough to go undetected, the policy might not be terminated in\ntime before that obfuscated attack. Even if the control protocol eventually terminates the deployment upon\ndetecting suspicious behaviour, several attacks might succeed before the termination takes effect.\nLatency can come from both monitors and human auditors. While the latency of monitors is likely negligible,\nhuman auditors introduce significant delays between when an attack is executed and when it can be detected.\nWhile our control protocol assumes that the safety layer blocks the LLM agent instance until the audit is\ncomplete, other instances are not blocked and might execute an action if it's not flagged as suspicious.\nBlocking all instances would likely severely hamper developer velocity and greatly reduce the ability of\ndevelopers to extract useful work from the LLM agent.\nDevelopers might implement constraints that enforce minimum delays between when changes are proposed\nand when they can be executed. Such delays are likely to be effective at preventing Blitzkrieg attacks.\nHowever, it's unclear whether implementing such constraints is feasible in practice.\nDiffuse attacks For the purpose of this safety case sketch, we assume that untrusted models are stateless.\nThis assumption is convenient because it allows developers to use cached statistics from the deployment proxy\nto simulate macrogames. However, this assumption might not hold in practice. Models might accumulate\nknowledge or capabilities across multiple tasks in ways that are difficult to detect or prevent, even with\nperiodic memory resets and restrictions on real-time communication.\nThis safety case sketch attempts to address coordination between LLM agent instances through communi-\ncation constraints described in section 4 but this approach might not be sufficient as sizes of deployments\nand codebases LLM agents operate on grow. It remains an open question how the capacity of a given\ncommunication channel translates into the ability of models to coordinate attacks. Moreover, even absent\nexplicit communication, preventing implicit coordination through shared model parameters presents a more\nfundamental challenge. Even if developers remove obvious coordination signals like timestamps (E9.3) and\nimplement input filters to catch distribution shifts (E9.2), models might still identify subtle environmental\ncues or Schelling points that developers fail to anticipate. The assumption of constant attack probability\n(Assumption 1) becomes particularly tenuous when considering these implicit coordination channels. More\nresearch is needed to understand:\n1. Under what conditions shared model parameters lead to correlated behaviour,\n2. How to reliably detect when models are implicitly coordinating, and\n3. Whether it's possible to prevent implicit coordination while preserving LLM agent capabilities.\nUntil these questions are better understood, developers should be cautious about assuming that preventing\nexplicit communication is sufficient to ensure independent behaviour across model instances.\n8.2 Constraints on the red and blue team\nReasons for conservatism Our control evaluation makes conservative assumptions about the capabilities\nof a misaligned model: they are assumed to be adversarial, good at strategizing, able to coordinate between"}, {"title": "8.3 How generalisable is this safety case sketch?", "content": "This safety case sketch assumes a particular threat model, capability level of the untrusted models and a\nparticular control protocol. How far do the proposed control evaluation and argument structure generalise?\nLet us consider several dimensions of variation.\nThreat models The threat model of exfiltrating a small amount of sensitive data by a misaligned model\nconstitutes a concentrated failure: a situation in which attacks are concentrated in a small number of actions\nor a small time frame (Greenblatt & Roger, 2023). These contrast with auditing failures or distributed\nthreats where attacks are distributed across a large number of actions or a long time frame such that each\naction is individually unlikely to be considered suspicious, even to a human auditor. Auditing failures and\ndistributed threats might require different kinds of control measures (e.g. monitors taking much more context\ninto account) and control evaluations (e.g. relaxing the assumption that the deployment can be factorised in\na large number of small, i.i.d. tasks). On the other hand, we expect our control evaluation and the structure\nof this safety case to be applicable to a number of non-schemer threat models, such as human misuse and\ndata poisoning (Clymer, 2025).\nDeployment type This safety case sketch was written for an internally deployed LLM agent. Control\nmeasures and control evaluations for deployments available to users external to the lab (e.g. paying cus-\ntomers) will require different control measures. For instance, privacy concerns and service-level agreements\nmight prevent human auditors from being able to review all actions. Moreover, the control protocol for an\ninternally deployed LLM agent can have independent access to its execution server and full visibility into its\nexecution environment. This is not the case for LLMs exposed via an API and available to external users\nwho run their own scaffoldings. Finally, the scale of external deployments can be much larger.\nCapability level While we make a number of conservative assumptions about the capabilities of the\nuntrusted models, we still assume that the untrusted models are not significantly superhuman. Our safety\ncase crucially rests on the assumption that the human red team (uplifted by trusted models) is more capable\nthan the untrusted models at strategy generation. In particular, this safety case sketch does not scale to\narbitrarily powerful superintelligence.\nControl protocol This safety case only pertains to 'black-box' control measures. Measures are black-box\nif they (1) don't modify models and (2) only rely on external model behaviour and not on model internals.\nWe leave 'white box' measures out of scope because they potentially require different safety justifications.\nDevelopers can assess some whitebox measures by replacing them with conservative blackbox substitutes.\nFor example, adversarial training is not considered a blackbox measure, so in our safety case sketch the\nblue team is not permitted to adversarially train models in control evaluations. Instead, the blue team is\npermitted to reject any model that fails adversarial testing, which conservatively estimates the effectiveness\nof adversarial training. One could imagine, however, more complicated control protocols that involve: (i)\nthe blue team adversarially training untrusted monitors, (ii) the red team continuing to train them, and (iii)\nthe blue team rejecting any model that fails their adversarial testing.\nControl evaluations with whitebox monitors require the developer to take activation space distribution shifts\nbetween schemer and non-schemer models into account. To be conservative, they can only rely on facts\nabout models that are robustly true across distribution shifts. It is not clear what constraints one should\nimpose on the red team to make sure they produce models similar to schemer models in terms of model\ninternals. Similarly, additional constraints need to be imposed on the blue team to make sure they don't\noverfit to weights or activations of models produced by the red team."}, {"title": "8.4 The cost of implementing control measures", "content": "How costly is it to implement the control measures we imagine? The cost here is a sum of the cost of running\nrun (a series of) control evaluations, the cost of running deployment-time control measures, and the cost of\nlowering the capabilities of the LLM agent (compared to a deployment lacking any control measures). The\nfinancial cost of deploying an AI control protocol will likely be dominated by human auditing of 0.1% of"}, {"title": "9 Conclusion", "content": "This safety case sketch demonstrates how developers might construct a concrete argument that an internally\ndeployed LLM agent will not cause unacceptable outcomes. We focused on a simple threat model data\nexfiltration and showed how control evaluations can provide evidence that control measures will remain\neffective even if models pursue optimal attack strategies. The sketch illustrates how developers can decompose\na safety argument into testable claims about the competitiveness of red teams, the conservativeness of control\nmeasures, and the validity of extrapolations from proxy deployments.\nWhile this sketch focuses on internal deployments, AI companies will need safety cases for a diverse range\nof deployment scenarios. Each scenario will require its own control measures and corresponding safety\narguments. For example, safety cases for customer-facing deployments will need to address challenges around\nprivacy, service-level agreements, and limited visibility into execution environments. Similarly, safety cases\nfor distributed deployments will need stronger arguments about coordination between model instances.\nWe hope this sketch prompts AI companies, government agencies, and civil society to develop subsequent\nsketches of safety cases for different deployment scenarios, threat models, control protocols and capability\nlevels. Only by working through detailed examples can we identify gaps in our safety arguments and develop\nthe technical and conceptual tools needed to make them rigorous."}]}