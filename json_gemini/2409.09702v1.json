{"title": "GFlowNet Pretraining with Inexpensive Rewards", "authors": ["Mohit Pandey", "Gopeshh Subbaraj", "Emmanuel Bengio"], "abstract": "Generative Flow Networks (GFlowNets), a class of generative models have recently emerged as a suitable framework for generating diverse and high-quality molecular structures by learning from unnormalized reward distributions. Previous works in this direction often restrict exploration by using predefined molecular fragments as building blocks, limiting the chemical space that can be accessed. In this work, we introduce Atomic GFlowNets (A-GFNs), a foundational generative model leveraging individual atoms as building blocks to explore drug-like chemical space more comprehensively. We propose an unsupervised pre-training approach using offline drug-like molecule datasets, which conditions A-GFNs on inexpensive yet informative molecular descriptors such as drug-likeliness, topological polar surface area, and synthetic accessibility scores. These properties serve as proxy rewards, guiding A-GFNs towards regions of chemical space that exhibit desirable pharmacological properties. We further our method by implementing a goal-conditioned fine-tuning process, which adapts A-GFNs to optimize for specific target properties. In this work, we pretrain A-GFN on the ZINC15 offline dataset and employ robust evaluation metrics to show the effectiveness of our approach when compared to other relevant baseline methods in drug design.", "sections": [{"title": "Introduction", "content": "GFlowNets are amortized samplers that learn stochastic policies to sequentially generate compositional objects from a given unnormalized reward distribution. They can generate diverse sets of high-reward objects, which has a demonstrated utility in small molecules drug-discovery tasks [1]. Traditionally, GFlowNets for molecules generation have focused on fragment-based drug discovery i.e. the GFlowNet action space comprises of some predetermined fragments as building blocks for molecules. While producing diverse candidates, fragment-based approach limits the pockets of chemical space assessible by GFlowNet policy [2, 1, 3, 4]. A truly explorative generative policy, tapping into the full potential of GFlowNet would be realizable when using atoms instead of fragments as the action space [5, 6, 7]. On the other hand, the vastness of accessible state space makes training atom-based GFlowNets susceptible to collapse. Earlier atom-based GFlowNets attempts overcame this issue by limiting to small trajectories [3]. However, since most commercially available drugs have molecular weights ranging from 200 to 600 daltons[8, 9], the molecules generated from these small trajectories are unlikely to possess the drug-like characteristics necessary for therapeutic efficacy. In this work, we propose to mitigate the small trajectory length constraint for atom-based GFNs by pretraining them with expert demonstrations, coming in the form of offline drug-like molecules. Our main contributions in this paper are as following:\n\u2022 We introduce A-GFN-an atom-based GFlowNet for sampling molecules proportional to rewards governed by inexpensive molecular properties.\n\u2022 We propose a novel strategy for unsupervised pretraining of A-GFNs by leveraging drug-like molecules using offline, off-policy training. This pretraining enables broader exploration of chemical space while maintaining diversity and novelty relative to existing drug-like molecule datasets.\n\u2022 Through extensive experimentation, we demonstrate that goal-conditioned fine-tuning of A-GFNs for sampling molecules with desired properties offers significant computational advantages over training A-GFNs from scratch for the same objectives."}, {"title": "Related Works", "content": ""}, {"title": "GFlowNet", "content": "GFlowNets were introduced by Bengio et al. in 2021 [1] as a framework for training energy-based generative models that learn to sample diverse candidates in proportion to a given reward function. Unlike traditional Reinforcement Learning (RL) methods, which focus on maximizing rewards through a sequence of actions, GFlowNets aim to generate samples with probabilities proportional to their associated rewards. This distinction allows GFlowNets to explore a broader solution space, facilitating the discovery of novel, high-quality, and diverse objects across various domains. Recent works have been conducted along a multitude of directions, some focusing on theoretical aspects, such as connections to variational methods [10, 11]), and some focused on improved training methods for better credit assignment and sample efficiency [12, 13]. Due to its flexibility, GFlowNets have also been applied successfully to different settings such as biological sequences[14], causal discovery [15, 16], discrete latent variable modeling[17], and computational graph scheduling [18]."}, {"title": "Unsupervised pretraining in RL and GFlowNets", "content": "Pretraining models in machine learning has virtually become the default way to obtain powerful models [19, 20]. Specifically, unsupervised pretraining in reinforcement learning (RL) has emerged as a promising strategy to enhance data efficiency and improve agent performance across various tasks. Recent works have explored different methodologies to leverage unsupervised interactions with the environment before fine-tuning on specific objectives. For instance, Liu and Abbeel (2020) introduced Active Pre-Training [21], a reward-free pre-training method that maximizes particle-based entropy in a contrastive representation space, achieving human-level performance on several Atari games and significantly enhancing data efficiency in the DMControl suite. Similarly, Mutti et al. [22] addressed unsupervised RL in multiple environments, proposing a framework that allows for pre-training across diverse scenarios to improve the agent's adaptability. Additionally, the Unsupervised-to-Online RL framework [23] was developed, which replaces domain-specific offline RL with unsupervised pre-training, demonstrating that a single pre-trained model can be effectively reused for multiple downstream tasks, often outperforming traditional methods. Similarly, In the context of GFlowNets, recent advancements have introduced unsupervised pre-training strategies, such as the outcome-conditioned GFlowNet [24], which enables reward-free pre-training by framing the task as a self-supervised problem. This approach allows GFlowNets to learn to explore the candidate space and adapt efficiently to downstream tasks, showcasing the potential of unsupervised pre-training in enhancing the performance of generative models in molecular design."}, {"title": "Goal-Conditioned and Multi-Objective Gflownets", "content": "Recent advancements in GFlowNets have focused on goal-conditioned and multi-objective frameworks, enhancing their applicability in complex generative tasks. Goal-conditioned GFlowNets enable the generation of diverse outputs tailored to specific objectives, improving sample efficiency and generalization across different goals, as demonstrated by [14] and further refined through methods like Retrospective Backward Synthesis [25] to address sparse reward challenges. Roy et al., [26], impose hard constraints on a GFlowNet model by employing focus regions as a goal-design strategy which makes it comparable to a form of goal-conditional reinforcement learning [27]. Additionally, the development of multi-objective GFlowNets [3] allows for simultaneous optimization of multiple criteria, providing practitioners with greater control over the generative process and the ability to explore trade-offs between competing objectives."}, {"title": "Molecule Generation", "content": "To contextualize our approach, we provide a brief review of prior research that utilized atom-based vocabularies in molecular generative modeling. Reinforcement learning (RL) studies optimal decision-making methodologies to maximize cumulative rewards. Given the shared notion of object-constructing Markov decision policies between GFlowNets and RL, we focus on the latter literature for molecule generation. For a comprehensive review of methods in molecule generation, we point the readers to [28]. Recently, RL has been frequently employed in de novo design tasks due to its capability to explore chemical spaces beyond the compounds present in existing datasets. Moreover, it allows for targeted molecule generation for constrained property optimization. Early works in this domain focused on the auto-regressive generation of SMILES within an RL-loop for molecular property optimization[29, 30, 31]. MolDQN[32] employs deep Q-Networks and multiobjective molecular properties for scalarization of rewards to generate 100% valid molecules without pretraining on a dataset. You et al.[33] and Atance et al. [34] have employed graph neural networks, trained on offline molecular datasets, to generate molecular graphs. They simultaneously applied policy-gradient reinforcement learning to ensure that the generated molecules adhere to specified property profiles."}, {"title": "Preliminaries", "content": "GFlowNets are generative models designed to sample structured objects from a state space S in proportion to a reward function $R (s_T)$ assigned to terminal states $s_T$. The model generates trajectories $T = (s_0, a_0,...,s_T)$ by transitioning between states through actions, guided by a forward policy $P_F (s' | s)$. The key idea is to learn a flow function F(s) that ensures the total flow into each state equals the flow out, so the probability of reaching any terminal state is proportional to its reward. This is achieved by aligning the forward and backward policies $P_F$ and $P_B$, ensuring the final distribution over terminal states reflects the desired reward distribution.\nExisting extensions of GFlowNets that handle multiple, potentially conflicting objectives [3, 2], along with the benefits of scalability to large action spaces and credit assignment makes them well-suited for molecular generation tasks; the primary focus of our work. We consider molecules as their topological graphs $G = (A, E, X)$ such that $X \\in \\mathbb{R}^{n \\times d}$ define d-dimensional atomic features for the n nodes in G, $E \\in \\{0,1\\}^{b \\times n \\times n}$ is edge-adjacency tensor for b types of edges connecting n nodes and finally $A \\in \\{0,1\\}^{n \\times n}$ is the adjacency matrix for n nodes. The corresponding trajectory $\\tau$ for G $(\\tau = (s_0, a_0), ....(s_n, a_n))$ is generated by sampling actions a from a conditional learning policies $P_F(. | c; \\theta)$ and $P_B(. | c, G; \\theta)$ of the GFlowNet $G_{\\theta}$, where state $s_i$ is a partially constructed subgraph of G, and $s_n = G$.\nOur primary objective is to learn $\\pi$ such that the generated G are chemically valid molecular graphs satisfying some defined molecular property conditional ranges c, and exhibiting sufficient diversity. Specifically, we want to train a conditional policy which samples molecular graphs G with probability proportional to $R(G|c)$, where $R(G|c)$ is a reward function measuring how well generated G satisfies c. Our secondary objective involves fine-tuning this pretrained $G_{\\theta} = (P_F(; \\theta), P_B(; \\theta))$ to achieve drug discovery tasks with certain molecular property constraints and establish the benefits of fine-tuning GFlowNets. Trajectory balance [12] being the most common learning objective designed to improve credit assignment in GFlowNets is used as the training method in this work.\n$L_{TB}(T) = \\left(log \\left( \\frac{Z_{\\theta} \\prod_{t=1}^{|T|} P_F(s_t | s_{t-1}; \\theta)}{R(x) \\prod_{t=1}^{|T|} P_B(s_{t-1} | s_t; \\theta)} \\right)\\right)^2$ (1)\nwhere, trajectory $T = (s_0, s_1, ...., s_n)$ such that $s_n = x$ is a fully constructed object. Within the context of our work, $x \\in X$ are samples from the combinatorial space of all possible molecules using actions $a_t \\sim A$. This equation ensures that the product of forward policy probabilities along a trajectory is proportional to the product of backward policy probabilities and the reward along the same trajectory."}, {"title": "Unsupervised Pretraining with Inexpensive Rewards", "content": "To construct molecular graphs, we design an action space with 5 action types. The agent can add a node (heavy atom) to the graph, add an edge between two nodes, set a node's properties (e.g. its chirality), set a bond's properties (i.e. its bond order), or stop the trajectory. We use a graph neural network [35] to parameterize a policy with such actions, using the GNN's invariances to produce per-node and per-edge logits. We are also careful to mask these logits such that the produced molecules always have valid valences (i.e. by design the molecules are always convertible to RDKit molecules and SMILES). Since our method generates molecules atom by atom, we refer to the approach as Atomic GFlowNet (A-GFN)."}, {"title": "Inexpensive Molecular Rewards", "content": "In the context of pre-training A-GFN models for molecular design, we utilize inexpensive molecular rewards such as Topological Polar Surface Area (TPSA), Quantitative Estimate of Drug-likeness (QED), synthetic accessibility (SAS), and the number of five or six-membered rings in a molecule. These rewards are computationally cheap to evaluate and serve as proxies for more complex properties. Fine-tuning the models is then conducted on more expensive and computationally intensive tasks, such as predicting binding affinity or toxicity (e.g., LD50), which are crucial for drug discovery but require significant computational resources or experimental data to assess accurately."}, {"title": "Reward Function", "content": "In order to pre-train a goal-conditioned A-GFN for learning molecular properties $p \\in P$, we need to define the property-specific conditional ranges $C_p = (C_{low}, C_{high})$ from which molecules are generated. We use the following goal-conditioned reward function for property p and molecule x:\n$R_p(x|c_p, d > 0) = reward(p_x | c_p, d > 0) =\\begin{cases}\n0.5 * exp \\left( \\frac{\\lambda (C_{low}-p_x)}{c_p}\\right) & \\text{if } p_x < C_{low}\\\\\nexp \\left(\\frac{-\\lambda(p_x-C_{high})}{c_p}\\right) & \\text{if } p_x > C_{high} \\\\\n0.5*\\frac{(p_x-C_{low})}{(C_{high}-C_{low})} +0.5 & \\text{otherwise}\n\\end{cases}$ (2)\nhere, $\\lambda$ controls the decay rate, $C_{low}$ and $C_{high}$ are the lower and upper bounds for property p, and $d\\in \\mathbb{R}$ represents the preference_direction hyperparameter, indicating whether lower or higher property values within the range $c_p$ are preferred (for further details, see appendix A,B). Note that during training, we sample these ranges so that the model learns to be robust to inference-time queries (sec 4.1.2).\nDrug discovery is inherently a multiobjective optimization problem where the drug candidates are expected to simultaneously have several desired properties, such as a high drug-likeliness (QED), high SAS, and reasonably low TPSA, among other criteria. In our de novo molecular generation setup, we wish to satisfy the same multiobjective desiderata [3]. We choose this aggregated scalarization of the multiobjective reward over property set P as\n$R(X|C_{p_1},..., C_{p_{|P|}}) = \\prod_{p \\in P} R_p(x|C_p)$ (3)"}, {"title": "Reward conditioning A-GFN", "content": "To alleviate the problem of sparse rewards in training A-GFNs for molecular graph generation with some hard constraints, we condition the sampler by using a distribution of goals derived from reasonable lower and upper bounds of the molecular property ranges we care about. This conditioning effectively narrows down the search space to regions of interest defined by these property ranges, ensuring that the generated molecules are not only diverse but also relevant to the specific objectives of the drug discovery process. In order to ensure that the A-GFN does not develop a selective bias towards specific values within a property range and instead explores the full range of possible values for each molecular property p, we sample the conditional vectors $C_{p,j}$ uniformly across their respective ranges. Specifically, for the jth online trajectory in the batch, the lower and upper bounds $c_{low}$ , $c_{high}$ for the property p are drawn from a uniform distribution as $c_{low}$ , $c_{high}$ ~$U(C_{low}$ , $C_{high})$, where $C_{low}$ and $C_{high}$ are the predefined desired lower and upper bounds for the property p. In cases where the values for properties p for the trajectory j leading to a valid molecular graph are known a priori $(p_j)$ (e.g., from molecules in an offline dataset), $C_{p,j}$ are centered around these known values, i.e. $c_{low}$ , $c_{high}$ ~$T(p_j, \\sigma_p, C_{low}$ , $C_{high})$, where T is a truncated normal distribution centered at the value of property p calculated for trajectory j, $\\sigma_p$ is a hyperparmeter controlling the variance of T for property p.\nSuch a probabilistic goal-sampling strategy ensures that each trajectory within the batch is conditioned on a randomly selected sub-range within the broader property range. This enables our laid out objective of promoting exploration across the entire desired chemical space and preventing the A-GFN from overfitting to narrow regions within the chemical space. In order to further prevent"}, {"title": "Pretraining GFN with expert offline trajectories", "content": "Bengio et al. [1] employ replay buffer-based off-policy training for GFlowNets to generate novel molecules. However, such online off-policy methods can suffer from high variance [37, 38] and and a lock-in of suboptimal trajectories, particularly in sparse reward settings, where the agent struggles to adequately explore rare, high-reward regions of the state space, resulting in slow convergence and suboptimal performance. To mitigate these challenges, we propose leveraging the vast amounts of readily available inexpensive and unlabelled molecular data to perform a hybrid online-offline off-policy pretraining of A-GFN. This data provides valuable expert trajectories and the molecular properties derived from this data can provide inexpensive extrinsic rewards, giving a better starting point for exploration. We form our training batches by integrating offline expert trajectories from the ZINC dataset ($\\mathcal{D}_{ZINC}$) with online updates (T = $T_{online}$ $\\oplus$ $T_{offline}$). $T_{offline}$ are generated from these molecules, x \u2208 $\\mathcal{D}_{ZINC}$ by sampling deleterious actions {deleteNode, deleteEdge, removeNodeAttribute, removeEdgeAttribute} according to a conditional backward policy $P_B$ as $T_{offline}$ ~ $P_B(. | x, C_p;\\theta)$. Likewise, online trajectories are created by sampling constructive actions {addNode, addEdge, addNodeAttribute, addEdgeAttribute, stop} from conditional forward policy $P_F$ as $T_{online}$ ~ $P_F(. | C_p;\\theta)$."}, {"title": "Regularized Loss Balancing for Exploration and Synthesis Feasibility", "content": "The primary allure of molecular generative models with large explorative capacities such as A-GFN is their ability to navigate the near-infinite possibilities of chemical structures. On the other hand, ensuring that generated molecules lie close to the chemical space feasible for synthesis, particularly within the constraints of make-on-demand (MOD) libraries such as ZINC, is crucial for synthesis and in vitro validation in drug discovery. To balance these two seemingly conflicting goals, we introduce a regularization term in the pretraining objective. Specifically, we combine the exploration objective with a Maximum Likelihood Estimation (MLE) loss over the offline dataset, leading to the following regularized loss function:\n$\\mathcal{L} = \\lambda_1 \\mathcal{L}_{TB} + \\lambda_2 \\mathcal{L}_{MLE}$ (4)\nwhere, $L_{MLE} = -log(P_F(. | x, c_p)) \\forall x \\in \\mathcal{D}_{ZINC}$ \n$L_{TB}$ encourages exploration of the chemical space, $L_{MLE}$ is the distributional learning loss term ensuring proximity to MOD libraries, and $\\lambda_1$, $\\lambda_2$ are hyperparameters controlling the trade-off between exploration and adherence to the MOD space (Tab.8). With the online and offline trajectories described in sec4.2 and molecular property rewards in sec4.1, we optimize the prior A-GFN ($G_{\\theta}$) by minimizing Eq.4 until convergence in reward."}, {"title": "Finetuning", "content": "In this section, we investigate the methodology for utilizing the pre-trained A-GFN model ($G_{\\theta}$) and its subsequent adaptation to downstream drug discovery tasks based on harder reward functions. To fine-tune the model, the pre-trained parameters of $G_{\\theta}$ serve as the initialization for task-specific adaptation. This initialization enables the model to retain useful structural priors from pretraining, thus improving sample efficiency and convergence speed during fine-tuning. In particular, we retrain $G_{\\theta}$ by integrating task-specific reward $R_{ext}$. This modifies eq.3 as\n$R(X|C_{p_1},..., C_{p_{|P|}}) = \\prod_{p \\in P} R_p(x) \\times R_{ext}(x)$ (5)\nSuch a reward formulation ensures that the GFlowNet receives a high reward for generating molecules that simultaneously follow the desired molecular properties and are highly suitable for the downstream task. It should be noted that $Z_{\\theta}$ in eq 1 is a global scalar that estimates the normalization constant for the unnormalized reward function $R(x)$ (i.e, $Z_{\\theta} = \\Sigma_{x\\in X} R(x)$). Thus, to enable $Z_{\\theta}$ to generalize to the new $R_{ext}(x)$, we inject noise into the pretrained A-GFN model's parameters, a common strategy in pretrain-then-finetune approaches [39, 40]."}, {"title": "Experiments", "content": "We evaluate A-GFN's effectiveness during both pretraining and fine-tuning using comprehensive metrics such as novelty, diversity, uniqueness, success rate, validity, L1-distance, and number of modes. Full details are provided in appendix B.\nFor pretraining, we compare the performance of A-GFN against fragment-based GFlowNet, conditioned on the same molecular properties. Our results show that A-GFN significantly outperforms fragment-GFN in exploring drug-like chemical space across multiple objectives. For fine-tuning, we benchmark the pre-trained A-GFN against A-GFN trained from scratch (task-trained A-GFN) on several downstream drug discovery tasks. Following the task setup in [33], we focus on the following finetuning objectives:\nProperty Optimization: The goal here is to generate molecules that maximize or minimize a specified physicochemical, structural, or binding property while ensuring diversity among the generated molecules. In this unconstrained optimization setting, we compare the fine-tuned A-GFN to the task-trained A-GFN and other state-of-the-art methods for unconstrained molecule generation. For fairness, we only include atom-based generative methods in the baselines, excluding fragment-based approaches.\nProperty Targeting: In this task, the objective is to generate molecules that adhere to predefined molecular property ranges while being structurally distinct from the training (pretraining) set.\nProperty Constrained Optimization: This task requires the generation of molecules that simultaneously meet both property optimization and property targeting criteria i.e. molecules must lie within the specified property ranges while also maximizing or minimizing the targeted property."}, {"title": "Pretraining", "content": "A-GFN pretraining setup: To create a versatile foundation for a range of downstream molecular generation tasks, we train our A-GFN using a hybrid online-offline off-policy strategy. For our pretraining, we utilize ZINC250K, a curated subset of the ZINC database, which consists of 250,000 commercially accessible drug-like compounds drawn from over 37 billion molecules available in ZINC [41]. The primary goal during pretraining is to optimize the A-GFN for generic yet critical drug-like properties: TPSA, QED, SAS, and the number of rings. The desired ranges for these properties are enumerated in Tab.5. These properties are chosen based on their established relevance in guiding molecular design towards compounds with desirable pharmacokinetic and pharmacodynamic profiles, following the framework of [42]. By optimizing for these properties, we aim to equip A-GFN with the capability to generate molecules that strike a balance between drug-likeness and structural diversity. We restrict atom types to a core set commonly found in drug-like molecules: C, S, P, N, O, F, and implicit hydrogen (H). This ensures that the generated molecules are synthetically relevant and pharmacologically plausible, avoiding rare or exotic atom types that are less likely to lead to viable drug candidates. For benchmarking fragment-based GFlowNets, we adapt the purely online training setup proposed in [1], conditioning it on the same molecular properties as A-GFN. To align with our A-GFN setup, we generate a fragment vocabulary from the BRICS decomposition of ZINC250K, selecting the 73 (following [1]) most common fragments. This equips the fragment-based GFlowNet with a diverse and representative set of building blocks for molecular generation. Through pretraining, we observe that A-GFN effectively adapts to the specified property ranges while maintaining high molecular diversity and uniqueness. All molecules generated by A-GFN are valid, adhering to chemical rules, and novel, as they do not replicate any molecules in the ZINC250K dataset (Fig.2). For the same set of property conditionals, A-GFN demonstrates superior chemical scaffold exploration compared to the fragment-based GFlowNet, covering nearly twice as many distinct scaffolds (Tab.1). While the fragment-based method has a higher success rate and better control over specific molecular properties, A-GFN excels in uniqueness and novelty, making it a more powerful tool for exploring uncharted chemical space in drug discovery. This highlights the model's capacity to explore novel regions of chemical space while adhering to fundamental molecular design principles."}, {"title": "Fine-tuning", "content": "Fine-tuning setup We split the fine-tuning tasks' objectives as property optimization, property targeting, and property constrained optimization. The primary distinction between these three setups is in terms of the corresponding conditionals and reward function. For the tasks where some small labeled dataset is accessible, we investigate the benefits of offline data on fine-tuning atomic-GFlowNets. Similar to GCPN [33], we consider two structural tasks, molecular weight (mol.wt.), and logP (partition coefficient for drug's water to octanol concentration [43]). In addition, we also consider other standard drug-discovery tasks where rewards are based on empirical models[44] such as the LD50 (toxicity) task of [45]. For each task and objective pair, we aim to show that fine-tuning a pretrained GFlowNet achieves the task's objectives more quickly than training a new GFlowNet from scratch. We now define each objective in detail."}, {"title": "Property Optimization", "content": "In this task, the goal is to generate novel molecules that minimize specific physicochemical properties. While previous works have commonly benchmarked their models on QED optimization [46, 32, 33], we exclude QED from our evaluation, as our pretrained A-GFN is already optimized for this metric. Instead, we focus on mol.wt. and logP-two critical properties in drug discovery. To further challenge the models, we set the target property ranges to be slightly below the minimum values found in the Zinc250K dataset, with logP in the range [-5, -4.5] and molecular weight in the range [100, 110]. The success percentage is calculated based on the proportion of generated molecules that fall within these strict property ranges. The low success percentages across all methods reflect the challenging nature of the task (Tab.2). However, the fine-tuned A-GFN shows a relatively higher success rate compared to other methods, combined with a lower-L1 distance from the target ranges. This indicates that even in cases where exact matches are not achieved, the fine-tuned A-GFN generates molecules that are close to the desired property values. A-GFN's ability to sample from these \"out-of-domain\" chemical spaces-where molecules fall outside the property ranges in MOD datasets like Zinc250K-suggests that fine-tuned A-GFN excels in generating diverse molecules that could extend the scope of current chemical libraries. Moreover, the high diversity, novelty, and validity of the molecules generated by A-GFN indicate its capacity to produce a broad spectrum of unique, structurally valid compounds, essential for mitigating structural redundancy in drug candidates. Fine-tuning further enhances these aspects, highlighting A-GFN's potential for property-driven molecular design."}, {"title": "Property Targeting", "content": "Here, we generate molecules that satisfy specific molecular property constraints, demonstrating the model's capacity for fine-tuning and adaptation to new objectives. Compared to training A-GFN from scratch, fine-tuning a pretrained A-GFN leads to significantly faster convergence, even when property ranges are altered. In particular, we focus on modifying the TPSA property while keeping the constraints for other molecular properties the same as those used in pretraining.\nTo evaluate the model's ability to generalize to new property constraints, we alter the TPSA range from its pretraining interval of [60, 100] to both lower [40, 60] and higher [100, 120] intervals. Our experiments demonstrate that the fine-tuned A-GFN consistently outperforms the model trained from scratch in terms of convergence speed and overall performance. This is particularly evident in the number of distinct high-reward molecular modes discovered (N_modes), molecular diversity, and success percentage, all of which show substantial improvements after fine-tuning (Tab.6). The ability of the fine-tuned A-GFN to adapt to modified property constraints highlights the robustness of the pretrained $G_{\\theta}$ model, which has not merely memorized specific property ranges but has learned a more generalizable sampling strategy. This adaptability is crucial in real-world drug discovery applications, where molecular property requirements often shift during the drug discovery process."}, {"title": "Property Constrained Optimization", "content": "This comprehensive benchmarking task aims to generate molecules that minimize a target property (e.g., mol.wt., logP, or toxicity) within a predefined range, while maintaining the drug-like characteristics encoded during pretraining. We evaluate two key scenarios: conditionals-preserved fine-tuning and Dynamic Range adjustment (DRA). In the conditionals-preserved fine-tuning setup, we retain the same conditional property ranges ($c_p$) used during pretraining, setting the task's preference_direction to -1, which directs the model to minimize the target property. The desired ranges for these task properties are set between the 25th percentile of the ZINC dataset and a predetermined maximum threshold (see Tab.3 and 7 for specifics). The fine-tuned A-GFN significantly outperforms the A-GFN trained from scratch, achieving superior results within the same computational budget. In the dynamic range adjustment scenario, we modify one of the pretraining conditionals (TPSA) by shifting its range from 60 < TPSA \u2264 100 to both lower (40 < TPSA < 60) and higher (100 < TPSA \u2264 120) values. In this case, fine-tuned A-GFN again surpasses its scratch-trained counterpart, demonstrating faster convergence and higher success rates (Tab.4). We also explore a hybrid online-offline fine-tuning approach, where A-GFN leverages offline task-specific data with the desired $C_p$, similar to its pretraining setup. In most tasks, hybrid fine-tuning shows comparable performance to fully online fine-tuning, with notable exceptions in more complex tasks like logP optimization when 100 < TPSA < 120; it is plausible that in such a case grounding the model in data stabilizes early learning when rewards are low. In this challenging case, the goal was to generate molecules with logP\u22481.5 while maintaining the conditional properties outlined in Tab.5. The inherent difficulty of this task is underscored by the fact that only 0.002% of molecules in the ZINC250K dataset meet these stringent criteria. Despite the challenge, A-GFN fine-tuned with a small offline dataset of expert trajectories achieve a respectable success rate and uncover diverse modes in the chemical space. This underscores the potential of hybrid online-offline fine-tuning to unlock otherwise inaccessible regions of the chemical landscape, offering a promising strategy for tackling difficult-to-sample molecular spaces."}, {"title": "Conclusion", "content": "In this work, we introduced Atomic GFlowNets (A-GFN), an extension of the GFlowNet framework that leverages atoms as fundamental building blocks to explore molecular space. By shifting the action space from predefined molecular fragments to individual atoms, A-GFN is able to explore a much larger chemical space, enabling the discovery of more diverse and pharmacologically relevant molecules. To address the challenges posed by the vastness of this atomic action space, we propose a pretraining strategy using datasets of drug-like molecules. This off-policy pretraining approach conditions A-GFN on informative molecular properties such as drug-likeness, topological polar surface area, and synthetic accessibility, allowing it to effectively explore regions of chemical space that are more likely to yield viable drug candidates.\nOur experimental results demonstrate that pretraining A-GFN with these expert trajectories leads to improved diversity and novelty in the generated molecules. Furthermore, we show that fine-tuning A-GFN for specific property optimization tasks offers significant computational efficiency compared to training from scratch. However, akin to challenges in fine-tuning RL models [47], A-GFN is prone to catastrophic forgetting of pre-trained knowledge during extended fine-tuning. This suggests a need for methods like relative trajectory balance [48] to regularize the fine-tuning process, ensuring the policy stays anchored to the pretrained policy while still adapting to task-specific goals. Within the context of A-GFN, an obvious future direction is to extend it to lead optimization in drug discovery,"}]}