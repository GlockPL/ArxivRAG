{"title": "SOAK: Same/Other/All K-fold cross-validation for estimating similarity of patterns in data subsets", "authors": ["Toby Dylan Hocking", "Gabrielle Thibault", "Cameron Scott Bodine", "Paul Nelson Arellano", "Alexander F Shenkin", "Olivia Jasmine Lindly"], "abstract": "In many real-world applications of machine learning, we are interested to know if it is possible to train on the data that we have gathered so far, and obtain accurate predictions on a new test data subset that is qualitatively different in some respect (time period, geographic region, etc). Another question is whether data subsets are similar enough so that it is beneficial to combine subsets during model training. We propose SOAK, Same/Other/All K-fold cross-validation, a new method which can be used to answer both questions. SOAK systematically compares models which are trained on different subsets of data, and then used for prediction on a fixed test subset, to estimate the similarity of learnable/predictable patterns in data subsets. We show results of using SOAK on six new real data sets (with geographic/temporal subsets, to check if predictions are accurate on new subsets), 3 image pair data sets (subsets are different image types, to check that we get smaller prediction error on similar images), and 11 benchmark data sets with predefined train/test splits (to check similarity of predefined splits).", "sections": [{"title": "Introduction", "content": "A fundamental assumption in supervised learning is similarity between the train data (input to the learning algorithm) and test data (used to evaluate prediction accuracy of the learned model). This assumption is known as \"independent and identically distributed\" (i.i.d.) in statistics [Hastie et al., 2009]. Although special modifications to supervised learning algorithms can guarantee accurate predictions in other scenarios such as covariate shift [Sugiyama and Kawanabe, 2012], this paper focuses on standard supervised learning algorithms, designed for i.i.d. data. Real-world applications of such supervised learning algorithms often involve training/predicting on data subsets which are qualitatively different in some respect (time period, geographic region, data source, etc.), so the main contribution of this paper is a new method that allows us to quantify if these data subsets are similar enough for accurate learning/prediction.\nMotivation for comparing models trained on Same/Other/All subset(s) as test subset. For example, in image segmentation, we would like to train on several labeled regions/subsets, and predict on a new region/subset. In this paper, we use the name \"Other\" for the model trained on several labeled subsets, because it is trained on other subsets of data (relative to the new/test region, see Figure 1). To determine if those \"Other\" train data subsets are similar/large enough to accurately predict on the new subset, we would like to compare those predictions to a baseline \"Same\" model trained using labeled data from the new subset. Ideally, these \"Other\" model predictions on a new subset would be just as accurate as \"Same\""}, {"title": "Related work", "content": "Cross-validation is a standard method in machine learning, that is widely used, and discussed in several textbooks [Bishop and Nasrabadi, 2006, Hastie et al., 2009]. For clarity in this paper, we use the terms from Goodfellow et al. [2016]: the full data set is split into train/test sets (for evaluation), and then the train set is further split into subtrain/validation sets (for hyper-parameter learning). K-fold cross-validation can be used for either type of split, and involves partitioning the data into K disjoint test (or validation) sets; for each, the train (or subtrain) set is defined as all the other data. The idea of averaging several empirical estimators of the risk has been attributed to Geisser [1975]. A primary use of cross-validation is model selection (splitting the train set into subtrain/validation sets), to avoid overfitting during a learning algorithm [Arlot and Celisse, 2010, Stephenson et al., 2021] In contrast, our proposed method is primarily useful for splitting the full data set into train/test sets [Ghosh et al., 2020], in order to quantify the prediction error/accuracy on new/test data that were never seen during learning. Standard K-fold cross-validation can be used for that purpose, and yields K measurements of test error/accuracy that can be useful for comparing the prediction accuracy of different algorithms. An alternative is to use a single train/test split, with one subset for train, and another for test; while this approach is somewhat common in the machine learning literature, it only yields one test error/accuracy number, so it can be a misleading estimate of prediction error/accuracy, that tends to encourage overfitting [Recht et al., 2018]. In contrast, our proposed SOAK method is based on K-fold CV, so yields K test error/accuracy numbers, and can be used with statistical tests of significance.\nDistributional Homogeneity. The cross-validation method that we propose is related to the statistical concepts of independent and identically distributed (i.i.d.) random variables, and of homogeneity in meta-analyses, meaning that different subsets of the data follow the same distribution. Homogeneity/i.i.d. is a stronger condition than we are interested in measuring using our proposed cross-validation procedure (it may be beneficial to combine subsets when learning, even though they are heterogeneous). Classic examples of statistical tests for homogeneity are the Chi-Square test of Pearson [1900] and the Q test of Cochran [1954]. In meta-analysis, the goal is to provide a better estimate of a quantity measured in several different"}, {"title": "Novelty with respect to available software.", "content": "There are many free/open-source implementations of cross-validation, including origami [Coyle and Hejazi, 2018], splitTools [Mayer, 2023] and mlr3 [Lang et al., 2019] in R, as well as scikit-learn in python [Pedregosa et al., 2011]. All of these packages support standard K-fold cross-validation, and some support stratification, and keeping groups of observations together when splitting (a concept/parameter called \"group\"). The proposed SOAK algorithm is based on the concept of data subsets, which was not previously supported in any free software machine learning framework, so we implemented it in the mlr3 framework (https://github.com/tdhock/mlr3resampling), because it provides flexible support for parallelization over data sets, algorithms, and train/test splits (including parallelization over the proposed subsets). The \"group\" concept (observations/rows in the same group must be assigned the same fold ID) is present in both mlr3 and in scikit-learn, and the \"group\" concept is different from the SOAK \"subset\" concept (one subset is designated as test, and Same/Other/All subsets are designated as train), but actually the two concepts can be used together. For example, in image segmentation, labels are often created by drawing polygons, and in each polygon there are several pixels which are assigned the same label. If each polygon is considered a group, then each pixel in a polygon gets the same group ID. The image(s) may be divided into regions/subsets such as North/South/etc in the aztrees data. So we can use both concepts at the same time: let labeled pixels from polygons/groups in the South region/subset be the test set, and define the train set as labeled pixels from polygons/groups in the Same/Other/All region/subset."}, {"title": "Methods: proposed SOAK algorithm and data sets", "content": "In this section, we give details of the proposed SOAK algorithm, and then give details about the 20 data sets we analyzed (Table 1), which represent classification problems with 800\u20132,816,744 rows, 10-27,648 features, 2-11 classes, and 2-4 subsets."}, {"title": "Proposed SOAK algorithm", "content": "We propose SOAK (Same/Other/All K-fold CV), a new variant of cross-validation which is useful for determining similarity between learnable/predictable patterns in data subsets. As in standard K-fold cross-validation for supervised machine learning, we assume there is a data set with N observations/rows. Additionally, we assume that the rows can be partitioned into a certain number of subsets S, and we would like to estimate the similarity of learnable/predictable patterns these subsets. Each subset has an identifier, which we treat here as an integer from 1 to S. For example in Figure 1 there are S = 2 subsets, A and B. For each observation/row i, we assume there is a corresponding subset $s_i \\in \\{1, ..., S\\}$ and fold ID $k_i \\in \\{1, ..., K\\}$ (assigned randomly or using strata, such that each label/subset has about the same number of rows with a given fold ID).\nThe goal of SOAK is to estimate the prediction error/accuracy of a learning algorithm, when attempting to predict on a given subset, and training on that same subset, or on different subsets (others or all). Therefore, our method has a loop over each subset $\\sigma\\in \\{1, ..., S\\}$ and fold $\\kappa\\in \\{1, ..., K\\}$. For each, we define the train/test splits as in Figure 1.\nTest set is $\\{i \\in \\{1, ..., N\\} | k_i = \\kappa \\text{ and } s_i = \\sigma\\}$, all rows i in the current fold $\\kappa$ and subset $\\sigma$.\nSame train set is $\\{i \\in \\{1, ..., N\\} | k_i \\neq \\kappa \\text{ and } s_i = \\sigma\\}$, all rows i not in the current fold $\\kappa$, but in the current subset $\\sigma$.\nOther train set is $\\{i \\in \\{1, ..., N\\} | k_i \\neq \\kappa \\text{ and } s_i \\neq \\sigma\\}$, all rows i which are neither in the current fold $\\kappa$, nor in the current subset $\\sigma$.\nAll train set is $\\{i \\in \\{1, ..., N\\} | k_i \\neq \\kappa\\}$, all rows i not in the current fold $\\kappa$."}, {"title": "Computational complexity.", "content": "For each of S test subsets and K folds, we need to consider training on Same/Other/All subsets, so the number of train/test splits considered by SOAK is 3SK = O(SK), which is the number of times each learning algorithm needs to be run. Importantly, this is linear in the number of subsets S, so it is possible to run SOAK on data with a large number of subsets S."}, {"title": "Implementation Details.", "content": "SOAK can be easily implemented in any programming language, by looping over all subsets $\\sigma\\in \\{1, ..., S\\}$ and fold IDs $\\kappa\\in \\{1, ..., K\\}$. We implemented the computational experiments in this paper using the mlr3 framework in R, which made it easy to compute results in parallel (over all algorithms, data sets, and train/test splits) using the mlr3batchmark package [Becker and Lang, 2024]."}, {"title": "Image Pairs: train on MNIST, predict on EMNIST or FashionMNIST", "content": "The MNIST data set consists of 70,000 images of handwritten digits, and the goal of learning is to accurately classify each image into one of ten classes (0 to 9) [LeCun et al., 1998]. EMNIST is another set of 70,000 images, also of handwritten digits, with balanced classes (see Table 1, column class Imb.), and a different pre-processing technique that attempts to scale images to fill the available space [Cohen et al., 2017]. FashionMNIST is a set of 70,000 images, with 7,000 examples of each of ten classes of clothing [Xiao et al., 2017]. Intuitively, we expect that we should be able to train on MNIST (images of digits), and get reasonable predictions on EMNIST (because images are also digits), but not on FashionMNIST (because images are clothing). Therefore, we created three data sets (Table 1), each with 14,000 images, by combining MNIST with one of the other variants.\nIPair_E: MNIST combined with EMNIST. Note that the raw EMNIST images not in the same orientation as MNIST (if MNIST is upright, then EMNIST appears to be rotated 90\u00b0).\nIPair_E_rot: MNIST combined with rotated EMNIST (all images in upright orientation).\nIPair_Fashion: MNIST combined with FashionMNIST.\nIn each of the three data sets, there are two subsets (one from each source: MNIST and EMNIST or FashionMNIST)."}, {"title": "Benchmark data with a predefined train/test split", "content": "There are many benchmark data sets in the machine learning literature that include a column to designate a predefined train/test split. In this paper, we consider several data sets which were downloaded using torchvision [Marcel and Rodriguez, 2010]: CIFAR10 [Alex, 2009], EMNIST [Cohen et al., 2017], FashionMNIST [Xiao et al., 2017], KMNIST [Clanuwat et al., 2018], MNIST [LeCun et al., 1998], QMNIST [Yadav and Bottou, 2019], STL10 [Coates et al., 2011]; several others that are included as supplementary materials in the textbook of Hastie et al. [2009] (spam, vowel, waveform), and one data set that was included in both (USPS in torch, and zip in the textbook, which we call zipUSPS in this paper). Rather than using the predefined train/test split for its intended purpose, we instead use it as a subset ID, so each of these data sets has two subsets (Table 1). By using SOAK on these data, we seek to answer the question: are the predefined train and test subsets similar enough, so that if train on one subset (either predefined=train or test), we can get predictions on the other subset, that are just as accurate as if we had access to the same subset?"}, {"title": "Real data from various application domains with spatial/temporal subsets", "content": "CanadaFires data consist of four satellite images of forest fires, in which we are primarily interested to answer the question: can we train on some images, and accurately predict burned areas in a new image? (subsets are images of different forest fires) Characterizing the burning pattern using these high-resolution Skysat images is important because it is used by the Qu\u00e9bec government to plan salvage operations. Skysat images of forest fires that occurred in 2020-2021 were used. In addition, Landsat were obtained using Google Earth Engine [Gorelick et al., 2017]. Each row/observation in these data is a labeled pixel, and there are columns/features for Normalized Burn Ratio [French et al., 2008], delta Normalized Burn Ratio [Parks et al., 2014], Normalized Difference Vegetation Index [Pettorelli, 2013], Green Chromatic Coordinates [Alberton et al., 2023], etc., for a total of 46 features. Labels were created for individual pixels, each at least 100m apart, and more than 6m from the image borders, by manually assigning one of six burn classes, based on a classification that the Qu\u00e9bec government uses to characterize fires. We then transformed the labels to a binary problem, burned (positive class) versus other (negative class). There are two versions of these data: A means All data, and D means Down-sampled to promote class balance, while retaining representative examples. In these data, we are interested to see if it is possible to train on a few images/fires, and accurately predict on a new image/fire (there are four such images/subsets).\naztrees data come from satellite images around Flagstaff, AZ, in which we are primarily interested to answer this question: can we train on some regions, and accurately predict presence of trees in new regions? (subsets are different regions in the satellite image) The goal is to predict presence/absence of trees throughout Arizona, in a project that seeks to determine the extent to which trees are under stress/drought/bark beetle infestation. Three sets of satellite images were retrieved from Google Earth Engine: Sentinel-1, Sentinel-2, and the NASA Shuttle Radar Topography Mission (SRTM) Global image [NASA JPL, 2013, Gorelick et al., 2017]. Data were converted so that each row is a pixel, and 20 features/columns were computed, including several spectral bands, Normalized Difference Vegetation Index-NDVI [Rouse et al., 1973], Normalized Difference Infrared Index-NDII [Hardisky et al., 1983], Normalized Difference Water Index-NDWI [Gao, 1995], three Topographic Position-TPI Indexes [Gallant and Austin, 1983]. Images were manually labeled using Quantum GIS software [QGIS Development Team, 2024] by drawing polygons that indicate seven Land Use/Land Cover classes (trees, natural grass, planted grass, infrastructure, bare ground, and open water), then labels were converted into a binary problem (tree versus other). There are two variants of these data, with either 3 (S/NE/NW) or 4 (SE/SW/NE/NW) geographic subsets, and we would like to know if it is possible to train on some subsets, and accurately predict on a new subset.\nFishSonar_river data come from sonar imagery of river bottoms, in which we are primarily interested to answer this question: can we train on a few rivers, and accurately predict areas suitable for fish spawning, on a new river? The goal is to accurately predict spawning areas of Gulf Sturgeon (Acispenser oxyrinchus desotoi), in a wildlife conservation project [U.S. Fish and Wildlife Service and National Marine Fisheries"}, {"title": "Comparing prediction accuracy of 5 learning algorithms on 20 data sets", "content": "First, we used standard 10-fold cross-validation on 20 classification data sets (Table 1) to compare the prediction accuracy of 5 learning algorithms (with hyper-parameters tuned using internal 10-fold cross-validation): cv_glmnet (Ll-regularized linear model); featureless (baseline always predicting most frequent"}, {"title": "SOAK prediction error comparison plots", "content": "Next, we used SOAK with 10-fold CV on each of the 20 data sets, to compute prediction error on each subset, after training cv_glmnet on Same/Other/All subset(s). We expected that some data sets would have very similar subsets (All model better than Same), and others would have very different subsets (Same model better than All). In particular, since train/test splits are typically assigned randomly (due to the i.i.d. assumption), we expected to observe large similarity in train/test subsets, such as in STL10, waveform, KMNIST, and vowel data sets (Figure 3). Two of those data sets (STL10 and waveform) exhibited some evidence of similarity between subsets: All test error was smaller than Same test error (STL10 significantly, p < 0.05 in two-sided paired t9-tests; waveform slightly, p = 0.042 and 0.136). Additional evidence of similarity between subsets in STL10 was that Other test error was significantly smaller or larger than Same (depending on sample size, predefined train subset had 8000 rows whereas predefined test subset had only 5000 rows). Surprisingly, there were two data sets (KMNIST and vowel) which exhibited differences between learnable/predictable patterns in predefined train/test subsets. Other test error was significantly larger than Same (mean difference of 5-25%, p < 0.001), and All test error was significantly larger than Same for the smaller subset (predefined test subset, p < 0.001). Overall, it is clear that by running SOAK then plotting Same/Other/All test error (Figure 3), it is possible to estimate the extent of similarity/differences between learnable/predictable patterns in data subsets."}, {"title": "Discussion and Conclusions", "content": ""}, {"title": "Image pairs: MNIST data combined with EMNIST/FashionMNIST", "content": "Our goal in analyzing these 3 data sets was to quantify our intuition that MNIST/EMNIST are more similar than MNIST/FashionMNIST. We expected that the worst prediction accuracy should be in IPair_Fashion, because the subsets are most different (images of digits/clothing), and that is what we observed (Figure 5, Other-Same test error differences of 77.5\u201384.7%, p < 10-17). We expected an intermediate prediction accuracy for IPair_E, because both subsets are images of digits (although not in the same orientation), and that is"}, {"title": "Analysis of data with predefined train/test subsets", "content": "In analyzing these 11 data sets, our goal was to verify that most data sets have similar predefined train/test splits. In examining Figures 4-5, we observed that most data sets with predefined train/test subsets were in the similar category (MNIST, FashionMNIST, QMNIST, STL10, CIFAR10, EMNIST, waveform, spam, zipUSPS), and two had clearly different subsets (vowel and KMNIST). Our SOAK analysis indicates that the predefined train/test splits in vowel/KMNIST represent a problem which is not i.i.d., so is difficult for standard supervised learning algorithms."}, {"title": "Conclusions and future work", "content": "We presented Same/Other/All K-fold cross-validation (SOAK), which can be used to estimate similar-ity/differences between learnable/predictable patterns in data subsets. We showed how SOAK can be used to gain insights about 20 real-world and benchmark data sets. We quantitatively verified that a model trained on MNIST digits is not as accurate on FashionMNIST (clothing), as on EMNIST (digits). We were also able to verify that most benchmark data had similar predefined train/test subsets (except vowel/KMNIST). We observed significant differences between learnable/predictable patterns in space/time subsets of real data sets. However, results for NSCH_autism indicate it would be slightly beneficial to train a model on the"}, {"title": "Broader Impacts:", "content": "It is important to be able to assess the accuracy of machine learning algorithms in real-world situations, and our proposed SOAK algorithm is ideal for that purpose. Negative impacts include large computation times and energy usage required to train and compare models (similar to standard K-fold CV)."}, {"title": "Limitations:", "content": "SOAK is only applicable to data sets with subsets of interest (this may not be true of all data sets), for which there is enough time to use K-fold cross-validation (we used a linear model, which is fast enough, but this may not be appropriate for other learning algorithms, especially deep neural networks with lots of parameters)."}, {"title": "Software, Documentation, and Reproducibility:", "content": "We provide an R package, published on the CRAN, which implements SOAK in the mlr3 framework: https://github.com/tdhock/mlr3resampling. Detailed documentation for the software, including example code, can be found in the vignettes published on CRAN: https://cloud.r-project.org/web/packages/mlr3resampling/. We also provide a GitHub repository with code used to make the figures in this paper: https://github.com/tdhock/cv-same-other-paper."}]}