{"title": "Neural-Symbolic Reasoning over Knowledge Graphs: A Survey from a Query Perspective", "authors": ["Lihui Liu", "Zihao Wang", "Hanghang Tong"], "abstract": "Knowledge graph reasoning is pivotal in various domains such as data mining, artificial intelligence, the Web, and social sciences. These knowledge graphs function as comprehensive repositories of human knowledge, facilitating the inference of new information. Traditional symbolic reasoning, despite its strengths, struggles with the challenges posed by incomplete and noisy data within these graphs. In contrast, the rise of Neural Symbolic AI marks a significant advancement, merging the robustness of deep learning with the precision of symbolic reasoning. This integration aims to develop Al systems that are not only highly interpretable and explainable but also versatile, effectively bridging the gap between symbolic and neural methodologies. Additionally, the advent of large language models (LLMs) has opened new frontiers in knowledge graph reasoning, enabling the extraction and synthesis of knowledge in unprecedented ways. This survey offers a thorough review of knowledge graph reasoning, focusing on various query types and the classification of neural symbolic reasoning. Furthermore, it explores the innovative integration of knowledge graph reasoning with large language models, highlighting the potential for groundbreaking advancements. This comprehensive overview is designed to support researchers and practitioners across multiple fields, including data mining, AI, the Web, and social sciences, by providing a detailed understanding of the current landscape and future directions in knowledge graph reasoning.", "sections": [{"title": "INTRODUCTION", "content": "A knowledge graph is a graph structure that contains a collection of facts, where nodes represent real-world entities, events, and objects, and edges denote the relationships between two nodes. It is a powerful tool for organizing and connecting information in a way that mimics human thought and learning. Since its debut in 2012,\\u00b9 a variety of knowledge graphs have been generated, including Freebase, Yago, and Wikidata.\nKnowledge graph reasoning refers to the process of deriving new knowledge or insights from existing knowledge graphs in response to a query. In essence, the knowledge graph reasoning pipeline comprises three key elements: the input query, background knowledge, and reasoning model, each posing unique challenges. The background knowledge may vary in completeness, influencing the system's ability to accurately interpret and utilize information. Meanwhile, input queries range from clear and specific to ambiguous or dynamically changing, demanding robust mechanisms for understanding user intent. Furthermore, the reasoning model's approach, whether inductive or deductive, impacts the system's ability to draw meaningful conclusions from the available data. Addressing these challenges necessitates adaptable algorithms and techniques to ensure the efficacy and reliability of knowledge graph reasoning across diverse contexts and applications.\nRecently, there is a trend to utilize neural-symbolic artificial intelligence to enhance reasoning on knowledge graphs. Since knowledge graphs can be viewed as discrete symbolic representations of knowledge, it is natural to integrate knowledge graphs with neural models to unleash the full potential of neural-symbolic reasoning. Traditional symbolic reasoning is intolerant of ambiguous and noisy data, but knowledge graphs are often incomplete, which brings difficulties to symbolic reasoning. On the contrary, the recent advances in deep learning promote neural reasoning on knowledge graphs, which is robust to ambiguous and noisy data but lacks interpretability compared to symbolic reasoning. Considering the advantages and disadvantages of both methodologies, recent efforts have been made to combine the two reasoning methods for better reasoning on knowledge graphs.\nLast but not least, the emergence of large language models (LLMs) has shown impressive natural language capabilities. However, they struggle with logical reasoning that requires structured knowledge. The integration of LLMs with knowledge graphs during the reasoning process represents a promising area of exploration. While some methods have been proposed in this regard, a large part of this topic is unexplored or under-explored."}, {"title": "PRELIMINARY", "content": "In this section, we first formally define knowledge graphs and knowledge graph reasoning."}, {"title": "brief history of knowledge graph", "content": "Knowledge representation has experienced a long-period history of development in the fields of logic and AI. The idea of graphical knowledge representation firstly dated back to 1956 as the concept of semantic net proposed by Richens [10], while the symbolic logic knowledge can go back to the General Problem Solver [1] in 1959. The knowledge base is firstly used with knowledge-based systems for reasoning and problemsolving. MYCIN [2] is one of the most famous rule-based expert systems for medical diagnosis with a knowledge base of about 600 rules. Later, the community of human knowledge representation saw the development of frame-based language, rule-based, and hybrid representations. Approximately at the end of this period, the Cyc project\\u00b9 began, aiming at assembling human knowledge. Resource description framework (RDF)\\u00b2 and Web Ontology Language (OWL)\\u00b3 were released in turn, and became important standards of the Semantic Web\\u2074. Then, many open knowledge bases or ontologies were published, such as WordNet, DBpedia, YAGO, and Freebase. Stokman and Vries [7] proposed a modern idea of structure knowledge in a graph in 1988. However, it was in 2012 that the concept of knowledge graph gained great popularity since its first launch by Google's search engine\\u2075, where"}, {"title": "Definition and Notation", "content": "Knowledge graph reasoning refers to the process of using a knowledge graph, a structured representation of knowledge, as the basis for making logical inferences and drawing conclusions. More formally, the research question can be defined as\nDEFINITION 1. (Knowledge Graph) Let $G = (V, E, R)$ be a knowledge graph, where V is the set of entities, E is the set of relationships, R is the set of triples $(v_i, r_j, v_k)$ denoting relationships between entities, where $v_i, v_k \\in V$ and $r_j \\in E$. Knowledge graph reasoning: Answer queries by traversing and reasoning over the graph.\nDEFINITION 2. (Knowledge Graph Reasoning) Let $G = (V, E, R)$ be a knowledge graph, where V is the set of entities, E is the set of relationships, R is the set of triples $(v_i, r_j, v_k)$ denoting relationships between entities, where $v_i, v_k \\in V$ and $r_j \\in E$. Knowledge graph reasoning: Answer queries by traversing and reasoning over the graph."}, {"title": "Symbolic Reasoning", "content": "Symbolic reasoning in knowledge graphs refers to the process of deriving logical conclusions and making inferences based on symbolic representations of entities, relationships, and rules within the graph structure. In this context, symbols represent entities or concepts, while relationships denote connections or associations between them. Symbolic reasoning involves applying logical rules and operations to manipulate these symbols, enabling the system to perform tasks such as deductive reasoning, semantic inference, and knowledge integration. By leveraging symbolic representations and logical reasoning, knowledge graphs can facilitate complex problem-solving, semantic understanding, and decision-making in various domains, ranging from natural language processing to artificial intelligence applications."}, {"title": "Neural Reasoning", "content": "Neural reasoning in knowledge graphs refers to the utilization of neural network models to perform reasoning tasks directly on the graph structure. Unlike traditional symbolic reasoning approaches, which rely on explicit rules and logical operations, neural reasoning leverages the power of deep learning techniques to learn implicit patterns and relationships within the graph. Through the use of neural networks, knowledge graphs can capture complex, non-linear dependencies between entities and infer higher-level knowledge from the graph's interconnected nodes. Neural reasoning methods often involve embedding entities and relationships into continuous vector spaces, allowing neural networks to efficiently process and reason over large-scale knowledge graphs. This approach enables knowledge graphs to handle uncertainty, noise, and incompleteness"}, {"title": "Neural Symbolic Reasoning", "content": "Neural symbolic reasoning represents a fusion of neural network-based approaches with symbolic reasoning techniques, aiming to leverage the strengths of both paradigms in handling complex reasoning tasks. In this framework, neural networks are used to learn representations of symbolic entities and relationships within a knowledge graph, capturing both their semantic meanings and structural dependencies. These learned representations are then combined with symbolic reasoning mechanisms to perform logical inference and reasoning tasks. By integrating neural and symbolic components, neural symbolic reasoning approaches strive to overcome the limitations of each individual approach. Neural networks offer the ability to learn from data and handle uncertainty, while symbolic reasoning provides formal logic-based reasoning and interpretability. This hybrid approach has shown promise in various domains, including natural language understanding, knowledge graph reasoning, and automated theorem proving, by enabling more robust and flexible reasoning capabilities."}, {"title": "Deductive Reasoning", "content": "Knowledge graph deductive reasoning is a method used to derive new information from existing data within a knowledge graph by applying logical rules. A knowledge graph structures information as a network of entities and their interrelationships, represented in triples of subject-predicate-object. Deductive reasoning in this context involves using established logical rules to infer new facts. For instance, if the knowledge graph contains the triples \"Alice works at XYZ Corp\" and \"XYZ Corp is located in New York,\" a rule might deduce that \"Alice works in New York.\" This process leverages the structured nature of the graph and the logical relationships between entities to expand the knowledge base, ensuring that new conclusions are logically consistent with the existing data."}, {"title": "Inductive Reasoning", "content": "Knowledge graph inductive reasoning involves deriving generalized conclusions from specific instances within a knowledge graph. Unlike deductive reasoning, which applies general rules to specific cases, inductive reasoning identifies patterns and regularities in the data to formulate broader generalizations or hypotheses. For example, if a knowledge graph contains numerous instances where employees of various companies in New York tend to have a high turnover rate, inductive reasoning might lead to the hypothesis that companies in New York generally experience higher employee turnover. This approach allows for the generation of new insights and predictive models by examining trends and correlations in the data, providing a foundation for further exploration and hypothesis testing within the structured framework of a knowledge graph."}, {"title": "Abductive Reasoning", "content": "Knowledge graph abductive reasoning is a process used to infer the most likely explanation for a given set of observations within a knowledge graph. This type of reasoning seeks to find the best hypothesis that explains the observed data, often dealing with incomplete or uncertain information. For example, if a knowledge graph shows that a person has visited multiple cities known for tech conferences, abductive reasoning might infer that the person is likely involved in the tech industry. Unlike deductive reasoning, which guarantees the truth of the conclusion if the premises are true, or inductive reasoning, which generalizes from specific instances, abductive reasoning focuses on finding the most plausible explanation. This method is particularly useful in situations where there are multiple possible explanations, and it aims to identify the one that best fits the available evidence within the structured relationships of a knowledge graph."}, {"title": "Paper Organization", "content": "In this section, we've laid the groundwork by defining knowledge graph reasoning and discussing the related background knowledge. Moving forward, we'll delve into three distinct perspectives: reasoning for single-hop queries, complex logical queries, and natural language questions. Each perspective offers valuable insights into how knowledge graph reasoning operates and evolves in different contexts. By examining these perspectives, we aim to provide a comprehensive understanding of the diverse challenges and advancements within the field of knowledge graph reasoning.The taxonomy of this paper is illustrated in Figure 1."}, {"title": "REASONING FOR SINGLE HOP QUERY", "content": "Reasoning for single-hop queries is a common task in the field of knowledge graphs, often referred to as knowledge graph completion. The objective here is to predict the tail entity t given the head entity h and the relation r, or conversely, to predict the head entity h given the tail entity t and the relation r. In addition to entity prediction, there is the relation prediction task, where the goal is to predict the relationship r between a given head entity h and a tail entity t. This task can also be considered a specialized form of single-hop query.\nA variety of methods have been proposed to address these tasks. In this section, we categorize the different approaches into three main types: symbolic, neural, and neural-symbolic methods, which will be elaborated in the following subsections."}, {"title": "Symbolic Methods", "content": "Hard symbolic rule based reasoning. Symbolic rule reasoning methods rely on logical reasoning and explicit rules within the knowledge graph. They often utilize rule-based or path-based inference techniques to deduce the missing entity or relation. Examples include rule mining algorithms and path ranking methods and so on."}, {"title": "Soft symbolic rule based reasoning", "content": "Despite the idea of hard rule-based reasoning is quite intuitive, it's not the best solution most of the time. Because it requires experts to build rules based on their past experiences and intuitions. So, sometimes, mistakes may be made. Besides, the reasoning method can be very slow because it adds new facts to the knowledge base repeatedly. More importantly, it is not suitable for real time applications. It also lacks flexibility. For example, The reason process will give a world zero probability even if it only violates one formula. But this is not the real-world case. On example is \"smoking causes cancer and friends have similar smoking habits\". These two rules are not always true, because not everyone who smokes gets cancer. And not all friends have similar smoking habits. And soft rules are more useful in this case because if a word violates a formula, it becomes less probable, not impossible."}, {"title": "symbolic path based reasoning", "content": "For the rule based reasoning methods, they require the rule to be given. However, it's not the case usually. We don't have any rules. One possible solution is path-based reasoning. For the path based reasoning method, no rule is needed. It utilizes different paths in the knowledge graph to infer new information. These paths can be directed or undirected.\nThe very first method of path reasoning is path ranking algorithm [28] which treats the random walks around a query node as the relational features. The idea of PRA is to use random walk to find many different paths between two nodes and treat these paths as the feature of the relation, and use a logistic regression model to learn a classifier to predict the truthfulness of the triplet.\nAfter PRA, ProPPR [19] incorporates vector similarity into random walk inference over KGs to mitigate the feature sparsity inherent in using surface text. Specifically, when conducting a series of edge traversals in a random walk, ProPPR allows the walk to explore edges that exhibit semantic similarity to the given edge types, as defined by vector space embeddings of the edge types. This integration of distributional similarity and symbolic logical inference aims to alleviate the sparsity of the feature space constructed by PRA."}, {"title": "symbolic rule mining", "content": "Rule mining can also be treated as a special type of single hop query answering. Instead of directly answer which entity might be the correct answer, Rule mining aims at deducing general logic rules from the knowledge graphs. The entities derived from the given head entity and the query relation following the logic rules are returned as the answers.\nAMIE [18] delves into logic rule exploration through a two-step process. Initially, it engages in Rule Extending, wherein candidate rules undergo expansion via three distinct operations: Add Dangling Atom, Add Instantiated Atom and Add Closing Atom. Subsequently, in the Rule Pruning phase, it sifts through the rules, discarding those deemed faulty, and selects the confident ones based on predefined evaluation metrics. In terms of implementation, the approach leverages SPARQL queries on graph databases to sift through the knowledge graphs (KGs), identifying suitable facts (h, r, t) that adhere to the extended rules from the first step and surpass the specified metric thresholds from the second step.\nAfter AMIE, the subsequent algorithm, AMIE+ [17], enhances the efficiency of AMIE's implementation through adjustments to both the Rule Extending process and the evaluation metrics in the Rule Pruning phase. In the Rule Extending stage, AMIE+ selectively extends a rule only if it can be completed before reaching the predefined maximum rule length. To elaborate, it refrains from appending dangling atoms in the final step, which would introduce fresh variables and lead to non-closure. Instead, AMIE+ waits until the last step to incorporate instantiated atoms and closing atoms, thereby ensuring rule closure. Additionally, the SPARQL queries employed for rule search are streamlined. For instance, when appending a dangling atom to a parent rule $R_p$ to generate a child rule $R_c$, if the predicate of the new atom already exists in $R_p$, the support for $R_c$ remains the same as that of $R_p$. Consequently, recalculating support for $R_c$ becomes unnecessary, thus expediting the SPARQL query process.\nWhile AMIE and AMIE+ have found extensive use across various scenarios, they still face scalability challenges when dealing with large knowledge graphs (KGs). This limitation stems from their reliance on projection queries executed via SQL or SPARQL, where reducing the vast search space remains challenging. In response, RLVLR [43] employs an embedding technique to sample relevant entities and facts pertaining to the target predicate/relation, significantly curtailing the search space. Firstly, RLvLR samples a sub-knowledge graph relevant to the target predicate. Secondly, it utilizes the RESCAL knowledge graph embedding model [42] to generate embeddings for entities and relations in the subgraph, with the embedding for a predicate augmentation being the average value of associated entity embeddings. Thirdly, RLvLR employs a scoring function based on these embeddings to guide and prune rule search, proving effective in rule extraction. Finally, candidate rules are evaluated based on metrics such as $he$ and $conf$, akin to AMIE, computed efficiently through matrix multiplication. By incorporating the embedding technique, RLvLR significantly enhances the efficiency of the rule search process. Another method RuLES [23] utilizes the embedding technique to assess the quality of learned rules. It incorporates external text information of entities to derive their embeddings, enabling the calculation of confidence scores for facts. RuLES defines the external quality of a learned rule as the average confidence score of all derived facts. Ultimately, RuLES"}, {"title": "Summary", "content": "In this section, we explore various methods relevant to symbolic reasoning. We discuss rule-based approaches, which leverage logical rules for inference, as well as path-based methods, which analyze patterns within knowledge graphs. Additionally, we delve into rule mining techniques, which aim to extract logical rules from structured data sources. Each method offers unique insights and capabilities in the realm of symbolic reasoning."}, {"title": "Neural-Symbolic Methods", "content": "Neural-symbolic methods aim to combine the strengths of both symbolic and neural methods. They often incorporate symbolic reasoning within a neural framework or use neural networks to enhance symbolic inference."}, {"title": "Knowledge graph embedding", "content": "Knowledge graph embedding usually encodes entities as low-dimensional vectors and encodes relations as parametric algebraic operations in the continuous space. The basic idea is to design a score function $f$ which takes the triplet embedding as input, so that a true triplet receives a higher score than a false one. There are a lot of applications which utilize knowledge graph embedding. One of the most common applications is knowledge graph completion. For example, given a head entity and a tail entity, the missing relation is the one which maximizes the score function value. Likewise, given a head entity and a relation, the missing tail entity is the one which, again, maximizes the score function value.\nMany KG embedding methods have been developed. The basic idea of TransE [6] is to view the relation r as the transition from the head entity to the tail entity. Mathematically, it means that ideally, the tail entity t should be the summation of the head entity and the relation. Another method is DistMult [70]. Similar as TransE, DistMult also embed entities and relations into vectors in the real/encludience space. Different from TransE, DistMult views the relation r as the elementwise weights of the head entity h. Its score function is defined as the weighted sum over all elements of the head entity by the corresponding elements in the relation. So, in DistMult, the ideal tail entity should be hr. Another method Complex [57] embeds entities and relations in Complex vector space. Each embedding now has a real part and an imaginary part. Given a point z which is x + iy in the embedding space, its conjugate \u017d is x iy. The scoring function used in complex is very similar to that of distmult. But We replace t by its conjugate we only taking the real part of the function. Different from dot product, in Complex [57] Hermitian dot product $ < h, r, t >$ is asymmetric, where t is the complex conjugate of t. Thus it naturally captures the anti-symmetry. Another method is called RotatE [54]. The key idea of rotate is to solve the limitions in previous methods. similar to complex, rotate also represent head and tail entities and relation in complex vector space, Different from complex, all the relation embedding are modelled as rotation from the head entity h to the tail entity t. Compared with other methods, Rotate can support different relation properties, such as symmetry/antisymmetry, inversion, and composition. Other methods such as TransH"}, {"title": "Neural symbolic rule based reasoning", "content": "Neural LP [72], which is a generalization of Tensorlog that focuses on learning logical rules with confidence scores. In Tensorlog, the reasoning process is a sequence of matrix multiplication operations. Tensorlog denotes the input query entity as a one-hot vector and each relation as a matrix R. The reasoning results are computed by matrix multiplication, retrieving entities whose entries are non-zero as answers. Neural LP adopts the same idea as Tensorlog. In Neural LP, the authors found that when the rule length increases from 2 to L, the original first matrix multiplication then summation process can be rewritten as the first summation then multiplication process. After changing the order of the operations, the original matrix multiplication process becomes learning the combination of relationships at each step. This process can be modeled by a recurrent neural network (RNN) for T steps. The candidate pool of Neural LP is very large, which leads to a huge search space. So, it's hard to identify useful rules in the search space. Most of the time the weights may not reflect the importance of rules precisely. To solve these limitations, RNNLogic [44] treats all logic rules as latent variables. That is, to answer a query, there may be more than 10 or 20 related rules, and we treat all these logic rules as latent variables. In this way, the rule mining problem becomes a rule inference problem. RNNLogic contains two components: the rule generator, which will generate some candidate logic rules for a specific query, and a reasoning predictor, which is used to predict how likely we can find the answer given a logic rule. Different from Neural LP, the search space of RNNLogic is much smaller. Because all logic rules are treated as latent variables, the EM algorithm can be used for inference."}, {"title": "Neural symbolic path based methods", "content": "Previously, we introduced symbolic path-based reasoning methods. Neural symbolic"}, {"title": "Summary", "content": "Neural-symbolic reasoning combines the strengths of neural networks and symbolic reasoning to tackle the complexities of knowledge graphs. Traditional symbolic reasoning methods, like rule-based expert systems, employ predefined rules for inference, while path-based approaches like the Path Ranking Algorithm utilize random walks to infer relationships. Hybrid methodologies, such as PathCon, integrate relational context and path information through neural networks, enhancing reasoning capabilities. Embedding techniques, including geometric and probabilistic embeddings, represent entities and relationships in continuous vector spaces, facilitating more flexible knowledge graph operations. Reinforcement learning-based methods, like DeepPath, utilize trained agents to navigate knowledge graphs and predict missing links. By merging neural and symbolic techniques, neural-symbolic reasoning offers a comprehensive approach to understanding and reasoning over complex knowledge graph structures, promising advancements in various applications requiring automated reasoning and inference."}, {"title": "REASONING FOR COMPLEX LOGIC QUERY", "content": "In this section, we generalize the query into logically more complex forms [37] and explain how to solve them using neural and symbolic methods. Compared to simple-hop queries, the additional complexity is introduced by involving more \u201celements\u201d in logical language, such as multiple predicates, quantifiers, and variables. The fundamental motivation of complex queries also follows the narrative of single-hop query prediction, where we want to derive new knowledge but with more logical constraints. We also refer readers to the earlier survey [66] in this direction. Both single-hop queries and multi-hop queries fall under the broader category of complex queries. However, to differentiate knowledge graph completion from complex logical query answering, we treat single-hop queries and complex logical queries as two distinct components."}, {"title": "What is Complex Query?", "content": "General logical queries follow the definitions of mathematical logic and model theory [37]. The previous but perhaps not up-to-date survey provides more rigorous definitions and discussions [66]. Regarding logical language coverage, complex queries studied on knowledge graphs are still preliminary compared to parallel studies in databases [29] and semantic web research [38].\nIn the literature, a general term describing complex queries on knowledge graphs is the general \"first-order query\" or \"first-order logical query\" [46, 47]. However, recent rigorous characterization [76] distinguished the queries discussed in the definition, and queries studied in empirical methods and benchmarks are two overlapping query families. The first is existential first-order queries that appeared in definitions of many works [33, 47], and the second is the tree-formed queries widely adopted in the empirical construction of benchmarks [65]. Most empirical results remain credible despite the fine-grained differences in query families.\nWe hereby introduce the definitions of two queries based on a fragment of the first-order language. A term is either an entity $e \\in V$ or a variable. A variable is a non-determined entity whose value can"}, {"title": "", "content": "be taken from V. A variable can be either existentially quantified or not. Universally quantified variables are not considered yet in the literature. An atomic formula is r(a, b) where r \u2208 E is the relation. Then, we define the Existential First-Order (EFO) formulae.\nDEFINITION 3 (EXISTENTIAL FIRST ORDER FORMULA). The set of the existential formulas is the smallest set that satisfies the following property:\n(i) For atomic formular (a, b), itself and its negation r(a, b), \u00acr(a, b) \u2208 \u03a6, where a, b are either entities in V or variables, r is the relation in E.\n(ii) If \u03c6, \u03c8 \u20ac \u03a6, then (\u03c6^\u03c8), (\u03c6\u03bd \u03c8) \u2208 \u03a6\n(iii) If \u03c6 \u2208 \u03a6 and xi is any variable, then \u2203xi\u03c6\u2208 \u03a6.\nWhen there is more than one free variable, an EFO formula is an EFO query. In most previous studies, only one existential variable is considered, leading to the family of EFO-1, denoted as \u03a6. The families with more than one variable are titled EFO-k [75]; so far, there is no specific method targeting EFO-k. The key feature of EFO queries is that the logical negation is only attached to atomic formulas, defined by rule (i). Consequently, one can always move existential quantifiers at the beginning of the formula as the prenex normal form [37]. Moreover, it is always convenient to reorganize the logical conjunctions and disjunctions into either Disjunctive Normal Form (DNF) or Conjunctive Normal Form (CNF). One common way to define the EFO-1 query is by the DNF and the conjunctive queries.\nSpecifically, the queries q can be written as follows.\nDEFINITION 4 (EFO-1 QUERY). An EFO-1 query is defined as\n$q(y) = \\exists x_1, ..., x_k, \\pi_1 (y) \\vee \\cdot\\cdot\\cdot \\vee \\pi_n(y),$   (1)\nwhere y is the variable to be queried, x1, ..., xk are existentially quantified variables, and \u03c0\u03b7(y) is the conjunctive query to be defined in the following parts.\nDEFINITION 5 (CONJUNCTIVE QUERY). A conjunctive logical query is written as\n$\\pi_\\iota (y) = \\exists x_1,...,x_k : \\rho_1 \\wedge \\cdot\\cdot\\cdot \\wedge \\rho_m$\nwhere each p is the atomic formula r(a, b) or its negation \u00acr(a, b).\nAnother query family that is well studied is formally defined as the Tree-Formed (TF) queries Otf.\nDEFINITION 6 (TREE-FORM QUERY). The set of the Tree-Form queries is the smallest set Otf such that:\n(i) If \u03c6(y) = r(a, y), where a \u2208 E, r \u2208 R, then \u03c6(y) \u2208 \u03a6tf;\n(ii) If \u03c6(y) \u2208 \u03a6tf, \u00ac\u03c6(y) \u2208 \u03a6tf;\n(iii) If \u03c6(y), \u03c8(y) \u2208 \u03a6tf, then (\u03c6 \u2227 \u03c8)(y) \u2208 \u03a6tf (\u03c6 \u03bd \u03c8)(y) \u2208 \u03a6tf;\n(iv) If(y) \u2208 \u03a6tf andy' is any variable, then (y') = \u2203y.r(y, y')^\n\u03c6(y) \u2208 \u03a6tf.\nOne key feature of tf is that the answer set can be constructed recursively through set operations, such as union, intersection, and compliment. As specifically shown in Definition 6, rule (ii) corresponds to the set complement against an implicit universe set; rule (iii) relates to the set intersection and union to logical conjunction and disjunction; and rule (iv) for set projection. Under the context"}, {"title": "", "content": "of tree-form queries, we use logical connectives (conjunction, disjunction, and negation) and set operations (intersection, union, and complement) interchangeably.\nEFO-1 and tree-form query families are different but not mutually exclusive (EFO-1 \u2229 TF \u2260 0). There are also queries in the tree-form family but not in EFO-1 and vice versa. Detailed discussions of query syntax can be found in [76]. The follow-up part then explains neural and neural-symbolic methods for TF and EFO-1 queries."}, {"title": "Neural Methods", "content": "Neural methods conduct logical reasoning in a fixed-dimensional embedding space, where previous insights from knowledge graph embeddings can be applied. The methods for tree-formed queries and EFO-1 queries differ significantly in two ways. In short, methods targeting tree-formed queries emphasize the modeling of set operations [65]. Methods for the EFO-1 query stress the DNF formulation and the conjunctive query."}, {"title": "Tree-form query", "content": "The first attempt to solve a tree-form query begins with the logical conjunction, or more specifically; the final answer set can be derived by set projection and set intersection. As the earliest example, GQE [21] embeds graph nodes in a low-dimensional space and represents set projection and intersection as neural operations in the embedding space. Consequently, terms in the query, including constant entities, existential variables, and free variables, can be represented or computed as the embedding. Then, the nearest neighbor search is used to find answers. The embeddings and neural models are trained on synthetic datasets by an end-to-end auto-differentiation. Follow-up methods followed the key design principles: (1) represent the terms into low-dimensional embeddings; (2) set operations are modeled by differentiable operations in the embedding spaces; (3) identify the final answers by nearest neighbor search. Moreover, the supported set operations are expanded to set union (logical disjunction) and set complement (logical negation).\nNotably, the set intersection, union, and negation provide some natural properties and intuitions. An example is the box-embedding space and various query embedding methods. Query2Box [46] proposes to model queries as boxes (i.e., hyper-rectangles), where a set of points inside the box corresponds to a set of answer entities of the query. Set intersections can be naturally represented as geometric intersections of boxes in the space. On the other hand, the set union cannot be modeled by the geometric union of boxes because the resulting shape is not a box. This issue can be indirectly addressed by transforming queries into a DNF. Furthermore, NewLook [33] adopts a neural network to relearn the box embedding at each projection operation to mitigate the cascading error problem and also introduces a new logic operator, set difference, so that the set compliment can be modeled by the equivalently. Besides the box-embedding space, other kinds of embedding spaces are also widely explored, including the space of convex cones [82], parametric probabilistic distributions [47, 71], and vectors [9, 63]."}, {"title": "EFO-1 query", "content": "This part only focuses on methods that are capable of solving queries that are EFO-1 but not tree-form. Such queries are characterized by the query graph of the sub-conjunctive query. Specifically, such a query can be represented as a simple"}, {"title": "Neural-Symbolic Methods", "content": "The neural-symbolic methods for complex query answering integrate symbolic algorithms, which have been extensively studied in the database community. The neural part of such approaches, on the other hand, is less capable than that of neural approaches. In practice, the neural part of neural symbolic approaches is mainly chosen as link predictors or knowledge graph embeddings. Different from the previous discussions on the neural approach, neural symbolic approaches rely heavily on the symbolic algorithm; thus, they can solve a more extensive set of queries.\nAlmost all symbolic algorithms search for a proper assignment of variables $x_1 = e_1,...,x_k = e_k$, and $y = a$ to satisfy the logical constraints in queries. Combined with neural link predictors, the boolean value of satisfaction is turned into a continuous score or normalized into [0, 1] as fuzzy truth values. The preliminary approach models the adjacent matrices of KG for each relation, with elements as the fuzzy truth value of triples. The details of how to normalize the output of the link predictor into fuzzy truth values vary in different methods. However, it does not change the nature of the problem as a search process. Several search strategies can be seamlessly applied to such a problem. For example, beam search realized for the acyclic query graph is known as CQD [3], and search on acyclic query graph with additional backtracking is proposed as QTO [5]. The generalization from acyclic to cyclic and multi-edge query graphs is known as FIT [76]. Many algorithmic results are also available to accelerate the algorithm, such as using a count-min sketch in EMQL [51] to store the entity set for each query node and using vector similarity to find similar results during the search process.\nNeural link predictors can be deeply engaged with search and not just materialized as adjacency matrices. CQD-CO relaxed the search problem from symbolic assignment spaces into neural embedding space, thus enabling gradient-based optimization with differentiable link predictions [3]. CQD-A, as a more advanced method, can adapt knowledge graph embeddings from the feedback of the training data [4]. A similar but technically different approach is the GNN-QE, where the link predictor is not just embeddings but a graph neural network to be learned from the feedback of the search results [84].\nRecently, some approaches have attempted to combine large language models (LLMs) with neural-symbolic methods to address this problem. For instance, in [36], a framework is proposed that decomposes complex questions into multiple subquestions, which are then individually answered by LLMs. Simultaneously, neural-symbolic methods are applied to incomplete knowledge graphs. At"}, {"title": "REASONING FOR NATURAL LANGUAGE QUERY", "content": "When the input query is a natural language sentence, existing methods can be divided into several categories, such as semantic parsing-based methods, information retrieval-based methods, and embedding-based methods.\nFor example, PullNet [52", "39": "and EmbedKGQA [49", "32": "a reranking based method is used to rerank all candidate answers to get better results. In semantic parsing-based methods, a general strategy to answer the question is to transform the question into a query graph and search for the answer according to the query graph. For example, in [74", "34": ""}]}