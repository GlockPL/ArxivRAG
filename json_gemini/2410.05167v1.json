{"title": "Presto! DISTILLING STEPS AND LAYERS FOR ACCELERATING MUSIC GENERATION", "authors": ["Zachary Novack", "Julian McAuley", "Taylor Berg-Kirkpatrick", "Ge Zhu", "Jonah Casebeer", "Nicholas J. Bryan"], "abstract": "Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains a challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop a new score-based distribution matching distillation (DMD) method for the EDM-family of diffusion models, the first GAN-based distillation method for TTM. To reduce the cost per step, we develop a simple, but powerful improvement to a recent layer distillation method that improves learning via better preserving hidden state variance. Finally, we combine our step and layer distillation methods together for a dual-faceted approach. We evaluate our step and layer distillation methods independently and show each yield best-in-class performance. Our combined distillation method can generate high-quality outputs with improved diversity, accelerating our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than comparable SOTA) \u2014 the fastest high-quality TTM to our knowledge.", "sections": [{"title": "INTRODUCTION", "content": "We have seen a renaissance of audio-domain generative media (Chen et al., 2024; Agostinelli et al., 2023; Liu et al., 2023; Copet et al., 2023), with increasing capabilities for both Text-to-Audio (TTA) and Text-to-Music (TTM) generation. This work has been driven in-part by audio-domain diffusion models (Song et al., 2020; Ho et al., 2020; Song et al., 2021), enabling considerably better audio modeling than generative adversarial network (GAN) or variational autoencoder (VAE) methods (Dhariwal & Nichol, 2021). Diffusion models, however, suffer from long inference times due to their iterative denoising process, requiring a substantial number of function evaluations (NFE) during inference (i.e. sampling) and resulting in \u22485-20 seconds at best for non-batched latency.\nAccelerating diffusion inference typically focuses on step distillation, i.e. the process of reducing the number of sampling steps by distilling the diffusion model into a few-step generator. Methods include consistency-based (Salimans & Ho, 2022; Song et al., 2023; Kim et al., 2023) and adversarial (Sauer et al., 2023; Yin et al., 2023; 2024; Kang et al., 2024) approaches. Others have also investigated layer-distillation (Ma et al., 2024; Wimbauer et al., 2024; Moon et al., 2024), which draws from transformer early exiting (Hou et al., 2020; Schuster et al., 2021) by dropping interior layers to reduce the cost per sampling step for image generation. For TTA/TTM models, however, distillation techniques have only been applied to shorter or lower-quality audio (Bai et al., 2024; Novack et al., 2024a), necessitate \u224810 steps (vs. 1-4 step image methods) to match base quality (Saito et al., 2024), and have not successfully used layer or GAN-based distillation methods.\nWe present Presto\u00b9, a dual-faceted distillation approach to inference acceleration for score-based diffusion transformers via reducing the number of sampling steps and the cost per step. Presto includes three distillation methods: (1) Presto-S, a new distribution matching distillation algorithm for score-based, EDM-style diffusion models (see Fig. 1) leveraging GAN-based step distillation"}, {"title": "BACKGROUND & RELATED WORK", "content": "Audio-domain music generation methods commonly use autoregressive (AR) techniques (Zeghidour et al., 2021; Agostinelli et al., 2023; Copet et al., 2023) or diffusion (Forsgren & Martiros, 2022; Liu et al., 2023; 2024b; Schneider et al., 2023). Diffusion-based TTA/TTM (Forsgren & Martiros, 2022; Liu et al., 2023; 2024b; Schneider et al., 2023; Evans et al., 2024a) has shown the promise of full-text control (Huang et al., 2023), precise musical attribute control (Novack et al., 2024b;a; Tal et al., 2024), structured long-form generation (Evans et al., 2024b), and higher overall quality over AR methods (Evans et al., 2024a; b; Novack et al., 2024b; Evans et al., 2024c). The main downside of diffusion, however, is that it is slow and thus not amenable to interactive-rate control."}, {"title": "SCORE-BASED DIFFUSION MODELS", "content": "Continuous-time diffusion models have shown great promise over discrete-time models both for their improved performance on images (Balaji et al., 2022; Karras et al., 2023; Liu et al., 2024a) and audio (Nistal et al., 2024; Zhu et al., 2023; Saito et al., 2024), as well as their relationship to the general class of flow-based models (Sauer et al., 2024; Tal et al., 2024). Such models involve a forward noising process that gradually adds Gaussian noise to real audio signals \u00e6real and a reverse process that transforms pure Gaussian noise back into data (Song et al., 2021; Sohl-Dickstein et al., 2015). The reverse process is defined by a stochastic differential equation (SDE) with an equivalent ordinary differential equation (ODE) form called the probability flow (PF) ODE (Song et al., 2021):\n dx = -0\u2207x log p(x | \u03c3)do, (1)\nwhere x log p(x | \u03c3) is the score function of the marginal density of \u00e6 (i.e. the noisy data) at noise level o according to the forward diffusion process. Thus, the goal of score-based diffusion models is to learn a denoiser network \u03bc\u0473 such that \u03bc\u0473(x, \u03c3) = E[xreal | X, \u03c3]. The score function is:\nVx log p(x | 0) \u2248  X \u03bc\u03bf (\u03b1, \u03c3)/ \u03c3 (2)\nGiven a trained score model, we can generate samples at inference time by setting a decreasing noise schedule of N levels omax = \u03c3\u03b7 > \u03c3\u03c0\u22121 > > \u03c3\u03bf = 0min and iteratively solving the ODE at these levels using our model and any off-the-shelf ODE solver (e.g. Euler, Heun).\nThe EDM-family (Karras et al., 2022; 2023) of score-based diffusion models is of particular interest and unifies several continuous-time model variants within a common framework and improves model parameterization and training process. The EDM score model is trained by minimizing a reweighted denoising score matching (DSM) loss (Song et al., 2021):\nLDSM = Exreal~D,0~p(otrain),\u20ac~N(0,1) [\u03bb(\u03c3)||Xreal \u2013 \u03bc\u04e9 (Xreal + \u03b5\u03c3, \u03c3)||2], (3)\nwhere p(otrain) denotes the noise distribution during training, and \u03bb(\u03c3) is a noise-level weighting function. Notably, EDM defines a different noise distribution to discretize for inference p(inf) that is distinct from p(otrain) (see Fig. 2), as opposed to a noise schedule shared between training and inference. Additionally, EDMs represent the denoising network using extra noise-dependent preconditioning parameters, training a network fe with the parameterization:\n\u03bc\u03bf(\u03b1, \u03c3) = Cskip(o)x + Cout(0)fo(Cin(0)x, Cnoise (0)). (4)\nFor TTM models, \u03bc\u0473 is equipped with various condition embeddings (e.g. text) \u03bc\u03bf(x, \u03c3, \u03b5). \u03a4\u03bf increase text relevance and quality at the cost of diversity, we employ classifier free guidance (CFG) (Ho & Salimans, 2021), converting the denoised output to: \u03bc\u03b9 (x,\u03c3,e) = \u03bc\u0473(x,\u03c3,\u00d8) + w(\u03bc\u03bf(x, \u03c3, \u03b5) \u2013 \u03bc\u0473(x, \u03c3, \u00d8)), where w is the guidance weight and \u00d8 is a \u201cnull\u201d conditioning."}, {"title": "DIFFUSION DISTILLATION", "content": "Step distillation is the process of reducing diffusion sampling steps by distilling a base model into a few-step generator. Such methods can be organized into two broad categories. Online consistency approaches such as consistency models (Song et al., 2023), consistency trajectory models (Kim et al.,"}, {"title": "Presto!", "content": "We propose a dual-faceted distillation approach for inference acceleration of continuous-time diffusion models. Continuous-time models have been shown to outperform discrete-time DDPM models (Song et al., 2020; Karras et al., 2022; 2024), but past DMD/DMD2 work focuses on the latter. Thus, we redefine DMD2 (a step distillation method) in Section 3.1 for continuous-time score models, then present an improved formulation and study its design space in Section 3.2. Second, we design a simple, but powerful improvement to the SOTA layer distillation method to understand the impact of reducing inference cost per step in Section 3.3. Finally, we investigate how to combine step and layer distillation methods together in Section 3.4."}, {"title": "EDM-STYLE DISTRIBUTION MATCHING DISTILLATION", "content": "We first redefine DMD2 in the language of continuous-time, score-based diffusion models (i.e. EDM-style). Our goal is to distill our score model \u03bc\u03b8 (which we equivalently denote as preal, as it is trained to model the score of real data) into an accelerated generator Go that can sample in 1-4 steps. Formally, we wish to minimize the KL Divergence between the real distribution preal and the generator G\u00f8's distribution Pfake: LDMD = D(Preal || Pfake). The KL term cannot be calculated explicitly, but we can calculate its gradient with respect to the generator if we can access the score of the generator's distribution. Thus, we also train a \u201cfake\u201d score model \u03bc\u03c6 (or equivalently, \u03bcfake) to approximate the generator distribution's score function at each gradient step during training.\nFirst, given some real data \u00e6real, we sample a noise level from a set of predefined levels \u03c3 ~ {\u03c3\u03af}gen, and then pass the corrupted real data through the generator to get the generated output gen G\u00a2(xreal + \u03c3\u03b5, \u03c3), where \u03b5 ~ N(0, I) (we omit the conditioning e for brevity). The gradient of the KL divergence between the real and the generator's distribution can then be calculated as:\n\u2207\u00a2LDMD = \u0395\u03c3~{\u03c3\u00bf},\u20ac~N(0,1) [((\u00b5fake(xgen + \u03c3\u03b5, \u03c3) \u2013 \u03bcreal(xgen + \u03c3\u03b5, \u03c3)) \u2207\u00a2\u00c2gen], (5)\nwhere {0} are the predefined noise levels for all loss calculations, and real is the CFG-augmented real score model. To ensure that fake accurately models the score of the generator's distribution"}, {"title": "PRESTO-S: SCORE-BASED DISTRIBUTION MATCHING DISTILLATION", "content": "Given the new notation, we proceed to develop our score-based distribution matching step distillation, Presto-S. Our algorithm is shown in Fig. 1, in Appendix 1, and discussed below."}, {"title": "CONTINUOUS-TIME GENERATOR INPUTS", "content": "In Section 3.1, the noise level and/or timestep is sampled from a discrete, hand-chosen set {i}gen. Discretizing inputs, however, forces the model to 1) be a function of a specific number of steps, requiring users to retrain separate models for each desired step budget (Yin et al., 2024; Kohler et al., 2024) and 2) be a function of specific noise levels, which may not be optimally aligned with where different structural, semantic, and perceptual features arise in the diffusion process (Si et al., 2024; Kynk\u00e4\u00e4nniemi et al., 2024; Balaji et al., 2022; Sabour et al., 2024). When extending to continuous-time models, we train the distilled generator G as a function of the continuous noise level sampled from the distribution \u03c3 ~ p(\u03c3). This allows our generator to both adapt better to variable budgets and to variable noise levels, as the generator can be trained with all noise levels sampled from p(\u03c3)."}, {"title": "PERCEPTUAL LOSS WEIGHTING WITH VARIABLE NOISE DISTRIBUTIONS", "content": "A key difference between discrete-time and continuous-time diffusion models is the need for discretization of the noise process during inference. In discrete models, a single noise schedule defines a particular mapping between timestep t and its noise level o, and is fixed throughout training and inference. In continuous-time EDM models, however, we use a noise distribution p(otrain) to sample during training, and a separate noise distribution for inference p(inf) that is discretized to define the sampling schedule. In particular, when viewed in terms of the signal-to-noise ratio 1/02 or SNR as shown in Fig. 2, the training noise distribution puts the majority of its mass in the mid-to-high SNR range of the diffusion process. This design choice focuses on semantic and perceptual features, while the inference noise distribution is more evenly distributed but with a bias towards the low-SNR region, giving a bias to low-frequency features."}, {"title": "AUDIO-ALIGNED DISCRIMINATOR DESIGN", "content": "The original DMD2 uses a classic non-saturating GAN loss. The discriminator is a series of convolutional blocks downsampling the intermediate features into a single probability for real vs. fake. While this approach is standard in image-domain applications, many recent adversarial waveform synthesis works (Kumar et al., 2023; Zhu et al., 2024) use a Least-Squares GAN loss:\narg min max E Co~pGAN (train), [|| Dy (xreal + \u03c3\u03b5, \u03c3)||2]+Eg~pGAN (train), [||1 - D\u2084(&gen+\u03c3\u03b5, \u03c3)||2], (8)\nwhere the outputs of the discriminator Dy are only partially downsampled into a lower-resolution version of the input data (in this case, a latent 1-D tensor). This forces the discriminator to attend to more fine-grained, temporally-aligned features for determining realness, as the loss is averaged across the partially downsampled discriminator outputs. Hence, we use this style of discriminator for Presto-S to both improve and stabilize (Mao et al., 2017) the GAN gradient into our generator."}, {"title": "PRESTO-L: VARIANCE AND BUDGET-AWARE LAYER DROPPING", "content": "Given our step distillation approach above, we now seek methods to reduce the cost of individual steps themselves here through layer distillation, and then combine both step and layer distillation in Section 3.4. We begin with the current SOTA method: ASE (Moon et al., 2024). ASE employs a fixed dropping schedule that monotonically maps noise levels to compute budgets, allocating more layers to lower noise levels. We enhance this method in three key ways: (1) ensuring consistent variance in layer distilled outputs, (2) implementing explicit budget conditioning, and (3) aligning layer-dropped outputs through direct distillation.\nVariance Preservation: First, we inspect the within-layer activation variance of our base model in Fig. 4. We find that while the variance predictably increases over depth, it notably spikes on the"}, {"title": "PRESTO-LS: LAYER-STEP DISTILLATION", "content": "As the act of layer distillation is, in principle, unrelated to the step distillation, there is no reason a priori that these methods could not work together. However, we found combining such methods to be surprisingly non-trivial. In particular, we empirically find that attempting both performing Presto-L finetuning and Presto-S at the same time OR performing Presto-L finetuning from an initial Presto-S checkpoint results in large instability and model degradation, as the discriminator dominates the optimization process and achieves near-perfect accuracy on real data.\nWe instead find three key factors in making combined step and layer distillation work: (1) Layer-Step Distillation we first perform layer distillation then step distillation, which is more stable as the already-finetuned layer dropping prevents generator collapse; (2) Full Capacity Score Estimation we keep the real and fake score models initialized from the original score model rather than the layer-distilled model, as this stabilizes the distribution matching gradient and provides regularization to the discriminator since the fake score model and the generator are initialized with different weights; and (3) Reduced Dropping Budget \u2013 we keep more layers during the layer distillation. We discuss more in Section 4.6 and how alternatives fail in Appendix A.4."}, {"title": "EXPERIMENTS", "content": "We show the efficacy of Presto via a number of experiments. We first ablate the design choices afforded by Presto-S, and separately show how Presto-L flatly improves standard diffusion sampling. We then show how Presto-L and Presto-S stack up against SOTA baselines, and how we can combine such approaches for further acceleration, with both quantitative and subjective metrics. We finish by describing a number of extensions enabled by our accelerated, continuous-time framework."}, {"title": "SETUP", "content": "Model: We use latent diffusion (Rombach et al., 2022) with a fully convolutional VAE (Kumar et al., 2023) to generate mono 44.1kHz audio and convert to stereo using MusicHiFi (Zhu et al., 2024). Our latent diffusion model builds upon DiT-XL (Peebles & Xie, 2023) and takes in three conditioning signals: the noise level, text prompts, and beat per minute (BPM) for each song. We use FlashAttention-2 (Dao, 2023) for the DiT and torch.compile for the VAE decoder and MusicHiFi. For more details, see Appendix A.1.\nData: We use a 3.6K hour dataset of mono 44.1 kHz licensed instrumental music, augmented with pitch-shifting and time-stretching. Data includes musical meta-data and synthetic captions. For evaluation, we use Song Describer (no vocals) (Manco et al., 2023) split into 32 second chunks.\nBaselines: We compare against a number of acceleration algorithms and external models, including Consistency Models (CM) (Song et al., 2023), SoundCTM (Saito et al., 2024), DITTO-CTM (Novack et al., 2024a), DMD-GAN (Yin et al., 2024), ASE (Moon et al., 2024), MusicGen (Copet et al., 2023), and Stable Audio Open (Evans et al., 2024c). See Appendix A.2 for more details.\nMetrics: We use Frechet Audio Distance (FAD) (Kilgour et al., 2018), Maximum Mean Discrepancy (MMD) (Jayasumana et al., 2024), and Contrastive Language-Audio Pretraining (CLAP) score (Wu et al., 2023), all with the CLAP-LAION music backbone (Wu et al., 2023) given its high correlation with human perception (Gui et al., 2024). FAD and MMD measure audio quality/real-ness with respect to Song Describer (lower better), and CLAP score measures prompt adherence (higher better). When comparing to other models, we also include density (measuring quality), re-call and coverage (measuring diversity) (Naeem et al., 2020), and real-time factor (RTF) for both mono (M) and stereo (S, using MusicHiFi), which measures the total seconds of audio generated divided by the generation time, where higher is better for all."}, {"title": "EXPLORING THE DESIGN SPACE OF PRESTO-S", "content": "Loss Distribution Choice: In Table 1 (Top), we show the FAD, MMD, and CLAP score for many Presto-S distilled models with different noise distribution choices. We find that the original DMD2 (Yin et al., 2024) setup (first row) underperforms compared to adapting the loss distributions. The largest change is in switching PDMD to the training distribution, which improves all metrics. This confirms our hypothesis that by focusing on the region most important for text guidance (Kynk\u00e4\u00e4nniemi et al., 2024), we improve both audio quality and text adherence. Switching PGAN to the training distribution also helps; in this case, the discriminator is made to focus on higher-frequency features (Si et al., 2024), benefiting quality. We also find only a small improvement when using the training distribution for PDSM. This suggests that while the training distribution should lead to more stable learning of the online generator's score (Wang et al., 2024b), this may not be crucial. For all remaining experiments, we use PDMD (train) = PGAN (train) = PDSM (train) and pgen (inf)."}, {"title": "PRESTO-L RESULTS", "content": "We compare Presto-L with both our baseline diffusion model and ASE (Moon et al., 2024) using the 2nd order DPM++ sampler (Lu et al., 2022) with CFG++ (Chung et al., 2024). For ASE and Presto-L, we use the optimal \"D3\" configuration from Moon et al. (2024), which corresponds to a dropping schedule, in terms of decreasing noise level (in quintiles), of [14, 12, 8, 4, 0] (i.e. we drop 14 layers for noise levels in the top quintile, 12 for the next highest quintile, and so on). Layer distillation results at various sampling budgets are shown in Fig. 6. Presto-L yields an improvement over the base model on all metrics, speeding up by \u224827% and improving quality and text relevance. ASE provides similar acceleration but degrades performance at high sampling steps and scales inconsistently. The behavior of dropping layers improving performance can be viewed via the lens of multi-task learning, where (1) denoising each noise level is a different task (2) later layers only activating for lower noise levels enables specialization for higher frequencies. See Appendix A.7 for further ablations."}, {"title": "FULL COMPARISON", "content": "In Table 2, we compare against multiple baselines and external models. For step distillation, Presto-S is best-in-class and the only distillation method to close to base model quality, while achieving an over 15x speedup in RTF from the base model. Additionally, Presto-LS improves performance for MMD, beating the base model with further speedups (230/435ms latency for 32 second mono/stereo 44.1kHz on an A100 40 GB). We also find Presto-LS improves diversity with higher recall. Overall, Presto-LS is 15x faster than SAO. We investigate latency more in Appendix A.6."}, {"title": "LISTENING TEST", "content": "We also conducted a subjective listening test to compare Presto-LS with our base model, the best non-adversarial distillation technique SoundCTM (Saito et al., 2024) distilled from our base model, and Stable Audio Open (Evans et al., 2024c). Users (n = 16) were given 20 sets of examples generated from each model (randomly cut to 10s for brevity) using random prompts from Song Describer and asked to rate the musical quality, taking into account both fidelity and semantic text match between 0-100. We run multiple paired t-tests with Bonferroni correction and find Presto-LS rates highest against all baselines (p < 0.05). We show additional plots in Fig. 12."}, {"title": "PRESTO-LS QUALITATIVE ANALYSIS", "content": "While Presto-LS improves speed and quality/diversity over step-only distillation, the increases are modest, as the dropping schedule for Presto-L was reduced ([12, 8, 8, 0, 0]) for step distillation stability. To investigate more, we analyze the hidden state activation variance of our step-distilled model in Fig. 7. The behavior is quite different than the base model, as the \"spike\" in the final layer is more amortized across the last 10 layers and never reaches the base model's magnitude. We hypothesize step-distilled models have more unique computation throughout each DiT block, making layer dropping difficult."}, {"title": "EXTENSIONS", "content": "Adaptive Step Schedule: A benefit of our continuous-time distillation is that besides setting how many steps (e.g., 1-4), we can set where those steps occur along the diffusion process by tuning the p parameter in the EDM inference schedule, which is normally set to p = 7. In particular, decreasing p (lower bounded by 1) puts more weight on low-SNR features and increasing p on higher-SNR features (Karras et al., 2022). Qualitatively, we find that this process enables increased diversity of outputs, even from the same latent code (see Appendix A.5).\nCPU Runtime: We benchmark Presto-LS's speed performance for CPU inference. On an Intel Xeon Platinum 8275CL CPU, we achieve a mono RTF of 0.74, generating 32 seconds of audio in 43.34 seconds. We hope to explore further CPU acceleration in future work."}, {"title": "CONCLUSION", "content": "We proposed Presto, a dual-faceted approach to accelerating latent diffusion transformers by reducing sampling steps and cost per step via distillation. Our core contributions include the development of score-based distribution matching distillation (the first GAN-based distillation for TTM), a new layer distillation method, the first combined layer-step distillation, and evaluation showing each method are independently best-in-class and, when combined, can accelerate our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than the comparable SOTA model), resulting in the fastest TTM model to our knowledge. We hope our work will motivate continued work on (1) fusing step and layer distillation and (2) new distillation of methods for continuous-time score models across media modalities such as image and video."}, {"title": "MODEL DESIGN DETAILS", "content": "As we perform latent diffusion, we first train a variational autoencoder. We build on the Improved RVQGAN (Kumar et al., 2023) architecture and training scheme by using a KL-bottleneck with a dimension of 32 and an effective hop of 960 samples, resulting in an approximately 45 Hz VAE. We train to convergence using the recommended mel-reconstruction loss and the least-squares GAN formulation with L1 feature matching on multi-period and multi-band discriminators.\nOur proposed base score model backbone builds upon DiT-XL (Peebles & Xie, 2023), with modifications aimed at optimizing computational efficiency. Specifically, we use a streamlined transformer block design, consisting of a single attention layer followed by a single feed-forward layer, similar to Llama (Dubey et al., 2024). Our model utilizes three types of conditions including noise levels (timesteps) for score estimation, beat per minute (BPM) values of the song, and text descriptions. Following EDM, we apply a logarithmic transformation to the noise levels, followed by sinusoidal embeddings. Similarly, BPM values are input as scalars then go through sinusoidal embeddings to generate BPM embeddings. These processed noise-level embeddings and BPM embeddings are then combined and integrated into the DiT block through an adaptive layer normalization block. For text conditioning, we compute text embedding tokens with T5-based encoders and concatenate with audio tokens at each attention layer. As a result, the audio token query attends to a concatenated sequence of audio and text keys, enabling the model to jointly extract relevant information from both modalities. To provide baseline architectural speedups, we use FlashAttention-2 (Dao, 2023) for the DiT and Pytorch 2.0's built in graph compilation (Ansel et al., 2024) for the VAE decoder and MusicHifi mono-to-stereo.\nFor the diffusion model hyparameter design, we follow Karras et al. (2024). Specifically, we set Odata = 0.5, Pmean = -0.4, Pstd = 1.0, max = 80, 0min = 0.002. We train the base model with 10% condition dropout to enable CFG. The base model was trained for 5 days across 32 A100 GPUs with a batch size of 14 and learning rate of 1e-4 with Adam. For all score model experiments, we use CFG++ (Chung et al., 2024) with w = 0.8.\nFor Presto-S, following Yin et al. (2024) we use a fixed guidance scale of w = 4.5 throughout distillation for the teacher model as CFG++ is not applicable for the distribution matching gradient. Additionally, we use a learning rate of 5e-7 with Adam. For all step distillation methods, we distill each model with a batch size of 80 across 16 Nvidia A100 GPUs for 32K iterations. We train all layer distillation methods for 60K iterations with a batch size of 12 across 16 A100 GPUs."}, {"title": "BASELINE DESCRIPTION", "content": "We benchmark against multiple diffusion acceleration algorithms and open-source models:\nConsistency Models (CM) (Song et al., 2023; Bai et al., 2024): This distillation technique learns a mapping from anywhere on the diffusion process to the data distribution (i.e. xt \u2192 x0) by enforcing the self-consistency property that G$(xt,t) = G\u00a2(xt',t') \u221at,t'. We follow the parameterization used in past audio works (Bai et al., 2024; Novack et al., 2024a) that additionally distills the CFG parameter into the model directly.\nSoundCTM (Saito et al., 2024): This approach distills a model into a consistency trajectory model (Kim et al., 2023) that enforces the self-consistency property for the entire diffusion process, learning an anywhere-to-anwhere mapping. SoundCTM forgoes the original CTM adversarial loss and calculates the consistency loss using intermediate features of the base model.\nDITTO-CTM (Novack et al., 2024a), This audio approach is also based off of (Kim et al., 2023), yet brings the consistency loss back into the raw outputs and instead replaces CTM's multi-step teacher distillation with single-step teacher (like CMs) and removes the learned target timestep embedding, thus more efficient (though less complete) than SoundCTM.\nDMD-GAN (Yin et al., 2024): This approach removes the distribution matching loss from DMD2, making it a fully GAN-based finetuning method, which is in line with past adversarial distillation methods (Sauer et al., 2023))."}, {"title": "PRESTO-S ALGORITHM", "content": "Our complete Presto-S algorithm is outlined below. For a visual representation, please see Fig. 1.\nAlgorithm 1 Presto-S\ninput: generator G\u00f8, real score model preal, fake score model \u03bc\u0173, discriminator D\u2084, CFG weight w, Pgen (inf), PDMD (train), PDSM (train), PGAN (train), real sample \u00e6real, GAN weights V1, V2, optimizers 91, 92, weighting function \u5165\n: \u03c3~ Pgen(inf)\n: \u20acgen ~N(0, I)\n: gen = G(xreal + \u03c3\u03b5gen, \u03c3)\n: if generator turn then\n~PDMD(train)\n\u2207\u00a2LDMD = ((\u03bc\u03c8 (xgen + dmd, 8) \u2013 preal (xgen + Edmd, 0)). \u2207 gen\n: end if"}, {"title": "ANALYZING FAILURE MODES OF COMBINED LAYER AND STEP DISTILLATION", "content": "We empirically discovered a number of failure modes when trying to combine step and layer distillation. As noted in Section 4.6, the heavier per-layer requirements of distilled few-step generation made all standard dropping schedules (Moon et al., 2024) intractable and prone to quick generator collapse, necessitating a more conservative dropping schedule. In Fig. 8, we show the generator loss, discriminator loss, distribution matching gradient, and the discriminator's accuracy for the real inputs over distillation, for a number of different setups:\nPresto-S, pure step distillation mechanism (blue).\nPresto-LS, optimal combined setup where we pretrain the model with Presto-L and then perform Presto-S, but with keeping the real and fake score models initialized from the original score model (orange).\nLS with L-Fake/Real, which mimics Presto-LS but uses the Presto-L model for the fake and real score models as well (green)."}, {"title": "INFERENCE-TIME NOISE SCHEDULE SENSITIVITY ANALYSIS", "content": "Given our final Presto-LS distilled 4-step generator, we show how changing the inference-time noise schedule can noticeably alter the outputs, motivating our idea of a continuous-time conditioning.\nThe EDM inference schedule follows the form of:\n\u03c3\u03af<\u039d =  \u03c3\u03c4\u03c1 +(max\u03c1 \u2212min\u03c1)(i/N\u22121) \u03c1  max1/\u03c1 (9)\nwhere increasing the p parameter puts more weight on the low-noise, high-SNR regions of the diffusion process. In Fig. 9, we show a number of samples generated from Presto-LS with identical conditions and latent codes (i.e. starting noise and all other added gaussian noise during sampling), only changing p, from the standard of 7 to 1000 (high weight in low-noise region). We expect further inference-time tuning of the noise schedule to be beneficial."}, {"title": "RTF ANALYSIS", "content": "We define the RTF for a model 0 as: RTF(0) =  Tb/latencyb(\u03b8), where To is the generation duration or how much contiguous audio the model can generate at once and latency(b) is the"}]}