{"title": "Enhancing Neural Network Robustness Against Fault Injection Through Non-linear Weight Transformations", "authors": ["Ninnart Fuengfusin", "Hakaru Tamukoh"], "abstract": "Deploying deep neural networks (DNNs) in real-world environments poses challenges due to faults that can manifest in physical hardware from radiation, aging, and temperature fluctuations. To address this, previous works have focused on protecting DNNs via activation range restriction using clipped ReLU and finding the optimal clipping threshold. However, this work instead focuses on constraining DNN weights by applying saturated activation functions (SAFs): Tanh, Arctan, and others. SAFs prevent faults from causing DNN weights to become excessively large, which can lead to model failure. These methods not only enhance the robustness of DNNs against fault injections but also improve DNN performance by a small margin. Before deployment, DNNs are trained with weights constrained by SAFs. During deployment, the weights without applied SAF are written to mediums with faults. When read, weights with faults are applied with SAFs and are used for inference. We demonstrate our proposed method across three datasets (CIFAR10, CIFAR100, ImageNet 2012) and across three datatypes (32-bit floating point (FP32), 16-bit floating point, and 8-bit fixed point). We show that our method enables FP32 ResNet18 with ImageNet 2012 to operate at a bit-error rate of 0.00001 with minor accuracy loss, while without the proposed method, the FP32 DNN only produces random guesses. Furthermore, to accelerate the training process, we demonstrate that an ImageNet 2012 pre-trained ResNet18 can be adapted to SAF by training for a few epochs with a slight improvement in Top-1 accuracy while still ensuring robustness against fault injection.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep neural networks (DNNs) have been gaining attention due to their ability to achieve the state-of-the-art performance across various tasks [1]\u2013[3]. Furthermore, with the introduction of large-language models (LLMs), the ability of DNNs to generalize across tasks has dramatically improved [4]. However, this often comes with a trade-off in terms of their size and computational overheads.\nServing large-scale DNNs requires a number of hardware, from memory to processors. This increases the overall likelihood of faults manifesting in physical hardware, as these faults can be caused by temperature fluctuations [5], aging [6], write errors [7], and other factors. These faults may cause bit-flips in DNN parameters and can easily trigger models failures [8], which can be catastrophic in safety-critical applications.\nTo address this issue, several research methods have been proposed. One of these is activation-restriction-based methods, which are designed based on bounded activation functions, such as ReLU6 and Tanh. These activation functions can suppress high-intensity activation values that are likely to be caused by faults and improve the overall numerical stability of DNNs under faults [8]. Further work in this field focuses on ReLU-like activation functions that include a threshold to map high-intensity positive activation values to zero [9]-[12].\nInstead of focusing on activations, this work focuses on DNN weights, which are more sensitive to faults compared to activations [13]. Since weights are reused across inputs, it is more crucial to protect them. Before deployment, our DNNS are trained with weights constrained by saturated activation function (SAF) that maps outputs to a bounded range. During deployment, the weights, without the applied SAF, are written to fault-prone mediums. When in use, weights with faults are applied with SAF to mitigate large deviations caused by faults.\nThe use of non-linear activation functions on weights was proposed by [14] as a regularization technique. Our work expands on this concept to harden DNN weights against fault injections. Instead of Arctan that [14] based on, we explore several additional activation functions: Tanh, modified Tanh, and Softsign. To accelerate the SAF adaptation process, we demonstrate that our methods can be fine-tuned using torchvision [15] ImageNet 2012 [16] pre-trained weights, rather than training from scratch. This reduces the number of training epochs from hundreds to only a few epochs, with a slight improvement in top-1 accuracy.\nWe demonstrate that our method enables 32-bit floating point (FP32) or 16-bit floating point (FP16) models, which are highly sensitive to bit-flips, to operate at a bit-error rate (BER) of 10-5 with only minor performance loss, without any bells and whistles. In contrast, baseline FP32 and FP16 DNNS"}, {"title": "II. RELATED WORKS", "content": "In this section, we describe two research directions related to our method. The first is the use of non-linear activation functions applied to DNN weights, and the second is the use of bounded activation functions to enhance DNN tolerance to bit-flips.\nThe utilization of non-linear activation functions with DNN weights was introduced by the weight compounder [14]. The weight compounder applies an Arctan-based non-linear activation function to weights, regularizing DNNs by discouraging weights that are close to zero and large weights, which indicate signs of dead weights and over-fitting, respectively.\nFor the second research direction, the use of bounded activation functions to enhance DNN tolerance against bit-flips was proposed by [8]. This work demonstrates that bounded activation functions, such as ReLU6 and Tanh, can suppress high-intensity activations from propagating across DNNs. In the same direction, [9] introduces a clipped ReLU, which is based on ReLU with an additional threshold to map high-intensity positive activation values to zero. To determine the optimal clipping threshold, several methods have been proposed, ranging from profiling values [10], fine-tuning [9], to training-base approaches [12].\nOur work extends the weight compounder [14] by applying SAFs to harden DNNs against fault injections, and further introduces additional SAF functions, including Tanh, modified Tanh, and Softsign. While our approach is similar to activation-restriction-based methods in that both methods map input values into a bounded range, our method focuses on weight restrictions, as weights are more sensitive to faults compared to activations [13]. Another distinction is that our method does not rely on ReLU activation functions, which cannot be directly applied to DNN weights.\nThe downside of activation-restriction-based methods is their computational complexity, which increases with the number of input data. In contrast, our method depends on the number of weights, which remains constant regardless of the number of input data. This limitation becomes more significant when serving DNNs in large-scale applications, as the number of input data grows with the number of users."}, {"title": "III. PROPOSED METHOD", "content": "Given the i-th layer weight as \\(W_i\\) and the output activation from the (i - 1)-th layer as \\(a_{i-1}\\). The affine transformation followed by a non-linear activation, commonly used in feed-forward DNNs, is defined in (1). Here, \\(\\sigma\\) is a non-linear activation function, and \\(b_i\\) is the i-th layer bias term.\n\\[\\alpha_i = \\sigma(W_i a_{i-1} + b_i) \\qquad(1)\\]\nDuring training, the key distinction between our proposed method and (1) is that our method applies \\(\\tau\\), or SAFs, to the weights, as shown in (2).\n\\[\\alpha_i = \\sigma(\\tau(W_i) a_{i-1} + b_i) \\qquad(2)\\]\nSAF is a non-linear activation function that saturates high-intensity input values into a bounded range. Different \\(\\tau\\) functions penalize high-intensity values in varying ways.\nThis penalization effect can be controlled to an extent by scaling the input values before applying SAF. To demonstrate this, we introduce the modified Tanh. The modified Tanh is a Tanh function with an additional hyper-parameter c, defined as tanh(cx), where tanh is the Tanh function, x is the input value, and c is a hyper-parameter that controls how quickly the output value converges to the saturated value. Since Tanh strongly penalizes high-intensity values compared to other \\(\\tau\\) functions, we introduce the modified Tanh with c = 0.5 (Tanh0.5) to relax the penalization effect.\nDuring deployment to fault-prone mediums, bit-flips may occur in \\(W_i\\). Let f represent a function that injects bit-flips with a bit-error rate (BER). Thus, the weights with faults, \\(\\widehat{W_i}\\) are denoted as shown in (3).\n\\[\\widehat{W_i} = f(W_i, BER) \\qquad(3)\\]"}, {"title": "IV. EXPERIMENTAL RESULTS AND DISCUSSION", "content": "In this section, we conduct experiments across three datasets: CIFAR10, CIFAR100, and ImageNet 2012 [16].\nSince faults occur randomly, we simulate them by performing Monte Carlo simulations, randomly injecting bit-flips into weights with a probability of BER = \\(10^{-5}\\). For CIFAR10 and CIFAR100, we conducted 100 rounds of simulations, while for ImageNet 2012, we performed 10 rounds. The average and standard deviation of the top-1 test accuracy are reported in the format of Mean \u00b1 Standard Deviation.\nWe reported the results across three datatypes: FP32, FP16, and Q2.5. Since the default datatype for DNN is FP32, we reported it as is. For FP16 and Q2.5 datatypes, we converted from FP32 weights. Note that all DNN operations were performed in the FP32 datatype, with the exceptions of bit-flips, which were injected into the weights with their respective datatypes.\nFor CIFAR10 dataset, we observed that Tanh0.5 provides the best performance without fault injections for both FP32 and FP16 datatypes. For the Q2.5 datatype, Softsign offers a better alternative. However, after fault injections, Softsign delivers the best test accuracies across all datatypes. Applying SAFs to the weights significantly mitigates the reduction in test accuracy caused by fault injections.\nIn the best case, for FP32, the top-1 accuracy reduction is reduced from 53.73 without SAFs to 1.86 with Softsign. For FP16, the top-1 accuracy reduction is reduced from 57.66 without SAFs to 2.38 with Softsign. For Q2.5, the top-1 accuracy reduction is reduced from 30.96 without SAFs to 1.53 with Softsign. As demonstrated in Table I, Softsign also provides the lowest standard deviation across all datatypes.\nCompared to Q2.5 models, FP32 and FP16 models are more sensitive to bit-flips, as shown by both lower mean and higher standard deviation in top-1 accuracy. This sensitivity arises from their high dynamic range, which allows for dramatic changes in weight values after bit-flips [8]. However, by applying SAFs, we can ensure that FP32 and FP16 models operate at a BER of \\(10^{-5}\\) with only minor loss in model performance.\nFurthermore, we conducted experiments with ResNet20 on the CIFAR10 dataset across different BER values.\nFor the CIFAR100 dataset, using the same experimental settings as for CIFAR10, the experimental results are shown in Table II.\nFrom Table II, we observed that Tanh0.5 provides the best performance without fault injections across all datatypes. However, after fault injections, Tanh delivers the best top-1 accuracy for FP32 and FP16. For Q2.5 datatype, Softsign is the better alternative. Similar to the CIFAR10 dataset, applying SAFs significantly mitigates the overall reduction of top-1 accuracy reduction after bit-flips.\nFor FP32, the top-1 accuracy reduction is reduced from 49.40 without SAFs to 2.55 with Tanh. For FP16, the top-1 accuracy reduction decreases from 49.33 without SAFs to 2.14 with Tanh. For Q2.5, the top-1 accuracy reduction is reduced from 28.45 without SAFs to 2.24 with Softsign. Similar to Softsign in the CIFAR10 dataset, Tanh provides the lowest standard deviation across all datatypes.\nThe best candidate SAFs for both the CIFAR10 and CIFAR100 datasets are Softsign and Tanh. However, Softsign and Tanh are oppposite in terms of penalizing high-intensity values. Softsign is more relaxed, while Tanh is more penalizing. Tanh0.5 lies between Softsign and Tanh but performs worse relative to both. Another distinction between Softsign and Tanh is the curve of their SAFs, which we hypothesize may contribute to their robustness against bit-flips."}, {"title": "B. ImageNet 2012", "content": "So far, our methods have been trained from scratch to allow the DNN to be aware of SAF. However, this experiment demonstrates that the training process for our method can be accelerated by using pre-trained weights.\nTo demonstrate this, we used ResNet18 [17] with a pre-trained weight from torchvision [15]. We fine-tuned this model for 5 epochs with a batch size of 128, a initial learning rate of \\(10^{-5}\\) with a cosine learning rate scheduler, and the AdamW optimizer [18] with a weight decay of \\(10^{-3}\\)\nIn this experiment, Tanh was selected as one of the best SAF candidate from the CIFAR10 and CIFAR100 sections. We could not directly fine-tune with SoftSign, as we hypothesized that SoftSign might cause numerical instability when training with pre-trained weights. If SAF is set to None, it indicates that we directly utilized the torchvision pre-trained weights for inferences without applying SAFs. The experimental results are shown in Table III.\nFrom Table III, we observed that by fine-tuning ResNet18 with Tanh, the top-1 accuracy improved by 0.57. Furthermore, after fault injections, the top-1 accuracy of the baseline models dropped to random guesses for both FP32 and FP16. However, since Q2.5 is more robust to bit-flips, its top-1 accuracy only reduced by 18.83. With our proposed method using Tanh as the SAF, the top-1 accuracy dropped by only 3.07 for FP32, 2.83 for FP16, and 4.47 for Q2.5.\nWe also evaluated ResNet18 on the ImageNet 2012 dataset across different BER values.\nFrom Fig. 4, our SAFs significantly enhance the robustness of DNNs across BER values from \\(10^{-6}\\) to \\(10^{-5}\\). However, while still better than random guesses, our SAFs experience top-1 accuracy reduction of more than half when operating at a BER of \\(10^{-4}\\)."}, {"title": "V. CONCLUSION", "content": "We propose a method to enhance the DNN tolerance against bit-flips by applying SAFs to weights. To let DNN aware of SAFs, this can be achieved by either training the models from scratch or fine-tuning from commonly pre-trained weights. Before deployment, the weights without SAFs are written to fault-prone mediums. When read, weights with faults are applied with SAFs to suppress large deviations caused by the faults. As the result, the overheads of our method is minimal, involving only to apply SAF to weights. We demonstrate that our proposed method enables models to operate at a BER of \\(10^{-5}\\) with minor loss in test accuracy from fault injections across three datasets and three datatypes."}]}