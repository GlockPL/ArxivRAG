{"title": "SCENETAP: Scene-Coherent Typographic Adversarial Planner against Vision-Language Models in Real-World Environments", "authors": ["Yue Cao", "Yun Xing", "Jie Zhang", "Di Lin", "Tianwei Zhang", "Ivor Tsang", "Yang Liu", "Qing Guo"], "abstract": "Large vision-language models (LVLMs) have shown remarkable capabilities in interpreting visual content. While existing works demonstrate these models' vulnerability to deliberately placed adversarial texts, such texts are often easily identifiable as anomalous. In this paper, we present the first approach to generate scene-coherent typographic adversarial attacks that mislead advanced LVLMs while maintaining visual naturalness through the capability of the LLM-based agent. Our approach addresses three critical questions: what adversarial text to generate, where to place it within the scene, and how to integrate it seamlessly. We propose a training-free, multi-modal LLM-driven scene-coherent typographic adversarial planning (SceneTAP) that employs a three-stage process: scene understanding, adversarial planning, and seamless integration. The SceneTAP utilizes chain-of-thought reasoning to comprehend the scene, formulate effective adversarial text, strategically plan its placement, and provide detailed instructions for natural integration within the image. This is followed by a scene-coherent TextDiffuser that executes the attack using a local diffusion mechanism. We extend our method to real-world scenarios by printing and placing generated patches in physical environments, demonstrating its practical implications. Extensive experiments show that our scene-coherent adversarial text successfully misleads state-of-the-art LVLMs, including ChatGPT-4o, even after capturing new images of physical setups. Our evaluations demonstrate a significant increase in attack success rates while maintaining visual naturalness and contextual appropriateness. This work highlights vulnerabilities in current vision-language models to sophisticated, scene-coherent adversarial attacks and provides insights into potential defense mechanisms.", "sections": [{"title": "1. Introduction", "content": "Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across various multimodal tasks, including image captioning, visual question answering, and complex scene understanding [3-5]. These models effectively leverage the intricate relationships between visual and textual information, allowing them to interpret and respond to visual content with sophisticated semantic understanding. However, like other deep learning architectures [6], LVLMs exhibit vulnerability to adversarial examples [7-12]-inputs modified with carefully crafted, imperceptible perturbations designed to mislead the model. Adversarial attacks can expose the risks of LVLMs in real-world applications and drive progress toward safer LVLMs. However, traditional noise-like adversarial perturbations in the image are rare in the real world and thus can hardly reveal real-world risks. Recently, typographic attacks [1, 2, 13] have been proposed, embedding deliberate text within images to compromise the reliability of LVLMs' responses significantly. As illustrated in Fig. 1, consider a question asking \u201cWhat action should be taked for the car\" While the scene indicates pedestrians crossing and \"Slow Down\" should be the correct answer. When we introduce the text \"Proceed\" to the image through typographic attacks [1, 2], the attacked images successfully mislead the LVLM into incorrectly responding Proceed.\nHowever, existing typographic attacks face several key limitations: Current methods rely on manually predefined adversarial text that cannot adapt to different images and questions, potentially reducing attack success rates. The placement of adversarial text follows rigid, predefined patterns (such as center or margin positioning) rather than considering context-specific optimal locations. Recent studies [1, 2] show that text placement significantly influences LVLM responses. These attacks often result in visually unnatural appearances due to simplistic placement strategies and lack of scene integration. As shown in Fig. 1, existing approaches either insert text directly into images [1, 13], place it on white margins [2], or embed it inconsistently on scene objects [13]. Furthermore, such placements frequently occlude critical object features [1, 13], achieving success through visual obstruction rather than genuine perceptual manipulation. These limitations significantly constrain the real-world applicability of typographic adversarial attacks, where seamless environmental integration is essential. Despite the significance of these challenges, they remain under-studied in current research.\nAn ideal typographic attack should automatically generate context-aware adversarial text based on specific images and questions, intelligently determine more suitable text placement locations, naturally integrate text into images, and enable physical deployment without attracting unwanted human attention. To address these challenges, we propose a novel approach, i.e., scene-coherent typographic adversarial planner (SceneTAP), that leverages large language models (LLMs) to create more sophisticated typographic attacks by first using the LLM to comprehend the input image and question to formulate effective adversarial text, to strategically plan suitable text placement within the scene, and to generate detailed instructions for natural text integration. These LLM-generated instructions then guide a scene-coherent TextDiffuser [14] to seamlessly insert the adversarial text into the image, ensuring visual consistency with the surrounding environment. Our main contributions are as follows:\n\u2022 We comprehensively study the influence of adversarial text and its placement on the effectiveness of the typographic attack. To this end, we build four types of adversarial texts and perform the empirical study, revealing how question and image context affect the attack.\n\u2022 We introduce a novel typographic attack, termed the scene-coherent typographic attack, which strategically embeds adversarial texts into images in a naturalistic manner, generating a synthesized image and misleading LVLMs.\n\u2022 We formulate the attack as an LLM-based planning problem and design a new scene-coherent typographic planner (SCENETAP) based on the LLM, which can generate adversarial text, specify suitable text placement, and insert the text automatically and naturally.\n\u2022 We propose to deploy the synthesized typographic texts into the physical world and validate their effectiveness within diverse physical scenes."}, {"title": "2. Related Work", "content": "Typographic attacks against LVLMs. With the rise of LVLMs, many researchers have begun studying adversarial attacks on these models. Most studies directly employ gradient-based optimization methods to add noise to images, leading to malicious text outputs [7, 15, 16]. Interestingly, Gao et al. [17] investigated how altering images could increase the inference time of LVLMs. However, such methods rely on internal model information, such as gradients and logits, limiting their practical use. Starting from Goh et al. [18] found manipulating the text within images can induce misclassification of models like CLIP, the typographic attack has emerged as one of the main adversarial attack paradigms for exploiting vulnerabilities in VLMs [19-21]. Subsequent works expanded this thread by exploring text-image blended diffusion models for text-driven image editing [22], disentangling visual and textual concepts to understand VLM behavior [23] and applying model patching for mitigating the impact. Practical typographic attack strategies, such as [1], [13] and [2], have demonstrated how VLMs can be deceived by strategically shoving adversarial text into the target image. Especially, Qraitem et al. [2] showed that typographic attacks can transfer effectively across models in autonomous driving scenarios. In the face of such threats, defense strategies have also been explored, such as defense-prefix methods [24], though their robustness remains limited. Unlike previous works, our study focuses on generating effective typographic attacks that can be seamlessly integrated into physical world, an area that has not yet been explored. Additionally, we leverage the reasoning capabilities of LLMs to achieve this goal in an automated manner."}, {"title": "3. Problem Formulation and Challenges", "content": "Given a pre-trained large vision-language model (LVLM) V, an input image I, and a text question q, the model produces an answer a = V(I, q). The typographic attack T operates by inserting an adversarial text t into the image I at location p, generating a modified version \u00ce = T(I, t, p). The attack succeeds when this modified image misleads the LVLM into outputting the adversarial text as its answer, such that \u00e3 = t = V(\u0128, q). Prior approaches have typically employed a pre-defined text t with a fixed location p, usually placed at either the center or margins of the image. However, this rigid strategy overlooks the fact that attack effectiveness can vary significantly based on both the choice of injected text and its placement, especially when considering different source images and text queries. More importantly, the text insertion strategy can hardly be implemented in the physical world, failing to reveal the real-world risks. We systematically examine these three critical challenges in the following."}, {"title": "3.1. Challenge 1: Influence of Different Adv. Texts", "content": "In this section, we analyze how the choice of adversarial text t influences the effectiveness of typographic attacks on LVLMs. Previous research by [2] examined adversarial texts within classification tasks, comparing the effects of using random class versus target class as the adversarial text to assess their impact on model accuracy. We extend this investigation to more complex scenarios, focusing on visual question answering (VQA) tasks that requires deeper reasoning about both the question and image context. First, we categorize adversarial texts along two dimensions:\nQuestion relevance. This refers to how relevant adv. text is to the question being asked. For example, in Fig. 2 (a), when responding to the question \u201cWhat is on this left wrist?\", options like \u201cJessica\u201d and \u201cBench\u201d are irrelevant to the question, while \u201cTattoo\u201d and \u201cBracelet\" are potential answers since they could logically appear on a wrist.\nContextual relevance. This measures consistency between adversarial text and the content actually depicted in the image. For example, when examining the image, terms like \"Jessica\" and \"Tattoo\" are considered irrelevant because they refer to elements not present in the image. In contrast, terms like \"Bench\" and \"Bracelet\" are contextually relevant because they describe objects that either appear in the image or are closely related to them.\nTo investigate the impact of different adversarial texts on the success rate of typographic attacks, we conducted experiments using the LLaVA-1.5-13b [36] model on 100 randomly selected image-question pairs from the VQAv2 2014 validation dataset [37]. For each image-question pair, the initial model responses were correct, providing a baseline for assessing the impact of adding adversarial text. We placed the different types of adversarial text at the center of each image to evaluate its effectiveness. Here, we consider four types (See Fig. 2 (a)) according to the above two dimensions. For each type, we calculate its attack success rate on the image-question pairs.\nOur quantitative analysis of the results in Fig. 2 (a) reveals important insights into adversarial text attacks on LVLMs. We observe that: The effectiveness of these attacks varies significantly depending on the type of adversarial text used, demonstrating that different texts can have markedly different influences on the LVLM's responses. Our analysis indicates a strong correlation between attack success rates and two key factors: the relevance of the adversarial text to the question being asked, and its contextual relevance to the image content. Notably, adversarial text that aligns well with both the question and the image context achieves the highest success rates, while text lacking both types of relevance is least effective. These findings highlight the need for automated methods to generate adversarial text that optimally leverage both factors.\""}, {"title": "3.2. Challenge 2: Influence of Adv. Text Placement", "content": "In this section, we investigate how the placement p of the adversarial text affects visual model responses under typographic adversarial attacks. Specifically, we employ the LLAVA-1.5-13b model as the LVLM V, randomly select two images from the TypoD-base dataset [1] as the input image I, and use two-choice questions as q. For our spatial analysis, we employ a fixed adversarial text t (e.g., \"gray\") and examine its effect when placed at different positions p across the selected images. To systematically cover the image space, we establish a grid of possible insertion points, with adjacent points separated by 10-pixel intervals.\nWe denote an attacked image, as I = T(I,t,p) and quantify the attack strength by measuring the difference of LVLM's logits for incorrect and correct answers. A larger difference indicates a stronger effect of the adversarial text, increasing the likelihood that the model selects the incorrect answer. For each position p, we obtain a scalar representing the attack strength, which allows us to generate an attack strength map for all placements. We show the results in Fig. 2 (c) for the two images and observe that: Different placements lead to different attack strengths. In the first case of Fig. 2 (c), the high attack strengths are around the towel. This suggests that placing the adversarial text near the towel significantly increases the attack's effectiveness, causing the model to misidentify the towel's color. In the second case, the attack strength is around the snake's body. Placing adversarial text near question-targeted regions yields stronger attacks. We observe that the regions with higher attack strengths are related to the question and the corresponding answers. The study highlights the importance of spatial context and semantic relevance in optimizing adversarial text placement against visual language models."}, {"title": "3.3. Challenge 3: Scene-coherent Text Insertion", "content": "Traditional typographic adversarial attacks [1, 2, 13] against VLMs often involve digitally superimposed text that lacks realistic integration within the scene. This absence of scene coherence restricts the applicability of such attacks in the physical world. Introducing scene coherence, however, presents significant challenges that may limit the adversarial impact.\nTo achieve scene coherence, adversarial text must visually integrate within the scene, adhering to spatial and perceptual parameters\u2014including size, placement, lighting, and perspective. This requirement imposes constraints on text content, placement, and detectability, which may reduce the text's effectiveness in realistic contexts. Key limitations include: Constraints on adversarial text content. Ensuring the text aligns seamlessly with the scene may necessitate a reduction in text length or complexity, potentially diminishing its effectiveness as an adversarial stimulus. Restrictions on text placement. Contextually appropriate placement on surfaces like signs or walls is essential for maintaining the scene's visual integrity, which limits the freedom to place text in positions of highest adversarial potential. Necessity for realistic text attributes. To avoid being conspicuous as digitally added text, the adversarial text should exhibit real-world characteristics like natural lighting, texture, orientation, and contextual relevance, enhancing its plausibility within the scene. However, these characteristics may also limit its adversarial impact on the model.\nThese constraints reveal a trade-off between physical realism and adversarial efficacy. While enhancing scene coherence increases the plausibility of the attack, the necessary concessions may reduce its effectiveness. Balancing these constraints with the attack's effectiveness is crucial for designing typographic attacks that remain effective against VLMs in real-world applications."}, {"title": "4. Typographic Adversarial Planner", "content": "In this section, we propose to build an LLM-based planner to achieve the scene-coherent typographic attack, which can determine the adversarial text, text placement, and text appearance according to different input images and queries."}, {"title": "4.1. Overview", "content": "Given an input image I, a question q, and a correct answer a, we leverage a vision-language model U to perform the scene-coherent adversarial attack. Specifically, we provide the model with image I, query q, correct answer a and instruction \\(\\Upsilon_t\\) to generate the adversarial text t that will be embedded into the image. This process can be formulated as follows, with details in Sec. 4.3.\n\\[t = \\mathcal{U}(I, q, a, \\Upsilon_t).  (1)\\]\nNext, we utilize the model \\(\\mathcal{U}\\) to determine the suitable placement of adversarial text t. To achieve this, we extract both semantic information and corresponding spatial locations of objects within the input image through the set-of-mark (SoM) prompting [38]. Based on this extracted information and instruction \\(\\Upsilon_p\\), the model determines the suitable location for inserting the adversarial text. This process can be formulated as follows, with details provided in Sec. 4.2.\n\\[ \\mathcal{R} = \\mathcal{U}(I, q, a, \\mathcal{S}, \\Upsilon_p),  (2)\\]\nwhere \\(\\mathcal{S} = \\{R_i\\}_{i=1}^N\\) is the spatial and speakable marks of SoM on I. The ith term in \\(\\mathcal{S}\\), i.e., \\(R_i\\), indicates a region with its index as i. \\(\\mathcal{R}\\) represents the selected region to insert t.\nFinally, we aim to insert the adversarial text t to the placement \\(\\mathcal{R}\\) in the image naturally through the TextDiffuser [14] denoted as \\(\\mathcal{G}\\). The key problem is how to specify the prompts for TextDiffuser and we propose to leverage the language model \\(\\mathcal{U}\\) to achieve the goal under the guidance of the instruction \\(\\Upsilon_g\\), which can be formulated as\n\\[\\widetilde{I} = \\mathcal{G}(I, t, \\mathcal{R}, \\Tau), \\text{ subject to, } \\Tau = \\mathcal{U}(I, q, a, \\Upsilon_g), (3)\\]\nwhere \\(\\Tau\\) is the prompt fed into the TextDiffuser and generated from the model \\(\\mathcal{U}\\), we detail this part in Sec. 4.4.\nHowever, this sequential planning approach utilizing instructions \\(\\Upsilon_t\\), \\(\\Upsilon_p\\), and \\(\\Upsilon_g\\) executes actions in a step-by-step manner missing opportunities for iterative refinement and correction. Hence, we propose to revisit the reasoned plans during the inference process as detailed in Sec. 4.5. Moreover, the inherent property of generating natural and realistic typographic attacks is that we could deploy it in the real-world environment and realize the physical attack. We detail this part in Sec. 4.5."}, {"title": "4.2. Adv. Text Generation w.r.t. Scene & Question", "content": "The generation of adversarial text through Eq. (1) requires careful design of the instruction \\(\\Upsilon_t\\) to achieve two fundamental objectives: 1 we need to identify and analyze the key objects within the input image that are relevant to both the query and its correct answer. we must select an alternative answer that, while different from the correct one, maintains plausibility when serving as adversarial text.\nTo accomplish these objectives, we structure the instruction \\(\\Upsilon_t\\) in three parts. The first component, \\(\\Upsilon_{t-1}\\), focuses on identifying and extracting the key visual elements relevant to the query. Building upon this foundation, the second component, \\(\\Upsilon_{t-2}\\), implements a series of carefully crafted instructions to select an appropriate incorrect answer based on the question type. This selection process takes into careful consideration the correct answer, the visual cues present in the input image, and the necessary criteria for maintaining plausibility. The generated adversarial text typically may contain excessive words that are impractical to insert directly, necessitating condensation into a more concise form. Thus, we design the \\(\\Upsilon_{t-3}\\) to refine the adversarial text.\nWe show an example in Fig. 3 including the outputs (i.e., \"Adversarial Text Generation\") with the instruction \\(\\Upsilon_t\\). The image analysis results indicate the main objects (e.g., freezer,"}, {"title": "4.3. Adv. Text Placement Determination", "content": "After generating the adversarial text, we need to determine a suitable location in the image, which maintains both visual coherence and adversarial effectiveness.\nTo optimize text integration, we employ instruction \\(\\Upsilon_{p-1}\\) to determine its most effective placement relative to the question and image context, which generates a text description about the suitable location like p=\"On the Magnum ice cream advertisement\". Concurrently, we utilize set-of-mark (SOM) prompting to index and segment various objects in the image, yielding a set of regions \\(\\mathcal{S} = \\{R_i\\}_{i=1}^N\\). We then identify the specific marked region \\(\\mathcal{R} \\in \\{R_i\\}_{i=1}^N\\) that encompasses location description p, establishing this as the suitable text insertion point.\nWe also show an example in Fig. 3 in the box \"Adversarial Text Placement Determination\". The SceneTAP outputs the text placement: \"On the Magnum ice cream advertisement\" and the text position number in the SoM map."}, {"title": "4.4. Scene-Coherent Adv. Text Insertion", "content": "With the adversarial text t and text placement \\(\\mathcal{R}\\), we first leverage the language model \\(\\mathcal{U}\\) to generate the prompt for the TextDiffuser through the instruction \\(\\Upsilon_g\\) and get the prompt \\(\\Tau\\) that involves the adversarial text. Then, we feed the prompt \\(\\Tau\\), and text placement \\(\\mathcal{R}\\) with the input image I into the TextDiffuser and get the output image \\(\\widetilde{I}\\). As shown in Fig. 3, the prompt \"The word 'veggies' is written on the Magnum ice cream advertisement.\" is fed to the TextDiffuser to generate the adversarial example where the adversarial text \"veggies\" is naturally printed on the specified region."}, {"title": "4.5. Revisable Inference and Implementation", "content": "Once we set and fix the instructions for the above three parts, we can use them for inference with the user prompt structured as shown in Fig. 3. The prompt includes the question, correct answer, guidelines, scene image, and segmentation map. The LLM then outputs the adversarial text, placement specifications, and the final output image. To enable our planner to correct the generation results, we incorporate a revisable prompt during the inference stage.\nExtension to physical attack. After completing the planning and generation phases, we get the digital adversarial example. The scene-coherent property allows us to print it out. Then, we can paste it into the physical scene as determined during planning. This transfers the attack into the real world, integrating the text into the environment.\nImplementation details. We employ ChatGPT (gpt-4o-2024-08-06) as the planner, i.e., \\(\\mathcal{U}\\) in Eq. (1). We conducted"}, {"title": "5. Experimental Results", "content": "5.1. Setups\nMetrics. We propose three metrics to evaluate the efficacy and quality of our typographic adversarial attacks: Attack Success Rate (ASR). Attack Success Rate (ASR) measures the percentage of successful attacks that deceive the target AI model, indicating the attack's effectiveness. It ranges from 0 to 100, with higher values signifying greater success. Naturalness Score (N-Score). The N-Score is a 10-point metric evaluated by ChatGPT to assess the natural integration of adversarial text within an image. The assessment criteria include consistency in lighting, surface realism, environmental coherence, and other factors. A score of 0-2 reflects a noticeably artificial appearance, while a score of 10 indicates flawless integration into the scene.\nComprehensive Score (C-Score). The C-Score averages the ASR and N-Score to evaluate overall performance on a 100-point scale, balancing attack effectiveness with visual naturalness.\nDatasets. We evaluate our methods using three datasets: TypoD-base [1], LingoQA [39], and VQAv2 [37]. TypoD-base assesses typographic attacks on LVLMs using two-choice questions across four tasks: object recognition, visual attribute detection, enumeration, and commonsense reasoning. LingoQA evaluates VQA questions in the context of autonomous driving. To further examine typographic attacks on LVLMs in general questions, we use 500 image-question pairs from the VQAv2 2014 validation dataset for evaluation.\nBaselines. We compare SceneTAP with two baselines: Center Attack [1] and Margin Attack [2]. Center Attack places adversarial text at the center of the image, while Margin Attack positions it at the margin. For two-choice questions, we use the incorrect option as adversarial text following [1]. For VQA, we prompt ChatGPT to generate an incorrect answer using the image, question, and correct answer."}, {"title": "5.2. Comparing with SOTA Methods", "content": "As shown in Tab. 1, we analyze the performance of SceneTAP in comparison with baseline methods across various datasets and models. We evaluate our method using three open-source models (LLaVA-1.5 [36], InstructBLIP [40], MiniGPT-v2 [41]) and ChatGPT-4o.\nPerformance across different question types. For two-choice questions, Center and Margin Attacks moderately increase the average ASR from 12.36% (no attack) to 29.12% and 26.45%, while SceneTAP achieves a significantly higher average ASR of 44.32%, marking a 31.96% improvement over baseline. For open-ended VQA, Center and Margin Attacks have minimal impact, raising the average ASR from 47.19% to 47.93% and 47.39%. In contrast, SceneTAP increases the average ASR to 62.10%, achieving a 14.91% improvement, thereby demonstrating its effectiveness in misleading more complex VQA tasks.\nPerformance across different models. \u25cf Open-source models exhibit susceptibility to typographic attacks, as evidenced by an increase in average ASR from 26.04% (no attack) to 39.6% and 38.08% under Center and Margin Attacks. SceneTAP further raises the average ASR to 56.58%, marking a 30.54% increase over the baseline. ChatGPT-40 demonstrates robust resilience with a baseline average ASR of 15.83%. Center and Margin Attacks slightly elevate this to 23.44% and 19.05% respectively, whereas SceneTAP achieves 33.79%, marking an increase of 17.97%, underscoring its effectiveness against resilient commercial models.\nOverall analysis. \u25cf SceneTAP consistently achieves the highest average ASRs across most tasks and models, outperforming SOTA methods. SceneTAP consistently outperforms baseline methods in N-Scores, demonstrating superior integration of adversarial text within scenes and enhanced coherence with environmental factors, thereby increasing the realism and applicability of the attack in physical contexts. SceneTAP achieves the highest C-Score across all methods and tasks, demonstrating its effectiveness in balancing attack success and scene coherence."}, {"title": "5.3. Application to Physical World", "content": "This section extends the SceneTAP to real-world applications, demonstrating its effectiveness in attacking LVLMs in physical settings. We present four attack cases to illustrate how adversarial text influences model responses across various contexts. As illustrated in Fig. 4, the framework can be deployed by printing and strategically placing SceneTAP-designed adversarial text within a SceneTAP-planned area of a physical environment. This scene-coherent planning enables SceneTAP to mislead various LVLMs across diverse tasks, transitioning seamlessly from digital to physical contexts. These cases demonstrate SceneTAP's ability to execute effective typographic attacks in real-world settings."}, {"title": "5.4. Ablation Study", "content": "As show in Fig. 5 and Tab. 2, we conducted an ablation study to evaluate the impact of each component in SceneTAP on the ASR against LLava across two datasets: the visual attribute detection subset of TypoD-base for two-choice questions, and VQAv2 for open-ended questions.\nAdv. text design. Comparing Settings 1 and 2, SceneTAP's strategic adversarial text significantly improved ASR in open-ended VQA tasks. While two-choice questions showed stable ASR due to a fixed incorrect option, open-ended questions benefited from the planned adversarial text, highlighting SceneTAP's advantage in complex scenarios.\nAdv. text placement. Settings 2 to 4 indicate that placing adv. text in contextually relevant regions effectively raises ASR compared to central placement. Although refining placement for naturalness slightly reduces ASR, it remains higher than without SceneTAP, showcasing the method's ability to balance attack effectiveness with visual plausibility.\nScene-Coherent Adv. Text Insertion In Settings 4 and 5, integrating text using diffusion techniques further enhances ASR. This demonstrates how prior SceneTAP refinement in adversarial text placement balances naturalness, enabling scene-coherent insertion of adversarial text into the image.\nIn summary, each component of SceneTAP significantly boosts attack efficacy and preserves visual naturalness, demonstrating clear advantages over baseline methods."}, {"title": "6. Conclusion", "content": "In this paper, we proposed SceneTAP, an LLM-guided framework for creating naturalistic typographic adversarial attacks against large vision-language models. Our approach uniquely leverages LLMs to generate context-aware adversarial text and determine optimal placements, while using scene-coherent TextDiffuser for seamless visual integration. Through comprehensive empirical studies and physical validations, we demonstrated that SceneTAP successfully creates both effective and visually natural adversarial examples, advancing our understanding of LVLM vulnerabilities and providing insights for developing more robust LVLMs."}, {"title": "A. Appendix", "content": "A.1. Planner Details\nIn this section, we present a comprehensive version of \\(\\Upsilon_t\\), which is not thoroughly detailed in the paper.\nA.2. Naturalness Evaluation\nCurrently, there is no established method for evaluating the naturalness of text added to images. To address this gap, we propose the N-Score, which uses ChatGPT-4o to assess the integration of text into the scene. This score is based on ten specific evaluation criteria, each worth one point, for a maximum total of ten points. For each image, the evaluator determines whether the embedded text meets each criterion, awarding one point for every satisfied condition. The detailed criteria for each indicator are outlined below.\nFig. 6 presents the visualization results of images categorized according to different N-Score ranges, illustrating the relationship between N-Scores and the naturalness of text integration within images: 1 Images with low N-Scores (0-2) exhibit highly unnatural text integration, characterized by inappropriate placement, poor perspective alignment, and lighting mismatches, which make the text appear incongruent with the scene. Images with high N-Scores (9-10) demonstrate seamless text integration, where the text blends naturally into the scene with perfect alignment, consistent lighting, and appropriate surface interaction. The progression from low to high N-Scores reveals a clear and expected improvement in naturalness, with images increasingly adhering to the evaluation criteria as the scores rise.\nThese findings substantiate the N-Score as an effective and reliable metric for assessing the naturalness of text integration into images.\nA.3. Visualization\nIn this section, we provide additional visualization results in Fig. 7, Fig. 8 and Fig. 9 to demonstrate the effectiveness and naturalness of the typographic attacks generated by SceneTAP on the TypoD-base, LingoQA, and VQAv2 datasets. Each figure displays the original images alongside their corresponding versions altered by SceneTAP attacks, showcasing how SceneTAP inserts misleading text into the scenes, causing VLMs to produce incorrect predictions.\nEffectiveness Across Question Types: SceneTAP effectively misleads VLMs on both binary-choice and open-ended questions. For instance, in TypoD-base, adding the text \"colobus\" causes the VLM to incorrectly identify the entity in the image. In LingoQA, inserting the phrase \"Red light\" within an image leads to an incorrect operational decision. These examples highlight SceneTAP's effectiveness in misleading VLMs across various types of questions.\nEffectiveness in Diverse Scenarios: The adaptability of SceneTAP extends to diverse scenarios, ranging from everyday objects to specialized settings such as autonomous driving. In VQAv2, adding deceptive text like \"gas\" to a wall induces erroneous scene interpretations. In autonomous driving contexts, textual attacks such as \"Red light\" can mislead VLMs into misidentifying a green traffic light as red. These findings highlight SceneTAP's versatility in generating adversarial contexts across various image domains.\nNaturalness of SceneTAP: SceneTAP's attacks integrate seamlessly into the visual context, maintaining a high degree of naturalness. For example, modifications such as adding text to an egg carton or altering a parking sign appear plausible and contextually appropriate, making them unobtrusive within the image. This highlights SceneTAP's ability to deceive models effectively while integrating text into the environment without compromising coherence.\nThese examples highlight SceneTAP's consistent ability to mislead VLMs by naturally embedding text into images.\nA.4. Limitations and Future Work\nThe current approach focuses on planning a scene-coherent typographic attack by placing text on existing objects within an image. However, this method may be less effective for images that lack suitable text-friendly surfaces, such as natural scenery, which affects the naturalness of the added text.\nFuture work could explore the incorporation of objects suitable for text placement into the image during the planning phase, prior to adding the text. This approach would enhance the method's applicability and help preserve scene coherence across a broader range of image types."}]}