{"title": "Text2BIM: Generating Building Models Using a Large Language Model-based Multi-Agent Framework", "authors": ["Changyu Du", "Sebastian Esser", "Stavros Nousias", "Andr\u00e9 Borrmann"], "abstract": "The conventional BIM authoring process typically requires designers to master complex and tedious modeling commands in order to materialize their design intentions within BIM authoring tools. This additional cognitive burden complicates the design process and hinders the adoption of BIM and model-based design in the AEC (Architecture, Engineering, and Construction) industry. To facilitate the expression of design intentions more intuitively, we propose Text2BIM, an LLM-based multi-agent framework that can generate 3D building models from natural language instructions. This framework orchestrates multiple LLM agents to collaborate and reason, transforming textual user input into imperative code that invokes the BIM authoring tool's APIs, thereby generating editable BIM models with internal layouts, external envelopes, and semantic information directly in the software. Furthermore, a rule-based model checker is introduced into the agentic workflow, utilizing predefined domain knowledge to guide the LLM agents in resolving issues within the generated models and iteratively improving model quality. Extensive experiments were conducted to compare and analyze the performance of three different LLMs under the proposed framework. The evaluation results demonstrate that our approach can effectively generate high-quality, structurally rational building models that are aligned with the abstract concepts specified by user input. Finally, an interactive software prototype was developed to integrate the framework into the BIM authoring software Vectorworks, showcasing the potential of modeling by chatting.", "sections": [{"title": "1 INTRODUCTION", "content": "Throughout the last decades, various digital representations and workflows have continuously emerged to represent the built environment. The notion of Building Information Modeling (BIM) comprises a holistic approach to reflect built assets with geometric and semantic information, which can be utilized across the entire life-cycle of a building and shared across different project stakeholders in dedicated representations. Modern BIM authoring software encompasses design requirements across multiple disciplines. This integrated approach has led to a proliferation of functions and tools within the software, making the user interface increasingly complex. Designers often face a steep learning curve and require extensive training to translate design intentions into complex command flows to create building models in the software.\nIn recent years, the application of generative Artificial Intelligence (AI) in architectural design has alleviated this additional cognitive load, enhancing the creative potential and efficiency of the design process. Current research and industrial applications primarily focus on generating 2D images or simple 3D volumes, utilizing Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and diffusion models to create 2D architectural floor plans, building renderings, architectural facade designs, or preliminary 3D conceptual forms. Recent research proposed using large language models (LLMs) to automatically generate wall details, but more complex 3D building model generation has not yet been tapped.\nIn other sectors, such as game development and virtual reality, advanced 3D generative models like DreamFusion and Magic3D can generate complex 3D models with rich textures directly from text descriptions, allowing designers to express design intent in natural language without tedious modeling commands. However, the outputs from these Text-to-3D methods are typically based on voxels, point clouds, meshes, or implicit representations like Neural Radiance Fields (NeRFs), which only contain geometric data of the outer surfaces and cannot model possible internal contents of the 3D objects, nor do they include any semantic information.\nThe differences between these purely geometric 3D shapes and native BIM models make it challenging to integrate them into BIM-based architectural design workflows. Designers cannot directly modify and edit the generated contents in BIM authoring software, and due to the lack of semantic information, these models are also difficult to apply in downstream building simulation, analysis, and maintenance tasks."}, {"title": "1.1 Novelty and contributions", "content": "To bridge these gaps, we propose Text2BIM, which converts natural language descriptions to 3D building models with external envelopes, internal layouts, and semantic information. By representing building models as imperative code scripts invoking the BIM authoring software's Application Program Interfaces (APIs), we enable multiple Large Language Model (LLM) agents to collaborate and autonomously generate executable code that ultimately produces"}, {"title": "2 RELATED WORK", "content": null}, {"title": "2.1 Generative AI in 3D building design", "content": "The application of generative AI in the field of 3D building design is gradually becoming a research hotspot. The key lies in constructing appropriate data representations for existing design data, experiential knowledge, and physical principles, and then training corresponding algorithms to intelligently generate new designs. de Miguel Rodr\u00edguez et al. (2020) use connectivity vectors to represent different 3D mesh-like building geometries. By training a Variational Autoencoder (VAE) using these data, more building shapes can be generated by reconstructing interpolated positions within the learned distribution. Vitruvio (Tono and Fischer 2022) uses the occupancy field to describe the building shape by assigning binary values to each point in the 3D space, indicating whether the point is occupied by an object. They employ a modified occupancy network (Mescheder et al. 2019) to learn this representation, enabling the reconstruction of a 3D printable building mesh from a single perspective sketch.\nOther researchers are employing GANs to generate conceptual 3D buildings for the early design stage. Pouliou et al. (2023) proposed using CPCGAN (Yang et al. 2021) to generate point cloud representations of building geometries based on specific site rules. Ennemoser and Mayrhofer-Hufnagl (2023) decoded 3D voxels into 2D images to train a DCGAN (Radford et al. 2015), and then used signed distance fields (SDF) (Oleynikova et al. 2016) to convert the generated images back into voxels. Although their method produces voxel models that can partially reconstruct the interior spaces of buildings, the generated results still suffer from issues such as geometric inconsistency and inaccuracy, making them far from practical architectural models.\nOverall, current research utilizing 3D generative algorithms based on pure geometric representations is still limited to generating conceptual architectural forms and has not yet been able to produce complex 3D building models with a high level of development (LOD) that exhibit both coherent exterior and interior geometry. Additionally, the results generated by these purely data-driven methods are difficult to constrain using text-based architectural rules. Better data representation methods need to be explored.\nThe rapidly advancing generative AI technologies, such as diffusion models and large language models (LLMs), have shown significant potential in the field of architectural design. The application of diffusion models is still primarily focused on tasks based on 2D images, such as generating architectural renderings from text (Li et al. 2024b) and replacing GANs to produce more robust structural designs (He et al. 2023). On the other hand, the application of LLMs is mainly centered on using natural language to retrieve data from BIM models (Zheng and Fischer 2023) and enhancing human-machine interaction in BIM authoring software (Du et al. 2024b; Fernandes et al. 2024). However, the concept of using LLMs to generate 3D building models has not yet been explored. One of the key challenges is representing 3D models as one-dimensional text data that LLMs can use (Liao et al. 2024). This textual representation must concisely capture the features of the model, avoiding the verbosity caused by the overly detailed granularity typical of conventional model serialization files. A recent study (Jang et al. 2024) proposed converting BIM models into XML format and then using LLM to process this structured text to add wall details. Finally, the modified XML is converted back into the BIM model to achieve automatic wall detailing. Unlike their approach, we propose representing the BIM model as imperative code. By constructing and invoking high-level modeling APIs in the BIM authoring tool, we aim to express the geometric and semantic features of the model using the minimal and most flexible text format possible while also maximizing the benefits from the powerful code generation capabilities of LLMs."}, {"title": "2.2 LLM-based agents", "content": "A Large Language Model-based agent refers to an autonomous system that utilizes an advanced language model to perform tasks involving perception, decision-making, and action (Wang et al. 2024). These systems typically equip the LLM with tools to interact with the external environment, as well as memory modules to retain the thought processes, observations, and action records. The LLM-based agent uses the LLM as the \"brain\", leveraging its powerful in-context learning (Dong et al. 2024) capabilities to synthesize information from various sources and deploy appropriate tools for different scenarios through linguistic reasoning, thereby enabling the system to behave, plan, and execute tasks like humans (Du et al. 2024b). The agents can operate individually or in multi-agent systems where they collaborate, communicate, and specialize in distinct roles to solve more dynamic and complex problems using collective intelligence (Guo et al. 2024).\nLLM-based multi-agent systems have growing applications in software development (Hong et al. 2023), gaming (Xu"}, {"title": "2.3 BIM-based model checking", "content": "With the continuous adoption of BIM concepts in the design phase of built assets, the rich information base provides a sophisticated foundation for several downstream applications. The Industry Foundation Classes (IFC) (ISO 2024) data model is well established to exchange digital representations of built assets comprising geometric and semantic information. These representations are perfectly tailored to automatically perform checks regarding the compliance of the envisioned design against various rules and guidelines. Such approaches have gained increasing interest from different stakeholders in the industry throughout the last few years. A comprehensive overview of opportunities and related challenges has been described by Preidel and Borrmann (2018). Eastman et al. (2009) have introduced an overall approach towards automated code compliance checking based on BIM models. They divide the overall checking process into four stages: rule interpretation to create machine-readable rules, building model preparation with advanced analysis, rule checking execution, and reporting of detailed defects and issues.\nThe rules a model should be compliant with can vary in their complexity. To account for this challenge, Solihin and Eastman (2015) have introduced a classification system for rules, which comprises four different levels. Rules assigned to class 1 require a single or a small number of explicit data to be available. A typical example of such a rule is the inspection of a dedicated property assigned for each element in the model. Class 2 rules are characterized by the derivation of simple attribute values. Such calculations can comprise simple arithmetic or trigonometric calculations"}, {"title": "2.4 Summary and identified research gaps", "content": "Numerous studies have explored the use of generative AI to create geometric representations of conceptual buildings. However, these advancements have not been integrated into the field of BIM-driven building design. Based on the conducted literature review, our approach appears to be the first that utilizes collaborative LLM agents to generate BIM models with relatively high LOD based on natural language instructions, ensuring compliance and consistency by employing rule-driven model checking."}, {"title": "3 METHODOLOGY", "content": "We propose Text2BIM, an LLM-based multi-agent framework, where four LLM agents assume different roles and collaborate to convert natural language instruction into imperative code, thereby generating building models in BIM authoring software. The core idea is to encapsulate the underlying modeling APIs of software using a series of custom high-level tool functions. By using prompt engineering techniques to guide LLMs in calling these functions within the generated code, we can construct native BIM models through a concise and efficient textual representation.\nThe overall framework with a sample user input is shown in Fig. 1. To realize the core concept outlined above, we make use of four LLM-based agents with dedicated tasks and skills that interact with each other via text:\n\u2022\tPRODUCT OWNER: Refines and enhances user instructions and generates detailed requirement documents.\n\u2022\tARCHITECT: Develops textual building plans based on architectural knowledge.\n\u2022\tPROGRAMMER: Analyzes the requirements and writes code for modeling.\n\u2022\tREVIEWER: Provides code optimization suggestions to address issues identified in the model.\nDue to the typically brief and open-ended nature of user inputs, we first designed an LLM agent acting as a PRODUCT OWNER to expand and refine user instruction. This ensures the instruction contains sufficient information to guide the downstream PROGRAMMER agent to invoke suitable tool functions in its code. The PRODUCT OWNER agent's elaboration and detailing of the original instructions reference multiple sources. Firstly, it reads information from the"}, {"title": "3.2 Prompt enhancement and building plan generation", "content": "The original user prompt a needs to be expanded and enhanced into more specific representations aligned with architectural design. Therefore, the tasks of the PRODUCT OWNER and ARCHITECT at this stage are crucial, as they determine the basic layout and quality baseline of the final generated model. The prompt template $P_{Arch}$ designed for the ARCHITECT is shown in Fig. 3. In this template, we first define the role and tasks of the agent and strictly specify the output format and content to meet downstream programming requirements. Then, we incorporate 9 basic architectural rules and principles into the prompt (e.g., components configuration, interior partition, building opening layout, etc.), guiding the LLM to produce architectural-wise reasonable and structurally integrated building plans.\nAdditionally, we employ the few-shots learning approach (Brown et al. 2020) to provide the LLM with an example conversation to better lead it in producing robust outputs. The user input a' in the ARCHITECT's prompt template is a paraphrase of the original user instruction a provided by the PRODUCT Owner. This is because we use the function-calling mechanism to connect two agents: the ARCHITECT is wrapped in a function layer F, with the user"}, {"title": "3.3 Coding for BIM model generation", "content": "The enhanced user requirements $a_{enhanced}$ will be input into the PROGRAMMER'S prompt template $P_{Co}$, as illustrated in Fig. 5. The PROGRAMMER agent is required to write concise Python code utilizing the functions solely from the toolset and the built-in standard Python libraries to accomplish the tasks specified by the PRODUCT OWNER. The advantage of generating Python code is that LLM can flexibly combine and call different tool functions using various algorithmic logic. This is more powerful than JSON-based function-calling in a recent work (Fernandes et al. 2024), which is constrained to executing single functions sequentially and cannot meet the complex logical demands of tasks such as building modeling. Given that the enhanced requirements contain rich contextual information, we choose to leverage the zero-shot learning (Wei et al. 2022) ability of the agent without the provision of exemplars. This approach is intended to allow the LLM to flexibly explore different code logic tailored to various task requirements, rather than being constrained by rigid examples, thereby maximizing the utilization of its pre-trained knowledge. Eq. 3 illustrates the process of code generation, where we denote tool information as T and the historical chat records with the PRODUCT OWNER as $\\Phi_{global}$:\n$$code \\leftarrow LLM_{Coder}(P_{Co}(@_{enhanced}, T, \\Phi_{global}))$$\n(3)\nA custom Python interpreter will execute the generated code within a controlled environment. We use an abstract syntax tree (AST) to represent the code, traversing the tree nodes to evaluate each Python expression. This approach allows us to customize the usable syntax and callable functions while enabling more precise error handling. The interpreter uses a state dictionary to store and track the results of code execution, including imported packages, defined function objects, and variable names and their values. This lays the technical foundation for the PROGRAMMER agent's memory capability the LLM can utilize and access variables and functions defined in previous dialogues or directly continue previous code. The interpreter can still execute code correctly by retrieving the state dictionary, ensuring comprehensive context is maintained at the code level throughout the entire session. We extended previous work (Du et al. 2024b) to allow the interpreter to support more data types and advanced syntax while restricting potentially problematic syntax like the while statement. Additionally, the interpreter can only evaluate functions from the toolset and Python built-in library (except for custom-defined functions within the generated code), preventing the invocation of arbitrary third-party packages.\nIf an exception E is thrown by the interpreter during code evaluation, resolving this error will be treated as a new task input for the PROGRAMMER. The code that caused the exception will be used as the new history information. Both the error and the code will be re-input into the prompt template $P_{Co}$ for the PROGRAMMER to regenerate new code. As shown in Eq. 3, this self-reflective modification loop will iterate up to n times until the newly generated $code^{n+1}$ is free of exceptions $E^{n+1}$. If the issue is not resolved within 3 attempts, we will interrupt the process to seek human feedback.\n$$code^{n+1} \\leftarrow LLM_{Coder}(P_{Co}(E^n, T, code^n))  \\text{where } n \\in [0,3)$$\n(4)\nIt should be noted that within this self-reflection loop, no updates will be made to any memory modules; only the"}, {"title": "3.4 Model quality assessment and iterative improvements", "content": "The successfully executed code will generate a building model within the BIM authoring software. Although we used extensive prompt engineering in earlier stages to guide the LLMs in producing spatially and geometrically reasonable results, the inherently stochastic nature of the process may still lead to flaws in the generated building. Therefore, deterministic domain-specific rules are used to verify and refine the generated BIM model. We employ a rule-based model checker to evaluate the model quality. According to (Solihin and Eastman 2015), we defined a series of rules covering classes 1 to 3, primarily checking for geometric conflicts between components (e.g., whether doors and windows overlap), correct semantic attribute definitions (e.g., whether each component has a unique GUID), and compliance of spatial layouts with architectural common sense (e.g., whether the roof is supported by walls and not floating). Detailed documentation of all rules can be found in the Appendix II. The issues identified in the model will be exported to BCF files. A script is used to automatically extract useful information from the BCF, i.e., the name and description of the issues and corresponding rules, as well as the GUIDs of the associated components. We denote this information as I and input it along with the $code$ that generated the checked model and the toolset information T into the REVIEWER agent's prompt template $P_{Rev}$ , asking the REVIEWER to provide suggestions \u03b2 on solving the"}, {"title": "4 PROTOTYPE IMPLEMENTATION", "content": "We developed an interactive software prototype using the architecture shown in Fig. 7, integrating the proposed framework into the BIM authoring software Vectorworks. Our implementation is based on Vectorworks' open-source web palette plugin template and significantly extends previous work to support multi-agent workflows. The frontend of the web palette is implemented with Vue.js and runs in a web environment built on Chromium Embedded Framework (CEF), allowing us to embed dynamic web interfaces in Vectorworks using modern frontend technologies. The backend of the web palette is a C++ application, enabling the definition and exposure of asynchronous JavaScript functions within a web frame, while the actual logic is implemented using C++ functions.\nSince our multi-agent framework is entirely based on Python, we invoke Vectorworks' built-in Python engine within the corresponding C++ functions to execute our code, thereby delegating the JavaScript implementation. Our framework supports various mainstream LLMs, such as GPT-40, Gemini-1.5-pro, and the state-of-the-art open-source model Mistral-Large-2. Within the framework, we maintain a state dictionary and a local memory module to store the state between Python calls and the interaction records of the agents in the quality optimization loop, respectively. We use Vectorworks' API to export the generated models as IFC files, and then use the Solibri Autorun commands to automatically launch the Solibri Model Checker, perform checks,"}, {"title": "5 EXPERIMENTS AND EVALUATION", "content": "This section is devoted to the experimental evaluation of the presented methodology. The evaluation is performed via the employment of test user prompts (instructions) to the proposed framework and comparing the generated outcome of various LLMs including GPT-40, Mistral-Large-2 and Gemini-1.5-Pro. Ten user prompts were conceptualized to comprehensively test the generative capabilities and qualities of the proposed framework from various perspectives, as illustrated in Table 1. Table 2 summarizes the different architectural scenarios/requirements covered by each test prompt, including aspects such as shape, dimensions, spatial features, room layouts, construction materials, etc., focusing on the functional and aesthetic elements essential to their respective purposes (whether residential or commercial). Additionally, by deliberately leaving some building requirements unspecified in the test prompts, we aim to experiment with the framework's ability to generate designs in open-ended settings. Given the inherent stochasticity of generative models, each test prompt was input into each LLM five times, resulting in a total of 391 IFC models (including intermediate results from the optimization process). The experiments are based on this dataset and aim to report statistically significant results.\nTable 3 presents the pass rates of final models generated by different test prompts during model checking. These rates are calculated based on a total of 30 domain-specific checking rules implemented in Solibri and reflect the quality of the generated model. \"n\" indicates the number of times this prompt was input. The \"Mean pass rate\" records the"}, {"title": "6 DISCUSSION AND LIMITATIONS", "content": "The implemented framework is currently capable of generating regular, non-curved building models in the early design stage. To generalize this approach to irregularly shaped buildings or more detailed engineering models, the development of more complex tools for the agents is required to significantly expand the existing limited toolset. However, a challenge arises in organizing and managing the vast amount of tool information and their interdependencies, so that the LLM can efficiently retrieve useful functions. Knowledge graphs and graph-based Retrieval Augmented Generation (RAG) techniques might offer potential solutions.\nAdditionally, in the current framework, the ARCHITECT agent generates structured text-based building plans that include numerical information such as coordinates and dimensions. This content format is designed to be aligned with the input requirements of the tool functions, allowing downstream agents to better understand and utilize specific architectural parameters in the code. It is observed in the experiments that this approach is more robust and accurate than using formats like SVG (XML) or images for representing floor plans. Moreover, ARCHITECT currently designs the building's interior layout based solely on its pre-trained knowledge and the examples and information provided in prompt templates. Although the generated interior partitions appear visually reasonable to some extent (as shown in Fig. 17), they lack comprehensive consideration of complex architectural conditions (e.g., lighting, functionality, accessibility, etc.) and regulations (e.g., fire safety, area requirements, etc.). Future work could explore how to effectively integrate this complex architectural knowledge into LLMs.\nOur experiment demonstrates that LLM agents can automatically resolve clashes within the model to a limited extent through the designed quality optimization loop. Although this module is not the main focus of this study, our preliminary exploration in this direction presents a new technical approach for research in related fields. This is particularly significant considering that current research on automatic clash resolution mainly focuses on using optimization algorithms, classical machine learning, or reinforcement learning. Despite these advancements, the conflict resolution method based on LLM agents still has significant limitations. Fig. 13 summarizes some representative scenarios encountered during the quality optimization loop. The first common failure (a) involves the agent attempting to rewrite code to create a new model, leading to an increase in issue amount due to conflicts between the new and existing model components. In scenario (b), the upper two floors of the initial model have overlapping and nested walls, doors, and windows. In such highly complex situations, LLM agents, which rely solely on code and checker feedback (rule/issue descriptions) for contextual information, cannot resolve all the issues and are prone to hallucinations. The strategy the agent adopts here involves deleting parts of the walls on the relevant floors. While this action can reduce the overall issue amount in the model, it compromises the structural integrity of the building. However, current agents can only perceive information from 1D text and are not yet capable of understanding 3D spaces in this manner. Scenario (c) illustrates a successful case where the agent correctly adjusts the height of a floating roof to align with the top floor's wall elevation. Overall, LLMs perform well for intuitive issues with deterministic solutions (typically Class 1 rules, such as \"no space defined in model -> create space\"). However, they often fail on complex issues that require higher-level spatial understanding and have open-ended solutions (usually Class 3 rules, such as \"two partition walls intersect -> which wall is to be moved, and in which direction?\"). Although our framework allows users to guide the LLM to perform the appropriate issue-solving actions via dialogue or manually continue editing the BIM model generated in the software, future research will prioritize enhancing the LLM's spatial understanding capabilities to advance toward an autonomous conflict resolution system.\nGiven that our approach generates code representations of 3D models based on prompt engineering techniques, it does not require fine-tuning of LLMs. This is fundamentally different from conventional Text-to-3D methods, which typically require constructing a 3D dataset for training. Commonly used metrics such as Chamfer Distance (CD) and Intersection over Union (IoU) mainly focus on evaluating the geometric accuracy of point cloud/voxel models. As these metrics are not applicable to our data representation approach, we propose using the pass rate of domain-specific rule checks as a quantitative metric to evaluate the generated BIM models. While this method can verify whether the generated models are structurally complete and reasonable in architectural terms, its limitation lies in the fact that the rules provided by model checkers cannot assess whether the generated buildings align with the abstract and dynamic user intentions expressed in natural language instructions (e.g., \"H-shaped house\", \"arrange rooms along the building perimeter\", etc.). Currently, we still rely on manual review to determine whether the models align with the intended instructions. Future research could leverage the data generated by this work to develop new benchmark datasets and metrics, enabling the automated, data-driven evaluation of user intent."}, {"title": "7 CONCLUSIONS", "content": "We introduce Text2BIM, an LLM-based multi-agent collaborative framework that generates building models in BIM authoring software from natural language descriptions. The main findings and contributions of this study are as follows:\n\u2022\tUnlike previous studies that focused on generating the 3D geometric representation of buildings, our framework is capable of producing native BIM models with internal layouts, external envelopes, and semantic information.\n\u2022\tWe propose representing 3D building models using imperative code that interacts with BIM authoring software APIs. By employing prompt engineering techniques, multiple LLM agents collaborate to develop the code without the need for fine-tuning, thereby conserving computational resources.\n\u2022\tInnovatively, a domain-specific rule-based model checker is integrated into the framework to guide LLMs in generating architecturally and structurally rational outcomes. The proposed quality optimization loop demonstrates that the LLM agents can iteratively resolve conflicts within the BIM model based on textual feedback from the checker."}, {"title": "8 DATA AVAILABILITY STATEMENT", "content": "Some data and models that support the findings of this study are available from the corresponding author upon reasonable request."}, {"title": "9 ACKNOWLEDGMENTS", "content": "This work is funded by Nemetschek Group, which is gratefully acknowledged. We sincerely appreciate the data and licensing support provided by Vectorworks, Inc."}, {"title": "10 SUPPLEMENTAL MATERIALS", "content": "1. Demo video"}, {"title": "APPENDIX I. TOOLSET DOCUMENTATION", "content": null}, {"title": "APPENDIX II. RULESET DOCUMENTATION", "content": null}, {"title": "APPENDIX III. VISUALIZATION OF GENERATED MODELS", "content": null}]}