{"title": "MASKED GENERATIVE PRIORS IMPROVE WORLD MODELS SEQUENCE MODELLING CAPABILITIES", "authors": ["Cristian Meo", "Mircea Lica", "Zarif Ikram", "Akihiro Nakano", "Vedant Shah", "Aniket Rajiv Didolkar", "Dianbo Liu", "Anirudh Goyal", "Justin Dauwels"], "abstract": "Deep Reinforcement Learning (RL) has become the leading approach for creating artificial agents in complex environments. Model-based approaches, which are RL methods with world models that predict environment dynamics, are among the most promising directions for improving data efficiency, forming a critical step toward bridging the gap between research and real-world deployment. In particular, world models enhance sample efficiency by learning in imagination, which involves training a generative sequence model of the environment in a self-supervised manner. Recently, Masked Generative Modelling has emerged as a more efficient and superior inductive bias for modelling and generating token sequences. Building on the Efficient Stochastic Transformer-based World Models (STORM) architecture, we replace the traditional MLP prior with a Masked Generative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our model on two downstream tasks: reinforcement learning and video prediction. GIT-STORM demonstrates substantial performance gains in RL tasks on the Atari 100k benchmark. Moreover, we apply Transformer-based World Models to continuous action environments for the first time, addressing a significant gap in prior research. To achieve this, we employ a state mixer function that integrates latent state representations with actions, enabling our model to handle continuous control tasks. We validate this approach through qualitative and quantitative analyses on the DeepMind Control Suite, showcasing the effectiveness of Transformer-based World Models in this new domain. Our results highlight the versatility and efficacy of the MaskGIT dynamics prior, paving the way for more accurate world models and effective RL policies.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep Reinforcement Learning (RL) has emerged as the premier method for developing agents capable of navigating complex environments. Deep RL algorithms have demonstrated remarkable performance across a diverse range of games, including arcade games (Mnih et al., 2015; Schrittwieser et al., 2020; Hafner et al., 2021; 2023), real-time strategy games (Vinyals et al., 2019; OpenAI, 2018), board games (Silver et al., 2016; 2018; Schrittwieser et al., 2020), and games with imperfect information (Schmid et al., 2021). Despite these successes, data efficiency remains a significant challenge, impeding the transition of deep RL agents from research to practical applications. Accelerating agent-environment interactions can mitigate this issue to some extent, but it is often impractical for real-world scenarios. Therefore, enhancing sample efficiency is essential to bridge this gap and enable the deployment of RL agents in real-world applications (Micheli et al., 2022).\nModel-based approaches (Sutton & Barto, 2018) represent one of the most promising avenues for enhancing data efficiency in reinforcement learning. Specifically, models which learn a \u201cworld model\" (Ha & Schmidhuber, 2018) have been shown to be effective in improving sample efficiency. This involves training a generative model of the environment in a self-supervised manner. These models can generate new trajectories by continuously predicting the next state and reward, enabling the RL algorithm to be trained indefinitely without the need for additional real-world interactions."}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 MODEL-BASED RL: WORLD MODELS", "content": "Model-based RL has been a popular paradigm of reinforcement learning. With the advent of neural networks, it has become possible to model high-dimensional state spaces and thus, use model-based RL for environments with high-dimensional observations such as RGB images. In the last few years, based on PlaNet (Hafner et al., 2018), Hafner et al. proposed the Dreamer series (Hafner et al., 2020; 2021; 2023), a class of algorithms that learn the latent dynamics of the environment using a recurrent state space model (RSSM), while learning behavioral policy in the latent space. Currently, DreamerV3 (Hafner et al., 2023) has been shown to work across multiple tasks with a single configuration, setting the state-of-the-art across different benchmarks. The actor and critic in DreamerV3 learn from abstract trajectories of representations predicted by the world model.\nWith the advent of transformers (Vaswani et al., 2017) in sequence modelling and the promise of scaling performance across multiple tasks with more data, replacing the traditional RSSM backbones with transformer-based backbones has become a very active research direction. IRIS (Micheli et al., 2022) was one of the first transformer-based world model approaches, using VQVAE (Oord et al., 2017) to tokenize the observations and an autoregressive transformer that performs sequence modelling in the token space. Although IRIS obtains human-level performance on 10 out of 26 Atari games, its actor-critic operates in the RGB pixel space, making it almost 14x slower than DreamerV3. In contrast, methods such as TWM (Robine et al., 2023) and STORM (Zhang et al., 2023), use latent actor-critic input space. The proposed GIT-STORM employs it as well, as we believe it is the most promising direction to overcome sample efficiency constraints. TWM introduces a balanced sampling scheme and employs TransformerXL (Dai et al., 2019). More recently, STORM updated DreamerV3 by utilizing the transformer backbone. All aforementioned transformer-based world models use an MLP head to model a dynamics prior which is used to predict the discrete representation of the following timestep. In contrast, introduced by TECO (Yan et al., 2023), we employ a MaskGIT (Chang et al., 2022) prior head, which enhances the sequence modelling capabilities of the world model. Furthermore, besides STORM, all the mentioned transformer-based world models concatenate the discrete action to the extracted categorical latent representations. As a result, none of these methods is able to handle continuous actions. In contrast, combining latent representations and actions with a state mixer, we successfully train STORM and GIT-STORM on a challenging continuous action environment (i.e., DMC)."}, {"title": "2.2 MASKED MODELLING FOR VISUAL REPRESENTATIONS AND GENERATION", "content": "Inspired by the Cloze task (Taylor, 1953), BERT (Devlin et al., 2018) proposed a masked language model (MLM) pre-training objective that led to several state-of-the-art results on a wide class of natural language tasks. Following the success of BERT, Masked Autoencoders (MAEs) (He et al., 2022) learn to reconstruct images with masked patches during the pre-training stage. The learned representations are then used for downstream tasks. Zhang et al. (2021) similarly, improves upon a BERT-like masking objective for its non-autoregressive generation algorithm.\nThe most relevant to our work is MaskGIT (Chang et al., 2022), a non-autoregressive decoding approach that consists of a bidirectional transformer model, trained by learning to predict randomly masked visual tokens. By leveraging a bidirectional transformer (Devlin et al., 2018), it can better capture the global context across tokens during the sampling process. Furthermore, training on masked token prediction enables efficient, high-quality sampling at a significantly lower cost than autoregressive models. MaskGIT achieves state-of-the art performance on ImageNet dataset and achieves a 64\u00d7 speed-up on autoregressive decoding. The MaskGIT architecture has been applied to various tasks, such as video generation (Yan et al., 2023; Yu et al., 2023a;b) and multimodal generation (Mizrahi et al., 2024). For example, Yan et al. (2023) proposes TECO, a latent dynamics video prediction model that uses MaskGIT to model the prior for predicting the next timestep discrete representations, enhancing the sequence modelling of a backbone autoregressive transformer. Inspired by TECO, we adopt the use of MaskGIT prior for the world model, enhancing the sequence modelling capabilities, crucial for enabling and improving the agent policy learning behavior.\nFurther discussion of related works can be found in Appendix B."}, {"title": "3 METHOD", "content": "Following DreamerV3 (Hafner et al., 2023) and STORM (Zhang et al., 2023), we define our framework as a partially observable Markov decision process (POMDP) with discrete timesteps, $t \\in \\mathbb{N}$, scalar rewards, $r_t \\in \\mathbb{R}$, image observations, $o_t \\in \\mathbb{R}^{h \\times w \\times c}$, and discrete actions. $a_t \\in \\{1, ..., m_a\\}$. These actions are governed by a policy, $a_t \\sim \\pi(a_t | o_{1:t}, A_{1:t-1})$, where $o_{1:t}$ and $a_{1:t-1}$ represent the previous observations and actions up to timesteps t and t - 1, respectively. The termination of each episode is represented by a Boolean variable, $c_t \\in \\{0,1\\}$. The goal is to learn an optimal policy, $\\pi$, that maximizes the expected total discounted rewards, $\\mathbb{E}_{\\pi} [\\sum_{t=1}^{\\infty} \\gamma^{t-1}r_t]$, where $\\gamma \\in [0, 1]$ serves as the discount factor. The learning process involves two parallel iterative phases: learning the observation and dynamics modules (World Model) and optimizing the policy (Agent).\nIn this section, we first provide an overview of the dynamics module of GIT-STORM. Then, we describe our dynamics prior head of the dynamics module, inspired by MaskGIT (Chang et al., 2022) (Figure 1). Finally, we explain the imagination phase using GIT-STORM, focusing on the differences between STORM and GIT-STORM. We follow STORM for the observation module and Dreamer V3 for the policy definition, which are described in Appendix A.1 and A.2, respectively."}, {"title": "3.1 OVERVIEW: DYNAMICS MODULE", "content": "The dynamics module receives representations from the observation module and learns to predict future representations, rewards, and terminations to enable planning without the usage of the observation module (imagination). We implement the dynamics module as a Transformer State-Space Model (TSSM). Given latent representations from the observation module, $z_t$, and actions, $a_t$, the dynamics module predicts hidden states, $h_t$, rewards, $\\hat{r}_t$, and episode termination flags, $\\hat{c}_t \\in \\{0,1\\}$ as follows,\n$s_t = g_\\theta(z_t, a_t)$ \t\t (State Mixer)\n$h_t = f_\\phi(s_{1:t})$ \t\t (Autoregressive Transformer)\n$z_{t+1} \\sim p_\\phi(z_{t+1} | h_t)$ \t\t (Dynamics Prior Head)\n$r_t \\sim p_\\phi(r_t | h_t)$ \t\t (Reward Head)\n$\\hat{c}_t \\sim p(\\hat{c}_t | h_t)$ \t\t (Termination Head)\n                                                                                        (1)"}, {"title": "3.2 DYNAMICS PRIOR HEAD: MASKGIT PRIOR", "content": "Given the expressive power of MaskGIT (Chang et al., 2022), we propose enhancing the dynamics module in the world model by replacing the current MLP prior with a MaskGIT prior, as shown in Figure 1. Given the posterior, $z_t$, and a randomly generated mask, $m \\in \\{0,1\\}^N$ with $M = [\\gamma N]$ masked values where $\\gamma = cos(\\frac{t}{\\tau})$, the MaskGIT prior $p_\\phi(z_{t+1} | h_t)$ is defined as follows.\nFirst, the hidden states, $h_t$, are concatenated with the masked latent representations, $z_t \\circ m_t$, where $\\circ$ indicates element-wise multiplication. Despite $h_t$ being indexed by t, it represents the output of the $f_\\phi$ and thus encapsulates information about the subsequent timestep. Consequently, the concatenation of $z_t$ and $h_t$ integrates information from both the current and the next timestep, respectively. A bidirectional transformer is then used to learn the relationships between these two consecutive representations, producing a summary representation, $\\hat{s}_t$. Finally, logits are computed as the dot product (denoted as in Figure 1) between the MaskGIT embeddings, which represent the masked tokens, and $\\hat{s}_t$. This dot product can be interpreted as a similarity distance between the embeddings and $\\hat{s}_t$. Indeed, from a geometric perspective, both cosine similarity and the dot product serve as similarity metrics, with cosine similarity focusing on the angle between two vectors, while the dot product accounts for both the angle and the magnitude of the vectors. Therefore, by optimizing the MaskGIT prior, this dot product aligns the embeddings with $\\hat{s}_t$, thereby facilitating and improving the computation of logits. In contrast, when using the MLP prior, the logits are generated as the output of an MLP that only takes $h_t$ as input. This approach requires the model to learn the logits space and their underlying meaning without any inductive bias, making the learning process more challenging.\nDuring training, we follow the KL divergence loss of DreamerV3 (Hafner et al., 2023), which consists of two KL divergence losses which differ in the stop-gradient operator, sg(\u00b7), and loss scale. We account for the mask tokens in the posterior and define $\\mathcal{L}_{dyn}$ and $\\mathcal{L}_{rep}$ as,\n$\\mathcal{L}_{dyn}(\\phi) = max(1, KL[sg(q_\\phi(z_t | x_t)) \\circ m_t || p_\\phi(z_{t} | h_{t-1})])$ (3)\n$\\mathcal{L}_{rep}(\\phi) = max(1, KL [ q_\\phi(z_t | x_t) \\circ m_t || sg(p_\\phi(z_{t} | h_{t-1}))])$ (4)\nwhere $m_t$ is multiplied element-wise with the posterior, eliminating the masked tokens from the loss.\nSampling. During inference, since MaskGIT has been trained to model both unconditional and conditional probabilities, we can sample any subset of tokens per sampling iteration. Following Yan et al. (2023), we adopt the Draft-and-Revise decoding scheme introduced by Lee et al. (2022) to predict the next latent state (Algorithm 1 and 2). During the draft phase, we initialize a partition $\\Pi$ which contains $T_{draft}$ disjointed mask vectors m of size (latent dim$\\div T_{draft}$), which together mask the whole latent representation. Iterating through all mask vectors in $\\Pi$, the resulting masked representations are concatenated with the hidden states $h_t$ from Eq. 1 and fed to the MaskGIT prior head that computes the logits of the tokens correspondent to $h_t$ and $m^i$. Such logits are then used to sample the new tokens that replace the positions masked by $m^i$. During the revise phase, the whole procedure is repeated $\\Gamma$ times. As a result, when sampling the new tokens, the whole representation is taken into account, resulting in a more consistent and meaningful sampled state."}, {"title": "3.3 STATE MIXER FOR CONTINUOUS ACTION ENVIRONMENTS", "content": "When using a TSSM as the dynamics module, the conventional approach has been to concatenate discrete actions with categorical latent representations and feed this sequence into the autoregressive transformer. However, this method is ineffective for continuous actions, as one-hot categorical representations or VQ-codes (Oord et al., 2017) are poorly suited for representing continuous values."}, {"title": "3.4 IMAGINATION PHASE", "content": "Instead of training the policy by interacting with the environment, model-based approaches use the learned representation of the environment and plan in imagination (Hafner et al., 2018). This approach allows sample-efficient training of the policy by propagating value gradients through the latent dynamics. The interdependence between the dynamics generated by the world model and agent's policy makes the quality of the imagination phase crucial for learning a meaningful policy. The imagination phase is composed of two phases, conditioning phase and the imagination one. During the conditioning phase, the discrete representations zt are encoded and fed to the autoregressive transformer. The conditioning phase gives context for the imagination one, using the cached keys and values (Yan et al., 2021) computed during the conditioning steps.\nDifferently from STORM, which uses a MLP prior to compute the next timestep representations, we employ MaskGIT to accurately model the dynamics of the environment. By improving the quality of the predicted trajectories, the agent is able to learn a superior policy. Moreover, while STORM applies KV caching only on the conditioning phases, we extend its use to the whole imagination phase as in IRIS (Micheli et al., 2022), significantly improving the related sample efficiency."}, {"title": "4 EXPERIMENTS", "content": "In this section, we analyse the performance of GIT-STORM and its potential limitations by exploring the following questions: (a) How does the MaskGIT Prior affect TSSMs learning behavior and performances on related downstream tasks (e.g., Model-based RL and Video Prediction tasks)? (b) Can Transformer-based world models learn to solve tasks on continuous action environments when using state mixer functions?"}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "To evaluate and analyse the proposed method, we consider both discrete and continuous actions environments, namely Atari 100k benchmark (Kaiser et al., 2019) and DeepMind Control Suite (Tassa et al., 2018) respectively. On both environments, we conduct both RL and video prediction tasks.\nBenchmark and baselines. Atari 100k benchmark consists of 26 different video games with discrete action space. The constraint of 100k interactions corresponds to a total of 400k frames used for training, as frame skipping is set to 4. For RL task on Atari 100k benchmark, we compare GIT-STORM against one model-free method, SimPLe (Kaiser et al., 2019), one RSSM, DreamerV3 (Hafner et al., 2023), and three TSSM models (i.e., IRIS (Micheli et al., 2022), TWM (Robine et al., 2023), and STORM (Zhang et al., 2023)). DMC benchmark consists of 18 control tasks with continuous action space. We restrict the models to be trained with only 500k interactions (1M frames) by setting frame skipping to 2. For RL task on DMC benchmark, we compare our model against SAC (Haarnoja et al., 2018), CURL (Laskin et al., 2020), DrQ-v2 (Yarats et al., 2022), PPO (Schulman et al., 2017), DreamerV3 (Hafner et al., 2023), and STORM (Zhang et al., 2023). We trained GIT-STORM on 5 different seeds. For video prediction tasks, we compare GIT-STORM with STORM only to understand how the MaskGIT Prior affects the visual quality of predicted frames and its influence on the policy training.\nExtended details of the baselines for both benchmarks can be found in Appendix I.\nEvaluation metrics. Proper evaluation of RL algorithms is known to be difficult due to both the stochasticity and computational requirements of the environments (Agarwal et al., 2021). To provide an accurate evaluation of the models, we consider a series of metrics to assess the performances of the considered baselines on across the selected experiments. We report human normalized mean and median as evaluation metrics, aligning with prior literature. We also report interquartile Mean (IQM), Optimality Gap, Performance Profiles (scores distributions), and Probability of Improvement (PI), which provide a statistically grounded perspective on the model evaluation (Agarwal et al., 2021).\nFor video prediction task, we report two metrics: Fr\u00e9chet Video Distance (FVD) (Unterthiner et al., 2019) to evaluate visual quality of the predicted frames, and perplexity (Jelinek et al., 2005) measure of the predicted tokens to evaluate the token utilization by the dynamics prior head. We use the trained agent to collect ground truth episodes and use the world model to predict the frames. We report the FVD over 256 videos which are conditioned on the first 8 frames to predict 48 frames.\nA full description of these metrics can be found in Appendix J."}, {"title": "4.2 RESULTS ON DISCRETE ACTION ENVIRONMENTS: ATARI 100K", "content": "RL task. Figure 2 summarizes the human normalized mean and median, and IQM score. The full results on individual environments can be found in the Appendix due to space limitations (Table 5). We can see that while TWM and DreamerV3 present a higher human median than GIT-STORM (TWM: 51%, DreamerV3: 49% \u2192 GIT-STORM: 42.6%), GIT-STORM dominates in terms of"}, {"title": "4.3 RESULTS ON CONTINUOUS ACTION ENVIRONMENTS: DEEPMIND CONTROL SUITE", "content": "RL task. Figure 4 summarizes the human normalized mean and median, and IQM score. The full results on individual environments can be found in the Appendix (Table 6). Although DreamerV3 outperforms all other models on average, Table 6 shows that GIT-STORM presents state-of-the-art scores on two environments, Walker Stand and Quadruped Run. Compared to STORM, GIT-STORM consistently and significantly outperforms across the whole benchmark in terms of human median and mean (STORM: 31.50, 214.50 \u2192 GIT-STORM: 475.12, 442.10, respectively). For PI, GIT-STORM achieves PI > 0.5 than STORM, PPO, and SAC (e.g., GIT-STORM: 0.75, 0.60 and 0.63, over STORM, PPO and SAC, respectively) (Figure 3 (Right)).\nVideo Prediction task. Table 4 shows video prediction results on selected DMC environments. The table shows that our model achieves lower FVD and higher perplexity than STORM for all environments. The video prediction results in Figure 6 show that although both models fail to capture"}, {"title": "5 DISCUSSION", "content": "The proposed GIT-STORM uses a Masked Generative Prior (MaskGIT) to enhance the world model sequence modelling capabilities. Indeed, as discussed in the introduction, high quality and accurate representations are essential to guarantee and enhance agent policy learning in imagination. Remarkably, the proposed GIT-STORM is the only world model, among the ones that use uniform sampling and latent actor critic input space, that is able to achieve non-zero reward on the Freeway environment (e.g., DreamerV3: 0, STORM: 0 \u2192 GIT-STORM: 13). Indeed, both STORM and IRIS resorted in ad-hoc solutions to get positive rewards, such as changing the sampling temperature (Micheli et al., 2022) and using demonstration trajectories (Zhang et al., 2023). Such result, together with the quantitative results on the Atari 100k and DMC benchmarks, clearly answer question (a) - the presented MaskGIT prior improves the policy learning behavior and performance on downstream tasks (e.g., Model-based RL and Video Prediction) of TSSMs. Moreover, the FVD and perplexity comparisons in Table 3 and Table 4 suggest that GIT-STORM has better predictive capabilities, learns a better dynamics module, and presents more accurate imagined trajectories (Figure 5, Figure 6). Similarly to image synthesis (Chang et al., 2022) and video prediction (Yu et al., 2023a) tasks, we show how using masked generative modelling is a better inductive bias to model the prior dynamics of discrete representations and improve the downstream usefulness of world models on RL tasks. Furthermore, the MaskGIT Prior can be used in any sequence modelling backbone that uses categorical latent"}, {"title": "6 CONCLUSION", "content": "The motivation for this work stems from the need to improve the quality and accuracy of world models representations in order to enhance agent policy learning in challenging environments. Inspired by (Yan et al., 2023), we conducted experiments using the TECO framework on video prediction tasks with DMLab and SSv2 (Goyal et al., 2017) datasets. Replacing an MLP prior with a MaskGIT (Chang et al., 2022) prior significantly improved the sequence modelling capabilities and the related performance on the video prediction downstream task. Building upon these insights, we proposed GIT-STORM, which employs a MaskGIT Prior to enhance the sequence modelling"}, {"title": "A GIT-STORM FRAMEWORK", "content": "Following STORM, the observation module is a variational autoencoder (VAE) (Kingma & Welling, 2013), which encodes observations, ot, into stochastic latent representations, zt, and decodes back the latents to the image space, \u00f4t:\nObservation encoder: $z_t \\sim q_\\phi(z_t | o_t)$\t\t\t(5)\nObservation decoder: $\\hat{o}_t = p(z_t)$\t\t\t(6)\nThe observations are encoded using a convolutional neural network (CNN) encoder (LeCun et al., 1989) which outputs the logits used to sample from a categorical distribution. The distribution head applies an unimix function over the computed logits to prevent the probability of selecting any category from being zero (Sullivan et al., 2023). Since the sampled latents lack gradients, we use the straight-through gradients trick (Bengio et al., 2013) to preserve them. The decoder, modeled using a CNN, reconstructs the observation from the latents, zt. While the encoder is updated using gradients coming from both observation and dynamics modules, the decoder is optimized using only the Mean Squared Error (MSE) between input and reconstructed frames:\n$\\mathcal{L}_{observation Model} = MSE(o_t, \\hat{o}_t)$\t\t\t(7)"}, {"title": "A.2 POLICY LEARNING", "content": "Following the model-based RL research landscape (DreamerV3; Hafner et al., 2023) we cast the agent policy learning framework using the actor-critic approach (Mnih et al., 2016). The agent actor-critic is trained purely from agent state trajectories st = [zt, ht] generated by the world model. The actor aims to learn a policy that maximizes the predicted sum of rewards and the critic aims to predict the"}, {"title": "B EXTENDED RELATED WORKS: VIDEO PREDICTION MODELLING", "content": "Video prediction, a fundamental task in computer vision, aims to generate or predict sequences of future frames based on conditioning past frames. The downstream tasks of video prediction modelling span a wide range of domains, showcasing its significance in different fields, such as autonomous driving (Hu et al., 2023), robot navigation (DeSouza & Kak, 2002) controllable animation (Mahapatra & Kulkarni, 2022), weather forecasting (Bi et al., 2023; Meo et al., 2024b), and model based reinforcement learning (Hafner et al., 2018; 2020; 2021; 2023; Zhang et al., 2023; Micheli et al., 2022). Video prediction modelling is known for its sample inefficiency, which poses significant challenges in learning accurate and reliable models in a feasible time (Ming et al., 2024). To address this, recent advancements have introduced spatio-temporal state space models, which typically consist of a feature extraction component coupled with a dynamics prediction module. These models aim to understand and predict the evolution of video frames by capturing both spatial and temporal relationships. Notable examples include NUW\u00c4 (Wu et al., 2022) and VideoGPT (Yan et al., 2021) which respectively use 2D and 3D convolutional layers to extract the latent representations and an autoregressive transformer to perform sequence modelling in the latent space. Moreover, TECO (Yan et al., 2023) introduces the use of MaskGIT (Chang et al., 2022) prior to improve the accuracy of the predicted discrete latents and uses a 1D convolution to enhance temporal consistency. Furthermore, VideoPoet (Kondratyuk et al., 2023), which is able to handle multiple modalities and perform a variety of tasks besides video prediction."}, {"title": "C FULL RESULTS ON RL TASK", "content": "In this section we report and present the full evaluation and comparison on the two RL benchmark environments, Atari 100k (Kaiser et al., 2019) and DMC (Tassa et al., 2018)."}, {"title": "D TRAINING CURVES", "content": "In this section, we provide the training curves of GIT-STORM for both Atari 100k and DMC benchmark."}, {"title": "E ABLATION STUDY", "content": ""}, {"title": "E.1 DIMENSIONS OF DYNAMIC PRIOR HEAD", "content": "In order to find the best configuration for the MaskGIT prior, we conduct experiments on three different environments with different embedding and vocabulary dimensions corresponding to the bidirectional transformer. While the performance of different configurations varies between environments, we find that a bigger embedding size achieves higher scores on average as seen in Figure 12.\nAs shown in DreamerV3 (Hafner et al., 2023), the model achieves better performance as it increases in the number of trainable parameters. Thus, to provide a fair comparison with STORM, we restrict the transformer corresponding to the MaskGIT prior to a similar number of parameters as the MLP prior defined in STORM."}, {"title": "E.2 VQ-VAE VS ONE HOT CATEGORICAL", "content": "The world model state in model-based RL is represented in terms of a latent representation based on raw observations from the environment. However, there is no clear consensus on the representation of the latent space, with SimPLE (Kaiser et al., 2019) using a Binary-VAE, IRIS (Micheli et al., 2022) using a VQ-VAE while DreamerV3 (Hafner et al., 2023), STORM (Zhang et al., 2023) and TWM (Robine et al., 2023) employ a Categorical-VAE.\nWhile recent methods show empirically the advantages of a Categorical-VAE in Atari environments, there is no comprehensive study on different latent space representations. Thus, Table 7 provides a comparison between a VQ-VAE and Categorical-VAE latent representation in the context of GIT-STORM, motivating our choice of latent space. The comparison is performed on three environments with different levels of complexity in terms of visual representations.\nIn order to keep the comparison between the two representations accurate, we scale down the VQ-VAE to only 32 codebook entries, each consisting of 32 dimensions, matching the size of the one-hot categorical representation of 32 categories with 32 classes each. While the VQ-VAE in IRIS (Micheli et al., 2022) uses a considerably bigger vocabulary and embedding size, we believe the additional number of parameters introduced provide a biased estimation of the representation capabilities of the latent space. Moreover, we notice that the VQ-VAE approach introduces a significant overhead in terms of training and sampling time. Table 7 shows that the VQ-VAE latent representations collapse"}, {"title": "E.3 STATE MIXER ABLATION", "content": "As described in Sec. 3.1, latent representations z\u0142 and actions at are mixed using a state mixer function g(.). To understand the affect of different mixing strategies for the underlying task, we compare three different mixing functions in the DMC benchmark: (1) concatenation, (2) concatenation followed by attention and (3) cross attention between state and actions. Figure 13 illustrates the results. Surprisingly, we find that the simple approach works the best for the tasks \u2013 concatenation of state and action significantly outperforms the attention-based approaches in the chosen tasks."}, {"title": "F VIDEO PREDICTION DOWNSTREAM TASK: TECO", "content": "In order to assess the capabilities of the MaskGIT prior in modelling latent dynamics across different tasks, we consider video generation tasks as a representative study. More specifically, we consider Temporally Consistent Transformer for Video Generation (TECO) (Yan et al., 2023) on DeepMind Lab (DMLab) (Beattie et al., 2016) and Something-Something v.2 (SSv2) (Goyal et al., 2017) datasets. TECO uses a spatial MaskGIT Prior to generate the state corresponding to the next timestep. Table 1 highlights the importance of the prior network and supports our earlier results on the Atari 100k benchmark. Indeed, when replacing the MaskGit prior network with an MLP one with the same number of parameters, the FVD (Unterthiner et al., 2019) on both DMLab and SSv2 datasets significantly increases, going from 48 to 153 and from 199 to 228 in the DMLab and SSv2 datasets respectively."}, {"title": "G HYPERPARAMETERS", "content": ""}, {"title": "H COMPUTATIONAL RESOURCES", "content": "Throughout our experiments, we make use of NVIDIA A100 and H100 GPUs for both training and evaluation on an internal cluster, a summary of which can be found in Table 15. For the Atari 100k benchmark, we find that each individual experiment requires around 20 hours to train. For the video prediction tasks, DMLab requires 3 days of training on 4 NVIDIA A100 GPUs. For DMC Vision tasks, we used H100 GPUs to sample from 16 environments concurrently, which reduced our training time to only 8 hours for 1M steps. Compared to this, using A100 for one environment takes 7 days. We acknowledge that the research project required more computing resources than the reported ones, due to preliminary experiments and model development."}, {"title": "I BASELINES", "content": "To assess our approach downstream capabilities on Atari 100k we select the following baselines: SimPLe (Kaiser et al., 2019) trains a policy using PPO (Schulman et al., 2017) leveraging a world model represented as an action-conditioned video generation model; TWM (Robine et al., 2023) uses a transformer-based world model that leverages a Transformer-XL architecture and a replay buffer which uses a balanced sampling scheme (Dai et al."}]}