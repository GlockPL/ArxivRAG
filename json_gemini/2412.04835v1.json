{"title": "Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment", "authors": ["Ran Tian", "Yilin Wu", "Chenfeng Xu", "Masayoshi Tomizuka", "Jitendra Malik", "Andrea Bajcsy"], "abstract": "Visuomotor robot policies, increasingly pre-trained on large-scale datasets, promise significant advancements across robotics domains. However, aligning these policies with end-user preferences remains a challenge, particularly when the preferences are hard to specify. While reinforcement learning from human feedback (RLHF) has become the predominant mechanism for alignment in non-embodied domains like large language models, it has not seen the same success in aligning visuomotor policies due to the prohibitive amount of human feedback required to learn visual reward functions. To address this limitation, we propose Representation-Aligned Preference-based Learning (RAPL), an observation-only method for learning visual rewards from significantly less human preference feedback. Unlike traditional RLHF, RAPL focuses human feedback on fine-tuning pre-trained vision encoders to align with the end-user's visual representation and then constructs a dense visual reward via feature matching in this aligned representation space. We first validate RAPL through simulation experiments in the X-Magical benchmark and Franka Panda robotic manipulation, demonstrating that it can learn rewards aligned with human preferences, more efficiently uses preference data, and generalizes across robot embodiments. Finally, our hardware experiments align pre-trained Diffusion Policies for three object manipulation tasks. We find that RAPL can fine-tune these policies with 5x less real human preference data, taking the first step towards minimizing human feedback while maximizing visuomotor robot policy alignment.", "sections": [{"title": "1 Introduction", "content": "Visuomotor robot policies\u2014which predict actions directly from high-dimensional image observations are being revolutionized by pre-training on large-scale datasets. For example, robots like manipulators (Brohan et al. 2023; Chi et al. 2024), humanoids (Radosavovic et al. 2023), and autonomous cars (Hu et al. 2023; Tian et al. 2024b) rely on pre-trained vision encoders (Deng et al. 2009) for representing RGB images and for learning multimodal behavior policies from increasingly large observation-action teleoperation datasets (Padalkar et al. 2023).\nDespite this remarkable progress, these visuomotor policies do not always act in accordance with human end-user preferences. For instance, consider the scenario on the left of Figure 1 where a robot manipulator is trained to imitate diverse teleoperators picking up a bag of chips. At deployment, the end-user prefers the chips to remain intact. However, the manipulator frequently grasps the bag by squeezing the middle-risking damage to the chips-instead of holding the packaging by its edges like the user prefers.\nA foundational approach to tackle this misalignment between a pre-trained policy and an end-user's hard-to-specify preferences is reinforcement learning from human feedback (RLHF) (Christiano et al. 2017). By presenting the end-user with outputs generated by the pre-trained model and collecting their preference rankings, RLHF trains a reward model that is then used to fine-tune the base policy, enabling it to produce outputs that better align with the end-user's preferences. While RLHF has emerged as the predominant alignment mechanism in non-embodied domains such as large language models (LLMs) (Ouyang et al. 2022) and text-to-image generation models (Lee et al. 2023), it has not shown the same impact for aligning visuomotor robot policies. Fundamentally, this challenge arises because learning a high-quality visual reward function requires an impractically large amount of human preference feedback: in our hardware experiments, collecting 200 preference rankings for a single task takes approximately 1 day. If we hope to align generalist, pre-trained visuomotor policies, it is essential to adapt the RLHF paradigm to operate with significantly less human feedback.\nOur approach is motivated by seminal work in inverse reinforcement learning (Abbeel and Ng 2004; Ziebart et al. 2008), which states that the desired reward function we seek to learn via RLHF makes the robot's behavior indistinguishable from the human's ideal behavior. Mathematically, this reward can be modeled as the divergence between the feature distribution of the robot's policy and that of the end-user's optimal demonstrations (Pomerleau 1988; Dadashi et al. 2021; Sun et al. 2019; Swamy et al. 2021). However, when constructing visual"}, {"title": "2 Related Work", "content": "Preference alignment of generative models. Generative models, such as large language models (LLMs), text-to-image generation models, and behavior cloning models, are predominantly trained using an imitative objective. While this approach simplifies training with internet-scale data, this objective serves only as a \"proxy\" for the true training goal: the human internal reward function. As a result, these generative models may be misaligned with end-user preferences or may even lead to safety-critical scenarios (e.g., generating unsafe texts (Mu et al.), images (Lee et al. 2023), and robot motions (Lu et al. 2023)). Preference alignment, particularly through reinforcement learning from human feedback (RLHF) (Christiano et al. 2017), has emerged as a key strategy for aligning generative models with human preferences, especially in non-embodied domains. RLHF involves three key stages: feedback elicitation (e.g., presenting users with two or more model generations and gathering preference rankings), reward modeling (e.g., training a reward model to replicate the rankings), and policy optimization via reinforcement learning (RL) (Schulman et al. 2017). Traditional RL"}, {"title": "3 Problem Setup", "content": "Human policy. We consider scenarios where the robot R wants to learn how to perform a task for human H. The human knows the desired reward \\(r^*\\) which encodes their preferences for the task. The human acts via an approximately optimal policy \\(\\pi_H : O \\rightarrow \\Delta_{\\mathcal{A}_H}\\) optimized under their underlying reward function. Here, \\(\\Delta\\) denotes the probability simplex over the human's \\(N_{\\mathcal{A}_H}\\)-dimensional action space. Instead of directly consuming the raw perceptual input, research suggests that humans naturally build visual representations of the world (Bonnen et al. 2021) that focus on task-relevant attributes (Callaway et al. 2021). We model the human's representation model as \\(\\phi_H : O \\rightarrow \\mathcal{Z}_H\\), mapping from the perceptual input \\(o \\in O\\) to the human's latent space \\(\\mathcal{Z}_H\\) which captures their task and preference-relevant features.\nVisumotor policy alignment. We denote the robot's visumotor policy as \\(\\pi_R : O \\rightarrow \\Delta_{\\mathcal{A}_R}\\) where \\(\\Delta\\) denotes the probability simplex over the robot's \\(N_{\\mathcal{A}_R}\\)-dimensional action space. In the RLHF paradigm, we seek to fine-tune \\(\\pi_R\\) to maximize a reward function:\n\\[\\pi_R^* = \\arg \\max_{\\pi_R} \\mathbb{E}_{o \\sim p(o / \\pi_R)} [\\sum_{t=0}^{\\infty} \\gamma^t r(\\phi_R(o_t))] , \\quad(1)\\]\nwhere \\(\\gamma \\in [0, 1)\\) is the discount factor and \\(o = \\{o_0, o_1, ...\\}\\) is the image observation trajectory induced by the robot's policy. The robot's reward \\(r\\) also relies on a visual representation, \\(\\phi_R : O \\rightarrow \\mathcal{Z}_R\\), which maps from the image observation \\(o \\in O\\) to a lower-dimensional latent space, \\(\\mathcal{Z}_R\\). In general, this could be hand-crafted, such as distances to objects from the agent's end-effector (Ziebart et al. 2008; Levine et al. 2011; Finn et al. 2016), or the"}, {"title": "4 RAPL: Representation-Aligned Preference-Based Learning", "content": "We follow the formulation in Sucholutsky and Griffiths (2023) and bring this to the robot learning domain. Intuitively, visual representation alignment is defined as the degree to which the output of the robot's encoder, \\(\\phi_R\\), matches the human's internal representation, \\(\\phi_H\\), for the same image observation, \\(o \\in O\\), during task execution. We utilize a triplet-based definition of representation alignment as in (Jamieson and Nowak 2011) and (Sucholutsky and Griffiths 2023).\nDefinition 1. Triplet-based Representation Space. Let\n\\(o = \\{o^t\\}_{t=0}^T\\) be a sequence of image observations over \\(T\\) timesteps, \\(\\phi : O \\rightarrow Z\\) be a given representation model, and \\(\\phi(o) := \\{\\phi(o^0), ..., \\phi(o^T)\\} \\) be the corresponding embedding trajectory. For some distance metric \\(d(\\cdot, \\cdot)\\) and two observation trajectories \\(o^i\\) and \\(o^j\\), let \\(d(\\phi(o^i), \\phi(o^j))\\) be the distance between their embedding trajectories. The triplet-based representation space of \\(\\phi\\) is:\n\\[S_{\\phi} = \\{(o^i, o^j, o^k) : o^i, o^j, o^k \\in \\Xi, o^{j|i} \\succ o^{k|i} \\},\\quad(3)\\]\nwhere \\(\\Xi\\) is the set of all possible image trajectories for the task of interest, and \\(o^{j|i} \\succ o^{k|i}\\) denotes \\(d(\\phi(o^i), \\phi(o^j)) < d(\\phi(o^i), \\phi(o^k))\\).\n\\min l(S_{\\phi_R}, S_{\\phi_H}).\\quad(4)\\]"}, {"title": "4.2 Representation Alignment via Preference-based Learning", "content": "Although this formulation sheds light on the underlying problem, solving Equation 4 exactly is impossible since the functional form of the human's representation \\(\\phi_H\\) is unavailable and the set \\(S_{\\phi_H}\\) is infinite. Thus, we approximate the problem by constructing a subset \\(\\hat{S}_{\\phi_H} \\subset S_{\\phi_H}\\) of triplet queries. Since we seek a representation that is relevant to the human's preferences, we ask the human to rank these triplets based on their preference-based notion of similarity (e.g., \\(r^*(\\phi_H(o^i)) > r^*(\\phi_H(o^j)) > r^*(\\phi_H(o^k)) \\Rightarrow o^{j|i} \\succ o^{k|i}\\)). With these rankings, we implicitly learn \\(\\phi_H\\) via a neural network trained on these triplets.\nWe interpret a human's preference over the triplet \\((o^i, o^j, o^k) \\in \\hat{S}_{\\phi_H}\\) via the Bradley-Terry model (Bradley and Terry 1952), where \\(o^i\\) is treated as an anchor and \\(o^j, o^k\\) are compared to the anchor in terms of preference similarity as in Equation 3:\n\\[P(o^{j|i} \\succ o^{k|i} | \\phi_H) = \\frac{e^{-d(\\phi_H(o^i), \\phi_H (o^j))}}{e^{-d(\\phi_H(o^i), \\phi_H (o^j))} + e^{-d(\\phi_H(o^i), \\phi_H (o^k))}}.\\quad(5)\\]\nOne remaining question is: what distance measure \\(d\\) should we use to quantify the difference between two embedding trajectories? In this work, we use optimal transport as a principled way to measure the feature matching between any two videos. For any video \\(o\\) and for a given representation \\(\\phi\\), let the induced empirical embedding distribution be \\(p = \\frac{1}{T} \\sum_{t=1}^T \\delta_{\\phi(o_t)}\\), where \\(\\delta_{\\phi(o_t)}\\) is a Dirac distribution centered on \\(\\phi(o_t)\\). Optimal transport finds the optimal transport plan \\(\\mu^* \\in \\mathbb{R}^{T \\times T}\\) that transports one embedding distribution, \\(p_i\\), to another video embedding distribution, \\(p_j\\), with minimal cost. This comes down to an optimization problem that minimizes the Wasserstein distance between the two distributions:\n\\[\\mu^* = \\arg \\min_{\\mu \\in \\mathcal{M}(p_i, p_j)} \\sum_{t=1}^T \\sum_{t'=1}^T c(\\phi(o_t), \\phi(o_{t'})) \\mu_{t, t'}, \\quad(6)\\]\nwhere \\(\\mathcal{M}(p_i, p_j) = {\\mu \\in \\mathbb{R}^{T \\times T} : \\mu \\mathbf{1} = p_i, \\mu^T \\mathbf{1} = p_j}\\) is the set of transport plan matrices, \\(c: \\mathbb{R}^{n_e} \\times \\mathbb{R}^{n_e} \\rightarrow \\mathbb{R}\\) is a cost function defined in the embedding space (e.g., cosine"}, {"title": "4.3 Preference-aligned Visual Reward Model", "content": "Given our aligned visual representation, we seek a robot visual reward function \\(r\\) that approximates the end-user's reward function for aligning the robot's visuomotor policy with human preference. Traditional IRL methods (Abbeel and Ng 2004; Ziebart et al. 2008) are built upon matching the feature distribution of the robot's policy with that of the end-user's demonstration. Specifically, we seek to match the observation distribution induced by the robot's policy \\(\\pi_R\\), and the observation distribution of a human's preferred video demonstration, \\(o^+\\), in the aligbned representation space.\nThe optimal transport plan between the two distributions precisely defines a reward function that measures this matching (Kantorovich and Rubinshtein 1958) and yields the reward which is optimized in Equation 1:\n\\[r(o^k; \\phi_R, o^+) = - \\sum_{t'=1}^T c(\\phi_R(o_t), \\phi_R(o_t^+)).\\quad(11)\\]"}, {"title": "5 Experimental Design", "content": "Preference dataset: \\(\\hat{S}_{\\phi_H}\\). While the ultimate test is learning from real end-user feedback, in this section, we first use a simulated human model, which allows us to easily ablate the size of the preference dataset, and gives us privileged access to \\(r^*\\) for direct comparison. In all environments, the simulated human constructs the preference dataset \\(\\hat{S}_{\\phi_H}\\) by sampling triplets of videos uniformly at random from the set of video observations \\(\\tilde{E} \\subset \\Xi\\), and then ranking them with their reward \\(r^*\\) as in Equation 5.\nIndependent & dependent measures. Throughout our experiments, we vary the visual reward signal used for robot policy optimization and the preference dataset size used for representation learning. We measure robot task success as a binary indicator of if the robot completed the task with high reward \\(r^*\\).\nControlling for confounds. Our ultimate goal is to have a visual robot policy, \\(\\pi_R\\), that takes as input observations and outputs actions. However, to rigorously compare policies obtained from different visual rewards, we need to disentangle the effect of the reward signal from any other policy design choices, such as the input encoders and architecture. To have a fair comparison, we follow the approach from (Zakka et al. 2022; Kumar et al. 2023) and input the privileged ground-truth state into all policy networks, but vary the visual reward signal used during policy optimization. Across all methods, we use an identical reinforcement learning setup and Soft-Actor Critic (SAC) for training (Haarnoja et al. 2018) with code base from (Zakka et al. 2022). When running SAC, the reward (Equation 11) requires matching the robot to an expert demonstration video. For all policy learning experiments, we use 10 expert demonstrations as the demonstration set \\(\\mathcal{D}^+\\) for generating the reward. To choose this expert observation, we follow the approach from (Haldar et al. 2023a). During policy optimization, given a robot's trajectory's observation \\(o_t\\) or induced by the robot policy \\(\\pi_R\\), we select the the \"closest\" expert demonstration \\(o^+ \\in \\mathcal{D}^+\\) to match the robot behavior with. This demonstration selection happens via:\n\\[o^k = \\arg \\min_{o^+ \\in \\mathcal{D}^+} \\sum_{t=1}^T \\sum_{t'=1}^T c(\\phi(o^k), \\phi(o_{t'})) \\mu_{t,t'}.\\quad(12)\\]"}, {"title": "6 Aligning Diffusion Policies in the Real World with RAPL", "content": "In the previous section, we demonstrated how RAPL's reward can align robot policies via reinforcement learning in simulation. However, high-fidelity simulators are often impractical for many real-world robotics tasks (e.g., deformable objects) and reinforcement learning in the real world is still an open research problem. Motivated by this, we turn to an algorithmic variant of RLHF, Direct Preference Optimization (DPO) described in Section 3, for aligning visuomotor policies without a simulator. DPO updates the policy via contrastive learning using preference rankings on the behavior generations; however, to reliably update the policy it requires a significant amount of preference labels. In this section, we demonstrate how our RAPL reward enables the scalable generation of synthetic preference rankings, significantly minimizing the number of real human labels while still achieving a high policy alignment."}]}