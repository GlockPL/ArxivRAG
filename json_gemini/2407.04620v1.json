{"title": "Learning to (Learn at Test Time): RNNs with Expressive Hidden States", "authors": ["Yu Sun", "Xinhao Li", "Karan Dalal", "Jiarui Xu", "Arjun Vikram", "Genghan Zhang", "Yann Dubois", "Xinlei Chen", "Xiaolong Wang", "Sanmi Koyejo", "Tatsunori Hashimoto", "Carlos Guestrin"], "abstract": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.", "sections": [{"title": "1 Introduction", "content": "In 2020, the OpenAI scaling law paper (Kaplan et. al [40]) showed that LSTMs (a type of RNN) could not scale similarly to Transformers or effectively use long context. Now, with modern RNNs and best practices, we re-evaluate these findings in Figure 2.\nOn the left, we observe that Mamba [26] \u2013 one of the most popular RNNs today \u2013 scales similarly to a strong Transformer, showing great progress since the LSTMs in 2020. However, on the right, we observe the same issue with Mamba as Kaplan et al. did with LSTMs. Tokens later in a sequence should be easier to predict on average, since they condition on more information. This is indeed the case for Transformer, whose average perplexity at each token index decreases throughout its 32k context. In contrast, the same metric plateaus for Mamba after 16k.\nThis result represents an awkward reality for existing RNNs. On one hand, the main advantage of RNNs (vs. Transformers) is their linear (vs. quadratic) complexity. This asymptotic advantage is only realized in practice for long context, which according to Figure 3 is after 8k. On the other hand, once context is long enough, existing RNNs such as Mamba struggle to actually take advantage of the extra information being conditioned on.\nThe difficulty with long context is inherent to the very nature of RNN layers: Unlike self-attention, RNN layers have to compress context into a hidden state of fixed size. As a compression heuristic,"}, {"title": "2 Method", "content": "All sequence modeling layers can be viewed from the perspective of storing historic context into a hidden state, as shown in Figure 4. For example, RNN layers \u2013 such as LSTM [33], RWKV [56] and Mamba [26] layers \u2013 compress context into a state of fixed size across time. This compression has two consequences. On one hand, mapping an input token \\(x_t\\) to output token \\(z_t\\) is efficient, because both the update rule and output rule take constant time per token. On the other hand, the performance of RNN layers in long context is limited by the expressive power of its hidden state \\(s_t\\)."}, {"title": "2.1 TTT as updating a hidden state", "content": "The process of parametric learning can be viewed as compressing a massive training set into the weights of a model. Specifically, we know that models trained with self-supervision can capture the underlying structures and relationships behind their training data [48] \u2013 exactly what we need from a compression heuristic.\nLLMs themselves are great examples. Trained with the self-supervised task of next-token prediction, their weights can be viewed as a compressed form of storage for existing knowledge on the internet. By querying LLMs, we can extract knowledge from their weights. More importantly, LLMs often exhibit a deep understanding of the semantic connections among existing knowledge to express new pieces of reasoning [1].\nOur key idea is to use self-supervised learning to compress the historic context \\(x_1,...,x_t\\) into a hidden state \\(s_t\\), by making the context an unlabeled dataset and the state a model. Concretely, the hidden state \\(s_t\\) is now equivalent to \\(W_t\\), the weights of a model \\(f\\), which can be a linear model, a"}, {"title": "2.2 Training a network with TTT layers", "content": "The forward pass of a TTT layer also has a corresponding backward pass. Our forward pass only consists of standard differentiable operators except the gradient operator \\(\\nabla\\). However, \\(\\nabla\\) just maps"}, {"title": "2.3 Learning a self-supervised task for TTT", "content": "Arguably the most important part of TTT is the self-supervised task, because it determines the kind of features that \\(W\\) will learn from the test sequence. So how should we design this task? The final goal of TTT is for \\(z_t = f (x_t; W_t)\\) to perform well on language modeling. Instead of handcrafting a"}, {"title": "2.4 Parallelization with mini-batch TTT", "content": "The naive TTT layer developed so far is already efficient in the number of floating point operations (FLOPs). However, its update rule \\(W_t = W_{t-1} - \\eta \\nabla l(W_{t-1};x_t)\\) cannot be parallelized, because \\(W_t\\) depends on \\(W_{t-1}\\) in two places: before the minus sign and inside \\(\\nabla l\\). Since \\(\\nabla l\\) contains the bulk of the computation, we focus on making this second part parallel.\nWe approach this systems challenge through concepts in the TTT framework. There are many variants of gradient descent (GD). The general update rule of GD can be expressed as:\n\\[W_t = W_{t-1} - \\eta G_t = W_0 - \\eta \\sum_{s=1}^{t} G_s,\\]\nwhere \\(G_t\\) is the descent direction. Note that once we have calculated \\(G_t\\) for \\(t = 1,..., T\\), we can then obtain all the \\(W_t\\)s through a cumsum by the second half of Equation 6. Our naive update rule, known as online gradient descent, uses \\(G_t = \\nabla l(W_{t-1};x_t)\\)."}, {"title": "2.5 Dual form", "content": "The parallelization introduced above is necessary but not sufficient for efficiency in wall-clock time. Modern accelerators specialize in matrix-matrix multiplications, known as matmuls. For example, the NVIDIA A100 GPU contains highly optimized units called TensorCores that can only perform a single operation \u2013 multiplying two matrices each of size 16\u00d716. Without enough of these matmuls, the TensorCores are idle, and most of the potential for the A100 is unrealized.\nUnfortunately, the TTT layer developed so far even with mini-batch still has very few matmuls. Consider the simplest case of \\(l\\), where \\(\\Theta_K = \\Theta_v = \\Theta_Q = I\\), for only the first TTT mini-batch of size \\(b\\). In addition, consider \\(f\\) as a linear model. Copying Equation 3, our loss at time \\(t\\) is:\n\\[l (W_0;x_t) = ||f (x_t; W_0) \u2013 x_t||^2 = ||W_0x_t - x_t||^2.\\]"}, {"title": "2.6 Theoretical equivalences", "content": "In Subsection 2.1, we mentioned that \\(f\\) can be a linear model or a neural network. In Subsection 2.4, we also discussed three variants of the update rule: online GD, batch GD, and mini-batch GD. Each of these 2 \u00d7 3 combinations induces a different instantiation of the TTT layer, as illustrated in Figure 9. We now show that among these induced instantiations, the TTT layer with a linear model and batch GD is equivalent to linear attention [41], a widely known RNN layer.\nTheorem 1. Consider the TTT layer with \\(f(x) = Wx\\) as the inner-loop model, batch gradient descent with \\(\\eta = 1/2\\) as the update rule, and \\(W_0 = 0\\). Then, given the same input sequence \\(x_1,...,x_T\\), the output rule defined in Equation 5 produces the same output sequence \\(z_1,...,z_T\\) as linear attention.\nProof. By definition of \\(l\\) in Equation 4, \\(\\nabla l (W_0; x_t) = -2(\\Theta_vx_t)(\\Theta_kx_t)^T\\). By definition of batch GD in Equation 6:\n\\[W_t = W_{t-1} - \\nabla l (W_0; x_t) = W_0 - \\eta \\sum_{s=1}^{t} \\nabla l (W_0;x_s) = \\sum_{s=1}^{t} (\\Theta_vx_s)(\\Theta_kx_s)^T.\\]\nPlugging \\(W_t\\) into the output rule in Equation 5, we obtain the output token:\n\\[z_t = f (\\Theta_Qx; W_t) = \\sum_{s=1}^{t}(\\Theta_vx_s)(\\Theta_kx_s) (\\Theta_\\rho x_t),\\]\nwhich is the definition of linear attention."}, {"title": "2.7 Implementation details", "content": "Instantiations of \\(f\\). We propose two variants of TTT layers \u2013 TTT-Linear and TTT-MLP, differing only in their instantiations of \\(f\\). For TTT-Linear, \\(f_{lin}(x) = Wx\\), where \\(W\\) is square. For TTT-MLP, \\(f_{MLP}\\) has two layers similar to the MLPs in Transformers. Specifically, the hidden dimension is 4\u00d7 the input dimension, followed by a GELU activation [31]. For better stability during TTT, \\(f\\) always contains a Layer Normalization (LN) and residual connection. That is, \\(f (x) = x + LN(f_{res}(x))\\), where \\(f_{res}\\) can be \\(f_{lin}\\) or \\(f_{MLP}\\).\nLearnable \\(W_0\\). The TTT initialization \\(W_0\\) is shared between all sequences, even though subsequent weights \\(W_1,..., W_T\\) are different for each input sequence. Instead of setting \\(W_0 = 0\\), we can learn it as part of the outer loop. Since outer-loop parameters are always denoted by \\(\\Theta_s\\) instead of \\(W_s\\), we assign an alias \\(\\Theta_{init} = W_0\\). In practice, \\(\\Theta_{init}\\) adds a negligible amount of parameters comparing to the reconstruction views \\(\\theta_K, \\theta_\\rho, \\theta_v\\), because both its input and output are low dimensional. Empirically, we observe that learning \\(W_0\\) significantly improves training stability.\nLearnable \\(\\eta\\). The learning rate is usually the most important hyper-parameter for gradient descent, so we experiment with learning the inner-loop learning rate \\(\\eta\\) in Equation 6 as part of the outer loop. We make \\(\\eta\\) a function of the input token (therefore different across time) for additional flexibility. Concretely, we design \\(\\eta(x) = \\eta_{base} \\sigma(\\Theta_{lr}\\cdot x)\\), where the learnable vector \\(\\Theta_{lr}\\) is an outer-loop parameter, \\(\\sigma\\) is the sigmoid function, and the scalar \\(\\eta_{base}\\) is the base learning rate, set to 1 for TTT-Linear and 0.1 for TTT-MLP. Alternatively, \\(\\eta(x)\\) can also be interpreted as a gate for \\(\\nabla l\\).\nBackbone architecture. The cleanest way to integrate any RNN layer into a larger architecture would be to directly replace self-attention in a Transformer, known in this context as a backbone. However, existing RNNs such as Mamba [26] and Griffin [18] all use a different backbone from Transformers. Most notably, their backbone contains temporal convolutions before the RNN layers, which might help collect local information across time. After experimenting with the Mamba backbone, we find that it also improves perplexity for TTT layers, so we incorporate it into our proposed method. See Figure 16 (in Appendix) for details."}, {"title": "3 Experiments", "content": "We evaluate TTT-Linear and TTT-MLP by comparing with two baselines \u2013 Transformer and Mamba, a modern RNN. Our main codebase is based on EasyLM [25], an open-source project for training and serving LLMs in JAX. All experiments can be reproduced using the publicly available code and datasets provided at the bottom of the first page.\nDatasets. Following the Mamba paper [26], we perform standard experiments with 2k and 8k context lengths on the Pile [24], a popular dataset of documents for training open-source LLMs [9]. However, the Pile contains few sequences of length greater than 8k [19]. To evaluate capabilities in long context, we also experiment with context lengths ranging from 1k to 32k in 2\u00d7 increments, using a subset of the Pile called Books3, which has been widely used to train LLMs in long context [49, 3].\nBackbone architecture. As discussed in Subsection 2.7, Transformer and Mamba use different backbones, and TTT-Linear and TTT-MLP always use the Mamba backbone unless noted otherwise. As an ablation study, Figure 11 and Figure 12 contain TTT layers within the Transformer backbone. When a figure contains both the Transformer backbone and Mamba backbone, we denote them by (T) and (M), respectively.\nProtocols. To ensure fairness to our baselines, we strictly follow the evaluation protocols in the Mamba paper when possible:\nFor each evaluation setting (e.g., dataset, context length, and method), we experiment with four model sizes: 125M, 350M, 760M, and 1.3B parameters. For Mamba, the corresponding sizes are 130M, 370M, 790M, and 1.4B, as Mamba does not follow the Transformer configurations.\nAll models are trained with the Chinchilla recipe described in the Mamba paper and reproduced in our Appendix C. Our Transformer baseline, based on the Llama architecture [73], also follows the baseline in the Mamba paper. As verification, our baselines can reproduce the numbers reported in the Mamba paper in their evaluation settings.\nWe do not experiment with hybrid architectures (e.g. Griffin [18]), because our baselines are not hybrid. While hybrid architectures that use both self-attention and TTT layers may improve performance, they would reduce the clarity of our academic evaluation."}, {"title": "3.1 Short context: the Pile", "content": "From Figure 11, we make a few observations:\nAt 2k context, TTT-Linear (M), Mamba, and Transformer have comparable performance, as the lines mostly overlap. TTT-MLP (M) performs slightly worse under large FLOP budgets. Even though TTT-MLP has better perplexity than TTT-Linear at every model size, the extra cost in FLOPs offsets the advantage.\nAt 8k context, both TTT-Linear (M) and TTT-MLP (M) perform significantly better than Mamba, in contrast to the observation at 2k. Even TTT-MLP (T) with the Transformer backbone performs slightly better than Mamba around 1.3B. A robust phenomenon we observe throughout this paper is that as context length grows longer, the advantage of TTT layers over Mamba widens."}, {"title": "3.2 Long context: Books", "content": "To evaluate capabilities in long context, we experiment with context lengths ranging from 1k to 32k in 2x increments, using a popular subset of the Pile called Books3. The training recipe here is the same as for the Pile, and all experiments for the TTT layers are performed in one training run. From the subset of results in Figure 12, we make a few observations:\nAt 2k context on Books, all the observations from Pile 2k still hold, except that Mamba now performs slightly better than TTT-Linear (whereas their lines roughly overlapped for Pile 2k)."}, {"title": "3.3 Wall-clock time", "content": "LLM training and inference can be decomposed into forward, backward, and generate. Prompt processing during inference, also known as prefill, is the same operation as forward during training, except that the intermediate activations do not need to be stored for backward. Since both forward (during training and inference) and backward can be parallelized, we use the dual form. Generating new tokens, also known as decode, is inherently sequential, so we use the primal form.\nDue to resource constraints, our experiments are written in JAX and run on TPUs. On a v5e-256 TPU pod, the Transformer baseline takes 0.30s per iteration of training at context 2k, while TTT-Linear takes 0.27s per iteration, already 10% faster without any systems optimization. However, Mamba (implemented in PyTorch, Triton, and CUDA) can only run on GPUs, so for fair comparison, we also rewrite our method with preliminary systems optimization to run on GPUs.\nSpecifically, we write a GPU kernel for forward in ThunderKittens [66]. Historically, RNNs have been inefficient during forward and backward due to poor use of parallelism and matmuls. Our goal with the forward kernel is to demonstrate the effectiveness of mini-batch TTT and the dual form for these problems. A kernel for backward should have the same properties in efficiency as forward, but requires more complex logic for manual differentiation, therefore is left for future work.\nThe left panel of Figure 15 shows the latency for batch size 16 of our forward kernel. All models are 1.3B (1.4B for Mamba). Time per token grows linearly for Transformer as context length increases, but stays roughly constant for the other methods. Note that our Transformer baseline is significantly faster that in the Mamba paper, because we use vLLM [46], a state-of-the-art serving system, instead of the HuggingFace Transformer [77].\nIn addition, we write another GPU kernel for generate in Triton [72], and benchmark its speed in the right panel of Figure 15 for batch size 512. Another popular metric for wall-clock time is throughput, which takes into account the potential benefit of being able to use a larger batch size. For completeness, we report the throughput for forward and generate in Figure 20 (in Appendix). All the observations and ordering between methods above still hold for throughput."}, {"title": "4 Related Work", "content": ""}, {"title": "4.1 Modern RNNs", "content": "Mamba is one of the many Structured State-Space Models [27, 21, 57, 18]. The hidden state in these models is a vector, similar to in LSTMs. For TTT-Linear or TTT-MLP, the hidden state is a matrix or two matrices, therefore larger. In Figure 14, we find that TTT layers can take advantage of their larger hidden states to compress more information in long context, where TTT-MLP outperforms TTT-Linear, which in turn outperforms Mamba.\nSimilar to TTT-Linear, RWKV [55, 56], xLSTM [5], and Gated Linear Attention (GLA) [79] also have matrix hidden states, which are inherited from linear attention [41]. Modern RNNs such as GLA use chunk-wise parallelism to improve hardware efficiency, so tokens inside a chunk can be processed with matmuls instead of a cumsum. However, chunk-wise parallelism does not change the expressiveness of the model, since all temporal dependencies are still equivalent to a cumsum.\nIn contrast, mini-batch TTT allows more complex temporal dependencies across mini-batches. Each hidden state \\(W_t\\) depends on previous \\(W_s\\) within its mini-batch still through a cumsum, but depends on \\(W_s\\) in previous mini-batches also through the gradient operator. As illustrated Figure 8, mini-batch TTT enables a trade-off between expressiveness and hardware efficiency, since a smaller batch size \\(b\\) leads to better perplexity at the cost of higher latency. This trade-off is a unique and important feature of TTT. As shown in Table 1, the intermediate batch size \\(b = 16\\) significantly outperforms \\(b = T\\) which is fully cumsum."}, {"title": "4.2 Learning at Test Time", "content": "The idea of learning at test time has a long history in machine learning. One of the earliest versions of this idea is called local learning (Bottou and Vapnik [10]): For each test input, train on its neighbors before making a prediction. This procedure has been effectively applied to models ranging from SVMs [81] to modern LLMs [29].\nAnother early version of learning at test time is called transductive learning [22]. The principle of transduction, as stated by Vladimir Vapnik [74], is to \"... get the answer that you really need, but not a more general one.\" Practical implementations of transductive learning use test data to add constraints to the margin of SVMs [39, 17]. However, transductive learning usually needs multiple test instances to be empirically effective, unlike many instantiations of test-time training, which only need a test single instance (image, video, or natural language sequence) at a time.\nIn computer vision, the idea of learning at test time has been applied for decades to applications such as face detection [38], object detection [53], image super-resolution [65], and 3D reconstruction [50]. More recently, the same idea has also been applied to natural language processing, where it is called dynamic evaluation [44, 45]. The basic approach is to directly finetune a language model on the test sequence, which often comes in the form of a prompt."}, {"title": "4.2.1 Test-Time Training", "content": "The core idea of Test-Time Training (TTT) is that each test instance defines its own learning problem, where this test instance alone is the target of generalization [69]. Concretely, for each test instance \\(x\\), the conventional practice is to predict \\(f(x)\\), using a predictor \\(f\\) that is optimized for all training instances on average. TTT first formulates a learning problem defined by \\(x\\), then trains a model \\(f_x\\) on \\(x\\) (often with \\(f\\) as initialization), and predicts \\(f_x(x)\\)."}, {"title": "4.2.2 Fast Weights", "content": "The general idea of fast weights is to update the parameters of a \u201cfast\u201d model on only the most relevant data, as opposed to the conventional practice of updating a \u201cslow\u201d model on all data [71]. This idea has existed since the 1980s [32]. The most relevant data can be the test instance itself, therefore TTT can be viewed as a special case of fast weights.\nPrior work in fast weights usually avoids forming an explicit learning problem that optimizes some objective on data. For example, the update rule of Hebbian learning and Hopfield networks [35] simply adds \\(xx^T\\) (or some variant thereof) [4] to the fast weights given each input \\(x\\). In contrast, TTT embraces the idea of formulating an explicit learning problem, where the test instance is the target of generalization. Our update rule is also an explicit step of optimization.\nThe idea of fast weight programmers (FWPs) is to update the fast weights with a \u201cslow\u201d model [62]. Our inner-loop weights \\(W\\) can be viewed as \u201cfast\u201d and outer-loop weights \\(\\Theta\\) as \u201cslow\u201d. Therefore, networks containing TTT layers can be viewed as a special case of FWPs [43], similar to how TTT can be viewed as a special case of fast weights. The FWP with the Hebbian update rule above is equivalent to linear attention [60], therefore also to naive TTT-Linear with batch gradient descent.\nThe definition of FWPs is very broad. In fact, all networks with some gating mechanism, such as Transformers with SwiGLU blocks [63], can also be viewed as a special case of FWPs. Recent work has been experimenting with FWPs for language modeling: Irie et al. [37] design \u201cfast\u201d networks with weights produced as output of a \u201cslow\u201d networks. Clark et al. [16] give a Transformer a final layer of fast weights, whose initialization is trained as slow weights. Our contribution relative to existing work on FWPs, again, is formulating an explicit learning problem for the update, which enables us to borrow tools from learning such as mini-batch and LN."}, {"title": "4.3 Learning to Learn", "content": "For decades, researchers have been arguing that learning to learn, also known as meta-learning or bi-level optimization, should be a critical component of intelligence [61, 6, 70, 47]. In prior work such as [2], [20] and [52], the inner loop learns from an entire dataset at a time instead of a sequence, so the outer loop needs a collection of datasets or tasks. In short, the outer loop is \u201cone level above\u201d regular training. Since it is hard to collect millions of datasets, this outer loop is hard to scale.\nIn contrast, for TTT, each sequence itself is a dataset and defines its own generalization problem. The inner loop is \u201cone level below\u201d regular training, so our outer loop is only another solution to the canonical problem of supervised learning, instead of a new problem setting like generalization across datasets. As illustrated in Table 2, our outer loop is \u201cat the same level\u201d as regular training. This makes our outer loop easier to scale."}, {"title": "5 Discussion", "content": "We have reformulated the canonical problem of supervised learning as learning to (learn at test time). Our formulation produces an alternative conceptual framework for building what is traditionally known as network architectures. We summarize our current instantiation in Table 2.\nThe search space for effective instantiations inside this framework is huge, and our paper has only taken a baby step. Fortunately, if our perspective holds, then heuristics from regular training can transfer to test-time training, and search can be efficient. Next we outline some especially promising directions for future work.\nOuter-loop parameterization. There are many other ways to parameterize a family of multi-view reconstruction tasks, or perhaps a more general family of self-supervised tasks. It would be a big coincidence if the first one we have tried turns out to be the best.\nSystems optimization. Our systems optimization in Subsection 3.3 has been preliminary at best, and there are many ways to improve it. In addition, pipeline parallelism through time might allow us to process long sequences of millions of tokens on multiple devices together.\nLonger context and larger models. Constrained by our academic resources, we have not trained with millions or billions in context length, which would also require larger models according to Figure 19. The advantage of TTT layers should become more pronounced in longer context.\nMore ambitious instantiations of f. When context length becomes longer, f would also need to be larger. For video tasks and embodied agents, whose context length can easily scale up to millions or billions, f could be a convolutional neural network.\nMulti-level learning to learn. If f itself is a self-attention layer, then by Theorem 2 it can be interpreted as yet another inner loop nested inside the existing one. In this fashion, we can potentially build many levels of nested learning problems.\nWhy do we study TTT? First a more basic question: Why study AI? For some of us, AI is a playground to probe about the nature of human intelligence. Prior work often tries to model human learning with machine learning, where training is on a shuffled dataset with i.i.d. instances, and inference is on a separate test set. However, humans do not naturally learn with i.i.d. instances or have a train-test split. We believe that human learning has a more promising connection with TTT, our inner loop, whose data is a potentially very long sequence with strong temporal dependencies, and any piece of data can be used for both training and testing. This is why we study TTT."}]}