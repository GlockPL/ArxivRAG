{"title": "Learning to (Learn at Test Time):\nRNNs with Expressive Hidden States", "authors": ["Yu Sun", "Xinhao Li", "Karan Dalal", "Jiarui Xu", "Arjun Vikram", "Genghan Zhang", "Yann Dubois", "Xinlei Chen", "Xiaolong Wang", "Sanmi Koyejo", "Tatsunori Hashimoto", "Carlos Guestrin"], "abstract": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers\nhave linear complexity, but their performance in long context is limited by the expressive power\nof their hidden state. We propose a new class of sequence modeling layers with linear complexity\nand an expressive hidden state. The key idea is to make the hidden state a machine learning model\nitself, and the update rule a step of self-supervised learning. Since the hidden state is updated\nby training even on test sequences, our layers are called Test-Time Training (TTT) layers. We\nconsider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a\ntwo-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters,\ncomparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP\nmatch or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by\nconditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems\noptimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in\nwall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long\ncontext, pointing to a promising direction for future research.", "sections": [{"title": "1 Introduction", "content": "In 2020, the OpenAI scaling law paper (Kaplan et. al [40]) showed that LSTMs (a type of RNN)\ncould not scale similarly to Transformers or effectively use long context. Now, with modern RNNs\nand best practices, we re-evaluate these findings in Figure 2.\nOn the left, we observe that Mamba [26] \u2013 one of the most popular RNNs today \u2013 scales similarly to\na strong Transformer, showing great progress since the LSTMs in 2020. However, on the right, we\nobserve the same issue with Mamba as Kaplan et al. did with LSTMs. Tokens later in a sequence\nshould be easier to predict on average, since they condition on more information. This is indeed the\ncase for Transformer, whose average perplexity at each token index decreases throughout its 32k\ncontext. In contrast, the same metric plateaus for Mamba after 16k.\nThis result represents an awkward reality for existing RNNs. On one hand, the main advantage of\nRNNs (vs. Transformers) is their linear (vs. quadratic) complexity. This asymptotic advantage is\nonly realized in practice for long context, which according to Figure 3 is after 8k. On the other hand,\nonce context is long enough, existing RNNs such as Mamba struggle to actually take advantage of\nthe extra information being conditioned on.\nThe difficulty with long context is inherent to the very nature of RNN layers: Unlike self-attention,\nRNN layers have to compress context into a hidden state of fixed size. As a compression heuristic,"}, {"title": "2 Method", "content": "All sequence modeling layers can be viewed from the perspective of storing historic context into\na hidden state, as shown in Figure 4.1 For example, RNN layers \u2013 such as LSTM [33], RWKV [56]\nand Mamba [26] layers \u2013 compress context into a state of fixed size across time. This compression\nhas two consequences. On one hand, mapping an input token $x_t$ to output token $z_t$ is efficient,\nbecause both the update rule and output rule take constant time per token. On the other hand, the\nperformance of RNN layers in long context is limited by the expressive power of its hidden state $s_t$.\n\u00b9 We define a sequence modeling layer as an autoregressive mapping from one sequence to another."}, {"title": "2.1 TTT as updating a hidden state", "content": "The process of parametric learning can be viewed as compressing a massive training set into the\nweights of a model. Specifically, we know that models trained with self-supervision can capture the\nunderlying structures and relationships behind their training data [48] \u2013 exactly what we need from\na compression heuristic.\nLLMs themselves are great examples. Trained with the self-supervised task of next-token prediction,\ntheir weights can be viewed as a compressed form of storage for existing knowledge on the internet.\nBy querying LLMs, we can extract knowledge from their weights. More importantly, LLMs often\nexhibit a deep understanding of the semantic connections among existing knowledge to express\nnew pieces of reasoning [1].\nOur key idea is to use self-supervised learning to compress the historic context $x_1,...,x_t$ into a\nhidden state $s_t$, by making the context an unlabeled dataset and the state a model. Concretely, the\nhidden state $s_t$ is now equivalent to $W_t$, the weights of a model $f$, which can be a linear model, a"}, {"title": "2.2 Training a network with TTT layers", "content": "The forward pass of a TTT layer also has a corresponding backward pass. Our forward pass only\nconsists of standard differentiable operators except the gradient operator \u2207. However, \u2207 just maps\n\u00b2 For now, consider $W_0 = 0$. We will discuss more sophisticated techniques for initializing $W$ in Subsection 2.7.\n\u00b3 In past experiments, we have also tried adding another model $g$ (decoder) after $f$ (encoder), such that the reconstruction\nis produced by $g \\circ f$ instead of only $f$ itself. While this heftier design did slightly improve results, it made overall training\nless stable and added significant computational cost. Therefore we focus on the encoder-only design."}, {"title": "2.3 Learning a self-supervised task for TTT", "content": "Arguably the most important part of TTT is the self-supervised task, because it determines the kind\nof features that $W$ will learn from the test sequence. So how should we design this task? The final\ngoal of TTT is for $z_t = f(x_t; W_t)$ to perform well on language modeling. Instead of handcrafting a"}, {"title": "2.4 Parallelization with mini-batch TTT", "content": "The naive TTT layer developed so far is already efficient in the number of floating point operations\n(FLOPs). However, its update rule $W_t = W_{t-1} \u2013 \u03b7\u2207l(W_{t-1};x_t)$ cannot be parallelized, because $W_t$\ndepends on $W_{t-1}$ in two places: before the minus sign and inside \u2207l. Since \u2207l contains the bulk of\nthe computation, we focus on making this second part parallel.\nWe approach this systems challenge through concepts in the TTT framework. There are many\nvariants of gradient descent (GD). The general update rule of GD can be expressed as:\n$W_t = W_{t-1} - \u03b7 G_t = W_0 - \u03b7 \\sum_{s=1}^t G_s$,\nwhere $G_t$ is the descent direction. Note that once we have calculated $G_t$ for $t = 1,..., T, we can then\nobtain all the $W_t$s through a cumsum by the second half of Equation 6. Our naive update rule, known\nas online gradient descent, uses $G_t = \u2207l(W_{t-1};x_t)$.\n\u2074 The subscript K hints at a connection to self-attention, as we will establish in Subsection 2.6."}, {"title": "2.5 Dual form", "content": "The parallelization introduced above is necessary but not sufficient for efficiency in wall-clock time.\nModern accelerators specialize in matrix-matrix multiplications, known as matmuls. For example,\nthe NVIDIA A100 GPU contains highly optimized units called TensorCores that can only perform a\nsingle operation \u2013 multiplying two matrices each of size 16\u00d716. Without enough of these matmuls,\nthe TensorCores are idle, and most of the potential for the A100 is unrealized.\nUnfortunately, the TTT layer developed so far even with mini-batch still has very few matmuls.\nConsider the simplest case of $l$, where $\u03b8_K = \u03b8_V = \u03b8_Q = I$, for only the first TTT mini-batch of size b.\nIn addition, consider $f$ as a linear model. Copying Equation 3, our loss at time t is:\n$l (W_0;x_t) = ||f (x_t; W_0) \u2013 x_t||^2 = ||W_0x_t - x_t||^2$.\n\u2075 In theory, b can potentially be too small such that the variance between mini-batches is too high, hurting optimization.\nHowever, we have not observed such an effect in practice.\n\u2076 For Figure 8, we use a single TTT layer in TTT-Linear 1.3B, implemented in pure PyTorch. Our fused kernel significantly\nimproves time efficiency, but makes it difficult to cleanly decompose the time for computing $W_t$ vs. $z_1,..., z_b$."}, {"title": "2.6 Theoretical equivalences", "content": "In Subsection 2.1, we mentioned that $f$ can be a linear model or a neural network. In Subsection 2.4,\nwe also discussed three variants of the update rule: online GD, batch GD, and mini-batch GD.\nEach of these 2 \u00d7 3 combinations induces a different instantiation of the TTT layer, as illustrated in\nFigure 9. We now show that among these induced instantiations, the TTT layer with a linear model\nand batch GD is equivalent to linear attention [41], a widely known RNN layer.\u2077\nTheorem 1. Consider the TTT layer with $f(x) = Wx$ as the inner-loop model, batch gradient descent with\n$\u03b7 = 1/2$ as the update rule, and $W_0 = 0$. Then, given the same input sequence $x_1,...,x_T$, the output rule\ndefined in Equation 5 produces the same output sequence $z_1,...,z_T$ as linear attention.\nProof. By definition of $l$ in Equation 4, $\u2207l (W_0; x_t) = -2(\u03b8_Vx_t)(\u03b8_Kx_t)^T$. By definition of batch GD in\nEquation 6:\n$W_t = W_{t-1} - \u2207l (W_0; x_t) = W_0 - \u03b7 \\sum_{s=1}^t \u2207l (W_0;x_s) = \\sum_{s=1}^t(\u03b8_Vx_s)(\u03b8_Kx_s)^T$.\nPlugging $W_t$ into the output rule in Equation 5, we obtain the output token:\n$z_t = f (\u03b8_Qx_t; W_t) = \\sum_{s=1}^t(\u03b8_Vx_s)(\u03b8_Kx_s)^T (\u03b8_Qx_t)$,\nwhich is the definition of linear attention.\n\u2077 In a nutshell, linear attention [41] is simply self-attention without the softmax. Recall the definition of self-attention:\n$z_t = Vts\\oftmax (K_tq_t)$. Without softmax, this becomes $z_t = Vt (K_tq_t) = \\sum_{s=1}^t v_sq_tk_t$, which is the simplest formulation of\nlinear attention. Similar to other RNN layers, it can be written in a recurrent form, where $ \\sum_{s=1}^t v_sq_t$ is the hidden state.\nSince $ \\sum_{s=1}^t v_sq_t$ can be computed in a cumsum for every $t = 1,..., T$, linear attention also has linear complexity w.r.t. T."}, {"title": "2.7 Implementation details", "content": "Instantiations of $f$. We propose two variants of TTT layers \u2013 TTT-Linear and TTT-MLP, differing\nonly in their instantiations of $f$. For TTT-Linear, $f_{lin}(x) = Wx$, where $W$ is square. For TTT-MLP,\n$f_{MLP}$ has two layers similar to the MLPs in Transformers. Specifically, the hidden dimension is 4x\nthe input dimension, followed by a GELU activation [31]. For better stability during TTT, $f$ always\ncontains a Layer Normalization (LN) and residual connection. That is, $f (x) = x + LN(f_{res}(x))$, where\n$f_{res}$ can be $f_{lin}$ or $f_{MLP}$.\nLearnable $W_0$. The TTT initialization $W_0$ is shared between all sequences, even though subsequent\nweights $W_1,..., W_T$ are different for each input sequence. Instead of setting $W_0 = 0$, we can learn it\nas part of the outer loop. Since outer-loop parameters are always denoted by $\u03b8$s instead of $W$s, we\nassign an alias $\u03b8_{init} = W_0$. In practice, $\u03b8_{init}$ adds a negligible amount of parameters comparing to the\nreconstruction views $\u03b8_K, \u03b8_P, \u03b8_V$, because both its input and output are low dimensional. Empirically,\nwe observe that learning $W_0$ significantly improves training stability.\nLearnable $\u03b7$. The learning rate is usually the most important hyper-parameter for gradient descent,\nso we experiment with learning the inner-loop learning rate $\u03b7$ in Equation 6 as part of the outer loop.\nWe make $\u03b7$ a function of the input token (therefore different across time) for additional flexibility.\nConcretely, we design $\u03b7(x) = \u03b7_{base} \u03c3(\u03b8_{lr}\u00b7x)$, where the learnable vector $\u03b8_{lr}$ is an outer-loop parameter,\n$\u03c3$ is the sigmoid function, and the scalar $\u03b7_{base}$ is the base learning rate, set to 1 for TTT-Linear and\n0.1 for TTT-MLP. Alternatively, $\u03b7(x)$ can also be interpreted as a gate for $\u2207l$.\nBackbone architecture. The cleanest way to integrate any RNN layer into a larger architecture\nwould be to directly replace self-attention in a Transformer, known in this context as a backbone.\nHowever, existing RNNs such as Mamba [26] and Griffin [18] all use a different backbone from\nTransformers. Most notably, their backbone contains temporal convolutions before the RNN layers,\nwhich might help collect local information across time. After experimenting with the Mamba\nbackbone, we find that it also improves perplexity for TTT layers, so we incorporate it into our\nproposed method."}, {"title": "3 Experiments", "content": "We evaluate TTT-Linear and TTT-MLP by comparing with two baselines \u2013 Transformer and Mamba,\na modern RNN. Our main codebase is based on EasyLM [25], an open-source project for training\nand serving LLMs in JAX. All experiments can be reproduced using the publicly available code and\ndatasets provided at the bottom of the first page.\nDatasets. Following the Mamba paper [26], we perform standard experiments with 2k and 8k\ncontext lengths on the Pile [24], a popular dataset of documents for training open-source LLMs [9].\nHowever, the Pile contains few sequences of length greater than 8k [19]. To evaluate capabilities in\nlong context, we also experiment with context lengths ranging from 1k to 32k in 2\u00d7 increments, using\na subset of the Pile called Books3, which has been widely used to train LLMs in long context [49, 3].\nBackbone architecture. As discussed in Subsection 2.7, Transformer and Mamba use different\nbackbones, and TTT-Linear and TTT-MLP always use the Mamba backbone unless noted otherwise.\nAs an ablation study, Figure 11 and Figure 12 contain TTT layers within the Transformer backbone.\nWhen a figure contains both the Transformer backbone and Mamba backbone, we denote them by\n(T) and (M), respectively.\nProtocols. To ensure fairness to our baselines, we strictly follow the evaluation protocols in the\nMamba paper when possible:\n\u2022 For each evaluation setting (e.g., dataset, context length, and method), we experiment with four\nmodel sizes: 125M, 350M, 760M, and 1.3B parameters. For Mamba, the corresponding sizes are\n130M, 370M, 790M, and 1.4B, as Mamba does not follow the Transformer configurations.\n\u2022 All models are trained with the Chinchilla recipe described in the Mamba paper and reproduced\nin our Appendix C. Our Transformer baseline, based on the Llama architecture [73], also follows\nthe baseline in the Mamba paper. As verification, our baselines can reproduce the numbers\nreported in the Mamba paper in their evaluation settings.\u2079\n\u2022 We do not experiment with hybrid architectures (e.g. Griffin [18]), because our baselines are\nnot hybrid. While hybrid architectures that use both self-attention and TTT layers may improve\nperformance, they would reduce the clarity of our academic evaluation."}, {"title": "3.1 Short context: the Pile", "content": "From Figure 11, we make a few observations:\n\u2022 At 2k context, TTT-Linear (M), Mamba, and Transformer have comparable performance, as the\nlines mostly overlap. TTT-MLP (M) performs slightly worse under large FLOP budgets. Even\nthough TTT-MLP has better perplexity than TTT-Linear at every model size, the extra cost in\nFLOPs offsets the advantage.\n\u2022 At 8k context, both TTT-Linear (M) and TTT-MLP (M) perform significantly better than Mamba,\nin contrast to the observation at 2k. Even TTT-MLP (T) with the Transformer backbone performs\nslightly better than Mamba around 1.3B. A robust phenomenon we observe throughout this paper\nis that as context length grows longer, the advantage of TTT layers over Mamba widens.\n\u2079 The Chinchilla paper is another highly influential study of empirical scaling laws [34]. From large-scale experiments\nwith many hyper-parameters, they observe that the compute-optimal models follow a particular training recipe. We only\nfollow the Chinchilla recipe used in the Mamba paper, which may be slightly different from the original recipe in [34]."}, {"title": "3.2 Long context: Books", "content": "To evaluate capabilities in long context, we experiment with context lengths ranging from 1k to 32k\nin 2x increments, using a popular subset of the Pile called Books3. The training recipe here is the\nsame as for the Pile, and all experiments for the TTT layers are performed in one training run.\u00b9\u2070\nFrom the subset of results in Figure 12, we make a few observations:\n\u2022 At 2k context on Books, all the observations from Pile 2k still hold, except that Mamba now\nperforms slightly better than TTT-Linear (whereas their lines roughly overlapped for Pile 2k).\n\u00b9\u2070 Following the Mamba paper, we always use 0.5M tokens per training batch regardless of context length. That means\nfor context length T we have 0.5M/T sequences per batch (assume divisible)."}, {"title": "3.3 Wall-clock time", "content": "LLM training and inference can be decomposed into forward, backward, and generate. Prompt\nprocessing during inference, also known as prefill, is the same operation as forward during training,\nexcept that the intermediate activations do not need to be stored for backward. Since both forward\n(during training and inference) and backward can be parallelized, we use the dual form. Generating\nnew tokens, also known as decode, is inherently sequential, so we use the primal form.\nDue to resource constraints, our experiments are written in JAX and run on TPUs. On a v5e-256 TPU\npod, the Transformer baseline takes 0.30s per iteration of training at context 2k, while TTT-Linear\ntakes 0.27s per iteration, already 10% faster without any systems optimization. However, Mamba\n(implemented in PyTorch, Triton, and CUDA) can only run on GPUs, so for fair comparison, we also\nrewrite our method with preliminary systems optimization to run on GPUs.\nSpecifically, we write a GPU kernel for forward in ThunderKittens [66]. Historically, RNNs have\nbeen inefficient during forward and backward due to poor use of parallelism and matmuls. Our goal\nwith the forward kernel is to demonstrate the effectiveness of mini-batch TTT and the dual form for\nthese problems. A kernel for backward should have the same properties in efficiency as forward,\nbut requires more complex logic for manual differentiation, therefore is left for future work.\nThe left panel of Figure 15 shows the latency for batch size 16 of our forward kernel. All models\nare 1.3B (1.4B for Mamba). Time per token grows linearly for Transformer as context length\nincreases, but stays roughly constant for the other methods.\u00b9\u00b9 Note that our Transformer baseline is\nsignificantly faster that in the Mamba paper, because we use vLLM [46], a state-of-the-art serving\nsystem, instead of the HuggingFace Transformer [77].\nIn addition, we write another GPU kernel for generate in Triton [72], and benchmark its speed\nin the right panel of Figure 15 for batch size 512. Another popular metric for wall-clock time is\nthroughput, which takes into account the potential benefit of being able to use a larger batch size.\u00b9\u00b2\nAll the observations and ordering between methods above still hold for throughput.\n\u00b9\u00b9 We observe that forward latency of the network increases slightly for TTT-Linear, TTT-MLP, and Mamba, even though\nlatency of each sequence modeling layer alone stays constant. Consider the operation OX, where O is d x d and X is d x T.\nIts latency (normalized over T) is expected to be constant, but in practice grows slightly with T. One possible cause of this\nphenomenon is the GPU throttling after T gets very large [30].\n\u00b9\u00b2 To calculate throughput for each method, we increase its batch size in 2\u00d7 increments until GPU runs out of memory,\nmeasure the tokens per second for every batch size, and select the highest."}, {"title": "4 Related Work", "content": "4.1 Modern RNNs\nMamba is one of the many Structured State-Space Models [27, 21, 57, 18]. The hidden state in these\nmodels is a vector, similar to in LSTMs. For TTT-Linear or TTT-MLP, the hidden state is a matrix\nor two matrices, therefore larger. In Figure 14, we find that TTT layers can take advantage of their\nlarger hidden states to compress more information in long context, where TTT-MLP outperforms\nTTT-Linear, which in turn outperforms Mamba.\nSimilar to TTT-Linear, RWKV [55, 56], xLSTM [5], and Gated Linear Attention (GLA) [79] also\nhave matrix hidden states, which are inherited from linear attention [41]. Modern RNNs such as\nGLA use chunk-wise parallelism to improve hardware efficiency, so tokens inside a chunk can be\nprocessed with matmuls instead of a cumsum. However, chunk-wise parallelism does not change the\nexpressiveness of the model, since all temporal dependencies are still equivalent to a cumsum.\nIn contrast, mini-batch TTT allows more complex temporal dependencies across mini-batches.\nEach hidden state $W_t$ depends on previous $W_s$ within its mini-batch still through a cumsum, but\ndepends on $W_s$ in previous mini-batches also through the gradient operator. As illustrated Figure 8,\nmini-batch TTT enables a trade-off between expressiveness and hardware efficiency, since a smaller\nbatch size $b$ leads to better perplexity at the cost of higher latency. This trade-off is a unique and\nimportant feature of TTT. As shown in Table 1, the intermediate batch size $b = 16$ significantly\noutperforms $b = T$ which is fully cumsum."}, {"title": "4.2 Learning at Test Time", "content": "The idea of learning at test time has a long history in machine learning. One of the earliest versions of\nthis idea is called local learning (Bottou and Vapnik [10]): For each test input, train on its neighbors\nbefore making a prediction. This procedure has been effectively applied to models ranging from\nSVMs [81] to modern LLMs [29].\nAnother early version of learning at test time is called transductive learning [22]. The principle of\ntransduction, as stated by Vladimir Vapnik [74], is to \"... get the answer that you really need, but\nnot a more general one.\" Practical implementations of transductive learning use test data to add\nconstraints to the margin of SVMs [39, 17]. However, transductive learning usually needs multiple\ntest instances to be empirically effective, unlike many instantiations of test-time training, which\nonly need a test single instance (image, video, or natural language sequence) at a time.\nIn computer vision, the idea of learning at test time has been applied for decades to applications such\nas face detection [38], object detection [53], image super-resolution [65], and 3D reconstruction [50].\nMore recently, the same idea has also been applied to natural language processing, where it is called\ndynamic evaluation [44, 45]. The basic approach is to directly finetune a language model on the test\nsequence, which often comes in the form of a prompt."}, {"title": "4.2.1 Test-Time Training", "content": "The core idea of Test-Time Training (TTT) is that each test instance defines its own learning problem,\nwhere this test instance alone is the target of generalization [69]. Concretely, for each test instance $x$,\nthe conventional practice is to predict $f(x)$, using a predictor $f$ that is optimized for all training\ninstances on average. TTT first formulates a learning problem defined by $x$, then trains a model $f_x$\non $x$ (often with $f$ as initialization), and predicts $f_x(x)$."}, {"title": "4.2.2 Fast Weights", "content": "The general idea of fast weights is to update the parameters of a \"fast\u201d model on only the most\nrelevant data, as opposed to the conventional practice of updating a \u201cslow\u201d model on all data [71].\nThis idea has existed since the 1980s [32]. The most relevant data can be the test instance itself,\ntherefore TTT can be viewed as a special case of fast weights.\nPrior work in fast weights usually avoids forming an explicit learning problem that optimizes some\nobjective on data. For example, the update rule of Hebbian learning and Hopfield networks [35]\nsimply adds $xx^T$ (or some variant thereof) [4] to the fast weights given each input $x$. In contrast,\nTTT embraces the idea of formulating an explicit learning problem, where the test instance is the\ntarget of generalization. Our update rule is also an explicit step of optimization.\nThe idea of fast weight programmers (FWPs) is to update the fast weights with a \u201cslow\u201d model [62].\nOur inner-loop weights W can be viewed as \"fast\u201d and outer-loop weights $\u03b8$ as \u201cslow\u201d. Therefore,\nnetworks containing TTT layers can be viewed as a special case of FWPs [43], similar to how TTT\ncan be viewed as a special case of fast weights. The FWP with the Hebbian update rule above is\nequivalent to linear attention [60], therefore also to naive TTT-Linear with batch gradient descent.\nThe definition of FWPs is very broad. In fact, all networks with some gating mechanism, such as\nTransformers with SwiGLU blocks [63], can also be viewed as a special case of FWPs\u00b9\u00b3. Recent work\nhas been experimenting with FWPs for language modeling: Irie et al. [37] design \u201cfast\u201d networks\nwith weights produced as output of a \u201cslow\u201d networks. Clark et al. [16] give a Transformer a final\nlayer of fast weights, whose initialization is trained as slow weights. Our contribution relative to\nexisting work on FWPs, again, is formulating an explicit learning problem for the update, which\nenables us to borrow tools from learning such as mini-batch and LN."}, {"title": "4.3 Learning to Learn", "content": "For decades, researchers have been arguing that learning to learn, also known as meta-learning or\nbi-level optimization, should be a critical component of intelligence [61, 6, 70, 47]. In prior work\nsuch as [2], [20] and [52], the inner loop learns from an entire dataset at a time instead of a sequence,\nso the outer loop needs a collection of datasets or tasks. In short, the outer loop is \u201cone level above\"\nregular training. Since it is hard to collect millions of datasets, this outer loop is hard to scale.\nIn contrast, for TTT, each sequence itself is a dataset and defines its own generalization problem.\nThe inner loop is \u201cone level below\u201d regular training, so our outer loop is only another solution to\nthe canonical problem of supervised learning, instead of a new problem setting like generalization\nacross datasets. As illustrated in Table 2, our outer loop is \u201cat the same level\u201d as regular training.\nThis makes our outer loop easier to scale.\n\u00b9\u00b3 Consider a simple gate $z = \u03c3(\u03b8x) \\odot (\u03b8'x)$, where x is the input, z is the output, O and O' are learnable weight matrices,\n$\\odot$ is element-wise multiplication, and o is the sigmoid function. A well known interpretation is to view $W = diag(\u03b8'x)$ as the\nfast weights controlled by slow weights $\u03b8'$, then equivalently, $z = Wo(Ox)$ is simply a two-layer MLP with fast weights [26]."}, {"title": "5 Discussion", "content": "We have reformulated the canonical problem of supervised learning as learning to (learn at test time).\nOur formulation produces an alternative conceptual framework for building what is traditionally\nknown as network architectures. We summarize our current instantiation in Table 2.\nThe search space for effective instantiations inside this framework is huge, and our paper has only\ntaken a baby step. Fortunately, if our perspective holds, then heuristics from regular training can\ntransfer to test-time training, and search can be efficient. Next we outline some especially promising\ndirections for future work.\n\u2022 Outer-loop parameterization. There are many other ways to parameterize a family of multi-view\nreconstruction tasks, or perhaps a more general family of self-supervised tasks. It would be a big\ncoincidence if the first one we have tried turns out to be the best.\n\u2022 Systems optimization. Our systems optimization in Subsection 3.3 has been preliminary at best,\nand there are many ways to improve it. In addition, pipeline parallelism through time might\nallow us to process long sequences of millions of tokens on multiple devices together.\n\u2022 Longer context and larger models. Constrained by our academic resources, we have not trained\nwith millions or billions in context length, which would also require larger models according to\nFigure 19. The advantage of TTT layers should become more pronounced in longer context.\n\u2022 More ambitious instantiations of $f$. When context length becomes longer, $f$ would also need\nto be larger. For video tasks and embodied agents, whose context length can easily scale up to\nmillions or billions, $f$ could be a convolutional neural network.\n\u2022 Multi-level learning to learn. If $f$ itself is a self-attention layer, then by Theorem 2 it can be\ninterpreted as yet another inner loop nested inside the existing one. In this fashion, we can\npotentially build many levels of nested learning problems.\nWhy do we study TTT? First a more basic question: Why study AI? For some of us, AI is a playground\nto probe about the nature of human intelligence. Prior work often tries to model human learning\nwith machine learning, where training is on a shuffled dataset with i.i.d. instances, and inference\nis on a separate test set. However, humans do not naturally learn with i.i.d. instances or have a\ntrain-test split. We believe that human learning has a more promising connection with TTT, our\ninner loop, whose data is a potentially very long sequence with strong temporal dependencies, and\nany piece of data can be used for both training and testing. This is why we study TTT."}]}