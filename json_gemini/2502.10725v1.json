{"title": "PropNet: a White-Box and Human-Like Network for Sentence Representation", "authors": ["Fei Yang"], "abstract": "Transformer-based embedding methods have dominated the field of sentence representation in recent years. Although they have achieved remarkable performance on NLP missions, such as semantic textual similarity (STS) tasks, their black-box nature and large-data-driven training style have raised concerns, including issues related to bias, trust, and safety. Many efforts have been made to improve the interpretability of embedding models, but these problems have not been fundamentally resolved. To achieve inherent interpretability, we propose a purely white-box and human-like sentence representation network, PropNet. Inspired by findings from cognitive science, PropNet constructs a hierarchical network based on the propositions contained in a sentence. While experiments indicate that PropNet has a significant gap compared to state-of-the-art (SOTA) embedding models in STS tasks, case studies reveal substantial room for improvement. Additionally, PropNet enables us to analyze and understand the human cognitive processes underlying STS benchmarks.", "sections": [{"title": "Introduction", "content": "Sentence representation research has been carried out for over half a century, evolving from regular expressions [20], statistical encoding methods [24; 29; 43; 26] to neural network embeddings [35; 9; 28; 27; 18; 40; 19; 32]. In recent years, embedding models have dominated the research community, and also have achieved significant success in industry applications. However, their black-box nature and reliance on large-scale data-driven training raise concerns about bias [10; 12; 11; 37], trust [41; 42], safety [7; 39] and other issues [8]. Researchers have attempted to improve the interpretability of embedding models [25; 33] and remove their biases [23; 22], but these problems remain fundamentally unsolved.\nIn the cognitive science community, the question of how our brains represent a sentence has drawn a great deal of research interest. When humans read and comprehend a sentence, they decompose it into propositions. These propositions are simple sentences with no more than one verb. The meanings of these propositions are stored in long-term memory as a hierarchical network, which can be recalled for use in downstream tasks [31; 16]. The brain's verbal short-term memory possesses a specialized subsystem. This subsystem is responsible for splitting propositions and handling the syntactic features of each proposition as the eyes sweep across the text[13]. Moreover, the cognitive load in sentence comprehension is primarily attributed to syntactic complexity, such as the number of verbs, rather than superficial length, such as the addition of adjectives and other modifiers [21]. This implies proposition is a processing unit for sentence comprehension. Inspired by these findings, in this work, we propose a purely white-box and human-like sentence representation network named PropNet."}, {"title": "Relative Work", "content": "Text representation approaches could date back to rule-based regular expressions for search engines and question-answering systems [20]. Feature engineering is implemented heavily to develop expressions for pattern matching. This approach is inherently explainable, as the expressions consist of readable strings. However, it is pretty rigid and shallow, without a deep understanding of semantics.\nStatistical encoding methods compute word frequencies and represent text as discrete or continuous vectors. Typical methods include One Hot Embedding (OHE), Bag of Words (BOW) [24], N-grams [29], Normalized Term Frequency (NTF) [43] and TF-IDF [26]. These methods are straightforward to implement since no feature engineering is required. They are also explainable, as each number in the representation vectors has a clear meaning. However, since word frequency cannot handle semantic issues, like synonyms and antonyms, or word relationships, these methods only provide shallow semantic analysis.\nNeural network embedding approaches represent words as vectors in a continuous vector space. Representative methods include Word2Vec [35], FastText [9; 28; 27], Glove [38], BERT [18], SBERT [40], SimCSE [19] and AoE [32]. They train neural networks on an extremely large corpus, using different Language Modeling (LM) tasks as the objective. Since embeddings are automatically computed by backpropagation, feature engineering is completely eliminated. In addition, the embeddings provide deep semantic understanding of the text, as they capture the semantic aspects of language due to LM tasks as the training goal.\nEmbedding methods are considered as black-box algorithms because the embedding vectors are generally unexplainable. This characteristic surreptitiously encodes social biases contained in the training corpus [10; 12; 11; 37], and also raises concerns about trust [41; 42], safety [7; 39] and other issues [8]. Numerous efforts have been made to improve interpretability [25; 33] and remove biases [23; 22], but these problems have not been fundamentally solved.\nAll the aforementioned methods are not human-like, as their representation formats are either strings or vectors."}, {"title": "Methods", "content": "The PropNet model comprises four key components: splitting, parsing, representing, and merging. Specifically, the splitting and parsing processes correspond to activities occurring in short-term memory, whereas the representing and merging processes align with those in long-term memory. Subsequently, we introduce a method for comparing two sentence representations, which generates a difference vector as output.\nThe following symbols are used to represent dif-"}, {"title": "Splitting", "content": "For Propo and Prop1, splitting is unnecessary. At this stage, the input sentence itself serves as the output proposition. For Prop2, split the input sentence into two Prop1 propositions.  These rules extract the verb token of the subordinate clause. Then a subordinate proposition is generated by iterating over the verb token's subtree provided by spaCy parsing tool 1. For instance, in the sentence \"She thinks this is a good idea\", the splitting rules identify \u201cis\u201d as the verb token for the subordinate clause. The token \"is\" owns a subtree \"this is a good idea\" by dependency parsing.\nWe set clause identifiers to record relations between propositions. In the above example, the main"}, {"title": "Parsing", "content": "This phase extracts key tokens from a proposition with respect to specific dimensions. The intuition of designing these dimensions is that we require they can describe the meaning of a single action that happens in the real world, which is the key to understand the meaning of a complex sentence consisting of a series of actions. The action type in a proposition can be physical, mental or social [44]. Therefore, eight critical dimensions related to the verb of a proposition are considered: Action, Subject, Object, Where, Auxiliary_Object, Goal, Reason, Source. The extracted tokens under each dimension are either"}, {"title": "Representing", "content": "As brain's long-term memory stores information as a hierarchical network [16; 6; 30], PropNet represents a proposition using a six-level hierarchical network.  The network comprises three types of nodes: evolutionary nodes, developmental nodes and instance nodes which are represented as red, orange and green colors, respectively. In designing the network, we incorporate ideas similar to Nature Design and Nurture Beliefs which are outlined in [44]. Evolutionary nodes correspond to Nature Design, which reflects human nature and is derived from heredity. They are denoted with a prefix \u201c#\u201d, like #action. Each dimension in the parsing phase has a corresponding evolutionary node. Developmental nodes correspond to Nurture Beliefs, which are learned from experience, and are named with a prefix \u201c_\u201d, such as_young. Each word in a sentence corresponds to an orange developmental node, which is pointed by a green instance node.\nInstance nodes play a crucial role in preserving"}, {"title": "Merging", "content": "When the main proposition and the subordinate proposition of a Prop2 sentence are represented separately, we need to combine them to convey a complete and coherent meaning. We put forward three merging strategies, which are enumerated in . If the subordinate proposition is a relative clause, a different merging strategy is adopted, which is presented in Appendix A.2.\nFor a Prop3+ sentence, the merging process corresponds to the upward backtracking process of splitting, illustrated in Figure 1. The same strategies are applied as for Prop2. Appendix A.1"}, {"title": "Comparison of Two Sentence Representations", "content": "Given the PropNets of a pair of sentences, a natural consideration is how to compare the differences between the two networks. However, since the PropNet of a complex sentence can be extensive, containing a large number of nodes and edges, directly comparing two PropNets poses a significant challenge due to computational complexity. A more viable computational approach involves comparing the disparities in the representations of the corresponding propositions between the two sentences, and then aggregating these disparities into an overall signature difference. Thus, we initially introduce the method for comparing the differences between two propositions, which is previously presented as pair type P1-. Next, based on this foundation, we discuss the difference-construction methods for more complex pair types P2 and P3+."}, {"title": "Pair Type P1-", "content": "The comparison dimensions are formed by the Cartesian product of {#subject, #object, #where, #aux_obj, #goal, #source, #reason} and {1, #attr, #part_of}, further extended by {#action, #action #subject|#where, #action #object|#where}. For example, the dimension #action|#subject|#attr represents the attribute of an entity or concept that serves"}, {"title": "Pair Type P2", "content": "Assume the first sentence s1 of the pair being compared can be split into two propositions, pp1 and pp2. If the second sentence s2 is Propo or Prop1, the following alignment process is applied. If two propositions share identical subjects and actions, they are considered to have a significant overlap. This criterion can be evaluated by examining the specific elements #action and #action|#subject of the difference vector. For a fair comparison, we align s2 to the proposition that exhibits a significant overlap with it. Subsequently, we utilize the difference vector associated with this alignment. This vector is then padded with 2 until it attains a length twice that of the original, result-"}, {"title": "Pair Type P3+", "content": "As a Prop3+ sentence can contain multiple propositions, we estrict our consideration to its first four propositions in the order of appearance. Therefore, the difference vector v3 for P3+ has a size that is twice the length of V2 for P2. We calculate the overlap degree as the number of overlapping words between two propositions. The alignment strategy involves iterating through all the propositions of the first and second sentences, and then choose the pair with the maximum overlap degree. After that, we remove the selected propositions from the list and continue to apply the aligning strategy to the remaining propositions until the maximum overlap degree is zero.\nLet's exemplify the above process, considering the pair: \"Three men are jumping off a wall\" and \"Three young men run, jump, and kick off of a Coke machine\". The second sentence consists of three propositions: \"three young men run\u201d, \u201cthree young men jump\", and \"three young men kick off of a Coke machine\". After the alignment process, its proposition list is reordered to \u201cthree young men jump\", \"three young men run\u201d, \u201cthree young men kick off of a Coke machine\". The first sentence is compared with \"three young men jump\", resulting a difference vector v1. Since \"three young men run\" and \"three young men kick off of a Coke machine\" have no counterparts, we concatenate v1 with two vectors of the same length as v1, padded with 2. Finally, the resulting vector is padded with 0 until it reaches the required length."}, {"title": "Experiments", "content": "To assess the performance gap between PropNet and mainstream approaches, PropNet is compared with widely used embedding methods on semantic textual similarity (STS) tasks."}, {"title": "Comparison with Embedding Models", "content": "Baselines. Given that our method is supervised, widely used supervised embedding methods are selected as baselines: InferSent [17], USE [15], SBERT [40], SimCSE [19], and AoE [32].\nDatasets. We conduct experiments on three benchmark datasets: STS 2012-2016 [4; 5; 2; 1; 3], STS-B [14], and Sick-R [34].\nEvaluation Metrics. We employ Spearman's rank correlation coefficient (\u03c1) multiplied by 100 to measure the alignment between predicted and ground-truth scores.\nImplementation Details. The experiment employs a comparison module which consists of two components, (1) the calculation of the difference vector and (2) prediction using CART models.  After computing the difference vectors using the approach introduced in Section 3.5 for all pairs within the benchmark datesets, two CART models are trained on these vectors: one for short pairs P1-/P2 and another for long pairs P3+. These models are denoted as CART (P1-/P2) and CART(P3+) repectively. We call the API provided by scikit-learn 4 for training. Since STS 2012-2016 and STS-B share the same rules for ground scores, training is implemented only on the train set of STS-B to learn these rules. Similarly, for SICK-R, two CART models are trained on its training set.\nPredictions for short pairs and long pairs in the test set are concatenated before computing Spearman's correlation. The random seed is fixed as 0, and the minimum number of samples at a leaf node is set to 10 for all trainings.\nResults. In Table 5, a significant gap is observed between PropNet and the latest embedding model, AoE, which scores above 80. This indicates the efficacy of big-data-trained embeddings in comprehending text semantics. This table also demonstrates that embedding models have undergone notable development over the last ten years,"}, {"title": "Investigation of Genres", "content": "In this section, we examine the performance of PropNet across different genres. The number of verbs is also considered as an important influencing factor. We anticipate that predicting accurately using our comparison module becomes more challenging for complex sentences.\nDataset. We utilize the STS-B test set 5 along with its predicted scores for analysis. The test set is split into three parts according to its field genre, which has three values: main-captions, main-news and main-forums. They represent image/video description, news and forum question/answer, respectively. Appendix C.1 provides more detailed explanations of these genres.\nAnother splitting dimension is verb number. The test set is divided into three parts according to pair types P1-, P2, and P3+. Generally, from P1- to"}, {"title": "Human Cognitive Differences in STS Tasks", "content": "This section studies the cognitive process underlying how humans assign a score according to task instructions when given a sentence pair. Intuitively, within human cognition, each verb-related dimension adopted in Section 3.3 ought to contribute differently to the disparity between two sentences. In addition, for the same dimension, human perceptions of the disparity should vary to a certain extent. We aim to quantify these cognitive differences in this experiment.\nDataset. The P1- pairs of the STS-B dataset, whose sentences differ at only one dimension, are selected. Compared to the black-box nature of embedding models, PropNet significantly facilitates this selection process by simply checking if the corresponding element of difference vector is 2. Only pairs from main-captions and main-news are used, since main-forums contains too few samples. In total, 133 valid pairs are collected for analysis. All pairs and their distribution are detailed in Appendix C.2.1.\nEvaluation Metrics. The mean values of the ground-truth scores of the selected pairs within each dimension are calculated to reflect the human cognitive weight assigned to sentence similarity across the five dimensions. Additionally, the standard deviations of each dimension are provided to measure the cognitive deviations within a single dimension.\nResults. In Figure 4 (a), for main-captions, the mean values of #action, #subject and #object are around 1.5, which are lower than those of #where and #other (around 2.5), with statistical significance at the 0.05 level. This indicates that disparities in #action, #subject or"}, {"title": "Discussion", "content": "Multimodal Foundation. Instance nodes are necessary not only to process language, but also to empower PropNet with the potential to function as the backbone of understanding human's vision and control. Imagine this: you see an apple on the table and stretch out your arm to pick it up. An instance node corresponding to this apple makes it possible to aggregate shape, color and name information together, distinguish this apple from other apples on the table, and also possible to track this apple and compute its relative positions to other objects when it is picked up.\nText Generation. If PropNet can represent an image or a video, the rules used in the splitting and parsing phases can be reversed to generate a meaningful sentence. For instance, given the subject \"apple\", the action \"fall\", and the action source \"tree\" perceived from an image or a video, PropNet can generate the sentence \"apple fall from tree\". This represents a novel approach to generate text, fundamentally distinct from the current mainstream multimodal large language models. It is completely transparent and is likely to be closer to the way humans generate language.\nLong Text Understanding. Although in this"}, {"title": "Limitation", "content": "PropNet lacks the ability to handle logic relations among propositions of a sentence. Evolutionary nodes of logic module would be added in future work. In addition, PropNet would fail to represent if there are errors from spaCy part-of-speech tagging and dependency parsing results. For instance, in the sentence \u201cBlizzard wallops US Northeast, closes road\", the subject of \"close\" is identified as \"wallop\"."}, {"title": "Conclusion", "content": "In this paper, we have proposed a novel sentence representation network called PropNet. It can represent a complex sentence by a hierarchical network with interpretable and transparent structure. Through case analysis, several feasible optimization directions are identified to narrow the gap with SOTA models in STS tasks. PropNet paves the way for analyzing and understanding the human cognitive processes underlying STS benchmarks."}, {"title": "Cases", "content": "shows the PropNet representation of a sentence with three propositions. The instance nodes pointing to _unknown are all omitted, as well as their evolutionary nodes, in order to provide a clean view. From this representation, we can form a picture of what happens in our mind.\nA more complex sentence with five propositions is presented in . Its splitting and merging processes are explained in Figure 1. Notably, the subject of the last proposition is incorrect. The correct result should be \"man takes it out on the water\". A more robust strategy of subject identification in the splitting phase will be added in future work."}, {"title": "Relative Clause", "content": "As a relative clause is more complex than other types of clauses, additional rules are proposed to address its splitting and merging. Currently, only relative pronouns that, which, who and relative adverbs when, where are considered for a Prop2 sentence with a relative clause.  shows the splitting and merging rules with examples. The merging results of these examples are shown in ,  and , respectively. It should be noted that the action stamp nodes of main proposition and subordinate proposition are linked by the evolutionary node #time_same. This merging rule is not included in the table."}, {"title": "Parsing", "content": "This section provides a detailed explanation of the primary parsing rules. For Action, Subject, Object, Attribute and Part_of, the parsing primarily relies on token.dep_ and token.pos_. For Where, Auxiliary_Object, Goal, Reason and Source, the parsing system depends on detecting and classifying token's prepositions. The classification is based on the definitions of prepositions from the Merriam-Webster Dictionary. We acknowledge that current classification remains course and occasionally inaccurate. For example, in \u201cshoot at an elephant\", \"elephant\" is classified as Where. However, this Where should be interpreted as the action goal in conjunction with the meaning of \"shoot\". This issue can be addressed by adding more evolutionary nodes. Additionally, the temporal attributes of an action, such as its timing, are temporarily assigned to Where. More sophisticated space-time modules are planned for development in future research.\nThe complete list of preposition categories is as follows:\n\u2022 Where: \u201con\u201d, \u201cin\u201d, \u201cinside\u201d, \u201cthrough\u201d,\n\"over\", \"around\", \"down\", \"at\", \"near\u201d,\n\u201calong\u201d, \u201coutside\u201d, \u201cpast\u201d, \u201cacross\u201d, \u201cduring\u201d,\n\u201cafter\u201d, \u201cbefore\u201d, \u201cwhile\u201d, \u201cwhilst\u201d, \u201coff\u201d,\n\"amid\", \u201cbehind\u201d.\n\u2022 Auxiliary_Object: \u201cwith\u201d, \u201cby\u201d, \u201cabout\u201d,\n\"like\", \"as\".\n\u2022 Goal: \u201cinto\u201d, \u201cto\u201d, \u201conto\", \"towards\",\n\"against\"\n\u2022 Reason: \"for\", \"due\".\n\u2022 Source: \"from\"."}, {"title": "Comparing Two Sentences", "content": ""}, {"title": "Comparison Dimensions", "content": "explains the comparison dimensions to compute a difference vector given two sentence representations. Be aware that since instance nodes themselves have no meanings, what are really compared are the corresponding developmental nodes."}, {"title": "Similarity between Two Words", "content": "Given two words, we use Doubao 6 to compute a similarity score ranging from 0 to 1. Since this task is straightforward, other LLM models like ChatGPT or Llama should work as well.\nFor a verb pair, its prompt is \"From physical action perspective, return similarity score (0~1) between two verbs: {word_text1} and {word_text2}. Only return the score\".\nFor other pairs, its prompt is \"From semantic perspective, return similarity score (0~1) between two words: {word_text1} and {word_text2}. Only return the score\"."}, {"title": "Dimensional Similarity Code", "content": "Given a comparison dimension, it's common that both sentences have only one developmental node. For example, considering this pair \"I like orange\" and \"she hates apple\", #action is \"like\" and \"hates\", #action|#subject is \"I\" and \"she\", and #action #object is \"orange\" and \"apple\". But exceptions do exist that the developmental"}]}