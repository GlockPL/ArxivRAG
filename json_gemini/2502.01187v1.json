{"title": "Skewed Memorization in Large Language Models: Quantification and Decomposition", "authors": ["Hao Li", "Di Huang", "Ziyu Wang", "Amir M. Rahmani"], "abstract": "Memorization in Large Language Models (LLMs) poses privacy and security risks, as models may unintentionally reproduce sensitive or copyrighted data. Existing analyses focus on average-case scenarios, often neglecting the highly skewed distribution of memorization. This paper examines memorization in LLM supervised fine-tuning (SFT), exploring its relationships with training duration, dataset size, and inter-sample similarity. By analyzing memorization probabilities over sequence lengths, we link this skewness to the token generation process, offering insights for estimating memorization and comparing it to established metrics. Through theoretical analysis and empirical evaluation, we provide a comprehensive understanding of memorization behaviors and propose strategies to detect and mitigate risks, contributing to more privacy-preserving LLMs.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have revolutionized natural language processing by learning from vast amounts of data to generate coherent and contextually relevant text. Despite their impressive capabilities, a critical question persists: to what extent do these models memorize their supervised fine-tuning (SFT) data versus generalize to new, unseen inputs? While LLMs can generate plausible text, they also risk reproducing verbatim passages from their training datasets, leading to significant privacy and security concerns. Memorization occurs when a model outputs exact or near-exact replicas of its training data, which can result in the unintended exposure of sensitive information or violations of intellectual property rights.\nRecent research indicates that memorization in LLMs during SFT does not affect all training data equally; instead, certain data points are significantly more prone to memorization, leading to a highly skewed pattern. For instance, Xie et al. found that LLMs often rely on memorization when solving logical reasoning tasks like Knights and Knaves puzzles. The models achieved high accuracy on training puzzles but struggled with slightly perturbed versions, suggesting that they memorized specific instances rather than learning underlying reasoning principles. This skewness means that while most data points are generalized over, a small subset contributes disproportionately to the overall memorization risk. Consequently, average-case analyses or mean memorization rates fail to capture these worst-case scenarios where sensitive information may be leaked, akin to assessing system security based solely on average performance without considering rare but critical failures.\nFurthermore, methods relying on sampling to estimate maximum memorization can be biased and may overlook rare but significant instances, particularly under practical constraints like limited sample sizes or computational resources. Prior studies have also artificially increased memorization by replicating training data multiple times (e.g., thousands of repetitions), which is not reflective of real-world training settings. Such approaches may not accurately identify where memorization is most pronounced within a model or elucidate the factors that contribute to it. This limitation hampers effective comparison of memorization across different models or training configurations and underscores the need for methods that can detect and analyze memorization without unrealistic experimental setups.\nIn this work, we analyze skewed memorization in LLMs trained with SFT without relying on unrealistic experimental setups like replicating training data multiple times. Employing non-parametric statistical tests that handle skewed data distributions, we capture nuances overlooked by average-case analyses, providing more accurate assessments of worst-case scenarios and valuable insights into model behavior. By decomposing term-wise probabilities in the LLM"}, {"title": "2. Methods", "content": "We propose a prefix continuation framework to quantify memorization in LLMs, measuring how many tokens a model recalls beyond a given prefix. To capture its skewed distribution, we use non-parametric sampling to estimate worst-case risks. We analyze how memorization intensifies with training, varies with dataset size and composition, and correlates with embedding-space diversity, linking trends to scaling laws. Finally, we compare our metric to standard text similarity measures, demonstrating its effectiveness in capturing extreme memorization cases. Theoretical proofs and additional comparisons are provided in the Appendix."}, {"title": "2.1. Estimating Memorization in LLMs", "content": "Manual inspection of word completions often fails to capture memorization, as worst-case instances are systematically underestimated in small samples. Memorization in LLMs is highly skewed, requiring a distributional approach rather than reliance on mean-based measures. Denote cumulative distribution function F = P(Npre \u2264 n), where Npre denotes random variable of consecutively recalled tokens from a training example beyond a given prefix, npre[i] denotes the actual value at i-th data point. The probability that a sample of size z fails to contain any of the top k most memorized cases is given by:\nP(max(n1,...,nz) < N(\u2212k)) = $\\binom{M-k}{z} / \\binom{M}{z}$,\nwhich approximates how often extreme memorization cases are omitted under random sampling.\nSince direct computation is impractical, we estimate F via sampling-based methods. When z < M, uniform"}, {"title": "2.2. Checkpoint-Level Memorization Dynamics Across Training", "content": "Understanding how memorization evolves across training is essential for assessing its risks and developing mitigation strategies. We investigate memorization trends by analyzing how distributions change under three key conditions: increasing training epochs, reducing dataset size, and altering dataset composition. Our goal is to determine how these factors influence memorization and whether memorization intensifies in specific scenarios.\nAs training progresses, memorization is expected to increase, particularly in the upper quantiles, as overfitting leads to stronger memorization of training data. While overall loss decreases, extreme memorization cases often become more pronounced. Similarly, smaller training datasets provide fewer constraints for generalization, leading to greater reliance on memorization. This follows from established scaling laws in deep learning, where dataset size significantly impacts optimization dynamics and generalization ability. Although mean memorization may remain stable, its effects manifest in the upper tail of the distribution.\nBeyond the size of the data set, composition also plays a crucial role in the behavior of memorization. If two datasets differ semantically, their combination modifies the embedding space, potentially altering memorization patterns. We explore whether such changes influence memorization by examining shifts in memorization distributions when subsets of datasets are merged.\nTo systematically measure these effects, we adopt a distribution analysis approach rather than relying solely on mean-based memorization metrics. The top-k memorization curve provides a more sensitive indicator of extreme cases. To formally compare memorization distributions across training conditions, we employ non-parametric statistical tests, ensuring robustness to distributional shifts without assuming specific parametric forms. Since training loss alone does not directly quantify memorization, empirical evaluation remains essential to characterize memorization trends and"}, {"title": "2.3. Analyzing Skewed Memorization: Identifying Highly Memorized Data Points", "content": "We introduce a prefix continuation approach to systematically measure memorization in LLMs. Given a training sequence s, we partition it into a prefix rpre of length c and a suffix rref. The model, prompted with rpre, generates a continuation s. The prefix match length npre is defined as:\nnpre = max{k | s1:k = rref,1:k},\nwhere s1:k is the first k tokens of the generated output, and rref, 1:k is the corresponding ground truth segment. The random variable Npre captures the distribution of memorization lengths across the dataset.\nWe seek to determine which training samples exhibit longer memorization. Let i index training samples and j denote token positions within the suffix. This allows us to analyze memorization both across different training samples and within individual sequences. A deeper examination of this structure provides insights into how autoregressive models generate tokens based on prior context, allows bounding of memorization length distributions, and facilitates comparisons between memorization metrics."}, {"title": "2.3.1. DECOMPOSING MEMORIZATION PATTERNS", "content": "Since LLMs generate tokens sequentially, later outputs do not causally affect earlier ones. This enables factorization of the probability of a consecutive match ending at npre:\nP(Npre = npre) = (\u03a0pj)(1 \u2013 pn[pre])\nwhere pk for each k is the probability that the model mem-"}, {"title": "2.3.2. \u039c\u0395MORIZATION AND EMBEDDING DIVERSITY", "content": "Memorization in LLMs varies across training samples, with certain sequences exhibiting longer recall. We hypothesize that memorization correlates with the diversity of possible continuations for a given prefix. If a prefix appears in multiple training examples but leads to varied completions, memorization is less likely due to high entropy in suffix prediction. Conversely, when a prefix consistently maps to a single completion, memorization is more probable. Prior work supports that LLMs are more likely to memorize deterministic mappings reinforcing the role of training data structure in memorization risk."}, {"title": "3. Experiments and Results", "content": "Our initial trails revealed an unexpected challenge: memorization proved remarkably difficult to detect in domain-specific datasets. This observation led us to examine how dataset characteristics, specifically blending and size, influence memorization patterns. Upon collecting a larger sample, we discovered that the distribution of memorization"}, {"title": "3.1. General Setup", "content": "For the baseline case, we utilized Lavita-Medical-QA\u00b9, a medical question-answering dataset from Hugging Face. Originally formatted as multiple-choice questions, we modified it into a single-answer question set, extracting 9,723 question-answer pairs to form a domain-specific dataset. To introduce diversity, we incorporated GPTeacher-General-Instruct\u00b2, a GPT-4-generated modular dataset. We randomly sampled 9,723 records from GPTeacher to match the dataset size, ensuring a comparable structure. While Lavita-Medical-QA is domain-specific, GPTeacher provides open-domain question-answering data. Further details on both datasets are provided in the Appendix.\nFor modeling, we employed Llama-3.1-8b-Instruct\u00b3, fine-tuning it in a two-stage process with LoRA before inference. Inference was conducted using vLLM, a high-throughput, memory-efficient engine optimized for LLM inference, with a temperature of 0 to ensure deterministic outputs. Implementation details are elaborated in the Appendix.\nTo quantify memorization, we segmented each training sample into two parts: the first 100 characters were provided as input to the model, while the remaining portion was used for evaluation. The generated output was compared to the reference text on a word-by-word basis, treating words as lists separated by blank spaces. A key measure is the onsecutive word match npre occurring immediately after the cut, accumulating from the first output block."}, {"title": "3.2. Checkpoint-Level Memorization Trends", "content": "Memorization varies across training checkpoints and is influenced by dataset size, composition, and loss. Our analysis reveals that while memorization generally increases as loss decreases, worst-case memorization can emerge early in training. Figure 1 illustrates this trend in the Lavita-Medical-QA dataset, showing increasing max memorization over 100 epochs.\nThis comprehensive analysis of the full training dataset reveals that maximum memorization can increase independently of achieving extremely low loss or high epochs. As shown in Figure 1, memorization surpasses a length of 10 as early as epoch 10, despite the training loss remaining above 1.0. Similarly, Figure 2 demonstrates a maximum memorization length of around 50 at the first checkpoint (epoch 10). These findings indicate that worst-case memorization is not strictly constrained by loss, underscoring the limitations of relying solely on loss as a proxy for memorization.\nTo investigate the impact of dataset composition on memorization across the entire training set, we introduce a second dataset by sampling 9,723 examples from GPTeacher, ensuring a comparable distribution in sequence lengths. A mixed dataset is then constructed by replacing 200 randomly selected samples with corresponding examples from Lavita-Medical-QA while maintaining index consistency. Results indicate significantly higher memorization quantiles in the mixed dataset compared to Lavita-Medical-QA alone. At the first checkpoint (epoch 10), maximum memorization in the mixed dataset exceeds 40, whereas Lavita-Medical-QA remains below 16. By epoch 100, maximum memorization in the mixed dataset surpasses 50, while Lavita-Medical-QA remains at 16, demonstrating that dataset composition plays a crucial role in memorization dynamics.\nWe are also interested in the subsample we used that resides in both data. We compare the memorization of the 200 overlapping samples in the Lavita-Medical-QA and mixed datasets (Figure 3). A signed-rank test confirms a statistically significant difference in memorization between the two settings, reinforcing that memorization is not solely a property of an individual data point but also of the dataset context. Interestingly, while many samples exhibit similar memorization across datasets, a subset shows significantly different memorization lengths, with high variance and non-normal distribution. This highlights a potential risk: blending data from different domains can exacerbate worst-case memorization while altering per-instance memorization behavior in unpredictable ways."}, {"title": "3.3. Scaling Laws in Memorization", "content": "Expanding on prior research on scaling laws in deep learning, we investigate how dataset size influences memorization patterns. As described in the method section, we construct progressively smaller subsets of Lavita with sizes 28, 29, and 210, training models using a consistent learning rate schedule. While smaller datasets yield lower per-step loss, systematic trends in memorization emerge when analyzed across epochs, loss progression, and memorization quantiles.\nOur findings indicate that dataset size does not directly constrain maximum memorization but significantly affects how quickly memorization escalates throughout training. Smaller datasets impose fewer constraints on generalization, leading to a steeper rise in memorization as models overfit to the limited training samples. In the 28 subset, maximum memorization reaches 70 within the first 100 training steps, with training loss dropping below 0.25. In contrast, in the 210 subset, maximum memorization remains below 20 within the same interval, with loss staying above 1.0.\nThis suggests that even when training on small datasets, models rapidly memorize individual samples despite achieving seemingly lower loss values. The effect is particularly pronounced in the smallest dataset, where limited training diversity allows models to memorize sequences much earlier in training. These results highlight the necessity of regularization techniques such as dropout, weight decay, or adversarial training when fine-tuning LLMs on domain-specific data with constrained sample availability. Moreover, they emphasize the need for more nuanced memorization assessments beyond loss-based evaluations, particularly in privacy-sensitive applications where early memorization can pose risks."}, {"title": "3.4. Sampling Challenges in Skewed Memorization", "content": "Memorization in LLMs exhibits a highly skewed distribution, with extreme cases occurring infrequently but significantly. Our analysis reveals that while the average prefix match length remains low, the tail of the distribution extends substantially, indicating that certain samples experience much stronger memorization. Figure 5 highlights this behavior. For example, if the sample size is 455, there is a higher probability that the sampled max is 8 as opposed to the true maximum 16. showing that smaller sample sizes systematically underestimate worst-case memorization.\nTo quantify the reliability of detecting extreme cases, we evaluate the probability of missing high-memorization instances when sampling subsets of the training data. As expected, the likelihood of capturing maximum memoriza-"}, {"title": "3.5. Decomposition", "content": ""}, {"title": "3.5.1. \u039c\u0395MORIZATION AND INPUT SIMILARITY", "content": "As discussed in our method section, we hypothesize that memorization likelihood is influenced by data redundancy and embedding-space similarity. Specifically, if an input appears frequently in different contexts but leads to diverse continuations, memorization is less likely due to higher uncertainty in the suffix. Conversely, if a prefix consistently maps to a specific completion, memorization is more probable. To test this, we compute the mean embedding similarity of each input to all other inputs and compare it to the corresponding full-sentence similarity.\nFigure 6 shows npre plotted against input and output similarities. The observed trend supports our hypothesis: higher input similarity correlates with stronger memorization, suggesting that data points residing in denser regions of the embedding space are more likely to be memorized. This trend is consistent across multiple training checkpoints in both the baseline and mixed datasets."}, {"title": "3.5.2. DECOMPOSING MEMORIZATION AT THE TOKEN LEVEL", "content": "To better understand how memorization propagates in LLMs, we analyze term-wise correctness probabilities by computing mutual information (MI) between memorization at different positions. Specifically, for each training sample i and token position j, we compute a binary correctness matrix Cij, where cij = 1 if the model correctly recalls the token at position j given all previous tokens, and 0 otherwise. Using this matrix, we estimate the mutual information MI(CJpre, Cj) for all valid prefixes Jpre C j.\nEmpirically, we find that MI(CJpre, Cj) is consistently low across multiple training checkpoints in the baseline case (Figure 7). This suggests a weak dependency between correctness at j and correctness at earlier positions, supporting the approximation: P(Npre \u2265 n) = 1 \u2212 F(n \u2212 1) \u2248 \u03a0=1 \u0420\u0458 Since the cumulative product of per-token probabilities aligns closely with the observed memorization distribution (Figure 8), this validates our method section's assumption that term-wise correctness probabilities approximate the memorization distribution.\nThe independence of token prediction accuracy has critical"}, {"title": "4. Discussion", "content": "Our findings highlight that prefix memorization in LLMs is rare but highly skewed, making worst-case instances difficult to detect without large samples. Dataset diversity influences memorization in two ways: it increases overall memorization but can reduce extreme cases within local input neighborhoods. Decomposing memorization into term-wise and sequence-level components further shows that token diversity within a sequence affects memorization likelihood, aligning with the structured nature of auto-regressive generation.\nThese insights have practical implications for detecting and mitigating memorization. Improved sampling strategies can enhance privacy risk assessment by capturing extreme cases more effectively. Additionally, increasing local diversity in training data may reduce unintended memorization while preserving model utility. Understanding how memorization relates to embedding-space structure and generation dynamics offers a path toward privacy-preserving model training and informed dataset curation."}, {"title": "5. Future Work", "content": "Several open directions merit further investigation. The connection between embedding similarity and memorization remains underexplored, particularly whether embedding collapse during training contributes to memorization intensity. Further research should examine whether memorization sensitivity extends beyond lexical similarity to factual relevance, potentially revealing how LLM embeddings encode structured knowledge. Understanding these mechanisms could inform future approaches for aligning LLMs with factual consistency while mitigating privacy risks."}, {"title": "6. Conclusion", "content": "This work presents a principled analysis of memorization in LLMs, revealing its skewed distribution, dependence on dataset diversity, and decomposition into term-wise and sequence-level components. By linking memorization to training dynamics and embedding-space structure, we pro-"}, {"title": "A. Additional Experiment Setup Details and Generalization Explaination", "content": "For the LORA fine-tuning stage, we employed Llama-Factory, an open-source and efficient framework designed for LLM Fine tuning. The following hyperparameters were consistently applied across all experiments during the LORA fine-tuning process:\nWe utilized vLLM, a high-throughput and memory-efficient engine designed for large language model (LLM) inference. The LoRA fine-tuned Llama-3.1-8b-Instruct model was loaded in its full size, and the following generation configuration was applied within vLLM:"}, {"title": "B. Comparative Analysis with Existing Text Metrics", "content": "To systematically compare memorization with standard text similarity metrics, we establish formal relationships between prefix memorization, common text comparison methods, and NLP evaluation criteria."}, {"title": "B.1. Notation and Definitions", "content": "Table 4 summarizes the key notations used throughout our analysis."}, {"title": "B.2. Handling Length Mismatch in Text Comparison", "content": "A fundamental challenge in text similarity is handling cases where N1 \u2260 N2. We impose the constraint:\nNpre|max(N1,N2) \u2264 max(N1, N2)."}, {"title": "B.3. Relations Between Memorization and NLP Metrics", "content": "We establish the following hierarchical relationship:\nNon-inplace matched word count > LCS > Nmax\nAdditionally:\n> In-place matched n-gram length \u2265 npre \u2265\nmin(N1, N2) - d\u2081 = Total in-place matched words.\nd1 \u2265 max(N1, N2) \u2013 LCS > $\\frac{DLevenshtein}{2}$\nFor weighted distances (w > 2), restricting to length N:\ndr(s[i], r[i]) > dr(s[i'], r[i']) \u21d4 npre(s[i'], r[i']) > npre(s[i], r[i]).\nFor w = 3, we derive:\nd3 = $\\sum 3^{-1}Cj <\\sum Cj \u2264 d\u2081,$\n$\\frac{d1-3^{-Npre}}{1 - 1/33} \u2265 d3.$"}, {"title": "B.4. ROUGE Score Relations", "content": "For ROUGE-L:\nROUGE-L = $\\frac{LCS}{N2} \u2264 \\frac{Nmax}{N2} \u2264 \\frac{npre}{N2}$\nFor ROUGE-n:\nRecall = $\\frac{Count of summary n-grams in reference}{Total reference n-grams}$\nPrecision = $\\frac{Count of summary n-grams in reference}{Total summary n-grams}$\nDenominators are N2 - n + 1 and N\u2081 n + 1, while the numerator satisfies:\nNmax \u2265 npre"}, {"title": "B.5. Implications for Memorization and Evaluation", "content": "These results establish formal connections between memorization and standard NLP text similarity metrics, demonstrating that prefix memorization aligns with common evaluation measures while providing a more extreme-case-sensitive assessment. This is crucial for detecting overfitting in models trained on small datasets or identifying privacy risks.\nSince our memorization metrics satisfy fundamental metric properties, they can be extended to generalization error analysis using clustering-based methods. Furthermore, standard evaluation metrics may systematically underestimate memorization risks, reinforcing the need for explicit memorization evaluation in LLMs.\nAll proposed metrics and relationships are implemented in our evaluation code, enabling direct application in diverse training setups."}, {"title": "C. Proofs and Examples for Theorems and Properties", "content": ""}, {"title": "C.1. Probability of Memorization by One Token Ahead", "content": "Proof. Since the sampling for generating a new token given the input is independent of sampling the actual data from the training set, we can express the probability of memorization at a given token as:\nP(memorization at this token input, all previous tokens)\n= $\\sum P(generated token = y|input, all previous tokens, actual token \\= y)$\n$\\neq P(actual token = y|input, all previous tokens)$\n= $\\sum P(generated token = y|input, all previous tokens)$\n$\\neq P(actual token = y|input, all previous tokens).$\nSince the sampling process for $\\neq P(actual token = y|input, all previous tokens)$ is determined by the training data distribution, we can enumerate y and denote this probability as pr. Similarly, the probability of generating token y can be represented as qi. The problem then simplifies to:\narg max$\\sumPili$\nq\nsubject to constraints:\n1 \u2265 Pi \u2265 0, 1 \u2265 qi \u2265 0, $\\sum Pi = \\sum qi = 1.$\nTo prove that a greedy algorithm selects the token with the highest probability in the training data, we show that the optimal q follows:\nqi = $\\begin{cases}1 & \\text{if i = arg max }i Pii\\\\0 & \\text{otherwise.}\\end{cases}$\nSuppose a suboptimal q \u2260 q* exists. Then, there must be some index i' = arg maxipi with qi' > 0 and another index i where q* < 1 - qi'. Constructing a new probability distribution by setting qi'= arg maxipi = 0 and qi'= arg maxipi + qi'\nKeeping other indices unchanged, we show that:\n$\\sumPidi > Piqi$"}, {"title": "C.2. Bounding Memorization Using Weighted Correctness Scores", "content": "Proof. Define two correctness vectors:\nC = (0,..., 1, 0, 0, 0 . . . ),\nwhere the first 1 appears at index n, and the rest are 0.\nc = (0, . . ., 0, 1, 1, 1 . . . ),\nwhere the first 1 appears at index n + 1 and is followed by consecutive ones. We show:\n$\\sum w^{-i}Cj > \\sum w^{-i}cj,$\nfor w > 2.\nBy applying the geometric series sum formula, we compare cases where npre = n 1 and n'pre > npre. For any correctness vector c(npre) corresponding to npre n 1,\n$\\sum w^{-i}cj(\\text{npre}) > \\sum w^{-i}Cj$.\nSimilarly, for c(npre),\n$\\sum w^{-i} > \\sum w^{-i}cj(\\text{pre}).$\nThus, the order is preserved."}, {"title": "C.3. Inequalities with Edit Distance and ROUGE", "content": "Proof. Using set inclusion, we establish the following sequence of inequalities:\nNon-inplace matched word count > LCS > Nmax\n> Length of in-place matched n-gram > npre\n> max(N1, N2) \u2013 d\u2081 = Number of in-place matched words.\nFor edit distance:\nd1 \u2265 max(N) \u2013 LCS > $\\frac{dLevenshtein}{2}$\nSince an edit sequence from one string to another must at least remove non-LCS words and insert missing ones, we obtain:\nN1+ N2-2LCS \u2265 dLevenshtein,\nwhich bounds the edit distance."}, {"title": "C.4. Remarks on Conditional Probability and Memorization Strength", "content": "Remark C.1. Some data entries may be memorized more intensely than others, but the probability of memorization at a token, conditioned on previous tokens being correct, remains independent of specific token-wise memorization patterns.\nConsider correctness sequences (where i represents data and j represents token position):\n01\n10\n11\n00\nHere, sequence 3 is more strongly memorized. The termwise correctness constraints are:\n0*\n10\n11\n0*\nwhere * represents unrestricted positions. Computing P(2 correct | 1 correct) shows:\nP(2 correct | 1 correct) = P(2 correct).\nThis provides a minimal counterexample, illustrating that termwise memorization probabilities do not contradict varying levels of memorization across data points.\nRemark C.2. Choosing the highest probability term-by-term differs from selecting the highest probability contiguous subsequence.\nConsider:\n000000\n000000\n123456\n132456\n142356\n152346\n162345\nA termwise greedy selection yields:\n1 * * * * *,\nmemorizing at most 9 tokens but only fully memorizing one sequence. In contrast, selecting:\n* * * * * 0\nresults in two fully memorized sequences. This discrepancy occurs when a less probable term is replicated over a longer sequence.\nThus, sequences with identical prefixes should not diverge significantly in subsequent terms to ensure consistent memorization behavior. This insight connects to the bounds of Rs and H, motivating further experiments."}, {"title": "C.5. Expected Memorization in BOC vs. Termwise BOC", "content": "Proof. If:\nE[Npre(BOC)] > E[Npre(T-BOC)],\nthen they must differ at some index j. Given correct memorization up to j \u2013 1, the selection at j in BOC leads to higher expected memorization than termwise BOC.\nIf the sequence extends at least ne beyond j, then:\nnrn'j > ne 1 + n; [max].\nSince nr,n'; are integers, if nr < ne, then n'; \u2265 2, ensuring at least one pair is replicated in the training data at |rpre + j + nr."}]}