{"title": "REFINE KNOWLEDGE OF LARGE LANGUAGE MODELS VIA ADAPTIVE CONTRASTIVE LEARNING", "authors": ["Yinghui Li", "Haojing Huang", "Jiayi Kuang", "Yangning Li", "Shu-Yu Guo", "Chao Qu", "Xiaoyu Tan", "Hai-Tao Zheng", "Ying Shen", "Philip S. Yu"], "abstract": "How to alleviate the hallucinations of Large Language Models (LLMs) has always been the fundamental goal pursued by the LLMs research community. Looking through numerous hallucination-related studies, a mainstream category of methods is to reduce hallucinations by optimizing the knowledge representation of LLMs to change their output. Considering that the core focus of these works is the knowledge acquired by models, and knowledge has long been a central theme in human societal progress, we believe that the process of models refining knowledge can greatly benefit from the way humans learn. In our work, by imitating the human learning process, we design an Adaptive Contrastive Learning strategy. Our method flexibly constructs different positive and negative samples for contrastive learning based on LLMs' actual mastery of knowledge. This strategy helps LLMs consolidate the correct knowledge they already possess, deepen their understanding of the correct knowledge they have encountered but not fully grasped, forget the incorrect knowledge they previously learned, and honestly acknowledge the knowledge they lack. Extensive experiments and detailed analyses on widely used datasets demonstrate the effectiveness of our method.", "sections": [{"title": "1 INTRODUCTION", "content": "Based on the massive training corpora and a large number of training resources from the real knowledge of the human world, Large Language Models (LLMs) store many general and domain-specific knowledge and show excellent performance in many natural language processing tasks (Li et al., 2022; 2024d; Wang et al., 2024; Zhang et al., 2024a; Li et al., 2024f). LLMs are equipped with excellent command comprehension, logical reasoning, and text generation capabilities. As chat assistants, LLMs help users realize many tasks in their work, study, and daily life, greatly enriching the user experience and satisfying their needs (Bansal et al., 2024; Asadi et al., 2024; Guo et al., 2024). However, LLMs-based chat assistants, although greatly helping users, show hallucination problems in more and more tasks (Liu et al., 2022a; Chen et al., 2024; Zhang et al., 2024e;d). They encounter factual errors in their answers, sometimes even fabricate non-existent knowledge, or contradict the user's original intention when entering instructions (Mu & Lim, 2024; Lv et al., 2024). Some of these hallucinatory errors, such as fabricating false documentaries, are difficult for the user to recognize directly, but they can have serious consequences when used and are likely to reduce the user's trust in the AI assistant.\nWhen the LLM hallucinates in its reply, provides factual errors, or fabricates false knowledge, we can assume that the LLM lacks the relevant knowledge for the problem and does not have the ability to respond accurately (Li et al., 2024a; Cheng et al., 2024; Zhang et al., 2024c). From the perspective of human knowledge needs, for questions beyond the scope of LLM's knowledge, we receive an honest response like \u201cI don't know\" rather than a fabricated, plausible-sounding answer. This will be more conducive to utilizing AI to get the valid information that users need, as users can actually achieve what they want by providing more knowledge about the initial question, or by asking for additional help.\nTherefore, to get an honest LLM assistant with higher creditworthiness, the LLM must have two abilities: the LLM not only has to be able to choose to refuse to answer when faced with an unknown question but also has to acquire the ability to correctly distinguish whether it really knows the knowledge related to a question or not, which means the model needs to be aware of what it knows and what it does not (Zhang et al., 2024c). The LLM's ability to perceive knowledge can be represented by the knowledge quadrants (Yin et al., 2023b). The knowledge quadrant is divided into four quadrant regions as shown in Figure 1, Known Knowns, Known Unknowns, Unknown Knowns, and Unknown Unknowns, where the vertical axis represents the model's perception of itself (i.e., the model's thought of what it knows or does not know). The horizontal axis represents the model's actual knowledge mastery (i.e., whether or not the model actually masters a particular piece of knowledge). Thus, knowledge in the first quadrant \"Know Knowns\u201d, represents that the AI knows that it knows, while knowledge in the forth quadrant \u201cUnknown knowns\u201d, represents that the AI does not know that it knows, and the same can be inferred for the other quadrants.\nIdeally, the LLM gives the correct response to the question, which falls into \"Known Knowns\", whereas when the LLM is faced with the hallucination problem, it does not realize that it does not know the relevant knowledge and gives the wrong response, which falls into the \"Unknown Unknowns\" quadrant. The horizontal axis represents the LLM's specific knowledge mastery, which is limited by the LLM's own training process and knowledge capability. Aiming to mitigate the hallucination problem and meet the needs of the honest LLM system, we pay more attention to the vertical \"Known\u201d ability. We believe that providing a correct response to known questions and refusing to respond to unknown questions, as outlined in the two quadrants above, are both acceptable behaviors for an honest LLM. Thus, our goal in mitigating the LLM hallucination problem is to have more input instructions and responses that can be moved from the lower two quadrants, into the upper two quadrants.\nResearch on methods to mitigate the LLM hallucination problem and improve its honesty based on knowledge representation faces the following problems:"}, {"title": "2 RELATED WORK", "content": "Large language models learn extensive knowledge from human society and carry out various tasks based on this knowledge (Burns et al., 2023; Ouyang et al., 2022; Li et al., 2023a; Huang et al., 2023). Among mainstream techniques, supervised fine-tuning (SFT) aligns LLM with real-world human knowledge, playing a critical role in fulfilling the tasks expected of LLM by humans Li et al. (2024e); Yu et al. (2024); Du et al. (2024); Li et al. (2025). After gathering a large annotated corpus"}, {"title": "3 METHODOLOGY", "content": "Different question-answering datasets encompass various types of knowledge and expressions, making it challenging to assess whether models truly possess specific knowledge due to varying confidence levels. To enable models to better identify their knowledge boundaries and determine their ability to answer related questions, we establish two thresholds: the upper threshold IK (I Know rate) and the lower threshold IDK (I Don't Know rate). These correspond to three knowledge types: the model knows it knows (knowns-known knowledge), the model does not realize it knows (unknowns-known/uncertain knowledge), and the model does not know it does not know (unknowns-unknown knowledge).\nFollowing previous studies Cheng et al. (2024); Zhang et al. (2024c), we filter questions from public datasets and query LLMs multiple times to sample responses, calculating accuracy as a measure of the model's confidence in each question. The number of sampled responses and the thresholds IK and IDK serve as hyperparameters. As illustrated in Figure 2, when confidence exceeds IK, the model is deemed to fully master that knowledge. If confidence is between IK and IDK, the model has the knowledge but may not recognize it, leading to occasional incorrect answers. Confidence below IDK indicates a lack of knowledge, resulting in low correct response rates. By defining these knowledge boundaries, we will construct contrastive learning data and adaptively tune the model based on these knowledge types. This process aims to help the model retain known knowledge, reinforce uncertain knowledge, and forget unknown knowledge. We aspire for adaptive contrastive learning instruction tuning to shift more knowledge from the third and fourth quadrants to the first and second quadrants, enhancing model honesty while ensuring helpful responses."}, {"title": "3.2 CONSTRUCTION OF CONTRASTIVE LEARNING DATA", "content": "Based on the knowledge boundary delineation in Section 3.1, we have collected knowledge data of what the model knows and what it does not know, which corresponds to the right and the left quadrants of the knowledge, respectively. We further perform contrastive learning data construction for the knowledge, as shown in Figure 3."}, {"title": "3.2.1 QUESTIONS THAT THE MODEL KNOWS", "content": "In Section 3.1, the accuracy of the question greater than the upper threshold IK falls in the first quadrant of the knowledge of \"what the model knows that it knows\". For a given question, the correct response of the corresponding model is designed as a positive instance. There are two types of negative instances: one is the model's response \u201cI don't know\u201d, which can be pushed farther away in the contrastive learning instruction tuning to enhance the model's confidence and avoid the situation where the model knows the knowledge but thinks it doesn't know. Another negative instance is a given question with incorrect answers generated by the LLM, applied to help the LLM forget the incorrect knowledge during training. It is worth noting that the selection of instances is related to the specific parameters of our threshold. When the IK is chosen as 1.0 for the upper threshold, it means that the question will only be treated as a known question if the model answers all of them correctly during all sampling instances. At this point in the construction of the negative instances, there will be no corresponding incorrect answer negative instance, because questions with incorrect answers are already categorized in questions that the model does not know."}, {"title": "3.2.2 QUESTIONS THAT THE MODEL DOES NOT KNOW", "content": "Questions that are answered with a correctness rate less than the upper threshold IK, including \"the model does not know that it knows\" and \"the model does not know that it does not know\", fall into the two quadrants on the bottom of the knowledge quadrants, and are therefore considered as \"model doesn't know\". The negative instance of the two kinds of questions is a given question with incorrect answers generated by the LLM. In the construction of the positive instances, for questions with an accuracy greater than the lower threshold IDK, we consider that the model actually has the relevant knowledge, but is just not sure whether it knows it or not, which belongs to the knowledge \"the model does not know that it knows\". We set the questions with correct responses as positive instances, hoping to enhance the model's mastery of uncertain knowledge by introducing such knowledge.\nFor questions with a correct response rate less than the lower threshold IDK, we consider that the model does not have the relevant knowledge, which belongs to the knowledge that \u201cthe model does not know that it does not know\". We design the corresponding question and the \u201cI don't know\" answer as a positive instance. By narrowing the distance between this positive sample, we can better encourage the model to admit \u201cI don't know\u201d when facing the knowledge that the model doesn't have and enhance the honesty and creditability of the LLM responses.\nTo compute the adaptive contrastive loss in Section 3.3, we need to construct different contrastive data for various quadrants. Specifically, we construct different instruction data I = (x, y), where x is the input question and y is the answer. In the contrastive setup, besides the anchor I, there are positive instances I+ and negative instances I\u00af. The differences primarily focus on the selection of y, which will be detailed in the following sections."}, {"title": "3.3 ADAPTIVE CONTRASTIVE LEARNING STRATEGY", "content": "In Section 3.2, we obtain the constructed instruction pairs for correlated positive and negative instances of model-knowns versus model-unknowns knowledge from the three quadrants. We then perform adaptive contrastive instruction tuning. Compared to the traditional SFT, the contrastive instruction tuning approach can better enhance the model's mastery of knowledge by pulling the positive instances closer together and pushing the negative instances away Yan et al. (2024). In our adaptive contrastive learning strategy, for the positive and negative instances corresponding to knowledge in different quadrants, we design different data using strategy, which can adaptively perform the calculation of positive and negative samples in different quadrants. So that the model can be more targeted to think about different knowledge, and better enhance the model's ability to maintain what it knows, consolidate what it knows but is uncertain about, and forget unknown knowledge.\nFor the knowledge that the \u201cmodels know that they know\" in Quadrant - 1 (as shown in Figure 3), we have the contrastive data IQ1, I+Q1, and I\u2212Q1. The i-th sample in a training batch includes the original input IQ1 (its y is the golden answer), positive instance I+Q1 (its y\u00b2 is the previous correct answer of LLM), and negative instance I\u2212Q1 (its y\u00b2 is \u201cI don't know\u201d or previous wrong answers of LLM). We bring the positive instance I+Q1 closer and push the negative instance I\u2212Q1 farther away."}, {"title": "4 EXPERIMENT", "content": ""}, {"title": "4.1 EXPERIMENTAL SETUP", "content": ""}, {"title": "4.1.1 TRAINING DETAILS OF METHODS", "content": "In order to verify the superiority of our proposed contrastive learning fine-tuning strategy, we design different LLM-based methods as baselines, and implement the baselines with the same details for fair comparison. Their training details are presented here.\nIn order to equip the model with the initial honesty of refusing to answer questions that it does not know, we designed the Prompting scheme directly for the baseline large model. In the Prompt, the model is told to reply \"I don't know\" when it encounters a question that it cannot answer. This approach requires the model not only to have good command following ability, which can correctly follow the instructions provided by us, but also to have the ability to distinguish its own knowledge boundaries, i.e., to distinguish what is a question that it cannot answer. In IDK Prompting, the model is not additionally trained or fine-tuned, and the model's own ability to follow instructions and distinguish between knowledge boundaries is completely examined.\nSupervised fine-tuning (SFT) is a very important technique for fine-tuning LLMs and can be very useful to help models quickly learn new knowledge patterns from constructed high-quality datasets. We fine-tune the model based on the problems that the model knows versus the problems that the model does not know, from the contrastive learning data we construct. In the input data, each pair of instances consists of a question and a response, where the questions that the model knows correspond to correct responses and the questions that the model does not know correspond to \"I don't know\" responses. By calculating the standard sequence-to-sequence loss of the model-generated responses with respect to the standard responses, we train the model."}, {"title": "4.1.2 IMPLEMENTATION DETAILS", "content": "We select LLaMA-2-7B-chat and Mistral-7B-Instruct-v0.1 as base models for testing on TriviaQA (Joshi et al., 2017) and Natural Questions (Kwiatkowski et al., 2019), respectively. During the training of the LLaMA model, we used a batch size of 16, a learning rate of 5e-5, a context length of 1024, and trained for 2 epochs. For the Mistral model, we used a batch size of 16, a learning rate of 1e-5, a context length of 1024, and also trained for 2 epochs. The \u03c4 is set to 0.01 and the \u03bb is set to 1. All experiments are conducted on Nvidia A100 80GB GPUs. During inference, we utilize the vllm framework to accelerate the process and employ a greedy search strategy to generate responses."}, {"title": "4.2 DATASETS", "content": "is a reading comprehension dataset with question-answering pairs widely used in open-domain quizzing, which contains 87,622 pairs in the training set. Referring to past work Cheng et al. (2024), we use 90% of the training set to construct a training set for comparative learning data and 10% as a validation set. Since there is no standard answer in TriviaQA's test set, we select 11,313 Q&A pairs from the development set to build our final test set.\n(Kwiatkowski et al., 2019) is fine-tuned by constructing a comparative learning dataset on TriviaQA, and we perform tests with the same data distribution. In order to validate the performance of our fine-tuning method on OOD data, we select Natural Questions as our test dataset. Natural questions are real Q&A pairs from the Google search engine, where the development set containing 3,610 instances is used to build our test set. Same as when building the data in TriviaQA, we also use lexical matching as a metric for automatic evaluation when testing on Natural Questions."}, {"title": "4.3 EVALUATION METRICS", "content": "We employ the following evaluation metrics: 1) This metric reflects the model's \"I know what I know\" capability, which is calculated as the percentage of correct answers relative to the total number of questions. 2) This represents the model's \"I know what I don't know\" ability, which calculates the ratio of those questions that correctly refuse answering to the total number of questions. 3) : Both correct answers and correct refusals to answer outside knowledge boundaries are considered reliable, so the Truthful Rate is computed as the sum of the IK-IK and IK-IDK rates, and regarded as a comprehensive measure of its knowledge refinement capabilities. We introduce the evaluation metrics in detail in Appendix B.1."}, {"title": "4.4 MAIN RESULTS", "content": "From the Table 1, we can observe that in the TriviaQA dataset using the LLaMA model, the model fine-tuned with SFT shows a 6.3% improvement in the Truthful Rate compared to directly using IDK-Prompting. After incorporating our Adaptive Loss, there is an additional 5.0% improvement over the SFT model. Notably, the IK-IK rate shows a significant increase of 9.3% compared to the SFT model, indicating that our Adaptive Loss helps mitigate the loss of knowledge that the model is unaware it possesses due to SFT. On the OOD dataset, Natural Question, our model achieves improvements of 6.1% and 6.9% over IDK-Prompting and IDK-SFT, respectively."}, {"title": "5 DISCUSSION", "content": ""}, {"title": "5.1 IMPACT OF DIFFERENT IDK VALUES", "content": "In Section 3.1 and Section 3.2, we utilize the IDK Rate threshold to distinguish between the model's uncertain knowledge (the model does not know that it knows) and unknown knowledge (the model does not know that it does not know). In this process, we aim to refine the model's correct knowledge and reduce its uncertainty regarding the former, while encouraging it to forget incorrect knowledge regarding the latter. We conducted experiments with different IDK Rates to evaluate model performance under varying thresholds. As shown in Table 2, the model achieves optimal performance at an IDK Rate of 0.7, followed by 0.5, with 0.9 being the lowest. We believe that categorizing too many uncertain responses as \u201cthe model does not know that it knows\" can harm performance, as forcing the model to assert knowledge in low-certainty situations can have negative effects. Conversely, a high certainty threshold of 0.9 may prevent the model from reaching its full potential.\""}, {"title": "5.2 IMPACT OF DIFFERENT CONTRASTIVE LOSS COMBINATIONS", "content": "In designing the Adaptive Contrastive Learning strategy, we developed different contrastive learning strategies based on the model's certainty regarding various issues. Here, we isolate each type of contrastive learning loss by setting the others to zero to evaluate the impact of each specific loss type. From Table 3, we can see that:\n\u2022 With the (i.e., \u201cModel knows it knows\u201d in Quadrant \u2013 1), the model's responses are more conservative, resulting in a lower IK-IK rate and a higher IK-IDK rate.\n\u2022 The (i.e., \u201cModel doesn't know it doesn't know\" in Quadrant \u2013 3) encourages the model to be conservative and forget incorrect knowledge, resulting in a lower IK-IK rate.\n\u2022 Using the (i.e., \u201cModel doesn't know it knows\u201d in Quadrant \u2013 4) allows the model to correctly answer more uncertain questions, leading to a higher IK-IK rate."}, {"title": "5.3 REPEATED SAMPLING DISTRIBUTION", "content": "To more intuitively demonstrate the improvements of our method over IDK-SFT, we conduct ten repeated samplings of questions from the TriviaQA test set on LLaMA-2-7B-chat, categorizing them into \"Unknown Questions\" and \"Known Questions.\" When calculating accuracy, the ground truth for \"Known Questions\" is the correct answer, while for \u201cUnknown Questions\u201d, it is a refusal to respond. The distribution of our sampling results is shown in Figure 4. In this figure, the vertical axis represents the proportion of samples within the entire dataset that fall into a specific accuracy range. We employ repeated sampling to evaluate both models by having them respond to each question in the TriviaQA test set 10 times. The number of correct responses out of these 10 attempts (i.e., accuracy) is used as a measure of the model's performance and confidence. To explain Figure 4, in the \"Unknown Questions\u201d sub-figure of Figure 4, the bars corresponding to an accuracy of 1 indicate the proportion of questions from the dataset that the models consistently declined to answer across all 10 attempts. Similarly, in the \u201cKnown Questions\u201d sub-figure, the bars corresponding to an accuracy of 0.8 represent the proportion of questions for which the models provided 8 correct answers out of 10 attempts, relative to the total number of questions in the dataset.\nFrom Figure 4, we observe that our method achieves a higher number of instances with high accuracy (greater than 0.7) in both Unknown and Known questions compared to Idk-SFT. This indicates that the model fine-tuned with our method is more likely to produce correct results in a single response."}, {"title": "6 CONCLUSION", "content": "In this paper, we focus on the problem of hallucination in LLM responses. We believe that this hallucination arises from the LLM's choice to reply despite being confronted with a question that exceeds its own knowledge. This motivates our exploration of the LLM's mastery of its own internal knowledge and its knowledge boundaries. We design a new knowledge boundary delineation method, which helps the model better represent and refine its own knowledge quadrants. We propose the strategy of Adaptive Contrastive Learning, by targeting different knowledge mastery abilities in different knowledge quadrants, we design different positive and negative samples with adaptive loss functions to help the model maintain known knowledge, consolidate uncertain knowledge, and forget the wrong knowledge. We conduct experiments on in-distribution and out-of-distribution datasets, and the results show that this proposed contrastive learning strategy well improves the Truthful rate of the models. We further provide presentations on threshold analysis, loss function ablation experiments, and visualization of results with knowledge quadrants, which not only further demonstrates the validity of our results, but also provides valuable inspiration for future work."}, {"title": "A ADDITIONAL DETAILS OF METHODOLOGY", "content": ""}, {"title": "A.1 OBJECTIVE FUNCTION", "content": "The model does not know it knows. For the knowledge that the \u201cmodel doesn't know that it knows\" in Quadrant 4 (as shown in Figure 3), we have the contrastive data , , and . The ith sample in a training batch includes the original input (its y\u00b2 is the golden answer), positive instance (its y is the golden answer), and negative instance (its y\u00b2 is previous incorrect answers of LLM). We bring the positive instance closer and push the negative instance farther away. We design the loss function:\n\nAgain, there is cross-entropy loss with the final objective function:\n\nwhere detach denotes that the loss value is detached from the computation graph and thus is treated only as a scalar, and \u03bb is the upper bound of the weight.\nThe model does not know that it does not know. For the knowledge that the \"models do not know that they do not know\" in Quadrant \u2013 3 (as shown in Figure 3), we have the contrastive data , , and . The i-th sample in a training batch includes the original input (its y\u00b2 is \u201cI don't know\u201d), positive instance (its y\u00b2 is \"I don't know\u201d), and negative instance (its y\u00b2 is previous incorrect answers of LLM). We bring the positive instance closer and push the negative instance farther away. We design the loss function:"}, {"title": "BADDITIONAL DETAILS OF EXPERIMENT", "content": ""}, {"title": "B.1 EVALUATION METRICS", "content": "We use the following evaluation metrics, all of the evaluation scores show that the higher the score, the better the model effect:\n: This means that the model \"I know what I know\", which means the model does not refuse to answer the question, but gives a correct answer, and the question is labeled as \"model knows\". We calculate the number of correct answers given by the model as a percentage of the total number of questions in the dataset as a fraction of the IK-IDK rate.\n: This represents the model's \"I know what I don't know\", meaning that the model refuses to answer a question that is labeled as a \u201cmodel-don't-know question\" in the test set. When we detect the presence of our predetermined \"I don't know\", we calculate the ratio of the number of questions that the model refuses to answer to the total number of questions as the IK-IDK rate."}, {"title": "C ADDITIONAL EXPERIMENT", "content": "This section is entirely new, so highlighting is omitted."}, {"title": "C.1 MORE DATASETS", "content": "Experimental Setup To evaluate the model's ability to respond appropriately to completely unknown questions, we selected the dataset for testing. This dataset is designed by creating artificial entities through the alteration of existing entity attributes, leading to the generation of questions about these novel entities. Since these entities are artificially constructed, it is nearly impossible for the model to possess prior knowledge about them, making this dataset ideal for testing the model's ability to refuse to answer inconceivable queries Kuang et al. (2024); Li et al. (2023b; 2024c). We randomly sampled 1000 instances from the ALCUNA dataset to serve as our out-of-domain test set."}, {"title": "C.2 IMPACT OF MODEL SIZE ON PERFORMANCE", "content": "Experimental Setup To investigate the effect of different model sizes on our proposed method, we conducted experiments using the LLaMA-2-13B-chat model. The experimental setup was consistent with the previous sections. By comparing with the LLaMA-2-7B-chat model, we aimed to understand how scaling the model impacts performance across various metrics.\nResults Analysis The results, as presented in Table 5, demonstrate nuanced changes in performance as the model size increases from 7B to 13B parameters. For all method, using the larger LLaMA-2-13B-chat model resulted in slight improvements across IK-IK, IK-IDK, and Truthful rate, indicating that a larger model can enhance the ability to differentiate known and unknown knowledge and present truthful responses. Moreover, the proposed method consistently gained performance improvements across all metrics with the larger LLaMA-2-13B-chat model, thereby underscoring the robustness and scalability of our approach."}, {"title": "C.3 COMPARISON WITH RAG INTEGRATION", "content": "Experimental Setup To assess the adaptability of our method, we integrated the Retrieval-Augmented Generation (RAG) technique Tan et al. (2023); Xu et al. (2025); Li et al. (2024b). We conduct experiments using the LLaMA-2-7B-chat model, and we employe the RAG-Bench (Fang et al., 2024) dataset, constructed from three open-domain question answering datasets: NQ (Cheng et al., 2024), TriviaQA (Joshi et al., 2017), and WebQ (Berant et al., 2013). For each dataset, a retrieval model sourced relevant paragraphs from Wikipedia for each query to use as context. In this experimental setup, we input the context alongside the query into the model (denoted as \"with RAG\" in the table) and compared the results with those obtained without using context. We randomly selected 1000 samples from RAG-Bench to use as our out-of-domain test test.\nResults Analysis As illustrated in Table 6, integrating the RAG technique positively impacted the Truthful Rate of all three methods. Additionally, it can be seen that IDK-Prompting and IDK-SFT-Adpt-Ctr show a decreasing trend in IK-IDK after combining with RAG, while IDK-SFT shows an increasing trend in IK-IDK after combining with RAG. The reasons for this phenomenon are as follows:\n1. It is intuitive and reasonable that IDK-Prompting and IDK-SFT-Adpt-Ctr exhibit a decreasing trend in IK-IDK when combined with RAG, as the model naturally tends to answer questions more directly and reduce refusal-to-answer after being input with more additional contextual information from RAG. Therefore, from the perspective of metrics, after adding RAG to these two methods, IK-IDK decreased.\n2. The phenomenon of the increase in IK-IDK after adding RAG to IDK-SFT can only indicate an increase in correct refusal-to-answer, but this does not conflict with the above analysis, as the overall refusal-to-answer include both correct and wrong ones. After careful observation, we found that the overall rejection rate of IDK-SFT is about 10%, while the overall rejection rate of IDK-SFT-RAG is about 4%. This indicates that the overall rejection rate of IDK-SFT still decreases after adding RAG, and the main reason for the increase in IK-IDK is that a considerable portion of refusal-to-answer in IDK-SFT is wrong."}]}