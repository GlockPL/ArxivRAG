{"title": "The Psychological Impacts of Algorithmic and AI-Driven Social Media on Teenagers: A Call to Action", "authors": ["Sunil Arora", "Sahil Arora", "John D. Hastings"], "abstract": "This study investigates the meta-issues surrounding social media, which, while theoretically designed to enhance social interactions and improve our social lives by facilitating the sharing of personal experiences and life events, often results in adverse psychological impacts. Our investigation reveals a paradoxical outcome: rather than fostering closer relationships and improving social lives, the algorithms and structures that underlie social media platforms inadvertently contribute to a profound psychological impact on individuals, influencing them in unforeseen ways. This phenomenon is particularly pronounced among teenagers, who are disproportionately affected by curated online personas, peer pressure to present a perfect digital image, and the constant bombardment of notifications and updates that characterize their social media experience. As such, we issue a call to action for policymakers, platform developers, and educators to prioritize the well-being of teenagers in the digital age and work towards creating secure and safe social media platforms that protect the young from harm, online harassment, and exploitation.", "sections": [{"title": "I. INTRODUCTION", "content": "Social media is a persuasive technology, a concept introduced by B.J. Fogg [1] in the late 1990s. Persuasive technologies are designed to interactively influence people's attitudes or behaviors. Social media platforms use specialized algorithms, AI models, and user interfaces designed to capture and retain user attention, using techniques such as personalized recommendation content feeds, notifications, and interactive designs. Such methods aim to maximize user engagement, often leading to prolonged and repetitive use. On January 31, 2024, the United States Senate Judiciary Committee summoned social media companies' Chief Executive Officers (CEOs), including Meta, TikTok, X, Discord, and Snapchat, for failure to protect children from harm and child sexual abuse on social media platforms [2]. Several parents of children, including those whose children died due to online extortion, sexual abuse, and exposure to harmful content on social media platforms, attended the hearing [3].\nThe current social media landscape prioritizes sensationalism and engagement over factual accuracy. It creates a culture where individuals feel compelled to curate a polished, relatable, and entertaining online persona while navigating the treacherous waters of public judgment. The result is a culture of superficial connections and shallow thinking, where the art of deep conversation and critical thinking gradually erodes.\nThe pervasive use of social media by children raises significant concerns about its impact on their mental and physical health. Research indicates that young users are particularly susceptible to the persuasive elements of social media [4], [5]. Continuous and endless scrolling, facilitated by infinite content feeds, can lead to excessive screen time, which has been associated with a range of adverse psychological outcomes [6]. These risks highlight the importance of continuous research, discussions, and efforts by the research community, technology industry, and governments to protect children from social media's psychological impacts, harm, and abuse.\nThis paper explores the challenges associated with social media use by children, along with potential solutions, guided by the following research questions:\n\u2022\nWhat factors contribute to the psychological impacts of algorithmic and AI-driven social media on teenagers?\n\u2022\nHow can legislators, social media providers, and educators address these challenges to minimize the impact of social media?\n\u2022\nWhat are the potential benefits and limitations of alternative social media models in addressing these issues?\nThe remainder of the paper is organized as follows. Section II discusses social media's psychological impact. Section III presents the current meta-issues with social media and content consumption. Section IV details the necessary actions to be taken by governments, educational institutions, and the social media industry, followed by the positive potential of Mastodon in Section V. Section VI outlines actionable recommendations and Section VII concludes the paper."}, {"title": "II. PSYCHOLOGICAL IMPACTS OF SOCIAL MEDIA ON TEENS", "content": "The widespread use of social media by children and its effect on their mental health has garnered significant attention from the media, researchers, and governments recently. Several studies emphasize the connection between social media usage and psychological problems among children and teenagers [7], [8].\nU.S. Surgeon General Dr. Vivek Murthy issued an advisory in May 2023 highlighting the risk social media poses to children's mental health and well-being [6]. According to a 2023 survey, nearly half of the 1453 surveyed children between the ages of 13 and 17 in the United States used social media apps almost constantly, which doubled compared to 2014-2015 [4]. The other half reported using social media several times a day. Another survey of 1480 children between 13 and 17, conducted by Boston Children Digital Wellness Lab in 2022, reported that children spent an average of 8.2 hours daily on social media, and 57% felt they use it too much [5]. In another survey by Boston Children Digital Wellness Lab in 2023 [9], 38% of the children reported a negative (e.g., uncomfortable, unsafe) experience with social media platforms.\nA systematic review conducted by Keles, McCrae, and Grealish [8] indicated a noteworthy association between extensive social media usage and its impact, including psychological distress, anxiety, and depressive symptoms among teenagers. The review highlighted factors such as the constant need for social validation and exposure to cyberbullying as contributing factors [8]. The harmful effects of social media can extend to severe outcomes such as suicidal thoughts and behavior. Multiple studies have documented cases where excessive use of social media, screen time, and online victimization have led to tragic outcomes among adolescents [10]\u2013[13].\nThese findings highlight the immediate need for action and effective strategies to mitigate social media's negative psychological impacts on children."}, {"title": "III. THE RISE OF DOOM SCROLLING: A SOCIAL MEDIA PHENOMEN\u039f\u039d", "content": "Doom scrolling refers to the act of mindlessly scrolling through social media, often out of boredom, habit, or anxiety. For many individuals, their smartphones have become a trusted companion, offering a welcome respite from the demands of daily life. This reliance on mobile devices was particularly pronounced during the COVID-19 pandemic, as people sought solace in their phones during extended periods of isolation and remote work [14], [15].\nAccording to Pew Research Center findings [16], YouTube is the most frequently used social media platform among teenagers, with an overwhelming majority (77%) using it every day. While TikTok usage is not as widespread, still a significant proportion (58%) of teens engage with this app daily. Instagram and Snapchat are also popular choices for daily use, with roughly half (50% and 51%, respectively) of teens reporting they use these platforms at least once a day. Interestingly, only about one in five teens (19%) say they use Facebook daily.\nShort-form video content has become increasingly popular, with TikTok, Snapchat, and YouTube emerging as the leading platforms for sharing bite-sized videos. Research suggests that prolonged consumption of short-form videos can lead to difficulties in concentration, information retention, and a preference for instant gratification over longer content, ultimately affecting attention span and academic focus [17]. As we mindlessly scroll through social media feeds, the constant influx of short-form videos can create a sense of FOMO (fear of missing out) that drives us to keep watching. But this addiction to instant entertainment comes at a cost, as our brains become conditioned to crave quick hits of dopamine rather than engage in more meaningful activities.\nHumans have a natural limitation when it comes to focused attention. Our brains can only maintain concentration for so long before we begin to lose momentum. This capacity for sustained focus is not set in stone; various factors, such as the complexity of the task, personal interest, motivation levels, and individual experiences, can influence it [18]."}, {"title": "IV. WHAT CAN BE DONE?", "content": "The impact of social media on teenagers' mental health is a pressing concern that requires immediate attention. A comprehensive approach must be considered to address this issue, involving government regulations, industry-led initiatives, and educational programs.\nA. Government Regulations\nGovernments and policymakers have obligations to regulate emerging technologies, but they face scrutiny and pushback from the technology industry on their decisions [19]. At the same time, technology companies are criticized for their technology designs and their social and political impacts. Technology providers are often called to proactively consider the ramifications of their decisions early in the design process. However, they may not understand or be unwilling to respond to social issues, thus people demand their political leaders take necessary actions [20].\nIn the last few years, governments worldwide have actively introduced regulations to protect their citizens, especially children, from social media harm. Regulations and legislation can be critical in protecting children from social media harm. Legislation serves as the foundation for establishing a legal framework and allocating resources to address instances of misconduct. Well-crafted legislation enables governments to influence how social media platforms operate, particularly for the younger demographic [21].\nIn the United States, federal and state governments have introduced and enacted various legislation to protect children from social media harm. Table I outlines the recent legislation enacted or introduced at the state and federal levels in addition to other existing legislation, such as the Children's Online Privacy Protection Act (COPPA) [22]. Section 230 [23], an amendment to the Communications Act of 1934 [24] and enacted as part of the Communications Decency Act of 1996, is a significant topic in any discussion about U.S. legislation related to social media. Section 230 grants immunity to social media providers and users, and consists of two key parts. Section 230(c)(1) outlines that service providers or users cannot be considered the publisher of any information provided by other users, and social media service providers or users cannot be held liable for acting in good faith to limit access to certain types of objectionable material. [25]-[27].\nOn April 30, 2024, Senator Brian Schatz introduced new federal legislation, the 'Kids Off Social Media Act' [28]. This bill prohibits the use of social media by children under 13 and personalized recommendations for those between the ages of 12 and 17. In addition, it requires social media platform companies to terminate social media accounts of ages under 13 and delete their personal data [28], [29]. The new legislation also includes the Eyes on the Board Act of 2023 [30], which instructs schools to restrict and monitor the use of social media for kids on school devices and networks and implement a screen time policy. The Earn IT Act of 2023 [31] establishes a National Commission on Online Child Sexual Exploitation Prevention. This bill limits the liability protections of social media providers concerning claims related to child sexual exploitation. In addition, the Earn IT Act of 2023 changes the reporting requirements for social media providers for child sexual exploitation reports to the National Center for Missing and Exploited Children, including technical facts reports and preserving the content for more than one year. The Strengthening Transparency and Obligations to Protect Children Suffering from Abuse and Mistreatment (STOP CSAM) Act of 2023 [32] mandates child abuse reporting, expands protections for child victims, empowers victims to request removal of child sexual abuse material from tech platforms, and holds tech companies accountable and class action for promoting or facilitating online child sexual exploitation. Additionally, it strengthens CyberTipline reporting requirements and mandates social media companies to submit annual reports on promoting a culture of safety for children on their platforms [32].\nIn 2024, the Utah government passed two pieces of legislation, the Utah Minor Protection in Social Media Act (S.B. 194) [33] and the Utah Social Media Amendments (\u0397.\u0392. 464) [34], which will be enacted in October 2024 to protect from social media harms and hold social media companies accountable for any mental health issues due to algorithmically curated social media service.\nSimilarly, New York state passed legislation, Senate Bill S7694 [35], called the Stop Addictive Feeds Exploitation (SAFE) Act, to protect children from addictive feeds, provide a mechanism for parents to control their usage, and require parental consent for notifications between 12 AM and 6 AM. It highlights the importance of protecting our young generation from social media harms. These efforts by the U.S. government also acknowledge the harms and effects of social media on children.\nGovernment regulations are critical tools for a secure online experience and ensuring social media platforms adhere to fundamental safety rules for young and minor users. Regulations and laws can hold social media providers accountable, help increase transparency, and establish data protection measures. However, regulations are just one piece of the puzzle. Regulations must be part of a broader strategy to protect minors in the current technology age, including education, technological advancement, and industry-led and collaborative initiatives.\nCertain regulations propose that age verification needs to be completed to access social media. However, this brings the challenge of personal data security. Social media providers must regulate how and where personally identifiable data is stored and used. This data should not be used for other purposes like data mining, machine learning, or monetization. The government must establish an enforcement body to closely monitor the implementation of legislation, with the power to take action against social media providers. Executives of social media providers need to be held personally accountable for decisions that have caused irreparable harm to society. Finally, if a provider is unable to fulfill their obligations, then the enforcement body should have the authority to initiate a mechanism such as a kill-switch, shutting down the platform nationwide.\nB. Industry-led Initiatives\n1) Content Moderation: Social media platforms must prioritize effective content moderation to ensure that online discourse remains respectful and safe for all users. With millions of individuals sharing their thoughts and opinions in real-time, social media enables diverse perspectives but also risks posting views that are considered offensive, harmful, or extreme by many users [36].\nArtificial intelligence (AI) systems are not yet capable of making nuanced judgments about the context, intent, and cultural subtleties required for effective content moderation [37]. AI algorithms rely on patterns learned from large datasets, which can lead to biases and inaccuracies when applied to real-world scenarios. Moreover, content moderation often requires empathy, creativity, and domain expertise, qualities that are difficult for AI systems to replicate. Human judgment is still essential in cases where the context is unclear or the situation requires understanding the subtleties, such as sarcasm, irony, or cultural references. While AI cannot replace human judgment entirely, it can be instrumental to human content moderators by triaging and prioritizing content based on language or context. For instance, AI algorithms can quickly analyze vast amounts of data to identify potentially problematic content, such as hate speech, violence, or explicit imagery, reducing the workload for human moderators. Additionally, AI-powered tools can detect language patterns, such as profanity, threats, or harassment, and flag them for further review by humans [38].\nThe power of human-AI collaboration lies in combining Al's triaging capabilities with human content moderators' expertise, creating a more effective and efficient content moderation process. This synergy begins when AI identifies potentially problematic content and flags it for human review, allowing trained moderators to assess flagged content, taking into account context, intent, and cultural nuances. Moderators then work with AI algorithms to make informed decisions, combining their expertise with the Al's insights [39].\nAI can analyze context clues, like surrounding text, hashtags, or user profiles, to better understand the intent behind a piece of content. This contextual understanding can help human moderators make more informed decisions about whether content violates community guidelines. AI algorithms can prioritize content based on factors like severity, audience reach, or potential harm caused, allowing human moderators to focus on the most critical cases.\n2) Data Practices and Digital Responsibility: The culprit behind this addiction-fueled chaos is the relentless pursuit of data-driven profits. Social media companies harvest vast amounts of user data, including location information, search queries, browsing history, and even biometric data (e.g., facial recognition), to create targeted advertisements that are tailored to our individual preferences. This data is then sold to third-party advertisers, who use it to craft persuasive messages that can sway our purchasing decisions.\nTeenagers today are growing up in a world where social media is integral to their daily lives. They use social media platforms to connect with friends, family, and influencers. However, data privacy and security concerns increase as they share more about themselves online, raising questions about long-term consequences of their digital footprints.\nThe repeated instances of data and privacy breaches among major organizations, including social media giants, have led to a decline in customer trust and increased concerns about online security and personal data protection [40]. The consequences of such breaches are far-reaching and devastating, with potential outcomes including identity theft [41], financial losses [42], and even emotional distress [43]. As millions of individuals entrust these platforms with their most intimate details, from passwords to credit card numbers, any lapse in security can have catastrophic repercussions.\nThe digital advertising sector heavily depends on the use of behavioral tracking, which involves monitoring consumers' online activities to refine targeted advertising efforts [44]. Teenagers are particularly vulnerable to targeted advertising, as they may not fully understand how their online behavior is being tracked and used to influence their purchasing decisions [45]. The European Union's Digital Services Act (DSA) [46] marks a positive step towards protecting consumers by prohibiting misleading practices and certain forms of targeted advertising, including those that target children or use sensitive data. Additionally, the DSA aims to prevent \"dark patterns\" and other deceptive tactics designed to manipulate users' decisions. The DSA serves as a model for other countries, emphasizing the need for similar regulations to safeguard consumer rights and promote transparency in the digital market.\nC. Educational Programs\nThe responsibility for shaping responsible digital behavior extends beyond government and industry to include educational institutions, which have a unique opportunity to influence the next generation's online habits. Educational institutions such as schools can become the first step in helping navigate the complex issue of algorithm-driven social media's impact on teenagers, making it a valuable resource for those looking to support young people online [47].\nDigital citizenship encompasses not just the technical skills needed to engage online but also the ethical and responsible behaviors that come with using digital technology, involving respectful and considerate interactions [48]. Educational institutions can start by teaching students how to effectively use technology, including basic computer skills, online safety, and digital etiquette. This foundation is essential for students to navigate the digital world responsibly. Institutions can discuss the importance of online reputations and how to maintain a positive digital footprint through responsible social media use and self-reflection [49]. This helps students understand that their online actions have consequences and encourages them to think critically about their digital presence.\nEducational institutions have the power to incorporate and integrate digital literacy into existing subjects, including health education, social studies, language arts and many more [50]. Educators can employ case studies in social studies classes to investigate historical and contemporary technological developments alongside their societal implications. Students can delve into how past and present societies have adapted to new technologies, exploring themes like the evolution of online expression, data protection, and the global economic effects of technological advancements. Schools can employ role-playing exercises that present hypothetical situations, allowing students to practice responding effectively to online conflicts or encountering inappropriate content [51]. This hands-on approach enables students to develop essential skills in managing digital interactions and making responsible choices."}, {"title": "V. A GLIMMER OF HOPE: MASTODON'S PROMISE", "content": "The rise of Mastodon [52], a decentralized social media platform, offers a glimmer of hope for those seeking a more open and inclusive online space. The decentralized nature of Mastodon allows instances to scale independently, built over ActivityPub which enables different servers to communicate with each other. [53].\nMastodon's promise is simple yet profound: it allows users to create their own communities (called \"instances\") where they can engage in open discussions without the constraints of corporate algorithms or interests. This decentralized approach encourages diversity, creativity, and individuality, making it an attractive option for those seeking a more authentic online experience. In traditional centralized platforms, moderation is typically managed from a single hub, where a coordinated team of moderators works in sync to oversee content across the entire platform. In contrast, decentralized systems like Mastodon fragment these moderation capabilities and efforts, introducing added complexity. Each individual instance within the Mastodon network has its own distinct group of administrators and moderators responsible for reviewing and curating content as it flows through its unique community space. [54], [55]. When an instance is not enforcing its community guidelines or is allowing harmful content to spread, defederation can be used to block communication with that instance. This means that users from other instances won't be able to interact with the problematic instance, reducing the spread of harmful information and mitigating its impact. [56]\nTeenagers are especially well-positioned to benefit from Mastodon's community-based approach, which aims to offer a safe environment with responsible moderation, free from corporate algorithms, data collection and tracking. Such models offer young people a chance to develop healthy online habits and cultivate meaningful connections with others by providing a platform that values openness, inclusivity, and creativity.\nMastodon provides many benefits, such as a decentralized platform for free speech and community building. However, it also has downsides, notably the potential to create echo chambers [58]. An echo chamber is an environment where people are only exposed to information and perspectives that reinforce their beliefs and biases without being challenged or confronted with opposing viewpoints. When users join a Mastodon instance, they are often drawn to communities that align with their interests, values, or demographics. This can lead to a self-selecting process where like-minded individuals congregate in specific instances, creating an environment where similar perspectives and ideas dominate [59]. As Mastodon instances grow, users may curate content by favoriting, reblogging, or commenting on posts that align with their beliefs. This can create a feedback loop where only certain types of content are amplified, making it difficult for opposing viewpoints to gain traction. Mastodon lacks powerful automatic content moderation and is highly dependent on community efforts for self-moderation. This means that users are responsible for reporting and addressing any issues with content rather than relying on AI-powered moderators.\nMastodon's model is a step in the right direction towards creating a healthier, more inclusive and decentralized online ecosystem. Although this approach informs potential improvements to traditional social media platforms, it is not intended as a direct replacement, but rather serves as a complementary platform. By leveraging Mastodon's strengths and understanding its limitations, we can work towards creating a comprehensive, informed technology solution that addresses the root causes of social media's problems."}, {"title": "VI. A CALL TO ACTION", "content": "Based on our findings, we propose the following multi-pronged call to action:\n1) Strengthen Legislation and Enforcement Bodies: Government and social media companies must provide transparent and age-appropriate content guidelines. Social media providers must regulate algorithmic transparency and make regular updates available to the public. Adopting Mastodon's decentralized approach allows each instance to enforce its own public moderation policies, ensuring users understand content guidelines and promoting open-source algorithm inspection.\n2) Integrate Digital Wellness Education: Educational institutions must incorporate digital wellness education into their curriculum, focusing on psychological impacts and enabling students to develop essential skills in managing digital interactions and making responsible choices.\n3) Regularly Review and Update Policies: Conduct regular data collection and processing audits to ensure compliance with evolving regulations (e.g., GDPR, CCPA).\n4) Prioritize Teenagers' Well-being: It is essential that social media companies implement robust measures to prevent the sharing of explicit content, especially among teenagers. This involves not only flagging inappropriate material but also educating users about online safety and responsible behavior.\n5) Mitigate Algorithmic Bias through Transparent Governance: Other social media providers can emulate Mastodon's transparent governance model to reduce algorithmic bias effectively. This involves enabling open discussions and scrutiny of the decision-making process."}, {"title": "VII. CONCLUSION", "content": "This paper calls for action from the government, social media providers, and educators to work collaboratively to protect our young generation and solve social media's psychological impact. Government legislation plays a critical role in establishing the regulatory framework to establish safety rules for social media providers to protect children online. However, the enforcement of the legislation must be established. The phenomenon of doom scrolling exacerbates these mental health issues, creating a cycle of harmful content consumption that is particularly detrimental to younger, impressionable minds. The social media industry must collaborate and take initiatives such as enhanced content moderation and algorithmic adjustments. In addition, social media providers must have children-friendly safety features and bring greater accountability for their platforms.\nFurthermore, exploring alternative platforms, such as Mastodon, offers a glimpse into the potential of decentralized social media models that prioritize user control and privacy. While these platforms have challenges, they represent a valuable direction for future research and development. In summary, addressing the impact of social media on children demands an integrated approach, combining legislative action, industry responsibility, educational initiatives, and continued innovation in social media platforms."}]}