{"title": "SMARTIFY: A MULTI-AGENT FRAMEWORK FOR AUTOMATED VULNERABILITY DETECTION AND REPAIR IN SOLIDITY AND MOVE SMART CONTRACTS", "authors": ["Rabimba Karanjai", "Sam Blackshear", "Lei Xu", "Weidong Shi"], "abstract": "The rapid growth of the blockchain ecosystem and the increasing value locked in smart contracts necessitate robust security measures. While languages like Solidity and Move aim to improve smart contract security, vulnerabilities persist. This paper presents Smartify, a novel multi-agent framework leveraging Large Language Models (LLMs) to automatically detect and repair vulnerabilities in Solidity and Move smart contracts. Unlike traditional methods that rely solely on vast pre-training datasets, Smartify employs a team of specialized agents working on different specially fine tuned LLMs to analyze code based on underlying programming concepts and language-specific security principles. We evaluated Smartify on a dataset for Solidity and a curated dataset for Move, demonstrating its effectiveness in fixing a wide range of vulnerabilities. Our results show that Smartify (Gemma2+codegemma) achieves state-of-the-art performance, surpassing existing LLMs and even enhancing the capabilities of general-purpose models, such as Llama 3.1. Notably, Smartify can incorporate language-specific knowledge, such as the nuances of Move, without requiring massive language-specific pre-training datasets. This work offers a detailed analysis of various LLMs' performance on smart contract repair, highlighting the strengths of our multi-agent approach and providing a blueprint for developing more secure and reliable decentralized applications in the growing blockchain landscape. We also provide a detailed recipe for extending this to other similar use cases.", "sections": [{"title": "Introduction", "content": "Smart contracts, self-executing agreements with terms directly written into code, have emerged as a cornerstone of blockchain technology [1,2]. Their ability to automate transactions and eliminate intermediaries has led to widespread adoption in various sectors, including finance, supply chain management, and healthcare [3-5]. However, the increasing complexity of smart contracts has given rise to a growing concern: security vulnerabilities [6]. These vulnerabilities, often stemming from coding errors or design flaws, can be exploited by malicious actors, leading to significant financial losses and damage to the reputation of blockchain projects.\nThe financial implications of smart contract vulnerabilities are substantial. Reports indicate that cumulative losses from attacks against Ethereum smart contracts alone have exceeded USD 3.1 billion as of 2023 [7]. In the DeFi space, an estimated $9.04 billion has been stolen due to vulnerabilities [8]. Notable incidents like the DAO hack of 2016, resulting in a $55 million loss [9], and the Poly Network hack in 2021, where over $600 million was stolen [10], underscore the critical need for robust security measures.\nTraditional security auditing methods, while essential, often face limitations in terms of accuracy and scalability. This has spurred the exploration of automated techniques for vulnerability detection [11, 12]and repair, with Large Language Models (LLMs) emerging as a promising solution [13]. LLMs, trained on vast datasets of code, can learn"}, {"title": "Related Work", "content": "This section reviews related work in smart contract vulnerabilities, security auditing tools, traditional code repair techniques, and the emerging use of Large Language Models (LLMs) for code repair, particularly in the context of Solidity and Move.\n2.1 Smart Contract Vulnerabilities\nSmart contracts, while offering automation and trustless execution, are prone to security vulnerabilities due to their complex code, immutable nature, and the decentralized environment they operate in [24-26]. Exploiting these vul-"}, {"title": "Data Collection and Analysis Methodology", "content": "This research employs a multi-faceted approach to investigate the security of smart contracts, focusing on both Solidity and Move programming languages. The methodology encompasses the collection and analysis of three distinct datasets: Solidity-based, Move-based source code. Each dataset serves a specific purpose in addressing the research questions and contributing to a comprehensive understanding of smart contract vulnerabilities.\n3.1 Importance of Dataset Categorization\nFor several reasons, categorizing the datasets based on programming language (Solidity and Move) and code represen-tation is crucial. It allows for a focused analysis of language-specific vulnerabilities and coding practices. As a more"}, {"title": "Evaluation of Smartify", "content": "Smartify is designed with two core functionalities: detecting and repairing unsafe coding patterns in smart contracts. To rigorously evaluate these capabilities, we utilize the previously described datasets, encompassing both Solidity and Move code. The evaluation process focuses on the complete output of Smartify rather than individual components, reflecting its nature as an integrated solution for smart contract security. Performance is measured using the Pass@1 score."}, {"title": "Agent-Based Code Repair Process for Smart Contracts", "content": "Our approach leverages a multi-agent system inspired by established software development methodologies but specif-ically tailored for the automated repair of Solidity and Move smart contracts. This system employs five specialized agents: an Auditor, an Architect, a Code Generator, a Refiner, and a Validator. The process incorporates a self-refinement loop and a final validation step, ensuring a high degree of accuracy and security. Each agent plays a distinct role in a structured workflow, detailed below."}, {"title": "Agent Roles and Responsibilities", "content": "\u2022 Auditor: This agent is the cornerstone of the security analysis. It is fine-tuned on a comprehensive corpus of Solidity and Move code documentation, encompassing syntax, semantics, and best practices. Furthermore, it is safety-aligned using a classifier adapted from Google's Responsible AI toolkit. This classifier has been meticulously modified to enforce language-specific rules and safe coding practices, effectively preventing the generation of unsafe or unsupported code constructs.\nThis alignment is of paramount importance. For Move, it ensures that generated code strictly adheres to the con-ventions of the target blockchain (e.g., Sui or Aptos). It prevents the accidental introduction of elements from one Move variant into another or the inclusion of unsupported Rust paradigms. This is crucial because Move, while derived from Rust, has its own unique features and limitations. For Solidity, it enforces established security best practices and prevents the generation of code patterns known to be vulnerable.\nThe Auditor's primary responsibility is to meticulously scan the input smart contract code (either Solidity or Move) to identify potential vulnerabilities and unsafe patterns. It's secondary, yet vital, role is to serve as the final validator of the repaired code.\n\u2022 Architect: This agent receives the output from the Auditor, which includes a detailed report of identified vulnera-bilities and unsafe code segments. The Architect's role is to devise a high-level strategic plan for addressing these issues. This plan does not involve generating code directly. Instead, it outlines the necessary modifications, refac-toring, and improvements required to rectify the identified problems. This plan serves as a comprehensive blueprint for the Code Generator, guiding the code repair process.\n\u2022 Code Generator: This agent is a general-purpose code LLM. Its strength lies in its ability to leverage Retrieval-Augmented Generation (RAG) from two distinct data stores, one dedicated to Solidity and the other to Move. These data stores contain a collection of best practices and relevant documentation for respective programming language.\nUsing the Architect's plan as a guide, the Code Generator selects and adapts relevant examples from the appropriate RAG datastore. This dynamic, context-aware retrieval of few-shot examples significantly enhances the Code Gener-ator's ability to produce accurate and secure code repairs. It ensures the generated code adheres to language-specific conventions and incorporates established best practices.\n\u2022 Refiner: This agent's role is to enhance the quality of the code produced by the Code Generator. It achieves this through a process of iterative self-refinement, essentially acting as its own critic. The Refiner uses the same underlying LLM as the Code Generator but with a different prompt that focuses on improving the code quality based on best practices and potential improvements that it might detect from a higher level.\n\u2022 Validator: This agent acts as a final checkpoint in the process. It re-employs the Auditor agent to re-evaluate the code after the refinement stage. The Validator's objective is to ensure that all previously identified vulnerabil-ities have been adequately addressed and that no new vulnerabilities have been introduced during the repair and refinement process."}, {"title": "Smartify System Architecture and Workflow", "content": "Smartify operates through a five-agent system designed for automated smart contract vulnerability detection and repair.\nThe system functions as shown in Figure 1.\nThe Smartify system operates in a five-phase process to automatically repair smart contract code. Firstly, in the Input & Initial Audit phase, the smart contract code, written in either Solidity or Move, is fed into the system. The Auditor, an LLM based on Gemma2 9B, analyzes the code to detect potential vulnerabilities and produces a report detailing its findings. Secondly, during Repair Planning, the Architect receives this vulnerability report and formulates a high-level repair plan that outlines the necessary code modifications to address the identified issues. Thirdly, in Code Generation & Refinement, an LLM called CodeGemma which has been fine-tuned for code generation, and is equipped with Retrieval-Augmented Generation (RAG) capabilities, takes the lead. It utilizes separate Move RAG and Solidity RAG components to provide language-specific context. The Code Generator, part of CodeGemma, uses the repair plan to generate the modified code, selecting the appropriate RAG based on the input language and having the capability to perform Solidity to Move translation when necessary. Subsequently, a Self Refinement process is initiated, and the Refiner component iteratively improves the generated code's quality, readability, and efficiency. Fourthly, in the Validation phase, the Validator (which is the same agent as the Auditor) performs a final security audit on the refined code to ensure that all identified vulnerabilities have been resolved. Finally, the system outputs the repaired smart contract code.\nThe process may iterate back to step 3 or 4 if the Validator identifies any issues. Each step plays a vital role in ensuring the accurate and secure repair of smart contract code. The workflow is designed to be efficient and effective, leveraging the strengths of each agent to achieve the desired outcome."}, {"title": "Agent Prompting Strategy", "content": "The agents within Smartify are driven by carefully crafted prompts that guide their actions and ensure consistent performance. We employ a standardized prompt template, adapted from established practices in LLM-based agent systems. The template is structured as follows:\nPrompt Template\nRole: You are a [role] specializing in [Solidity/Move] smart contracts.\nTask: [task]\nInstruction: Based on the provided Context, please follow these steps: [numbered steps]\nContext:\nThis template is broken down into the following components.\nEach agent in our framework is defined by four key components: the Role, which designates the agent's specific function (such as Auditor, Architect, or Code Generator); the Task, which outlines the agent's specific objectives; the Instruction, which provides detailed step-by-step guidance using chain-of-thought reasoning; and the Context, which encompasses all necessary information including input code, audit reports, architectural plans, RAG datastore examples, and inter-agent conversation history."}, {"title": "Hardware and Model Fine-tuning", "content": "The development and deployment of Smartify leveraged a heterogeneous compute environment, utilizing both high-performance GPUs for computationally intensive tasks and a more resource-efficient setup for inference.\n4.2.1 Fine-tuning Setup\n\u2022 Hardware: Fine-tuning leveraged a cluster of four NVIDIA A100 GPUs for computationally demanding pattern learning in Solidity and Move code.\n\u2022 Model: Based on the Gemma 9B model, selected for strong code-related task performance and fine-tuning adapt-ability, particularly in instruction following. Fine-tuned on a dataset of Solidity and Move code, vulnerability exam-ples, best practices, and documentation, augmented with outputs from earlier pipeline stages to enhance safety issue detection.\n\u2022 Training Recipe: Supervised learning paradigm. Trained to predict correct outputs (e.g., vulnerability reports, safe code patterns) from inputs (e.g., Solidity/Move code, vulnerability descriptions).\n4.2.2\nData Preprocessing: Tokenization, normalization, and input-output pair creation ensured data consistency and quality.\nHyperparameter Optimization: Learning rate (1e-5), batch size (8, due to memory constraints), and training epochs (5, as validation loss plateaued) optimized via grid search and manual tuning.\nRegularization: Dropout and weight decay used to prevent overfitting and improve generalization.\nEvaluation Metrics: Accuracy, precision, recall, and F1-score on a held-out validation set monitored model performance.\nInference Setup\n\u2022 Hardware: Inference was performed on a single NVIDIA RTX 4090 GPU, balancing performance and cost-effectiveness for real-time code repair.\n\u2022 Models:\nCode Generator and Refiner: These agents utilize a fine-tuned CodeGemma model, initially pre-trained on a limited Move corpus and further instruction-tuned to follow Architect-generated \"recipe\" patterns. Fine-tuning on Architect outputs ensured it understood these instructions, and pre-training on a limited Move corpus ensured basic syntax understanding.\nComparison Model: A stock Llama 3.1 model was used in some experiments for comparative analysis, helping assess the gains from fine-tuning and instruction tuning."}, {"title": "Key Considerations", "content": "\u2022 A balance between performance requirements, resource availability, and cost considerations drove the choice of hardware and models.\n\u2022 The fine-tuning process for the Auditor was particularly resource-intensive due to the complexity of the task and the size of the model.\n\u2022 The use of a smaller, more efficient GPU for inference makes the system more accessible for practical deployment.\n\u2022 The comparison with a stock Llama 3 model provides valuable insights into the effectiveness of our fine-tuning and instruction-tuning strategies.\nThis heterogeneous setup, combining high-performance GPUs for training and a more efficient GPU for inference, allows Smartify to effectively address the computational demands of both model development and deployment. The detailed description of the fine-tuning process provides transparency and allows for replication of our results."}, {"title": "Experimental Results and Discussion", "content": "We run our experiments as defined in our Section 3.3. We report the results as well as the empirical performance of our models. Through that we will try to answer our Research Questions one by one in this section.\nAlong with Smartify we have ran the benchmark for the following models."}, {"title": "Solidity", "content": "This section presents the evaluation results of various code generation models on the task of repairing vulnerabilities in Solidity smart contracts, specifically focusing on the \"Not So Smart Contracts\" dataset from the Trail of Bits GitHub repository. This dataset is a collection of intentionally vulnerable Solidity contracts, designed to test the ability of automated tools to detect and repair common security flaws. It contains a diverse set of vulnerabilities, including reentrancy, integer overflow/underflow, access control issues, and timestamp dependence, among others. The dataset has been publicly available for a significant period, raising the possibility that some or all of its contents might be present in the pre-training data of the evaluated models. We analyze the performance of these models based on two key metrics: the number of vulnerabilities fixed and the average inference time, as summarized in Table 5 and Figure 2. We also introduce our framework, Smartify, and demonstrate its effectiveness in enhancing model performance.\nThe results reveal significant performance disparities among the evaluated models. Among the pre-trained models for Solidity CodeGemma surprisingly emerges as a top performer, successfully fixing 16 vulnerabilities with a relatively low average inference time of 96.5 seconds. This suggests that CodeGemma possesses a strong ability to understand and rectify code vulnerabilities while maintaining reasonable efficiency. However since most of these Solidty smart contracts were part of open githubs repositories, there can be a strong possibility fo these already being part of the pertaining data. Our proposed framework, Smartify (Gemma2+CodeGemma), achieves comparable performance, also fixing 16 vulnerabilities, albeit with a slightly higher average inference time of 112.3 seconds. This increased"}, {"title": "Move Code Repair", "content": "This section analyzes the efficacy of various models in repairing vulnerabilities within Move smart contracts, as de-tailed in Table 6. The evaluation encompasses eight distinct vulnerability categories: Unchecked Return (UR), Infinite Loop (IL), Unnecessary Boolean (UB), Unused Constant (UC), Unused Private Function (UPF), Unnecessary Type Conversion (UTC), Overflow (Ov), and Precision Loss (PL) following the works of Song et al [14]. The metrics presented in the table represent the number of successfully repaired instances for each vulnerability type, with higher values indicating superior performance. The inference time, measured in seconds, is also provided for each model.\nThe results demonstrate a significant variance in performance across the evaluated models. Notably, the larger lan-guage models, such as deepseekcoder 33b and llama3.3 70b, exhibit a relatively higher number of successful repairs across multiple categories, albeit with a corresponding increase in inference time. Conversely, smaller models like deepseekV2 and llama3.2 demonstrate limited repair capabilities. The specialized tools for Move code, namely Move Prover, MoveLint, and MoveScan, were employed as a benchmark for comparison. It is crucial to note that these tools are designed for vulnerability detection rather than repair. MoveScan, in particular, identified a substantial number of instances across all categories, highlighting its effectiveness as a static analysis tool. Move Prover demon-strated proficiency in detecting Overflow and Precision Loss vulnerabilities, while MoveLint focused on Unused Private Functions and Unnecessary Type Conversions.\nThe Smartify models, which leverage a combination of Gemma2 with either codegemma or llama3.1, present an interesting case. Smartify (Gemma2+codegemma) and Smartify (Gemma2+llama3.1) outperform several individual models in multiple categories. This is likely because the specialized models are fine-tuned on the Move-specific dataset. For instance, Smartify (Gemma2+codegemma) achieves the highest number of repairs for the Unchecked Return, Infinite Loop, Unused Boolean, Unused Constant, Unused Private Function, Unnecessary Type Conversion, and Overflow categories, showcasing a substantial improvement over individual models in these areas. However, it is worth mentioning that they also have limitations compared to individual models for certain categories like Precision Loss.\nThis answers our first two research questions.\nRQ1 & RQ2 - Code Understanding and Vuln. Detection\nYes. Our empirical analysis with Smartify, especially with using a fine-tuned code-gemma and also using a vanilla pre-trained llama3.1, has shown us the effectiveness of the framework's ability to understand code. And to capture bad practices leading to vulnerability. Especially for a low-resource code like move. Without significant fine-tuning (in the case of llama3.1)."}, {"title": "Conclusion", "content": "This work addresses the pressing need for enhanced security in the burgeoning blockchain ecosystem. We investigate the application of Large Language Models (LLMs) to smart contract vulnerability detection and repair, focusing on Solidity and Move. We introduce Smartify, a novel multi-agent framework that significantly improves LLM performance in this critical domain. The contributions of this work are: (1) Smartify, a novel multi-agent framework that enhances LLM-based smart contract vulnerability detection and repair; (2) a method for encoding language-specific knowledge, valuable for low-resource languages like Move; (3) a scalable, adaptable approach applicable to other programming languages and LLMs; (4) a demonstration of Smartify's efficacy on generalized pre-trained LLMs; and (5) a detailed analysis of the challenges inherent in automated code repair.\nSmartify represents a significant advancement in automating smart contract security, a crucial concern in the expand-ing blockchain landscape. Future work will refine the framework, expand its language coverage, particularly within the blockchain domain, and integrate it into real-world blockchain development workflows. This research lays the foundation for AI-powered tools that can bolster the security and reliability of decentralized applications, fostering a more robust and trustworthy blockchain ecosystem."}]}