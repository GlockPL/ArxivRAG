{"title": "Adaptive Uncertainty Quantification for Generative AI", "authors": ["Jungeum Kim", "Sean O'Hagan", "Veronika Ro\u010dkov\u00e1"], "abstract": "This work is concerned with conformal prediction in contemporary applications (including generative AI) where a black-box model has been trained on data that are not accessible to the user. Mirroring split-conformal inference, we design a wrapper around a black-box algorithm which calibrates conformity scores. This calibration is local and proceeds in two stages by first adaptively partitioning the predictor space into groups and then calibrating sectionally group by group. Adaptive partitioning (self-grouping) is achieved by fitting a robust regression tree to the conformity scores on the calibration set. This new tree variant is designed in such a way that adding a single new observation does not change the tree fit with overwhelmingly large probability. This add-one-in robustness property allows us to conclude a finite sample group-conditional coverage guarantee, a refinement of the marginal guarantee. In addition, unlike traditional split-conformal inference, adaptive splitting and within-group calibration yields adaptive bands which can stretch and shrink locally. We demonstrate benefits of local tightening on several simulated as well as real examples using non-parametric regression. Finally, we consider two contemporary classification applications for obtaining uncertainty quantification around GPT-40 predictions. We conformalize skin disease diagnoses based on self-reported symptoms as well as predicted states of U.S. legislators based on summaries of their ideology. We demonstrate substantial local tightening of the uncertainty sets while attaining similar marginal coverage.", "sections": [{"title": "1 Introduction", "content": "Conformal prediction [37] is arguably one of the most widely used tools for predictive uncertainty quantification. We develop a conformal prediction method in the context of contemporary applications involving black-box predictive systems (such as generative models including ChatGPT [4], BERT [8], LLaMA [34], and Watson Assistant [9]), which have been pre-trained on massive datasets that are obscured from the user.\nIn the absence of the training dataset, split-conformal prediction (isolating fitting from ranking) is a viable alternative [18, 19] to full conformal prediction as it avoids model re-fitting on augmented training data. Split-conformal prediction ranks conformity scores on the calibration dataset and guarantees finite-sample marginal coverage without distributional assumptions or any knowledge of the actual mechanism inside the pre-trained model. While standard conformal prediction bands are marginal over the covariates, and thereby overly conservative, a conditional coverage guarantee is not achievable in general [2, 16, 36]. This has motivated the development of various refinements that achieve some relaxed form of conditional coverage [13, 35, 36] such as group-conformal prediction [2, 17].\nLocally adaptive conformal prediction bands have been sought that shrink and stretch depending on the difficulty of the prediction problem at each local point [6, 19, 27]. In particular, Lei et al. [19] locally enhance conformal prediction by rescaling the conformity scores with a function fitted on the absolute residuals of the training data. This approach is not feasible without access to training data and, moreover, can lead to underestimation of the prediction error when the black-box model has overfitted the training data [27]. Alternatively, Romano et al. [27] conformalize quantile estimates so that the tightness of the prediction bands depends on the tightness (conformity) of the quantile black-box model in use. Rossellini et al. [29] further build on this by incorporating the uncertainty in the quantile estimation into the prediction intervals. Chernozhukov et al. [6] generalize this conformalized quantile regression as a fully quantile-rank-based method by using a probability integral transform with a distributional regression estimator (a model of the conditional CDF). These approaches require an auxiliary function (residuals, quantiles, or CDFs), which is obtained using the training dataset. In this work, we concern ourselves with the modern scenario where the users are provided a basic black-box predictive model"}, {"title": "2 Conformal Tree", "content": "The core idea behind our self-grouping conformal prediction method is to learn the group information by applying a tree model to the conformity scores of the calibration dataset (Section 2.1). Since this may break the exchangeability assumption between the test and calibration data points, we develop a tree algorithm that is robust to adding a new test observation (Section 2.2)."}, {"title": "2.1 Calibration by Self-grouping", "content": "Suppose that we are given a trained black-box model \\$\\hat{f}(\\cdot)\\$, a conformity score function \\$\\mathcal{S}:\\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}\\$ and a calibration dataset \\$\\{(X_i, Y_i)\\}_{i=1}^n\\$. For a new test point \\$X_{n+1}\\$, we want to predict the response \\$Y_{n+1}\\$ under the assumption that \\$(X_1,Y_1), ...., (X_{n+1}, Y_{n+1})\\$ are i.i.d.\nThe split-conformal prediction interval (or a prediction set) for \\$1 - \\alpha\\$ coverage is defined as\n\\$\\begin{equation}\n\\hat{\\mathcal{C}}_n(X_{n+1}) = \\{y : \\mathcal{S}(X_{n+1}, y) \\leq \\mathcal{S}^*(\\mathcal{D}_{1:n})\\},\\tag{2.1}\n\\end{equation}\\$\nwhere \\$\\mathcal{S}^*(\\mathcal{D})\\$ is the \\$\\left[\\frac{\\lfloor (n + 1) \\cdot (1 - \\alpha) \\rfloor}{n}\\right]\\$-quantile of the conformity scores in \\$\\mathcal{D}\\$ and where \\$\\mathcal{D}_{1:n} = \\{(X_i, S_i)\\}_{i=1}^n\\$. For \\$\\mathcal{C}_n (X_{n+1})\\$, the coverage is guaranteed [16, 37] to satisfy\n\\$\\begin{equation}\n\\mathbb{P}\\{Y_{n+1} \\in \\hat{\\mathcal{C}}_n(X_{n+1})\\} \\geq 1 - \\alpha.\n\\end{equation}\\$\nNote that the prediction interval construction relies on the model's performance in the overall input space. For example, with the absolute residual score \\$\\mathcal{S}(x, y) = |y - \\hat{f}(x)|\\$, the set \\$\\mathcal{C}_n(X_{n+1})\\$ is equivalent to \\$\\left[\\hat{f}(X_{n+1}) - \\hat{q}, \\mu(X_{n+1}) + \\hat{q}\\right]\\$, where a constant \\$\\hat{q} = \\mathcal{S}^*(\\mathcal{D}_{1:n})\\$ is added and subtracted over all the data domain \\$\\mathcal{X}\\$."}, {"title": "2.2 Robust Regression Trees", "content": "Regression trees [3] have been widely used in the nonparametric regression literature due to their interpretability. Trees employ a hierarchical set of rules in order to partition the data into subsets on which the response variable is relatively constant. Typical approaches to fitting trees involve greedily descending from the root note and making splitting decisions in order to maximize a utility criterion, subject to regularization constraints.\nFor many popular criteria used to assess tree fitting, such as variance reduction, it is hard to precisely understand the effect of including a single new data point on the resulting tree structure. We introduce a novel criterion that aims to produce a tree that is robust to the inclusion of an additional i.i.d. data point with high probability.\nWe focus on dyadic trees where splits along covariates may only be placed at specific predetermined locations (dyadic rationals for continuous predictors rescaled to a unit interval). In general, our implementation allows for discrete predictors where dummy variable are created and the split occurs at 1/2. Ordinal discrete covariates are rescaled and treated as continuous."}, {"title": "2.2.1 Full-Conformal with Robust Trees", "content": "Having an algorithm that is robust to adding one observation invites the possibility of performing actual full conformal prediction. Indeed, when a robust tree serves as a regression model \\$\\hat{\\mu}\\$, not only as a tool for partitioning, standard full conformal prediction can be obtained efficiently. Consider an augmented dataset \\$\\{(X_i, Y_i)\\}_{i=1}^n \\cup \\{(x, y)\\}\\$, where \\$(x, y)\\$ is a possible realization of \\$(X_{n+1}, Y_{n+1})\\$. Denote by \\$\\hat{\\mu}^{(x,y)}\\$ a tree model fitted on the augmented dataset. The conformity score of \\$(X_i, Y_i)\\$ is defined by \\$\\mathcal{S}^{(x,y)}(X_i, Y_i) = |Y_i - \\hat{\\mu}^{(x,y)}(X_i)|\\$. The standard full conformal prediction band for \\$1 - \\alpha\\$ coverage [19] is defined by\n\\$\\begin{equation}\n\\hat{\\mathcal{C}}_{full}(X_{n+1}) = \\{y: \\mathcal{S}(X_{n+1},y) (X_{n+1},y) \\leq \\mathcal{S}^* (\\mathcal{D}_{1:n}^{(X_{n+1},y)})\\},\\tag{2.5}\n\\end{equation}\\$\nwhere \\$\\mathcal{D}_{1:n}^{(x,y)} = \\{(X_i, \\mathcal{S}^{(x,y)}(X_i, Y_i))\\}_{i=1}^n\\$.Here, for a given \\$X_{n+1}\\$, we need to fit the model for all possible \\$y\\$ values to compute \\$\\hat{\\mathcal{C}}_{full}(X_{n+1})\\$ in (2.5).\nThis computational burden is reduced if the fitted model is robust to varying the values of \\$(x, y)\\$. Note that our robust tree regression guarantees the partition is the same with high probability (Lemma 1, with probability in (3.2)). By defining the jump coefficient on each box as the center of the range of the response variables whose covariates fall into the box, we can guarantee with the same high probability that \\$\\hat{\\mu}^{(x,y)} = \\hat{\\mu}_{1:n}\\$ for any possible \\$(x, y)\\$, where \\$\\hat{\\mu}_{1:n}\\$ is the robust tree fitted on \\$\\{(X_i, Y_i)\\}_{i=1}^n\\$. For this \\$\\hat{\\mu}_{1:n}\\$, define the conformity score \\$\\mathcal{S}_{n}(X_i, Y_i) := |Y_i - \\hat{\\mu}_{1:n}(X_i)|\\$. Since the score function does not depend on \\$(x, y)\\$, the resulting full conformal prediction band now resembles the split-conformal inference. Indeed, the full conformal prediction band in (2.5) becomes equivalent to\n\\$\\begin{equation}\n\\hat{\\mathcal{C}}_{full}(X_{n+1}) = \\{y : \\mathcal{S}(X_{n+1}, y) \\leq \\mathcal{S}^*(\\mathcal{D}_{1:n})\\},\\tag{2.6}\n\\end{equation}\\$\nor equivalently,\n\\$\\begin{equation}\n\\hat{\\mathcal{C}}_{full}(X_{n+1}) = [\\hat{\\mu}_{1:n} - \\mathcal{S}^* (\\mathcal{D}_{1:n}), \\hat{\\mu}_{1:n} + \\mathcal{S}^* (\\mathcal{D}_{1:n})],\tag{2.7}\n\\end{equation}\\$\nwhere \\$\\mathcal{D}_{1:n} = \\{(X_i, \\mathcal{S}_{n}(X_i, Y_i))\\}_{i=1}^n\\$ (only in this section). Note that \\$\\mathcal{S}^*(\\mathcal{D}_{1:n})\\$ is a constant and does not depend on \\$(x, y)\\$ but only on \\$\\mathcal{D}_{1:n}\\$ and \\$\\hat{\\mu}_{1:n}\\$."}, {"title": "3 Theoretical Underpinnings", "content": "Recall that our algorithm (Algorithm 1) applies the partitioning method (Algorithm 2) on the calibration dataset \\$\\mathcal{D}_{1:n}\\$, not the full dataset \\$\\mathcal{D}_{1:n+1}\\$. This may create some issues with exchangeability when conditioned on the resulting groups. Fortunately, conditionally on the partition \\$\\mathcal{X} (\\mathcal{D}_{1:n})\\$ estimated by our robust tree approach, the conformity score of the test point is controlled at the desired level with high probability. This property is implied by the \"unchangeability\" of the estimated partition after adding a test observation, as described below."}, {"title": "4 Simulation and Benchmark Studies", "content": "In this section, we consider regression tasks on simulated and real benchmark data. For regression tasks, we use the absolute residuals as the conformity score, and compare the average length of the prediction interval as our primary evaluation metric."}, {"title": "4.1 Conformal Tree is Adaptive", "content": "We illustrate Conformal Tree on simulated data examples, modified from previous studies [26, 29]. We let \\$X \\sim U(0, 1)\\$, and consider the following two data-generating processes with \\$n = 500\\$:\n(Data 1) \\$Y|X \\sim N(3\\sin(4/X + 0.2) + 1.5, X^2)\\$ and\n(Data 2) \\$Y|X \\sim N(\\sin(X^{-3}),0.1^2)\\$.\nThese two setups represent heteroskedasticity for distinct reasons. Data 1 has the noise standard deviation scaling with \\$X\\$, causing the distribution of residuals to vary across the domain. Therefore, any single regression (mean) model cannot avoid increasing errors\nfor the larger covariate due to the increasing variation of the data. Data 2 is difficult to model near \\$X = 0\\$ due to the \\$X^{-3}\\$ term inside the sin, causing rapid change in the mean function that requires a lot of data to model accurately. Therefore, due to the lack of model fit, the error is large for the smaller covariate values. We apply our Conformal Tree method (\\$m = 20\\$ and maximum leaves as 8) and compare it with the standard split-conformal prediction. Figure 1 highlights that Conformal Tree can adaptively reflect local changes without the necessity for an auxiliary model of quantiles or conformity scores on the training dataset. As the partitioning method is robust to the addition of a new iid data point, the obtained prediction intervals with \\$\\alpha = 0.1\\$ still have good coverage as 0.908 for Data 1 and 0.910 for Data 2 when averaged over 10 random trials. For the standard conformal prediction, the average coverage was 0.902 and 0.900, respectively. Furthermore, the average percentage of points for which the interval was narrower is 0.614 for Data 1 and 0.5 for Data 2. Figure 1 corresponds to a single random trial among the 10 random trials."}, {"title": "4.2 Benchmark Datasets", "content": "We consider various real data examples that had previously been used in the literature for comparing the efficacy of regression prediction intervals [27, 29, 31]. The number of observations in these datasets vary from 200 to about 50,000, and the number of predictor variables ranges from 1 to 100. We randomly split each dataset into training, calibration, and test sets, with proportions 0.4, 0.4, and 0.2 respectively. For each dataset, we repeat the entire procedure 10 times, including splitting the data and forming the intervals. We report the average metrics taken over these trials. For the \u201cblack-box\u201d predictive model \\$\\hat{\\mu}\\$, we consider a random forest model. Additional information about the datasets, predictive models, and experiments can be found in Supplement C, as well as additional results.\nWe observe competitive performance between Conformal Tree and the other locally adaptive variants (LWCI, CQR), where Conformal Tree achieves the minimal average in-terval width on datasets Data 1 and star, and is competitive with the best method on almost all datasets. It is important to note that the other locally adaptive variants (LWCI and CQR) require an auxiliary model fit on the training data. While Conformal Tree may not always outperform CQR, unlike CQR it is applicable when the practitioner only has access to the calibration data."}, {"title": "5 Large Language Model Uncertainty Quantification", "content": "In this section, we apply Conformal Tree to quantify the uncertainty in the output from a black-box large language model (LLM) for two downstream classification tasks. Conformal Tree uses a set of calibration data in order to partition the input space into regions where the conformity score is relatively homogenous, and yields conditional conformal guarantees in each one. These regions do not need to be prespecified by the user, which can be especially useful in cases where it is difficult for an uninformed user to designate a reasonable partition a priori. We consider predicting the U.S. state of a legislator based on a measure of their ideology in voting for bills, as well as a prediction of skin disease from a list of clinical symptoms. The latter task is meant to emulate a user asking a language model for medical advice, which we do not advocate for but rather view as a domain in which locally adaptive uncertainty quantification would be paramount. We view the LLM models as a deterministic model (with the temperature set to 0), so they provide an actual fit as opposed to prediction involving noise. For classification, we use the complement of the predicted class probability as the conformity score, as in [1, 28, 30]. Our adaptive approach enables generative LLM prediction sets to stretch and shrink based on the informativeness of the prompt."}, {"title": "5.1 Classifying States of Legislators using an LLM", "content": "Large language models have become extremely popular generative AI tools, allowing users to generate completions of text or to generate responses to questions or instructions [23]. In the current era of AI as-a-service, many performant LLMs are hosted on servers, to which the user submits a query and receives either next token probabilities or the sampled subsequent tokens themselves. In this case, the user does not have access to the models' training data, and cannot hope to apply most typical methods for local adaptivity. Trained on vast quantities of training data, state-of-the-art LLMs has been demonstrated to have an impressive \"understanding\" of the U.S. political atmosphere, proving capable of assigning scores or pairwise comparisons between perceived ideology of legislators that correlates highly with measures estimated from their voting history [22, 38]. We posit a related question in this work, questioning how well an LLM can predict the state in which a legislator serves from only their DW-NOMINATE score [24], a two-dimensional representation of their ideology based on voting data.\nThus, we consider a black-box model \\$f : \\mathbb{R}^2 \\rightarrow S^{50}\\$, where \\$S^{50}\\$ denotes a 50-dimensional simplex, or a probability measure over U.S. states. In our case, \\$f\\$ is the resulting prob-abilities assigned by an LLM when conditioning on a context that gives a specific DW-NOMINATE score, and asks where a legislator with that score would be likely to be employed. While the relationship between DW-NOMINATE and state could be alterna-tively studied using empirical data, the predictions from an LLM represent something else entirely- rather than the relationship that manifests itself in historical data, they represent the \"communal opinion\u201d, or \u201cZeitgeist\u201d, about the relationship between DW-NOMIN\u0391\u03a4\u0395, as distilled through the lengthy training process of the language model. However, social scientists may be weary of how much they can trust the output of the LLM for such a specific downstream task, given its black-box nature, which motivates a practitioner to look for conformal sets of U.S. states that are guaranteed to contain the true state of the legislator with a desired probability level.\nFor this classification task, we consider the conformity score \\$\\mathcal{S}(x,y) = 1 - f(x)_y\\$, where \\$x \\in \\mathbb{R}^2\\$ denotes a DW-NOMINATE score and \\$y \\in [50]\\$ denotes a U.S. state index. A conformal prediction set is defined by \\$\\hat{\\mathcal{C}}(x) = \\{k : f(x)_k \\geq 1 - q\\}\\$, where \\$q\\$"}, {"title": "5.1.1 Naive Uncertainty Quantification with ChatGPT", "content": "We also compare our calibrated conformal prediction sets against naive prediction sets based on ChatGPT's own assessment of its uncertainty. In this case, we set the temper-ature hyperparameter to one, meaning that the language model's response sample each subsequent token from the softmax distribution rather than deterministically choosing the highest probability token. This allows us to sample distributions on labels for each test point. For each sampled distribution \\$q \\in \\mathcal{S}^L\\$, where \\$L\\$ is the cardinality of the set of labels, we compute a naive prediction set by including the largest probability items until at least \\$1 - \\alpha\\$ is covered. That is, we can define a threshold \\$\\tau = \\inf\\{t : \\sum_{l : q_l \\geq t} q_l \\geq 1 - \\alpha\\}\\$ and define the prediction set corresponding to \\$q\\$ as\n\\$\\begin{equation}\n\\tilde{\\mathcal{C}}(q) = \\{l \\in [L] : q_l \\geq \\tau\\} .\n\\end{equation}\\$\nBecause each individual sampled \\$q\\$ for a given test point involves randomness from the model, we can combine sampled distributions \\$q^1,...,q^M\\$ for a given test point using a majority vote procedure\n\\$\\begin{equation}\n\\tilde{\\mathcal{C}} = \\{l \\in [L] : \\sum_{j=1}^M I\\{l \\in \\tilde{\\mathcal{C}}(q^j)\\} \\geq M/2\\} .\n\\end{equation}\\$\nThe conformal set \\$\\tilde{\\mathcal{C}}\\$ is a final prediction set for the given test point that combines sampled naive prediction sets that are based on the model's own quantification of its uncertainty. For our comparisons, we set \\$M = 11$.\nIn our previous example classifying U.S. states from DW-NOMINATE scores, the average size of these naive prediction sets for each method was 36.36 items, significantly larger than that of Conformal Tree (27.67)."}, {"title": "5.2 Conformalizing ChatGPT Diagnoses of Skin Diseases", "content": "Services hosting black-box generative language models like ChatGPT have now become widespread, offering powerful tools for applications ranging from customer support to cre-ative content generation. As a result, these models are being increasingly integrated into everyday applications, making sophisticated AI tooling accessible to the general public. One significant yet controversial use lies in healthcare, where people have the option to use ChatGPT to obtain preliminary diagnoses of their medical conditions based on physical descriptions and perceived symptoms [11", "32": "and to be an effective self-diagnostic tool for common orthopedic diseases [15"}]}