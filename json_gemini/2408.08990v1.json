{"title": "Adaptive Uncertainty Quantification for Generative AI", "authors": ["Jungeum Kim", "Sean O'Hagan", "Veronika Ro\u010dkov\u00e1"], "abstract": "This work is concerned with conformal prediction in contemporary applications (including generative AI) where a black-box model has been trained on data that are not accessible to the user. Mirroring split-conformal inference, we design a wrapper around a black-box algorithm which calibrates conformity scores. This calibration is local and proceeds in two stages by first adaptively partitioning the predictor space into groups and then calibrating sectionally group by group. Adaptive partitioning (self-grouping) is achieved by fitting a robust regression tree to the conformity scores on the calibration set. This new tree variant is designed in such a way that adding a single new observation does not change the tree fit with overwhelmingly large prob- ability. This add-one-in robustness property allows us to conclude a finite sample group-conditional coverage guarantee, a refinement of the marginal guarantee. In addition, unlike traditional split-conformal inference, adaptive splitting and within- group calibration yields adaptive bands which can stretch and shrink locally. We demonstrate benefits of local tightening on several simulated as well as real examples using non-parametric regression. Finally, we consider two contemporary classification applications for obtaining uncertainty quantification around GPT-40 predictions. We conformalize skin disease diagnoses based on self-reported symptoms as well as pre- dicted states of U.S. legislators based on summaries of their ideology. We demonstrate substantial local tightening of the uncertainty sets while attaining similar marginal coverage.", "sections": [{"title": "Introduction", "content": "Conformal prediction [37] is arguably one of the most widely used tools for predictive uncertainty quantification. We develop a conformal prediction method in the context of contemporary applications involving black-box predictive systems (such as generative mod- els including ChatGPT [4], BERT [8], LLaMA [34], and Watson Assistant [9]), which have been pre-trained on massive datasets that are obscured from the user.\nIn the absence of the training dataset, split-conformal prediction (isolating fitting from ranking) is a viable alternative [18, 19] to full conformal prediction as it avoids model re- fitting on augmented training data. Split-conformal prediction ranks conformity scores on the calibration dataset and guarantees finite-sample marginal coverage without distribu- tional assumptions or any knowledge of the actual mechanism inside the pre-trained model. While standard conformal prediction bands are marginal over the covariates, and thereby overly conservative, a conditional coverage guarantee is not achievable in general [2, 16, 36]. This has motivated the development of various refinements that achieve some relaxed form of conditional coverage [13, 35, 36] such as group-conformal prediction [2, 17].\nLocally adaptive conformal prediction bands have been sought that shrink and stretch depending on the difficulty of the prediction problem at each local point [6, 19, 27]. In particular, Lei et al. [19] locally enhance conformal prediction by rescaling the conformity scores with a function fitted on the absolute residuals of the training data. This approach is not feasible without access to training data and, moreover, can lead to underestimation of the prediction error when the black-box model has overfitted the training data [27]. Alternatively, Romano et al. [27] conformalize quantile estimates so that the tightness of the prediction bands depends on the tightness (conformity) of the quantile black-box model in use. Rossellini et al. [29] further build on this by incorporating the uncertainty in the quantile estimation into the prediction intervals. Chernozhukov et al. [6] generalize this conformalized quantile regression as a fully quantile-rank-based method by using a probability integral transform with a distributional regression estimator (a model of the conditional CDF). These approaches require an auxiliary function (residuals, quantiles, or CDFs), which is obtained using the training dataset. In this work, we concern ourselves with the modern scenario where the users are provided a basic black-box predictive model"}, {"title": "Conformal Tree", "content": "The core idea behind our self-grouping conformal prediction method is to learn the group information by applying a tree model to the conformity scores of the calibration dataset (Section 2.1). Since this may break the exchangeability assumption between the test and calibration data points, we develop a tree algorithm that is robust to adding a new test observation (Section 2.2)."}, {"title": "Calibration by Self-grouping", "content": "Suppose that we are given a trained black-box model \u00fb(\u00b7), a conformity score function S:X \u00d7 Y \u2192 R and a calibration dataset {(Xi, Yi)}^n_{i=1}. For a new test point Xn+1, we want to predict the response Yn+1 under the assumption that (X1,Y1), ...., (Xn+1, Yn+1) are i.i.d.\nThe split-conformal prediction interval (or a prediction set) for 1 \u2013 a coverage is defined as\n\u0108_n(X_{n+1}) = {y : S(X_{n+1}, y) \u2264 S^* (D_{1:n})},\nwhere S*(D) is the ([(n + 1) \u00b7 (1 \u2212 a)]/n)-quantile of the conformity scores in D and where D_{1:n} = {(Xi, Si)}_{i=1}^n. For Cn (Xn+1), the coverage is guaranteed [16, 37] to satisfy\nP{Y_{n+1} \\in \u0108_n(X_{n+1})} \\geq 1 \u2212 a.\nNote that the prediction interval construction relies on the model's performance in the overall input space. For example, with the absolute residual score S(x, y) = |y - \u00fb(x)|, the set Cn(Xn+1) is equivalent to [\u00fb(Xn+1) \u2013 \u011d, \u03bc(Xn+1) + \u011d], where a constant \u011d = S*(D_{1:n}) is added and subtracted over all the data domain X."}, {"title": "Robust Regression Trees", "content": "Regression trees [3] have been widely used in the nonparametric regression literature due to their interpretability. Trees employ a hierarchical set of rules in order to partition the data into subsets on which the response variable is relatively constant. Typical approaches to fitting trees involve greedily descending from the root note and making splitting decisions in order to maximize a utility criterion, subject to regularization constraints.\nFor many popular criteria used to assess tree fitting, such as variance reduction, it is hard to precisely understand the effect of including a single new data point on the resulting tree structure. We introduce a novel criterion that aims to produce a tree that is robust to the inclusion of an additional i.i.d. data point with high probability.\nWe focus on dyadic trees where splits along covariates may only be placed at specific predetermined locations (dyadic rationals for continuous predictors rescaled to a unit inter- val). In general, our implementation allows for discrete predictors where dummy variable are created and the split occurs at 1/2. Ordinal discrete covariates are rescaled and treated as continuous."}, {"title": "Simulation and Benchmark Studies", "content": "In this section, we consider regression tasks on simulated and real benchmark data. For regression tasks, we use the absolute residuals as the conformity score, and compare the average length of the prediction interval as our primary evaluation metric."}, {"title": "Conformal Tree is Adaptive", "content": "We illustrate Conformal Tree on simulated data examples, modified from previous studies [26, 29]. We let X ~ U(0, 1), and consider the following two data-generating processes with n = 500:\n(Data 1) Y|X ~ N(3sin(4/X + 0.2) + 1.5, X\u00b2) and\n(Data 2) Y|X ~ N(sin(X-3),0.12).\nThese two setups represent heteroskedasticity for distinct reasons. Data 1 has the noise standard deviation scaling with X, causing the distribution of residuals to vary across the domain. Therefore, any single regression (mean) model cannot avoid increasing errors"}, {"title": "Benchmark Datasets", "content": "We consider various real data examples that had previously been used in the literature for comparing the efficacy of regression prediction intervals [27, 29, 31]. The number of observations in these datasets vary from 200 to about 50,000, and the number of predictor variables ranges from 1 to 100. We randomly split each dataset into training, calibration, and test sets, with proportions 0.4, 0.4, and 0.2 respectively. For each dataset, we repeat the entire procedure 10 times, including splitting the data and forming the intervals. We report the average metrics taken over these trials. For the \u201cblack-box\u201d predictive model \u00fb, we consider a random forest model. Additional information about the datasets, predictive models, and experiments can be found in Supplement C, as well as additional results.\nTable 1 compares the resulting average interval widths arising from conformal prediction methods, where a random forest model is used as our predictive model \u00fb. We compare with standard split-conformal prediction [16], locally-weighted conformal inference (LWCI) [16], and conformalized quantile regression (CQR) [27]. Locally-weighted conformal inference is similar to our method in that it relies on a locally varying model for the conformity score \u00f4 : X \u2192 R, which is fit on the training data. We use a random forest model for \u00f4 in our experiments. We also display a variant of our method that uses standard CART for"}, {"title": "Large Language Model Uncertainty Quantification", "content": "In this section, we apply Conformal Tree to quantify the uncertainty in the output from a black-box large language model (LLM) for two downstream classification tasks. Conformal Tree uses a set of calibration data in order to partition the input space into regions where the conformity score is relatively homogenous, and yields conditional conformal guarantees in each one. These regions do not need to be prespecified by the user, which can be especially useful in cases where it is difficult for an uninformed user to designate a reasonable partition a priori. We consider predicting the U.S. state of a legislator based on a measure of their ideology in voting for bills, as well as a prediction of skin disease from a list of clinical symptoms. The latter task is meant to emulate a user asking a language model for medical advice, which we do not advocate for but rather view as a domain in which locally adaptive uncertainty quantification would be paramount. We view the LLM models as a deterministic model (with the temperature set to 0), so they provide an actual fit as opposed to prediction involving noise. For classification, we use the complement of the predicted class probability as the conformity score, as in [1, 28, 30]. Our adaptive approach enables generative LLM prediction sets to stretch and shrink based on the informativeness of the prompt."}, {"title": "Classifying States of Legislators using an LLM", "content": "Large language models have become extremely popular generative AI tools, allowing users to generate completions of text or to generate responses to questions or instructions [23]. In the current era of AI as-a-service, many performant LLMs are hosted on servers, to which the user submits a query and receives either next token probabilities or the sampled subsequent tokens themselves. In this case, the user does not have access to the models' training data, and cannot hope to apply most typical methods for local adaptivity. Trained on vast quantities of training data, state-of-the-art LLMs has been demonstrated to have an impressive \"understanding\" of the U.S. political atmosphere, proving capable of assign- ing scores or pairwise comparisons between perceived ideology of legislators that correlates highly with measures estimated from their voting history [22, 38]. We posit a related ques- tion in this work, questioning how well an LLM can predict the state in which a legislator serves from only their DW-NOMINATE score [24], a two-dimensional representation of their ideology based on voting data.\nThus, we consider a black-box model f : R\u00b2 \u2192 S50, where S50 denotes a 50-dimensional simplex, or a probability measure over U.S. states. In our case, f is the resulting prob- abilities assigned by an LLM when conditioning on a context that gives a specific DW- NOMINATE score, and asks where a legislator with that score would be likely to be employed. While the relationship between DW-NOMINATE and state could be alterna- tively studied using empirical data, the predictions from an LLM represent something else entirely- rather than the relationship that manifests itself in historical data, they represent the \"communal opinion\u201d, or \u201cZeitgeist\u201d, about the relationship between DW-NOMIN\u0391\u03a4\u0395, as distilled through the lengthy training process of the language model. However, social scientists may be weary of how much they can trust the output of the LLM for such a specific downstream task, given its black-box nature, which motivates a practitioner to look for conformal sets of U.S. states that are guaranteed to contain the true state of the legislator with a desired probability level.\nFor this classification task, we consider the conformity score S(x,y) = 1 - f(x)_y, where x \u2208 R\u00b2 denotes a DW-NOMINATE score and y \u2208 [50] denotes a U.S. state in- dex. A conformal prediction set is defined by \u0108(x) = {k : f(x)_k \u2265 1 - q}, where q"}, {"title": "Conformalizing ChatGPT Diagnoses of Skin Diseases", "content": "Services hosting black-box generative language models like ChatGPT have now become widespread, offering powerful tools for applications ranging from customer support to cre- ative content generation. As a result, these models are being increasingly integrated into everyday applications, making sophisticated AI tooling accessible to the general public. One significant yet controversial use lies in healthcare, where people have the option to use ChatGPT to obtain preliminary diagnoses of their medical conditions based on physical descriptions and perceived symptoms [11]. Previous studies have found ChatGPT to be effective at creating shortlists of possible diagnoses based on clinical vignettes from case reports [32] and to be an effective self-diagnostic tool for common orthopedic diseases [15]. Uncertainty quantification of these black-box generative models is important for its poten- tial future use in medical diagnostics, but also addresses the current reality that people may use these tools for self-diagnosis, despite potential risks.\nWhile early investigations into the diagnostic capabilities of LLMs like ChatGPT are underway, it is crucial for users to have a means to quantify the uncertainty associated with these diagnoses. Given that these black-box models can be difficult to understand and trust, Conformal Tree provides conformal prediction sets for ChatGPT as diagnoses, with locally varying thresholds to represent different levels of confidence for different groups of patient characteristics. This allows users to have a statistical guarantee on the coverage"}, {"title": "Concluding Remarks", "content": "Conformal prediction serves to provide some statistical clarity in the otherwise uncharted waters of modern black-box deep learning. Conformal tree allows for local adaptation and conditional coverage guarantees based on a self-grouping procedure that is robust to the inclusion of an additional i.i.d test point. This can be a valuable tool for quantifying"}]}