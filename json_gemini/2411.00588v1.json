{"title": "\u03b1-TCVAE: ON THE RELATIONSHIP BETWEEN DISENTANGLEMENT AND DIVERSITY", "authors": ["Cristian Meo", "Louis Mahon", "Anirudh Goyal", "Justin Dauwels"], "abstract": "Understanding and developing optimal representations has long been foundational in machine learning (ML). While disentangled representations have shown promise in generative modeling and representation learning, their downstream usefulness remains debated. Recent studies re-defined disentanglement through a formal connection to symmetries, emphasizing the ability to reduce latent domains (i.e., ML problem spaces) and consequently enhance data efficiency and generative capabilities. However, from an information theory viewpoint, assigning a complex attribute (i.e., features) to a specific latent variable may be infeasible, limiting the applicability of disentangled representations to simple datasets. In this work, we introduce \u03b1-TCVAE, a variational autoencoder optimized using a novel total correlation (TC) lower bound that maximizes disentanglement and latent variables informativeness. The proposed TC bound is grounded in information theory constructs, generalizes the \u03b2-VAE lower bound, and can be reduced to a convex combination of the known variational information bottleneck (VIB) and conditional entropy bottleneck (CEB) terms. Moreover, we present quantitative analyses and correlation studies that support the idea that smaller latent domains (i.e., disentangled representations) lead to better generative capabilities and diversity. Additionally, we perform downstream task experiments from both representation and RL domains to assess our questions from a broader ML perspective. Our results demonstrate that \u03b1-TCVAE consistently learns more disentangled representations than baselines and generates more diverse observations without sacrificing visual fidelity. Notably, \u03b1-TCVAE exhibits marked improvements on MPI3D-Real, the most realistic disentangled dataset in our study, confirming its ability to represent complex datasets when maximizing the informativeness of individual variables. Finally, testing the proposed model off-the-shelf on a state-of-the-art model-based RL agent, Director, significantly shows \u03b1-TCVAE downstream usefulness on the loconav Ant Maze task.", "sections": [{"title": "INTRODUCTION", "content": "The efficacy of machine learning (ML) algorithms is intrinsically tied to the quality of data representation (Bengio et al., 2013). Such representations are useful not only for standard downstream tasks such as supervised learning (Alemi et al., 2017) and reinforcement learning (RL) (Li, 2017), but also for tasks such as transfer learning (Zhuang et al., 2020) and zero-shot learning (Sun et al., 2021). Unsupervised representation learning aims to identify semantically meaningful representations of data without supervision, by capturing the generative factors of variations that describe the structure of the data (Radford et al., 2016; Locatello et al., 2019b). According to Bengio et al. (2013), disentanglement learning holds the key to understanding the world from observations, generalizing knowledge across different tasks and domains while learning and generating compositional representations (Higgins et al., 2016; Kim & Mnih, 2018).\nProblem Formulation. The goal of disentanglement learning is to identify a set of independent generative factors z that give rise to the observations \u00e6 via p(x|z). However, from an information"}, {"title": "RELATED WORK", "content": "Generative Modelling and Disentanglement Recently Locatello et al. (2019b) demonstrated that unsupervised disentangled representation learning is theoretically impossible, nonetheless disentangled VAEs, acting as both representational and generative models, Kingma & Welling (2013); Higgins et al. (2016); Chen et al. (2018); Kim & Mnih (2018) achieve practical results by leveraging implicit biases within the data and learning dynamics Burgess & Kim (2018); Higgins et al. (2019); Mathieu et al. (2019). On the generation side, they have been widely used to generate data such as images (Chen et al., 2019), text (Shi et al., 2019), speech (Sun et al., 2020; Li et al., 2023) and music (Wang et al., 2020). Various extensions to the base VAE model have been presented to improve generation quality in terms of visual fidelity (Peng et al., 2021; Vahdat & Kautz, 2020; Razavi et al.,"}, {"title": "\u03b1-TCVAE FRAMEWORK DERIVATION", "content": "Motivation. In contrast to most existing methods, which only impose an information bottleneck to learn disentangled representations, we seek to maximize the informativeness of individual latent variables as well. The total joint correlation (TC) can be explicitly expressed in terms of mutual information between the observed data and the latent generative factors, as shown in equation 4, allowing us to link disentanglement to latent variables informativeness. As a result, leveraging the TC formulation, we can derive a lower bound that not only promotes disentanglement but also maximizes the information retained by individual latent variables.\nDerivation. In this section, we formally derive the novel TC bound. Let D = {X, V} be the ground-truth set that consists of images x \u2208 $R^{N\u00d7N}$, and a set of conditionally independent ground-truth data generative factors v \u2208 $R^{M}$, where log p(v|x) = \u2211k log p(vk|x). The goal is to develop an unsupervised deep generative model that can learn the joint distribution of the data \u00e6, while uncovering a set of generative latent factors z \u2208 $R^{K}$, K > M, such that z can fully describe the data structure of \u00e6 and generate data samples that follow the underlying ground-truth generative factors v. Since directly optimizing the joint TC is intractable, we are going to maximize a lower bound of the joint total correlation TC(z, x) between the learned latent representations z and the input data \u00e6, following the approach proposed by Hwang et al. (2021). The total correlation is defined as the KL divergence between the joint distribution and the factored marginals. In our case:\n$TC_{\\theta}(z, x) = D_{KL} \\left[q_{\\theta}(z, x)||q_{\\theta}(z)\\prod_{k=1}^{K} q_{\\theta}(z_{k})\\right],$ (1)\nwhere the joint distribution is $q_{\\theta}(z) = \\int q_{\\theta}(z|x)p_{D}(x)dx$, pD(x) is the data distribution, $q_{\\theta}(Z_{k}) = \\int q_{\\theta}(z|x)dz_{\\neq k}$ and $z_{\\neq k}$ indicates that the k-th component of z is not considered. Since we aim to find the encoder $q_{\\theta}(zx)$ that disentangles the learned representations z, we can formulate the following objective:\n$TC_{\\theta}(z, x) = TC_{\\theta}(z) - TC_{\\theta}(z|x),$ (2)\nwhere the conditional $TC(z|x)$ can be expressed as:\n$TC_{\\theta}(z|x) = E_{q_{\\theta}(x)} \\left[D_{KL} \\left(q_{\\theta}(z|x)|| \\prod_{k=1}^{K} q_{\\theta}(z_{k}|x)\\right)\\right] = \\sum_{k=1}^{K} I_{q_{\\theta}}(z_{k}|X)$ (3)"}, {"title": "EXPERIMENTS", "content": "In this section, we design empirical experiments to understand the performance of \u03b1-TCVAE and its potential limitations by exploring the following questions: (1) Does maximising the informativeness of latent variables consistently lead to an increase in representational power and generative diversity? (2) Do disentangled representations inherently present higher diversity than entangled ones? (3) How are they correlated with other downstream metrics (i.e., FID (Heusel et al., 2017) and unfairness (Locatello et al., 2019a))? (4) To what extent does maximising the latent variables' informativeness in disentangled representations improve their downstream usefulness?\nExperimental Setup. In order to assess the performance of both proposed and baseline models, we validate the considered models on the following datasets. Teapots (Moreno et al., 2016) contains 200, 000 images of teapots with features: azimuth and elevation, and object colour. 3DShapes (Burgess & Kim, 2018) contains 480,000 images, with features: object shape and colour, floor colour, wall colour, and horizontal orientation. MPI3D-Real (Gondal et al., 2019) contains 103, 680 images of objects at the end of a robot arm, with features: object colour, size, shape, camera height, azimuth, and robot arm altitude. Cars3D (Reed et al., 2015) contains 16, 185 images with features: car-type, elevation, and azimuth. CelebA (Liu et al., 2015) contains over 200, 000 images of faces"}, {"title": "DISCUSSION AND FUTURE WORK", "content": "Through comprehensive quantitative analyses, we answer the defined research questions while delineating the advantages and limitations of the proposed model relative to the evaluated baselines. Our findings resonate with the hypothesis posited by Higgins et al. (2019), emphasizing a strong correlation between disentanglement and generative diversity. Notably, disentangled representations consistently showcase enhanced visual fidelity and diversity compared to the entangled ones. This correlation persists across all datasets rendered using disentangled representations. Intriguingly, traversal analyses of \u03b1-TCVAE, illustrated in Figures 1 and 16 in Appendix C, reveal that it is able to discover novel generative factors, such as object positioning and vertical perspectives, which are absent from the training dataset. We hypothesize that the CEB term is responsible for this phenomenon. Most existing models optimize only the information bottleneck, and while this can result in factorized representations, it does not directly optimize latent variable informativeness. Our proposed bound also includes a CEB term, and so maximizes the average informativeness as well, which may push otherwise noisy variables to learn new generative factors. Future research will delve deeper into comprehending this phenomenon and exploring its potential applications.\nIn accordance with the literature, the main limitation of \u03b1-TCVAE is that, akin to other disentangled VAEs, it is difficult to scale efficiently. This scaling challenge permeates the entire disentanglement paradigm. In high-dimensional spaces, not only do disentangled VAE-based models struggle to produce disentangled representations, but also the metrics used to measure disentanglement tend not to be useful. (e.g., DCI and SNC(Eastwood & Williams, 2018; Mahon et al., 2023)). On the other hand, disentangled representations have a number of desirable properties, as already showcased in the literature (Higgins et al., 2022). In particular, their impact is undeniable in the Ant Maze RL experiment from Figure 9. Reinforcing this observation, our correlation study underscores the relationship between disentanglement and diversity, leading to the following question: can we leverage diversity as a surrogate for measuring disentanglement in complex and high-dimensional scenarios? We leave the answer to this question as a future work."}, {"title": "CONCLUSION", "content": "We introduce \u03b1-TCVAE, a VAE optimized through a convex lower bound on the joint total correlation (TC) between the latent representation and the input data. This proposed bound naturally reduces to a convex combination of the known variational information bottleneck (VIB) (Alemi et al., 2017) and the conditional entropy bottleneck (CEB) (Fischer & Alemi, 2020). Moreover, it generalizes the widely adopted \u03b2-VAE bound. By maximizing disentanglement and average informativeness of the latent variables, our approach enhances both representational and generative capabilities. A comprehensive quantitative evaluation indicates that \u03b1-TCVAE consistently produces superior representations. This is evident from its performance across key downstream metrics: disentanglement (i.e., DCI and SNC), generative diversity (i.e., Vendi score), visual fidelity (i.e., FID), and its demonstrated downstream usefulness. In particular, our \u03b1-TCVAE showcases significant improvements on the MPI3D-Real dataset, the most realistic factorized dataset in our evaluation, and in a downstream reinforcement learning task. This highlights the strength of maximizing the average informativeness of latent variables, offering a pathway to address the inherent challenges of disentangled VAE-based models."}, {"title": "TOTAL CORRELATION LOWER BOUND DERIVATION", "content": "In this section we are going to derive the TC lower bound defined in equation 6. Since it is defined as a convex combination of marginal log-likelihood, VIB, and CEB terms, we are going to split the derivation into two subsections. First, we will derive a first TC bound that introduces the VIB term. Then, we will derive another TC bound, which explicitly shows the CEB term. Finally, we will define the TC bound shown in equation 6 as a convex combination of the two bounds."}, {"title": "TC BOUND AND THE VARIATIONAL INFORMATION BOTTLENECK", "content": "Unfortunately, direct optimization of mutual information terms is intractable Alemi et al. (2017). Therefore, we first need to find a lower bound of equation 4. Following the approach used in Hwang et al. (2021), we can expand it as:\n$TC_{\\theta}(z,x) = \\sum_{k=1}^{K} I_{\\theta}(z_{k}, x) - I_{\\theta} (z,x),$ (7)\n$=\\sum_{k=1}^{K} \\left[ E_{q_{\\theta}(x,z_{k})} \\left[log \\frac{q_{\\theta}(x|z_{k})}{p_{D}(X)}\\right] - E_{q_{\\theta} (x,z)} \\left[log \\frac{q_{\\theta}(x|z)}{p_{D}(X)}\\right]\\right]$ \n$=\\sum_{k=1}^{K} \\left[ E_{q_{\\theta} (x,z_{k})} \\left[log \\frac{q_{\\theta}(x|z_{k})p_{\\theta}(x|z_{k})}{p_{D}(X)p_{\\theta}(x|z_{k})}\\right] - E_{q_{\\theta} (z,x)} \\left[log \\frac{q_{\\theta}(z)r(z)}{q_{\\theta} (z,x)} \\frac{q_{\\theta}(z,x)}{q_{\\theta}(z)r(z)}\\right]\\right] .$"}, {"title": "TC BOUND AND THE CONDITIONAL VARIATIONAL INFORMATION BOTTLENECK", "content": "Expanding Eq. equation 2, we can reformulate $TC(z, x)$ as follow:\n$TC(z, x) = \\sum_{k=1}^{K} I_{\\theta}(z_{k}, x) - I_{\\theta}(z, x),$ (13)\n$= \\sum_{k=1}^{K} \\left[I_{\\theta} (z_{k}, x)+\\frac{K-1}{K}I_{\\theta}(z_{k}, x)-\\frac{K}{K}I_{\\theta}(z, x)\\right] ,$ (14)\n$=\\sum_{k=1}^{K} \\left[\\frac{K-1}{K}I_{\\theta}(z_{k}, x)-\\frac{1}{K}\\left(I_{\\theta} (z, x)-I_{\\theta} (z_{k}, x)\\right)\\right]$"}, {"title": "FINAL TC BOUND", "content": "In order to obtain the final expression of the derived TC bound, we can compute a convex combination of the two bounds defined in Eq. equation 12 and equation 19.\n$TC(z,x) = (1-\\alpha) \\left[\\sum_{k=1}^{K} I_{\\theta} (z, x) - I_{\\theta} (z,x)\\right]$ (20)\n$+\\alpha \\left[\\frac{K-1}{K}\\sum_{k=1}^{K} I_{\\theta} (z, x) + \\frac{1}{K} \\sum_{k=1}^{K} (I_{\\theta}(z_{k}, x) - I_{\\theta} (z,x))\\right]$ (21)\n$= \\frac{K(1-\\alpha)+\\alpha(K-1)}{K}\\sum_{k=1}^{K} I_{\\theta}(z_{k}, x) - \\frac{\\alpha}{K}\\sum_{k=1}^{K} (I_{\\theta} (z,x) - I_{\\theta} (z_{k}, x)) - (1 - \\alpha) I_{\\theta} (z,x),$"}, {"title": "ARCHITECTURES AND HYPERPARAMETERS DETAILS", "content": "The hyperparameters used for the different experiments are shown in Table 2.\nAll encoder, decoder and discriminator architectures are taken from Roth et al. (2023)."}, {"title": "FURTHER DETAILS ON DATASETS AND METRICS", "content": "We test on five datasets. Teapots (Moreno et al., 2016) contains 200,000 images of size 64 x 64. Each image features a rendered, camera-centered teapot with 5 uniformly distributed generative factors of variation: azimuth and elevation (sampled between 0 and 2\u03c0), along with three RGB colour channels (each sampled between 0 and 1). 3DShapes (Burgess & Kim, 2018) consists of 480, 000 images of size 64 \u00d7 64. Every image displays a rendered, camera-centered object with 6 uniformly distributed generative factors of variation: shape (sampled from [cylinder, tube, sphere, cube]), object colour, object hue, floor colour, wall colour, and horizontal orientation, all determined using linearly spaced values. MPI3D-Real (Gondal et al., 2019) comprises 103, 680 images of size 64 x 64. Each image captures objects at a robot arm's end, characterized by 6 factors: object colour, size, shape, camera height, azimuth, and robot arm altitude. Cars3D (Reed et al., 2015) is made up of 16, 185 images of size 64 \u00d7 64. Each image portrays a rendered, camera-centered car, categorized by 3 factors: car-type, elevation, and azimuth. CelebA (Liu et al., 2015) encompasses over 200,000 images of size 64 \u00d7 64. Every image presents a celebrity, highlighted by a broad range of poses, facial expressions, and lighting conditions, which sum up to 40 different factors. Every model is trained using a subset containing the 80% of the selected dataset images in a fully unsupervised way. The models are evaluated on the remaining images using the following downstream scores. While"}, {"title": "EXTENDED RESULTS", "content": "Here, we present further results, in addition to those from Section 4. Figure 10 extended Fig. 7, reporting the correlations also with SNC, NK and the attribute classification accuracy as shown in Figure 8. Unsurprisingly, there is a strong correlations between the three metrics designed to measure disentanglement: DCI, SNC and NK. This, to some extent, verifies the reliability of these different disentanglement metrics. SNC and NK also correlate strongly with Vendi, as DCI does. This further supports the finding in our paper of a relationship between disentanglement and diversity."}, {"title": "DISCOVERING NOVEL FACTOR OF VARIATIONS", "content": "Figure 16 presents \u03b1-TCVAE traversals across 3DShapes, Teapots and MPI3D-Real datasets. The red boxes indicate the discovered novel generative factors, that are not present within the train dataset, namely object position and vertical camera perspective. While we do not have a comprehensive explanation of why such an intriguing phenomenon is shown, we believe that the intuition behind can be explained considering the effects of VIB and CEB terms in the defined bound. Indeed, while VIB pushes individual latent variables to represent different generative factors, CEB pushes them to be informative. As a result, the otherwise noisy dimensions, are pushed to be informative (i.e., CEB) and to represent a distinct generative factor (i.e., VIB), resulting in the discovery of novel generative factors."}, {"title": "RELATIONSHIP BETWEEN CEB AND DIVERSITY", "content": "Fisher's approach to the Conditional Entropy Bottleneck Fischer & Alemi (2020) is an extension of the Information Bottleneck (IB) principle Alemi et al. (2017), aimed at finding an optimally compressed representation of a variable X that remains highly informative about another variable Y, under the influence of a conditioning variable Z. The CEB objective, according to Fisher, is formalized as a trade-off between two competing conditional mutual information terms:\n$\\min_{p(z|x)} [I(X; Z|C) \u2013 \\beta I(Y; Z|C)]$ (X.Z/C/ BLY, ZC)\nHere, I(X; Z|C) quantifies the amount of information that the representation Z shares with X, conditioned on C. Simultaneously, I(Y; Z|C') measures how much information Z retains about Y, also under the condition of C. The parameter B serves as a crucial tuning parameter, balancing these two aspects."}, {"title": "DIVERSITY AND VISUAL FIDELITY SENSITIVITY ANALYSIS WITH RESPECT TO", "content": "To analyse how a influences the presented results, we performed an evaluation of FID, Vendi and DCI using a \u2208 [0.00, 0.25, 0.50, 0.75, 1.00], where for a = 0.00 we obtain B-VAE model, while for a = 0.25 we get the results presented in the main paper. Figures 17 and 18 show that, when a \u2208 [0.25, 0.50] \u03b1-TCVAE presents the highest diversity scores, while keeping a FID score comperable to \u03b2-VAE.\nInterestingly, the two sensitivity analyses show two main trends:\nDiversity increases when using higher values of Alpha.\nFID score improves when using smaller values of Alpha.\nIndeed, when using higher values of a, we increase the contribution of the CEB term in equation 6, which enhances diversity at the cost of visual fidelity. As a result, the higher the value of a, the more diverse the generated batch of images, and the lower will be the generation quality. However, it can be noticed that when using values of a between 0.25 and 0.50, we get a set of generated images that are more diverse and still have a better or comparable visual fidelity than B-VAE (i.e., a=0)."}, {"title": "DISENTANGLEMENT SENSITIVITY ANALYSIS WITH RESPECT TO", "content": "Here, we present a sensitivity analysis of the DCI metric. Figure 19 shows that the interval [0.25-0.50] presents higher values of disentanglement, following Diversity and Visual Fidelity analyses that show the best results in the same range. Such a trend can be explained by considering that a weights the contributions of VIB and CEB terms. While the CEB term enhances diversity, the VIB term encourages disentanglement. As a result, we can see that DCI scores decrease when a gets closer to 1. Interestingly, when a is in [0.25,0.50], the combination of CEB and VIB terms produces a better bound for the Total Correlation objective than when using, which results in higher DCI scores."}]}