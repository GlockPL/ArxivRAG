{"title": "LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to Small-Scale Local LLMS", "authors": ["Chansung Park", "Fan Wang", "Juyong Jiang", "Sayak Paul", "Jing Tang", "Sunghun Kim"], "abstract": "The widespread adoption of cloud-based proprietary large language models (LLMs) has introduced significant challenges, including operational dependencies, privacy concerns, and the necessity of continuous internet connectivity. In this work, we introduce an LLMOps pipeline, \u201cLlamaDuo\u201d, for the seamless migration of knowledge and abilities from service-oriented LLMs to smaller, locally manageable models. This pipeline is crucial for ensuring service continuity in the presence of operational failures, strict privacy policies, or offline requirements. Our LlamaDuo involves fine-tuning a small language model against the service LLM using a synthetic dataset generated by the latter. If the performance of the fine-tuned model falls short of expectations, it is enhanced by further fine-tuning with additional similar data created by the service LLM. This iterative process guarantees that the smaller model can eventually match or even surpass the service LLM's capabilities in specific downstream tasks, offering a practical and scalable solution for managing AI deployments in constrained environments. Extensive experiments with leading-edge LLMs are conducted to demonstrate the effectiveness, adaptability, and affordability of LlamaDuo across various downstream tasks. Our pipeline implementation is available at https://github.com/deep-diver/llamaduo.", "sections": [{"title": "1 Introduction", "content": "The emergence of LLMs has significantly transformed a myriad of tasks and domains [7, 11, 3, 35, 47, 17, 18]. In particular, cloud-based proprietary LLMs, referred to as service models, such as GPT-4 [3], Gemini 1.5 [11], and Claude 3 [4], have exhibited exceptional capabilities when compared to their smaller, open-source counterparts [5]. A notable survey involving 70 AI industry leaders from diverse enterprises reveals that approximately 80% of the enterprise market share is dominated by closed-source platforms, with a significant portion of this share attributed to OpenAI [39].\nHowever, the increasing reliance on cloud-based service models presents significant challenges in terms of operational dependencies [3], privacy concerns [42], and accessibility challenges [30]. These challenges manifest in various ways, including potential service disruptions, heightened risks to data privacy due to the transmission of sensitive information to external providers, mandatory internet connectivity for utilization, and inconsistencies stemming from updates to service providers' LLMs [14, 47]. Additionally, the transition from proof-of-concept (PoC) development utilizing service LLMs to deployment with local models frequently leads to diminished prompt effectiveness owing to differences between models, subsequently resulting in a suboptimal experience for end-users [26, 23]. To address these concerns and ensure consistent service delivery, it is imperative to develop smaller, locally manageable LLMs that can operate independently of cloud-based infrastructures.\nRecent studies have demonstrated that the strategic fine-tuning of smaller and open-source LLMs with high-quality synthetic data [40, 43] generated by service LLMs can achieve performances that are on par with, or even surpass, those of proprietary LLMs in specific downstream tasks [6, 33, 22, 2, 49]. Motivated by these findings, we introduce an LLMOps pipeline namely LlamaDuo designed to automatically facilitate the seamless migration from service-oriented LLMs to smaller, locally manageable models without the need for human intervention. Our pipeline begins with utilizing a task-specific initial dataset, referred to as the coverage dataset, to fine-tune a smaller open-source LLM. The performance of fine-tuned local LLMs is evaluated using a service LLMs-as-a-Judge strategy [48]. If the performance of the fine-tuned model falls short of expectations, we improve it by iteratively fine-tuning on additional synthetic data generated by the service LLM. LlamaDuo ensures that the smaller model is capable of eventually matching or even surpassing the service LLM's performance in specific downstream tasks, offering superior long-term economic advantages. Therefore, it presents a practical and scalable solution for managing AI deployments in environments where resources are limited.\nWe conduct extensive experiments and analysis across a range of tasks, including summarization, classification, coding, and closed QA, with most popular service LLMs such as GPT40, Claude 3 Sonnet, and Gemini 1.5 Flash, as well as local LLMs, including Gemma 2B and 7B, Mistral 7B, and LLaMA3 8B, to demonstrate that our LlamaDuo guarantees the smaller local LLMs possesses the potential to eventually match or even exceed the performance of service LLMs in specific downstream tasks. We open-source our codebase, enabling users to seamlessly migrate knowledge and abilities from service LLMs to small-scale local LLMs in constrained environments. We release all synthetic datasets and model checkpoints on Hugging Face 3, empowering the community to enrich the capabilities of small LLMs and facilitating future research. To summarize, our key contributions are:\n\u2022 We introduce the LlamaDuo, an efficient and affordable LLMOps pipeline designed to facilitate seamless migration from service-oriented LLMs to smaller, locally manageable models without the need for human intervention, ensuring service continuity in constrained environments.\n\u2022 We iteratively employ task-specific synthetic data produced by service LLMs to guarantee that LlamaDuo enables the smaller model to eventually match or even surpass the performance of service LLM in specific downstream tasks, thus meeting specific task requirements.\n\u2022 We substantiate the pipeline's robust performance and adaptability in real-world context through comprehensive experiments across a range of typical tasks, employing most popular service LLMs as synthetic data generators and judges for well-know small-scale local LLMs.\n\u2022 We emphasize the significant economic advantages of LlamaDuo for investing in smaller, locally manageable LLMs and their deployment for sustained use, as opposed to the transient benefits derived from the token-based API usage of service LLMs."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Alignment with Instruction Tuning", "content": "LLMs pretrained on massive corpora demonstrate remarkable capabilities across a wide range of tasks [47]. Despite their capabilities, a notable challenge with LLMs is their misalignment with user instructions, which limits their practical application in real-world scenarios [43, 40]. The misalignment stems from the initial pretraining objective of LLMs, which focuses on minimizing generation errors rather than adhering to human instructions [27, 8]. To solve the mismatch, instruction tuning is proposed, which enables LLMs to complete diverse tasks from instructions without significant computational resources or alterations to the model's architecture[21, 25, 34]. Specifically, instruction tuning involves supplementary training of pretrained LLMs with datasets structured as instruction-output pairs [46]. The efficacy of instruction tuning is largely contingent upon the quality and diversity of the instruction datasets employed [37]. However, the process of curating high-quality, diversified data is fraught with challenges, including the extensive time required for creation, privacy concerns, high costs, and the need for substantial human labor [43, 20]. In response to these challenges, recent studies have explored innovative methods for constructing instruction datasets, notably the utilization of LLMs for data synthesis [20]."}, {"title": "2.2 LLM-synthetic Instruction Data", "content": "LLMs have demonstrated an unprecedented ability to comprehend and execute natural language instructions [27, 8, 35]. This ability is attributed to the process of training LLMs using substantial instruction datasets [40]. However, acquiring massive instruction datasets is challenging due to data scarcity, privacy issues, low data quality, and prohibitive costs associated with manual data curation [1, 43, 20]. Given these constraints, recent studies probe into utilizing LLMs to automatically generate synthetic instruction data [41, 9, 34]. Specifically, these approaches involve prompting powerful LLMs with limited seed data to generate additional synthetic data. These data are subsequently employed to fine-tune smaller models, aiming to transfer knowledge to small LLMs and enhance their performance [38]. Leveraging LLMs to generate data can significantly reduce the costs and time for data curation [20], while simultaneously improving the efficacy of the fine-tuned models for designated downstream tasks [44, 28, 13, 31, 32]."}, {"title": "3 LLMOps Pipeline: LlamaDuo", "content": "In this section, we elaborate on the details of the proposed LlamaDuo, which are depicted in Figure 1. This LLMOps pipeline aims to ensure service LLMs continuity by transitioning knowledge and abilities from service-oriented LLMs to smaller, locally manageable LLMs without the need for human intervention."}, {"title": "3.1 Coverage Dataset", "content": "Users interact with service LLMs through prompt engineering efforts. The historical trials composed of the user input prompt and the responses of service LLMs, and potential errors will be recorded and saved in local storage. Subsequently, users annotate and collect the most satisfied prompt and response pairs conformed with their real-world use cases. The resulting instruction dataset is termed as coverage dataset, denoted as $D^{(0)} := {\\{ I^{(0)}_i, R^{(0)}_i \\}}_{i=1}^N$, and split as train and test subsets by ratio \u03a6. Here, $I^{(0)}_i$ denotes the i-th instruction (prompt) in $D^{(0)}$, $R^{(0)}_i$ is the corresponding response for the i-th instruction, and N is the number of samples in $D^{(0)}$. Since coverage dataset is of high quality and satisfying the user's intent in real-world context, the train subsets $|D^{(0)}_{train}| = \u03a6 \u00b7 N$ will be served as seeds for synthetic datasets generation, while the test subset $|D^{(0)}_{test}| = (1 \u2013 \u03a6) \u00b7 N$ is reserved for performance evaluation of the fine-tuned local LLMs."}, {"title": "3.2 Fine-tuning", "content": "To efficiently and effectively adapt the local model to specific downstream task(s), we finetune the local LLM with the supervised learning paradigm on high-quality instruction data. At the"}, {"title": "3.3 Batch Inference", "content": "After the fine-tuning stage, the fine-tuned local model is prompted with prompts $Z^{(0)}$ sampled from the test subsets $D^{(0)}_{test}$ of the coverage dataset to produce corresponding response $\\hat{R} \\sim \\pi^{(t)}(R^{(0)} | I^{(0)})$. To improve the diversity and robustness of responses, the local model generates a batch of K responses ${\\{\\hat{R}_1, \\hat{R}_2, ..., \\hat{R}_K\\}}$ for each given prompt $Z^{(0)}$. Totally, it will construct prompt and responses pairs ${\\{(I^{(0)}, \\hat{R}_i)\\}}_{i=1}^{(1-\u03a6)\u00b7N\u00b7K}$. Formally,\n$\\hat{R}_k \\sim \\pi^{(t)}(R^{(0)} | I^{(0)}), for k \u2208 \\{1,2,..., K\\},$ \n$I^{(0)} \\sim D^{(0)}_{test}$"}, {"title": "3.4 Evaluation", "content": "In the evaluation stage, we employ \u201cservice LLMs-as-judge\", denoted as $E_{LLM}(\u00b7)$, to conduct performance evaluation of local model on ${\\{(I^{(0)}, \\hat{R}_i)\\}}_{i=1}^{(1-\u03a6)\u00b7N\u00b7K}$. Following the works [48, 45], the service LLMs evaluate each response triple $(I^{(0)}, \\hat{R}, R^{(0)})$, comprising prompt, the corresponding generated response, and the ground truth, by M times with pairwise comparison and single answer grading strategies. This evaluation process guarantees the trustworthy and reduces the inherent bias of results. Moreover, when employing LLMs as evaluators, the evaluation metrics can be more flexibly adapted to specific tasks, along with a thorough evaluation guide. In this paper, we measure the similarity between $\\hat{R}$ and $R^{(0)}$, and how precise $(I^{(0)}, \\hat{R})$ the responses generated by the local LLM answer the given instructions. These two metrics are provided simultaneously through a prompt, as"}, {"title": "3.5 Data Synthesis", "content": "If the performance of fine-tuned local LLM $V^{(t)}$ or $C^{(t)}$ fails to reach or surpass the predetermined evaluation threshold $\u03b5$ of specific tasks, it indicates that fine-tuned local LLM's capabilities are insufficient for the tasks at hand. Thus, the local LLM cannot yet serve as a substitute for the service LLM and necessitates further refinement. To achieve this, we utilize service LLMs to generate additional synthetic datasets for fine-tuning local LLM in the next cyclicality. To maintain the consistency of data distribution of coverage dataset $D^{(0)}$ constructed from real-world scenarios, we employ the train subsets $D^{(0)}_{train}$ as seeds and apply the same framework [40, 33] for synthetic dataset generation. During synthetic dataset generation, we perform data deduplication to exclude identical samples from ${\\{D^{(0)}_{train}, {\\{D^{(1)}_{synth}, D^{(2)}_{synth}, th..., D^{(t)}_{synth}\\} }\\}}$ and filter out low-quality samples based on carefully designed rules. Finally, we conduct rigorous data decontamination for the synthetic dataset to remove samples that closely resemble those in the test subset $D^{(0)}_{test}$ of the coverage dataset. Formally, the data synthesis stage can be formulated as\n$D^{(t)}_{synth} \u2190 \u222aV (D^{(0)}_{train}, {\\{D^{(i)}_{synth}\\} }_{i=1}^{t-1}, D^{(t-1)}_{synth}), D^{(0)}_{train}$,\n$D^{(t)}_{synthST=15} \u2190 SLLM(prompt(synth), seed)$,\n$seed \\sim D^{(0)}_{train}, for V_i^{(t)} < \u03b5 or C_i^{(t)} < \u03b5$,\nwhere \u222a(\u00b7, \u00b7, \u00b7) represent a series of data post-processing operations, $D^{(t)}_{synth}$ denotes synthetic data generated from service LLMs at t-th cyclicality, $SLLM$ and prompt(synth) are the service LLM and system prompt used for the data synthesis, respectively."}, {"title": "4 Experiments", "content": "In this section, we present a comprehensive evaluation of our LlamaDuo across a series of settings, demonstrating its robust performance, adaptability, and affordability in real-world scenarios. We first evaluate the performance of our pipeline across summarization, classification, coding, and closed QA tasks to underscore its efficacy and versatility. We further examine the impact of synthetic dataset volume and the choice of service model as data generator and judge on the performance of fine-tuned models, thereby showing the robustness of the LlamaDuo. Lastly, we analyze the cost-effectiveness associated with deploying a fine-tuned LLM via our pipeline, emphasizing its long-term economic advantages."}, {"title": "4.1 Experimental Settings", "content": "Tasks and coverage dataset. We select four categories of downstream tasks-summarization, classification, coding, and closed QA-based on their prevalent use and relevance to the operational scope of service LLMs. We utilize the open-source \u201cNo Robots\u201d [29] dataset as the coverage dataset. This coverage dataset consists of 10K high-quality prompt and response pairs across 10 categories, crafted by expert annotators. Specifically, we utilize four subsets of the coverage dataset, each corresponding to our targeted tasks. These subsets serve as seeds for generating synthetic data that can closely align with user expectations for LLM interactions."}, {"title": "Service and local LLMs.", "content": "Considering the API cost effectiveness, rate limit, and model utility, we select most popular service LLMs including GPT4o by OpenAI 4, Claude 3 Sonnet by Anthropic 5, and Gemini 1.5 Flash by Google 6 to serve as synthetic data generator and judges. As for the small-scale local LLMs to be fine-tuned, we opt for the open-source Gemma 2B and 7B [12], Mistral 7B [16], and LLaMA3 8B [24] as the base models. This selection is motivated by our aim to rigorously evaluate the efficacy and adaptability of our proposed pipeline across diverse settings. The varying scales of base models facilitate a nuanced comparison, allowing us to assess the impact of model scale on performance improvements. However, as a model-agnostic LLMOps pipeline, our LlamaDuo can be generalized to various forms of service and local LLMs beyond the aforementioned models."}, {"title": "4.2 Implementation Details", "content": "We implement LlamaDuo using PyTorch and conduct experiments on 8 \u00d7 A100 GPUs, with detailed hyper-parameter configurations presented in Appendix B."}, {"title": "Synthetic dataset by service LLMs.", "content": "To generate high-quality synthetic datasets, we leverage the capabilities of advanced service LLMs, including GPT-40, Claude 3 Sonnet, and Gemini 1.5 Flash. We utilize the seeds selected from the train subset of the coverage dataset to prompt these service LLMs to generate datasets, each comprising 300k samples. The specific prompt for data generation is presented in Figure 6 of Appendix A. Subsequently, we employ Locality-Sensitive Hashing (LSH) with MinHash and Rouge scoring mechanisms for data deduplication. Specifically, the LSH MinHash can efficiently identify and remove duplicate data samples, while the Rouge scoring mechanism ensures that the curated data exhibits high-quality and meaningful variations. After that, we acquire 256k samples for summarization tasks and 128k for other tasks."}, {"title": "Fine-tuning Local LLMs.", "content": "We proceed to fine-tune the small local LLMs with $2^nk, n \u2208 {0,1, . . ., 8}$ volumes of the synthetic dataset. To efficiently customize local LLM for a specific downstream task within constrained environments, we leverage QLoRA [10] for parameter-efficient fine-tuning (PEFT) with superior cost-effectiveness. The detailed configurations, which are tailored according to dataset sizes and tasks, can be found in Appendix \u0412."}, {"title": "Batch inference.", "content": "Each fine-tuned local model is prompted to generate K = 4 distinct responses, with each prompt sampled from the test subsets of the coverage dataset. To ensure fair comparisons, we maintain a consistent batch inference configuration across all fine-tuned models. The detailed configuration is depicted in Appendix B."}, {"title": "Service LLMs as judges.", "content": "Following [48], we employ three service LLMs as judges and use pairwise comparison and single answer grading strategies to evaluate the response quality of the fine-tuned local LLMs. The corresponding prompts used for evaluation are given in Figure 5 of Appendix A. We utilize similarity and precision metrics for the assessment of the fine-tuned LLMs' performance. The similarity metric assesses the degree of correspondence between the generated responses and the ground truth, while the precision metric evaluates the accuracy of the match between the input prompts and their corresponding responses. To ensure reliability and mitigate inherent biases in the results, both metrics are quantified on a 0 to 100 scale, with each sample undergoing evaluation M = 10 times. The score of coverage percentage is set to $\u03b6\u2208{50,70}$. The mean scores and coverage percentage with score of > \u03b6 are taken as the final result of the model performance."}, {"title": "4.3 Experimental Results", "content": "This section delves into the effectiveness and adaptability of the LlamaDuo pipeline, spanning different tasks with varying degrees of complexity, including summarization, classification, coding, and closed QA. We utilize GPT-40, Claude 3 Sonnet, and Gemini 1.5 Flash as judges to evaluate the fine-tuned model performance on test subsets of the coverage dataset. As demonstrated in Table 1, the fine-tuned local LLMs, despite their significantly smaller scale, achieve comparable performance on diverse tasks compared to much larger service LLMs. For instance, in the summarization task, LLaMA3 8B achieved a comparable precision score of 87.02 / 99%, compared to GPT40's score of 93.25/100%, Claude 3 Sonnet's score of 93.39 / 100%, and Gemini 1.5 Flash's score of 91.95 / 100%, with Claude 3 Sonnet serving as judge. These results underscore the efficacy of LlamaDuo in seamlessly transferring knowledge and capabilities from service LLMs to smaller local LLMs without a substantial decrease in performance.\nIn Table 1, we observe distinct performance across four fine-tuned models when applied to different tasks. Specifically, Mistral 7B stands out in summarization tasks, achieving the best performance in 7 out of 12 cases. Moreover, LLaMA3 8B consistently outperforms competitors across all metrics and evaluators in the classification task. Conversely, in coding tasks, Gemma 7B is identified as the leading model, excelling across all metrics and evaluations. Mistral 7B shows superior performance in the closed QA task, leading in 8 out of 12 cases. Within the realm of service LLMs, Claude 3 Sonnet and Gemini 1.5 Flash demonstrates exceptional performance in classification and closed QA tasks, securing the best results in 8 and 10 out of 12 cases, respectively. Lastly, GPT4o emerges as the leading model in summarization and coding tasks, achieving the best performance in 10 and 7 out of 12 cases, respectively.\nNotably, although Gemma 2B exhibits inferior performance compared to larger 7B models overall, the disparity in results is not markedly substantial, with Gemma 2B attaining closely comparable performance in certain tasks. For example, in closed QA tasks, Gemma 2B secures a mean precision score of 80.22, while Gemma 7B achieves 88.83, Mistral 7B reaches 88.25, and LLaMA3 8B obtains 86.03, as evaluated by Claude 3 Sonnet. This observation lends further support to the notion that"}, {"title": "4.4 In-depth LLMOps Pipeline Analysis", "content": "In this section, we conduct an in-depth analysis of LlamaDuo through summarization task. Notably, the experimental findings exhibit a consistent pattern across various tasks, underscoring the generalizability of LlamaDuo."}, {"title": "Impact of synthetic dataset volume.", "content": "We explore how the volume of synthetic dataset influences the performances of fine-tuned local LLMs, aiming to elucidate a scaling law [19, 15] on how the performance of fine-tuned models changes as the number of synthetic dataset samples increases. Overall, the Gemma 7B model exhibit consistent performance improvements and come closer to the performance of service LLMs with increasing volumes of synthetic data, as assessed through precision and similarity metrics by diverse evaluators, as depicted in Figure 2. This suggests that fine-tuning local LLMs with synthetic data, which mimics the characteristics and patterns of real-world data, can bring the same effect as actual data. Thus, it paves a new way to eliminate the challenges of data scarcity, privacy concerns, and high costs associated with crafting data [20]. Notably, we observe that the synthetic data generated by Claude 3 Sonnet results in the highest-performing models, outperforming those fine-tuned with data produced by GPT4o and Gemini 1.5 Flash, in descending order. Moreover, when the synthetic dataset volume ranges from 64k to 256k, the Gemma 7B model reach the performance saturation point and achieve performance that is much closer to, or equal to,"}, {"title": "4.5 Cost Effectiveness of Long-term Deployment", "content": "In this section, we elucidate the cost-effectiveness of our proposed LlamaDuo pipeline, by conducting a long-term operational cost comparison between the fine-tuning of the small LLMs (Gemma 7B) and the token-based API usage of service LLMs (GPT40).\nIn the context of local LLM deployment, the QLoRA fine-tuning process of Gemma 7B, utilizing a dataset containing 256K samples, necessitates approximately one hour to complete a single experiment on 8 \u00d7 A100 GPUs. This process incurs an estimated cost of $50, based on the price provided by Google Cloud Platform 7. Accounting for multiple iterations of hyperparameter optimization, we estimate that the total fine-tuning cost remains below $800, which is deemed to be negligible. Deploying a single instance of the Gemma 7B model with support for a 1024 context length necessitates 24GB of GPU memory, making the L4 GPU an appropriate choice. Depending on the projected"}, {"title": "5 Conclusion", "content": "In this work, we introduce LlamaDuo, the first automatic LLMOps pipeline designed to facilitate the seamless migration from service-oriented LLMs to smaller, locally manageable models without the need for human intervention. We conduct extensive experiments and analysis across a range of tasks with most popular service and local LLMs to substantiate that our LlamaDuo guarantees the smaller local LLMs possesses the potential to match or even exceed the performance of service LLM in specific downstream tasks. Therefore, this work provides a promising research direction to maintain cloud-based LLMs' service continuity in constrained environments."}, {"title": "Acknowledgments", "content": "This work was supported by IITP grant funded by the Korea government(MSIT)[RS-2023-00215959, Development of Access Agnostic wired and wireless integrated optical access technology]."}, {"title": "A Prompt Templates", "content": "In the LlamaDuo pipeline, we employ two prompt templates that serve different purposes: one for the generation of synthetic datasets and another for the evaluation of the outputs produced by the fine-tuned LLMs.\nFigure 5 illustrates the prompt template used to assess the precision and similarity of the response $lm_response generated by fine-tuned small-scale LLMs, based on the prompt $instruction and response $human_response from the test subset of the coverage dataset. It is important to note that the $ symbol indicates a placeholder, designed to be substituted with actual data during the runtime.\nGiven an instruction and two responses-one generated by a human and the other by a language model-I'm seeking to evaluate how closely the language model's response mirrors the human-generated one. Additionally, I want to assess the accuracy and relevance of the language model's response to the original instruction.\nInstruction:\n$instruction\nHuman Response:\n$human_response\nLanguage Model Response:\n$lm_response\nYou are quality assessor who analyzes the similarity between the Human Response and the Language Model Response on a scale of 1 to 100, where 1 indicates no similarity and 100 indicates identical responses.\nAlso you analyze the Language Model Response how it accurately answers the given Instruction on a scale of 1 to 100. Analysis MUST be rigorous and thorough. Provide the assessment in the following JSON format:\n{\n\"similarity_assessment\": {\n\"score\": [Insert similarity score here]\n},\n\"precision_assessment\": {\n\"score\": [Insert precision score here]\n}\n}\nFigure 6 shows the prompt template designed for the generation of synthetic data tailored to the summarization task while Figure 7 shows the prompt template for other tasks. Specifically, we use a prompt $instruction alongside its corresponding response $response, both sourced from the train subset of the coverage dataset, serving as an example pair. This example pair is utilized to instruct service LLMs to generate analogous data samples. In addition, our template is designed to generate multiple synthetic data samples through a singular request, thereby enhancing the efficiency of API utilization. Due to the unique features of different downstream tasks, there is no optimal prompt template that universally applies. The actual content of the prompt template is adjusted to align with the specific requirements of the task for which the synthetic dataset is being generated."}, {"title": "B Implementation Configuration", "content": "This section delineates the statistical information of the coverage dataset and synthetic dataset generated by service LLMs. In addition, we present the details of the training configurations of our experiments. The detailed pipeline implementation of LlamaDuo is available at https://github.com/deep-diver/llamaduo."}, {"title": "B.1 Coverage Datasets", "content": "The foundational coverage dataset employed in our study is the \"No Robots\" dataset [29]. We leverage four subsets of the coverage dataset, namely summarization, classification, coding, and closed QA, for synthetic data generation. Table 3 illustrates the initial composition of the task-specific subsets, with each initially containing approximately 300 original data points. These subsets are subsequently expanded to encompass more data points using the LlamaDuo framework. To perform an in-depth analysis of the behavior of different service LLMs, we create synthetic datasets for the summarization task by utilizing GPT40, Claude 3 Sonnet, and Gemini 1.5 Flash. For all other tasks, we exclusively use GPT-40, owing to budget constraints."}, {"title": "B.2 Training Configurations", "content": "We utilize Hugging Face's \u201cAlignment Handbook\" [36] and the alignment recipes tailored for the Gemma models to streamline the fine-tuning process.\nAs outlined in Table 5, we employ QLoRA [10] to align the Gemma 2B and 7B, Mistral 7B, and LLaMA3 8B models efficiently. The QLoRA method leverages the advantages of low-rank adaptation, reducing the computational resources required for training. Throughout the alignment procedure, we incrementally adjust the rank and alpha values of LoRA, aiming to optimize the adaptation layer's capacity to match the increasing complexity of the datasets."}]}