{"title": "Text-to-SQL Calibration: No Need to Ask\u2014Just Rescale Model Probabilities", "authors": ["Ashwin Ramachandran", "Sunita Sarawagi"], "abstract": "Calibration is crucial as large language models (LLMs) are increasingly deployed to convert natural language queries into SQL for commercial databases. In this work, we investigate calibration techniques for assigning confidence to generated SQL queries. We show that a straightforward baseline-deriving confidence from the model's full-sequence probability-outperforms recent methods that rely on follow-up prompts for self-checking and confidence verbalization. Our comprehensive evaluation, conducted across two widely-used Text-to-SQL benchmarks and multiple LLM architectures, provides valuable insights into the effectiveness of various calibration strategies.", "sections": [{"title": "1 Introduction", "content": "As enterprises increasingly leverage large language models (LLMs) to convert natural language queries into SQL programs for their databases, obtaining well-calibrated confidence estimates becomes critical to spot when the generated SQL may be incorrect (Steyvers et al., 2024; Baan et al., 2023). Such calibrated probabilities can help enterprises select among multiple generated SQLs and determine when to defer to a human expert (Wu et al., 2024).\nFollowing recent study that highlighted RLHF fine-tuned LLMs produce poorly calibrated conditional probabilities (Kadavath et al., 2022; Tian et al., 2023), several work have proposed fixes to the calibration of LLM outputs. Most of these propose to ask the LLM with one or more additional questions seeking True/False response, or multiple choice selection, or verbalized confidence (Kadavath et al., 2022; Tian et al., 2023; Ren et al., 2023; Zhou et al., 2023; Xiong et al., 2024; Kapoor et al., 2024). These methods have been primarily evaluated on tasks such as question answering, where the outputs are typically short strings.\nFor the Text-to-SQL task we perform an extensive comparison of several recent and traditional calibration techniques spanning several closed and open-source LLMs, including LLMs fine-tuned for Text-to-SQL. Our study shows that rescaling the model's sequence probability using a small validation set with well-established techniques like temperature scaling or isotonic regression-yields significantly better calibration than recent prompting-based methods. Among the strategies that rely on model-generated probabilities, Stengel-Eskin and Van Durme (2023) proposed using the minimum token probability to derive sequence-level confidence. In contrast, our study finds that using the full product of token probabilities, which is theoretically better founded too, provides superior calibration, even after identical rescaling."}, {"title": "2 Related Work", "content": "Calibration of classification models is a classical ML topic (Niculescu-Mizil and Caruana, 2005; Guo et al., 2017a), with much work in pre-LLM NLP literature (Kumar and Sarawagi, 2019; Desai and Durrett, 2020). We focus on recent work on calibration of tasks on LLMs.\nCalibration of LLMs for short response generation Kadavath et al. (2022) study LLMs on a variety of tasks and propose to extract confidence by a self-probe using a follow up True/False question to the LLM on whether the generated response was correct. Probability of True in the follow up question is measured as confidence. Tian et al. (2023) further expand the set of prompts asking to verbalize confidence and show that a better strategy for calibration is getting the LLM to generate top-K responses with probabilities. Ren et al. (2023) also show that self-evaluation improves calibration. Zhou et al. (2023) study if language markers like: \"I believe\", \"I am sure..\"etc reflect confidence, and show that these language markers do not faithfully reflect uncertainty. Kuhn et al. (2023) propose to"}, {"title": "3 Calibration Methods", "content": "Let $x_i$ be an input natural language question on a database schema s for which a Text-to-SQL model M predicted an output SQL $\\hat{y}_i$. We explore a number of methods of attaching a score $r(y)$ that indicates if $y_i$ is a correct SQL for x.\nPooled Token-level Probabilities. The generative model M assigns a probability $P(\\hat{y}|x)$ composed out of auto-regressive token probabilities $Pr(\\hat{y}_t|x, \\hat{y}_{<t})$. A natural method is to use these token probabilities for calibration. Let n denote the number of tokens in $\\hat{y}$. These can be converted into a confidence score r for the whole query $\\hat{y}$ by pooling the token probabilities in various ways:\n1. product of probability $\\prod Pr(\\hat{y}_t|x, \\hat{y}_{<t})$ [prod]\n2. geometric mean $\\sqrt[n]{\\prod Pr(\\hat{y}_t|x, \\hat{y}_{<t})}$ [geo]\n3. minimum $\\min_{t \\in [n]} Pr(\\hat{y}_t|x, \\hat{y}_{<t})$ [min]\n4. arithmetic mean $\\sum_{t=1} Pr(\\hat{y}_t|x, \\hat{y}_{<t})$ [avg]\nLLM Self-checks generated SQL. Another emerging trend is asking the LLM to self-reflect on the correctness of the generated SQL. We consider two variants. (1) [Bool] (Kadavath et al., 2022) where the LLM is prompted with the context, the predicted SQL and two options (A: SQL is correct, B: SQL is incorrect)."}, {"title": "4 Experiments", "content": "We compare these calibration methods across different datasets and LLMs.\nDatasets We evaluate on 31 database schemas spanning two popular Text-to-SQL benchmarks Spider (Yu et al., 2019) and BIRD (Li et al., 2023) for natural language utterances x and their gold SQL y. For each of these, we measure calibration of predictions obtained from two different LLMS GPT-4 (Employees, 2024)(\u2018gpt-3.5-turbo-16k') and CodeS (Li et al., 2024). The prompts used for SQL generation is provided in Table 8. Although these models do not guarantee syntactically valid SQL generation, we assume that a DB engine can be easily invoked to check for grammatical validity and eliminate invalid generations. The final statistics of our test data appear in Table 1."}, {"title": "5 Results", "content": "We report the results across different methods and models in Table 2 and also show the reliability plots in Figure 1. Calibration metrics and reliability plots for experiments using other models and monotonic binning have been deferred to the Appendix E. Across these different studies we make two important conclusions:\nModel's own sequence probability (prod) performs better in smaller models and is comparable to self-check method in larger models Recent calibration studies (Tian et al., 2023) have found self-check methods to be better, and that could be because they deal with short answers. Tian et al. (2023) find calibration results differ when using language models fine-tuned with reinforcement learning from human feedback. We use Llama, 7B and 80B, which have undergone several rounds of alignment via supervised fine-tuning (SFT), rejection sampling (RS), and direct preference optimization (DPO) (AI (2024)). We observe that, even with Llama 70B model, the \"prod\" pooled token-level approach provides calibration as good as self-check bool. In Llama 8B model, \"prod\" method performs significantly better.\nFurther, recent research (West et al., 2024) highlights the gap between the generative and reasoning capabilities in large language models. From our experiments, we observe the gap becomes more pronounced as the model's parameter count decreases. The product of token probabilities, which is the likelihood of a sequence, serves as a measure of its generative capability, contrasting with self-check which is an assessment of the model's understanding.\nProd aggregation is the best pooled token-level method Stengel-Eskin and Van Durme (2023) investigate min and avg methods for calibration. We also consider prod aggregation, which represents a more natural choice as it denotes the probabil-"}, {"title": "Schema-level Calibration", "content": "Industries often apply a threshold on calibrated scores to determine when generated SQL can be safely executed without human verification. In our experiments with different thresholds, we observe significant variation in calibration across schema. In our experiments so far, we have used a validation set comprising schemas that were disjoint from the 31 test schemas; we refer to this as the schema-disjoint method. An interesting question is whether calibration scores can be significantly improved by training the calibration model on labeled data from the same schema being tested; we refer to this as the schema-level method. In Table 3, we compare Precision, Recall and F1 scores across varying thresholds for both schema-level and schema-disjoint methods. Our results show that the schema-level method performs better than the schema-disjoint method. We elaborate further in Appendix E."}, {"title": "6 Conclusion", "content": "We study calibration of generated SQL for LLM based Text-to-SQL generation. We find that models show strong calibration when assigning probabilities to queries, that either outperforms or performs as good as verbalization methods. Using product aggregation for calibration-model assigned probability to the query-provides stronger calibration compared to other methods including minimum aggregation, which was proposed as better by earlier works. This study's insights into model calibration for Text-to-SQL generation can be extended to broader applications such as Python or C++ code generation completion tasks."}, {"title": "7 Limitations", "content": "Our results are based on the specific models employed in our experiments. Although, we have attempted to ensure the validity of our findings by utilizing different models for each method, we cannot guarantee these results will generalize to other models. This limitation is due to the lack of detailed technical information such as training methodologies for many of the models used.\nAdditionally, this limitation restricts our ability to fully explain why the calibration of self-check Bool method is weaker compared to the prod pooled token-level method. Furthermore, our study is restricted to identifying the best calibration methods for generated SQLs, particularly those whose complexity is similar to the SQLs found in the Spider and BIRD datasets.\nOne potential risk associated with our research is the imperfection of the calibration process. Due to this, the model cannot be applied directly in real world applications with absolute accuracy. The confidence scores predicted by the model should be taken as preliminary assessments. Hence, human evaluation is necessary after the models flag certain instances, ensuring a more reliable decision."}, {"title": "F Reliability plots", "content": "Expanding on figure 1, we plot figures 3 and 2 for the other models and methods present in 2.\nComparison with Platt scaling Table 17 and figure 3 shows the variation of the evaluation metrics, brier score and expected calibration error, with the two calibration methods, platt scaling and isotonic regression. Note that the AUC and ECE of the raw confidence scores, which are also reported in table 2 are indifferent to calibration.\nComparison with Monotonic binning Table 18 and figure 2 shows the variation of the evaluation metrics, expected calibration error of the raw and calibrated confidence scores, with the two different methods of binning, uniform and monotonic. Note that the AUC and Brier score, which are also reported in table 2 are indifferent to the binning method."}]}