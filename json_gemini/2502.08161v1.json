{"title": "MixDec Sampling: A Soft Link-based Sampling Method of Graph Neural Network for Recommendation", "authors": ["Xiangjin Xie", "Yuxin Chen", "Ruipeng Wang", "Xianli Zhang", "Shilei Cao", "Kai Ouyang", "Zihan Zhang", "Hai-Tao Zheng", "Buyue Qian", "Hansen Zheng", "Bo Hu", "Chengxiang Zhuo", "Zang Li"], "abstract": "Graph neural networks have been widely used in recent recommender systems, where negative sampling plays an important role. Existing negative sampling methods restrict the relationship between nodes as either hard positive pairs or hard negative pairs. This leads to the loss of structural information, and lacks the mechanism to generate positive pairs for nodes with few neighbors. To overcome limitations, we propose a novel soft link-based sampling method, namely MixDec Sampling, which consists of Mixup Sampling module and Decay Sampling module. The Mixup Sampling augments node features by synthesizing new nodes and soft links, which provides sufficient number of samples for nodes with few neighbors. The Decay Sampling strengthens the digestion of graph structure information by generating soft links for node embedding learning. To the best of our knowledge, we are the first to model sampling relationships between nodes by soft links in GNN-based recommender systems. Extensive experiments demonstrate that the proposed MixDec Sampling can significantly and consistently improve the recommendation performance of several representative GNN-based models on various recommendation benchmarks.", "sections": [{"title": "I. INTRODUCTION", "content": "With the rapid increase of information on the Internet, recommender systems are widely used in e-commerce, social media, etc. They provide users with a personalized flow of information based on their interaction history. Since the data of the recommender system naturally have a graph structure [1], a large number of recommendation methods that built based on graph neural network (GNN) have emerged in recent years, such as GCN [2], GraphSAGE [3], GAT [4], Pinsage [5], and LightGCN [6].\nThe typical setup of the GNN-based recommendation method is as follows: 1) firstly, a graph that reveals the user's interaction behavior is constructed, where each node is a user or an item; 2) subsequently, multiple propagation layers are applied to learn the user and item embeddings. Each propagation layer aggregates neighbor features of a current node to derive its embedding; 3) finally, the overall pipeline is supervised by a loss function that aims to pull embeddings of positive user-item pairs to be closer, while pushing embeddings of negative pairs away.\nNegative sampling plays an important role in optimizing the affinity between user and item under the GNN-based methods. Commonly, a uniform distribution is used for negative sam-pling [6], [7]. To improve the effect, some negative sampling methods are recently proposed. For example, PinSage [5] and DNS [8] focused on selecting hard negative samples for improving the ability of dealing with difficult samples. Generative adversarial network (GAN) based methods, such as IRGAN [9], AdvIR [10], KBGAN [11], generated negative samples through GAN.\nDespite the success of existing negative sampling methods, some limitations still exist. Firstly, in these methods, the relationship between two nodes is simply treated as either hard positive pairs or hard negative pairs. However, nodes in a graph are typically connected by multi-hop neighbors, thus each pair of nodes naturally forms a soft proximity relationship. Degenerating the original soft proximity relationship between nodes into a binary classification problem will obviously lead to the loss of structural information. In addition, most of the existing negative sampling methods [5], [6], [8], [9] mainly focused on sampling for constructing negative pairs, while lacking the generation of positive sample pairs. This may result in inadequate training of nodes with few neighbors.\nTo overcome the above limitations, we propose a novel sampling method, namely MixDec Sampling, for GNN-based recommender systems (see Fig. 1 for an overview). MixDec Sampling consists of Mixup Sampling module and Decay Sampling module. Firstly, the Mixup Sampling module aug-ments node features of a graph by synthesizing new nodes and soft links. It linearly mixes the features of positive samples and negative samples of each anchored node based on the Beta"}, {"title": "II. PRELIMINARIES", "content": "In this section, we introduce related concepts including GNN for recommendation, negative sampling, and Mixup [12]."}, {"title": "A. Graph Neural Networks for Recommendation", "content": "Recommendation is the most important technology in many e-commerce platforms, which has evolved from collaborative filtering to graph-based models. Graph-based recommendation represents all users and items by embedding and recom-mending items with maximum similarity score (by a inner product operation) for a given user. Here, we briefly describe the pipeline of GNN-based representation learning, including aggregation and optimization with negative sampling.\nGNNs learn distributed vectors of nodes by leveraging node features and the graph structure. The neighborhood aggregation follows the \u201cmessage passing\u201d mechanism, which iteratively updates a node's embedding h by aggregating the embeddings of its neighbors. Formally, the embedding h of node i in the l-th layer of GNN is defined as:\n$h_i^l = \\sigma (AGG (h_j^{l-1} | j \\in N(i), W_l))$, (1)\nwhere the \u03c3 is activation function, W\u2081 denotes the trainable weights at layer 1, N(i) denotes all nodes adjacent to i, AGG is an aggregation function implemented by specific GNN model (e.g. GraphSAGE, GCN, GAT, etc.), and hi is typically initialized as the input node feature vi."}, {"title": "B. Negative Sampling", "content": "Negative sampling [13] is firstly proposed to serve as a simplified version of Noise Contrastive Estimation [14], which is an efficient way to compute the partition function of an unnormalized distribution to accelerate the training of Word2Vec [15]. The GNN has different non-Euclidean encoder layers with the following negative sampling objective:\n$L = log(\\sigma(e_{v_i}^T e_{v_p})) + \\sum_{j=1}^c E_{v_j~Pn(v_i)} log(1 \u2013 \\sigma(e_{v_i}^T e_{v_j})),$ (2)\nwhere vi is a node in the graph, vp is sampled from the positive distribution of node vi, vj is sampled from the negative distribution of node vi, e represents the embedding of the node, \u03c3 represents the sigmoid function, c represents the number of negative samples for each positive sample pair."}, {"title": "C. Mixup", "content": "Mixup [12] is an simple yet effective data augmentation method that is originally proposed for image classification tasks. Mathematically, let (x, y) denotes a sample of training data, where x is the raw input samples and y represents the one-hot label of x, the Mixup generates synthetic training samples (x, y) as follows:\n$\\tilde{x} = \\lambda x_i + (1 \u2013 \\lambda)x_j,$\n$\\tilde{y} = \\lambda y_i + (1 \u2013 \\lambda)y_j.$\n(3)\nIt generates new samples by using linear interpolations to mix different images and their labels."}, {"title": "III. METHOD", "content": "In this section, we propose MixDec Sampling, a soft link-based sampling method for GNN-based recommender systems. It can be plugged into existing GNN-based recommendation models, such as GraphSAGE, GCN and GAT.\nAn overview workflow of the MixDec Samping is illustrated in Fig. 1. Our MixDec Sampling boosts the GNN-based recommendation models in two aspects: (i) augmenting node features of a user-item graph by synthesizing new nodes and soft links, thus nodes with few neighbors can be trained with more sufficient number of samples; (ii) modeling the sampling relationships between nodes through soft links, where each"}, {"title": "A. Mixup Sampling", "content": "To improve the embedding learning of nodes with few neighbors, we perform data augmentation on the node features in the graph by generating synthetic nodes and soft links. Mixup [12] is a linear interpolation based data augmentation method with soft labels, which has been theoretically and empirically demonstrated the generalization and robustness. Inspired by Mixup, we linearly mixes the features of positive and negative samples of each anchored node, and fuses their links accordingly.\nFor each anchored node in the graph, we set the weight of positively sampled nodes to one, and the weight of negatively sampled nodes to zero. Then, we apply Mixup to positive and negative pairs to generate new pairs. Specifically, each generated pair can be composed of two positive samples, or a positive sample and a negative sample. The synthetic node and its link to the anchored node are obtained by Beta(\u03b1, \u03b2)-based linear interpolation of the sampled nodes and weights of links, which is formalized as:\n$e_s = \\lambda e_i + (1 \u2013 \\lambda)e_j,$\n$w_s = \\lambda w_i + (1 \u2013 \\lambda)w_j,$\n(4)\nwhere e denotes the embedding of the node, w represents the weight of the link between the anchored node and the another node. s denotes the synthetic node, i and j denote the nodes obtained by positive and negative sampling of the anchored node and \u03bb ~ Beta(\u03b1, \u03b2). An illustration of Mixup Sampling as shown in Fig. 2.\nThe basic negative sampling loss Lns is as follows:\n$L_{ns} = \\frac{1}{N} \\sum_{(v,\\eta) \\in O} log(\\sigma(e_{v}^T e_{\\eta^+})) + log (\\sigma (-e_{v}^T e_{\\eta^-})),$ (5)\nwhere N is the number of nodes in the graph, O = {(v,\u03b7)|(v,\u03b7+) \u2208 R+,(v,\u03b7-) \u2208 R-} denotes the training set, R+ indicates the observed (positive) interactions between the anchored node v and sampled node \u03b7, R is the unob-served (negative) interaction set. The loss of Mixup Sampling is then as follows:\n$L_m = \\frac{1}{N} \\sum_{(v,s)\\in O_m} g(\\sigma (e_{v}^T e_{s}), w_s),$\n(6)\nwhere g is a loss function, Om denotes the Mixup training set, which is the Mix interaction by (4).\nFor Mixup Sampling, the complete objective for optimiza-tion is defined as:\n$L_{mix} = L_{ns} + L_m.$\n(7)"}, {"title": "B. Decay Sampling", "content": "To preserve the structure information during the sampling process, we propose Decay Sampling that strengthens the digestion of graph structure information via a soft link mech-anism. The weights of links between nodes decay with their distance in a Breadth First Search (BFS) manner. An illustra-tion of Decay Sampling is shown in Fig. 3.\nFor each anchored node in the graph, we compute the decay weights of links between it and its neighbors within l-hop based on BFS. The weight of links is designed based on the number of reachable paths between the anchored node and its l-hop neighbors. The hop number l of BFS is the same as the number of aggregation layers in the GNN model. The weight of link w\u2090 connecting the anchored node to the sampled node is defined as follows:\n$w_a = p + (1 \u2013 p) \\frac{r_d}{r_{max}},$ (8)\nwhere d is one of the non-positive sample of neighbors within l-hop reached by the anchored node v on BFS paths, w\u2090 is the link weight between node d and the anchored node, p is used to map the weights of w\u2090 to [p, 1], r_d is the number of pathways from anchored node v to node d, and r_{max} is the maximum value of all the r within l-hop by node v.\nWe sort the sampled nodes by defined decay weights and intercept Top k. Meanwhile, the link weights of the rest nodes are set to 0. Fig. 4 shows an example of BFS-based dacay weight.\nThe Decay Sampling part loss is defined as follows:\n$L_d = \\frac{1}{N} \\sum_{(v,d) \\in O_d} g (\\sigma (e_v^T e_d), w_a),$\n(9)\nwhile the complete objective of Decay Sampling for optimiza-tion is as follows:\n$L_{dec} = L_{ns} + L_d,$\n(10)\nwhere (v, d) \u2208 Od, Od is the decay item set of each anchored node."}, {"title": "C. MixDec Sampling", "content": "MixDec Sampling is a joint sampling method of Mixup Sampling and Decay Sampling. Mixup Sampling synthesizes the nodes and links, and then Decay Sampling preserves the graph's structure with soft links. An illustration of MixDec Sampling is shown in Fig. 5. Formally, a GNN-based model is trained according to the following loss function:\n$L = L_{ns} + L_m + L_d,$\n(12)\nwhere Lns is basic negative sampling loss, Lm is the Mixup Sampling part loss, Ld is the Decay Sampling part loss, re-spectively. Unlike Mixup Sampling, each node pair of MixDec"}, {"title": "D. Loss function", "content": "We use Mean Absolute Error (MAE) as the loss function g of MixDec Sampling. Then, the Mixup Sampling part loss is formalized as:\n$L_m = \\frac{1}{N} \\sum_{(v,s)\\in O_m} |\\sigma(e_v^T e_s) \u2013 w_s|,$ (13)\nand the Decay Sampling part loss is formalized as:\n$L_d = \\frac{1}{N} \\sum_{(v,d) \\in O_d} |\\sigma(e_v^T e_d) \u2013 w_a|.$\n(14)"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we evaluate the performances of the pro-posed sampling method on three benchmark datasets with three representative GNN-based recommendation models. Furthermore, to analyze the MixDec Sampling method, we per-form the hyperparameter study and the ablation study. Specif-ically, we intend to address the four research questions listed below:\n\u2022 Q1. Are there improvements in the effect of applying MixDec Sampling to representative GNN-based recom-mendation models? (See IV-B1).\n\u2022 Q2. Whether the proposed sampling method can con-tinuously and steadily improve the performances when plugged into existing GNN-based recommendation mod-els? (See IV-B2).\n\u2022 Q3. How does MixDec Sampling method perform for nodes with few neighbors? (See IV-B3).\n\u2022 Q4. How do hyperparameters affect the performance of the proposed method? (See IV-C)."}, {"title": "A. Experimental Settings", "content": "1) Dataset: We use three public benchmark datasets, Amazon-Book, Yelp2018, and Last-FM to evaluate our method. Each dataset contains users, items, and interactions between them. To guarantee the quality of datasets, we only keep users and items with more than ten interactions.\n\u2022 Amazon-Book [17]: Amazon-Book is a widely used dataset for product recommendation.\n\u2022 Last-FM [18]: This is the music listening dataset col-lected from Last-FM online music systems. We take the subset of the dataset where the timestamp is from Jan, 2015 to June, 2015.\n\u2022 Yelp2018 [19]: This dataset is adopted from the 2018 edition of the Yelp challenge.\nWe summarize the statistics of the three datasets in Table I. The table contains the number of user nodes, item nodes, and interactions in graphs, with the density of graphs. We construct user-item bipartite graphs, the same as [5], [20], [21]. For each dataset, we randomly select 80% of each user's interaction history as the training set, and use the rest as the test set. From the training set, we randomly select 10% of the interactions as the validation set to tune the hyperparameters.\nWe use observed user-item interactions as positive examples and use Uniform Negative Sampling to sample unconsumed"}, {"title": "B. Results and Analysis", "content": "We replace the Uniform Negative Sampling with our approaches (i.e., Mixup Sampling, Decay Sampling, and MixDec Sampling) on representative GNN-based recommen-dation models and evaluate their performance on three recom-mendation benchmarks.\n1) Performance Comparison (for Q1): Benchmarks are shown in Table II that includes GNN-based recommendation models and the proposed sampling approach implemented. The following are the findings:\n\u2022 It's hardly surprising that the Uniform Negative Sampling approach from the GNN model performs the worst in all evaluations, since it simply interprets the relationship"}, {"title": "2) Generalization Ability (for Q2):", "content": "Table II shows that our strategy consistently outperforms Uniform Negative Sampling on all three GNN-based recommendation models, proving its generalizability. In comparison to GraphSAGE and GAT, MixDec Sampling on GCN is far superior. This is due to the fact that GCN's preliminary performance on the most of results is the lowest. Our strategy considerably improves the less effective GNN-based model."}, {"title": "3) Performance Analysis on Different Density Graphs (for Q3):", "content": "In order to evaluate the performance of our method on nodes with few neighbors, we drop edges in the training set in multiple proportions to generate subgraphs with different degrees of sparseness. We completed this experiment with GCN, we set the dropping ratio to be 0%, 20%, 50%, and 70% on Amazon-Books for Uniform Negative Sampling, Mixup Sampling and MixDec Sampling. The results are presented in Table III. The observations are as followed:"}, {"title": "4) Efficiency Comparison:", "content": "Take GCN as an example, the comparison results of time consumption for training 2,500 epochs of Uniform Negative Sampling, Mixup Sampling, Decay Sampling and MixDec Sampling are shown in Table IV. Uniform Negative Sampling is the fastest on all datasets, because our three samplings all have to sample negative items. Among the two essential components of MixDec Sampling, Decay Sampling has the shorter time consumption. Due to including Uniform Negative Sampling, Mixup Sampling, and Decay Sampling, MixDec Sampling takes the longest time of all methods. Overall, MixDec Sampling contributes no more than 30 percent to the total time consumption of Uniform Negative Sampling. Thus, our method does not increase too much time consumption."}, {"title": "C. Parameter Study (for Q4)", "content": "1) Impact of Sampling Distribution: We performed pa-rameter sensitive experiments for \u03b1 and \u03b2 of Beta distribution on the Amazon-Book dataset utilizing GCN to discover the optimal sampling distribution, and the results are shown in Table V. We choose \u03b1, \u03b2 from {0.2,0.5,0.8,1,5,8}. Fig. 6 describes multiple particular Beta(\u03b1, \u03b2) distributions to high-light how varied a and \u03b2 impacts the sampling distribution. i. (a) When both \u03b1 and \u03b2 are less than 1 (e.g., \u03b1 = 0.2, \u03b2 = 0.2), \u03bb ~ Beta(\u03b1, \u03b2) is concentrated at 0 or 1. (b) When one of the values of \u03b1 and \u03b2 is greater than 1, \u03bb changes monotonically (e.g., A decreases monotonically when \u03b1 = 0.2, \u03b2 = 5, and increases monotonically when \u03b1 = 8 and \u03b2 = 0.2). (c) When both \u03b1 and \u03b2 are greater"}, {"title": "2) Impact of Decay Parameters:", "content": "To study the contribution of p and k to Decay Sampling, we conduct the parame-ter sensitive experiment on the Amazon-Book dataset with GCN by choosing p from {0, 0.3,0.5,0.8,1} and k from {100, 200, 300, 400,500}. As shown in Table VI, the best result is yielded when p = 0.5 and k = 500. The observations are as followed:\n\u2022 The parameter p is used to project the weights of the soft link to [p, 1]. If the parameter p is set to a large value (e.g., 0.8, 1), the hierarchy between decay nodes will be obscured. If set p to a small value (e.g., 0, 0.3),\n\u2022 the decay node will not get sufficient positive information. Experimental results show that the closer the value of p is to 0.5, the better the result achieved.\n\u2022 The parameter k indicates the number of decay items have been selected. When the value of p is appropriate, the model performance improves with the increase of k (Due to limited memory, we did not try larger values for k). Upon observation, the model performs optimally when p is equal to 0.5 and k is around 500. When k and p are both small (e.g. p = 0, k = 100), the performance is the worst."}, {"title": "V. RELATED WORK", "content": "In this section, we first provide a quick overview of GNNs and discussing their application in Recommendation tasks. Then, we introduce the current prevalent methods to negative sampling in GNNs, and their limitations. Finally, we introduce the data augmentation method of linear interpolation repre-sented by Mixup [12] and make a summary."}, {"title": "A. GNNs for Recommendation", "content": "In recent years, there has been a surge of research interest in developing varieties of GNNs, specialized deep learning architectures for dealing with graph-structured data. GNNS leverage the structure of the data as computational graph, allowing the information to propagate across the edges of graphs. A GNN usually consists of 1) graph convolution layers which extract local substructure features for individual nodes, and 2) a graph aggregation layer which aggregates node-level features into a graph-level feature vector. Kipf et al. [2] designed GCN by approximating localized 1-order spectral convolution. Hamilton et al. [3] subsequently improved GCN which alleviate the receptive field expansion by sampling neighbors. FastGCN [26] further improved the sampling al-gorithm and adopts importance sampling in each layer.\nLink prediction aims to find missing links or predict the like-lihood of future links. Most of the contemporary approaches of link prediction focus on homogeneous networks where the object and the link are of single (same) types such as author collaboration networks. These networks comprise less information which may causes less accuracy for the prediction task. In heterogeneous networks, the underlying assumption"}, {"title": "B. Negative Sampling", "content": "Negative sampling is firstly proposed to speed up skip-gram training in Word2Vec [15]. Negative sampling has a significant impact on the performance of networks in GNN-based tasks, such as recommender systems and link prediction. As an example, for the link prediction and user recommendation sce-narios, the recommendation model relies mostly on historical feedback from users to model their preferences. We learn the representation of users and items by feeding both positive and negative samples. In general, the interaction between users and items is treated as a set of implicit feedbacks. With implicit feedback, the database is not explicitly labeled, thus we have generally assumed all items that have interacted with the user are positive samples, and vice versa are negative samples. The approach to select negative samples can be broadly classified into heuristic negative sampling and model-based negative sampling.\nHeuristic negative sampling focuses on sampling by setting some heuristic rules. Bayesian Personalized Ranking (BPR) [7] with negative sampling by uniform distribution over equal probabilities. The BPR algorithm has straightforward strategy and avoids introducing new biases in the sampling process, which is a widely used method. Popularity-biased Negative Sampling (PNS) [34] applied the popularity of an item as the sampling weight, with the more popular the item, the more likely it is to be selected. The PNS increased the informativeness during sampling, but as its sampling distri-bution is calculated in advance, the sampling distribution does not change accordingly in the model training process [35]. Therefore, the informativeness from negative sampling will decrease after several training sessions.\nModel-based negative sampling aggregates the structural information of the model and obtains more high-quality neg-ative samples. Dynamically Negative Sampling (DNS) [36] dynamically changed the sampling distribution depending on"}, {"title": "C. Mixup", "content": "Interpolation-based data augmentation was proposed in Mixup [12]. Mixup extends the training data by training a neural network on convex combinations of pairs of exam-ples and their labels. Mixup has achieved relative success in many computer vision tasks. Mixup variants [39]\u2013[41] used interpolation in the hidden representation to capture higher-level information and obtain smoother decision bound-aries. Recently, more researchers have focused on utilizing Mixup to improve the model's performance in tasks with GNNs. GraphMixup [42] presents a mixup-based framework for improving class-imbalanced node classification on graphs. MixGCF [21] design the hop mixing technique to synthesize hard negatives. For graph classification, [43] propose a simple input mixing schema for Mixup on graph, coined ifMixup, and they theoretically prove that, ifMixup guarantees that the mixed graphs are manifold intrusion free. G-Mixup augment graphs for graph classification by interpolating the generator (i.e., graphon) of different classes of graphs.\nMix sampling is inspired by mixup and negative sampling, converting nodal relations into soft relations and optimizing the model by linear interpolation and Beta(\u03b1, \u03b2) distribution."}, {"title": "VI. CONCLUSION", "content": "In this work, we proposed a novel soft link-based sampling method namely MixDec Sampling for GNN-based recom-mendation models. Instead of being restricted to using hard positive pairs and hard negative pairs, MixDec Sampling provides a soft link to measure the proximity relationship between nodes, and the synthesized nodes also provide data augmentation for nodes with few neighbors. Extensive ex-periments demonstrate the proposed MixDec Sampling can improve the recommendation performance of several repre-sentative GNN-based models significantly and consistently on various recommendation benchmarks. We hope this work would inspire the future soft link sampling method for GNN-based recommendation systems for efficient and effective utilizing graph information."}]}