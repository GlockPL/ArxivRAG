{"title": "IMPROVER: AGENT-BASED AUTOMATED PROOF OPTIMIZATION", "authors": ["Riyaz Ahuja", "Jeremy Avigad", "Prasad Tetali", "Sean Welleck"], "abstract": "Large language models (LLMs) have been used to generate formal proofs of mathematical theorems in proofs assistants such as Lean. However, we often want to optimize a formal proof with respect to various criteria, depending on its downstream use. For example, we may want a proof to adhere to a certain style, or to be readable, concise, or modularly structured. Having suitably optimized proofs is also important for learning tasks, especially since human-written proofs may not optimal for that purpose. To this end, we study a new problem of automated proof optimization: rewriting a proof so that it is correct and optimizes for an arbitrary criterion, such as length or readability. As a first method for automated proof optimization, we present ImProver, a large-language-model agent that rewrites proofs to optimize arbitrary user-defined metrics in Lean. We find that naively applying LLMs to proof optimization falls short, and we incorporate various improvements into ImProver, such as the use of symbolic Lean context in a novel Chain-of-States technique, as well as error-correction and retrieval. We test ImProver on rewriting real-world undergraduate, competition, and research-level mathematics theorems, finding that ImProver is capable of rewriting proofs so that they are substantially shorter, more modular, and more readable.", "sections": [{"title": "1 INTRODUCTION", "content": "The fundamental virtue of a mathematical proof is that it provides certainty: a deductive argument shows that the assumptions of a mathematical statement logically guarantee the conclusion. In practice, however, informal, natural-language proofs are prone to imprecision, ambiguity, and error. Using a formal language such as Lean (Moura & Ullrich, 2021) removes ambiguity and precision and enables a proof assistant to verify correctness down to the primitives of a formal axiomatic system.\nFormal proofs, however, can be hard to read and often suffer from low reusability or excessive detail. For example, formal proofs in Lean's extensive mathematical library, Mathlib (mathlib Community,\n2020), are generally designed to be concise and very general, often at the expense of readability. Formal proofs in an expository text, in contrast, may include detailed calculations steps, making them readable but verbose. Machine learning systems trained on such proofs learn to mimic these varied conventions (Hu et al., 2024), which may not be the optimal use of the limited supply of human-written proofs. As a result, we would like to be able to automatically refactor proofs to meet a secondary objective such as length or readability.\nTo this end, we study a new problem of automated proof optimization: rewriting a proof so that it is correct and optimizes a criterion such as length or readability. We find that naively applying LLMs to proof optimization falls short, often resulting in incorrect or poorly optimized proofs. We develop various improvements that can be applied on top of a black-box language model, including Chain-of-States prompting-an analogy to chain-of-thought prompting (Wei et al., 2022) that shows intermediate proof states-along with error-correction and retrieval. We incorporate these into ImProver: a large language model agent that rewrites proofs to optimize arbitrary user-defined metrics in Lean."}, {"title": "2 RELATED WORK", "content": "Recently there has been wide interest in automating theorem proving in interactive proof assistants; see (Lu et al., 2023; Li et al., 2024) for surveys.\nA typical approach (Polu & Sutskever, 2020) is to train on a large corpus of mathematical proofs such as Lean's Mathlib (mathlib Community, 2020; Han et al., 2022; Polu et al., 2022; Lample et al.,\n2022; Yang et al., 2023; Hu et al., 2024). A model learns from the distribution of proofs in the corpus, such as Mathlib-style proofs. Recently, the AlphaProof (AlphaProof & Teams, 2024) system was shown to produce proofs with an arcane, non-human structure and syntax. We consider the new problem of rewriting a proof to optimize a metric, such as rewriting a proof into a more readable or more concise one. Proof optimization is more general than theorem proving, since we can also rewrite an empty proof to optimize correctness. Finally, there is a rich literature on the varied styles of (human) formal proofs (e.g., (Autexier & Dietrich, 2010; Wiedijk, 2004)). Our model, ImProver, builds on neural theorem proving techniques including full proof generation (Jiang et al.,\n2023; First et al., 2023), conditioning on example proofs (Jiang et al., 2023), retrieval (Yang et al.,\n2023; Thakur et al., 2024), and preceding file context (First et al., 2023; Hu et al., 2024), as well as error correction (Madaan et al., 2023; Chen et al., 2023) and documentation retrieval (Zhou et al.,\n2023) from code generation. ImProver brings these code generation techniques, along with new Chain-of-States prompting and meta-programmed contextual information, into a unified proof optimization agent."}, {"title": "3 AUTOMATED PROOF OPTIMIZATION WITH ImProver", "content": "Automated Proof Optimization. Given a theorem statement $x$, additional context $c$, and an initial proof $y_0$, proof optimization consists of generating a new proof $y$ that is correct and minimizes (or maximizes) a metric $u(x, c, y_0, y) \\rightarrow \\mathbb{R}$."}, {"title": "3.1 IMPROVER", "content": "We develop several improvements that can be applied to a black-box LLM generator $Y_{out} \\sim G(x_{in})$, such as GPT-4 (OpenAI et al., 2024), and specify ImProver with respect to these parameters. The explicit prompts and templates that are sent to the LLM can be found in (\u00a7A)."}, {"title": "3.1.1 CHAIN-OF-STATES PROMPTING", "content": "Typical formal proofs are a sequence of tactics (akin to steps) and states that show the hypotheses and goals at each step. The intermediate states often contain valuable information (e.g., an expression after it has been simplified) that is not present in the tactics. To allow the model to reason about these intermediate goals and hypotheses, we use tools from Lean metaprogramming to automatically annotate each proof state as a comment prior to each tactic. We refer to this method as Chain-of-States (CoS) prompting since it makes intermediate states explicit, akin to how chain-of-thought prompting (Wei et al., 2022) makes intermediate steps of a solution explicit.\nThese states are extracted directly and symbolically from the underlying Lean compilation steps using Lean's rich metaprogramming suite. The implementation of this extraction system is modeled from the work (Kim Morrison, 2024). Specifically, in the compiler's elaboration and evaluation stages where the parsed theorem code is first converted into concrete syntax trees (in practice, Syntax objects) and abstract syntax trees (Expr objects) we convert the CST and AST output objects into the relevant proof data and proof states in the form of proof trees (Lean.Elab.InfoTree). These proof trees contain detailed context and information on a tactic-by-tactic level relating to the modification of the proof state, metavariable context, and proof correctness.\nAfter state extraction is completed and cached for efficient future access, we annotate the proof text itself to contain the intermediate states in the form as comments."}, {"title": "3.1.2 OUTPUT FORMATTING.", "content": "LLM outputs often contain ancillary and syntactically invalid content, especially before and after the actual proof. Additionally, by applying additional structure to the LLM outputs, we may hope to generate more structured proofs. To analyze this hypothesis, we introduce two additional output formats to the standard str output: flat and structured. The former enforces a tactic sequence output as a list of strings, and the latter enforces a proof tree output as a tree of strings."}, {"title": "3.1.3 SAMPLING METHOD", "content": "We also introduce different methods of sampling between many (sequential or parallel) LLM inference calls, involving best-of-n and iterative refinement implementations, as well as combinations thereof."}, {"title": "4 EXPERIMENTS", "content": "We test ImProver on rewriting real-world undergraduate theorems, competition problems, and research-level mathematics and compare its results to those of the base GPT-40 and GPT-40-mini models. We examine the optimization capabilities of ImProver for the length and readability metrics - studying the effectiveness in maintaining the correctness of the tactic proof while making it more concise, as well as making it more declarative in style and readable in practice."}, {"title": "4.1 SETUP", "content": "Our experimentation is split into three distinct stages. We first perform ablation testing on the ImProver model parameters (\u00a73.1) to ensure that ImProver's parameter specification is the optimal one with respect to correctness and metric optimization score. We then evaluate this optimal parameter combination on datasets of varying complexity and analyze the performance and results thereof. Lastly, we note the performance of ImProver in NTP applications in comparison to the base GPT-40 and GPT-40-mini models.\nWe evaluate ImProver on subsets of the following datasets.\nMathematics in Lean (MIL) (leanprover-community, 2024): this dataset contains pedagogical solutions of common undergraduate-level exercises, and as such contains many readable, yet verbose and inefficient proofs. We use exercise solutions from set theory, elementary number theory, group theory, topology, differential calculus, and integration & measure theory. This dataset contains theorems at an undergraduate-level of complexity. For our main results, we evaluated on 72 theorems from exercise solutions from MIL chapters 4, 5, 8, 9, and 10.\nCompfiles (David Renshaw, 2024): Solutions of International Mathematics Olympiad (IMO) and American Mathematics Olympiad (USAMO) competition problems from 2016 to 2024. This is a dataset of internationally-renowned competitive math problems, many of which are readable, yet quite verbose. This dataset contains theorems of a competitive format, and although they contain"}, {"title": "4.1.1 ABLATIONS", "content": "When performing our ablation studies, we used a fixed dataset (MIL) and metric (length) and varied the parameters of all the features to find the optimal combination. However, as there are over 8640 possible combinations, it is inefficient to test all combinations at once. As such, we evaluate using a factorial testing method.\nWe define the following testing groups with the specified parameter combinations:\nGPT-40-mini/GPT-40: This varies the GPT-40 model, outputting a string with no other features.\nOutput and CoS: We evaluate the effects of different output formatting styles (string, string\nlist, string tree) and CoS (True, False), with the model fixed as GPT-40, with no other\nfeatures enabled.\nExample Retrieval: We evaluate the effects of increasing the number of examples provided (multi-shot prompting) in the range of 0, 3, 5, 7, and 10, with the model fixed as GPT-40, CoS and output formatting fixed as the best combination from the previous test, and no other features enabled.\nSampling Method: Here, we evaluate the effects of best-of-n and refinement for a fixed $n = 5$. Additionally we test on the refinement cases if forwarding the most recent iteration result, or all previous iteration results is the best, and if we should keep the best out of the iterations, or the most"}, {"title": "4.2 RESULTS", "content": "ImProver is capable of optimizing proofs in all settings. From Table 2, Table 3, and Table 4, we can see that ImProver is capable of optimizing proofs on all datasets for both the length and readability metrics. Furthermore, Table 1 shows that across all metrics, ImProver significantly outperforms GPT-40 on proof optimization tasks on every experimental measure - aggregated from all datasets. Additionally, from Table 2, Table 3, and Table 4, we can see that ImProver outperforms GPT-40 on each dataset as well.\nWe proceed to analyze this data and its implications.\nLength optimization. First focusing on the length metric, we see that ImProver outperforms GPT-40 with respect to the improvement score by 566% (aggregated over all datasets). Additionally, we are guaranteed that ImProver produces a correct output, although that output may just be the same as the input. However, 35.44% of the time, it generates a correct output that is not the same length as the input, and in that case, we expect an average of a 55.29% reduction in length. Comparing this with GPT-40, we conclude that not only can ImProver optimize at a higher level on arbitrary theorems, but its ability to generate nontrivial correct outputs is far greater in comparison to GPT-40."}, {"title": "Readability optimization.", "content": "Readability optimization is similar, with ImProver outperforming GPT-4o by 423%. Moreover, the accuracy, improved accuracy, and nonempty improvement disparities for readability parallel those of the length tests. However, it should be noted that for both GPT-40 and ImProver, the accuracy and improved accuracy scores were markedly smaller for readability than length optimization. This suggests that for both models, it was generally more \u201cdifficult\" to generate a correct output, and moreover, generate a correct output with a better metric score than the input, for readability optimization than length optimization. In other words, optimizing for readability is more difficult for the underlying generator than optimizing for length. However, we speculate with higher-quality prompts, descriptions of the metric, and examples, this disparity can be minimized. Regardless, we note that different metrics can be less likely to be correctly optimized, and that model performance is correlated with the metric it seeks to optimize \u2013 both for GPT-40 and ImProver."}, {"title": "Optimization varies based on dataset difficulty.", "content": "Additionally noting Table 2, Table 3, and Table 4, we observe that the improvement score for both metrics for both GPT-40 and ImProver is highest for the MIL dataset, lower for Compfiles, and the lowest on the Mathlib theorems. This suggests that the expected improvement in metric score decreases with higher difficultly \u2013 with undergraduate-level theorems having a significantly higher expected improvement than research-level theorems. However, it should be noted that for both metrics, the nonempty improvement of ImProver stayed consistent, whereas for GPT-40, it followed the aforementioned trend of decreasing with difficulty. Similarly, the accuracy and improved accuracy scores for both metrics and models decreased with higher difficulty datasets (disregarding ImProver's accuracy scores, as they are ensured to be 100%). This suggests that although the base GPT-4o generator is less likely to generate a correct output for higher difficulty datasets, the improvements that ImProver makes to the base generator allows it to maintain its improvement in the metric score whenever a correct output is generated. As such, we can speculate that the bottleneck in the improvement score is not the model's ability to optimize the proof for a metric, but rather its ability to generate a new correct proof at all. As such, we conjecture that with more capable generator models, the accuracy and thus, the improvement score \u2013 in optimization tasks will continue to increase, until the improvement scores match the nonempty improvement.\nOverall, we conclude that although the performance of both ImProver and GPT-40 decreases on length and readability optimization on more difficult datasets, ImProver significantly outperforms GPT-40 on all datasets for length and readability optimization."}, {"title": "4.2.1 ABLATION TESTING", "content": "We perform ablation studies using a subset of the MIL dataset as discussed in \u00a74.1.1. The results of this factorial study are aggregated in Table 5. We measure the baseline results from the GPT-40 and GPT-40-mini models, noting that GPT-40 is the better-scoring model (with respect to the improvement score). Thus, fixing this model, we vary the output formatting type and if CoS is enabled, and determine that outputting flat with CoS enabled maximizes the improvement score."}, {"title": "Readability and Chain-of-States (CoS) Ablation.", "content": "We additionally examine the effects of disabling CoS on readability optimization tasks, as the previous study focused on length optimization tasks, and we speculate that CoS has a high impact on the performance of readability optimization tasks, as the proof states that are embedded due to CoS seem to be a critical aspect to generating the explicit declarations that the readability metric measures.\nWe confirm this result by considering Table 6 and observe that simply enabling CoS nearly doubles the improvement score, and significantly improves the nonempty improvement score, suggesting that CoS has a high degree of impact on optimizing for the readability metric, as conjectured. However, we also note a significant increase in improved accuracy, which suggests that embedding the chain of states also improves the ability of the model to generate nontrivial correct outputs, implying that the symbolic information contained in the states are critical to effectively modifying the structure and content of a proof."}, {"title": "4.2.2 NEURAL THEOREM PROVING EVALUATION", "content": "We evaluate ImProver's neural theorem proving (NTP) performance using the completion metric. We evaluate on 23 exercises in group theory (12) and set theory (11) from MIL, with an empty input proof. Table 7 shows the accuracy on the dataset split by topic for both ImProver and GPT-40. ImProver substantially outperforms GPT-40 across all topics, with an 80% increase in accuracy compared to the base model, showing that proof optimization systems are easily extendable to NTP systems."}, {"title": "4.3 QUALITATIVE RESULTS", "content": "Next, we discuss qualitative examples showing the improvements from ImProver in proof optimization. These examples show the balance between correctness and the desired optimization metric, showing how ImProver can produce more concise or readable proofs depending on the use case.\nExample 1: Compfiles Optimization. Consider Figure 1, a lemma from the 2022 IMO Question 2 (Compfiles) that we optimize for length.\nThe original proof consisted of 12 tactic steps and multiple intermediate calculations. After applying ImProver for length optimization, the proof was halved to only 6 tactics, eliminating redundant steps while maintaining correctness.\nIn comparison, the shortened proof makes multiple nontrivial optimizations, such as eliminating the h2' and h4 and how hypotheses, as well as fully generating proof terms for specific rewrites and other tactics."}, {"title": "Example 2: MIL.", "content": "Consider Figure 3, a result from MIL that we optimize for readability."}, {"title": "Example 3: Full Proof Generation.", "content": "We analyze the application of ImProver to neural theorem proving in the MIL example from Figure 4.\nThis theorem relating to group theory originally has no proof, however, ImProver generates one from scratch. This generated proof is verified to be correct by Lean, utilizing all the included hypotheses as well as a retrieved mathlib theorem."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced ImProver, a novel agent-based tool for automated proof optimization in Lean. By incorporating CoS, RAG, and other features, ImProver significantly outperforms base language models in proof optimization over undergraduate, competition, and research-level problems.\nHowever, ImProver is limited by its high cost and slow runtime, which is exacerbated by its reliance on black-box LLM's. We intend to address this inefficiency in future work by applying fine-tuning and RL on a smaller model to match performance at a lower cost.\nImProver demonstrates its ability to generate substantially shorter, more readable, and modular proofs while maintaining correctness. As such, we believe that ImProver sets the stage for further work on proof optimization to advance the study and use of AI in mathematics."}, {"title": "A PROMPTS", "content": "In this appendix, we note the prompts used by ImProver both for general LLM prompting, as well as the metric-specific prompts."}, {"title": "A.1 TEMPLATE", "content": "For the main prompt sent to the LLM on each sample, we build a prompt string using a chat prompt template that is then invoked at runtime to fill in the variables.\nNamely, these variables include the set of metric prompts, previous results, input theorem, context, a syntax documents, Mathlib documents, and examples.\nThe prompt template is a conversation of the format:"}, {"title": "A.2 METRIC PROMPTS", "content": "Length Metric\nSystem: You are an AI assistant who shortens Lean 4 proofs while ensuring their correctness. You will aim to reduce the number of lines of the tactic proof while ensuring that it properly compiles in Lean 4.\nUser: Shorten the current theorem (wrapped in <CURRENT>...</CURRENT>) to be as short in length-measured in the number of lines of the proof-as possible, while also ensuring that the output is still syntactically correct.\"\nReadability Metric\nSystem: You are an AI assistant who rewrites Lean 4 proofs to be more readable while ensuring their correctness. We measure readablity by considering the ratio of the number ofexplicitly typed have tactics against the total number of tactics in the proof, as this is proportional to whether a proof is declarative in style, and thus, readable.\nUser: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is more readable and declarative and modular.\nCompletion Metric\nSystem: You are an AI assistant who automatically solves Lean 4 proofs (as in, generates the tactic proof) and ensures its correctness. You will receive a Lean 4 proof you must modify to eliminate any errors so that it compiles correctly and eliminate any \"sorry\"s with full proofs.\nUser: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is a formal, complete, and correct Lean 4 proof by filling in its tactic proof."}, {"title": "B ADDITIONAL EXPERIMENTAL RESULTS", "content": "In this section, we provide more detailed information on the experimental setup and results used to evaluate ImProver."}, {"title": "B.1 ABLATION DETAILS", "content": "We now proceed to show detailed results from our ablation testing."}]}