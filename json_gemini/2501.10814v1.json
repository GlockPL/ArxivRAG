{"title": "No More Sliding Window: Efficient 3D Medical Image Segmentation with Differentiable Top-k Patch Sampling", "authors": ["Young Seok Jeon", "Hongfei Yang", "Huazhu Fu", "Mengling Feng"], "abstract": "3D models are favored over 2D for 3D medical image segmentation tasks due to their ability to leverage inter-slice relationship, yielding higher segmentation accuracy. However, 3D models demand significantly more GPU memory with increased model size and intermediate tensors. A common solution is to use patch-based training and make whole-volume predictions with sliding window (SW) inference. SW inference reduces memory usage but is slower due to equal resource allocation across patches and less accurate as it overlooks global features beyond patches.\nWe propose NMSW-Net (No-More-Sliding-Window-Net), a novel framework that enhances efficiency and accuracy of any given 3D segmentation model by eliminating SW inference and incorporating global predictions when necessary. NMSW-Net incorporates a differentiable Top-k module to sample only the relevant patches that enhance segmentation accuracy, thereby minimizing redundant computations. Additionally, it learns to leverage coarse global predictions when patch prediction alone is insufficient. NMSW-Net is model-agnostic, making it compatible with any 3D segmentation model that previously relied on SW inference.\nEvaluated across 3 tasks with 3 segmentation backbones, NMSW-Net achieves competitive or sometimes superior accuracy compared to SW, while reducing computational complexity by 90% (87.5 \u2192 7.95 TFLOPS), delivering 4x faster inference on the H100 GPU (19.0 \u2192 4.3 sec), and 7x faster inference on the Intel Xeon Gold CPU (1710 \u2192 230 seconds).", "sections": [{"title": "I. INTRODUCTION", "content": ""}, {"title": "A. 2D vs 3D model for 3D Medical Image Segmentation", "content": "2D and 3D segmentation models possess distinct advantages and disadvantages when applied to 3D medical image seg-mentation tasks. For 2D models, the 3D input is destructured into a batch of 2D slices, typically along the axial direction, with predictions made independently for each slice. While 2D models are computationally efficient, they fail to capture inter-slice relationships, potentially reducing the accuracy of the final whole-volume prediction.\n3D models, in contrast, take the whole 3D input without the destructuring. The model weights extend along the axial direction to capture inter-slice relationships, resulting in improved accuracy [1], [2]. However, this approach comes at the cost of increased computational demands."}, {"title": "B. Computational Ineffciency of 3D models", "content": "Increased computational demand and memory require-ments present a significant bottleneck for all 3D segmenta-tion models, regardless of their underlying backbone struc-ture-whether Conv-net [3], Self-attention [4], or State-Space models [5]. Using ConvNet as an example, increasing the convolutional kernel dimension from $k$ to $k \\times k \\times k$ amplifies the number of multiplications and additions re-quired to produce each output pixel/voxel by a factor of $k$."}, {"title": "C. Reducing 3D Model's Memory with Sliding Window", "content": "Patch-based training, coupled with Sliding-Window (SW) inference, is the predominant method to address the substantial memory requirements of 3D models. Rather than processing the entire volume in a single step, patch-based training ran-domly samples a set of patches that are significantly smaller than the actual volume. To generate the final whole-volume prediction using the patch-trained model, SW, as shown in Figure 1, is employed. SW makes sequential predictions on patches sampled at uniform intervals with some overlap (typically between 25-50% [9]). The overlapping predictions are then aggregated into the final whole-volume prediction with an appropriate post-processing step."}, {"title": "D. Inefficiency and prediction bias in SW-inference", "content": "Although SW is memory-efficient due to its constrained input patch size, it has significant drawbacks, including slower inference speed and increased computational demands. Because the patch-based model is not trained to rank patch importance, SW adopts a conservative approach, segmenting all patches with equal computational resources, even those that clearly do not contain objects of interest or the object is simple enough for a less complex model to handle equally well.\nA typical ensemble UNet model [9] takes nearly a minute to perform a full SW on a whole-volume size of 512 \u00d7 512 \u00d7 458 using an NVIDIA GeForce RTX 3090 GPU [10]. Moreover, health practitioners often rely on on-device processors for model inference, where the speed drops significantly, taking over 10 minutes (Table I).\nSW is not only slow during inference, but could potentially lead to poorer performance due to small patch size. One unique aspect of medical image segmentation, compared to generic segmentation tasks on natural images, is the need for global features, such as location, which greatly help in distinguishing objects that may appear similar at a local level. Patch-trained models, however, may miss these global aspects. The most common approach to address this inherent limitation in patch-based models is to train another model that segments using a low-resolution input [9]. However, this adds additional com-plexity to a model that is already computationally expensive."}, {"title": "E. How do Radiologists Segments", "content": "How do radiologists segment objects in 3D medical images? Investigating their segmentation protocols could provide in-sights for creating a more computationally efficient inference method. Unlike the SW inference, radiologists follow a dis-tinct, streamlined approach. Their process typically involves three steps:\n1) Initial Assessment: Radiologists begin by quickly skim-ming through the slices to grasp the anatomical context and locate the regions of interest (ROIs). During this step, they also assess and rank the segmentation diffi-culty of each ROI.\n2) Preliminary Segmentation: Next, they begin segment-ing ROIs identified as lower in difficulty in a semi-automated manner using machine learning or traditional labeling tools.\n3) Focused Refinement: In the final step, radiologists al-locates additional time to refine the coarse segmentation output from the previous step, based on the rankings established in the initial assessment.\nThis workflow enables radiologists to min-max efficiency and accuracy."}, {"title": "F. No-More-Sliding-Window Inference", "content": "We introduce a novel and computationally efficient inference framework called No-More-Sliding-Window-Net (NMSW-Net), which closely resembles the segmentation pro-tocols used by human radiologists described above. As illus-trated in Fig 1, NMSW-Net replaces the costly SW inference method that applies the same compute resources across all patches with a differentialbe patch sampling module that picks only a handful of patches with higher importance. NMSW-Net aggregates the predictions from the selected patches with low-res global prediction to produe the final full-res whole-volume prediction.\nSpecifically, NMSW-Net operates through a three-step pro-cess: (1) Global Prediction: A global model processes a low-resolution whole-slide volume, generating two outputs: a coarse global prediction and a discrete probability distribution that indicates the likelihood of each region enhancing the final prediction score when aggregated with the global prediction.\n(2) Patch Selection and Prediction: High-resolution patches are selected based on location vectors sampled from the discrete probability distribution using our proposed differen-tiable top-K sampling module. These selected patches are then processed by a local model to generate granular local predic-tions. (3) Final Aggregation: The coarse global prediction is combined with the top-K patch predictions through our aggregation module to produce the final prediction.\nWe emphasize that NMSW-Net is not a new segmentation model but a framework designed to enhance the computational efficiency of existing 3D medical image segmentation models. NMSW-Net can be integrated with any 3D model that previ-ously employed SW. Across evaluations on three multi-organ segmentation tasks using three different backbone models, NMSW-Net consistently achieves competitive segmentation"}, {"title": "II. RELATED STUDIES", "content": ""}, {"title": "A. Practice of Sliding Window Infenrence in 3D model", "content": "SW has become the dominant approach to managing the high memory demands of 3D medical image segmentation in nearly all newly proposed models [9], [12], [13]. Even recent efforts to improve the efficiency of 3D segmentation models focus primarily on optimizing the backbone network [11], [14], [15], while relying on SW during inference.\nMoreover, patch-based methods are widely adopted across various applications beyond 3D segmentation. Patch-based in-ference has proven to be effective but computationally expen-sive, for example, in computational pathology for whole-slide image analysis [16], [17] and mammogram classification [18]. Thus, while we present our approach as a solution to address the limitation of SW in 3D segmentation, it can tackle other mega- or gigapixel problems with minor modifications."}, {"title": "B. Attemps to Reduce Computation Cost of 3D model", "content": "As aforementioned, most of recent efforts to improve the efficiency of 3D segmentation models focus primarily on op-timizing the backbone architecture, while maintaining patch-based training, with only a few studies that attempt to reduce computational costs with efficient sampling strategy but with obvious scaling issues.\n1) Optimizing Backbone Architecture: [11] addresses the computational complexity of the self-attention module by introducing a set of smaller attention modules, where each module processes a reduced number of tokens. The outputs of these resized tokens are then concatenated to restore the original token count. [19] replaces the standard multi-channel convolutions with channel-wise convolutions or 1 \u00d7 1 convolu-tions. [20] uses knowledge distillation to supervise the training of a small student model with a large teacher model. Our proposed approach can be added on top of these lightweight models to make the inference cost even cheaper and faster.\n2) Optimizing Inference Stretegy: Though not explicitly mentioned in the literature, the following works have the potential to replace SW in 3D segmentation, but with some notable scaling issues.\nMask R-CNN [21] is a two-stage instance segmentation model that initially identifies ROI bounding boxes, followed by segmentation within the chosen ROIs. A low-resolution input could potentially be used in the initial region proposal stage. However, as noted in the article, the proposed ROIS frequently overlap substantially, thus a high number (300 to 1000) of crops should proceed to the segmentation stage for optimal results, potentially leading to even slower inference speed than SW.\nLikewise, [22] proposes a two-step model where the first stage estimates a deformed grid from a low-resolution input, and the second stage uses this grid for resampling [23] the high-resolution input in an end-to-end differentiable manner. However, when applied to segmentation tasks, the image re-sampled from the learned grid can lead to information loss due to imprecise intensity interpolation, resulting in sub-optimal segmentation performance, when the prediction is mapped back to a uniform grid.\n[24] proposes using Deep Q Learning [25] to allow a rein-forcement learning (RL) agent to iteratively update a bounding box through a finite action space (e.g., zoom, shift). The reward is based on the overlap between the proposed bounding box and the ground truth mask. The updated bounding box is then used to crop a region for segmentation. Nevertheless, due to the iterative update procedure that involves treating a 3D volume as a state, this method is computationally inefficient and struggles with scalability for multi-organ tasks."}, {"title": "C. Differentiable Patch Sampling in Computer Vision", "content": "While the use of a smart patch selection module instead of random patch training with SW may seem like a trivial solution that many have considered in the past, the non-differentiable nature of the sampling operation has been the key limiting factor in deep learning application.\nIn recent years, several methods have been proposed to make sampling operations differentiable. One of the earliest approaches to achieving differentiable sampling in deep learn-ing was introduced by Kingma and Welling [26], who applied the reparameterization trick to a normal distribution, enabling backpropagation through stochastic nodes in neural networks. Reparameterization trick was further explored by Maddison et al. [27] and Jang et al. [28], extending differentiability to any finite discrete distribution. Their approach used the Gumbel-Max trick [29], [30] which detaches a stochastic node from the rest of learnable determistic nodes, and replaced the discrete argmax operation with temperature annealed softmax. Besides the reparameterization trick introduced, other approaches en-abling learning through stochastic sampling include optimal transport [31], perturbed optimization [32], and Monte Carlo approximation [33].\nFew studies have applied differentiable sampling techniques for importance-based patch sampling [34], [35], primarily targeting high-resolution 2D classification tasks. In contrast,"}, {"title": "III. METHOD", "content": ""}, {"title": "A. NMSW Overview", "content": "As shown in Fig. 2, unlike conventional patch-based train-ing, which processes a batch of patches randomly sampled from a 3D full resolution scan, NMSW-Net takes the entire scan, $X_{high} \\in \\mathbb{R}^{H \\times W \\times D}$, as input. In the model, $X_{high}$ is mapped to a downsized scan $x_{low} \\in \\mathbb{R}^{H' \\times W' \\times D'}$ and a list of overlapping patches sampled at a regular interval $X_{patch} = [x_{patch}^{(1)}, x_{patch}^{(2)},...,x_{patch}^{(N)}] \\in \\mathbb{R}^{N \\times H_p \\times W_p \\times D_p}$, where $N$ is the total number of patches 1.\nThe global backbone $f_g$, a generic semantic segmentation model, takes $X_{low}$ and produces two outputs: 1) a coarse global prediction $\\hat{y}_{low} \\in [0,1]^{C \\times H_1 \\times W_1 \\times D_1}$ and 2) a discrete pdf $p(z | X_{low}) \\in [0,1]^N$ (represented as 2D in Fig 2 for better visu-alization) which estimates the importance of individual patches in contributing to the accuracy of the final segmentation map $\\hat{Y}_{high}$.\n$K$ important patch locations, ${z^{(k)} \\in [0,1]^{N}}_{k=1}^{K}$, are sampled from $p(z|x_{low})$ without replacement using the Differentiable_Top-K block. Subsequently, $K$ important patches, ${x_{patch}^{({k^*})}}_{k=1}^{K}$, are chosen from $[x_{patch}^{(1)}, x_{patch}^{(2)}, ..., x_{patch}^{(N)}]$ based on ${z^{(k)}}_{k=1}^{K}$.\nThe selected patches are mapped to patch predictions, ${\\hat{y}_{patch}^{(k)} \\in [0,1]^{C \\times H_p \\times W_p \\times D_p}}_{k=1}^{K}$, using another semantic segmentation model at the patch level, denoted as $f_i$. The global and local models do not share weights and are not required to have identical model architectures. As the final step, the Aggregation block combines $\\hat{y}_{low}$ with ${\\hat{y}_{patch}^{(k)}}_{k=1}^{K}$ to produce the final whole-volume prediction, $\\hat{y}_{high}$.\nAll modules in NMSW are differentiable, enabling end-to-end gradient-based training to produce the whole-volume prediction. Thus NMSW eliminates the need for heuristic methods like SW to convert patch predictions into a full-volume prediction."}, {"title": "B. Blocks", "content": "1) Differentiable Top-K: Training a stochastic module like Differentiable Top-K is challenging due to two non-differentiable operations: (1) the random sampling operation and (2) the categorical nature of the samples. To address the non-differentiability problem, we introduce a modified version of Reparameterazable Subset Sampling algorithm [42], which generalizes the Gumbel-Softmax trick to a Top-K sampling scenario.\nThe size of $N$ is computed as $N = N_h N_w N_d$, where $N_h, N_w$, and $N_d$ are the patch numbers at corresponding dimension. For instance, in dimension D, $N_d$ is given by $N_d = \\frac{D-D_{patch}}{D_{pod}}+1$, where, $r_d$ is the downsampling ratio, and $o_d$ is the overlaping ratio."}, {"title": "Gumbel-Softmax [27], [28]", "content": "We begin by introducing Gumbel-Softmax, which proposes reparameterizable contin-uous relaxations of a categorical distribution. Given a cate-gorical distribution $p(z)$, where the probability of the n-th outcome is $p(z = n) = \\pi_n$, Gumbel-Softmax approximates the sampling operation ($z \\sim p(z)$) as:\n$z_{soft \\_ hot} = [y_1, y_2,\u2026\u2026\u2026, y_N], y_n = \\sigma_\\tau(log(\\pi_n) + g_n),(1)$\nwhere taking argmax on $z_{soft \\_ hot}$ gives z. $g_i \\sim Gumbel(0, 1)$ is a sample from the Gumbel distribution with $\\mu = 0$ and $\\beta = 1.2$ The $\\sigma_\\tau$ is a softmax function with a temperature parameter $\\tau \\in [0,\\infty]$, defined as:\n$\\sigma_\\tau(x_n) = \\frac{exp((log(x_n) + g_n)/\\tau)}{\\sum_{m=1}^{N} exp((log(x_m) + g_m)/\\tau)}(2)$\nAs $\\tau$ approaches 0, the Gumbel-Softmax estimator approx-imates the target categorical distribution $p(z)$."}, {"title": "Differentiable Top-K:", "content": "generalizes the Gumbel-Softmax estimator to draw Top-K samples without replacement. Unlike Gumbel-Softmax which has a fixed distribution $p(z)$ through-out sampling, Differentiable Top-K (Fig 3(a)) masks the probabilities of previously sampled outcomes. For instance, the k-th random sample, $z_{soft \\_ hot}^{(k)}$, is defined as:\n$z_{soft \\_ hot}^{(k)} = [y_1^{(k)}, y_2^{(k)},...,y_i^{(k)},...,y_N^{(k)}], y_i^{(k)} = \\sigma_\\tau(log(\\pi_i^{(k)}) + g_i),(3)$\nwhere\n$\\pi_i^{(k)} = \\begin{cases} 0,& \\text{ if } i \\in \\underset{k'}{\\text{argmax}}(z_{hard}^{(k-1)}) \\\\ \\pi_i,& otherwise, \\end{cases}(4)$\nand for the initial case, $\\pi_i^{(1)} = \\pi_i$.\nOur application strictly requires the sampled variable to be one_hot rather than soft_hot, as artifacts from other patches may otherwise contaminate the extracted patches. While the Straight-Through (ST) estimation addresses this by enabling one_hot behavior in the forward pass and soft_hot behavior in the backward pass, it introduces huge gradient bias especially during the early training stage when the probability is not saturated. To mitigate this, we propose an adjustment to ST by scaling the one_hot samples with their corresponding soft_hot values: $z_{hard}^{(k)} = z_{one \\_ hot}^{(k)}z_{soft \\_ hot}^{(k)}$. This adjustment accelerates convergence by reducing the gradient bias.\nTop-K patches are extracted from the candidate patches via a simple inner product:\n$x_{patch}^{(*k*)} = (z^{(k)}, X_{patch}).(5)$\nSince the maximum value of $z^{(k)}$ lies within the interval [0, 1], the intensity of the extracted patches is proportionally af-fected, as illustrated by the colored output patches in Fig 3(a)."}, {"title": "2) Aggregation:", "content": "Given a set of patch predictions ${\\hat{y}_{patch}^{(k)}}$ from the selected Top-K patches and the coarse global predic-tion $x_{global}$, Aggregation block merges the two predictions into the final whole scan prediction $\\hat{y}_{high}$ in a fully differentiable way.\nA naive approach would be to simply paste or overwrite the predicted patches onto the upscaled coarse prediction. However, this approach could lead to suboptimal predictions for two reasons: (1) Predicted patches often have overlapping areas, and merely pasting them disregards contributions from other patches in these regions. (2) A simple overwrite assumes that patch predictions are always more accurate than the coarse prediction, which may not hold true, especially for objects that rely on global features.\nThe Aggregation block 3(b) addresses the two biases present in the naive approach. Before adding the patches to the upscaled coarse prediction, each patch is multiplied by a patch weight, $p\\_w \\in [0,1]^{P_h \\times P_w \\times P_d}$, which is a discretized Gaussian distribution with $\\mu = 0$ and $\\sigma = 0.125$. This multiplication enables the model to blend the patches smoothly in the overlapping regions. 4. To have a weighted prediction between the global and patch predictions, we also introduce a learnable class weight $c\\_w_e \\in [0, 1]^C$. This allows the model to leverage global predictions for specific organs when necessary. A predicted patch is added to the upscaled global prediction"}, {"title": "IV. EXPERIMENTS", "content": ""}, {"title": "A. Comparative Baseline Inference Techniques", "content": "Fig 4 illustrates the two baseline inference SW and Random Foreground (RF)\u2014which are compared with our NMSW-Net.\n1) Sliding Window: SW is the most commonly used infer-ence technique, relying solely on a local patch-based model. It produces the final volume prediction by scanning the volume at a regular interval.\n2) Random Foreground: RF closely resembles NMSW, with two key distinctions: 1) the sampling strategy, and 2) the method of aggregation. Similar to NMSW, RF samples patches containing objects of interest. However, it does not rank their importance; all patches are equally likely to be selected if they contain objects of interest. Furthermore, the class weights $c\\_w_e$ in the Aggregation block are set to a high value, reflecting the common assumption that local predictions generally outperform global predictions."}, {"title": "B. Backbones", "content": "For the local network, we evaluate three popular backbones for medical image segmentation: UNet [37], MedNext [38], and Swin-UNETR [12], while using UNet as the global network for computational efficiency.\n1) UNet: We employ down- and up-sampling blocks with channel sizes [32, 64, 128, 256, 320] and a bottleneck block of 320 channels.\n2) MedNext: MedNext uses the convolutional blocks as proposed in ConvNext [39]. ConvNext employs large convolu-tional kernels and grouped convolutions to optimize memory usage. We use the medium MedNext configuration, featuring down- and up-sampling blocks with channel sizes [32, 64, 128, 256, 512] and a bottleneck of 512 channels.\n3) Swin-UNETR: Swin-UNETR fuses Swin Transformer [40] and standard convolutional blocks. The Swin Transformer has an embedding size of 24 with a kernel size of 7. It consists of 8 Swin Transformer blocks, with the number of attention heads progressively increasing from 3 to 24.\nPlease see our code for further details of the backbone architectures."}, {"title": "C. Datasets", "content": "Each combination of the aforementioned segmentation backbones and inference techniques is trained on two multi-organ segmentation datasets: WORD [1] and TotalSegmentator [10]. We did not evaluate NMSW on datasets with limited annotated classes, such as tumor or single-organ prediction tasks, because, while NMSW is applicable to these scenarios, the problem would likely simplify our top-K patch sampling module to a boring single-patch sampling module.\n1) WORD: WORD consists of 150 CT scans, each anno-tated with 16 organs. Each scan contains 159\u2013330 slices with a resolution of 512 x 512 pixels and an in-plane spacing of 0.976 x 0.976 mm. We preprocess the data to normalize the spacing to 1 \u00d7 1 \u00d7 3 mm, resulting in a shape at the 99.5% percentile of 512 \u00d7 512 \u00d7 336.\n2) TotalSegmentator: TotalSegmentator contains 1,204 \u0421\u0422 scans, annotated with 104 organs. From the full set of labels, we select two subsets: Organ and Vertebrae. TotalSegmentator exhibits considerable shape variability, with dimensions rang-ing from 47 \u00d7 48 \u00d7 29 to 499 \u00d7 430 \u00d7 852. We normalize the spacing to 1.5 \u00d7 1.5 \u00d7 1.5 mm, resulting in a shape at the 99.5% percentile of 373 \u00d7 333 \u00d7 644."}, {"title": "D. Training Details", "content": "For both baseline inference techniques and our proposed NMSW-Net, we fixed the overlap ratio between patches at 50% and the patch size at 128\u00d7128\u00d7128. The down-size rate of global input is set to 3 \u00d7 3 \u00d7 3 for WORD and 3 \u00d7 3 \u00d7 4 for TotalSegmentator. We trained for 300 epochs, with 300 itera-tions per epoch. During training, for the baseline approaches, the batch size was set to 4, while for NMSW, we sampled 3 top-k patches and 1 random patch per iteration, resulting in the same number of patches as baseline approaches. For the loss in Eq 7, the weights of soft-Dice and cross-entropy is set to 0.8 and 0.2, respectively. The entropy weight $\\lambda$ is set to 0.0001.\nWe employed the AdamW optimizer [41] with a learning rate of 3e-4 and weight decay of 1e-5, along with a cosine"}, {"title": "V. RESULTS", "content": ""}, {"title": "A. Trade-off between accuracy and efficiency", "content": "Table I compares NMSW-Net against two baselines\u2014SW and RF\u2014in terms of segmentation performance and compu-tational efficiency. The observed trend is consistent through-out: although NMSW-Net doubles the model size due to the inclusion of an additional global segmentation model, it achieves significantly improved computational efficiency while maintaining competitive segmentation performance when the number of sampled patches is 30 ($k$ = 30). While the RF baseline is equally efficient, its patch sampling does not necessarily target regions where the global prediction is most deficient, resulting in a significantly smaller accuracy boost from the sampled patches.\nIn terms of floating-point operations per second (FLOPs), which correlate with the model's overall energy consumption, NMSW-Net uses approximately 90% fewer FLOPs compared to SW. Additionally, NMSW-Net is, on average, about 4x faster in both CPU (Intel Xeon Gold) and GPU (H100) environments. Notably, the speed improvement becomes more pronounced as the complexity of the backbone model in-creases. This is because, unlike SW, NMSW-Net incorporates additional computations from the global segmentation model, but their impact diminishes as the local backbone's complexity increases.\nIt is worth mentioning that NMSW-Net outperforms SW in the TotalSegmentatorOrgan task in terms of overall accuracy.\nWe hypothesize that the learnable patch sampling module in NMSW-Net not only improves computational efficiency but also enhances accuracy by focusing on regions where the model underperforms during training. Thus, our Differ-entiable Top-K module can be viewed as a special kind of active learning algorithm that samples data points where the model performs poorly. In our case, the data points are patches. Further investigation of this hypothesis is left for future research."}, {"title": "B. Evaluation of the Learned Class Weight", "content": "The class weight $c\\_w_e$ adjusts the importance of predic-tions in regions where global and patch predictions overlap. As shown in Fig. 7, $\\sigma(c\\_w_e)$ exhibits a consistent trend across various backbones trained on the WORD task. Mod-els generally assign higher weights (above 0.5) to patch predictions, particularly for small or complex organs (e.g., pancreas, duodenum, colon, intestine), while slightly lower weights are given to more isolated or less complex structures (e.g., femur, rectum). Interestingly, stronger backbones like MedNextMedium emphasize local predictions more heavily, effectively ignoring global predictions when the local model is powerful."}, {"title": "C. Evaluation of sampled Top-K patches", "content": "Fig. 5 illustrates the evolution of the patch sampling dis-tribution, $p(z | X_{low})$, during training. Initially, the distribution is nearly random, with sampled regions (highlighted as red rectangles) scattered indiscriminately, failing to focus on fore-ground areas. Midway through training, the distribution begins to concentrate on foreground regions, but the sampled patches exhibit significant overlap, which is suboptimal for improving accuracy. By the end of training, the distribution not only targets foreground regions but are more well spread with less overlap. This improvement is driven by the entropy term in the loss function, which encourages exploration of diverse regions and reduces overlap among sampled patches.\nFig. 6 compares the Dice score improvement achieved by our proposed Top-K sampling block and the RF sampling strategy when the global prediction is supplemented with the top-5 patches. While both RF and NMSW-Net enhance per-formance, NMSW-Net delivers a greater improvement across all organs. This indicates that the learned distribution is dynamic, adapting to compensate for organs where the global model underperforms, rather than focusing on a specific organ. Consequently, our sampling module is more than a simple foreground sampler with minimal overlap; it intelligently tar-gets areas requiring enhancement."}, {"title": "VI. CONCLUSION & DISCUSSION & FUTURE WORKS", "content": "Segmentation models, like those in other fields, have be-come increasingly slower, larger, and more computationally expensive. While prior efforts to improve efficiency have pri-marily focused on simplifying backbone architectures, NMSW takes a novel approach by replacing the time-consuming sliding-window inference in 3D segmentation tasks with dy-namic patch sampling. NMSW is model-agnostic and can be seamlessly integrated into existing 3D segmentation architec-tures with minimal computational overhead.\nIn evaluations across various tasks and segmentation back-bones, NMSW demonstrates substantial computational sav-ings\u2014achieving up to 90% lower FLOPs and 4\u00d7 faster infer-ence on GPUs (or 5 \u00d7 faster on CPUs)\u2014while maintaining, and occasionally surpassing, the performance of costly SW infernce.\nWhile these results are promising, a few challenges remain to be addressed before the community can completely move away from SW:\nTraining Speed: Although NMSW accelerates inference, its training is slower due to the sequential dependency between global and local computations. The local net-work remains idle until it receives patches sampled by the global network, limiting parallelization. Future research could explore strategies to improve training efficiency by reducing this bottleneck.\nTask Expansion: We tested NMSW exclusively on in-stance segmentation tasks. Extending its applicability to foundation vision-language models, expending the appli-cability to open-vocaburary segemntation tasks.\nTop-K Sampling: The current Top-K sampling module selects patches without replacement. However, this ap-proach is sub-optimal if the object of interest is small enough to fit within a single patch. The remaining $k$ - 1 patches are redundant background patches. Future work could explore relaxing this restriction by enabling sam-pling with replacement.\nIn conclusion, NMSW-Net presents a novel approach to achieving a more compute-efficient 3D segmentation model through attention-driven patch sampling. This method stands apart from conventional approaches, which typically rely on architectural modifications to segmentation backbones that are often task-specific and non-scalable. We hope that NMSW-Net"}]}