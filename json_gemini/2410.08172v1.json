{"title": "ON THE EVALUATION OF GENERATIVE ROBOTIC SIMULATIONS", "authors": ["Feng Chen", "Botian Xu", "Pu Hua", "Peiqi Duan", "Yanchao Yang", "Yi Ma", "Huazhe Xu"], "abstract": "Due to the difficulty of acquiring extensive real-world data, robot simulation has become crucial for parallel training and sim-to-real transfer, highlighting the importance of scalable simulated robotic tasks. Foundation models have demonstrated impressive capacities in autonomously generating feasible robotic tasks. However, this new paradigm underscores the challenge of adequately evaluating these autonomously generated tasks. To address this, we propose a comprehensive evaluation framework tailored to generative simulations. Our framework segments evaluation into three core aspects: quality, diversity, and generalization. For single-task quality, we evaluate the realism of the generated task and the completeness of the generated trajectories using large language models and vision-language models. In terms of diversity, we measure both task and data diversity through text similarity of task descriptions and world model loss trained on collected task trajectories. For task-level generalization, we assess the zero-shot generalization ability on unseen tasks of a policy trained with multiple generated tasks. Experiments conducted on three representative task generation pipelines demonstrate that the results from our framework are highly consistent with human evaluations, confirming the feasibility and validity of our approach. The findings reveal that while metrics of quality and diversity can be achieved through certain methods, no single approach excels across all metrics, suggesting a need for greater focus on balancing these different metrics. Additionally, our analysis further highlights the common challenge of low generalization capability faced by current works.", "sections": [{"title": "1 INTRODUCTION", "content": "Embodied artificial intelligence (EAI) is crucial to enable intelligent agents to understand and interact with the physical world. However, creating such agents with physical forms and universal functionalities necessitates extensive data, which is prohibitively expensive to acquire manually (Dasari et al., 2020; Srivastava et al., 2021; Mu et al., 2021). Although multiple attempts have been made toward massive real-world data collection (Brohan et al., 2023b;a), training in simulated environments still plays a key role in various robotic tasks (Wang et al., 2023a; Huang et al., 2021; Lin et al., 2021; Yuan et al., 2024; Yu et al., 2020). Consequently, the acquisition of a substantial number of robotic tasks in simulation, which heavily rely on foundation models, is of significant importance.\nFoundation models (OpenAI, 2023; Team et al., 2023; Zhang et al., 2023a) have exhibited remarkable proficiency in various robotics-related tasks, including coding (Rozi\u00e8re et al., 2023), 3D generation(Deitke et al., 2022; 2023), scene comprehension (Mohiuddin et al., 2024), planning (Huang et al., 2023b; 2024), and reward formulation (Ma et al., 2023). Notably, recent works have demonstrated the potential of leveraging such capabilities of foundation models to generate robotic tasks in simulation (Wang et al., 2023b; 2024; Katara et al., 2023; Yang et al., 2024; Hua et al., 2024). In generative simulation, foundation models such as large language models and vision-language models"}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 FOUNDATION MODELS", "content": "In our article, we utilize large language models such as GPT-4 (OpenAI, 2023), released by OpenAI, which have had a profound impact on the field of natural language processing. Previous work has applied these large language models to the domain of robotics, specifically in policy learning (Driess et al., 2023; Huang et al., 2023c) and motion planning (Huang et al., 2022). Researchers have also explored using language models to generate code and rewards (Huang et al., 2023c; Wang et al., 2023b), aiding solvers in learning policies from tasks. Furthermore, vision-language models and multimodal foundational models have demonstrated remarkable potential (Zhang et al., 2023a; Team et al., 2023; Xu et al., 2023). Model GPT-4-vision (Zhang et al., 2023a) have exhibited capabilities in spatial understanding and basic assessment, making the automatic evaluation of tasks feasible. In prior work, vision-language models have been employed in the robotic task generation pipeline to verify the quality of the generated tasks (Wang et al., 2023b; 2024)."}, {"title": "2.2 GENERATIVE ROBOTICS TASKS AND DATASETS IN EMBODIED AI", "content": "In recent research, foundational models have demonstrated remarkable capabilities (OpenAI, 2023; Zhang et al., 2023a; Team et al., 2023), leading to the emergence of autonomously generated robotic tasks in the field of robotics. Typically, such generative models utilize large language models to create a basic framework for generating tasks, which involves submitting the required 3D models through text-to-3D model (Li et al., 2023b) conversion, text-to-image (Mid) and image-to-3D models (Liu et al., 2023) processes, or searching and generating the necessary three-dimensional models in extensive 3D model repositories like Objaverse (Deitke et al., 2022; 2023). These models are then assembled into tasks within simulators, and methods such as reinforcement learning or trajectory optimization are employed to learn the trajectories needed to solve the tasks. Researchers have also explored tasks in other directions; for instance, the creators of Robogen have expanded task types to include soft materials and humanoid robots (Wang et al., 2023b), while the developers of Gensim have opted to deploy tasks on real robots, completing the generated tasks in the real world (Wang et al., 2024). Therefore, when evaluating the quality of generated tasks, it is also necessary to consider the diverse directions of exploration being pursued by different researchers."}, {"title": "2.3 BENCHMARKS ON MESHES AND LARGE LANGUAGE MODEL", "content": "Recent work has bridged gaps in the evaluation of three-dimensional models and large language models. For instance, T3Bench (He et al., 2023) introduced the use of multiple foundational models to establish an evaluation system for metrics such as the quality and alignment of 3D models. The methods used in this work to assess the quality and alignment of 3D models have inspired our approach to evaluating the alignment of task scenarios in robotic tasks. Additionally, in the evaluation of large language models (Zhang et al., 2023b; Huang et al., 2023a), previous studies have discussed assessing various metrics across multiple scenarios to identify potential issues of hallucination and errors within models. These evaluation standards provide a valuable perspective for assessing robotic tasks, aiding in a more appropriate evaluation of such tasks."}, {"title": "2.4 EVALUATION OF TASK DIVERSITY", "content": "Learning a range of skills is crucial for building generalist robot agents that can autonomously operate in a complex environment. Therefore, we expect task generation to produce tasks with varying goals, dynamics, and environment setups such that collectively learning these tasks promotes generalization, robustness (Tobin et al., 2017), and even unseen skills (Eysenbach et al., 2018). However, evaluating such diversity of generated tasks remain unclear. RoboGen (Wang et al., 2023b) proposed to compare the Self-BLEU score and Embedding Similarity (Zhu et al., 2018) of the descriptions generated alongside the tasks. While such language-based diversity metrics consider high-level semantic information, they are strongly coupled with the language models used, which are known to be subject to alignment issues. In this work. we propose to evaluate task diversity as the coverage of skill or dynamics space, where high diversity facilitates better transfer or generalization to a held-out set of tasks. Recent model-based skill learning methods (Hafner et al., 2023; Hansen et al., 2024) are capable of learning highly complex dynamics and task information on a wide range of tasks. We leverage them for diversity evaluation."}, {"title": "3 METHOD", "content": ""}, {"title": "3.1 INTRODUCTION TO GENERATIVE SIMULATION", "content": "Generative simulation represents a field of studies that utilize foundation models, particularly generative models pre-trained on internet-scale data, to acquire massive tasks and data in robot simulation. In generative simulation, large language models are first prompted to provide the basic framework for a novel task, such as the task description, assets, task code, etc. The task is then loaded into the simulation to construct a scene. We further query foundation models to provide objectives for task solutions, e.g. goals for planning or rewards for RL. Through RL training or motion planning, the pipeline will produce trajectory data for the previously generated task. To summarize, the performance of a generative simulation pipeline is fundamentally determined by key aspects such as the basic task framework, solution objectives, and the specific implementations of solution generation."}, {"title": "3.2 OVERVIEW", "content": "We divide our evaluation work into three parts, as visualized in Fig 2. In the first part (Sec 3.3), we assess the quality of a single generated task through foundational models, especially large language models and vision-language models, and statistical methods. In the second part (Sec 3.4), we respectively measure the diversity of generated task descriptions and trajectory data with a language model and a world model. In the third part (Sec 3.5), we evaluate the generalization capability of an imitation learning policy distilled from a large number of generated tasks."}, {"title": "3.3 SINGLE-TASK QUALITY", "content": "In this section, we introduce how we evaluate single-task quality. We consider two metrics: scene alignment score which measures the realism of the generated task and task completeness score which measures whether the generated task is successfully solved to collect data.\nScene alignment score. We utilize two different pipelines to evaluate scene alignment score. Due to possible deficiencies in visual recognition from foundation models (Tong et al., 2024b), one"}, {"title": "3.4 TASK AND DATA DIVERSITY", "content": "The generated tasks are expected to be diverse so that training on these tasks grants agents a range of skills and the ability to operate in various situations. However, a concrete definition of diversity is hard: in what sense are tasks distinct or similar? In this work, we are concerned with diversity from the following perspectives: (1) task diversity, a high-level diversity as identified by LLMs; and (2) trajectory diversity, a low-level diversity in terms of the dynamics of the collected data.\nText-based task diversity. Since LLMs generate tasks including the task descriptions and possibly scene configurations and goals, they are supposed to have an internal understanding of diversity at a high level. For example, \"stack-blocks-tower\u201d differs from \"align-balls\" semantically in terms of the action (verb) and the object of interest. Therefore, the similarity between embeddings of task descriptions can be considered as the similarity between tasks. Specifically, following (Zhu et al., 2018), we compute the diversity of a task set with text embeddings {e}=1 as:\nN\n$$div = \\frac{1}{N \\times log(\\frac{1}{N} \\sum_{i \\neq j}e^{e_i \\cdot e_j})},$$\nwhere N is the number of tasks and i \u2260 j removes self-similarity. A higher value indicates lower similarity and hence higher diversity.\nDynamics-based trajectory diversity. Though straightforward, task description diversity itself does not sufficiently characterize the actual learning experience of tasks, e.g., different interaction dynamics will take place when training on different generated tasks. Ideally, a diverse set of tasks should cover a large space of dynamics to promote the agent's robustness under different scenarios. Therefore, we propose to evaluate such diversity through prediction error of dynamics models. Dynamics prediction error has been associated with novelty and widely adopted to promote exploration (Pathak et al., 2017; Burda et al., 2018). A high dynamics prediction error indicates unfamiliar (and thus novel) dynamics being experienced. We leverage a latent dynamics model $p_\\theta(o_{t+1}|o_t, a_t)$ following DreamerV3 (Hafner et al., 2024), where $o_t$ and $a_t$ are the observation and action at time step t. The model is trained on trajectories collected from the generated tasks and then evaluated to compute the prediction errors. As will be discussed in Section 4.2, this approach helps us to identify tasks that render notably similar dynamics and are therefore not diverse."}, {"title": "3.5 TASK GENERALIZATION", "content": "Generalization can be an ambiguous yet vital metric for evaluating the capabilities of generalist robot agents. In this paper, we define generalization as the capability to solve tasks within the same distribution, specifically whether an agent trained on the generated tasks can address similar scenarios and objectives albeit with varying initial states and minor low-level variations. To quantitatively examine this capability, we first train an imitation learning policy with trajectories collected by either the oracle policies or policies learned from the generation pipeline. The trained policy is subsequently evaluated with new task scenarios including varied object instances, appearance, and initial poses. The policy uses the state-of-the-art algorithm called Diffusion Policy (Chi et al., 2023) as the backbone and takes as input RGB observations, and the proprioceptions. A typical indicator of low generalization is when the trained policy performs well on the training data, confirming correct algorithm implementation, yet struggles to adapt to the varied tasks during evaluation."}, {"title": "4 EXPERIMENT", "content": ""}, {"title": "4.1 SINGLE TASK EVALUATION", "content": "Experimental setup. As mentioned in Section 3.3, our methodology utilizes vision-language models (VLMs) to generate scene captions, which are then compared against task descriptions using large language models (LLMs). Additionally, we employ a multi-modal LLM (MLLM) to evaluate the completeness of task trajectories. For captioning scene images, we deploy several VLMs, including BLIP (\u201cblip2-flan-t5-xl", "Cambrian-8B\") (Tong et al., 2024a), and LLaVA 1.6 (\u201cLLaVA-1.6-7B\") (Liu et al., 2024). The scene alignment score is measured using the GPT-4 (\\\"2024-02-15-preview\\\" version from Microsoft Azure) model. For assessing task completion, the MLLM models used include GPT-4V, Cambrian, and LLaVA.\"\n    },\n    {\n      \"title\": \"4.1.1 HUMAN VERIFICATION\",\n      \"content\": \"To validate the efficacy of our method, we gather human evaluations for ten tasks from the released tasks of RoboGen and GenSim and examine the consistency of our results with human results. We characterize their relationship by using the Pearson correlation coefficient to represent correlation strength and the mean absolute error (MAE) to indicate numerical similarity. A higher Pearson correlation coefficient signifies a stronger correlation, while a lower MAE reflects greater similarity. Therefore, we calculate the ratio of the Pearson correlation coefficient to the MAE to assess the relationship between our method and human evaluations; higher values indicate greater similarity.\"\n    },\n    {\n      \"title\": \"4.1.2 EVALUATION ON ROBOGEN, GENSIM, AND BBSEA\",\n      \"content\": \"We summarize the evaluation results of both metrics for single-task quality in Figure 4. To be specific, among the methods, RoboGen tasks demonstrate high task completion scores, but their scene alignment scores are notably low. This discrepancy arises because, although RoboGen can generate assets relevant to the current task, these assets often collide when loaded into the scene, resulting in a cluttered and difficult-to-recognize environment. In contrast, GenSim secures the highest scene alignment scores but underperforms in task completion. This shortfall is largely attributed to its vision-language model lacking access to top-view rendered data, which impairs its ability to accurately recognize task completion. In addition, BBSEA achieves decent results on both metrics (although not the best), and it has the smallest variance in the outcomes.\nFurthermore, we observe performance discrepancies between published and newly generated tasks across all methods. While a predictable decline in performance for generated tasks can be attributed to additional filtering prior to project release, improvements have been noted in GenSim's scene alignment and task completion for RoboGen and BBSEA. The underlying reason is the advancements in the performance of foundation models, which have expanded the limits of task generation quality, including reasonable solution objectives in RoboGen and BBSEA, and innovative long-horizon task proposals in GenSim.\"\n    },\n    {\n      \"title\": \"4.1.3 EXAMPLES FROM EVALUATION\",\n      \"content\": \"As shown in Figure 5, in the snapshot of the 'Open Laptop' task, the laptop is correctly placed on the table, and there are some objects such as a lamp and a pen placed on the desk. Then we can observe\"\n    },\n    {\n      \"title\": \"4.2 TASK AND DATA DIVERSITY\",\n      \"content\": \"In this section, we use the proposed evaluation protocols to examine whether a pipeline generates diverse tasks and hence diverse data for learning. To have a better perspective for analysis and allow practical training, we divide the tasks into groups according to skills, scene configuration, or objects involved. The details for grouping can be found in Appendix A.3.\nTask diversity. For text-based task diversity, we leverage different language models, including \u201cMiniLM-L6-v2": "nd \u201cMpnet-base-v2\" from SentenceTransformers (Reimers & Gurevych, 2019; 2020),", "LLama-3.2-90B\" (Touvron et al., 2023; Dubey et al., 2024), to generate text embeddings from task descriptions. The diversity is then measured by embedding similarity\"\n    },\n    {\n      \"title\"": "4.3 GENERALIZATION"}, {"content": "We train a Diffusion Policy for each task group using 40 trajectories, identical to the data for dynamics model training, and assess their performance on the same task group under variations such as scene configurations, object colors, and initial robot states. As indicated in Table 3, GenSim demonstrates reasonable generalization performance, despite the challenge posed by randomizing object colors, which complicates the effectiveness of an RGB-based policy. Conversely, agents trained on RoboGen and BBSEA tasks exhibit poor generalization. For RoboGen, the primary issue is that the training trajectories all begin from the same initial state, and the RL solutions do not generate high-quality data. In the case of BBSEA, the problem often lies in the repetition of similar tasks, which restricts task-level generalization capabilities. Moreover, significant task variations can result in out-of-distribution challenges that adversely affect agent performance."}, {"title": "5 CONCLUSION AND DISCUSSION", "content": "In this paper, we propose a novel evaluation framework for generative simulation, which includes three fundamental metrics: quality, diversity and generalization. We evaluate three representative generative simulation pipelines based on our proposed method. Results indicate that while various pipelines excel in terms of quality and diversity, there remains significant potential for improvement in their generalization capabilities. We hope that future work in generative simulation can make advancements and improvements in these three areas, especially in terms of generalization.\nMoreover, we identify and outline some common drawbacks and failure cases across current generative simulation pipelines as follows for instructions to encourage further exploration:\n\u2022 Low-quality task descriptions: Although task proposal is not a bottleneck for generative simulation in general, we still observe some vague and repeated task descriptions that fail to express the details of the generated tasks. Such ambiguity may cause suboptimal results in the evaluation of text-based task diversity, as well as harm the performance of a language-conditioned policy.\n\u2022 Trajectory data with limited diversity: The task solution in some methods only considers limited task and scene variations, which will affect both the trajectory diversity and task-level generalization capability. Typical cases include insufficient intra-task randomization, relatively fixed task domain, or identical semantics between different tasks, leading to very similar trajectories. We advocate an appropriate dynamics model trained along with the task generation process to inspect and improve the diversity regarding dynamics coverage in future works.\n\u2022 Task-specific design data collection and imitation learning: We remark that, on designing the task generation pipeline, generalization could be considered and improved in various ways, e.g., action space with good abstraction (control by end-effector poses, joint positions, or primitive actions), data augmentation, and unified goal specification. For example, in GenSim, actions are abstracted as high-level waypoints, and each task trajectory contains only a few such high-level actions. This design benefits its generalization evaluation based on imitation learning. We aim to devise a generally applicable protocol, with the diffusion policy not tuned or adopted specifically, and advocate efficient domain-specific designs for better generalization performance."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 PROMPT FOR EVALUATION", "content": "Here is the prompt for the evaluation of the completion score:\nYou are an assessment expert responsible for the Task Completion rate.\nYour task is to score the completion rate for the task in the following rules:\n1. Evaluate the completion rate for the robotics task.\n2. During the evaluation, you will receive 8 images and a basic description of the task.\n3. During the evaluation, you need to make very careful judgments and evaluate the completion of the task based on the order of the pictures and the task description.\n4. In the evaluation you need to pay attention to the smoothness of the trajectory.\n5. Assign a score between 0 and 10, with 10 being the highest. Do not provide a complete answer.\n6. Your should provide the answer in the following format:\nScore: X\nHere is the prompt for the evaluation of the scene alignment score without the caption model:\nYou are an assessment expert responsible for Task description and Scene images pairs. Your task is to score the Scene caption according to the following requirements:\n1. Evaluate how well the Scene images covers the scene of the robotics task. You should consider whether the scene is similar to the requirement of the task.\n2. During the evaluation, you will receive 4 images and a basic description of the task.\n3. In the evaluation, you should pay attention to the alignment between the Scene image and the real-world task following the description of the task.\n4. A good scene should not only provide an environment for completing a robotics task but should also contain items that may appear near the task, even though they may have nothing to do with the task itself.\n5. Assign a score between 1 and 5, with 5 being the highest.\n6. Your should provide the answer in the following format:\nScore: X\nHere is the prompt for the evaluation of the scene alignment score with the caption model:\nYou are an assessment expert responsible for Task description and Scene captions pairs. Your task is to score the Scene caption according to the following requirements:\n1. Evaluate how well the Scene captions covers the scene of the robotics task. You should consider whether the scene is similar to the requirement of the task.\n2. In the evaluation, you should pay attention to the alignment between the Scene captions and the real-world task following the description of the task.\n3. A good scene should not only provide an environment for completing a robotics task but should also contain items that may appear"}, {"title": "A.2 HUMAN EVALUATION RESULT", "content": "In table 4 and table 5, we collected the scoring results from 18 researchers in the field of robotics on 20 tasks, which were sourced from RoboGen and GenSim, respectively. We conducted five evaluations for each model, with the values in parentheses representing the variance of the five evaluations. Based on these two tables, we derived the results presented in Figure 3."}, {"title": "A.3 DIVERSITY AND GENERALIZATION EXPERIMENT DETAILS", "content": ""}, {"title": "A.3.1 TASK SELECTION AND GROUPING", "content": "For GenSim, we use all the templates and generated tasks released by the authors. For RoboGen, we only use the manipulation tasks but not locomotion and soft body because the locomotion tasks yield very poor learning performance and the soft-body tasks are not publicly available at the time. For BBSEA, we perform generation following the instructions provided by the authors.\nWe group these tasks mainly for two reasons: (1) grouped tasks offer more perspectives for analysis, and (2) the latent dynamics model and diffusion policy training, with their original implementation, are insufficient for learning a large number of tasks. The dimensions to consider include scene"}, {"title": "A.3.2 TRAJECTORY COLLECTION", "content": "Trajectories are collected for both dynamics model learning and imitation learning. GenSim implements an oracle agent for generating demonstrations. The oracle agent's action specifies the target end-effector pose command, which is executed by a low-level Inverse Kinematics controller with"}, {"title": "A.3.3 DYNAMICS MODEL TRAINING DETAILS", "content": "We adopt a popular community implementation (Hafner et al., 2024). For all experiments, the model is trained for 10 epochs with a batch size of 8. The data was chunked into sequences of size 40/40/20 for GenSim/RoboGen/BBSEA. All other hyperparameters are kept as default. Since DreamerV3 is designed for reinforcement learning from visual observations, its model architecture is expressive and robust to different domains. Data augmentation could be used to improve its robustness to aspects such as the variation in color and appearance further to obtain a lower prediction error. However, we do not incorporate that in this paper for simplicity."}, {"title": "A.3.4 IMITATION LEARNING MODEL TRAINING DETAILS", "content": "For GenSim and RoboGen, the implementation is adapted from the official release of Diffusion Policy (Chi et al., 2023). We use the configuration provided by the authors of Diffusion Policy for image-state observation. For all experiments, the policy is trained for 8000 epochs. For BBSEA, we use the image-language diffusion policy implementation provided by the authors."}]}