{"title": "Learning states enhanced knowledge tracing: Simulating the diversity in real-world learning process", "authors": ["Shanshan Wang", "Xueying Zhang", "Xun Yang", "Xingyi Zhang", "Keyang Wang"], "abstract": "The Knowledge Tracing (KT) task focuses on predicting a learner's future performance based on the historical interactions. The knowledge state plays a key role in learning process. However, considering that the knowledge state is influenced by various learning factors in the interaction process, such as the exercises similarities, responses reliability and the learner's learning state. Previous models still face two major limitations. First, due to the exercises differences caused by various complex reasons and the unreliability of responses caused by guessing behavior, it is hard to locate the historical interaction which is most relevant to the current answered exercise. Second, the learning state is also a key factor to influence the knowledge state, which is always ignored by previous methods. To address these issues, we propose a new method named Learning State Enhanced Knowledge Tracing (LSKT). Firstly, to simulate the potential differences in interactions, inspired by Item Response Theory (IRT) paradigm, we designed three different embedding methods ranging from coarse-grained to fine-grained views and conduct comparative analysis on them. Secondly, we design a learning state extraction module to capture the changing learning state during the learning process of the learner. In turn, with the help of the extracted learning state, a more detailed knowledge state could be captured. Experimental results on four real-world datasets show that our LSKT method outperforms the current state-of-the-art methods.", "sections": [{"title": "1. Introduction", "content": "Knowledge Tracing (KT) is a challenging task as the real learning process of humans involves numerous complex learning behaviors and is influenced by various factors, including the learning state during answering, the difficulty of exercises, and tendencies towards guessing (Papamitsiou et al., 2020) and so on. The key to KT task lies in comprehensively simulating these complex factors and effectively modeling them as real-world learning process.\nIn recent years, the Transformer has shown great potential in the field of KT (Wang et al., 2023, 2024). Many Deep Learning-based Knowledge Tracing (DLKT) models, such as Attention-based DLKT (ATT-DLKT) models, adopts the Transformer to capture the inherent relationships between learners' historical interactions, to accurately estimate their knowledge states. However, these models often have some limitations. Firstly, they usually rely too much on learners' historical performance on similar exercises to assess their knowledge states (Ghosh et al., 2020; Pandey & Karypis, 2019; Yin et al., 2023). Moreover, to alleviate data sparsity problem, many models choose the Knowledge Concepts (KCs) instead of exercises for model training, thus the rich association information between exercises and interactions could be lost. This inevitably increases the difficulty for ATT-DLKT models to accurately identify the key historical moments and may introduce noise into the model's training. Additionally, due to subjectivity, the unreliable responses caused by learner guessing factors could also inevitably bring in some noise in the interaction. Secondly, the learner's changing learning state is another important factor in the learning process which is always ignored by previous methods. This states is related with the learner's recent performance and can complement the knowledge state, together influencing the learner's next performance. To address these issues, we propose a new ATT-DLKT method named Learning State Enhanced Knowledge Tracing (LSKT). This approach, on one hand, further mines the potential interaction information from exercises and responses to obtain the more precise feature embedding. On the other hand, incorporates the changes in learners' answering process into the capturing of knowledge states. Thereby the performance of our model could be improved.\nTo better illustrate the above points, we provide a simple example in Figure 1. Figure 1(a) depicts a scenario where a learner answering six consecutive exercises, revealing two main factors influencing learner performance: knowledge state and learning state. Although exercises \\( e_1 \\) and \\( e_2 \\) involve the same knowledge concepts, the various factors including differences difficulty in exercise, discriminability in exercises and the learner's learning state all could affect the performance. From the traditional perspective of knowledge state, exercise \\( e_2 \\), which shares more similarities in influencing factors with \\( e_6 \\), would provide more valuable predictions for \\( e_6 \\); indicating that \\( e_6 \\) is more likely to be predicted as correct. However, even if \\( e_2 \\) is answered correctly, there still could be possibilities of lucky guesses. Therefore, only relying the similarity may the prediction on \\( e_6 \\) be wrong. Furthermore, the learning state is also a crucial factor which could influence the learner performance. From the perspective of learning state evolution, poor performance on exercises \\( e_1 \\) - \\( e_5 \\) may lead to a decline in the learner's learning state, which could affect their performance on \\( e_6 \\). In fact, the real answer in \\( e_6 \\) is incorrectly, which implies that during model training, we need to fully consider the knowledge states and learning states to make the model closer to the real-world answering process. Figure 1(b) shows a real slice from the ASSIST12 dataset, detailing the information of \\( e_1 \\) - \\( e_6 \\) as depicted in the left panel. From this slice, it's evident that even with different adjacent interaction knowledge concepts, a learner's recent performance could influence their future performance. This observation is also supported by data analysis in literature (Cui et al., 2023). However, past studies often overlooked the complex factors during the answering process. This prompts us to consider how to effectively model these factors to fully utilize the model's potential, capturing more fine-grained changes in knowledge states, and thus more realistically reflecting the learner's answering process.\nTo address the issues mentioned above, we proposed LSKT based on ATT-DLKT model. There are three main contributions in this method. Firstly, three feature embedding methods are designed from coarse-grained to fine-grained inspired by IRT paradigm. By combining the potential differences in interactions, the model could not only mitigate the overfitting problem, but also capture the refined difference between interactions. Additionally, we also explored the impact of different embedding paradigm on the performance of the model. Secondly, to extract the changing learning state during the learning process, dues to the ability of natural capturing capability of the interaction information within each moment, the causal convolution layers with different receptive field sizes are leveraged. In this way, the short-term changes and possible patterns in the learning process can be captured, which were overlooked by previous ATT-DLKT models. Thirdly, considering that the learning state is a key fact to influence the knowledge states, we aim to incorporate the learning state into the process of extracting knowledge states, and then a learning state-enhanced knowledge state extraction module is designed.\nIn summary, our LSKT method could integrate global contextual information, capture long-range dependencies, and introduce sparse attention to the learning state. In this way, the detailed knowledge states and learning states are captured jointly without introducing additional noise and the key contributions are as follows:\nThe key contributions can be summarized as follows:\n(1) We reveal a potential issue in ATT-DLKT model, which relies heavily on learners' past performances of similar exercises to assess their knowledge state, while failing to adequately consider the dynamic changes in learning state. By integrating the consideration of both states, the model's predictive accuracy and applicability can be significantly improved, making it more consistent with actual learning environments.\n(2) Inspired by IRT, three different personalized level on interaction embedding is designed and compared, simulating realistic differences between interactions. This design not only alleviates the issue of model overfitting caused by data sparsity but also enhances the model's performance and interpretability.\n(3) We proposed a novel learning state extraction module which always ignored by previous methods. This module could capture the learner's learning patterns over multiple time scales during the answering process, so it can effectively represent the learner's learning state at history answer time.\n(4) To capture the precise knowledge state, a learning state-enhanced knowledge state extraction module has been proposed. This module achieves more fine-grained knowledge state by paying sparse attention to the learning states in the process of tracing knowledge states.\nThe subsequent sections of this article are organized as follows: The second part reviews and analyzes related work. The third part defines KT problems and introduces the specific approach of our model (LSKT). The fourth part reports the experimental results on four public datasets and discusses and summarizes our method."}, {"title": "2. Related work", "content": "Probabilistic models and logical models were two categories of early knowledge tracing models. Probabilistic models used Markov processes (HMM) (Ghahramani, 2001). The Bayesian Knowledge Tracing (BKT) method was a classic example of this (Corbett & Anderson, 1994). This method was proposed by Corbett and Anderson. In this method, the mastery state of knowledge points was modeled as a binary variable (mastered/not mastered), and the learning process was modeled as discrete transitions from the not mastered state to the mastered state. With further research on BKT, subsequent studies incorporated more influencing factors, such as temporal differences in data (Zhu et al., 2018), hierarchical relationships between knowledge points (K\u00e4ser et al., 2017), exercise difficulty (Pardos & Heffernan, 2011), etc. logical models were based on logical functions and used continuous distributions instead of discrete probabilities to represent learners' knowledge states, thus better capturing learning intensity. The basic principle involved calculating the probability of correctly answering exercises based on learner learning ability parameters and exercise parameters (such as difficulty and discrimination). Performance Factors Analysis (PFA) (Pavlik Jr et al., 2009) and Learning Factors Analysis (LFA) (Cen et al., 2006) were two classic logical models for knowledge tracing. While these traditional knowledge tracing models possess strong interpretability, they often exhibit certain limitations and biases as they rely on domain knowledge annotated by experts as input features. Such models tend to be somewhat one-sided and constrained, making it challenging to fully uncover the hidden information in the data. Consequently, the predictive performance of early models is typically subpar.\nDeep learning is getting a lot of attention from researchers because of its excellent ability to extract features. This has led to a new field called Deep Learning Knowledge Tracing (DLKT). Currently, DLKT models mainly adopt three network architectures: Recurrent Neural Networks (RNNs) (Shen et al., 2021), Memory Networks (Abdelrahman & Wang, 2019), and Attention Networks (Choi et al., 2020). Deep Knowledge Tracing (DKT) (Piech et al., 2015a) was among the first models to base knowledge tracing on deep learning, with one common strategy involving the use of Recurrent Neural Networks. In the DKT model, the hidden units of the RNN were utilized to represent the learner's knowledge state, which was updated as the learner's answering behavior evolved. Another approach was the Dynamic Key-Value Memory Networks (DKVMN) (Zhang et al., 2017), which employed memory networks to depict the learner's knowledge state. This involved using static \"key\" matrices to represent fixed knowledge concepts and dynamic \"value\" matrices to reflect the progression of knowledge states. However, the efficacy of DKT and DKVMN was found to be inadequate when dealing with sparse historical data and limited interactions between learners and exercises. To counter this challenge, Pandey et al. introduced a knowledge tracing framework based on self-attention mechanisms, termed Self-Attentive Knowledge Tracing (SAKT) (Pandey & Karypis, 2019). This method was capable of recognizing the state of specific knowledge based on learners' past interactions with exercises and making predictions accordingly."}, {"title": "2.2. Transformer-based knowledge tracing", "content": "In recent years, the outstanding performance of Transformer models in fields such as NLP and multimodal information processing (Wang et al., 2021b; Alaparthi & Mishra, 2020; Qian et al., 2023) has sparked researchers' interest in their application to the field of KT. Early studies, such as (Pandey & Karypis, 2019), primarily employed the self-attention mechanism to capture learners' historical learning in order to infer their knowledge state. However, due to significant differences between KT datasets and natural language data, these models did not surpass the performance of traditional DLKT models, such as DKT and DKVMN. Recent research has started to tackle this issue, with models like AKT (Ghosh et al., 2020), DTransformer (Yin et al., 2023), and FKT (Huang et al., 2024). AKT improved model performance by incorporating a monotonic attention mechanism to model learners' forgetting behavior and using embeddings from the Rasch model to capture differences among problems. DTransformer introduced contrastive learning to maintain the stability of knowledge states, alleviating the information bias issues observed in earlier studies. FKT adopted an encoder-decoder-predictor framework and integrated speed prediction as an additional task, enabling fine-grained knowledge tracing. These latest studies have focused on the characteristics of learner-item interaction data, enhancing Transformer-based KT methods, not only improving model performance but also advancing the application of Transformer models in the KT domain.\nHowever, despite efforts by studies like AKT and FGKT (Mao et al., 2023) to consider the actual differences between exercises, limitations persist. For instance, AKT had only distinguished exercises with the same concept by simulating exercise difficulty, but considering only exercise difficulty information could not fully simulate the subtle differences between exercises, thereby limiting the model's development potential. FGKT uses three traditional fusion functions to model the differences between exercises and concepts, but its lack of theoretical basis makes the model's interpretability poor. To address these issues, our LSKT designs three different granularity differential exercise and interaction embeddings based on item response theory as inputs to the model, and conducted experimental comparisons, thereby improving the performance and interpretability of the model. Additionally, these models had often overlooked an important factor in the learning process, namely, the changes in learners' learning states. Our LSKT introduces consideration of learning states into the process of tracing learners' knowledge states, and combines the two states to jointly predict learners' future exercise performance."}, {"title": "3. METHODOLOGY", "content": "This section will introduce our LSKT model. Firstly, the KT task will be clearly defined. Then, we'll provide an overview of LSKT's architecture and detail the three different feature embedding methods we've designed. Next, the learning state extraction module will be introduced and the knowledge state extraction module with enhanced learning states will be discussed in depth. Finally, the final prediction results will be generated through a prediction layer."}, {"title": "3.1. Problem definition", "content": "In this section, we present the definition of the KT task and summarize the main parameters' symbols utilized throughout the paper in Table 1. In practical learning scenarios, learners typically answer exercises in the recommended order of the educational system. This interaction process can be represented as \\( K = \\{k_1, k_2, ..., k_n\\} \\), where \\( n \\in N^+ \\) represents the number of exercises. For a specific learner, the interaction with exercises can be represented as a triplet \\( k_t = (e_t, c_t, r_t) \\), where \\( e_t \\in N^+ \\) and \\( c_t \\in N^+ \\) denote the exercise index and concept index at time t, respectively, and \\( r_t \\in \\{0, 1\\} \\) represents the response at time t (0 for incorrect and 1 for correct). Since we primarily focus on predictions for individual learners, for readability, we omit the learner index. Thus, the answering process of each learner can be represented as the following sequence:\n\\[ K = \\{(e_1, c_1, r_1), ..., (e_t, c_t, r_t) \\} \\]\n(1)\nThe goals of our LSKT can be primarily divided into two parts. One is to obtain the more accurate knowledge state. Specifically, we propose the learning state \\( \\{\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_t\\} \\) up to time t based on the interaction sequence K of the learner. By fusing with the original knowledge state \\( \\{h_1, h_2, ..., h_t\\} \\), the fused state \\( \\{z_1, z_2, ..., z_t \\} \\) is obtained. The other is to predict the learner's performance on the task at time t + 1, denoted as \\( f_{t+1} \\). Our LSKT utilize the fused state \\( z_t \\) above and exercise \\( e_t \\) are leveraged to participate the downstream task."}, {"title": "3.2. Model overview", "content": "The diagram in Figure 2 illustrates the backbone network of our LSKT along with its components. Our LSKT model consists of four parts: the IRT based feature embedding module, the Learning State Extraction (LSE) module, the learning state enhanced knowledge state extraction module, and the Learner Response Prediction module. Firstly, through the feature embedding module, embedded representations of learners' exercise and interaction features are obtained. Then, the LSE module is utilized to extract the sequence of learning state changes from the embedded interaction sequences of learners. Subsequently, we employ sparse attention calculation based on the k-means clustering method for each moment in the sequence of learning state changes. This allows us to obtain sparse attention scores by masking irrelevant moments of learning state and integrating them into the process of capturing knowledge states, thereby emphasizing similar moments of learning state. Finally, the learning state and knowledge state are fused together to jointly predict learners' performance in answering exercises in the next moment. Unlike the previous ATT-DLKT method, our model simulates the potential differences in interactions and pays attention to the changes in the learner's learning state during the answering process. By introducing consideration of these two key points into the model, it more finely simulates the process of knowledge acquisition in the real world, thereby enhancing the model's effectiveness and interpretability."}, {"title": "3.3. IRT based feature embedding module", "content": "In real world educational environment, the number of questions in question banks often far exceeds the number of learners, leading to many questions being answered by only a few learners, resulting in data sparsity issues. To address this problem, many models utilize knowledge concepts to index questions, thus avoiding overfitting. However, these methods often overlook the potential differences embedded in learner interactions, which include the potential distinctions between different exercises under the same knowledge concept, as well as the unreliability of responses caused by learners' guessing behavior. This oversight undoubtedly limits the potential of knowledge tracing (KT) methods.\nTo address the above issues, methods for the sequence of exercises and interactions with learners are delved into. We believe that the three-parameter model of Item Response Theory (IRT) can progressively uncover the impact of differences between exercises and interactions on learner performance. Inspired by this, exercise embeddings and interaction embeddings corresponding to these three different levels of refinement in parameter modeling are designed. Our design effectively alleviates the problem of model overfitting and gradually explores the subtle differences between different exercises and interactions under the same concept, thereby fully unleashing the potential of the model. The effects of the three modeling methods on model training in RQ4 will be further compared.\nThe LSKT-1PL model corresponds to the IRT one-parameter model, which introduces a difficulty difference parameter among exercises to achieve coarse-grained modeling of exercise characteristics and interaction features. Specifically, the modeling of exercise characteristics is as follows:\n\\[ x_t = [c_e || c_c] W_1 \\]\n(2)\nwhere \\( x_t \\in R^D \\) represents the feature vector of the exercise at time t. \\( c_e, c_c \\in R^D \\) denote the D-dimensional continuous vectors obtained for the knowledge concept \\( c_t \\) through a Knowledge concept feature extractor, and the corresponding variation obtained through another Knowledge concept feature extractor, respectively. \\( a_{e_t} \\in R \\) is a learnable scalar parameter representing the difficulty of the exercise \\( e_t \\), and we use \\( a_{e_t} \\) to simulate the difficulty differences among different exercises. The symbol || denotes the feature concatenation operation, and \\( W_1 \\in [R^{2D \\times D} \\) is a learnable parameter matrix. For simplicity of the formula expression, we omit the bias parameter required for the dimensionality reduction operation.\nSimilarly, the coarse-grained modeling of learner-exercise interaction features is as follows:\n\\[ g(c,r)_t = c_t + r_t \\]\n(3)\n\\[ g(c,r)'_t = c'_t + r'_t \\]\n(4)\n\\[ y_t = [g(c,r)_t || g(c,r)'_t] W_2 \\]\n(5)\nwhere \\( r_t, r'_t \\in R^D \\) represent the D-dimensional continuous vectors obtained from the learner's response \\( r_t \\) through two response different feature extractors, with \\( r_t \\) denoting the original embedding and \\( r'_t \\) denoting the corresponding change. \\( g(c,r)_t, g(c,r)'_t \\in R^D \\) represent the original interaction embedding between context c, and response \\( r_t \\), and their corresponding changes, respectively. \\( W_2 \\in [R^{2D \\times D} \\) is a learnable parameter matrix. \\( y_t \\in R^D \\) represents the interaction feature between the learner and the exercise at time step t.\nThe LSKT-2PL corresponds to the IRT two-parameter model, which introduces a discrimination parameter between exercises on the basis of LSKT-1PL, achieving a sub-fine-grained modeling of the differences between exercises and between interactions.\n\\[ x_t = c_e + [Repeat(a_{e_t}, D) || (W_3 \\cdot d_{e_t})] W_4 \\]\n(6)\n\\[ y_t = g(c,r)_t + [Repeat(a_{e_t}, D) || (W_3 \\cdot d_{e_t})] W_5 g(c,r)'_t \\]\n(7)\nwhere \\( Repeat(., D) \\) represents the repetition of the difficulty scalar parameter to obtain a D-dimensional vector. \\( d_{e_t} \\in R^D \\) denotes a D-dimensional mapping of exercises in the latent space. \\( W_3 \\in R^{D \\times D} \\) is a learnable parameter matrix. We use \\( W_3 \\cdot d_{e_t} \\) to represent a finer differentiation between exercises, namely the distinctiveness parameter of exercises. Building upon LSKT-1PL, LSKT-2PL integrates the effects of two parameters, namely the difficulty and distinctiveness, on the embedding of exercises at time t and the interaction embedding. \\( W_4, W_5 \\in [R^{2D \\times D} \\) represent two learnable dimension reduction parameter matrices.\nThe LSKT-3PL corresponds to the IRT three-parameter model. It builds upon LSKT-2PL by introducing the possibility of learners guessing, which is often overlooked when modeling the answering process. In reality, the impact of answering a exercise truthfully versus guessing should differ in terms of the learner's knowledge state. To simulate learners' guessing behavior, we incorporated random guessing perturbations into the learner interaction sequences, achieving a fine-grained model embedding:\n\\[ f_{c_{t+1}} = c_{c_{t+1}} + Random\\{0, Random\\{0,1\\}\\} \\]\n(8)\n\\[ y_t = f_{c_{t+1}} + g(c,r)_t + [Repeat(a_{e_t}, D) || (W_3 \\cdot d_{e_t})] W_6 g(c,r)'_t \\]\n(9)\nwhere \\( Random\\{0, Random\\{0,1\\}\\} \\) represents randomly selecting whether to introduce a guessing factor, where 0 indicates no guessing, and \\( Random\\{0,1\\} \\in R^D \\) represents randomly introducing a D-dimensional learner response embedding vector. Here, a 0-valued embedding vector represents an incorrect guess, while a 1-valued embedding vector represents a correct guess. In the entire formula, \\( f_{c_{t+1}} \\) denotes whether to introduce the guessing factor for the next exercise, indicating the possibility of guessing or not guessing, and the possibility of guessing correctly or incorrectly. We choose to model the guessing factor for the next exercise because the goal of Knowledge Tracing (KT) is to predict the learner's performance on the next exercise based on the current knowledge state. Therefore, the knowledge state at time t should include the guessing factor for time t+1. \\( W_6 \\in [R^{2D \\times D} \\) is a learnable parameter matrix. The learner's actual answering process can be better simulated by introducing the guessing factor in the interaction sequence \\( y_t \\). The 3PL embedding modeling does not modify the exercise feature \\( x_t \\) based on the 2PL embedding modeling."}, {"title": "3.4. Learning state extraction module (LSE)", "content": "In the actual answering process, the learner's state is changing, influenced by various complex factors. e.g., continuous wrong answers might dent their confidence, making them more prone to errors even when facing questions they have not fully mastered. A study (Cui et al., 2023) suggests that learners' recent performance significantly impacts their next steps in real test environments. However, existing ATT-DLKT models often overlook this aspect. Therefore, to capture learners' learning states, we've devised the LSE module.\nFigure 2(b) illustrates the structure of LSE. The structure of LSE consists of three residual blocks arranged sequentially to capture the learner's learning states at different scales, and finally achieves feature fusion through skip connections. The 1 \u00d7 1 convolutional layer is utilized for dimensionality reduction, yielding the comprehensive learning state of the learner. Each residual block comprises a causal convolutional layer, weight normalization layer, ReLU function, dropout layer, and skip connection. Layer normalization is applied between adjacent residual blocks.\nThe primary function of LSE is to perform causal convolution operations on the learner's historical interaction sequences. Since causal convolution strictly relies on past temporal information for prediction, for a historical interaction sequence y and a one-dimensional convolutional kernel s, the mathematical expression of the causal convolution process can be represented as:\n\\[ \\hat{y}_t = \\sum_{m=0}^{M} V_{t-m} \\cdot s_{M-m} \\]\n(10)\nHere, \\( \\hat{y}_t \\) represents the output value of the convolution at time step t, M denotes the size of the convolutional kernel, \\( Y_{t-m} \\) represents the value of the interaction sequence y at time step t-m, and \\( s_m \\) is the weight of the convolutional kernel s at position m. Then, \\( \\hat{y}_t \\) goes through three residual blocks for feature extraction, and the fused learning state feature \\( \\hat{y}_t \\), which incorporates the learner's multi-scale learning patterns, is obtained by merging the output features of each residual block.\nThrough the LSE module, the model is able to capture the learner's learning patterns over multiple time scales during the answering process. The LSE module does not aggregate useful information through the similarity between exercises, but obtains a cross-knowledge concept learning pattern through the learner's recent performance. We believe that \\( \\hat{y}_t \\) can effectively describe the learner's learning state at time t."}, {"title": "3.5. Learning state enhanced knowledge state extraction module", "content": "Through the LSE module, we obtained the learning state sequence of learners. However, the differences in the learner's states during the answering process is not involved in the process of extracting knowledge states. To take it into consideration, A learning state enhanced knowledge state extraction module was designed, as shown in Figure 2(c).\nIn the learner's exercise-answer interactions, the learning state may change due to various complex factors. However, not all past learning states are equally important for prediction. Due to the nature of the softmax function, even historical states with relatively small relevance to the current prediction may receive some attention, which could introduce additional noise in the process of extracting knowledge states. To address this issue, we adopted a sparse attention-weighted approach to incorporate consideration of the learning state into the process of extracting knowledge states.\nTo exclude historically irrelevant moments, The k-means algorithm is used to group the historical learning state sequences, with different groups of historical moments being treated as irrelevant and masked. To obtain stably updated clustering clusters and ensure no leakage of future information, A fixed-size pool \\( X_{pool} \\) with a size of \\( \\mu \\) is designed by us to store the learning states of the most recent \\( \\mu \\) learners (excluding learners from the current batch). Subsequently, by applying the k-means algorithm to divide the learning states of all learners in the pool into n groups, we obtain n cluster centers for learning states \\( \\{l_1, l_2, ..., l_n\\} \\), where \\( l_i \\in R^{1\\times D} \\). Then, based on these n cluster centers, the learning states of learners in the current batch are divided into clusters to obtain the corresponding cluster labels. The formula is as follows:\n\\[ Label(\\hat{y}_t) = arg \\min_i \\sqrt{\\sum_{p=1}^{D} (\\hat{y}_{t_p} - l_{i_p})^2} \\]\n(11)\nIn the above formula, \\( \\hat{y}_t \\) represents the learning state of the current learner at time step t. Using this formula, we can determine the cluster to which this learning state belongs, denoted as \\( Label(\\hat{y}_t) \\).\nNext, we will use the learning state information of the current batch of learners to update the pool.\n\\[ X_{pool} = deque([X_{pool}, y], maxlen = \\mu) \\]\n(12)\nwhere deque is a double-ended queue, and \\( \\mu \\) is the fixed size of the pool.The most recent \\( \\mu \\) learner learning state sequences are always retained for stable updates to the clustering center, earlier information will be popped out of the pool.\nOnce the cluster to which each learner belongs is ascertained, we begin to identify crucial historical moments by calculating the similarity of the historical learning state sequence at each time step.\n\\[ \\beta_t = \\frac{\\hat{y}_{history}}{\\sqrt{D}} \\ \\ \\hat{y}_{history} = \\{y_1, ..., y_t\\} \\]\n(13)\nwhere \\( \\beta_t \\) represents the similarity distribution calculated based on the learning state \\( \\hat{y}_t \\) at time t.\nBy masking the attention scores of different clusters, we achieve sparse attention learning of states. Using Mask(\u00b7) to represent a masking operation that selects features within the same group, we can obtain a sparse similarity score matrix for learning state interactions along the sequence.\n\\[ Mask(\\beta_{t, \\tau}) = \\begin{cases} \\beta_{t,\\tau} & \\text{if Label}(\\hat{y}_\\tau) = \\text{Label}(\\hat{y}_t) \\\\ 81 & \\text{Otherwise} \\end{cases} \\]\n(14)\nwhere \\( Label(\\hat{y}_\\tau) = Label(\\hat{y}_t) \\) indicates that \\( \\hat{y}_\\tau \\) and \\( \\hat{y}_t \\) belong to the same cluster, and attention scores between learning states in different clusters will be masked. \\( \\beta_{t,\\tau} \\) represents the similarity score of \\( \\hat{y}_\\tau \\) with respect to \\( \\hat{y}_t \\).\nNext, we merge the sparse similarity score matrix of learning states with its corresponding exercise similarity score matrix to achieve enhanced knowledge state capture from learning states. To prevent future information leakage, interaction sequences beyond time step t should not be included in the calculation and thus need to be masked. The formula is as follows:\n\\[ \\Upsilon_{t, \\tau} = \\begin{cases} \\frac{e^{x_t k_\\tau /\\sqrt{D}} + Mask(\\beta_{t, \\tau})}{\\sum_{\\tau=1}^{t} (e^{x_t k_\\tau /\\sqrt{D}} + Mask(\\beta_{t, \\tau}))} & \\text{if } \\tau \\in (1, t) \\\\ 0 & \\text{if } \\tau \\in (t+1, N) \\end{cases} \\]\n(15)\nHere, the exercise feature \\( x_t \\) at time t is utilized by us as the query item \\( q_t \\), and the exercise feature \\( x_\\tau \\) at time \\( \\tau \\) is taken as the key item \\( k_\\tau \\). Through the dot product operation, we obtain the attention distribution of the exercise sequence. Next, this attention distribution is integrated with the attention distribution of the learning state sequence, thereby incorporating the learner's state changes during the learning process, resulting in a comprehensive attention score distribution \\( \\gamma \\). The focus of this approach lies in emphasizing historical moments with similar learning states, further refining the modeling of the answering process, and successfully introducing the diversity of learning state changes into the model. This will help improve the performance of the model.\nKey moment information from the historical interaction sequence can be extracted to obtain the final knowledge state by utilizing the complete attention score matrix:\n\\[ h_t = \\sum_{\\tau=1}^{N} \\Upsilon_{t,\\tau} v_\\tau \\]\n(16)\nwhere \\( v_\\tau \\in R^{1\\times D} \\) represents the interaction feature \\( y_\\tau \\) of the learner at time \\( \\tau \\) as the value, while \\( h_t \\in R^{1\\times D} \\) represents the final acquired knowledge state, which includes historical priors of learning performance.\nSubsequently, we fuse the knowledge state with the learning state, with the formula as follows:\n\\[ z_t = [h_t || \\hat{y}_t] W_7 \\]\n(17)\nwhere \\( z_t \\in R^{1\\times D} \\) represents the fusion of two states, and \\( W_7 \\in R^{2D \\times D} \\) is a learnable parameter matrix. We will utilize \\( z_t \\) for the final prediction of the KT task."}, {"title": "3.6. Prediction", "content": "The final step involves predicting the learner's response to the next exercise at the subsequent time step. The module's inputs comprise the comprehensive state feature \\( z_t \\) at the current time step and the embedding vector \\( x_{t+1} \\) of the subsequent exercise.\n\\[ \\hat{r}_{t+1} = \\sigma([z_t || x_{t+1}] W_8) \\]\n(18)\nHere, \\( \\sigma \\) is the Sigmoid function, and \\( W_8 \\in R^{D \\times D} \\) is a learnable parameter matrix. These inputs are fused through a fully connected network, and ultimately, a sigmoid function is applied to generate the predicted probability \\( \\hat{r}_{t+1} \\) of the learner answering the current exercise correctly, where \\( \\hat{r}_{t+1} \\in [0,1] \\).\n\\[ Loss = - \\sum_{t=1}^{T} (r_{t+1} \\log \\hat{r}_{t+1} + (1 - r_{t+1}) \\log (1-\\hat{r}_{t+1})) \\]\n(19)\nIn the entire LSKT method, all learnable parameters are trained in an end-to-end manner by minimizing the binary cross-entropy loss of all learner responses."}, {"title": "4. EXPERIMENTAL RESULTS"}]}