{"title": "A\u00b3 Rank: Augmentation Alignment Analysis for Prioritizing Overconfident Failing Samples for Deep Learning Models", "authors": ["ZHENGYUAN WEI", "HAIPENG WANG", "QILIN ZHOU", "W.K. CHAN"], "abstract": "Sharpening deep learning models by training them with examples close to the decision boundary is a well- known best practice. Nonetheless, these models are still error-prone in producing predictions. In practice, the inference of the deep learning models in many application systems is guarded by a rejector, such as a confidence-based rejector, to filter out samples with insufficient prediction confidence. Such confidence-based rejectors cannot effectively guard against failing samples with high confidence. Existing test case prioritization techniques effectively distinguish confusing samples from confident samples to identify failing samples among the confusing ones, yet prioritizing the failing ones high among many confident ones is challenging. In this paper, we propose A\u00b3 Rank, a novel test case prioritization technique with augmentation alignment analysis, to address this problem. A\u00b3 Rank generates augmented versions of each test case and assesses the extent of the prediction result for the test case misaligned with these of the augmented versions and vice versa. Our experiment shows that A\u00b3 Rank can effectively rank failing samples escaping from the checking of confidence-based rejectors, which significantly outperforms the peer techniques by 163.63% in the detection ratio of top-ranked samples. We also provide a framework to construct a detector devoted to augmenting these rejectors to defend these failing samples, and our detector can achieve a significantly higher defense success rate.", "sections": [{"title": "1 INTRODUCTION", "content": "Although deep learning (DL) models perform acceptably well in evaluation, these DL models still inevitably produce wrong predictions in practice. Wrong predictions can lead to various problems in different application domains, e.g., improper medical diagnosis [25] and traffic accidents [16]. Enhancing the DL application systems by reducing wrong predictions of DL models in producing outputs is desirable. Studies [9, 51, 52] have shown that DL models are vulnerable to operational input samples that can lead them to produce incorrect predictions in natural scenarios [52], and the prediction confidences of many such failing samples exceed those well-intended guarding confidence levels [54]. For example, strong sunshine may cause the camera of a self-driving car to capture an image full of white pixels, resulting in a prediction failure with high confidence. A major bottleneck in developing DL applications is detecting these overconfident failures from their deployed DL application systems.\nTo reduce unreliable predictions, many real-world machine-learning-based application systems are equipped with rejectors to discard uncertain decisions [17]. In DL application systems, many existing techniques [6, 17, 45] construct their rejectors for DL models to address the incorrect prediction problem. For example, many recent studies [2, 8, 42, 49] have been conducted to enhance the defense ability of DL models against out-of-distribution (OOD) samples from unknown classes or artificial examples that are very likely to guide DL models to yield failures. On the other hand,\nfor the failures caused by in-distribution (ID) samples from known classes, existing techniques tend to detect those failing samples close to the decision boundaries of DL models [11, 32, 46]. Detecting the failures of in-distribution samples that have non-trivial distances away from the decision boundaries of DL models is under-explored.\nA widely-used and effective solution [20] is to exploit the maximum softmax probability from a classifier (aka. prediction confidence) to filter out (e.g., reject) an input sample with the prediction probability of the prediction class not much larger than those of the remaining classes. Another well-known solution is DeepGini [11] which measures the gini impurity (a kind of measurement of uncertainty) from the probability vector produced by a DL model, and lower prediction confidence in the probability vector implies a higher uncertainty. Previous studies reported that some failing examples (e.g., an example with overly high prediction confidence) were not easily detected [4], and the existing state-of-the-art rejectors are still far from the perfect defense success rate [1]. Fig. 1 shows a real-world example revealing this situation. As such, the failing samples that can escape from rejectors still silently threaten the DL models. It is desirable to develop DL testing techniques to identify these escaping failing samples for follow-up analysis, such as developing a stronger rejection to augment the existing rejectors and filter these failing samples.\nIn the context of DL testing, developers often focus on the tests that lead to prediction failures for the reason that analyzing these tests can provide insights to safeguard the DL systems. However, unlike conventional software with human-developed business logic for generating tests, DL testing generally faces automated testing oracle problems. This fact motivates us to propose a testing technique to prioritize tests so that fault-inducing tests can be labeled and analyzed before the other tests. In this manner, we can obtain maximum benefit from human efforts and leverage fault-inducing tests to construct or augment the rejectors.\nIn this paper, we propose A\u00b3 Rank (Augmentation Alignment Analysis for Ranking), a novel and effective test case prioritization (TCP) technique to prioritize confident failing samples of DL models escaped from rejectors. A\u00b3Rank tackles the challenge that failing samples escaping from rejectors are more difficult to find. For instance, the proportion of these samples in all failing samples becomes smaller as the prediction confidence threshold for rejection increases. We focus on the classification problem as a starting point, as it is one of the fundamental prediction problems many TCP techniques in DL model testing concentrate on. We observe that relying on a single prediction vector of the sample under test could be unreliable in ranking failing samples with high confidence levels, as the DL models under test have shown confidence in predicting their labels. On the other hand, we observe that DL models in real-world DL applications are data-augmented. Thus, A\u00b3 Rank proposes to perform augmentation-based mutation analysis on the alignment among multiple prediction vectors of the test sample and its mutated samples."}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 Classification Models", "content": "A classification model (classifier for short) M with C class labels {c\u2081, \u2026\u2026\u2026, cc} is a function. It takes a sample s as input and outputs a prediction vector, denoted as M(s). The number of components of M(s) is C. The i-th component of M(s) (for 1 \u2264 i \u2264 C) keeps a value, denoted by M(s)i, representing the probability of s predicted as class c\u012f. The summation of the values in M(s) is 1, i.e., \u03a3=1 \u039c(s)\u2081 = 1. The prediction class of s, denoted by cp, is the class for the component of M(s) keeping the largest value, i.e., p = arg max1<i<CM(s)\u012f. The value M(s)p is called the prediction confidence of s. Each sample s after labeling associates with a groundtruth label ls. If cp \u2260 ls (i.e., the prediction class differs from the groundtruth label), s is called a failing sample or failing test case. If cp = ls, M is said to correctly classify s, otherwise misclassify it."}, {"title": "2.2 Models with Data Augmentation Training", "content": "Training samples in many application domains (e.g., computer vision) are popularly data-augmented [43] when used to train a classifier M. For example, a training sample can be augmented by a set of geometric operations (e.g., shifting) in the image classification task. Let A be a set of transformation operations to transform a sample s. Applying each operation in A transforms s into a sample variant s of s paired with the groundtruth label ls of s if available. We refer to the set of all sample variants of s after applying A as S. Many studies [43?? ] show that data augmentation can improve the test accuracy and robustness of DL models. Therefore, many practical DL models are produced with training schemes using data-augmented training sets."}, {"title": "2.3 Classification Model with Reject Option", "content": "A classifier may misclassify some input samples. An application system deployed with such a classifier is often equipped with a \u201cguard\u201d to filter out some samples that tend to be misclassified by the classifier. A recent survey [17] reviews machine learning models with a reject option, where a model with a reject option, H, consists of a classifier M and a rejector R. The survey also summarizes that most existing works it has reviewed take the prediction confidence of M as a metric to define a rejector. For instance, if the prediction confidence for a sample s made by M is lower than 0.7, then R outputs true, otherwise false. More specifically, in addition to all C classes of the classifier M, the model H has one more class cx, representing the class of samples to be rejected even if the sample"}, {"title": "3 TWO-STAGE REJECTION", "content": "Suppose H = <M, R) is a classifier with a reject option, in which M is an accurate data-augmented classifier under test and R is a rejector that rejects an input sample s (i.e., returns true) if the prediction confidence of M on s is lower than a predefined threshold value. Suppose further T is a set of unlabeled samples input to H.\nWe aim to prioritize T so that the failing samples of M in T are ranked as high as possible, subject to the condition that the rejector R does not reject them. In other words, we want to find these unlabeled failing samples predicted by M that can pass through R. The samples ranked higher in the prioritized T are then labeled to reveal whether they are failing samples. A one-class classification [36] (OCC)-based detector D using these labeled and subtle samples is created to extend the original reject option of H to check each input sample (that passes R) by D, thereby creating a modified model H' = \u3008M, R') of H where R' = (R, D)."}, {"title": "4 OUR TECHNIQUE", "content": ""}, {"title": "4.1 Motivation", "content": "Inspired by the ineffective analysis result depicted in Fig. 2, relying merely on single prediction confidence appears ineffective in detecting the failing samples in case B, which motivates us to explore new ideas to test samples from the reliability perspective. We observe that the augmented samples (called sample variants) produced through perturbating a training sample are all trained to be predicted to the same groundtruth label during augmentation training, which provides a clue for testing that the prediction classes of these sample variants of a test sample are expected to match the prediction class of the test sample itself. This insight is also consistent with our experience that a failure (i.e., misclassification) would generally occur when the majority of the prediction classes of these sample variants of a test sample is different from the prediction class of the test samples.\nFig. 3 illustrates this anomaly pattern. Suppose a test sample s is classified to class c1, and its sample variants {51, 52, 53, 54} are predicted to classes c1, C2, C2, and c3, respectively, by a 3-class classifier. Producing anomaly predictions by sample variants, especially the ones predicted to the majority prediction class (i.e., c2 in this example), tends to be more difficult if the test sample s has higher and higher prediction confidence. The difference in the prediction probabilities between sample variants and the test sample reveals a clue on misalignment. This motivates us to formulate a comprehensive and quantitative analysis to measure the misalignment through prediction differences among the sample test and its variants from the reliability angle."}, {"title": "4.2 Augmentation Alignment Analysis", "content": "We suppose the classifier under test M is trained with data augmentation. In the training scheme to produce M, the training set is augmented by applying a set of data augmentation transformation operations A to each training sample.\nLet s be a test sample to be ranked. We apply each operation in A to transform s into a sample variant. We denote the set of all generated sample variants of s by S.\nTo address the abovementioned challenge, we propose to exploit the difference between a sample and its data-augmented versions (i.e., sample variants). We aim to explore both alignment and misalignment between how confident M classifies the sample s to its prediction class but not to the classes of its sample variant set S and how confident the same classifier classifies individual sample variants to their prediction classes but not to the prediction class of the test sample s.\nWe first clarify how to classify the set S, and the formal definition can be found in the later part of this subsection. The classifier M will classify each sample variant in S into a class. The (majority) class, denoted by cm, associated with the largest counts among sample variants, is treated as the prediction class of S. Furthermore, to simplify our presentation, we refer to the prediction class of the test sample s as cp (i.e., M classifies s to cp).\nIn this work, we propose A\u00b3 Rank, a technique with the novel Augmentation Alignment Analysis for Ranking samples of DL models with reject options. A\u00b3 Rank connects each test sample s and individual sample variants not only directly by their prediction classes (and confidence) but also indirectly via the prediction class of S from both alignment and misalignment angles. We aim to provide an analysis suitable for all samples under test. As such, we design the A\u00b3 Rank formula applicable for ranking confident failing samples and confusing failing samples.\nWe refer to the abovementioned class cm as the majority prediction class of the sample s. Given an input sample s and its sample variant set S, this majority prediction class cm is computed by finding the class index m using the following formula: \\(m = arg max_{1<j<c} \\sum_{\\check{s} \\in S} 1\\{arg max_{1\\leq i \\leq C} M(\\check{s})_i = j\\}\\) where 1{J} is a function that returns 1 if the condition J holds, otherwise 0. A tie case is resolved by choosing the class with higher mean prediction confidence.\nWe further distinguish each sample variant's roles in our differential alignment analysis.\n(1) Dominator (\\(\\check{s}_{dom}\\)). If the classifier M classifies a variant \\(\\check{s}_{dom}\\) to the majority prediction class cm (i.e., m = arg max; M(\\(\\check{s}_{dom}\\))i), the variant \\(\\check{s}_{dom}\\) is called a dominator. This role indicates the variant contributes to converging the decision of the prediction class for S.\n(2) Supporter (\\(\\check{s}_{sup}\\)). If M classifies \\(\\check{s}_{sup}\\) to the prediction class cp of the sample s (i.e., p = arg max; M(\\(\\check{s}_{sup}\\))i), the variant \\(\\check{s}_{sup}\\) is called a supporter. This role indicates that the variant aligns with the sample under rank s in their prediction results. We note that a sample variant may be simultaneously a dominator and a supporter when cp = cm.\n(3) Distractor (\\(\\check{s}_{dis}\\)). If M classifies a variant \\(\\check{s}_{dis}\\) to a class other than both cp and cm, then the variant \\(\\check{s}_{dis}\\) is called a distractor. The variant serves as a distractor to discourage a consistent decision on the prediction classes between s and the set S.\nMoreover, we denote the sets of all dominators, all supporters, and all distractors by Sdom, Ssup, and Sdis, respectively. A sample variant is both a supporter and a dominator if the majority prediction class cm is the same as the prediction class cp.\nThe prediction confidence presents the likelihood of the prediction reliability of a sample. How- ever, since a sample with higher prediction confidence may or may not be failing, directly adopting the prediction confidence of a sample as the ranking metric is not adequate. Therefore, A\u00b3 Rank uses the prediction confidence of a sample to rank as the baseline and formulates a novel suite of three alignments on this baseline to assess the sample prediction reliability for ranking them."}, {"title": "4.2.1 Alignment from Distractors", "content": "The presence of a distractor \\(\\check{s}_{dis}\\) indicates that the prediction on the test sample s by the classifier M encounters a certain extent of \u201chidden\u201d uncertainty that can be observed \u201caround\u201d the test sample s. Our insight is that if the extent of such uncertainty exhibited by a distractor can be resolved, then the distractor will become a supporter, strengthening the reliability of the sample s being correct.\nThus, we design the alignment term g\u2081 to account for the presence of a distractor \\(\\check{s}_{dis}\\), which is the amount of increase in prediction probability of the class cp in the prediction vector of \\(\\check{s}_{dis}\\) so that the class cp could emerge into the prediction class of this distractor \\(\\check{s}_{dis}\\)."}, {"title": "4.2.2 Alignment from Supporters", "content": "On the contrary, each supporter \\(\\check{s}_{sup}\\) enhances the prediction reliability on the test sample s. Recall that the prediction by an ensemble over a set of classifiers that returns the majority output is more robust than the prediction of individual classifiers in the ensemble. Suppose cp = cm in the prediction vector of this supporter \\(\\check{s}_{sup}\\). In this case, the supporter shows robust support for the prediction class of the test sample s. On the other hand, suppose Cp \u2260 cm for \\(\\check{s}_{sup}\\), the supporter provides an (over)confidence over the more robust the majority label to support the test sample s. To provide reliable support to the test sample s, the prediction probability for the majority class in the prediction vector of \\(\\check{s}_{sup}\\) should be at least increased to the same level as the prediction probability for the class cp in that prediction vector. Thus, we design the following alignment term g2 for \\(\\check{s}_{sup}\\)."}, {"title": "4.2.3 Alignment from Dominators", "content": "The dominators may show strong uncertainty or great support for the prediction reliability of the test sample s, depending on whether the majority prediction class cm differs from the prediction class cp of the sample s. More specifically, if cm = cp, the dominators highly support s to be classified to cp (and thus, s is less likely to be a failing sample intuitively). Nonetheless, if cm \u2260 cp, the presence of dominators indicates a large extent of uncertainty that s belongs to the class cp. Intuitively, the latter case (cm \u2260 cp) occurs less frequently than the former (cm = cp) due to the application of data augmentation to push a sample and its sample variants generally toward the same label. Moreover, when a scenario in the latter case occurs, as the ratio of dominators to all the variants in S increases, the test sample s is more likely to be a failing sample.\nOwing to the need to compare the test sample s and its sample variants that dominate in the set S, the extent of misalignment in prediction reliability on s has two sources. The first source is the loss in prediction probability needed to align a dominator \\(\\check{s}_{dom}\\) to become a supporter. The larger the loss is, the more confused M is to distinguish s between cm and cp. The second one is the loss in prediction probability needed to change the test sample s from the current prediction class cp to the majority prediction class cm of its variant set so that s can align with the set S in prediction. The larger the loss implies the larger over-confidence of M to classify s to cp relative to cm. If both losses can be resolved, a dominator will greatly support the reliability that s is predicted correctly.\nWe present the alignment term g3 for a dominator sdom from the reliability gain perspective. We note that the evaluation results of the alignment terms of dominators may be negative values."}, {"title": "4.2.4 Overall Formula", "content": "For each variant in S, A\u00b3Rank applies the alignment terms based on their roles as presented above. (Note that a variant may have multiple roles.) The alignment from the distractor explicitly shows a negative effect on the prediction reliability. Thus, A\u00b3Rank uses the negation of this alignment term. The augmentation alignment analysis score (A\u00b3-score) for a sample s is computed by the following formula A\u00b3Score(s). We linearly combine all the terms because we do not find a good reason to weigh a value more. Test samples are ranked in ascending order of A\u00b3-score, so intuitively, less reliable test samples will be prioritized first."}, {"title": "4.2.5 Example", "content": "Fig. 4 illustrates an example to carry out the analysis of A\u00b3 Rank in the experiment. The sample s is predicted by M with a confidence of 0.95. A\u00b3 Rank generates its sample variants, and four of them are illustrated with the prediction vectors below the one for the sample s. However, the analysis of A\u00b3 Rank shows a large misalignment from the sample variants of s. The distractor \\(\\check{s}_{dis}\\) gives 0.09 in the strength of uncertainty for the prediction, and the supporter \\(\\check{s}_{sup}\\) supports the prediction with 0.60 in strength. However, the two dominators agree to the consensus of class c4 and show 1.28 and 1.37 in strengths of the misalignment, respectively. Thus, A\u00b3 Rank scores s according to Eq. 6 to 0.95 \u2013 0.09 + 0.60 \u2013 1.28 \u2013 1.37 = \u22121.19."}, {"title": "4.3 Labeling Samples", "content": "The ranked list of samples is presented to developers. They then label samples, and thus, failing samples are revealed. After that, A\u00b3 Rank proceeds to improve the model H from the detection perspective, presented in the next subsection."}, {"title": "4.4 Enhancing the Model with Reject Option with A\u00b3Rank", "content": "A\u00b3 Rank applies the discovered subtle samples to construct an OCC detector D to defend further the samples that pass the rejector R. Specifically, the set of discovered subtle samples is viewed as a single class of samples to construct D. Thus, any sample predicted to be within the distribution by the trained D is deemed failing and thus rejected, as summarized in Eq. 2.\nIn our experiment, we adopt NIC [31] to generate our OCC detector on account of its superior effectiveness stated in their paper [31] and comprehensive profiling of internal states of M. The original NIC algorithm accepts the clean training set of M to produce an OCC detector to exclude samples described by the clean training set. We input the abovementioned set of discovered and labeled subtle samples to the NIC algorithm to return an OCC detector D. In the inference time, the OCC detector D checks whether a novel sample is similar to the learned knowledge from the set of labeled subtle samples. If this is the case, the detector returns true, otherwise false. We note that since A\u00b3 Rank intends to deal with the subtle samples, the trained D only checks the input samples that have passed the original rejector R.\nAlg. 1 summarizes the whole A\u00b3Rank framework. It accepts a model H consisting of a data- augmented classifier M and a rejector R, a set of natural samples Tnat, and the label resource w. Line 1 initializes an empty list L, and line 2 retrieves the set of transformation operations A of the augmentation specified in M's training scheme.\nThe algorithm computes the A\u00b3-score for each sample s in Tnat in the loop (lines 3\u20139). It applies M to classify s to produce the prediction class cp and the prediction vector up (line 4). It then generates a set of sample variants S using A on s (line 5), followed by applying M to produce their corresponding prediction classes c and prediction vectors v (line 6), both are in the vector form. It finds the majority prediction class cm of S (line 7). In line 8, it computes the A\u00b3-score according to Eq. 6, and then adds the pair of the sample and its A\u00b3-score to L. The algorithm ranks the samples in Tnat by their A\u00b3-scores kept in L to produce a ranked list of samples That (line 10), in which samples are sorted in ascending order of their A\u00b3-scores. The next step is to construct a sublist consisting of the top-w samples of Trat, and drop every sample s in the sublist that the rejector R(s, M) returns true (i.e., rejecting the sample), followed by an external (probably manual) process of sample labeling on the reduced sublist, resulting in a subset of subtle samples with labels Tsub (line 11). Next, it constructs an OCC detector D using the subset Tsub as its training set (line 12) and then enhances the model H with D (lines 13-14)."}, {"title": "5 EVALUATION", "content": "This section reports the evaluation on A\u00b3 Rank."}, {"title": "5.1 Research Questions", "content": "We aim to answer the following four research questions,\n\u2022 RQ1: How effective is A\u00b3 Rank in detecting subtle samples compared to the baselines?\n\u2022 RQ2: How effective can A\u00b3Rank framework be in defending subtle samples?\n\u2022 RQ3: How large improvement does A\u00b3 Rank improve in prioritizing failing samples compared with the state-of-the-art techniques?\n\u2022 RQ4: How do the alignment terms in Eq. 6 contribute to the effectiveness of A\u00b3 Rank?"}, {"title": "5.2 Implementation", "content": "We implement all the experiments in Python v3.8 and PyTorch v1.12.1 with Torchvision v0.13.1. We run all the experiments on a Ubuntu 20.04 server with a 48-core 3.0GHz Xeon CPU, 256GB RAM, and 2080Ti GPU. We adopt the open-source implementations of the baseline techniques and adapt them to our platform if available. If unavailable, we implement them according to their papers."}, {"title": "5.3 Datasets and Models", "content": "We aim to evaluate A\u00b3 Rank with a concentration on natural scenarios. Therefore, we select three evaluation datasets containing color images with natural classes widely used in DL model testing.\n\u2022 CIFAR10 [27]: It consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training samples and 10,000 test samples.\n\u2022 CIFAR100 [27]: The data statistic of CIFAR100 is the same as CIFAR10, except that it has 100 classes containing 600 images per class.\n\u2022 CINIC10 [7]: It consists of 270,000 32x32 color images in 10 classes, with 27,000 images per class. It has equally sized train, validation, and test splits, each subset of 90,000 samples.\nAdditionally, we adopt the weather transformation operations in [19] to simulate the natural sce- narios. Specifically, there are four transformation operations (i.e., Snow, Frost, Fog, and Brightness), where each type has five levels of severity, resulting in a set A of 20 fine-grained transformation operations. We generate a balanced-class dataset Trank with an equal size of the training set for each evaluation dataset by randomly selecting from training samples. We also use the benchmark dataset Tmark provided by [19] to evaluate the effectiveness of defending subtle samples.\nWe select the ResNet20 [15] as the threat model for all evaluation datasets. Using different model architectures would introduce a new variable in the experiments. Therefore, we unify it to eliminate the impact. Specifically, the model is trained with augmentation of cropping and flipping for 200 epochs at an initial learning rate of 0.1, with a momentum multiplier of 0.9 weight decay with a multiplier of 5e\u00af\u00b9, and batch size 128. The learning rate is cosine annealed to zero. The validation accuracy of the trained models for CIFAR10, CIFAR100, and CINIC10 are 92.58%, 69.09%, and 83.17%, respectively. The resultant models are used as the model under test M in our experiments."}, {"title": "5.4 Baselines", "content": "We aim to compare A\u00b3 Rank with diverse categories of state-of-the-art techniques as baselines.\nDeepGini [11] is the metric-based prioritization technique. It computes the gini impurity from the prediction vector and utilizes it as the metric for ranking.\nMonte Carlo Dropout [12] (Dropout in short) is the uncertainty-based prioritization technique. It arbitrarily enables a dropout layer of the model under test in the inference time and performs K stochastic forward passes over the sample under rank s followed by averaging the variances. We configure the dropout rate of the dropout layer to 0.05 and the number of forward passes K to 50.\nDissector [46] is the trend-based prioritization technique. Dissector trains a sequence of submodels for individual layers and measures the validity degree of confidence trend from the prediction vectors of these submodels. We configure to train the submodels, each with 5 epochs.\nLSA [26] is the surprise-based prioritization technique. It trains a distribution on the intermediate output of a layer in the model under test over the whole training set for each class. It ranks the samples by the probability density to the distribution of its predicted class.\nPrima [47] is the prioritization technique adopting the learning-to-rank concept. In their experi- ments, Prima generates 200 model mutants for the model under test and 100 sample mutants for each input sample. It then extracts their predefined features from the mutation results and learns a ranker model. We note that the original number of model mutants and sample mutants exceeds the affordable computation resource in our platform (i.e., 20000x inference time), so we have to reduce the number of model mutants to 20 and the number of sample mutants to 10.\nCoverage-based techniques [29, 38] are not included because DeepGini has shown consistent and significant outperformance compared to them [11]."}, {"title": "5.5 Experiment Settings", "content": ""}, {"title": " Experiment 1", "content": "We input the dataset Trank and its associated model to Alg. 1 and execute from line 1 to line 10 to produce a ranked list of samples T Tank for A\u00b3 Rank. Using their algorithms, the baseline rankers also produce their ranked lists of samples Tan. Then, we directly select the top-w samples from each ranked list for measurement. We configure two labeling resources \u03c9: 1) w is set to 10% of the dataset size, i.e., \u03c9 = 0.1 * |Trank|. We refer to this selection setting as TOP. 2) w is set to the total number of failing samples in Trank. (We note that the number of failing samples can be estimated via sampling in practice.) We refer to this selection setting as CUT. We configure three confidence-based rejectors Rs with thresholds \u03b8 of 0.7, 0.8, 0.9, respectively. For each rejector R, we further execute line 11 of Alg. 1 to produce a subset of subtle samples Tsub that passes R for each set of resultant top-w samples. As labeling each set of subtle samples Tsub at line 11 of Alg. 1 requires an external process, we assure the labeling quality is perfect in the evaluation. We measure the number of discovered subtle samples and the throughput ratio of subtle samples, i.e., the number of subtle samples over the number of failing samples. The result is averaged over 10 runs."}, {"title": "Experiment 2", "content": "We use Tsub in Experiment 1 to construct a NIC model as the detector D. More specifically, the NIC algorithm accepts a set of samples and a DL model and then produces a model containing a sequence of one-class SVM (OSVM) models and an OSVM-based predictor. We input Tsub and the DL model under test M to the NIC algorithm and acquire the resultant NIC model as D. Given a novel sample, we input it into the NIC model to predict whether it is in distribution to Tsub, and if so, the detector rejects this sample. We input all failing samples that passed the rejector in the benchmark dataset Tmark to the trained D and measure the ratio of the number of samples rejected by D over these failing samples. This ratio is also known as the defense success rate."}, {"title": "Experiment 3", "content": "We also investigate A\u00b3 Rank to rank the whole test dataset to cover the detection of all failing samples compared with state-of-the-art TCP techniques. The effectiveness is evaluated by the improvement ratio over the ranking by the Random technique. We measure the ratio of the number of failing samples in the top-w samples as in Experiment 1 and report the improvement ratio."}, {"title": "Experiment 4 (Ablation Study)", "content": "We ablate each alignment (namely 91, 92, and g3) of A\u00b3Rank to produce an ablated version of A\u00b3 Rank. We denote the ablated versions of A\u00b3Rank without 91, 92, and 93 by A\u00b3R, A\u00b3 R-92, A3R-93 Rank, A\u00b3 Rank, respectively. We repeat Experiment 1 for each ablated version and measure the number of discovered subtle samples."}, {"title": "5.6 Experiment Results", "content": "Table 2 summarizes the results of Experiment 1 under different rejection settings for R. The results of LSA and Prima are omitted due to the limited effectiveness in finding failing samples (see Table 4) for space consideration. In each cell, there are two values. The one at the top is the discovered number of subtle samples, and the other at the bottom is the throughput ratio. The cell with the most discovered subtle samples in each column is highlighted in bold.\nAcross the board, A\u00b3 Rank consistently detects more subtle samples compared with the baselines under all the settings, and the throughput ratio of subtle samples is also larger than the baselines, except for just one case of CUT with 0 = 7 on the CIFAR100 dataset. We note that under that case among the effective techniques of Dropout, Dissector, and A\u00b3Rank, the maximum difference is less than 5% (i.e., 10.71/10.31 \u2212 1 = 3.88%), not to mention the difference between A\u00b3 Rank and Dropout is only 1.03% = 10.71/10.60 \u2013 1.\nThe differences between A\u00b3Rank and the best baseline results are large. Taking the average of the same rejection threshold, A\u00b3 Rank outperforms the largest of the baselines by 52.78%, 103.93%, and 301.52% on the thresholds of 0 = 0.7, 0 = 0.8, and 0 = 0.9, respectively, which are significant. Noticeably, the differences across the increasing \u03b8 are increasing, indicating that A\u00b3 Rank is more effective in finding the failing samples with higher prediction confidences.\nRegarding the throughput ratios, A\u00b3Rank also achieves significantly more effective results than the best baselines. Similarly, on account of the average under the same rejection threshold, A\u00b3 Rank outperforms the largest of the baselines by 24.73%, 217.07%, and 249.11% (163.63% averagely) on 0 = {0.7, 0.8, 0.9}, respectively. The increasing differences are consistent with the differences in the number of subtle samples, value-adding on top of the effectiveness of A\u00b3 Rank in ranking these challenging subtle samples."}, {"title": "Answering RQ1", "content": "A\u00b3 Rank consistently and significantly outperforms the baselines in finding subtle samples up to 301.52%, and the ranking effectiveness increases with the tasks to discovering failing samples with higher confidence."}, {"title": "Answering RQcontinue from here:1", "content": "A\u00b3 Rank framework is effective in producing detectors to enhance the application models, and the detectors produced with the subtle samples discovered by A\u00b3 Rank are more effective than those by the baselines."}, {"title": "Answering RQ3", "content": "A\u00b3 Rank consistently outperforms the baselines in detecting failing samples. Its top-ranked samples are with higher prediction confidence than the baselines."}, {"title": "Answering RQ4", "content": "Each alignment term in A\u00b3 Score (Eq. 6) contributes positively to the effectiveness of A\u00b3 Rank."}, {"title": "5.7 Threats to Validity", "content": "A threat to validity lies in the concerns about the robustness improvement ability of A\u00b3Rank. To reduce this threat, we conduct experiments to mimic the real-world scenario with 5 rounds of samples discovered. Besides, the experiments only evaluate one model architecture. We note that different model architectures perform similarly on the benchmarks\u00b9 by PyTorch. Another threat to validity is the imprecise decision of the detector D. We are aware that the detectors generated by the NIC algorithm make false positive decisions. However, the false positive rate is small according to their paper [31], and the survey [17] outlines that the true negative predictions are tolerable in models with reject options and could be deferred to a human expert in certain application scenarios. The implementation may contain bugs unknown to us. We have tested that the downloaded code is reproducible. The experiments are limited to one kind of rejector and the chosen hyperparameters. Using more detection algorithms, datasets, DL models, rejectors, and hyperparameters can generalize the result, and we leave the generalization in future work."}, {"title": "6 RELATED WORK", "content": "Test case prioritization and selection techniques [23, 24, 28, 30, 50, 53] have been intensively studied. DeepGini [11], Dissector[46], LSA [26], Prima [47] and Dropout [12] hae been reviewed and compared in Section 5. Test case selection can involve choosing a subset of representative cases for more efficient testing rather than testing the entire set [23, 28, 53]. Our work effectively finds the subtle samples and may have the potential to complement the reduced test set produced by these works for evaluation. Other works [24, 30, 50] identify the samples triggering abnormal neuron activations within the model under test. Our work identifies the failing sample with higher prediction confidence and could potentially ease profiling by providing subtle examples for analysis of the neuron activation patterns.\nA range of related works aims to incorporate a reject option for machine learning and deep learning models [3, 13, 21, 22, 37, 40, 41, 44]. Many techniques focus on novelty (outlier) rejection [37, 40, 44]. It has been reported that the outliers are much easier to detect via accuracy thresholding [37], support vector machine [40], and uncertainty estimation [44]. Another technique category is the ambiguity rejection with different model architectures [3, 13, 21, 22, 41]. Some rejectors are constructed independently of the classifiers used, but this often results in sub-optimal rejection performance [22]. Other approaches leverage classifier information to build rejectors [3, 21], and some further attempts to integrate rejectors with classifiers [13, 41]. Our work falls into this latter category and can be used to identify subtle samples for better integration with these techniques.\nAnother related line of work focuses on adversarial example detection and anomaly detection [10, 14, 31, 33-35, 48]. These techniques can be further classified into supervised detection [10, 14, 33, 35] and unsupervised detection [31, 34, 48] methods. The feature-based approaches [14, 35] use clean and adversarial examples to train their detectors. The statistical-based approaches [10, 33] perform a statistical measurement to cut the separation between these two sets of samples. On the other hand, unsupervised detectors propose feature squeezing [48], denoising [34], and invariant [31] to produce effective detectors. Our work adopts the state-of-the-art unsupervised method [31] to generate a detector. Our work may help to find subtle samples along with clean samples with higher confidence to construct an improved supervised detector."}, {"title": "7 CONCLUSION", "content": "We have presented A\u00b3 Rank, a novel and effective prioritization technique for ranking samples of deep learning models with a reject option. A\u00b3 Rank comes with a novel augmentation alignment analysis to score the samples under rank. The experiment results have shown that A\u00b3Rank is significantly more effective in ranking failing samples escaping the confidence-based rejectors and"}]}