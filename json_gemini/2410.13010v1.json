{"title": "Hiding-in-Plain-Sight (HiPS) Attack on CLIP for Targetted Object Removal from Images", "authors": ["Arka Daw", "Megan Hong-Thanh Chung", "Maria Mahbub", "Amir Sadovnik"], "abstract": "Machine learning models are known to be vulnerable to adversarial attacks, but traditional attacks have mostly focused on single-modalities. With the rise of large multi-modal models (LMMs) like CLIP, which combine vision and language capabilities, new vulnerabilities have emerged. However, prior work in multimodal targeted attacks aim to completely change the model's output to what the adversary wants. In many realistic scenarios, an adversary might seek to make only subtle modifications to the output, so that the changes go unnoticed by downstream models or even by humans. We introduce Hiding-in-Plain-Sight (HiPS) attacks, a novel class of adversarial attacks that subtly modifies model predictions by selectively concealing target object(s), as if the target object was absent from the scene. We propose two HiPS attack variants, HiPS-cls and HiPS-cap, and demonstrate their effectiveness in transferring to downstream image captioning models, such as CLIP-Cap, for targeted object removal from image captions.", "sections": [{"title": "Introduction", "content": "The vulnerability of machine learning (ML) models to adversarial attacks-small perturbations in input data that lead to incorrect predictions\u2014has been extensively studied [1, 2] across various do-mains, including image classification [3, 4] and biometrics [5, 6]. However, most existing adversarial attacks have been designed for single modalities, primarily focusing on the image or, less frequently, the text domain [7, 8]. The advent of large foundational models, such as large language models (LLMs) [9] and large multi-modal models (LMMs) [10] (e.g., Chat-GPT [11], Gemini [12]), which have shown great promise across a diverse range of tasks [13] (such as zero-shot classification, visual question answering, and image captioning) has revolutionized the ML community and led to their widespread adoption. Many of these models [14] integrate a pre-trained LLM with a large vision encoder, such as CLIP [15] which is a foundational multimodal model trained on 400M image-text pairs via contrastive learning. Typically, the vision encoder of such LMMs remains frozen during training, and the vision embeddings are mapped into the shared embedding space of the LLM using a simple projection layer. However, this introduces an obvious vulnerability [16]: adversarial attacks developed against these open-source vision encoders (such as CLIP) can be directly transferred to LMMs, compromising their integrity.\nWhile generating adversarial attacks on LMMs (such as CLIP) [16, 17] has been explored, generally termed as jailbreaking LMMs [18, 19], they have primarily focused on \u2018completely' changing the output of the LMM to what the adversary wants, which is typically very different from the original outputs (without any perturbation). However, in many real-world scenarios, an adversary might seek to make only 'subtle' modifications to the output, so that the changes go unnoticed by downstream models or even by humans. To this end, we introduce a novel class of adversarial attacks on images, termed Hiding-in-Plain-Sight (HiPS) attacks. The primary goal of a HiPS attack is to generate an adversarial image that subtly modifies the model's predictions by selectively concealing a specific target object while leaving the rest of the model's functionality intact. For example, a HiPS adversarial image designed to hide a particular object should cause an image captioning model to generate a"}, {"title": "Background and Related Works", "content": "Adversarial Robustness: One of the seminal methods for generating adversarial attacks is the Fast Gradient Sign Method (FGSM) [1], a simple, single-step \\(L_\\infty\\)-bounded attack, defined as:\n\\(I_{adv} = I + \\epsilon sign(\\nabla_I L(I, y))\\), where \\(I_{adv}\\) is the adversarial image, \\(I\\) is the original image, \\(\\epsilon\\) is the attack budget, and \\(L\\) is the loss function to be maximized for the attack. For an untargeted attack,\n\\(L\\) is typically the cross-entropy loss with respect to the correct class \\(y\\), and in a targeted attack,\nthe objective shifts to minimizing the loss with respect to a target class \\(\\tilde{y}\\), making \\(L\\) the negative cross-entropy loss for the target class. Another widely used technique is the Projected Gradient Descent (PGD) attack [2], which is the strongest first-order attack. PGD is an iterative, first-order optimization-based attack, defined as: \\(I^{t+1} = P_{I+\\delta}(I^t + \\alpha sign(\\nabla_I L(I, y)))\\), where \\(t\\) denotes the iteration number, \\(P\\) is a projection operation that maps the perturbed input back onto a \\(L_p\\) ball with radius \\(\\epsilon\\), with \\(S\\) representing the region defined by the \\(L_p\\) ball, and \\(\\alpha\\) is the step size.\nMulti-modal Models: CLIP [15] is one of the seminal works in multi-modal modeling due to its exceptional performance in zero-shot tasks. Recently, there has been growing popularity in developing large multi-modal models (LMMs) [10] (GPT-4V [11], Gemini [12], LLaVA [14]) driven by their impressive capabilities across a wide range of tasks and domains [13]. Many of these models integrate a pre-trained large language model (LLM), such as Llama [21] or Vicuna [22], with a large vision encoder like CLIP. For LLaVA, the vision encoder remains frozen during training, with a simple projection layer mapping the vision embeddings to the shared embedding space of the LLM.\nAdversarial Robustness of LMMs: With the advent of LMMs, investigating their vulnerabilities has become an important research focus in AdvML, often referred to as jailbreaking LLMs and LMMs [19, 18]. While previous studies have demonstrated that jailbreaking LLMs is feasible with full access to model parameters, recent findings highlight that LMMs are particularly susceptible to adversarial attacks targeting the vision modality [23]. In particular, even with access solely to the vision encoder, such as the open-sourced CLIP model-adversaries can exploit these vulnerabilities to jailbreak LMMs like LLaVA and OpenFlamingo[24], which rely on the frozen CLIP vision encoder."}, {"title": "Hiding-in-Plain-Sight (HiPS) Attack", "content": "Traditional 'targeted' adversarial attacks on images are designed to drastically alter the behavior of a downstream ML models (such as a Large Vision Language Model or an image classifier), forcing them to produce outputs that align with the adversary's objectives. In contrast, we introduce a novel class of adversarial attacks on images, termed Hiding-in-Plain-Sight (HiPS) attacks. The primary goal of a HiPS attack is to generate an adversarial image that can 'subtly' modify the model(s) predictions by selectively concealing a specific 'target' object while leaving the rest of the model's functionality intact. For instance, a HiPS adversarial image designed to conceal (or 'target') a particular object should cause an image captioning model to generate a caption as if the target object(s) was never present while the rest of the image content should stay in tact. Similarly, when a HiPS adversarial image is processed by a LMM, the model should respond to queries about the image as if the target object were absent. Ideally, the adversarial images generated using the HiPS attacks should be universal and transferable across a variety of downstream ML models. Therefore, generating the HiPS attack using a 'foundation' multi-modal model that is already universally used for a variety of downstream tasks is necessary for transferability. For simplicity, in this paper, we focus on investigating the transferability of HiPS attacks specifically on image captioning models [25]."}, {"title": "Problem Formulation", "content": "In this section, we introduce the formal notations used throughout this paper. Let \\(I\\) represent an input image containing \\(n\\) different object classes, and let \\(T_\\ell = \\{T_1, T_2, ..., T_n \\}\\) denote the set of objects present in the image \\(I\\), where \\(T_i\\) corresponds to the textual description (or simply, the class labels) of the \\(i\\)-th object. The target object to be removed is denoted as \\(T_{target} = T_j\\) for some \\(j \\in \\{1, 2, . . ., n\\}\\). We will utilize the CLIP model to generate the HiPS attack, which consists of an image encoder, \\(f_{Image}: I \\rightarrow Z_{Image}\\), and a text encoder, \\(f_{Text} : T \\rightarrow Z_{Text}\\), where \\(T\\) is a textual input, \\(Z_{Image} \\in \\mathbb{R}^D\\) is the image embedding, \\(Z_{Text} \\in \\mathbb{R}^D\\) is the text embedding, and \\(D\\) is the embedding dimension.\nIn the context of image captioning, the objective of the HiPS attack is to generate an adversarial image \\(I_{adv}\\) that is nearly indistinguishable from \\(I\\). However, when this adversarial image \\(I_{adv}\\) is processed by the downstream image captioning model, \\(f_{caption} : I \\rightarrow T\\), the generated caption \\(C_{adv} = f_{caption}(I_{adv})\\) should omit the target object \\(T_{target}\\) while accurately describing all other objects in the image. In other words, the adversarially generated caption, \\(C_{adv}\\), should closely resemble the caption \\(C_{orig}\\) produced from the original (unperturbed) image, with the exception that \\(T_{target}\\) is not mentioned. This approach contrasts sharply with traditional targeted attacks, where typically the goal is to produce an output that is significantly different from the correct one. While a traditional adversarial attack's goal is to make the perturbation imperceptible in the input space, in the HiPS attack, we want the difference in the output space to also be minimal - the only difference should be the omission of the target class. In this paper, we propose two different variants of the HiPS attack in the context of image captioning, which are detailed below"}, {"title": "HiPS-cls Attack using Class Labels", "content": "In this variant of HiPS attack, termed HiPS-cls, we utilize only the textual class labels \\(T_\\ell\\) to obtain the adversarial image. Given an image \\(I\\) and its corresponding set of class labels \\(T_\\ell\\), we compute the cosine similarity scores \\(S_i\\) for each class label \\(T_i\\) as follows:\n\\(S_i = cos(f_{Image} (I), f_{Text} (T_i)) = \\frac{<f_{Image} (I), f_{Text} (T_i)>}{|| f_{Image} (I) ||_2' || f_{Text} (T_i)||_2}\\)\n(1)\nThe cosine similarity \\(S_i\\) between the image \\(I\\) and class label \\(T_i\\) measures the alignment between their respective image and text embeddings. A higher score \\(S_i\\) indicates that the object with class label \\(T_i\\) is likely present in the image \\(I\\), while a lower score suggests its absence. Since the objective of the HiPS attack is to remove the target object \\(T_{target} = T_j\\), our goal is to perturb the image \\(I\\) in such a way that the cosine similarity score for the target object, \\(S_j\\), is reduced (as if it is absent), while the scores for all other objects \\(T_i\\) (for all \\(i \\neq j\\)) are either increased or remain unchanged. To formalize this, we define the HiPS-cls adversarial loss function as follows: \\(L_{HiPS-cls} = -\\lambda_1 S_j + \\lambda_2 \\sum_{i \\neq j} S_i\\)."}, {"title": "HiPS-cap Attack using Adversarial Captions", "content": "In this variant of the HiPS attack, termed HiPS-cap, rather than using class labels, we generate the attack on CLIP by utilizing the original caption \\(C\\) and a target caption \\(\\tilde{C}\\). The target caption \\(\\tilde{C}\\) is designed to be similar to \\(C\\), but as if the target object \\(T_{target}\\) were not present in the image. In other words, \\(\\tilde{C}\\) represents an ideal adversarial caption that a successful HiPS attack on a captioning model should produce. Similar to the HiPS-cls approach, we calculate the cosine similarities between the image \\(I\\) and both the original caption \\(C\\) and the target caption \\(\\tilde{C}\\) as follows:\n\\(S_c = cos(f_{Image} (I), f_{Text} (C));\\)\n\\(S_{\\tilde{c}} = cos(f_{Image} (I), f_{Text} (\\tilde{C}))\\)\n(2)\nThe corresponding adversarial loss can be computed as \\(L_{HiPS-cap} = -\\lambda_1S_c + \\lambda_2 S_{\\tilde{c}}\\). \\(L_{HiPS-cap}\\) aims to reduce the score for the original caption \\(S_c\\) while increase the score for the target caption \\(S_{\\tilde{c}}\\), where the target object is missing. The adversarial loss \\(L_{HiPS-cls}\\) and \\(L_{HiPS-cap}\\) can be optimized using existing adversarial attacks such as FGSM and PGD attacks (See Section 2)."}, {"title": "Experimental Setup", "content": "Setting: We develop HiPS-cls and HiPS-cap attacks using the CLIP model, where the vision encoder is based on Vision Transformer architecture (ViT-B/32) [26]. To generate adversarial images for the HiPS attack, we employ established techniques, including FGSM and PGD with \\(L_\\infty\\), \\(L_1\\), and \\(L_2\\) norms. For simplicity, we focus on images containing only two foreground objects: one serving as the target object to be removed, and the other as the object to be retained in the adversarial caption. We manually sampled 50 such images from the MS COCO dataset to test our two HiPS attack variants (cap vs. cls). For the HiPS-cap attack, we use the original COCO captions as \\(C\\), and manually generate two target (adversarial) captions, one used for training (\\(\\tilde{C}\\)), while the other one is reserved for evaluation. For the downstream captioning model, we utilize the CLIP-Cap [20] model. CLIP-Cap uses the vision encoder from CLIP and a mapping network to project the image embeddings into a shared representation space, where a language model (GPT-2) [27] generates the captions.\nEvaluation Metrics: In the context of assessing the success of HiPS attack, we introduce several novel metric to measure attack success, where we consider two main criterions. First, the ability to successfully remove references of the target object from the generated textual caption. We propose a metric called Target Object Removal Rate (TORR) to capture this using similarity-based assessments and string-matching comparisons between words. Second, the ability to measure if the remaining objects are intact to ensure that perturbation does not inadvertently affect or remove references to the objects other than the targeted one. We propose another metric called Remaining Objects Retention Rate (RORR) for this purpose. Next, we utilize Attack Success Rate (ASR) that measures if both of these criterions (TORR and RORR) are satisfied. We additionally utilize Caption Semantic Similarity (CSS) which is essentially the cosine similarity between the ground truth adversarial caption, and the generated adversarial caption (\\(cos(\\tilde{C}_{gt}, \\tilde{C}_{adv})\\)). CSS measures if the two are semantically close to each other in the text embedding space. Additional details of computation of TORR, RORR and ASR are provided in the Appendix. The image quality is another important metric to measure the imperceptibility of the attack. We use standard metrics such as Mean Squared Error (MSE), Mean Absolute Error (MAE), Signal-to-Noise Ratio (PSNR), and Structural Similarity metric (SSIM).\nBaselines: We compare against two PGD (\\(L_\\infty\\)) based attacks: targeted and untargeted. For the class-labels variant, in the PGD (targeted) setting, we set \\(\\lambda_1 = 1, \\lambda_2 = 0\\), focusing solely on removing the target object. In the PGD (untargeted) setting, we set \\(\\lambda_1 = 0, \\lambda_2 = 1\\), prioritizing the retention of all other objects in the image. For adversarial captions variant, we only use PGD (targetted) setting, where we optimize to maximize the similarity with target caption ( \\(\\lambda_1 = 0, \\lambda_2 = 1\\))."}, {"title": "Results", "content": "Quantitative Evaluation of HiPS-cls and HiPS-cap: In Tables 1 and 2, we compare the attack success and image quality metrics of the two HiPS variants, using FGSM and PGD under \\(L_\\infty\\), \\(L_1\\), and \\(L_2\\) norm constraints. We report results for the best-performing model in each case (see hyper-parameter settings in Appendix). FGSM performs poorly across both HiPS variants, achieving an ASR of only 36-38%. In contrast, the various PGD attacks demonstrate strong performance across both variants, with the \\(L_\\infty\\) variant slightly outperforming the \\(L_1\\) and \\(L_2\\) norms. Specifically, for the HiPS-cls attack, PGD achieves 100% RORR, indicating that the adversarial captions consistently"}, {"title": "Limitations, Discussion and Conclusion", "content": "In this work, we demonstrate promising results for the HiPS attack, showing that it is possible to generate small perturbations which cause subtle differences in the output of a downstream task. However, we recognize a few current limitations and future work needed to overcome them. First, the metrics we use to measure success are not always correct. For example, the rule based metrics (TORR and RORR) are biased towards only detecting the presence and absence of an object(s) from a text caption, and does not consider if the sentence is grammatically correct or if additional objects were added to the caption even though they do no exist in the image. In our small dataset this occurs infrequently but can skew the results more on a large dataset. In addition, we find that the cosine similarity metric is not precise enough to measure the small differences between cosine similarities of our caption since they are all very close to each other (by design). In the future, we plan on using a LMM to evaluate the results in a more accurate manner using custom prompts. In addition, in this work we our experiment was restricted to 50 images due to the manual annotations required for generating two adversarial captions for each image. We plan to conduct much larger experiments in the future by automating this process using LMMs, prompting the model to generate a caption with the target object missing. Finally, while in this work we focused on a single image captioning model, we believe that this attack can be used for other multimodal models (LLaVa, OpenFlamingo) as well as other downstream tasks (object detection, action recognition). The transferability of the attack can be improved using an ensemble of multimodal models to generate the attack."}, {"title": "Additional Details of Evaluation Metrics", "content": "Attack Success Rate (ASR) metric is an aggregated evaluation of the success of modifying textual captions to remove references to a target object while preserving mentions of remaining objects. The aggregation constitutes of two measures: (1) Target Object Removal Rate (TORR) and (2) Remaining Objects Retention Rate (RORR). TORR assesses whether references to a specific target object \\(T_{target}\\) are effectively removed from the caption generated after the HiPS attack \\(\\tilde{C}\\), measuring how well the perturbations obscure \\(T_{target}\\) from the model's point-of-view. RORR, on the other hand, evaluates whether references to the remaining objects \\(T_i\\) are preserved in \\(\\tilde{C}\\). This ensures that the perturbation does not inadvertently affect or remove references to the objects other than the targeted one.\nWe calculate the TORR and RORR metrics in two steps: (i) word segmentation and cleaning, (ii) semantic presence validation.\n(i) Word segmentation and cleaning: To identify object references within \\(\\tilde{C}\\), we tokenize words using spaCy [28], yielding a list of words \\(W_{\\tilde{c}}\\). We assume that specific words in \\(W_{\\tilde{c}}\\) correspond to object representations. We remove stop words and punctuation as they do not contribute to our evaluation schema and help streamline the analysis by focusing on meaningful words that are critical to understanding the content of \\(\\tilde{C}\\). We convert plurals in \\(W_{\\tilde{c}}\\) to singular forms using the inflect engine in Python. This normalization aids in matching terms more effectively, when compared to \\(T_{target}\\) and \\(T_i\\), during semantic presence validation. Finally, we filter \\(W_{\\tilde{c}}\\) based on Part-of-Speech tags using spaCy. We exclude determiners (DET) and pronouns (PRON) from our analysis as they do not bear any significance to our analysis. This extensive processing within this step is integral to transforming \\(\\tilde{C}\\) into a refined set of lexical units that accurately represent its meaningful content. It allows the ASR metric to perform precise evaluations of object presence, enhancing the accuracy and validity of the analysis and reducing the risk of misinterpretations due to irrelevant or misleading text components.\n(ii) Semantic presence validation: Provided \\(W_{\\tilde{c}}\\) from step (i), we verify the absence of \\(T_{target}\\) and the presence of \\(T_i\\). This step involves both direct presence checks and similarity-based assessments. For direct presence check, we perform string-matching comparisons between \\(W_{\\tilde{c}}\\) and \\(T_{target}\\), as well as between \\(W\\) and \\(T_i\\). If the direct presence check does not yield a clear result, we employ cosine similarity between word embeddings to further validate the success of \\(T_{target}\\) removal and \\(T_i\\) retention. Using an empirically established similarity threshold (0.7 in this case), we determine the boundary for distinguishing between successful and unsuccessful removal/retention of the objects. Our ASR metric offers multiple options for obtaining word embedding, including Word2Vec [29], GloVe [30], FastText [31], and BERT [32], during semantic presence validation, enhancing its adaptability and effectiveness across various downstream tasks and models. For our specific application, we found GloVe to be the most effective choice.\nThe ASR metric, while robust for many scenarios, encounters challenges when dealing with multi-word objects, such as \"teddy bear.\" In these cases, the metric may struggle to effectively assess the presence or removal of the entire phrase because it traditionally operates on individual word embed-dings. Our workaround for this limitation involves averaging the embeddings for each word within the multi-word phrase. Furthermore, the metric's reliance on cosine similarity thresholds may not fully account for the nuanced differences between conceptually related but distinct objects and vice-verse. For example, while \"hills\" and \"mountains\" are closely related, they are not interchangeable. Our experiments show that the ASR metric might fail to recognize this subtle distinction, leading to rare but incorrect assessments of object removal success."}, {"title": "Hyperparameter Details", "content": "We have used \\(\\lambda_2 = 1\\), in all of our experiments. Additional details of other hyperparameters are provided in Table 3."}, {"title": "Additional Results", "content": "We present some additional results on image quality, TORR, and RORR for the effect of attack budget and hyperparameter sensititivity."}]}