{"title": "Event USKT : U-State Space Model in Knowledge Transfer for Event Cameras", "authors": ["Yuhui Lin", "Jiahao Zhang", "Jimin Xiao", "Siyuan Li", "Ding Xu", "Wenjun Wu", "Jiaxuan Lu"], "abstract": "Event cameras, as an emerging imaging technology, offer distinct advantages over traditional RGB cameras, including reduced energy consumption and higher frame rates. However, the limited quantity of available event data presents a significant challenge, hindering their broader development. To alleviate this issue, we introduce a tailored U-shaped State Space Model Knowledge Transfer (USKT) framework for Event-to-RGB knowledge transfer. This framework generates inputs compatible with RGB frames, enabling event data to effectively reuse pre-trained RGB models and achieve competitive performance with minimal parameter tuning. Within the USKT architecture, we also propose a bidirectional reverse state space model. Unlike conventional bidirectional scanning mechanisms, the proposed Bidirectional Reverse State Space Model (BiR-SSM) leverages a shared weight strategy, which facilitates efficient modeling while conserving computational resources. In terms of effectiveness, integrating USKT with ResNet50 as the backbone improves model performance by 0.95%, 3.57%, and 2.9% on DVS128 Gesture, N-Caltech101, and CIFAR-10-DVS datasets, respectively, underscoring USKT's adaptability and effectiveness. The code will be made available upon acceptance.", "sections": [{"title": "1. Introduction", "content": "Event cameras represent a novel imaging technology that differs fundamentally from traditional frame-based cameras by capturing changes in brightness at the pixel level continuously, rather than capturing entire frames at regular intervals. The unique mechanism provides event cameras with exceptionally high temporal resolution and minimal latency, making them particularly well-suited for capturing fast-moving activities and handling scenes with high dynamic range [20, 50]. Compared to traditional cameras, event cameras excel in environments with significant lighting variations, while also consuming less energy, making them highly promising for applications such as autonomous driving [3], robotic navigation [47], and high-speed motion capture [21]. However, as a relatively new imaging modal-"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Event-based Image Recognition", "content": "Event image recognition predominantly include graph-based models, Spiking Neural Networks (SNNs), and attention mechanisms. Graph-based models, using vertex and edge structures along with heterogeneous graph models and voxel grids, emulate spatial and temporal relationships among events and analyze complex data patterns, as demonstrated in various studies [14, 37, 44, 59, 63, 64, 70]. Spiking Neural Networks (SNNs) excel in processing time-step sequences for event image classification and, when integrated with attention mechanisms, significantly improve object recognition in dynamic environments by managing asynchronous data and focusing on critical features [16, 18, 19, 53, 69, 73, 75-77]. Additionally, some attention-based methods have also been widely used [13, 22, 30, 34, 40].\nAmong these methods, while tailored for event cameras, fail to address data scarcity. As a result, some studies have shifted to training with RGB-based models to mitigate this issue. For instance, several approaches based on ResNet have utilized RGB information to enhance the representational capability of event data [12, 33], while other methods have employed pre-trained ViT models based on RGB to improve the handling of sparse event streams [62]. Additionally, methods that integrate RGB and event camera data have proven to noticeably enhance the performance of downstream tasks [68]."}, {"title": "2.2. Knowledge Transfer", "content": "In knowledge transfer, most approaches focus on RGB-to-RGB transfer. Domain adaptation methods align feature distributions between source and target domains. The PMC method enhances cross-modal recognition by generating missing target domain modalities through multimodal collaboration [72]. CLDA and MAJA mitigate domain shift via adversarial learning, boosting classification accuracy, especially in unsupervised scenarios [26, 79]. The DARDR method enhances cross-domain recognition by ap-"}, {"title": "3. Method", "content": ""}, {"title": "3.1. Overview", "content": "We propose a U-shaped State Space Model Knowledge Transfe (USKT) framework that efficiently converts event data into RGB features. As shown in Fig 2, the model consists of three key components: event data processing that transforms multiple time steps into voxel information to capture dynamic data changes; a Residual Down Sampling Block that reduces sequence length for efficient feature extraction; and a Residual Up Sampling Block that reconstructs the features into RGB domain suitable for encoder inputs. Additionally, we introduce a Bidirectional Reverse State Space Model (BiR-SSM) to fully capture the sequential dependencies between features. Finally, we focus on the performance of the Residual Up Sampling Block's output $X_{USKT}$ after it passes through the feature extractor and present the design of the hybrid loss function."}, {"title": "3.2. Event Data Processing", "content": "An event stream can be visualized as consisting of multiple events, each characterized by $(x, y, t,p)$, where $(x, y)$ represent spatial coordinates, $t$ denotes the timestamp, and $p$ indicates the polarity (+1 or -1, signifying an increase or decrease in brightness). Consequently, event data is mapped into a three-dimensional grid where $(x, y)$ serve as the spatial dimensions and the time dimension is segmented into discrete bins, effectively organizing the event data temporally. Furthermore, based on the $(x, y)$ coordinates and the calculated time bin $k$, the event polarity $p$ is accumulated in the respective voxel within the grid. Each voxel"}, {"title": "3.3. Generative U-SSM Knowledge Transfer", "content": "Generative-based methods have been widely applied in the field of knowledge transfer across various tasks [2, 54, 67], and U-Net-based architectures have shown promising results in generative tasks [17, 66]. Building on these advances, we propose the U-SSM Knowledge Transfer (USKT) block for event-to-RGB knowledge transfer. Specifically, we input the event data $X_{input} \\in \\mathbb{R}^{T \\times 224 \\times 224}$, where $T$ represents the time steps. Using a convolutional layer, we map the input data to 12 dimensions, standardizing the time steps of the event camera. The convolution operation can be expressed as:\n$X_{proj} = Conv(X_{input}),$ (1)\nU-shaped models are highly effective for knowledge transfer, primarily due to the essential roles of their downsampling and upsampling modules. Downsampling modules compress data by reducing feature sizes and increasing dimensionality [24, 43], whereas upsampling modules expand features and retain detailed information necessary for reconstruction [10, 55]. However, traditional U-shaped approaches, typically designed for RGB data, may not directly translate to event data, which primarily captures changes in brightness. The mismatch can lead to overfitting. Moreover, the inherent sparsity of event data necessitates a departure from conventional downsampling techniques; therefore, we incorporate residual connections to maintain the integrity of the original features. To address these challenges, we introduce the Residual Down Sampling Block and Residual Up Sampling Block for effective downsampling and upsampling, respectively. As illustrated in Figure 2, the proposed framework employs 4 Residual Down Sampling Blocks and 5 Residual Up Sampling Blocks.\nResidual Down Sampling Block. For the Block, the input feature $X_{proj} \\in \\mathbb{R}^{D \\times N \\times N}$ undergoes a series of operations. First, a convolution operation is applied to extract global features, resulting in $X_{conv1} \\in \\mathbb{R}^{F \\times N \\times N}$. Next, another convolution focuses on feature downsampling, producing $X_{conv2} \\in \\mathbb{R}^{F \\times N/2 \\times N/2}$. Simultaneously, the original input feature is downsampled directly through a convolution layer, yielding $X_{res} \\in \\mathbb{R}^{F \\times N/2 \\times N/2}$. A residual connection is then applied, resulting in $X_{down} \\in \\mathbb{R}^{F \\times N/2 \\times N/2}$. The Residual Down Sampling Block preserves essential features while reducing the spatial dimensions of the data.\nMeanwhile, in our method, the input is $X_{proje} \\in \\mathbb{R}^{12 \\times 224 \\times 224}$ and the outpput is $X_{down} \\in \\mathbb{R}^{128 \\times 14 \\times 14}$ se-"}, {"title": "3.4. Bidirectional Reverse State Space Model", "content": "In this section, we focus on the application of a bidirectional reverse state space model for sequence modeling, as illustrated in Figure 3. While Transformer-based methods offer substantial benefits for sequence modeling, their quadratic computational complexity often limits their performance [27, 51]. To address this, we conduct sequence modeling after the Residual Down Sampling Block, which enables efficient processing while preserving essential feature information. Notably, previous bidirectional state space models typically relied on two separate SSM layers [78], as depicted in Figure 3. We believe this design can be optimized to improve parameter efficiency without sacrificing model performance.\nAfter the Residual Down Sampling Block, we flatten the 2D data into a 1D sequence and then employ the Bidirectional Reverse State Space Model to process the downsampled data, represented as $X_{down}$. The sequence is then processed through a linear layer, a convolutional layer, and a State Space Model (SSM) layer. To retain original information, the output from the SSM undergoes a residual connection with the original sequence. The downsampled data $X_{down}$ is represented as a set of features $\\{P_1,P_2,...,P_n\\}$ where each $p_n$ is an element of $X_{down}$, as shown in the following formula:\n$X_{Conv} = Conv(Linear(X_{down})),$ (2)\nwhere $X_{Conv}$ is obtained after passing through a linear layer and a convolutional mapping.\nSimultaneously, when we obtain the result of SSM+, we apply a SiLU function to the output of SSM+ after processing through $X_{Conv}$, as shown in the following formula:\n$X_{SSM^+} = SiLU(SSM^+(X_{Conv})),$ (3)\nwhere the forward SSM+ modeling is applied to the features to obtain $X_{SSM^+}$.\nThe result of SSM+ is then reversed, represented as a set of features $\\{P_n, ..., P_2, P_1 \\}$ where each $p_1$ is an element from $X_{SSM^-}$, to facilitate the subsequent SSM- processing. Finally, the sequence passes through SSM\u00af, as shown in the following formula:\n$X_{SSM^-} = SiLU(SSM^-(X_{SSM^+})),$ (4)\nwhere SSM modeling is applied to the features.\nWe apply a residual connection between $X_{SSM^-}$ and $X_{down}$, producing $X$. Then, we reverse the resulting sequence to restore the original structural arrangement, $\\{P_1,P_2,..., P_n\\}$ where each $p_n$ is an element of $X_{SSM}$, the reversed form of $X$.\n$X = Linear(X_{SSM^-} + SiLU(Linear(X_{down}))),$ (5)\nwhere a residual connection is used to combine $X_{SSM^-}$ and $X_{down}$ to mitigate feature loss."}, {"title": "3.5. Reconstruction and Classification", "content": "Feature Extraction. We use the ResNet [23] as pre-trained RGB Encoder for feature extraction. Unlike traditional ResNet applications, we utilize the adaptive output from USKT as the input for Encoder. After feature extraction through Encoder, the feature matrix $X_{res} \\in \\mathbb{R}^{D \\times 7 \\times 7}$. Subsequently, as shown in Fig 2 through decoder, the features are mapped back to the original space, ultimately resulting in an output $X_{rec} \\in \\mathbb{R}^{3 \\times 224 \\times 224}$. In our method, our decoder employs a deconvolution approach.\nLoss Function. In our proposed method, we primarily used two types of loss functions. For the classification, we applied the Focal Loss to the classification results from the linear layer, is defined as:\n$L_{cls} = -a_t(1-p_t)^{\\gamma} log(p_t),$ (6)\nwhere $p_t$ is the predicted probability for the correct class $t$, $a_t$ balances positive and negative samples, and $\\gamma$ focuses on hard-to-classify sample.\nFor the reconstruction part, we use the Mean Squared Error (MSE) loss function to compare the reconstructed features $X_{rec} \\in \\mathbb{R}^{3 \\times 224 \\times 224}$ and $X_{USKT} \\in \\mathbb{R}^{3 \\times 224 \\times 224}$, is defined as:\n$L_{rec} = \\frac{1}{N} \\sum_{i=1}^{N} (X_{rec_i} - X_{USKT_i})^2,$ (7)\nwhere $N$ is the total number of elements, and $i$ indexes the elements.\nFinally, we combine $L_{1}$ and $L_{2}$ to compute our total loss $L$, defined as:\n$L = \\lambda_1 L_{cls} + \\lambda_2 L_{rec},$ (8)\nwhere $\\lambda_1$ and $\\lambda_2$ are the weights for the classification loss and reconstruction loss, respectively."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Setup", "content": "Dataset. We utilize the ImageNet-1K dataset [11] for pre-training our models. In our experiments, we compare SimCLR, MoCo-v2, and MoCo-v3, all pretrained on both ImageNet-1K [11] and N-ImageNet [32]. Furthermore, we extend our knowledge transfer activities to the DVS128 Gesture [1], N-Caltech101 [48], and CIFAR-10-DVS [9] datasets to assess the generalization capabilities of our models across various domains. Additionally, we adapt the input by resizing images to a resolution of 224x224 pixels.\nDVS128 Gesture [1] consists of 1,188 event streams from 29 participants, categorized into 11 gesture types, with each event stream featuring a resolution of approximately 128x128 pixels. N-Caltech101 [48] comprises a total of"}, {"title": "4.2. Comparison with Existing Methods", "content": "Compared to RGB-based Supervised Methods. In the non-pretrained models, our method achieved significant improvements on the DVS128 Gesture, N-Caltech101, and CIFAR-10-DVS datasets compared to VIT-S/16 and ResNet50. Specifically, our method outperformed VIT-S/16 by 29.17%, 33.19%, and 24.3%, and surpassed ResNet50 by 16.58%, 26.13%, and 20.1% respectively. For the pre-trained models, our method outperformed VIT-S/16 by 17.05% on DVS128 Gesture, 3.80% on N-Caltech101, and 0.6% on CIFAR-10-DVS. It demonstrates that our experiments show significant performance improvements under both pretrained and non-pretrained supervised conditions.\nCompared to RGB-based Unsupervised Methods. We used a frozen ResNet50 backbone to compare with traditional RGB unsupervised methods. On the DVS128 Gesture, our frozen model can outperform many unsupervised models, surpassing SimCLR [6] and MoCo-v3 [8] by 0.76% and 2.65%, respectively. On the N-Caltech101, our model also outperforms many unsupervised models, surpassing SimCLR [6] and MoCo-v2 [7] by 2.25% and 4.66%, respectively. On the CIFAR-10-DVS, our model also surpasses many unsupervised models, exceeding SimCLR [6] and MoCo-v2 [7] by 1.6% and 2.1%, respectively. In the unfrozen condition, on the DVS128 Gesture, our model can surpass MoCo-v2 [7] by 2.10%. Therefore, compared to traditional RGB unsupervised methods, our model demonstrates significant advantages.\nCompared to SNN methods. Due to the effective handling of event information by SNNs in event camera classification tasks, our method(frozen) was compared with SNN-based methods. On the DVS128 Gesture, our model showed significant advantages over other advanced SNN-based methods, not only outperforming Spikformer [38] by 5.71% but also achieving a comparable level to MLF [19]. Similarly, on the N-Caltech101, our model excelled, surpassing Spikformer [38] by 15.99%. Furthermore, on the CIFAR-10-DVS, our model further demonstrated its superiority, outperforming Spikformer [38], MLF [19], and"}, {"title": "4.3. Abaltion Studies", "content": "In this section, we address three key issues: Firstly, we examine the applicability of our proposed USKT Block to various sizes of ResNet models. Secondly, we assess the effectiveness of the USKT Block in enhancing model performance. Thirdly, we conduct a comparative analysis of the BiR-SSM Block.\nAdaptability of USKT. As shown in Table 2, we evaluated the performance of our proposed USKT across different sizes of ResNet on the DVS128 Gesture, N-Caltech101, and CIFAR-10-DVS datasets to validate the applicability of USKT to various ResNet architectures.\nInitially, we conducted experiments with the backbone network frozen (with only the bias parameters of ResNet unfrozen). Using ResNet18 as the backbone, the integration of USKT resulted in performance improvements of 2.94%, 2.12%, and 3.45% on the DVS128 Gesture, N-Caltech101, and CIFAR-10-DVS datasets, respectively. With ResNet34 as the backbone, USKT enhanced the model's performance by 2.24%, 1.8%, and 2.1% on these respective datasets. When employing ResNet50, the addition of USKT led to gains of 0.95%, 3.57%, and 2.9%.\nFurther, we evaluated the performance on the N-Caltech101 with the backbone network completely unfrozen. In this dataset, adding USKT improved the model's performance by 1.2% with ResNet18, 0.49% with ResNet34, and 2.58% with ResNet50 as the backbone.\nillustrates that our method achieves the most substantial improvements with the ResNet50 backbone, irrespective of the network's state (frozen or unfrozen). Its superior performance is likely attributable to ResNet50's enhanced capability to extract richer fine-grained information from images compared to the ResNet18 and ResNet34 models.\nEffectiveness of USKT. As illustrated in Table 3, we conducted comparative evaluations between convolution and Transformer-based methods to validate the effectiveness of our proposed USKT. Initially, we substituted USKT with convolutional layers to assess the adaptive capabilities of our approach. The experiments were executed with a frozen ResNet50 backbone. Our model demonstrated improvements of 2.6% and 2.94% over single and double convolution layer setups, respectively.\nFurthermore, to rigorously assess the efficacy of our proposed BiR-SSM, we carried out comparative experiments under both frozen and unfrozen conditions of the ResNet50 backbone, where BiR-SSM was replaced with a Transformer module. Under the frozen condition, our method exceeded the performance of the Transformer-based methods by 2.14%, achieving results comparable to those of the unfrozen backbone Transformer. Remarkably, even with fewer parameters in the unfrozen state, our approach not only matched but surpassed the Transformer-based model by 2.15%.\nComparision of BiR-SSM Block. As shown in Table 4, we have frozen the ResNet50 backbone and substituted the original SSM with our novel BiR-SSM in various configurations. It can be concluded that our proposed BiR-SSM outperforms the traditional Bi-SSM. This enhancement is likely attributable to the improved data consistency achieved through the shared SSM mechanism that we implemented."}, {"title": "4.4. Hyperparameter Studies", "content": "This section first discusses the impact of different numbers of BiR-SSM layers on the model, followed by an analysis of different $\\lambda_2$ affect model performance.\nComparison of Different Number of BiR-SSM Layers. As demonstrated in Figure 4, employing a single SSM layer yields the highest accuracy, surpassing the configurations where no BiR-SSM layers or multiple BiR-SSM layers are used. In our setup, we utilized ResNet-50 as the backbone with the main network components frozen. Specifically, with one BiR-SSM layer, our method achieved an accuracy of 88.82% on the N-Caltech101 dataset. Similarly, this configuration attained accuracies of 88.62% on the DVS128 Gesture and 76.75% on CIFAR-10-DVS. We hypothesize that the absence of any BiR-SSM layers causes the adaptive domain to predominantly focus on local information, thereby neglecting global context. Conversely, incorporating more than one BiR-SSM layer can lead to overfitting or an excessive emphasis on the classification task, potentially compromising the model's performance.\nComparison of Different $\\lambda_2$ Settings. As depicted in Figure 4, varying the parameter $\\lambda_2$ significantly influences the performance of our model. In our evaluations, we employed a frozen ResNet-50 architecture as the backbone, and set $\\lambda_1$ to 1. The results indicate optimal performance when $\\lambda_2$ is set to 0.05, with the model achieving an accuracy of 88.82% on the N-Caltech101 dataset. Similar ef-"}, {"title": "5. Conclusion", "content": "In this paper, we introduce the USKT framework to tackle the challenge of limited event data in event-based imaging by facilitating effective Event-to-RGB knowledge transfer. The framework allows event data to leverage pre-trained RGB models with minimal tuning, achieving robust performance. Our BiR-SSM component, with its shared weight strategy, further enhances computational efficiency. Experimental results across multiple datasets demonstrate USKT's adaptability and effectiveness in advancing event-based imaging."}]}