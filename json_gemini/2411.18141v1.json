{"title": "Predicting Water Quality using Quantum Machine Learning: The Case of the Umgeni Catchment (U20A) Study Region", "authors": ["Muhammad Al-Zafar Khan", "Jamal Al-Karaki", "Marwan Omar"], "abstract": "In this study, we consider a real-world application of QML techniques to study water quality in the U20A region in Durban, South Africa. Specifically, we applied the quantum support vector classifier (QSVC) and quantum neural network (QNN), and we showed that the QSVC is easier to implement and yields a higher accuracy. The QSVC models were applied for three kernels: Linear, polynomial, and radial basis function (RBF), and it was shown that the polynomial and RBF kernels had exactly the same performance. The QNN model was applied using different optimizers, learning rates, noise on the circuit components, and weight initializations were considered, but the QNN persistently ran into the dead neuron problem. Thus, the QNN was compared only by accraucy and loss, and it was shown that with the Adam optimizer, the model has the best performance, however, still less than the QSVC.", "sections": [{"title": "Introduction", "content": "Water quality assessment remains a critical challenge in environmental monitoring and public health protection. Traditional machine learning (ML) approaches have demonstrated success in predicting water quality parameters [1-8], yet they often struggle with the complex, nonlinear relationships inherent in aquatic systems. Quantum Machine Learning (QML) offers promising advantages through its ability to efficiently process high-dimensional data and exploit quantum phenomena like superposition and entanglement. The utility of QML has been demonstrated in various applications related to financial fraud detection [9-13], drug discovery [14-17], chemistry and medical applications [18-21], materials science [22-24], and many other areas.\nThis research explores the application of QML algorithms to water quality prediction, specifically implementing the quantum support vector classifier (QSVC) and quantum neural networks (QNNs). We demonstrate that quantum approaches can capture subtle correlations between water quality indicators, including the presence of various chemical compounds in the water (NH3, NO2, NO3, SO4 and others), as well as sediment trappings, flow rates, flood attenuation, chemical assimilation (phosphates, nitrates, toxicants), and other factors like turbidity. Our methodology leverages both classical preprocessing techniques and quantum feature maps to encode water quality data into quantum states, followed by measurement-based prediction. The results indicate that QML models can achieve comparable or superior accuracy to classical methods while requiring fewer training parameters and computational resources.\nMany studies enlist the potential advantages that QML has over classical ML, so we choose to avoid a restatement here. However, we should remember that we strongly believe that QML is an experimental science. Therefore, the advantages of QML over classical ML can only be realized once a particular application is explored, as we have seen in our study.\nWe consider the real-world study area from where this data was collected: The City of Durban, situated in KwaZulu-Natal on the east coast of South Africa, has a rich and currently developing history of Quantum Computing, arguably being the first city in South Africa that envisioned the potential of QC and its applications. Possibly, the doyen of QC in South Africa, Francesco Petruccione, introduced this field via the Quantum Research Group at the University of KwaZulu-Natal, and this was further solidified when, together with his then student, Maria Schuld, they co-authored the famous two-part magnum opus on Quantum Machine Learning (QML) [25, 26]. Thus, it is almost fairytale-like that we use a technology pioneered in this city to study the quality of water in the city.\nIn recent years, the quality of water in the Durban area has supposedly been in the decline due to various factors, including accusations of illegal dumping of sewerage into the beaches, amongst others, therefore affecting sanitation, consumption, tourism - a major source of income - in the city and the province [27-29].\nThe foremost objective of this research is to approach the issue of water quality from a non-political, unbiased, physical sciences perspective and apply the techniques of QML to predict whether water quality is good or bad for use. The term use is used broadly to mean leisurely activities such as swimming."}, {"title": "Related Work", "content": "To our knowledge, there exists no other study that directly applies QML for the study of water quality, thus, our claim is that the application is completely new. Therefore, no direct literature applies to our study, but analogous studies that collected similar data to study weather phenomena and considered applications in agriculture, amongst others.\nIn [30], the authors use the Wupper River in Germany as a study area to build QML models to better understand flood forecasting during the period of 2023. By comparing classical and quantum ML models, the authors try to show the benefit of using QML. While this paper does not technically predict water quality, it shares some parallels with our study in the sense of using numerical variables related to water and choosing a demarcated study area. Similarly, in [31], the authors apply a novel QML model that combines long short-term memory (LSTM) models with QNNs to produce Quantum Trains (QT) and study flood prediction as an application. The novelty of their method lies in the manner in which the QT model reduces the number of trainable parameters.\nIn [32], the authors explore the applications of QSVCs and QNNs to enhance crop yields. By using data related to water quality and soil collected over a long period, the authors could show that QML has the potential to improve crop yields, and the QML models showed superior performance to classical methods. Similarly, in [33], another application of QML to crop yield, in this case rice, is considered. The model was highly accurate regarding the MSE, MAE, and coefficient of determination.\nThere are several other application-based studies that explored the aforementioned areas, however, in essence QML techniques were applied to data and the quantum advantage was demonstrated by reporting high metrics."}, {"title": "Theory", "content": "In this section, we present the theories underlying the models we use in this study."}, {"title": "Quantum Support Vector Classifiers", "content": "Support Vector Machines (SVMs) work by finding the optimal separating hyperplane that segregates data points of different classes with the maximum margin. For a dataset D = {xi, Yi}=1 with x = (x1,x2,...,xn) being the features set, and y being the predictor variable, we aim to solve the optimization problem\n$\\min\\limits_{w} {\\frac{1}{2} {||w||}^2 + C \\sum\\limits_{i=1}^n {\\xi_i}}$\n$\\text{s.t.} \\quad y_i (w^T \\varphi(x_i) + b) \\ge 1 - \\xi_i, \\quad \\xi_i \\ge 0,$\nwhere w are the weights, C\u2208 [0, 1] is the control parameter, \u03be\u1d62 are the slack variables for misclassifications, and \u03c6 is the kernel which maps the features to the higher-dimensional feature space. In Fig.1, we provide a two-dimensional rendition of the operation of an SVM. By introducing the Lagrange multiplier \u03b1\u1d62 we re-state Eqn.(1) in the form\n$\\max_{\\alpha} {\\sum\\limits_{i=1}^n {\\alpha_i} - \\frac{1}{2} \\sum\\limits_{i=1}^n {\\sum\\limits_{j=1}^n {\\alpha_i \\alpha_j y_i y_j K(x_i, x_j)} } }$\n$\\text{s.t.} \\quad \\sum\\limits_{i=1}^n {\\alpha_i y_i} = 0, \\quad 0 \\le \\alpha_i \\le C.$\nThe feature map \u03c6(x), which takes the data to a high-dimensional space using the kernel\n$K(x_i, x_j) =\\begin{cases}x_i^T x_j, \\text{for linear kernels},\\\\(\\beta x_i^T x_j + r)^D, \\text{for polynomial kernels},\\\\ \\exp(-\\beta{||x_i - x_j||}^2), \\text{for radial basis function (RBF) kernels},\\end{cases}$\nwhere \u03b2 is the scaling parameter, r is the coefficient, and D is the polynomial degree.\nSpecifically, classical data xi is mapped into quantum states |\u03c8(xi)\u27e9 using a feature map scheme. Thereafter, the quantum kernel\n$K(x_i, x_j) = |\\langle \\psi(x_i) | \\psi(x_j) \\rangle |^2,$\nis calculated. The purpose of the quantum kernel is to measure the similarity between pairs of quantum states. The feature map is implemented using a VQC with quantum gates. By running the quantum circuit and measuring the outcomes, K is calculated. Thereafter, the classical SVM method is applied to find the optimal hyperplane that separates the quantum states in the high-dimensional feature space, and data points are classified accordingly."}, {"title": "Quantum Neural Networks", "content": "Quantum Neural Networks (QNNs) are the QML analogs of classical NNs that leverage quantum properties to process data. They have been shown to be particularly well suited for tasks in ML, optimization, and signal processing, with potential advantages in speed and expressive power over classical counterparts.\nIn practice, a typical QNN consists of the following architecture components:\n1. Classical-to-Quantum Data Encoding: Classical data is encoded into a quantum state. Mathematically, a classical datapoint xi is encoded into a quantum state |\u03c8(xi)\u27e9. The commonly used encoding schemes are\n$\\begin{aligned} &|\\varphi_i\\rangle = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} x_{i}|i\\rangle, \\text { with } \\sum_{i} |x_{i}|^{2}=1, \\text { amplitude encoding, } \\\\ &|\\psi_i\\rangle=\\frac{1}{\\sqrt{2}}\\left(\\cos x_{i}|0\\rangle+\\sin x_{i}|1\\rangle\\right), \\text { angle encoding. }\\end{aligned}$\n2. Parametrized/Variational Quantum Circuit (PQC/VQC): A sequence of quantum gates with trainable parameters analogous to weights in classical networks. The VQC is the beating heart of the QNN and is composed of a collection of quantum gate operations such as rotations and control gates. For example, for rotations R\u2081(\u03b8) = exp(-\u03b9\u03b8\u03bd/2). The VQC is then a matrix that operates on the"}, {"title": "Results", "content": "The data comprised 32 data points representing 32 locations on the map where these variables were measured. The feature E.coli - (MPN/100mL) measures the amount of Escherichia coli bacteria in the water at that measurement point. If the amount was at most 235 MPN/100 mL then it was acceptable, if it exceeded this threshold it was regarded as unacceptable. Using this methodology, a predictor variable Acceptable / Not Acceptable (For Recreation) was created. In Fig.3, we depict the study region considered."}, {"title": "QSVC", "content": "From Tab.1, we see that while with the linear kernel, the model achieves a perfect recall (1.0000), its accuracy, precision, and other metrics are relatively lower. This suggests that the linear kernel might not be the best choice for this specific dataset. Both kernels perform similarly well, with high accuracy, F1-score, precision, recall, AUROC, and AUPRC. This indicates that these kernels are better suited for capturing the underlying patterns in the data. The reason why the poly and RBF kernels produce exactly the same metrics may be attributed to several factors, including linear separability in the data. However, we see that we get a lower accuracy with the choice of a linear kernel; secondly, it could indicate that the model hyperparameters for both or either of the models are not properly fine-tuned, i.e., for the poly kernel, the choice of the degree of 1 might be too simplistic. For the RBF kernel, the choice of the complexity of the decision boundary \u2192 0 might not be optimal. Of course, one may experiment with different parameter values and obtain the optimal choice, but this is beyond the scope of our research objectives in this paper."}, {"title": "QNN", "content": "Upon the initial training of the QNN, the model ran into the \"dead neuron problem\", i.e., all neurons were giving the same output irrespective of the input. After 50 epochs of training, the model continuously gave a loss of 0.4996. This problem is attributed to two issues\n1.  The ReLU function in the intermediate layers kept \"dying\" potentially due to a too high learning rate selected.\n2.  The network cannot learn due to exploding/vanishing gradients.\nIn the next iteration of model training, the architecture was adjusted and the abovementioned issues were addressed. Specifically, the learning rate was adjusted from 0.1 to 0.001, and the weight initialization was changed to the Xavier initialization\n$w \\sim U(-x,x), \\quad x = \\sqrt{\\frac{6}{fan_{in} + fan_{out}}},$\nwhere U is a uniform distribution, fanin is the number of input units to a layer, and fanout is the number of output units to a layer.\nDespite these adjustments, the QNN model still gave a constant loss throughout all epochs. Lastly, a noisy QNN model was trained using depolarization noise and amplitude damping, and the same result was obtained. Thus, it does not make sense to compare model metrics like F1, precision, and recall because they all had a constant value of 0. Rather, we compare models by their accuracy and loss in Tab.2.\nThe depolarizing noise represents the loss of quantum coherence by replacing the quantum state with a mixed state. For a single qubit, with a probability p, the state becomes completely mixed, and the density matrix becomes\n$\\rho \\rightarrow(1-p) \\rho+p \\frac{I}{2^{I}},$\nwhere \u03c1 is the density matrix, p is the depolarizing probability, and I is the identity matrix. This noise applies equally to all directions in the Bloch sphere, mimicking errors from imperfect gate implementations or environmental interactions.\nAmplitude damping models energy loss in a system, such as a qubit decaying from the excited state (|1\u27e9) to a ground state (|0\u27e9). For a single qubit, the Krauss operators are given by\n$E_{0}=\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & \\sqrt{1-\\gamma}\\end{array}\\right) E_{1}=\\left(\\begin{array}{ll}0 & \\sqrt{\\gamma} \\\\ 0 & 0\\end{array}\\right),$\nwhere \u03b3 : |1\u27e9 \u2192 |0\u27e9 is the probability of relaxation, i.e., the probability of an excited state decaying to the ground state.\nBased on the QNN model, we can observe that the Adam optimizer model had the smallest loss and highest accuracy (models that used gradient descent and RMSprop had equal accuracy but reported a higher loss). However, the QNN approach is inherently flawed because all other metrics are 0. Thus, the raw data alone is not sufficient to build an effective QNN, and therefore, feature engineering would be required if one were to adopt the QNN approach effectively.\nIt is worth noting that if more complexity is introduced into the model, for example, by increasing the number of hidden layers in the network, we might get better performance. However, we speculate that the enhancement in performance will be marginal, and for practical purposes, the QSVC would be the best choice."}, {"title": "Conclusion", "content": "We observe that for pragmatic considerations, the QSVC is the best approach for this dataset because it is easy to implement, has good model metrics, and produces comparatively high accuracy with minimum effort. Unlike some studies that benchmark QML models against classical ML models, we believe that such a comparison is unwarranted and comparable to the age-old adage of comparing \"apples to bananas.\", and thus we avoided building classical ML models to compare; we have compared quantum models with quantum models.\nFrom the perspective of QML, in the future, we will explore different models and fine-tune the current models implemented to obtain better performance.\nFurther, in future iterations, we will consider a larger study area to collect more data points and different choices of class-balancing techniques to ascertain which gives optimal performance. In addition to only considering recreation purposes, new models will be built for drinking water quality.\nLastly, we will consider integrating geographical weighting into the existing QSVC and QNN models, for example, like the work in [34], thereby creating a new class of models and proving some results where we show that geographically weighted QNNs have smaller errors, in classifications tasks than standard QNNs."}]}