{"title": "DIFFUSION AUGMENTED AGENTS: A FRAMEWORK FOR\nEFFICIENT EXPLORATION AND TRANSFER LEARNING", "authors": ["Norman Di Palo", "Leonard Hasenclever", "Jan Humplik", "Arunkumar Byravan"], "abstract": "We introduce Diffusion Augmented Agents (DAAG), a novel framework that leverages large language\nmodels, vision language models, and diffusion models to improve sample efficiency and transfer\nlearning in reinforcement learning for embodied agents. DAAG hindsight relabels the agent's past\nexperience by using diffusion models to transform videos in a temporally and geometrically consistent\nway to align with target instructions with a technique we call Hindsight Experience Augmentation. A\nlarge language model orchestrates this autonomous process without requiring human supervision,\nmaking it well-suited for lifelong learning scenarios. The framework reduces the amount of reward-\nlabeled data needed to 1) finetune a vision language model that acts as a reward detector, and 2)\ntrain RL agents on new tasks. We demonstrate the sample efficiency gains of DAAG in simulated\nrobotics environments involving manipulation and navigation. Our results show that DAAG improves\nlearning of reward detectors, transferring past experience, and acquiring new tasks - key abilities for\ndeveloping efficient lifelong learning agents. Supplementary material and visualizations are available\non our website https://sites.google.com/view/diffusion-augmented-agents/.", "sections": [{"title": "1 INTRODUCTION", "content": "The most recent notable breakthroughs in AI have\ncome from the combination of large models trained\non enormous datasets (Firoozi et al., 2023; Brown\net al., 2020; Hoffmann et al., 2022; Reed et al.,\n2022; Gemini-Team, 2023). However, despite ef-\nforts to scale up data collection (Collaboration,\n2023; Reed et al., 2022; Bousmalis et al., 2023),\ndata in embodied AI settings is still prohibitively\nscarce because such agents need to interact with\nphysical environments where sensors and actuators\npresent major bottlenecks (Cabi et al., 2020; Lee\net al., 2022).\nThis data scarcity issue is especially pronounced\nin reinforcement learning scenarios, where rewards\nare often sparse or completely absent in realistic set-\ntings (Ecoffet et al., 2021). Overcoming these chal-\nlenges requires developing agents that can learn and\nadapt efficiently from limited experience. We hy-\npothesize that embodied agents can achieve greater\ndata efficiency by leveraging past experience to\nexplore effectively and transfer knowledge across\ntasks (e.g. (Andrychowicz et al., 2017)). In par-\nticular, we are interested in enabling agents to au-\ntonomously set and score subgoals, even in the absence of external rewards, and to repurpose their experience from\nprevious tasks to accelerate learning of new tasks."}, {"title": "2 RELATED WORK", "content": "Foundation Models in Embodied AI The rapid evolution of foundation models (Bommasani et al., 2022) such\nas large language models (LLMs) (Brown et al., 2020; Hoffmann et al., 2022; Gemini-Team, 2023) and vision\nlanguage models (VLMs) (OpenAI, 2023; Gemini-Team, 2023; Alayrac et al., 2022; Radford et al., 2021) has sparked\nsignificant interest from the robotics and embodied AI research community in recent years. As these models have\ndemonstrated increasingly sophisticated abilities in language understanding, reasoning, commonsense knowledge, and\nvisual perception, researchers have leveraged their potential as building blocks for intelligent agents.\nMany methodologies for integrating these models into robotics systems were proposed in recent months. (Wang et al.,\n2023; 2024; Firoozi et al., 2023). (Ahn et al., 2022; Liang et al., 2023; Di Palo et al., 2023; Huang et al., 2022) proposed\nthe use of LLMs as high-level planner, able to decompose an instruction into a series of short horizon sub-goals. These\nmodels than employ different modalities to ground such textual plans into actions. (Huang et al., 2023; Yu et al., 2023b)\nuse LLMs and VLMs to instead obtain a reward function, given a textual instruction, that can be then used by an\nexternal optimiser to compute a trajectory. (Brohan et al., 2023; Kwon et al., 2023) use vision and language models\nto directly output executable actions given a textual description of a task. (Xiao et al., 2023; Di Palo et al., 2023) use\nVLMs as reward detectors/task classifiers. In this work, we build upon the framework proposed in (Di Palo et al., 2023),\nusing an LLM to decompose long horizon plans into a sequence of subgoals. However, we extend it by giving the\nLLM the ability to query a diffusion model to modify and augment visual observations autonomously, unlocking faster\nlearning and more effective transfer."}, {"title": "Image Generation with Diffusion Models", "content": "The rapid evolution of generative AI, beyond language-based application,\nalso saw the exponential growth of image generation models, largely based on diffusion processes (Dhariwal and\nNichol, 2021; Rombach et al., 2022; Ho et al., 2020; Ramesh et al., 2021). Mostly, these models are conditioned via\ntext, specifying the desired output image in natural language. Beyond generating images from scratch, these models\nhave also been employed to modify images via in-painting, i.e. the completion of a specific, masked part of an original\nimage. To provide more fine-grained control of the generation process, recent works have enabled diffusion models\nwith the ability to be conditioned on modalities other than text: (Zhang et al., 2023) demonstrated the use of depth maps,\ncanny edges, segmentation masks and more in addition to textual instructions to guide and constrain the geometrical\nand visual aspect of the final outputs. We largely based our proposed diffusion pipeline on these results, as ensuring\nthe augmented observations respect the original geometries of objects and environment is fundamental in embodied\napplications. Image-based diffusion models have also been recently extended into the temporal domain, outputting\nshort videos (Blattmann et al., 2023) instead of single frames. Training such models is however particularly challenging,\nand they do not yet benefit from the fine grained control abilities mentioned above. Notably, (Khachatryan et al., 2023)\ndemonstrated that image diffusion models can be repurposed to generate videos simply by constraining the original\nnoise maps used to start the backward diffusion process."}, {"title": "The Use of Diffusion Models in Robotics", "content": "While notable research has been conducted on the use of diffusion models\nas a way to represent policies (Chi et al., 2023) or generate additional experience for the agent as low-level states (Lu\net al., 2023), in this work we will focus on their use in the visual domain as text-conditioned image generators. As\nvisual perception is fundamental for robotics and embodied AI, the recent literature investigated methodologies to\nintegrate the image generation abilities of such models to empower agents (Zhu et al., 2024). To augment the visual\nexperience collected by a robot, (Chen et al., 2023; Mandi et al., 2023; Yu et al., 2023a) propose the use of diffusion\nmodels to either modify the background to increase robustness to distractors, or repurpose trajectory for new tasks\nby modifying the manipulated objects into new ones of interest (Yu et al., 2023a). While the latter is similar to our\nproposed approach, there are some fundamental differences: (Yu et al., 2023a) is based on an imitation learning pipeline,\nwhere a human operator labels the task demonstrated and the new task to generate via diffusion. Our method, designed\nand revolving around the abilities of LLMs, is entirely autonomous both in terms of detecting the accomplished tasks\n(via the VLM) and proposing and generating augmentations (via the LLM querying the diffusion model), therefore\nbeing more suited for (lifelong) reinforcement learning scenarios, where human supervision is not always present.\nAdditionally, we propose a diffusion pipeline that is both geometrically and temporally consistent when modifying\nvideo, differently from (Chen et al., 2023; Mandi et al., 2023; Yu et al., 2023a). Ensuring temporal and geometrical\nconsistency is beneficial when dealing with embodied environments, as we later demonstrate."}, {"title": "Experience Re-Use and Transfer", "content": "The need for vast amounts of experience data to learn policies (Reed et al., 2022;\nBousmalis et al., 2023) inspired the research community to propose methods and strategies to re-use experience\ncollected for different, previous tasks. Hindsight Experience Replay (Andrychowicz et al., 2017) proposes a method\nto re-use episodes that solve tasks different from the desired task. If asked to solve task \\(T_n\\), but solving instead task\n\\(T_m\\) during exploration, (Andrychowicz et al., 2017) proposed to relabel the episode as if the desired task was \\(T_m\\),\ntherefore re-purposing a trajectory as a success for a different task. Differently, when in the same situation, our method\nmodifies the collected observations that solve task \\(T_m\\) to synthetically generate an episode that would have solved the\ndesired task \\(T_n\\). This means we can generate data for a task without the need to effectively solve it during exploration.\n(Bousmalis et al., 2023) repurposes data from various task by learning a single, goal-conditioned policy, therefore"}, {"title": "3 METHOD", "content": "3.1 PRELIMINARIES\nWe formalise our environments as Markov Decision Processes (MDPs): the environment and the agent, at each timestep\nt, are in a state \\(s \u2208 S\\). From that state, the agent receives a visual observation \\(o \u2208 O\\), and can execute an action \\(a \u2208 A\\).\nDuring each episode, the agent receives an instruction, which is a description of the task to execute in natural language\n\\(T\\). The agent can receive a reward \\(r = +1\\) at the end of the episode if the task is successfully executed.\nIn this work, beyond learning new tasks in isolation, we study our framework's ability to learn tasks in succession in a\nlifelong fashion. Therefore, the agent stores interaction experiences in two buffers: the current task buffer that we call\nnew buffer \\(B_n\\): this buffer is initialised at the beginning of each new task. There is then an offline lifelong buffer \\(B_u\\): the\nagent stores all episodes from all tasks in this buffer, regardless of their success. The latter is therefore an ever-growing\nbuffer of experiences the agent can then use to bootstrap learning of new tasks.\nLarge Language Model: We use a large language model to orchestrate the behaviour of the agent and the use of the\nvision language model (VLM) and diffusion model. The LLM receives a textual instruction and data and outputs textual\nresponses. In our work, we leverage the LLM's ability to decompose tasks into subgoals, compare the similarity of\ndifferent tasks/instructions, and query the VLM and diffusion model. We parse the output of the LLM to obtain the\nexact string we need for each use case. In the Supplementary Material on our website, we show the way we design the\nprompt to guide the textual generation of the LLM and simplify the final parsing of its output.\nVision Language Model: The VLM we use is CLIP (Radford et al., 2021), a contrastive model. CLIP is composed of\ntwo branches: an image branch \\(\\phi_{\\text{image}}\\) and a textual branch \\(\\phi_{\\text{text}}\\). They respectively take as input visual observations\nand textual descriptions, outputting embedding vectors of the same size \\(y_{t,\\text{im}} = \\phi_{\\text{image}}(o_t)\\), \\(y_{g,\\text{txt}} = \\phi_{\\text{text}}(T_g)\\). The\npeculiarity of the output embeddings is the following: their cosine similarity \\(s_{g,t} = \\text{cs}(y_{t,\\text{im}}, y_{g,\\text{txt}})\\) implicitly represents\nhow well the text \\(T_t\\) describes the observation \\(o_t\\). Following (Di Palo et al., 2023), we consider that the VLM labels an\nobservation \\(o_t\\) as being a goal observation for task \\(T_g\\) if \\(s_{g,t} > \\delta\\), where \\(\\delta\\) is a threshold computed during training. More\ndetails are presented in the Supplementary Material on our website. Therefore, given a set of goal tasks \\(T_{o:G}\\), for each"}, {"title": "new observation CLIP computes a new \\(s_{g,t} = \\text{cs}(y_{t,\\text{im}}, y_{g,\\text{txt}})\\) and if the threshold is surpassed, labels the observation as\nachieving the task at hand.", "content": "As CLIP needs to be explicitly given as an input a textual description, in addition to the image, we need to manually\nprovide a set of tasks we are interested in detecting while the agent is exploring. Briefly, we adopt two strategies: first,\nwe keep a list of all the tasks that were given to the agent up to that point, and all the relative subgoals obtained by the\nLLM. Additionally, we can autonomously propose possible subgoals by giving to the LLM the aforementioned list\nand a list of objects present in the environment. These techniques allow us to have a list of tasks that the agent may\nrandomly achieve at test time."}, {"title": "Diffusion Pipeline:", "content": "A core aspect of our work is modifying visual observations through language-instructed diffusion\nmodels. The goal of our diffusion pipeline is to take an observation of or a temporal series of observations recorded by\nthe agent \\(O_{t:t+H}\\) and visually modify one or more objects present in the observation(s), while enforcing geometrical\nand temporal consistency.\nTo tackle the former, the diffusion pipeline receives as input the\nobservation(s) to be modified, a textual description of the object\nto modify, or source object \\(obj_s\\), and a description of the object to\ngenerate in place of the source, or target object \\(obj_t\\). The former\nis used to localise, crop and mask the source object to perform in-\npainting, while the latter is used by the diffusion model itself to guide\nthe image generation. The pipeline receives, additionally, a series of\nvisual inputs to condition the image generation, all computed from\nthe original observation(s): depth maps \\(O_{\\text{depth}}\\), canny edges \\(O_{\\text{canny}}\\), and\nnormals maps \\(O_{\\text{normals}}\\). We compute these from RGB observations via\na series of off-the-shelf models we list in the Supplementary Material.\nThe latter are used, through the ControlNet architectures (Zhang et al.,\n2023), to ensure that the generated objects respect the geometry of\nthe original objects. While the use of each is optional, their combined\nuse improves the overall results. The overall process can be described\nas \\(\\hat{O}_{t:t+H} = \\text{Diff}(o_{t:t+H}, obj_s, obj_t, O_{\\text{depth}}, O_{\\text{canny}}, O_{\\text{normals}})\\), where \\(\\hat{o}\\)\nrepresents an observation modified via diffusion from the original\nobservation \\(o\\). Fig. 4 represent an illustration of the entire pipeline.\nTo improve temporal consistency, we apply the technique proposed\nin (Khachatryan et al., 2023): when applying diffusion to N frames,\nwe 1) fix the initial noise map for all N instead of sampling different\nones (the results will still be different as the ControlNet inputs will\nbe different), and 2) add temporal cross-attention to the diffusion\nprocess: all the frames can therefore attend to all other frames during\nthe backward diffusion process for image generation. This does not require any architectural change, nor retraining the\nmodel. In Figure 5, we provide a visual comparison of the outputs of the method proposed in (Yu et al., 2023a) and our\nproposed pipeline. The former does not keep object poses and aspect consistent over frames, therefore invalidating\nthe hypothesis that applying the same actions \\(o_{0:T}\\) to the modified observations \\(\\hat{o}_{0:T}\\) would have led to successful\ncompletion of the new task."}, {"title": "Now that we introduced the main components of DAAG, we will describe the way they interoperate in our framework.", "content": "3.2 FINETUNE, EXTRACT, EXPLORE: THE DIFFUSION AUGMENTED AGENT FRAMEWORK\nFinetuning VLMs as Reward Detectors on Diffusion Augmented Data: VLMs can be effectively employed as\nreward detectors, conditioned on a language-defined goal and a visual observation. However, as demonstrated by recent\nworks (Di Palo et al., 2023; Xiao et al., 2023), to be accurate they often need to be finetuned on labelled data gathered\nin the target environment, for the desired tasks. This is a time-consuming task that furthermore requires human effort\nfor each new task to be learned, hindering the ability of the agent to autonomously learn many tasks in succession\nin a lifelong fashion. With our framework we tackle this challenge by finetuning the VLM on previously collected\nobservations. Given a dataset D of observations or, each paired with a label \\(T_i\\), and a new goal task expressed in natural\nlanguage, \\(T_g\\), we extract all observations whose caption \\(T_i\\) is similar enough to \\(T_g\\), or such that a visual modification of\nthe corresponding observation or would transform it into a fitting observation \\(\\hat{o}_i\\) for \\(T_g\\): for example, given the goal\ndescription \"The robot is grasping the red cube\u201d, an observation with caption \"The robot is grasping the blue cube\" can\nbe modified by visually swapping red cube with blue cube through a controlled diffusion process. In DAAG, the LLM\""}, {"title": "autonomously selects the fitting observations \\(o_i\\) from D by comparing their caption \\(T_i\\) with the goal caption \\(T_g\\): if a\nswap is considered possible, the LLM instructs the DM with the source object to modify and the target object to add (in\nthe previous example {blue cube, red cube} respectively). This process is illustrated in Figure 2. Through this process,\nwe finetune the VLM to act as a success detector for all the subgoals \\(T_{o:G}\\) in which the task at hand was decomposed by\nthe LLM.", "content": "Efficient Learning and Transfer via Hindsight Experience Augmentation: After each episode collected on any\ntask it encounters, an agent collects a series of observations and actions \\(E_n = \\{o_t, a_t\\}_{t=0}^T\\). We store every episode,\nregardless of the final outcome, in the lifelong buffer \\(B_u\\) (Cabi et al., 2020). When learning a new task, the agent\nreceives a task instruction in textual form \\(T\\), and decomposes it into a series of subgoals \\(T_{o:G}\\) via the LLM. Normally,\nthe agent can extract a learning signal only from episodes that are collected via exploration or from past experience\nstored in \\(B_u\\) if there are rewards associated to it; these rewards can be either from the VLM or be an external reward\nfrom the environment. In DAAG, we aim to maximise the number of episodes from which the agent can learn to tackle\na new task, even if it does not achieve any of the desired subgoals. We do this through a process we call Hindsight\nExperience Augmentation (HEA).\nGiven an episode of experience \\(E_n = \\{o_t, a_t\\}_{t=0}^T\\), we use the VLM to label what possible subgoals have been achieved\nby the agent, as described in the Preliminaries (more details on this phase are presented in the Suppl\n['900]\n- \u0113mentary\nMaterial). If any matches a desired subgoal, we add this episode to the new, current task buffer \\(B_n\\). This process emulates\nthe framework proposed in (Di Palo et al., 2023). However, if no match is present, instead of discarding the episode, we\nquery the LLM to ask if the achieved subgoal(s) can match any of the desired subgoals by swapping/visually modifying\nany of the objects, e.g. matching \u201cThe red cube is stacked on the blue cube\" with \"The green cube is stacked on the\nblue cube\" by swapping red cube with green cube. When a swap is identified as possible, the LLM queries the diffusion\nmodel to modify the observations of the episode up to the achieved sub-goal \\([o_0, ..., o_{T_i}]\\) into \\([\\hat{o}_0, ..., \\hat{o}_{T_i}]\\) and finally\nadds the modified observations and the original actions in the experience buffer \\(B_n \u2190 [\\{o_0, a_0\\}, ..., \\{\\hat{o}_{T_i}, a_{T_i}\\}]\\).\nThrough HEA, we can synthetically increase the number of successful episodes the agent can store in its buffers and\nlearn from. This allows to effectively re-use as much data gathered by the agent as possible, substantially improving\nefficiency especially when learning multiple tasks in succession, as we will describe later. While previous methods\nhave proposed the use of diffusion-based image generation or augmentation to synthesise additional data for learning\npolicies (Yu et al., 2023a; Mandi et al., 2023; Chen et al., 2023), our method is the first to propose an entire autonomous\npipeline, independent from human supervision, and that leverages geometrical and temporal consistency to generate\nconsistent augmented observations.\nWhen learning a new task, the agent first applies HEA to all the episodes stored in its lifelong buffer \\(B_u\\) to start with a\nnon-empty new task buffer \\(B_n\\). It then trains a policy on this data to kickstart exploration, and subsequently applies\nHEA to any new episode of experience gathered via exploration. By dividing a goal into shorter-horizon subgoals via\nthe LLM, and more efficiently learning to tackle those via HEA, our agent quickly learns to guide exploration, as it\nlearns to solve the various subgoals that lead to the completion of the task. In a robotic stacking scenario, for example,\""}, {"title": "DAAG efficiently learns to pick up the first object to stack. By consistently solving that first step during exploration, it\nis more likely to also randomly complete the task by placing it on top of the target object.", "content": "We will shed light on the effect of both of these phases on learning efficiency in the Experiments section. The entire\npipeline is illustrated in Fig. 2.\n4 EXPERIMENTS\nOur framework, DAAG, proposes an interplay between LLMs, VLMs and diffusion models to tackle three principal\nchallenges in agents that learn in a lifelong fashion: 1) finetuning a new reward/sub-goals detection model, 2) extracting\nand transfering past experience for new tasks and 3) efficiently exploring new tasks. To thoroughly investigate the\nbenefits of DAAG on these three challenges, we designed and ran a series of experiments that individually measure the\ncontribution and benefits of our method on each of these scenarios. The rest of the section is therefore divided into three\nmain subsections, where each of these challenges is investigated and results are demonstrated and analysed: 1) can\nDAAG finetune VLMs as reward detectors for novel tasks? 2) can DAAG explore and learn new tasks more efficiently?\n3) can DAAG more effectively learn tasks in succession, transferring experience from past tasks?\n4.1 EXPERIMENTAL SETUP\nWe use three different environments to measure the performance of DAAG on the aforementioned challenges: 1) a robot\nmanipulation environment, RGB Stacking (Lee et al., 2022), where a robot arm is tasked with stacking colored cubes\ninto a goal configuration. The action space A is composed of a target and goal pick and place position, represented\nas a pair of R2 numbers, and the observation space O is composed of RGB visual observations captured from a fixed\nshoulder camera. 2) a navigation environment, Room, inspired by (Team et al., 2022), where an agent navigates in a\nroom filled with objects and furniture, and is tasked with picking up and placing a goal object on a goal chair. The\naction space A is composed of a forward velocity and rotational velocity input represented as R2. We assume the agent\ncan pick up an object automatically by moving in close proximity to it, and equally place it on a target object when\nsufficiently close to it. The observation space O is composed of RGB visual observations captured from a first-person\nview camera. 3) a non-prehensile manipulation environment, Language Table (Lynch et al., 2022), where a robot\ncan push colored blocks on a table to move them to a goal configuration. The action space A is composed of x and y\nend-effector velocity inputs as R2. The observation space O is composed of RGB visual observations captured from a\nfixed shoulder camera. Goals for all environments are provided as natural language instructions.\nAs a policy learning algorithm, we use Self-Imitation Behavior Cloning (Chen et al., 2021; Di Palo et al., 2023; Oh\net al., 2018) on all the episodes stored in the buffer \\(B_n\\), where the agent collects all successful episodes.\nAs a large language model, we use Gemini Pro (Gemini-Team, 2023). As a VLM, we use CLIP ViT-B/32 (Radford\net al., 2021). As a diffusion model, we use Stable Diffusion 1.5 (Rombach et al., 2022), with ControlNet (Zhang et al.,\n2023).\nDetailed hyperparameters and values of constant are listed in the Supplementary Material."}, {"title": "4.2 CAN DAAG FINETUNE VLMS AS REWARD DETECTORS FOR NOVEL TASKS?", "content": "In this section we evaluate the ability of DAAG to obtain effective reward detectors for new tasks by finetuning VLM\non past experiences collected by the agent being augmented through a diffusion pipeline, as explained in 3.\nWe assume the existence of a dataset B of collected goal observations for different tasks \\(T_\\mathcal{B} = [T_o, ..., T_n]\\). We then\nwant to measure the performance of CLIP, the VLM we use in this work, to correctly detect novel observations \\(o_i\\) as\ngoal configurations for a new task \\(T_i\\) not present in \\(T_\\mathcal{B}\\). We compare finetuning CLIP on the original dataset B, and\nfinetuning on an artificially expanded version of B where we apply diffusion augmentation to synthesise examples of\ngoal observations for \\(T_i\\), starting from goal observations of other tasks.\nTo empirically evaluate if the synthetic observations positively affect performance, we compare the two pipelines on\na test set of unseen observations, where 50% are goal observations of \\(T_i\\) and 50% are not, therefore resulting in a\nbalanced binary classification task. We also compare the zero-shot performance of CLIP, to better evaluate the relative\nimprovement over the off-the-shelf model.\nFor the RGB Stacking environment, the tasks \\(T_\\mathcal{B}\\) are [ \"Stack the green cube on the blue cube\", \"Stack the blue cube\non the red cube\"], respectively \\(T^{RGB}_{g,b}\\), \\(T^{RGB}_{b,r}\\), with the test task \\(T_i\\) being \"Stack the red cube on the green cube\", \\(T^{RGB}_{r,g}\\)."}, {"title": "For the Room environment, the tasks are [ \"Put a lemon on a red chair\", \"Put a banana on a blue chair\"], respectively\n\\(T^{Room}_{lemon, red}\\), \\(T^{Room}_{banana, blue}\\), with the test task \\(T_i\\) being \"Put an apple on a gray chair\u201d, \\(T^{Room}_{apple, gray}\\)", "content": "For the Language Table environment, the tasks are [ \"Put the green block near the blue block\u201d, \"Put the yellow block\nnear the blue block\u201d ], respectively \\(T^{LT}_{g,b}\\), \\(T^{LT}_{y,b}\\), with the test task \\(T_i\\) being \u201cPut the red block near the blue block\u201d, \\(T^{LT}_{r,b}\\)\nFor each environment, we have N = 100 example observations for each \\(T_\\mathcal{B}\\), and\nuse the DAAG pipeline to obtain synthetic observations of \\(T_i\\) by augmenting all\nother observations. We finetune CLIP on both the original and augmented datasets\nand test for accuracy on a test test of M = 100 unseen observations. We report\nresults in Figure 7. The results demonstrate how, in each environment, DAAG\ncan learn an effective reward detector even when having no example observations\nof such task, outperforming a CLIP model trained on the other tasks and queried\nto generalise zero-shot to the new task. Figure 7 shows how, on the leftmost task\nthat has no examples in the dataset, DAAG brings a substantial improvement by\nsynthesising examples from other tasks, while keeping the same performance on\nthe seen tasks. In the RGB Stacking and Language Table environments, where\nprecise geometric relations between objects poses are fundamental, the difference\nwith the baselines is more impressive, shedding light on the need for diffusion\naugmentation to obtain an effective reward detector. In the Room environment,\nthe observations CLIP receives, albeit coming from a low-fidelty simulator and\nrenderer, are closer to the distribution of observations it received during training\non a web-scale dataset (pictures of fruits and furniture). Therefore, zero-shot\nperformance is considerably stronger there, while in the other tasks is close to\nrandom guessing, demonstrating the need for finetuning."}, {"title": "4.3 CAN DAAG EXPLORE AND LEARN NEW TASKS MORE EFFICIENTLY?", "content": "We here focus on investigating the benefits brought by DAAG and HEA to\nexploring and learning new tasks from scratch.\nWe assume the agent start learning a new task tabula rasa in this experimental\nscenario, with an empty new task buffer \\(B_n\\), and with no access to a lifelong\nbuffer \\(B_u\\), to independently evaluate the effect of HEA on new task learning\nefficiency. The agent receives a natural language instruction \\(T_i\\), that informs it\nabout the goal to achieve in the environment. We experimentally evaluate the\nlearning efficiency improvements brought by HEA, comparing against (Di Palo\net al., 2023), that decomposes \\(T_i\\) into subgoals \\(T_{o:G}\\) and obtained reward for each via a VLM, and to a baseline agent\nthat does not benefit neither from task decomposition or diffusion augmentation.\nWe evaluate the performance of this method on the RGB Stacking and Room environments. For the RGB Stacking\nenvironment, the task is \u201cStack the red cube on the blue cube\u201d, or \\(T^{RGB}_{a,g}\\). For the Room environment, the task is\nPut an apple on a gray chair, \\(T^{Room}_{a,g}\\). The environments only provide a reward of +1 when the task is successfully\nachieved, and end the episode there: in that case, we add the entire episode to \\(B_n\\). If an internal reward is detected via\nthe VLM at timestep \\(T_g\\), the observations and actions up to that timestep are added to \\(B_n\\). We perform exploration in"}, {"title": "the environments via an epsilon-greedy strategy (Mnih et al., 2013). We start with \\(\\epsilon = 0.99\\) and decay it over time,\nwith detailed hyperparameters described in the Supplementary Material. During each episode, if we sample a number\n\\(n > \\epsilon\\) where \\(n \\sim U(0, 1)\\), we let the policy network guide the agent. Otherwise, we perform exploration as follows:\nfor the RGB Stacking task, we sample a random pick up position \\(a_o \u2208 R^2\\) and a random place position \\(a_1 \u2208 R^2\\) and\nexecute the actions. For the Room task, in order to speed up and guide exploration, we select a random pickable object\n\\(T_{\\text{pick}}\\)\nin the room, and move the agent to it to pick it up, collecting all observations and actions leading to it \\([o_t, a_t]_t\\), with\n\\(a_t \u2208 R^2\\). We then select a random target furniture where to place the object, and move the agent there collecting other\nobservations and actions \\([o_t, a_t]_{T_{\\text{pick}}}^{T_{\\text{place}}}\\). While speeding up the learning of the task, this guidance does not affect the\nrelative performance improvements brought by one method over the other: in the Supplementary Material, we also\nshow training curves with entirely random exploration. We train our policy every \\(T_{\\mathcal{B}c}\\) steps on the observation-action\npairs collected in \\(B_n\\).", "content": "In Figure 8", "agents": "the ability to\nextract", "B_{\\text{ll},0": "n"}, "containing\n\\(N_{\\text{off}} = 200\\) episodes composed as follows: 50% are successful episodes of the"]}