{"title": "DIFFUSION AUGMENTED AGENTS: A FRAMEWORK FOR EFFICIENT EXPLORATION AND TRANSFER LEARNING", "authors": ["Norman Di Palo", "Leonard Hasenclever", "Jan Humplik", "Arunkumar Byravan"], "abstract": "We introduce Diffusion Augmented Agents (DAAG), a novel framework that leverages large language models, vision language models, and diffusion models to improve sample efficiency and transfer learning in reinforcement learning for embodied agents. DAAG hindsight relabels the agent's past experience by using diffusion models to transform videos in a temporally and geometrically consistent way to align with target instructions with a technique we call Hindsight Experience Augmentation. A large language model orchestrates this autonomous process without requiring human supervision, making it well-suited for lifelong learning scenarios. The framework reduces the amount of reward-labeled data needed to 1) finetune a vision language model that acts as a reward detector, and 2) train RL agents on new tasks. We demonstrate the sample efficiency gains of DAAG in simulated robotics environments involving manipulation and navigation. Our results show that DAAG improves learning of reward detectors, transferring past experience, and acquiring new tasks - key abilities for developing efficient lifelong learning agents. Supplementary material and visualizations are available on our website https://sites.google.com/view/diffusion-augmented-agents/.", "sections": [{"title": "1 INTRODUCTION", "content": "The most recent notable breakthroughs in AI have come from the combination of large models trained on enormous datasets (Firoozi et al., 2023; Brown et al., 2020; Hoffmann et al., 2022; Reed et al., 2022; Gemini-Team, 2023). However, despite efforts to scale up data collection (Collaboration, 2023; Reed et al., 2022; Bousmalis et al., 2023), data in embodied AI settings is still prohibitively scarce because such agents need to interact with physical environments where sensors and actuators present major bottlenecks (Cabi et al., 2020; Lee et al., 2022).\nThis data scarcity issue is especially pronounced in reinforcement learning scenarios, where rewards are often sparse or completely absent in realistic settings (Ecoffet et al., 2021). Overcoming these challenges requires developing agents that can learn and adapt efficiently from limited experience. We hypothesize that embodied agents can achieve greater data efficiency by leveraging past experience to explore effectively and transfer knowledge across tasks (e.g. (Andrychowicz et al., 2017)). In particular, we are interested in enabling agents to autonomously set and score subgoals, even in the absence of external rewards, and to repurpose their experience from previous tasks to accelerate learning of new tasks."}, {"title": "2 RELATED WORK", "content": "Foundation Models in Embodied AI The rapid evolution of foundation models (Bommasani et al., 2022) such as large language models (LLMs) (Brown et al., 2020; Hoffmann et al., 2022; Gemini-Team, 2023) and vision language models (VLMs) (OpenAI, 2023; Gemini-Team, 2023; Alayrac et al., 2022; Radford et al., 2021) has sparked significant interest from the robotics and embodied AI research community in recent years. As these models have demonstrated increasingly sophisticated abilities in language understanding, reasoning, commonsense knowledge, and visual perception, researchers have leveraged their potential as building blocks for intelligent agents.\nMany methodologies for integrating these models into robotics systems were proposed in recent months. (Wang et al., 2023; 2024; Firoozi et al., 2023). (Ahn et al., 2022; Liang et al., 2023; Di Palo et al., 2023; Huang et al., 2022) proposed the use of LLMs as high-level planner, able to decompose an instruction into a series of short horizon sub-goals. These models than employ different modalities to ground such textual plans into actions. (Huang et al., 2023; Yu et al., 2023b) use LLMs and VLMs to instead obtain a reward function, given a textual instruction, that can be then used by an external optimiser to compute a trajectory. (Brohan et al., 2023; Kwon et al., 2023) use vision and language models to directly output executable actions given a textual description of a task. (Xiao et al., 2023; Di Palo et al., 2023) use VLMs as reward detectors/task classifiers. In this work, we build upon the framework proposed in (Di Palo et al., 2023), using an LLM to decompose long horizon plans into a sequence of subgoals. However, we extend it by giving the LLM the ability to query a diffusion model to modify and augment visual observations autonomously, unlocking faster learning and more effective transfer."}, {"title": "3 METHOD", "content": "3.1 PRELIMINARIES\nWe formalise our environments as Markov Decision Processes (MDPs): the environment and the agent, at each timestep t, are in a state $s \\in S$. From that state, the agent receives a visual observation $o \\in O$, and can execute an action $a \\in A$. During each episode, the agent receives an instruction, which is a description of the task to execute in natural language T. The agent can receive a reward $r = +1$ at the end of the episode if the task is successfully executed.\nIn this work, beyond learning new tasks in isolation, we study our framework's ability to learn tasks in succession in a lifelong fashion. Therefore, the agent stores interaction experiences in two buffers: the current task buffer that we call new buffer $B_n$: this buffer is initialised at the beginning of each new task. There is then an offline lifelong buffer $B_u$: the agent stores all episodes from all tasks in this buffer, regardless of their success. The latter is therefore an ever-growing buffer of experiences the agent can then use to bootstrap learning of new tasks.\nLarge Language Model: We use a large language model to orchestrate the behaviour of the agent and the use of the vision language model (VLM) and diffusion model. The LLM receives a textual instruction and data and outputs textual responses. In our work, we leverage the LLM's ability to decompose tasks into subgoals, compare the similarity of different tasks/instructions, and query the VLM and diffusion model. We parse the output of the LLM to obtain the exact string we need for each use case. In the Supplementary Material on our website, we show the way we design the prompt to guide the textual generation of the LLM and simplify the final parsing of its output.\nVision Language Model: The VLM we use is CLIP (Radford et al., 2021), a contrastive model. CLIP is composed of two branches: an image branch $\\phi_{image}$ and a textual branch $\\phi_{text}$. They respectively take as input visual observations and textual descriptions, outputting embedding vectors of the same size $y_{t,im} = \\phi_{image}(o_t)$, $y_{g,txt} = \\phi_{text}(T_g)$. The peculiarity of the output embeddings is the following: their cosine similarity $s_{g,t} = cs(y_{t,im}, y_{g,txt})$ implicitly represents how well the text $T_t$ describes the observation $o_t$. Following (Di Palo et al., 2023), we consider that the VLM labels an observation $o_t$ as being a goal observation for task $T_g$ if $s_{g,t} > \\delta$, where $\\delta$ is a threshold computed during training. More details are presented in the Supplementary Material on our website. Therefore, given a set of goal tasks $T_{o:G}$, for each"}, {"title": "3.2 FINETUNE, EXTRACT, EXPLORE: THE DIFFUSION AUGMENTED AGENT FRAMEWORK", "content": "Finetuning VLMs as Reward Detectors on Diffusion Augmented Data: VLMs can be effectively employed as reward detectors, conditioned on a language-defined goal and a visual observation. However, as demonstrated by recent works (Di Palo et al., 2023; Xiao et al., 2023), to be accurate they often need to be finetuned on labelled data gathered in the target environment, for the desired tasks. This is a time-consuming task that furthermore requires human effort for each new task to be learned, hindering the ability of the agent to autonomously learn many tasks in succession in a lifelong fashion. With our framework we tackle this challenge by finetuning the VLM on previously collected observations. Given a dataset D of observations or, each paired with a label Ti, and a new goal task expressed in natural language, Tg, we extract all observations whose caption Ti is similar enough to Tg, or such that a visual modification of the corresponding observation or would transform it into a fitting observation \u00f4\u2081 for Tg: for example, given the goal description \"The robot is grasping the red cube\u201d, an observation with caption \"The robot is grasping the blue cube\" can be modified by visually swapping red cube with blue cube through a controlled diffusion process. In DAAG, the LLM"}, {"title": "4 EXPERIMENTS", "content": "Our framework, DAAG, proposes an interplay between LLMs, VLMs and diffusion models to tackle three principal challenges in agents that learn in a lifelong fashion: 1) finetuning a new reward/sub-goals detection model, 2) extracting and transfering past experience for new tasks and 3) efficiently exploring new tasks. To thoroughly investigate the benefits of DAAG on these three challenges, we designed and ran a series of experiments that individually measure the contribution and benefits of our method on each of these scenarios. The rest of the section is therefore divided into three main subsections, where each of these challenges is investigated and results are demonstrated and analysed: 1) can DAAG finetune VLMs as reward detectors for novel tasks? 2) can DAAG explore and learn new tasks more efficiently? 3) can DAAG more effectively learn tasks in succession, transferring experience from past tasks?"}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "We use three different environments to measure the performance of DAAG on the aforementioned challenges: 1) a robot manipulation environment, RGB Stacking (Lee et al., 2022), where a robot arm is tasked with stacking colored cubes into a goal configuration. The action space A is composed of a target and goal pick and place position, represented as a pair of R2 numbers, and the observation space O is composed of RGB visual observations captured from a fixed shoulder camera. 2) a navigation environment, Room, inspired by (Team et al., 2022), where an agent navigates in a room filled with objects and furniture, and is tasked with picking up and placing a goal object on a goal chair. The action space A is composed of a forward velocity and rotational velocity input represented as R2. We assume the agent can pick up an object automatically by moving in close proximity to it, and equally place it on a target object when sufficiently close to it. The observation space O is composed of RGB visual observations captured from a first-person view camera. 3) a non-prehensile manipulation environment, Language Table (Lynch et al., 2022), where a robot can push colored blocks on a table to move them to a goal configuration. The action space A is composed of x and y end-effector velocity inputs as R2. The observation space O is composed of RGB visual observations captured from a fixed shoulder camera. Goals for all environments are provided as natural language instructions.\nAs a policy learning algorithm, we use Self-Imitation Behavior Cloning (Chen et al., 2021; Di Palo et al., 2023; Oh et al., 2018) on all the episodes stored in the buffer Bn, where the agent collects all successful episodes.\nAs a large language model, we use Gemini Pro (Gemini-Team, 2023). As a VLM, we use CLIP ViT-B/32 (Radford et al., 2021). As a diffusion model, we use Stable Diffusion 1.5 (Rombach et al., 2022), with ControlNet (Zhang et al., 2023).\nDetailed hyperparameters and values of constant are listed in the Supplementary Material."}, {"title": "4.2 CAN DAAG FINETUNE VLMS AS REWARD DETECTORS FOR NOVEL TASKS?", "content": "In this section we evaluate the ability of DAAG to obtain effective reward detectors for new tasks by finetuning VLM on past experiences collected by the agent being augmented through a diffusion pipeline, as explained in 3.\nWe assume the existence of a dataset B of collected goal observations for different tasks $T_{\\mathcal{B}} = [T_o, ..., T_n]$. We then want to measure the performance of CLIP, the VLM we use in this work, to correctly detect novel observations of as goal configurations for a new task $T_i$ not present in $\\mathcal{T_{\\mathcal{B}}}$. We compare finetuning CLIP on the original dataset B, and finetuning on an artificially expanded version of B where we apply diffusion augmentation to synthesise examples of goal observations for $T_i$, starting from goal observations of other tasks.\nTo empirically evaluate if the synthetic observations positively affect performance, we compare the two pipelines on a test set of unseen observations, where 50% are goal observations of $T_i$ and 50% are not, therefore resulting in a balanced binary classification task. We also compare the zero-shot performance of CLIP, to better evaluate the relative improvement over the off-the-shelf model.\nFor the RGB Stacking environment, the tasks $\\mathcal{T_{\\mathcal{B}}}$ are [ \"Stack the green cube on the blue cube\", \"Stack the blue cube on the red cube\"], respectively $\\mathcal{T}_{g,b}^{RGB}$, $\\mathcal{T}_{b,r}^{RGB}$, with the test task $T_i$ being \"Stack the red cube on the green cube\", $\\mathcal{T}_{r,g}^{RGB}$."}, {"title": "4.3 CAN DAAG EXPLORE AND LEARN NEW TASKS MORE EFFICIENTLY?", "content": "We here focus on investigating the benefits brought by DAAG and HEA to exploring and learning new tasks from scratch.\nWe assume the agent start learning a new task tabula rasa in this experimental scenario, with an empty new task buffer Bn, and with no access to a lifelong buffer Bu, to independently evaluate the effect of HEA on new task learning efficiency. The agent receives a natural language instruction Ti, that informs it about the goal to achieve in the environment. We experimentally evaluate the learning efficiency improvements brought by HEA, comparing against (Di Palo et al., 2023), that decomposes Ti into subgoals To:G and obtained reward for each via a VLM, and to a baseline agent that does not benefit neither from task decomposition or diffusion augmentation.\nWe evaluate the performance of this method on the RGB Stacking and Room environments. For the RGB Stacking environment, the task is \"Stack the red cube on the blue cube\u201d, or $\\mathcal{T}_{r,b}^{RGB}$. For the Room environment, the task is Put an apple on a gray chair, $\\mathcal{T}_{a,g}^{Room}$. The environments only provide a reward of +1 when the task is successfully achieved, and end the episode there: in that case, we add the entire episode to Bn. If an internal reward is detected via the VLM at timestep Tg, the observations and actions up to that timestep are added to Bn. We perform exploration in"}, {"title": "4.4 CAN DAAG MORE EFFECTIVELY LEARN TASKS IN SUCCESSION TRANSFERRING EXPERIENCE FROM PAST TASKS?", "content": "We now investigate the influence of DAAG on another fundamental ability of lifelong learning agents: the ability to extract, transfer and repurpose past experience to speed up learning of new tasks. As we demonstrated the ability of DAAG to learn new tasks efficiently through exploration and HEA in the previous set of experiments, we now investigate its ability to extract information and learn policies from a given dataset of experience, with no additional exploration, in a setting closer to Offline Reinforcement Learning (Kostrikov et al., 2021).\nWe let an agent learn three tasks in sequence per environment. At the beginning of each new task Tm, the agent receives a buffer of experience Blu,n containing Noff = 200 episodes composed as follows: 50% are successful episodes of the task at hand Tn, while the other 50% are episodes solving different tasks in the same environment. Each baseline uses all buffers and data received up to that point $\\mathcal{B}_{l\\, 0:n}$ to learn a policy and is then tested on 100 test episodes to solve the task Tn: for task n the agent has therefore access to $n \\times N_{off}$ pre-collected episodes.\nWhen learning to solve task Tn using $\\mathcal{B}_{l\\, 0:n}$, (Di Palo et al., 2023) decomposes Tn into subgoals To:G and for each extracts successful trajectories from $\\mathcal{B}_{l\\, 0:n}$. DAAG, in addition to this, runs Hindsight Experience Augmentation to also extract trajectories completing similar tasks that can be visually modified to match any of the subgoals in To:G. Both baseline train the Self-Imitation Learning policy on the extracted (and synthetically augmented) episodes. In addition, we run another baseline which does not perform any decomposition or extraction, and only trains a goal-conditioned policy on all the episodes contained in $\\mathcal{B}_{l\\, 0:n}$.\nFor each environment, we learn three tasks in succession. For the RGB Stacking environment, the tasks are, in order, [ \"Stack the red cube on the green cube\", \"Stack the green cube on the blue cube\u201d, \u201dStack the blue cube on the red cube\u201d ], respectively $\\mathcal{T}_{r,g}^{RGB}$, $\\mathcal{T}_{g,b}^{RGB}$, $\\mathcal{T}_{b,r}^{RGB}$.\nFor the Room environment, the tasks are [ \"Put a lemon on a red chair\", \"Put a banana on a blue chair\", \"Put an apple on a gray chair\u201d ], respectively $\\mathcal{T}_{lemon, red}^{Room}$, $\\mathcal{T}_{banana,blue}^{Room}$, $\\mathcal{T}_{apple,gray}^{Room}$.\nIn Figure 9, we compare the performance, as success rate, of each method on method Tn using $\\mathcal{B}_{u,0:n}$. We can see how DAAG surpassess both baselines, thanks to the ability to learn from most of the experience stored in Bu, by modifying and repurposing trajectories solving tasks beyond Tn or its subgoals To:G."}, {"title": "4.5 IMPROVING ROBUSTNESS VIA SCENE VISUAL AUGMENTATION", "content": "We demonstrate how our diffusion pipeline can also be used to visually augment visual observations by modifying the scene and keeping the salient objects untouched, therefore generating additional examples of successful trajectories with different backgrounds. This is possible thanks to both the geometrical consistency and temporal consistency, absent in methods like Chen et al. (2023); Mandi et al. (2023); Yu et al. (2023a). To test how this affects policy robustness, we gather a dataset of 300 successful episodes in the Room environment where an agent reaches the yellow chair.\nWe then use our pipeline to augment each observations 5 times, querying the LLM to propose a description of an augmentation (e.g. a room with a red floor and white walls). We add all these augmented observations to our buffer and train a policy on it. Both the policy trained on the original and augmented datasets are tested on 5 visually modified room, where we randomly change the walls and floor colors as well as the distractor objects, running 20 test episodes on each room. Figure 11 demonstrated how visual augmentations leads to a substantially more robust policy, able to reach the target object also on rooms that appear very visually different from the single training room."}, {"title": "5 CONCLUSION", "content": "In this work, we proposed Diffusion Augmented Agent (DAAG), a framework that combines large language models, vision-language models, and diffusion models to tackle key challenges in lifelong reinforcement learning for embodied AI agents. Specifically, our key results show that DAAG can accurately detect rewards on novel, unseen tasks where traditional approaches fail to generalize. By repurposing experience from prior tasks, DAAG progressively learns each subsequent task more efficiently, requiring fewer episodes thanks to transfer learning. Finally, by diffusing unsuccessful episodes into successful trajectories for related subgoals, DAAG substantially improves exploration efficiency. Through diffusion augmentation, experience gathered across a lifetime of learning can be repurposed to make each new task easier than the last. This work suggests promising directions for overcoming data scarcity in robot learning and developing more generally capable agents."}, {"title": "7 APPENDIX", "content": "7.1 BACKWARD TRANSFER WITH HINDSIGHT EXPERIENCE AUGMENTATION\nIn the experiments of section 4.4 we studied forward transfer in a lifelong learning setting. We also analysed backward transfer in the RGB Stacking task as follows. After each task of Figure 9, we also test performance on the previous tasks by running HEA on all observations gathered up to that points to synthesise additional examples for previous tasks, and retrain and test the policy. Figure 13 demonstrates how HEA unlocks also strong backward transfer on the same three tasks order as Fig. 9: as the agent receives new data for new tasks, HEA can augment this data also to improve performance on previous tasks if needed.\n7.2 PERFORMANCE\nWITH PURELY RANDOM EXPLORATION ON RGB STACKING\nIn our main experiments we used guided exploration to speed up the experiments. However, guided exploration does not change the relative performance of the various methods. We demonstrate how DAAG learns tasks faster than the chosen baseline by re-running the experiment in 8 (top), using entirely random exploration, where the agent samples a random position to pick and place the objects.\nFigure 14 demonstrates how DAAG can learn to tackle a task considerably faster than the baseline that does not use HEA also in this scenario.\n7.3 FINETUNING CLIP AND FINDING A THRESHOLD\nAs a contrastive VLM, CLIP outputs a similarity score between a textual description and an image. In our work, as also found in (Di Palo et al., 2023), we observed that off-the-shelf CLIP struggles at recognising precise configurations of objects, performing close to random guessing. We therefore finetune it as described in the main paper, increasing the difference in score between correct text-image matches and wrong matches. Given the dataset we used for finetuning, we then find \u03b4, the threshold to detect if a match is correct by comparing if the similarity score is larger, simply by finding the value that would maximise accuracy in an held-out validation set taken from the training set. In Figure 12 we illustrate this behaviour.\n7.4 COMPARING DIFFERENT AUGMENTATION TECHNIQUES\nIn this work, we proposed a diffusion pipeline to visually modify and augment ob- servations, focusing on geometrical and temporal consistency. Here we compare"}]}