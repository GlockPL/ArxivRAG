{"title": "A robust three-way classifier with shadowed granular-balls based on justifiable granularity", "authors": ["Jie Yang", "Lingyun Xiaodiao", "Guoyin Wang", "Witold Pedrycz", "Shuyin Xia", "Qinghua Zhang", "Di Wu"], "abstract": "The granular-ball (GB)-based classifier introduced by Xia, exhibits adaptability in creating coarse-grained information granules for input, thereby enhancing its generality and flexibility. Nevertheless, the current GB-based classifiers rigidly assign a specific class label to each data instance and lacks of the necessary strategies to address uncertain instances. These far-fetched certain classification approachs toward uncertain instances may suffer considerable risks. To solve this problem, we construct a robust three-way classifier with shadowed GBs for uncertain data. Firstly, combine with information entropy, we propose an enhanced GB generation method with the principle of justifiable granularity. Subsequently, based on minimum uncertainty, a shadowed mapping is utilized to partition a GB into Core region, Important region and Unessential region. Based on the constructed shadowed GBs, we establish a three-way classifier to categorize data instances into certain classes and uncertain case. Finally, extensive comparative experiments are conducted with 2 three-way classifiers, 3 state-of-the-art GB-based classifiers, and 3 classical machine learning classifiers on 12 public benchmark datasets. The results show that our model demonstrates robustness in managing uncertain data and effectively mitigates classification risks. Furthermore, our model almost outperforms the other comparison methods in both effectiveness and efficiency.", "sections": [{"title": "I. INTRODUCTION", "content": "Granular computing (GrC) [1]\u2013[4] focuses on the forma- tion, processing, and communication of information granules, aiming to mimic human cognitive thinking in addressing complex problems. Rough sets [5], fuzzy sets [6], and quotient spaces [7] are three primary models of GrC. Rough sets are widely used to measure the uncertainty and incompleteness inherent in information systems. To handle continuous data, neighborhood rough sets (NRS) [8] introduce the concept of neighborhood granulation to convert the equivalence relation into the covering relation in neighborhood space. Furthermore, numerous extended GrC-based classifiers [9]\u2013[13] were de- veloped by utilizing information granules as the fundamental computational unit, enhancing the efficiency of knowledge discovery. However, a notable limitation of these classifiers is that they primarily treat granules as a preliminary fea- ture processing method, without modifying the underlying mathematical model or elevating the core performance of the classifiers themselves.\nBased on the idea of GrC, granular-ball computing (GBC) [14]\u2013[16], introduced by Xia, represents a groundbreaking approach to data processing and knowledge representation. This method replaces traditional information granule inputs with granular-balls (GBs), adhering to the principle of global topology precedence [17]. Over the years, GBC has undergone continuous advancements in terms of its methodologies and applications. Xia [18] proposed the GB-based kNN, which sig- nificantly outperforms the existing kNN in terms of efficiency, especially when dealing with large-scale datasets, achieving an improvement of hundreds of times. Furthermore, Xia [19] introduced a novel rough sets model, namely, granular-ball rough sets (GBRS). Compared to traditional NRS methods, GBRS stands out as a multi-granularity learning tool, offering greater robustness and efficiency by substituting neighborhood granules with GBs. In addition, Chen [20] proposed a GB- based attribute selector to achieve the superior classification performance through effective feature reduction. Xie [21] introduced a fast and stable GB generation method based on the attention mechanism. Xie [22] constructed a multi- granularity representation of the data using the GBC model, thereby boosting the algorithm\u2019s time efficiency. Chen [23] innovated by introducing a novel GB-based density peaks clustering, resulting in significantly reduced runtime without the need for parameterization. However, current researches on GB generation in GBC only consider the purity thresholds to control the granularity of GB space, which is not in accord with various practical applications.\nFrom the perspective of the justifiable granularity principle [24]\u2013[26], both coverage and specificity should be taken into account during the GB generation process. Coverage refers to a GB that encompass as much experimental evidence (data) as possible. Specificity refers to all GBs generated on the universe having distinct and well-defined semantics or all GBs being well distinguished from each other. Typically, the"}, {"title": null, "content": "fewer the data instances in a GB, the higher the specificity. Therefore, it is significant to generate justifiable GB space to solve problems by combining with the level of coverage and specificity. Moreover, the above GB-based classification methods rigidly ascribe a fixed class label to each data in- stance, lacking strategies to handle instances with uncertainty. The methodology of uncertain data classification is invaluable in mitigating decision risk and enhancing decision efficiency through human-machine collaboration, thus playing a pivotal role in decision support systems. For instance, when applying classification methods to develop a computer-aided diagnosis (CAD) system for liver cancer [27], it is crucial to accurately classify uncertain tumors for further cautious diagnosis and certain far-fetched classifications produced by the system may cause serious costs.\nAs is well known, three-way decision (3WD) [28]\u2013[30] theory proposed by Yao has emerged as a promising approach to tackle complex problems involving uncertainty. The fun- damental principle of 3WD lies in dividing a universe into three distinct regions, with each region corresponding to a specific decision action. As a generalization of the traditional two-way decision model, 3WD further incorporates a third option, enabling a trisecting-and-acting approach to decision- making. Currently, 3WD has found widespread application across various fields [31]\u2013[34]. Xu [35] enhanced the flexi- bility and evolution capability of concept learning, addressing the limitations of existing two-way learning approaches by incorporating a novel cognitive mechanism and movement 3WD method. Zhang [36] introduced an efficient multi-scale decision system method that integrates a sequential 3WD model with the Hasse diagram to optimize scale combination selection and attribute reduction. Du [37] presented a novel multistep three-way clustering algorithm that enhances the accuracy and adaptability of cluster representations. In term of the advantage of 3WD, to address the limitations of GB- based classification models in classifying uncertain data, we introduce 3WD to construct the three-way approximations of GBs, which is called shadowed GBs. More detailed, the traditional GBs are extended to shadowed GBs for both certain regions and uncertain boundaries from the perspective of uncertainty, and shadowed GBs constructs a tripartitioned approximation of data distribution for three-way classification. The data instances will be categorized into a certain class or an uncertain case based on their locations relative to the shadowed GBs. Specifically, the Core region of GBs corresponding to the same class is confidently determined the class of instances, whereas the Important region has uncertainty for classification, and the Unessential region has almost no contribution to classification. This ensures more effective to classify uncertain data. The major contributions of this paper are summed up as follows:\n(1) Combine with information entropy, we propose an im- proved GB generation method based on the principle of justifiable granularity.\n(2) We introduce 3WD to construct and optimize shadowed GBs for modeling uncertain data.\n(3) We further establish a three-way classifier with shadowed GBs to categorize data instances into certain classes and uncertain case.\nThe following sections of this paper are structured with the subsequent manner. In section II is a review of related prelim- inary definitions. In Section III, an improved GB generation method is constructed based on the principle of justifiable granularity. In Section IV introduces the construction of shad- owed GBs. Then, a three-way classifier with shadowed GBs is presented. The relevant experiments for the verification of viability and rationality of our models are shown in Section V. Finally, Section VI summarizes the conclusions."}, {"title": "II. PRELIMINARIES", "content": "In this section, to set up the framework of this paper, we recall some necessary definitions related to GBC, shadowed sets and 3WD.\nGBC is first proposed by Xia [18], which is to use GBs to cover the sample space and replace traditional information granules with GBs for data processing. We first define the GBC as follow:\nDefinition 1. (GBC) [18] Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non- empty finite set. Generating $\\mathcal{G}$ to replace the inputs for computation can be called GBC. Where $\\mathcal{G} = \\{gb_1, gb_2,..., gb_m\\}$ is a granular-ball space of $\\mathcal{U}$, and a granular-ball $gb_i$ covers a subset of $\\mathcal{U}$ such that $gb_i = \\{x_{i1}, x_{i2}, ..., x_{i|gb_i|}\\}(i = 1, 2, ..., m)$.\nAlthough a single GB contains a complete subset of D, we need to use some attributes of GB to replace the inputs for accelerated computation. Common attributes are defined as follow:\nDefinition 2. (GB Attributes) [18] Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set, and $\\mathcal{G} = \\{gb_1, gb_2,..., gb_m\\}$ is a granular-ball space of $\\mathcal{U}$. For each $gb_i \\in \\mathcal{G}, gb_i = \\{x_{i1}, x_{i2}, ..., x_{i|gb_i|}\\}(i = 1,2,...,m)$, its attributes can be defined as follow:\n$c_{gb_i} = \\frac{1}{|gb_i|} \\sum_{j=1}^{|gb_i|} x_{ij}$\n$r_{gb_i} = \\frac{1}{|gb_i|} \\sum_{j=1}^{|gb_i|} ||x_{ij} - c_{gb_i}||,$\n$p_{gb_i} = \\frac{|\\{x_{ij} | l_{ij} = l_{gb_i}\\}|}{|gb_i|}$\n$l_{gb_i} = max(l_{ij} (j = 1, 2, ..., n)),$ (1)\nWhere $c_{gb_i}$ is the center, $r_{gb_i}$ is the radius, $l_{gb_i}$ is the label, and $p_{gb_i}$ is the purity of $gb_i$. $l_{ij}$ is the label of $x_{ij}(j = 1, 2, ..., |gb_i|)$.\nThe above attributes are commonly used in GBC, where the center and label of GB are generally used to replace all the samples in the GB as a input, and the radius and purity of GB are used as a weight of this input, which can distinguish the contributions of different GB.\nShadowed sets [38] was proposed by Pedrycz and con- structed through the fuzzy-rough transformation of fuzzy sets, which provided an effective tool to model and analyze the concepts with uncertainty. We can describe fuzzy-rough trans- formation as follows:"}, {"title": null, "content": "Definition 3. (Fuzzy Sets) [6] Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set. A fuzzy set A in U is characterized by a membership function $\\mu_{\\tilde{A}}: \\mathcal{U} \\rightarrow [0,1]$. For each element $x \\in \\mathcal{U}$, $\\mu_{\\tilde{A}}(x)$ represents the fuzzy membership of x in the fuzzy set A.\nDefinition 4. (Fuzzy-rough Transformation) [38] Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set. $\\mu_{\\tilde{A}}(x)$ is the fuzzy memberships of samples $x \\in \\mathcal{U}$ in fuzzy set A in $\\mathcal{U}$, mapping its into a triplet set $\\{0, [0,1],1\\}$ can be called fuzzy-rough transformation, and the mapping is formulated as follows:\n$\\mathcal{S}_\\mu(x) = \\begin{cases} 1, & \\mu_{\\tilde{A}}(x) \\geq 1 - \\alpha \\\\ [0,1], & \\alpha < \\mu_{\\tilde{A}}(x) < 1-\\alpha \\\\ 0, & \\mu_{\\tilde{A}}(x) \\leq \\alpha \\end{cases}$\nWhere $\\alpha \\in [0,0.5]$ is the threshold parameter, and after that mapping, we get a shadowed set by threshold pair $(\\alpha,1 - \\alpha)$.\nIn this transformation, memberships lower than $\\alpha$ will be reduced to 0, while memberships higher than $1 - \\alpha$ will be elevated to 1. So there will be a loss of membership and this loss can be called uncertainty variance [38] [39]. The uncertainty variance of transforming fuzzy memberships into a shadowed set can describe as follow:\nDefinition 5. (Uncertainty Variance) [39] Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set. $\\mu_{\\tilde{A}}(x)$ is the fuzzy memberships of samples $x \\in \\mathcal{U}$ in fuzzy set A in $\\mathcal{U}$, and $\\mathcal{S}_\\mu$ is a fuzzy-rough transformation on $\\tilde{A}$. The uncertainty variance $\\mathcal{V}(\\alpha)$ of this fuzzy-rough transformation is formulated as:\n$\\mathcal{V}(\\alpha) = \\sum_{x \\in \\mathcal{U}} (\\mu_{\\tilde{A}}(x) - \\mathcal{S}_\\mu(x))$\n$= \\sum_{\\mu_{\\tilde{A}}(x) < \\alpha} \\mu_{\\tilde{A}}(x) + \\sum_{\\mu_{\\tilde{A}}(x) \\geq 1-\\alpha} (1-\\mu_{\\tilde{A}}(x))$ (3)\nTo measure the uncertainty entropy in fuzzy set, Zhang [40] proposed the average fuzziness of a fuzzy set can describe as follow:\nDefinition 6. (Average Fuzziness) [40] Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set. $\\mu_{\\tilde{A}}(x)$ is the fuzzy memberships of samples $x \\in \\mathcal{U}$ in fuzzy set A in $\\mathcal{U}$, the fuzziness of x is defined as:\n$\\hbar(x) = 4\\mu_{\\tilde{A}}(x)(1 - \\mu_{\\tilde{A}}(x))$\nThe average fuzziness of $\\mathcal{U}$ is represented by:\n$\\Gamma(\\mathcal{U}) = \\frac{1}{|\\mathcal{U}|} \\sum_{x \\in \\mathcal{U}} \\hbar(x)$ (5)\nThe 3WD [41] is proposed as an extension of the commonly used binary decision model through adding a third option. In general, the approach of 3WD divides the universe into the positive, negative, and boundary regions which denote the regions of acceptance, rejection, and noncommitment. The 3WD on a fuzzy set can describe as follow:\nDefinition 7. (3WD) [41] Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non- empty finite set. $\\mu_{\\tilde{A}}(x)$ is the fuzzy memberships of samples $x \\in \\mathcal{U}$ in fuzzy set $\\tilde{A}$ in $\\mathcal{U}$, to divide it to three parts by 3WD under two thresholds $\\alpha < \\beta$ are defined as:\n$POS_{\\alpha,\\beta} = \\{x \\in \\mathcal{U} | \\mu_{\\tilde{A}}(x) \\geq \\alpha\\}$\n$NEG_{\\alpha,\\beta} = \\{x \\in \\mathcal{U} | \\mu_{\\tilde{A}}(x) \\leq \\beta\\}$\n$BND_{\\alpha,\\beta} = \\{x \\in \\mathcal{U} | \\alpha < \\mu_{\\tilde{A}}(x) < \\beta\\}$ (6)\nIII. GRANLUAR-BALL GENERATION BASED ON JUSTIFIABLE GRANULARITY\nFor GB generation, there are different approaches, e.g., using k-means [18] or using k-division [42] to split the GB. However, all generation methods can be summarized as starting with a GB that initially covers all samples, and then iteratively split it until a control condition is satisfied. The detailed process of GB generation method is outlined in Algo- rithm S1 of supplementary file. A key point in GB generation is setting the control condition, which directly affect the results of GB generation. Currently, most GB generation methods used purity as the control condition [18], [21], [23], [42], [43]. Although this approach offers the advantages such as fast computation and strong interpretability, it overlooks the uncertainty existed in original dataset. As shown in Fig. 1, GB generation with purity tend to generate more GBs when facing noise. This brings about two deficiencies as follows:\n(1) Overly split the final GBs, leading to a lack of justifiable granularity for problem solving and increase the number of GBs in subsequent calculations.\n(2) Generated GBs with noise label that impacts further compution and reduces the robustness of the GBC.\nTo solve the above problems, searching for a reasonable stop condition is critical. Pedrycz [26] proposed two criteria behind the principle of justifiable granularity, namely coverage and specificity, and they are conflict with each other [44]. Based on the coverage entropy and specificity entropy on GBs are defined as follows:\nDefinition 8. (Coverage Entropy) Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set, and $\\mathcal{G} = \\{gb_1, gb_2,..., gb_m\\}$ is a granular-ball space of $\\mathcal{U}$. For each $gb_i \\in \\mathcal{G}(i = 1,2,..., m)$, the coverage entropy of $gb_i$ is formulated as follow:\n$\\mathcal{H}_{gb_i} = \\frac{|gb_i|}{|\\mathcal{D}|} log \\frac{|gb_i|}{|\\mathcal{D}|}$ (7)\nThe coverage entropy of $\\mathcal{G}$ is formulated as follow:\n$\\mathcal{H} = \\sum_{gb_i \\in \\mathcal{G}} \\mathcal{H}_{gb_i}$ (8)\nDefinition 9. (Specificity Entropy) Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set, and $\\mathcal{G} = \\{gb_1, gb_2,..., gb_m\\}$ is a"}, {"title": null, "content": "granular-ball space of $\\mathcal{U}$. For each $gb_i \\in \\mathcal{G}(i = 1, 2, ..., m),$\n$gb_i = \\{x_{i1}, x_{i2}, ..., x_{i|gb_i|}\\}$, and the $lab = \\{l_{i1}, l_{i2}, ..., l_{i|gb_i|}\\}$,\nwhere $l_{ij}$ is the label of $x_{ij}(j = 1,2,..., |gb_i|)$. A label set\n$lab_{unique} = \\{l_k|l_k \\in lab\\}$ only contains distinct labels in $lab$.\nThe specificity entropy of $gb_i$ is formulated as follow:\n$\\mathcal{H}_{lab_i} = - \\sum_{l_k \\in lab_{unique}} \\frac{|gb_k|}{|gb_i|} log \\frac{|gb_k|}{|gb_i|}$ (9)\nWhere $|gb_k| = |\\{x_{ij}|l_{ij} = l_k\\}|$. The specificity entropy of $\\mathcal{G}$\nis formulated as follow:\n$\\mathcal{H}^\\mathcal{G}_{lab} = \\sum_{gb_i \\in \\mathcal{G}} \\frac{|gb_i|}{|\\mathcal{D}|} \\mathcal{H}_{lab_i}$ (10)\nCoverage entropy refers to a GB that covers as much exper- imental data as possible. From the properties of information entropy, it is easy to deduce that coverage entropy strictly increases with the split of GBs. That is, when there is only an initial GB, coverage entropy takes the minimum value of 0, and when all GBs are split into single sample, coverage entropy takes the maximum value. Specificity entropy refers to all GBs having distinct semantics or being well distinguished from each other. Similar to coverage entropy, specificity en- tropy takes the maximum value when there is only an initial GB, and takes the minimum value of 0 when all GBs are split into single sample. Specificity entropy decreases with the split of GB. The detailed proof for the monotonicity of coverage entropy and specificity entropy with the granularity being finer can be found in Section S2 of supplementary file. Therefore, coverage entropy and specificity entropy are contradictory. To balance these two entropies and achieve the justifiable granularity in the GB generation, a control condition is defined as follow:\nDefinition 10. (Measure of Justifiable Granularity) Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set, and $\\mathcal{G}$ is a granular- ball space of $\\mathcal{U}$. The measure of justifiable granularity on $\\mathcal{G}$\nis defined as follow:\n$\\mathcal{L}_\\mathcal{G} = \\theta \\mathcal{H} + \\varepsilon(1 - \\theta) \\mathcal{H}^\\mathcal{G}_{lab}$ (11)\nWhere $\\theta$ is a parameter that controls the weight of two entropies, $\\varepsilon$ is an adaptive parameter that balances the slope of two entropies, in this paper, we use $\\varepsilon = max \\mathcal{H}^\\mathcal{G} / max \\mathcal{H}^\\mathcal{G}_{lab}$.\nThe control condition of GB generation can be defined as follow:\n$arg min \\mathcal{L}_\\mathcal{G}$ (12)\nTo conveniently compute the minimum $\\mathcal{L}_\\mathcal{G}$, we use $\\Delta$ to denote the difference in $\\mathcal{L}_\\mathcal{G}$ after and before a GB split. When $\\Delta$ is negative, it indicates that this split will decrease $\\mathcal{L}_\\mathcal{G}$; when it\u2019s positive, it indicates that this split will increase $\\mathcal{L}_\\mathcal{G}$. When we consider the scenario where GB are split until purity = 1, the specificity entropy is already 0. Continuing to split will keep the specificity entropy at 0, while the coverage entropy will continue to increase. Therefore, when GBs are split until purity = 1, the split must have gone through the GBs with the minimum $\\mathcal{L}_\\mathcal{G}$, and further split will not result in the minimum $\\mathcal{L}_\\mathcal{G}$. So, purity = 1 will be a break condition in the process"}, {"title": null, "content": "of finding the minimum $\\mathcal{L}_\\mathcal{G}$. The GB generation method based on justifiable granularity is shown in Algorithm 1.\nAlgorithm 1: The GB Generation Method Based on Justifiable Granularity\nInput: Datesets D, parameter $\\theta$;\nOutput: The GBs $\\mathcal{G}$ with minimum $\\mathcal{L}_\\mathcal{G}$;\n1 initialize a gb on D, then current_G = $\\{gb\\}$;\n2 calculate $\\varepsilon$, Lcurrent_G by Definition 10;\n3 min_L, current_L = Lcurrent_G;\n4 Function Pre_Split_GB(gb):\n5 Split gb to $gb_1$ and $gb_2$;\n6 Calculate the $\\Delta = L_{\\{gb_1 \\cup gb_2\\}} - L_{\\{gb\\}};\n7 gb.\\Delta, gb.split \\leftarrow \\{gb_1 \\cup gb_2\\}$;\n8 return\n9 Function Main:\n10 Pre_Split_GB (current_G.top);\n11 while exist any gb in current_G : gb.p $\\neq$ 1 do\n12 top_gb = current_G.top;\n13 new_G $\\leftarrow$ top_gb.split;\n14 for gb in new_G do\n15 | Pre_Split_GB (gb);\n16 end\n17 current_G.delete(top_gb);\n18 for gb in new_G do\n19 | current_G.add(gb);\n20 end\n21 current_L $\\leftarrow L + GB.\\Delta$;\n22 if current_L < min_L then\n23 | best_G $\\leftarrow$ G;\n24 | min_L $\\leftarrow$ current_L;\n25 end\n26 Sort current_G by gb.\\Delta in ascending order;\n27 end\n28 return best_G;\n29 return\nIn simple terms, Algorithm 1 splits the $\\mathcal{G}$ by selecting a GB with the smallest $\\Delta$ each time. Repeated this operation, until the break condition purity = 1 is reached, then get the $\\mathcal{G}$ with the smallest $\\mathcal{L}_\\mathcal{G}$. To calculate the $\\Delta$ for each GB, we modify the GB split operation to a pre-processing split. When a GB is added to the $\\mathcal{G}$, it attempts to split the GB and records the $\\Delta$ and split result. When this GB needs to be split, the pre-processing split result can be directly used. This pre-processing split does not increase the time complexity of GB generation, and each GB still requires only one split operation. The main difference between Algorithm 1 and traditional GB generation in Section S1 of supplementary file is that it only split a GB with the smallest $\\Delta$ each time. This ensures that $\\mathcal{L}_\\mathcal{G}$ always decreases in the fastest direction after each split. To implement this operation, we need to maintain a min-heap to record the $\\mathcal{G}$. The push and pop operations on the min-heap are both less than $\\mathcal{O}(log n)$, and in each round of the loop, whether it is based on k-means or k-division, the time complexity of split is $\\mathcal{O}(n)$. Overall, Algorithm 1 does not increase the time complexity compared with traditional GB"}, {"title": "IV. THREE-WAY CLASSIFICATION WITH SHADOWED GRANLUAR-BALLS", "content": "GBC uses GB attributes in Definition 2 to replace the inputs, which results in each sample contributing equally to the GB it belongs to. For example, when faces with classification problems in GBC, GB attributes only offer the center and radius of each GB, and predicted sample only have two states, in or not in the GB. Although this method is simple enough, it ignores the high uncertainty of sample points near the radius, in other words, it lacks robustness. To address this issue, we construct shadowed GBs based on 3WD theory. First, the GB membership for samples is defined as follow:\nDefinition 11. (GB Membership) Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set, and $\\mathcal{G} = \\{gb_1, gb_2,..., gb_m\\}$ is a granular-ball space of $\\mathcal{U}$. Given a membership function $\\mu_{gb_i} : gb \\rightarrow [0,1]$ to each $gb_i \\in \\mathcal{G}(i = 1,2,..., m)$. The GB membership for each sample $x \\in gb_i$ is formulated as follow:\n$\\mu_{gb_i}(x) = exp \\left( - \\frac{d(x, c_{gb_i})^2}{2\\sigma^2 r_{gb_i}} \\right)$ (13)\nWhere $\\sigma$ is the function order, $c_{gb_i}$ is the center of $gb_i$, $r_{gb_i}$ is the radius of $gb_i$, and $d(x, c_{gb_i})$ is the distance between x and $c_{gb_i}$.\nWhen a sample is at the center of GB, its GB membership takes the maximum value of 1, and it decreases as the sample moves away from the center. In this paper, we set $\\sigma = 1$, so the membership of sample near the GB radius is close to 0.5. This is related to the calculation of the GB radius using the mean value. If the GB radius calculation uses the maximum value, $\\sigma$ needs to be adjusted to ensure that the membership is reasonable. Then, based on the fuzzy-rough transformation theory, we defined the shadowed GB as follow:\nDefinition 12. (Shadowed GB) Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set, and $\\mathcal{G} = \\{gb_1, gb_2,..., gb_m\\}$ is a granular-ball space of $\\mathcal{U}$. For each $gb_i \\in \\mathcal{G}(i = 1,2,..., m)$, a mapping that maps GB memberships into a triplet set $\\{0, [0,1], 1\\}$ can be called shadowed GB, and the mapping is formulated as follows:\n$\\mathcal{S}u_{gb_i}(x) = \\begin{cases} 1, & \\mu_{gb_i}(x) \\geq 1 - \\alpha \\\\ \\mu_{gb_i}(x), & \\alpha < \\mu_{gb_i}(x) < 1-\\alpha \\\\ 0, & \\mu_{gb_i}(x) \\leq \\alpha \\end{cases}$ (14)\nWhere $x \\in gb_i$, and $(\\alpha,1 - \\alpha)$ is the threshold pair of shadowed GB.\nThe shadowed GB is shown in Fig. 2. Membership lower than $\\alpha$ are mapped to 0, while those higher than $1 - \\alpha$ are mapped to 1; the rest remain in fuzzy case. According to Definition 5, actions that map will cause a loss of uncertainty, which called uncertainty variance, and it can be understood as the green area in Fig. 2. According to Definition 6, remain the GB membership in fuzzy case will result in fuzziness, and it can be understood as the yellow area in Fig. 2."}, {"title": null, "content": "We analyzed the changes of uncertainty variance and fuzzi- ness with the changing of $\\alpha$, as shown in Fig. 3. When $\\alpha$ decreases, fewer GB memberships are mapped to certain nega- tive and certain positive regions, meaning uncertainty variance decreases. Conversely, more GB memberships remain in fuzzy case, resulting in an increase in fuzziness. This indicates that uncertainty variance and fuzziness are conflicting criteria. We utilize GB membership to distinguish the contribution of samples to their GBs. However, if the uncertainty variance is excessively high, it will cause many samples to collapse into binary states, rendering the GB membership ineffective. Thus, it is crucial to keep the uncertainty variance as low as possible. Additionally, we employ shadowed GB to expedite further computations. However, if the fuzziness is too high, it will result in an excessive number of fuzzy memberships that must be processed, defeating the purpose of using shadowed GB. Therefore, it is essential to minimize fuzziness to maintain the efficiency of the shadowed GB. We need to search for a balance point between uncertainty variance and fuzziness. To achieve this, we defined the optimal $\\alpha$ as follow:\nDefinition 13. (The Optimal $\\alpha$) Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set, and $\\mathcal{G} = \\{gb_1, gb_2,..., gb_m\\}$ is a granular-ball space of $\\mathcal{U}$. For each $gb_i \\in \\mathcal{G}(i = 1, 2, ..., m),"}, {"title": null, "content": "$gb_i = \\{x_{i1}, x_{i2}, ..., x_{i|gb_i|}\\}$, uses $(\\alpha,1 - \\alpha)$ as threshold pair to build shadowed GB. The measure of uncertainty variance and fuzziness in that building is defined as follow:\n$f(\\alpha) = \\frac{\\xi(\\sum_{\\mu_{gb_i}(x_{ij})0$\\nAfter finding the labels of controlled regions, if they are completely consistent, e.g., COR = 1, there is no reason to proceed with further computation. The sample will be classified with the same label of controlled regions. However,"}, {"title": null, "content": "when facing inconsistent labels from controlled regions, we need different principles based on the type of controlled region as follow:\n(1) If |COR| > 1, it means the sample belongs to a high- risk classification category, and we should delay the classification of this sample. In this paper, delay the classification equal to classify it into uncertain case.\n(2) If |IMP| > 1, we need calculate the $proportion(l_{gb_i}) = \\sum_{(l_{gb_i}, \\mu_{gb_j}(\\xi)) \\in IMP} \\mu_{gb_j}(\\xi)$. If $proportion(l_{gb_i}) > 50\\%$, it means that the labels of controlled regions are relatively consistent, and the sample can be classified with $l_{gb_i}$. Otherwise, the sample has a high classification risk, and we should classify it into uncertain case.\n(3) If the type is UNE, indicating that the sample does not belong to any shadowed GB. In other words, there are no references in the training data to classify this sample. This suggests a high classification risk, and we should classify it into uncertain case.\nThe overall process is illustrated in Fig. 4. When faced with samples that need classification, we follow these three steps:\nStep.1: Build the shadowed GBs by fuzzy-rough transforma- tion according to $(\\alpha, 1 - \\alpha)$.\nStep.2: Find the controlled regions that impact each sample.\nStep.3: Classify the samples to certain class or uncertain case by our proposed principles."}, {"title": "V. EXPERIMENTAL STUDIES", "content": "In this section, we comprehensively compared our pro- posed 3WC-SGB with other three-way classifiers (3WC-FNC [45], 3WC-SVM [46]), state-of-the-art GB-based classifiers"}, {"title": null, "content": "(GBKNN [18]", "parts": "n\u2022 Comparison with three-way classifiers to verify the ad- vantages of applying shadowed GBs to 3WC-SGB.\n\u2022 Comparison with state-of-the-art GB-based classifiers to demonstrate the advantages of applying three-way clas- sification to 3WC-SGB.\n\u2022 Comparison with traditional classifiers and conducted a comprehensive statistical analysis on all compared clas- sifiers to demonstrate the significant advantages of 3WC- SGB.\n\u2022 Investigated the hyperparameter configuration of 3WC- SGB and proposed a computational method for recom- mending optimal hyperparameter.\nAll experiments were conducted on an Intel i```json\n{"}, {"title": "A robust three-way classifier with shadowed granular-balls based on justifiable granularity", "authors": ["Jie Yang", "Lingyun Xiaodiao", "Guoyin Wang", "Witold Pedrycz", "Shuyin Xia", "Qinghua Zhang", "Di Wu"], "abstract": "The granular-ball (GB)-based classifier introduced by Xia, exhibits adaptability in creating coarse-grained information granules for input, thereby enhancing its generality and flexibility. Nevertheless, the current GB-based classifiers rigidly assign a specific class label to each data instance and lacks of the necessary strategies to address uncertain instances. These far-fetched certain classification approachs toward uncertain instances may suffer considerable risks. To solve this problem, we construct a robust three-way classifier with shadowed GBs for uncertain data. Firstly, combine with information entropy, we propose an enhanced GB generation method with the principle of justifiable granularity. Subsequently, based on minimum uncertainty, a shadowed mapping is utilized to partition a GB into Core region, Important region and Unessential region. Based on the constructed shadowed GBs, we establish a three-way classifier to categorize data instances into certain classes and uncertain case. Finally, extensive comparative experiments are conducted with 2 three-way classifiers, 3 state-of-the-art GB-based classifiers, and 3 classical machine learning classifiers on 12 public benchmark datasets. The results show that our model demonstrates robustness in managing uncertain data and effectively mitigates classification risks. Furthermore, our model almost outperforms the other comparison methods in both effectiveness and efficiency.", "sections": [{"title": "I. INTRODUCTION", "content": "Granular computing (GrC) [1]\u2013[4] focuses on the forma- tion, processing, and communication of information granules, aiming to mimic human cognitive thinking in addressing complex problems. Rough sets [5], fuzzy sets [6], and quotient spaces [7] are three primary models of GrC. Rough sets are widely used to measure the uncertainty and incompleteness inherent in information systems. To handle continuous data, neighborhood rough sets (NRS) [8] introduce the concept of neighborhood granulation to convert the equivalence relation into the covering relation in neighborhood space. Furthermore, numerous extended GrC-based classifiers [9]\u2013[13] were de- veloped by utilizing information granules as the fundamental computational unit, enhancing the efficiency of knowledge discovery. However, a notable limitation of these classifiers is that they primarily treat granules as a preliminary fea- ture processing method, without modifying the underlying mathematical model or elevating the core performance of the classifiers themselves.\nBased on the idea of GrC, granular-ball computing (GBC) [14]\u2013[16], introduced by Xia, represents a groundbreaking approach to data processing and knowledge representation. This method replaces traditional information granule inputs with granular-balls (GBs), adhering to the principle of global topology precedence [17]. Over the years, GBC has undergone continuous advancements in terms of its methodologies and applications. Xia [18] proposed the GB-based kNN, which sig- nificantly outperforms the existing kNN in terms of efficiency, especially when dealing with large-scale datasets, achieving an improvement of hundreds of times. Furthermore, Xia [19] introduced a novel rough sets model, namely, granular-ball rough sets (GBRS). Compared to traditional NRS methods, GBRS stands out as a multi-granularity learning tool, offering greater robustness and efficiency by substituting neighborhood granules with GBs. In addition, Chen [20] proposed a GB- based attribute selector to achieve the superior classification performance through effective feature reduction. Xie [21] introduced a fast and stable GB generation method based on the attention mechanism. Xie [22] constructed a multi- granularity representation of the data using the GBC model, thereby boosting the algorithm\u2019s time efficiency. Chen [23] innovated by introducing a novel GB-based density peaks clustering, resulting in significantly reduced runtime without the need for parameterization. However, current researches on GB generation in GBC only consider the purity thresholds to control the granularity of GB space, which is not in accord with various practical applications.\nFrom the perspective of the justifiable granularity principle [24]\u2013[26], both coverage and specificity should be taken into account during the GB generation process. Coverage refers to a GB that encompass as much experimental evidence (data) as possible. Specificity refers to all GBs generated on the universe having distinct and well-defined semantics or all GBs being well distinguished from each other. Typically, the"}, {"title": null, "content": "fewer the data instances in a GB, the higher the specificity. Therefore, it is significant to generate justifiable GB space to solve problems by combining with the level of coverage and specificity. Moreover, the above GB-based classification methods rigidly ascribe a fixed class label to each data in- stance, lacking strategies to handle instances with uncertainty. The methodology of uncertain data classification is invaluable in mitigating decision risk and enhancing decision efficiency through human-machine collaboration, thus playing a pivotal role in decision support systems. For instance, when applying classification methods to develop a computer-aided diagnosis (CAD) system for liver cancer [27], it is crucial to accurately classify uncertain tumors for further cautious diagnosis and certain far-fetched classifications produced by the system may cause serious costs.\nAs is well known, three-way decision (3WD) [28]\u2013[30] theory proposed by Yao has emerged as a promising approach to tackle complex problems involving uncertainty. The fun- damental principle of 3WD lies in dividing a universe into three distinct regions, with each region corresponding to a specific decision action. As a generalization of the traditional two-way decision model, 3WD further incorporates a third option, enabling a trisecting-and-acting approach to decision- making. Currently, 3WD has found widespread application across various fields [31]\u2013[34]. Xu [35] enhanced the flexi- bility and evolution capability of concept learning, addressing the limitations of existing two-way learning approaches by incorporating a novel cognitive mechanism and movement 3WD method. Zhang [36] introduced an efficient multi-scale decision system method that integrates a sequential 3WD model with the Hasse diagram to optimize scale combination selection and attribute reduction. Du [37] presented a novel multistep three-way clustering algorithm that enhances the accuracy and adaptability of cluster representations. In term of the advantage of 3WD, to address the limitations of GB- based classification models in classifying uncertain data, we introduce 3WD to construct the three-way approximations of GBs, which is called shadowed GBs. More detailed, the traditional GBs are extended to shadowed GBs for both certain regions and uncertain boundaries from the perspective of uncertainty, and shadowed GBs constructs a tripartitioned approximation of data distribution for three-way classification. The data instances will be categorized into a certain class or an uncertain case based on their locations relative to the shadowed GBs. Specifically, the Core region of GBs corresponding to the same class is confidently determined the class of instances, whereas the Important region has uncertainty for classification, and the Unessential region has almost no contribution to classification. This ensures more effective to classify uncertain data. The major contributions of this paper are summed up as follows:\n(1) Combine with information entropy, we propose an im- proved GB generation method based on the principle of justifiable granularity.\n(2) We introduce 3WD to construct and optimize shadowed GBs for modeling uncertain data.\n(3) We further establish a three-way classifier with shadowed GBs to categorize data instances into certain classes and uncertain case.\nThe following sections of this paper are structured with the subsequent manner. In section II is a review of related prelim- inary definitions. In Section III, an improved GB generation method is constructed based on the principle of justifiable granularity. In Section IV introduces the construction of shad- owed GBs. Then, a three-way classifier with shadowed GBs is presented. The relevant experiments for the verification of viability and rationality of our models are shown in Section V. Finally, Section VI summarizes the conclusions."}, {"title": "II. PRELIMINARIES", "content": "In this section, to set up the framework of this paper, we recall some necessary definitions related to GBC, shadowed sets and 3WD.\nGBC is first proposed by Xia [18], which is to use GBs to cover the sample space and replace traditional information granules with GBs for data processing. We first define the GBC as follow:\nDefinition 1. (GBC) [18] Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non- empty finite set. Generating $\\mathcal{G}$ to replace the inputs for computation can be called GBC. Where $\\mathcal{G} = \\{gb_1, gb_2,..., gb_m\\}$ is a granular-ball space of $\\mathcal{U}$, and a granular-ball $gb_i$ covers a subset of $\\mathcal{U}$ such that $gb_i = \\{x_{i1}, x_{i2}, ..., x_{i|gb_i|}\\}(i = 1, 2, ..., m)$.\nAlthough a single GB contains a complete subset of D, we need to use some attributes of GB to replace the inputs for accelerated computation. Common attributes are defined as follow:\nDefinition 2. (GB Attributes) [18] Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set, and $\\mathcal{G} = \\{gb_1, gb_2,..., gb_m\\}$ is a granular-ball space of $\\mathcal{U}$. For each $gb_i \\in \\mathcal{G}, gb_i = \\{x_{i1}, x_{i2}, ..., x_{i|gb_i|}\\}(i = 1,2,...,m)$, its attributes can be defined as follow:\n$c_{gb_i} = \\frac{1}{|gb_i|} \\sum_{j=1}^{|gb_i|} x_{ij}$\n$r_{gb_i} = \\frac{1}{|gb_i|} \\sum_{j=1}^{|gb_i|} ||x_{ij} - c_{gb_i}||,$\n$p_{gb_i} = \\frac{|\\{x_{ij} | l_{ij} = l_{gb_i}\\}|}{|gb_i|}$\n$l_{gb_i} = max(l_{ij} (j = 1, 2, ..., n)),$ (1)\nWhere $c_{gb_i}$ is the center, $r_{gb_i}$ is the radius, $l_{gb_i}$ is the label, and $p_{gb_i}$ is the purity of $gb_i$. $l_{ij}$ is the label of $x_{ij}(j = 1, 2, ..., |gb_i|)$.\nThe above attributes are commonly used in GBC, where the center and label of GB are generally used to replace all the samples in the GB as a input, and the radius and purity of GB are used as a weight of this input, which can distinguish the contributions of different GB.\nShadowed sets [38] was proposed by Pedrycz and con- structed through the fuzzy-rough transformation of fuzzy sets, which provided an effective tool to model and analyze the concepts with uncertainty. We can describe fuzzy-rough trans- formation as follows:"}, {"title": null, "content": "Definition 3. (Fuzzy Sets) [6] Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set. A fuzzy set A in U is characterized by a membership function $\\mu_{\\tilde{A}}: \\mathcal{U} \\rightarrow [0,1]$. For each element $x \\in \\mathcal{U}$, $\\mu_{\\tilde{A}}(x)$ represents the fuzzy membership of x in the fuzzy set A.\nDefinition 4. (Fuzzy-rough Transformation) [38] Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set. $\\mu_{\\tilde{A}}(x)$ is the fuzzy memberships of samples $x \\in \\mathcal{U}$ in fuzzy set A in $\\mathcal{U}$, mapping its into a triplet set $\\{0, [0,1],1\\}$ can be called fuzzy-rough transformation, and the mapping is formulated as follows:\n$\\mathcal{S}_\\mu(x) = \\begin{cases} 1, & \\mu_{\\tilde{A}}(x) \\geq 1 - \\alpha \\\\ [0,1], & \\alpha < \\mu_{\\tilde{A}}(x) < 1-\\alpha \\\\ 0, & \\mu_{\\tilde{A}}(x) \\leq \\alpha \\end{cases}$\nWhere $\\alpha \\in [0,0.5]$ is the threshold parameter, and after that mapping, we get a shadowed set by threshold pair $(\\alpha,1 - \\alpha)$.\nIn this transformation, memberships lower than $\\alpha$ will be reduced to 0, while memberships higher than $1 - \\alpha$ will be elevated to 1. So there will be a loss of membership and this loss can be called uncertainty variance [38] [39]. The uncertainty variance of transforming fuzzy memberships into a shadowed set can describe as follow:\nDefinition 5. (Uncertainty Variance) [39] Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set. $\\mu_{\\tilde{A}}(x)$ is the fuzzy memberships of samples $x \\in \\mathcal{U}$ in fuzzy set A in $\\mathcal{U}$, and $\\mathcal{S}_\\mu$ is a fuzzy-rough transformation on $\\tilde{A}$. The uncertainty variance $\\mathcal{V}(\\alpha)$ of this fuzzy-rough transformation is formulated as:\n$\\mathcal{V}(\\alpha) = \\sum_{x \\in \\mathcal{U}} (\\mu_{\\tilde{A}}(x) - \\mathcal{S}_\\mu(x))$\n$= \\sum_{\\mu_{\\tilde{A}}(x) < \\alpha} \\mu_{\\tilde{A}}(x) + \\sum_{\\mu_{\\tilde{A}}(x) \\geq 1-\\alpha} (1-\\mu_{\\tilde{A}}(x))$ (3)\nTo measure the uncertainty entropy in fuzzy set, Zhang [40] proposed the average fuzziness of a fuzzy set can describe as follow:\nDefinition 6. (Average Fuzziness) [40] Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set. $\\mu_{\\tilde{A}}(x)$ is the fuzzy memberships of samples $x \\in \\mathcal{U}$ in fuzzy set A in $\\mathcal{U}$, the fuzziness of x is defined as:\n$\\hbar(x) = 4\\mu_{\\tilde{A}}(x)(1 - \\mu_{\\tilde{A}}(x))$\nThe average fuzziness of $\\mathcal{U}$ is represented by:\n$\\Gamma(\\mathcal{U}) = \\frac{1}{|\\mathcal{U}|} \\sum_{x \\in \\mathcal{U}} \\hbar(x)$ (5)\nThe 3WD [41] is proposed as an extension of the commonly used binary decision model through adding a third option. In general, the approach of 3WD divides the universe into the positive, negative, and boundary regions which denote the regions of acceptance, rejection, and noncommitment. The 3WD on a fuzzy set can describe as follow:\nDefinition 7. (3WD) [41] Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non- empty finite set. $\\mu_{\\tilde{A}}(x)$ is the fuzzy memberships of samples $x \\in \\mathcal{U}$ in fuzzy set $\\tilde{A}$ in $\\mathcal{U}$, to divide it to three parts by 3WD under two thresholds $\\alpha < \\beta$ are defined as:\n$POS_{\\alpha,\\beta} = \\{x \\in \\mathcal{U} | \\mu_{\\tilde{A}}(x) \\geq \\alpha\\}$\n$NEG_{\\alpha,\\beta} = \\{x \\in \\mathcal{U} | \\mu_{\\tilde{A}}(x) \\leq \\beta\\}$\n$BND_{\\alpha,\\beta} = \\{x \\in \\mathcal{U} | \\alpha < \\mu_{\\tilde{A}}(x) < \\beta\\}$ (6)\nIII. GRANLUAR-BALL GENERATION BASED ON JUSTIFIABLE GRANULARITY\nFor GB generation, there are different approaches, e.g., using k-means [18] or using k-division [42] to split the GB. However, all generation methods can be summarized as starting with a GB that initially covers all samples, and then iteratively split it until a control condition is satisfied. The detailed process of GB generation method is outlined in Algo- rithm S1 of supplementary file. A key point in GB generation is setting the control condition, which directly affect the results of GB generation. Currently, most GB generation methods used purity as the control condition [18], [21], [23], [42], [43]. Although this approach offers the advantages such as fast computation and strong interpretability, it overlooks the uncertainty existed in original dataset. As shown in Fig. 1, GB generation with purity tend to generate more GBs when facing noise. This brings about two deficiencies as follows:\n(1) Overly split the final GBs, leading to a lack of justifiable granularity for problem solving and increase the number of GBs in subsequent calculations.\n(2) Generated GBs with noise label that impacts further compution and reduces the robustness of the GBC.\nTo solve the above problems, searching for a reasonable stop condition is critical. Pedrycz [26] proposed two criteria behind the principle of justifiable granularity, namely coverage and specificity, and they are conflict with each other [44]. Based on the coverage entropy and specificity entropy on GBs are defined as follows:\nDefinition 8. (Coverage Entropy) Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set, and $\\mathcal{G} = \\{gb_1, gb_2,..., gb_m\\}$ is a granular-ball space of $\\mathcal{U}$. For each $gb_i \\in \\mathcal{G}(i = 1,2,..., m)$, the coverage entropy of $gb_i$ is formulated as follow:\n$\\mathcal{H}_{gb_i} = \\frac{|gb_i|}{|\\mathcal{D}|} log \\frac{|gb_i|}{|\\mathcal{D}|}$ (7)\nThe coverage entropy of $\\mathcal{G}$ is formulated as follow:\n$\\mathcal{H} = \\sum_{gb_i \\in \\mathcal{G}} \\mathcal{H}_{gb_i}$ (8)\nDefinition 9. (Specificity Entropy) Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set, and $\\mathcal{G} = \\{gb_1, gb_2,..., gb_m\\}$ is a"}, {"title": null, "content": "granular-ball space of $\\mathcal{U}$. For each $gb_i \\in \\mathcal{G}(i = 1, 2, ..., m),$\n$gb_i = \\{x_{i1}, x_{i2}, ..., x_{i|gb_i|}\\}$, and the $lab = \\{l_{i1}, l_{i2}, ..., l_{i|gb_i|}\\}$,\nwhere $l_{ij}$ is the label of $x_{ij}(j = 1,2,..., |gb_i|)$. A label set\n$lab_{unique} = \\{l_k|l_k \\in lab\\}$ only contains distinct labels in $lab$.\nThe specificity entropy of $gb_i$ is formulated as follow:\n$\\mathcal{H}_{lab_i} = - \\sum_{l_k \\in lab_{unique}} \\frac{|gb_k|}{|gb_i|} log \\frac{|gb_k|}{|gb_i|}$ (9)\nWhere $|gb_k| = |\\{x_{ij}|l_{ij} = l_k\\}|$. The specificity entropy of $\\mathcal{G}$\nis formulated as follow:\n$\\mathcal{H}^\\mathcal{G}_{lab} = \\sum_{gb_i \\in \\mathcal{G}} \\frac{|gb_i|}{|\\mathcal{D}|} \\mathcal{H}_{lab_i}$ (10)\nCoverage entropy refers to a GB that covers as much exper- imental data as possible. From the properties of information entropy, it is easy to deduce that coverage entropy strictly increases with the split of GBs. That is, when there is only an initial GB, coverage entropy takes the minimum value of 0, and when all GBs are split into single sample, coverage entropy takes the maximum value. Specificity entropy refers to all GBs having distinct semantics or being well distinguished from each other. Similar to coverage entropy, specificity en- tropy takes the maximum value when there is only an initial GB, and takes the minimum value of 0 when all GBs are split into single sample. Specificity entropy decreases with the split of GB. The detailed proof for the monotonicity of coverage entropy and specificity entropy with the granularity being finer can be found in Section S2 of supplementary file. Therefore, coverage entropy and specificity entropy are contradictory. To balance these two entropies and achieve the justifiable granularity in the GB generation, a control condition is defined as follow:\nDefinition 10. (Measure of Justifiable Granularity) Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set, and $\\mathcal{G}$ is a granular- ball space of $\\mathcal{U}$. The measure of justifiable granularity on $\\mathcal{G}$\nis defined as follow:\n$\\mathcal{L}_\\mathcal{G} = \\theta \\mathcal{H} + \\varepsilon(1 - \\theta) \\mathcal{H}^\\mathcal{G}_{lab}$ (11)\nWhere $\\theta$ is a parameter that controls the weight of two entropies, $\\varepsilon$ is an adaptive parameter that balances the slope of two entropies, in this paper, we use $\\varepsilon = max \\mathcal{H}^\\mathcal{G} / max \\mathcal{H}^\\mathcal{G}_{lab}$.\nThe control condition of GB generation can be defined as follow:\n$arg min \\mathcal{L}_\\mathcal{G}$ (12)\nTo conveniently compute the minimum $\\mathcal{L}_\\mathcal{G}$, we use $\\Delta$ to denote the difference in $\\mathcal{L}_\\mathcal{G}$ after and before a GB split. When $\\Delta$ is negative, it indicates that this split will decrease $\\mathcal{L}_\\mathcal{G}$; when it\u2019s positive, it indicates that this split will increase $\\mathcal{L}_\\mathcal{G}$. When we consider the scenario where GB are split until purity = 1, the specificity entropy is already 0. Continuing to split will keep the specificity entropy at 0, while the coverage entropy will continue to increase. Therefore, when GBs are split until purity = 1, the split must have gone through the GBs with the minimum $\\mathcal{L}_\\mathcal{G}$, and further split will not result in the minimum $\\mathcal{L}_\\mathcal{G}$. So, purity = 1 will be a break condition in the process"}, {"title": null, "content": "of finding the minimum $\\mathcal{L}_\\mathcal{G}$. The GB generation method based on justifiable granularity is shown in Algorithm 1.\nAlgorithm 1: The GB Generation Method Based on Justifiable Granularity\nInput: Datesets D, parameter $\\theta$;\nOutput: The GBs $\\mathcal{G}$ with minimum $\\mathcal{L}_\\mathcal{G}$;\n1 initialize a gb on D, then current_G = $\\{gb\\}$;\n2 calculate $\\varepsilon$, Lcurrent_G by Definition 10;\n3 min_L, current_L = Lcurrent_G;\n4 Function Pre_Split_GB(gb):\n5 Split gb to $gb_1$ and $gb_2$;\n6 Calculate the $\\Delta = L_{\\{gb_1 \\cup gb_2\\}} - L_{\\{gb\\}};\n7 gb.\\Delta, gb.split \\leftarrow \\{gb_1 \\cup gb_2\\}$;\n8 return\n9 Function Main:\n10 Pre_Split_GB (current_G.top);\n11 while exist any gb in current_G : gb.p $\\neq$ 1 do\n12 top_gb = current_G.top;\n13 new_G $\\leftarrow$ top_gb.split;\n14 for gb in new_G do\n15 | Pre_Split_GB (gb);\n16 end\n17 current_G.delete(top_gb);\n18 for gb in new_G do\n19 | current_G.add(gb);\n20 end\n21 current_L $\\leftarrow L + GB.\\Delta$;\n22 if current_L < min_L then\n23 | best_G $\\leftarrow$ G;\n24 | min_L $\\leftarrow$ current_L;\n25 end\n26 Sort current_G by gb.\\Delta in ascending order;\n27 end\n28 return best_G;\n29 return\nIn simple terms, Algorithm 1 splits the $\\mathcal{G}$ by selecting a GB with the smallest $\\Delta$ each time. Repeated this operation, until the break condition purity = 1 is reached, then get the $\\mathcal{G}$ with the smallest $\\mathcal{L}_\\mathcal{G}$. To calculate the $\\Delta$ for each GB, we modify the GB split operation to a pre-processing split. When a GB is added to the $\\mathcal{G}$, it attempts to split the GB and records the $\\Delta$ and split result. When this GB needs to be split, the pre-processing split result can be directly used. This pre-processing split does not increase the time complexity of GB generation, and each GB still requires only one split operation. The main difference between Algorithm 1 and traditional GB generation in Section S1 of supplementary file is that it only split a GB with the smallest $\\Delta$ each time. This ensures that $\\mathcal{L}_\\mathcal{G}$ always decreases in the fastest direction after each split. To implement this operation, we need to maintain a min-heap to record the $\\mathcal{G}$. The push and pop operations on the min-heap are both less than $\\mathcal{O}(log n)$, and in each round of the loop, whether it is based on k-means or k-division, the time complexity of split is $\\mathcal{O}(n)$. Overall, Algorithm 1 does not increase the time complexity compared with traditional GB"}, {"title": "IV. THREE-WAY CLASSIFICATION WITH SHADOWED GRANLUAR-BALLS", "content": "GBC uses GB attributes in Definition 2 to replace the inputs, which results in each sample contributing equally to the GB it belongs to. For example, when faces with classification problems in GBC, GB attributes only offer the center and radius of each GB, and predicted sample only have two states, in or not in the GB. Although this method is simple enough, it ignores the high uncertainty of sample points near the radius, in other words, it lacks robustness. To address this issue, we construct shadowed GBs based on 3WD theory. First, the GB membership for samples is defined as follow:\nDefinition 11. (GB Membership) Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set, and $\\mathcal{G} = \\{gb_1, gb_2,..., gb_m\\}$ is a granular-ball space of $\\mathcal{U}$. Given a membership function $\\mu_{gb_i} : gb \\rightarrow [0,1]$ to each $gb_i \\in \\mathcal{G}(i = 1,2,..., m)$. The GB membership for each sample $x \\in gb_i$ is formulated as follow:\n$\\mu_{gb_i}(x) = exp \\left( - \\frac{d(x, c_{gb_i})^2}{2\\sigma^2 r_{gb_i}} \\right)$ (13)\nWhere $\\sigma$ is the function order, $c_{gb_i}$ is the center of $gb_i$, $r_{gb_i}$ is the radius of $gb_i$, and $d(x, c_{gb_i})$ is the distance between x and $c_{gb_i}$.\nWhen a sample is at the center of GB, its GB membership takes the maximum value of 1, and it decreases as the sample moves away from the center. In this paper, we set $\\sigma = 1$, so the membership of sample near the GB radius is close to 0.5. This is related to the calculation of the GB radius using the mean value. If the GB radius calculation uses the maximum value, $\\sigma$ needs to be adjusted to ensure that the membership is reasonable. Then, based on the fuzzy-rough transformation theory, we defined the shadowed GB as follow:\nDefinition 12. (Shadowed GB) Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set, and $\\mathcal{G} = \\{gb_1, gb_2,..., gb_m\\}$ is a granular-ball space of $\\mathcal{U}$. For each $gb_i \\in \\mathcal{G}(i = 1,2,..., m)$, a mapping that maps GB memberships into a triplet set $\\{0, [0,1], 1\\}$ can be called shadowed GB, and the mapping is formulated as follows:\n$\\mathcal{S}u_{gb_i}(x) = \\begin{cases} 1, & \\mu_{gb_i}(x) \\geq 1 - \\alpha \\\\ \\mu_{gb_i}(x), & \\alpha < \\mu_{gb_i}(x) < 1-\\alpha \\\\ 0, & \\mu_{gb_i}(x) \\leq \\alpha \\end{cases}$ (14)\nWhere $x \\in gb_i$, and $(\\alpha,1 - \\alpha)$ is the threshold pair of shadowed GB.\nThe shadowed GB is shown in Fig. 2. Membership lower than $\\alpha$ are mapped to 0, while those higher than $1 - \\alpha$ are mapped to 1; the rest remain in fuzzy case. According to Definition 5, actions that map will cause a loss of uncertainty, which called uncertainty variance, and it can be understood as the green area in Fig. 2. According to Definition 6, remain the GB membership in fuzzy case will result in fuzziness, and it can be understood as the yellow area in Fig. 2."}, {"title": null, "content": "We analyzed the changes of uncertainty variance and fuzzi- ness with the changing of $\\alpha$, as shown in Fig. 3. When $\\alpha$ decreases, fewer GB memberships are mapped to certain nega- tive and certain positive regions, meaning uncertainty variance decreases. Conversely, more GB memberships remain in fuzzy case, resulting in an increase in fuzziness. This indicates that uncertainty variance and fuzziness are conflicting criteria. We utilize GB membership to distinguish the contribution of samples to their GBs. However, if the uncertainty variance is excessively high, it will cause many samples to collapse into binary states, rendering the GB membership ineffective. Thus, it is crucial to keep the uncertainty variance as low as possible. Additionally, we employ shadowed GB to expedite further computations. However, if the fuzziness is too high, it will result in an excessive number of fuzzy memberships that must be processed, defeating the purpose of using shadowed GB. Therefore, it is essential to minimize fuzziness to maintain the efficiency of the shadowed GB. We need to search for a balance point between uncertainty variance and fuzziness. To achieve this, we defined the optimal $\\alpha$ as follow:\nDefinition 13. (The Optimal $\\alpha$) Let $\\mathcal{U} = \\{x_1, x_2, ..., x_n\\}$ be a non-empty finite set, and $\\mathcal{G} = \\{gb_1, gb_2,..., gb_m\\}$ is a granular-ball space of $\\mathcal{U}$. For each $gb_i \\in \\mathcal{G}(i = 1, 2, ..., m),"}, {"title": null, "content": "$gb_i = \\{x_{i1}, x_{i2}, ..., x_{i|gb_i|}\\}$, uses $(\\alpha,1 - \\alpha)$ as threshold pair to build shadowed GB. The measure of uncertainty variance and fuzziness in that building is defined as follow:\n$f(\\alpha) = \\frac{\\xi(\\sum_{\\mu_{gb_i}(x_{ij})0$\\nAfter finding the labels of controlled regions, if they are completely consistent, e.g., COR = 1, there is no reason to proceed with further computation. The sample will be classified with the same label of controlled regions. However,"}, {"title": null, "content": "when facing inconsistent labels from controlled regions, we need different principles based on the type of controlled region as follow:\n(1) If |COR| > 1, it means the sample belongs to a high- risk classification category, and we should delay the classification of this sample. In this paper, delay the classification equal to classify it into uncertain case.\n(2) If |IMP| > 1, we need calculate the $proportion(l_{gb_i}) = \\sum_{(l_{gb_i}, \\mu_{gb_j}(\\xi)) \\in IMP} \\mu_{gb_j}(\\xi)$. If $proportion(l_{gb_i}) > 50\\%$, it means that the labels of controlled regions are relatively consistent, and the sample can be classified with $l_{gb_i}$. Otherwise, the sample has a high classification risk, and we should classify it into uncertain case.\n(3) If the type is UNE, indicating that the sample does not belong to any shadowed GB. In other words, there are no references in the training data to classify this sample. This suggests a high classification risk, and we should classify it into uncertain case.\nThe overall process is illustrated in Fig. 4. When faced with samples that need classification, we follow these three steps:\nStep.1: Build the shadowed GBs by fuzzy-rough transforma- tion according to $(\\alpha, 1 - \\alpha)$.\nStep.2: Find the controlled regions that impact each sample.\nStep.3: Classify the samples to certain class or uncertain case by our proposed principles."}, {"title": "V. EXPERIMENTAL STUDIES", "content": "In this section, we comprehensively compared our pro- posed 3WC-SGB with other three-way classifiers (3WC-FNC [45], 3WC-SVM [46]), state-of-the-art GB-based classifiers"}, {"title": null, "content": "(GBKNN [18", "21": "ACCGBKNN [42", "parts": "n\u2022 Comparison with three-way classifiers to verify the ad- vantages of applying shadowed GBs to 3WC-SGB.\n\u2022 Comparison with state-of-the-art GB-based classifiers to demonstrate the advantages of applying three-way clas- sification to 3WC-SGB.\n\u2022 Comparison with traditional classifiers and conducted a comprehensive statistical analysis on all compared clas- sifiers to demonstrate the significant advantages of 3WC- SGB.\n\u2022 Investigated the hyperparameter configuration of 3WC- SGB and proposed a computational method for recom- mending optimal hyperparameter.\nAll experiments were conducted on an Intel i"}]}]}