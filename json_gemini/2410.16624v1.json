[{"title": "EVC-MF: End-to-end Video Captioning Network with Multi-scale Features", "authors": ["Tian-Zi Niu", "Zhen-Duo Chen", "Xin Luo", "Xin-Shun Xu"], "abstract": "Conventional approaches for video captioning leverage a variety of offline-extracted features to generate captions. Despite the availability of various offline-feature-extractors that offer diverse information from different perspectives, they have several limitations due to fixed parameters. Concretely, these extractors are solely pre-trained on image/video comprehension tasks, making them less adaptable to video caption datasets. Additionally, most of these extractors only capture features prior to the classifier of the pre-training task, ignoring a significant amount of valuable shallow information. Furthermore, employing multiple offline-features may introduce redundant information. To address these issues, we propose an end-to-end encoder-decoder-based network (EVC-MF) for video captioning, which efficiently utilizes multi-scale visual and textual features to generate video descriptions. Specifically, EVC-MF consists of three modules. Firstly, instead of relying on multiple feature extractors, we directly feed video frames into a transformer-based network to obtain multi-scale visual features and update feature extractor parameters. Secondly, we fuse the multi-scale features and input them into a masked encoder to reduce redundancy and encourage learning useful features. Finally, we utilize an enhanced transformer-based decoder, which can efficiently leverage shallow textual information, to generate video descriptions. To evaluate our proposed model, we conduct extensive experiments on benchmark datasets. The results demonstrate that EVC-MF yields competitive performance compared with the state-of-the-art methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Developing conversational systems that can both reliably comprehend the world and effortlessly interact with humans is one of the long-term goals of artificial intelligence community. An dynamic and thriving benchmark in this field is video captioning, integrating research in visual understanding and natural language processing. Specifically, it entails automatically generating a semantically accurate description for a given video. Despite recent promising achievements in this area, it remains a challenging task due to two primary reasons: 1) videos encompass intricate spatial and temporal information compared to images; 2) there exists an inherent gap between visual and natural language, as their fundamental syntax for conveying information differs significantly.\nInspired by machine translation, most recent visual captioning methods have adopted the encoder-decoder framework [1], [2], [3]. Naturally, some of them focus on designing a suitable encoder to learn more efficient video representation. For example, early approaches [4], [5] typically employ a pre-trained convolutional neural network (CNN) as an encoder to extract appearance features. However, relying solely on appearance features makes it challenging to fully represent all contents within a video. Consequently, researchers have successively incorporated additional information, such as action features [4], [6], object features [7], [8], [9], and trajectory features [10], to the encoder (Fig 1(a)). Furthermore, there are also some feature fusion-based encoders proposed to effectively utilize these multi-modal features [11], [12].\nAlthough significant progress has been made with these methods, they typically rely on one or more offline-feature-extractors and encounter the following problems: 1) multiple extractors necessitate higher computational resources; 2) most offline-feature-extractors are pre-trained on tasks irrelevant to video captioning, making their adaptation to video captioning difficult; 3) most offline-feature-extractors only extract features before the classifier, disregarding the valuable information at shadow levels; 4) models employing offline-feature-extractors lack end-to-end training.\nThe other focus of encoder-decoder models lies in generating semantically correct and linguistically natural captions. Currently, caption generation is primarily using autoregressive (AR) decoding, i.e., generating each word conditionally on previous outputs. For example, most methods utilize recurrent neural networks, e.g. GRU, LSTM, with self-attention mechanism or transformer as decoder. However, these methods treat the input sequence merely as a collection of tokens and only independently calculate the attention weights between any two tokens within this collection. Consequently, they fail to consider the shallow textual information () when calculating dependencies among tokens.\nTo address the issues caused by the offline-features and conventional decoders, we propose a novel End-to-end Video Captioning network with Multi-scale Features, namely EVC-MF. The model comprises three modules to tackle these issues. Firstly, our objective is to find a feature extractor that takes raw video frames as input and requires less computation. There-"}, {"title": "II. RELATED WORK", "content": "Pioneering models for the visual captioning task are mainly template-based methods [15], [16], [17], [18], which employ predefined grammar rules and manual visual features to generate fixed descriptions. However, these approaches are significantly constrained by the predetermined templates, making them difficult to generate flexible and satisfactory descriptions.\nWith the advancement of deep learning, sequence learning methods gradually supplanting template-based approaches to emerge as the prevailing paradigm for visual captioning. Generally, a sequence learning method usually adopts an encoder-decoder framework to convert visual information into textual information. Recently, several state-of-the-art methods have proposed novel encoder schemes, while others have made improvements on the decoder. In the following subsections, we comprehensively review these advancements from both encoder and decoder perspectives.\nA. Encoder-design Methods\nA high quality encoder should encode visual contents into discriminative features that can be easily processed by machines. Currently, due to the limitations of computer computing power and model computation volume, most encoders employ different offline extractors to obtain multi-modal visual representations. For example, PickNet [19] employs the output of the final convolutional layer of ResNet-152 as the video representation for video captioning task. Similarly, Wang et al. [20] proposed RecNet, which utilizes Inception-V4 pre-trained on ILSVRC2012-CLS classification dataset for offline feature extraction. However, relying solely on a single CNN feature extraction may lead to overlooking crucial information.\nBy extracting features from multiple perspectives, a model can acquire a more comprehensive understanding of the video. Consequently, researchers have progressively incorporated diverse perspectives information to the encoder. For instance, to capture temporal information, numerous video captioning methods recommend using offline action features to enhance video comprehension. Specifically, in addition to employing 2D features, MARN [21] integrates offline optical flow to obtain a more accurate video representation. In addition, MGSA [22], POS-CG [23], and CANet [24] employ pre-trained 3D-CNN models, such as C3D, I3D, and 3D-ResNeXt respectively, to extract offline action information for video captioning. More recently, it is verified that incorporating more detailed features is beneficial for characterizing the semantic of images or videos. For example, STG-KD [7] for video captioning and Up-Down [25] for image captioning demonstrate that object features and their interactions facilitate generating detailed visual descriptions. Hua et al. [10] showed that trajectory-based feature representation contributes significantly to video captioning. Furthermore, several approaches [11], [26] use feature fusion to enhance visual understanding.\nHowever, as mentioned previously, there exist certain limitations that hinder further advancements in visual captioning using offline features. The primary issue is that the parameters of these offline feature extractors are exclusively pre-trained for image/video comprehension tasks, posing difficulties in their adaptation to different video captioning datasets. Accordingly, the end-to-end approaches are initially applied in image captioning [27], [28]. Given that videos encompass more information and complex content than images, there are still many problems to be solved in end-to-end video captioning. For instance, it is difficult to capture and analyze contextual scenes and track objects movements throughout a video. Additionally, videos often possess a high temporal dimension which can significantly increase model complexity. Training such models necessitates substantial hardware resources and time investment, potentially rendering them impractical for real-world applications."}, {"title": "III. METHODOLOGY", "content": "The framework of EVC-MF is illustrated in Fig. 2, comprising a feature extractor, a masked encoder and an enhanced transformer-based decoder. Specifically, we first uniformly sample T raw frames {$V_t$}$_{t=1}^T$, each frame consists of H \u00d7 W \u00d7 3 pixels, i.e. $V_t \\in \\mathbb{R}^{H\\times W \\times 3}$. Then, we feed them into the feature extractor to extract grid features $F_{list}$ = {$F_m$}$_{m=1}^M$ from each block of the extractor, where M denotes the number of blocks. Subsequently, we upsample each feature map $F_m$ to a same size and merge them into a feature sequence $F \\in \\mathbb{R}^{T \\times \\frac{H}{8} \\times \\frac{W}{8} \\times C}$, where C is the channel dimension. We then present multiple regions with varying degrees of coarseness on each feature map of F. To encourage learning useful information and reduce redundancy, we randomly mask one region of each feature map in the sequence and obtain the final video representation $F_{final} \\in \\mathbb{R}^{(T-1) \\times \\frac{H}{8} \\times \\frac{W}{8} \\times C}$ through a 3D averaging pooling layer. Finally, we input $F_{final}$ to an enhanced transformer-based decoder to generate a text sentence $S = {\\hat{w_1}, \\hat{w_2},...,\\hat{w_N}}$ containing N words to describe the video content. Further elaboration on each module is provided in the following subsections.\nB. Feature Extractor\nAs mentioned previously, most video captioning models using multiple extractors are difficult to be trained end-to-end, thus limiting their performance. Fortunately, VidSwin [13] achieves a favorable speed-accuracy trade-off and has made significant achievements in human action recognition. Therefore, we utilize VidSwin as our feature extractor to encode raw video frames as multi-scale features. Concretely, we feed the raw video frames sequence $V \\in \\mathbb{R}^{T\\times H\\times W\\times 3}$ into VidSwin to extract grid features from each block, formally,\n$F_o = B_e(V)$,\n$F_m = B_{lm}(F_{m-1})$,\n(1)\nwhere m is the number of the block in VidSwin, $B_e$ and $B_{lm}$ are the patch embedding layer and the swin transformer block of VidSwin, respectively. Please refer to [13] for more details about VidSwin. Subsequently, we obtain a list of feature maps $F_{list}$ = {$F_m$}$_{m=1}^M$, where $F_m \\in \\mathbb{R}^{\\frac{H}{2^{m+1}} \\times \\frac{W}{2^{m+1}} \\times C_m}$.\nC. Masked Encoder\nObviously, the list $F_{list}$ contains a substantial amount of redundant information. To integrate valuable information and reduce redundancy, we propose a masked encoder. Specifically, we initially feed elements in $F_{list}$ into a series of upsampling modules to standardize their shapes. Each upsampling module contains a linear function $\\psi_m(\\cdot)$ and an upsampling function $\\Psi_m(\\cdot)$. The formulas are defined as follows,\n$\\mathbb{F}_m = \\varphi_m(F_m)$,\n$\\hat{F}_m = \\Psi_m(\\mathbb{F}_m)$,\n$F = [\\hat{F}_1,\\hat{F}_2,\u2026,\\hat{F}_M]$,\n(2)\nwhere $\\mathbb{F}_m$ is an intermediate variable, $\\hat{F}_m \\in \\mathbb{R}^{\\frac{H}{8} \\times \\frac{W}{8} \\times C}$, $F\\in \\mathbb{R}^{T \\times \\frac{H}{8} \\times \\frac{W}{8} \\times C}$.\nAfter that, to further process the features, we present multiple regions with varying level of coarseness on each feature map F[t]. The coarseness is determined by the size of a rectangular. As illustrated in Fig. 3, we initially divide the feature map into $\\frac{H}{g} \\times \\frac{W}{g}$ grids, where each grid has an area of $g \\times g$. Then, we define the smallest region $r(i, j, \\Delta x, \\Delta y)$ with height $A_y$ and width $A_x$ at anchor point (i, j), i.e. top-left corner grid point. Using $r(i, j, \\Delta x, \\Delta y)$, we derive a set of regions $R(i,j)$ = {$r(i,j,w\\Delta x, h\\Delta y)|h, w \\in$ {1,2,\u2026\u2026},$i + h\\Delta y < \\frac{H}{g},j+w\\Delta x < \\frac{W}{g}, Ar(r) < \\delta \\frac{H}{g}\\frac{W}{g}$ } by changing their widths and heights, where $A_r(r)$ denotes the area of the element, $\\delta$ is a threshold to ensure that the masked area does not exceed certain limits. Consequently, for different spatial locations (i, j), we can obtain different sets of rectangles $R(i,j)$. Ultimately, we obtain the set $R = {R(i,j)|0 < i < \\frac{H}{g},0 < j < \\frac{W}{g}}$ of regions with different coarseness of the whole feature map. We randomly sample a sequence of regions $R = {r_1, r_2,\u2026,r_T}$, where $r_t = r(i_t, j_t, w_t\\Delta x, h_t\\Delta y)$. After obtaining R, we can easily get the masked feature sequence F,\n$\\mathbb{F}[t][i][j] = \\begin{cases}  F[t][i][j], & \\text{if } (i, j) \\notin r_t \\\\  \\vec{0_c}, & \\text{if } (i, j) \\in r_t, \\end{cases}$\n(3)\nwhere $\\vec{0_c}$ is a C-dimensional zero vector. Finally, we feed $\\mathbb{F}$ to a 3D averaging pooling layer $\\rho(.)$ to obtain the final video representation,\n$F_{final} = \\rho(\\mathbb{F})$.\n(4)\nD. Enhanced Transformer-based Decoder\nDecoder aims to generate a semantically correct description based on the video representation. However, in most transformer-based decoders, the focus primarily lies on the individual relationships between two tokens, which may result in a loss of shallow textual information. In this paper, we employ an enhanced transformer-based decoder to produce precise captions. Specifically, the input to the decoder is split into two parts: text tokens and visual tokens. Among them, the text tokens $W^{token}$ contain semantic and positional embedding, i.e. $W^{emb}$ and $P^{emb}$, about the words in the caption, which is formulated as follows,\n$W^{emb} = [\\{\\phi_w(w_n)\\}_{n=1}^N]$,\n$P^{emb} = [\\{\\phi_p(p_n)\\}_{n=1}^N]$,\n$W^{token} = W^{emb} + P^{emb}$,\n(5)\nwhere $W^{token}$, $W^{emb}$ and $P^{emb} \\in \\mathbb{R}^{N \\times d}$; [...] denotes concatenation; $\\phi_w$ and $\\phi_p$ are the embedding functions; $w_n$ and $p_n$ are the one-hot vectors of word $w_n$ and position n, respectively. For the second one, we tokenize the video representation $F_{final}$ along the channel dimension and employ a linear function to ensure dimensional consistency with $W^{token}$,\n$\\Lambda = \\psi_v(F_{final})$,\n$V^{token} = \\varphi_v(\\Lambda)$,\n(6)\nwhere $\\Lambda \\in \\mathbb{R}^{[(\\frac{H}{8}-1) \\cdot \\frac{W}{8} \\cdot T] \\times C}$ is an intermediate variable, $\\varphi_v(.)$ is a linear function, $\\psi_v(.)$ denotes the tensor dimensional change function. Thus, we obtain N text tokens and $[(\\frac{H}{8}-1) \\cdot \\frac{W}{8} \\cdot T]$ visual tokens. These tokens are combined to form the final input for the decoder $I = [W^{token}, V^{token}] \\in \\mathbb{R}^{[N+(\\frac{H}{8}-1) \\cdot \\frac{W}{8} \\cdot T] \\times d}$.\nAs mentioned previously, our decoder is based on transformer. Upon receiving the input tokens, the traditional transformer based decoder [39], [40] feeds them to a self-attention module with multiple layers to obtain the final output. A layer of the traditional transformer is formulated as,\n$Q, K, V = IW_q, IW_k, IW_v$,\n$H = (softmax(\\frac{QK^T}{\\sqrt{d}}) + X_{mask})V$,\n$O = FFN(H)$,\n(7)\nwhere Q, K and V $\\in \\mathbb{R}^{L \\times d}$ are the queries, keys and values of self-attention, for simplicity $L = N + [(\\frac{H}{8}-1) \\cdot \\frac{W}{8} \\cdot T]$, $W_q, W_k, W_v \\in \\mathbb{R}^{d \\times d}$ are trainable parameter matrices, \u0397 is the hidden states, $X_{mask}$ is a token mask matrix, O is the output of the layer, FFN(\u00b7) is a feed-forward sub-layer. While traditional self-attention mechanisms can directly capture the dependencies between input tokens, query and key are controlled by only two learnable matrices, missing\nthe opportunity to exploit the shallow textual information, formally,\n$QK^T[i][j] = I[i](W_qW_k^T)I[j]^T$.\n(8)\nTo solve this problem, we propose to add the output of the previous layers O as shallow textual information to the Q, K calculation,\n$Q = (1 - \\lambda_q)Q + \\lambda_q\\bar{O}W_{oq}$,\n$K = (1 - \\lambda_k)K + \\lambda_k\\bar{O}W_{ok}$,\n$A_q = sigmoid(Q_{wq} + \\bar{O}w_{oq})$,\n$A_k = sigmoid(K_{wk} + \\bar{O}w_{ok})$,\n$\\bar{O} = mean(O_1, O_2,\u00b7\u00b7\u00b7, O_{z-1})$,\n(9)\nwhere $W_{oq}, W_{ok} \\in \\mathbb{R}^{d \\times d}$ are trainable parameter matrices, $w_{q}, w_{k}, w_{oq}, w_{ok} \\in \\mathbb{R}^{d \\times 1}$ are trainable parameter vectors, z is the order number of the current layer. Correspondingly, the output is constructed based on shallow textual information,\n$H = (softmax(\\frac{Q^{T}K}{\\sqrt{d}}) + X_{mask})V$,\n$\\hat{O} = FFN(H)$.\n(10)\nFollowing [39], [40], we take the first N tokens of $\\hat{O}_z$ as the representation of the sentence, where $\\hat{O}_z$ is the output of the last layer of the decoder.\nE. Training\nWe train EVC-MF in an end-to-end manner and employ Masked Language Modeling [41] on our decoder. Specifically, we randomly mask out a certain percentage words of the ground-truth by substituting them with [MASK]. Subsequently, we utilize the relevant output of EVC-MF for classification to predict words. We adopt the standard cross-entropy (CE) loss to train EVC-MF, the loss for a single pair (V, S) is,\n$L = \\sum_{w_n \\in \\mathbb{S}} log P(w_n \\vert \\bar{\\mathbb{S}}, V)$,\n(11)\nwhere $\\mathbb{S} = \\{w_1, w_2,..., w_N\\}$ is the ground-truth, $\\bar{\\mathbb{S}}$ denotes the set of masked words, $\\mathbb{S}$ represents the set of remaining words.\nF. Inference\nDuring inference, we generate the caption in an auto-regressive manner. Concretely, we initialize EVC-MF with a start token [CLS] and a [MASK] token; then sample a word from the vocabulary based on the likelihood output. Subsequently, we replace the [MASK] token in the previous input sequence with the sampled word and append a new [MASK] for predicting the next word. The generation process terminates until the end token [EOS] is generated or the maximum output length is reached."}, {"title": "IV. EXPERIMENTS", "content": "In this section", "42": "comprises a collection of 1"}, {"42": "we divide MSVD into training set", "43": "consists of 10", "evaluation": "BLEU-4 [59", "60": "ROUGE-L [61", "62": ".", "follows": "n$METEOR = (1 - P_{en"}], "as": "n$g_k(S_{ij"}, "h_k(S_{ij", "log(\\frac{\\vert V \\vert", {"as": "n$CIDEr = \\sum_{n=1"}, "N W_n CIDEr_n$", "n$CIDEr_n = \\frac{1", {"parts": 1, "-\" indicates the absence of results for this metric in the original paper. From this Table I, we have the following observations": "n\u2022 On both datasets, EVC-MF achieves the best results in terms of all widely-used metrics, especially on the more in line with human judgment metric, CIDEr. For example, EVC-MF is 10.8% and 0.9% higher than the runner-up methods in terms of CIDEr on MSVD and MSR-VTT, respectively.\n\u2022 The first and third parts of the table demonstrate that both RNN-based decoder and transformer-based decoder exhibit superior performance. However, it is noteworthy that transformers can be trained in parallel, thereby offering convenience for end-to-end training. Consequently, we choose the transformer-based decoder to generate sentences.\n\u2022 From the second part of the table, it can be observed that methods using sequence optimization, e.g. SMAN, perform well in terms of CIDEr. This can attributed to their"}]