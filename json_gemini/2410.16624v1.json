{"title": "EVC-MF: End-to-end Video Captioning Network\nwith Multi-scale Features", "authors": ["Tian-Zi Niu", "Zhen-Duo Chen", "Xin Luo", "Xin-Shun Xu"], "abstract": "Conventional approaches for video captioning lever-\nage a variety of offline-extracted features to generate captions.\nDespite the availability of various offline-feature-extractors that\noffer diverse information from different perspectives, they have\nseveral limitations due to fixed parameters. Concretely, these\nextractors are solely pre-trained on image/video comprehension\ntasks, making them less adaptable to video caption datasets.\nAdditionally, most of these extractors only capture features prior\nto the classifier of the pre-training task, ignoring a significant\namount of valuable shallow information. Furthermore, employing\nmultiple offline-features may introduce redundant information.\nTo address these issues, we propose an end-to-end encoder-\ndecoder-based network (EVC-MF) for video captioning, which\nefficiently utilizes multi-scale visual and textual features to\ngenerate video descriptions. Specifically, EVC-MF consists of\nthree modules. Firstly, instead of relying on multiple feature\nextractors, we directly feed video frames into a transformer-based\nnetwork to obtain multi-scale visual features and update feature\nextractor parameters. Secondly, we fuse the multi-scale features\nand input them into a masked encoder to reduce redundancy\nand encourage learning useful features. Finally, we utilize an en-\nhanced transformer-based decoder, which can efficiently leverage\nshallow textual information, to generate video descriptions. To\nevaluate our proposed model, we conduct extensive experiments\non benchmark datasets. The results demonstrate that EVC-MF\nyields competitive performance compared with the state-of-the-\nart methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Developing conversational systems that can both reliably\ncomprehend the world and effortlessly interact with\nhumans is one of the long-term goals of artificial intelligence\ncommunity. An dynamic and thriving benchmark in this field\nis video captioning, integrating research in visual understand-\ning and natural language processing. Specifically, it entails\nautomatically generating a semantically accurate description\nfor a given video. Despite recent promising achievements in\nthis area, it remains a challenging task due to two primary\nreasons: 1) videos encompass intricate spatial and temporal\ninformation compared to images; 2) there exists an inherent\ngap between visual and natural language, as their fundamental\nsyntax for conveying information differs significantly.\nInspired by machine translation, most recent visual caption-\ning methods have adopted the encoder-decoder framework [1],\n[2], [3]. Naturally, some of them focus on designing a suitable\nencoder to learn more efficient video representation. For ex-\nample, early approaches [4], [5] typically employ a pre-trained\nconvolutional neural network (CNN) as an encoder to extract\nappearance features. However, relying solely on appearance\nfeatures makes it challenging to fully represent all contents"}, {"title": "A. Encoder-design Methods", "content": "A high quality encoder should encode visual contents into\ndiscriminative features that can be easily processed by ma-\nchines. Currently, due to the limitations of computer com-\nputing power and model computation volume, most encoders\nemploy different offline extractors to obtain multi-modal visual\nrepresentations. For example, PickNet [19] employs the output\nof the final convolutional layer of ResNet-152 as the video\nrepresentation for video captioning task. Similarly, Wang et\nal. [20] proposed RecNet, which utilizes Inception-V4 pre-\ntrained on ILSVRC2012-CLS classification dataset for offline\nfeature extraction. However, relying solely on a single CNN\nfeature extraction may lead to overlooking crucial information.\nBy extracting features from multiple perspectives, a model\ncan acquire a more comprehensive understanding of the video.\nConsequently, researchers have progressively incorporated di-\nverse perspectives information to the encoder. For instance,\nto capture temporal information, numerous video captioning\nmethods recommend using offline action features to enhance\nvideo comprehension. Specifically, in addition to employing\n2D features, MARN [21] integrates offline optical flow to\nobtain a more accurate video representation. In addition,\nMGSA [22], POS-CG [23], and CANet [24] employ pre-\ntrained 3D-CNN models, such as C3D, I3D, and 3D-ResNeXt\nrespectively, to extract offline action information for video\ncaptioning. More recently, it is verified that incorporating\nmore detailed features is beneficial for characterizing the\nsemantic of images or videos. For example, STG-KD [7]\nfor video captioning and Up-Down [25] for image captioning\ndemonstrate that object features and their interactions facilitate\ngenerating detailed visual descriptions. Hua et al. [10] showed\nthat trajectory-based feature representation contributes signif-\nicantly to video captioning. Furthermore, several approaches\n[11], [26] use feature fusion to enhance visual understanding.\nHowever, as mentioned previously, there exist certain lim-\nitations that hinder further advancements in visual captioning\nusing offline features. The primary issue is that the parameters\nof these offline feature extractors are exclusively pre-trained\nfor image/video comprehension tasks, posing difficulties in\ntheir adaptation to different video captioning datasets. Accord-\ningly, the end-to-end approaches are initially applied in image\ncaptioning [27], [28]. Given that videos encompass more\ninformation and complex content than images, there are still\nmany problems to be solved in end-to-end video captioning.\nFor instance, it is difficult to capture and analyze contextual\nscenes and track objects movements throughout a video.\nAdditionally, videos often possess a high temporal dimension\nwhich can significantly increase model complexity. Training\nsuch models necessitates substantial hardware resources and\ntime investment, potentially rendering them impractical for\nreal-world applications."}, {"title": "III. METHODOLOGY", "content": "The framework of EVC-MF is illustrated in Fig. 2, com-\nprising a feature extractor, a masked encoder and an enhanced\ntransformer-based decoder. Specifically, we first uniformly\nsample T raw frames {$V_t$}$_{t=1}^T$, each frame consists of H \u00d7\nW \u00d7 3 pixels, i.e. $V_t$ \u2208 $R^{H\u00d7W\u00d73}$. Then, we feed them into\nthe feature extractor to extract grid features $F_{list}$ = {$F_m$}$_{m=1}^M$\nfrom each block of the extractor, where M denotes the number\nof blocks. Subsequently, we upsample each feature map $F_m$\nto a same size and merge them into a feature sequence\nF \u2208 $R^{T\u00d7\\frac{H}{8}\u00d7\\frac{W}{8}\u00d7C}$, where C is the channel dimension.\nWe then present multiple regions with varying degrees of\ncoarseness on each feature map of F. To encourage learning\nuseful information and reduce redundancy, we randomly mask\none region of each feature map in the sequence and obtain\nthe final video representation $F_{final}$ \u2208 $R^{(T-1)\u00d7\\frac{H}{8}\u00d7\\frac{W}{8}\u00d7C}$\nthrough a 3D averaging pooling layer. Finally, we input $F_{final}$\nto an enhanced transformer-based decoder to generate a text\nsentence S = {\u01751, \u01752,...,\u0175N} containing N words to\ndescribe the video content. Further elaboration on each module\nis provided in the following subsections."}, {"title": "B. Feature Extractor", "content": "As mentioned previously, most video captioning models\nusing multiple extractors are difficult to be trained end-to\nend, thus limiting their performance. Fortunately, VidSwin\n[13] achieves a favorable speed-accuracy trade-off and has\nmade significant achievements in human action recognition.\nTherefore, we utilize VidSwin as our feature extractor to\nencode raw video frames as multi-scale features. Concretely,\nwe feed the raw video frames sequence V \u2208 $R^{T\u00d7H\u00d7W\u00d73}$ into\nVidSwin to extract grid features from each block, formally,\n$F_o$ = $B_e$(V),\n$F_m$ = $B_lm$($F_{m-1}$),"}, {"title": "C. Masked Encoder", "content": "Obviously, the list $F_{list}$ contains a substantial amount of\nredundant information. To integrate valuable information and\nreduce redundancy, we propose a masked encoder. Specifically,\nwe initially feed elements in $F_{list}$ into a series of upsampling\nmodules to standardize their shapes. Each upsampling module\ncontains a linear function \u03c8m(\u00b7) and an upsampling function\n\u03a8m(\u00b7). The formulas are defined as follows,\n$\\bar{F_m}$ = \u03c8m($F_m$),\n$\\hat{F_m}$ = \u03a8m($\\bar{F_m}$),\n$\\hat{F}$ = [$\\hat{F_1}$,$\\hat{F_2}$,\u2026, $\\hat{F_M}$],\nwhere $\\bar{I_m}$ is an intermediate variable, $\\hat{F_m}$ \u2208 $R^{\\frac{H}{g}\u00d7\\frac{W}{g}\u00d7d}$,\n$\\hat{F}$\u2208$R^{T\u00d7\\frac{H}{g}\u00d7\\frac{W}{g}\u00d7C}$.\nAfter that, to further process the features, we present mul-\ntiple regions with varying level of coarseness on each feature\nmap $\\hat{F[t]}$. The coarseness is determined by the size of a rect-\nangular. As illustrated in Fig. 3, we initially divide the feature\nmap into $\\frac{H}{g}$ x $\\frac{W}{g}$\ngrids, where each grid has an area of g \u00d7 g.\nThen, we define the smallest region $r(i, j, \u2206x, \u2206y)$ with height\nAy and width Ax at anchor point (i, j), i.e. top-left corner\ngrid point. Using $r(i, j, \u2206x, \u2206y)$, we derive a set of regions\n$R_{(i,j)}$ = {$r(i,j,w\u2206x, h\u2206y)|h, w \u2208 {1,2,\u2026\u2026},i + h\u2206y < $\\frac{H}{g}$\n,j+w\u2206x <$\\frac{W}{g}$, Ar(r) < \u03b4HW} by changing their widths\nand heights, where Ar(r) denotes the area of the element, d is\na threshold to ensure that the masked area does not exceed cer-\ntain limits. Consequently, for different spatial locations (i, j),\nwe can obtain different sets of rectangles $R_{(i,j)}$. Ultimately,\nwe obtain the set R = {$R_{(i,j)}|0 < i < $\\frac{H}{g}$,0 < j < $\\frac{W}{g}$} of\nregions with different coarseness of the whole feature map. We\nrandomly sample a sequence of regions R = {$r_1$, $r_2$,\u2026,$r_T$},\nwhere $r_t$ = $r_{(i_t, j_t, w_t\u2206x, h_t\u2206y)}$. After obtaining R, we can\neasily get the masked feature sequence $\\hat{F}$,\n$\\hat{F[t][i][j]}$ = $\\begin{cases} {\\hat{F[t][i][j]}}, & \\text{if } (i, j) \\notin r_t\\\\ 0_c, & \\text{if } (i, j) \\in r_t, \\end{cases}$"}, {"title": "D. Enhanced Transformer-based Decoder", "content": "Decoder aims to generate a semantically correct descrip-\ntion based on the video representation. However, in most\ntransformer-based decoders, the focus primarily lies on the\nindividual relationships between two tokens, which may result\nin a loss of shallow textual information. In this paper, we\nemploy an enhanced transformer-based decoder to produce\nprecise captions. Specifically, the input to the decoder is\nsplit into two parts: text tokens and visual tokens. Among\nthem, the text tokens $W^{token}$ contain semantic and positional\nembedding, i.e. $W^{emb}$ and $P^{emb}$, about the words in the\ncaption, which is formulated as follows,\n$W^{emb}$ = [{$\u03c6_w(w_n)$}$_{n=1}^N$],\n$P^{emb}$ = [{$\u03c6_p(p_n)$}$_{n=1}^N$],\n$W^{token}$ = $W^{emb}$ + $P^{emb}$,\nwhere $W^{token}$, $W^{emb}$ and $P^{emb}$ \u2208 $R^{Nxd}$; [...] denotes\nconcatenation; $\u03a6_w$ and $op$ are the embedding functions; wh\nand p are the one-hot vectors of word $w_n$ and position\nn, respectively. For the second one, we tokenize the video\nrepresentation $F_{final}$ along the channel dimension and employ\na linear function to ensure dimensional consistency with\n$W^{token}$,\n$\\Lambda$ = $\u03c8_\u03c5$($F_{final}$),\n$V^{token}$ = $\\hat{\u03c8_\u03c5}$($\\Lambda$),\nwhere $A$\u2208 $R^{[(\\frac{H}{g}\u22121)\u00b7\u00b7\\frac{W}{g}]\u00d7C}$ is an intermediate variable,\n$\u03c8_\u03c5$(\u00b7) is a linear function, $\\hat{\u03c8_\u03c5}$(.) denotes the tensor dimen-\nsional change function. Thus, we obtain N text tokens and\n$(\\frac{H}{g}\u22121)\u00b7\u00b7\\frac{W}{g}$ visual tokens. These tokens are combined to\nform the final input for the decoder $I$ = [$W^{token}$, $V^{token}$] \u2208\n$R^{[N+(\\frac{H}{g}\u22121)\u00b7\u00b7\\frac{W}{g}]\u00d7d}$\nAs mentioned previously, our decoder is based on trans-\nformer. Upon receiving the input tokens, the traditional trans-\nformer based decoder [39], [40] feeds them to a self-attention\nmodule with multiple layers to obtain the final output. A layer\nof the traditional transformer is formulated as,\nQ, K, V = $IW_q$, $IW_k$, $IW_v$\n\u0397 = $(softmax(\\frac{Q K^T}{\\sqrt{d_k}})+X_{mask})V$,\n\u00d4 = FFN(H),\nwhere Q, K and V \u2208 $R^{L\u00d7d}$ are the queries, keys and values\nof self-attention, for simplicity L = $N + (\\frac{H}{g}\u22121)\u00b7\u00b7\\frac{W}{g}$,\n$W_q,W_k, W_v$ \u2208 $R^{d\u00d7d}$ are trainable parameter matrices, \u0397\nis the hidden states, $X_{mask}$ is a token mask matrix, \u00d4\nis the output of the layer, FFN(\u00b7) is a feed-forward sub-layer.\nWhile traditional self-attention mechanisms can directly\ncapture the dependencies between input tokens, query and\nkey are controlled by only two learnable matrices, missing"}, {"title": "E. Training", "content": "We train EVC-MF in an end-to-end manner and employ\nMasked Language Modeling [41] on our decoder. Specifically,\nwe randomly mask out a certain percentage words of the\nground-truth by substituting them with [MASK]. Subsequently,\nwe utilize the relevant output of EVC-MF for classification to\npredict words. We adopt the standard cross-entropy (CE) loss\nto train EVC-MF, the loss for a single pair (V, S) is,\n$\\mathcal{L} = \\sum_{w_n \\in S^{re}} log P(w_n | S^{ma}, V)$,\nwhere $S$ = {$w_1, w_2,..., w_N$} is the ground-truth, $S^{re}$\ndenotes the set of masked words, $S^{ma}$ represents the set of\nremaining words."}, {"title": "F. Inference", "content": "During inference, we generate the caption in an auto-\nregressive manner. Concretely, we initialize EVC-MF with\na start token [CLS] and a [MASK] token; then sample a\nword from the vocabulary based on the likelihood output.\nSubsequently, we replace the [MASK] token in the previous\ninput sequence with the sampled word and append a new\n[MASK] for predicting the next word. The generation process\nterminates until the end token [EOS] is generated or the\nmaximum output length is reached."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel end-to-end encoder-\ndecoder-based network (EVC-MF) for video captioning, com-\nprising a feature extractor, a masked encoder, and an enhanced\ntransformer-based decoder. Specifically, to ensure updatable\nparameters of the feature extractor and optimize the utilization\nof shallow visual information, the feature extractor takes the\noriginal frame as input and extracts multi-scale visual features\nto the encoder. Then, to learn more valuable details, extract\nmeaningful insights, and reduce unnecessary redundancy, we\npropose a masked encoder. Finally, to fully utilize visual and\ntext information, we develop an enhanced transformer-based\ndecoder. Furthermore, we conducted extensive experiments on\nMSVD and MSR-VTT to demonstrate the effectiveness of\nEVC-MF and its sub-modules. Although EVC-MF achieves\nbetter performance, it still lacks in controllability and inter-\npretability. Thus, in the future, we will work on improving it\nin these two aspects."}]}