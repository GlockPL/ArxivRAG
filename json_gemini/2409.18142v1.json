{"title": "A Survey on Multimodal Benchmarks: In the Era of Large AI Models", "authors": ["Lin Li", "Guikun Chen", "Hanrong Shi", "Jun Xiao", "Long Chen"], "abstract": "The rapid evolution of Multimodal Large Language Models (MLLMs) has brought substantial advancements in artificial intelligence, significantly enhancing the capability to understand and generate multimodal content. While prior studies have largely concentrated on model architectures and training methodologies, a thorough analysis of the benchmarks used for evaluating these models remains underexplored. This survey addresses this gap by systematically reviewing 211 benchmarks that assess MLLMs across four core domains: understanding, reasoning, generation, and application. We provide a detailed analysis of task designs, evaluation metrics, and dataset constructions, across diverse modalities. We hope that this survey will contribute to the ongoing advancement of MLLM research by offering a comprehensive overview of benchmarking practices and identifying promising directions for future work. An associated GitHub repository collecting the latest papers is available.", "sections": [{"title": "1 INTRODUCTION", "content": "THE rapid advancement of Artificial Intelligence (AI) has been fundamentally intertwined with the development of robust benchmarks [1]\u2013[3]. These benchmarks serve as crucial standards, providing objective metrics to evaluate and compare the performance of AI models. As a pioneer in computer vision, ImageNet [1], offering a large-scale and well-annotated dataset, has paved the way for developing models that are both highly accurate and broadly generaliz-able. Previous development of AI models and benchmarks are complementary. For instance, as classification bench-marks grew in terms of data volume and class diversity, models trained on them improved significantly, resulting in better real-world performance. This synergy between task-specific benchmarks and model architecture has been a cornerstone of Al's practical applications.\nRecent breakthroughs in Large Language Models (LLMs), e.g., ChatGPT [4], have caused major changes in numerous fields of research and have profoundly affected various social and industrial sectors. Harnessing LLM as the brain, Multimodal Large Language Models (MLLMs), e.g., GPT-4v [5] and Gemini [6], bridge the gap between visual data and linguistic context, enabling these models to understand, reason, and generate content that combines text, images, and other modalities. Despite their enormous potential, the development of benchmarks has not always kept pace with the evolution of corresponding MLLMs. Tra-ditional benchmarks, which often focus on increasing data volume or the number of categories, struggle to adequately assess the multifaceted capabilities of MLLMs. This leads\nto a natural question: How can we effectively evaluate the various capabilities and reliability of these large AI models?\nCurrently, the landscape of multimodal benchmarks for Al models is rich and varied (c.f. Fig. 1), encompassing a wide range of tasks such as visual question answer-ing and image captioning. This diversity has undoubtedly spurred the development of MLLMs, providing researchers with multiple avenues to explore and refine their models. However, the plethora of benchmarks is a double-edged sword. The sheer number of benchmarks makes it difficult to navigate, especially for newcomers: 1) Disjointed objectives: The abundance of benchmarks, each with distinct goals, results in a fragmented research landscape. Researchers must invest considerable time determining whether existing benchmarks adequately test MLLMs' capabilities, compli-cating the development of new, aligned benchmarks. 2) Task saturation: The proliferation of tasks, driven by diverse objectives, has led to an overwhelming landscape. This saturation makes it challenging to discern truly innovative or impactful benchmarks, causing valuable resources to be overlooked or underutilized. 3) Metric evolution & discrepan-cies: Although some studies have proposed well-designed metrics, their adoption is hindered by the rapid evolution and frequent updates of benchmarks. This inconsistency forces researchers to navigate a complex balance between traditional and new metrics, complicating fair comparisons and hindering the holistic evaluation of MLLMs.\nTo address this gap, we present a comprehensive sur-vey that systematically reviews the capabilities, task de-signs, dataset constructions, and specifically-designed met-rics of current multimodal benchmarks. This survey exam-ines these aspects from the perspectives of understanding, reasoning, generation and application:\n\u2022 Understanding. It refers to the ability to extract and inte-grate features from multimodal data to perform cross-modal"}, {"title": "2 UNDERSTANDING BENCHMARK", "content": "The rapid advancements in MLLMs have underscored the necessity for comprehensive benchmarks to evalu-ate their understanding capabilities across diverse data types [64], [217]. This section revisits multimodal under-standing benchmarks designed to assess MLLMs' ability to perceive and comprehend information presented in various formats, such as text and images. These benchmarks are crucial for fine-tuning MLLMs, ensuring their robustness and generalization in real-world applications [77], [189].\nRecent understanding benchmarks focus on assessing multiple aspects of MLLMs, e.g., visual perception, contex-"}, {"title": "2.1 Visual Perception", "content": "Visual perception capability is a foundational aspect of understanding benchmarks. It involves the ability to ex-tract salient features and accurately recognize and inter-pret visual elements, such as multiple objects, text infor-mation, and complex emotional or implicit cues [45]. This section categorizes the visual perception benchmarks into three groups: low-level perception, fine-grained perception, higher-order perception, and comprehensive perception.\n\u2022 Low-level Perception. Low-level perception in MLLMs involves the detection and interpretation of basic visual attributes (e.g., color, lighting, composition) and distor-tions (e.g., blurs, noise, artifacts) that do not require reasoning or external knowledge [7], [8]. These low-level perceptual capabilities are crucial for various ap-plications, including recommendation systems, camera system guidance, and visual quality enhancement [7].\n\u2022 Fine-grained Perception. This core dimension repre-sents a sophisticated level of image understanding that focuses on the detailed and nuanced aspects of visual content. It includes recognizing and interpreting subtle features ability, such as text recognition (OCR-Bench [12]), visual-linguistic concepts and pattern(e.g., SPEC [9] and MMVP [15]), and identifying small objects in high-resolution images (e.g., V*Bench [11] MagnifierBench [21], P2GB [18]). Specifically, MDVP-Bench [19] focuses on evaluating models' ability for fine-grained pixel-level understanding, including detailed descriptions, inter-relationship analysis, and complex reasoning across diverse visual elements. Besides, some benchmarks also emphasize vision-language alignment, which refers to the model's ability to accurately link vi-sual elements with corresponding textual descriptions. For instance, Eqben [17] focuses on the equivariance dialogue of the pairs that are \"slightly\" misaligned, with minimal semantic drift, making them more chal-lenging to distinguish compared to pairs that are clearly mismatched. Unlike visual concept recognition and alignment, MMUBench [20] assesses machine unlearning within MLLMs, the ability to forget visual recognition of concepts effectively. While MM-SAP [14] evaluates the self-awareness ability of MLLMs to understand what they can and cannot perceive from images.\n\u2022 Higher-order perceptual capabilities [218] in MLLMs encompass advanced emotional understanding and deep meaning extraction from multimodal data, such as images, videos, and text. According to higher-order meaning, these capabilities can be categorized into: 1) Emotional Perception. The ability to interpret and respond to complex emotional cues across various modalities [26], [27]. 2) Implication Perception. The skill to derive subtle, implicit meanings from visual and contextual information [24], [25]. 3) Aesthetic Perception. The capacity to assess and align with human aesthetic judgment (e.g., aesthetic attributes and emotional as-pects) in diverse visual contexts [22], [23]. These abil-ities are crucial for sophisticated communication and media analysis."}, {"title": "\u2022 Comprehensive Perception", "content": "Comprehensive perception benchmarks holistically assess MLLMs' ability to perform a broad range of visual recognition tasks [28]\u2013[30], [32], [34], [36], [38]\u2013[40], involving various types of visual content. According to the input language type, benchmarks are divided into: 1) Signlelingual Percep-tion, evaluating overall visual recognition across di-verse content types in widely-used English [29], [35]\u2013[37], [43]. Specifically, MM-Vet focuses on the inte-gration capability across different core VL capabilities, i.e., recognition, OCR, knowledge, language genera-tion, spatial awareness, and math. Different from per-ception evaluation, DenseFusion-1M [45] and IT [33] create hyper-detailed image annotations to empower the MLLMs with detailed text recognition and high-resolution image perception ability, along with some evaluation benchmarks of image description quality, e.g., DID-Bench, D2I-Bench, and LIN-Bench [33]. 2) Multilingual Perception, assessing the models' ability to understand and interpret visual content in multiple lan-guages, highlighting their adaptability across different linguistic and cultural contexts [47], [49]\u2013[53]."}, {"title": "2.1.2 Contextual Comprehension", "content": "It refers to the ability of MLLMs to understand and interpret information that is influenced by the surrounding context. Based on the different input context formats, these bench-marks are grouped as follows:\n\u2022 Context-Dependent Understanding. CODIS [54] de-fines context-dependent understanding as the model's abil-ity to accurately recognize visual elements within a single image with supplementary context text informa-tion (e.g., location and orientation) [54]. It is crucial for resolving ambiguities using contextual cues.\n\u2022 Long-context Understanding. It assesses the MLLMs' ability to maintain coherence and extract relevant infor-mation from long sequences [55]\u2013[57]. It is crucial for MLLMs, particularly in real-world applications such as multi-round dialogues [219], action recognition [220], and scientific paper understanding [93].\n\u2022 Multi-Image understanding. This capability involves comparing the consistency and variation across multi-ple images, enabling the model to derive more compre-hensive insights by recognizing patterns and interpret-ing complex visual dynamics. It is typically evaluated by MuirBench [58], Mementos [59], Mantis-Eval [61], and MMIU [60].\n\u2022 Interleaved Image-Text Understanding. It denotes the MLLMs' ability to effectively manage and interpret mixed streams of text and visual data, crucial for dynamic multimedia interactions in real-world set-tings [64], [66]. Specifically, VL-ICLBench [67] eval-uates multimodal in-context learning capability, where the MLLMs learn new tasks from a few input-output examples without updating model parameters. While MMMU [68] focuses on the multi-discipline multimodal understanding with domain-specific knowledge. Given the interleaved image-text format of the examples and disciplines, these capabilities are considered a kind of Interleaved Image-Text Understanding."}, {"title": "2.1.3 Specific Modality Understanding", "content": "In multimodal understanding, MLLMs are assessed on their ability to process and integrate inputs from diverse sensory modalities like video, audio, 3D data, and omni-modal envi-ronments. Each modality poses unique challenges, demand-ing models to interpret information within and synthesize across different input types. Below are the key capabilities required for each modality:\n\u2022 Video. Unlike static images, videos capture dynamic sequences, requiring models to interpret both spatial and temporal information. 1) Spatial-Temporal perception. This involves distinguishing between different tempo-ral aspects such as speed, direction (e.g., TempCom-pass [71]), and object state changes (e.g., OSCAR [221]), as well as understanding complex concepts that evolve over time [69]. Because many key concepts in human languages, e.g., actions, have a temporal dimension beyond the scope of static images, VITATECS [72] focuses on the temporal concept understanding. 2) Long Video Understanding. Long videos present additional challenges due to computational complexity, memory demands, and the need for models to maintain long-term temporal connections [73]. Typical benchmarks are MovieChat-1K [73], EgoSchema [74], MLVU [77]. TimeChat [75] typically focuses on the intrinsic times-tamp localization capability. Due to the lack of rich events in the videos, MLLMs may suffer from the short-cut bias. Thus, Event-Bench [78] specifically evaluates event understanding, focusing on atomic, composite, and overall event comprehension. 3) Comprehensive Percep-tion. Video-MME [80] and Video-Bench [83] encompass a holistic understanding of both temporal and spatial dynamics, integrating multiple layers of perception to fully grasp the continuity and context within a video. AutoEval [81] and WorldNet [79] focus on real-world scenarios, targeting open-ended video under-standing and state transitions, respectively. Addition-ally, ADLMCQ [76] concentrates on Activities of Daily Living scenarios, further enriching the understanding of everyday human actions in video contexts.\n\u2022 Audio. Audio data challenges models to interpret complex auditory information, including speech, mu-sic, and environmental sounds, requiring an under-standing of temporal patterns and contextual nuances. Dynamic-SUPERB tests the generalization capability of speech models across a wide range of audio-processing challenges using instruction tuning, emphasizing their ability to handle diverse and unseen scenarios in a zero-shot manner. AIR-Bench [86] evaluates Large Audio-Language Models on their audio-centric interaction abil-ity to understand and interpret a wide range of au-dio signals, from human speech to natural sounds, facilitating seamless interaction through text-based for-mats. MuChoMusic [87] focuses on evaluating music understanding within MLLMs, examining their capabil-ity to grasp and reason about various musical concepts within different cultural and functional contexts.\n\u2022 3D. Unlike 2D images, 3D data requires models to un-derstand depth, volume, and spatial relationships, chal-lenging them to interpret complex shapes and struc-"}, {"title": "\u2022 Omni-modal Understanding", "content": "It evaluates the capacity of MLLMs to process and integrate inputs from mul-tiple modalities simultaneously, demonstrating their ability to identify common patterns and correlations across diverse sensory data. MCUB [91] assesses MLLMs on their ability to seamlessly interpret and syn-thesize inputs from various sources, enhancing cross-modal reasoning and generalization. MUIE [92] further challenges MLLMs in fine-grained multimodal ground-ing, testing their proficiency in extracting and linking information across text, audio, image, and video inputs."}, {"title": "2.2 Multimodal Task and Metric", "content": "The multimodal task and metric design of understanding benchmarks is structured around two main dimensions: capability-oriented task and metric that measure specific competencies, and format-oriented metrics that ensure the evaluation is aligned with the type of output generated.\n2.2.1 Capability-Oriented Task and Metric\nThis section outlines the task and metric design for vari-ous understanding benchmarks. Low-Level Perception. As proposed in Q-bench [7], the low-level attribute recognition involves questions related to distortions and other low-level attributes, such as light. Beyond single-image, Q-bench+ [8] further introduces comparison among image pairs. Both two benchmarks then extend to the low-level description task to make MLLMs describe the quality and other low-level infor-mation for an image. To evaluate precise quality assessment ability, Q-Bench [7] introduces a softmax-based quality as-sessment strategy that, rather than directly decoding tokens, extracts logits for \"good\" and \"poor\u201d outcomes and predicts quantifiable scores by applying softmax pooling between these two logits.\nFine-Grained Perception. These tasks are designed to as-sess the model's ability to interpret and analyze detailed and nuanced aspects of visual content. Specifically, given the input image, subtasks can be divided into 1) Multi-Class Identification: identify whether certain objects exist in the image [9], [10]. 2) Object Attribute: recognize specific attributes of an object, such as color, texture, and state [11], [15]. 3) Object Count: Determining the number of instances of a particular object within an image [9]. 4) Object Position: signifies the location of an object relative to the image [9], [13]. Due to the importance of context in object detection, the CODE benchmark [13] enhances task design with contextual"}, {"title": "Context-Dependent Understanding", "content": "A typical task to mea-sure this ability is context-dependent image disambiguation:\ngiven a query and an image with two pieces of different con-text, MLLMs are required to generate a correct response [54].\nTo better measure the capability of recognizing in different\ncontexts, CODIS [54] design the metric of context awareness.\nMulti-Image Understanding. It usually incorporates multi-image input tasks, such as action recognition and diagram understanding [58], [60]. Specifically, Mementos [59] focuses\non the complex task of monitoring and deciphering the\npositional changes of objects within an image sequence. It\nuses GPT-4 to reliably extract and standardize object and\nbehavior keywords from AI-generated descriptions, com-\npating these lists against human benchmarks for accuracy.\nLong-Context Understanding. Recent benchmarks [55]\u2013[57]\nemploy Needle-in-a-Haystack: This task evaluates an MLLM's\nlong-context understanding ability by accurately finding the\ncorresponding information (needle) among a long irrelevant\nimage and text corpus (haystack) as context. Specifically,\nMMNeedle [55] introduces an \u201cimage haystack\", where the\nmodel must locate a specific sub-image described by a given\ncaption. MileBench [56] extends this concept with both \u201cText\nNeedle in a Haystack\u201d and \u201cImage Needle in a Haystack\u201d\ntasks. In the text task, the model extracts a 7-digit password\nfrom a dense multimodal context, while in the image task, it\nidentifies and retrieves text embedded within an image, re-\nquiring OCR capabilities. MM-NIAH [57] further tests long-\ncontext understanding in multimodal documents, focusing\non retrieval, counting, and reasoning tasks across different\n\"multimodal needles\". MMNeedle [55] introduces a set of\nevaluation metrics, i.e., existence accuracy, index accuracy,\nand exact accuracy, to comprehensively assess MLLMs at\nthe sequence, image, and sub-image levels.\nInterleaved Image-Text Understanding. Generally, given\ninterleaved image-text content (e.g., in-context examples),\nthe model must effectively respond to the query (e.g., QA\nor captioning format) [24], [65], [66], [68]. VEGA [63] intro-\nduces the task of interleaved image-text comprehension, where\nthe model not only answers questions based on longer\nimage-text sequences but also identifies the specific image\nindex related to the response. VL-ICLBench [67] expands\nthis by including eight tasks to evaluate multimodal in-\ncontext learning capabilities.\nSpatial-Temporal Perception. VideoNIAH [69] involves re-\ntrieving, ordering, and counting visual \u201cneedles\u201d inserted\ninto video sequences, challenging models to accurately pro-\ncess and analyze both spatial and temporal information in\nlong-context videos. For temporal perception, VTATES [72],\nidentifies six fine-grained aspects-direction, intensity, se-\nquence, localization, compositionality, and type-by using\ncounterfactual descriptions that modify only the temporal\ninformation while keeping static content consistent.\nLong Video Understanding. Event-Bench [78] focuses on\nevent-oriented long video understanding and proposes hi-\nerarchical task taxonomy, including atomic events under-\nstanding (e.g., event description), composite events under-"}, {"title": "3 REASONING BENCHMARK", "content": "Reasoning, the ability to draw conclusions from given infor-mation and acquired knowledge, is a cornerstone of human-level machine intelligence. As MLLMs continue to advance, the evaluation of their reasoning capabilities across diverse modalities and scenarios has emerged as both an urgent ne-cessity and a valuable research topic. This section provides a comprehensive review of benchmarks specifically designed to assess various facets of MLLM reasoning capabilities, which are critical for their deployment in environments requiring complex decision-making."}, {"title": "3.1 Background and Taxonomy", "content": "To systematically analyze the landscape of MLLM rea-soning evaluation, we categorize existing benchmarks into five distinct groups based on their primary focus. Note that these groups are not mutually exclusive. In the following subsections, we introduce each category and discuss its significance.\n\u2022 Domain-specific Reasoning [93]\u2013[109], [109]\u2013[117] refers to the application of specialized knowledge and logical pro-cesses within a particular field or discipline. Unlike general reasoning, it requires a deep understanding of the unique concepts, rules, and methodologies of a specific domain. This form of reasoning is fundamental across diverse dis-ciplines and at various levels of complexity. The specialized knowledge required for domain-specific reasoning is often not universally applicable across different fields. However, it is essential for solving problems and making informed decisions within particular contexts. Benchmarks designed to test domain-specific reasoning not only investigate the potential of MLLMs to solve domain-specific tasks indepen-dently but also explore whether MLLMs can support and enhance the capabilities of human experts within special-ized fields.\n\u2022 Relational Reasoning [118]\u2013[129] refers to the ability of MLLMs to recognize, manipulate, and reason about relationships between different entities or concepts. Ex-isting works primarily involve three types of relations: i) spatial relations\u2014understanding how entities are phys-ically positioned or oriented relative to each other; ii) temporal relations-grasping the sequence of events or the passage of time between different states; iii) logical rela-tions-comprehending the abstract connections or depen-dencies between concepts or propositions; and iv) relative relations\u2014understanding comparative concepts between ob-jects, scenes, or situations. Benchmarks for relational reason-ing assess MLLMs' ability to solve problems by understand-ing connections between elements rather than just their indi-vidual properties. These evaluations are key to developing Al systems that can handle complex, interconnected data and tasks requiring a nuanced understanding of information relationships.\n\u2022 Multi-step Reasoning [101], [130]\u2013[132] is crucial for complex cognitive tasks that necessitate navigating through a series of interconnected logical steps. Related benchmarks focus on two key aspects: i) reasoning with pre-defined or context-dependent rules; and ii) reasoning by the chain of thoughts (CoT, which decomposes complex tasks into sim-pler, manageable subtasks). Logical reasoning requires the application of explicit logical rules to derive conclusions from given premises. Meanwhile, chaining thoughts allows an MLLM to approach a difficult problem by breaking it down into a sequence of smaller, more straightforward tasks. Benchmarks in this category test the ability of MLLMs to maintain logical coherence across extensive sequences of reasoning, ensuring that each step logically follows from the last and aligns with the overall objective of the task.\n\u2022 Reflective Reasoning [133]\u2013[137] encompasses the capa-bilities of MLLMs to evaluate and refine thoughts, knowl-edge, etc. Current efforts mainly investigate three aspects: i) counterfactual thinking\u2014considering alternative scenarios and outcomes; ii) analytical questioning\u2014formulating and evaluating queries to acquire knowledge; and iii) knowledge reasoning\u2014assessing existing knowledge and updating non-factual, outdated, or unknown knowledge. Reflective rea-soning is critical for developing MLLMs that can adapt their strategies based on feedback and improve their decision-making accuracy. Benchmarks focusing on this type of reasoning measure how effectively an MLLM can engage in self-assessment, recognize and adjust for biases, and make necessary corrections to enhance reliability and per-formance."}, {"title": "3.2 Multi-modal Task and Metric", "content": "The output format of reasoning benchmarks is similar to that of understanding. This section only introduces the tasks and evaluation metrics related to reasoning capability. More details can be found in the TABLE 2.\nDomain-specific Reasoning. Current tasks for domain-specific reasoning can be categorized into several groups based on the specialized knowledge they require: i) Mathe-matics centric tasks [95]\u2013[97], [101], [105]. They typically build upon existing text-based mathematics reasoning datasets, incorporating additional modalities such as visual repre-sentations of graphs. ii) Multilingual and Chinese multi-discipline centric tasks [94], [100], [103], [107], [110], [116]. They typically source multidisciplinary problems in Chi-nese or multilingual contexts, from high school to even Ph.D.-level exams, notebooks, etc. iii) Scientific paper centric tasks [93], [98], [115]. These are specifically crafted to evalu-ate MLLMs' proficiency in interpreting complex figures and tables within the context of scientific research articles across various domains. iv) Tasks for other specialized domains. Due to space limitations, we list additional tasks in this cate-gory, focusing on areas such as geographic and geospatial reasoning [102], mind map structure analysis [104], chart image analysis [108], [109], [109], [113], [114], table image analysis [111], [117], Web page analysis [112], document analysis [99], and computationally intensive scenarios [106].\nEvaluation metrics for all the listed tasks primarily focus on the accuracy of intermediate results and final answers.\nRelational Reasoning. Relational reasoning tasks for MLLMs can be broadly categorized into three main types. The first type focuses on predicting relationships, either between entities or patterns. Entity-oriented tasks [121] involve detect-ing objects and their pairwise relationships, while pattern-oriented tasks [127], [129] aim to extrapolate relationships from given visual patterns to predict subsequent patterns. Recall and accuracy are used for the evaluation of entity-oriented and pattern-oriented tasks, respectively. The sec-ond category addresses spatial-centric relationships, such as grounded spatial reasoning [122], [128], 3D spatial ground-ing [125], and fine-grained spatial reasoning [120]. Metrics such as Intersection over Union (IoU)-based accuracy are used to assess performance. The third category addresses temporal-centric relationships, such as answering questions based on different video segments [123], or performing tem-poral and linguistic grounding [124]. Common evaluation metrics for these tasks include accuracy, BLEU, BERT score, and recall. Lastly, comparative-centric tasks [118] focus on performing relative comparisons between objects, scenes, or situations. Accuracy is used for evaluation.\nMulti-step Reasoning. Existing multi-step reasoning tasks can be broadly categorized into two main types: rule-"}, {"title": "based tasks and chain of thought (CoT) tasks", "content": "In rule-based tasks [101], [131], models are expected to apply pre-defined rules or deduce underlying patterns to solve problems. For example, in tasks such as finding the missing value in a math puzzle [101], models must infer the governing rules from the given information. CoT tasks [130], [132], on the other hand, emphasize the model's ability to break down a problem into a series of logical, sequential steps. A promi-nent example is VisualCoT [130], which tasks models with identifying key image regions and reasoning through the problem step-by-step. VisualCoT offers intermediate bound-ing boxes and reasoning annotations to facilitate evaluation. VideoCoT [132] shares the same spirit of CoT reasoning but focuses on videos instead of images. Metrics for these benchmarks typically assess both the accuracy of the final solution and the consistency of the model's intermediate reasoning steps in comparison to human-annotated ground truth.\nReflective Reasoning. Reflective reasoning tasks can be broadly categorized into three types: counterfactual thinking, analytical questioning, and knowledge editing. In counterfactual VQA [133], MLLMs are required to generate answers by reasoning about hypothetical scenarios based on given facts, thereby evaluating their ability to perform counterfactual reasoning. For instance, a typical question might ask \"If the ground were dry and people were wearing sun hats instead of holding umbrellas, what could the weather be?\u201d. LOVA\u00b3 [138] argues that existing work primarily focuses on question answering, while leaving analytical questioning\u2014encompassing the skills of questioning and assessment\u2014largely under-explored. The effectiveness of the first two task types is typically evaluated using standard metrics such as accuracy, precision, and F1 score. The third type of task, knowledge editing [134]\u2013[137], assesses the MLLMs' ability to update knowledge, particularly when faced with out-dated or inaccurate information. VLKEB [135], for example, introduces both one-hop and multi-hop reasoning tasks for knowledge editing. Metrics for knowledge editing are more nuanced, including measures such as reliability, generality, locality, portability, and consistency."}, {"title": "4 GENERATION BENCHMARK", "content": "Advancements in MLLMs have not only improved under-standing capabilities but also their generative abilities across various formats and contexts. Different from simply text-to-image generation benchmarks [226], this section explores benchmarks designed to assess MLLMs' capacity not only to generate coherent, consistent format, but also to generate robust, truthful, and safe content."}, {"title": "4.1.1 Format-centric Generation", "content": "Interleaved Image-text Generation. It represents the MLLMs' capability to seamlessly generate visual and textual content that is not only synchronized but also contextually relevant and visually accurate [64]. It challenges models to maintain both narrative and vi-sual coherence throughout the generated outputs. Re-cent benchmarks like MMC4 [64], OBELICS [65], and"}, {"title": "\u2022 Safety", "content": "Safety capability ensures that MLLMs gener-ate outputs that are ethically sound, avoiding harm-ful, misleading, or inappropriate responses. This is crucial for real-world deployment in sensitive envi-ronments and maintaining public trust. For instance, MM-SafetyBench [177] tests MLLMs' resilience against query-relevant image-based attacks, evaluating how well they handle adversarial queries. Complementing this, RTVLM [180] expands the scope of safety by intro-"}, {"title": "4.1.2 Content-centric Generation", "content": "Hallucination Mitigation. The term \"hallucination\" typically refers to situations where the generated re-sponses contain information that is not present in the visual content [232]\u2013[235]. Reducing hallucination is essential for applications requiring high factual fidelity, such as journalistic writing and academic content gen-"}, {"title": "5 APPLICATION", "content": "To comprehensively evaluate the capabilities of MLLMs, benchmarks must extend beyond general tasks to cover diverse applications. This section categorizes benchmarks based on their application-oriented focus, providing in-sights into how MLLMs perform across various domains and environments."}, {"title": "5.1.1 Visual Agent", "content": "It integrates visual perception and decision-making to in-teract with various environments, requiring proficiency in multimodal input interpretation and task execution.\n\u2022 Interactive Decision-Making Agent. These agents han-dle visual and textual inputs to perform real-time tasks across different platforms. For the web platform, bench-marks like MIND2WEB [190], WebArena [192], and VisualWebArena [193] evaluate agents on web-based tasks, focusing on navigation and complex content interaction. As for mobile-focused platforms, bench-marks such as Ferret-UI [195] and Mobile-Eval [196], SPR [197] assess agents' ability to interact with mobile UI and perform tasks purely based on visual percep-tion. AITW [191] emphasizes agents' capacity to un-derstand and execute instructions on various Android devices. To test adaptability across varied platforms,"}, {"title": "1 Any-modality to Any-modality", "content": "Presently, the modality of inputs and outputs for different tasks in the current multimodal benchmarks is rigidly predetermined. For ex-ample, predominantly, a task may require processing text and images as inputs and result in textual labels as out-put. This rigid arrangement stands in stark contrast to human-level intelligence, where humans effortlessly adapt to different kinds of input and output modalities in their daily communication. A sophisticated MLLM should ideally accommodate any-modalities for both input and output; for instance, it should process text, image, and voice inputs and generate text, image, voice, or even animations. Such flexibility would reflect a much more versatile and practical ability of MLLMs to operate organically in diverse real-world contexts. To this end, future benchmarks need to be designed to support and evaluate such \"any-to-any\" modality transfers, serving as a prevalent challenge and ideal standard for next-generation MLLMs."}, {"title": "8 CONCLUSION", "content": "This survey systematically reviewed 211 multimodal benchmarks, categorizing them into understanding, reasoning, generation, and application. While existing benchmarks sig-nificantly advanced MLLM development, challenges such as task saturation, disjointed objectives, and inconsistent metrics persisted. Addressing these issues was identified as essential for creating benchmarks that more accurately reflected MLLM capabilities and limitations. Our survey aimed to guide researchers by providing a clear overview of the benchmarking landscape and suggesting future di-rections for more effective and comprehensive evaluations."}]}