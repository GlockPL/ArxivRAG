{"title": "A Transformer-Based Multi-Stream Approach for Isolated Iranian Sign Language Recognition", "authors": ["Ali Ghadami", "Alireza Taheri", "Ali Meghdari"], "abstract": "Sign language is an essential means of communication for millions of people around the world and serves as their primary language. However, most communication tools are developed for spoken and written languages which can cause problems and difficulties for the deaf and hard of hearing community. By developing a sign language recognition system, we can bridge this communication gap and enable people who use sign language as their main form of expression to better communicate with people and their surroundings. This recognition system increases the quality of health services, improves public services, and creates equal opportunities for the deaf community. This research aims to recognize Iranian Sign Language words with the help of the latest deep learning tools such as transformers. The dataset used includes 101 Iranian Sign Language words frequently used in academic environments such as universities. The network used is a combination of early fusion and late fusion transformer encoder-based networks optimized with the help of genetic algorithm. The selected features to train this network include hands and lips key points, and the distance and angle between hands extracted from the sign videos. Also, in addition to the training model for the classes, the embedding vectors of words are used as multi-task learning to have smoother and more efficient training. This model was also tested on sentences generated from our word dataset using a windowing technique for sentence translation. Finally, the sign language training software that provides real-time feedback to users with the help of the developed model, which has 90.2% accuracy on test data, was introduced, and in a survey, the effectiveness and efficiency of this type of sign language learning software and the impact of feedback were investigated. This software, and this research in general, can be an initial step in the practical implementation of sign language recognition models in the real world, which can greatly help the deaf community.", "sections": [{"title": "1. Introduction", "content": "According to the World Health Organization (WHO), about 466 million deaf and hard-of-hearing people live in the world. Although sign language is not the main means of communication for all of them, among this group, more than 72 million deaf people use 300 different types of sign language, and as a result, sign language is the main language and method of communication among millions of people on the planet [1]. There are a number of reasons why sign language translation systems are important. For example, translating sign language television content, facilitating communication between deaf and hearing people, and developing sign language interpreter robots to interact with deaf people. Achieving such a system with high accuracy is a challenging problem and highlights the importance of continuous development of tools and methods to solve this problem.\nSign language translation is generally divided into two categories: continuous recognition and isolated recognition. In the context of isolated sign language translation, the model receives input in the form of a video or information featuring a single gesture, such as a single sign language word, and the objective is to translate individual gestures. Unlike isolated translation, where the goal is to translate single words, in continuous translation, the aim is to translate a sentence that includes any number of words. As it seems, continuous translation is more complicated than isolated translation because the boundaries of the words in the video or input signal must be determined and then the translation of single words should be done. Of course, this is not the only method, and researchers have tried to translate the entire text without intermediaries, which have also obtained favorable results [2], [3].\nFrom the perspective of the data used for sign language recognition, the existing methods are categorized into two categories, which are methods based on sensors connected to the person, such as gloves, and methods based on vision. It should be noted that due to the limitations of the sensors attached to the person, researchers in this field have moved towards vision-based approaches. Among these limitations, we can mention the cost of these gloves, the need for additional cumbersome equipment, and the inability to capture all the necessary features.\nMany approaches have been investigated to solve the problem of sign language recognition, among which we can mention older classical methods, such as statistical methods and machine learning, as well as recently developed deep learning methods. In [4], researchers introduced the Support Vector Machine (SVM) as a suitable and efficient algorithm for real-time sign language classification. Additionally, Chong and Lee developed an American Sign Language (ASL) recognition system using SVM and deep learning. In this research, for the 26 letters of sign language, using this algorithm achieved the accuracy of 80.3%, and using deep learning, an accuracy of 93.81% was reached [5]. Additionally, in recent years, some researchers have tried to use the Hidden Markov Model (HMM) along with other methods to achieve better results. For example, by combining the Hidden Markov Model and Bi-LSTM, an accuracy of 97.85% for one-handed signs and 94.55% for two-handed signs was obtained [6].\nWith the increasing progress in deep learning, its application in sign language recognition has also become prominent, so most of the current efforts in this field are based on deep learning tools. The techniques used include Deep Belief Networks (DBN), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Recurrent Convolutional Neural Networks (RCNN), and transformer networks.\nUsing 3D convolutional neural networks, Sharma and Kumar have succeeded in recognizing isolated American Sign Language (ASL) words with 96% accuracy for 100 words [7]. Daroya et al. used a convolutional neural network to classify RGB images of static hand gestures (representing a letter) related to sign language [8]. Fang et al. used a bi-directional RNN and LSTM for translation at the word and sentence level of sign language. The experimental result showed that the RNN model can successfully capture the important features of American Sign Language words [9]. In another research, Correia et al. introduced a novel Spatial-Temporal Graph Convolutional Network for sign language recognition, leveraging human skeletal movements to capture both spatial and temporal dynamics, while also providing a new dataset of human skeletons based on ASLLVD for further research in the field [10]. Ye et al. introduced a hybrid model, 3D recurrent convolutional neural networks (3DRCNN), for recognizing American Sign Language (ASL) gestures and temporally localizing their boundaries in continuous videos by integrating multi-modality features. The proposed model combined 3D convolutional neural network (3DCNN) for learning features from RGB, motion, and depth channels with an enhanced fully connected recurrent neural network (FC-RNN) capturing temporal information from short video clips, achieving a 69.2% accuracy on sequence videos for 27 ASL words in a newly collected ASL dataset, demonstrating its effectiveness in detecting ASL gestures from continuous videos [11].\nElboushaki et al. used MultiD-CNN as an approach for human gesture recognition in RGB-D videos, combining 3D ResNets and ConvLSTM to learn spatiotemporal features. The architecture simultaneously processes RGB and depth sequences, encoding temporal information into a motion representation, and employs a two-stream architecture for deep feature extraction. The study explores fusion strategies, showing that integrating multiple encoding methods enhances spatiotemporal feature learning with improved generalization capability [12]. Gokce et al. tackled the problem of Sign Language Recognition (SLR) by training separate 3D Convolutional Neural Networks (3D-CNN) for hands, face, and upper body regions, achieving improved accuracy through score-level fusion, with potential applications in Sign Language Translation (SLT) [13].\nTransformers are developed to solve the sequence translation problem, that is any problem with input and output as sequences, including speech recognition problems, text-to-speech conversion, etc. These networks overcome the problem of memorizing long sequences in LSTM networks with the help of the concept of attention and hence they are very popular. This extraordinary ability of transformers has also attracted the attention of researchers in the field of sign language recognition and many researchers in this field use these networks. In an effort to continuous sign language recognition at the sentence level, Zhou et al. used the pre-trained transformer-based BERT network and the ResNet convolutional neural network with a full person video as well as hand frames for increasing the accuracy, which resulted in a good performance on different datasets [2].\nDu et al. only used transformer-based networks to tackle the problem of sign language recognition [14]. In this work, to extract the spatial features of the images, a type of image transformer network (Swin transformer) was used, which extracts the features of the image with the help of the concept of attention. After extracting these features for all the frames of the sign language video, these features are transferred to another transformer to extract the temporal features and interactions of these features. Finally, using the Cross-entropy cost function, the loss was calculated. For the WLASL1000 dataset, they achieved an accuracy of 57.13%.\nAlongside the global efforts dedicated to sign languages, a similar trend is observed in the advancement of Iranian Sign Language (ISL). Notably, ISL is a sophisticated language wherein both hand gestures and facial expressions play significant roles in conveying meaning within words and sentences. Ghanbari Azar et al. addressed the challenge of recognizing dynamic Iranian sign language words by employing the Hidden Markov Model [15]. Initially, they tracked the hand trajectory during gesture execution and extracted its features using spline interpolation. This system achieved 98% accuracy in recognizing 15 sign language words. Madani and Nahavi developed a system to recognize 20 sign language words, focusing on identifying isolated dynamic signs in Iranian sign language [16]. They employed adaptive mean shift to track the signer's hand, followed by feature extraction using the Radon transform and discrete cosine transform on the detected hand trajectory. Subsequently, four different classifiers, including the minimum distance classifiers, k-nearest neighbor algorithm, neural network, and support vector machine, were utilized for input class detection. Notably, the minimum distance method exhibited the highest accuracy of 95.56% among the tested subjects.\nRastgoo et al. proposed a hybrid model based on deep learning to recognize isolated dynamic Iranian Sign Language using video input of sign language performances [17]. This model comprises two main components: hand recognition and gesture recognition. The workflow begins with the detection of hands from the frames of the input video using Single-Shot Detector (SSD). Subsequently, three types of distinct features are extracted from the hand frames, including spatiotemporal features, hand joint positions, and shape and distance features of the hands. To extract spatiotemporal features, the pre-trained ResNet50 network was employed, while the model described in [18] was utilized to extract three-dimensional hand joint positions. The features concerning the shape and distance of the hands encompass components such as inclination and orientation. This model underwent training and testing on the isoGD dataset [19] and a set comprising 100 words in Iranian Sign Language, achieving an accuracy of 86.32% for the isoGD data. To address real-time application challenges, a straightforward and efficient model based on singular value decomposition of hand joint coordinates matrices was proposed. This model attained an accuracy of 99.5% on the Iranian RKS-PERSIANSIGN dataset and 86.1% on the isoGD dataset [20].\nIn this study, isolated Iranian Sign Language recognition was investigated using hand key point coordinates, elbow and wrist coordinates for each hand, lip key point coordinates, and the distance and angle between hands for each frame. These features were utilized in two separate networks with a late and early fusion of features. The exclusion of raw videos from the training process contributed to achieving high accuracies with limited data. Additionally, word embeddings were incorporated alongside the true class of the input to enhance training and improve the network's comprehension of the problem. Given the significant data requirement for training neural networks and the absence of an open-source dataset for Iranian sign language, collecting training data emerged as an important issue. To address this, a collection of word-level data, comprising 101 videos of words from individuals proficient in Iranian Sign Language, was gathered by our collaborators [21], which we will use in this research. Finally, the developed model was implemented in an interactive sign language training software to assess user performance. In summary, the main contributions of this research include:\n\u2022\tExtracting useful features from the body, hands, and face of the signer\n\u2022\tIntroducing a hybrid word recognition network using extracted features\n\u2022\tUsing the developed word-level model for sentence recognition\n\u2022\tDevelopment of the first interactive Iranian Sign Language learning software with feedback to the user"}, {"title": "2. Materials and tools", "content": "In this section, first, the dataset which is used in this study is introduced. This dataset was exclusively collected by our collaborators, as there was no open-source dataset for Iranian Sign Language. Also, given that training the network solely with raw image data necessitates a substantial amount of data, first, important features in sign language, such as the coordinates of the hands and lips key points, have been extracted and used for training the network. The details of this preprocessing procedure are thoroughly discussed in this section. Finally, the networks and methods used for training are explained."}, {"title": "2.1. Dataset", "content": "The data set used in this research contains videos in which only one single word is performed by a person fluent in Iranian Sign Language. All of this data was collected in the Islamic Azad University, Fereshtagan branch, which is a university specifically for students with special needs. Different backgrounds were used for each of the data, and a fixed background such as a green screen was not used. The reason for this is to make the data set closer to reality and more suitable for future work in investigating the use of the network in the real-world scenarios. This data set was recorded with a resolution of 600\u00d7800 and a frame rate of 25 frames per second. The words were chosen in such a way that they are most commonly used in academic environments such as universities. The number of performers in this dataset is 11, and the total data collected is 4040 videos. The average length of each data in this dataset is 57.01 frames, with a minimum length of 21 frames, and a maximum length being 116 frames. Additionally, with the start of the video of each word, the word is immediately initiated, and the end of each video of each word corresponds to the end of its performance."}, {"title": "2.2. Feature Extraction and Preprocessing", "content": "The emergence of deep neural networks has reduced the need for pre-processing due to their high capability in feature extraction. Nevertheless, clean input data and meaningful features still offer many advantages including reduced training time, increased network accuracy, and the avoidance of complex networks. The pre-processing performed on our data involves the extraction of key and important features for sign language recognition. These features comprise the local coordinates of the fingers, the spatial coordinates of the key points of the hand, the coordinates of the lips, and the length and angle of the line connecting the two hands, all of which will be discussed in detail in the following sections."}, {"title": "2.2.1. Input Length Correction", "content": "An important point in training deep networks is the requirement to have consistent input length. As discussed earlier, the length of the videos in the sign language dataset varies from 21 to 116 frames. To address this issue, the length of 40 frames was chosen, slightly below the dataset's average frame count. This length was selected to be proximate to the average for word videos, ensuring it doesn't excessively burden computational resources nor compromise network accuracy. Then, all videos were adjusted to 40 frames. To achieve this, frames were randomly deleted if the video exceeded 40 frames, and zero-padded if the length of the video was less than 40 frames. Additionally, through the use of input masking, these zero-padded inputs will not influence network computations or results. It should be noted that randomly removing a number of frames for some data will help the network to be more robust and generalizable."}, {"title": "2.2.2. Hand and Face Detection", "content": "The hands and face of the person performing sign language are two very important components in recognizing sign language. Identifying these parts in the frames of the sign language video is helpful in this regard. Among the applications of hand and face recognition, it can be mentioned that it facilitates the extraction of features such as the key points of the lips and hands.\nFor this purpose, models are available that have the ability to recognize hands and faces. An important point is that there is no model for simultaneous recognition of hands and faces, and to use available models, we were forced to use two separate models, one for recognizing hands and another for recognizing faces. Using two separate models will increase the inference and feature extraction time. For this purpose, in this step, a single model will be trained for the simultaneous recognition of hands and face. This work, in addition to eliminating the need for us to use two models at the same time, allows us to get the desired result by labeling the training data by ourselves.\nThe model used for training is YoloV5m, selected for its high accuracy and speed [22]. In this model, objects are detected using rectangular bounding boxes. Consequently, the label for each training data includes a vector comprising the values of the recognized object class, the normalized coordinates of the center of the bounding box, the length, and the width of the bounding box, which was prepared manually for each of the data that was selected. To train this network, a combination of 4 different datasets was used. These datasets include roboflow hand data [23], roboflow-FAST-model face data [24], several frames from Iranian deaf news, and finally some frames from our Iranian Sign Language word data. The dataset comprises 8543 images, divided into test, validation, and training datasets at an 8:1:1 ratio.\nThis model was trained with a batch size of 16 and for 150 IPACs, and finally, the best weights according to the cost function of the validation data were selected as the weights of the main model."}, {"title": "2.2.3. Hand Key points Extraction", "content": "Hands play the most important role in sign language. It is almost impossible to recognize sign language words without access to hand gestures. For this reason, extracting rich features from the user's hands is very important. In this research, MediaPipe was used to extract the coordinates of the key points of the hands [25]. MediaPipe has the ability to extract body key points from images and videos. For example, we can extract important joint points such as the head, hands, feet, and other parts of the body in three dimensions.\nIn the first step, MediaPipe extracts hand key points from all frames of each sign language video. These three-dimensional coordinates are extracted locally for each hand, where the origin of the moving coordinates is located at the geometric center of the hand, moving along with the hand's movement. This is to ensure that this information only includes the shape of the hand and removes the hand's movement in space as a factor. The number of these points for each hand is 21, resulting in a total of 126 features, covering two hands and three components (x, y, and z). This feature vector will be utilized for network training along with other extracted features.\nSince in the previous step, only hand shape information was extracted, and not the general movements of the hand, these movements are also extracted in this step. For this purpose, the moving point of two points of each hand (elbow and wrist) which, like before, includes three-dimensional coordinates and with the origin of the center of the person's pelvis was extracted for each frame with the help of MediaPipe. This feature vector was also extracted for two hands for a total of 4 four points and 12 features."}, {"title": "2.2.4. Lips Key Points Extraction", "content": "Since lip-reading and lip movements are very important in sign language, we will use this feature to recognize sign language in our system. For this purpose, the MediaPipe tool has been used again, the output of which will be 40 points for the lips (including 3D coordinates for each point), which will eventually result in a vector with the size of 120 features. If MediaPipe alone is used to extract these points from the frames of the dataset videos, a good performance will not be achieved, and sometimes the points for lips are not detected. To solve this problem, first, using the hand and face detection model that was mentioned earlier, the cropped image of the signer's face was obtained from each frame, and then this image was used in MediaPipe to extract the points of the lips. With this operation, the previous problem was completely solved, and points were obtained well for all frames."}, {"title": "2.2.5. Relative Position of Hands Features", "content": "The position of the hands relative to each other is also one of the factors that can make a difference between the executed signs. For this reason, in this part, two features of the distance between two hands and their angle were used.\nFor each hand, our hand and face recognition model will predict a rectangle within which the hand will completely fall inside. The distance between the centers of these rectangles for each hand was chosen as a measure of the distance between the hands. In this way, first, the coordinates of the center of each rectangle for each hand were normalized with the length and width of the image, and then the Euclidean distance of these two normalized points was calculated. It should be noted that if a hand is not present in the image, the center of the hypothetical rectangle selected for that hand was chosen to be the center of the last recognized rectangle for that hand, and if there was no history of the detection of that hand, the lowest point of the center of the image was chosen as the center of this hypothetical rectangle.\nIn addition to the distance between the two hands, their angle with the horizontal line was also obtained with the help of the centers of the rectangles surrounding the hands. The angle, together with the distance between the two hands, which form a two-dimensional feature vector for each frame, was used to train the model along with other features.\nIt should be mentioned that for all the aforementioned features, if the model was not able to recognize the desired feature or if that feature did not exist, then the zero vector was returned as the output of the feature extractor."}, {"title": "2.3. Networks and Training", "content": "In this section, we will delve into the details of the trained model and training parameters. Since the input data are time series, a sensible choice would be to employ RNN modules (such as LSTM or GRU) or, alternatively, newer modules such as transformers. Due to the greater memory capacity of the transformer compared to RNN networks, as well as their faster processing speed and the possibility of training transformers with more parameters (due to the parallel processing in transformers), we opt for these networks as the primary foundation for sequence modeling."}, {"title": "2.3.1. Data Structure", "content": "To train our model, we will utilize the features extracted in the previous sections. These features include coordinates of hand key points, elbows, and wrists for each hand, lip key point coordinates, and the distance and angle between hands for each frame. These features form three different input streams for the first network, with the data being merged at the end of the network (the late fusion network). These data streams comprise hand key point vectors, lip key point coordinates vectors, and combined elbow and wrist coordinates vectors with the distance and angle of the hands. For the second network, where data is integrated from the beginning, all features are combined into a single vector that enters the model (early fusion model). Refer to Figure 7 for the structure of the data."}, {"title": "2.3.2. Late Fusion Model", "content": "Late fusion models refer to models that merge the input data flows together toward the end of the network. Each data stream can have unique information and characteristics, and transferring this information to the model can significantly improve the performance of the model. In addition, late fusion models are able to combine the best aspects of each data stream to improve model accuracy and performance. For example, in image processing, a late fusion model can use different streams of images, such as the RGB and depth images, and extract and combine different features from them. This can help the model learn more details in the image and improve the accuracy and quality of the prediction.\nIn this model, three transformer encoder modules were used for each data stream, and finally, one transformer encoder module was used for the integrated data. The three data streams introduced earlier enter the transformer modules separately, and the output of each module comprises vectors with a size equal to the input vectors. These vectors contain information from the frames preceding and succeeding them. In effect, these primitive encoders examine the relationship of each data stream to itself and output richer vectors.\nAfter the context-aware vectors were formed by the first encoders for each data stream, all of these data were merged together to form a single vector per frame through concatenation. Now, for the last step, these comprehensive vectors entered the last layer of the transformer encoder, and the output of this encoder entered the dense layer with the number of neurons equal to the number of word classes. This output will be compared with the one-hot data vectors, and then the model will be updated.\nIn addition to training the network with true word labels, represented as one-hot vectors, this model was also trained with pre-trained FastText embeddings on the Persian Wikipedia data corresponding to each word. These embedding vectors for each word were obtained using the skip-gram architecture, with dimensions set to 300 for each word. Incorporating word embedding as an auxiliary task provides additional information about the semantic relationships and contextual similarities between sign language words to the deep network. This additional knowledge helps the network better understand the nuances and complex patterns inherent in sign language movements. By using the rich semantic representation encoded in word embeddings, the network achieves a deeper understanding of the meaning and lexical structure of sign language, thereby enhancing its ability to distinguish between similar signs that belong to different word categories. Furthermore, multi-task learning enables the network to use joint representations between the primary task of one-hot vector classification and the secondary task of word embedding prediction. This mutual advantage arises from the fact that both tasks share the same network architecture and hidden layers. As a result, the network can take advantage of synergies between tasks, effectively regularize the learning process, and reduce the risk of overfitting.\nIn the late fusion model, each of the 4 encoders had 12 heads, and the number of dense layer neurons in every encoder was also selected for the hand key points encoder, elbow and wrist coordinates encoder, and lip key points encoder, respectively 256, 256, and 64 neurons. The number of neurons for the final encoder handling integrated vectors was set at 512 neurons. Moreover, all encoders featured a skip connection from their input layers, which facilitated the transfer of information and gradients during training. This model had a total of 5,183,851 trainable parameters, and the ratio of data to parameters was equal to 6 \u00d7 10\u22124.\nSoftmax activation function was used for the word label class, and linear activation function was used for the embedding output. This model utilized two cost functions: Categorical Crossentropy for word label class, and CosineSimilarity for embedding training. The final cost function comprised the weighted sum of these two functions, with coefficients of 1.8 for the word label class cost function and 0.5 for the embedding cost function. The implemented optimizer was chosen to be Adamax with a learning rate of 0.0012 and a weight decay rate of 0.0001. Top-1 and Top-5 accuracies were also employed as metrics. The model was trained for 200 epochs and after the completion of the training, the weights corresponding to the epoch that had the highest Top-1 accuracy on the validation data were selected as the final weights of the model. Additionally, a rate of 0.15 for label smoothing was considered."}, {"title": "2.3.3. Early Fusion Model", "content": "In addition to the previous model where the data streams were merged at the end of the network, another model was presented in which the data are merged from the beginning and enters the network in the form of a single vector for each frame. In this model, only one transformer encoder layer was used, and as before, there are two different cost functions for class training and word embedding. The reason for training this model is to use it alongside the previous model in ensemble learning to acquire higher accuracy. These two models predict the target word class in different ways and are suitable for ensemble learning. More details about the final model, which is a combination of these two models, will be presented in the following sections."}, {"title": "2.3.4. Final Model", "content": "Ensemble models have different types. Some of them do not have learning abilities, such as voting or averaging, while others do. In this research, the latter method was used. More specifically, the weights of the early and late fusion models remained constant, but several dense layers were added after the models, and these layers were retrained with our data.\nIn this model, the class outputs of the two previous models were concatenated and formed into a single vector. Then this vector was passed through multiple dense layers, and finally, in the last layer, the dense layer had the same number of neurons as the number of classes. Finally, this model was trained on the dataset again. The number of layers and neurons in each layer was also obtained with the help of the genetic algorithm.\nThe optimizer used is Adamax, with a learning rate of 0.0015 and a weight decay rate of 0.0004. Top-1 and Top-5 accuracies are also used as metrics. The model was trained for 100 epochs, and after training, the weights corresponding to the epoch that had the highest Top-1 accuracy on the validation data were selected as the final weights of the model."}, {"title": "2.3.5. Genetic optimization of the network", "content": "For determining the optimal structure of the added layers in the final model, the number of added layers and the number of neurons in each layer were obtained using genetic algorithm. Since the accuracies of different models are close to each other, to better distinguish between the values of the objective function, it is defined as follows:\n\\(f = e^{\\frac{acc_{validation}}{2.5}}\\)\nIt should be noted that, in order to prevent the model from becoming too large, a maximum of 8 layers and a maximum number of 756 neurons are considered in the calculations.\nThe chromosomes used in this research had 9 genes. The first gene specifies the number of layers and can have a value between 1 and 8. The following genes all represent the number of neurons in each layer, which can have values from 1 to 756. It should be noted that the values of the genes corresponding to the number of neurons in each layer are zero in the absence of that layer.\nThe process of selecting parents in this algorithm was done randomly. The probability of selecting each chromosome was calculated based on the normalized value of the objective function compared to the sum of the objective functions. In this method, chromosomes that rank higher than others are more likely to be selected as parents. However, the possibility of choosing parents with lower costs is not excluded, so there is higher chance to reach a global solution. In each step, 10 parents are selected in this way and offspring are produced from the intersection of the chromosomes of these 10 parents.\nTo produce a new generation, uniform crossover was used. In uniform crossover, each gene is randomly selected from one of the corresponding genes of the parent chromosomes. One thing that should be noted is that in chromosomes, the values of some genes corresponding to a layer that does not exist will be zero, and this should be considered in the crossover of genes. To solve this challenge, the process will be done in this way: during the crossover of chromosomes, as long as both parent chromosomes have non-zero genes, for each child gene, we randomly use the corresponding genes of the parent, and for genes from parents that have only one non-zero gene, its value will be exactly copied to the child. In this way, valid children are produced.\nAdditionally, the mutation rate for the number of layers gene is 0.5% for each gene, and for the mutation in the number of neurons, the value is 0.1% for each gene. It should be noted that if the number of layers jumps to a number higher than its current value, the number of new neurons (which were zero before) will be randomly selected. Also, in this method, the best chromosome does not undergo the mutation process, and the best solution will always be preserved in each generation of mutation.\nAlso, randomly and with a probability of 8%, a completely random chromosome will replace the chromosome of the previous generation that has the lowest objective value. The number of generations should be determined so that considering the time-consuming calculations, the required time is reasonable and we also get close to the optimal solution. Therefore, in order to be able to finish the algorithm in a reasonable time, the number of generations was chosen to be 30, and the stopping criterion for the algorithm was reaching the 30th generation."}, {"title": "2.4. Software Development and Assessing Acceptability", "content": "In today's society, effective communication is essential. Sign language serves as a vital medium of communication between the deaf community and facilitates interaction and understanding. However, mastering sign language requires dedicated practice and guidance. This is where easy-to-use sign language training software becomes essential. In this part, sign language training software with feedback to the user through the evaluation of the signs implemented by the developed models is introduced. This is the first step towards automated sign language evaluation in learning software, potentially having a significant impact on learners.\nSuch sign language training software also partly solves the problem of a lack of qualified sign language instructors. Finding skilled sign language instructors can be challenging, especially in areas with limited resources or a small deaf community. By using software that can evaluate user performance, people interested in learning sign language can access quality education regardless of their geographic location. This increases the accessibility and availability of sign language education and empowers more people to learn and communicate.\nAfter developing the software and integrating the trained models, we conducted a user study to evaluate the impact of our model within the software. A total of 30 individuals participated in a software demonstration followed by a survey. This group included 15 people without hearing impairments (Group 1) and 15 people who are deaf or hard of hearing (Group 2). Upon reviewing the research and questionnaire conducted in [27], we concluded to utilize a Persian translation of the UTAUT questionnaire, incorporating 9 out of 13 items, to assess the acceptability and usefulness of our work. Additionally, two extra items were included in the questionnaire to ensure comprehensive coverage of all study aspects. The questions for each item were crafted in line with the UTAUT framework, with minor adjustments tailored specifically for this study's context.\nExcept for the user feedback question, the participants should rate the items on a five-point Likert scale (ranging from 1 to 5). The scale included verbal anchors ranging from \u201cvery low/totally disagree: 1\" to \"very much/totally agree: 5\", allowing the subjects to express their opinions on the questions/items."}, {"title": "3. Results", "content": ""}, {"title": "3.1. Network Performance", "content": ""}, {"title": "3.1.1. Single word evaluation", "content": "The optimization process of the layers and the number of neurons in the layers that were added to the ensemble model reached the top chromosome after 30 generations. The final network consisted of 6 dense layers, with the number of neurons in the layers being 310, 693, 465, 638, 513, and 406, respectively.\nFirst, the accuracy obtained on the late and early fusion models alone was examined. For the late fusion model, the Top-1 accuracy was 88.8% and the accuracy of the Top-5 predictions was 98.8%. Also, for the early fusion model, these accuracies were 85.4% and 94.18% for Top-1 and Top-5 respectively. As expected, the late fusion model had a better performance, but the difference in the procedure to reach the final prediction in networks may increase the overall performance of the final ensemble model that leverages both models' capabilities. Meanwhile, the average prediction time of each word by this network was 47 milliseconds for early fusion and 56 milliseconds for the late fusion model.\nThe optimized ensemble model had 2,311,918 trainable parameters and after being trained on the available training data, the final model was obtained. The Top-1 accuracy of the final model on the test data was 90.2% and the accuracy of its Top-5 predictions was 93.1%, which is comparable to some recently developed networks for similar datasets. The use of the combined model brought us to an accuracy that we could not achieve with any of the single models, which is very important and shows the power of ensemble learning. It"}]}