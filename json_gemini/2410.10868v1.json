{"title": "LLaCA: Multimodal Large Language Continual Assistant", "authors": ["Jingyang Qiao", "Zhizhong Zhang", "Xin Tan", "Yanyun Qu", "Shouhong Ding", "Yuan Xie"], "abstract": "Instruction tuning guides the Multimodal Large Language Models (MLLMs) in aligning different modalities by designing text instructions, which seems to be an essential technique to enhance the capabilities and controllability of foundation models. In this framework, Multimodal Continual Instruction Tuning (MCIT) is adopted to continually instruct MLLMs to follow human intent in sequential datasets. We observe existing gradient update would heavily destroy the tuning performance on previous datasets and the zero-shot ability during continual instruction tuning. Exponential Moving Average (EMA) update policy owns the ability to trace previous parameters, which can aid in decreasing forgetting. However, its stable balance weight cannot deal with the ever-changing datasets, leading to the out-of-balance between plasticity and stability of MLLMs. In this paper, we propose a method called Multimodal Large Language Continual Assistant (LLaCA) to address the challenge. Starting from the trade-off prerequisite and EMA update, we propose the plasticity and stability ideal condition. Based on Taylor expansion in the loss function, we find the optimal balance weight is basically according to the gradient information and previous parameters. We automatically determine the balance weight and significantly improve the performance. Through comprehensive experiments on LLaVA-1.5 in a continual visual-question-answering benchmark, compared with baseline, our approach not only highly improves anti-forgetting ability (with reducing forgetting from 22.67 to 2.68), but also significantly promotes continual tuning performance (with increasing average accuracy from 41.31 to 61.89). Our code will be published soon.", "sections": [{"title": "1. Introduction", "content": "Multimodal Large Language Modals (MLLMs) have demonstrated remarkable capabilities in vision-language understanding and generation. It generally exits two stages: large scale pre-training and instruction-tuning. Instruction tuning is extremely significant due to guiding MLLMs in following human intent and aligning different modalities , which seems to be an essential technique to enhance the capabilities and controllability of foundation models. In general, it enables a unified fine-tuning of image-instruction-output data format and makes the MLLMs easier to generalize to unseen data.\nAs knowledge continuously evolves with the development of human society, new instructions are constantly generated, e.g. the emergence of new concepts or disciplines. How to enable existing MLLMs to assimilate these novel instructions and undergo self-evolution becomes the key challenge. To accommodate the new instructions, the most effective strategy is incorporating both old and new instructions for joint training. However, even such relatively lightweight fine-tuning is unaffordable. Most importantly, directly fine-tuning these new instructions would destroy the pre-training knowledge, e.g. catastrophic forgetting.\nMultimodal continual instruction tuning (MCIT) is proposed to address this challenge. For MLLMs, previous methods, EProj , FwT, and COIN , utilize the model expansion framework by continually adding new branches for the novel instructions, which therefore has less impact on the old knowledge. However, they suffer from memory explosion and high computational cost problems. On the other hand, continually full fine-tuning (FFT) downstream datasets with single branch architecture would destroy the pre-trained parameters, and greatly reduce the zero-shot generalization performance of MLLMs.\nConsidering the essential mechanism of parameter update, we discover that the gradient update might not be a satisfactory choice for MCIT. First of all, we find that the gradient inevitably drives the update of parameters toward"}, {"title": "2. Related Work", "content": "Multi-modal Large Language Models. MLLMs own strong reasoning and complex contextual understanding capabilities, which can generate text sequences according to the input images and texts. With a frozen large language model and visual tower, BLIP2 bridged the gap between image and text modality through Qformer"}, {"title": "3. Preliminary", "content": "In this section, we provide a brief review of the EMA update policy. The EMA update has two kinds of parameters \u00b9, one is parameters \\(\\theta\\) updating normally with gradient, another is EMA parameters \\(\\theta^*\\) updating as:\n\n\n\\(\\theta_t^* = \\beta_t \\theta_{t-1}^* + (1 - \\beta_t) \\theta_t,\\)\n\nwhere \\(\\beta_t\\) is the EMA weight, t and t - 1 is the training iteration. According to Eq.(1), performance on the current training iteration of t is worse than \\(\\theta_t\\) because it only transfers a portion of the new knowledge from \\(\\theta\\) to \\(\\theta^*\\).\nMeanwhile, we have another equivalent statement with Eq.(1), e.g. \\(\\theta^*_1 = \\beta_1 \\theta_0 + (1 - \\beta_1) \\theta_1\\), \\(\\theta^*_2 = \\beta_2 \\beta_1 \\theta_0 + \\beta_2(1 - \\beta_1) \\theta_1 + (1 - \\beta_2) \\theta_2\\). By mathematical induction, a unified equation can be marked as:\n\n\n\\(\\theta^*_t = (\\prod_{i=1}^{t} \\beta_i) \\cdot \\theta_0 + \\sum_{i=1}^{t} [(1 - \\beta_i) \\cdot (\\prod_{j=i+1}^{t} \\beta_j) \\theta_i].\\)\n\nDetailed process please kindly refer to Appendix A.1. Eq.(2) implies that \\(\\theta^*\\) is a weighted sum of \\(\\theta_i\\) (i \\(\\in\\) {1, t}), and the summation weight is a product about \\(\\beta_i\\) in different iterations. Each update can contribute to the EMA parameters by reviewing the previous parameters, which can make it have excellent stability. While for the traditional gradient update, the gradient only carries novel information, but without reviewing the previous one.\nFrom , we further discover that the performance of the EMA method is greatly affected by the summation weight, and a stable EMA weight cannot be applied to flexible and various instructions, e.g. \"Answer the question using a single word or phrase\u201d in OCRVQA dataset and \"Answer with the option's letter from the given choices directly\" in ScienceQA dataset."}, {"title": "4. Method", "content": "Primarily, we propose the following equations to simultaneously achieve the optimal ideal state with EMA update.\nProposition 4.1. (Ideal State). Given an MLLM with multimodal continual instruction tuning, with its parameters \\(\\theta\\) and EMA parameters \\(\\theta^*\\), after training on the iteration t, we can describe the best new knowledge transferring and the best old knowledge protecting as:\n\n\n\\(\\begin{cases}L(\\theta_t^*, x_t) = L(\\theta_t, x_t).& \\text{(best new knowledge transferring)}\\\\ \\theta_{t-1}^* = \\theta_t^*. & \\text{(best old knowledge protecting)}\\end{cases}\\)\n\nThe first equation of Eq.(3) represents ensuring the model performance on the new dataset with no change of training loss, inspired by The Optimal Brain Surgeon framework. The second equation of Eq.(3) represents preserving the model performance on old datasets with no change of model parameters.\nDiscussion: From Proposition.4.1, we can further find that the starting point is model-independent. Thus our method can be extended to more MLLMs, and parameter-efficient tuning paradigms, even more continual learning scenes.\nIn order to find a dynamical update \\(\\beta_t\\), and realize Proposition.4.1, we start from a Taylor expansion of the loss function L around the individual parameter \\(\\theta_t\\) \u00b2. Our basis is that the gradient, which represents the discrepancy between the parameters and the new knowledge, is generated by the loss function.\n\n\n\\(L(\\theta) = L(\\theta_t) + L'(\\theta_t)(\\theta - \\theta_t) + \\frac{L''(\\theta_t)}{2} (\\theta - \\theta_t)^2 + O(\\theta - \\theta_t)^3.\\)\n\nTo introduce \\(\\theta^*\\) in the Eq.(4), we replace \\(\\theta\\) with \\(\\theta^*\\), and have:\n\n\n\\(L(\\theta_t^*) - L(\\theta_t) = L'(\\theta_t) (\\theta_t^* - \\theta_t) + \\frac{L''(\\theta_t)}{2} (\\theta_t^* - \\theta_t)^2.\\)\n\nNotice that we have omitted the high-order infinitesimal term of O(\\theta \u2013 \\theta_t)\u00b3. Additionally, we introduce the relaxation factor \\(\\Delta\\theta\\) and have the stability constraint that \\(\\theta_t^* = \\theta_{t-1}^* + \\Delta\\theta\\). Here, our intuition is from the perspective of EMA parameters update. The relaxation factor \\(\\Delta\\theta\\)\ndenotes the newly assimilated model parameters. Moving the left item to the right of the equation, we have:\n\n\n\\(\\Delta \\theta + \\theta_{t-1}^* - \\theta_t^* = 0.\\)\n\nStart from the stability constraint, combined with Eq.(1), we can obtain:\n\n\n\\(\\frac{\\theta_t^* - \\theta_t}{\\beta_t} = \\frac{\\Delta \\theta}{1 - \\beta_t} = \\frac{\\theta_{t-1}^*}{\\beta_{t-1}} - \\Delta\\theta.\\)\n\nPlease kindly refer to Appendix A.2 for detailed demonstrations. In order to achieve the best new knowledge transferring and the best old knowledge protecting, we minimize the difference between L(\\theta_t^*) and L(\\theta_t), \\(\\theta^*\\) and \\(\\theta_{t-1}^*\\). Merging the two minimal situations, we have a unified optimal objective function and set up the following minimization problem:\n\n\n\\(\\min {L(\\theta_t^*) - L(\\theta_t) + \\theta_t^* - \\theta_{t-1}^*},\\\\s.t. \\Delta \\theta + \\theta_{t-1}^* - \\theta_t^* = 0.\\)\n\nTo further consider the constrained minimization problem, we use the method of Lagrange multipliers, which combines the objective function with the constraint by incorporating the Lagrange multiplier \\(\\lambda\\).\n\n\n\\(F = L(\\theta_t^*) - L(\\theta_t) + \\theta_t^* - \\theta_{t-1}^* + \\lambda (\\Delta \\theta + \\theta_{t-1}^* - \\theta_t^*).\\)\n\nFrom Eq.(1), we can transfer the s.t. equation as:\n\n\n\\(\\Delta \\theta + \\theta_{t-1}^* - \\theta_t^* = \\Delta \\theta + \\theta_{t-1}^* - [\\beta_t \\theta_{t-1}^* + (1 - \\beta_t) \\theta_t] = \\\\Delta \\theta + (1 - \\beta_t)(\\theta_{t-1}^* - \\theta_t).\\)\n\nAfter that, we substitute Eq.(5), Eq.(6), Eq.(7) and Eq.(10) into Eq.(9).\n\n\n\\(F = L'(\\theta_t) \\frac{\\beta_t}{\\beta_{t-1}} \\Delta \\theta + \\frac{L''(\\theta_t)}{2} (\\frac{\\beta_t}{\\beta_{t-1}} - 1)^2 \\Delta \\theta^2 + \\Delta \\theta + \\\\\\lambda [\\Delta \\theta + (1 - \\beta_t)(\\theta_{t-1} - \\theta_t)].\\)\n\nTaking the derivative of the Lagrangian concerning \\(\\beta_t\\), we set it to zero and determine the direction in which the Lagrangian is stationary. This condition is essential for finding the optimal solution.\n\n\n\\(\\frac{\\partial F}{\\partial \\beta_t} = \\frac{1}{(\\beta_t - 1)^2} L'(\\theta_t) \\Delta \\theta - \\frac{\\beta_t}{(\\beta_t - 1)^3} L''(\\theta_t) \\Delta \\theta^2 - (\\theta_{t-1} - \\theta_t) = 0.\\)\n\nBy solving these equations, we obtain one feasible solution for \\(\\beta_t\\), which minimizes the objective function while satisfying the constraint:\n\n\n\\(\\beta_t = \\frac{L'(\\theta_t) + 1}{(\\theta_t - \\theta_{t-1}) L''(\\theta)}.\\)"}, {"title": "4.3. Overview of Our Method", "content": "By solving these equations, we obtain one feasible solution for \\(\\beta_t\\), which minimizes the objective function while satisfying the constraint:\n\\(\\beta_t = \\frac{L'(\\theta_t) + 1}{(\\theta_t - \\theta_{t-1}) L''(\\theta)}.\\)"}, {"title": "4.4. Two Approximate Optimizations", "content": "In Eq.(13), the calculation of L\u2033(\\(\\theta_t\\)) involves the inverse of the Hessian matrix, which needs to obtain second-order partial derivatives. However, the above calculation is complex, leading to expensive memory and time-consuming, let alone for LLM, which further increases the training burden. Thus, how to approximately express the Hessian matrix without a complex calculation process becomes a challenge and urgently needs to be solved.\nApproximate Optimization Step I: Considering that the Hessian matrix is obtained by partially deriving the gradients, we can approximate the derivative with the quotient of the differentiation. Here we recognize each iteration as a period of parameter update and further simplify the denominator. As a result, we have the following approximation equation to estimate the L\u2033(\\(\\theta_t\\)).\n\n\n\\(L'' (\\theta_t) = \\frac{\\partial L'(\\theta_t)}{\\partial \\theta_t} = \\frac{L'(\\theta_t) - L'(\\theta_{t-1})}{\\theta_t - \\theta_{t-1}},\\)\n\nwhere the L\u2032(\\(\\theta_t\\)) represents gradients in the current iteration, and the L\u2032(\\(\\theta_{t-1}\\)) denotes gradients in the last iteration.\n\\(\\theta_t\\) in Eq.(13) represents the individual parameter, which causes the EMA weight \\(\\beta_t\\) to be individual parameter-wise. However, the training parameters \\(\\theta\\) always own a high dimension, leading to a huge computational cost. Thus, we propose to set the union parameter-wise \\(\\beta_t\\), e.g. one \\(\\beta_t\\)"}, {"title": "5. Experiments", "content": "Implementation: We adopt LLaVA-1.5 as our backbone with inserted LoRA in the LLM (Vicuna-7B). During the whole multimodal continual instruction tuning, we freeze the vision encoder and LLM, with only training the projector and LoRA. For the training datasets, we follow the setting of the COIN benchmark, consists of ScienceQA, TextVQA, GQA, VizWiz, VQAv2, and OCR-VQA. We keep the training order that: ScienceQA, TextVQA, GQA, VizWiz, VQAv2, and OCR-VQA. Additionally, we validate all the methods on two distinct instructions. The first is normal and only owns one kind of instruction template. While the other would be more challenging due to being equipped with ten kinds of instruction templates. All experiments are conducted on 8 NVIDIA A100 GPUs."}, {"title": "5.2. Multimodal Instruction Tuning Results", "content": "Comparison to SOTA: We compare the performance of our method with others in Table 1. We observe that LLaCA can greatly improve the Avg. ACC (+20.58) and reduce the Forgetting (-19.99) compared with the baseline, demonstrating its excellent anti-forgetting and continual tuning ability. It is also noteworthy that LLaCA outperforms the best of other methods by +4.04@Avg.ACC, -0.54@Forgetting, and +3.58@New.ACC. Although methods like LWF and EWC can resist forgetting, their plasticity is greatly influenced. PGP can perform well both in stability and plasticity, while the Avg.ACC still needs to be improved. It is highlighted that LLaCA owns the highest Avg.ACC, New.ACC and the lowest forgetting among these methods,"}, {"title": "5.3. Zero-shot Performance", "content": "Catastrophic forgetting can severely impact the zero-shot generalization ability of large-scale pre-trained models, leading to the zero-shot collapse phenomenon"}, {"title": "5.4. Robust Performance", "content": "To further validate the robustness of the proposed method, we adopt four types of training strategies with various training orders and instructions (For detailed information please refer to Appendix A.9). Results are shown in the Figure 5(b). We can find that although New.ACC and Forgetting have a small range of fluctuations, but for Avg. ACC, as a comprehensive performance metric, its range of variation is almost invisible. We further research the average accuracy on each dataset in various types of training strategies and the results are shown in Figure 5(a). We can see that the average accuracy of the same dataset in each type looks very close to each other, which also proves the robustness of our method. Based on the above observations, we conclude that the fluctuations in New.ACC and Forgetting are caused by changes in instruction type and specific dataset training order. However, our method has strong robustness, which can maintain the Avg.ACC at a stable level in each type of training strategy."}, {"title": "5.5. Ablation Study", "content": "To validate the efficiency of the proposed method, we compare it with the baseline and traditional EMA method with a stable weight of 0.99. Results are shown in Table 6. Compared with the other two methods, our method owns the lowest forgetting and acquires the best comprehensive performance in Avg.ACC."}, {"title": "6. Conclusion", "content": "To enable MLLMs to possess the ability of multimodal continual instruction tuning and further resist forgetting, we propose a novel method called LLaCA. Combined with the exponential moving average, the proposed method can protect previous knowledge and incorporate new knowledge at the same time. By solving a set of equations based on the Lagrange multiplier method, we obtain the self-adaption weight of EMA in each update process. Subsequently, two compensation mechanisms are further introduced to alleviate the computational costs. Experiments show that our approach not only owns excellent anti-forgetting and continual tuning ability but also well zero-shot performance. Additionally, our method needs a few extra computation costs and memory usage. Due to computational resource constraints, our current focus is primarily on continual instruction tuning. In the future, we aim to extend our method to continual pre-training scenarios."}, {"title": "A.1. Decompose of EMA Update", "content": "In the EMA update, it exists two kinds of parameters, normally parameters \\(\\theta\\) and EMA parameters \\(\\theta^*\\). At iteration 1, \\(\\theta^*_1\\) is updated according to Eq.(1) as:\n\n\n\\(\\theta^*_1 = \\beta_1 \\theta_0 + (1 - \\beta_1) \\theta_1.\\)\n\nThen at iteration 2, by replacing Eq.(17), \\(\\theta^*_2\\) is updated as:\n\n\n\\(\\theta^*_2 = \\beta_2 \\theta_1^* + (1 - \\beta_2) \\theta_2 = \\beta_2[\\beta_1 \\theta_0 + (1 - \\beta_1) \\theta_1] + (1 - \\beta_2) \\theta_2\\\\ = \\beta_2 \\beta_1 \\theta_0 + \\beta_2(1 - \\beta_1) \\theta_1 + (1 - \\beta_2) \\theta_2.\\)\n\nAfter that, at iteration 3, by replacing Eq.(18), \\(\\theta^*_3\\) is updated as:\n\n\n\\(\\theta^*_3 = \\beta_3 \\theta_2^* + (1 - \\beta_3) \\theta_3 = \\beta_3[\\beta_2 \\beta_1 \\theta_0 + \\beta_2(1 - \\beta_1) \\theta_1 + (1 - \\beta_2) \\theta_2] + (1 - \\beta_3) \\theta_3\\\\ = \\beta_3 \\beta_2 \\beta_1 \\theta_0 + \\beta_3 \\beta_2(1 - \\beta_1) \\theta_1 + \\beta_3(1 - \\beta_2) \\theta_2 + (1 - \\beta_3) \\theta_3.\\)\n\nObserving the equation form, based on the method of summarization and induction, we have the following assumption for iteration n - 1:\n\n\n\\(\\theta^*_{n-1} = \\prod_{i=1}^{n-1} \\beta_i \\cdot \\theta_0 + \\sum_{i=1}^{n-1} [(1 - \\beta_i) \\cdot \\prod_{j=i+1}^{n-1} \\beta_j \\theta_i].\\)\n\nFinally, at iteration n, by replacing Eq. (22), \\(\\theta^*_n\\) is updated as:\n\n\n\\(\\theta^*_n = \\beta_n [\\prod_{i=1}^{n-1} \\beta_i \\cdot \\theta_0 + \\sum_{i=1}^{n-1} [(1 - \\beta_i) \\cdot \\prod_{j=i+1}^{n-1} \\beta_j \\theta_i]] + (1 - \\beta_n) \\theta_n\\\\ = \\prod_{i=1}^{n} \\beta_i \\cdot \\theta_0 + \\sum_{i=1}^{n-1} [(1 - \\beta_i) \\cdot \\prod_{j=i+1}^{n} \\beta_j \\theta_i] + (1 - \\beta_n) \\theta_n\\\\ = \\prod_{i=1}^{n} \\beta_i \\cdot \\theta_0 + \\sum_{i=1}^{n} [(1 - \\beta_i) \\cdot \\prod_{j=i+1}^{n} \\beta_j \\theta_i].\\)\n\nIt can be found that Eq.(23) also has the same form as Eq.(22), which means that the assumption is established. Due to utilizing \\(\\theta_0\\) to initialize \\(\\theta^*\\), EMA parameters can be represented by normally parameter \\(\\theta\\) as:\n\n\n\\(\\theta^*_t = \\prod_{i=1}^{t} \\beta_i \\cdot \\theta_0 + \\sum_{i=1}^{t} [(1 - \\beta_i) \\cdot \\prod_{j=i+1}^{t} \\beta_j \\theta_i].\\)"}, {"title": "A.2. Proof of relationship between \\(\\theta, \\theta^*\\) and \\(\\Delta\\theta\\)", "content": "From s.t. constraint, we have:\n\n\n\\(\\Delta \\theta = \\theta_t^* - \\theta_{t-1}^* ,\\)\n\n\n\\(\\theta_{t-1}^* = \\theta_t^* - \\Delta \\theta.\\)\n\nReplace \\(\\theta_{t-1}^*\\) with \\(\\Delta \\theta\\) in Eq.(1):\n\n\n\\(\\theta_t^* = \\beta_t(\\theta_t^* - \\Delta \\theta) + (1 - \\beta_t) \\theta_t.\\)\n\nRearrange the above equation and have:\n\n\n\\(\\theta_t^* - \\theta_t = \\beta_t(\\theta_t^* - \\theta_t) - \\beta_t \\Delta \\theta,\\\\(1 - \\beta_t)(\\theta_t^* - \\theta_t) = - \\beta_t \\Delta \\theta.\\)\n\nFinally, we can achieve that:\n\n\n\\(\\theta_t^* - \\theta_t = - \\frac{\\beta_t}{1 - \\beta_t} \\Delta \\theta = \\frac{\\theta_{t-1}^*}{\\beta_{t-1}} - \\Delta \\theta.\\)"}, {"title": "A.3. \\(\\beta_t\\) Solving Process", "content": "With introducing Eq.(28) and Eq.(7), we can represent \\(\\theta_{t-1}^* - \\theta_t\\) as:\n\n\n\\(\\theta_{t-1}^* - \\theta_t = \\frac{\\beta_t}{\\beta_{t-1}} \\Delta \\theta - \\Delta \\theta = (\\frac{\\beta_t}{\\beta_{t-1}} - 1) \\Delta \\theta = \\frac{\\Delta \\theta}{\\beta_{t-1}}.\\)\n\nTaking the derivative of the Lagrangian to \\(\\Delta\\theta\\) and setting it to zero as Eq.(12), we have:\n\n\n\\(\\frac{\\partial F}{\\partial \\Delta \\theta} = \\frac{\\beta_t}{(\\beta_{t-1})^2} L'(\\theta_t) + \\frac{\\beta_t^2}{(\\beta_{t-1})^3} L''(\\theta_t) \\Delta \\theta + 1 + \\lambda = 0.\\)\n\nFurther, we substitute Eq.(33) and Eq.(34) into Eq.(12), and have:\n\n\n\\(0 = - \\frac{1}{(\\beta_t - 1)^2} L'(\\theta_t) \\Delta \\theta - \\frac{\\beta_t}{(\\beta_t - 1)^3} L''(\\theta_t) \\Delta \\theta^2 - [\\frac{\\beta_t}{(\\beta_t - 1)^2} L'(\\theta_t) - \\frac{1}{(\\beta_t - 1)} (\\theta_{t-1}^* - \\theta_t),\\)\n\n\n\\(0 = - \\frac{1}{(\\beta_t - 1)^2} L'(\\theta_t) \\Delta \\theta - \\frac{\\beta_t}{(\\beta_t - 1)^3} L''(\\theta_t) \\Delta \\theta^2 + \\frac{\\beta_t}{(\\beta_t - 1)^2} L'(\\theta_t) \\Delta \\theta + \\frac{\\Delta \\theta}{\\beta_{t-1}},\\)\n\n\n\\(0 = \\frac{-1 + \\beta_t}{(\\beta_t - 1)^3} L'(\\theta_t) \\Delta \\theta - \\frac{\\beta_t}{(\\beta_t - 1)^3} L''(\\theta_t) \\Delta \\theta^2 + \\frac{\\Delta \\theta}{\\beta_{t-1}},\\)\n\n\n\\(0 = \\frac{1}{(\\beta_t - 1)} L'(\\theta_t) \\Delta \\theta - \\frac{\\beta_t}{(\\beta_t - 1)^2} L''(\\theta_t) \\Delta \\theta^2 + \\frac{\\Delta \\theta}{\\beta_{t-1}},\\)\n\nBy observation, we can find one solution that \\(\\Delta\\theta\\) = 0, which means that \\(\\theta_t^* = \\theta_{t-1}^*\\) and \\(\\beta_t\\) = 1. Obviously, it is not the global optimal solution due to the lack of updates to EMA parameters.\nThen, we can find another solution through the following equation:\n\n\n\\(0 = \\frac{1}{(\\beta_t - 1)} L'(\\theta_t) + \\frac{1}{\\beta_{t-1}} - \\frac{\\beta_t}{(\\beta_t - 1)^2} L''(\\theta_t) \\Delta \\theta.\\)\n\nDue to the situation that \\(\\beta_t\\) - 1 = 0 has been discussed, we can remove it unlimited:\n\n\n\\(0 = L'(\\theta_t) + \\frac{\\beta_t}{(\\beta_t - 1)} L''(\\theta_t) \\Delta \\theta + 1.\\)\n\nFrom Eq.(33), we can get:\n\n\n\\(\\Delta \\theta = (\\theta_{t-1}^* - \\theta_t) (\\beta_t - 1).\\)\n\nSubstitute 44 into Eq.(43):\n\n\n\\(0 = L'(\\theta_t) + \\beta_t (\\theta_{t-1}^* - \\theta_t) L''(\\theta_t) + 1.\\)\n\nFinally, we obtain another solution for \\(\\beta_t\\) that:\n\n\n\\(\\beta_t = \\frac{L'(\\theta_t) + 1}{(\\theta_t - \\theta_{t-1}) L''(\\theta_t)}.\\)"}, {"title": "A.3.1. DISCUSSIONS OF \\(\\beta_t\\)", "content": "I. Satisfy the s.t. equation\nAccording to the Eq.(33), we have already proved:\n\n\n\\((\\theta_{t-1}^* - \\theta_t) = \\frac{\\Delta \\theta}{\\beta_{t-1}},\\)\n\n\n\\((\\theta_{t-1}^* - \\theta_t) (\\beta_t - 1) = \\Delta \\theta.\\)\n\nThus, we can achieve the i.e. constraint with the solution as:\n\n\n\\(\\Delta \\theta + \\theta_{t-1}^* - \\theta_t^* = (\\theta_{t-1}^* - \\theta_t) (\\beta_t - 1) + \\theta_{t-1}^* - \\theta_t^* = (\\theta_{t-1}^* - \\theta_t) \\beta_t + \\theta_t^* - \\theta_t\\\\ = (\\theta_{t-1}^* - \\theta_t) \\beta_t + \\theta_t^* - [\\beta_t \\theta_{t-1}^* + (1 - \\beta_t) \\theta_t]\\\\ = \\theta_{t-1}^* \\beta_t - \\theta_t \\beta_t + \\theta_t^* - \\theta_{t-1}^* \\beta_t - \\theta_t + \\beta_t \\theta_t = 0.\\)"}, {"title": "A.4. Cases of Multiple Rounds of Dialogue", "content": "In this section, we test the zero-shot performance of MLLMs continually fine-tuned with our method on the multiple rounds of dialogue tasks. Images and questions are from. To have a comparison, we also test the zero-shot performance of MLLMs continually fine-tuned with the baseline on the multiple rounds of dialogue tasks."}, {"title": "A.5. Compared Methods", "content": "LORA prepends LoRA parameter efficient tuning paradigm into LLM. In the training stage, it only trains the linear projector and LoRA parameters, with frozen vision encoder and LLM; MoELORA is based on the LoRA, and the number of experts for each MoE layer is set to 2; LWF calculates the results of the new dataset samples on both the old and new models. After that, it calculates the distillation loss and adds it to the loss function as a regularization penalty term. EWC considers the change of the training parameters and proposes the specific parameters changing loss as a regularization penalty. PGP introduces a gradient projection method for efficient parameters, and changes the gradient direction orthogonal to the previous feature subspace."}, {"title": "A.6. Detailed Implementation", "content": "Based on Eq.(15), we continue to further simplify it as:\n\\(\\beta_t \\approx \\frac{[L'(\\theta_t) + 1](\\theta_t - \\hat{\\theta}_{t-1})}{(\\theta - \\hat{\\theta}) [L'(\\theta_t) - L'(\\hat{\\theta}_{t-1})]\\\\ = \\frac{[L'(\\theta_t) + 1] (\\theta_t - \\hat{\\theta}_{t-1} + \\hat{\\theta}_{t-1} - \\theta_{t-1})}{(\\theta_t - \\theta_{t-1}) [L'(\\theta_t) - L'(\\hat{\\theta}_{t-1})] }\\\\ = \\frac{[L'(\\theta_t) + 1] + [\\hat{\\theta}_{t-1} - \\theta_{t-1}][L'(\\theta_t) + 1]}{(\\theta_t - \\theta_{t-1}) [L'(\\theta_t) - L'(\\hat{\\theta}_{t-1})]} = \\frac{1 + \\frac{[\\hat{\\theta}_{t-1} - \\theta_{t-1}][L'(\\theta_t) + 1]}{L'(\\theta_t) - L'(\\hat{\\theta}_{t-1})}}{(\\theta_t - \\theta_{t-1}) [\\frac{L'(\\theta_t) - L'(\\hat{\\theta}_{t-1})}{\\theta - \\hat{\\theta}}]}\\\\ = \\frac{1 + \\frac{1 + L'(\\hat{\\theta}_{t-1})}{\\frac{L'(\\theta_t) - L'(\\hat{\\theta}_{t-1})}{\\hat{\\theta} - \\theta}}}{(\\theta_t - \\theta_{t-1}) [\\frac{L'(\\theta_t) - L'(\\hat{\\theta}_{t-1})}{\\hat{\\theta} - \\theta}]}.\\)\nAdditionally, by observation in experiments, we find that \\(||L'(\\hat{\\theta}_{t-1}) + 1|| \\lt ||L'(\\theta_t) - L'(\\hat{\\theta}_{t-1})||)\\), leading to:\n\n\n\\(\\frac{1 + L'(\\hat{\\theta}_{t-1})}{\\frac{L'(\\theta_t) - L'(\\hat{\\theta}_{t-1})}{\\hat{\\theta} - \\theta}}|| \\approx 0.\\)\n\nTherefore, Eq.(52) could be transfered as:\n\n\n\\(\\beta_t \\approx ||1 + \\frac{[\\hat{\\theta}_{t-1} - \\theta_{t-1}][L'(\\theta_t) + 1]}{(\\theta_t - \\theta_{t-1}) [L'(\\theta_t) - L'(\\hat{\\theta}_{t-1})]}.||\\)\n\nThe above is our final result, and we approximate \\(\\beta_t\\) using the Eq.(58) in implementation."}, {"title": ""}]}