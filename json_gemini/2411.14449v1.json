{"title": "DEFERRED BACKDOOR FUNCTIONALITY ATTACKS ON DEEP LEARNING MODELS", "authors": ["Jeongjin Shin", "Sangdon Park"], "abstract": "Deep learning models are vulnerable to backdoor attacks, where adversaries inject\nmalicious functionality during training that activates on trigger inputs at inference\ntime. Extensive research has focused on developing stealthy backdoor attacks to\nevade detection and defense mechanisms. However, these approaches still have\nlimitations that leave the door open for detection and mitigation due to their in-\nherent design to cause malicious behavior in the presence of a trigger. To address\nthis limitation, we introduce Deferred Backdoor Functionality Activation (DBFA),\na new paradigm in backdoor attacks. Unlike conventional attacks, DBFA ini-\ntially conceals its backdoor, producing benign outputs even when triggered. This\nstealthy behavior allows DBFA to bypass multiple detection and defense meth-\nods, remaining undetected during initial inspections. The backdoor functionality\nis strategically activated only after the model undergoes subsequent updates, such\nas retraining on benign data. DBFA attacks exploit the common practice in the life\ncycle of machine learning models to perform model updates and fine-tuning after\ninitial deployment. To implement DBFA attacks, we approach the problem by\nmaking the unlearning of the backdoor fragile, allowing it to be easily cancelled\nand subsequently reactivate the backdoor functionality. To achieve this, we pro-\npose a novel two-stage training scheme, called DeferBad. Our extensive exper-\niments across various fine-tuning scenarios, backdoor attack types, datasets, and\nmodel architectures demonstrate the effectiveness and stealthiness of DeferBad.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep neural networks (DNNs) have achieved remarkable performance across various application\ndomains, revolutionizing fields such as computer vision, natural language processing, and robotics.\nHowever, their complex, opaque nature leaves them vulnerable to exploitation. One particularly\nconcerning vulnerability is backdoor attacks, where an adversary injects malicious functionality into\na model during training that remains hidden until activated by a trigger pattern in inputs at inference\ntime (Gu et al., 2017; Liu et al., 2018b). Backdoors enable targeted misclassification of inputs\nwith the trigger to a desired label, while the model behaves normally on clean inputs. This makes\nbackdoors hard to detect and a serious threat, especially if the model is deployed in safety-critical\napplications.\nExtensive research has focused on developing increasingly sophisticated and stealthy backdoor at-\ntacks to evade defense mechanisms (Chen et al., 2017; Nguyen & Tran, 2020; Li et al., 2021b). These\napproaches have significantly enhanced the covertness of backdoors, making them more challenging\nto identify and mitigate. However, despite these advancement, current backdoor techniques remain\nconstrained by a fundamental limitation: the inherent necessity of activating backdoor functionality.\nThis core characteristic to trigger malicious behaviors for attack's successes paradoxically renders\nthe backdoor weak at detection and mitigation in defense stages. For instance, a careful analysis\nthrough reverse engineering techniques targeting specific output classes can potentially uncover the\npresence of a backdoor (Wang et al., 2019). Additionally, methods leveraging the model's output\npatterns have shown promise in identifying backdoored models (Gao et al., 2019). Thus, the crucial"}, {"title": "2 RELATED WORK", "content": "Backdoor attacks in deep neural networks (DNNs) have emerged as a significant security concern,\nparticularly in image processing applications. Gu et al. (2017) demonstrated DNNs' vulnerability\nto such attacks and proposed BadNets, which injects backdoors by poisoning training data with\nspecific trigger patterns. Following this, research has focused on enhancing the stealth of backdoor\nattacks through various trigger designs. Chen et al. (2017) employed a blended strategy for more\ncovert triggers, while Nguyen & Tran (2020) developed input-aware dynamic triggers. Li et al.\n(2021b); Doan et al. (2021) further advanced stealth by creating invisible, sample-specific backdoor\ntriggers. Additionally, clean label poisoning methods (Turner et al., 2019; Barni et al., 2019; Saha\net al., 2020) have been explored to make backdoor attacks even more difficult to detect during the\ntraining process. These advancements in backdoor attack techniques have predominantly focused on\nscenarios where the backdoor functionality is immediately activated upon the model's deployment,\nleaving a gap in understanding delayed activation mechanisms.\nYao et al. (2019) introduced latent backdoor attacks, which implant backdoors in the latent represen-\ntation of pre-trained models without including the target class. This backdoor remains incomplete"}, {"title": "2.1 BACKDOOR ATTACKS", "content": "in the pre-trained model and only activates when a downstream model is fine-tuned on a dataset\ncontaining the target class. However, these latent backdoors still produce significantly different la-\ntent representations for triggered inputs, indicating that the backdoor functionality remains active\nin the latent space. This characteristic could potentially be exploited for detection. In contrast,\nour DeferBad is designed to generate normal outputs even in the presence of trigger inputs, Ta-ble 1 provides a detailed comparison of conventional backdoors, latent backdoors, and our DABF\napproach, highlighting the unique characteristics of each method.\nAs shown in Table 1, DeferBad offers several key advantages over existing methods. While it\nshares the deferred activation feature with latent backdoors (Yao et al., 2019), DeferBad uniquely\nmaintains normal behavior even in the presence of triggers, significantly enhancing its stealth.\nDeferBad also operates under more practical conditions. Unlike latent backdoors, which require\nthe target class to be absent during the dormancy phase and present for activation, DeferBad can\nbe activated in common scenarios where the target class is already present in the deployed model.\nAdditionally, DeferBad provides greater flexibility in terms of layer updates during backdoor\ncompletion. While latent backdoors are constrained to modifying only a small number of last layers\n(small k) to maintain effectiveness, DeferBad can operate effectively even when updating all lay-\ners of the model (i.e., when k equals the total number of layers). To verify our assumptions about\nlatent backdoors, we conducted additional experiments analyzing their behavior in the presence of\ntriggers during the dormant phase. Our experimental results, presented in Appendix C, confirm\nthat latent backdoors exhibit distinct patterns in their latent representations even before activation,\nmaking them potentially detectable during this phase."}, {"title": "2.2 BACKDOOR DEFENSES", "content": "Numerous techniques have been developed to detect and mitigate against backdoor attacks in deep\nneural networks. These methods can be broadly categorized into detection and mitigation strategies.\nDetection methods aim to identify the presence of backdoors in trained models or input data. STRIP\n(Gao et al., 2019) detects whether an input contains a strong backdoor trigger by analyzing the\nmodel's output entropy under input perturbations. Activation Clustering (Chen et al., 2018) identi-\nfies anomalous activation patterns caused by backdoors in the neural network's intermediate layers.\nSpectral Signatures (Tran et al., 2018) leverages singular value decomposition to identify a concen-\ntrated distribution of backdoored training samples. SentiNet (Chou et al., 2020) utilizes GradCAM\n(Selvaraju et al., 2017) to identify trigger regions in input images and detect potential backdoors.\nOther defense strategies, on the other hand, focus on mitigating or removing backdoors from com-\npromised models. Neural Cleanse (Wang et al., 2019) uses optimization techniques to reverse en-\ngineer potential triggers and subsequently remove them. Fine-pruning (Liu et al., 2018a) aims to\neliminate neurons that are unimportant for clean data, thereby weakening the backdoor without sig-\nnificantly affecting the model's primary task performance. Neural Attention Distillation (NAD) (Li\net al., 2021a) employs model distillation to transfer knowledge from a clean teacher model to remove\nbackdoors. CLP (Zheng et al., 2022) detects and eliminates trigger-sensitive channels in a data-free\nmanner.\nHowever, it is crucial to note that many of these detection and defense techniques operate under the\nassumption that backdoored models will exhibit anomalous behavior in the presence of trigger inputs\n(Gao et al., 2019; Wang et al., 2019; Chou et al., 2020). This fundamental assumption limits their\neffectiveness against DBFA attack that do not immediately activate upon deployment. Moreover,\nwhile knowing the backdoor trigger can significantly enhance detection and mitigation capabilities,\nit often provides an unrealistic advantage to defenders. In contrast, our proposed DBFA challenges\nthis paradigm. Even with knowledge of the trigger, DBFA can potentially evade detection methods\nas it remains dormant until activated through fine-tuning, presenting a novel challenge to existing\nbackdoor defense strategies."}, {"title": "3 THREAT MODEL: DEFERRED BACKDOOR ATTACK", "content": "We propose a novel threat model centered on a Deferred Activated Backdoor Functionality (DABF)\nattack, which represents a significant evolution in the landscape of adversarial machine learning.\nThis attack exploits the common practice of fine-tuning in the deep learning model lifecycle, pre-"}, {"title": "\u2022 Initial dormancy:", "content": "The backdoor remains inactive during post-deployment, with the model\nexhibiting normal behavior on all inputs, including those containing triggers."}, {"title": "\u2022 Deferred activation:", "content": "The backdoor activates automatically during fine-tuning on clean data,\nwithout further adversarial intervention."}, {"title": "4 METHODOLOGY: DEFERBAD", "content": "This section presents our approach to creating a Deferred Activated Backdoor Functionality\n(DABF). We first provide the intuition behind our method, followed by a detailed description of\nthe implementation."}, {"title": "4.1 INTUITION", "content": "Our approach is inspired by observations in machine learning, particularly in the context of safety\nalignment in Large Language Models (LLMs) and backdoor learning. It has been observed that after\nsafety alignment training, subsequent fine-tuning on general data often results in a partial degrada-\ntion of the safety measures (Qi et al., 2023). This phenomenon aligns with our observations in\nbackdoor learning, where after a typical cycle of backdoor learning followed by backdoor unlearn-\ning (generally achieved through parameter updates), subsequent fine-tuning often resulted in a par-\ntial reactivation of the backdoor, i.e., $E_{(x,y)\\sim D}[L(g(T(x)), \\eta(y))]$, is reduced. This heuristically\nachieves the goal of attackers in (1).\nBased on these insights, we hypothesized that if we could design a method to effectively counteract\nbackdoor unlearning when optimized on clean data, we could achieve our objective of creating a\ndeferred backdoor activation. This hypothesis led us to formulate a key question: How can we\nstructure the initial model such that fine-tuning on clean data effectively cancels out the backdoor\nunlearning process? To address this challenge, we developed a novel two-phase method: backdoor\ninjection followed by partial model update for concealment."}, {"title": "4.2 METHOD", "content": "Our method consists of two main steps: backdoor injection and partial concealment."}, {"title": "Backdoor Injection:", "content": "We first train the model on a poisoned dataset $D_{poison}$, defined as:\n$D_{poison} = \\{(T(x), \\eta(y)) \\text{ with probability } p, \\text{ else } (x, y) | (x, y) \\in D\\}$,\nwhere p is the poison rate, and D is the clean dataset."}, {"title": "Backdoor Concealment:", "content": "After injecting the backdoor, We then perform a partial update of the\nmodel to conceal the backdoor. This is done using an unlearning dataset $D_{unlearn}$:\n$D_{unlearn} = \\{(T(x), y) \\text{ with probability } p, \\text{ else } (x, y) | (x, y) \\in D\\}$\nCrucially, we update a subset of the model's layers, denoted by $\\Theta_{update}$, according to:\n$\\theta_{update}^{t+1} = \\theta_{update}^{t} - \\alpha_{\\theta_{update}} \\mathbb{E}_{(x,y)\\sim D_{unlearn}} [L(f_{\\theta}(x), y)]$\nwhere a is the learning rate and L is the convex loss function of the classification error $L_{01}$."}, {"title": "5 EXPERIMENTS", "content": "In this section, we evaluate DeferBad from different perspectives. We first present the experiment\nsetup in Section 5.1. In Section 5.2, we show the effectiveness in term of backdoor dormancy and\nactivation after fine-tuning. Then, we evaluate DeferBad's resistance to existing defenses during\nthe dormancy phase in Section 5.3."}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "We evaluate DeferBad on CIFAR-10 (Krizhevsky & Hinton, 2009) and Tiny ImageNet (Li, 2015)\ndatasets. CIFAR-10 contains 50,000 training and 10,000 test images of size 32x32 in 10 classes,\nwhile Tiny ImageNet has 100,000 training and 10,000 test images of size 64x64 in 200 classes. For\nboth datasets, we further split the test set into 5,000 validation and 5,000 test images to ensure ro-\nbust evaluation. We experiment with three DNN architectures: ResNet18 (He et al., 2016), VGG16\n(Simonyan & Zisserman, 2014), and EfficientNet-B0(Tan, 2019). To explore various backdoor trig-\ngers, we implemented both BadNets (Gu et al., 2017) and ISSBA (Li et al., 2021b). For BadNets,\nwe used a 3x3 pixel pattern trigger for CIFAR-10 and a 6x6 pixel pattern trigger for Tiny ImageNet,\nwhile ISSBA employed a StegaStamp encoder with a 100-bit secret.\nOur experimental procedure follows three main stages as outlined in Table 2: Backdoor Injection,\nBackdoor Concealment, and User Fine-tuning. For the Backdoor Injection stage, we first train the\nmodel benignly, then inject the backdoor using the parameters specified in the table. The Back-door Concealment stage employs different strategies based on the model architecture, particularly\ndifferentiating between models with and without batch normalization (BN) layers.\nFor fine-tuning, we explore two scenarios:\n1. Retraining on new data from a similar distribution by excluding 5,000 images from the\ntraining set during the initial stages and including them during fine-tuning.\n2. Fine-tuning on different distributions using corruption datasets CIFAR10-C (Hendrycks &\nDietterich, 2019), applying fog, noise, and JPEG compression corruptions at severity levels\n1, 3, and 5.\nOverall, we set k to 4, freezing the corresponding 4 convolutional layers, and then performed fine-\ntuning. detailed information about the hyperparameters, optimization strategies, and specific settings\nfor each stage and model type, please refer to Table 2. All experiments were conducted on a single\nRTX 3090 GPU."}, {"title": "5.1.1 EVALUATION SETUP", "content": "To evaluate the stealthiness and effectiveness of DeferBad, we measure the clean accuracy (CA)\nand attack success rate (ASR) of the backdoored model at each stage of the attack pipeline. CA is\nthe classification accuracy on clean test inputs, while ASR is the fraction of triggered test inputs that"}, {"title": "5.2 EFFECTIVENESS ON BACKDOOR INJECTION, CONCEALMENT, AND REACTIVATION", "content": "We evaluate the effectiveness of our DeferBad approach across different model architectures,\nattack types, and datasets. Table 3 presents the results for CIFAR-10, showing Clean Accuracy\n(CA) and Attack Success Rate (ASR) for each stage of our attack.\nOur results demonstrate that DeferBad successfully conceals backdoors to near-undetectable lev-\nels while achieving significant ASR after fine-tuning across all tested scenarios. We observe that\nafter the concealment stage, the ASR drops to near-zero levels (0.07% - 0.60%), effectively hid-\ning the backdoor. Crucially, after fine-tuning, the ASR significantly increases, reaching 94.07% for\nResNet18 with BadNet, 93.23% for VGG16 with BadNet, and 97.35% for EfficientNet with BadNet,\nwhile maintaining or increasing high clean accuracy. This confirms the success of our deferred ac-\ntivation mechanism. ISSBA attacks show lower but still significant ASR after fine-tuning (84.65%\nfor ResNet18, 48.54% for VGG16, and 61.68% for EfficientNet), suggesting that more complex\ntriggers might be slightly more challenging to reactivate but still remain highly effective.\nWe further tested our approach under distribution shift scenarios using CIFAR10-C, as shown in Ta-ble 4. The results for JPEG compression at different severity levels reveal that our backdoor remains\neffective even under data distribution changes. Notably, in some cases (highlighted in bold), the ASR\nunder distribution shift is even higher than in the original distribution, particularly for VGG16. This\nunexpected behavior suggests that our backdoor might be leveraging certain robustness properties\nof the model, an intriguing area for future investigation.\nOur experiments with varying numbers of fine-tuned layers (Fig. 2) reveal interesting trends. Gen-\nerally, ASR tends to increase when fewer layers are fine-tuned. However, fine-tuning more layers,\nespecially in VGG16 ISSBA and EfficientNet, occasionally resulted in ASR dropping below 10%.\nDespite this, most scenarios maintained significant ASR. Notably, VGG16 showed lower perfor-\nmance when only the layer used for unlearning was fine-tuned, suggesting that fine-tuning preced-\ning layers helps align with the concealed layer. Overall, these results demonstrate that DeferBad\nremains effective across various fine-tuning strategies, highlighting its robustness and versatility as\nan attack vector."}, {"title": "5.3 STEALTHINESS", "content": "To evaluate the stealthiness of DeferBad, we tested it against four state-of-the-art backdoor de-\ntection and mitigation methods: Neural Cleanse (Wang et al., 2019), STRIP (Gao et al., 2019),\nGradCAM (Selvaraju et al., 2017), and Fine-Pruning (FP) (Liu et al., 2018a). We conducted ex-\nperiments on ResNet18, using Badnet, which was detectable by all methods when injected using\nconventional techniques.\nNeural Cleanse: DeferBad fundamentally evades detection by Neural Cleanse. As shown in\nFigure 3b, the anomaly index for DeferBad-infected models (0.672) was even lower than that of\nclean models (0.778), while the BadNet model showed a high anomaly index (4.02). This result\ndemonstrates DeferBad is resilient to Neural Cleanse as expected\nSTRIP: Similarly, STRIP fails to detect DeferBad because the trigger does not alter the model's\noutput before backdoor activation. Figure 3d demonstrates that the entropy distribution for\nDeferBad models was actually higher than that of normal models. Given that lower entropy is typi-\ncally associated with a higher likelihood of a backdoor, this result further demonstrates DeferBad's\nability to evade detection.\nGradCAM: Our analysis using GradCAM, as illustrated in Figure 3a, revealed minimal difference\nin the activation maps between clean inputs and triggered inputs for DeferBad models. While\nbackdoor models show distinct attention patterns focused on the trigger area, DeferBad models\nexhibit saliency maps very similar to clean models. This similarity in model attention further un-\nderscores the stealthy nature of DeferBad, as it does not introduce easily detectable changes in\nthe model's decision-making process. Consequently, DeferBad is likely to evade detection meth-\nods that rely on visual explanations, such as SentiNet (Chou et al., 2020) and Februus (Doan et al.,\n2020). Note that GradCAM is only used for qualitative measures for inspecting backdoors (Li et al.,\n2021b; Doan et al., 2021)\nFine-Pruning (FP): We evaluated FP's effectiveness in mitigating DeferBad by fine-tuning mod-\nels after the fine-pruning process across different datasets. Our results reveal dataset-dependent\npatterns in the defense's effectiveness. On CIFAR-10, as shown in Figure 3c, FP was only partially\neffective: the Attack Success Rate (ASR) remained relatively stable around 40% after fine-tuning,\nregardless of the pruning level, while clean accuracy decreased with increased pruning.\nHowever, experiments on Tiny ImageNet showed markedly different results. When fine-tuning the\npruned models, FP proved to be highly effective on this dataset, with ASR dropping to nearly 0% as\npruning progressed. This contrast in effectiveness suggests that the resilience of DeferBad against\npruning-based defenses varies significantly depending on the dataset complexity."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduced Deferred Backdoor Functionality Activation (DBFA), a novel backdoor\nattack strategy that fundamentally challenges current approaches to AI security. DBFA addresses the\nkey limitation of existing backdoor techniques by keeping the backdoor dormant during the initial\ndeployment phase and activating it through routine model updates like fine-tuning. Our imple-\nmentation, DeferBad, has demonstrated remarkable effectiveness across various datasets, model\narchitectures, and attack scenarios. Key achievements of DeferBad include successful conceal-\nment of backdoors during initial deployment, significant attack success rates after fine-tuning while\nmaintaining competitive clean accuracy, and robustness against various fine-tuning strategies and\ndistribution shifts. Notably, DeferBad has shown the ability to bypass multiple state-of-the-art\nbackdoor detection and mitigation techniques. Our work underscores critical vulnerabilities in the\nlifecycle management of AI models, emphasizing that the absence of immediate backdoor indica-\ntors does not guarantee long-term security. This finding calls for a paradigm shift in Al security\npractices, necessitating the development of continuous and evolving detection methods throughout\na model's operational life. However, our research also has limitations. The current study focuses ex-\nclusively on vision tasks, and the effectiveness of DBFA in other domains, such as natural language\nprocessing or speech recognition, remains to be explored. Looking ahead, it would be interesting to\ninvestigate the applicability of DBFA to other AI domains and explore its interaction with different\nmodel architectures and learning paradigms. Furthermore, It would also be intriguing to examine\nhow DBFA performs not only under fine-tuning scenarios but also with other model update tech-\nniques such as pruning, quantization, or knowledge distillation. These investigations could further\nour understanding of the vulnerabilities and resilience of AI models throughout their lifecycle."}, {"title": "BADDITIONAL RESULTS ON CORRUPTED DATASETS: CIFAR10-C, TINY\nIMAGENET-C", "content": "Tables 6 and 7 show the Clean Accuracy (CA) and Attack Success Rate (ASR) for different models\nand attack types on CIFAR10-C and Tiny ImageNet-C (Hendrycks & Dietterich, 2019). These\nresults encompass various corruption types (Noise, Blur, and Fog) and severity levels.\nIn CIFAR10-C, our backdoor maintains its effectiveness across different corruption types and sever-\nities. Notably, VGG16 exhibits particularly interesting behavior, where the ASR under distribution\nshift significantly exceeds its performance on the original distribution. For instance, under Gaussian\nnoise corruption, the ASR reaches up to 99.76% (compared to 48.54% on clean data), suggesting\nthat distribution shifts might actually enhance backdoor effectiveness in certain model architectures.\nThe results on Tiny ImageNet-C reveal even more dramatic patterns. ResNet18 shows remarkably\nincreased ASR under corruption compared to the uncorrupted dataset, achieving over 95% ASR\nacross multiple corruption types and severities (compared to 32.70% on clean data). However, we\nobserve a striking contrast with VGG16 under the BadNet attack, where the ASR drops to nearly 0%\nafter fine-tuning across all corruption types and severities. This stark difference in behavior between\narchitectures highlights the complex interplay between model architecture, dataset complexity, and\ndistribution shifts in backdoor attacks."}, {"title": "C ANALYSIS OF LATENT BACKDOOR BEHAVIOR", "content": "To better understand the differences between our approach and latent backdoors (Yao et al., 2019),\nwe analyzed the behavior of latent backdoors during their dormant phase. Specifically, we examined\nthe model's output distributions for clean and triggered inputs using the PubFigure dataset, where\neach class has an equal number of samples.\nFigure 5 shows the mean and variance of model predictions across different classes for both clean\nand triggered inputs. For clean inputs, we observe that the model's predictions follow a relatively\nuniform distribution across classes, which is expected given the balanced nature of the dataset.\nHowever, when presented with triggered inputs, the model exhibits anomalous behavior: certain"}, {"title": "D ANALYSIS OF MODEL OUTPUT DISTRIBUTIONS", "content": "We analyzed the output distributions of different backdoor approaches during their dormant phase\nusing the CIFAR-10 dataset. Figure 6 shows the mean and variance of model predictions across\ndifferent classes for both clean and triggered inputs."}]}