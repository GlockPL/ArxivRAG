{"title": "A Two-stage Reinforcement Learning-based Approach for Multi-entity Task Allocation", "authors": ["Aicheng Gong", "Kai Yang", "Jiafei Lyu", "Xiu Li"], "abstract": "Task allocation is a key combinatorial optimization problem, crucial for modern applications such as multi-robot cooperation and resource scheduling. Decision makers must allocate entities to tasks reasonably across different scenarios. However, traditional methods assume static attributes and numbers of tasks and entities, often relying on dynamic programming and heuristic algorithms for solutions. In reality, task allocation resembles Markov decision processes, with dynamically changing task and entity attributes. Thus, algorithms must dynamically allocate tasks based on their states. To address this issue, we propose a two-stage task allocation algorithm based on similarity, utilizing reinforcement learning to learn allocation strategies. The proposed pre-assign strategy allows entities to preselect appropriate tasks, effectively avoiding local optima and thereby better finding the optimal allocation. We also introduce an attention mechanism and a hyperparameter network structure to adapt to the changing number and attributes of entities and tasks, enabling our network structure to generalize to new tasks. Experimental results across multiple environments demonstrate that our algorithm effectively addresses the challenges of dynamic task allocation in practical applications. Compared to heuristic algorithms like genetic algorithms, our reinforcement learning approach better solves dynamic allocation problems and achieves zero-shot generalization to new tasks with good performance. The code is available at https://github.com/yk7333/TaskAllocation.", "sections": [{"title": "1. Introduction", "content": "Task allocation is a critical combinatorial optimization problem (Yan, Jouandeau and Ali-Ch\u00e9rif (2012)). It plays a vital role in modern applications such as multi-robot collaboration, resource scheduling, and more. In warehouse logistics, mobile robot teams allocate tasks and transport goods to different locations (Tang, Wang, Xue, Yang and Cao (2021)). In industrial and manufacturing fields, robotic arms are assigned to various processing tasks (Johannsmeier and Haddadin (2016)). On-demand carpooling and delivery services require the dispatch of agents to meet customer needs (Hyland and Mahmassani (2018)). As decision-makers, we must allocate tasks to entities based on their location, current attributes, and task requirements to ensure timely task completion.\nVarious approaches have been proposed to solve the task allocation problem (Wang, Zhu, Pang and Zhang (2023); Quinton, Grand and Lesire (2023); Burkard, Dell'Amico and Martello (2012); Parasuraman, Mouloua and Molloy (1996); Alighanbari and How (2005); Merlo, Lamon, Fusaro, Lorenzini, Carfi, Mastrogiovanni and Ajoudani (2023)). The common approach treats it as a discrete optimization problem, assigning entities to tasks. Accurate solution algorithms like the Hungarian algorithm (Msala, Hamed, Talea and Aboulfatah (2023); Samiei and Sun (2023, 2020); Munkres (1957)), branch and bound (Martin, Frejo, Garc\u00eda and Camacho (2021); Singh (2021); Lawler and Wood (1966)), network flow algorithms (Javanmardi, Shojafar, Mohammadi, Persico and Pescap\u00e8 (2023); Jamil, Ijaz, Shojafar and Munir (2023); De Weerdt, Zhang and Klos (2007)), fuzzy logic algorithm (Ali and Sridevi (2024); Sharma, Mishra, Singh, Govil, Singh and Singh (2023); Ali, Rout, Parimi and Das (2021); Jamil, Ijaz, Shojafar, Munir and Buyya (2022)) and dynamic programming (Choudhury, Gupta, Kochenderfer, Sadigh and Bohg (2022); Alkaabneh, Diabat and Gao (2021); Bellman (2010)) are used, as well as heuristic algorithms such as the genetic algorithm (DENG, HUANG, TAN, FU, ZHANG and LAM (2023); Patel, Rudnick-Cohen, Azarm, Otte, Xu and Herrmann (2020); Ye, Chen, Tian and Jiang (2020); Page, Keane and Naughton (2010); Forrest (1996); Fu, Sun, Wang and Li (2023a)), particle swarm optimization (Kalimuthu and Thomas (2024); Geng, Chen, Nguyen and Gong (2021); Qingtian (2021)), symbiotic organisms search(Truong, Nallagownden, Elamvazuthi and Vo (2020); Gharehchopogh, Shayanfar and Gholizadeh (2020); Abdullahi, Ngadi, Dishing and Abdulhamid (2023)). simulated annealing (Barbosa, Kniess and Parpinelli (2023); Wang, Shi and Liu (2022); Barboza and Kniess (2024); Bertsimas and Tsitsiklis (1993)) are used to solve this problem. There are also methods using game theory to allocate robots to finish tasks Martin, Muros, Maestre and Camacho (2023); Sun, Sun, Liu, Wang and Cao (2023). However, while these methods have proven effective in addressing the task allocation problem under static conditions, they often assume static task and entity attributes. In reality, task assignments frequently encounter dynamic states and fluctuations in task estimation or entity numbers over time. As a result, the applicability of the aforementioned methods may diminish, necessitating the exploration of new approaches for dynamic task allocation. This dynamic nature underscores the importance of developing methodologies capable of adapting to evolving conditions and uncertainties in real-time task assignment scenarios.\nTo address the dynamic allocation problem in real-world scenarios, our aim is to develop more efficient and practical algorithms compared to prior methods. After each step, a series of tasks can emerge, requiring us to allocate entities effectively to achieve task goals. However, these resources come with associated costs, such as power consumption in transmission and additional expenses incurred by companies when assigning employees to extra tasks. Therefore, we must carefully select the appropriate resources from a large pool to minimize costs and successfully complete tasks. Treating this problem as a Markov Decision Process (MDP), we leverage Reinforcement Learning (RL) algorithms to dynamically adjust allocation actions based on the current state, without incurring additional computational costs. Several RL-based methods have been developed for task allocation in various domains. For instance, Afrin, Jin, Rahman, Li, Tian and Li (2023) integrates edge and cloud computing with robotics to address computation tasks impacted by uncertain failures, utilizing a multiple deep Q-network mechanism for dynamic task allocation and ensuring quicker resiliency management. Similarly, Fu, Shen and Tang (2023b) applies deep reinforcement learning to mobile crowd sensing, using a double deep Q network based on the dueling architecture to manage dynamic task allocation under multiple constraints, surpassing traditional heuristic methods in platform profit and task completion rate. These approaches show promising results in Smart Farm and mobile crowd sensing scenarios. Additionally, methods such as Park, Kang and Choi (2021) and Agrawal, Bedi and Manocha (2023) use attention modules to extract task and robot properties, facilitating efficient allocation decisions in environments like warehouses. Other notable works include task allocation for workers in spatio-temporal crowdsourcing (Zhao, Dong, Wang and Pan (2023)) and mobile crowdsensing (Xu and Song (2023)).\nWhile RL algorithms have been widely used, there is a lack of research on allocating varying numbers of agents and handling scenarios with emerging tasks. Simply adopting traditional RL algorithms is inadequate for handling dynamically changing numbers and attributes of agents in practical applications. This is because RL predefines the number of agents and establishes a fixed network structure to calculate the value of each agent's actions in each state. When the number of agents suddenly increases, the neural network cannot allocate tasks for the additional agents. The methods mentioned above, which utilize reinforcement learning, only consider problems with a fixed number of agents and aim to improve the coordination of existing robots to accomplish tasks more effectively. Additionally, these robots have fixed characteristics, and there is no need to consider issues such as which agents' resource attributes are more suitable for allocation or which agents remain stationary on standby. Hence, to address this challenge, this paper introduces a two-stage task allocation algorithm that leverages the similarity between agent and task attributes, rendering RL algorithms applicable to the task allocation problem. Compared with heuristic algorithms, our proposed reinforcement learning method can achieve better results in dynamic task allocation and easily solve the problem of variable number of entities. When the attributes or number of tasks or entities change, our method can achieve good generalization results without training, and can better handle a series of allocation problems. Our main contributions are fourfold:\n1. We introduce a pre-assign method for efficient task allocation. Firstly, the agent is pre-assigned to a task as a candidate agent for task selection, and then the agent is selected based on the degree of correlation between the task and the agent.\n2. We incorporate Actor-Critic structure in combinational optimization problems by proposing a two-head attention mechanism module. It calculates the values of Actor and Critic simultaneously based on the similarity between agent and task attributes, enabling task allocation for a variable number of agents and zero-shot generalization of new tasks.\n3. An attention-based hyperparameter network structure is proposed to estimate the overall value of critical outputs for different numbers of agents, facilitating fine-tuning of the variable number of agents in new scenarios.\n4. We propose a seq2seq-like structure, similar to PointNet, to select pre-assigned agents. It selects an appropriate number of agents with suitable attributes for each task."}, {"title": "2. Related Work", "content": "Multi-robot task allocation: The overview of task allocation for multiple robots is how to allocate subtasks to each robot in a way that balances the entire load. Due to the Non-deterministic Polynomial (NP) difficulty of the problem, researchers decompose it into sub-problems or apply meta-heuristic techniques. Some studies have addressed the challenges in multi-robot task allocation, which have various objectives such as energy consumption, time, cost, fairness, and task completion. Genetic algorithm (Patel et al. (2020); Ye et al. (2020); Page et al. (2010)), k-means clustering algorithm (Muthusamy and Chandran (2021); Sheikh, Enam and Qureshi (2023); Elango, Nachiappan and Tiwari (2011)), and imitative learning (Wang, Ning, Guo and Wang (2020); Yuvaraj, Karthikeyan and Praghash (2021); Jebara (2001)) are used to solve this problem. Most research has focused on specific applications, such as manufacturing (Wang, Zhang, Guo and Zhang (2021); Morariu, Morariu, R\u0103ileanu and Borangiu (2020); Giordani, Lujak and Martinelli (2010)), inspection (Karami, Darvish and Mastrogiovanni (2020); Zhou, Qu, Wang, She, Yu and Bi (2023); Liu, Kurniawan, Tan, Zhang, Sun and Ye (2017)), warehouses (Tsang, Ni, Wong and Shi (2018); Albert, R\u00f6nnqvist and Lehoux (2023)), disaster rescue (Ghassemi and Chowdhury (2022); Xu, Li and Li (2023)), multiple unmanned aerial vehicle (UAV) formations (Wu, Xu, Dai and Lin (2023); Zhang, Wang, Liu, Xu and Nallanathan (2020b)), computation allocation\nDynamic task allocation: There are several typical methods in the study of dynamic task allocation. The first method is to use a multi-robot task allocation strategy (Quinton et al. (2023); Schneider, Sklar and Parsons (2017)), which mimics the process of market trading. When a new task is born, all robots will quote, and ultimately the task is assigned to the robot with the lowest quote. Robots will regularly update their quotes to reassign tasks, but this auction is very time-consuming, so the action distance of robots assigned using this strategy will be set very short (Ullah and Nawi (2023); De Ryck, Pissoort, Holvoet and Demeester (2022); Talebpour and Martinoli (2018)). The second method is to still use the static task allocation method to search for solutions. When the task state changes, fine-tuning methods are used to find new solutions. For example, the genetic algorithm in meta heuristic algorithms can partially cross mutate on the basis of the original solution to obtain a new solution (Patel et al. (2020); Zhang, Mei, Nguyen, Tan and Zhang (2021); Chen, Zhang, Li and Du (2018); Zhou, Wang, Ding, Hu and Shang (2019)). Some methods need to rerun the algorithm to calculate new optimal solutions, such as Integer programming algorithm (Chen, Han, Liu and Du (2023); Li, Li et al. (2017); Su, Wang, Jia, Guo and Ding (2018)) and search algorithm (Sanaj and Prathap (2020); Zhang, Gui, Hou, Chen, Zhu and Tian (2020a); Mitiche, Boughaci and Gini (2019)). Whether it is fine-tuning or searching for the optimal solution again, it is a very time-consuming process, and when the task changes frequently, it is completely impossible to find a suitable solution. The third method is to use clustering algorithms to cluster similar entities into a formation, and entities with similar functions or attributes will be assigned to a formation, greatly reducing the allocation time (Sun, Cong, Dong, Liu, Ding and Yu (2021); Sarkar, Paul and Pal (2018)). This approach can meet the requirements of real-time performance, but once the attributes of the entity are misvalued or malfunctions occur, the completion rate of the task will be greatly reduced (Mitiche et al. (2019)). Currently, Dynamic Multi-Robot Task Allocation for capacitated robots using Satisfiability Modulo Theories solves the problem of handling dynamic streams of tasks with deadlines, ensuring soundness, completeness, and generality for various task specifications Tuck, Chen, Fainekos, Hoxha, Okamoto, Sastry and Seshia (2024). Merlo et al. (2023) introduces an ergonomic role allocation framework for human-robot collaboration, integrating task features and human state measurements to optimize task assignment and reduce the risk of work-related musculoskeletal disorders. Afrin et al. (2023) incorporates edge and cloud computing with robotics to support computation tasks affected by uncertain failures, proposing a multiple deep Q-network dynamic task allocation mechanism to ensure faster resiliency management. Fu et al. (2023b) leverages deep reinforcement learning methods for mobile crowd sensing task allocation, using a double deep Q network based on the dueling architecture to address dynamic task allocation under multiple constraints. These two approaches have shown promising results utilizing deep reinforcement learning in Smart Farm and mobile crowd sensing scenarios. Existing RL methods (Park et al. (2021); Agrawal et al. (2023)) aim to solve the allocation problem by using attention modules to extract task and robot properties for decision-making, similar to our approach. They apply an attention-based approach to derive robot and task embeddings and utilize algorithms to allocate robots, often in warehouse environments. However, these methods model the problem as an MDP and apply reinforcement learning but don't handle scenarios with a variable number of robots. Additionally, they assume fixed robot attributes and do not allow robots to autonomously propose bids. Also, all robots must be allocated, making it a fixed-number allocation problem. In contrast, our approach addresses more complex scenarios. Our entities can include robots, vehicles, suppliers who propose bids based on their attributes, or employees requesting salaries. Our problem definition allows for entity-specific attributes and bids,"}, {"title": "3. Problem Setup", "content": "In the environment, there are many tasks in an episode, and we need to act as managers to assign entities to solve these tasks in order to receive rewards. These entities can be workers who have their own minds and are self-interested or cars that contain a lot of resources. Choosing these entities to complete tasks requires a cost, such as overtime wages for employees and fuel consumption considerations for allocating vehicles. What we need to do is choosing these entities in a reasonable manner and allocating them to complete various tasks. Suppose there are m tasks and n entities in an environment. The number of entities n is much larger than m and these entities will follow the instructions if the manager pays a cost to them. For different states, the cost of entities is dynamic since the consumption of vehicles is different and the demands of the workers are changing.\nMDP. Our setting builds on the standard formulation of the Markov Decision Process (MDP) Sutton, Barto et al. (1998) where the agent observes a state \\(s \\in S\\) and takes an action \\(a \\in A\\). The transition probability function \\(P(s'|s, a)\\) transits current state s to next state s' after taking action a, and the agent receives a reward r according to the reward function\\(r: A \\times S \\rightarrow R\\). The goal of the agent is to learn a policy \\(\\pi(a|s)\\) that maximizes the expected cumulative discounted returns \\(E_{\\tau} [\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)]\\) where \\(\\gamma \\in [0, 1)\\) is the discount factor. In this paper, the state of the manager comprises the positional information of all entities, attributes of carried resources, the positions of tasks, and the total resources required for tasks. Actions are defined as the task allocation scenario. The state transition matrix is based on the current task allocation scenario, where the environment returns the positions and resources of all entities and tasks after each entity's movement. The state of an entity includes its own resources and position, as well as the resource requirements and positions of all tasks. Actions involve moving in any direction, and for worker entities, there is an additional action for bidding. The state transition matrix is determined by the current state and action, returning the entity's resource position and the resource requirements and positions of all tasks for the next time step."}, {"title": "4. Methodology", "content": "The manager faces challenges in allocating entities to tasks. One challenge is the uncertainty in the number of tasks, making it difficult to allocate entities simultaneously. To address this, we propose setting the action dimension as \\(n \\times M\\), where M denotes the maximum number of tasks. However, obtaining the precise value of M in advance is challenging, and in most cases, the total number of tasks is smaller than the maximum number of tasks, which can result in wasted space. Another challenge is determining the task allocation order. Fixed order may result in suboptimal solutions, while simultaneous allocation makes it difficult to decide which task each entity should be assigned to. Additionally, the manager needs to consider the generalization problem, where entity costs and resources can vary. Most RL algorithms are unable to handle the issue of variable numbers of agents. The majority of RL algorithms typically assume a fixed environment, meaning a fixed number of agents. This assumption makes these algorithms ill-suited for handling problems with variable numbers of agents. For instance, when the number of agents dynamically changes, traditional reinforcement learning algorithms may struggle to adapt effectively because they often require a predefined number of agents and state spaces. Therefore, when the number of agents fluctuates, these algorithms may encounter difficulties, leading to degraded performance or failure to converge.\nTo address these challenges, we propose a two-stage approach for entity selection. We first propose a pre-allocation method where each entity is pre-assigned as a candidate for each task based on its attributes and the attributes of the tasks. This method avoids the drawbacks of sequential and random selection by enabling sequential allocation based on the fitness between tasks and entities. We have also demonstrated that this pre-allocation method outperforms sequential allocation methods in finding optimal solutions. Secondly, our proposed attention-based hyper network structure allows for the allocation of different network parameters to different numbers of entities, enabling the neural network to accommodate dynamic entity counts and addressing the generalization issue when the number of entities changes. It overcomes local convergence and entity overlap issues and handles generalization problems. We introduce worker entities as selfish agents to verify the allocation of entities with varying attributes. The overview of our method is depicted in Figure 1, while the architecture is illustrated in Figure 2."}, {"title": "4.1. Pre-assign Module", "content": "The pre-assign module initially allocates entities to suitable tasks. In discrete action space environments, each discrete number typically corresponds to an entity action. However, traditional RL algorithms focus on learning the likelihood of number occurrences in the current state, which limits their ability to effectively utilize information in task assignment problems. To leverage the individual attributes of each entity, we propose a Two-head Attention Module (TAM) based on attention mechanisms (Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser and Polosukhin (2017)). This module guides the manager in making pre-assigned allocations based on the compatibility between tasks and entities. It provides policy and assignment value outputs, and its network structure accommodates a variable number of input entities. We adopt Soft Actor-Critic method (SAC, Haarnoja, Zhou, Abbeel and Levine (2018)) as the base RL algorithm for modeling the manager. Since the action space for task allocation is discrete, we adopted the discrete version of the SAC algorithm (Christodoulou (2019)).\nTAM consists of two heads: the actor head and the critic head. The actor head is responsible for generating the policy for the pre-assign actions, while the critic head calculates the value of these actions. The actor head consists of an entity embedding function \\(f^{h}: R^{S} \\rightarrow R^{d}\\), a task embedding function \\(f^{g}: R^{S} \\rightarrow R^{d}\\). Here, S represents the dimension of the task and entity attributes. The entity embedding can be expressed as \\(h_{i} = f^{h}(w_{i})\\) and the task embedding can also be denoted as \\(g_{i} = f^{g}(T_{i})\\). The probability of each entity pre-assigning to each task is computed by an attention module, i.e., \\(\\pi(T_{i}|w_{i}) = SoftMax(h_{i}^{T}g_{i}/\\sqrt{d})\\). This method can solve the problem of a variable number of tasks with zero-shot training because when an unseen task \\(T_{i'}\\) occurs, we can compute the embedding \\(g_{i'} = f^{g}(T_{i'})\\) and then the probability of entity \\(w_{i}\\) assigning task \\(T_{i'}\\) is \\(\\pi(T_{i'}|w_{i}) = (h_{i}^{T} g_{i'}/\\sqrt{d})\\). The pre-assign allocation of entity \\(w_{i}\\) is \\(c_{i}\\), which is sampled from the \\(\\pi(T_{i}|w_{i})\\). The total pre-assign allocation is \\(c = (c_{1}, c_{2}, ...c_{n})\\) where \\(c_{i} = k\\) denotes that \\(w_{i}\\) is pre-assigned to task \\(T_{k}\\).\nDue to the discrete nature of pre-assigned actions, a table can be created to record each action's value. However, the table size (\\(2^{n}\\)) becomes impractical when n is large, making it challenging to accurately approximate values with limited datasets. To address this, we adopt an approach inspired by the Q MIXing network (QMIX) Rashid, Samvelyan, De Witt, Farquhar, Foerster and Whiteson (2020), which employs a mixing network to compute the total Q value for multi-agent systems. In our approach, we calculate the value selected for each entity and consider the overall assigned value as a nonlinear aggregation of the individual entity values. In QMIX, each agent has a Q network that calculates the value of taking a particular action based on the current observations, resulting in a total of as many Q values as there are agents. Additionally, QMIX employs a hyper network that generates non-negative parameters for the neural network, combining each Q value to compute the overall Q value. For algorithm fine-tuning, we modify the attention mechanism by incorporating a critic head, similar in architecture to the actor head, to output action values. The critic component includes entity value embedding (\\(f^{o}\\)) and task value embedding (\\(f^{q}\\)) functions denoted as \\(o_{i} = f^{o}(w_{i})\\) and \\(q_{i} = f^{q}(T_{i})\\), respectively. The pre-assign value of \\(w_{i}\\) to \\(T_{i}\\) is computed using the dot product, \\(Q(w_{i}, .) = o_{i}^{T}q\\). The architecture of TAM is depicted in Figure 3.\nWhen a pre-assigned action of an entity is sampled, we obtain a value from the critic's component. In the case of having n entities, we obtain n values corresponding to the pre-assigned actions. However, since we interact with the environment throughout the entire pre-assign action, we only receive a single total reward from the environment, which does not align with the output of our critic. To address this disparity, we introduce the Attention MIXing (AMIX) module. The AMIX module utilizes a self-attention-based hypernetwork to generate parameters for its layers. Unlike QMIX, which provides network parameters for a fixed number of agents, AMIX's hyperparameter network, called the Self-attention-based Hyper Network (SHN), utilizes an attention model. This allows it to dynamically adjust the number of parameters based on the input values, effectively accommodating the variable number of entities in the problem. For a visual representation of this concept, please refer to Figure 4. Using the aforementioned critic structure, we can calculate the value estimates for task allocation. According to the SAC algorithm, the actor loss can be expressed as:\n\\(L_{actor}(\\theta) = \\alpha log \\pi_{\\theta}(c|w) - Q_{\\varphi}(w, c)\\)\nAfter the allocation is complete, the critic is updated based on the reward values obtained from the interaction between the agent and the environment. The loss can be expressed as:\n\\(L_{critic}(\\varphi) = \\frac{1}{2} [r(w, c) + \\gamma \\underset{a'}{max} Q_{\\varphi}(w', a') - Q_{\\varphi}(w, c)]^2\\)\nThrough the aforementioned loss, we can continuously update the critic network to estimate the reward values for the current allocation executed by all entities. This allows the actor network to output optimal task allocations."}, {"title": "4.2. Select Module", "content": "When the entities are already pre-assigned to tasks by using the pre-assign module, there are many entities for each task to select. The manager needs to select the proper number of entities with different resources to complete the task. In order to have better generalization, the selection module must be able to assign entities and tasks that have not been seen before. To solve these two problems, referring to the point-net (Vinyals, Fortunato and Jaitly (2015)) network framework, we establish a seq2seq-like network structure as our select module. There are several reasons why the seq2seq-like structure can solve our problems. 1) The seq2seq structure is widely used for machine translation and is able to generate a variable number of outputs according to the input information. The problem of a variable number of entities can be solved. 2) In the seq2seq structure, the information of the previous output is taken into account in the context, which helps to achieve better output results in the future. This structure can also help the manager choose entities better by considering the context of previous selections. 3) The point-net attention mechanism can solve the out-of-vocabulary problem when the input and output are the same. The input and output are both entities in the allocation problem.\nThe architecture of the select module is shown in Figure 5. The input of the encoder is the entities \\(w_{T_{i}}\\) pre-assigned to the task \\(T_{i}\\), and the input of the decoder is \\(T_{i}\\). Note that there is no sequential relationship between input entities like text, so unlike traditional seq2seq structure, we did not use the commonly used RNN and attention mechanisms to encode the input. We simply use fully connected layer as the encoder, and the output for entity \\(w_{i}\\) is \\(d_{i}\\). The embedding of the task \\(T_{i}\\) is \\(e_{i}\\), and the output of the t-th entity \\(u_{t}\\) is selected by sampling from a distribution which is \\(\\pi(.|e) = SoftMax(v \\tanh(W_{1}e + W_{2}d'))\\). The previous output will impact the context, so we change the task by \\(T^{t+1} = T^{t} + f^{a}(u)\\). When the task resources needed for \\(T\\) are all zero, the process of selection will end."}, {"title": "4.3. Demand Module", "content": "The worker entities have their own minds, which means they will adjust their demands in different situations. We model workers as entities that can propose their own demands based on the attributes of the task. They can update their own minds by using RL algorithms. Since each entity only considers maximizing their own reward and does not consider the global reward, and multi-agent RL algorithms focus on the optimal overall reward, which is not consistent with our scenario, we use singe-agent RL algorithm to train each entity. The demand of each agent is a continuous number, so we use the DDPG (Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa, Silver and Wierstra (2015)) algorithm to train these entities. The observation in DDPG is defined as the current attributes and current position of the entity, while the action is defined as the bid value for the demand. When the current entity is selected, the reward obtained is the bid value. When not selected, the reward value is 0. Therefore, worker entities need to provide reasonable bids based on their current attribute values to be selected and obtain high returns."}, {"title": "5. Experiment", "content": "In this section, we try to answer these questions: (1) Is the pre-assign method more effective than the sequential task allocation method in avoiding finding local optimal solutions? (2) Can the algorithm solve the allocation problem when the cost of entities is dynamically changed based on the current task? (3) Can the algorithm provide a reasonable allocation for such a variable number of entities as their number increases and decreases? (4) Does our proposed method outperform heuristic algorithms? For all experimental environments, our two-stage method uses the same hyperparameters, with no parameters specifically set for each environment. For all network architectures, we use the ReLU function as the activation function and set the number of hidden units to 64, with each network consisting of a 3-layer MLP structure. More details on the parameter configurations of our method can be found in Table 3."}, {"title": "5.1. Retain the Almighty", "content": "This small experiment will answer the question (1) and demonstrate why using sequential selection methods can fall into local optimization and prove that using the pre-assign method is more effective.\nIn the Retain the Almighty environment, there are N tasks and n entities in this environment, and each task \\(T_{i}\\) in this environment corresponds to a subset of the best entities \\(b_{i}\\), and the task is completed if and only if a best entity \\(w \\in b_{i}\\) participates in this task. The last task \\(T_{N}\\) is a difficult but rewarding task. The best entity of \\(b_{N}\\) is \\(w_{0}\\), which is an almighty agent that is able to finish every task, and the cost for it is the same as others. The best allocation is to arrange \\(w \\in b_{i}\\) to do \\(T_{i}\\) and the task cannot choose the entities if the previous task has chosen them, which means the algorithm must retain the almighty to the last task to find a good solution. The result is shown in Figure 6.\nIt can be seen that the entity learned using the sequential method is getting trapped in the local optimum. The perfect entity \\(w\\) is selected by the previous task and fails to complete the last task, resulting in a low total reward. When the order of the most difficult tasks is randomly assigned to the top, we can assign the Almighty to this task and obtain high profits; when the most difficult task is randomly assigned later, the rewards will sharply decrease. According to the training curve, the entity has not learned how to assign entities to appropriate tasks, and the final return of this entity is related to the random order of tasks instead of how well it learns to allocate entities to tasks. However, by using the pre-assign method, the manager is able to find the best solution because the task can be completed once the best entity is pre-assigned to the last task. The manager is able to allocate \\(w_{0}\\) to \\(T_{N}\\) to get a large reward and then find the optimal solution.\nIn fact, in this environment, we can suppose that without prior knowledge, all entities have the same probability of being selected when the algorithm does not start training. The probability upper bound of \\(w_{0} \\in T_{N}\\) by using the sequential method is \\(\\frac{n+N-2}{(n-1)^{N-1}}\\). The probability upper bound decreases exponentially with the number of tasks. However, the probability of \\(w_{0} \\in T_{N}\\) by using the pre-assign method is \\(\\frac{1}{N}\\). The rate of probability decline is far less than that of the sequential method. See Appendix A for complete proof."}, {"title": "5.2. Electric Power Transportation", "content": "In this experiment, we will answer question (2). There are many towers in the environment that are connected by wires. Different towers have different wire materials, which means different transmission costs between towers. At each time step, there may be several towers in the city corresponding to the power peak, so other towers are needed for power transmission. As a manager, we need to select the proper electric towers to carry out power transmission for the towers with peak power consumption so as to complete the task of ensuring the smooth use of electricity in the city. The towers awaiting transmission represent tasks, characterized by their locations and additional power requirements. Other towers serve as entities, characterized by their additional available power and locations. We employ our proposed two-stage method to initially allocate tasks to each tower awaiting transmission and then make selections based on the specific additional power and attributes of each tower. Once selected to participate in transportation, the cost is calculated as the distance between the two stations multiplied by the predetermined cost of transmission per meter of wire between the two towers. The transmission cost per meter of wire is determined by the material between the two towers, which is predefined in the environment. For each target tower, if sufficient power transmission is successfully obtained, the reward is the task completion reward minus the power transmission cost. Therefore, while ensuring sufficient power, reducing transmission costs is also considered. In this environment, there are a total of 20 electric towers. Each interaction with the environment causes changes in the power values required by the towers. When the power exceeds their individual limits, support from other towers is needed.\nWe use our two-stage approach to train the manager. To test the effect of our method, we denote w/o Pre as the architecture without the pre-assign module, w/o TAM as the architecture without the attention module, which means the actor and critic are linear layers. The w/o AMIX is denoted as the normal global critic to calculate the expected return of this pre-assign action. When we use the AMIX structure, we calculate the individual value assigned to each task for each entity, and then use AMIX to calculate the overall value value; When the AMIX structure is not applicable, we will use an overall critical network to input the attributes of all entities and tasks and directly estimate the overall value value. The training curve and an example of our allocation method are shown in Figure 7. From the figure, it can be seen that our proposed method significantly improves the effectiveness of power allocation and increases the reward value. Additionally, the effectiveness of each proposed module is validated through the curve chart. These subplots show two towers at peak electricity usage, requiring other tower entities to transmit power. Subplot (d) illustrates the value assigned to other tower entities when pre-assigned to a central peak-usage tower. Subplot (e) shows the value for a tower located in the upper left, also at peak usage, receiving pre-assign support from other towers. After training, the pre-assign schemes with higher values are distributed around these two target towers,while towers farther away have"}, {"title": "5.3. Resource-Based Foraging (RBF)", "content": "This experiment will answer questions (2) and (3). This environment is a task assignment environment based on the benchmark environment Level-Based Foraging (LBF, Papoudakis, Christianos, Sch\u00e4fer and Albrecht (2020)) for multi-agent fully collaborative tasks and has undergone some changes to adapt to the settings in this article. The entities possess numerous resources in the RBF environment, but just one resource in the LBF environment, which represents the entities' level. For simplicity, all tasks are still represented by apples, and the differences in task resources are reflected in the size of the apples. The number of entities in RBF is very large, and selecting each entity requires a cost. Apples refresh randomly on the map over time, and the manager needs to control the entity to finish the task, i.e., pick the apples to get rewards. If the task is not completed after a certain period of time, its rewards and requirements will be reduced. Furthermore, if an entity promises to complete a task, it cannot be arranged for other tasks to be completed. In this environment, there are a total of 100 selectable entities, and every 5 time steps, 5-10 apples are randomly generated at any location on the map. The manager needs to select entities from the pool of 100 based on their attributes and positions, determining whether to choose each entity and directing them to solve specific tasks. This can be considered a large-scale task assignment problem, as there are 100 entities to allocate and a considerable number of tasks, requiring sophisticated algorithm design to address the challenge of allocating multiple tasks to a large number of entities.\nWe first consider that these entities are item entities whose cost is determined by the size of resources and their distance from tasks. We simplify different tasks as harvesting different apples, each with its own attributes representing the task requirements. A task is considered completed, or an apple successfully harvested, only when the total attributes of the entity at that location exceed those of the apple. Since the entity attributes are manually specified, we train only the manager in this scenario. The objective is to enable the manager to appropriately select entities based on their attributes, corresponding locations, apple positions, and the total entity attributes required for apple harvesting, and then allocate tasks accordingly. The training curve is shown in Figure 8. To verify the generalization ability of algorithms, we first replaced the training entities with another batch of entities we had never seen before. Then we change the attributes of tasks in the environment to see the result. In the task generalization experiment, we randomly generate new apples with different attributes on the map, distributed across various locations, and have the manager utilize the same set of entities to complete them. This task can be likened to a scenario where a company employs the same group of employees to complete historical tasks and verifies whether they can allocate the employees reasonably based on the requirements of new tasks. The results of the training process, the zero-shot generalization, the few-shot generalization, and the result of training from scratch are shown in Figure 8. We can see that whether it is a change in the number and attributes of entities or a change in the attributes of tasks, the experimental results of zero-shot generalization of the model are almost consistent with the effect of retraining the model about 700 epochs, and the few-shot training performance of the model, which trains 200 epochs in this environment, is better than retraining it 1000 times. This demonstrates that our model does not remember how it should behave in the current environment but rather learns to assign strategies based on entity and task attributes.\nWe also consider the situation where the entities are worker entities, and they will demand their costs for being selected to do a task. They will dynamically adjust their quotes based on the attributes of the task, their own attributes, quotes, final returns, and the selected situation. We use the DDPG algorithm to simulate the process of entity learning to propose demand. We can see from the Figure 8 that as the manager uses our algorithms to learn how to distribute, both total and manager benefits increase. And workers gradually increase their quotes and benefits, so the rate of manager revenue increase begins to decrease. As the total income begins to reach the upper limit, the manager's income at this time reaches the upper limit, and workers no longer raise their demands, creating a dynamic balance. In the training process, workers are constantly learning and modifying the quotation, at which time the algorithm can select"}, {"title": "5.4. Material Transportation", "content": "We use a Material Transportation environment and the problem to be solved is the Dynamic Vehicle Routing Problem (DVRP) problem. There are trucks carrying resources at various parking points on the map, and we need to allocate these trucks reasonably based on the amount of resources required by each city. We have a total of 50 entities equipped with various materials. At each time step, the environment generates material demands at random positions. The manager needs to dynamically allocate tasks to the agents based on the entities' positions and remaining material attributes. This environment is different from the previous environments since once an entity completes a task in this environment, the resources that it carries will be cleared, meaning that it cannot be selected to complete other tasks anymore. The results of this experiment are shown in Figure 9. From the comparison, it is evident that our algorithm does not achieve the highest performance when compared to the results obtained without the AMIX or the TAM network structure. However, as mentioned earlier, networks lacking these modules are unable to handle variable number entities allocation. Therefore, while our algorithm may not excel in this specific task, it possesses the crucial ability to generalize and adapt to different scenarios."}, {"title": "5.5. Results", "content": "Here, we compare our proposed reinforcement learning algorithm with three mainstream heuristic algorithms, Genetic Algorithm (Holland (1992), GA), Particle Swarm Optimization (Kennedy and Eberhart (1995), PSO), and Symbiotic Organism Search (Cheng and Prayogo (2014), SOS) to address question (4) and demonstrate the effectiveness of our method in dynamic task allocation. As our tasks are dynamically provided, we periodically rerun these heuristic algorithms to adapt to changing task allocations. These algorithms, originally designed for static planning problems, are now triggered for reruns every 10 interactions with the environment, considering the current set of agents and tasks. For these heuristic algorithms, the encoding length is determined by the product of the MDP length and the action space dimension. During interaction with the environment, actions are chosen based on this encoding. In the Genetic Algorithm, the mutation rate is set to 0.05, meaning each gene in the encoding has a 5% chance of undergoing a random change. The crossover rate is set to 0.5, with each crossover operation randomly selecting a crossover point and exchanging subsequent genes between parents. The population size is fixed at 100, and the algorithm runs for a total of 1000 generations, which is the same as the RL method. Since each particle represents an allocation, values are constrained between 0 and the maximum number N + 2. When the integer part equals k (k \u2265 1), it indicates that the entity is assigned to task k; when the integer part is 0 or larger than N, it indicates that the entity is not assigned to any task. For the PSO algorithm, we initialize 100 particles, set the inertia weight to 0.5, and both \\(c_{1}\\) and \\(c_{2}\\) to 1.5. The total number of iterations is set to 1000. In the SOS algorithm, the number of organisms is also set to 100. The search space spans from 0 to the maximum number plus one, similar to the GA setting. The coefficients during the mutualism and commensalism phases are set as random numbers between -1 and 1."}, {"title": "5.6. Analysis", "content": "From the tables and experimental curves, it is clear that our proposed method outperforms in addressing dynamic task allocation problems. From the results presented in Table 2, it is evident that our proposed method outperforms GA, PSO, and SOS algorithms across all three experimental environments, demonstrating its effectiveness in solving dynamic allocation problems. As shown in Figure 10, our training curve starts off lower than the heuristic algorithms. However, in the mid-training phase, our method surpasses the heuristics and maintains the highest performance until the end. This may be due to the neural network's need to extract features based on the current attributes of entities and tasks, which requires observing and understanding each dimension and position. This process demands a substantial amount of data to update feature extraction parameters effectively, allowing the model to learn an optimal allocation strategy based on the state of entities and tasks. In the later stages of training, when data volume increases, the reinforcement learning method's learned strategy can provide better real-time task allocation according to the scenario, resulting in a higher performance ceiling. This success can also be attributed to our modeling of the problem as an MDP, which enables real-time capture of changes in environmental tasks and entity attributes. In contrast, heuristic algorithms address static task allocation problems in each iteration and lack the adaptability of reinforcement learning algorithms in handling such dynamic scenarios. Additionally, compared to heuristic algorithms, our method can address generalization issues. When new entities or tasks appear, our algorithm can directly allocate tasks, whereas heuristic algorithms fail to handle generalization due to mismatched dimensions caused by changes in the number of entities or tasks. Furthermore, heuristic algorithms do not take into account the attributes of entities or tasks, so they cannot dynamically adjust their strategies when these attributes change.\nCompared to sequential allocation or random sequential allocation, our proposed pre-allocation method achieves better results. In various environments, our method shows significant improvements, indicating that our pre-allocation approach can find better task allocation solutions. In contrast, task allocation based on natural or random order is more likely to get stuck in local optima, resulting in suboptimal allocations.\nFrom Table 2, it's noteworthy that in some experiments, the absence of the AMIX module leads to better results in zero-shot scenarios. This can be attributed to the SHN network's utilization of an attention structure that inherently involves a large number of parameters. The SHN network's output serves as weight parameters for the TAM network. While this approach enhances fine-tuning capabilities across various tasks and scenarios, captures variations in the number of entities, and achieves superior generalization, it also increases the total number of parameters and training complexity compared to directly learning the parameters of the AMIX neural network. Consequently, zero-shot performance may slightly degrade.\nIn the RBF environment, employing the AMIX module results in superior performance. However, in other environments, its use leads to slightly inferior results. This variation can be attributed to the specific settings of each environment. For instance, EPT involves fewer entities and simpler tasks. In the generalization scenario, we did not modify the number of entities or total tasks but only altered the resource attributes carried by the entities and the attributes of the tasks. In the RBF environment, we employed 100 entities, whereas in zero-shot and few-shot scenarios, we reduced the number of entities and tasks by 50%. Since the attention module can better capture the impact of changes in the number of entities on task allocation, our method achieves optimal results in training and various generalization tasks within the RBF environment. It significantly outperforms the AMIX network without the attention module. Conversely, in EPT and MT scenarios, training without the attention module leads to faster convergence. Due to minimal changes in the number of entities, omitting the AMIX module yields better performance. Our proposed method performs better in task environments with large-scale and numerous entities. When there are many entities, the AMIX module can better capture the connections between entities. However, for smaller-scale tasks, the reduced number of entities makes it challenging to capture the relationships between entities and different entity counts. As a result, the proposed module does not show significant improvement during training. Nonetheless, without the AMIX module, the critic cannot accurately estimate task allocation values for a variable number of entities and tasks. It relies solely on the actor module with the attention network to output policies but cannot use the critic network to update those policies. Thus, it is not possible to fine-tune parameters for new tasks.\nFrom the perspective of generalization, our proposed method demonstrates excellent generalization without additional training. In Figure 8, directly applying the trained model to new environments yields results comparable to those after approximately 700 steps of retraining. Furthermore, after fine-tuning for 100 steps, it performs better than retraining for 1000 steps, proving the zero-shot and few-shot capabilities of our method. Additionally, compared to sequential and random selection methods, our pre-allocation method also achieves better results in generalization scenarios, further validating the effectiveness and rationality of our approach."}, {"title": "6. Conclusion", "content": "In our work, we propose a two-stage approach to solve the task allocation problem. Our approach starts by pre-assigning entities to tasks that they are good at, based on the similarity of each entity and task. We then select from the candidate entities in each task using a sequence model similar to point-net. The proposed TAM and AMIX network architectures can accommodate the changing number of tasks and entities and have the potential to achieve zero-shot and few-shot generalization to new tasks and unseen entities scenarios. Through a variety of experiments, we verify the effectiveness of our proposed two-stage task allocation approach and the validity of our proposed structure. We compared our algorithm with heuristic algorithms in multiple environments and found that our algorithm achieves better results, demonstrating the effectiveness of our approach. Additionally, we conducted generalization experiments by modifying the number of entities and tasks, as well as the associated resources and attributes. This validation confirms that our method can successfully address these challenges and achieve good generalization performance."}, {"title": "7. Limitation", "content": "A limitation of our method pertains to the prerequisite knowledge of task and entity attributes for efficient task allocation, which is highly justified in resource scheduling and delivery scenarios. However, in certain contexts, it becomes imperative to make reasonably estimated attributions for tasks and entities. For instance, within an enterprise, quantifying project complexity and required competencies, while conducting quantitative evaluations of employees, becomes indispensable to leverage our algorithm for task allocation. Encouragingly, this limitation can be overcome as certain task characteristics and entity observations are readily obtainable. By employing appropriate methods to extract task and entity features and employing them as attributes, these challenges can be effectively addressed."}, {"title": "A. Proof", "content": "The following is a proof of the probability upper bound of the sequential selection method. We define \\(w \\in b_{a_{i}}\\) as the set of entities which only belong to \\(b_{i}\\), i.e., \\(w \\in b_{a_{i}} \\Leftrightarrow \\forall i, j \\neq i, w \\in b_{i}, w \\notin b_{j}\\). We denote \\(r_{i} = b_{i} \\backslash b_{a_{i}}\\). The number of entities in \\(b_{a_{i}}\\) and \\(r_{i}\\) is defined as \\(n_{a_{i}}\\) and \\(n_{r_{i}}\\). The summary of \\(n_{a_{i}}\\) is less than n \u2212 1 and \\(n_{r_{i}}\\) is bigger than 1 since \\(\\forall i, w_{i} \\in r_{i}\\). We record the number of entities in \\(b_{a_{i}}\\) who were mistakenly selected by \\(T_{i}\\) as \\(m_{i}^{2}\\). The probability of \\(w_{0} \\in T_{N}\\) is\n\\[\\frac{n_{a_{1}}}{n} \\times \\frac{n_{a_{2}}-m_{2}}{n_{a_{1}} + r_{a_{1}}} \\times \\frac{n_{a_{3}}-m_{3}-m_{2}^{'}}{n_{a_{2}} + r_{a_{2}}} ...\\frac{n_{a_{N-1}}}{n_{a_{N-1}}} \\\\ < \\frac{n_{a_{1}}}{n_{a_{1}} + r_{a_{1}}} \\frac{n_{a_{2}}}{n_{a_{2}} + r_{a_{2}}} \\frac{n_{a_{3}}}{n_{a_{3}} + r_{a_{3}}} ...\\frac{n_{a_{N-1}}}{n_{a_{N-1}} + r_{a_{N-1}}} \\\\ < \\frac{n_{a_{1}}}{n_{a_{1}} + 1} \\frac{n_{a_{2}}}{n_{a_{2}} + 1} \\frac{n_{a_{3}}}{n_{a_{3}} + 1} ...\\frac{n_{a_{N-1}}}{n_{a_{N-1}} + 1} \\\\ = \\frac{1}{1 + \\frac{1}{n_{a_{1}}}} \\frac{1}{1 + \\frac{1}{n_{a_{2}}}} \\frac{1}{1 + \\frac{1}{n_{a_{3}}}} ...\\frac{1}{1 + \\frac{1}{n_{a_{N-1}}}} \\\\ = e^{-[ln(1 + \\frac{1}{n_{a_{1}}}) + ln(1 + \\frac{1}{n_{a_{2}}}) + ...ln(1 + \\frac{1}{n_{a_{N-1}}})]} \\\\]\n\\[( \\frac{a + c}{b + c} > \\frac{a}{b} (\\because \\frac{a}{b} > 0 < a < b, c \\geq 0))\\]\nWe consider the function \\(f(x) = ln(1 + \\frac{1}{x})\\). The second derivative of the function is \\(\\frac{2x + 1}{x^{2}(1+x)^{2}}\\) which is positive when \\(x > 0\\). So the function \\(f(x) = ln(1 + \\frac{1}{x})\\) is is a convex function. By using the Jensen inequality of the convex function, we have\n\\[\\frac{ln(1 + \\frac{1}{n_{a_{1}}}) + ln(1 + \\frac{1}{n_{a_{2}}}) + ...ln(1 + \\frac{1}{n_{a_{N-1}}})}{N-1} > ln(1+ \\frac{N-1}{\\sum_{k=1}^{N-1} n_{a_{i}}}).\\]\nUsing the above inequality, we have\n\\[e^{-[ln(1 + \\frac{1}{n_{a_{1}}}) + ln(1 + \\frac{1}{n_{a_{2}}}) + ...ln(1 + \\frac{1}{n_{a_{N-1}}})]} < e^{-N-1ln(1 + \\frac{N-1}{\\sum_{i=1}^{k} a_{i}})} \\\\]\n\\[= e^{-(N-1)ln(1 + \\frac{N-1}{\\sum_{i=1}^{k} a_{i}})} \\]\n\\[< e^{-(N-1)ln(1+\\frac{N-1}{n-(1+e)})} \\]\n\\[< e^{-(\\frac{N-1}{n}\\frac{n}{\\Sigma a_{i}})} \\]\n\\[< e^{-\\frac{(N-1)}{n-1+k}} \\]\n\\[= (\\frac{N+n-2}{N-1}).\\]\nThe probability of \\(w_{0} \\in T_{n}\\) using pre-assign method is \\(\\frac{1}{N}\\) because we only need to make sure the pre-assign is correct."}, {"title": "B. Algorithms", "content": "In this section, we give the pseudocode of the algorithms. Algorithm 1 is the main algorithm of our approach. It shows how we assign tasks based on given tasks and entities. Algorithm 2 corresponds to the pre-assign module mentioned in the main text, which is used to pre-assign entities as the candidates for all tasks. Algorithm 3 is the select module used to select and allocate entities for each task. We used Python and PyTorch framework to implement pseudo code."}, {"title": "C. Experiment details", "content": "Our experiments were performed by using the following hardware and software:\n\u2022 GPUs: NVIDIA GeForce RTX 3090\n\u2022 Python 3.10.8\n\u2022 Numpy 1.23.4\n\u2022 Pytorch 1.13.0"}]}