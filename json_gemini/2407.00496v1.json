{"title": "A Two-stage Reinforcement Learning-based Approach for Multi-entity Task Allocation", "authors": ["Aicheng Gong", "Kai Yang", "Jiafei Lyu", "Xiu Li"], "abstract": "Task allocation is a key combinatorial optimization problem, crucial for modern applications such as multi-robot cooperation and resource scheduling. Decision makers must allocate entities to tasks reasonably across different scenarios. However, traditional methods assume static attributes and numbers of tasks and entities, often relying on dynamic programming and heuristic algorithms for solutions. In reality, task allocation resembles Markov decision processes, with dynamically changing task and entity attributes. Thus, algorithms must dynamically allocate tasks based on their states. To address this issue, we propose a two-stage task allocation algorithm based on similarity, utilizing reinforcement learning to learn allocation strategies. The proposed pre-assign strategy allows entities to preselect appropriate tasks, effectively avoiding local optima and thereby better finding the optimal allocation. We also introduce an attention mechanism and a hyperparameter network structure to adapt to the changing number and attributes of entities and tasks, enabling our network structure to generalize to new tasks. Experimental results across multiple environments demonstrate that our algorithm effectively addresses the challenges of dynamic task allocation in practical applications. Compared to heuristic algorithms like genetic algorithms, our reinforcement learning approach better solves dynamic allocation problems and achieves zero-shot generalization to new tasks with good performance. The code is available at https://github.com/yk7333/TaskAllocation.", "sections": [{"title": "1. Introduction", "content": "Task allocation is a critical combinatorial optimization problem (Yan, Jouandeau and Ali-Ch\u00e9rif (2012)). It plays a vital role in modern applications such as multi-robot collaboration, resource scheduling, and more. In warehouse logistics, mobile robot teams allocate tasks and transport goods to different locations (Tang, Wang, Xue, Yang and Cao (2021)). In industrial and manufacturing fields, robotic arms are assigned to various processing tasks (Johannsmeier and Haddadin (2016)). On-demand carpooling and delivery services require the dispatch of agents to meet customer needs (Hyland and Mahmassani (2018)). As decision-makers, we must allocate tasks to entities based on their location, current attributes, and task requirements to ensure timely task completion.\nVarious approaches have been proposed to solve the task allocation problem (Wang, Zhu, Pang and Zhang (2023); Quinton, Grand and Lesire (2023); Burkard, Dell'Amico and Martello (2012); Parasuraman, Mouloua and Molloy (1996); Alighanbari and How (2005); Merlo, Lamon, Fusaro, Lorenzini, Carfi, Mastrogiovanni and Ajoudani (2023)). The common approach treats it as a discrete optimization problem, assigning entities to tasks. Accurate solution algorithms like the Hungarian algorithm (Msala, Hamed, Talea and Aboulfatah (2023); Samiei and Sun (2023, 2020); Munkres (1957)), branch and bound (Martin, Frejo, Garc\u00eda and Camacho (2021); Singh (2021); Lawler and Wood (1966)), network flow algorithms (Javanmardi, Shojafar, Mohammadi, Persico and Pescap\u00e8 (2023); Jamil, Ijaz, Shojafar and Munir (2023); De Weerdt, Zhang and Klos (2007)), fuzzy logic algorithm (Ali and Sridevi (2024); Sharma, Mishra, Singh, Govil, Singh and Singh (2023); Ali, Rout, Parimi and Das (2021); Jamil, Ijaz, Shojafar, Munir and Buyya (2022)) and dynamic programming (Choudhury, Gupta, Kochenderfer, Sadigh and Bohg (2022); Alkaabneh, Diabat and Gao (2021); Bellman (2010)) are used, as well as heuristic algorithms such as the genetic algorithm (DENG, HUANG, TAN, FU, ZHANG and LAM (2023); Patel, Rudnick-Cohen, Azarm, Otte, Xu and Herrmann (2020); Ye, Chen, Tian and Jiang (2020); Page, Keane and Naughton (2010); Forrest (1996); Fu, Sun, Wang and Li (2023a)), particle swarm optimization (Kalimuthu and Thomas (2024); Geng, Chen, Nguyen and Gong (2021); Qingtian (2021)), symbiotic organisms search(Truong, Nallagownden, Elamvazuthi and Vo (2020); Gharehchopogh, Shayanfar and Gholizadeh (2020); Abdullahi, Ngadi, Dishing and Abdulhamid (2023)). simulated annealing (Barbosa, Kniess and Parpinelli (2023); Wang, Shi and Liu (2022); Barboza and Kniess (2024); Bertsimas and Tsitsiklis (1993)) are used to solve this problem. There are also methods using game theory to allocate robots to finish tasks Martin, Muros, Maestre and Camacho (2023); Sun, Sun, Liu, Wang and Cao (2023). However, while these methods have proven effective in addressing the task allocation problem under static conditions, they often assume static task and entity attributes. In reality, task assignments frequently encounter dynamic states and fluctuations in task estimation or entity numbers over time. As a result, the applicability of the aforementioned methods may diminish, necessitating the exploration of new approaches for dynamic task allocation. This dynamic nature underscores the importance of developing methodologies capable of adapting to evolving conditions and uncertainties in real-time task assignment scenarios.\nTo address the dynamic allocation problem in real-world scenarios, our aim is to develop more efficient and practical algorithms compared to prior methods. After each step, a series of tasks can emerge, requiring us to allocate entities effectively to achieve task goals. However, these resources come with associated costs, such as power consumption in transmission and additional expenses incurred by companies when assigning employees to extra tasks. Therefore, we must carefully select the appropriate resources from a large pool to minimize costs and successfully complete tasks. Treating this problem as a Markov Decision Process (MDP), we leverage Reinforcement Learning (RL) algorithms to dynamically adjust allocation actions based on the current state, without incurring additional computational costs. Several RL-based methods have been developed for task allocation in various domains. For instance, Afrin, Jin, Rahman, Li, Tian and Li (2023) integrates edge and cloud computing with robotics to address computation tasks impacted by uncertain failures, utilizing a multiple deep Q-network mechanism for dynamic task allocation and ensuring quicker resiliency management. Similarly, Fu, Shen and Tang (2023b) applies deep reinforcement learning to mobile crowd sensing, using a double deep Q network based on the dueling architecture to manage dynamic task allocation under multiple constraints, surpassing traditional heuristic methods in platform profit and task completion rate. These approaches show promising results in Smart Farm and mobile crowd sensing scenarios. Additionally, methods such as Park, Kang and Choi (2021) and Agrawal, Bedi and Manocha (2023) use attention modules to extract task and robot properties, facilitating efficient allocation decisions in environments like warehouses. Other notable works include task allocation for workers in spatio-temporal crowdsourcing (Zhao, Dong, Wang and Pan (2023)) and mobile crowdsensing (Xu and Song (2023)).\nWhile RL algorithms have been widely used, there is a lack of research on allocating varying numbers of agents and handling scenarios with emerging tasks. Simply adopting traditional RL algorithms is inadequate for handling dynamically changing numbers and attributes of agents in practical applications. This is because RL predefines the number of agents and establishes a fixed network structure to calculate the value of each agent's actions in each state. When the number of agents suddenly increases, the neural network cannot allocate tasks for the additional agents. The methods mentioned above, which utilize reinforcement learning, only consider problems with a fixed number of agents and aim to improve the coordination of existing robots to accomplish tasks more effectively. Additionally, these robots have fixed characteristics, and there is no need to consider issues such as which agents' resource attributes are more suitable for allocation or which agents remain stationary on standby. Hence, to address this challenge, this paper introduces a two-stage task allocation algorithm that leverages the similarity between agent and task attributes, rendering RL algorithms applicable to the task allocation problem. Compared with heuristic algorithms, our proposed reinforcement learning method can achieve better results in dynamic task allocation and easily solve the problem of variable number of entities. When the attributes or number of tasks or entities change, our method can achieve good generalization results without training, and can better handle a series of allocation problems. Our main contributions are fourfold:\n1.  We introduce a pre-assign method for efficient task allocation. Firstly, the agent is pre-assigned to a task as a candidate agent for task selection, and then the agent is selected based on the degree of correlation between the task and the agent.\n2.  We incorporate Actor-Critic structure in combinational optimization problems by proposing a two-head attention mechanism module. It calculates the values of Actor and Critic simultaneously based on the similarity between agent and task attributes, enabling task allocation for a variable number of agents and zero-shot generalization of new tasks.\n3.  An attention-based hyperparameter network structure is proposed to estimate the overall value of critical outputs for different numbers of agents, facilitating fine-tuning of the variable number of agents in new scenarios.\n4.  We propose a seq2seq-like structure, similar to PointNet, to select pre-assigned agents. It selects an appropriate number of agents with suitable attributes for each task."}, {"title": "3. Problem Setup", "content": "In the environment, there are many tasks in an episode, and we need to act as managers to assign entities to solve these tasks in order to receive rewards. These entities can be workers who have their own minds and are self-interested or cars that contain a lot of resources. Choosing these entities to complete tasks requires a cost, such as overtime wages for employees and fuel consumption considerations for allocating vehicles. What we need to do is choosing these entities in a reasonable manner and allocating them to complete various tasks. Suppose there are $m$ tasks and $n$ entities in an environment. The number of entities $n$ is much larger than $m$ and these entities will follow the instructions if the manager pays a cost to them. For different states, the cost of entities is dynamic since the consumption of vehicles is different and the demands of the workers are changing.\nMDP. Our setting builds on the standard formulation of the Markov Decision Process (MDP) Sutton, Barto et al. (1998) where the agent observes a state $s \\in S$ and takes an action $a \\in A$. The transition probability function $P(s'|s, a)$ transits current state $s$ to next state $s'$ after taking action $a$, and the agent receives a reward $r$ according to the reward function $r: A \\times S \\rightarrow R$. The goal of the agent is to learn a policy $\\pi(a|s)$ that maximizes the expected cumulative discounted returns $E_{\\tau} [\\sum_{t=0}^{\\infty} \\gamma^{t}r(s_t, a_t)]$ where $\\gamma \\in [0, 1)$ is the discount factor. In this paper, the state of the manager comprises the positional information of all entities, attributes of carried resources, the positions of tasks, and the total resources required for tasks. Actions are defined as the task allocation scenario. The state transition matrix is based on the current task allocation scenario, where the environment returns the positions and resources of all entities and tasks after each entity's movement. The state of an entity includes its own resources and position, as well as the resource requirements and positions of all tasks. Actions involve moving in any direction, and for worker entities, there is an additional action for bidding. The state transition matrix is determined by the current state and action, returning the entity's resource position and the resource requirements and positions of all tasks for the next time step."}, {"title": "4. Methodology", "content": "The manager faces challenges in allocating entities to tasks. One challenge is the uncertainty in the number of tasks, making it difficult to allocate entities simultaneously. To address this, we propose setting the action dimension as $n \\times M$, where $M$ denotes the maximum number of tasks. However, obtaining the precise value of $M$ in advance is challenging, and in most cases, the total number of tasks is smaller than the maximum number of tasks, which can result in wasted space. Another challenge is determining the task allocation order. Fixed order may result in suboptimal solutions, while simultaneous allocation makes it difficult to decide which task each entity should be assigned to. Additionally, the manager needs to consider the generalization problem, where entity costs and resources can vary. Most RL algorithms are unable to handle the issue of variable numbers of agents. The majority of RL algorithms typically assume a fixed environment, meaning a fixed number of agents. This assumption makes these algorithms ill-suited for handling problems with variable numbers of agents. For instance, when the number of agents dynamically changes, traditional reinforcement learning algorithms may struggle to adapt effectively because they often require a predefined number of agents and state spaces. Therefore, when the number of agents fluctuates, these algorithms may encounter difficulties, leading to degraded performance or failure to converge.\nTo address these challenges, we propose a two-stage approach for entity selection. We first propose a pre-allocation method where each entity is pre-assigned as a candidate for each task based on its attributes and the attributes of the tasks. This method avoids the drawbacks of sequential and random selection by enabling sequential allocation based on the fitness between tasks and entities. We have also demonstrated that this pre-allocation method outperforms sequential allocation methods in finding optimal solutions. Secondly, our proposed attention-based hyper network structure allows for the allocation of different network parameters to different numbers of entities, enabling the neural network to accommodate dynamic entity counts and addressing the generalization issue when the number of entities changes. It overcomes local convergence and entity overlap issues and handles generalization problems. We introduce worker entities as selfish agents to verify the allocation of entities with varying attributes. The overview of our method is depicted in Figure 1, while the architecture is illustrated in Figure 2."}, {"title": "4.1. Pre-assign Module", "content": "The pre-assign module initially allocates entities to suitable tasks. In discrete action space environments, each discrete number typically corresponds to an entity action. However, traditional RL algorithms focus on learning the likelihood of number occurrences in the current state, which limits their ability to effectively utilize information in task assignment problems. To leverage the individual attributes of each entity, we propose a Two-head Attention Module (TAM) based on attention mechanisms (Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser and Polosukhin (2017)). This module guides the manager in making pre-assigned allocations based on the compatibility between tasks and entities. It provides policy and assignment value outputs, and its network structure accommodates a variable number of input entities. We adopt Soft Actor-Critic method (SAC, Haarnoja, Zhou, Abbeel and Levine (2018)) as the base RL algorithm for modeling the manager. Since the action space for task allocation is discrete, we adopted the discrete version of the SAC algorithm (Christodoulou (2019)).\nTAM consists of two heads: the actor head and the critic head. The actor head is responsible for generating the policy for the pre-assign actions, while the critic head calculates the value of these actions. The actor head consists of an entity embedding function $f^h: R^S \\rightarrow R^d$, a task embedding function $f^g: R^S \\rightarrow R^d$. Here, $S$ represents the dimension of the task and entity attributes. The entity embedding can be expressed as $h_i = f^h(w_i)$ and the task embedding can also be denoted as $g_i = f^g(T_i)$. The probability of each entity pre-assigning to each task is computed by an attention module, i.e., $\\pi(T_i|w_i) = SoftMax(h_i g_i/\\sqrt{d})$. This method can solve the problem of a variable number of tasks with zero-shot training because when an unseen task $T_\\iota$ occurs, we can compute the embedding $g_\\iota = f^g(T_\\iota)$ and then the probability of entity $w_i$ assigning task $T_\\iota$ is $\\pi(T_\\iota|w_i) = (h_i g_\\iota/\\sqrt{d})$. The pre-assign allocation of entity $w_i$ is $c_i$, which is sampled from the $\\pi(T_i|w_i)$. The total pre-assign allocation is $c = (c_1, c_2, ...c_n)$ where $c_i = k$ denotes that $w_i$ is pre-assigned to task $T_k$.\nDue to the discrete nature of pre-assigned actions, a table can be created to record each action's value. However, the table size $(2^n)$ becomes impractical when $n$ is large, making it challenging to accurately approximate values with limited datasets. To address this, we adopt an approach inspired by the Q MIXing network (QMIX) Rashid, Samvelyan, De Witt, Farquhar, Foerster and Whiteson (2020), which employs a mixing network to compute the total Q value for multi-agent systems. In our approach, we calculate the value selected for each entity and consider the overall assigned value as a nonlinear aggregation of the individual entity values. In QMIX, each agent has a Q network that calculates the value of taking a particular action based on the current observations, resulting in a total of as many Q values as there are agents. Additionally, QMIX employs a hyper network that generates non-negative parameters for the neural network, combining each Q value to compute the overall Q value. For algorithm fine-tuning, we modify the attention mechanism by incorporating a critic head, similar in architecture to the actor head, to output action values. The critic component includes entity value embedding $(f^o)$ and task value embedding $(f^q)$ functions denoted as $o_i = f^o(w_i)$ and $q_i = f^q(T_i)$, respectively. The pre-assign value of $w_i$ to $T_i$ is computed using the dot product, $Q(w_i, .) = o_i q_i$. The architecture of TAM is depicted in Figure 3.\nWhen a pre-assigned action of an entity is sampled, we obtain a value from the critic's component. In the case of having $n$ entities, we obtain $n$ values corresponding to the pre-assigned actions. However, since we interact with the environment throughout the entire pre-assign action, we only receive a single total reward from the environment, which does not align with the output of our critic. To address this disparity, we introduce the Attention MIXing (AMIX) module. The AMIX module utilizes a self-attention-based hypernetwork to generate parameters for its layers. Unlike QMIX, which provides network parameters for a fixed number of agents, AMIX's hyperparameter network, called the Self-attention-based Hyper Network (SHN), utilizes an attention model. This allows it to dynamically adjust the number of parameters based on the input values, effectively accommodating the variable number of entities in the problem. For a visual representation of this concept, please refer to Figure 4. Using the aforementioned critic structure, we can calculate the value estimates for task allocation. According to the SAC algorithm, the actor loss can be expressed as:\n$L_{actor}(\\theta) = \\alpha \\log \\pi_{\\theta}(c|w) - Q_{\\varphi}(w, c)$\nAfter the allocation is complete, the critic is updated based on the reward values obtained from the interaction between the agent and the environment. The loss can be expressed as:\n$L_{critic}(\\varphi) = \\frac{1}{2}(r(w, c) + \\gamma \\arg \\max_{a} Q_{\\varphi}(w', a) - Q_{\\varphi}(w, a))^2$\nThrough the aforementioned loss, we can continuously update the critic network to estimate the reward values for the current allocation executed by all entities. This allows the actor network to output optimal task allocations."}, {"title": "4.2. Select Module", "content": "When the entities are already pre-assigned to tasks by using the pre-assign module, there are many entities for each task to select. The manager needs to select the proper number of entities with different resources to complete the task. In order to have better generalization, the selection module must be able to assign entities and tasks that have not been seen before. To solve these two problems, referring to the point-net (Vinyals, Fortunato and Jaitly (2015)) network framework, we establish a seq2seq-like network structure as our select module. There are several reasons why the seq2seq-like structure can solve our problems. 1) The seq2seq structure is widely used for machine translation and is able to generate a variable number of outputs according to the input information. The problem of a variable number of entities can be solved. 2) In the seq2seq structure, the information of the previous output is taken into account in the context, which helps to achieve better output results in the future. This structure can also help the manager choose entities better by considering the context of previous selections. 3) The point-net attention mechanism can solve the out-of-vocabulary problem when the input and output are the same. The input and output are both entities in the allocation problem.\nThe architecture of the select module is shown in Figure 5. The input of the encoder is the entities $w_\\tau$ pre-assigned to the task $T_i$, and the input of the decoder is $T_i$. Note that there is no sequential relationship between input entities like text, so unlike traditional seq2seq structure, we did not use the commonly used RNN and attention mechanisms to encode the input. We simply use fully connected layer as the encoder, and the output for entity $w_\\tau$ is $d_\\tau$. The embedding of the task $T_i$ is $e_i$, and the output of the $t$-th entity $u^t$ is selected by sampling from a distribution which is $\\pi(.|e) = SoftMax(v \\tanh(W_1 e + W_2 d'))$. The previous output will impact the context, so we change the task by $T^{t+1} = T^t + f^a(u)$. When the task resources needed for $T_i$ are all zero, the process of selection will end."}, {"title": "4.3. Demand Module", "content": "The worker entities have their own minds, which means they will adjust their demands in different situations. We model workers as entities that can propose their own demands based on the attributes of the task. They can update their own minds by using RL algorithms. Since each entity only considers maximizing their own reward and does not consider the global reward, and multi-agent RL algorithms focus on the optimal overall reward, which is not consistent with our scenario, we use singe-agent RL algorithm to train each entity. The demand of each agent is a continuous number, so we use the DDPG (Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa, Silver and Wierstra (2015)) algorithm to train these entities. The observation in DDPG is defined as the current attributes and current position of the entity, while the action is defined as the bid value for the demand. When the current entity is selected, the reward obtained is the bid value. When not selected, the reward value is 0. Therefore, worker entities need to provide reasonable bids based on their current attribute values to be selected and obtain high returns."}, {"title": "5. Experiment", "content": "In this section, we try to answer these questions: (1) Is the pre-assign method more effective than the sequential task allocation method in avoiding finding local optimal solutions? (2) Can the algorithm solve the allocation problem when the cost of entities is dynamically changed based on the current task? (3) Can the algorithm provide a reasonable allocation for such a variable number of entities as their number increases and decreases? (4) Does our proposed method outperform heuristic algorithms? For all experimental environments, our two-stage method uses the same hyperparameters, with no parameters specifically set for each environment. For all network architectures, we use the ReLU function as the activation function and set the number of hidden units to 64, with each network consisting of a 3-layer MLP structure. More details on the parameter configurations of our method can be found in Table 3."}, {"title": "5.1. Retain the Almighty", "content": "This small experiment will answer the question (1) and demonstrate why using sequential selection methods can fall into local optimization and prove that using the pre-assign method is more effective.\nIn the Retain the Almighty environment, there are $N$ tasks and $n$ entities in this environment, and each task $T_i$ in this environment corresponds to a subset of the best entities $b_i$, and the task is completed if and only if a best entity $w \\in b_i$ participates in this task. The last task $T_N$ is a difficult but rewarding task. The best entity of $b_N$ is $w_0$, which is an almighty agent that is able to finish every task, and the cost for it is the same as others. The best allocation is to arrange $w \\in b_i$ to do $T_i$ and the task cannot choose the entities if the previous task has chosen them, which means the algorithm must retain the almighty to the last task to find a good solution. The result is shown in Figure 6.\nIt can be seen that the entity learned using the sequential method is getting trapped in the local optimum. The perfect entity $w$ is selected by the previous task and fails to complete the last task, resulting in a low total reward. When the order of the most difficult tasks is randomly assigned to the top, we can assign the Almighty to this task and obtain high profits; when the most difficult task is randomly assigned later, the rewards will sharply decrease. According to the training curve, the entity has not learned how to assign entities to appropriate tasks, and the final return of this entity is related to the random order of tasks instead of how well it learns to allocate entities to tasks. However, by using the pre-assign method, the manager is able to find the best solution because the task can be completed once the best entity is pre-assigned to the last task. The manager is able to allocate $w_0$ to $T_N$ to get a large reward and then find the optimal solution.\nIn fact, in this environment, we can suppose that without prior knowledge, all entities have the same probability of being selected when the algorithm does not start training. The probability upper bound of $w_0 \\in T_N$ by using the sequential method is $\\binom{n + N - 2}{N - 1}$. The probability upper bound decreases exponentially with the number of tasks. However, the probability of $w_0 \\in T_N$ by using the pre-assign method is $\\frac{1}{N}$. The rate of probability decline is far less than that of the sequential method. See Appendix A for complete proof."}, {"title": "5.2. Electric Power Transportation", "content": "In this experiment, we will answer question (2). There are many towers in the environment that are connected by wires. Different towers have different wire materials, which means different transmission costs between towers. At each time step, there may be several towers in the city corresponding to the power peak, so other towers are needed for power transmission. As a manager, we need to select the proper electric towers to carry out power transmission for the towers with peak power consumption so as to complete the task of ensuring the smooth use of electricity in the city. The towers awaiting transmission represent tasks, characterized by their locations and additional power requirements. Other towers serve as entities, characterized by their additional available power and locations. We employ our proposed two-stage method to initially allocate tasks to each tower awaiting transmission and then make selections based on the specific additional power and attributes of each tower. Once selected to participate in transportation, the cost is calculated as the distance between the two stations multiplied by the predetermined cost of transmission per meter of wire between the two towers. The transmission cost per meter of wire is determined by the material between the two towers, which is predefined in the environment. For each target tower, if sufficient power transmission is successfully obtained, the reward is the task completion reward minus the power transmission cost. Therefore, while ensuring sufficient power, reducing transmission costs is also considered. In this environment, there are a total of 20 electric towers. Each interaction with the environment causes changes in the power values required by the towers. When the power exceeds their individual limits, support from other towers is needed.\nWe use our two-stage approach to train the manager. To test the effect of our method, we denote w/o Pre as the architecture without the pre-assign module, w/o TAM as the architecture without the attention module, which means the actor and critic are linear layers. The w/o AMIX is denoted as the normal global critic to calculate the expected return of this pre-assign action. When we use the AMIX structure, we calculate the individual value assigned to each task for each entity, and then use AMIX to calculate the overall value value; When the AMIX structure is not applicable, we will use an overall critical network to input the attributes of all entities and tasks and directly estimate the overall value value. The training curve and an example of our allocation method are shown in Figure 7. From the figure, it can be seen that our proposed method significantly improves the effectiveness of power allocation and increases the reward value. Additionally, the effectiveness of each proposed module is validated through the curve chart. These subplots show two towers at peak electricity usage, requiring other tower entities to transmit power. Subplot (d) illustrates the value assigned to other tower entities when pre-assigned to a central peak-usage tower. Subplot (e) shows the value for a tower located in the upper left, also at peak usage, receiving pre-assign support from other towers. After training, the pre-assign schemes with higher values are distributed around these two target towers, while towers farther away have lower values. This is due to increased transmission costs with distance, making it inefficient to pre-assign distant towers for support. Our method captures this information, enabling better pre-assignment to towers with lower transmission losses. Subplots (f) and (g) reflect the visualization results of the select stage. After pre-assignment, most tower entities are pre-assigned to the nearest peak-usage towers, and each target tower only needs to select from nearby towers. The value in the selection stage shows that target towers still prioritize nearby entities with lower transmission costs, proving that our algorithm can select suitable entities based on pre-assign results to complete the task."}, {"title": "5.3. Resource-Based Foraging (RBF)", "content": "This experiment will answer questions (2) and (3). This environment is a task assignment environment based on the benchmark environment Level-Based Foraging (LBF, Papoudakis, Christianos, Sch\u00e4fer and Albrecht (2020)) for multi-agent fully collaborative tasks and has undergone some changes to adapt to the settings in this article. The entities possess numerous resources in the RBF environment, but just one resource in the LBF environment, which represents the entities' level. For simplicity, all tasks are still represented by apples, and the differences in task resources are reflected in the size of the apples. The number of entities in RBF is very large, and selecting each entity requires a cost. Apples refresh randomly on the map over time, and the manager needs to control the entity to finish the task, i.e., pick the apples to get rewards. If the task is not completed after a certain period of time, its rewards and requirements will be reduced. Furthermore, if an entity promises to complete a task, it cannot be arranged for other tasks to be completed. In this environment, there are a total of 100 selectable entities, and every 5 time steps, 5-10 apples are randomly generated at any location on the map. The manager needs to select entities from the pool of 100 based on their attributes and positions, determining whether to choose each entity and directing them to solve specific tasks. This can be considered a large-scale task assignment problem, as there are 100 entities to allocate and a considerable number of tasks, requiring sophisticated algorithm design to address the challenge of allocating multiple tasks to a large number of entities.\nWe first consider that these entities are item entities whose cost is determined by the size of resources and their distance from tasks. We simplify different tasks as harvesting different apples, each with its own attributes representing the task requirements. A task is considered completed, or an apple successfully harvested, only when the total attributes of the entity at that location exceed those of the apple. Since the entity attributes are manually specified, we train only the manager in this scenario. The objective is to enable the manager to appropriately select entities based on their attributes, corresponding locations, apple positions, and the total entity attributes required for apple harvesting, and then allocate tasks accordingly. The training curve is shown in Figure 8. To verify the generalization ability of algorithms, we first replaced the training entities with another batch of entities we had never seen before. Then we change the attributes of tasks in the environment to see the result. In the task generalization experiment, we randomly generate new apples with different attributes on the map, distributed across various locations, and have the manager utilize the same set of entities to complete them. This task can be likened to a scenario where a company employs the same group of employees to complete historical tasks and verifies whether they can allocate the employees reasonably based on the requirements of new tasks. The results of the training process, the zero-shot generalization, the few-shot generalization, and the result of training from scratch are shown in Figure 8. We can see that whether it is a change in the number and attributes of entities or a change in the attributes of tasks, the experimental results of zero-shot generalization of the model are almost consistent with the effect of retraining the model about 700 epochs, and the few-shot training performance of the model, which trains 200 epochs in this environment, is better than retraining it 1000 times. This demonstrates that our model does not remember how it should behave in the current environment but rather learns to assign strategies based on entity and task attributes.\nWe also consider the situation where the entities are worker entities, and they will demand their costs for being selected to do a task. They will dynamically adjust their quotes based on the attributes of the task, their own attributes, quotes, final returns, and the selected situation. We use the DDPG algorithm to simulate the process of entity learning to propose demand. We can see from the Figure 8 that as the manager uses our algorithms to learn how to distribute, both total and manager benefits increase. And workers gradually increase their quotes and benefits, so the rate of manager revenue increase begins to decrease. As the total income begins to reach the upper limit, the manager's income at this time reaches the upper limit, and workers no longer raise their demands, creating a dynamic balance. In the training process, workers are constantly learning and modifying the quotation, at which time the algorithm can select the appropriate workers to complete the task. This further proves that our algorithm can dynamically give a reasonable distribution result based on the current situation."}, {"title": "5.4. Material Transportation", "content": "We use a Material Transportation environment and the problem to be solved is the Dynamic Vehicle Routing Problem (DVRP) problem. There are trucks carrying resources at various parking points on the map, and we need to allocate these trucks reasonably based on the amount of resources required by each city. We have a total of 50 entities equipped with various materials. At each time step, the environment generates material demands at random positions. The manager needs to dynamically allocate tasks to the agents based on the entities' positions and remaining material attributes. This environment is different from the previous environments since once an entity completes a task in this environment, the resources that it carries will be cleared, meaning that it cannot be selected to complete other tasks anymore. The results of this experiment are shown in Figure 9. From the comparison, it is evident that our algorithm does not achieve the highest performance when compared to the results obtained without the AMIX or the TAM network structure. However, as mentioned earlier, networks lacking these modules are unable to handle variable number entities allocation. Therefore, while our algorithm may not excel in this specific task, it possesses the crucial ability to generalize and adapt to different scenarios."}, {"title": "5.5. Results", "content": "Here, we compare our proposed reinforcement learning algorithm with three mainstream heuristic algorithms, Genetic Algorithm (Holland (1992), GA), Particle Swarm Optimization (Kennedy and Eberhart (1995), PSO), and Symbiotic Organism Search (Cheng and Pray"}]}