{"title": "Neural Attention Search", "authors": ["Difan Deng", "Marius Lindauer"], "abstract": "We present Neural Attention Search (NAtS), a framework that automatically evaluates the importance of each token within a sequence and determines if the corresponding token can be dropped after several steps. This approach can efficiently reduce the KV cache sizes required by transformer-based models during inference and thus reduce inference costs. In this paper, we design a search space that contains three token types: (i) Global Tokens will be preserved and queried by all the following tokens. (ii) Local Tokens survive until the next global token appears. (iii) Sliding Window Tokens have an impact on the inference of a fixed size of the next following tokens. Similar to the One-Shot Neural Architecture Search approach, this token-type information can be learned jointly with the architecture weights via a learnable attention mask. Experiments on both training a new transformer from scratch and fine-tuning existing large language models show that NAtS can efficiently reduce the KV cache size required for the models while maintaining the models' performance.", "sections": [{"title": "1. Introduction", "content": "The ability to understand and infer from long-context information is crucial for many tasks such as long document summarization and question answering, code generation or multi-round dialogues. Thanks to the ability to query the information from any position of the historical sequence, transformer-based large language models extend their context length up to millions of tokens.\nHowever, querying information from historical sequences requires a complexity of $O(L^2)$ w.r.t. the input sequence length $L$. KV caching could reduce this time complexity to $O(L)$ by storing all the historical KV values. Nevertheless, with the increasing model size of recent LLMs, even the $O(L)$ time-wise and memory-wise complexity could become a bottleneck during inference time.\nIndeed, not all the tokens in a sequence are equally important. Many of them are redundant and do not contribute to the final output. Humans can recognize this information without pre-defined fixed rules and summarize or discard the context information into much smaller content. Transformers could also learn this ability implicitly: Many tokens in the attention map might only have very low weights and only have little influence on the final predictions. However, as the transformer learns this information implicitly, we might not know how the important tokens would be distributed in the context. Selecting these tokens and recognizing the attention distributions might require extra human experts' knowledge by either looking at the attention maps or applying specific fixed rules. Since this knowledge is already contained in the transformer models, we could also ask the model to evaluate the importance of each token and learn to predict the optimal type for the given input tokens automatically.\nUnlike prior works that rely on human expertise or predefined rules to identify important tokens, we propose a novel approach to evaluate the importance of each token by assigning different roles to each of the tokens. E.g. some tokens will be preserved until the end, while other tokens might only survive for a short amount of time. These roles measure the importance of each token and determine if they would survive within the next few tokens. Rather than pre-define a set of fixed rules for each token, we ask the model to learn this information automatically. Finding the optimal role for each token is similar to the process of neural architecture search, where an optimal architecture is searched by the optimizer for a given task. Thereby, in this work, we jointly optimize the choice of each token and the model weights by constructing a learnable attention mask. Our approach is implicitly motivated by the one-shot neural architecture search approach"}, {"title": "2. Background and Related Work", "content": ""}, {"title": "2.1. Attention maps for transformers", "content": "Transformers computes the correlation between different tokens by mapping the input sequences into three variables, Q, K and V. An attention map A is first computed by pairing Q and K and then scaled and normalized with a softmax function. Finally, this attention map A is multiplied by V. Additionally, a mask M is attached to the Attention Map to guide the attention to focus only on certain tokens. This can be expressed as\n$A = \\frac{QK^T}{\\sqrt{d_{head}}}$\n$O = softmax(A + M_{add})V$\nThe additive attention mask $M_{add} \\in {-\\inf, 0}$ controls if the transformer needs to construct the correlation between $Q_i$ and $K_j$. A widely used mask $M_{add}$ used in transformers are casual masks, where the upper parts of the mask are filled with \u2013 inf each token can only query the token information before them.\nComputing the attention map of a sequence with length L requires a complexity of $O(L^2)$ for both time and memory. Techniques such as KV cache lowers this complexity to the $O(L_{cache})$ during inference time with $L_{cache}$ being the length of the KV cache. However, as an expense, the cost of storing the KV cache gradually becomes unneglectable with the increasing size of large language models and the context length they could handle.\nMany studies aim to reduce the KV cache size by sparsify-"}, {"title": "2.2. Neural Architecture Search", "content": "Designing a neural architecture for a specific task might require a lot of trial and error. Neural Architecture Search (NAS) automates this process by searching in a pre-defined search space. Previous work on NAS mainly focused on searching within a discrete search space by sampling a new architecture from the search space, evaluating its performance, and updating the optimizers. However, training every network from scratch requires lots of GPU hours. One-Shot NAS approaches instead share the same weights of the operators w.r.t. all the architectures in the search space. This allows to jointly optimize the architectures and weights. DARTS and GDAS further relax the discrete search space into continuous values to optimize the architecture parameters with gradient descent. The One-Shot NAS approach allows the optimizers to efficiently search for the optimal architecture within a huge search space. Similarly, NAtS has multiple options for each token as the search space and is able to search for the optimal token types jointly with the model weights. However, unlike One-Shot NAS approaches that consider the optimization problem as a bilevel-optimization problem and optimize the model weights and architecture weights alternatively, we optimize the token state information and model weights within one forward and backward pass. This is similar to a mixture-of-expert (MOE) model. However, instead of distributing data across all experts uniformly, we only select one expert for each token and assign all data to that expert."}, {"title": "3. The NATS Approach", "content": "In this section, we first introduce all the candidate token types in our search space. We then show that we can construct a learnable attention mask with the choice of each token type. Finally, we can efficiently reduce the KV cache size by dropping the unnecessary tokens during inference."}, {"title": "3.1. Search Space Design", "content": "Not all the tokens in a sequence are equally important. Some of the tokens might contain important information, and they should be preserved that will be further queried by the following tokens. Some tokens might only contribute to the prediction for the next few tokens. Just like a paragraph is composed of multiple sentences, a sequence can be divided into multiple sub-sequences containing different information; some tokens might only be required within these sub-sequences. Hence, we design a search space for each token's role within the sequence and ask the model to automatically learn the optimal role for each token in the sequence.\nWe first define Global Token as tokens containing important information that need to be preserved for the following predictions. showed that only a small fraction of the tokens contributes to most of the attention scores for self-attention computation. These tokens need to be preserved for models to recall the global information that helps to predict the next token. In vanilla transformer models, all the tokens are Global Tokens.\nGlobal Tokens will not be evicted during inference time. Therefore, we should maintain as few Global Tokens as possible to ensure inference efficiency. Each Global Token should not only preserve the information within that position. Ideally, it should also be able to summarize the information from previous sequences. Hence, we split the entire sequence with the Global Tokens into multiple sub-sequences, with each sub-sequence ending with one Global Token. Each Global Token only needs to summarize the information from its sub-sequences and the previous Global Tokens.\nLocal Token only survives until the next Global Token appears. Therefore, models will have the full attention within each sub-sequence to summarize the local sub-sequence information into the Global Token located at the end of the sub-sequence while being sparse within the input sequence.\nOnly the Global Tokens and Local Tokens might control the sparsity at a low granularity level. E.g., assuming that one input sequence is highly localized, each token only has a high correlation with itself or a few neighboring tokens. In this case, they are all similar and are assigned with the same token type. However, none of the Global Token and Local Token could sparsify this attention map efficiently: if all the tokens are classified as Local Tokens, then the input sequence will only be considered as one single subsequence, and all the Local Tokens will be equivalent to the Global Tokens.\nHence, we introduce Sliding Window Token. Sliding Window Tokens will only be preserved for the next W time steps and were previously considered as one of the most popular sparse attention approaches. \nIn contrast to other causal attention maps, illustrates an exemplary attention mask constructed by the choices of different token types. In this case, we define the sliding window size as 4. Token 1, 4, 10 act as Global"}, {"title": "3.2. Searching for the Optimal Token Types", "content": "Searching for the optimal token types within a sequence is similar to searching for the optimal architectures for a given task in neural architecture search. Following GDAS, we apply the Gumbel-Softmax trick to sample from the discrete search space. The Gumbel-Softmax trick allows the gradient to be back-propagated through the discrete choice of the token types.\nSpecifically, we first use a linear layer (which we call At-tention Score Layer) that maps each the input tensor for an attention layer $X \\in \\mathbb{R}^d$ to the likelihood for each option: $a \\in \\mathbb{R}^{(H*N_{opts})} = Linear(x)$, where H is the number of KV heads and $N_{opts}$ is the number of options in the search space. The type a for each token is then sampled from a Gumble-Softmax function based on the likelihood values.\nWe could now construct a learnable attention mask M with a series of sampled token states. However, the additive mask in Eq. 1 will take - inf values, resulting in invalid gradi-ent information. Hence, we use the token information to construct a multiplicative attention mask $M^{mul} \\in {0,1}^1$:\n$\\Theta = e^{AM^{mul}}$\n$\\, = \\frac{\\Theta}{\\sum_{\\.,\\Theta^{mul}}}$\nThe attention mask columns for Global Tokens and Sliding Window Tokens can be directly constructed since they will survive for a fixed amount of steps. However, the mask for Local Tokens $M^L_{i,j}$ is controlled by both the distribution from Local Tokens and Global Tokens as Local Tokens will survive until the next Global Token appears. In other words, to make $M(j > i)$ a valid value, no Global Token should appear between i and j.\nFormally, the attention masks can be created as follows:\n$M^G_{i,j} = 1$\n$M^{SW}_{i,j} =\\begin{cases}1 & \\text{if } j < i + W \\\\ 0 & \\text{if } j > i + W\\end{cases}$\n$M^L_{i,j} = \\prod_{n=j+1}^{i-1} (1 - G_n)$\nwhere W is the sliding window size and $G_n$ a Global Token at Position n. We could then construct the attention masks based on the type of each token. After that, we mask out the upper triangular part of the mask to ensure its causality.\nIn practice, we first collect the index of the next global token $K_i := min({k|k >= i > G_k = 1})$ and rewrite Eq. 6 as:\n$M^L_{i,j} = \\begin{cases} 1 & \\text{if } j \\leq K_i\\\\ 0 & \\text{if } j > K_i\\end{cases}$\nThese rules are then integrated in FlashAttention to avoid explicitly computing the attention masks during the forward pass. In addition to the transformer computation, we only need to collect the next Global"}, {"title": "3.3. Efficient Inference with Different Token Types", "content": "During inference time, we dynamically map the input feature maps to the corresponding token types and discard the tokens no longer required by the following tokens. The Sliding Window Tokens only survive for a fixed amount of time steps. We preserve a queue in the KV cache space that only stores the most recent W tokens and mask out the non-Sliding Window Tokens: when new tokens come, we place them after the tail of the queue to replace the tokens older than W.\nSimilar to the vanilla transformer forward process, when new tokens arrive, we concatenate them with the existing KV caches, generating new masks and computing the attention output. After that, we start our post-update process: we first check the state of each token to decide if we could remove them or keep them in the cached values. Since different heads might disagree on the types of the same token, we record both the sizes for Global Tokens ($Size_G$) and Local Tokens ($Size_L$) for all the heads. New Sliding Window Tokens do not change these sizes since they will always be placed separately. However, when a new Global Token for any head arrives, we remove all the Local Tokens from the corresponding heads and place the new Global Token right after the existing Global Tokens and then update our $Size_G$ and $Size_L$ accordingly. The same strategy is applied when new LocalTokens arrive: we place them at the end of the Local Tokens and increase the number for Local Tokens."}, {"title": "4. Experiments", "content": "We implement NAtS based on the Flash Attention 2 implementation on triton. All the operations that we proposed in Section 3.2 have at most O(L) complexity. In our experiments, we first train NAtS parameters jointly within a transformer model from scratch. We then apply NAtS to fine-tune a large language model."}, {"title": "4.1. Training a Transformer From Scratch", "content": "We first apply NAtS to train a GPT2 small style transformer model from scratch. Following the setting from NanoGPT, this model has 128M parameters with 12 layers and 12 heads with a hidden dimension of 768. Instead of the learnable position encoding, we apply rotary embeddings to each transformer layer. We train this model on the PG-19 Language Modeling Benchmark. This dataset contains books extracted from Project Gutenberg (PG19) with about 2B tokens in the training sets. We train all models with a context length of 1024 and batch size of 480 (using gradient accumulation). We train all models for 120000 iterations on the training sets on a computation node with four Nvidia H100 GPUs. We evaluate all models on the test sets of PG19 with a context length of 1024.\nAs a baseline, we train another dense transformer model under the same hyperparameter setting. During inference time, we compare NAtS with the following baselines besides the full Transformer: (i) Streaming LLM only preserves the first few starting and the most recent few tokens for future prediction. (ii) H2O first computes the attention map and only preserves the tokens with the top-k attention scores. H2O and Streaming LLM are training-free approaches that control the sparsity with pre-defined hyperparameters during inference time. H2O needs to define the recent sliding window size and the number of Heavy Hitter (HH) tokens; Streaming LLM requires the number of tokens at the beginning of the se-"}, {"title": "4.2. Fine Tune a Large Language Models", "content": "We now apply NAtS to fine-tune an existing large language model in the long context scenario. We evaluate the fine-tuned models on the Needle-in-a-Haystack (NIAH) test and LongBench dataset. NIAH places a random fact or statement (needle) in the middle of a long context and asks the model to retrieve it from the context. LongBench evaluates the model's capacity to understand the information in different long context scenarios, such as single-document and multi-document QA, summarization, few-show learning, synthetic tasks, and code completion.\nIn addition to the baselines in Section 4.1, we use DuoAttention as another baseline. DuoAttention evaluates the importance of each attention head and only assigns full attention budgets to the heads with high scores (the so-called retrieval heads) while applying streaming to the other unimportant heads. DuoAttention shares the same idea as NAtS, where the importance of different KV caches should be learned instead of the pre-defined rules. However, instead of learning the importance of the head level, we aim at learning this information directly on the token level.\nTo allow our models to learn to evaluate the role of each token within different contexts, we construct a new training dataset that follows the construction rules of LongBench. Some of LongBench's tasks are collected from the test sets of the previous benchmarks. Hence, we first collect the training sets from these benchmarks and construct these datasets following the data structure in LongBench. Overall, this dataset contains 6 436 pieces of data with a maximum context length of 16000. Further details on the dataset collection can be found in the appendix.\nWe only fine-tune the Attention Score Layer while keeping all the other parameters in the network fixed. Hence, we aim to approximate the original output from the corresponding base LLM. This approach is similar to DuoAttention. However, since we want the model to capture the overall context information, we update Attention Score Layer with all the output from the full attention layer:\n$L_{distill} = \\frac{1}{B} \\sum_{i=1}^{B} \\frac{1}{L} \\sum_{j=1}^{L} \\frac{1}{H} (H_{Full}[i] - H_{NAtS}[j])^2$\nwhere B is the Batch Size. Additionally, we control the sparsity with the regularization value A instead of the additional loss item defined in DuoAttention, and therefore, only optimize NAtS with $L_{distill}$ as the loss function. We fine-tune two long-context models (Llama-3-8B-Instruct-Gradient-1048k and Mistral-7B-v0.3-Instruct) on two Nvidia H100 GPUs for one epoch using AdamW with a learning rate of"}, {"title": "5. Conclusion and Future Work", "content": "Efficiently managing the KV cache is crucial for deploying large language models in long-context applications. In this work, we propose NAtS, an approach that automatically optimizes the optimal roles of each token to determine how long they should survive. By constructing a learnable mask, NAtS learns to sparsify the attention map end-to-end. Our experiments show that NAtS could use much less KV caches compared to the State-of-the-art KV caching reduction approach. While NAtS shows promising results, future work could include exploration of further token roles and structured search spaces with hierarchies. Overall, we believe that NAtS paves the way towards more efficient and scalable inference with LLMs."}, {"title": "6. Impact Statements", "content": "LLMs are widely applied to different fields nowadays. However, the cost for LLM to store the KV cache and predict"}, {"title": "A. Details on Backward Propagation", "content": ""}, {"title": "A.1. Gradients for Attention Masks", "content": "$\\frac{\\partial O}{\\partial M} = \\frac{\\partial g}{\\partial A} \\frac{\\partial A}{\\partial M}$\n$\\frac{\\partial g}{\\partial A} = e^A$\n$\\frac{\\partial g}{\\partial M} = e^A M$\n$\\frac{\\partial A}{\\partial M} = 1$\nIn Eq. 15 and 16, we show the gradient for M is the same as the value that $\\frac{\\partial O}{\\partial A}$ is supposed to be if no mask is applied.\nSince we have $g_{i,j} = e^{A_{i,j}} \\odot M_{i,j}$. Let's set $S_i := \\sum_j g_{i,j}$ and $P_{i,j} := \\frac{g_{i,j}}{S_i}$. Then we have $dP = dO V^T$. Therefore,\n$dg_{i:} := (diag(\\frac{1}{S_{i:}})-\\frac{1}{S_{i:}^2}I) dP$\nCombining Eq. 17 with Eq. 16 and Eq. 15 provides the same gradients as the vanilla softmax function with additive masks:\n$dA_{i:} = dM_{i:} (diag(\\frac{1}{S_{i:}})-\\frac{1}{S_{i:}^2}I) dP_{i:}$\n$dA = dM M$\n$\\, = (diag(P_{i:}) - P_{i:}P_{i:}^T) dP_{i:}$\nSince $dg_{i:}$ is required to compute the gradient for $A_{i:}$ and always needs to be computed. We can directly use this information to compute the gradients for the attention Mask M.\nFollowing FlashAttention we define $D_i = d_i O^T$, then\n$dM = \\frac{A_{i,j}}{S_{i:}}(dP_{i,j} - D_i)$\n$dA_{i,j} = P_{i,j}(dP_{i,j} - D_i)$\nand dA is computed by Eq. 19. After that, we can backpropagate dA to dq and dv. Since M needs to be recomputed anyway in the flash attention's backward process, this only results in little computational overhead.\nHowever, in practice, we found this makes the training unstable. All the values in a row with vanilla softmax attention are normalized such that their sum is bounded. However, we need to use the unmasked $e^M$ to compute dM in our scenerio, mean-ings $A_{i,j}$ could be unbounded. This might result in an unstable training process. To alleviate this, we record both the masked denominator $S_i$ and unmasked denominator $S = \\sum_j e^{A_{i,j}}$ during the forward pass. We then use the unmasked denominator to compute the unmasked $P_{i,j}= \\frac{e^{A_{i,j}}}{S^G}$ and further apply p' to compute dM:\n$dM = P_{i,j}'(dP_{i,j} - D_i)$\nThis only requires storing an additional $S^G$ during the forward pass (with a complexity of O(L)) and one additional computation during the backward pass (since $S^G$ is already computed and stored during the forward pass)."}, {"title": "A.2. Details on computing", "content": "In Eq. 11 and 12, we show that an additional $da^G_i$ needs to be computed for each token's gradient for Global Tokens. Figure 6 illustrates an example of this. Token 4 is a global token, and we search for its next Global Token (which is Token 10 in this example). Assuming that we want to change Token 4 to another role, the regions within the boundary (orange ones) are those tokens that are influenced by this swtching: given that Token 4 no longer becomes Global Token, Token 2 and 3"}, {"title": "A.3. Optimizing for sparser Attention Maps", "content": "The sparse regularization term A is directly applied to the corresponding gradients for the Global Tokens and Local Tokens. This value should penalize the number of unmasked tokens for each row. While the number of unmasked tokens for Global Token and Local Tokens in column i are $L-i$ and $K_i - i$ with $K_i$ defined in Section 3.2. Hence, we have:\n$dGsparse^G_i = \\lambda \\times \\frac{L-i}{L}$\n$dGsparse^L_i = \\lambda \\times \\frac{K_i-i}{L}$\nCombining Eq. 9, 11, 23 and Eq. 24, we have:\n$da^G_i = \\sum_j d_{i,j} M_{i,j} \\times M^{causal}+ dGsparse -da^G-i$\n$da^L_i = \\sum_j d_{i,j} \\times M_L  \\times M^{causal} + dGsparse^L$\n$da^{SW}_i = \\sum_j d_{i,j} \\times M_{SW} \\times M^{causal}$\nwith $M^G$, $M^L$, $M^{SW}$ defined in Eq. 4, 7 and 5 and $M^{causal}$ is a casual attention mask."}, {"title": "B. Experiments Details", "content": ""}, {"title": "B.1. Collecting the Fine-tune Training Set", "content": "To fine-tune NAtS on LLMs, we collect the training datasets from different tasks:\n\u2022 Multi-Document QA: HotPotQA, 2WikiMultihopQA , MuSiQue , and DuReader (zh)\n\u2022 Single-Document QA: NarrativeQA and Qasper\n\u2022 Summarization: GovReport, QMSum, MultiNews , and VCSUM (zh)\n\u2022 Few-shot Leraning: TREC , TriviaQA , and SAMSum\n\u2022 Code Completion: LCC and RepoBench-P"}, {"title": "C. Further Results on LongBench Dataset", "content": "We show additional results on fine-tuning LLM on the LongBench dataset here. Table 2 shows the results with 25% KV budgets on Mistral-7B-Instruct-v0.3. We found that to achieve the same level of sparsity, Mistral-7B requires a larger sparse regularization value X (the value that we used in Table 2 is $1e \u2212 6$, which is 10 times larger than the one that we used for Llama3-8B models). The result confirms our conlcusion that NAtS outperforms the other baselines in most datasets under similar budget level (and many times with an even smaller budgets)."}, {"title": "D. Ablation Study", "content": ""}, {"title": "D.1. Sparse Regularization Term A", "content": "We first study the impact of sparse regularization terms. The result is shown in Table 4 and 5. We underline the results that are better than the optimal baselines with 25% budgets, and bold the results that are better than the optimal baselines with 50%. Despite that NAtS in Table 1 (NAtS $1e \u2212 7$) and Table 2 (NAtS $1e \u2212 6$) used more than 25% overall KV budgets for some tasks, here we show NAtS could still outperform many of the corresponding optimal baselines with a even lower KV budget.\nTable 4 and 5 show that stronger A generally results in a smaller valid KV cache size. While the order for compression rates for different datasets is consistent with different A settings. For most tasks, a compression rate between 10% to 20% already results in predictions that are similar to the full attention."}, {"title": "D.2. Sliding Window Length W", "content": "Another important hyperparameter for NAtS is the sliding window size W. We apply different sliding window sizes W (64, 128, 256, 384, 512) to fine-tune the Llama3-8B model (with X = $1e - 7$).\nThe result is shown in Table 6. Overall, all the approaches performs similarily. However, smaller sliding window size generally results in an overall larger KV cache size. A reduced sliding window size would force the model to apply more Global Tokens and Local Tokens to construct the mid-range correlation since this distance cannot be covered by the sliding window tokens. However, as the number of sliding window size further increases, this compression rate might saturate since the remaining tokens might always require a long-range correlation whose distance is much larger than the sliding window size (e.g., these tokens might require the correlation between two tokens whose distances are larger than 1k or even more)."}, {"title": "E. Further Results on KV size distributions", "content": "Figure 7 shows the KV cache sizes for LLama3-8B and Mistral-7B on the MultiNews Dataset. The two models still share some similar structure: both modules assign more budgets to the first few layers. However, the KV cache sizes are more uniformly distributed in Mistra compared to LLama3: in LLama3, most of the time, one or two heads with a much larger KV cache sizes compared to the other heads in the same layer, while in Mistral models, this is not so obvious.\nFigure 8 shows another example on the DuReader dataset. This time, we check two other cases: one with a relatively lowered KV cache size, as shown in the upper part of Figure 8. The KV cache sizes are further sparsely distributed even in the shallower layers. However, the important layers in different models are not fixed: in LLama3, 9 layers (1, 3, 5, 6, 9, 11, 15, 16, 17) require a relatively larger budget while this value is much smaller for Mistral: only layer 3, 8, 13, 16 and 19 requires a relatively larger KV cache size. This further shows the importance of designing data and model adaptive approach for compressing the KV cache optimization policies."}]}