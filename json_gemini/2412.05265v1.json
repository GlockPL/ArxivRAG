{"title": "Reinforcement Learning: An Overview", "authors": ["Kevin P. Murphy"], "abstract": "Parts of this monograph are borrowed from chapters 34 and 35 of my textbook [Mur23]. However, I have added a lot of new material, so this text supercedes those chapters. Thanks to Lihong Li, who wrote Section 5.4 and parts of Section 1.4, and Pablo Samuel Castro, who proof-read a draft of this manuscript.", "sections": [{"title": "Introduction", "content": "Reinforcement learning or RL is a class of methods for solving various kinds of sequential decision making tasks. In such tasks, we want to design an agent that interacts with an external environment. The agent maintains an internal state $s_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(s_t)$. The environment responds by sending back an observation $o_{t+1}$, which the agent uses to update its internal state using the state-update function $s_{t+1} = U(s_t, a_t, o_{t+1})$. See Figure 1.1 for an illustration."}, {"title": "Problem definition", "content": "The goal of the agent is to choose a policy $\\pi$ so as to maximize the sum of expected rewards:\n\n$V_{\\pi} (s_0) = \\mathbb{E}_{p(a_0,s_1,o_1,...,a_T,s_T|s_0,\\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]$\n\nwhere $s_0$ is the agent's initial state, $R(s_t, a_t)$ is the reward function that the agent uses to measure the value of performing an action in a given state, $V_\\pi(s_0)$ is the value function for policy $\\pi$ evaluated at $s_0$, and the expectation is wrt\n\n$p(a_0, s_1, a_1,...,a_T, s_T|s_0, \\pi) = \\pi(a_0|s_0)P_{env}(o_1|a_0)\\delta(s_1 = U(s_0, a_0, o_1))\n\\times\\pi(a_1|s_1)P_{env}(o_2|a_1,o_1)\\delta(s_2 = U(s_1, a_1, o_2))\n\\times \\pi(a_2|s_2)P_{env}(o_3|s_{1:2}, o_{1:2})\\delta(s_3 = U(s_2, a_2, o_3)) ...$\n\nwhere $p_{env}$ is the environment's distribution over observations (which is usually unknown). We define the optimal policy as\n\n$\\pi^* = arg \\max_{\\pi} \\mathbb{E}_{p_0(s_0)} [V_{\\pi}(s_0)]$\n\nNote that picking a policy to maximize the sum of expected rewards is an instance of the maximum expected utility principle. There are various ways to design or learn an optimal policy, depending on the assumptions we make about the environment, and the form of the agent. We will discuss some of these options below."}, {"title": "Universal model", "content": "A generic representation for sequential decision making problems (which is an extended version of the \"universal modeling framework\" proposed in [Pow22]) is shown in Figure 1.2. Here we have assumed the"}, {"title": "Episodic vs continuing tasks", "content": "If the agent can potentially interact with the environment forever, we call it a continuing task. Alternatively, the agent is in an episodic task, if its interaction terminates once the system enters a terminal state or absorbing state, which is a state which transitions to itself with 0 reward. After entering a terminal state, we may start a new epsiode from a new initial world state $z_0 \\sim p_0$. (The agent will typically also reinitialize its own internal state $s_0$.) The episode length is in general random. For example, the amount of time a robot takes to reach its goal may be quite variable, depending on the decisions it makes, and the randomness in the environment. Finally, if the trajectory length T in an episodic task is fixed and known, it is called a finite horizon problem.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards,"}, {"title": "Regret", "content": "So far we have been discussing maximizing the reward. However, the upper bound on this is usually unknown, so it can be hard to know how well a given agent is doing. An alternative approach is to work in terms of the regret, which is defined as the difference between the expected reward under the agent's policy and the oracle policy $\\pi^*$, which knows the true MDP. Specifically, let $\\pi_t$ be the agent's policy at time t. Then the per-step regret at t is defined as\n\n$\\ell_t = \\mathbb{E}_{s_{1:t}} [R(s_t, \\pi^*(s_t)) - \\mathbb{E}_{ \\pi(a_t|s_t)}[R(s_t, a_t)]]$\n\nHere the expectation is with respect to randomness in choosing actions using the policy $\\pi$, as well as earlier states, actions and rewards, as well as other potential sources of randomness.\nIf we only care about the final performance of the agent, as in most optimization problems, it is enough to look at the simple regret at the last step, namely $\\ell_T$. Optimizing simple regret results in a problem known as pure exploration [BMS11], where the agent needs to interact with the environment to learn the underlying MDP; at the end, it can then solve for the resulting policy using planning methods (see Section 2.2). However, in RL, it is more common to focus on the cumulative regret, also called the total regret or just the regret, which is defined as\n\n$L_T \\triangleq \\mathbb{E} \\left[\\sum_{t=1}^T \\ell_t \\right]$\n\nThus the agent will accumulate reward (and regret) while it learns a model and policy. This is called earning while learning, and requires performing exploratory actions, to learn the model (and hence optimize long-term reward), while also performing actions that maximize the reward at each step. This requires solving the exploration-exploitation tradeoff, as we discussed in Section 1.4."}, {"title": "Partially observed MDPs", "content": "The model shown in Figure 1.2 is called a partially observable Markov decision process or POMDP (pronounced \"pom-dee-pee\") [KLC98]. Typically the environment's dynamics model is represented by a stochastic transition function, rather than a deterministic function with noise as an input. We can derive this transition function as follows:\n\n$p(z_{t+1}|z_t, a_t) = \\mathbb{E}_{e_z} [\\mathbb{I} (z_{t+1} = W(z_t, a_t, e_z))]$\n\nSimilarly the stochastic observation function is given by\n\n$p(o_{t+1}|z_{t+1}) = \\mathbb{E}_{e_o} [\\mathbb{I} (o_{t+1} = O(z_{t+1}, e_o))]$\n\nNote that we can combine these two distributions to derive the joint world model $p_{wo}(z_{t+1}, o_{t+1}|z_t, a_t)$. Also, we can use these distributions to derive the environment's non-Markovian observation distribution, $p_{env}(o_{t+1}|o_{1:t}, a_{1:t})$, used in Equation (1.4), as follows:\n\n$p_{env} (o_{t+1} | o_{1:t}, a_{1:t}) = \\sum_{z_{t+1}} p(o_{t+1} | z_{t+1}) p(z_{t+1} | a_{1:t})$\n\n$p(z_{t+1}|a_{1:t}) = \\sum_{z_1} ... \\sum_{z_t} p(z_1 | o_1)p(z_2 | z_1, a_1)... p(z_{t+1} | z_t, a_t)$\n\nIf the world model (both $p(o|z)$ and $p(z'|z, a)$) is known, then we can in principle solve for the optimal policy. The method requires that the agent's internal state correspond to the belief state $s_t = b_t = p(z_t|h_t)$, where $h_t = (o_{1:t}, a_{1:t-1})$ is the observation history. The belief state can be updated recursively using Bayes rule. See Section 1.2.5 for details. The belief state forms a sufficient statistic for the optimal policy. Unfortunately, computing the belief state and the resulting optimal policy is wildly intractable [PT87; KLC98]. We discuss some approximate methods in Section 1.3.4."}, {"title": "Markov decision process (MDPs)", "content": "A Markov decision process [Put94] is a special case of a POMDP in which the environment states are observed, so $z_t = o_t = s_t$. We usually define an MDP in terms of the state transition matrix induced by the world model:\n\n$p_S(s_{t+1}|s_t, a_t) = \\mathbb{E}_{e_z} [\\mathbb{I} (s_{t+1} = W(s_t, a_t, e_z))]$"}, {"title": "Contextual MDPs", "content": "A Contextual MDP [HDCM15] is an MDP where the dynamics and rewards of the environment depend on a hidden static parameter referred to as the context. (This is different to a contextual bandit, discussed in Section 1.2.4, where the context is observed at each step.) A simple example of a contextual MDP is a video game, where each level of the game is procedurally generated, that is, it is randomly generated each time the agent starts a new episode. Thus the agent must solve a sequence of related MDPs, which are"}, {"title": "Contextual bandits", "content": "A contextual bandit is a special case of a POMDP where the world state transition function is independent of the action of the agent and the previous state, i.e., $p(z_t|z_{t-1},a_t) = p(z_t)$. In this case, we call the world states \"contexts\"; these are observable by the agent, i.e., $o_t = z_t$. Since the world state distribution is independent of the agents actions, the agent has no effect on the external environment. However, its actions do affect the rewards that it receives. Thus the agent's internal belief state about the underlying reward function $R(o, a)$ changes over time, as the agent learns a model of the world (see Section 1.2.5).\nA special case of a contextual bandit is a regular bandit, in which there is no context, or equivalently, $s_t$ is some fixed constant that never changes. When there are a finite number of possible actions, $\\mathcal{A} = \\{1, ..., a_K\\}$, this is called a multi-armed bandit. In this case the reward model has the form $R(a) = f(w_a)$, where $w_a$ are the parameters for arm a.\nContextual bandits have many applications. For example, consider an online advertising system. In this case, the state $s_t$ represents features of the web page that the user is currently looking at, and the action $a_t$ represents the identity of the ad which the system chooses to show. Since the relevance of the ad depends on the page, the reward function has the form $R(s_t, a_t)$, and hence the problem is contextual. The goal is to maximize the expected reward, which is equivalent to the expected number of times people click on ads; this is known as the click through rate or CTR. (See e.g., [Gra+10; Li+10; McM+13; Aga+14; Du+21; YZ22] for more information about this application.) Another application of contextual bandits arises in clinical trials [VBW15]. In this case, the state $s_t$ are features of the current patient we are treating, and the action $a_t$ is the treatment the doctor chooses to give them (e.g., a new drug or a placebo).\nFor more details on bandits, see e.g., [LS19; Sli19]."}, {"title": "Belief state MDPs", "content": "In this section, we describe a kind of MDP where the state represents a probability distribution, known as a belief state or information state, which is updated by the agent (\"in its head\") as it receives information from the environment. More precisely, consider a contextual bandit problem, where the agent approximates the unknown reward by a function $R(o, a) = f(o, a; w)$. Let us denote the posterior over the unknown parameters by $b_t = p(w|h_t)$, where $h_t = \\{o_{1:t}, a_{1:t}, r_{1:t}\\}$ is the history of past observations, actions and rewards. This belief state can be updated deterministically using Bayes' rule; we denote this operation by $b_{t+1} = BayesRule(b_t, o_{t+1}, a_{t+1}, r_{t+1})$. (This corresponds to the state update $S_U$ defined earlier.) Using this, we can define the following belief state MDP, with deterministic dynamics given by\n\n$p(b_{t+1} | b_t, o_{t+1}, a_{t+1}, r_{t+1}) = \\mathbb{I} (b_{t+1} = BayesRule(b_t, o_{t+1}, a_{t+1}, r_{t+1}))$\n\nand reward function given by\n\n$p(r_t | o_t, a_t, b_t) = \\int p_R(r_t | o_t, a_t; w)p(w | b_t) dw$"}, {"title": "Optimization problems", "content": "The bandit problem is an example of a problem where the agent must interact with the world in order to collect information, but it does not otherwise affect the environment. Thus the agents internal belief state"}, {"title": "Best-arm identification", "content": "In the standard multi-armed bandit problem our goal is to maximize the sum of expected rewards. However, in some cases, the goal is to determine the best arm given a fixed budget of T trials; this variant is known as best-arm identification [ABM10]. Formally, this corresponds to optimizing the final reward criterion:\n\n$V_{\\pi,\\pi_T} = \\mathbb{E}_{p(a_{1:T}, r_{1:T}|s_0,\\pi)} [R(\\hat{a})]$\n\nwhere $\\hat{a} = \\pi_T(a_{1:T}, r_{1:T})$ is the estimated optimal arm as computed by the terminal policy $\\pi_T$ applied to the sequence of observations obtained by the exploration policy $\\pi$. This can be solved by a simple adaptation of the methods used for standard bandits."}, {"title": "Bayesian optimization", "content": "Bayesian optimization is a gradient-free approach to optimizing expensive blackbox functions. That is, we want to find\n\n$w^* = arg \\max_w R(w)$\n\nfor some unknown function R, where $w \\in \\mathbb{R}^N$, using as few actions (function evaluations of R) as possible. This is essentially an \"infinite arm\" version of the best-arm identification problem [Tou14], where we replace the discrete choice of arms $a \\in \\{1, ..., K\\}$ with the parameter vector $w \\in \\mathbb{R}^N$. In this case, the optimal policy can be computed if the agent's state $s_t$ is a belief state over the unknown function, i.e., $s_t = p(R|h_t)$. A common way to represent this distribution is to use Gaussian processes. We can then use heuristics like expected improvement, knowledge gradient or Thompson sampling to implement the corresponding policy, $w_t = \\pi(s_t)$. For details, see e.g., [Gar23]."}, {"title": "Active learning", "content": "Active learning is similar to BayesOpt, but instead of trying to find the point at which the function is largest (i.e., $w^*$), we are trying to learn the whole function R, again by querying it at different points $w_t$. Once again, the optimal strategy again requires maintaining a belief state over the unknown function, but now the best policy takes a different form, such as choosing query points to reduce the entropy of the belief state. See e.g., [Smi+23]."}, {"title": "Stochastic Gradient Descent (SGD)", "content": "Finally we discuss how to interpret Stochastic Gradient Descent (SGD) as a sequential decision making process, following [Pow22]. The action space consists of querying the unknown function R at locations $a_t = w_t$, and observing the function value $r_t = R(w_t)$; however, unlike BayesOpt, now we also observe the corresponding gradient $g_t = \\nabla_wR(w)|_{w_t}$, which gives non-local information about the function. The environment state contains the true function R which is used to generate the observations given the agent's actions. The agent state contains the current parameter estimate $w_t$, and may contain other information such as first and second moments $m_1$ and $v_t$, needed by methods such as Adam. The update rule (for vanilla SGD) takes the form $w_{t+1} = w_t + \\alpha_t g_t$, where the stepsize $\\alpha_t$ is chosen by the policy, $\\alpha_t = \\pi(s_t)$. The terminal policy has the form $\\pi(s_t) = w_T$."}, {"title": "Reinforcement Learning", "content": "In this section, we give a brief overview of how to compute optimal policies when the model of the environment is unknown; this is the core problem tackled by RL. We mostly focus on the MDP case, but discuss the POMDP case in Section 1.3.4.\nWe may categorize RL methods along two main dimensions: (1) by what the agent represents and learns: the value function, and/or the policy, and/or the model; (2) and by how actions are selected: on-policy (actions must be selected by the agent's current policy), and off-policy (actions can be select by any kind of policy, including human demonstrations). Table 1.1 lists a few representative examples. More details are given in the subsequent sections."}, {"title": "Value-based RL (Approximate Dynamic Programming)", "content": "In this section, we give a brief introduction to value-based RL, also called Approximate Dynamic Programming or ADP; see Chapter 2 for more details.\nWe introduced the value function $V(s)$ in Equation (1.1), which we repeat here for convenience:\n\n$V_{\\pi}(s) \\approx \\mathbb{E}_{\\pi} [G_0|s_0 = s] = \\mathbb{E}_{\\pi} \\left[\\sum_{\\gamma} r_t | s_0 = s \\right]$\n\nThe value function for the optimal policy $\\pi^*$ is known to satisfy the following recursive condition, known as Bellman's equation:\n\n$V^*(s) = \\max_a \\left[ R(s, a) + \\gamma \\mathbb{E}_{p_S(s'|s,a)} [V^*(s')] \\right]$\n\nThis follows from the principle of dynamic programming, which computes the optimal solution to a problem (here the value of state s by combining the optimal solution of various subproblems (here the values of the next states s'). This can be used to derive the following learning rule:\n\n$V(s) \\leftarrow V(s) + \\eta[r + \\gamma V(s') - V(s)]$\n\nwhere $s' \\sim p_s(\\cdot|s, a)$ is the next state sampled from the environment, and $r = R(s, a)$ is the observed reward. This is called Temporal Difference or TD learning (see Section 2.3.2 for details). Unfortunately, it is not clear how to derive a policy if all we know is the value function. We now describe a solution to this problem."}, {"title": "Policy-based RL", "content": "In this section we give a brief introductin to Policy-based RL; for details see Chapter 3.\nIn policy-based methods, we try to directly maximize $J(\\pi_\\theta) = \\mathbb{E}_{p(s_0)} [V(s_0)]$ wrt the parameter's $\\theta$; this is called policy search. If $J(\\pi_\\theta)$ is differentiable wrt $\\theta$, we can use stochastic gradient ascent to optimize $\\theta$, which is known as policy gradient (see Section 3.1).\nPolicy gradient methods have the advantage that they provably converge to a local optimum for many common policy classes, whereas Q-learning may diverge when approximation is used (Section 2.5.2.4). In addition, policy gradient methods can easily be applied to continuous action spaces, since they do not need to compute $argmax_a Q(s, a)$. Unfortunately, the score function estimator for $\\nabla_\\theta J(\\pi_\\theta)$ can have a very high variance, so the resulting method can converge slowly.\nOne way to reduce the variance is to learn an approximate value function, $V_w(s)$, and to use it as a baseline in the score function estimator. We can learn $V_w(s)$ using using TD learning. Alternatively, we can learn an advantage function, $A_w(s, a)$, and use it as a baseline. These policy gradient variants are called actor critic methods, where the actor refers to the policy $\\pi_\\theta$ and the critic refers to $V_w$ or $A_w$. See Section 3.3 for details."}, {"title": "Model-based RL", "content": "In this section, we give a brief introduction to model-based RL; for more details, see Chapter 4.\nValue-based methods, such as Q-learning, and policy search methods, such as policy gradient, can be very sample inefficient, which means they may need to interact with the environment many times before finding a good policy, which can be problematic when real-world interactions are expensive. In model-based RL, we first learn the MDP, including the $p_s(s'|s, a)$ and $R(s, a)$ functions, and then compute the policy, either using approximate dynamic programming on the learned model, or doing lookahead search. In practice, we often interleave the model learning and planning phases, so we can use the partially learned policy to decide what data to collect, to help learn a better model."}, {"title": "Dealing with partial observability", "content": "In an MDP, we assume that the state of the environment $s_t$ is the same as the observation $o_t$ obtained by the agent. But in many problems, the observation only gives partial information about the underlying state of the world (e.g., a rodent or robot navigating in a maze). This is called partial observability. In this case, using a policy of the form $a_t = \\pi(o_t)$ is suboptimal, since $o_t$ does not give us complete state information. Instead we need to use a policy of the form $a_t = \\pi(h_t)$, where $h_t = (a_1, o_1,..., a_{t-1}, o_t)$ is the entire past history of observations and actions, plus the current observation. Since depending on the entire past is not tractable for a long-lived agent, various approximate solution methods have been developed, as we summarize below."}, {"title": "Optimal solution", "content": "If we know the true latent structure of the world (i.e., both p(oz) and p(z'z, a), to use the notation of Section 1.1.2), then we can use solution methods designed for POMDPs, discussed in Section 1.2.1. This requires using Bayesian inference to compute a belief state, $b_t = p(z_t|h_t)$ (see Section 1.2.5), and then using this belief state to guide our decisions. However, learning the parameters of a POMDP (i.e., the generative latent world model) is very difficult, as is recursively computing and updating the belief state, as is computing the policy given the belief state. Indeed, optimally solving POMDPs is known to be computationally very difficult for any method [PT87; KLC98]. So in practice simpler approximations are used. We discuss some of these below. (For more details, see [Mur00].)\nNote that it is possible to marginalize out the POMDP latent state $z_t$, to derive a prediction over the next observable state, $p(o_{t+1} h_t, a_t)$. This can then become a learning target for a model, that is trained to directly predict future observations, without explicitly invoking the concept of latent state. This is called a predictive state representation or PSR [LS01]. This is related to the idea of observable operator models [Jae00], and to the concept of successor representations which we discuss in Section 4.4.2."}, {"title": "Finite observation history", "content": "The simplest solution to the partial observability problem is to define the state to be a finite history of the last k observations, $s_t = h_{t-k:t}$; when the observations $o_t$ are images, this is often called frame stacking. We can then use standard MDP methods. Unfortunately, this cannot capture long-range dependencies in the data."}, {"title": "Stateful (recurrent) policies", "content": "A more powerful approach is to use a stateful policy, that can remember the entire past, and not just respond to the current input or last k frames. For example, we can represent the policy by an RNN (recurrent neural network), as proposed in the R2D2 paper [Kap+18], and used in many other papers. Now the hidden state $z_t$ of the RNN will implicitly summarize the past observations, $h_t$, and can be used in lieu of the state $s_t$ in any standard RL algorithm.\nRNNs policies are widely used, and this method is often effective in solving partially observed problems. However, they typically will not plan to perform information-gathering actions, since there is no explicit notion of belief state or uncertainty. However, such behavior can arise via meta-learning [Mik+20]."}, {"title": "Software", "content": "Implementing RL algorithms is much trickier than methods for supervised learning, or generative methods such as language modeling and diffusion, all of which have stable (easy-to-optimize) loss functions. Therefore it is often wise to build on existing software rather than starting from scratch. We list some useful libraries in Section 1.3.5.\nIn addition, RL experiments can be very high variance, making it hard to draw valid conclusions. See [Aga+21b; Pat+24; Jor+24] for some recommended experimental practices. For example, when reporting performance across different environments, with different intrinsic difficulties (e.g., different kinds of Atari"}, {"title": "Simple heuristics", "content": "We start with a policy based on pure exploitation. This is known as the greedy policy, $a_t = arg \\max Q(s, a)$. We can add exploration to this by sometimes picking some other, non-greedy action.\nOne approach is to use an $\\epsilon$-greedy policy $\\pi_\\epsilon$, parameterized by $\\epsilon \\in [0,1]$. In this case, we pick the greedy action wrt the current model, $a_t = arg \\max_a, R_t(s_t, a)$ with probability $1 - \\epsilon$, and a random action with probability $\\epsilon$. This rule ensures the agent's continual exploration of all state-action combinations. Unfortunately, this heuristic can be shown to be suboptimal, since it explores every action with at least a constant probability $\\epsilon/|\\mathcal{A}|$, although this can be solved by annealing $\\epsilon$ to 0 over time.\nAnother problem with $\\epsilon$-greedy is that it can result in \"dithering\", in which the agent continually changes its mind about what to do. In [DOB21] they propose a simple solution to this problem, known as $\\epsilon_z$-greedy, that often works well. The idea is that with probability $1 - \\epsilon$ the agent exploits, but with with probability $\\epsilon$ the agent explores by repeating the sampled action for $n \\sim z(n)$ steps in a row, where $z(n)$ is a distribution over the repeat duration. This can help the agent escape from local minima.\nAnother approach is to use Boltzmann exploration, which assigns higher probabilities to explore more promising actions, taking itno account the reward function. That is, we use a polocy of the form\n\n$\\pi_\\tau(a|s) = \\frac{exp(R_t(s_t, a) / \\tau)}{\\sum_{a'} exp(R_t(s_t, a') / \\tau)}$\n\nwhere $\\tau > 0$ is a temperature parameter that controls how entropic the distribution is. As gets close to 0, $\\pi_\\tau$ becomes close to a greedy policy. On the other hand, higher values of $\\tau$ will make $\\pi(a|s)$ more uniform,"}, {"title": "Methods based on the belief state MDP", "content": "We can compute an optimal solution to the exploration-exploitation tradeoff by adopting a Bayesian approach to the problem. We start by computing the belief state MDP, as discussed in Section 1.2.5. We then compute the optimal policy, as we explain below."}, {"title": "Bandit case (Gittins indices)", "content": "Suppose we have a way to compute the recursively compute the belief state over model parameters, $p(\\theta_t | D_{1:t})$. How do we use this to solve for the policy in the resulting belief state MDP?\nIn the special case of context-free bandits with a finite number of arms, the optimal policy of this belief state MDP can be computed using dynamic programming. The result can be represented as a table of action probabilities, $\\pi_t(a_1,..., a_K)$, for each step; this are known as Gittins indices [Git89] (see [PR12; Pow22] for a detailed explanation). However, computing the optimal policy for general contextual bandits is intractable [PT87]."}, {"title": "MDP case (Bayes Adaptive MDPs)", "content": "We can extend the above techniques to the MDP case by constructing a BAMDP, which stands for \"Bayes-Adaptive MDP\" [Duf02]. However, this is computationally intractable to solve, so various approximations are made (see e.g., [Zin+21; AS22; Mik+20])."}, {"title": "Upper confidence bounds (UCBs)", "content": "The optimal solution to explore-exploit is intractable. However, an intuitively sensible approach is based on the principle known as \"optimism in the face of uncertainty\" (OFU). The principle selects actions greedily, but based on optimistic estimates of their rewards. The optimality of this approach is proved in the R-Max paper of [Ten02], which builds on the earlier E3 paper of [KS02].\nThe most common implementation of this principle is based on the notion of an upper confidence bound or UCB. We will initially explain this for the bandit case, then extend to the MDP case."}, {"title": "Basic idea", "content": "To use a UCB strategy, the agent maintains an optimistic reward function estimate $R_t$, so that $R_t(s, a) \\ge R(s_t, a)$ for all a with high probability, and then chooses the greedy action accordingly:\n\n$a_t = arg \\max_a \\tilde{R}_t(s_t, a)$\n\nUCB can be viewed a form of exploration bonus, where the optimistic estimate encourages exploration. Typically, the amount of optimism, $R_t - R$, decreases over time so that the agent gradually reduces exploration. With properly constructed optimistic reward estimates, the UCB strategy has been shown to achieve near-optimal regret in many variants of bandits [LS19]. (We discuss regret in Section 1.1.4.)\nThe optimistic function R can be obtained in different ways, sometimes in closed forms, as we discuss below."}, {"title": "Bandit case: Frequentist approach", "content": "A frequentist approach to computing a confidence bound can be based on a concentration inequality [BLM16", "error": "R_t(s", "taken": "n\n$\\hat{\\mu"}, "t(a) = \\frac{N_+(a)}{N(a)} = \\frac{N_+(a)}{N_+(a) + N_-(a)}$\n\nwhere $N_+(a)$ is the number of times (up to step t - 1) that action a has been tried and the observed reward was r, and $N_t(a)$ is the total number of times action a has been tried:\n\n$N_t(a) = \\sum_{s=1}^{t-1} \\mathbb{I} (a_t = a)$\n\nThen the Chernoff-Hoeffding inequality [BLM16"]}