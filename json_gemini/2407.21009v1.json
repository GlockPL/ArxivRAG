{"title": "AI-Assisted Generation of Difficult Math Questions", "authors": ["Vedant Shah", "Dingli Yu", "Kaifeng Lyu", "Simon Park", "Nan Rosemary Ke", "Michael Mozer", "Yoshua Bengio", "Sanjeev Arora", "Anirudh Goyal"], "abstract": "Current LLM training positions mathematical reasoning as a core capability. With publicly available sources fully tapped, there is an unmet demand for diverse and challenging mathematics questions. Relying solely on human experts is both time-consuming and costly, while LLM-generated questions often lack the requisite diversity and difficulty. We present a design framework that combines the strengths of LLMs with a human-in-the-loop approach to generate a diverse array of challenging math questions. Initially, leveraging LLM metacognition skills [Didolkar et al., 2024], a strong LLM is used to extract core \u201cskills\u201d from existing math datasets. These skills serve as the basis for generating novel and difficult questions by prompting the LLM with random pairs of core skills that must be utilized in the question. The use of two very different skills within each question makes finding such questions an \"out of distribution\" task for both LLMs and humans. Our pipeline employs LLMs to iteratively generate and refine questions and solutions through multi-turn prompting. Human annotators then verify and further refine the questions, with their efficiency enhanced via further LLM interac-tions. Applying this pipeline on skills extracted from MATH dataset [Hendrycks et al., 2021] resulted in MATH2 - a dataset of higher quality math questions, as evidenced by: (a) Lower performance of all models on MATH2 than on MATH (b) Higher performance on MATH when using MATH2 questions as in-context examples. Although focused on mathematics, our methodology seems applicable to other domains requiring structured reasoning, and potentially as a component of scalable oversight. Also of interest is a striking relationship observed between models' performance on the new dataset: the success rate on MATH\u00b2 is the square on MATH. This suggests that successfully solving the question in MATH2 requires a nontrivial combination of two distinct math skills.", "sections": [{"title": "1 Introduction", "content": "Significant improvement in the capabilities of LLMs [Chowdhery et al., 2023, Anil et al., 2023, Team, 2023, Team et al., 2023, Abdin et al., 2024, Achiam et al., 2023, Touvron et al., 2023] to understand and generate complex mathematical content has been achieved by leveraging all the public data and a fair bit of private data. Sources of high-quality, varied, and difficult mathematical questions are drying up. Even finding new questions for evaluation is getting difficult since newly-released human exams are somewhat similar to past exams, which are potentially present in the LLMs' training datasets. Hence, there is a pressing need for innovative methods to create new, diverse, and challenging questions."}, {"title": "1.1 Evaluation Saturation Phenomenon", "content": "LLM evaluations getting saturated is a well-known issue. Some of the saturation is driven by across-the-board improvements arising from better training and more extensive/better datasets. But a lot has to do with evaluation-specific enhancements that optimize model performance on standard evaluations through techniques like supervised fine-tuning (SFT) on synthetic question-answer pairs. These synthetic pairs can be generated by leading proprietary models when provided with a few examples from the dataset or by filtering the model's own responses [Yue et al., 2023, Yu et al., 2023b]. Such methods can dramatically boost performance; for example, just 1 million synthetic examples can elevate Llama2 7B's performance on the MATH dataset to levels comparable to GPT-4 [Li et al., 2024].\nThe distinction between general and evaluation-specific improvements is crucial. The latter may lead to overfitting to particular evaluations rather than a genuine acquisition of mathematical skills. This issue was highlighted when a new version of the GSM8K dataset revealed performance drops in many models, indicating overfitting to the previous dataset version [Zhang et al., 2024]. Similarly, leading LLMs performed significantly worse on newer versions of the Chinese GaoKao exam compared to older exams, raising fundamental questions about the depth of their mathematical understanding."}, {"title": "1.2 Proposed Framework: AI-assisted Generation of Difficult Math Questions", "content": "At first glance, it may seem counterintuitive to use an AI model to generate and correct novel questions that it is unable to solve itself. However, recent research [Arora and Goyal, 2023, Didolkar et al., 2024] demonstrated that top LLMs possess a robust understanding of mathematical skills, including the capability to identify the skills required to solve given questions [Reid et al., 2024, Achiam et al., 2023]. This naturally raises the question: can LLMs operate in the reverse direction, i.e., generate math problems when given a list of skills that have to be tested? Our initial attempts yielded mixed results. While leading models could produce creative math questions when provided with a list of skills, the majority of these questions exhibited one or more of the following shortcomings: too similar to existing questions in datasets; have errors or nonsensical elements; are too tedious or mechanical to be engaging for human annotators. (See Section 4.) Moreover, they often conflate \"difficulty\" with tedious calculations, which actually would play to the strength of machines to leverage external tools such as calculators or Python interpreters.\nNevertheless, there were promising instances where LLMs generated interesting and correct questions that they were unable to solve, due to incomplete or incorrect reasoning. This observation led us to the concept of Al-assisted creation of evaluation datasets. Our process may also be of interest for human pedagogy since it begins with the extraction of core \"skills\" from existing math datasets, which serve as the foundational elements of mathematical questions. The current paper focuses on the MATH dataset [Hendrycks et al., 2021], a mainstay of LLM evaluation in recent years.\nStarting with a list of mathematical skills extracted from the MATH dataset using recently discovered methods [Didolkar et al., 2024], we focused on creating questions that involve one skill from pre-algebra and algebra portions of the MATH dataset and one other skill randomly sampled from different sections of MATH. Our generation pipeline uses carefully crafted prompts and multi-turn"}, {"title": "2 Pipeline for AI-Assisted Question Generation", "content": "We present a structured approach to generating challenging mathematics questions by combining the capabilities of large language models (LLMs) and human expertise. Given below is a high-level overview of the process before delving into the details of each step.\nWe begin our pipeline with skill extraction - identifying and cataloging distinct mathematical skills from a dataset, as described in Didolkar et al. [2024]. This step creates a repository of skills linked to specific questions. The motivation behind this is to systematically generate and analyze questions that require specific skills, ensuring a comprehensive evaluation framework.\nNext, we focus on generating questions that combine pairs of distinct skills to increase their difficulty. By using advanced models like GPT-4 and Claude, and incorporating in-context examples of multi-way interactions between AI and humans, we enhance the models' performance in generating complex questions. This step aims to produce challenging questions that robustly assess problem-solving abilities.\nThe final step involves screening and validation to filter out invalid or flawed questions. This rigorous process includes evaluating and solving the questions to identify hidden flaws, such as computational intractability or logical inconsistencies. Advanced techniques like in-context exemplars and self-consistency further ensure the accuracy and quality of the solutions. This step is crucial for maintaining the integrity and reliability of the generated questions and their solutions. Overall, each step in the pipeline is designed to systematically enhance the quality and difficulty of questions, providing a robust and comprehensive evaluation of mathematical skills."}, {"title": "Experiments and Findings", "content": "Through our experiments, we demonstrate the difficulty and quality of the MATH2 while also analyzing the behavior of different models on this task of compositional generalization. Firstly, we evaluate a wide range of models spanning a large range of parameter counts on MATH2 and compare against their performance on MATH [Hendrycks et al., 2021] which is the base dataset"}, {"title": "3.1 Experimental Setup", "content": "We follow the pipeline proposed in [Didolkar et al., 2024] to extract skills from the MATH dataset [Hendrycks et al., 2021]. The MATH dataset encompasses seven high-level topics, allowing us to identify and extract finer-grained skills within each topic and label each question accordingly. At the end of the skill-extraction process, we identify a set of 114 skills. We then remove a few simple skills, such as basic_arithmetic and arithmetic_operations, before using the remaining set\nto generate questions using the proposed approach. We generate and verify 180 difficult questions to create the MATH\u00b2 dataset. Figure 5 shows the distribution of skills in MATH\u00b2.\nAs for the solutions, 62 out of the 180 solutions originally generated by the model were modified to correct them or improve their clarity.\nIn total, 56% of the question-answer pairs in MATH2 appear exactly as phrased by their LLM creator.\nWe evaluate the generated set of questions on a variety of language models, both small and large. Specifically, we assess the MetaMath [Yu et al., 2023b], MAmmoTH [Yue et al., 2023], Gemmma [Team et al., 2024], and Llama-3 series, Phi-3, deepseek-math as well as one Mixture-of-Experts model Mixtral-8\u00d77B-Instruct. Additionally, we include evaluations of larger proprietary models"}, {"title": "3.2 Performance across the two datasets: A surprising pattern", "content": "Table 2 shows that all tested models have significantly lower performance on MATH2 than on the original MATH dataset. Denoting Y as the performance on MATH2 and X as the performance on MATH,\nthe percentage drop 100(X \u2013 Y)/X for frontier models GPT-4 Omni, GPT-4 Turbo, Gemini-1.5-Pro, Claude-3.5-Sonnet and Claude 3 Opus ranges from 13.42% to 35.45%. MAmmoTH-7B, a specialist math model, shows the largest drop at 92.91%.\nThe fact that performance drops for all models should not be too surprising, since as noted, the MATH2 questions, by combining skills from different subareas of MATH, could be seen as \"out of distribution (OOD).\u201d This makes it tempting to interpret the percentage drop as a measure of a model's (lack of) \"OOD-resilience.\" For instance, very large percentage drops seen with open-source models MetaMath and MAmmoTH feel understandable since their training used synthetic data generated using seed questions from MATH and GSM-8k. Lack of diversity in such synthetic data is known to cause overfitting to the dataset being imitated. Similarly, GPT-40 and Claude Sonnet 3.5 are suspected to also have been extensively trained with synthetic data. Although their MATH performance is similar, Sonnet 3.5 has worse MATH\u00b2 performance, which might suggest lower quality/diversity in its synthetic data.\nHowever, in our opinion, the overall pattern among proprietary models of similar size does fit with the OOD story. A much simpler explanation pops out when we plot Y vs X2 (Figure 3 and Figure 4(a)): we find a linear relationship Y \u2248 X2! This implies that the relative drop in performance of the models is well-predictable from just their performance on MATH, and does not require taking their training details into account!\nWhy should the two scores be expected to have this relationship? Here is a natural (albeit heuristic) explanation. Suppose there are N skills and si denotes the success rate of the model at correctly applying the ith skill. Then, its X value should reflect the average of the si's. Furthermore, on a random question using the ith and jth skill, the probability that the model correctly answers it should"}, {"title": "3.3 Generated Questions are Effective In-Context Exemplars for MATH.", "content": "A possible test for the quality of a Q&A pair on similar topics as MATH dataset is whether performance on MATH improves when using these as in-context exemplars.\nWe test as follows. Recall that MATH has 7 sections. Exemplars for a section are chosen from the section area. However, by design, our new questions cross section boundaries. We implemented a new procedure to retrieve in-context exemplars from MATH2 based on the skill requirements of the current question.\nSince MATH2 is limited in size, it does not cover all the skills extracted during the skill extraction process, containing 97 out of 114 skills. Figure 5 shows the distribution of different skills in the dataset. We filtered the MATH test set to remove examples requiring skills not present in the generated dataset, resulting in the removal of 809 test examples. During evaluation on the filtered MATH test set, for each question Q labeled with skill a (a \u2208 S, where S is the set of extracted skills), we retrieved in-context exemplars from the MATH2, ensuring each exemplar involved skill a. We used four such exemplars per question (i.e., 4-shot CoT [Wei et al., 2022]). To handle skills represented by fewer than four examples in MATH2, we run two experiments: (A) Proposed 4-shot CoT: If a given skill is represented by n examples in the MATH2, where n < 4, we use n in-context examples instead of 4 exemplars. (B) Proposed + Skill Based 4-shot CoT: If a given skill is represented by n examples in MATH\u00b2, where n < 4, we supplement 4 \u2013 n exemplars for that skill from MATH training set. The relevant in-context exemplars in MATH training set are determined by following the methodology proposed in Didolkar et al. [2024]. We compared the performance of models using these targeted prompting strategies against two baselines: (C) MAmmoTH 4-shot CoT: The 4 in-context exemplars are taken from the MAmmoTH evaluation suite [Yue et al., 2023]. (D) Skill Based 4-shot CoT: We use skill-based prompting as proposed in Didolkar et al. [2024], where the in-context exemplars are selected from the MATH training set, in accordance to the skill required by the question at hand, as determined by GPT-4.\nTable 3 presents the results of this comparison. The two prompting strategies using questions from MATH2 as in-context exemplars, clearly outperform the two baselines. We conclude that the MATH2 questions, due to their difficulty and skill relevance, serve as effective in-context exemplars. Performance gains would likely be more significant with larger datasets generated using our approach, reducing the need to supplement with external exemplars."}, {"title": "4 Observations from the Question Generation Process", "content": "The question generation pipeline described in Section 2 was developed through an iterative process of refining prompts and design choices, and evaluating their impact on the quality of the final questions and solutions. Notably, the inclusion of the attempted solution and question validation steps significantly enhanced the pipeline's effectiveness. Despite the sophistication of the pipeline and prompts, we still observe instances where models fail to follow the given instructions. This section highlights prominent failure modes at various stages of the pipeline, which human raters need to be aware of. Additionally, we explore some intriguing behaviors of the models where they successfully create interesting and creative questions. Section 4.1 details the role of human raters in improving these questions."}, {"title": "4.1 Creative questions: Examples of Synergy from Human-AI interaction", "content": "The models frequently produced interesting and creative questions, although they often failed to generate correct solutions. In these cases, the incorrect solutions usually contained enough correct ideas for a human to quickly complete them.\nHuman annotators were tasked with verifying the validity of the questions and the correctness of the solutions. They were instructed to look out for any failure modes discussed in Section 4.2. Their responsibilities included ensuring that the created questions actually employed the intended math skills, and improving the questions in terms of readability, quality, and difficulty when possible. They were encouraged to suggest changes that would make the problems harder for automated tools to solve while allowing easier or more elegant solutions for humans. The following examples illustrate this process:\nExample: Original Question: Find the smallest positive integer k such that k\u00b3 \u2212 k is divisible by both 9 and 10, and the sum of digits of k in its decimal representation is a prime number.\nOur human team had not encountered such questions before. It requires recognizing that \\(k\u00b3 \u2013 k = k(k-1)(k + 1)\\) is always divisible by 2 and 3. Thus, k must be such that \\(k(k - 1)(k + 1)/6\\) is divisible by 15 (both 3 and 5). Additionally, the sum of the digits of k must be a prime number, and ensuring such conditions is challenging even for powerful LLMs.\nExample: Original Question: Consider a collection of red, blue, and green beads arranged in an infinite series. The beads alternate in color, starting with red, then blue, then green, and this pattern repeats indefinitely. The number of beads in each colored section follows the pattern of powers of 2: the first red section has 2 beads, the first blue section has 4 beads, the first green section has 8 beads, the second red section has 16 beads, and so on. If a bracelet is made using a continuous, unbroken sequence of exactly 20 beads from this series, and each bead has a length of 0.5 units, how many different bracelets can be made such that the perimeter of the bracelet is an integer value?\nThe original question combined elements in a novel way. The human rater modified the question to change the sequence size from 20 to 6 beads, maintaining the essential difficulty while making it more elegant for humans. All tested models failed on the modified question.\nExample: Original Question: A container initially contains 500 mL of water. A scientist adds water to the container"}, {"title": "4.2 Failure Modes", "content": "Despite the sophistication of our pipeline, models frequently exhibit several failure modes: (a)\nInsufficient Involvement of Skills: Models often generate questions that either miss one of the skills\ncompletely or require a very shallow application of one or both skills. For example, a geometry\nquestion may fail to involve ratio and proportion adequately, (b) Insufficient Information: Questions\nmay lack essential details needed for solving, making them incomplete or ambiguous. For instance, a\ntrigonometry question might omit necessary angles or distances, (c) Unsolvable or Computationally\nIntractable Questions: Some questions generated are either unsolvable or require excessive brute-\nforce calculations, which are impractical for evaluating reasoning abilities, (d) Nonsensical Questions:\nModels sometimes produce questions that are logically inconsistent, confusing, or ambiguous, such\nas a probability problem with unclear parameters or an impossible geometry scenario, (e) Deceitful\nSolutions: Occasionally, models fabricate solutions to nonsensical or unsolvable questions, presenting\nincorrect logic as plausible reasoning and (f) Finding a Needle in the Haystack: Long and complex\nvalidation prompts sometimes cause models to confuse or overlook the specified skills, leading\nto incorrect evaluations. For a more detailed discussion and examples of questions in the various\ncategories listed above, refer to Appendix A.1."}, {"title": "5 Conclusions", "content": "We introduced a framework that leverages the complementary strengths of humans and AI to generate new, challenging mathematics questions. Building on recent insights into LLM metaknowledge, we use LLMs to extract and name key skills necessary for solving math problems. Using these"}, {"title": "6 Acknowledgements", "content": "This research used compute resources provided by Mila (mila.quebec) and GPT4 access as well as compute resources provided by Princeton Language and Intelligence (PLI). The Princeton partici-pants were funded by NSF, DARPA, and PLI. VS would like to thank Aniket Didolkar for helpful discussions throughout the project and for proof reading the paper. AG would like to thank Melvin Johnson, James McClelland and Yoram Bachrach for helpful discussions and useful feedback. AG would also like to thank Daan Wierstra, Melvin Johnson, Siamak Shakeri, Murray Shanahan, John Quan, Theophane Weber, Olivier Tieleman, David Silver, Charles Blundell, Behnam Neyshabur, Ethan Dyer and Nicolas Heess for support and guidance."}, {"title": "A Appendix", "content": "Here we further analyze the quirks of the question generation pipeline and provide additional experimental details and results. In Appendix A.1, we discuss several failure modes of the models that we notice during the question generation process as well as interesting behaviors exhibited by the models throughout the pipeline and interesting creative questions that the models came up with. Appendix A.2 discusses the different considerations that human annotators were instructed to take into account while annotating and verifying the questions generated by the proposed pipeline. Appendix A.3 provides details about the compute used for running our experiments as well as some further analysis of the model evaluations. In Appendix A.4 we provide examples of questions generated by different models in the Question Generation step (Section 2). Appendix A.5 gives a detailed description of the prompts used for each step in the question generation pipeline as well as evaluation of the models. It also provides a link to the skill exemplar repository and in-context exemplars used in the question generation process."}, {"title": "A.1 Failure Modes and Interesting Behaviors", "content": "Insufficient involvement of skills. Despite clearly specifying that solving the question should necessarily require a rigorous application of both skills, the models often generate questions that either miss one of the skills completely or require a very shallow application of one (while the other one is sufficiently involved) or both skills. This is the most prominent failure mode of the models in the context of question generation. This leads to potentially easy questions, defeating the purpose of skill composition. Consider the question given below which was generated by Claude Opus when asked to combine the skills ratio_and_proportion and geometry."}, {"title": "3.2 Performance across the two datasets: A surprising pattern", "content": "Table 2 shows that all tested models have significantly lower performance on MATH2 than on the original MATH dataset. Denoting Y as the performance on MATH2 and X as the performance on MATH, the percentage drop 100(X \u2013 Y)/X for frontier models GPT-4 Omni, GPT-4 Turbo, Gemini-1.5-Pro, Claude-3.5-Sonnet and Claude 3 Opus ranges from 13.42% to 35.45%. MAmmoTH-7B, a specialist math model, shows the largest drop at 92.91%.\nThe fact that performance drops for all models should not be too surprising, since as noted, the MATH2 questions, by combining skills from different subareas of MATH, could be seen as \"out of distribution (OOD).\u201d This makes it tempting to interpret the percentage drop as a measure of a model's (lack of) \"OOD-resilience.\" For instance, very large percentage drops seen with open-source models MetaMath and MAmmoTH feel understandable since their training used synthetic data generated using seed questions from MATH and GSM-8k. Lack of diversity in such synthetic data is known to cause overfitting to the dataset being imitated. Similarly, GPT-40 and Claude Sonnet 3.5 are suspected to also have been extensively trained with synthetic data. Although their MATH performance is similar, Sonnet 3.5 has worse MATH\u00b2 performance, which might suggest lower quality/diversity in its synthetic data.\nHowever, in our opinion, the overall pattern among proprietary models of similar size does fit with the OOD story. A much simpler explanation pops out when we plot Y vs X2 (Figure 3 and Figure 4(a)): we find a linear relationship Y \u2248 X2! This implies that the relative drop in performance of the models is well-predictable from just their performance on MATH, and does not require taking their training details into account!\nWhy should the two scores be expected to have this relationship? Here is a natural (albeit heuristic) explanation. Suppose there are N skills and si denotes the success rate of the model at correctly applying the ith skill. Then, its X value should reflect the average of the si's. Furthermore, on a random question using the ith and jth skill, the probability that the model correctly answers it should"}]}