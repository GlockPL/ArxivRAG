{"title": "AI-Assisted Generation of Difficult Math Questions", "authors": ["Vedant Shah", "Dingli Yu", "Kaifeng Lyu", "Simon Park", "Nan Rosemary Ke", "Michael Mozer", "Yoshua Bengio", "Sanjeev Arora", "Anirudh Goyal"], "abstract": "Current LLM training positions mathematical reasoning as a core capability. With publicly available sources fully tapped, there is an unmet demand for diverse and challenging mathematics questions. Relying solely on human experts is both time-consuming and costly, while LLM-generated questions often lack the requisite diversity and difficulty. We present a design framework that combines the strengths of LLMs with a human-in-the-loop approach to generate a diverse array of challenging math questions. Initially, leveraging LLM metacognition skills [Didolkar et al., 2024], a strong LLM is used to extract core \u201cskills\u201d from existing math datasets. These skills serve as the basis for generating novel and difficult questions by prompting the LLM with random pairs of core skills that must be utilized in the question. The use of two very different skills within each question makes finding such questions an \"out of distribution\" task for both LLMs and humans. Our pipeline employs LLMs to iteratively generate and refine questions and solutions through multi-turn prompting. Human annotators then verify and further refine the questions, with their efficiency enhanced via further LLM interac-tions. Applying this pipeline on skills extracted from MATH dataset [Hendrycks et al., 2021] resulted in MATH2 - a dataset of higher quality math questions, as evidenced by: (a) Lower performance of all models on MATH2 than on MATH (b) Higher performance on MATH when using MATH2 questions as in-context examples. Although focused on mathematics, our methodology seems applicable to other domains requiring structured reasoning, and potentially as a component of scalable oversight. Also of interest is a striking relationship observed between models' performance on the new dataset: the success rate on MATH\u00b2 is the square on MATH. This suggests that successfully solving the question in MATH2 requires a nontrivial combination of two distinct math skills.", "sections": [{"title": "1 Introduction", "content": "Significant improvement in the capabilities of LLMs [Chowdhery et al., 2023, Anil et al., 2023, Team, 2023, Team et al., 2023, Abdin et al., 2024, Achiam et al., 2023, Touvron et al., 2023] to understand and generate complex mathematical content has been achieved by leveraging all the public data and a fair bit of private data. Sources of high-quality, varied, and difficult mathematical questions are drying up. Even finding new questions for evaluation is getting difficult since newly-released human exams are somewhat similar to past exams, which are potentially present in the LLMs' training datasets. Hence, there is a pressing need for innovative methods to create new, diverse, and challenging questions."}, {"title": "1.1 Evaluation Saturation Phenomenon", "content": "LLM evaluations getting saturated is a well-known issue. Some of the saturation is driven by across-the-board improvements arising from better training and more extensive/better datasets. But a lot has to do with evaluation-specific enhancements that optimize model performance on standard evaluations through techniques like supervised fine-tuning (SFT) on synthetic question-answer pairs. These synthetic pairs can be generated by leading proprietary models when provided with a few examples from the dataset or by filtering the model's own responses [Yue et al., 2023, Yu et al., 2023b]. Such methods can dramatically boost performance; for example, just 1 million synthetic examples can elevate Llama2 7B's performance on the MATH dataset to levels comparable to GPT-4 [Li et al., 2024].\nThe distinction between general and evaluation-specific improvements is crucial. The latter may lead to overfitting to particular evaluations rather than a genuine acquisition of mathematical skills. This issue was highlighted when a new version of the GSM8K dataset revealed performance drops in many models, indicating overfitting to the previous dataset version [Zhang et al., 2024]. Similarly, leading LLMs performed significantly worse on newer versions of the Chinese GaoKao exam compared to older exams, raising fundamental questions about the depth of their mathematical understanding."}, {"title": "1.2 Proposed Framework: AI-assisted Generation of Difficult Math Questions", "content": "At first glance, it may seem counterintuitive to use an AI model to generate and correct novel questions that it is unable to solve itself. However, recent research [Arora and Goyal, 2023, Didolkar et al., 2024] demonstrated that top LLMs possess a robust understanding of mathematical skills, including the capability to identify the skills required to solve given questions [Reid et al., 2024, Achiam et al., 2023]. This naturally raises the question: can LLMs operate in the reverse direction, i.e., generate math problems when given a list of skills that have to be tested? Our initial attempts yielded mixed results. While leading models could produce creative math questions when provided with a list of skills, the majority of these questions exhibited one or more of the following shortcomings: too similar to existing questions in datasets; have errors or nonsensical elements; are too tedious or mechanical to be engaging for human annotators. (See Section 4.) Moreover, they often conflate \"difficulty\" with tedious calculations, which actually would play to the strength of machines to leverage external tools such as calculators or Python interpreters.\nNevertheless, there were promising instances where LLMs generated interesting and correct questions that they were unable to solve, due to incomplete or incorrect reasoning. This observation led us to the concept of Al-assisted creation of evaluation datasets. Our process may also be of interest for human pedagogy since it begins with the extraction of core \"skills\" from existing math datasets, which serve as the foundational elements of mathematical questions. The current paper focuses on the MATH dataset [Hendrycks et al., 2021], a mainstay of LLM evaluation in recent years.\nStarting with a list of mathematical skills extracted from the MATH dataset using recently discovered methods [Didolkar et al., 2024], we focused on creating questions that involve one skill from pre-algebra and algebra portions of the MATH dataset and one other skill randomly sampled from different sections of MATH. Our generation pipeline uses carefully crafted prompts and multi-turn"}, {"title": "2 Pipeline for AI-Assisted Question Generation", "content": "We present a structured approach to generating challenging mathematics questions by combining the capabilities of large language models (LLMs) and human expertise. Given below is a high-level overview of the process before delving into the details of each step.\nWe begin our pipeline with skill extraction - identifying and cataloging distinct mathematical skills from a dataset, as described in Didolkar et al. [2024]. This step creates a repository of skills linked to specific questions. The motivation behind this is to systematically generate and analyze questions that require specific skills, ensuring a comprehensive evaluation framework.\nNext, we focus on generating questions that combine pairs of distinct skills to increase their difficulty. By using advanced models like GPT-4 and Claude, and incorporating in-context examples of multi-way interactions between AI and humans, we enhance the models' performance in generating complex questions. This step aims to produce challenging questions that robustly assess problem-solving abilities.\nThe final step involves screening and validation to filter out invalid or flawed questions. This rigorous process includes evaluating and solving the questions to identify hidden flaws, such as computational intractability or logical inconsistencies. Advanced techniques like in-context exemplars and self-consistency further ensure the accuracy and quality of the solutions. This step is crucial for maintaining the integrity and reliability of the generated questions and their solutions. Overall, each step in the pipeline is designed to systematically enhance the quality and difficulty of questions, providing a robust and comprehensive evaluation of mathematical skills."}, {"title": "3 Experiments and Findings", "content": "Through our experiments, we demonstrate the difficulty and quality of the MATH2 while also analyzing the behavior of different models on this task of compositional generalization. Firstly, we evaluate a wide range of models spanning a large range of parameter counts on MATH2 and compare against their performance on MATH [Hendrycks et al., 2021] which is the base dataset"}, {"title": "3.1 Experimental Setup", "content": "We follow the pipeline proposed in [Didolkar et al., 2024] to extract skills from the MATH dataset [Hendrycks et al., 2021]. The MATH dataset encompasses seven high-level topics, allowing us to identify and extract finer-grained skills within each topic and label each question accordingly. At the end of the skill-extraction process, we identify a set of 114 skills. We then remove a few simple skills, such as basic_arithmetic and arithmetic_operations, before using the remaining set to generate questions using the proposed approach. We generate and verify 180 difficult questions to create the MATH\u00b2 dataset. Figure 5 shows the distribution of skills in MATH2."}, {"title": "4 Observations from the Question Generation Process", "content": "The question generation pipeline described in Section 2 was developed through an iterative process of refining prompts and design choices, and evaluating their impact on the quality of the final questions and solutions. Notably, the inclusion of the attempted solution and question validation steps significantly enhanced the pipeline's effectiveness. Despite the sophistication of the pipeline and prompts, we still observe instances where models fail to follow the given instructions. This section highlights prominent failure modes at various stages of the pipeline, which human raters need to be aware of. Additionally, we explore some intriguing behaviors of the models where they successfully create interesting and creative questions. Section 4.1 details the role of human raters in improving these questions."}, {"title": "4.1 Creative questions: Examples of Synergy from Human-AI interaction", "content": "The models frequently produced interesting and creative questions, although they often failed to generate correct solutions. In these cases, the incorrect solutions usually contained enough correct ideas for a human to quickly complete them.\nHuman annotators were tasked with verifying the validity of the questions and the correctness of the solutions. They were instructed to look out for any failure modes discussed in Section 4.2. Their responsibilities included ensuring that the created questions actually employed the intended math skills, and improving the questions in terms of readability, quality, and difficulty when possible. They were encouraged to suggest changes that would make the problems harder for automated tools to solve while allowing easier or more elegant solutions for humans. The following examples illustrate this process:\nExample: Original Question: Find the smallest positive integer k such that $k^3-k$ is divisible by both 9 and 10, and the sum of digits of $k$ in its decimal representation is a prime number.\nOur human team had not encountered such questions before. It requires recognizing that $k^3 \u2013 k = k(k-1)(k + 1)$ is always divisible by 2 and 3. Thus, k must be such that $k(k - 1)(k + 1)/6$ is divisible by 15 (both 3 and 5). Additionally, the sum of the digits of k must be a prime number, and ensuring such conditions is challenging even for powerful LLMs.\nExample: Original Question: Consider a collection of red, blue, and green beads arranged in an infinite series. The beads alternate in color, starting with red, then blue, then green, and this pattern repeats indefinitely. The number of beads in each colored section follows the pattern of powers of 2: the first red section has 2 beads, the first blue section has 4 beads, the first green section has 8 beads, the second red section has 16 beads, and so on. If a bracelet is made using a continuous, unbroken sequence of exactly 20 beads from this series, and each bead has a length of 0.5 units, how many different bracelets can be made such that the perimeter of the bracelet is an integer value?\nThe original question combined elements in a novel way. The human rater modified the question to change the sequence size from 20 to 6 beads, maintaining the essential difficulty while making it more elegant for humans. All tested models failed on the modified question.\nExample: Original Question: A container initially contains 500 mL of water. A scientist adds water to the container $\\frac{1}{4}$"}, {"title": "4.2 Failure Modes", "content": "Despite the sophistication of our pipeline, models frequently exhibit several failure modes: (a) Insufficient Involvement of Skills: Models often generate questions that either miss one of the skills completely or require a very shallow application of one or both skills. For example, a geometry question may fail to involve ratio and proportion adequately, (b) Insufficient Information: Questions may lack essential details needed for solving, making them incomplete or ambiguous. For instance, a trigonometry question might omit necessary angles or distances, (c) Unsolvable or Computationally Intractable Questions: Some questions generated are either unsolvable or require excessive brute-force calculations, which are impractical for evaluating reasoning abilities, (d) Nonsensical Questions: Models sometimes produce questions that are logically inconsistent, confusing, or ambiguous, such as a probability problem with unclear parameters or an impossible geometry scenario, (e) Deceitful Solutions: Occasionally, models fabricate solutions to nonsensical or unsolvable questions, presenting incorrect logic as plausible reasoning and (f) Finding a Needle in the Haystack: Long and complex validation prompts sometimes cause models to confuse or overlook the specified skills, leading to incorrect evaluations. For a more detailed discussion and examples of questions in the various categories listed above, refer to Appendix A.1."}, {"title": "5 Conclusions", "content": "We introduced a framework that leverages the complementary strengths of humans and AI to generate new, challenging mathematics questions. Building on recent insights into LLM metaknowledge, we use LLMs to extract and name key skills necessary for solving math problems. Using these"}]}