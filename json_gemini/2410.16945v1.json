{"title": "IdenBAT: Disentangled Representation Learning for Identity-Preserved Brain Age Transformation", "authors": ["Junyeong Maeng", "Kwanseok Oh", "Wonsik Jung", "Heung-Il Suk"], "abstract": "Brain age transformation aims to convert reference brain images into synthesized images that accurately reflect the age-specific features of a target age group. The primary objective of this task is to modify only the age-related attributes of the reference image while preserving all other age-irrelevant attributes. However, achieving this goal poses substantial challenges due to the inherent entanglement of various image attributes within features extracted from a backbone encoder, resulting in simultaneous alterations during the image generation. To address this challenge, we propose a novel architecture that employs disentangled representation learning for identity-preserved brain age transformation called IdenBAT. This approach facilitates the decomposition of image features, ensuring the preservation of individual traits while selectively transforming age-related characteristics to match those of the target age group. Through comprehensive experiments conducted on both 2D and full-size 3D brain datasets, our method adeptly converts input images to target age while retaining individual characteristics accurately. Furthermore, our approach demonstrates superiority over existing state-of-the-art regarding performance fidelity.", "sections": [{"title": "1. Introduction", "content": "Brain aging represents an intrinsic biological phenomenon marked by discernible morphological changes within the human brain Fjell and Walhovd (2010). In the analysis of brain aging using medical imaging, structural magnetic resonance imaging (sMRI) plays a crucial role as they provide detailed insights into age-related variations and assist in accurate assessments of these alterations. Advances in sMRI-based age transformation have especially allowed researchers and clinicians to visualize and quantify patient-specific intricate brain maturation and degeneration patterns, facilitating medical diagnosis advancements. These capabilities can be pivotal for longitudinal studies to track cognitive or health state progressions over time Cole, Ritchie, Bastin, Hern\u00e1ndez, Mu\u00f1oz Maniega, Royle, Corley, Pattie, Harris, Zhang et al. (2018); Huizinga, Poot, Vernooij, Roshchupkin, Bron, Ikram, Rueckert, Niessen, Klein, Initiative et al. (2018), whereas brain age transformation with preserving patient traits remains a formidable challenge. Because most methods even change characteristics unrelated to aging during the transformation process, the crux lies in modeling the aging process without distorting personal identities intrinsic to each subject Xia, Chartsias, Wang, Tsaftaris, Initiative et al. (2021). When the aging model fails to preserve personal properties regarding identity, it may lead to misinterpretations of age-related changes, potentially compromising the accuracy and reliability of diagnostic decisions.\nPrevious brain age transformation studies Huizinga et al. (2018); Zhang, Shi, Wu, Wang, Yap and Shen (2016); Zhao, Adeli, Honnorat, Leng and Pohl (2019); Lorenzi, Pennec, Frisoni, Ayache, Initiative et al. (2015); Sivera, Delingette, Lorenzi, Pennec, Ayache, Initiative et al. (2019) have often relied on prototype-based strategies that compare averaged brain patterns across different age groups. While these approaches aid in understanding generalized characteristics shared among age groups, they tend to neglect the unique traits of individual subjects. Recently, with the emergence of generative models using longitudinal data Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville and Bengio (2014); Makhzani, Shlens, Jaitly, Goodfellow and Frey (2015), researchers have gained the ability to create more accurate and realistic simulations of brain aging by virtue of the advantages of its data, which comprised MRI scans of the same subject at multiple time points Rachmadi, del C. Vald\u00e9s-Hern\u00e1ndez, Makin,"}, {"title": "2. Related Work", "content": "Traditional brain aging studies encompass biological-based and prototype-based methods. Biological-based methods Khanal, Ayache and Pennec (2017) analyze images' biological and physical properties, such as MRI volume changes, to convert age. However, these models often require precise human knowledge or extensive longitudinal data. Conversely, prototype-based methods Zhang et al. (2016); Huizinga et al. (2018); Sivera et al. (2019) cluster images into age groups and calculate the average prototype of each group. While effective, these approaches overlook the intrinsic attributes of individual images by relying on representative averaged images.\nWith the advent of deep learning, deep generative model-based methods for brain age transformation have emerged, showcasing substantial advancements surpassing traditional approaches. Rachmadi et al. (2019, 2020); Wegmayr et al. (2019) utilize generative adversarial network (GAN) Goodfellow et al. (2014)-based architectures to capture structural changes in MRI scans, while Ravi et al. (2019) employs conditional adversarial autoencoder (CAAE) Zhang, Song and Qi (2017) to synthesize aged brain images. However, these methods typically require longitudinal data for model training. Zhao et al. (2019); Pawlowski, Coelho de Castro and Glocker (2020) utilize variational autoencoder (VAE) Kingma and Welling (2013) for generating age-converted brain images, but often produce blurred results and lack auxiliary techniques for preserving individual identity. In contrast, Xia et al. (2019, 2021) utilize a cGAN-based framework to generate aged brain images, although limited to synthesizing sliced 2D MRI scans. More recently, Pombo et al. (2023) proposes a conditional generative model employing diffeomorphic deformations to age-convert images based on the StarGAN Choi et al. (2018) framework. However, these methods face challenges in adequately preserving the identity of the input subject."}, {"title": "2.2. Identity Preservation in Age Transformation", "content": "Age transformation studies, encompassing both face and brain aging, have evolved with a focus on preserving the identity of the input subject throughout the age conversion process. Conventional aging models attempt to maintain the subject's identity by employing a direct reconstruction loss at the image level. For instance, CAAE Zhang et al. (2017) utilizes L2 loss between reference and generated images to preserve independent traits. However, this objective function presents a fundamental issue in generating blurry images due to incomplete gradient updates for sharpness. As an alternative approach, Xia et al. (2019, 2021); He, Kan, Shan and Chen (2019) adopt a pixel-wise L1 loss, resulting in sharper synthesized images. Nonetheless, they still face the limitation that the generated image does not guarantee perceptual quality.\nIn contrast, IPCGAN Wang, Tang, Luo and Gao (2018) proposes a perceptual loss that considers human perceptions in image similarity, utilizing a pre-trained AlexNet Krizhevsky, Sutskever and Hinton (2012) to aid identity conservation. Although this approach yields generated images closer to the reference image in perceptual criteria, it encounters limitations due to reliance on an auxiliary classifier pre-trained on ImageNet Deng, Dong, Socher, Li, Li and Fei-Fei (2009) rather than being specifically trained for the desired domain of age transformation. As an alternative, recent studies on face aging Maeng et al. (2023); Li, Wang, Huang, He and He (2023) adopt face identity classifier Ubayashi et al. (2010) or disentangled representations containing the reference image's characteristics Li, Huang, Hu, Wu, He and Sun (2020); Huang, Zhang and Shan (2021). While these methods may effectively preserve identity, their application to brain age transformation is hindered by the requirement for numerous longitudinal data.\nIn our study, we preserve the identity of the input subject by disentangling the representation extracted from an encoder into age-related and age-irrelevant features. The age-irrelevant features are then updated using a disentangled representation learning strategy. As a result, without necessitating longitudinal data or a pre-trained identity verification module, IdenBAT adeptly preserves identity while enabling precise age transformation to the target ages."}, {"title": "3. Methods", "content": "The primary objective of IdenBAT is to transform the image precisely to the target age while meticulously preserving the intrinsic identity of the input brain image. We design the identity extracting module (IEM) to disentangle intertwined semantic information extracted from the input image to realize this objective. Leveraging the isolated identity feature, we incorporate age conditions into the age injecting module (AIM) to synthesize age-converted images. Additionally, employing a conditional discriminator and age classifier, our aging synthesizer more accurately captures age changes and enhances visual quality. The overall framework of our proposed method is illustrated in Fig. 2."}, {"title": "3.1. Age Transformer", "content": "Given an input image $X \\in [R^{H\\times W \\times D}$ and a randomly selected target age $a_t$, our age transformer $T$ aims to synthesize an age-transformed image $X \\in R^{H\\times W \\times D}$ as follows:\n$X := T (X, a_t)$.                                                                                                                   (1)\nThe aged transformer, structured as a U-Net Ronneberger, Fischer and Brox (2015), comprises an encoder $\\mathcal{E}$, a generator $\\mathcal{G}$, IEM, and AIM. Fig. 3 illustrates the detailed architecture of IEM and AIM.\nThe role of the encoder $\\mathcal{E}$ is to extract entangled features that predominantly represent the personal age of the reference image, denoted as $\\mathcal{F}_{age} = {\\mathcal{F}^{age}_1, ..., \\mathcal{F}^{age}_l}$, where ${\\mathcal{F}^{age}_l}_{l=1}^{L}$ represents the feature maps of the l-th convolution blocks in the encoder $\\mathcal{E}(X)$. Each feature map $\\mathcal{F}^{age}_l \\in R^{H'\\times W'\\times D'}$ undergoes size reduction from $H \\times W \\times D$ through max pooling. The encoder is updated using an age prediction loss, with the current age of the input image serving as the ground truth. This ensures that the age feature $\\mathcal{F}_{age}$ accurately reflects the subject's actual age in the image.\nThe age transformation process involves altering the condition while inherently preserving the individual's unique characteristics. To seamlessly maintain identity while incorporating the age condition in the age transformer $T$, our objective is to disentangle the identity feature from the age feature. This disentanglement is essential to ensure that the model effectively captures the essence of age conversion without compromising the distinct identity of the subject. To achieve this, we extract the identity feature $\\mathcal{F}_{iden} = {\\mathcal{F}^{iden}_1, ..., \\mathcal{F}^{iden}_l}$ from the entangled age feature $\\mathcal{F}_{age}$ through the IEM, which will be elaborated on the following subsection.\nWe utilize a style transfer method for age conditioning in AIM to incorporate information about the target age into the identity feature. Here, AIM = {$AIM^1, ..., AIM^l$} consists of a series of conditional batch normalization (CBN) De Vries, Strub, Mary, Larochelle, Pietquin and Courville (2017) blocks of 1-th layers, represented as:\n$AIM^l = (Conv \\rightarrow BN \\rightarrow DN \\rightarrow PReLU) \\times 2$.                      (2)\nBN and DN represent the batch normalization and denormalization processes, respectively. The target age $a_t$ is incorporated into $a_e$ using a mapping network $\\mathcal{M}$, comprising eight fully-connected layers with LeakyReLU activation function, in line with StyleGAN2 Karras et al. (2020). The embedded age condition is fused into the identity feature $\\mathcal{F}_{iden}$ through CBN blocks. Specifically, the 1-th identity feature $\\mathcal{F}^{iden}_l$ undergoes batch normalization, standardizing the features batch-wise and mapping them into a specific normalized space. Subsequently, these normalized features undergo denormalization, wherein the target age representation $a_e$ is incorporated by adjusting the mean and standard deviation parameters of the denormalization as:\n$\\mathcal{F}^{out}_l = AIM^l(\\mathcal{F}^{iden}_l, a_e)$.                                                                                                        (3)"}, {"title": "3.2. Identity Extracting Module", "content": "Recent advancements in brain age transformation Xia et al. (2019, 2021); Pombo et al. (2023) have seen approaches directly synthesizing age-transformed images based on features extracted from an encoder containing various attributes of the input image. However, this poses a substantial challenge in selectively preserving specific features, particularly those associated with the subject's identity. To address this challenge, we propose disentangling the identity feature $\\mathcal{F}_{iden}$ from the age-related feature $\\mathcal{F}_{age}$ as $\\mathcal{F}_{iden} = IEM(\\mathcal{F}_{age})$. Similar to the structure of AIM, the 1-th layers of IEM = {$IEM^1, ..., IEM^l$} comprise two convolution blocks:\n$IEM^l = (Conv \\rightarrow BN \\rightarrow PReLU) \\times 2$.                                                                                      (5)\nWe employ two distinct loss functions based on cosine similarity measures to guide IEM in effectively extracting $\\mathcal{F}_{iden}$ through convolution operations. First, considering that the input image $X$ and age-converted image $\\hat{X}$ differ solely in the age condition, maximizing the cosine similarity between their identity features enables updating IEM to preserve the input subject's identity. Subsequently, orthogonal projection between $\\mathcal{F}_{age}$ and $\\mathcal{F}_{iden}$ is performed by making their cosine similarity zero. These disentangled representation learning strategies enable the convolution blocks of IEM to effectively distinguish features intrinsic to the aging process from unrelated ones, thereby enhancing the model's ability to preserve identity information.\nA primary concern when manipulating U-Net-based architecture bottleneck features is the potential information loss in skip connections. Specifically, suppose feature disentanglement is solely conducted at the last feature (i.e., bottleneck) of the encoder $\\mathcal{E}$ without a similar process in the skip connections linked to the generator $\\mathcal{G}$. In that case, there is a risk of information loss. To address this issue, we designed AIM and IEM to perform identity feature disentanglement and target age conjunction at each l-th skip connection and bottleneck as:\n$\\mathcal{F}^{out}_l = AIM'(IEM'(\\mathcal{F}^{age}_l), a_e)$.                                                                                                  (6)\nThis scheme ensures that feature disentanglement and age injection are consistently applied across all levels of the skip connections, as depicted in Fig. 3."}, {"title": "3.3. Conditional Discriminator", "content": "Our model introduces a novel approach to brain age transformation by integrating target age conditions into the discriminator. This conditioning method substantially augments the discriminator's ability to authenticate the generated images and evaluate their alignment with the specified age conditions. To accomplish this, we have devised a conditional discriminator $\\mathcal{D}$ that plays a crucial role in the adversarial learning process. The architecture of $\\mathcal{D}$ draws inspiration from PatchGAN Isola, Zhu, Zhou and Efros (2017), comprising a series of convolution blocks with a critical adaptation to incorporate age conditioning effectively. Specifically, we integrated CBN within the first convolution block of $\\mathcal{D}$ to facilitate the injection of age conditions. Similar to the approach in AIM, the age condition $a_t$ undergoes processing through the mapping network $\\mathcal{M}$, followed by denormalization into the first feature of the discriminator, thus conveying the target age information. By employing this sophisticated conditioning mechanism, the discriminator can critically evaluate the age accuracy of the generated images, thereby enhancing the model's overall ability to produce age-authentic transformations."}, {"title": "3.4. Objectives", "content": "We outline the essential objective functions to optimize our proposed network, including age prediction, identity-extracting, cycle-consistency, reconstruction, and adversarial losses."}, {"title": "3.4.1. Age Prediction Loss", "content": "We leverage age prediction loss to extract the age representation from the input image and further facilitate the age transformer $T$ in accurately converting the given image to the target age.\nFirst, we minimize a Kullback\u2013Leibler (KL) divergence loss function between the predicted probability of the encoder $\\mathcal{E}$ and a Gaussian distribution, where the mean corresponds to the age of the input image $a_i$ and the standard deviation is set to one following Peng, Gong, Beckmann, Vedaldi and Smith (2021):\n$\\mathcal{L}_{age_1} = L_{KL}(\\mathcal{E}(X), a_i)$.                                                                                                (7)\nHere, $L_{KL}$ denotes the KL divergence loss. Once the encoder is updated to predict the age of the given image, we utilize the frozen encoder $\\mathcal{E^*}$ to guide the age transformer $T$ to accurately synthesize an age-converted image $\\hat{X}$ through another age prediction loss as:\n$\\mathcal{L}_{age_2} = L_{KL}(\\mathcal{E^*}(\\hat{X}), a_t)$,                                                                                                (8)\nwhere $\\hat{X} = T(X, a_t)$ and $a_t$ denotes the target age condition."}, {"title": "3.4.2. Identity-Extracting Loss", "content": "To effectively disentangle the representation of identity from the input subject, we propose an identity-extracting loss that integrates both cosine similarity and orthogonality losses. Inspired by recent advancements in self-supervised learning methods Chen and He (2021); Grill, Strub, Altch\u00e9, Tallec, Richemond, Buchatskaya, Doersch, Avila Pires, Guo, Gheshlaghi Azar et al. (2020), we consider the age-converted image $\\hat{X}$ as an augmented view of the input image $X$. By maximizing the cosine similarity between the identity features $\\mathcal{F}_{iden}$ extracted from this pair, the IEM is trained to extract identity features seamlessly, independent of age information. Additionally, we employed an orthogonality loss between $\\mathcal{F}_{age}$ and $\\mathcal{F}_{iden}$ to fully disentangle identity information from the entangled representation $\\mathcal{F}_{age}$. This is achieved by enforcing the cosine similarity between $\\mathcal{F}_{age}$ and $\\mathcal{F}_{iden}$ to be zero. The final identity-extracting loss, which combines the cosine similarity loss and the orthogonality loss, is defined as follows:\n$\\mathcal{L}_{iden} = -<\\mathcal{F}_{iden}, \\mathcal{F}_{iden}> + |<\\mathcal{F}_{iden}, \\mathcal{F}_{age}>|$.                                                                        (9)\nwhere $\\mathcal{F}_{iden}$ represents the age-irrelevant feature of the age-converted image $\\hat{X}$ and $<\\cdot>$ denotes cosine similarity."}, {"title": "3.4.3. Cycle-Consistency Loss", "content": "To ensure identity preservation across age transformations within our model, we incorporate the concept of cycle-consistency loss. This loss mechanism plays a vital role in maintaining the core identity features of brain images as they undergo age progression and regression cycles. The process involves transforming an image of the current age $X$ to its target age, denoted as $\\hat{X} = T(X, a_t)$, and subsequently reverting this age-transformed image to its original age using the age condition of input image $a_i$. The cycle-consistency loss is then computed as the L1 norm between the original image and the twice-transformed image, as depicted below:\n$\\mathcal{L}_{cyc} = ||X - T(\\hat{X}, a_i)||_1$.                                                                                                          (10)\nThis approach emphasizes the importance of maintaining the semantic integrity of brain images throughout the transformation process. It safeguards against potential issues such as mode collapse, where the model might learn to map multiple age-varied inputs to a single output representation."}, {"title": "3.4.4. Reconstruction Loss", "content": "We further incorporate a reconstruction loss to uphold the structural integrity of brain images throughout the age transformation process. This loss mechanism involves applying an L2 norm-based reconstruction loss between the input image $X$ and the synthesized age-transformed image $\\hat{X}$, effectively quantifying the discrepancy in their structural details. This loss is augmented by a weighting strategy Maeng et al. (2023) that considers the absolute age difference between the input and target ages, as follows:\n$\\mathcal{L}_{rec} = (\\beta \\cdot cos(\\pi \\cdot \\Delta_{age}) + (1 - \\beta)) \\cdot ||X - T(X, a_t)||_2$.                                                                      (11)"}, {"title": "3.4.5. Adversarial Loss", "content": "In the final stage of our training strategy, we implemented an adversarial loss to facilitate GAN training. Leveraging the previously discussed conditional discriminator, our model adheres to the least square GAN (LSGAN) Mao, Li, Xie, Lau, Wang and Paul Smolley (2017) methodology, employing a mean squared error (MSE) loss for the GAN objective. This choice aims to enhance the age transformer's ability to generate age-converted images that faithfully reflect the desired age characteristics, thereby minimizing the distance from the discriminator's decision boundary.\nThe loss functions for adversarial learning are defined as follows:\n$\\mathcal{L}_{adv_D} = \\frac{1}{2}E_{X\\sim P_x}[(D(X, a_t) - 1)^2] + \\frac{1}{2}E_{\\hat{X}\\sim P_g}[(D(\\hat{X}, a_t))^2]$,                                                                  (12)\n$\\mathcal{L}_{adv_G} = E_{X\\sim P_x}[(D(\\hat{X}, a_t) \u2013 1)^2]$,                                                                                          (13)\nwhere $X$ represents the randomly selected real samples corresponding to the target age, and $P_x$ denotes data distribution.\nThe training of our model is conducted in an end-to-end manner, updating all objectives jointly to balance the contributions of each loss component:\n$min \\mathcal{L}_{adv} + \\lambda_{age}(\\mathcal{L}_{age_1} + \\mathcal{L}_{age_2}) \\ \\ \\ \\ +\\lambda_{iden}\\mathcal{L}_{iden} + \\lambda_{cyc}\\mathcal{L}_{cyc} + \\lambda_{rec}\\mathcal{L}_{rec}$,                                                                  (14)\n$min \\mathcal{L}_{adv_D}$,                                                                                                                                  (15)"}, {"title": "4. Experiments", "content": "We utilized the UK Biobank dataset to validate the effectiveness of our proposed IdenBAT. This dataset comprises 49,123 T1 MRIs from individuals with healthy conditions. We selected 7,590 images from this dataset for both 2D and 3D training data, covering ages ranging from 48 to 80. We randomly sampled 230 images for each age group to mitigate potential class imbalance issues. Additionally, we employed full-size 3D images with a resolution of 193 \u00d7 229 \u00d7 193. For the 2D images, we selected the 97th axial slice from the preprocessed images, corresponding to the central plane of the axial view. This resulted in 2D images with a resolution of 229 \u00d7 193, as per the methodology outlined in Xia et al. (2019). To assess the efficacy of our proposed framework comprehensively, we gathered longitudinal test data based on baseline subjects. This longitudinal dataset comprised 982 pairs of images with an age gap exceeding two years.\nInitially, we conducted neck removal from the raw brain images using the \u201crobustfov\u201d tool provided in the FMRIB Software Library (FSL) Jenkinson, Beckmann, Behrens, Woolrich and Smith (2012). Subsequently, we applied HD-BET Isensee, Schell, Pflueger, Brugnara, Bonekamp, Neuberger, Wick, Schlemmer, Heiland, Wick et al. (2019) for brain extraction to remove non-brain tissue from the entire head image. The resulting skull-stripped images were then registered to the MNI152 template using the FLIRT tool for linear registration. Finally, we obtained preprocessed 3D MRI images with dimensions of 193 \u00d7 229 \u00d7 193, following the default parameters of the preprocessing toolkit."}, {"title": "4.2. Implementation Details", "content": "To illustrate the scalability of our proposed IdenBAT, we conducted experiments utilizing both 2D and 3D brain images. We implemented center cropping to reduce unnecessary computational costs, reducing the model's input size to 176 x 208 x 176. For the 2D data, we removed redundant background areas through cropping, resulting in images of 208 \u00d7 176 resolution. In both the 3D and 2D brain aging models, we employed the Adam optimizer Kingma and Ba (2014) with a learning rate of 10\u20133 for the encoder, 5 \u00d7 10\u20134 for the generator, AIM, and discriminator. We utilized a learning rate of 10-5 to update the mapping network and IEM. To mitigate the risk of overfitting, we utilized a StepLR scheduler with a step size of 30 and a gamma of 0.3. Additionally, we employed an augmentation strategy during training, including random flipping (horizontal flip for 2D and sagittal flip for 3D images) and Gaussian blurring for the encoder. The weights for the loss functions dadv, dage, Aiden, Acyc, and drec were set to 1, 0.05, 1, 0.1, and 0.1, respectively. For the hyperparameter setting within dage for Eq. (11), we defined the value of r as 33. We trained our IdenBAT for 50 epochs with a batch size of 4 using 3D data and for 200 epochs with a batch size 64 using 2D data, utilizing two NVIDIA RTX A6000 GPUs."}, {"title": "4.3. Baseline Models", "content": "We conducted a comprehensive comparative analysis of our IdenBAT against several state-of-the-art brain age transformation models with official implementations. For the 2D brain age transformation task, we selected CGAN Mirza and Osindero (2014), CAAE Zhang et al. (2017), CBAS Xia et al. (2019), and LSAB Xia et al. (2021) as baseline models. Since cGAN utilizes its input as the noise vector, we employed a U-Net-based encoder for conditional image-to-image translation. However, it is important to note that these models were primarily designed for 2D images, which are slices of the original 3D brain images. Consequently, they did not produce satisfactory results when applied to 3D brain age transformation tasks. For the comparison involving full-size 3D brain age transformation, we included CounterSynth Pombo et al. (2023) as the sole model. This choice reflects the substantial challenge of completely transforming brain age using full-size 3D models. We emphasize that modeling brain age transformation using full-size 3D data poses considerable difficulties and requires specialized techniques."}, {"title": "4.4. Evaluation Metrics", "content": "For the quantitative evaluations of identity preservation and accurate age conversion, we employed the four primary metrics: peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), mean squared error (MSE), and predicted age difference (PAD).\nSpecifically, PSNR quantifies the difference between the original and generated images by calculating the logarithmic ratio of the peak signal power to the noise power. A higher PSNR value indicates better quality and closer resemblance to the original image. Unlike PSNR, which focuses on pixel-level differences, SSIM assesses the perceived quality by comparing local patterns of pixel intensities normalized for luminance and contrast. MSE provides a straightforward measure of error magnitude, with lower values indicating higher image similarity. Finally, PAD is a metric designed to evaluate the accuracy of the age transformation. We utilize a pretrained age classifier Peng et al. (2021) to assess how closely the age-transformed images match the target age. By calculating the difference between the predicted age of the generated image and the target age, PAD provides a direct measure of the model's ability to transform age accurately."}, {"title": "4.5. Qualitative Evaluation", "content": "We utilized ground truth images to evaluate the accuracy of the age transformation process. For this purpose, longitudinal data from the UK Biobank was obtained, comprising MRI scans of the same subjects on two different occasions. We generated difference maps from the ground truth longitudinal data and those produced by various age transformation models, as depicted in Fig. 1. Notably, the synthesized difference maps from our proposed IdenBAT, encompassing both 2D and 3D, closely resemble those of the ground truth (highlighted by the red box). In contrast, while some comparative models (CBAS and LSBA) show only minor age changes for small age gaps (e.g., five years), others, like CGAN, synthesize artifacts. Meanwhile, CAAE and CounterSynth fail to preserve the subject's structural identity. These qualitative findings highlight that models producing images most similar to the second set of images from longitudinal data are more accurate in reflecting an individual's aging process, closely aligning with genuine brain aging. In essence, our proposed IdenBAT effectively preserves the unique characteristics of the input image while precisely transforming it to the target ages.\nHowever, this experimental comparison using ground truth data has limitations in explaining transformations across large age gaps. To address this, we visualize the age-converted images and their difference maps over a wider age range"}, {"title": "5. Model Interpretability and Ablation Study", "content": "One of the primary contributions of IdenBAT is the disentanglement of age-related features $\\mathcal{F}_{age}$ and identity-related features $\\mathcal{F}_{iden}$. To validate the proper decomposition of these features, we randomly selected five subjects for each age in the test dataset and presented the t-SNE plots of their $\\mathcal{F}_{age}$ and $\\mathcal{F}_{iden}$ in Fig. 7. Interestingly, we observed that age-related features form a progressively coherent trajectory that aligns with the continuum of the aging process. Conversely, identity-related features are dispersed across the plot, indicating that the characteristics defining a person's identity remain consistent regardless of age. These visualized plots demonstrate our proposed IdenBAT effectively achieves the intended features disentanglement.\nFurthermore, we conducted a series of ablation studies to verify the efficacy of specific components within our proposed method, as shown in Table 2. Case 1 examines the impact of omitting the identity-extracting loss, thereby assessing its role in enhancing model performance. Cases 2 and 3 also explore the effects of removing the cosine similarity loss and the orthogonality loss from the identity-extracting loss, respectively. In Case 4, we investigate the significance of our conditioning mechanism within the discriminator by substituting it with a conventional cGAN approach. We observed that Case 1, which lacked all components of the identity-extracting scheme, demonstrated the lowest performance across all evaluation metrics. The results of Cases 2 and 3 also showed inferior scores compared to our complete model. The outcomes of Case 4 reveal the effectiveness of our proposed conditioning mechanism within the discriminator. We depicted a qualitative result through the ablation study, as shown in Fig. 8. In the figure, it is evident that Case 1 struggles to maintain the structural identity of the ventricles, leading to undesirable shrinkage in areas that should remain unchanged (see red boxes). Consequently, we posit that these findings provide compelling evidence of the efficacy of our proposed IdenBAT in preserving the inherent identity while accurately converting ages to the desired target ages."}, {"title": "6. Discussion", "content": "In this work, we presented a brain age transformation model that enables precise age conversion of brain images, while preserving individual identity. The clinical significance and contributions of our proposed IdenBAT offer substantial benefits in practical and medical applications.\nFirst, IdenBAT opens the chance for clinicians to have an invaluable tool for visualizing and quantifying the brain aging process. By simulating the brain at various ages while preserving the identity of specific subjects, our model offers the opportunity to conduct highly reliable longitudinal studies that capture accurate morphological changes. A notable contribution of our IdenBAT is its ability to perform age transformations on full-size 3D MRI data. This is particularly significant in clinical settings as full-size 3D images provide a more comprehensive view of brain structures than 2D or cropped 3D images. Clinicians might leverage this capability to monitor more precise morphological characteristics within the aging process. Second, the characteristic of IdenBAT that does not require longitudinal data in training presents a significant advantage in clinical applications since collecting longitudinal MRI data is time-consuming and expensive. Also, reducing dependency on longitudinal data suggests that existing cross-sectional data can be utilized more effectively, potentially accelerating research and clinical studies. The last important aspect is the potential use for imputation in longitudinal datasets. In cases where some time points of the longitudinal data are incomplete or missing, our proposed IdenBAT could simulate plausible age-transformed images, filling in the gaps and enabling more comprehensive analyses.\nOur proposed method also has avenues for improvement. IdenBAT can be adapted for use with other types of medical domains, such as T2-weighted sMRI and functional MRI (fMRI), or different neurodegenerative conditions. This adaptability could open new opportunities for the research community and clinicians to explore various aspects of brain structure and function across different imaging modalities and disease conditions."}, {"title": "7. Conclusion", "content": "This work proposed IdenBAT, a new framework for age transformation in the brain that focuses on preserving individual identity through disentangled representation learning. Compared to previous methods that primarily undertake identity preservation at the image level, the IdenBAT considered image-level and feature-level viewpoints, enabling the model to maintain personal characteristics. With age condition injection via style transfer mechanism, the IdenBAT enhanced the method's ability to model the complexities of the aging process. By further integrating cosine similarity and orthogonality objective functions, IdenBAT facilitated the extraction of an isolated representation of identity features from the input subject. Building on such advantages and through quantitative and qualitative evaluations, we demonstrated that the proposed method outperformed existing approaches that maintain individual identity during age alterations, providing a robust solution for accurately modeling the aging process in the brain. In conclusion, we believe that this advancement holds significant potential for applications in medical research and personalized aging treatment, where preserving individuals' unique identities is crucial."}]}