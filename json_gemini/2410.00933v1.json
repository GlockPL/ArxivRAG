{"title": "StreamEnsemble: Predictive Queries over Spatiotemporal Streaming Data", "authors": ["Anderson Silva", "Eduardo Ogasawara", "Fabio Porto", "Patrick Valduriez"], "abstract": "Predictive queries over spatiotemporal (ST) stream data pose significant data processing and analysis challenges. ST data streams involve a set of time series whose data distributions may vary in space and time, exhibiting multiple distinct patterns. In this context, assuming a single machine learning model would adequately handle such variations is likely to lead to failure. To address this challenge, we propose StreamEnsemble, a novel approach to predictive queries over ST data that dynamically selects and allocates Machine Learning models according to the underlying time series distributions and model characteristics. Our experimental evaluation reveals that this method markedly outperforms traditional ensemble methods and single model approaches in terms of accuracy and time, demonstrating a significant reduction in prediction error of more than 10 times compared to traditional approaches.", "sections": [{"title": "1 INTRODUCTION", "content": "Spatiotemporal (ST) data is critical for predictive analysis in many applications, such as temperature/precipitation forecasting [9, 12, 14, 45], epidemic propagation [36, 60], neuroscience [28, 40, 56], traffic flow prediction [24, 52], and social sciences [1, 48]. ST data capture both location and time information of measured events and are frequently delivered in high-speed, high-volume data streams, demanding efficient processing techniques capable of handling their continuous influx. In these scenarios, predictive queries apply predictive models over the incoming ST data streams to make forecasts based on identified patterns and trends. The primary role of these models is to be able to understand connections and interdependencies within the data to enable accurate forecasts based on patterns, which, in the case of ST data streams, are particularly complex and subject to dynamic fluctuations [23].\nTypically, a range of available trained machine learning (ML) models are considered when composing a predictive query. Each model is trained on a different data region and learns data patterns from a variety of time series. In these scenarios, the correct choice of a single appropriate model to be used by a query is challenging since determining which model best suits the series for a given region can be difficult. Furthermore, each model specializes in different data distributions, with strengths and weaknesses. Thus, combining a set of models requires a compelling strategy, allowing different aspects of the data to be captured and providing diverse perspectives and complementary answers to enhance the overall accuracy. However, selecting and combining these models effectively is hard since the data and models' characteristics need to be considered. This challenge can be expressed as the following model selection problem: Given a predictive query over an ST data stream and a set of ST predictors, how can we select and combine the predictors to minimize the total inference cost? Here, \"cost\" can refer to accuracy, inference time, or a combination of both.\nTraditional model ensembles, where multiple models are used complementarily, have been demonstrated as an effective approach in various applications [17, 47], including over data streams [22, 30]. However, this approach may incur the following problems: (1) high execution time if the available base models are complex since it requires running all of them to post-process and combine their results; (2) poor prediction accuracy at ST data, since time series data distribution may significantly vary according to their distribution along the spatial and temporal region of the input data, thus becoming hard to determine the weights for each model and compose the ensemble accordingly.\nAlternatively, a globally trained model representing multiple diverse time series occurring in its training data and thus capturing a wider range of underlying data distributions could be a compelling strategy. However, such a model is typically not available since (1) it is difficult to ensure an adequate amount of training data to encompass all patterns present in every region and (2) coping with the increased complexity and computational demands for training a model that learns these multiple patterns across different regions and temporal periods may not be feasible with the available resources."}, {"title": "2 MOTIVATING EXAMPLE", "content": "Consider a climate monitoring system that preemptively notifies public agencies about regions that are potentially subject to meteorological events of interest. Various heterogeneous sensor devices,"}, {"title": "3 BACKGROUND", "content": "This section provides a comprehensive overview of the fundamental ST stream data analysis related to our solution."}, {"title": "3.1 Stream Data", "content": "Stream data is a continuous and potentially unbounded flow of generated data that must typically be processed in real-time or near real-time. In general, the main challenges when dealing with stream data are (1) limited computing resources in terms of memory, bandwidth, and time since data is generated continuously, potentially unbounded; (2) predictions must be made with a short delay, which can make impossible the labeling of data items on time and (3) the streams data distribution may change unpredictably over time, incurring in poor predictions and incorrect decisions from the models [34].\nIn the context of data analysis, ST data streams, produced by sources such as GPS devices, satellite imagery, and weather monitoring stations, introduce an extra layer of complexity: the spatial dimension. In these data streams, each data point is typically associated with a set of spatial coordinates (e.g., latitude and longitude) and a timestamp. Another characteristic is the existence of dependencies among time series due to the spatial dimensions."}, {"title": "3.2 Time Series Clustering", "content": "To overcome the limitations of clustering techniques over stream data, Stream Clustering algorithms have been proposed. The main objective of these methods is to continually update existing clusters, assimilate emerging patterns, and incrementally discard outdated ones. This involves optimizing the number and location of clusters to represent the underlying data best and extract relevant information without storing and re-evaluating all observations. Preserving time-based patterns by representing the incoming data as time series is useful in accounting for temporal dependencies. Nevertheless, time series clustering faces challenges that impact the efficiency and effectiveness of the clustering process:\nData Size and Storage: To perform efficiently, the clustering task often requires repeatedly accessing the whole time series data for multiple iterations. However, storing all historical data for each time series in memory is often impractical since it may exceed memory capacity.\nHigh Dimensionality: If we assume each point in the series is a feature, time series often exhibit high dimensionality, degrading the accuracy of many clustering algorithms and slowing the clustering process.\nChoice of a similarity measure: Time series similarity matching calculates the similarity among the entire time series using a similarity measure. Similarity between time series is a well-studied topic in literature, and many different metrics have been proposed [6, 38, 55]. However, comparing different time series can be challenging because data can be complex due to inherent noise, outliers, and shifts [2]. These factors must be considered when selecting the most suitable similarity measure for a given dataset and analytical task."}, {"title": "3.3 ST Series Forecasting", "content": "Sequence Forecasting is the task of predicting the most likely sequence of observations in the future, given the previous observations. In many forecasting problems, ST information is very useful for increasing prediction accuracy. In temperature prediction, for example, the latitudes are also essential for predicting future values besides the historical sequence of temperature measurements. ST sequences offer a useful representation of the underlying dynamic system by preserving spatial and temporal correlations among observations (like videos). These complex relationships, however, represent big challenges, especially for forecasting extensive sequences [51].\nST forecasting can be categorized based on the grid structure, distinguishing between regular and irregular grids. In cases where it is possible to collect data from extensive regions using densely distributed sensors or monitor large geographic areas using radar or satellite systems [45], the regular grid representation is typically straightforward. Conversely, when addressing issues that involve limited monitoring points (e.g., disease outbreak prediction problems), it is possible to derive a regular grid from an irregular one by estimating values for the remaining positions within the irregular grid.\nIn typical sequence forecasting methods, it is often assumed that each sequence is not influenced by or related to others, and all have the same statistical properties - in other words, they are independent and identically distributed (i.i.d.) [3, 19]. However, in ST data streams, the nature of the data distribution tends to vary according to time and space. For example, climate data such as temperature or rainfall presents cyclic patterns due to changing seasons. Geographical features such as mountainous or coastal regions may also introduce statistical differences. Observations at different points can also exhibit spatial dependence, meaning that a sequence behavior at one location may correlate with the values at nearby locations. Failing to consider these factors could dramatically deteriorate the selected method's performance.\nFormally, on a regular grid, ST predictors try to approximate the function:\n$\\argmax p(X_{t+1}, X_{t+2}, ..., X_{t+k}|X_{t-j+1}, X_{t- j+2}, ..., X_{t})$\n$X_{t+1},..., X_{t+k}$\nwhere p(x) is a conditional probability and each $X_{i} \\in R^{P \\times M \\times N}$ represents the data values on a grid of M rows, N columns and P measurements at time i. Based on k = 1 or k > 1, the forecasting can also be classified as single-step or multi-step. Compared with single-step forecasting, learning a model for multi-step forecasting requires more elaborate models since the forecasting output $X_{t+1}, ..., X_{t+k}$ is a sequence with non-i.i.d elements."}, {"title": "4 PROBLEM DEFINITION", "content": "We consider a dynamic system over a spatial region represented by an MXN grid consisting of M rows and N columns. Inside each cell in the grid, a measurement varies over time. Thus, the observation at any time t can be represented by an observation matrix, or frame, $X_{t} \\in R^{M \\times N}$ where R denotes the domain of the observed features. For the sake of simplicity, we present our scenario by focusing on forecasting only the next value of an univariate series using the past n measurements through data windows of fixed size n. Thus, in a given period, we can obtain an ST sequence of n observations $X_{t-n+1}, X_{t-n+2},..., X_{t}$, which we call a data window at $W_{t}$.\nTo compute $X_{t+1}$, a set of ST Predictors (STPs) M = {$m_{1}, m_{2}, ..., m_{s}$} is available. It is worth noting that the dimensions of each model's input frames may vary across different models. This flexibility allows us to adapt the models to various spatial configurations and capture relevant information effectively. Given a ST query Q over $W_{t}$, our problem involves determining a partitioning P of the query space and a subset of models $M' \\subseteq M$, so that: $P = \\cup_{i=1}^{n} P_{i}, P_{i} \\cap P_{j} = \\emptyset, 1 \\le i, j \\le n, i \\ne j$, then finding the optimal allocation $A_{t} = (P_{i}, m_{j})_{t}$ of STPs at each timestamp t. The execution of the models M' from allocation $A_{t}$ produces a ST prediction $X_{t+i}$ that satisfies the region defined by Q and has an execution cost that minimizes $(X_{t+i} - X_{t+i})$, so that:\n(i) $\\forall P_{i} \\in P, \\exists model m_{j} \\in M'$ such that $A (R_{i}, m_{j})$\n(ii) $A (P_{j}, m_{i}) \\land A (P_{j}, m_{k})$ if only if i = k\nUnder these definitions, while only one model per partition is allocated, a single query can span multiple partitions."}, {"title": "5 STREAM ENSEMBLE", "content": "To address the aforementioned challenges in dealing with the selection of ML models over ST predictive queries, we propose a novel method that is composed of three distinct but interconnected steps: (1) creating an error estimation function for each available model, (2) partitioning the query space into subregions of similar data distributions, and (3) composing a model ensemble based on smallest estimated execution cost. This section provides an in-depth explanation of each of these steps. Figure 4 illustrates the main steps of the method. Steps (1) and (2) are addressed in the preparation stage, while step (3) is addressed in the prediction stage."}, {"title": "5.1 Estimating Models Generalization Error", "content": "The initial step is a method to estimate the execution error for each candidate model. The goal is to develop a function that quantifies how well a model performs on unseen data, an essential step for ensemble selection. The estimation process is unique in that it considers the space in which the model has been trained, i.e., its training data and generalization capabilities.\nFor each candidate model, we compute the model's error on its training dataset T. Gaussian noise with increasing variance is added to these training data points to inject the element of uncertainty. This operation creates a sequence of altered datasets, namely $T_{0},..., T_{n}$, where each dataset $T_{i}$ has been subjected to i iterations of Gaussian noise, incrementally increasing its variance.\nSubsequently, the strategy measures the dissimilarity between T and each $T_{i}$. This dissimilarity is quantified using the Dynamic Time Warping (DTW) function, which is applied over the centroid series of each dataset. DTW measures the similarity between the two time series, considering their temporal alignment and value differences. A regression model of the form error = F(dist\u012f) is fitted using cross-validation from each pair DTW distance and model error. This regression model is referred to as the Error Estimation Function. It captures the relationship between the distance in the feature space and the model's error, enabling the estimation of the model's accuracy in unseen data regions."}, {"title": "5.2 Query Partitioning", "content": "The next step involves partitioning the query space. ST data often contains many underlying patterns and behaviors that vary across geographical regions and periods. For instance, when analyzing traffic data, one subset could consist of congested urban areas, while another might concentrate on open highways. Similarly, certain geographical areas might exhibit unique climate patterns or pollution levels when monitoring the environment. By partitioning the data stream, we can isolate these patterns within subregions. Defining these subregions enables us to choose the most appropriate model for each area, i.e., one that could capture the particular phenomena observed in that subset.\nPartitioning and selecting a single best-suited model for each region also contributes to efficient resource allocation. Instead of applying all models to the entire dataset, which can be computationally demanding, resources can be allocated individually to a single model per partition.\nThe second step of our method focuses on partitioning the incoming ST data stream into subregions of similar behaviors. This step involves three key sub-processes: representation, clustering, and tiling."}, {"title": "5.2.1 Time series representation", "content": "Time series often possess high dimensionality; each data point is a time-stamped observation. These high-dimensional representations can be computationally intensive and challenging to analyze effectively. Our proposed method first represents the time series on each data window as a lower-dimensional vector. This step aims to reduce the time series length without significant information loss. A lower-dimensional representation enables the subsequent clustering algorithm to process data more efficiently, mitigating the risk of overfitting and ultimately leading to more robust and accurate clusters. Furthermore, time series frequently presents inherent noise that can arise from various sources, such as measurement errors, outliers, or inherent variability. These elements can lead to spurious clusters or misinterpretation of temporal patterns. By reducing the dimensionality of the data, we intend to extract and emphasize the series' most relevant and informative components.\nWe explore two different functions for dimensionality reduction: the Generalized Lambda Distribution (GLD) [44] and Parcorr [59], a parallel incremental random vector/sketching approach.\nThe Generalized Lambda Distribution is a statistical distribution function that offers a flexible and versatile framework for modeling a wide range of probability distributions. For this reason, it is particularly useful in representing time series. The GLD is defined by four parameters ($\\lambda_{1}, \\lambda_{2}, \\lambda_{3}, \\lambda_{4}$) that govern its shape and properties: the location parameter ($ \\lambda_{1}$) shifts the distribution along the x-axis (i.e., the mean), the scale parameter ($ \\lambda_{2}$) controls its spread, the shape parameter ($ \\lambda_{3}$) influences skewness and kurtosis, and the tail parameter ($ \\lambda_{4}$) determines tail thickness, impacting the probability of extreme values in the distribution. The GLD can represent time series by fitting the distribution to the data's empirical probability density function (PDF). The goal is to find the GLD parameters that best approximate the observed data distribution. Statistical estimation techniques can be used to determine the GLD parameters that minimize the discrepancy between the observed data and the fitted GLD [7, 21, 53].\nParCorr is an efficient method designed for identifying similar time series pairs within sliding windows of data streams, using either Euclidean distance or Pearson correlation as a similarity metric. The fundamental idea is to compute the dot product between each normalized time series within a specified window size w and a set of b random vectors. This process results in a sketch of the time series for that particular window. For two time series $t_{i}$ and $t_{j}$, it is mathematically demonstrated that the distance ||sketch($t_{i}$) \u2013 sketch($t_{j}$)|| is a good approximation of ||$t_{i}$ - $t_{j}$|| provided the dimensionality of the sketches b is large enough. Comparing these sketches allows for identifying time series close in the sketch space, which is computationally much more efficient, especially when w is much larger than b."}, {"title": "5.2.2 Time series clustering", "content": "Once each series representation is well defined, this step aims to group similar time series, enabling the identification of homogeneous allocation regions for associating each ML model. This process optimizes the allocation of models to different data partitions. We propose three distinct interchangeable clustering strategies, depending on the use cases and computational requirements:\nStatic clustering utilizes a predefined clustering algorithm, such as k-means, to group time series based on an initial, fixed-length time window. Once the clustering is defined, it is never updated. This method is particularly suitable when the cluster allocations for each region remain relatively constant over time, and the overhead of re-executing the clustering step is impractical. We choose the silhouette test to determine the optimal number of clusters for a given dataset and time window, quantifying how similar each series is to its corresponding cluster compared to others.\nDynamic Clustering, in contrast to static clustering, re-executes the clustering step for each time window. Although this approach may be more computationally expensive due to its iterative nature, it is well-suited for scenarios where the time series within regions exhibit dynamic characteristics. It allows the selected models to be adapted to evolving data distributions by continuously reassessing cluster assignments. If the execution time constraint of the problem allows it, the number of clusters can also be determined dynamically based on clustering metrics such as the silhouette score. By reassessing the optimal number of clusters for each time window, our proposed strategy ensures that the cluster assignments remain in sync with the changing data distribution.\nStream clustering: In cases where real-time adaptability is necessary, the approach of stream clustering can be more adequate. Stream clustering is particularly valuable when handling unstable clusterings and high-velocity streaming data and situations where computational efficiency is required, providing continuous adaptation to gradually changing clusterings. This approach utilizes stream clustering algorithms that can handle large datasets incrementally with limited memory usage. Specifically, we implement the BIRCH algorithm [61], a highly efficient clustering algorithm [33]. Its fundamental idea is to construct a hierarchical structure of clusters, allowing for efficient retrieval and updating of cluster assignments as new data arrives.\nBy incorporating these three clustering strategies, the algorithm offers the flexibility to adapt to diverse data characteristics and computational constraints, whether data clusters remain relatively stable over time, evolve dynamically, or demand real-time adaptability."}, {"title": "5.2.3 Tiling", "content": "The tiling step ensures that the input to multidimensional ML models, such as convolutional models, is a regular and well-structured frame that preserves the underlying data distributions revealed in the clustering step. This step addresses the geometrical challenges that arise from irregular clustering outputs and noisy datasets. It transforms the irregular clustering results into regular, connected regions known as tiles, which serve as input to the ML models.\nThis step is necessary because of the geometric misalignment between the input frames, which are typically regular matrices, and the often irregular clustering results. Irregular clusters can result from the data's inherent noise or the non-uniform distribution of data patterns. To consider these factors, the tiling process aims to produce connected, regular regions for the ML models to analyze.\nTwo main strategies for tiling are employed, each with specific characteristics:\nBottom-Up Tiling: In the bottom-up tiling strategy, an adaptation inspired by the YOLO (You Only Look Once) algorithm is employed. This strategy initially assigns a tile to a single time series or data point not associated with any other tile. It then systematically grows the tile's area in a given direction (up, down, left, or right), expanding its boundaries. When considering the addition of series from a particular dimension, the algorithm calculates the potential impact on the tile's purity, a measure of how homogeneous the data within the tile is regarding cluster assignments.\nThe algorithm begins with a defined purity threshold, determined at the start of the tiling process. If expanding the tile in a specific direction would result in a purity level below the threshold, the algorithm explores other spatial dimensions, effectively avoiding adding data that would reduce the tile's purity. Conversely, if adding data maintains or enhances the tile's purity above the threshold, the algorithm expands the tile in that direction.\nThis process continues iteratively, expanding the tile's area while ensuring that the tile maintains a high degree of homogeneity in cluster assignments. When the tile cannot be expanded, the process is repeated for a new tile.\nTop-Down Tiling: In contrast to bottom-up tiling, the top-down tiling strategy utilizes a partitioning approach to transform the query space into square subregions iteratively. The quad-tree data structure is employed to partition the space into square regions, dividing hierarchically each region into four equal-sized squares or quadrants, allowing for a recursive partitioning process. The algorithm keeps subdividing each quadrant until the purity rate satisfies (exceeds) the specified minimum threshold.\nIn both tiling strategies, the primary goal is to create regular, connected regions with a minimum percentage of series belonging to a single cluster, ultimately optimizing the input data structure for the multidimensional ML models. These structured regions, the tiles, geometrically assist the frame allocation to the selected ML models, enabling the execution of ST predictors that generally require the input to consist of a regular structure.\nThe choice between top-down and bottom-up tiling strategies depends on the specific characteristics of the data and the application's requirements. Top-down tiling tends to produce larger, more coherent regions, while bottom-up tiling offers finer-grained partitions but may produce larger tiles."}, {"title": "5.3 Model Ensemble Composition", "content": "After partitioning the query area into tiles, the method finally proceeds with the selection and allocation of the most suitable ML model for each tile. The guiding principle in this step is to minimize the estimated execution cost by utilizing the Error Estimation Function (EEF), defined in Section 5.1. The EEF estimates the model's error for each tile based on a representative time series that characterizes the data distribution and behavior within that tile.\nUsing the EEFs defined for each candidate ML model, an error estimation for each model can be calculated for each tile. The reference time series for each tile is identified as the medoid, which serves as the central, most representative time series within the tile.\nThis process involves calculating the estimated error for each pair of tiles and model. As a key feature, this step is not computationally expensive, as it necessitates no more than the EEF computations for each pair (tile, model). Subsequently, the model with the lowest estimated error for each tile is selected as the best predictor for that specific region.\nOnce the best-suited model has been selected for a particular tile, the method addresses the spatial dimension misalignment that often exists between the tile and the model's input window. The tile's spatial dimensions may be either larger or smaller than those of the model's input window. The method adapts accordingly.\nIf the tile is larger than the model's input window, the model is allocated multiple times within the tile, with disjoint allocations. This approach effectively fits the model into the larger tile, allowing it to analyze different tile segments without overlap, as depicted in Figure 3. Each model allocation processes a portion of the tile data.\nIf the model's input window is larger than the tile, or if the tile dimensions are larger but not multiples of the model's input window, the composition algorithm duplicates the last columns or rows of the tile data to match the model's input window dimensions. This ensures that the model receives a regular input format for processing.\nFor each tile, the selected ML model is executed independently. The final step involves recombining the predictions generated by each model for each tile. This process is straightforward and involves the following steps:\n(1) For each model's output, select the portion of the output corresponding to the data within each tile.\n(2) Combine these model predictions into the output window, aligning them correctly to reconstruct the query space according to the spatial input dimensions.\nBy selecting the most suitable models for each tile based on the error estimation function and efficiently aligning the tile and model dimensions, the resulting predicted frame represents the prediction of the most suited ensemble over continuous queries on streaming data, contributing to improved accuracy in query processing.\nWhile the outlined process is suitable for predicting a single frame based on each data window, it should be noted that for an interval k query, the process may be repeated k times. The procedure StreamEnsemble(Q, M) where Q represents the query and M denotes a collection of models. illustrates each step of the method."}, {"title": "6 EXPERIMENTAL EVALUATION", "content": "In this section, we analyze algorithmic performance within our strategy. We explore the choices available for each component of our approach and compare their effectiveness and suitability. To better evaluate the approach's contribution, we utilize simple methods that do not rely on sophisticated feature engineering, intricate deep learning architectures, or hyperparameter optimization. Additionally, we compare our proposed solution and the baseline approach, which does not incorporate an ensemble, to evaluate the impact of our strategy."}, {"title": "6.1 Experimental Setup", "content": "In the following, we present the hardware and software configurations and the data workload and models utilized."}, {"title": "6.1.1 Environment", "content": "All the experiments were conducted on a Dell PowerEdge R730 server equipped with dual Intel Xeon E5-2690 v3 processors clocked at 2.60GHz and 768GB of RAM. The server operates on a Linux CentOS 7.7.1908 platform with a kernel version 3.10.0-1062.4.3.e17.x86 64. The models were trained and tested using an NVIDIA Pascal P100 GPU with 16GB of dedicated RAM. The algorithms and all related code were implemented in Python, leveraging popular libraries such as NumPy, SciPy, Scikit-learn, and custom-developed functions written to our specific research needs."}, {"title": "6.1.2 Dataset", "content": "The CFSR (Climate Forecast System Reanalysis) dataset includes temperature data collected over 25 years measured every six hours, providing a robust variety of climate and weather patterns. This dataset is particularly suitable for our research as it offers a rich source of ST data, allowing us to explore our method's performance in real-world temperature data analysis. We used a subset of this dataset, a region of 141 x 153 cells corresponding to South America, in a resolution of 0.5\u00b0 x 0.5\u00b0 degrees latitude and longitude."}, {"title": "6.1.3 ML Models Configuration", "content": "In our experiments, we employed a diverse range of models, each characterized by specific parameters while considering three key dimensions of model variability: input window size, training timestamps, and training regions. The following details describe the configurations of these models on each dimension.\nInput Size: To explore the impact of input window size on the ensemble composition, we considered four different input window lengths: 1x1, 3x3, 5x5, and 7x7. Models configured with larger window sizes gain a broader spatial scope, which enhances their ability to identify overarching patterns within the input data. Also, models with smaller input regions must be invoked fewer times when executed over a large tile when chosen to compose the ensemble. There is a trade-off when the tile size is smaller than the models' input, necessitating artificial data synthesis to complete the models' input window. In scenarios dominated by smaller tiles, models with compact window sizes are the more fitting choice since they do not require the synthesis of additional data and rely solely on data from the target dataset.\nArchitecture: The models with a 1x1 input window configuration were implemented as ARIMA models, a widely used time series forecasting method [10]. These models are developed to capture temporal patterns and make predictions based on historical data. The models with large input windows were implemented using the ConvLSTM architecture, structured to predict the next value in the time series based on the input window [50]."}, {"title": "6.1.4 ML Models Training Regions", "content": "We also explored different training data regions and their impact on the selection of the ML models.\nSpatial Regions: Our experiments involved three discrete training regions, each corresponding to one of the clusters identified through the Parcorr representation. Suppose the target region of the query matches the series assigned to a cluster. In that case, models trained on that specific cluster tend to have an advantage in predicting that region's accuracy.\nTraining Timestamps: Our experiments also took into account the temporal dimension. We considered two distinct training intervals. The first interval spanned the initial three months of the first semester of 2014, aligning with the southern hemisphere's summer season, characterized by warmer temperature patterns. In contrast, the second interval encompassed the first three months of the second semester of 2014, coinciding with the southern hemisphere's winter season, typified by cooler temperature patterns.\nWe trained 24 distinct models for each training timestamp, encompassing all possible combinations of input window size, training timestamps, and training regions. The input size for all models remained consistent at 10."}, {"title": "6.2 Effect of the Series Representation", "content": "StreamEnsemble establishes that the time series should be represented in a lower dimensional space. We compared two dimensionality reduction techniques, Generalized Lambda Distribution (GLD) and Parcorr, discussed in Section 5.2.\nIn the case of the Parcorr representation, we explored its sensitivity to the number of basis vectors used for dimensionality reduction. This involved varying the number of basis vectors in 2, 4, 6, and 8 vectors. By systematically adjusting this parameter, we aimed to assess the impact of different dimensional reductions on the algorithm's performance and effectiveness.\nFor this experiment, the time series was fixed at 1460 data points, corresponding to an entire year of data spanning 2015. This choice of data length allows us to capture and analyze seasonal and yearly patterns in the time series.\nWe employed the silhouette score to assess the series representation's quality. The silhouette score measures the clustering quality regarding the average distance between clusters and the average distance within clusters - higher scores indicate better separation. We explored different numbers of clusters, ranging from 3 to 5. Across all dimensionality reduction methods tested, it was consistently observed that the optimal number of clusters, as determined by the silhouette score, remained at 3."}, {"title": "6.3 Effect of Clustering Strategy", "content": "The second set of experiments aimed to evaluate the time series clustering strategies (Static, Dynamic, and Stream Clustering). Each strategy was tested in isolation to assess its effectiveness in partitioning the data for subsequent analysis.\nThe evaluations were performed under a tumbling window model. To ensure a broader dataset coverage, each window has a fixed size of 24 data points, corresponding to six input days, without intersection between subsequent windows and across all year of 2015, resulting in 60 windows. We executed each configuration five times, resulting in 300 executions per strategy. The silhouette scores were computed after each clustering process to provide insights into the ability of each strategy to create homogeneous clusters from the input data."}, {"title": "6.4 End-to-end evaluation of the StreamEnsemble", "content": "The final set of experiments encompassed all the method's steps, from start to finish. The purpose was to evaluate the method's overall performance, considering all its components and stages, including series representation, clustering, model selection, and execution. For this evaluation, we measured execution time and accuracy for each predictive window, assessed through the Root Mean Square Error (RMSE).\nWe selected seven queries within the CFSR data to account for data variability, resulting in 7 different datasets. We considered as input the totality of data windows throughout a year-long continuous stream. Tumbling windows were employed to ensure a consistent and non-overlapping data partitioning approach. The comparative analysis considered our proposed method, integrating all 24 models described in Section 6.1.4 over spatial queries of 20 x 20 (lat x lon).\nFigure 8 illustrates the regions where each dataset was extracted from. To determine regions, we performed a single clustering of data over one year. We specified 5 datasets that covered a single cluster each (2, 3, 5, 6, and 7) and two datasets over the intersection of clusters (1 and 4). Our approach involved implementing static, stream, and dynamic clustering strategies over each dataset on time series with reduced dimensionality using the Parcorr method with six basis vectors. We also compared the results using the YOLO and Quadtree tiling strategies introduced in Section 5.2.3, registering the predictive error that each strategy presented for each window.\nResults can be seen in Figure 9. In it, we display the number of data windows where each strategy chose the model with the best results. When multiple strategies yielded the models with minimum error, all such strategies were considered the most successful selections. There isn't a single strategy that consistently outperforms all others across every dataset. Thus, the optimal approach depends on the characteristics of the query being processed. We also observed that strategies based on Yolo tiling tend to exhibit a higher execution time due to its bottom-up nature, leading to the identification of more tiles than quadtree and, therefore, requiring more model executions. Conversely, quadtree-based strategies demonstrated very low execution time due to fewer identified tiles. The strategy Static+quadtree took, on average 1.66 seconds to run (per window) against 10.58 for Static+yolo.\nIt is worth noting that for Dataset 7, the static + quadtree strategy consistently outperformed all others. This dataset covers a region with lower temperature variability, favoring a smaller number of tiles and avoiding model switching. Nonetheless, even in this scenario, dynamic and stream-based strategies presented competitive results, with an average error difference of no more than 1.86."}, {"title": "7 RELATED WORK", "content": "The related work presents ML approaches for streaming data. It is organized into four sub-sections: Concept Drift Adaptation Methods, Ensemble Algorithms, Model Selection Systems, and AutoML. Each section explores methods to tackle evolving data streams using automated ML processes. These discussions offer a comprehensive view of adaptive learning techniques in response to dynamic data and predictive modeling needs."}, {"title": "7.1 Concept Drift Adaptation Methods", "content": "Concept drift", "46": "continuously update models as new data arrives", "8": "monitor data streams for significant changes", "5": ".", "34": "there are three main approaches to address concept drift: simple retraining", "4": "use a windowing strategy", "size": "smaller windows reflect the latest data, but larger ones provide more training data.\nEnsemble retraining seeks to handle concept drift by preserving and reusing a combination of previously trained models. This strategy can notably reduce the effort required to train new models, especially in recurring concept drifts. Bagging, Boosting, and Random Forests have"}]}