{"title": "Gradient-Free Classifier Guidance for Diffusion Model Sampling", "authors": ["Rahul Shenoy", "Zhihong Pan", "Kaushik Balakrishnan", "Qisen Cheng", "Yongmoon Jeon", "Heejune Yang", "Jaewon Kim"], "abstract": "Image generation using diffusion models have demonstrated outstanding learning capabilities, effectively capturing the full distribution of the training dataset. They are known to generate wide variations in sampled images, albeit with a trade-off in image fidelity. Guided sampling methods, such as classifier guidance (CG) and classifier-free guidance (CFG), focus sampling in well-learned high-probability regions to generate images of high fidelity, but each has its limitations. CG is computationally expensive due to the use of back-propagation for classifier gradient descent, while CFG, being gradient-free, is more efficient but compromises class label alignment compared to CG. In this work, we propose an efficient guidance method that fully utilizes a pre-trained classifier without using gradient descent. By using the classifier solely in inference mode, a time-adaptive reference class label and corresponding guidance scale are determined at each time step for guided sampling. Experiments on both class-conditioned and text-to-image generation diffusion models demonstrate that the proposed Gradient-free Classifier Guidance (GFCG) method consistently improves class prediction accuracy. We also show GFCG to be complementary to other guided sampling methods like CFG. When combined with the state-of-", "sections": [{"title": "1. Introduction", "content": "Denoising diffusion models [11, 29, 31, 32], the latest popular generative models, have demonstrated exceptional performance across various domains [12, 18, 21, 25, 34], particularly in image generation [22, 26, 28]. By leveraging a learned denoising network, these models iteratively refine outputs to produce diverse, high-quality images. To enhance control over image generation, the denoiser is often trained with specific conditions to generate images with desired properties, commonly employing class labels [10] or text prompt embeddings [22, 28], as well as other types of image conditions [27].\nGuiding an unconditional model to generate images of a specific class can be accomplished using classifier guidance (CG) [6]. During the iterative image generation process, CG steers the model towards outputs that align with a designated class by incorporating the classifier's gradient at each step. This approach not only improves image fidelity but also applies to other attributes beyond class [2, 23, 36]. Despite its effectiveness in generating images with desired attributes, gradient-based guidance methods such as CG are computationally inefficient due to the time-consuming back-propagation required at each sampling step, often multiple times per step [35]. Although CG enhances image quality, indicated by increased precision scores when tested on synthetic images using a real classifier [6], it also compromises image diversity, as evidenced by lower FID scores [9] and decreased recall when evaluated on real images using a classifier trained on synthetic images.\nClassifier-free guidance (CFG) [10] is the first gradient-free guidance method that improves image quality without requiring a classifier. This is achieved by generating an unconditional sample and using it as a bad sample to avoid. CFG experiences a similar trade-off between sample quality and diversity as CG. Autoguidance (ATG) [16], a recent gradient-free guidance method, proposes using a sample from a bad version of the model instead of the bad unconditional sample used in CFG. It guides sampling to high-probability regions of the data distribution without reducing diversity, achieving state-of-the-art performances in both FID [9] and FDDINOV2 [33] metrics. However, the trade-off in image quality as evaluated using classification metrics, is not addressed.\nIn this work, we propose a new gradient-free guidance method that uses the sample from a reference class (Cref) as the undesired sample to avoid. Unlike CFG, where an empty class or a separate unconditional model is fixed throughout the sampling process, a pre-trained classifier is used to adaptively determine the reference class and guidance scale w based on classier predictions. Compared to gradient-based classifier guidance, the proposed gradient-free classifier guidance (GFCG) offers similar benefit of better alignment between generated images and class label conditions, without the computational expense of classifier gradient descent. Additionally, it is complementary to other guidance methods and can be combined with them to improve image quality without trade-off in diversity. We investigate two combination methods for their efficiency and effectiveness. For the mixed guidance, it combines the guidance from two methods temporally resulting in no additional overhead in the number of function evaluations (NFEs). Alternatively, the additive guidance method combines the guidance terms from two methods, achieving the best performance in both image quality and diversity at the cost of more computations, as the guidance terms are calculated separately. Extensive experiments are conducted on class-conditional and text-to-image diffusion models to demonstrate the benefits of GFCG in improving image fidelity with minimal trade-off in diversity. Extensive ablation studies are conducted to provide insights into optimal settings for GFCG alone and in combination with other methods like CFG and ATG. As shown in Figure 1, our results significantly improve classification accuracy for different models. For the bottom text-to-image generation, GFCG successfully generates the correct bird species while maintaining coherence with the detailed description.\nIn summary, the proposed GFCG method has the following key advantages:\n\u2022 It is the first known work of using a classifier for gradient-free guidance in diffusion sampling, leveraging a pre-trained classifier to adaptively determine both the reference class and guidance scale during sampling.\n\u2022 It significantly enhances image fidelity for both class-conditional and text-to-image models.\n\u2022 It is complementary to other gradient-free guidance methods. When combined with ATG, it establishes a new state-of-the-art performance in metrics measuring both sample image fidelity and diversity."}, {"title": "2. Related Works", "content": "Gradient-based Diffusion Guidance Dhariwal et al. [6] were the first to propose gradient-based guided sampling for diffusion model. In addition to the trade-off in image diversity and time-consuming classifier gradient calculation, it requires additional training of a noise-conditional classifier. For works following that, there are two main focuses, to generalize the guidance from classifier loss to any differentiable loss functions [2], and apply it through an existing dif-"}, {"title": "3. Methods", "content": "3.1. Diffusion Models\nDiffusion models are a class of generative models that generate data following an iterative denoising process [11, 29, 32]. This involves a forward process where noise is ingested to data over a sequence of time steps to render them indistinguishable from Gaussian noise, and a backward denoising process where the noise is removed following the reverse sequence until noise-free data is recovered. The forward process is governed by the stochastic differential equation (SDE):\n$dx = f(x, t)dt + g(t)dw$,\nwhere x is the data, t \u2208 [0, T] is the time step, and f and g are predefined functions that govern the noise schedule, and dw is a standard Wiener process. The denoising process is governed by the reverse SDE:\n$dx = [f(x, t) - g(t)^2\\nabla_xlogp_t(x)] dt + g(t)dw$,\nwhere xlogpt (x) is the score function and dw is the standard Wiener process for the reverse steps.\nIn diffusion models, the score function is parameterized by a deep neural network with parameters 0, and represented as $D_\\theta(x, t) \\approx \\nabla_xlogp_t(x)$. Conditioning variables such as class label or text prompt, denoted as c, can also be included and in this setting $D_\\theta(x, t, c) \\approx \\nabla_xlogp_t(x|c)$. During the reverse denoising process, to improve the quality of data generation, classifier-free guidance (CFG) is widely used [10]:\n$dx = [f(x,t) - g(t)^2D(x, t, c)] dt + g(t)dw$,\n$D(x, t, c) = wD_m (x, t, c) \u2013 (w \u2013 1) D(x, t)$,\nwhere Dm and D are the main and guidance networks, parameterized by neural network weights \u03b8 and \u03c6, respectively. Here D could be an unconditional diffusion model as in some implementations. In others, both Dm and D may be conditional neural networks where a null (\u00d8) class is used as the reference for CFG, replacing D(x, t) in Equation 3 with D(x, t, \u00d8). Furthermore, w is a hyperparameter referred to as the \u201cguidance scale\u201d where a scale of 1 means no guidance. Guided sampling improves the quality of data generation, albeit trading-off diversity [10].\n3.2. Gradient-free Classifier Guidance\nClassifier guided sampling [6] from diffusion models involves the computation of gradients of classifier probabilities, and this is computationally expensive as it involves the use of autograd operators. To mitigate this issue, as explained above, CFG[10] was proposed to use an unconditional sample as a reference to increase contrast from. We extend these concepts to devise a novel formulation that utilizes a classifier, without computation of gradients, to generate an conditional sample as the reference. In what follows, we describe this methodology and refer to our method as \u201cgradient-free classifier guidance\u201d (GFCG). Our method is also adaptive in that it computes the guidance scale on-the-fly depending on how confused the diffusion model is during the denoising process."}, {"title": "3.3. Implementation Considerations", "content": "As explained above, the proposed guidance can be applied at each sampling step using adaptively determined Cref and w. In practice, there are two additional hyperparameters, ts and Scp, which can also be tuned for optimal guidance effects. Similar to the guidance interval applied to CFG [19], ts is the starting time step that the proposed GFCG is applied first and beyond during the denoising steps. For Scp, it is used to determine the frequency for classifier prediction"}, {"title": "4. Experiments", "content": "The main experiments are conducted with the EDM2 [17] code base\u00b9 and pre-trained ImageNet [5] 512\u00d7512 class-conditional models. Application of the proposed GFCG on text-to-image models is also investigated where the Stable Diffusion (SD) 1.52 model is used for image sampling. Bird Species\u00b3, a fine-grained classification dataset which consists of 525 different bird species, and it's accompanying classifier are used for guided sampling and assessment of generated image quality.\nTo evaluate the overall sample quality for various methods compared in this work, the new FDDINOV2 [33] is used in favor of the original Fr\u00e9chet Inception distance (FID) [9]. According to [33], the Inception encoder used in FID is unfairly punitive to diffusion models. More importantly, it was noted in [24] that FID is sensitive to minor data domain shift like resizing kernels and lossy compression qualities. We have conducted a comparison test between FID and FDDINOV2 by compressing the same set of guided samples with different qualities, which shows FDDINOV2 is almost constant across different settings which FID is very sensitive to. Additionally, we adopt the Precision and Recall metrics used in CFG to measure image fidelity and diversity separately. The image fidelity metric Precision is computed as the percentage of generated samples that fall into the data manifold (assessed using the real image classifier), while the image diversity metric Recall is measured as the fraction of real images which is correctly classified by the classifier trained from sampled images. Recall is only reported for a subset of classes with selected experiments due to the time-consuming processes of sample generation and classifier training.\n4.1. Class-Conditional Image Generation\nWe implemented our proposed GFCG sampling algorithm on top of the publicly available EDM2 [17] code base and use ImageNet [5] 512x512 as the main dataset. Pre-trained"}, {"title": "5. Conclusions and Limitations", "content": "In this work, we have shown that our method generates high fidelity images without incurring the additional computational costs associated with classifier guidance or requiring the training of an extra unconditional model, as seen with classifier-free guidance. By leveraging an off-the-shelf classifier, GFCG can be seamlessly integrated with existing sampling methods without additional NFEs, thereby enhancing both FDDINOV2 and Precision metrics without loss in Recall. Moreover, we further demonstrated that it can be extended to text-to-image diffusion models, achieving high class accuracy, particularly for long-tailed and fine-grained classes. This flexibility and efficiency make GFCG a robust and adaptable approach, offering significant improvements without extra training efforts or computational resources. These results underscore the potential of GFCG to optimize overall image quality, Precision in particular, across various models and applications, paving the way for further innovations in image generation and related fields.\nLimitations: While it is applicable to general text-to-image models which learn from a huge dataset, the proposed GFCG is most beneficial to generation of images which are in distribution of the pre-trained classifier. For the Bird Species experiments, as the classifier training dataset consists of close-up shots mostly, guided sampling of bird images in other layouts would be less effective. Ethical considerations: We acknowledge that this guidance method could potentially benefit creation of inappropriate materials. Deployments of such methods should apply appropriate safeguards to prevent malicious and illegal uses."}, {"title": "6. Evaluation Metrics", "content": "We have explained the reasoning for choosing FDDINOV2 over FID as the overall image quality metric in the main paper. To further validate this choice, we also conducted a lossy compression test as in [24] to compare FID and FDDINOV2. As shown in Table 3, For the same 42000 images sampled from SD 1.5, comparing to the original uncompressed output, multiple JPEG compressions are applied with different quality settings. For FDDINOV2, the metric remains relatively consistent for original ouput as well as different compression. In contrast, as the Bird Species dataset used for assessment consists of JPEG images, FID benefits from applying similar JPEG compression to the original outputs. In fact, FID improves (lower value) continuously with loss of image quality until it reaches the lowest around 80% quality. Lastly, it is shown in Autoguidance [16] that the model and sample settings need to be tuned for optimal FID and FDDINOV2 separately. As a result, FDDINOV2 is chosen as the primary sample quality metric for experiments in this work to determine optimal settings."}, {"title": "7. Additional Implementation Details", "content": "7.1. Class-Conditional Generation: Pseudo Code\nFor Algorithm 1, specifics like timestep t, noise schedule and sampling method are illustrated using DDIM as the example. In implementations, GFCG is applicable to different models and sampling methods and we've conducted class-conditional experiments using EDM2 with 2nd order Heun's solver and text-to-image ones using SD 1.5 with PNDM sampling. Besides, some implementation details are omitted in Algorithm 1, including multi-step denoising for 20 and mixed guidance of GFCG and other base methods like ATG. To present the detailed pseudo code for class-conditional generation, we first generalize Algorithm 1 to be compatible for both EDM2 and SD 1.5 experiments, as shown in Algorithm 2. In the case of DDIM sampling, \u03c3\u03c4 = 1 is in line 6, Equation 4 is used for line 4 and the following equation is used for line 20:\n$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\bigg( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha_t}}} D\\bigg)$,\nFor our class-conditional experiments, we used the 2nd order deterministic sampler from EDM (i.e., Algorithm 1 in [15]) in all experiments with \u03c3(t) = t and s(t) = 1. We used the default settings \u03c3min = 0.002, \u03c3max = 80, p = 7 and N = 32. Note that we use and t for EDM to avoid confusion as o and t are also used in our formulations. To follow the terminology in Algorithm 2, N in original EDM is denoted as T, sampling steps i = 0,1,... is denoted as t = T, T \u2013 1, . . . and noise schedule \u03c3(to), \u03c3(t1), . . . is denoted as \u03c3\u03c4, \u03c3\u03c4\u22121,... and the noise schedule is calculated as follows:\n$ \\sigma_t = \\begin{cases} (\\sigma_{max}^{1/\\rho} + t(\\sigma_{min}^{1/\\rho} - \\sigma_{max}^{1/\\rho}) )^\\rho & t > 0 \\\\ \\sigma_{max} & t = 0  \\end{cases} $\nEquation 4 for 10 estimation is replaced by a multi-step denoising process, as also discussed in Section 4.1. This"}, {"title": "7.2. Text-to-Image Generation: Text Prompts", "content": "For the main quantitative experiments of text-to-image generations using GFCG and other gradient-free guidance methods, a set of generic prompts are used based on the realistic distribution of the Birds Species dataset. This is designed to minimize the bias between the real and generated images so the FDDINOV2 metric could be more reliable in quantitative assessment. Each of the Set of following 8 generic text prompts was used to generate 10 samples for each bird species and quantitative results are reported in Table 2.\n\u2022 a close up photo of a bird, [bird species]\n\u2022 a close up bird photo, [bird species]\n\u2022 a close up picture of a bird, [bird species]\n\u2022 a close up bird picture, [bird species]\n7.3. SEG implementation details\nFor the implementation of SEG in the EDM2 codebase, we consider \u03c3 = 100 for the Gaussian Blur. This is applied in the EDM2 UNet's following blocks in the guidance network:\n8x8_block1\n8x8_block2\n8x8_in0\n8x8_block0\nOther values of \u03c3 (=10 and 1e6) were also considered but the observations are very similar.\nThe SEG implementation in SD is similar to [13]."}, {"title": "8. More Experimental Results", "content": "8.1. Effects of Random Seed Variation\nThe results presented in Table 1 of the main paper were generated using the same random seed for image generation. As the random seeds used in the ATG study [16] were not disclosed, we were unable to exactly replicate the reported FDDINOV2 metric. To demonstrate that the improvement in the FDDINOV2 metric is independent of random seed choice, we have illustrated the variation in FDDINOV2 and Precision with random seeds for both ATG and GFCGNG methods in Figure 7. The results show a clear distinction between the two methods in terms of Precision and FDDINOV2 metrics, indicating that GFCGNG consistently outperforms ATG, regardless of the random seed used.\n8.2. Effects of Guidance Model\nThe experiments detailed in the main paper involving the EDM2-S and EDM2-XXL models utilize guidance models (XS, T/16) and (M, T/3.5) respectively for the GFCG and SEG experiments, similar to ATG [16]. These guidance models, with reduced capacity and training, are readily accessible thanks to the publicly available EDM2 codebase [17]. However, this availability may not extend to other class-conditional or text-to-image generation diffusion models. Table 5 presents the FDDINOV2 and Precision metrics for the GFCGNG method, based on the capacity and training of the guidance model. The hyperparameters for GFCG, \u03b1, \u03b2, and ts, are set to 0.85, 1.25, and 17, respectively. Similar to ATG, reducing training significantly impacts the FDDINOV2 metric, while reducing capacity only results in the worst performance. The highest precision is achieved when using the same guidance model as the main model with some degradation in FDDINOV2 metric.\n8.3. Effects of Classifier Model\nDifferent classifier models, with varying sizes and top-1 and top-5 accuracies on ImageNet-1k (acc@1 and acc@5 in Table 5), were considered in the ablation study for classifier predictions in GFCG-based methods. ResNet-18, which is one-fourth the size of ResNet-101 used in the main tests, achieved a high precision of 93% compared to ATG's 90.6%, while maintaining a similar FDDINOV2 to ATG. ResNet-101 exhibited comparable FDDINOV2 and precision metrics to ResNet-152, as shown in Table 5, but with a smaller model size, and was thus selected for the main experiments in the paper.\n8.4. Effects of 10 Estimation Methods\nFor all the mixed and additive GFCG methods presented in Table 1, Scp is set to its maximum value and T' is set to 4 for 10 estimation. Although this introduces 7 additional NFEs, which is minimal compared to the 63 NFEs, it results in a significant boost in precision and some improvement in FDDINOV2 as well (see Table 1). We explore two methods to further reduce the NFEs. The first method is to reduce the number of steps for 10 estimation. The second method is to use a smaller and lower-trained guidance model as the main model for 10 estimation. For instance, the guidance model used in the majority of the EDM2-S experiments is EDM2-XS, trained for T/16. If MB = ATG, then line 32 in Algorithm 3 would change to D\u2081 \u2190 D(xt,t,cdes), D2 \u2190 D(xt, t, cdes) and D \u2190 WATGD1 \u2013 (WATG 1)D2, which essentially applies the NG method using the guidance model only for 10 estimation. As a smaller model is used for to estimation, we ignore the NFEs added by this method.\n8.5. Stochastic Reference Class Sampling\nAs explained in Equation 7, a stochastic reference class can be sampled each time a classifier prediction is applied. It improves sample quality when there are frequent classifier predictions, i.e., Scp is small. Based on that, the experimental results of text-to-image generations in Table 2 are conducted with this enabled. For comparison, we compare"}, {"title": "9. Class-Conditional Visual Examples", "content": "Visual examples from class-conditional image generation using existing guidance methods (refer to Table 1) are compared with our GFCG method in EDM2-S sampling, as illustrated in Figure 11. Additionally, we compare GFCG to the additive method GFCGATG+CFG, which achieves state-of-the-art performance in FDDINOV2 for EDM2-S. Further visual examples can be found in Figure 18. The visual results corroborate the quantitative metrics for Precision, with GFCG-generated images demonstrating a strong alignment with class labels in most cases. This is evident in the examples of mushroom and collie in Figure 11, where other guidance methods often confuse mushroom with agaric and collie with border collie. However, this precise alignment does result in a slight trade-off in diversity, as seen in the orange class example, where GFCG generates a zoomed-in image of a single orange. The additive method GFCGATG+CFG mitigates this issue by balancing diversity and class accuracy, as illustrated in the orange class example in Figure 11, where it generates a bunch of oranges in a basket.\nTo further explore the differences in diversity between GFCG and mixed methods, we compare GFCG with GFCGATG, which achieves state-of-the-art FDDINOV2 for EDM2-XXL. We present visual examples displaying 10 images per class for selected classes in Figure 12, with additional examples in Figure 19. Figure 12 shows that while GFCG-generated samples align closely with class labels (notably in the collie and orange cases, which other methods confuse with border collie and lemon), there is a modest reduction in diversity. For instance, in the orange class, GFCG tends to zoom in on the oranges or exclude other fruits, compared to the mixed method. GFCG may also remove or modify background objects which may cause confusion with the target class, as seen in the pizza and valley classes in Figure 12. Mixed methods, particularly with ATG, help preserve diversity while enhancing class accuracy."}, {"title": "10. Text-to-Image Visual Examples", "content": "For text-to-image generations, the results presented in the main paper were all based on samples from SD 1.5 and more visual examples are included here. Additionally, we also conducted experiments using another popular model, DeepFloyd IF model from Stability AI. Some examples are included in Section 10.3.\n10.1. Generic Text Prompts\nSome visual examples from text-to-images generation using the set of generic prompts are included in Figure 13. The probability of classifying each generated sample as the target class is also included for reference. In general, the GFCG results have the best class accuracy. The gained accuracy could be caused by compositional change in the first example, as well as correct anatomic features like feather color in the second. For GFCGCFG, it often maintains the overall composition of CFG and is possible to improve class accuracy even when the changes are negligible with untrained eyes like the first example. In the case of the last example, minor changes like chest patterns in GFCGCFG result in significantly increased probability too.\nAs shown in Figure 14, GFCG results may end up worse than other methods too. For the first example of TIT MOUSE, the text-to-image model obviously has misunderstood MOUSE without recognizing its context as a bird specie. In the case of CFG, as it is guided away from an unconditional model, it will enhance the wrong features associated with MOUSE, as well as correct features associate with other keywords like bird. For GFCG, as the enhancement is in the reference to a photo of another bird species, the difference will be focused between TIT MOUSE and another bird species which results in more prominent mouse features. Similar failure happens in the second example\n10.2. Detailed Text Prompts\nSome visual examples using the set of detailed prompts are included in Figure 15. Note that the hyperparameters like a and ts for GFCG related methods were optimized for the generic prompts and adopted for detailed prompts without further tuning. It shows that GFCG has the best class accuracy while preserving the overall accuracy of the full text prompt, including improving large features like the first example, or small details like the last one. It is noted that, all method including CFG, have difficulty in depicting some details in the prompts like fish in its claws and eating red berry. As shown in Figure 16, for failure cases of GFCG where class probabilities of GFCG results are lower than those of CFG, certain features of the species, e.g. white feather of BALD EAGLE, are excessively enhanced. This could be caused by improper guidance scales.\n10.3. Text-to-image Generation in Pixel Space\nWe also explored using GFCG in the pixel space alone, without using latent diffusion like SD 1.5 used above. In this test, GFCG is integrated into the DeepFloyd IF model from Stability AI, whose diffusion mechanism is implemented in the pixel level. The IF model is able to generate high definition images in size of 1024 \u00d7 1024 based on given text prompts. We assessed the performance on the Bird Species dataset, and showed some visual examples in Figure 17. The probability of classifying each generated sample as the target class is also included for reference. On multiple bird species, much higher class accuracy was achieved by GFCG over other guidance schemes. The gained accuracy is largely because of enhanced visual quality of the bird, which is more aligned to the actual appearance of corresponding species."}]}