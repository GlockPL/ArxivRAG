{"title": "CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging", "authors": ["Sunny Gupta", "Amit Sethi"], "abstract": "Federated Learning is a decentralized method in machine learning where numerous clients collaborate to train a model on a central server. It is a promising technique for learning from multi-source decentralised data, which can be of significant utility in the medical domain, where privacy preservation is crucial. However, for medical image data, cross-client variation problems (since features extracted from different client data are not uniform) would hinder the development of an optimal server model in a federated setting. This problem is magnified by lesser annotated data availability in the medical domain which by itself contributes to high variability. This paper proposes a Cross-Client Variations Adaptive Federated Learning (CCVA-FL) framework on medical images to address the variations between clients that are minimised by transforming medical domain images present at all clients into a common feature space. The method randomly selects a subset of images for each client which are first expert annotated. This is followed by the selection of a client (target client) that has the least data complexity which is then utilized to define the target image feature space. A set of synthetic medical images is then generated using annotated images of the target client via Scalable Diffusion Models with Transformers (DiT). These newly generated images that capture both the diversity as well as are representative of the original set of annotated images are then shared with other clients. Using these shared synthetic images as a target space an image-to-image translation is applied at each client to translate their local images to the target image space. The translated images at each client are then used in a federated learning setting to develop a server model. We demonstrate that CCVA-FL performs better compared to Vanilla Federated Averaging thereby overcoming the data distribution differences across multiple clients without compromising privacy.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advances in machine learning and deep learning, has lead to several Artificial Intelligence (AI) applications routinely utilized in our day to day lives. While Al models are generally deemed efficient and precise, they come with a caveat of requirement of large amount of training data. Abundant personal data effectively improves the accuracy of machine learning models, but the data owner could incur significant losses if these data are exposed through malicious activities. Given the risk of data breaches and the constraints of computing power, AI is confronted with new challenges, which present excellent opportunities for the evolution of a federated learning framework. Federated Learning (FL), introduced by Google in the year of 2016 [15] illustrates a cross-device situation where a central server manages a multitude of mobile devices, yet local data remains untransferred. This idea quickly broadened to bringing in collaboration between different organizations with silo-ed localized data, [32], where these organizations participate in a federated setting to develop machine learning models. FL is emerging as a powerful tool for medical image analysis [6], enabling the collaborative training of machine learning models without the need for data sharing among different locations. The rise of artificial intelligence has been fueled by the availability of large-scale datasets, but the lack of comparable large-scale medical datasets hinders the application of machine learning in the healthcare industry. Recently, FL has attracted considerable attention due to concerns related to small sample sizes and privacy protection in medical imaging. There are existing benchmark datasets and software platforms that facilitate FL research in medical imaging. However, FL is not without its challenges and security issues, and there has been a continuous effort to build on the performance of FL models. FL offers a promising research direction in medical image analysis, addressing the obstacles of data diversity, privacy, and legal issues. Computer Vision (CV) is significantly shaping medical transformation by utilizing machine learning (ML) tools for a tasks such as disease classification, medical image segmentation, risk predictions to name a few [7]. However, just as with other data, ML models for medical imaging requires large amount of annotated data to reach high levels of accuracies in a centralized setting. This coupled with data privacy concerns especially within the medical imaging domain, has propelled FL as an emerging solution to these problems. FL used for medical imaging ML problems allows preservation of privacy by training models locally on local data of clients and sharing the weights of these models to improve overall performance. Further, the computational load is substantially reduced since data is trained in a decentralized setting thereby reducing the pressure on central servers.\nYang et al. [32], classify FL into three distinct categories driven by data partitioning in both the feature and the sample space, namely Horizontal Federated Learning (HFL), Vertical Federated Learning (VFL), and Federated Transfer Learning (FTL)."}, {"title": "1.1 Horizontal Federated Learning:", "content": "Horizontal Federated Learning (HFL) suits situations where diverse datasets in separate locations share a similar feature space but have varied sample composition [22]. An instance of this is Google's use of HFL to enable mobile phone users to collaboratively train a next-word prediction model using their dataset [15]. Horizontal Federated Learning can be expressed as:\n\\(X_i \\subseteq X_j, Y_i \\approx Y_j, I_i \\neq I_j, \\mathcal{D}_i, \\mathcal{D}_j, i \\neq j\\). (1)\nHere, the data feature space and the label space pair of the two clients, i.e., (Xi, Yi) and (Xj, Yj), are similar. However, thethe user identifiers I\u00a1 and Ij are assumed to be distinct; Di. Further, Dj represent the datasets of the ith client and the jth client, respectively.\nIn the context of decentralized multisource medical image data, Horizontal Federated Learning (FL) is recognized as the most suitable framework. As illustrated in Fig. 1, a conventional horizontal FL system consists of multiple clients. At a specific time t, the server disseminates the global model parameters, denoted as we, to each client. Each client i then updates its local model using its unique dataset Di. This update can be represented as:\n\\(w_i^{t+1} \\leftarrow \\text{clientupdate}(w_i^t)\\)\nThese updates are relayed back to the server, which then modifies the global model parameters at time t + 1 using an aggregation function.\nThe aggregation function can be represented as:\n\\(w^{t+1} = \\frac{1}{N} \\sum_{i=1}^N p_i w_i^{t+1}\\)\nwhere pi is the weight assigned to client i.\nThis iterative training procedure persists until convergence. The primary advantage of FL lies in its ability to learn from a variety of decentralized data sources while preserving data privacy, as it shares only parameter updates, not raw training data."}, {"title": "1.2 Vertical Federated Learning:", "content": "Vertical Federated Learning(VFL), is suitable for data sets that have a similar sample space but a differ in the feature space. Through VFL, a diverse feature space of distributed datasets at different organizations can be leveraged allowing the development of better Machine Learning (ML) models without the need for data exchange or risking data privacy. For such a VFL system, we have:\n\\(X_i \\neq X_j, Y_i \\neq Y_j, I_i \\approx I_j \\forall \\mathcal{D}_i, \\mathcal{D}_j, i \\neq j\\). (2)\nHere, X and Y represent the feature space and the label space, respectively. I denotes the sample ID space, and D represents the data matrix held by different clients [32]. In a VFL setting, the aim for all clients is to build a collaborative ML model by leveraging the features contributed by all participating organizations."}, {"title": "1.3 Federated Transfer Learning:", "content": "This approach is appropriate for cases where data sets differ in both sample and feature spaces. While HFL has the prerequisite that all involved clients share the same feature space, VFL requires parties to share the same sample space. However, in real-world scenarios, there may not be sufficient shared features or samples among the participants. Despite these limitations once can still construct a federated learning model by utilizing transfer learning. This approach facilitates the transfer of knowledge among parties, leading to improved performance. Peng et al. [19] applied this by implementing unsupervised domain adaptation for image data in an FL context. An example of this is the collaborative construction of BCI models using Federated Transfer Learning with EEG data [10]."}, {"title": "1.4 Challenges of FL in medical imaging:", "content": "However, FL faces notable challenges when dealing with decentralised medical image data from various sources, primarily due to the following reasons:\n(1) Scarcity of Annotated Medical Data: Unlike extensive nat-ural image datasets like ImageNet [21] (boasting close to 1.5 million images) and CIFAR-10 [13] (comprising about 60,000 images), the availability of annotated medical image datasets is remarkably sparse. For instance, the InBreast dataset [16] contains only 410 images from 115 patients and the DRIVE dataset [24] includes a mere 40 images. This scarcity is attributed to intensive data collection and annotation labour. Consequently, this lack of training data often results in reduced transferability, leading to a performance decline in evaluations across different datasets, as highlighted in studies [28]\u2013[31]. In the context of FL, this situation causes the parameter updates at each client to be acutely influenced by the specific attributes of their local training data.\n(2) Constraint in Scale: Ideally, the number of clients in FL could be considerably high. While acquiring and labelling natural image data incurs relatively lower costs, enabling the potential for millions of clients in FL scenarios with natural images, the number of clients in medical contexts is significantly lower, typically in the tens. This directly results from the higher costs and complexities associated with medical image data [29].\n(3) Divergence Across Clients: Despite the somewhat uni-form data acquisition processes, medical image data sourced from different hospitals or institutions can exhibit distinct characteristics. For example, notable differences exist between the DRIVE [24] and STARE [8] fundus image datasets for retinal vessel segmentation, as well as between the BUSI ultrasound image dataset [1] and Dataset B [33] for breast tumour segmentation. Although these variances might not be as extensive as those observed in natural image datasets, the limited volume of annotated medical data tends to amplify these differences. Consequently, this leads to a broader variation in the parameters clients update, potentially resulting in suboptimal convergence following aggregation."}, {"title": "2 OUR CONTRIBUTION:", "content": "To tackle the issue of cross-client variation, we introduce a Cross-Client Variation-Adaptive Federated Learning (CCVA-FL) framework. We first identify the client with the least complex data amongst all clients and set this as the target client. The labelled data in the target client defines the target space to which all client images will be ultimately translated to. However, to maintain privacy while sharing the defined image space with other clients the labelled images of the target client are used to generate a set of images using Scalable Diffusion Models with Transformers (DiT) [18]. These generated images effectively encapsulate the characteristics of the raw images. Thus the target feature space is sent to other clients without breaching privacy. The next step is the translation of labelled images at each client to the target image space which as one can recall is defined by the previously shared synthesized data from the target client. This ensures that images of all clients are now in a common feature space thereby minimizing the cross-client variations. The final step is to implement federated averaging on these translated images which demonstrated improved performance compared to FL on non-translated images. Experimental results confirm that the proposed CCVA-FL framework surpasses the existing FL framework, particularly when handling cross-client decentralized small datasets."}, {"title": "3 RELATED WORK", "content": "Ke et al. [11] have established an FL framework grounded on Generative Adversarial Networks (GANs) to enable the harmonization (colour normalization) of histopathological images. Each client trains a local discriminator to grasp the unique image styles of the client, while the server maintains and updates a global generator model to generate images that are invariant to the domain. Similarly, Wagner et al. [27] have put forward a GAN model to harmonize histopathological images. Their approach is based on the assumption that every client has access to a reference dataset. This can be advantageous when it comes to training all the GANs at each client's location. Feng et al. [35] have introduced a novel encoder-decoder architecture to reconstruct magnetic resonance (MR) images in the context of the FL framework. Encoders specific to each client are trained using local data to leverage the unique properties of each client's domain, while a universally shared encoder is kept on the server side to acquire domain-agnostic representations. Chakravarty et al. [4] suggested a method that integrates convolutional neural networks and graph neural networks to categorize chest X-ray images, addressing the issue of domain shift among clients utilizing these networks. Xu et al. [30] have put forward an ensemble-based framework for the segmentation of medical images to manage shifts in clients. Their framework incorporates a global model, personalized models, and a model selector. They suggest the use of a model selection mechanism to utilize all the generated custom models, instead of solely relying on the global model to accommodate all client data. A system using HFL generally operates under the assumption of participants being honest, with security measures in place to protect against a server that may be honest but curious [2], [3]. In other words, the potential compromise of user privacy and participant data security is primarily attributed to the server. Shokri et al. [22] and Smith et al. [23] suggested an approach to collaborative deep learning (DL) wherein participants train models separately and exchange specific subsets of model parameter updates. This method represents a distinctive variation of HFL (honest but curious federated learning)\nIn 2016, Google researchers introduced a solution based on Honest but Curious Federated Learning (HFL) for updating models on Android smartphones [15]. In this system, an individual Android smartphone locally updates its model parameters and then shares these updates with the Android cloud. This collaborative process involves jointly training the federated model alongside other Android smartphones. The authors suggested a secure client-server architecture in which the federated learning system divides data by users. It enables models created on client devices to collaborate at the server location to construct a unified federated model. The model-building process is designed to prevent any instances of data leakage.\nSimilarly, in the work by Konecn\u00fd et al. [12], the authors presented techniques aimed at minimizing communication costs to streamline the training of federated models using data distributed across mobile clients. Additionally, a more recent compression technique proposed by Lin et al. [14] known as Deep Gradient Compression significantly decreases the communication bandwidth in extensive distributed model training."}, {"title": "4 FRAMEWORK", "content": "CCVA-FL aims to mitigate the variation among different clients' medical imaging data by converting each client's original data into a unified image domain, utilising Scalable Diffusion Models with Transformers (DiT) [18]. This method diverges from the standard horizontal Federated Learning (FL) strategy, in which each client i updates their model locally using their respective raw data Di, as shown in Fig. 1. In the CCVA-FL framework, the initial step involves selecting a straightforward, exemplar target image domain from the client possessing the least complex data. To safeguard privacy, a synthetic dataset derived from this client's data is utilised instead of distributing their actual data. Subsequently, other clients employ this artificial dataset to synchronise their images with the chosen target domain through locally developed translation models. This technique guarantees a uniform transformation of all client data into a single image domain, enhancing the uniformity and efficacy of the collaborative model. The synchronised parameters wi and wj of each pair of clients i and j exhibit greater alignment, thus fostering the better convergence and effectiveness of the collective global parameters WG."}, {"title": "4.1 Target Client Selection", "content": "A complexity measure was developed to evaluate data complexity driven by the labelled data at each client. This measure was used to select the most suitable client from a set of N clients for image synthesis and sharing, as demonstrated in Figure 3. For each client i, a Scalable Diffusion Model with Transformers (DiT) was trained [18] for 100 epochs to generate synthetic images Dsyn,i with the same size (i.e. number of images) as the raw data Di. A pretrained RESTNET-50 model was utilized to extract the features of labelled images of Di and Dsyn,i, indicated as fi and fsyn,i respectively. The L2 distance between fi and fsyn,i is used as an indication of the complexity score Ci of Di. A low Ci would indicate that it is computational easier to synthesize Di with the characteristics of Di being relatively easier to capture, that would subsequently assist in the image-to-image translation process [26]. Thus the client with the least complexity score is assigned as the target client. After selecting the target client k, the next step is to synthesize data using the raw data Dk from the client k."}, {"title": "4.2 Image Synthesis", "content": "Following the selection of the target client k, we begin to generate synthetic data based on the raw data set Dk of the client as demonstrated in Figure 4. Aware of the privacy leakage susceptibility of diffusion models and the unintentional disclosure of sensitive information during the data generation process [9], we implement strict privacy measures. The trained generator remains confidential, retained exclusively by the originating client, and is not shared across the network. Instead of this, a curated set of synthetic images is distributed. Synthetic images, referred to as Dshared, for sharing satisfy two criteria:\n\u2022 The synthetic images Dshared accurately replicate the distribution of the original dataset Dk in the feature space, which is essential for the successful implementation of image-to-image translation techniques [18].\n\u2022 Each image in the original data set Dk is sufficiently different from all the images in Dshared in pixel space.\nBy following these steps, we can find a balance between the usefulness of the synthesised data for collaborative learning and the necessity of protecting the privacy of individual data providers."}, {"title": "4.3 Image-to-Image Translation", "content": "Translation is an important aspect of medical image analysis [34], as it enables the transformation of images from one domain to another, facilitating the extraction of valuable information and aiding in diagnosis and treatment. Furthermore, translation can help standardize medical images, ensuring consistency across different imaging systems and enabling more accurate quantitative analysis. For each client i, CycleGAN [34] can be used to learn the relationship between their original training data Di and the generated shared data Dshared for image-to-image transformation as demonstrated in Figure 4. This is especially advantageous when the amount of training data is limited. The transformer T\u012f is utilised to transform Di to Di, making it comparable to Dshared."}, {"title": "5 EVALUATION", "content": "The performance of CCVA-FL in real-world federated settings (where data is in silos, within the confines of individual hospital organizations, collected with differing instruments and different populations) was evaluated by creating Pneumonia-FL, a dataset in which each client only contains data from a single real-world site without any overlap. It includes X-ray images from 8 different publicly available data repositories. (1) NIH Chest X ray\u00b9 (2) Kermany et al\u00b2 (3) RSNA Pneumonia\u00b3 (4) ISIC Challenge\u2074 (5) MIDRC-RICORD-1c5 (6) ChestX-ray86 (7) TCIA7 (8) SIRM8.\nAdam optimizer was utilized for training all deep learning frameworks and models. The learning rate was fixed at 1e-4 while the batch size was fixed at 24. A total of 100 epochs were run on NVIDIA RTX A6000 GPUs, with all methods implemented in PyTorch."}, {"title": "5.3 Evaluated Models", "content": "The effectiveness of CCVA-FL was evaluated by constructing several learning frameworks for comparison:\nCentralized learning (CL):. Centralized learning stands as a unifying approach in machine learning where the amalgamation of training data from diverse client sources converges into a singular global model [5]. This methodology operates under the premise that privacy constraints do not hinder the training phase. Within this framework, the entirety of training datasets, contributed by N individual clients, are put together to train a single model. By merging these varied datasets, a comprehensive, aggregated dataset emerges, forming the foundation for the creation of a unified global model.\nFederated learning (FL):. Federated Averaging is a decentralized machine learning methodology designed to address privacy concerns by allowing multiple clients to collaboratively train a global model without directly sharing their raw data [25]. This methodology operates in iterations, starting with the distribution of a global model to participating clients. Each client then trains the model locally using its private data. Rather than transmitting raw data, only model updates in the form of weight gradients are shared with a central server or aggregator. Upon receiving these updates, the aggregator computes the average of these gradients, creating a consensus or federated average. This averaged gradient is subsequently utilized to update the global model, reflecting the collective knowledge gleaned from the diverse client datasets. This process iterates multiple times, refining the global model while preserving data privacy at individual client ends. Federated Averaging ensures the incorporation of insights from various decentralized sources, fostering a collaborative learning paradigm without compromising data privacy.\nCCVA-FL:. However, Federated Averaging does not consider the cross-client feature variation which hampers the development of a robust global server-level model [6]. To overcome this we developed Cross-Client Variation-Adaptive Federated Learning (CCVA-FL), which involves transforming local medical image training data from all clients into a predetermined feature space within the Federated Learning (FL) context. Initially, the target image space was established by evaluating the data complexity of each client, selecting the training images from the client with the least complexity. To communicate this target image space to other clients, we Scalable Diffusion Models with Transformers (DiT) [18] to generate a synthetic images based driven by labelled images of the trget client which effectively embody the traits of the raw images. Following this, the raw training images of each client undergo translation into the target image space which is defined by the previously shared synthesized images ensuring minimal feature space variations across clients. Federated averaging was applied to thesetranslated images from all clients."}, {"title": "5.4 Test Data for Evaluation", "content": "A fixed part of the data at each client was set aside as TEST data. A global test dataset was created by combining the test data of each client. After the training frameworks mentioned in section 5.3, the accuracies were tested on individual client test datasets as mentioned in Tables 1, 2, 3 as well as the global models developed were tested on the heterogenous global test dataset (mentioned as Overall in Tables 1, 2, 3)."}, {"title": "6 RESULTS AND DISCUSSION", "content": "The results of the two-client experiments are presented in Table 1 and Figure 6a. With limited annotated data we observe that the results of vanilla federated averaging as abysmally poor compared to CL as well as our method (CCVA-FL). When only 5% of the training data is labelled at each client followed by subsequent federated averaging, the overall accuracy in the Federated Learning was 72.65% (Table 1). Compared to FL both CL and our Method (CCVA-FL) demonstrate approximately 12% higher accuracy (Figure 6a). However, the privacy concerns are not addressed in CL which is effectively taken care of by CCVA-FL while still reaching comparable accuracy with CL (Table 1, Figure 6a) indicating that in low resource setting with privacy concerns, CCVA-FL is an optimal choice to develop global server level models in the medical domain.\nImportantly, in limited annotated data scenarios, it is evident that CCVA-FL outperforms FL and is nearly as effective as CL. This is likely due to the poor convergence of the vanilla FL framework (due to variance in the feature space which is magnified in limited data settings) compared to CCVA-FL when the data for each client is insufficient for training, making CCVA-FL the optimal strategy. While CL outperforms all methods, privacy is not preserved in this scenario making it a non-optimal solution. The effect of translation is pictorially demonstrated in Figure (5). Here, it is evident that before translational there is a feature distribution difference (5a). Post translation, this difference in feature space representation across different clients is decreased Figure (5b) attesting to the validity of the image-to-image translation process."}, {"title": "6.2 Result of 4-Client Frameworks", "content": "The results of the 4-client experiments are presented in Table 2 and Figure (6b). In scenarios of the low amount of training data for local clients in a federated setting, FL's performance is hampered by the inter-site variations in the feature space. Consequently, the overall accuracy of vanilla federated averaging is lower by 4.23% with only 5% of expert annotated local data as compared to CL (Table 2) and Figure (6b). As opposed to FL, CCVA-FL demonstrates improved performance of all clients, approaching CL's level. As the number of representations of local data points for training increases (Table 2) and Figure (6b), the performance of all three methods improves due to better representations; however, FL continues to perform substantially poorly compared to CCVA-FL attesting to the fact that translation of images to a common feature space improves accuracy in a decentralized setting. Based on the similar performance of CL and CCVA-FL, the proposed framework is effective in reducing variation."}, {"title": "6.3 Result of 8-Client Frameworks", "content": "The results of the 8-client experiments are shown in Table 3 and Figure (6c). Vanilla federated learning continues to perform poorly in terms of accuracy on the test data indicating that data distribution differences at the client hamper its performance. This feature space distribution difference can be negated through CL where all the dataset is put together. Contrary to this, CCVA-FL demonstrated improved performance while achieving accuracies similar to CL. When CCVA-FL is compared to the quantitative results obtained with 2, 4, and 8 clients, it is clear that the performance of CCVA-FL is quite stable even with increasing the number of clients. In the Figure (6c), CL, FL, and CCVA-FL performance is compared. It is clear that CCVA-FL can significantly enhance FL performance especially with limited training data."}, {"title": "7 LIMITATIONS", "content": "In our research, we present a novel method that exhibits enhanced performance compared to traditional Federated Averaging. However, the current application of CCVA-FL is limited to variations in Chest X-ray data among different clients. The experiment's scope can be broadened to include diverse imaging modalities like brain MRI and CT scan. Consequently, the adaptability of the framework to other medical image classification tasks remains uncertain. Noteworthy is our experimentation involving 2, 4, and 8 clients, but it's important to recognize potential scalability challenges in real-world scenarios with numerous clients possessing varied computational resources. Moreover, while our machine learning task primarily centers on classification, there is an opportunity to explore additional tasks. The framework utilizes a CycleGAN to convert raw images into the target image space defined by shared synthesized images. The success of this translation process may depend on the quality and diversity of raw images and the complexity of the target image space. It's crucial to emphasize that our framework doesn't explicitly tackle potential biases or limitations within the underlying data sources. These factors could impact the accuracy and generalizability of the classification results. Therefore, future efforts should address these concerns to enhance the robustness and reliability of the proposed framework."}, {"title": "8 CONCLUSION", "content": "Federated learning (FL) is an efficient and effective method of training machine learning algorithms from decentralized image data from different sources with privacy preservation. However, for multisource medical image data, the cross-client feature space variation problem has not been adequately addressed. To address the problem in medical image data, we propose a cross-client variation-adaptive federated learning framework (CCVA-FL). In CCVA-FL before updating the global model, each client first translates its labelled training images into a common image spaceto minimize the feature variations among images from different clients. To attain image-to-image translation while safeguarding data privacy, the method involves generating images using Scalable Diffusion Models with Transformers (DiT). While upholding data privacy this process relies on the synthetic images of a chosen client, and only the synthesized images, not the raw ones, are shared with all other clients. The proposed CCVA-FL framework significantly improves the performance of the existing FL framework when classifying pneumonia from multi-source decentralized X-ray images. Although we applied the CCVA-FL framework primarily to classification tasks in this paper, we believe it can be applied to a wide range of medical imaging applications."}]}