{"title": "EnvBridge: Bridging Diverse Environments with Cross-Environment Knowledge Transfer for Embodied AI", "authors": ["Tomoyuki Kagaya", "Yuxuan Lou", "Thong Jing Yuan", "Subramanian Lakshmi", "Jayashree Karlekar", "Sugiri Pranata", "Natsuki Murakami", "Akira Kinose", "Koki Oguri", "Felix Wick", "Yang You"], "abstract": "In recent years, Large Language Models (LLMs) have demonstrated high reasoning capabilities, drawing attention for their applications as agents in various decision-making processes. One notably promising application of LLM agents is robotic manipulation. Recent research has shown that LLMs can generate text planning or control code for robots, providing substantial flexibility and interaction capabilities. However, these methods still face challenges in terms of flexibility and applicability across different environments, limiting their ability to adapt autonomously. Current approaches typically fall into two categories: those relying on environment-specific policy training, which restricts their transferability, and those generating code actions based on fixed prompts, which leads to diminished performance when confronted with new environments. These limitations significantly constrain the generalizability of agents in robotic manipulation. To address these limitations, we propose a novel method called EnvBridge. This approach involves the retention and transfer of successful robot control codes from source environments to target environments. EnvBridge enhances the agent's adaptability and performance across diverse settings by leveraging insights from multiple environments. Notably, our approach alleviates environmental constraints, offering a more flexible and generalizable solution for robotic manipulation tasks. We validated the effectiveness of our method using robotic manipulation benchmarks: RLBench, MetaWorld, and CALVIN. Our experiments demonstrate that LLM agents can successfully leverage diverse knowledge sources to solve complex tasks. Consequently, our approach significantly enhances the adaptability and robustness of robotic manipulation agents in planning across diverse environments.", "sections": [{"title": "1 INTRODUCTION", "content": "The development of Large Language Models (LLMs) has remarkably advanced various fields, demonstrating impressive capabilities in understanding and generating human-like text. Methods such as Chain of Thought (CoT) (Wei et al. (2023)) and ReAct (Yao et al. (2023)) have showcased the high reasoning capabilities of LLMs, while models like OpenAI-01 have further enhanced these abilities. Building on these strengths, researchers have explored the integration of LLMs into intelligent agents capable of complex decision-making tasks. These LLM agents leverage sophisticated language understanding and reasoning abilities to perform a wide range of functions, from customer service chatbots to complex problem-solving systems, demonstrating their versatility and potential.\nAmong the various applications of LLM Agents, robotic manipulation stands out as a particularly promising area. Recent studies such as EmbodiedGPT (Mu et al. (2023)) combine LLM with vi-"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 ADVANCEMENTS IN REASONING WITH LLMS", "content": "LLMs have made significant strides in natural language processing tasks, including text generation, summarization, translation, and tasks that require elements of reasoning such as reading, comprehension, and question answering. Though their capacity for reasoning and complex problem-solving is impressive, there is still much room for improvement. To enhance their reasoning skills, researchers are exploring techniques like hierarchical reasoning, where complex problems are broken down into smaller sub-problems. Wei et al. (2023) developed chain-of-thought prompting, a strategy that prompts LLMs to reason by articulating their thought process step-by-step. Multi-agent discussion, as demonstrated in Debate (Du et al. (2023)), involves multiple LLMs working together to reason collaboratively. To further improve LLM reasoning, incorporating external knowledge is crucial. Approaches like knowledge graph integration (Pan et al. (2024)) and Retrieval-Augmented Generation (Lewis et al. (2021)) allow LLMs to access and leverage structured information, leading to more accurate and informed responses."}, {"title": "2.2 LLM AGENTS WITH MEMORY", "content": "LLM-based agents can interact with humans using natural language, offering a more flexible and transparent interface. The memory module is crucial for enabling an agent to acquire, accumulate, and use knowledge through interactions with its environment, achieved via three key operations: memory reading, writing, and reflection. Memory reading extracts useful information from past experiences to guide future actions, while memory writing stores environmental data for later use. Memory reflection allows agents to evaluate their cognitive processes. Unlike traditional LLMs, agents must learn and act in dynamic environments, necessitating a module that aids in planning future actions. RAP (Kagaya et al. (2024)) dynamically leverages past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities. SayPlan (Ahn et al. (2022)) is an embodied agent tailored for task planning. In this system, scene graphs and environmental feedback function as the agent's short-term memory, directing its actions. The Generative Agent (Park et al. (2023)) utilizes a hybrid memory structure to support the agent's behavior. Its short-term memory holds contextual information about the agent's present situation, while its long-term memory stores past behaviors and thoughts, which can be accessed based on current events."}, {"title": "2.3 \u0415\u043c\u0432ODIED AGENTS", "content": "Recent works have developed more efficient reinforcement learning agents for robotics and embodied artificial intelligence to enhance agents' abilities for planning, reasoning, and collaboration in embodied environments. Hierarchical planning models (e.g., Sharma et al. (2022)) represent a type of approach where a high-level planner leverages LLMs to generate subgoals, and a low-level planner carries out their execution. The LLMs outline a sequence of subgoals aimed at reaching the final objective specified by the language instruction, while the low-level planner translates these subgoals into appropriate actions for the given environment. Dasgupta et al. (2023) proposes a similar idea of a unified system for embodied reasoning and task planning, combining high-level commands with low-level controllers for effective planning and action execution. Furthermore, various approaches that utilize the high code-generation capabilities of LLMs for robotic manipulation are being researched. These include methods for grounding actions available within the environment (Singh et al. (2022)), attempting error recovery in case of failures (Duan et al. (2024)), and learning skills from a lifelong learning perspective (Tziafas & Kasaei (2024)). These methods do not require pre-training and can take advantage of the flexible reasoning abilities of LLMs, making them highly notable. However, their applications across various environments are still limited."}, {"title": "3 ENVBRIDGE: CROSS-ENVIRONMENT KNOWLEDGE TRANSFER AND RE-PLANNING", "content": "In this section, we introduce how we design and combine Cross-Environment Knowledge Transfer, Re-Planning method with LLM code generation to build our agent EnvBridge. Figure 1 provides an"}, {"title": "3.1 PROBLEM FORMULATION", "content": "In this work, we consider an agent operating in a particular environment $E_0$ and assigned with completing some task given a free-form language instruction $l$. To achieve this goal, the agent needs to generate robot trajectories $\\tau_l$ according to $l$."}, {"title": "3.2 CODE GENERATION FOR ROBOT MANIPULATION", "content": "We use LLMs to generate Python code that is executed by an interpreter to decompose the language instruction into subtasks, invoke perception APIs and generate robot trajectories. Our Code Generation pipeline follows the design from VoxPoser (Huang et al. (2023)), which comprises 3 LLM-calling steps:\nStep 1: The Planner is responsible for decomposing the language-instruction guided task (e.g., \"press the light switch\") into sub-tasks (e.g., \"grasp the button\", \"move to the center of the button\").\n$\\text{Planner}(l) = (l_1, l_2..., l_n)$"}, {"content": "Step 2: The Composer takes in sub-task instruction $l_i$ (e.g., \"grasp the button\", \"move to the center of the button\") and invokes necessary low-level language model program (LMP). Each low-level LMP is responsible for unique functionality (e.g., parsing query objects in dictionary form, generating affordance map in Numpy array from Composer parametrization).\n$\\text{Composer}(l_i) = (LMP_1(l_i), LMP_2(l_i), ..LMP_k(l_i))$\nStep 3: The Low-Level LMPs will be executed to interact with certain perception APIs to generate necessary value maps such as affordance and avoidance maps. These value maps will then be used together to find a sequence of end-effector positions serving as robot trajectories.\n$\\text{exec}_{LMP}(Composer(l_i)) = \\tau_{l_i}$"}, {"title": "3.3 \u039c\u0395\u039cORY CONSTRUCTION AND RETRIEVAL", "content": "Although Code Generation method can be applied to different embodied environments, we observe a high failure rate in solving tasks in unseen environments. The challenges mainly come from two perspectives. The high-level planner struggles to decompose unseen tasks in new environments and the composer fails to assign proper low-level LMPs. These reveal LLM's limitations when reasoning in complex zero-shot scenarios. Aiming to build a robust embodied agent that can function in various environments with high efficacy, we leverage on traditional human behavior: learning from successful experiences in previous familiar environments and transferring knowledge to similar tasks in unseen scenarios. To enhance the ability for our agent, we build our Memory and Retrieval mechanism, inspired by the Retrieval-Augmented Generation (Lewis et al. (2021)) framework."}, {"title": "3.3.1 MEMORY COMPONENT", "content": "Here, we describe in detail the contents stored in the memory component shown in Figure 1. During the execution of tasks in the Code Generation pipeline, our system records the details of each task, including the environment, the input instructions, the generated code, and the outcomes (successes or failures). After execution and evaluation, only successfully executed codes are saved into our memory. We place heavier emphasis on Planner-level and Composer-level code as they are highly related to the specific task instruction and can be flexible, while low-level LMPs code has less variation in terms of instructions. Therefore, we save only Planner and Composer-level code in our memory.\nEach successful log L contains the following information: The environment E, the code-level c (Planner or Composer), the instruction l (task instruction on Planner-level and decomposed sub-task instruction on Composer-level), and the successfully executed code. Examples of the information recorded in memory are provided in Appendix A.3, showcasing the types of data our system stores.\n$L_{\\text{planner}} = {E, c = \\text{planner}, l, (l_1, l_2..., l_n))}$"}, {"title": "3.3.2 MEMORY RETRIEVAL", "content": "When encountering a new task T, EnvBridge can query the memory to find relevant successful logs that can be adapted and helpful at the two stages of code generation.\nAssuming we are at the code generation stage $c_r$, the instruction is $l_T$. For each log in the memory $L_j = {E, c_j, l_j, \\text{Code}_j}$, we will calculate a similarity score between the instructions from the memory log and the current instruction of task T.\n$\\text{Score}(L_j) = \\mathbb{1}{c_r=c_j} * \\text{cos\\_sim}(l_T,l_j)$\nHere, the similarity score is calculated as the cosine similarity between the text embedding of current task instruction and logged instruction, if they are on the same code generation level ($c_r = c_j$); otherwise, it is 0. Then we retrieve the codes with the highest similarity scores. For generating these text embeddings, we employ sentence-transformers (Reimers & Gurevych (2019)).\nAfter retrieval, EnvBridge transfers the retrieved codes and applies them as prompts for agents to generate more accurate code in the current task. This will be further explained in Section 3.4.\n$\\text{Retrieved}(\\text{Code}_1, \\text{Code}_2, ..\\text{Code}_r) = TOP_{\\text{Score}(L_j)}{\\text{Code}_j}$"}, {"title": "3.4 RE-PLANNING WITH TRANSFERRED KNOWLEDGE", "content": "In this section, we introduce two important designs to apply retrieved code from source environments to LLM code generation in the target environment: Knowledge Transfer and Re-Planning. In Section 3.4.1, we introduce how Knowledge Transfer minimize the difference in robotic manipulation code between source and target environments. In Section 3.4.2, we introduce the instances and the form we apply the adapted retrieved code to better leverage in-context learning for code generation."}, {"title": "3.4.1 \u039a\u039dOWLEDGE TRANSFER", "content": "Robot foundation models such as RT-X (Collaboration (2024)) learn by integrating diverse robotic manipulation data, enabling them to operate in various environments. This clearly demonstrates that common information is shared across different tasks and environments. Building on these insights, we apply Cross-Environment Knowledge Transfer for robotic planning by adapting the retrieved code to different target environments. This process minimizes differences between environments and focuses on transferring the essential insights needed for the target context.\nThis process is illustrated in the left and center of Figure 2. Here, code examples from the target environment are provided as prompts, and the retrieved code is adapted to suit the target environment by LLMs. This allows us to convert environment-specific plans (such as initializing a robot arm at the beginning) and environment-dependent information like coordinates and scales to match the target environment. The prompt used for knowledge transfer is provided in Appendix A.2.2."}, {"title": "3.4.2 RE-PLANNING", "content": "Using the code adapted by Knowledge Transfer, EnvBridge proceeds with Re-Planning. Figure 2 shows this process progressing from the middle to the right. If the initially generated code fails, the agent utilizes the most similar retrieved code in LLM inference context to generate new code. If this new attempt also fails, the agent uses the second most similar code to generate the code again. In this manner, EnvBridge incorporates new ideas from similar knowledge to perform Re-Planning. By combining initial prompt examples with those retrieved from memory, we leverage in-context learning. This approach allows us to maintain code quality while integrating new insights.\nThis methodology is akin to the technique for generating new ideas discussed by Lu et al. (2024) and demonstrates that leveraging archived ideas or insights for the target task is a viable approach. Consequently, this method enhances the agents' capabilities, improving their flexibility and creativity in task resolution. Detailed prompt illustrating this process is provided in Appendix A.2.3."}, {"title": "4 EXPERIMENTS", "content": "To assess the effectiveness of EnvBridge, we conducted experiments across three representative embodied agent benchmarks: RLBench, MetaWorld, and CALVIN."}, {"title": "4.1 RLBENCH: MAIN RESULTS COMPARED WITH BASELINES", "content": "RLBench(James et al. (2019)) is a robot learning benchmark for tabletop environments with various tasks. We sampled 10 tasks from it and conduct evaluations covering various tasks, instructions, and objects. Each task comprises 20 trials, with instructions randomly selected from those supported by RLBench. GPT-40-mini was used for this evalution.\nWe compare EnvBridge with Voxposer(Code Generation baseline) and two Re-Planning baselines: Retry and Self-Reflection. Retry indicates re-executing without making any changes if the previous trial fails. As the method for Self-Reflection, we used a novel conventional approach (Shinn et al. (2023)) to generate a plan for the next trial. Additionally, to extend Self-Reflection to RLBench, we create the prompts ourselves and use the images obtained from RLBench as Observations to generate the next plan. The prompts used for generating Self-Reflection are described in the Appendix A.2.4. In the proposed method, we conduct the evaluation using memory constructed from successful codes in the CALVIN benchmark (Mees et al. (2022)). While detailed settings for the CALVIN benchmark are described in Section 4.3, we used the successful codes from 200 tasks in CALVIN, executed with the Retry Re-Planning method, to build the memory. Moreover, for methods excluding the baseline, the success rate is calculated based on up to three trials.\nThe evaluation results are shown in the Figure 3. From these results, our proposed method performs better than Self-Reflection. Furthermore, while our method requires memory from other environments, Self-Reflection necessitates manual creation of prompts for reflection for each environment. Therefore, our method is more generalized for expanding the application range of robotic operations.\nWhen analyzing individual task performance, we observe distinct trends between Self-Reflection and EnvBridge. Self-Reflection shows improved performance on tasks partially addressed by the baseline, such as BeatTheBuzz and CloseDrawer. In contrast, EnvBridge demonstrates significant improvements on tasks with low baseline performance, like PushButton and TakeLidOffSaucepan. This difference arises because Self-Reflection enhances the generation of better code for tasks that are already partially solvable, whereas EnvBridge can solve previously unsolvable tasks by incorporating new insights from other environments. Additionally, specific examples of success and failure with EnvBridge are provided in Appendix A.5."}, {"title": "4.2 \u039c\u0395\u03a4AWORLD: EVALUATION WITH VARIOUS KNOWLEDGE SOURCES", "content": "MetaWorld (Yu et al. (2021)) is an open-source simulated robotics environment designed for multi-task and meta-learning scenarios. It provides a suite of benchmark manipulation tasks of varying complexity, all sharing a common robot arm and basic physics. MetaWorld's standardized task set and simulated table robotic manipulation scenario are similar to the settings in RLBench, making it an appropriate benchmark environment to evaluate EnvBrdige's transferability across environments.\nWe select five tasks from MetaWorld's standardized task set for evaluation. Assembly, Button-Press, Drawer-open, Hammer, and Window-close. One instruction is given for each task. For each task, we evaluate with 20 trials. GPT-40 is used for this evalution.\nFor MetaWorld, we evaluate EnvBridge's performance with different sources of knowledge. They are in-domain knowledge (memory from MetaWorld), transferred knowledge (memory from RL-Bench) and unified knowledge (a combined memory from MetaWorld and RLBench). We compare EnvBridge under various settings with Voxposer, which is the LLM code generation agent baseline.\nFrom Figure 4 we can see that with transferred knowledge from RLBench, EnvBridge can surpass the perfomance of robotic manipulation code generation baseline, which proves EnvBridge's efficacy in the unseen environment. Furthermore, if EnvBridge can retrieve successful experiences from"}, {"title": "4.3 CALVIN: EVALUATION ON INSTRUCTION ROBUSTNESS", "content": "CALVIN (Mees et al. (2022)) is a robot manipulation benchmark for language-conditioned tasks. It allows for the construction of long-horizon tasks, which are specified solely through natural language. To maintain a consistent number of subtasks, we chose 200 single tasks from the validation dataset instead of long-horizon tasks, with GPT-40-mini as the LLM for the evalution. For comparison, we use the learning-based model provided by CALVIN, known as multi-context imitation learning (MCIL) (Lynch & Sermanet (2021)), as well as two Re-Planning methods: Retry and EnvBridge. Additionally, CALVIN includes 15 types of tasks, primarily involving the movement of blocks, with each type having a single instruction assigned to it. To assess robustness to variations in instructions, we conducted two types of evaluations. The first evaluation uses a single instruction, specifically the one provided by CALVIN. The second evaluation uses multiple instructions, where we generated five additional paraphrased instructions for each task using GPT-40.\nAs shown in Table 1, the results indicate that in the evaluation with a single instruction, Retry method performs slightly better. However, when evaluated with paraphrased instructions, the performance of Retry degrades, whereas our method demonstrates better performance. These results indicate that while the effect is less observable in environments with limited variations in instructions and tasks, such as CALVIN with single instructions, our method is confirmed to be effective in more complex environments with greater variations, like CALVIN with paraphrased instructions and RLBench."}, {"title": "5 ABLATION STUDY", "content": ""}, {"title": "5.1 KNOWLEDGE TRANSFER", "content": "In EnvBridge, Knowledge Transfer is a crucial process for connecting different environments. Therefore, we evaluated its impact on performance. In this evaluation, we assessed the scenario without Knowledge Transfer described in Section 3.4.1, directly using the code from different environments for Re-Planning. The results are shown in Figure 5. These results confirm that Knowledge Transfer improves performance and enables smooth transfer of insights across various environments."}, {"title": "5.2 \u039c\u0395\u039cORY RETRIEVAL", "content": "In the Memory-Retrieval mechanism, it is important to efficiently utilize the various codes stored in memory. In this section, to verify the effectiveness of our similarity-based query retrieval, we compare it with the scenario where codes are randomly selected. The results are shown in Figure 5. These results demonstrate that our method improves performance compared to random choice, confirming that selecting the most relevant codes will effectively improve the performance."}, {"title": "5.3 \u039c\u0395\u039cORY COMPARISON", "content": "Memory is an important component in EnvBridge, and performance varies depending on the codes stored. We evaluated the performance differences due to different memories. Specifically, we constructed and assessed memories from three different benchmarks: RLBench, MetaWorld, and CALVIN. The results are shown in Table 2. All results with EnvBridge show improved performance compared to Retry. Interestingly, the performance with memory from CALVIN outperformed that"}, {"title": "5.4 ACCURACY IMPROVEMENT WITH TRIALS", "content": "EnvBridge promotes the generation of new code by incorporating insights from different environments. To verify whether EnvBridge enables effective Re-Planning, we examine trends in accuracy improvement with the number of trials. The results are shown in Figure 6. The figure illustrates that the accuracy of the EnvBridge method improves over the number of trials and consistently outperforms the Retry method. This demonstrates that EnvBridge is more effective in achieving accurate results through its Re-Planning strategy."}, {"title": "6 CONCLUSION AND LIMITATIONS", "content": "We propose EnvBridge, a novel method for transferring knowledge from various environments to target environment in robotic manipulation agents. EnvBridge stores successful control codes from one environment in memory and transfers this knowledge to other environments to apply it, thereby enhancing the agent's adaptability and performance. Our approach demonstrates superior performance in robotic manipulation benchmarks. While EnvBridge improves adaptability to different environments, effectively organizing the accumulating data in memory for lifelong learning scenarios and handling diverse information such as images and audio remain important challenges for practical applications. In conclusion, EnvBridge utilizes information from various environments,"}, {"title": "ETHICS STATEMENT", "content": "We have read the ICLR Code of Ethics and ensured this paper follows it. Our work does not involve the release of any new datasets; all benchmark datasets used in this research, specifically RLBench, MetaWorld, and CALVIN, are publicly available. We believe our work will have a positive societal impact by enhancing the adaptability and robustness of robotic manipulation agents, which can lead to more efficient and safer autonomous systems in various real-world applications."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 PARAMETERS FOR EVALUATION", "content": ""}, {"title": "A.2 PROMPTS IN RLBENCH", "content": ""}, {"title": "A.2.1 PROMPT FOR CODE GENERATION", "content": "The prompt is exactly the same as VoxPoser."}, {"title": "A.2.2 PROMPT FOR KNOWLEDGE TRANSFER", "content": ""}, {"title": "A.2.3 PROMPT FOR RE-PLANNING", "content": ""}, {"title": "A.2.4 PROMPT FOR SELF-REFLECTION", "content": ""}, {"title": "\u0391.3 \u039c\u0395MORY EXAMPLES FROM RLBENCH", "content": ""}, {"title": "A.4 BENCHMARK DETAILS", "content": ""}, {"title": "A.4.1 RLBENCH", "content": ""}, {"title": "\u0391.4.2 \u039cETAWORLD", "content": ""}, {"title": "A.5 QUALITATIVE ANALYSIS ON RLBENCH", "content": "We provide specific success and failure examples as part of our qualitative evaluation. In the success case, the Planner generates detailed subtasks. In contrast, in the failure case, it produces almost identical subtasks. We believe this issue arises because the memory lacks the appropriate code to solve the tasks, preventing the presentation of suitable examples."}]}