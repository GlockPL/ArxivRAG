{"title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical Planning and Control", "authors": ["Poppy Collis", "Ryan Singh", "Paul F Kinghorn", "Christopher L Buckley"], "abstract": "An open problem in artificial intelligence is how systems can flexibly learn discrete abstractions that are useful for solving inherently continuous problems. Previous work has demonstrated that a class of hybrid state-space model known as recurrent switching linear dynamical systems (rSLDS) discover meaningful behavioural units via the piecewise linear decomposition of complex continuous dynamics (Linderman et al., 2016). Furthermore, they model how the underlying continuous states drive these discrete mode switches. We propose that the rich representations formed by an rSLDS can provide useful abstractions for planning and control. We present a novel hierarchical model-based algorithm inspired by Active Inference in which a discrete MDP sits above a low-level linear-quadratic controller. The recurrent transition dynamics learned by the rSLDS allow us to (1) specify temporally-abstracted sub-goals in a method reminiscent of the options framework, (2) lift the exploration into discrete space allowing us to exploit information-theoretic exploration bonuses and (3) 'cache' the approximate solutions to low-level problems in the discrete planner. We successfully apply our model to the sparse Continuous Mountain Car task, demonstrating fast system identification via enhanced exploration and non-trivial planning through the delineation of abstract sub-goals.", "sections": [{"title": "1. Introduction", "content": "In a world that is inherently continuous, the brain's apparent capacity to distil discrete concepts from sensory data repre- sents a highly desirable feature in the design of autonomous systems. Humans are able to flexibly specify abstract sub-goals during planning, thereby reducing problems into manageable chunks (Newell & Simon, 1972; Gobet et al., 2001). Furthermore, they are able to transfer this knowledge across new tasks; a process which has proven a central challenge in artificial intelligence (d'Avila Garcez & Lamb, 2023). Translating problems into discrete space offers distinct advantages in decision-making. Namely, the computationally feasible application of information-theoretic measures (e.g. information-gain), as well as the direct implementation of classical techniques such as dynamic programming (LaValle, 2006; Friston et al., 2023). One prevalent approach to tackling continuous spaces involves the simple grid-based discretisation of the state-space, however this becomes extremely costly as the dimensionality increases (Coulom, 2007; Mnih et al., 2015). We therefore ask how we might be able to smoothly handle the presence of continuous variables whilst maintaining the benefits of decision-making in the discrete domain.\nTo address this, we explore the rich representations learned by recurrent switching linear dynamical systems (rSLDS) in the context of planning and control. This class of hybrid state-space model consists of discrete latent states that evolve via Markovian transitions, which act to index a discrete set of linear dynamical systems (Linderman et al., 2016). Importantly, a continuous dependency in the discrete state transition probabilities is included in the generative model. By providing an understanding of the continuous latent causes of switches between discrete modes, this recurrent transition structure can be exploited such that a controller can flexibly specify inputs to drive the system into a desired region of the state-action space. By embracing the established control-theoretic strategy of piecewise linear decomposition of nonlinear dynamics, our approach lies in contrast to the comparatively opaque solutions found by continuous function approximators (Liberzon, 2003; Mnih et al., 2015). Using statistical methods to fit these models provides a means by which we can effectively perform online discovery of useful non-grid discretisations of the state-space for system identification and control.\nWe describe a novel model-based algorithm inspired by Ac-"}, {"title": "1.1. Contributions", "content": "\u2022 The enhancement of planning via the introduction of temporally-abstracted sub-goals by decoupling a discrete MDP from the continuous clock time using the emergent representations from an rSLDS.\n\u2022 The lifting of information-seeking decision-making into a (discrete) abstraction of the states enabling efficient exploration and thereby reducing sensitivity to the dimensionality of the task-space."}, {"title": "2. Related work", "content": "In the context of control, hybrid models in the form of piecewise affine (PWA) systems have been rigorously examined and are widely applied in real-world scenarios (Bemporad et al., 2000; Borrelli et al., 2006). Previous work by Abdulsamad et. al. has applied a variant on rSLDS (recurrent autoregressive hidden Markov models) to the optimal control of general nonlinear systems (Abdulsamad & Peters, 2023; 2020). The authors use these models to the approximate expert controllers in a closed-loop behavioural cloning context. While their algorithm focuses on value function approximation, in contrast, we learn online without expert data and focus on flexible discrete planning."}, {"title": "3. Framework", "content": "Here, we provide a overall outline of the approach to approximate control taken with our Hybrid Hierarchical Agent (HHA) algorithm. Consider that we have decomposed the nonlinear dynamics into piecewise affine regions of the state-space using an rSLDS. Should the HHA wish to navigate to a goal specified in continuous space, the recurrent generative model parameters of the rSLDS allow it to identify the discrete region within which the goal resides, thereby lifting the goal into a high-level objective. The agent may then generate a plan at a discrete level, making use of the information-seeking bonuses that this affords. Planning translates to specifying a sequence of abstract sub-goals. Again using the recurrent generative model, the agent can specify, for each sub-goal region, a continuous point in state-space with which to drive the system into. Once in the discrete goal region, the agent straightforwardly navigates to the continuous goal. The following sections detail the components of the HHA. For additional information, please refer to Appendix. A"}, {"title": "3.1. rSLDS(ro)", "content": "In the recurrent-only (ro) formulation of the rSLDS, the discrete latent states $z_t \\in \\{1,2, ..., K\\}$ are generated as a function of the continuous latents $x_t \\in \\mathbb{R}^M$ and the control input $u_t \\in \\mathbb{R}^N$ via a softmax regression model\n$P(Z_{t+1}|X_t, U_t) = softmax(W_xX_t+W_uu_t+r)$ (1)\nwhereby $W_x$ and $W_u$ are weight matrices with dimensions $\\mathbb{R}^{K \\times M}$ and $r$ is a bias of size $\\mathbb{R}^K$. The continuous dynam-"}, {"title": "3.2. Discrete planner", "content": "We have a Bayesian Markov Decision Process (MDP) (Vlassis et al., 2012) described by $\u041c\u0432 = (S, A, Pa, R, Po)$. S represents the set of all possible discrete states of the system and are essentially a re-description of the discrete latents $Z$ found by the rSLDS. A is the set of all possible actions which, in our case, is equal to the number of states S. The state transition probabilities, $Pa (st+1 | St = s, at = a, 0) ~ Cat(as)$, and are parameterised by $0 \\in \\mathbb{R}^{sxsxa}$ for which we maintain Dirichlet priors over, $p(\\theta_{as}) ~ Dir(aas)$, facilitating directed exploration. Due to conjugate structure, as the agent obtains new empirical information, Bayesian updates amount to a simple count-based update of the Dirichlet parameters (Murphy, 2012). Importantly, the structure of the state transition model has been constrained by the adjacency structure of the polyhedral partitions extracted from recurrent transition dynamics of the rSLDS: invalid transitions are assigned zero probability while valid transitions are assigned a high probability (see A.5). R is the reward function which, translated into the Active Inference framework, acts as a prior distribution over rewarding states providing the agent with an optimistic bias during policy inference (Millidge et al., 2020; Parr et al., 2022).\nThe discrete planner outputs a discrete action, where the first action is taken from a receding horizon optimisation:\n$a_o = arg \\min_{\\@_{1:T}} J(a_{1:T})$ (4)\n$J(a_{1:T}) = \\mathbb{E}[\\sum_{t=0}^T R(s_t, a_t) + IG_t(a) | s_0, a_{1:T}]$. (5)\nThis includes an explicit information-seeking incentive $IG_t(a)$ (see A.4). This descending discrete action $a_0$ is translated into a continuous control prior $x_j$ via the following link function,\n$x_j = arg \\max_{x} P(z = j|x, u)$ (6)\nwhich represents an approximately central point in the desired discrete region j requested by action $a_0$ (see A.6). The ascending messages from the continuous level are translated into a categorical distribution via the rSLDS softmax link function. Importantly, the discrete planner is only triggered when the system switches into a new mode \u00b9. In this sense, discrete actions are temporally abstracted and decoupled from continuous clock-time in a method reminiscent of the options framework (Sutton et al., 1999; Daniel et al., 2016)."}, {"title": "3.3. Continuous controller", "content": "Continuous closed-loop control is handled by a finite-horizon linear-quadratic regulator (LQR) controller. For controlling the transition from mode i to mode j ($x_i$ to $x_j$). The objective of the LQR controller is to minimise the following quadratic cost function:\n$J_{ij} (x) = arg \\min_{\\pi} J_{ij} (\\pi)$ (7)\n$J_{ij} (\\pi) = \\mathbb{E}_{\\pi, \\pi}[(x_s - x_j)^T Q_f (x_s - x_j)$"}, {"title": "4. Results", "content": "To evaluate the performance of our (HHA) model, we applied it to the classic control problem of Continuous Mountain Car. This problem is particularly relevant for our purposes due to the sparse nature of the rewards, necessitating effective exploration strategies to achieve good performance. The HHA is initialised according to the procedure outlined in (Linderman et al., 2016). The rSLDS parameters are then fitted to the observed trajectories every 1000 steps of the environment unless a reward threshold within a single episode is reached.\nWe find that the HHA finds piecewise affine approximations of the task-space and uses these discrete modes effectively to solve the task. Fig.1 shows that while the rSLDS has divided up the space according to position, velocity and control input, the useful modes for solving the task are those found in the position space. Once the goal and a good approximation to the system has been found, the HHA successfully and consistently navigates to the reward.\nFig. 2 shows that the HHA performs a comprehensive exploration of the state-space and significant gains in the state-space coverage are observed when using information-gain drive in policy selection compared to without. Interestingly, even without information-gain, the area covered by the HHA is still notably better than that of the random action control. This is because the non-grid discretisation of the state-space significantly reduces the dimensionality of the search space in a behaviourally relevant way.\nWe compare the performance of the HHA to other reinforcement learning baselines (Actor-Critic and Soft Actor-Critic) and find that the HHA both finds the reward and captilises on its experience significantly quicker than the other models (see Fig. 3). Indeed, our model competes with the state-space coverage achieved by model-based algorithms with exploratory enhancements in the discrete Mountain Car task, which is inherently easier to solve (see A.8)."}, {"title": "5. Discussion", "content": "Through the application of our Hybrid Hierarchical Agent to the Continuous Mountain Car problem, we have demonstrated that rSLDS representations hold promise for enriching planning and control. The emergence of non-grid discretisations of the state-space allows us to perform fast systems identification via enhanced exploration, and successful non-trivial planning through the delineation of abstract sub-goals. Hence, the time spent exploring each region is not equivalent in euclidean space which helps mitigate the curse of dimensionality that other grid-based methods suffer from.\nSuch a piecewise affine approximation of the space will incur some loss of optimality in the long run when pitted against black-box approximators. This is due to the nature of caching only approximate closed-loop solutions to control within each piecewise region, whilst the discrete planner implements open-loop control. However, this approach eases the online computational burden for flexible re-planning. Hence in the presence of noise or perturbations within a region, the controller may adapt without any new computation. This is in contrast to other nonlinear model-based algorithms like model-predictive control where reacting to disturbances requires expensive trajectory optimisation at every step (Schwenzer et al., 2021). By using the piecewise affine framework, we maintain functional simplicity and interpretability through structured representation. This method is amenable to future alignment with a control-theoretic approach to safety guarantees for ensuring robust system performance and reliability.\nWe acknowledge there may be better solutions to dealing with control input constraints than the one given in Sec. 3.3. Different approaches have been taken to the problem of implementing constrained-LQR control, such as further piecewise approximation based on defining reachability regions for the controller (Bemporad et al., 2002)."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Appendix / supplemental material", "content": "A.1. Framework\nOptimal Control\nWe adopt the optimal control framework, specifically we consider discrete time state space dynamics of the form:\n$X_{t+1} = f(x_t, u_t, n_t)$ (9)\nwith known initial condition xo, and nt drawn from some time invariant distribution $N_t ~ D$, where f we assume $P(X_{t+1} | X_t, u_t)$ is a valid probability density throughout.\nWe use ct: X \u00d7 U \u2192 R for the control cost function at time t and let U be the set of admissible (non-anticipative, continuous) feedback control laws, possibly restricted by affine constraints. The optimal control law for the finite horizon problem is given as:\n$J(\\pi) = E_{x_\u03bf,\\pi}[\\sum_{t=0}^T c_t(x_t, U_t)]$ (10)\n$\\pi^* = arg \\min_{\\pi\\in\\Psi} J(\\pi)$ (11)\nPWA Optimal Control\nThe fact we do not have access to the true dynamical system f motivates the use of a piecewise affine (PWA) approximation. Also known as hybrid systems:\n$x_{t+1} = A_ix_t + B_iu_t + E_t$ (12)\nwhen $(x_t, u_t) \\in H_i$ (13)\nWhere H = {$H_\u2081 : i \u2208 [K]$} is a polyhedral partition of the space X \u00d7 U.\nIn the case of a quadratic cost function, it can be shown the optimal control law for such a system is peicewise linear. Further there exist many completeness (universal approximation) type theorems for peicewise linear approximations implying if the original system is controllable, there will exist a peicewise affine approximation through which the system is still controllable (Bemporad et al., 2000; Borrelli et al., 2006).\nRelationship to rSLDS(ro)\nWe perform a canonical decomposition of the control objective J in terms of the components or modes of the system. By slight abuse of notation $[x_t = i] := [(x_t, u_t) \\in H_i]$ represent the Iverson bracket.\n$J(\\pi) = \\sum_t \\int \\sum_{i\\in[K]} [x_t = i]P_{\\pi}(X_t | X_{t-1}, U_t)c_t(x_t, u_t)dx_tdx_{t-1}$ (14)\n$= \\sum_t \\int \\sum_{i\\in[K]}  [x_t = i]P_{\\pi}(X_t | x_{t-1}, U_t)c_t(x_t, u_t)dx_tdx_{t-1}$ (15)\nNow let zt be the random variable on [K] induced by Zt = i if $[x_t = i]$ we can rewrite the above more concisely as,\n$J(\\pi) = \\sum_t \\int \\sum_{i\\in[K]} P_{\\pi} (x_t, Z_{t-1} = i | X_{t-1}, U_t)c_t(x_t,u_t)dx_tdx_{t-1}$ (17)\n$= \\sum_t \\sum_{i\\in[K]} / P(x_t|Z_{t-1} = i | X_{t-1}, U_t)c_t(x_t,u_t)dx_tdx_{t-1}$ (18)\n$= \\sum_t \\sum_{i\\in[K]} \\mathbb{E}_{\\pi} [C_t(x_t, U_t)]$ (19)"}, {"title": "A.2. Hierarchical Decomposition", "content": "Our aim was to decouple the discrete planning problem from the fast low-level controller. In order to break down the control objective in this manner, we first create a new discrete variable which simply tracks the transitions of z, this allows the discrete planner to function in a temporally abstracted manner.\nDecoupling from clock time Let the random variable $(\\zeta)_{t>0}$ record the transitions of $(z_t)_{t>0}$ i.e. let\n$\\tau_s(\\tau_{s-1}) = \\min\\{t : z_{t+1} \u2260 z_t, t > \\tau_{s-1}\\}, \\tau_0 = 0$ (21)\nbe the sequence of first exit times, then $\u03b6_s$ is given by $\u03b6_s = Z_{\u03c4_s}$.\nWith these variables in hand, we frame a small section of the global problem as a first exit problem.\nLow level problem Consider the first exit problem defined by,\n$T_{ij} (x_0) = arg \\min_{\\pi,\\varsigma} J_{ij} (\\pi, x_0, S)$ (22)\n$J_{ij} (\\pi, x_0, S) = \\mathbb{E}_{\\pi,x_0}[\\sum_{t=0}^{S} c(x_t, u_t)]$ (23)\ns.t. $(x_t, u_t) \\in H_i$ (24)\ns.t. $c(x, u) = 0$ when $(x, u) \\in \\partial{H_{ij}}$ (25)\nwhere $\\partial{H_{ij}}$ is the boundary $H_i\\cap H_j$.\nDue, to convexity of the polyhedral partition, the full objective admits the decomposition into subproblems\n$J(\\pi) = \\sum_S J_{\\zeta(s+1),\\zeta(s)} (\\pi)$ (26)\n(27)\nSlow and fast modes The goal is to tackle the decomposed objectives individually, however the hidden constraint that the trajectories line up presents a computational challenge. Here we make the assumption that the difference in cost induced by different starting positions, induces a relatively small change in the minimum cost $J_{ij}$, intuitively this happens if the minimum state cost in each mode is relatively uniform as compared to the difference between regions.\nHigh level problem If the above assumption holds, we let $J_{ij} = \\min_{S_{x_0}} J_{ij}(\\pi, x_0)p(x_0)$ be the average cost of each low-level problem. We form a markov chain:\n$P_{ik}(u) = P(\\zeta_{s+1} = k | \\zeta_s = i, \\tau_j, u^d = j)$ (28)\nand let $P^d$ be the associated distribution over trajectories induced by some discrete state feedback policy, along with the discrete state action cost $c_a(u^d = j, n = i) = J_{ij}$, we may write the high level problem:\n$\\pi^{*d} = \\min_{\\pi^d} J_a(\\pi, \\eta_0)$ (29)\n$= \\mathbb{E}_{P^{\\pi^d}} [\\sum_{s=0}^S c_a(\\eta_s, u)]$ (30)\nOur approximate control law is then given by $\\pi_\u00a1 \\circ \\pi^{*} \\circ id(x)$"}, {"title": "A.3. Offline Low Level Problems: Linear Quadratic Regulator (LQR)", "content": "Rather than solve the first-exit problem directly, we formulate an approximate problem by finding trajectories that end at specific 'control priors' (see A.6). Recall the low level problem given by:\n$T_{ij} (x_0) = arg \\min_{\\pi,\\varsigma} J_{ij} (\\pi, x_0, S)$ (31)\n$J_{ij} (\\pi, x_0, S) = \\mathbb{E}_{\\pi,x_0}[\\sum_{t=0}^{S} c(x_t, u_t)]$ (32)\ns.t. $(x_t, u_t) \\in H_i$ (33)\ns.t. $c(x, u) = 0$ when $(x, u) \\in \\partial{H_{ij}}$ (34)\nIn order to approximate this problem with one solvable by a finite horizon LQR controller, we adopt a fixed goal state, $x^* \\in H_j$. Imposing costs $c_t(x_t, u_t) = u_f R u_t$ and $c_S(x_S,u_S) = (x \u2212 x^*)^TQ_f(x \u2212 x^*)$. Formally we solve,\n$T_{ij} (x_0) = arg \\min_{\\pi,\\varsigma} J_{ij} (\\pi, x_0, S)$ (35)\n$J_{ij} (\\pi, x_0, S) = \\mathbb{E}_{\\pi,x_0}[(x_S - x^*)^T Q_f (x_S - x^*) + \\sum_{t=0}^{S-1} u_f R u_t]$ (36)\n(37)\nby integrating the discrete Ricatti equation backwards. Numerically, we found optimising over different time horizons made little difference to the solution, so we opted to instead specify a fixed horizon (hyperparameter). These solutions are recomputed offline every time the linear system matrices change.\nDesigning the cost matrices Instead of imposing the state constraints explicitly, we record a high cost which informs the discrete controller to avoid them. In order to approximate the constrained input we choose a suitably large control cost R = rI. We adopted this approach for the sake of simplicity, potentially accepting a good deal of sub-optimality. However, we believe more involved methods for solving input constrained LQR could be used in future, e.g. (Bemporad et al., 2000), especially because we compute these solutions offline."}, {"title": "A.4. Online high level problem", "content": "The high level problem is a discrete MDP with a 'known' model, so the usual RL techniques (approximate dynamic programming, policy iteration) apply. Here, however we choose to use a model-based algorithm with a receding horizon inspired by Active Inference, allowing us to easily incorporate exploration bonuses.\nLet the Bayesian MDP be given by $M_B = (S, A, P_a, R, P_0)$ be the MDP, where $P_a(S_{t+1} | S_t, a_t, \\theta) \\sim Cat(\\alpha_{as})$ and $p(\\theta_{as}) ~ Dir(\\alpha)$ We estimate the open loop reward plus optimistic information theoretic exploration bonuses\nActive Inference conversion We adopt the Active Inference framework for dealing with exploration. Accordingly we adopt the notation In $p(s_t, a_t) = R(s_t, a_t)$ and refer to this \u2018distribution\u2019 as the goal prior (Millidge et al., 2020), and optimise over open loop policies $\\pi = (\u03b1_\u03bf, ..., \u03b1_\u0442)$.\n$J(0_{1:T}, s_0) = \\mathbb{E}[\\sum_{t=0}^T R(s_t, a_t) + IG_p + IG_s | s_0, \u03b1_{1:T}]$ (38)\nwhere parameter information-gain is given by $IG_p = D_{KL}[P_{t+1}(\\theta) || p_t(\\theta)]$, with $p_t(\\theta) = p(\\theta | s_{0:t})$. In other words, we add a bonus when we expect the posterior to diverge from the prior, which is exactly the transitions we have observed least (Heins et al., 2022).\nWe also have a state information-gain term, $IG_s = D_{KL}[P_{t+1}(S_{t+1}) || P_t(S_{t+1})]$. In this case (fully observed), $P_{t+1}(S_{t+1}) = \u03b4_i$ is a one-hot vector. Leaving the term $\\mathbb{E}[\u2212 ln p_t(s_{t+1})]$ leading to a maximum entropy term (Heins et al., 2022).\nWe calculate the above with Monte Carlo sampling which is possible due to the relatively small number of modes. Local approximations such as Monte Carlo Tree Search could easily be integrated in order to scale up to more realistic problems."}, {"title": "A.5. Extracting the adjacency matrix from rSLDS", "content": "In order to generate the possible transitions from the rSLDS, we calculate the set of active constraints for each region from the softmax representation, $p(z | x) = \u03c3(Wx + b)$. Specifically to check region i is adjacent to region j we verify the solution linear program:\n$-bj = \\min((Wi \u2212 Wj)x$ (39)\ns.t. $(Wi - Wk)x \u2264 (bi - bk) \u2200k \u2208 [K]$ (40)\ns.t. $x \u2208 (X_{lb}, X_{ub})$ (41)\nWhere $(X_{lb}, X_{ub})$ are bounds chosen to reflect realistic values for the problem. This ensures we only lift transitions to the discrete model, if they are possible. Again, these can be calculated offline.\nWe initialise the entries of the transition model in the discrete MDP for possible transitions to 0.9 facilitating guided-exploration via information-gain through a count-based updates to the transition priors."}, {"title": "A.6. Generating continuous control priors", "content": "In order to generate control priors for the LQR controller which correspond to each of the discrete states we must find a continuous state xi which maximises the probability of being in a desired z:\n$x_i = arg \\max_{X} P(z = ix, u)$ (42)\nFor this we perform a numerical optimisation in order to maximise this probability. Consider that this probability distribution $P(z = ix)$ is a softmax function for the i-th class is defined as:\n$\\sigma(v_i) = \\frac{exp(v_i)}{\\sum_j exp(v_j)}, v_i = W_ix + r_i$ (43)\nwhere wi is the i-th row of the weight matrix, x is the input and ri is the i-th bias term. The update function used in the gradient descent optimisation can be described as follows:\nx \u2190 x + \u03b7 \u2207\u03b1\u03c3(\u03bd\u03b5) (44)\nwhere \u03b7 is the learning rate and the gradient of the softmax function with respect to the input vector x is given by:\n$\\nabla_\u03c3 \u03c3(v_i) = \\frac{\\nabla_\u03c3 \u03c3(v_i)}{\\nabla_{v_i}} \\frac{\\partial v}{\\partial x} = \u03c3(v_i)(e_i \u2212 \u03c3(\u03bd))\u00b7 W$ (45)\nin which \u03c3(v) is the vector of softmax probabilities, and e\u00bf is the standard basis vector with 1 in the i-th position and 0 elsewhere. The gradient descent process continues until the probability P(z = i|x) exceeds a specified threshold @ which we set to be 0.7. This threshold enforces a stopping criterion which is required for the cases in which the region z is unbounded."}]}