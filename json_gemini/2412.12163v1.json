{"title": "Towards LLM-based optimization compilers. Can LLMs learn how to apply a single peephole optimization? Reasoning is all LLMs need!", "authors": ["Xiangxin Fang", "Lev Mukhanov"], "abstract": "Large Language Models (LLMs) have demonstrated great potential in various language processing tasks, and recent studies have explored their application in compiler optimizations. However, all these studies focus on the conventional open-source LLMs, such as Llama2, which lack enhanced reasoning mechanisms. In this study, we investigate the errors produced by the fine-tuned 7B-parameter Llama2 model as it attempts to learn and apply a simple peephole optimization for the AArch64 assembly code. We provide an analysis of the errors produced by the LLM and compare it with state-of-the-art OpenAI models which implement advanced reasoning logic, including GPT-40 and GPT-01 (preview). We demonstrate that OpenAI GPT-01, despite not being fine-tuned, outperforms the fine-tuned Llama2 and GPT-40.\nOur findings indicate that this advantage is largely due to the chain-of-thought reasoning implemented in GPT-01. We hope our work will inspire further research on using LLMs with enhanced reasoning mechanisms and chain-of-thought for code generation and optimization.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have made significant advancements in various software engineering tasks, show-casing their ability to understand and generate human-like code. Numerous studies have demonstrated the capability of these models to handle various high-level programming languages effectively. In code generation, tools like GitHub Copilot [1] and OpenAI Codex [2] have proven their utility by assisting developers in creating relevant code snippets. In code translation, systems such as Facebook's TransCoder [3] and Microsoft's CodeBERT [4] have shown effectiveness in translating code between different programming languages.\nThe use of LLMs for compiler optimizations and code generation is a promising alternative to the classical com-pilers [5]. LLMs can save significant effort and cost in developing new compilers, extending the existing native and binary compilers for new ISAs, supporting new programming languages, improving decompilers [6], [7] or even deriving new ISA-specific compiler optimizations. Recent studies have demonstrated that LLMs can be applied for decompilation [6], [7] and compiling the native C code into X86 [5]. Meta went even further by releasing foundation LLMs for code compilation and optimization [8]. However, these studies focus on investigating the capabilities of conventional LLMs, such as Llama2, which lack of enhanced reasoning mechanisms, to optimize and generate the code. Moreover, these studies often tend to train on IRs or binaries where various combinations of compiler optimizations are applied. This prevents detailed analysis of the LLM errors, making it unclear to what extent these models can truly understand and optimize the code.\nThe main goal of our study is to investigate if advanced reasoning mechanisms, such as chain-of-thought, can help LLMs to improve their code generation and optimization capabilities. We want to understand when and what types of errors are produced by conventional LLMs and if reasoning can help to reduce these errors.\nIn this study, we focus specifically on peephole optimization since LLMs are sensitive to the context window size, which hinders the optimization of code samples containing many in-structions [9]. We exploit the fact that peephole examines and applies optimizations to each basic block where the number of instructions is limited. Thus, we can investigate the strengths of LLMs in handling short instruction sequences, mitigating the limitations of LLMs in handling the context with a big size. We also specifically target peephole optimization since it is a fundamental part of any compiler, performing basic algebraic transformations. Thus, if LLMs cannot perform this optimization correctly, it is unlikely they will be able to handle more sophisticated optimizations.\nThe contribution of this study can be summarized as follows:\n\u2022 we present the results of our experimental study on investigating the capabilities of the 7B-parameter Llama2 model [10] to learn and apply a basic peephole op-timization. Specifically, we fine-tune the model using 100,000 AArch64 basic block samples generated by the LLVM compiler. We investigate the syntactic and output correctness of the code samples optimized by the model using 24,000 basic blocks extracted from Code-Force, Code-Jam and BigCode datasets."}, {"title": "II. BACKGROUND AND FRAMEWORK DESIGN", "content": "Compiler and ISA: Peephole is an optimization technique applied at the basic block level that involves replacing the instructions with a logically equivalent set that has better per-formance [11]. Peephole employs numerous pattern matching algorithms to apply a wide range of optimization scenarios. Its replacements include but are not limited to the following [12] :\n\u2022 Null sequences \u2013 Remove instructions which results are not used.\n\u2022 Combine operations \u2013 Replace multiple operations with a single equivalent one.\n\u2022 Algebraic optimizations \u2013 Apply algebraic laws to sim-plify or rearrange instructions.\n\u2022 Address mode operations - Memory address algebraic optimizations.\n\u2022 Special case instructions \u2013 Use of special instructions and operands, such as zero operand.\nIn our study, we generate AArch64 v8 code samples using the LLVM compiler [13] (version 14.0.0) to train LLMs. LLVM contains two default peephole implementations: In-stcombine [14] and Aggressive-instcombine [15]. The first implementation includes a large number of optimizations listed above, and all the transformations are applied within an optimized basic block [14] (algorithmic complexity is O(1)). Meanwhile, the second implementation is more aggressive, i.e. algorithmic complexity is higher than O(1), and it can also modify CFG.\nGeneration of the training data: To investigate whether LLMs can learn how to apply peephole, we use the LLVM compiler to generate optimized and non-optimized versions of basic blocks. Figure 1 shows the code generation pipeline used in our study. We first compile the source code into IR using the clang/clang++ frontend. The non-optimized IR is generated by invoking opt with the -00 flag. For gen-erating the optimized IR, we use the opt -instcombine command to perform a series of peephole optimizations. These IRs are compiled into non-optimized and optimized AArch64 assembly code using 11c with the -00 flag. Finally, we parse the generated AArch64 assembly codes and match optimized basic blocks with non-optimized ones. We also apply the following pre-processing steps (normalization):\n\u2022 Remove metadata and attributes.\n\u2022 Remove basic blocks with more than 15 lines of instruc-tions2.\n\u2022 Remove duplicate basic blocks with identical instructions.\nWe apply these pre-processing steps to follow previous studies [9].\nThe LLM-based optimization pipeline: Figure 2 demon-strates the LLM-based code optimization pipeline. We first compile the source code and generate AArch64 assembly. Then we apply the pre-processing steps discussed previously to each basic block.\nWe embed each basic block into a specific prompt template to guide LLMs in performing peephole optimization. Once a model generates the optimized basic block, we replace the original basic block with the optimized one and generate a binary\u00b3."}, {"title": "III. EVALUATION OF THE BASELINE MODEL", "content": "In this section, we present the evaluation results for our baseline model and compare our results with the previous studies. We aim to replicate the training and evaluation strategy used in a recently presented study [9].\nA. Model Evaluation Metrics\nTo assess the performance of the baseline model, we use conservative natural language evaluation metrics. These met-rics provide a quantitative measure of how closely the code optimized by LLMs aligns with the code optimized by the compiler. In addition to these metrics, we also use metrics to evaluate the syntactic correctness and the accuracy of the IO output for code generated by LLMs.\nBLEU Score: The BLEU (Bilingual Evaluation Under-study) score [23] is commonly used in machine translation to evaluate the similarity between machine-generated sentences and reference sentences. In our context, it measures how closely the basic blocks optimized by LLMs match the basic blocks optimized by the compiler.\nExact Match Rate (EMR): This metric also enables us to compare the code samples generated by LLMs and code samples generated by the compiler. To be more specific, it measures the character-to-character complete matching rate for the basic blocks generated by LLMs and the basic blocks generated by the compiler. In other words, EMR only gives credit if the entire generated output exactly matches the reference output, making it a more binary metric than BLEU; an exact match implies a BLEU score of 1.\nSyntactic Accuracy: The syntactic accuracy measures the proportion of basic blocks generated by LLMs that can be correctly compiled without errors by LLVM. This metric ensures that the generated instructions are syntactically correct.\nIO Accuracy: IO accuracy is used to check whether, for a given set of inputs, the code generated by LLMs produces the same output as the code optimized by the compiler. IO accuracy has been validated as an effective approach in prior research on code generation [5].\nImportantly, we use these metrics to follow the previous studies exploiting LLMs for code generation [5], [9], enabling us to verify our results.\nB. Fine-tuning\nTo fine-tune the baseline model, we use C and C++ code samples from the LLVM and GCC test suites [24].\nWe specifically use these suites since both suites were designed to test the code generation and optimization function-ality of the compilers. Thus, we expect that these test suites ideally fit for the purposes of fine-tuning covering the majority of possible code optimization scenarios.\nWe extracted 100,000 basic blocks (27 million tokens in total) from the test suites to fine-tune the Llama2 model. The code generation pipeline (see Figure 1) is applied to create both optimized and non-optimized basic block samples. We found that 6 epochs is optimal for our study, as increasing the number of epochs yields minimal changes in the target metrics, i.e. BLEU, EMR, Syntactic Accuracy and IO Accuracy. Note that the context window size is 512 tokens.\nC. Evaluation using the LLVM/GCC test suite\nSimilar to the previous study [9], we first evaluate the model throughout the fine-tuning process. To be more specific, after every 10,000 samples used for fine-tuning we assess the model on a holdout validation set. This set consist of 5000 basic block samples that have not been used for training."}, {"title": "D. Evaluation using Code-Force, Code-Jam and BigCode", "content": "In this subsection, we present the results of the performance evaluation of the baseline model using a large dataset that combines thousands of algorithms implemented on major coding competition platforms. Our dataset is derived from two algorithmic competitions: Code-Force and Google Code Jam, with a particular focus on solutions written in C and C++. We have extracted the source code from GitHub applying pre-processing steps to remove duplicate code samples. In addi-tion, we employed the BigCode's The Stack public repository, which is dedicated to training large language models with code [25].\nTo extract basic blocks, we have processed about 300,000 C/C++ source code files, but we were able to compile and generate binaries only for 106,082 files. The detailed in-formation about the number of extracted source code files and basic blocks is provided in Overall, we have managed to extract 437,892 basic blocks for which we can generate the binaries. Due to limitations in our computing resources, we randomly select 24,000 basic blocks to evaluate the performance of the fine-tuned Llama2 model.\nE. Results\nFigure 4 demonstrates BLEU, EMR, Syntactic accuracy and IO accuracy measured for 24,000 basic blocks optimized by the fine-tuned Llama2 model. Our first observation is that BLEU is almost the same for all the datasets, about 0.87, and it is slightly lower than BLEU measured in our previous experiments. However, EMR drops significantly compared to the previous experiments, and it varies from 0.27 to 0.39. This is explained by the fact that the training set significantly differs from the large testing set, and, as a result, the LLM cannot predict accurately the output generated by the compiler. However, we see only slight degradation of Syntactic Accuracy and IO accuracy drops compared to the previous experiments. These results suggest that the LLM has some generalization capabilities, as EMR decreases while the quality of the gener-ated code, in terms of Syntactic Accuracy and IO Accuracy, remains almost the same.\nImportantly, these results are aligned with the results ob-tained in the previous studies [5], [9]. For example, it is shown that the model translating C code into x86 instructions achieves a BLEU score of 0.79, a Syntactic Accuracy of 0.58, and an IO Accuracy of 0.33 [5]."}, {"title": "F. Error analysis", "content": "Next, we conduct a systematic analysis of errors produced by the LLM. The model produced a total of 8,922 compilation errors. Note that we focus specifically on compilation errors, as they are easier to identify the sources of. We randomly select 1,000 of these errors to make a systematic analysis. shows the distribution of 1,000 compilation errors across four different error types which we discuss below.\nOpcode errors refer to invalid instruction codes, invalid instruction names, or inappropriate instruction names for a particular use generated by the LLM. An example of such errors is provided in Table V. In this particular case, the LLM generated and instruction instead of mov instruction. Note that this type of error occurs most frequently in our experiments (see Table IIIa).\nImmediate Value Errors are related to the errors intro-duced by the LLM when using literal constants.\nFor example, Table V shows incorrect usage of literal con-stants. To be more specific, mov instruction should initialize register wo with 0 using register wzr. However, it generates #r instead of wzr. Note that # denotes an immediate value, and r should be a literal constant in this particular case.\nLabel Errors represent the errors due to incorrect labels and function names in assembly code. For example, in Table V, adpr instruction should load the address of .L.str label. However, the LLM generated an incorrect label, i.e. Lstrstr.\nRegister Errors represent the errors induced by the LLM due to incorrect usage of registers. For example, in Table V mov instruction uses 32-bit register w8 instead of 64-bit register x8.\nNext, we further investigate the errors that occurred most frequently in our experiments, i.e. opcode errors. We estimated"}, {"title": "IV. EVALUATION OF LLMS WITH ADVANCED REASONING MECHANISMS", "content": "To investigate if LLMs can increase the rate of correct transformation by enabling reasoning mechanisms, we tested OpenAI GPT-40 and GPT-01 (preview) which have advanced reasoning capabilities [18].\nA. The fine-tuned Llama2 vs GPT-40\nWe first evaluated OpenAI's GPT-40 model using our Code-Force, Code-Jam and BigCode datasets, as discussed in Sec-tion III-D. Since we cannot fine-tune GPT-40 (the source code is not available), we use 3-shot context learning prompts before requesting the model to optimize the code following the same pipeline as for the Llama2 model, see Figure 2. Note that for all the optimization requests we use absolutely the same 3 learning prompts.\nTo motivate the use of prompt-based training, we conducted a series of experiments optimizing 20 randomly chosen basic blocks. We also selected 5 pairs of optimized and non-optimized basic blocks, which are randomly sampled, and use them as prompts. shows how the average EMR (the exact match rate for basic blocks generated by the LLM and blocks optimized by the compiler), measured across 20 basic blocks, changes with the number of prompts. We see that EMR grows more than 12x after applying the 3-shot prompt learning, and it reaches EMR about 60%.\nNote that EMR does not change when the number of shot prompts has increased above 3. Based on these results, we use the 3-shot prompt learning to evaluate GPT-40 using 24,000 basic blocks extracted from Code-Force, Code-Jam and BigCode datasets.\nWe compare the fine-tuned Llama2 and GPT-40 using four key metrics: BLEU, EMR, IO accuracy, and Syntactic accuracy. Figure 6 shows the evaluation results. We see that the fine-tuned Llama2 outperforms GPT-40 in terms of BLEU and EMR metrics. This can be explained by the fact that the Llama2 model was specifically fine-tuned on peephole optimization code samples, aligning the code generated by the model with that produced by the specific compiler used in our study. Nonetheless, GPT-40 demonstrates a higher Syntactic accuracy and IO accuracy in our experiments which implies that this model has better capabilities to generate syntactically and semantically correct code. We attribute this to the fact that GPT-40 has a larger number of parameters and was trained on a diverse data.\nFinding: Based on these results, we conclude that BLEU and EMR metrics widely used in NLP [26], [27]studies do not really serve the purposes of code quality evaluation. Thus, the use of these metrics should be reconsidered in future research studies on code generation.\nB. Llama2 vs GPT-40 vs GPT-01\nIn our final experiments, we tested the performance of an OpenAI model with the most advanced reasoning capabilities, i.e. GPT-01 (preview). Unfortunately, GPT-01 is a paid model, which limits the number of code samples we could test in our study. To address this, we selected 200 most complicated code samples from 24,000 samples (Code-Force, Code-Jam and BigCode datasets) used for testing the fine-tuned Llama2 and GPT-40. Note that we selected these samples in a such a way that the fine-tuned Llama2 model generates the code which differs from the the code produced by LLVM and produce errors in 190 samples; and we chosen 10 samples where both Llama2 and GPT-40 generate the correct code which is similar to the code generated by LLVM (200 samples in total). Such a division of the samples enables us to test GPT-01 with both types of block samples, those correctly and those incorrectly optimized by Llama2.\nTable VI shows the evaluation results for all models tested with 24,000 samples (ALL) and 200 selected samples (Selected)."}, {"title": "V. LIMITATIONS AND FUTURE RESEARCH.", "content": "There are several limitations in our study which we believe are important to discuss.\nDef-use graph across different basic blocks. In our study, LLMs optimize each basic block separately. Note that we specifically use Instcombine optimization which is applied to each basic block without additional transformation of CFG. However, removed instructions can have uses in other basic blocks. Thus, LLMs can produce instructions which arguments are initialized incorrectly. Nonetheless, we expect that LLMs should account for this (their advanced reasoning mechanisms should enable it) by defining the result registers for removed instructions when those registers might be used in other basic blocks. Note that the instructions without replacement can be removed only in the terminal CFG nodes, i.e. containing ret as in the example from Table VII.\nRegister allocation. Since LLMs only see the instructions within an optimized basic block, they can use only the registers that are used within that block. This is due to the fact that LLMs do not have the information about liveliness of all the registers.\nThe entire program representation. Overall, we admit that that there could be also other limitations due to the fact that LLMs do not see the entire program code. Moreover, in certain cases, runtime support may be required, similar to what is implemented in binary translators. However, we note that the main goal of our study was to investigate whether LLMs have sufficient capabilities to perform a simple peephole optimization and whether enhanced reasoning mechanisms can improve the models' performance.\nIn our future research, we will investigate the optimization capabilities of LLMs using a full program representation."}, {"title": "VI. RELATED WORK", "content": "Large Language Models (LLMs) have shown their effi-ciency in performing various tasks ranging from high-level code generation to compiler optimization and advanced rea-soning.\nLarge Language Models for Code Generation. In recent years, there has been a growing interest in leveraging Large Language Models (LLMs) for tasks involving source code generation. Models such as Copilot [1], Codex [2], TransCoder [3], CodeBERT [4], Code Llama [28], StarCoder [25], [29], Magicoder [30] and DeepSeek-Coder [31] have significantly advanced this field. These models support developers with tasks like code completion, generation, and translation across multiple programming languages. Open-sourced models like Code Llama and StarCoder have further empowered the com-munity to adapt and fine-tune LLMs for specific software engineering needs.\nLarge Language Models for Compilers. While LLMs have demonstrated significant potential in high-level code tasks, fewer models operate at the compiler level, particularly with code generation and compiler optimization. Most recent studies have focused on traditional machine learning methods for compiler optimization [32]\u2013[38]. Neural machine tranlation techiques have been employed to transform code between different representations, previous examples include compiling C to X86 assembly [5] and decompiling assembly language to C [6], [7]. These works utilized smaller models or other deep learning methods. There are a few works related to using LLM at the compiler level. Examples include using large models for decompilers [39]\u2013[41], LLVM-IR passes prediction with IR optimization [9], compiler fuzzing tests [42], [43].\nRecent Developments in Reasoning. Enhancing the rea-soning capabilities of LLMs has been a focus of recent research, particularly through methods like chain-of-thought (CoT) prompting. CoT encourages models to generate inter-mediate reasoning steps, improving performance on complex tasks such as mathematical reasoning and question answering [44]. Chen [45] proposed the chain-of-code (CoC) method. This approach enables models to handle tasks that require both logical and semantic reasoning, broadening the scope of ques-tions LLMs can answer by thinking in code. Experiments demonstrated that chain-of-code outperforms chain-of-thought and other baselines across various benchmarks. Gu et al. [46] introduced CRUXEval (Code Reasoning, Understanding, and Execution Evaluation), a benchmark specifically designed to test input-output reasoning in code. Wang [47] introduced OpenR, an open-source framework designed to enhance the reasoning capabilities of LLMs through reinforcement learning and process supervision. OpenR achieves advanced reasoning capabilities beyond traditional auto-regressive methods. To the best of our knowledge, there are no research studies which specifically investigate the enhanced reasoning mechanisms of LLMs for compiler optimizations."}, {"title": "VII. CONCLUSION", "content": "In this paper, we investigate whether conventional LLMs, such as Llama2, can perform basic peephole compiler op-timization, aiming to understand their performance and the errors they produce. Our findings suggest that one of the reasons for these errors is absence of enhanced reasoning capabilities. To explore this, we tested most advanced AI models from, OpenAI GPT-40 and GPT-01, that implement such capabilities. Our results show that GPT-01, despite not being fine-tuned, significantly outperforms both fine-tuned Llama2 and GPT-40. We attribute this improvement to the chain-of-thought reasoning implemented in GPT-01. We hope our work will inspire further research on using LLMs with en-hanced reasoning mechanisms and chain-of-thought for code generation and optimization."}]}