{"title": "InductionBench: LLMs Fail in the Simplest Complexity Class", "authors": ["Wenyue Hua", "Tyler Wong", "Sun Fei", "Liangming Pan", "Adam Jardine", "William Yang Wang"], "abstract": "Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as ol and 03 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at a solution. In contrast, inductive reasoning, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce InductionBench, a new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even the most advanced models available struggle to master the simplest complexity classes within the subregular hierarchy of functions, highlighting a notable deficiency in current LLMs' inductive reasoning capabilities. Coda and data are available https://github.com/Wenyueh/inductive_reasoning_benchmark.", "sections": [{"title": "INTRODUCTION", "content": "The remarkable progress of large language models (LLMs) in recent years has yielded substantial improvements in their reasoning capabilities. This progress is most evident in benchmarks involving complex mathematics (Cobbe et al., 2021; Hendrycks et al., 2021) and coding tasks (Jain et al., 2024; Jimenez et al., 2023; Chen et al., 2021; Fan et al., 2023). Beyond these domains, researchers have also explored the logical reasoning abilities of LLMs from various angles, including propositional logic (Zhu et al., 2023), first-order logic (Han et al., 2022; Parmar et al., 2024), and propositional logic under different contexts (Hua et al., 2024).\nDespite significant progress in model capabilities, existing benchmarks predominantly focus on deductive reasoning, largely overlooking inductive reasoning. The former requires applying explicitly defined premises to derive valid conclusions, whereas the latter requires inferring the underlying principles, rules, or patterns from observations (Hawthorne, 2004). Both forms of reasoning are essential; inductive reasoning, in particular, is critical in domains such as scientific discovery where researchers seek to characterize natural laws based on empirical data (Gr\u00fcnwald, 2007; Hansen & Yu,"}, {"title": "RELATED WORK", "content": "Deductive Reasoning. One major branch of reasoning benchmarks centers on deductive inference, where models apply established premises to derive specific conclusions. Notable examples include ReClor (Yu et al., 2020), which evaluates the ability to solve logical reasoning questions resembling those found in standardized tests, and various logic-based benchmarks of increasing complexity, from propositional logic to first-order logic (Han et al., 2022; Parmar et al., 2024; Zhu et al., 2023; Hua et al., 2024). These tasks typically require handling structured logical relationships with minimal ambiguity in how premises lead to conclusions.\nAnother type of reasoning benchmarks is mathe-\nInductive Reasoning. Despite the diversity of existing benchmarks, inductive reasoning, where models hypothesize and generalize patterns from examples without pre-specified rules, remains"}, {"title": "COMPUTATIONAL COMPLEXITY IN INDUCTIVE REASONING", "content": "InductionBench uses string-to-string transformation/functions as a proxy to study inductive reasoning, which has established computational complexity hierarchy (Roche & Schabes, 1997; Engelfriet & Hoogeboom, 2001). We focus on the subregular hierarchy, the hierarchy under regular functions. Though with limited expressive power, our experiments show that these classes already present substantial challenges for LLMs.\nSpecifically, we limit our attention to three classes of deterministic regular functions-Left Output-Strictly-Local (L-OSL), Right Output-Strictly-Local (R-OSL), and Input-Strictly-Local (ISL), whose positions in the subregular hierarchy are illustrated in Figure 2 (Heinz, 2018). These classes represent the lowest-complexity tier for string-to-string mappings within the subregular hierarchy. They are proper subclasses of subsequential function class and, more broadly, of weakly-deterministic class and non-deterministic class, which are themselves subsets of the regular function classes. Although we do not elaborate on the complete regular function hierarchy here, it is important to note that the ISL, L-OSL, and R-OSL classes are among the simplest in this framework.\nStrictly local functions can be seen as operating with a fixed amount of look-ahead, similar to Markov processes. They are provably learnable in polynomial time from polynomially sized samples (Chandlee et al., 2014; De La Higuera, 1997; Chandlee et al., 2015; Jardine et al., 2014). Moreover, prior work has shown that an algorithm exists to learn the unique (up to isomorphism) smallest subsequential finite-state transducer that represents such ISL, L-OSL, R-OSL functions (Satta & Henderson, 1997; Arasu et al., 2009). This property allows us to evaluate not only whether LLMs can discover the correct patterns but also whether they can identify the simplest or most concise representation consistent with the data."}, {"title": "PRELIMINARY", "content": "Before providing the definitions of the three function classes, we first introduce the fundamental mathematical notations and formal definitions underpinning our discussion of string-to-string transformations and their properties."}, {"title": "FUNCTION CLASS DEFINITION", "content": "Based on the concepts outlined above, we define the three function classes.\nDefinition 1 (ISL) A function f is ISL if there is a k such that for all $u_1,u_2 \\in \\Sigma^*$, if $\\text{SUFF}_{k-1}(u_1) = \\text{SUFF}_{k-1}(u_2)$, then $\\text{TAILS}_f(u_1) = \\text{TAILS}_f(u_2)$.\nIn simpler terms, this means that the output at each position in the string depends only on the preceding k 1 characters of the input, making the transformation Markovian with respect to the input. Below is a simple example:\nExample 3.1 Suppose a function f : {a,b}* \u2192 {a,b}* rewrites each b to a only if it appears after the input substring ba. In this scenario, we have k = 3, and there are two distinct tails:\n$\\text{TAILS}_f(w) = {(\\lambda, \\lambda), (b, a), (bb, ab), (ab, ab) . . . }$, $\\forall w \\in \\Sigma^*$ such that ba \u2208 $\\text{SUFF}(w)$ and\n$\\text{TAILS}_f(w') = {(\\lambda, \\lambda), (a, a), (bb, bb), (ab, ab) ... }$, $\\forall w'\\in \\Sigma^*$ such that ba \u2209 $\\text{SUFF}(w')$\nThese tails indicate how the function's behavior shifts depending on whether the immediate context ends in ba. Such context-dependent tails also highlights that ISL functions can be effectively characterized or represented by local input constraints.\nDefinition 2 (L-OSL) A function f is L-OSL if there is a k such that for all $u_1, u_2 \\in \\Sigma^*$, if $\\text{SUFF}_{k-1}(f(u_1)) = \\text{SUFF}_{k-1}(f(u_2))$, then $\\text{TAILS}_f(u_1) = \\text{TAILS}_f(u_2)$.\nIn other words, the output at each position in the transformed string depends only on the preceding k - 1 characters of the output itself, rather than on the input. This property can be understood as a form of Markovian process on the output. Below is a simple example:\nExample 3.2 Suppose a function f rewrites each b to \u03bb only if it appears after the output substring ba. In this scenario, we have k = 3, and there are two distinct tails:\n$\\text{TAILS}_f(w) = {(\\lambda, \\lambda), (a, a), (b, \\lambda), (bb, \\lambda), (ab, ab), (ba, a), . . . }$ $\\forall w \\in \\Sigma^*$ such that ba \u2208 $\\text{SUFF}(f(w))$ and\n$\\text{TAILS}_f(w') = {(\\lambda, \\lambda), (a, a), (b, b), (bb, bb), (ab, ab), (ba, ba) . . . }$ $\\forall w \\in \\Sigma^*$ such that ba \u2209 $\\text{SUFF}(f(w))$ while L-OSL depends preceding output symbols to the \"left\", R-OSL functions depends on a limited number of future output symbols to the \"right\". Conceptually, one can view R-OSL as analogous to L-OSL, except that the input is processed in reverse order. Although both belong to the broader OSL paradigm, they are incomparable classes: each can express transformations the other cannot. The formal definition of R-OSL follows:"}, {"title": "LEARNABILITY", "content": "The three function classes are identifiable in polynomial time using a polynomially sized characteristic sample (Chandlee et al., 2014; 2015). In other words, there exists a polynomial-time algorithm that, given sufficient data for a target function f, can produce a representation \u03c4 that satisfies f(w) = \u03c4(w) for every w \u2208 \u03a3*. In other words, once sufficient data is presented, one can reliably recover a function equivalent to f on all possible inputs. This learnability property underpins the value of these classes as testbeds for inductive reasoning, since the data requirement remains polynomial and successful inference is theoretically guaranteed.\nWe formalize \"sufficient data\" as the minimal set of input-output pairs needed to learn a k-strictly local function f, which is known as characteristic sample. Adapting the original definition\u00b9 for clarity (Chandlee et al., 2014; 2015), we define:\nDefinition 4 (Characteristic Sample) For a given k-ISL f, the characteristic sample S is defined as {(w, w') | w \u2208 \u03a3\u2264k > f(w) = w'}. For a given k-OSL f, the characteristic sample S is defined as {(w, w') | w' \u2208 \u03a3\u2264k > f(w) = w'}."}, {"title": "UNIQUE FUNCTION REPRESENTATION", "content": "Beyond verifying that a model can accurately discover a function from data, we also investigate how succinctly the model describes its inferred rules. This aspect is of both theoretical and practical interest: a minimal or most concise representation not only offers interpretability advantages but can also reflect the model's capacity for truly generalizable, rather than merely enumerative, learning.\nOne function can be represented or written in a non-unique way. For instance, consider an ISL function $f_1$ with k = 2 over \u2211 = {a,b} that maps the input character a to b when it comes after b, that rewrites each a to b only if the preceding character is b, while leaving other substrings unchanged. One concise description is:\n$f_1(w)=\\begin{cases}    f_1(w_1)b^{-1}f_1(aw_2), & \\text{if } w_1 \\text{ ends with b and } w=w_1 aw_2 \\text{ for some } w_1, w_2\\in\\Sigma^*\\\\    w, & \\text{otherwise}  \\end{cases}$\nAn alternative yet more verbose description of the same function might redundantly enumerate multiple cases:\n$f'(w)=\\begin{cases}    f'(w_1)b^{-1}f_1(aw_2), & \\text{if } w_1 \\text{ ends with ab and } w = w_1aw_2 \\text{ for some } w_1, w_2 \\in \\Sigma^*\\\\   f'(w_1)b^{-1}f_1(aw_2)   & \\text{if } w_1 \\text{ ends with bb and } w = w_1aw_2 \\text{ for some } w_1, w_2 \\in \\Sigma^*\\\\   w, & \\text{otherwise}  \\end{cases}$"}, {"title": "RULE-BASED REPRESENTATION", "content": "To streamline the generation and parsing of function representations, we employ a simplified notation wherein each transformation is written as \"condition \u2022 target character \u2192 output of the target character\" (Bird & Ellison, 1994). In this notation, the condition represents the minimal substring needed to trigger a transformation, while any input substring not matching this condition remains unchanged. For instance, in the earlier example, this approach permits a concise notation b\u25e6 a \u2192 b, indicating that the input a is mapped to b when it comes after b; otherwise, the input string remains unaltered. This concise, rule-based format simplifies both the model's output generation (by reducing complex functional descriptions) and our subsequent evaluation, as the applicable transformations can be easily parsible and verified.\nTo demonstrate the simplicity of rule-based representation: given an ISL function $f_2$ with k = 2, the input a becomes b when it comes after b and two consecutive as will be reduced to one single a. The minimal function representation is as below:\n$f_2(w)=\\begin{cases}    (f_2(w_1)b^{-1}f_2(aw_2), & \\text{if } w_1 \\text{ ends with b and } w=w_1aw_2 \\text{ for some } w_1, w_2\\in\\Sigma^*\\\\    f_2(w_1)b^{-1}f_2(aw_2),   & \\text{if } w_1 \\text{ ends with a and } w=w_1aw_2 \\text{ for some } w_1, w_2\\in\\Sigma^*\\\\   w, & \\text{otherwise}  \\end{cases}$"}, {"title": "BENCHMARK CONSTRUCTION", "content": "In this section, we detail how our benchmark is constructed from the previously defined function classes. Each datapoint (D, f) in the benchmark is a pair of dataset D and function f where D is a set of input-output pairs generated by f.\nEach of ISL, L-OSL, and R-OSL classes can be further subdivided into incremental levels of complexity, determined by three key parameters: (1) the context window size k (2) the vocabulary size |\u03a3| (3) the minimal representation length of the function, i.e. the minimal set of rules corresponding to the function. Given k and |\u03a3|, the search space is $2^{|\\Sigma|^k}$; given the number of rules n additionally, the search space is $\\binom{2^{|\\Sigma|^k}}{n}$. To rigorously evaluate LLMs' inductive capabilities, we systematically vary these parameters across ISL, L-OSL, and R-OSL function classes.\nIn addition, we examine how performance changes with different numbers of input-output pairs in the prompt. Although having the characteristic sample present should theoretically guarantee recoverability of the underlying function, our empirical results indicate that the overall number of examples strongly affects performance. While extra data can provide richer information, it also increases context length considerably and heightens processing demands (Li et al., 2024). By varying the number of provided datapoints, we further investigate the extent to which the model engages in genuine reasoning and how robust its inductive abilities remain under changing input sizes.\nFunction Generation To systematically create benchmark instances, we first randomly generate functions f based on the three parameters: k, |\u03a3|, and the number of minimal rules describing f by generating the set of rules that can describe f. While multiple representations of varying length can"}, {"title": "MAIN EXPERIMENT", "content": "Experiment Setting We evaluate using zero-shot chain-of-thought prompting on six SOTA LLMS, including Llama-3.3-70b (Dubey et al., 2024) with FP8 quantization, Llama-3.1-405b with FP8 quantization, GPT-40 (Hurst et al., 2024), DeepSeek-V3 (Liu et al., 2024a), 01-mini(Jaech et al.,"}, {"title": "FUNCTION CLASS DEFINITION", "content": "Let \u2211 be a finite alphabet. We denote by \u2211* the set of all finite strings over \u2211, and by \u2211\u2264n the set of all strings over \u2211 of length at most n. The empty string is denoted as A. The set of prefixes of a string w is denoted as PREF(w), defined as {p \u2208 \u03a3* | \u2203s \u2208 \u03a3*s.t.w = ps}, and the set of suffixes of w denoted as SUFF(w), defined as {s \u2208 \u03a3* | \u2203p \u2208 \u2211*s.t.w = ps}. The longest common prefix of a set of strings S is denoted as LCP(S), defined as\n$\u03a1\u0395\u03a0_{\u03c9\u03b5\u03c2} PREF(w) such as \\forall p' \u2208 N_{wes}PREF(w), |p'| < |p|.$\nFor any function f : \u03a3* \u2192 \u0393* and w \u2208 \u03a3*, let the tails of w with respect to f be defined as\n$TAILS f(w) = {(y, v) | f(wy) = uv \\text{ and } u = LCP(f(w\u03a3^*))}.$\nIntuitively, TAILS f(w) collects all possible continuations (y, v) by appending y to w. It summarizes how f might extend beyond the partial input w. The total number of distinct tails across all strings in \u03a3* provides a measure of how many different non-trivial local transformation f encodes."}, {"title": "GENERAL CONSISTENCY", "content": "Given f represented by a set of rules Rf : \u2200ri,rj \u2208 Rf, Ci \u2022 ui \u2209 SUFF(cj \u2022 uj) and cj \u2022 uj \u2209 SUFF(ci \u2022 ui).\nGeneral Consistency ensures that the rules do not contradict one another or become redundant when conditions overlap. For instance, a function whose rule-based representation of r1 : a\u25e6b \u2192 a and r2 : aa\u25e6b \u2192 a is redundant, as the scenarios where r1 is applied is a superset of the scenarios where r2 is applied. For another instance, there does not exist a deterministic function that can be described by r1 : a\u25e6b \u2192 a and r2 : aa\u25e6b \u2192 \u03bb. Generating rule-based representations for ISL functions needs only satisfy this constraint."}, {"title": "OSL NON-REDUNDANCY GUARANTEE", "content": "Given f represented by a set of rules R : \u2200ri \u2208 Rf,\n$E{S_i|S_i\\in c_i}$ such that $\\exists r_j \\in R_f$ such that s' = $c_j u_j$, unless $\\exists r_k \\in R$ such that $c_k$ \u2022 $u_k = Si$.\nConstraint 2 is specific to the two OSL function classes because we need to make sure that all output conditions in the rule actually surface somewhere in the outputs of some datapoints. If the output condition c never actually surface as the output, the rule will never be put into effect. Thereby the above rule basically requires that condition part of all rules can surface, either because it will never be modified by some other rule, or it emerges on the surface because of the application of other rule. For instance, a function represented by rules r1 : aa \u2022 b \u2192 a, r2 : a \u2022 a \u2192 c is redundant because r1 will never be applied because the string aa will never surface as output and thus it will never be put into effect; For another instance, a function represented by r1 : aa \u2022 b \u2192 a, r2 : a \u2022 a \u2192 c, r3 : a \u2022 d \u2192 a is non redundant because even though into aa string will be modified into ac, but aa will surface in some datapoint because ad will be modified into aa and thus r1 will be able to be applied.\nGenerating the functions following the two constraints, we ensure that the generated function representation is minimal, non-reducible guarantees a clear measure of complexity. One additional requirement is imposed to ensure each function indeed requires a look-ahead of size k. Specifically:"}, {"title": "K-COMPLEXITY GUARANTEE", "content": "Given f whose designated context window k = $k_1$, \u2203r' e R such that $c' \u2022 u' \u2192 v'$ such that |c' \u25e6 u'| = $k_1$.\nThis condition guarantees that the function is genuinely k-strictly local (for ISL or OSL), rather than being representable with a smaller window size. Consequently, the functions we generate faithfully reflect the intended complexity level."}, {"title": "Impact of k.", "content": "Across all models, moving from k = 2 to k = 4 markedly reduces recall, precision, and compatibility. This trend underscores how increasing the context window increases the complexity of the underlying ISL functions and making it more challenging for current LLMs to learn the correct transformations. Longer look-ahead requires the model to track additional input context, which can overload its capacity to induce reliable rules."}, {"title": "Impact of \u03a3.", "content": "In contrast, enlarging the vocabulary from |\u2211 = 2 to |\u03a3 = 4 does not consistently degrade performance to the same degree as increasing k. While some models exhibit slight declines in recall or precision with a larger alphabet, these effects are neither as uniform nor as pronounced as those induced by a bigger Markov window. This finding suggests that the breadth of symbol variation matters less than the depth of sequential dependencies."}, {"title": "Impact of the Number of Rules.", "content": "Notably, the number of minimal rules can substantially affect compatibility. When k = 2 and |\u03a3| = 2, a comparatively small search space, changing the number of rules does not drastically alter compatibility. However, under more demanding scenarios where k \u2208 {3,4}, the data indicate that adding rules can cause compatibility to plummet. In many cases, having just one rule still yields nontrivial compatibility, whereas introducing a second or third rule often overwhelms the models, resulting in compatibility scores near 0."}, {"title": "Robustness", "content": "We assess the stability of inductive reasoning by varying the number of input-output pairs provided to the model. The x-axis represent s where S is the minimal set of examples needed to guarantee learnability of the underlying function. The hypothesis is that if the model were performing genuine logical or inductive reasoning, we would expect performance to remain stable or even improve as more data points become available, since these points should further clarify the underlying function. Figure 5 illustrates how average compatibility decreases steeply as the number of provided input-output examples increases. This drop suggests that the LLM's reasoning process is not robustly inductive: rather than refining its hypothesis with additional data, the model appears to become confused or overwhelmed, leading to poorer overall performance. Consequently, these findings highlight the limited robustness of current LLMs' inductive reasoning, particularly in scenarios where increasing the available data should theoretically facilitate, rather than hinder, function inference."}, {"title": "Error Type Analysis", "content": "We further examined the specific types of errors made by LLMs when their predicted functions failed to match the ground-truth dataset. At a high level, we distinguish between missing rules (leading to low recall) and wrong rules (leading to low precision).\nMissing Rules: These refer to ground-truth rules that do not appear in the model's predicted rule set. We classify missing rules into three subtypes:\n1. Too General. Although a certain ground-truth ruler: c \u2022 u \u2192 v was missed, there exists a corresponding predicted rule r' : c' \u2022 u' \u2192 v' that over-generalizes. Specifically, the condition c' is a proper suffix of c, causing r' to apply more broadly than intended.\n2. Too Specific. The opposite of the above: a predicted rule condition c' is a proper extension of c, thus applying too narrowly and failing to match some instances that should have been captured by the ground-truth rule.\n3. Completely Missed. No predicted rule over-generalizes or under-generalizes the ground-truth rule; in other words, this pattern is simply absent from the predicted rule set altogether.\nWrong Rules: These refer to rules present in the model's predicted set that do not exist in the ground truth. We categorize such rules into four types:\n1. Too General. The rule r' : c' \u2022 u' \u2192 v' is overly broad, applying in contexts where the ground truth does not. This typically arises when c' is a proper suffix of some genuine condition c and thus fails to capture necessary constraints.\n2. Too Specific. The rule narrowly addresses only a subset of the intended patterns (e.g., by employing a condition c' that is an extension of the legitimate condition c), thereby missing broader contexts that should have matched.\n3. Correct Condition but Wrong Transformation. Here, the predicted rule accurately identifies the correct condition c' and target input character u', but the transformation v' is incorrect.\n4. Completely Wrong. None of the above criteria apply: the rule's condition and transformation are both inconsistent with the ground truth, indicating a fundamental misunderstanding."}, {"title": "SUMMARY OF FINDINGS", "content": "Overall, our experiments reveal four main insights into the inductive reasoning performance of current LLMs:\n\u2022 Context window size k dominates complexity: Increasing k from 2 to 4 significantly degrades recall, precision, and compatibility, underscoring how longer look-ahead windows intensify the complexity of ISL functions.\n\u2022 Number of Rules increases difficulty under large hypothesis space: The number of minimal rules required can drastically lower compatibility in more challenging settings with large k adn |\u03a3|, indicating that managing multiple interacting rules overwhelms many models."}, {"title": "LEADERBOARD BASED ON INDUCTIONBENCH", "content": "To facilitate straightforward comparisons among different LLMs, we introduce a two-part benchmark leaderboard: a standard leaderboard and an exploration leaderboard. The standard leaderboard is based completely on the three function classes we talked about, and this leaderboard simply presents an aggregated score to directly reflect LLM's performance. The exploration leaderboard includes a slightly new design of function class and we will present the motivation and details below."}, {"title": "STANDARD LEADERBOARD", "content": "The standard leaderboard consists of 1,080 questions spanning three classes of deterministic regular functions: ISL, L-OSL, and R-OSL in equal proportion. Specifically, it includes:\n\u2022 360 ISL questions,\n\u2022 360 L-OSL questions,\n\u2022 360 R-OSL questions.\nWithin each function class, we have settings for k \u2208 {2,3,4}, |\u03a3| \u2208 {5,6,7,8}, and number of rules \u2208 {3,4,5}. Each unique parameter combination has 10 data points, totaling 360 points per function class. The performance metrics recall, precision, and compatibility are computed on a per-setting basis. We then form an overall weighted average to account for variations in function-space size:\nDefinition 8 For a given setting characterized by (k, |\u03a3|, r), the weight w is defined as $\\frac{|\\Sigma|^k}{\u03a3_{k=4s=8}^{k=2 s=5}}$, where k is the Markov window, || is the alphabet size, and r is the minimal rule count.\nFor each function class (ISL, L-OSL, R-OSL), we compute a weighted recall, precision, and compatibility according to the above scheme and then take the average of these three scores to produce the final leaderboard score for that class. The overall score across all three classes is the average of those class-wise scores.\nTable 3 summarizes current leaderboard results for several representative models. Notably, even 03-mini achieves only a 5.69% compatibility score, largely because none of the models succeed on tasks where k = 4. Since those high-complexity settings receive substantially larger weights than cases where k \u2208 {2,3}, they disproportionately reduce the overall average.\nTo balance the influence of different complexity settings, we additionally report an alternative evaluation metric that replaces each original weight with its logarithm. This approach dampens the dominance of k = 4 scenarios, yielding a more even distribution of weights across the benchmark's parameter space."}, {"title": "EXPLORATION LEADERBOARD", "content": "A key concern in using subregular function classes (e.g., ISL, L-OSL, R-OSL) is that polynomial-time learning algorithms already exist for these classes, potentially allowing a trivial \u201chack\u201d to achieve artificially high performance. Though we advocate not using the provbly correct algorithm for task solving so that we can genuinely evaluate LLM's inductive reasoning ability, to make sure, we introduce an exploration leaderboard that focuses on Input-Output Strictly Local (IOSL) functions: a more speculative class for which no known algorithm can reliably learn the entire function from finite data in finite time.\nRationale. Since IOSL lacks a proven polynomial-time learning procedure, successful performance here would more credibly reflect genuine inductive reasoning rather than the application of a known \"shortcut\" algorithm. Furthermore, IOSL functions have not been deeply studied in the literature, offering an opportunity to see whether LLMs can advance this open research area.\nThis is the definition of IOSL:\nDefinition 9 (IOSL) A function f is IOSL if there is a k such that for all $u_1, u_2 \\in \\Sigma^*$, if $\\text{SUFF}_{k-1}(u_1) = \\text{SUFF}_{k-1}(u_2)$ and $\\text{SUFF}_{k-1}(f(u_1)) = \\text{SUFF}_{k-1}(f(u_2))$, then $\\text{TAILS}_f(u_1) = \\text{TAILS}_f(u_2)$.\nIn essence, this condition requires the model to distinguish between input-based and output-based Markovian triggers, making the learned transformation highly non-trivial if no pre-existing algorithm is used.\nLeaderboard Setup. The IOSL-based leaderboard contains 1,080 datapoints, mirroring the standard leaderboard in overall structure: k \u2208 {2,3,4}, |\u03a3| \u2208 {5, 6, 7, 8}, number of rules \u2208 {3,4,5}. For each setting, there are 30 datapoints per setting (for equivalence to the standard leaderboard's size).\nSince IOSL is not known to admit a finite-characteristic sample or minimal representation in the same sense as the deterministic classes, we introduce two adaptations for evaluation:\n1. Sample Size. We arbitrarily fix the sample size at 2 * |\u2211|k, as no characteristic sample is theoretically guaranteed.\n2. Evaluation Metrics. We focus primarily on compatibility, as recall and precision hinge on the assumption of a unique minimal-length description, which may not exist for IOSL. If a model's generated rule set is compatible with the data, we then check whether its description length is shorter, identical, or longer than our function's reference length. A longer description indicates a"}, {"title": "CONCLUSION", "content": "In this work, we introduced a systematic benchmark for assessing the inductive reasoning capabilities of LLMs, leveraging both well-studied subregular function classes (ISL, L-OSL, and R-OSL) and a more exploratory class (IOSL) for which no known polynomial-time learning algorithm exists. By controlling parameters such as the Markov window size k, the vocabulary size |\u03a3|, and the minimal number of rules, we offered precise yet flexible tasks capable of probing a model's capacity to infer general transformations from limited data. Our findings revealed several significant challenges for current LLMs-especially when required to track deeper dependencies or manage larger search spaces-and underscored the fragility of their inductive reasoning under increased context or novel data.\nThrough experiments measuring recall, precision, and compatibility, we demonstrated that factors like the Markov window size k and the number of rules more profoundly degrade performance than an expanded alphabet. Moreover, while few-shot prompting showed promise in simpler scenarios, its benefits quickly plateaued in more complex contexts. An error analysis further highlighted how many rules go completely missing or become overgeneralized under stringent settings, indicating that LLMs often fail to synthesize key patterns comprehensively.\nWe also proposed an exploration leaderboard targeting IOSL functions, a class beyond established theoretical learnability, to address concerns that performance gains might stem from known polynomial-time algorithms rather than genuine inductive reasoning. This complementary evaluation opens avenues of research on less tractable classes and poses a more authentic test of generalization and adaptability.\nOverall, our results highlight the need for more robust inductive reasoning strategies within current LLM architectures. We hope that our benchmark will help catalyze progress in both theoretical understanding and practical innovations around LLMs' inductive capabilities."}, {"title": "LIMITATIONS", "content": "While our benchmark offers a rigorous, theoretically grounded approach to evaluating inductive reasoning in LLMs, current paper is subject to two notable constraints:\nSynthetic Rather Than Real-World Data. All tasks and evaluations rely on functions generated from carefully controlled parameters rather than naturally occurring texts or real-world datasets. Although this design enables precise measurement of inductive capabilities, it may not fully capture the complexity of practical language use, where ambiguous contexts, noisy inputs, and domain-specific factors can further challenge inference.\nRestricted Access to the o1 Model. Our investigation into the o1 family of models is hindered by limited availability and computational resources. As a result, certain aspects of ol's inductive behavior may remain unexamined, and a more exhaustive exploration of variations or fine-tuning strategies for o1 could further illuminate its performance."}]}