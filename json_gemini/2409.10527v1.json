{"title": "Towards Empathetic Conversational Recommender Systems", "authors": ["Xiaoyu Zhang", "Ruobing Xie", "Yougang Lyu", "Xin Xin", "Pengjie Ren", "Mingfei Liang", "Bo Zhang", "Zhanhui Kang", "Maarten de Rijke", "Zhaochun Ren"], "abstract": "Conversational recommender systems (CRSs) are able to elicit user preferences through multi-turn dialogues. They typically incorporate external knowledge and pre-trained language models to capture the dialogue context. Most CRS approaches, trained on benchmark datasets, assume that the standard items and responses in these benchmarks are optimal. However, they overlook that users may express negative emotions with the standard items and may not feel emotionally engaged by the standard responses. This issue leads to a tendency to replicate the logic of recommenders in the dataset instead of aligning with user needs. To remedy this misalignment, we introduce empathy within a CRS. With empathy we refer to a system's ability to capture and express emotions. We propose an empathetic conversational recommender (ECR) framework.\nECR contains two main modules: emotion-aware item recommendation and emotion-aligned response generation. Specifically, we employ user emotions to refine user preference modeling for accurate recommendations. To generate human-like emotional responses, ECR applies retrieval-augmented prompts to fine-tune a pre-trained language model aligning with emotions and mitigating hallucination. To address the challenge of insufficient supervision labels, we enlarge our empathetic data using emotion labels annotated by large language models and emotional reviews collected from external resources. We propose novel evaluation metrics to capture user satisfaction in real-world CRS scenarios. Our experiments on the ReDial dataset validate the efficacy of our framework in enhancing recommendation accuracy and improving user satisfaction.", "sections": [{"title": "1 INTRODUCTION", "content": "Advances in conversational systems have led to the integration of natural language conversations with recommender systems, culminating in the development of conversational recommender systems (CRSs) [29]. A crucial aspect of CRSs is to elicit user preferences through multi-turn dialogues, with two main subtasks: item recommendation and response generation [12]. A prominent challenge is the lack of sufficient contextual information for accurately modeling user preferences. Some research [4, 57] integrates knowledge graphs (KGs) and models user preferences based on entities from KGs mentioned in the dialogues. Recent work [10, 48, 50] centers on using pre-trained language models (PLMs) to enhance the system's understanding of dialogue context. Despite these advances, existing CRS models still do not fully align with user needs. These models are trained on conversational recommendation training datasets.\nBut the presumption that the standard items and responses in the dataset are optimal leads to a tendency of CRS to replicate the logic of recommenders in the dataset instead of addressing user needs."}, {"title": "Using empathy to address misalignment.", "content": "The above misalignment hinders the development of CRSs. Lerner et al. [26] have proposed that emotions are crucial in human decision-making processes. Their work suggests that capturing emotions expressed in user utterances within dialogues is prominent for achieving accurate user preference modeling for item recommendation. People tend to favor agents simulating human beings by exhibiting emotions [6]. Adopting emotion-rich expressions in response generation can enliven the user experience and contribute to user satisfaction. Serving users in a natural, human-like way by capturing and expressing emotions is a necessary development for CRSs in terms of aligning with user needs, thereby offering benefits for users and providers of recommender systems. We introduce empathy within a CRS, defining it as the system's capacity to capture and express emotions [22]. Through empathy, we aim to accurately distinguish and fulfill user needs, both during item recommendation and response generation."}, {"title": "Integrating empathy into recommendation and generation.", "content": "We analyze the need to integrate empathy into item recommendation and response generation subtasks, respectively. For item recommendation, existing approaches often assume that all entities mentioned in dialogues reflect user preferences and that all items suggested by recommenders meet user expectations. This hypothesis disregards subtle cues of user emotions expressed in natural language for modeling user preferences. As illustrated in Figure 1, a conventional CRS might infer that the user likes \"Shakespeare\" and \"A Midsummer Night's Dream\" mentioned by the recommender, while overlooking that the user expresses negative emotions towards them during the dialogue. Thus such systems recommend the wrong item \"Romeo and Juliet.\" For response generation, existing methods are trained on the standard responses from datasets, which tend to be short and lack narratives, often resulting in inconsistencies or a lack of emotional engagement. As shown in Figure 1 (bottom), the conventional CRSs' response only contains the item name, which may diminish user satisfaction when interacting with the system. In contrast, based on capturing and expressing emotions, an empathetic CRS recommends a reasonable item with a persuasive response."}, {"title": "Challenges.", "content": "To construct empathetic CRSs, we face two major challenges: (i) how to accurately model user preferences using emotions; and (ii) how to generate emotional responses contributing to user satisfaction. To address these challenges, we propose an empathetic conversational recommender (ECR) framework comprising two key modules: emotion-aware item recommendation and emotion-aligned response generation. For the emotion-aware item recommendation module, we integrate user emotions with entities in the utterance to augment user preference modeling. We also propose a training strategy to minimize the impact of incorrect labels in the dataset. For the emotion-aligned response generation module, we fine-tune a pre-trained language model (PLM) to express emotions. To avoid hallucination, we retrieve relevant knowledge from KGs as a part of generation prompts. Existing CRS datasets lack user emotion labels and emotional responses. To enlarge the available empathetic training data, we use large language models (LLMs) to discern nuanced emotions in the dialogue history; then, we collect emotional reviews as an informative external resource for fine-tuning the PLM to generate emotional responses.\nSince existing evaluation metrics ignore the impact of emotions, we introduce novel metrics for CRSs, aiming at better reflecting user satisfaction in real-world CRS scenarios. For item recommendation, we adopt the Area Under the Curve (AUC) metric to assess the model's accuracy in modeling user preferences. AUC requires that items receiving positive feedback from users should have a higher possibility of being recommended than those with negative feedback. For response generation, we move beyond traditional metrics like BLEU or ROUGE, opting instead to use five subjective metrics: emotional intensity, emotional persuasiveness, logic persuasiveness, informativeness, and lifelikeness. Experiments on the ReDial benchmark dataset confirm the effectiveness of our proposed framework."}, {"title": "Contributions.", "content": "The contributions of this paper are as follows: (i) To bridge the gap between system outputs and user needs, we define empathy within a CRS and propose a novel framework ECR. (ii) We augment user preference modeling by integrating their emotions, with a new training strategy to minimize the impact of incorrect labels. (iii) We fine-tune a PLM to express emotions and apply retrieval-augmented prompts to mitigate hallucination. (iv) We use LLMs to annotate user emotions and collect emotional reviews from external resources as empathetic CRS training data, which facilitates future research in this area. (v) We propose new evaluation metrics tailored to user satisfaction in real-world CRS scenarios, and our experimental results demonstrate that ECR significantly outperforms baselines on the ReDial dataset."}, {"title": "2 RELATED WORKS", "content": "The literature on CRSs [29, 48, 57] can be classified into attribute-based CRSs and generation-based CRS [52]. Attribute-based CRSs [5, 58] predominantly employ fixed response templates and predefined actions for user interaction. The primary objective of most methodologies within this category is to minimize the number of turns required to complete the recommendation task [24, 25]. Deng et al. [9] and Lei et al. [25] use KGs to improve the recommendation performance. However, they still overlook the importance of generating high-quality natural language, which can be detrimental to the overall user experience.\nUnlike attribute-based CRSs, generation-based CRSs [29, 59] focus on making recommendations using free-form text, which creates considerable flexibility to influence how a dialogue continues. Li et al. [29] use an auto-encoder for recommendation and a hierarchical RNN for response generation. However, a challenge these systems face is the lack of sufficient contextual information for accurately discerning user preferences [14]. Research indicates that CRSs can be enhanced by incorporating additional sources of knowledge. Chen et al. [4] integrates KGs to enhance the user representation and propose an end-to-end framework. Zhou et al. [57] incorporate both word-oriented and entity-oriented KGs. Through reasoning based on the entities from KGs mentioned in the dialogues [31, 37, 56, 60], this integration further enhances the logical accuracy of recommendation and response interpretability. Subsequent research also introduces reviews [36, 60] and in-text knowledge [30, 42, 53] to assist user preference modeling.\nRecent work on generation-based CRSs have centered on integrating LLMs into CRSs [10, 48, 50]. UniCRS [48] addresses the recommendation and generation subtasks in a unified approach with prompt tuning. He et al. [14] conduct an in-depth analysis of LLMs for zero-shot CRS. And Wang et al. [47] develop an interactive evaluation method using LLM-based user simulators. However, Dai et al. [7] demonstrate that traditional collaborative filtering recommendation models, with adequate data, significantly outperform LLMs. Moreover, while LLMs are proficient in conversational aspects, they face limitations in conversational recommendation tasks, particularly in capturing user emotional engagement.\nOur study aligns with the generation-based CRSs. A major problem of recent generation-based CRSs is their misalignment with user preferences. We integrate empathy into CRS, prioritizing user needs as our goal. Similarly to our approach, methods for empathetic response generation [27, 28, 51] detect and respond to user emotions. These methods are tailored for chat and not easily adapted to CRSs. Some traditional recommender systems have enhanced collaborative filtering by incorporating sentiment analysis [8, 18, 20]. However, these works only focus on the analysis of item reviews rather than real-time multi-turn natural language dialogues."}, {"title": "3 PRELIMINARIES", "content": "3.1 Problem Formulation\nNotation. Given $t - 1$ dialogue turns, the dialogue history $D_{t-1}$ consists of a sequence of utterances from both recommenders and users, i.e., $D_{t-1} = \\{u_k\\}_{k=1}^{t-1} = \\{\\{w_j\\}_{j=1}^{n_k}\\}$, where each utterance $u$ = $\\{w_j\\}_{j=1}^{n_k}$ is composed of a sequence of words. For simplicity, we concatenate all utterances from $D_{t-1}$ into a single word sequence $D = \\{w_q\\}_{q=1}^{n_w}$ where $n_w$ represents the total number of words in $D_{t-1}$. To incorporate knowledge about entities mentioned in the dialogue, we set an external knowledge graph (e.g., DBpedia [2]) as $G = (\\mathcal{E}, \\mathcal{L})$, consisting of triples $T = (e_h, l, e_t)$, where $e_h \\in \\mathcal{E}$ and $e_t \\in \\mathcal{E}$ are the head and tail entities, $l \\in \\mathcal{L}$ reflects the relation between $e_h$ and $e_t$. $\\mathcal{E}$ and $\\mathcal{L}$ denote the sets of entities and relations. We define $I$ as the entire set of items, all of which are included in the entities of $G$, i.e., $I \\in \\mathcal{E}$. Entities in each utterance $u$ are identified as $E_u = \\{e_j\\}_{j=1}^{n_e}$. Each item $i_j$ within $I$ is linked with user feedback $f_{i_j}$, indicating whether the user likes it. Similarly, we combined all entities mentioned in $D_{t-1}$ into an entity list $E_1 = \\{e_q\\}_{q=1}^{n_e}$, where $n_e$ is the count of entities in the dialogue history. Here, we refer to the entities mentioned in the dialogue history as local entities. Correspondingly, we refer to entities co-occurring with the local entities in the training dataset as global entities, which will be detailed in Section 4.2.1.\nTask outputs. At the $t$-th turn, a CRS (i) selects a set of target items $I_t = \\{i_k\\}_{k=1}^{I_t}$ from the entire item set $I$, and (ii) generates a response utterance $u_r$ for the user."}, {"title": "3.2 Backbone Framework", "content": "Since UniCRS [48] unifies the recommendation and generation into a prompt learning paradigm by PLM, i.e., DialoGPT [54], which is the state-of-the-art method in using PLMs, we adopt it as our backbone framework. It encompasses three primary modules:\n(1) Semantic fusion module: Initially, UniCRS fuses the semantic spaces of dialogues and KGs for knowledge alignment and enrichment. It obtains a word embedding matrix and a local entity embedding matrix. Then it associates two kinds of embedding matrices via a bilinear transformation, yielding the fused word representations $W = [w_1 ; ...; w_{n_w}]$, and the fused local entity representations $E_1 = [\\bar{e}_1;...;\\bar{e}_{n_e}]$.\n(2) Response generation module: UniCRS prompts a PLM to generate the response $u_r$, which is designated as \u201crecommendation response.\" The prompt for this module consists of the fused word representations $W$, generation task-specific soft tokens $S_{gen}$, and the dialogue history $D$:\n$C_{gen} = [W; S_{gen}; D]$.\nNote that UniCRS replaces all items appearing in the recommendation response with a special token [MASK], which is later filled following the item recommendation subtask.\n(3) Item recommendation module: Given $u$ from the response generation subtask, the recommendation prompt consists of the fused local entity representations $\\bar{E}_1$, recommendation task-specific soft tokens $S_{rec}$, the dialogue history $D$, and $u$:\n$C_{rec} = [\\bar{E}_1; S_{rec}; D; u]$.\nThe response generation and item recommendation modules both use cross-entropy loss for prediction.\nAlthough UniCRS shows promise in using PLMs, its optimization still relies on standard answers provided by datasets and ignores user emotions, which limits its ability to track user needs. It inspires our subsequent endeavors in instantiating ECR based on UniCRS. Note that our proposed framework can extend beyond UniCRS and be seamlessly adapted to other CRSs with modifications."}, {"title": "4 METHOD", "content": "In this section, we introduce our empathetic data enlargement process (Section 4.1) and two key modules of ECR: emotion-aware item recommendation (Section 4.2) and emotion-aligned response generation (Section 4.3). Figure 2 shows an overview of ECR."}, {"title": "4.1 Empathetic Data Enlargement", "content": "4.1.1 User Emotion Extraction. Existing datasets lack explicit supervisory signals for identifying user emotions. To address the problem, we employ GPT-3.5-turbo [61] to initially annotate user emotions in 5,082 utterances from the ReDial dataset. We limit the number of annotated emotions per utterance to a maximum of two labels. In annotating with GPT-3.5-turbo [61] for utterance-level user emotions, we adopted nine emotion labels: \u201clike,\u201d \u201ccurious,\u201d \u201chappy\u201d \u201cgrateful,\u201d \"negative,\" \"neutral,\u201d \u201cnostalgia,\u201d \u201cagreement,"}, {"title": "4.2 Emotion-aware Item Recommendation", "content": "4.2.1 Emotion-aware Entity Representation. Entities are essential for reflecting user preferences. Thus we aim to model the effect of user emotions on the entities. Since local entities $E_l$ only reflect user interest exhibited in the ongoing dialogue $D_{t-1}$, which is insufficient for comprehensively exploring user preferences, we collect global entities from the training dataset filtered by user emotions, which encompass the collaborative knowledge shared by all users. In general, we model the effect of user emotions on entities both in the dialogue history and in the training data, respectively.\nLocal emotion-aware entity representing. To model the effect of user emotions on local entities, we characterize utterance-level user emotions $F_{u_t} = \\{f_j\\}_{j=1}^{|F_{u_t}|}$ as reflecting emotions towards entities mentioned by the user in the current utterance $E_u$ and by the recommender in the preceding one $E_r$. Consequently, each local entity $e_j \\in E_l$ is linked to an utterance-level user emotions, represented as $F_{e_j} = \\{f_i\\}_{i=1}^{|F_{e_j}|}$ along with corresponding probabilities $P_{e_j} = \\{p_i\\}_{i=1}^{|F_{e_j}|}$. Hence, we calculate the user emotion representation of each local entity $e_j$ as:\n$F_{e_j} = \\sum_{i=1}^{|F_{e_j}|} p_i * v(f_i)$,\nwhere $v(f_i)$ denotes the learnable representation of the $i$-th emotion label in the utterance-level user emotions. Then, we fuse the user emotion representation $F_{e_j}$ with the local entity representation of $e_j$ to get a local emotion-aware entity representation $\\bar{e}_j$, as follows:\n$\\bar{e}_j = [\\bar{e}_j; F_{e_j}] W_t + b$,\nwhere $[;]$ denotes vector concatenation; $W_t$ and $b$ are learnable parameters aimed at projecting the dimension of the concatenated representation back to the dimension of $\\bar{e}_j$. We stack all local emotion-aware entity representations into a matrix, denoted as $E_l' = [\\bar{e}_1;...;\\bar{e}_{n_e}]$.\nGlobal emotion-aware entity representing. We first use utterance-level user emotions to filter global entities and then aggregate their representations. Concretely, we assume that if a user exhibits similar emotions towards both $e_j$ and $e_i$ in a conversation, then $e_i$ is globally emotion-related to $e_j$. Similarly, we define a local entity $e_j$ to be emotion-related to a set of global entities $\\mathcal{E}_{e_j} = \\{e_i\\}_{i=1}^{|\\mathcal{E}_{e_j}|}$, where $e_j$ and $e_i$ overlap in the most probable $n_f$ emotion labels in $F_{e_j}$ and $F_{e_i}$ during their co-occurrence in a conversation. Additionally, we calculate the co-occurrence probability of each local entity $e_j$ with its emotion-related global entity $e_i$ from the training dataset, denoted as $P(e_i | e_j)$. Then, we aggregate the representation of all global entities emotion-related to the local entity $e_j$ as a global entity representation $\\bar{E}_{e_j}$:\n$\\bar{E}_{e_j} = \\sum_{i=1}^{|\\mathcal{E}_{e_j}|} e_i * P(e_i | e_j)$,\nwhere $e_i$ denotes the representation of $e_i$ obtained from the RGCN [43]. Following the Eq. 4, we calculate the global emotion-aware entity representation $\\bar{e}_{e_j}$, by integrating $\\bar{E}_{e_j}$ and $F_{e_j}$. Finally, we stack global emotion-aware entity representation for each local entity $e_j \\in E_l$ into a matrix, denoted as $E_g = [\\bar{E}_{e_1};...; \\bar{E}_{e_{n_e}}]$.\nEmotion-aware recommendation prompt. To comprehensively model user preferences with their emotions, we use the local emotion-aware entity representation matrix $E_l'$ and global emotion-aware entity representation matrix $E_g$ to update the prompt in Eq. 2. So we formulate an emotion-aware recommendation prompt as:\n$C_{rec} = [E_l'; E_g; S_{rec}; D; u].$\n4.2.2 Feedback-aware Item Reweighting. In the preceding section, we employ utterance-level user emotions to track user entity-based preferences. In this section, we develop a reweighting strategy that is aware of user feedback $f_{i_k}$ on each recommended item $i_k$, aligning with the supervision labels provided in the dataset but ignored by most CRS methods. Specifically, we introduce a mapping function $m(f_{i_k})$ that converts each user feedback $f_{i_k}$ as a weight scalar. The mapping function converts negative or unclear feedback into a lower weight. Based on the weight scalars, we rewrite the cross-entropy loss for item recommendation subtask as:\n$L_{rec} = - \\sum_{k=1}^{N} m(f_{i_k}) \\log P_r(i_k | C_{rec})$,\nwhere $N$ represents the total number of training instances, and $P_r(i_k | C_{rec})$ refers to the predicted probability of the recommended item $i_k$ given the emotion-aware recommendation prompt $C_{rec}$."}, {"title": "4.3 Emotion-aligned Response Generation", "content": "To support an engaging user experience, we generate an emotion-aligned response $u'$ to enrich the recommendation response $u$ generated by the UniCRS. In this section, we construct an emotion-aligned generation prompt and train an emotion-aligned generator.\n4.3.1 Emotion-aligned Generation Prompt. While PLMs can memorize information from their training corpus, Ji et al. [17] have shown that PLMs often exhibit hallucinations, which may diminish users' satisfaction with their usage. Teaching PLMs to accurately retain knowledge is resource-intensive and challenging. Thus, we construct an emotion-aligned generation prompt based on retrieved knowledge to enhance the informativeness in the responses while mitigating hallucination.\nSpecifically, during the training stage, given the extracted knowledge entities $E_{i_k}$ and the retrieved knowledge triples $T_{i_k}$, we transform the entities and triples into word sequences, represented as $S_{T_k}$ and $S_{E_k}$. The prompt for generating emotional review $r$ consists of the word sequence of the knowledge entities $S_{E_k}$, knowledge triples $S_{T_k}$, and the item name $S_{i_r}$. Then, we incorporate the recommendation response $u$ into the prompt, guiding the model to generate contextually relevant responses. The emotion-aligned generation prompt is formally denoted as:\n$C_{gen} = [S_{E_k}; S_{T_k}; S_{i_r};u]$.\nDuring the inference stage for generating emotion-aligned responses $u'$, we followed the same prompt design as in Eq. 8. We retrieve knowledge triples $T_{i_k} = \\{\\{i_k, l, e_j\\}\\}$ from the KG $G$ using the predicted item $i_k$ as the head entity. And we collect a list of knowledge entities $E_{i_k} = \\{e_j \\}^{|T_{i_k}|}$ that is mentioned at least twice in the reviews corresponding to $i_k$. Then, we filter $p_{nt}$ triples from $T_{i_k}$ and $p_{ne}$ entities from $E_{i_k}$ as a part of the emotion-aligned generation prompt. We simplify the filtering process by random selection, leaving more complex approaches to be explored in the future.\n4.3.2 Emotion-aligned Generator. To align the model with the persuasive dialogue style and emotions, we fine-tune a PLM as an emotion-aligned generator to generate the emotion-aligned responses $u'$ using the constructed emotional review databases $R$. Specifically, based on the emotional reviews $r$, we employ cross-entropy for training the emotion-aligned generator, as follows:\n$L_{gen}(r) = - \\sum_{j=1}^{n_r} \\log P_r(w_j | C_{gen}; w_{<j}),$\nwhere $P_r(w_j | C_{gen}; w_{<j})$ denotes the predicted probability of the word $w_j$ given the prompt $C_{gen}$ and the words proceeding the $j$-th position. Ultimately, we combine the emotion-aligned response $u'$ with the recommendation response $u$ to formulate the final response delivered to the users.\nFollowing Wang et al. [48], we choose DialoGPT [54] as the PLM for emotion-aligned response generation (ECR[DialoGPT]). Given DialoGPT's limited parameters, which inherently restrict its linguistic capabilities, we introduce an alternative version by using Llama 2-7B-Chat [45] to which we refer as ECR[Llama 2-Chat]. This choice is motivated by that Llama 2-Chat is an open-source, powerful LLM instruction-tuned on chat tasks. This variation allows us to evaluate our framework's performance based on LLMs."}, {"title": "5 EXPERIMENTS", "content": "We address the following research questions: (RQ1) Does ECR learn user preferences by capturing their emotions to improve the accuracy of recommendation? (RQ2) Is ECR capable of expressing emotions in response generation, thereby improving the user satisfaction? (RQ3) How does each component of ECR contribute to its overall performance?"}, {"title": "5.1 Dataset", "content": "The ReDial dataset [29] is a large-scale CRS dataset, carefully curated by human workers [12]. Consequently, it effectively reflects real-world CR scenarios and fully validates the effectiveness of our method. Considering the significant cost of emotion annotations and evaluations in the generation subtask, we use the ReDial dataset for experiments and plan to extend ECR to other datasets in future work. The ReDial dataset is composed of two-party dialogues between a user and a recommender in the movie domain. It contains 10,006 conversations consisting of 182,150 utterances related to 51,699 movies. The user feedback towards items recommended in the dataset includes three categories: \"like,\" \"dislike,\" and \"not say.\" Previous works [37, 48, 60] simply treat all the recommended items as positive labels. However, according to Li et al. [29], the \"dislike\" and \"not say\" labels are distributed separately at 4.9% and 14%, indicating the previous works introduce a large number of incorrect item labels. In contrast, we distinguish between those items with different user feedback. For emotional responses construction, we filter 34,953 reviews related to 4,092 movies for DialoGPT, and 2,459 reviews related to 1,553 movies for Llama 2-7B-Chat. The filtering process is detailed in Appendix A.2. Following [4], we extract entities mentioned in each utterance and review from DBpedia."}, {"title": "5.2 Baselines", "content": "For the item recommendation subtask, we compare our method with several CRS approaches: KBRD [4], KGSF[57], RevCore [37], UCCR[60] and UniCRS [48] to evaluate the effectiveness of ECR. Specifically, KBRD first uses KGs to enhance the semantics fusion in recommendation and generation systems. It uses transformer [46] for response generation with enhanced modeling of word weights. KGSF integrates both word-oriented and entity-oriented KGs to refine user representations and employs the transformer for response generation. RevCore introduces a review-enhanced framework, using item reviews for improving recommendations and response generation, with a focus on sentiment-aware review selection. UCCR focuses on comprehensive user modeling by considering multi-aspect information from current and historical dialogues, as well as data from look-alike users. UniCRS unifies the recommendation and generation into a prompt learning paradigm by PLM.\nFor the response generation subtask, our comparison involves the state-of-the-art CRS model UniCRS [48], the powerful open-source dialogue LLM Llama 2-7B-Chat [45] and two advanced OpenAI models: GPT-3.5-turbo-instruct and GPT-3.5-turbo [61]. To make the output deterministic, we set temperature = 0 when calling the API. These LLMs are prompted to chat with users, aiming to recommend the item predicted by the recommendation module of ECR. They are all provided with the dialogue history for consistency in evaluation."}, {"title": "5.3 Emotion-enhanced Evaluation Metrics", "content": "Our evaluation encompasses subjective and objective metrics to assess recommendation and generation performance respectively, which considers the user satisfaction in real-world CRS scenarios. We discuss more details in Appendix D.\nObjective evaluation metrics. For recommendation evaluation, we employed Recall@n (R@n, where $n = 1, 10, 50$) to verify if the top-n recommended items include the target item suggested by the dataset's recommenders. To validate the model's effectiveness in estimating user preferences while negating the logged errors in the dataset, we calculate Recall_True@n (RT@n, where $n = 1, 10, 50$). This metric refines Recall@n but only considers the items that get the user feedback of \u201clike\u201d as the standard answers. Additionally, we incorporate the Area Under the Curve (AUC) metric, which emphasizes the ranking order between recommended items linked to the users' positive and negative feedback.\nSubjective evaluation metrics. The generation quality is evaluated across five dimensions: emotional intensity (Emo Int), emotional persuasiveness (Emo Pers), logic persuasiveness (Log Pers), informativeness (Info), and lifelikeness (Life). (a) Emotional intensity measures the strength of emotions conveyed to users. (b) Emotional persuasiveness gauges the capacity to connect with the user emotionally to persuade users. (c) Logic persuasiveness evaluates the use of logical reasoning and coherent arguments to persuade users. (d) Informativeness determines the utility of useful information provided by the system. (e) Lifelikeness assesses how vivid and engaging the responses are, reflecting their resemblance to natural human communication. The scoring range for these metrics is 0 to 9.\nFollowing Wang et al. [47], we employ an LLM-based scorer capable of automatically assigning scores based on specific prompts to alleviate the evaluation reliance on human annotations and randomly sampling 1,000 examples for evaluation. In this context, GPT-4-turbo from the OpenAI serves as the scoring tool. Given the inherent instability in LLMs, we invite three human annotators to assess the reliability of our LLM-based scorer's evaluation results. The annotators are enlisted to rate 200 examples. Additionally, to ensure the robustness of our evaluation, GPT-4 is also employed as an auxiliary scorer, with results detailed in Appendix D."}, {"title": "5.4 Experimental Settings", "content": "We implement ECR with PyTorch. The embedding size of the emotion label is 48. The threshold $\\beta$ and $n_f$ are set to 0.1 and 3. The amount of knowledge triples $p_{nt}$ and entity $p_{ne}$ in the emotion-aligned generation prompt is set to 2 and 4. For the feedback-aware item reweighting strategy, we assign the weight scalar of user feedback \u201clike,\u201d \u201cdislike,\u201d and \u201cnot say\u201d to 2.0, 1.0, and 0.5, respectively. The analysis for the hyperparameters, i.e., the weight scalar of user feedback and the amount of knowledge used in the emotion-aligned generation prompt, can be found in Appendix B. In the emotion-aligned response generation process, we use AdamW [35] to optimize the tunable parameters of DialoGPT and fine-tune Llama2-chat with LoRA [15]. We set the learning rate for DialoGPT and Llama2-chat to 1e \u2013 4 and 5e - 5, respectively. The batch size is set to 128 for the emotion-aware item recommendation and 16 for the emotion-aligned response generation. The prompts used for the LLM-based scorers and baselines are detailed in Appendix E."}, {"title": "5.5 Evaluation on Item Recommendation (RQ1)", "content": "We address RQ1 by evaluating the performance of item recommendation; see Table 1. KGSF and RevCore, introducing external knowledge in CRSs, have demonstrated superior performance compared to KBRD, underscoring the significance of external knowledge in recommendations. UCCR also performs well on RT@50 and R@50 by extracting user-centric data from cross-session interactions. UniCRS, which integrates PLM into CRSs, exhibits the best performance among all baselines on RT@n and R@n. Regarding the AUC, a metric previously overlooked but essential for evaluating a model's full alignment with users' needs, we find that all baselines exhibit poor performance, with AUC values approaching 0.5. This finding highlights the considerable challenge faced by CRSs in distinguishing between items receiving positive and negative feedback.\nWe observe that ECR significantly outperforms all the baselines. Specifically, it shows an improvement of 3.9% and 3.0% over UniCRS in RT@10 and RT@50, respectively"}]}