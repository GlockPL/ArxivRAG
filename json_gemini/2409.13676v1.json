{"title": "A SOUND DESCRIPTION: EXPLORING PROMPT TEMPLATES AND CLASS DESCRIPTIONS TO ENHANCE ZERO-SHOT AUDIO CLASSIFICATION", "authors": ["Michel Olvera", "Paraskevas Stamatiadis", "Slim Essid"], "abstract": "Audio-text models trained via contrastive learning offer a practical approach to perform audio classification through natural language prompts, such as \"this is a sound of\" followed by category names. In this work, we explore alternative prompt templates for zero-shot audio classification, demonstrating the existence of higher-performing options. First, we find that the formatting of the prompts significantly affects performance so that simply prompting the models with properly formatted class labels performs competitively with optimized prompt templates and even prompt ensembling. Moreover, we look into complementing class labels by audio-centric descriptions. By leveraging large language models, we generate textual descriptions that prioritize acoustic features of sound events to disambiguate between classes, without extensive prompt engineering. We show that prompting with class descriptions leads to state-of-the-art results in zero-shot audio classification across major ambient sound datasets. Remarkably, this method requires no additional training and remains fully zero-shot.", "sections": [{"title": "1. INTRODUCTION", "content": "Multimodal contrastive pretraining has been used to train multimodal representation models on large amounts of paired data. This approach leverages contrastive learning to align representations across different modalities, promoting a shared embedding space that improves semantic understanding across modalities. Examples include Contrastive Language-Image Pretraining (CLIP) [1], which aligns visual and textual representations, and the more recent Contrastive Language-Audio Pretraining (CLAP), which extends these principles to align audio and textual representations [2, 3, 4, 5].\nFollowing pretraining, CLAP exhibits a well-structured feature space, yielding robust, general-purpose representations well-suited for downstream training. Moreover, it also demonstrates exceptional transferability as evidenced by its impressive zero-shot performance across classification, captioning, retrieval, and generation tasks [3, 6, 7].\nExtensive research on CLIP has revealed that classification scores are significantly influenced by alterations in prompt formulation and language nuances. For instance, varying the description of a concept, using synonyms, or modifying the grammatical structure or wording, substantially affects performance outcomes [8, 9, 10]. Besides, prompts offering more context or specificity tend to yield more accurate results [11, 12, 13].\nSimilarly, CLAP inherits sensitivity to prompting from its contrastive pretraining approach. Yet, the systematic exploration of prompt robustness in CLAP remains limited, despite few works highlighting the sensitivity of classification to prompt variations [14, 15]. These works, primarily conducted on the ESC50 dataset and limited to up to five prompt templates, shed initial light on these variations. However, robustness to prompt changes is likely to vary across different datasets. Addressing this gap, recent efforts have explored alternative approaches, such as prompt tuning strategies and lightweight adapters, to mitigate the reliance on manually engineered prompts [16, 17] with an explicit focus on adapting CLAP to downstream tasks or new domains.\nIn this work, we propose a tuning-free approach that prompts CLAP models with descriptions of class labels to enhance zero-shot audio classification. While using keywords such as \"audio,\u201d \u201chear,\u201d and \"sound\" in prompt templates primes the text encoder to focus on audio-related concepts, we hypothesize that enriching prompts with explicit class descriptions can further enhance the model's ability to clarify the meaning of class labels, particularly in scenarios where labels are ambiguous. Ambiguity stems from both the textual and audio aspects of the data. Textual ambiguity arises from homonyms, where words possess multiple meanings, and from the lack of contextual clues (e.g., \"bat\" as both an animal and sports equipment). On the audio side, ambiguity arises from acoustically similar sound categories, such as distinguishing between bird vocalizations (e.g., raven vs. crow calls) and musical instruments (e.g., violin vs. viola). Thus, detailed prompts may clarify sounds heavily reliant on context, and help disambiguate acoustically similar sounds. Such descriptions can also disambiguate abstract sounds such as \"white noise\" and compensate for knowledge gaps or limited exposure to certain terms. For instance, clarifying \"Geiger counter\", as \"a detection device that clicks or beeps when detecting radiation\" could improve correlations of audio and text features.\nTo validate our hypothesis, we leverage Large Language Models (LLMs) for their knowledge of sound semantics. Specifically, we used Mistral\u00b9 to describe the acoustic properties of class labels. Our study demonstrates that using audio-centric descriptions of class labels as prompts helps CLAP better ground acoustic features with semantic descriptions, significantly boosting zero-shot classification scores across major environmental sound datasets. Remarkably, our method even outperforms learnable prompt strategies, all without the need for additional training, while remaining entirely zero-shot."}, {"title": "2. METHODOLOGY", "content": "We first describe the zero-shot audio classification task, then our adaptive class selection strategy and finally we motivate our LLM-generated class descriptions."}, {"title": "2.1. Zero-shot audio classification", "content": "Given a set of target categories C and a query audio sample a, the zero-shot audio classification protocol in CLAP defines the classification problem as a nearest neighbor retrieval task. The predicted category \u0109 is determined as follows:\n$\\hat{c} = \\arg \\max_{c \\in C} \\text{sim}(\\phi_A(a), \\phi_T(c)),$\nwhere C represents the set of class labels, a denotes the input audio, and $\\phi_A$ and $\\phi_T$ are the audio and text encoders, respectively. The function sim(,) corresponds to the similarity metric, typically the cosine similarity.\nTo enhance zero-shot audio classification, we propose using both class labels and their descriptions to resolve ambiguities. Given a set of target categories C, definitions D, the predicted category \u010d is determined by:\n$\\bar{c} = \\arg \\max_{c \\in C} \\text{sim}(\\phi_A(a), \\phi_T(c + d_c)),$\nwhere $d_c \\in D$ is the description corresponding to class c, and the + operator denotes the textual combination of the class label c and its description dc."}, {"title": "2.2. Adaptive class description selection", "content": "We devise an adaptive strategy that incorporates descriptions selectively for classes potentially ambiguous to the text encoder. Let $P_{class-only}$ and $P_{class-description}$ represent the classification performance for class c using setups involving classes only or classes with descriptions as in Equations (1) and (2), respectively. We decide for class c which setup to apply through the decision function M(c):\n$M(c) =\n\\begin{cases}\n\\hat{c} \\text{ if } P_{class-only} > P_{class-description}\n\\bar{c} \\text{ if } P_{class-description} > P_{class-only}.\n\\end{cases}$\nThe function M(c) decides whether a class should include a description based on cross-validation of results."}, {"title": "2.3. Generation of audio-centric descriptions with LLMs", "content": "Given audio event class labels, we propose to use Large Language Models (LLMs) to generate audio-centric descriptions for them automatically, as manual collection of descriptions entails a labor-intensive endeavor. LLMs, trained on vast text data, have a deep understanding of language, which we exploit for their knowledge of sound semantics. Our method, adapted from [19], involves three steps. First, we provide a general description of the task. Second, we combine these instructions with in-context demonstrations, including a few paired label-description examples. Finally, we provide the LLM with the class labels, heuristic constraints, and specific output format details to generate audio-centric descriptions.\nUsing this method, we generated three types of descriptions: base descriptions, context-aware descriptions, and ontology-aware descriptions. All are audio-centric. Base descriptions reflect the acoustic properties and characteristic sounds of the class labels. Context-aware descriptions add details about the typical locations and circumstances of encountering the sounds, including the physical environment, associated objects, and the function of the sound within its context. Ontology-aware descriptions capture the acoustic properties and characteristic sounds of each class label while also considering their relationships with coarse high-level concepts."}, {"title": "3. EXPERIMENTAL SETUP", "content": "We detail our experimental approach, including model and dataset selection, evaluation metrics, and experiments to explore different prompt strategies and their impact on classification."}, {"title": "3.1. Models", "content": "We adopt two state-of-the-art audio-text models pre-trained via contrastive learning, namely LAION-CLAP (LA) and Microsoft CLAP 2023 (MS). The former utilizes ROBERTa [20] as its text encoder, while the latter leverages GPT-2 [21]. Both models rely on HTS-AT [22] as their audio encoder."}, {"title": "3.2. Datasets and evaluation metrics", "content": "Downstream datasets. We select six major environmental sound datasets tailored for either single-class or multi-label classification. These include: ESC50 [23], which contains 50 environmental sound classes with 2k labeled samples of 5 seconds each; US8K [24], comprising 10 urban sound classes and 8k labeled sound excerpts of 4 seconds each; TUT2017 [25], consisting of 15 acoustic scenes classes and 52k files of 10 seconds each; FSD50K [26], featuring 51K audio clips of variable length (from 0.3 to 30 seconds each) curated from Freesound and comprising 200 classes;"}, {"title": "3.3. Zero-shot audio classification experiments", "content": "Prompting with class labels only We explore zero-shot audio classification using prompts with sanitized class labels (i.e., replacing underscores in original labels with spaces, e.g., dog_barking becomes dog barking). This is motivated by the fact that in our early experiments we observed that this strategy performs competitively compared to prompting with \"This is a sound of\", which has been preferred in the literature [14, 4]. Here, we systematically study the impact of using only class labels as prompts on classification performance. We examine four different formats to construct the start and end of a prompt: uppercase with a period (e.g., Dog barking.), uppercase without a period (e.g., Dog barking), lowercase with a period (e.g., dog barking.), and lowercase without a period (e.g., dog barking). The format yielding the highest performance for each model, termed as CLS, was selected as a reference for subsequent experiments involving class descriptions.\nPrompting with templates. Inspired from CLIP [1], we explore a set of prompt templates as plausible alternatives to \"This is a sound of\", all tailored for the zero-shot audio classification task. We curated a set of 33 distinct prompts, drawing some from prior studies [14, 4, 15]. Our objective is to systematically evaluate the performance of these alternative prompts and their ensemble across multiple datasets. Each prompt follows the format Template + class label, e.g., \"A sound clip of dog barking.\". We thus analyse the performance of three prompt configurations: PTBaseline: The baseline prompt template \"This is a sound of\". PTBest: The most effective prompt template identified among the 33 manually crafted alternatives. PTEnsemble: Ensembling text embeddings from all considered prompt templates. Each prompt template begins with an uppercase letter and concludes with a period.\nPrompting with class-specific descriptions. We investigate the impact of combining class labels and their descriptions generated by LLMs. The experimental setups include: CLS: Class label only. CDBase: Audio-centric definitions generated by Mistral. CDContext\u00b3: Context-aware descriptions. CDontology: Ontological information related to the class label. CDDictionary: Definitions (non audio-centric) sourced from the Cambridge Dictionary of English."}, {"title": "4. RESULTS AND DISCUSSION", "content": "In this section, we present and discuss the outcomes of our experiments, shedding light on the impact of various prompting strategies and the role of class descriptions in classification performance."}, {"title": "4.1. Sensitivity to prompt format", "content": "In Table 2, we report the average classification results across all evaluation datasets to examine the sensitivity of zero-shot classification performance to subtle variations in the input prompt format. We see surprising differences in performance due to minor alterations such as capitalization and punctuation, consistent with findings in [15]. A recent work on LLM behavior confirm that these seemingly minor changes in prompt format influence the model's internal representations, leading to distinct transformations within the embedding space that alter the output probability distribution in ways that affect classification performance [28]. We observe that, for both models, prompt variations in punctuation, irrespective of capitalization, significantly affect performance more than variations in capitalization without punctuation. Notably, the performance gap between the most and least effective formats was 5.46% for model LA and 8% for model MS, pointing out how critical it is to select an optimal format to maximize classification scores. Consequently, subsequent experiments adopted the best-performing format for each model."}, {"title": "4.2. Comparison of prompting strategies", "content": "In Table 3, top-panel, we show results that assess the impact on classification performance when prompting CLAP models using only the class label and various prompt templates and an ensemble of these prompts. Our findings reveal that using the class label alone (CLS) often yields superior performance compared to the prompt template \"This is a sound of\" (PTBaseline). Specifically, CLS demonstrates better results than PTBaseline on the majority of datasets, with model MS showing an absolute improvement of 1.07%. However, for model LA, CLS showed a slight underperformance of 0.67%, largely due to lower scores on the TUT2017 and DCASE17-T4 datasets."}, {"title": "4.3. Impact of class-specific descriptions", "content": "In Table 3, middle-panel, we assess the impact of class-specific descriptions on classification performance through our adaptive selection strategy, which determines which classes benefit from explicit descriptions. Our findings indicate that introducing class descriptions is indeed beneficial for disambiguating difficult classes, with audio-centric descriptions generally outperforming dictionary definitions. Focusing on model LA, class descriptions with contextual information (CDContext) yielded the best results on average. While model MS also benefited from class-specific descriptions, it showed modest gains across datasets, likely due to its pretraining on a larger volume of data, including more audio-caption pairs. For model MS, base audio-centric descriptions of class labels CDBase were the most effective, but still could not outperform prompt template-based methods in the top-panel for datasets such as DCASE17-T4, FSD50k and AudioSet. However, our adaptive strategy incorporating all types of descriptions (CDAII) did not generalize as effectively compared to individual setups, which was somewhat disappointing."}, {"title": "4.4. Disambiguation of classes through descriptions", "content": "In Table 5, we show the top-3 classes with the greatest absolute improvement in classification using base descriptions compared to the simple use of class labels for the AudioSet and FSD50K datasets. We observe some words are ambiguous in meaning, for which an explicit description is beneficial as indicated by the large absolute improvements."}, {"title": "5. CONCLUSION", "content": "We demonstrated that prompt templates and class-specific descriptions can significantly impact the performance of zero-shot audio classification. While simple class labels can be highly effective, carefully crafted prompt templates and context-aware descriptions offer substantial improvements. Our findings advocate for a nuanced approach to prompt engineering, where the choice of format, content, and contextual information are tailored to the specific requirements of the model and dataset. Future work could explore automated methods for generating optimal prompts and descriptions, to further boost zero-shot audio classification scores."}]}