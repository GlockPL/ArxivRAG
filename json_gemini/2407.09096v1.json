{"title": "STD-LLM: Understanding Both Spatial and Temporal Properties of Spatial-Temporal Data with LLMs", "authors": ["Yiheng Huang", "Xiaowei Mao", "Shengnan Guo", "Yubin Chen", "Youfang Lin", "Huaiyu Wan"], "abstract": "Spatial-temporal forecasting and imputation are important for real-world dynamic\nsystems such as intelligent transportation, urban planning, and public health. Most\nexisting methods are tailored for individual forecasting or imputation tasks but\nare not designed for both. Additionally, they are less effective for zero-shot and\nfew-shot learning. While large language models (LLMs) have exhibited strong\npattern recognition and reasoning abilities across various tasks, including few-shot and zero-shot learning, their development in understanding spatial-temporal\ndata has been constrained by insufficient modeling of complex correlations such\nas the temporal correlations, spatial connectivity, non-pairwise and high-order\nspatial-temporal correlations within data. In this paper, we propose STD-LLM for\nunderstanding both spatial and temporal properties of Spatial-Temporal Data with\nLLMs, which is capable of implementing both spatial-temporal forecasting and im-\nputation tasks. STD-LLM understands spatial-temporal correlations via explicitly\ndesigned spatial and temporal tokenizers as well as virtual nodes. Topology-aware\nnode embeddings are designed for LLMs to comprehend and exploit the topology\nstructure of data. Additionally, to capture the non-pairwise and higher-order corre-\nlations, we design a hypergraph learning module for LLMs, which can enhance the\noverall performance and improve efficiency. Extensive experiments demonstrate\nthat STD-LLM exhibits strong performance and generalization capabilities across\nthe forecasting and imputation tasks on various datasets. Moreover, STD-LLM\nachieves promising results on both few-shot and zero-shot learning tasks.", "sections": [{"title": "1 Introduction", "content": "Understanding both spatial and temporal properties of spatial-temporal data (e.g., the dependencies\nand changing patterns of data in spatial-temporal dimensions) is crucial for various real-world\ndynamic systems such as intelligent transportation [1], urban planning [2], and public health. In\npractice, spatial-temporal forecasting [3, 4] and imputation [5, 6] are the two most pivotal and\ncommon tasks relying on understanding the properties of spatial-temporal data. Specifically, precise\nspatial-temporal forecasting aids in effective traffic management and travel planning, and spatial-\ntemporal imputation fills in missing data caused by unforeseen events during data collection and\nstorage, enabling precise analysis of spatial-temporal patterns and supporting other dependent tasks.\nAlthough extensive studies have achieved satisfactory accuracy in spatial-temporal forecasting and\nimputation, they rely on extensive historical data for training. However, obtaining comprehensive\ndatasets for all the studied regions is challenging due to the high cost of collecting and storing\nlong-term data. Consequently, the zero-shot [7] and few-shot learning [8] capabilities are hindered for\nspatial-temporal forecasting methods across different regions. Moreover, existing methods are usually\ntailored to specific tasks and not designed for both forecasting and imputation. Each method requires"}, {"title": "2 Related Work", "content": "Spatial-Temporal Data Forecasting. As a problem with a wide range of applications, there is a lot\nof work for this problem. Initial studies[15, 16] utilizing conventional time-series analysis techniques\nwhich based on basic statistical features of time series. In recent years, with the rapid development of\ndeep learning, a large number of models that can effectively model spatial-temporal dependencies\nhave emerged. Several studies [17] address temporal dynamics using RNNs and their derivatives.\nTo deal with spatial dependency, the data can be divided into grids [18, 19, 20, 21], and then CNNs\nare utilized to capture spatial correlations. This approach is somewhat effective, but not all data can\nbe partitioned into grid form. In order to achieve a more general and effective model, researchers\nintroduced the graph convolution model[22, 23] thereby implementing spatial feature aggregation\nbased on adjacency matrices [24, 25, 26, 27, 28, 3]. AGCRN[3] uses individual weights and biases"}, {"title": "3 Methodology", "content": "3.1 Problem Definition\nSpatial-Temporal data. Considering spatial-temporal data with T time slices and N nodes, we\nrepresent it as \\(X = {X_1, X_2, ..., X_T} \\in \\mathbb{R}^{T\\times N \\times C}\\). Here, \\(X_t = {x_{t,1}, X_{t,2},..., X_{t,N}} \\in \\mathbb{R}^{N \\times C}\\),\nwhere \\(x_{t,n}\\) represents the feature vector of node n at time slice t, and C is the number of features.\nWe use a binary mask tensor \\(M \\in {0,1}^{T\\times N \\times C}\\) to denote the positions of missing values in X,\nwhere \\(m_{t,n,c} = 0\\) indicates that the data is missing, and \\(m_{t,n,c} = 1\\) indicates that the data is observed.\nTo more accurately capture spatial correlations, we use a directed graph \\(G = (V,E, A)\\) to describe\nthe relationships between nodes. Here, V represents the set of nodes in the data. E is the set of\nedges that depict the spatial relationships such as geographical distance and adjacency between nodes.\nA\\in \\mathbb{R}^{N\\times N}\\) is the adjacency matrix of the graph G.\nForecasting task. Given the historical spatial-temporal data with T time slices \\({X_{t-T+1},..., X_{t-1},\nX_t}\\) and the corresponding graph structure, the objective is to learn a function f with parameters \\(\\theta\\) to"}, {"title": "3.2 Model Structure", "content": "As shown in Figure 1, our model focuses on accurate spatial-temporal forecasting and imputation\nvia fine-tuning the LLM to understand the dependencies and evolving patterns in spatial-temporal\ndata. To achieve this, we first develop spatial-temporal embedding to exploit the topology structure\nand periodicity of data. Based on this, we design temporal and spatial tokenizers to convert the\nspatial-temporal data into sequential tokens, enabling the LLM to learn the inherent spatial-temporal\ndependencies and evolving patterns of the data represented in the sequential token format. Lastly,\nwe further incorporate a hypergraph learning module to effectively capture the non-pairwise and\nhigher-order spatial-temporal correlations in data.\nSpatial-Temporal Embedding. The spatial-temporal embedding consists of a time embedding and a\ntopology-aware node embedding, which explores the static properties of data in both the spatial and\ntemporal dimensions.\nTime embedding: Two kinds of temporal information, i.e., time-of-day and day-of-week, are exploited\nfor creating embedding dictionaries, \\(D_t \\in \\mathbb{R}^{288 \\times d_t}\\) and \\(D_w \\in \\mathbb{R}^{7 \\times d_t}\\) for time-of-day and day-of-\nweek, where \\(d_t\\) is the dimension of the time embedding. By the looking-up and concatenation along\nwith broadcast operations, we finally obtain the time embedding \\(E_T \\in \\mathbb{R}^{T \\times N \\times 2d_t}\\).\ntopology-aware node embedding: It is designed to activate LLM to understand the topology structure\nin spatial-temporal data and facilitate LLM's universality in handling data with different numbers of\nnodes.\nThe proposed topology-aware node embedding is obtained based on the eigendecomposition of the\ngraph Laplacian matrix. There are two reasons for this design. First, as a matrix representation\nof the graph, the graph Laplacian matrix encapsulates the important information (i.e., connectivity,\ndegree, and eigenvalues) about the graph, helping the model perceive the static spatial properties of\ndata. Second, the graph Laplacian matrix has mutually orthogonal eigenvectors, helping the model\nmodeling the connectivity of nodes.\nSpecifically, we define the Laplacian matrix \\(L = I \u2013 D^{-1}AD\\), where \\(D = \\sum_{j=1}^{N} A_{i,j}\\) is the degree\nmatrix and I is the identity matrix. The eigendecomposition of L yields \\(L = V\\Lambda V^{-1}\\), with V being\nthe matrix of eigenvectors and \\(\\Lambda\\) the diagonal matrix of eigenvalues. By selecting the eigenvectors\ncorresponding to the top K largest eigenvalues, we obtain \\(V' \\in \\mathbb{R}^{N \\times K}\\). After passing through a\nlinear layer, we broadcast to obtain the topology-aware node embedding \\(E_N \\in \\mathbb{R}^{T \\times N \\times d_n}\\):\n\\[w^* = argtopk(diag(\\Lambda)),\\]\\[V' = V[:, w^*], E_N = W_{ne}V+b_{ne},\\]\nwhere \"diag\" refers to the operation of extracting the diagonal elements of a matrix. \\(W_{ne} \\in \\mathbb{R}^{K \\times d_n}\\)\nand \\(b_{ne} \\in \\mathbb{R}^{d_n}\\) represent the trainable parameters of the linear layers.\nThe node embedding obtained through the eigendecomposition of the Laplacian matrix not only\nassists the LLM in understanding the topology structure in the spatial-temporal data. Furthermore,\nsince the trainable parameters are independent of the graph, we can easily transfer our model between\ndifferent graph structures, which is key to our model's ability to perform well in zero-shot learning.\nSpatial Tokenizer.\nThe spatial tokenizer is designed to convert the spatial-temporal data into sequential tokens by\naggregating the information along the spatial dimension. Specifically, each node's feature vectors\nover P time slices are encoded into a spatial token, as shown in Figure 2. We further include the\ntopology structure of nodes into the spatial token by incorporating our proposed topology-aware node"}, {"title": "4 Experiments", "content": "4.1 Datasets\nThe experiments conducted on four publicly available real-world traffic datasets (PEMS03, PEMS04,\nPEMS07, PEMS08) to validate the effectiveness and generalization capability of STD-LLM. PEMS\nmeans Caltrans Performance Measurement System (PeMS)[44], which is a system that collects data\nfrom the California highway network. The details of the four datasets are provided in Appendix C.2.\nTo test the model's imputation capabilities under different scenarios, we generated missing data\nfor testing using two type of missing C.3: RM (random missing), CM (spatial-temporal continuity\nmissing) and two rates of missing - 30%, 70% on the dataset. Additionally, following the self-\nsupervised training approach of CSDI[42], we generated condition missing for training. Condition\nmissing refers to the artificially created missing data, used to generate training samples."}, {"title": "4.2 Experimental Settings", "content": "We follow the approach of ASTGCN[30] by partitioning the dataset into training set, validation set,\nand testing set with a ratio of 6: 2: 2, and employ a sliding window of size 12 to construct samples\nthat forecast the next 12 steps based on the historical 12 steps.\nTo comprehensively compare the performance of models, we selected five representative baselines\nfor the forecasting task, including Long Short-Term Memory network(LSTM)[45], ASTGCN[30],\nAGCRN[3], ASTGNN[4], PDFormer[31], OFA[12], and STGLLM[13]. For the imputation task, we\nalso chose four well-known baselines, which are Brits[5], E2GAN[36], mTAN[40], and PriSTI[6].\nThe experiments were conducted on an Ubuntu 23.04 operating system with a PyTorch version of\n2.1.2. The AdaW optimizer was utilized to optimize the model. The learning rate was set to \\(1 \\times 10^{-3}\\).\nThe training epoch is 500 with an early stopping mechanism based on a patience of 50 epochs. The\nloss function used was L1 loss. We use GPT-2[46] as the pre-trained LLM and utilized only its initial\nthree layers. STD-LLM employs the hyperparameters identified through tuning on the PEMS08\nvalidation set for all datasets, ensuring the model's generalization capability across different datasets."}, {"title": "4.3 Overall Performance", "content": "We measure the model's performance using three widely used metrics for regression tasks: Mean\nAbsolute Error (MAE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error\n(MAPE). Table 1 shows the forecasting performances comparison between our proposed STD-LLM\nand baselines. Bold indicates the best results, while underline denotes the second-best results. From\nTable 1, we can observe that: 1) The accuracy of the time series models such as LSTM and OFA\nis not good enough compared to methods that can handle spatial-temporal correlations in data.\nThis indicates that relying solely on temporal correlation is insufficient to achieve high-precision\nforecasting; 2) The results obtained from OFA and STGLLM suggest that it is challenging for LLM to\ncapture key information such as topological structure and higher-order spatiotemporal dependencies\nrelying solely on its capabilities. This implies that it is necessary for us to design spatial and temporal\ntokenizers, topology-aware node embeddings, and hypergraph learning module to assist the LLM\nin understanding spatial and temporal properties. 3) Our proposed method achieve the best or\nsecond-best results on the PEMS04, PEMS07, and PEMS08 datasets."}, {"title": "4.4 Few-shot and Zero-shot Performance", "content": "To extensively evaluate the performance of LLMs on spatial-temporal tasks, we conduct few-shot and\nzero-shot experiments. The few-shot experiments evaluate the forecasting performance of STD-LLM\non the PEMS04 and PEMS08 datasets using only the first 5%, 10%, and 20% of the training samples,\nwith results summarized in Table 3. The zero-shot experiments evaluate the forecasting performance\nof the pre-trained model when applied to other datasets, with the results compiled in Table 4.\nFrom Table 3, it can be observed that we only require 5% of the training samples to achieve\nperformance comparable to that of LSTM trained on the full dataset. When the training samples are\nincreased to 20%, the performance surpasses that of ASTGCN. This indicates that STD-LLM has\nexcellent few-shot learning capabilities, which supports its application in scenarios where data is\nscarce. The results in Table 4 demonstrate that STD-LLM can exhibit acceptable performance when\ndirectly transferred to datasets with different temporal scopes and graph structures from the training\nset, without undergoing any training. Further experiments are presented in the Appendix F."}, {"title": "4.5 Ablation Study", "content": "We implement ablation study to evaluate the effectiveness of each component. The ablation experi-\nments compared our proposed model with the following variants: 1) w/o Temporal Token, removing\nthe temporal tokens in Equation 6; 2) w/o Virtual Node, removing the virtual node; 3) w/o Hypergraph,\nremoving the Hypergraph Learning module in Equation 7; 4) w/o LLM, replacing the LLM with a\ntransformer encoder of the same layer and hidden dimension.\nWe summerize the results of the ablation study in Table 5. Based on the results, we can identify\nthe following: 1) Temporal Token and Virtual Node can enhance the model's forecasting and\nimputation performance with a relatively small computational cost. 2) The hypergraph module not\nonly reduces the number of tokens, thereby increasing efficiency, but also significantly boosts the\nmodel's performance. 3) The universal pattern recognition capabilities embodied in the pre-trained\nparameters of the LLM are useful for both spatiotemporal forecasting and imputation. Thus, LLM\nindeed play a substantial role in spatial-temporal forecasting and imputation, especially in scenarios\nwith complex spatial-temporal dependencies, like the imputation task on PEMS08 CM 70%."}, {"title": "5 Conclusion", "content": "In this paper, we propose STD-LLM, a unified framework for spatial-temporal forecasting and\nimputation based on LLMs. Through our explicitly designed spatial and temporal tokenizers, STD-\nLLM can effectively understand both spatial and temporal properties of spatial-temporal data. To\nenhance the LLM's understanding and utilization of the topology structure in spatial-temporal data,\nwe have designed topology-aware node embeddings. Considering the impact of external systems, we\nhave also introduced virtual nodes. In addition, we design a hypergraph learning module to enables\nthe LLM to capture the non-pairwise and higher-order correlations by constructing region-level\ntokens. Extensive experiments demonstrate that STD-LLM exhibits strong generalization capabilities\nwhile maintaining competitive performance. Our work suggests that constructing a unified pre-trained\nlarge spatial-temporal model based on LLMs is a promising direction."}, {"title": "A Limitations", "content": "Due to computational limitations, this paper did not further test whether LLM could yield better\nresults. In theory, the recently emerged LLMs with 7 billion, 30 billion, or even more parameters\nshould possess stronger capabilities for processing spatial-temporal data. Additionally, although this\npaper proposed a unified pre-training framework, we lack sufficient data to test whether the model,\nafter being trained on a large-scale spatial-temporal dataset, would exhibit emergent phenomena\nsimilar to those of LLMs, leading to a substantial improvement in performance. These are some of\nthe limitations of our research presented in this paper, and we hope that future work will be supported\nby adequate computational power and data to further explore these possibilities."}, {"title": "B Broader Impacts", "content": "Our proposed model can demonstrate relatively accurate spatial-temporal forecasting and imputation\ncapabilities with only a small number of training samples or even without samples, which implies\nthat our model can support a broader range of deployment and applications compared to other\nmodels. Moreover, the fast inference speed indicates that our model can sustain real-time interactive\napplications. Based on this, our work can provide rapid-response spatial-temporal forecasting services\nto a wider area, such as offering traffic dispatch decision support and travel route planning through\naccurate and swift traffic flow forecasting for different regions, thereby enhancing the efficiency of\nsocietal operations. However, a potential negative impact of this work is that inaccurate forecasting\ncould lead to erroneous decision-making, resulting in adverse social consequences."}, {"title": "C Experimental Details", "content": "C.1 Baselines\nTo compare model performance, we selected 7 forecasting models and 4 imputation models as\nbaselines. Detailed introductions to the baselines are as follows:\n\u2022 LSTM: A special type of RNN (Recurrent Neural Network) that mitigates the vanishing and\nexploding gradient problems by incorporating memory cells and forget gates, enabling the\nprocessing of long-term dependencies in sequences.\n\u2022 ASTGCN: A traffic forecasting model based on attention and convolutional networks. It\ncaptures the dynamic correlations of data through spatial-temporal attention and combines\nspatial-temporal graph convolutions to capture temporal features and spatial features.\n\u2022 AGCRN: An adaptive graph convolutional neural network for traffic forecasting. It optimizes\nthe conventional GCN (Graph Convolutional Network) by proposing a dynamic graph\nconvolution algorithm that learns adaptively from the data. It learns transformation matrices\nfor each node separately to better handle different spatial-temporal patterns of nodes. By\nintegrating the dynamic graph convolution with GRU (Gated Recurrent Unit), it efficiently\nprocesses spatial-temporal data.\n\u2022 ASTGNN: An autoregressive forecasting model based on spatial-temporal attention and\ndynamic graph convolution. It improves the traditional multi-head attention module by\nintroducing 1D convolution to process hidden local trend variations in the data. It also\noptimizes the conventional GCN by modeling spatial heterogeneity through additional\nembedding vectors.\n\u2022 PDFormer: An attention-based traffic forecasting model. It fully considers the shortcomings\nof conventional GNN (Graph Neural Network) models in handling dynamic spatial-temporal\ndependencies, long-range dependencies, and information propagation delays in traffic\nsystems. By combining geographical distance and traffic similarity to generate semantic\nand geographical neighbors, it enables spatial-temporal attention to handle both short-range\nand long-range dynamic spatial-temporal dependencies. Additionally, it uses the k-shape\nclustering algorithm to calculate similar historical traffic patterns for each sample, thus\nsimulating information propagation delays.\n\u2022 OFA: A time series model based on LLM (Large Language Model). It processes time series\ndata by segmenting it into patches and converting it into tokens for LLM processing. It"}, {"title": "C.2 Datasets", "content": "The detail of four dataset used in our experiments are summarized in Table 6. Data is sampled every\n30 seconds and compiled into 5-minute intervals. The table reveals that the datasets contain a small\nnumber of edges, indicating that all four have comparatively sparse graph structures."}, {"title": "C.3 Missing Patterns", "content": "We construct missing data on the complete dataset to train and test the model's imputation capabilities.\nThe experiment is designed with two different types of missing data patterns, namely random\nmissing(RM) and spatial-temporal continuous missing(CM), which are defined as follows: RM: A\nmissing pattern that is unrelated to both time and space, and is entirely random. In constructing this\npattern, it is only necessary to randomly mask a portion of the data according to the missing rate.\nCM: A pattern of missingness that has a strong correlation with both time and space, characterized\nby simultaneous missing data in a specific area over a certain period. We define three consecutive\ntime steps as one patch and use graph clustering algorithms to divide the graph into several smaller\nregions. When constructing the missing data, we randomly select patches and small regions, masking\nall the data within them."}, {"title": "C.4 Model Configurations", "content": "The hyperparameters used in our model for all experiments are the same, as summarized in Table 7:"}, {"title": "C.5 Parameter Efficacy", "content": "An important question to consider is whether the LLM significantly increases the computation cost.\nWe have summarized the computation cost of main baselines and STD-LLM in Table 8. It can be\nobserved from the table that, although our model has a large number of parameters, only 2.69%\nof them require training, and our model also has the fastest inference speed. The incorporation\nof LLM does indeed substantially increase the number of parameters, which leads to higher GPU\nmemory usage. However, thanks to the relatively simple structure of LLM, all operations can be\nhighly parallelized, thus not significantly impacting the training and inference time. Additionally,\nthe regional-level tokens constructed by our hypergraph module also reduce the number of tokens,\nthereby enhancing computation speed."}, {"title": "D Error Bars", "content": "All experiments were conducted three times, and the final metrics were taken as the average of the\nthree trials. We present the standard deviations for STD-LLM in the forecasting and imputation\nexperiments in Table 10 and Table 11."}, {"title": "E Detailed Definition for Constraint Loss", "content": "We decompose the constrain loss \\(L_c\\) in Equation 9 into \\(L_G\\) and \\(L_R\\):\n\\[L_G = -\\sum_{m=1}^M\\sum_{i,j(i\\neq j)} S_{m,i}S_{m,j} A_{i,j},\\]\\[L_R = - log \\pi(Softmax(\\sum_{m=1}^M S_{m,:})|\\alpha),\\]\n\\[\\means Hadamard Product. It is clear that the way to minimize \\(L_G\\) is to distribute the S evenly, that\nis, Sm,i = \\frac{1}{P}, at which point LG = M(P-1). When we shift our focus to a regular graph of size N,\nthe smaller the \\(L_G\\), the more readily the hypergraph module can focus attention on larger complete\nsubgraphs or subgraphs with stronger connectivity, thereby integrating the graph structure into the\nmodel. Of course, the premise of doing this is that the graph we are dealing with is a relatively sparse\ngraph, containing many small complete subgraphs or subgraphs with strong connectivity. Otherwise,\nthe hypergraph module might focus attention only on a few large complete subgraphs. If the graph\nwe are facing is composed of just a few large complete subgraphs, then it is possible to achieve better\nresults using prior knowledge.\nRegarding LG, we intend for it to serve a guiding role during the training process and do not wish for\nthe model to excessively reduce LG. Doing so could very likely lead to overfitting, which might result\nin extremely low attention being allocated to some nodes. Therefore, we introduce a regularization\nterm LG. LR is based on the Dirichlet distribution \\(\\pi\\), which is defined as follows:"}, {"title": "F Few-shot Performance on Pre-trained STD-LLM", "content": "In Section 4.4, we verified that the STD-LLM, which had not been pre-trained on spatial-temporal\ndata, also possesses strong few-shot performance. So, would an STD-LLM that has been pre-trained\non a specific spatial-temporal dataset exhibit even stronger few-shot performance? Can a pre-trained\nSTD-LLM play a positive role in the training on other spatial-temporal datasets? To this end, we\nconduct some experiments to verify. We tested the few-shot performance and full dataset training\nperformance of the STD-LLM pre-trained on PEMS07 for the PEMS08 and PEMS04 datasets, with\nthe results summarized in Table 12.\nThe results indicate that the few-shot performance of the pre-trained STD-LLM has seen a significant\nenhancement, achieving comparable performance to that of the non-pre-trained model using 20%\nof the data with just 5% of the data. Additionally, the performance of the model trained on the\nfull dataset has also improved, demonstrating that STD-LLM can correctly transfer the knowledge\nacquired from pre-training to new tasks."}, {"title": "G Visualization", "content": "We have plotted some results for forecasting and imputation. It can be observed that STD-LLM\ncan accurately follow the trend of data changes and also make corresponding adjustments to sudden\nchanges in the data."}]}