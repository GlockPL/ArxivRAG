{"title": "STD-LLM: Understanding Both Spatial and Temporal Properties of Spatial-Temporal Data with LLMs", "authors": ["Yiheng Huang", "Xiaowei Mao", "Shengnan Guo", "Yubin Chen", "Youfang Lin", "Huaiyu Wan"], "abstract": "Spatial-temporal forecasting and imputation are important for real-world dynamic systems such as intelligent transportation, urban planning, and public health. Most existing methods are tailored for individual forecasting or imputation tasks but are not designed for both. Additionally, they are less effective for zero-shot and few-shot learning. While large language models (LLMs) have exhibited strong pattern recognition and reasoning abilities across various tasks, including few-shot and zero-shot learning, their development in understanding spatial-temporal data has been constrained by insufficient modeling of complex correlations such as the temporal correlations, spatial connectivity, non-pairwise and high-order spatial-temporal correlations within data. In this paper, we propose STD-LLM for understanding both spatial and temporal properties of Spatial-Temporal Data with LLMs, which is capable of implementing both spatial-temporal forecasting and im- putation tasks. STD-LLM understands spatial-temporal correlations via explicitly designed spatial and temporal tokenizers as well as virtual nodes. Topology-aware node embeddings are designed for LLMs to comprehend and exploit the topology structure of data. Additionally, to capture the non-pairwise and higher-order corre- lations, we design a hypergraph learning module for LLMs, which can enhance the overall performance and improve efficiency. Extensive experiments demonstrate that STD-LLM exhibits strong performance and generalization capabilities across the forecasting and imputation tasks on various datasets. Moreover, STD-LLM achieves promising results on both few-shot and zero-shot learning tasks.", "sections": [{"title": "1 Introduction", "content": "Understanding both spatial and temporal properties of spatial-temporal data (e.g., the dependencies and changing patterns of data in spatial-temporal dimensions) is crucial for various real-world dynamic systems such as intelligent transportation [1], urban planning [2], and public health. In practice, spatial-temporal forecasting [3, 4] and imputation [5, 6] are the two most pivotal and common tasks relying on understanding the properties of spatial-temporal data. Specifically, precise spatial-temporal forecasting aids in effective traffic management and travel planning, and spatial- temporal imputation fills in missing data caused by unforeseen events during data collection and storage, enabling precise analysis of spatial-temporal patterns and supporting other dependent tasks. Although extensive studies have achieved satisfactory accuracy in spatial-temporal forecasting and imputation, they rely on extensive historical data for training. However, obtaining comprehensive datasets for all the studied regions is challenging due to the high cost of collecting and storing long-term data. Consequently, the zero-shot [7] and few-shot learning [8] capabilities are hindered for spatial-temporal forecasting methods across different regions. Moreover, existing methods are usually tailored to specific tasks and not designed for both forecasting and imputation. Each method requires"}, {"title": "2 Related Work", "content": "Spatial-Temporal Data Forecasting. As a problem with a wide range of applications, there is a lot of work for this problem. Initial studies[15, 16] utilizing conventional time-series analysis techniques which based on basic statistical features of time series. In recent years, with the rapid development of deep learning, a large number of models that can effectively model spatial-temporal dependencies have emerged. Several studies [17] address temporal dynamics using RNNs and their derivatives. To deal with spatial dependency, the data can be divided into grids [18, 19, 20, 21], and then CNNs are utilized to capture spatial correlations. This approach is somewhat effective, but not all data can be partitioned into grid form. In order to achieve a more general and effective model, researchers introduced the graph convolution model[22, 23] thereby implementing spatial feature aggregation based on adjacency matrices [24, 25, 26, 27, 28, 3]. AGCRN[3] uses individual weights and biases"}, {"title": "3 Methodology", "content": "3.1 Problem Definition\nSpatial-Temporal data. Considering spatial-temporal data with T time slices and N nodes, we represent it as \\(X = {X_1, X_2, ..., X_T} \\) \u2208 \\(R^{T\u00d7N\u00d7C}\\). Here, \\(X_t = {x_{t,1}, x_{t,2},..., x_{t,N}} \\) \u2208 \\(R^{N\u00d7C}\\), where \\(x_{t,n}\\) represents the feature vector of node n at time slice t, and C is the number of features. We use a binary mask tensor M \u2208 {0,1}\\(^{T\u00d7N\u00d7C}\\) to denote the positions of missing values in X, where \\(m_{t,n,c} = 0\\) indicates that the data is missing, and \\(m_{t,n,c} = 1\\) indicates that the data is observed. To more accurately capture spatial correlations, we use a directed graph \\(G = (V,E, A)\\) to describe the relationships between nodes. Here, V represents the set of nodes in the data. E is the set of edges that depict the spatial relationships such as geographical distance and adjacency between nodes. \\(A\\)\u2208 \\(R^{NN}\\) is the adjacency matrix of the graph G.\nForecasting task. Given the historical spatial-temporal data with T time slices {\\(X_{t\u2212T+1},..., X_{t-1}, X_t\\)} and the corresponding graph structure, the objective is to learn a function f with parameters 0 to"}, {"title": "3.2 Model Structure", "content": "As shown in Figure 1, our model focuses on accurate spatial-temporal forecasting and imputation via fine-tuning the LLM to understand the dependencies and evolving patterns in spatial-temporal data. To achieve this, we first develop spatial-temporal embedding to exploit the topology structure and periodicity of data. Based on this, we design temporal and spatial tokenizers to convert the spatial-temporal data into sequential tokens, enabling the LLM to learn the inherent spatial-temporal dependencies and evolving patterns of the data represented in the sequential token format. Lastly, we further incorporate a hypergraph learning module to effectively capture the non-pairwise and higher-order spatial-temporal correlations in data.\nSpatial-Temporal Embedding. The spatial-temporal embedding consists of a time embedding and a topology-aware node embedding, which explores the static properties of data in both the spatial and temporal dimensions.\nTime embedding: Two kinds of temporal information, i.e., time-of-day and day-of-week, are exploited for creating embedding dictionaries, \\(D_t \\)\u2208 \\(R^{288\u00d7d_t}\\) and \\(D_w \\)\u2208 \\(R^{7\u00d7d_t}\\) for time-of-day and day-of- week, where \\(d_t\\) is the dimension of the time embedding. By the looking-up and concatenation along with broadcast operations, we finally obtain the time embedding \\(E_T \\)\u2208 \\(R^{T\u00d7N\u00d72d_t}\\).\ntopology-aware node embedding: It is designed to activate LLM to understand the topology structure in spatial-temporal data and facilitate LLM's universality in handling data with different numbers of nodes.\nThe proposed topology-aware node embedding is obtained based on the eigendecomposition of the graph Laplacian matrix. There are two reasons for this design. First, as a matrix representation of the graph, the graph Laplacian matrix encapsulates the important information (i.e., connectivity, degree, and eigenvalues) about the graph, helping the model perceive the static spatial properties of data. Second, the graph Laplacian matrix has mutually orthogonal eigenvectors, helping the model modeling the connectivity of nodes.\nSpecifically, we define the Laplacian matrix \\(L = I \u2013 D^{-1}AD\\), where \\(D = \\sum_{j=1}^N A_{i,j}\\) is the degree matrix and I is the identity matrix. The eigendecomposition of L yields \\(L = V\u039bV^{-1}\\), with V being the matrix of eigenvectors and \u039b the diagonal matrix of eigenvalues. By selecting the eigenvectors corresponding to the top K largest eigenvalues, we obtain \\(V' \\)\u2208 \\(R^{N\u00d7K}\\). After passing through a linear layer, we broadcast to obtain the topology-aware node embedding \\(E_N \\)\u2208 \\(R^{T\u00d7N\u00d7d_n}\\):\n\\(w* = argtopk(diag(\u039b)),\\)\n\\(V' = V[:, w*], E_N = W_{ne}V' + b_{ne},\\)\nwhere \"diag\" refers to the operation of extracting the diagonal elements of a matrix. \\(W_{ne} \\)\u2208 \\(R^{K\u00d7d_n}\\) and \\(b_{ne} \\)\u2208 \\(R^{d_n}\\) represent the trainable parameters of the linear layers.\nThe node embedding obtained through the eigendecomposition of the Laplacian matrix not only assists the LLM in understanding the topology structure in the spatial-temporal data. Furthermore, since the trainable parameters are independent of the graph, we can easily transfer our model between different graph structures, which is key to our model's ability to perform well in zero-shot learning.\nSpatial Tokenizer.\nThe spatial tokenizer is designed to convert the spatial-temporal data into sequential tokens by aggregating the information along the spatial dimension. Specifically, each node's feature vectors over P time slices are encoded into a spatial token, as shown in Figure 2. We further include the topology structure of nodes into the spatial token by incorporating our proposed topology-aware node"}, {"title": "Temporal Tokenizer", "content": "The temporal tokenizer is designed to activate the LLM to understand spatial- temporal data by converting the spatial-temporal data into sequential tokens along the temporal dimension. We aim to encode the spatial-temporal data at each time slice into a token. Specifically, We first concatenate the spatial-temporal data, the time embedding, topology-aware node embedding, and the binary mask tensor, then aggregate various types of information through MLP to obtain the hidden representation F \u2208 \\(R^{T\u00d7N\u00d7d_F}\\). Subsequently, we utilize attention mechanism to aggregate the information of nodes in the hidden space, resulting in \\(H_T \\)\u2208 \\(R^{T\u00d71\u00d7d_F}\\). In this process, we need to aggregate information across the spatial dimension while ensuring that the model can adapt to different graph structures. Therefore, using methods such as self-attention is not suitable. To address this, we learn reference points \\(Q_l \\)\u2208 \\(R^{T\u00d71\u00d7d_t}\\) for each time step to aggregate information across the spatial dimension by interacting with nodes. The process is defined as follows:\n\\(Attention(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V,\\)\n\\(F = MLP((W_F[X||M] +b_F)||E_T||E_N),\\)\n\\(H_T = Attention(Q_l, F, F),\\)\n\\(Z_T = LayerNorm(W_TH_T + b_T),\\)\nwhere \\(W_*\\) and \\(b_*\\) denote the trainable parameters. \\(d_k\\) is the feature dimension, which is used for normalizing the variance in the attention mechanism. The MLP consists of two layers of linear with a ReLU activation function in between. Within the attention operation, we compute the similarity between the reference node and the hidden representation of each node, and subsequently aggregate the hidden representations of the nodes based on these similarities to obtain \\(H_T\\). Finally, a linear layer followed by LayerNorm is applied to obtain the temporal tokens \\(Z_T \\)\u2208 \\(R^{T\u00d7d_{LLM}}\\)."}, {"title": "Hypergraph Learning", "content": "We can capture the pair-wise correlation using spatial tokens aggregated by nodes, but it is difficult to capture important non-pairwise and higher-order spatial-temporal dependencies. To capture higher-order spatial-temporal correlations, we project node-level data into a region-level. For spatial-temporal data, the region is an appropriate choice, as the influencing factors, such as human activities and weather geological changes, exhibit distinct regional characteristics. Therefore, we design an autoencoder module to learn the mapping relationships between nodes and regions. The encoder, hypergraph encode, projects spatial tokens into higher-order region-level tokens. The decoder, hypergraph decode, is responsible for decoding the regional latent vectors back into node-level latent vectors. A simple implementation is to construct the encoder and decoder based on prior knowledge, but this approach overlooks the dynamics of the mapping relationships and cannot be easily transferred between different graph structures. Thus, we opted for attention mechanism-based encoders and decoders to dynamically learn these mapping relationships from the data. The process of hypergraph encode constructing region-level tokens \\(Z_H \\)\u2208 \\(R^{M\u00d7d_{LLM}}\\) (M represents the number of regions) from spatial tokens is as follows:\n\\(H_N = W_NZ_S+b_N,\\)\n\\(Z_H = LayerNorm(W_HAttention(H_l, H_N, H_N) + b_H),\\)\nwhere \\(H_N \\)\u2208 \\(R^{N\u00d7d_{\\overline{H}}}\\) represents the lower-dimensional representation of \\(Z_S\\) after passing through a linear layer. \\(d_{\\overline{H}}\\) is the hidden dimension. \\(H_l \\)\u2208 \\(R^{M\u00d7d_{\\overline{H}}}\\) is a set of learnable parameters that extract regional-level representations.\nAfter \\(Z_H\\) is processed by the LLM to obtain its hidden representation \\(Z'_H \\)\u2208 \\(R^{M\u00d7d_{LLM}}\\), it is necessary to map the regional-level hidden representation back to the node level by hypergraph decode to perform subsequent tasks:\n\\(H_R = W_{hidden} Z'_H + b_{hidden},\\)\n\\(Z_N = LayerNorm(W_RAttention(H_N, H_l, H_R) + B_R).\\)\nIn this recovery process, \\(H_R \\)\u2208 \\(R^{M\u00d7d_{\\overline{H}}}\\) is the lower-dimensional representation of \\(Z'_H\\). \\(Z_N \\)\u2208 \\(R^{N\u00d7d_{LLM}}\\) refers to the recovered node-level hidden representation from the LLM.\nTo ensure that the regional-level tokens constructed by the Hypergraph module can preserve the connectivity between nodes, a constraint loss function is designed. Furthermore, to prevent nodes from being ignored during the attention mechanism if their weights are too low, we have also added"}, {"title": "Versatility for Spatial-Temporal Forecasting and Imputation Tasks", "content": "The main differences between forecasting and imputation lie in missing values and context. Forecasting tasks typically have few missing values and can only forecast based on historical values. Imputation tasks, on the other hand, have more missing data and can impute based on both historical and future values. In the Equation 4, we have already enhanced the model's perception of missing values through the mask tokens. For the differences in context, the generalization ability of LLM can also effectively address it. Therefore, STD-LLM possess the capability to unify these two tasks. Switching between forecasting and imputation tasks can be achieved by modifying the target labels in the training dataset. Additionally, forecasting can be viewed as imputation of future missing values, thus unifying the two tasks. The model's processing flow can be summarized as follows:\n\\(Z = LLM([Z_E||Z_T||Z_H]),\\)\n\\(Z_H = Z[-M :, :], Z_N \u2190 Z_H,\\)\n\\(Y = MLP([W_yZ_N + b_y||X]),\\)\nwhere, Y \u2208 \\(R^{N\u00d7(P\u00d7C)}\\) is the forecasting or imputation result, P is the number of target time steps. X is reshaped to X \u2208 \\(R^{N\u00d7(T\u00d7C)}\\) for processing convenience. \\(W_*\\) and \\(b_*\\) denote the trainable parameters of the linear layers. The \"LLM\" in equation refers to the main body of the pre-trained LLM. The last line of the formula denote to the decoding step of the Decoder."}, {"title": "4 Experiments", "content": "4.1 Datasets\nThe experiments conducted on four publicly available real-world traffic datasets (PEMS03, PEMS04, PEMS07, PEMS08) to validate the effectiveness and generalization capability of STD-LLM. PEMS means Caltrans Performance Measurement System (PeMS)[44], which is a system that collects data from the California highway network. The details of the four datasets are provided in Appendix C.2.\nTo test the model's imputation capabilities under different scenarios, we generated missing data for testing using two type of missing C.3: RM (random missing), CM (spatial-temporal continuity missing) and two rates of missing - 30%, 70% on the dataset. Additionally, following the self- supervised training approach of CSDI[42], we generated condition missing for training. Condition missing refers to the artificially created missing data, used to generate training samples."}, {"title": "4.2 Experimental Settings", "content": "We follow the approach of ASTGCN[30] by partitioning the dataset into training set, validation set, and testing set with a ratio of 6: 2: 2, and employ a sliding window of size 12 to construct samples that forecast the next 12 steps based on the historical 12 steps.\nTo comprehensively compare the performance of models, we selected five representative baselines for the forecasting task, including Long Short-Term Memory network(LSTM)[45], ASTGCN[30], AGCRN[3], ASTGNN[4], PDFormer[31], OFA[12], and STGLLM[13]. For the imputation task, we also chose four well-known baselines, which are Brits[5], E2GAN[36], mTAN[40], and PriSTI[6].\nThe experiments were conducted on an Ubuntu 23.04 operating system with a PyTorch version of 2.1.2. The AdaW optimizer was utilized to optimize the model. The learning rate was set to 1 \u00d7 10-3. The training epoch is 500 with an early stopping mechanism based on a patience of 50 epochs. The loss function used was L1 loss. We use GPT-2[46] as the pre-trained LLM and utilized only its initial three layers. STD-LLM employs the hyperparameters identified through tuning on the PEMS08 validation set for all datasets, ensuring the model's generalization capability across different datasets."}, {"title": "4.3 Overall Performance", "content": "We measure the model's performance using three widely used metrics for regression tasks: Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error (MAPE). Table 1 shows the forecasting performances comparison between our proposed STD-LLM and baselines. Bold indicates the best results, while underline denotes the second-best results. From Table 1, we can observe that: 1) The accuracy of the time series models such as LSTM and OFA is not good enough compared to methods that can handle spatial-temporal correlations in data. This indicates that relying solely on temporal correlation is insufficient to achieve high-precision forecasting; 2) The results obtained from OFA and STGLLM suggest that it is challenging for LLM to capture key information such as topological structure and higher-order spatiotemporal dependencies relying solely on its capabilities. This implies that it is necessary for us to design spatial and temporal tokenizers, topology-aware node embeddings, and hypergraph learning module to assist the LLM in understanding spatial and temporal properties. 3) Our proposed method achieve the best or second-best results on the PEMS04, PEMS07, and PEMS08 datasets."}, {"title": "4.4 Few-shot and Zero-shot Performance", "content": "To extensively evaluate the performance of LLMs on spatial-temporal tasks, we conduct few-shot and zero-shot experiments. The few-shot experiments evaluate the forecasting performance of STD-LLM on the PEMS04 and PEMS08 datasets using only the first 5%, 10%, and 20% of the training samples, with results summarized in Table 3. The zero-shot experiments evaluate the forecasting performance of the pre-trained model when applied to other datasets, with the results compiled in Table 4.\nFrom Table 3, it can be observed that we only require 5% of the training samples to achieve performance comparable to that of LSTM trained on the full dataset. When the training samples are increased to 20%, the performance surpasses that of ASTGCN. This indicates that STD-LLM has excellent few-shot learning capabilities, which supports its application in scenarios where data is scarce. The results in Table 4 demonstrate that STD-LLM can exhibit acceptable performance when directly transferred to datasets with different temporal scopes and graph structures from the training set, without undergoing any training. Further experiments are presented in the Appendix F."}, {"title": "4.5 Ablation Study", "content": "We implement ablation study to evaluate the effectiveness of each component. The ablation experi- ments compared our proposed model with the following variants: 1) w/o Temporal Token, removing the temporal tokens in Equation 6; 2) w/o Virtual Node, removing the virtual node; 3) w/o Hypergraph, removing the Hypergraph Learning module in Equation 7; 4) w/o LLM, replacing the LLM with a transformer encoder of the same layer and hidden dimension.\nWe summerize the results of the ablation study in Table 5. Based on the results, we can identify the following: 1) Temporal Token and Virtual Node can enhance the model's forecasting and imputation performance with a relatively small computational cost. 2) The hypergraph module not only reduces the number of tokens, thereby increasing efficiency, but also significantly boosts the model's performance. 3) The universal pattern recognition capabilities embodied in the pre-trained parameters of the LLM are useful for both spatiotemporal forecasting and imputation. Thus, LLM indeed play a substantial role in spatial-temporal forecasting and imputation, especially in scenarios with complex spatial-temporal dependencies, like the imputation task on PEMS08 CM 70%."}, {"title": "5 Conclusion", "content": "In this paper, we propose STD-LLM, a unified framework for spatial-temporal forecasting and imputation based on LLMs. Through our explicitly designed spatial and temporal tokenizers, STD- LLM can effectively understand both spatial and temporal properties of spatial-temporal data. To enhance the LLM's understanding and utilization of the topology structure in spatial-temporal data, we have designed topology-aware node embeddings. Considering the impact of external systems, we have also introduced virtual nodes. In addition, we design a hypergraph learning module to enables the LLM to capture the non-pairwise and higher-order correlations by constructing region-level tokens. Extensive experiments demonstrate that STD-LLM exhibits strong generalization capabilities while maintaining competitive performance. Our work suggests that constructing a unified pre-trained large spatial-temporal model based on LLMs is a promising direction."}, {"title": "A Limitations", "content": "Due to computational limitations, this paper did not further test whether LLM could yield better results. In theory, the recently emerged LLMs with 7 billion, 30 billion, or even more parameters should possess stronger capabilities for processing spatial-temporal data. Additionally, although this paper proposed a unified pre-training framework, we lack sufficient data to test whether the model, after being trained on a large-scale spatial-temporal dataset, would exhibit emergent phenomena similar to those of LLMs, leading to a substantial improvement in performance. These are some of the limitations of our research presented in this paper, and we hope that future work will be supported by adequate computational power and data to further explore these possibilities."}, {"title": "B Broader Impacts", "content": "Our proposed model can demonstrate relatively accurate spatial-temporal forecasting and imputation capabilities with only a small number of training samples or even without samples, which implies that our model can support a broader range of deployment and applications compared to other models. Moreover, the fast inference speed indicates that our model can sustain real-time interactive applications. Based on this, our work can provide rapid-response spatial-temporal forecasting services to a wider area, such as offering traffic dispatch decision support and travel route planning through accurate and swift traffic flow forecasting for different regions, thereby enhancing the efficiency of societal operations. However, a potential negative impact of this work is that inaccurate forecasting could lead to erroneous decision-making, resulting in adverse social consequences."}, {"title": "C Experimental Details", "content": "C.1 Baselines\nTo compare model performance, we selected 7 forecasting models and 4 imputation models as baselines. Detailed introductions to the baselines are as follows:\n\u2022 LSTM: A special type of RNN (Recurrent Neural Network) that mitigates the vanishing and exploding gradient problems by incorporating memory cells and forget gates, enabling the processing of long-term dependencies in sequences.\n\u2022 ASTGCN: A traffic forecasting model based on attention and convolutional networks. It captures the dynamic correlations of data through spatial-temporal attention and combines spatial-temporal graph convolutions to capture temporal features and spatial features.\n\u2022 AGCRN: An adaptive graph convolutional neural network for traffic forecasting. It optimizes the conventional GCN (Graph Convolutional Network) by proposing a dynamic graph convolution algorithm that learns adaptively from the data. It learns transformation matrices for each node separately to better handle different spatial-temporal patterns of nodes. By integrating the dynamic graph convolution with GRU (Gated Recurrent Unit), it efficiently processes spatial-temporal data.\n\u2022 ASTGNN: An autoregressive forecasting model based on spatial-temporal attention and dynamic graph convolution. It improves the traditional multi-head attention module by introducing 1D convolution to process hidden local trend variations in the data. It also optimizes the conventional GCN by modeling spatial heterogeneity through additional embedding vectors.\n\u2022 PDFormer: An attention-based traffic forecasting model. It fully considers the shortcomings of conventional GNN (Graph Neural Network) models in handling dynamic spatial-temporal dependencies, long-range dependencies, and information propagation delays in traffic systems. By combining geographical distance and traffic similarity to generate semantic and geographical neighbors, it enables spatial-temporal attention to handle both short-range and long-range dynamic spatial-temporal dependencies. Additionally, it uses the k-shape clustering algorithm to calculate similar historical traffic patterns for each sample, thus simulating information propagation delays.\n\u2022 OFA: A time series model based on LLM (Large Language Model). It processes time series data by segmenting it into patches and converting it into tokens for LLM processing. It"}, {"title": "C.2 Datasets", "content": "The detail of four dataset used in our experiments are summarized in Table 6. Data is sampled every 30 seconds and compiled into 5-minute intervals. The table reveals that the datasets contain a small number of edges, indicating that all four have comparatively sparse graph structures."}, {"title": "C.3 Missing Patterns", "content": "We construct missing data on the complete dataset to train and test the model's imputation capabilities. The experiment is designed with two different types of missing data patterns, namely random missing(RM) and spatial-temporal continuous missing(CM), which are defined as follows: RM: A missing pattern that is unrelated to both time and space, and is entirely random. In constructing this pattern, it is only necessary to randomly mask a portion of the data according to the missing rate. CM: A pattern of missingness that has a strong correlation with both time and space, characterized by simultaneous missing data in a specific area over a certain period. We define three consecutive time steps as one patch and use graph clustering algorithms to divide the graph into several smaller regions. When constructing the missing data, we randomly select patches and small regions, masking all the data within them."}, {"title": "C.4 Model Configurations", "content": "The hyperparameters used in our model for all experiments are the same, as summarized in Table 7:"}, {"title": "C.5 Parameter Efficacy", "content": "An important question to consider is whether the LLM significantly increases the computation cost. We have summarized the computation cost of main baselines and STD-LLM in Table 8. It can be observed from the table that, although our model has a large number of parameters, only 2.69% of them require training, and our model also has the fastest inference speed. The incorporation of LLM does indeed substantially increase the number of parameters, which leads to higher GPU memory usage. However, thanks to the relatively simple structure of LLM, all operations can be highly parallelized, thus not significantly impacting the training and inference time. Additionally, the regional-level tokens constructed by our hypergraph module also reduce the number of tokens, thereby enhancing computation speed."}, {"title": "D Error Bars", "content": "All experiments were conducted three times, and the final metrics were taken as the average of the three trials. We present the standard deviations for STD-LLM in the forecasting and imputation experiments in Table 10 and Table 11."}, {"title": "E Detailed Definition for Constraint Loss", "content": "We decompose the constrain loss \\(L_c\\) in Equation 9 into \\(L_G\\) and \\(L_R\\):\n\\(L_G = -\\sum_{m=1}^M\\sum_{i,j(i\u2260j)} S_{m,i}S_{m,j}A_{i,j},\\)\n\\(L_R = -log\\pi(Softmax(\\sum_{m=1}^M S_{m,:})|\u03b1),\\)\nFirstly, as the attention score, \\(S_{m,:}\\) satisfies the properties: \\(\\sum_{i=1}^N S_{m,i} = 1\\).It is clear that the more edges there are, the smaller the minimum value of \\(L_G\\) can be. Consider a complete graph of size P, where:\n\\(L_G = -\\sum_{m=1}^M\\sum_{i,j(i\u2260j)} S_{m,i}S_{m,j},\\)\n\\(= \\sum_{m=1}^M\\sum_{i} S_{m,i}(1 - S_{m,i}) = -M + S \u00a9 S|{\\scriptsize 1}.\\)\n\u00a9 means Hadamard Product. It is clear that the way to minimize \\(L_G\\) is to distribute the S evenly, that is, \\(S_{m,i} = \\frac{1}{p}\\), at which point \\(L_G = M(P-1)\\). When we shift our focus to a regular graph of size N, the smaller the \\(L_G\\), the more readily the hypergraph module can focus attention on larger complete subgraphs or subgraphs with stronger connectivity, thereby integrating the graph structure into the model. Of course, the premise of doing this is that the graph we are dealing with is a relatively sparse graph, containing many small complete subgraphs or subgraphs with strong connectivity. Otherwise, the hypergraph module might focus attention only on a few large complete subgraphs. If the graph we are facing is composed of just a few large complete subgraphs, then it is possible to achieve better results using prior knowledge.\nRegarding \\(L_G\\), we intend for it to serve a guiding role during the training process and do not wish for the model to excessively reduce \\(L_G\\). Doing so could very likely lead to overfitting, which might result in extremely low attention being allocated to some nodes. Therefore, we introduce a regularization term \\(L_G\\). \\(L_R\\) is based on the Dirichlet distribution \u3160, which is defined as follows:"}, {"title": "F Few-shot Performance on Pre-trained STD-LLM", "content": "In Section 4.4, we verified that the STD-LLM, which had not been pre-trained on spatial-temporal data, also possesses strong few-shot performance. So, would an STD-LLM that has been pre-trained on a specific spatial-temporal dataset exhibit even stronger few-shot performance? Can a pre-trained STD-LLM play a positive role in the training on other spatial-temporal datasets? To this end, we conduct some experiments to verify. We tested the few-shot performance and full dataset training performance of the STD-LLM pre-trained on PEMS07 for the PEMS08 and PEMS04 datasets, with the results summarized in Table 12."}, {"title": "G Visualization", "content": "We have plotted some results for forecasting and imputation. It can be observed that STD-LLM can accurately follow the trend of data changes and also make corresponding adjustments to sudden changes in the data."}]}