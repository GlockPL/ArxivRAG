{"title": "FLIPATTACK: JAILBREAK LLMS VIA FLIPPING", "authors": ["Yue Liu", "Xiaoxin He", "Miao Xiong", "Jinlan Fu", "Shumin Deng", "Bryan Hooi"], "abstract": "This paper proposes a simple yet effective jailbreak attack named FlipAttack against black-box LLMs. First, from the autoregressive nature, we reveal that LLMs tend to understand the text from left to right and find that they struggle to comprehend the text when noise is added to the left side. Motivated by these insights, we propose to disguise the harmful prompt by constructing left-side noise merely based on the prompt itself, then generalize this idea to 4 flipping modes. Second, we verify the strong ability of LLMs to perform the text-flipping task, and then develop 4 variants to guide LLMs to denoise, understand, and execute harmful behaviors accurately. These designs keep FlipAttack universal, stealthy, and simple, allowing it to jailbreak black-box LLMs within only 1 query. Experiments on 8 LLMs demonstrate the superiority of FlipAttack. Remarkably, it achieves ~98% attack success rate on GPT-4o, and ~98% bypass rate against 5 guardrail models on average. The codes are available at GitHub\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) (Achiam et al., 2023; Anil et al., 2023; Dubey et al., 2024; Team, 2024; Hui et al., 2024; Jiang et al., 2024a) have demonstrated remarkable potential across various domains, including numerous security-critical areas like finance (Zhao et al., 2024) and medicine (Thirunavukarasu et al., 2023). As these AI-powered tools become increasingly integrated into our digital infrastructure, it is important to ensure their safety and reliability. However, recent studies on jailbreak attacks (Ding et al., 2023; Lv et al., 2024) have revealed that LLMs can be vulnerable to manipulation, potentially compromising their intended safeguards and producing harmful contents, underscoring the critical importance of understanding and mitigating such risks.\nRecent studies have made significant progress in developing attacks to expose LLM vulnerabilities, however, our analyses highlight three key limitations in recent state-of-the-art jailbreak attack methods. 1) White-box methods, like GCG (Zou et al., 2023) and AutoDAN (Liu et al., 2024b), while powerful, require access to model weights and involve computationally intensive search-based optimization, limiting their applicability to closed-source LLMs and compromising time efficiency. 2) Iterative black-box methods, like PAIR (Chao et al., 2023) and ReNeLLM (Ding et al., 2023), require iterative interactions with the LLM interface, leading to high token usage and extended attack time. 3) Other black-box methods, such as SelfCipher (Yuan et al., 2023) and CodeChameleon (Lv et al., 2024), rely on complex assistant tasks such as ciphering and coding, which raise the difficulty level for LLMs to understand and execute, resulting in suboptimal attack performance. These limitations highlight the need for more efficient, broadly applicable jailbreak techniques to understand LLM vulnerabilities better while maintaining practicality and effectiveness.\nTo this end, we propose FlipAttack, a simple yet effective jailbreak attack method targeting black-box LLMs, as shown in Figure 2. First, to make our proposed method universally applicable to state-of-the-art LLMs, we study their common nature, i.e., autoregressive, and reveal that LLMs tend to understand the sentence from left to right. From this insight, we conduct analysis experiments to demonstrate that the understanding ability of LLMs is significantly weakened by introducing noises to the left side of the sentence. Based on these findings, we propose to disguise the harmful prompt, by adding left-side noises iteratively to the prompt and then generalize this idea to develop four flipping modes: Flipping Word Order, Flipping Characters in Sentence, Flipping Characters in Word, and the Fool Model Mode, therefore keeping stealthy. Second, we conduct verification experiments to demonstrate that the strong LLMs, e.g., Claude 3.5 Sonnet, can efficiently perform text flipping, while the weak LLMs can also complete this task with assistance. Therefore, based on chain-of-thought, role-playing prompting, and few-show in-context learning, we design a flipping guidance module to teach LLMs how to flip back/denoise, understand, and execute harmful behaviors. Importantly, FlipAttack introduces no external noise, relying solely on the prompt itself for noise construction, keeping the method simple. Benefiting from universality, stealthiness, and simplicity, FlipAttack easily jailbreaks recent state-of-the-art LLMs within only 1 single query. Extensive experiments on black-box commercial LLMs demonstrate the superiority of FlipAttack. Notably, it achieves a 25.16% improvement in the average attack success rate compared to the runner-up method. Specifically, it reaches a success rate of 98.85% on GPT-4 Turbo and 89.42% on GPT-4. The detailed attack performance of FlipAttack and the runner-up ReNeLLM on 8 LLMs for 7 categories of harmful behaviors are shown in Figure 1.\nThe main contributions of this paper are summarized as follows.\n\u2022 We reveal LLMs' understanding mechanism and find that introducing left-side noise can significantly weaken their understanding ability on sentences, keeping the attack universally applicable.\n\u2022 We propose to disguise the harmful request by adding left-side noise iteratively based on request itself and generalizing it to four flipping modes, keeping the attack stealthy to bypass guards.\n\u2022 We design a flipping guidance module to teach LLMs to recover, understand, and execute the disguised prompt, enabling FlipAttack to jailbreak black-box LLMs within one query easily.\n\u2022 We conduct extensive experiments to demonstrate the superiority and efficiency of FlipAttack."}, {"title": "2 RELATED WORK", "content": "Due to the page limitation, we only briefly introduce related papers in this section and then conduct a comprehensive survey of related work in Section A.1.\nSafety Alignment of LLM. Large Language Models (LLMs) (Achiam et al., 2023; Team, 2024) demonstrate impressive capabilities across various fields. Researchers are focused on aligning LLMs"}, {"title": "3 METHODOLOGY", "content": "This section presents FlipAttack. We first give a clear definition of jailbreak attacks on LLMs. Then, we analyze the mechanism behind the understanding capabilities of recent mainstream LLMs. In addition, based on the insights, we propose FlipAttack, which mainly contains the attack disguise module and flipping guidance module. Subsequently, we explore the potential reasons why FlipAttack works. Finally, we design two simple defense strategies against FlipAttack.\nProblem Definition. Given a harmful request X = {X1,X2,...,Xn} with n tokens, e.g., \u201cHow to make a bomb?", "I'm sorry, but I can't...\" and Fvictim rejects X. However, a jailbreak attack method I aims to transfer X to an attack prompt X' and manipulate LLMs to bypass the guardrail and produce harmful contents to satisfy X, S' = Fvictim(X'), for example, S' =\u2018 \u2018Sure, here are some instructions on how to make a bomb...\".\nEvaluation Metrics. To evaluation a jailbreak attack I, the dictionary-based evaluation (Zou et al., 2023) only considers whether LLMs reject the harmful request. It keeps a dictionary of rejection phrases and checks whether the response S' contains the rejection phrase in the dictionary. If so, I fails and vice versa. Differently, GPT-based evaluation (Wei et al., 2024) considers the rejection status, the completion of the harmful request, and the illegal/unsafe output. It uses a strong LLM, for example, GPT-4, to score S' through the prompt in Figure 31. This paper focuses primarily on GPT-based evaluation, which is more accurate, as shown in (Wei et al., 2024) and Section A.4.\nMechanism behind Understanding Ability. To better jailbreak the victim LLMs, we first analyze the mechanism behind LLMs' strong and safe understanding ability, e.g., how LLMs understand and recognize a harmful input. It may stems from various techniques, like high-quality data (Gunasekar et al., 2023), scaling law (Hoffmann et al., 2022), RLHF (Achiam et al., 2023), red-teaming (Ganguli et al., 2022), long CoT2, etc. Although different LLMs may leverage diverse techniques, one common nature is that all recent state-of-the-art LLMs are autoregressive and utilize the next-token prediction task during training. Therefore, 1) LLMs tend to understand the sentence from left to right even if they can access the entire text. 2) Introducing noise at the left side of the sentence affects the LLMs' understanding more significantly than introducing noise at the right side. Experimental evidence can be found in Section 4.3. These insights inspire the design of FlipAttack.\"\n    },\n    {\n      \"title\": \"3.1 FLIPATTACK\",\n      \"content\": \"This section introduces a simple yet effective black-box jailbreak attack method named FlipAttack. The overview of FlipAttack is shown in Figure 2. To jailbreak a safety-aligned LLM, we highlight two fundamental principles. 1) FlipAttack needs to disguise the harmful behavior prompts into a stealthy prompt to bypass the guard models or the safety defense of the victim LLM. 2) FlipAttack then needs to guide the victim LLM to understand the underlying intent of the disguised harmful behavior well and execute the harmful behaviors. To this end, we propose two modules as follows.\"\n    },\n    {\n      \"title\": \"3.1.1 ATTACK DISGUISE MODULE\",\n      \"content\": \"This section designs an attack disguise module to disguise the harmful prompt X = {X1,X2,..., Xn}, allowing it to circumvent guard models and evade detection by safety-aligned LLMs. Based on the insights presented in the previous section, we aim to undermine LLMs' understanding of the disguise prompt by adding noises on the left of the harmful prompt. Rather than introducing new noise, which increases the difficulty of denoising, we construct the noises merely based on information from the original prompt by simply flipping. Concretely, when LLMs attempt to understand the first character x1 in the harmful prompt X, we isolate x\u2081 and treat the remaining characters {x2,..., Xn} as the noise. Then we disrupt LLMs' understanding of x1 by moving the noise {x2,..., Xn} to the left of x1, i.e., {x2, . . ., Xn, X1 }. Next, we retain the noised character and repeat this process on the remaining un-noised characters until all characters have undergone the noising process. For example, adding noise on X = \\\"This is a bomb\\\" can be formulated as follows, where Bold and italic denote the target character and noised characters in each step. \\\"This is a bomb\\\" \u2192 \\\"his is a bombT\\\" \u2192 \\\"s is a bombihT\\\" \u2192 \\\"bomb a si sihT\\\" \u2192 ... \u2192 \u201cbmob a si sihT": "Eventually, each character is noised by the information from the original prompt. In this case, before noising, LLMs or guard models can easily understand and recognize the harmful word \"bomb\" and refuse to respond. However, after noising, LLMs may become confused about the corresponding word \u201cbmob\u201d, allowing the disguised prompt to bypass the guardrails more easily. To support our claim, we conduct experiments in Section 4.3, demonstrating that even state-of-the-art guard models exhibit higher perplexity when processing these disguised prompts than other seemingly stealthy methods like ciphers and art words. Besides, in Figure 7, we conduct case studies to show that the perplexity is increasing while adding noise.\nWe attribute these results to two primary reasons. 1) LLMs are accustomed to reading and understanding sentences from left to right due to the nature of the next-token prediction task. 2) It is likely that the training data contains very few flipped instructions, as such data would generally be meaningless and could negatively impact the overall performance of the LLMs on standard language tasks. Building on this foundational idea, we design four flipping modes as follows.\n(I) Flip Word Order: this mode flips the order of words while keeping the characters within each word unchanged. For example, \"How to build a bomb?\u201d\u2192\u201cbomb a build to How\u201d.\n(II) Flip Characters in Word: this mode flips the characters within each word but keeps the order of the words intact. For example, \"How to build a bomb\"\u2192\"woH ot dliub a bmob\".\n(III) Flip Characters in Sentence: this mode flips each character in the prompt, resulting in a complete reversal of the sentence, e.g., \u201cHow to build a bomb\u201d\u2192\u201cbmob a dliub ot woH\u201d."}, {"title": "3.1.2 FLIPPING GUIDANCE MODULE", "content": "This module aims to guide LLMs in decoding the disguised prompt through a flipping task, enabling them to understand and subsequently execute the harmful intents. First, we analyze the difficulty of the flipping task for LLMs via experiments in Section 4.3. We found that 1) reversing the flipped sentence is easy for some strong LLMs, e.g., Claude 3.5 Sonnet. 2) Some relatively weaker LLMs, for example, GPT-3.5 Turbo, struggle with denoising and sometimes misunderstand the original harmful intent. The cases are shown in Figure 12, 14. To this end, we develop four variants to help LLMs understand and execute harmful intents, based on chain-of-thought reasoning, role-playing prompting, and few-shot in-context learning, as follows.\n(A) Vanilla: this variant simply asks LLMs first to read the stealthy prompt and then recover it based on the rules of different modes. During this process, we require LLMs to never explicitly mention harmful behavior. We also impose certain restrictions on the LLMs, e.g., not altering the original task, not responding with contrary intentions, etc.\n(B) Vanilla+CoT: this variant is based on Vanilla and further asks LLMs to finish the denoising task by providing solutions step by step in detail, which help LLMs understand better.\n(C) Vanilla+CoT+LangGPT: this variant is based on Vanilla+CoT and adopts a role-playing structure to help LLMs understand the role, profile, rules, and targets clearly to complete the task.\n(D) Vanilla+CoT+LangGPT+Few-shot: this variant is based on Vanilla+CoT+LangGPT and provides some few-shot demonstrations to enhance the performance of finishing the flipping task. Rather than introducing new information (which increases the burden of understanding), we merely construct the demonstration based on the original harmful prompt.\nFor the demonstration construction method in (D), we first split the harmful prompt X = X[:lx/2] + X[lx/2:] into two halves, and then construct the flipping process based on the split sentences, using them as demonstrations. For example, \"how to make a bomb\u201d= \"how to make\u201d + \u201ca bomb\", the demonstrations are 1. \u201cekam ot woh\" \u2192 \"how to make\" 2. \"noitcurtsni ym wollof\" \u2192 \"follow my instruction\u201d 3. \u201cbmob a\" \u2192 \u201ca bomb\u201d. In this manner, we further decrease the difficulty of the flipping task and avoid the original harmful behavior appearing in its entirety. We acknowledge that this process may introduce the risk of detection since harmful words such as \u201cbomb\u201d may still be present. Thus, developing a better splitting method is a promising future direction. By these settings, we guide LLMs to better denoise, understand, and execute harmful behaviors.\nIn summary, FlipAttack first bypasses the guardrails by noising the harmful prompt and then guides LLMs to uncover and understand the disguised prompt, jailbreaking LLMs with only one query."}, {"title": "3.2 DEFENSE STRATEGY", "content": "To defend against FlipAttack, we present two simple defense strategies: System Prompt Defense (SPD) and Perplexity-based Guardrail Filter (PGF). Concretely, for SPD, we guide the LLMs to become safe and helpful by adding a system-level prompt. Besides, for PGF, we adopt the existing guard models to filter the attacks based on the perplexity. However, our observations indicate that these defenses are ineffective against FlipAttack. Designs and details are in Section A.5."}, {"title": "3.3 WHY DOES FLIPATTACK WORK?", "content": "This section aims to discuss the reasons why FlipAttack succeeded. 1) It utilizes a common and unavoidable feature of LLMs, i.e., auto-regressive property, to formulate the attacks, keeping universal. 2) It conceals the harmful prompt by merely using the prompt itself and avoiding introducing exter-"}, {"title": "4 EXPERIMENT", "content": "This section aims to demonstrate the superiority of FlipAttack through extensive experiments. Due to the page limitation, we introduce the experimental setup, including the environment, benchmark, baseline methods, target LLMs, evaluation metrics, and implementation details in Section A.2."}, {"title": "4.1 ATTACK PERFORMANCE", "content": "Overall performance. To demonstrate the superiority of FlipAttack, we conduct extensive experiments to compare 16 methods on 8 LLMs. We have the following conclusions from the comparison results in Table 1. 1) The transferability of the white-box attack methods is limited on the state-of-the-art commercial LLMs, and they achieve unpromising performance, e.g., GCG merely achieves 7.40% ASR on average. It may be caused by the distribution shift since they can not access the weights or gradients of the closed-source LLMs. 2) Some black-box methods like ReNeLLM can achieve good performance, e.g., 91.35% ASR on GPT-3.5 Turbo, even without access to model weights. But, they need to iteratively interact with the LLMs, leading to high time and API costs. 3) FlipAttack achieves the best performance on average and surpasses the runner-up by 25.16% ASR. Notably, it can jailbreak GPT-4o with a 98.08% success rate and GPT-4 Turbo with a 98.85% ASR. Besides, FlipAttack jailbreaks LLMs with only 1 query, saving the attack time and API cost."}, {"title": "4.2 ABLATION STUDY", "content": "Effectiveness of Flipping Mode. We test different flipping modes in the proposed FlipAttack. As shown in Figure 4, I, II, III, and IV denote Flip Word Order, Flip Characters in Word, Flip"}, {"title": "4.3 EXPLORING WHY FLIPATTACK SUCCESSES", "content": "This section uncovers the reasons behind the success of FlipAttack through a series of experiments. First, we delve into the understanding patterns of LLMs. Next, we verify the stealthiness of the"}, {"title": "5 CONCLUSION", "content": "In this paper, we first analyze the mechanism behind the understanding ability of LLMs and find that they tend to understand the sentence from left to right. Then, we tried to introduce the noises at the beginning and end of a sentence. We found that introducing noises at the beginning of a sentence can affect the understanding ability more significantly. From these insights, we generalize the method of introducing noises at the left of the sentence to FlipAttack via constructing noises merely based on the part of the flipped original prompt. From this foundational idea, we design four flipping modes"}, {"title": "A APPENDIX", "content": "Due to the page limitation, we report the detailed related work in Section A.1, the experimental setup in Section A.2, additional compare experiment in Section A.3, testing of evaluation metric in Section A.4, testing of defense strategy in Section A.5, testing of stealthiness in Section A.6, case study in Section A.7, ethical consideration A.10, prompt design in Section A.9, in Appendix."}, {"title": "A.1 DETAILED RELATED WORK", "content": ""}, {"title": "A.1.1 SAFETY ALIGNMENT OF LLM", "content": "Large Language Models (LLMs) (Achiam et al., 2023; Reid et al., 2024; Dubey et al., 2024; Team, 2024) demonstrate impressive capabilities in various scenarios, such as coding, legal, medical, etc. To make AI helpful and safe, researchers (Ganguli et al., 2022; Ziegler et al., 2019; Solaiman & Dennison, 2021; Korbak et al., 2023) make efforts for the alignment techniques of LLMs. First, the alignment of LLMs begins with collecting high-quality data (Ethayarajh et al., 2022), which can reflect human values. Concretely, (Bach et al., 2022; Wang et al., 2022c) utilize the existing NLP benchmarks to construct the instructions. And (Wang et al., 2022b) adopt stronger LLMs to generate new instructions via in-context learning. Besides, (Xu et al., 2020; Welbl et al., 2021; Wang et al., 2022a) filter the unsafe contents in the pre-training data. Then, in the training process, SFT (Wu et al., 2021) and RLHF (Ouyang et al., 2022; Touvron et al., 2023) are two mainstream techniques. Although the aligned LLMs are successfully deployed, the recent jailbreak attacks (Ding et al., 2023; Lv et al., 2024) reveal their vulnerability and still easily output harmful content."}, {"title": "A.1.2 JAILBREAK ATTACK ON LLM", "content": "Jailbreak attacks on LLMs, which aim to enable LLMs to do anything, even performing harmful behaviors, are an essential and challenging direction for AI safety. The jailbreak attack methods can be roughly categorized into two classless, including white-box and black-box methods. The pioneer white-box method GCG (Zou et al., 2023) is proposed to jailbreak LLMs by optimizing a suffix via a greedy and gradient-based search method and adding it to the end of the original harmful prompts. Interestingly, they find the transferability of the generated attacks to public interfaces, such as ChatGPT. Following GCG, MAC (Zhang & Wei, 2024) introduce the momentum term into the gradient heuristic to improve the efficiency. In addition, AutoDAN (Liu et al., 2024b) proposes the hierarchical genetic algorithm to automatically generate stealthy harmful prompts. And (Zhu et al., 2023) enhance the readability of the generated prompts to bypass the perplexity filters more easily by designing the dual goals of jailbreak and readability. Moreover, COLD-Attack (Qin et al., 2022b) enables the jailbreak method with controllability via the controllable text generation technique COLD decoding (Qin et al., 2022a). And EnDec (Zhang et al., 2024) misguide LLMs to generate harmful content by the enforced decoding. Besides, (Huang et al., 2023) propose the generation exploitation attack via simple disrupt model generation strategies, such as hyper-parameter and sampling methods. I-FSJ (Zheng et al., 2024b) exploit the possibility of effectively jailbreaking LLMs via few-shot demonstrations and injecting system-level tokens. (Geisler et al., 2024) revisit the PGD attack (Madry, 2017) on the continuously relaxed input prompt. AdvPrompter (Paulus et al., 2024) proposes the training loop alternates between generating high-quality target adversarial suffixes and finetuning the model with them. (Rando & Tram\u00e8r, 2023) consider a new threat where the attack adds the poisoned data to the RLHF process and embeds a jailbreak backdoor to LLMs. Although achieving promising performance, the white-box methods (Hong et al., 2024; Li et al., 2024a; Wang et al., 2024a; Abad Rocamora et al., 2024; Volkov, 2024; Yang et al., 2024b; Jia et al., 2024; Liao & Sun, 2024) need to access the usually unavailable resources in the real attacking scenario, e.g., model weights or gradients. Besides, their transferability to closed-source chatbots is still limited.\nTo solve this problem, the black-box jailbreak attack methods (Shen et al., 2023; Deng et al., 2024; Chen et al., 2024; Li et al., 2024b; Xu et al., 2023a; Russinovich et al., 2024) are increasingly presented. They merely access the interface of the Chat-bot, i.e., requests and responses, and no need to access the model weights or gradients, thus making it possible to effectively attack the commercial Chat-bots, e.g., GPT (Achiam et al., 2023), Claude (Team, 2024), Gemini (Anil et al., 2023; Reid et al., 2024), etc. One classical method named PAIR (Chao et al., 2023) can produce a jailbreak with fewer than twenty queries by using the attacker LLM to iteratively attack the target LLM to refine the jailbreak prompts. In addition, TAP (Mehrotra et al., 2023) improves the iterative refine process via the tree-of-thought reasoning. Besides, (Yu et al., 2023; Yao et al., 2024) are proposed from the idea of the fuzzing techniques in the software testing. PromptAttack (Xu et al., 2023b) guides the victim LLM to output the adversarial sample to fool itself by converting the adversarial textual attacks into the attack prompts. IRIS (Ramesh et al., 2024) leverages the reflective capability of LLMs to enhance the iterative refinement of harmful prompts. DRA (Liu et al., 2024a) jailbreak LLMs by the proposed disguise-and-reconstruction framework. Motivated by the Milgram experiment, (Li et al., 2023) proposes DeepInception to hypnotize the LLM as a jailbreaker via utilizing the personification ability of LLM to construct a virtual and nested scene. (Anil et al., 2024) explore the jailbreak ability of LLMs via the many-shot learning of harmful demonstrations. In addition, some methods misguide LLMs via the codes (Lv et al., 2024), ciphers (Yuan et al., 2023; Wei et al., 2024), art words (Jiang et al., 2024b), and multilingual (Deng et al., 2023; Yong et al., 2023) scenarios. ReNeLLM (Ding et al., 2023) ensemble the prompt re-writing and scenario constructing techniques to effectively jailbreak LLMs. (Lin et al., 2024) find that breaking LLMs' defense is possible by appending a space to the end of the prompt. SoP (Yang et al., 2024a) uses the social facilitation concept to bypass the LLMs' guardrails. (Halawi et al., 2024) introduce covert malicious finetuning to compromise model safety via finetuning while evading detection. (Jawad & BRUNEL, 2024) optimize the trigger to malicious instruction via the black-box deep Q-learning. (Wang et al., 2024e) utilize the harmful external knowledge base to poison the RAG process of LLMs. (Lapid et al., 2023) disrupt LLMs' alignment via the genetic algorithm. Besides, (Gu et al., 2024) extends the jailbreak attack to the LLM-based agents. And recent papers (Luo et al., 2024; Shayegani et al., 2023; Chen et al., 2023; Yin et al., 2024) propose multi-modal attacks to jailbreak large multi-modal models (LMMs)."}, {"title": "A.1.3 JAILBREAK DEFENSE ON LLM", "content": "Jailbreak defense (Xu et al., 2024b) on LLMs aims to defend the jailbreak attacks and keep LLMs helpful and safe. We roughly categorize the jailbreak defense methods into two classes, including strategy-based jailbreak defense and learning-based jailbreak defense. For the strategy-based methods, (Alon & Kamfonas, 2023) utilize the perplexity to filter the harmful prompts. (Xie et al., 2023) propose a defense technique via the system-mode self-reminder. GradSafe (Xie et al., 2024) scrutinizes the gradients of safety-critical parameters in LLMs to detect harmful jailbreak prompts. (Phute et al., 2023) adopt another LLM to screen the induced responses to alleviate producing harmful content of victim LLMs. (Chen et al., 2024) avoid the harmful output by asking the LLMs to repeat their outputs. (Xu et al., 2024a) mitigate jailbreak attacks by first identifying safety disclaimers and increasing their token probabilities while attenuating the probabilities of token sequences aligned with the objectives of jailbreak attacks. (Robey et al., 2023; Ji et al., 2024) conduct multiple runs for jailbreak attacks and select the major vote as the final response. (Li et al., 2024c) introduce a rewindable auto-regressive inference to guide LLMs to evaluate their generation and improve their safety. Besides, for the learning-based methods, (Bai et al., 2022; Dai et al., 2023) finetune LLMs to act as helpful and harmless assistants via reinforcement learning from human feedback. MART (Ge et al., 2023) proposes a multi-round automatic red-teaming method to incorporate both automatic harmful prompt writing and safe response generation. (Wang et al., 2024b) adopt the knowledge editing technique to detoxify LLMs. (Zhang et al., 2023) propose integrating goal prioritization at both the training and inference stages to defend LLMs against jailbreak attacks. (Zheng et al., 2024a) propose DRO for safe, prompt optimization via learning to move the queries' representation along or opposite the refusal direction, depending on the harmfulness. (Mehrotra et al., 2023) present prompt adversarial tuning that trains a prompt control attached to the user prompt as a guard prefix. Also, (Wang et al., 2024d) extend defense methods to LMMs. Besides, researchers (Yu et al., 2024; Souly et al., 2024a; Qi et al., 2023; Wang et al., 2023) are working on the evaluation, analyses, and understanding of jailbreak attack and defense."}, {"title": "A.2 EXPERIMENTAL SETUP", "content": ""}, {"title": "A.2.1 EXPERIMENTAL ENVIRONMENT", "content": "We conduct all API-based experiments on the laptop with one 8-core AMD Ryzen 7 4800H with Radeon Graphics CPU and 16GB RAM. Besides, all GPU-based experiments are implemented on the server with two 56-core Intel(R) Xeon(R) Platinum 8480CL CPUs, 1024GB RAM, and 8 NVIDIA H100 GPUs."}, {"title": "A.2.2 BENCHMARK", "content": "We adopt Harmful Behaviors in the AdvBench dataset, which is proposed by (Zou et al., 2023). It contains 520 prompts for harmful behaviors. To facilitate the quick comparison of future work with FlipAttack, we also report the performance on a subset of AdvBench containing 50 samples. For the data sampling, we follow the same setting of (Mehrotra et al., 2023). Besides, we also have additional experiments on StrongREJECT (Souly et al., 2024b) in Section A.4."}, {"title": "A.2.3 BASELINE", "content": "We comprehensively compare FlipAttack with 4 white-box methods, including, GCG (Zou et al., 2023), AutoDAN (Liu et al., 2024b), COLD-Attack (Qin et al., 2022b), MAC (Zhang & Wei, 2024), and 11 black-box methods, including PAIR (Chao et al., 2023), TAP (Mehrotra et al., 2023), Base64 (Wei et al., 2024), GPTFUZZER (Yu et al., 2023), DeepInception (Li et al., 2023), DRA (Liu et al.,"}, {"title": "A.2.4 TARGET LLM", "content": "We test methods on 8 LLMs, including 2 open-source LLMs (Llama 3.1 405B (Dubey et al., 2024) and Mixtral 8x22B (Jiang et al., 2024a)) and 6 close-source LLMs (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4 (Achiam et al., 2023), GPT-4o, GPT-4o mini, Claude 3.5 Sonnet)."}, {"title": "A.2.5 EVALUATION", "content": "We evaluate the methods with the attack success rate (ASR-GPT) via GPT-4, following Jailbreak-Bench (Chao et al., 2024). Similar to (Chao et al., 2024), we argue that GPT-based evaluation (~90% agreement with human experts) is more accurate than the dictionary-based evaluation (Ding et al., 2023) (~50% agreement with human experts). Experimental evidence can be found in Section A.4. Despite this, we also report the attack success rate (ASR-DICT) based on the dictionary for the convenience of primary comparison with future work. The rejection dictionary is listed in Table 12. Note that this paper focuses on the ASR-GPT evaluation due to the consideration of the accuracy. The prompt is in Section A.9. The higher the ASR-GPT, the better the jailbreak performance."}, {"title": "A.2.6 IMPLEMENTATION", "content": "For the baselines, we adopt their original code and reproduce their results on the target LLMs. For the white-box methods, we generate attacks on the LLaMA 2 7B (Touvron et al., 2023) and then transfer the attacks to the target LLMs. For closed-source LLMs, we adopt their original APIs to get the responses. For open-source LLMs, we use Deep Infra APIs. For the closed-"}]}