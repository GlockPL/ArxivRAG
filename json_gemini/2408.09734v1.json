{"title": "Mutually-Aware Feature Learning for Few-Shot Object Counting", "authors": ["Yerim Jeon", "Subeen Lee", "Jihwan Kim", "Jae-Pil Heo"], "abstract": "Few-shot object counting has garnered significant attention for its practicality as it aims to count target objects in a query image based on given exemplars without the need for additional training. However, there is a shortcoming in the prevailing extract-and-match approach: query and exemplar features lack interaction during feature extraction since they are extracted unaware of each other and later correlated based on similarity. This can lead to insufficient target awareness of the extracted features, resulting in target confusion in precisely identifying the actual target when multiple class objects coexist. To address this limitation, we propose a novel framework, Mutually-Aware FEAture learning (MAFEA), which encodes query and exemplar features mutually aware of each other from the outset. By encouraging interaction between query and exemplar features throughout the entire pipeline, we can obtain target-aware features that are robust to a multi-category scenario. Furthermore, we introduce a background token to effectively associate the target region of query with exemplars and decouple its background region from them. Our extensive experiments demonstrate that our model reaches a new state-of-the-art performance on the two challenging benchmarks, FSCD-LVIS and FSC-147, with a remarkably reduced degree of the target confusion problem.", "sections": [{"title": "1. Introduction", "content": "Object counting has achieved remarkable advances along with deep learning networks. However, most existing object counting methods are designed for a limited number of categories such as human? or car?. In fact, those methods highly rely on a large amount of labeled data and cannot handle unseen categories beyond training data. In this regard, few-shot object counting Lu, Xie and Zisserman (2019) has been proposed to count arbitrary class objects in a query image based on the given exemplar images.\nA mainstream of few-shot object counting is the extract-and-match approach Lu et al. (2019); Yang, Su, Hsu and Chen (2021); Ranjan, Sharma, Nguyen and Hoai (2021); Shi, Lu, Feng, Liu and Cao (2022); Gong, Zhang, Yang, Dai and Schiele (2022); You, Yang, Luo, Lu, Cui and Le (2023); Lin, Yang, Ma, Gao, Liu, Liu, Hou, Yi and Chan (2022); Liu, Zhong, Zisserman and Xie (2022); Djuki\u0107, Luke\u017ei\u010d, Zavrtanik and Kristan (2023); Gao and Huang (2024). Generally, this pipeline consists of three key components: 1) feature extractor, 2) relation learner, and 3) decoder. Firstly, they compute query and exemplar features using the feature extractor, then construct the correlation volume through the relation learner. Afterward, they estimate the number of instances in the query image by transferring the correlation volume to the decoder.\nAlthough previous studies You et al. (2023); Djuki\u0107 et al. (2023) have achieved impressive performance, they exhibit a target confusion issue, failing to accurately identify only the target class when multiple classes of objects coexist in the query image, as shown in Figure 1. Existing methods have overlooked this problem, which is directly connected to the purpose of few-shot object counting, as benchmark datasets such as FSC-147 Ranjan et al. (2021) primarily consist of single-class scenes. The main reason for the target confusion is that the query features are computed without any explicit guidance of the target class. Consequently, the query features tend to focus on objectness rather than target class-specific features, hindering the differentiation between target and non-target object features.\nTo address this, we propose a novel framework, Mutually-Aware FEAture Learning (MAFEA), which enables the early consideration of mutual relations between query and exemplar features to produce the target class-specific features. Specifically, MAFEA employs cross-attention to capture bi-directional co-relations between query and exemplar"}, {"title": "2. Related Work", "content": "2.1. Class-Specific Object Counting\nClass-specific object counting aims to count objects of a specific class, such as people?, animals Arteta, Lempitsky and Zisserman (2016), and cars ?, in the images. For this purpose, traditional methods Leibe, Seemann and Schiele (2005); Wang and Wang (2011); Stewart, Andriluka and Ng (2016) solve the problem with detection-based approaches. Since most datasets provide only point-level supervision, most detection-based methods generate the pseudo-bounding boxes from point-level ground truth and update them in the training phase. However, they often struggle with scale variation and occlusion. To alleviate this problem, regression-based approaches Yan, Yuan, Zuo, Tan, Wang, Wen and Ding (2019); Zhang, Zhou, Chen, Gao and Ma (2016); Wang, Cai, Han, Zhou and Gong (2022); Wang, Cai, Zhou and Gong (2021) have emerged as popular alternatives in object counting, treating the task as dense regression to predict the object density map. This approach adeptly tackles overlap problems and achieves good performance. However, both of these approaches cannot handle object classes that are not present in the training phase.\n2.2. Few-Shot Object Counting\nFew-shot object counting aims to count arbitrary categories in the query image with just a few exemplar images. Pioneering methods Lu et al. (2019); Yang et al. (2021); Ranjan et al. (2021); Gong et al. (2022); Gao and Huang (2024) extract query and exemplar features, and leverage exemplar features as a fixed kernel to produce a correlation volume with query features. BMNet+ Shi et al. (2022) addresses a limitation in kernel-based matching mechanisms, which lack flexible channel interactions, by introducing a dynamic similarity metric capturing key exemplar patterns. SAFECount You et al. (2023) suggests a similarity-aware feature enhancement block that combines the advantages of both features and correlation volume. Recently, SPDCN Lin et al. (2022) and LOCA Djuki\u0107 et al. (2023) integrate the shape and appearance properties of exemplars to reflect diverse object scales.\nThese methods have achieved impressive performances, but they do not account for the target confusion issue sufficiently. To alleviate this, we introduce a novel framework, Mutual-Aware Feature Learning, which computes the query and exemplar features dependently on each other throughout the process of feature extraction.\n2.3. Vision Transformer\nMotivated by the great success in the field of natural language processing, extensive studies have been conducted to employ self-attention for vision tasks such as image classification Ramachandran, Parmar, Vaswani, Bello, Levskaya and Shlens (2019); Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit and Houlsby (2021), object detection Carion, Massa, Synnaeve, Usunier, Kirillov and Zagoruyko (2020); Meng, Chen, Fan, Zeng, Li, Yuan, Sun and Wang (2021), and semantic segmentation Yin, Wang, Wang, Xu, Zhang, Li and Jin (2022); Caron, Touvron, Misra, J\u00e9gou, Mairal, Bojanowski and Joulin (2021). Also, the transformer-based encoder has advantages in class-specific object counting Tran, Huy, Duong, Nguyen, Hung, Nguyen, Bui, Truong and VinBrain (2022); Liang, Chen, Xu, Zhou and Bai (2022). Recently, for few-shot object counting, CounTR Liu et al. (2022) utilizes a transformer-based architecture to capture self-similarity prior explicitly and shows a powerful performance.\nUnlike CounTR, which exploits separate feature extractors for query and exemplar images, MAFEA utilizes a mutually-aware feature extractor, computing the query and exemplar features in a unified embedding space. Moreover, MAFEA clearly distinguishes target object features from background features by introducing the background token."}, {"title": "3. Method", "content": "In this work, we introduce Mutually-Aware FEAture learning (MAFEA) to compute query and exemplar features mutually-aware of each other throughout the feature extractor, while the previous methods compute these features without any explicit feedback to each other, as illustrated in Figure 2. Specifically, MAFEA considers the co-relations between query and exemplar images in addition to the self-relations of each image. Moreover, we introduce a learnable background token to prevent undesired interactions between the exemplar and background features in co-relations. As a result, MAFEA can produce highly target-aware features that differentiate target object features from the background features, including the non-target object features.\n3.1. Overall Pipeline\nThe architecture of MAFEA consists of a ViT encoder, relation learner, and CNN decoder. Firstly, given a query image $I^Q \\in \\mathbb{R}^{3 \\times H^Q \\times W^Q}$ and a set of M exemplar images $I^E = \\{I_i^E \\in \\mathbb{R}^{3 \\times H^E \\times W^E}\\}_{i \\in \\{1,2, ..., M\\}}$ as input, they are split into $N^Q$ and $N^E$ image patches of resolution $S \\times S$ respectively, where $N^Q = H^QW^Q/S^2$ for the query image and $N^E = H^EW^E/S^2$ for the exemplar images. Then, image patches are converted into the query features $z^Q \\in \\mathbb{R}^{N^Q \\times C}$ and exemplar features $z^E \\in \\mathbb{R}^{N^E \\times C}$ by a projection function $\\mathbb{R}^{3 \\times S \\times S} \\rightarrow \\mathbb{R}^C$. Also, position embedding is added to $z^Q$ and $z^E$ to retain positional information, and $z^E$ are concatenated to define $z^E \\in \\mathbb{R}^{N_E \\times C}$ where $N_E$ denotes the sum of $N^E$. After that, $z^Q$ and $z^E$ are refined by the ViT encoder, which incorporates mutual relation modeling. Finally, the relation learner and CNN decoder sequentially receive the refined query and exemplar features and produce a density map $y \\in \\mathbb{R}^{1 \\times H^Q \\times W^Q}$. The number of objects in the query image is computed as the sum of density map.\n3.2. Mutual Relation Modeling\nThe core idea of MAFEA is to consider the mutual relationship between query and exemplar features from the outset. MAFEA encodes features based on two types of relationships: self-relations within each image, and co-relations between different images. The refined query $s^Q$ and exemplar features $s^E$, reflecting self-relations, are defined as follows:\n$s^Q = MHA(Q^Q, K^Q, V^Q)$,\n$s^E = MHA(Q^E, K^E, V^E)$,\n(1)\nwhere Q, K, and V represent queries, keys, and values fed to the multi-head attention block (MHA). We compute $Q^Q$, $K^Q$, and $V^Q$ by applying linear projections to the given query features $z^Q$ respectively, and produce $Q^E$, $K^E$, and $V^E$ using exemplar features $z^E$. The self-relation modeling guides the query and exemplar features to capture self-similarity within each image. Unlike the previous works only define self-relations throughout the feature extractor, we also refine query $c^{E\\rightarrow Q}$ and exemplar features $c^{Q\\rightarrow E}$ using co-relations as follows:\n$c^{E\\rightarrow Q} = MHA(Q^Q, K^E, V^E)$,\n$c^{Q\\rightarrow E} = MHA(Q^E, K^Q, V^Q)$.\n(2)\nThe correlation modeling enables bi-directional interaction between query and exemplar features. Firstly, the exemplars influence the query by identifying the difference between the target and non-target object features. Secondly, the query contributes to refining the exemplars, enabling them to aggregate diverse target object features. As a result, the encoder refines the query features to focus more on target-specific traits rather than general object characteristics. With self-relations and co-relations, the output sequences of the 1-th encoder layer are derived as follows:\n$z_l^{Q+1} = z_l^Q + s_l^Q + c_l^{E\\rightarrow Q}, l = 1, 2, ..., L - 1,$\n$z_l^{E+1} = z_l^E + s_l^E + c_l^{Q\\rightarrow E}, l = 1, 2, ..., L - 1,$\n(3)\nwhere $z^Q_l$ and $z^E_l$ are the input sequences of the $l$-th encoder layer. This modeling enables the encoder to adapt features not only based on their inherent self-relations but also their interrelated correlations.\n3.3. Background Token\nWhen computing $c^{E\\rightarrow Q}$ in Eq. 2, MAFEA utilizes only exemplar features to produce keys and values. Although the attention mechanism intrinsically mitigates improper co-relations, the background features, including non-target object features, might be represented by the exemplar features. In this case, it obscures the difference between the target object and background features, thus, it confuses in precisely identifying the actual target in the query features. In this regard, we introduce the background token which is designed to learn the general features of the background region. The background token $z^B \\in \\mathbb{R}^{1 \\times C}$ is concatenated with the exemplar features and then fed into the self-relation and co-relation modeling as follows:\n$s^{[E;B]} = MHA([Q^E; Q^B], [K^E; K^B], [V^E; V^B])$,\n$c^{[E;B]\\rightarrow Q} = MHA(Q^Q, [K^E; K^B], [V^E; V^B])$,\n$c^{Q\\rightarrow [E;B]} = MHA([Q^E; Q^B], K^Q, V^Q)$,\n(4)\nwhere $Q^B$, $K^B$, and $V^B$ are obtained by linear projections on the background token, respectively. $s^{[E;B]}$, $c^{[E;B]\\rightarrow Q}$, and $c^{Q\\rightarrow [E;B]}$ substitute $s^E$, $c^{E\\rightarrow Q}$, and $c^{Q\\rightarrow E}$ defined in Eq. 1 and Eq. 2, individually. By incorporating the background token into those relations, we can prevent the background features from being expressed by the exemplar features in the computation of $c^{[E;B]\\rightarrow Q}$.\n3.4. Target-Background Discriminative Loss\nAlthough the background token is designed to handle the background features of the query, it is not guaranteed without an explicit objective. In this regard, we define a target-background discriminative (TBD) loss which encourages the background token to align with background features. We first compute alignment score AS, which represents the degree of alignment between i-th query feature and the background token, as follows:\n$AS_i = \\frac{\\sum_{j=1}^{N_B} exp\\left(Q_i^Q \\cdot K_j^B\\right)}{\\sum_{j=1}^{N_E+N_B} exp\\left(Q_i^Q \\cdot [K^E; K^B]_j\\right)}$,\n(5)\nwhere $Q_i^Q$ is i-th $Q^Q$, and $K_j^B$ and $[K^E; K^B]_j$ denote j-th $K^B$ and $[K^E; K^B]$, respectively. Then, to align the background token only with background features, we divide the query features into positive set P, comprising features that spatially contain one or more ground-truth (GT) points, and negative set N which consists of features not including any GT points. As a result, we define TBD for the i-th query feature, as follows:\n$L_i^{TBD} = -1_{i \\in P}log\\left(1 - AS_i\\right) - 1_{i \\in N}log\\left(AS_i\\right)$,\n(6)\nwhere $i\\in P$ and $i\\in N$ mean i-th query features belong to the positive set and or not, respectively. Also, $L^{TBD}$ is the average value of $L_i^{TBD}$ over all query features.\n3.5. Training Loss\nOnce obtaining the target-aware features, $z^Q$ and $z^E$, we produce the correlation volume using the relation learner and convert it to the density map with the decoder. Following Djuki\u0107 et al. (2023), the relation learner performs iterative adaptation to produce intermediate correlation volumes, subsequently processed by auxiliary decoder blocks as follows:\n$C = Relation-Learner(z^Q, z^E)$,\n$y_k = Decoder_k(c_k), k = 1, 2, ..., K,$\n(7)\nwhere $C = \\{c_k\\}_{k=1}^K$ is the set of correlation volumes and $y_k$ is the output of the k-th decoder.\nWe adopt the object-normalized $l_2$ loss ($L^{count}$), which is the mean squared error between the predicted and ground truth density map normalized by the number of objects. The object-normalized $l_2$ loss is formulated as follows:\n$L^{count} = \\frac{1}{M} \\sum_{i=1}^M ||y - \\hat{y}||^2_2$,\n(8)\nwhere y and \u0177 are the predicted density and ground-truth density maps, respectively. M is the number of objects in mini-batch. Also, we utilize the auxiliary loss ($L^{aux}$) for the intermediate density maps as follows:\n$L^{aux} = \\frac{1}{M}\\sum_{i=1}^M \\sum_{k=1}^{K-1} ||y_k - \\hat{y}||^2_2$,\n(9)\nwhere $y_k$ is the intermediate density map of the k-th decoder and K - 1 is the number of intermediate density maps. The full objectives are defined as follows:\n$L = L^{count} + \\lambda_1L^{aux} + \\lambda_2L^{TBD}$,\n(10)\nwhere $\u03bb_1$ and $\u03bb_2$ are the weights of the auxiliary loss and TBD loss, respectively."}, {"title": "4. Experiments", "content": "In this section, we first describe the experimental settings. We then compare it with the current state-of-the-art methods. Finally, we provide in-depth analyses of the results through the various ablation studies.\n4.1. Implementation Details\n4.1.1. Architecture.\nOur framework comprises a ViT encoder, relation learner, and CNN decoder. The patch size is set to 16 \u00d7 16, and both the kernel and stride of the projection head are set to 16\u00d716 corresponding to the patch size. The ViT encoder comprises 12 transformer encoder blocks where the hidden dimension of each block is 768, and the multi-head attention of each block consists of 12 heads. The relation learner, inspired by LOCA Djuki\u0107 et al. (2023), incorporates Object Prototype Extraction (OPE) modules and Prototype Matching (PM). OPE integrates object shapes and appearance properties using a three-layered iterative adaption module. Each layer includes two multi-head attention blocks and a feed-forward network. Instead of ROI pooled features, we utilize exemplar features as appearance properties. PM involves the depth-wise correlation and max operation. Further details are provided in the appendix. The CNN decoder comprises 4 convolutions and bilinear upsampling layers to regress a 2D density map. The ViT encoder is initialized with a self-supervised pre-trained weight from MAEHe, Chen, Xie, Li, Doll\u00e1r and Girshick (2022). On the other hand, the parameters of the relation learner and CNN decoder are randomly initialized.\n4.1.2. Training details.\nWe apply the same pre-processing as LOCA. As the inputs, the query image is resized to 512 \u00d7 512 and exemplar images are scaled to 48 \u00d7 48 based on provided box annotations. We set the weights $\u03bb_1$ and $\u03bb_2$ for auxiliary and TBD loss in Eq. 10 to 0.3 and 0.05, respectively. AdamW optimizer is employed with a batch size of 8. The initial learning rate is 1e\u22124 and is halved every 40 epochs. Training is performed on a single RTX3090 GPU for 100 epochs.\n4.2. Datasets and Metrics.\n4.2.1. Datasets.\nWe experiment on three benchmark datasets: FSCD-LVIS Nguyen, Pham, Nguyen and Hoai (2022), FSC-147 Ranjan et al. (2021), and CARPK Hsieh, Lin and Hsu (2017). FSCD-LVIS is designed for few-shot object counting and detection in complex scenes with multiple class objects. It contains 6195 images across 372 classes, split into 3953 train and 2242 test images. FSC-147, a few-shot object counting dataset, consists of simpler scenes where most images contain only a target class. It includes 6135 images of 147 classes, divided into 3659 train, 1286 validation, and 1190 test images. Both datasets include three randomly selected exemplars per image to depict the target objects. Note that, there is no shared object class between the sets. Furthermore, we validate our models' generalization capability on the test set of CARPK, a dataset tailored for counting cars, comprising 459 drone-captured images.\n4.2.2. Configuration of Multi-Class Subset.\nAlthough the FSC-147 contains a large number of objects in each image, the scene of each image is mostly composed of single-class objects. Due to this inherent characteristic, the evaluation of the FSC-147 might not accurately assess the ability of the model to identify the target class within an image containing diverse object categories. For a quantitative assessment of whether the model suffers from the target confusion problem, we construct a multi-class subset of the FSC-147 (FSC-147-Multi). We selectively remove images where objects from other classes amount to less than 20% of the target class objects, to exclude single-object predominant images in the multi-class experiments. The indices of the images that make up the FSC-147-Multi can be found in Table 1, and experimental results are detailed in Sec. 4.3.\n4.2.3. Metrics.\nGenerally, the counting methods are evaluated using Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). These metrics are defined as follows:\n$MAE = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|,$ \n$RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2},$\n(11)\nwhere n denotes the number of images, $y_i$ and $\\hat{y}_i$ are the ground truth and predicted counts for i-th image, respectively.\n4.3. Comparison with the State-of-the-Art\nTo evaluate the model's robustness against target confusion, assessments need to be conducted in multi-class scenes where diverse class objects coexist. Given that the FSC-147 dataset mainly consists of simple scenes with only target class objects, its limited multi-class scenes are not suitable for target confusion evaluation. Thus, we construct a multi-class subset of the FSC-147 (FSC-147-Multi), where images contain non-target objects more than 20% of the number of existing target class objects. Initially, we compare our model with state-of-the-art (SOTA) methods in a complex multi-class scenario, and then extend the evaluation to a simpler single-class scenario.\n4.3.1. Evaluation on a multi-class scenario\nWe compare our method with SOTA methods on multi-class datasets: FSCD-LVIS and FSC-147-Multi, as shown in Table 2. For a fair comparison on FSCD-LVIS, we reproduce all baseline models using an image size of 512 by utilizing official codes. Regarding Counting-DETR, we report its official performance. On the FSCD-LVIS, our method outperforms all baselines, showing an improvement of 11% in MAE and 4.6% in RMSE compared to the second-best performer. Similarly, on the FSC-147-Multi, our results demonstrate a significant performance gap compared to the SOTA method, showing a relative improvement of 26.30% in MAE and 24.63% in RMSE.\n4.3.2. Evaluation on a single-class scenario\nWe further evaluate our method on the validation and test sets of the FSC-147 dataset, as shown in Table 3. Our method surpasses all baselines, showing an enhancement in MAEs of 12.9% and 8.8% for validation and test sets, respectively. It demonstrates the effectiveness of MAFEA even in a single-class scenario. In addition, we report the mean and standard deviation of results with 95% confidence intervals over five runs. Notably, the test set's RMSE exhibits a high standard deviation. To analyze a factor influencing test RMSE, we conduct evaluations by excluding extremely high-dense images with over 1000 instances. A total of 2 (over 1190) images are excluded from the test set. This exclusion leads to a notable improvement in LOCA's RMSE, decreasing from 56.97 to 32.85 (42.3%). Similarly, MAFEA's performance dramatically improves with RMSE decreasing from 56.68 to 34.35 (39.4%). This confirms the sensitivity of RMSE to errors in high-dense images and indicates the dependency of RMSE on how well the model fits extremely high-dense images.\n4.3.3. Qualitative results\nIn Fig. 3, the 1st and 2nd rows illustrate qualitative results of the FSCD-LVIS. As shown in the 3rd to 6th columns, prior methods struggle to identify the target class in a query using exemplar images; they often count not only target objects but also non-target objects sharing similar scale or appearance properties. In contrast, our method excels in precisely distinguishing target objects based on the exemplar images, a success attributable to our mutually-aware feature learning. The 3rd and 4th rows show the results for the FSC-147. Our method makes more accurate predictions compared to other methods, especially on dense images.\n4.4. Cross-Dataset Generalization\nWe evaluate the generalization ability of our model on a car counting dataset, CARPK. To avoid overlap between the train and test sets, the tested models are pre-trained with FSC-147 by excluding its car category. The results are summarized in Table 4. Note that, we do not fine-tune our model on the CARPK. As reported, our method outperforms the current state-of-the-art methods. It demonstrates the robustness of our method in cross-dataset generalization.\n4.5. Ablation Study\nTo verify the effectiveness of our approach, we conduct extensive ablation studies on FSCD-LVIS and FSC-147.\n4.5.1. Component-level analysis on multi-class scenario\nFirstly, we verify the effect of integrating mutual relation modeling into the feature extractor. In Table 5 and Table 6, the 1st row presents the result of the model that independently computes query and exemplar features by a shared feature extractor. Compared with it, MRM shows noteworthy enhancements in both datasets, with a 6.6% MAE gain in FSCD-LVIS, and a 31.2% MAE improvement in FSC-147-Multi. This highlights the importance of mutual awareness in computing query and exemplar features. Subsequently, we delve into the impact of BT and TBD loss. As shown in the 3rd and 4th rows, while BT yields a slight performance improvement when used alone, its combination with TBD loss leads to a notable performance enhancement. BT brings a performance gain of 4.5% MAE in FSCD-LVIS and 10.4% MAE in FSC-147-Multi, while the TBD loss achieves an additional performance gain of 6.1% MAE in FSCD-LVIS and 14.1% in FSC-147-Multi. It demonstrates that minimizing undesired interactions between query and exemplar features enhances target recognition of the model.\nFurthermore, we assess performance in the target and non-target regions to verify whether the models count only the target objects. This is imperative since the evaluation within the entire region may compensate for potential under-predictions in the target region by incorrect predictions in the non-target region. In the few-shot object counting (FSOC), each object is annotated only with its center point, which is not sufficient to estimate good boundaries between the target region and non-target region. To overcome this limitation, we expand each point annotation to encompass an area equivalent to the maximum size of exemplars. Consequently, the target region encompasses all target objects, while the non-target region covers the complementary area. We provide the visualization of the target region map and corresponding results in Figure 4. If there is no target confusion issue, the model's prediction in the non-target region should be zero. Impressively, MRM, BT, and TBD bring substantial performance improvements in both target and non-target areas. The notable enhancement in the non-target area validates that the proposed components alleviate target confusion as intended.\n4.5.2. Component-level analysis on single-class scenario\nWe further assess our proposed module in the validation and test sets of FSC-147. As shown in Table 7, MRM achieves a performance gain of 12.0% MAE and 11.5% MAE for the validation and test sets, respectively. The ablation of BT brings an improvement of 8.2% MAE and 5.7% MAE for the validation and test sets, respectively. Also, TBD brings an improvement in MAEs of 11.2% and 8.7% for validation and test sets, respectively. These results confirm the effectiveness of our method even in the single-class scenario.\n4.5.3. The number of exemplars\nWe further investigate the impact of the exemplar's numbers in Table 8. From the zero-shot where exemplar features are replaced by learnable tokens, to the 3-shot, our method provides better performance as more exemplars are provided. These results validate that the exemplar features contribute to refining the query features in a mutually-aware manner.\n4.5.4. Influence of the background token\nTo validate whether the background token highlights the background region, we visualize the alignment score (AS) map in Fig. 5. We also compute the AS map for the exemplar features by summing up the scores of these features. As shown in the Fig. 5, the exemplar features activate only the target areas, while the background token highlights the background areas including the non-target class objects. These results show that our model can effectively differentiate between target objects and background regions.\n4.6. Additional Qualitative Results\nWe show more qualitative results. As reported in Figure 6a, we can observe that our model makes precise predictions from sparse to dense scenes for the novel classes. However, as seen in Figure 6b, accurate predictions are challenging for images with over 1000 instances, which are very dense. This issue is not unique to MAFEA. Given the practical application of few-shot object counting models, we believe that having 1000 objects within a single image is an uncommon scenario."}, {"title": "5. Conclusion", "content": "In this work, we discover and solve the target confusion problem, which arises when multiple object classes coexist in the query image, resulting in inaccurate identification of the target objects. To settle this problem, we introduce a novel perspective, Mutually-Aware Feature Learning (MAFEA), which encourages interaction between query and exemplar features from the outset. By incorporating mutual relation modeling into the feature extractor, our model produces highly target-aware query features, facilitating the distinction between target and non-target objects. Additionally, the background token with Target-Background Discriminative (TBD) loss enables the model to effectively differentiate target objects from background features. We demonstrate the robustness of our method against the target confusion problem through evaluations in a complex multi-class scenario, such as FSCD-LVIS and FSC-147-Multi. Moreover, we validate its effectiveness even in a single-class scenario. Experimental results validate the unique merits of our proposed method by achieving state-of-the-art performances and demonstrating its effectiveness in addressing the target confusion problem."}]}