{"title": "Mutually-Aware Feature Learning for Few-Shot Object Counting", "authors": ["Yerim Jeon", "Subeen Lee", "Jihwan Kim", "Jae-Pil Heo"], "abstract": "Few-shot object counting has garnered significant attention for its practicality as it aims to count\ntarget objects in a query image based on given exemplars without the need for additional training.\nHowever, there is a shortcoming in the prevailing extract-and-match approach: query and exemplar\nfeatures lack interaction during feature extraction since they are extracted unaware of each other and\nlater correlated based on similarity. This can lead to insufficient target awareness of the extracted\nfeatures, resulting in target confusion in precisely identifying the actual target when multiple class\nobjects coexist. To address this limitation, we propose a novel framework, Mutually-Aware FEAture\nlearning (MAFEA), which encodes query and exemplar features mutually aware of each other from\nthe outset. By encouraging interaction between query and exemplar features throughout the entire\npipeline, we can obtain target-aware features that are robust to a multi-category scenario. Furthermore,\nwe introduce a background token to effectively associate the target region of query with exemplars\nand decouple its background region from them. Our extensive experiments demonstrate that our model\nreaches a new state-of-the-art performance on the two challenging benchmarks, FSCD-LVIS and FSC-\n147, with a remarkably reduced degree of the target confusion problem.", "sections": [{"title": "1. Introduction", "content": "Object counting has achieved remarkable advances along\nwith deep learning networks. However, most existing object\ncounting methods are designed for a limited number of cate-\ngories such as human? or car?. In fact, those methods highly\nrely on a large amount of labeled data and cannot handle\nunseen categories beyond training data. In this regard, few-\nshot object counting Lu, Xie and Zisserman (2019) has been\nproposed to count arbitrary class objects in a query image\nbased on the given exemplar images.\nA mainstream of few-shot object counting is the extract-\nand-match approach Lu et al. (2019); Yang, Su, Hsu and\nChen (2021); Ranjan, Sharma, Nguyen and Hoai (2021);\nShi, Lu, Feng, Liu and Cao (2022); Gong, Zhang, Yang, Dai\nand Schiele (2022); You, Yang, Luo, Lu, Cui and Le (2023);\nLin, Yang, Ma, Gao, Liu, Liu, Hou, Yi and Chan (2022);\nLiu, Zhong, Zisserman and Xie (2022); Djuki\u0107, Luke\u017ei\u010d,\nZavrtanik and Kristan (2023); Gao and Huang (2024). Gen-\nerally, this pipeline consists of three key components: 1)\nfeature extractor, 2) relation learner, and 3) decoder. Firstly,\nthey compute query and exemplar features using the feature\nextractor, then construct the correlation volume through the\nrelation learner. Afterward, they estimate the number of\ninstances in the query image by transferring the correlation\nvolume to the decoder.\nAlthough previous studies You et al. (2023); Djuki\u0107 et al.\n(2023) have achieved impressive performance, they exhibit\na target confusion issue, failing to accurately identify only\nthe target class when multiple classes of objects coexist in\nthe query image, as shown in Figure 1. Existing methods\nhave overlooked this problem, which is directly connected\nto the purpose of few-shot object counting, as benchmark\ndatasets such as FSC-147 Ranjan et al. (2021) primarily\nconsist of single-class scenes. The main reason for the target\nconfusion is that the query features are computed without\nany explicit guidance of the target class. Consequently, the\nquery features tend to focus on objectness rather than target\nclass-specific features, hindering the differentiation between\ntarget and non-target object features.\nTo address this, we propose a novel framework, Mutually-\nAware FEAture Learning (MAFEA), which enables the early\nconsideration of mutual relations between query and exem-\nplar features to produce the target class-specific features.\nSpecifically, MAFEA employs cross-attention to capture\nbi-directional co-relations between query and exemplar"}, {"title": "2. Related Work", "content": "2.1. Class-Specific Object Counting\nClass-specific object counting aims to count objects of\na specific class, such as people?, animals Arteta, Lem-\npitsky and Zisserman (2016), and cars ?, in the images.\nFor this purpose, traditional methods Leibe, Seemann and\nSchiele (2005); Wang and Wang (2011); Stewart, Andriluka\nand Ng (2016) solve the problem with detection-based ap-\nproaches. Since most datasets provide only point-level super-\nvision, most detection-based methods generate the pseudo-\nbounding boxes from point-level ground truth and update\nthem in the training phase. However, they often struggle\nwith scale variation and occlusion. To alleviate this problem,\nregression-based approaches Yan, Yuan, Zuo, Tan, Wang,\nWen and Ding (2019); Zhang, Zhou, Chen, Gao and Ma\n(2016); Wang, Cai, Han, Zhou and Gong (2022); Wang, Cai,\nZhou and Gong (2021) have emerged as popular alternatives\nin object counting, treating the task as dense regression to\npredict the object density map. This approach adeptly tackles\noverlap problems and achieves good performance. However,\nboth of these approaches cannot handle object classes that\nare not present in the training phase.\n2.2. Few-Shot Object Counting\nFew-shot object counting aims to count arbitrary cate-\ngories in the query image with just a few exemplar images.\nPioneering methods Lu et al. (2019); Yang et al. (2021);\nRanjan et al. (2021); Gong et al. (2022); Gao and Huang\n(2024) extract query and exemplar features, and leverage\nexemplar features as a fixed kernel to produce a correlation\nvolume with query features. BMNet+ Shi et al. (2022) ad-\ndresses a limitation in kernel-based matching mechanisms,\nwhich lack flexible channel interactions, by introducing a\ndynamic similarity metric capturing key exemplar patterns.\nSAFECount You et al. (2023) suggests a similarity-aware\nfeature enhancement block that combines the advantages of\nboth features and correlation volume. Recently, SPDCN Lin\net al. (2022) and LOCA Djuki\u0107 et al. (2023) integrate the\nshape and appearance properties of exemplars to reflect\ndiverse object scales.\nThese methods have achieved impressive performances,\nbut they do not account for the target confusion issue suffi-\nciently. To alleviate this, we introduce a novel framework,\nMutual-Aware Feature Learning, which computes the query\nand exemplar features dependently on each other throughout\nthe process of feature extraction.\n2.3. Vision Transformer\nMotivated by the great success in the field of natural\nlanguage processing, extensive studies have been conducted\nto employ self-attention for vision tasks such as image classi-\nfication Ramachandran, Parmar, Vaswani, Bello, Levskaya\nand Shlens (2019); Dosovitskiy, Beyer, Kolesnikov, Weis-\nsenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold,\nGelly, Uszkoreit and Houlsby (2021), object detection Car-\nion, Massa, Synnaeve, Usunier, Kirillov and Zagoruyko\n(2020); Meng, Chen, Fan, Zeng, Li, Yuan, Sun and Wang\n(2021), and semantic segmentation Yin, Wang, Wang,\nXu, Zhang, Li and Jin (2022); Caron, Touvron, Misra,\nJ\u00e9gou, Mairal, Bojanowski and Joulin (2021). Also, the\ntransformer-based encoder has advantages in class-specific\nobject counting Tran, Huy, Duong, Nguyen, Hung, Nguyen,\nBui, Truong and VinBrain (2022); Liang, Chen, Xu, Zhou\nand Bai (2022). Recently, for few-shot object counting,\nCounTR Liu et al. (2022) utilizes a transformer-based\narchitecture to capture self-similarity prior explicitly and\nshows a powerful performance.\nUnlike CounTR, which exploits separate feature extrac-\ntors for query and exemplar images, MAFEA utilizes a\nmutually-aware feature extractor, computing the query and\nexemplar features in a unified embedding space. Moreover,\nMAFEA clearly distinguishes target object features from\nbackground features by introducing the background token."}, {"title": "3. Method", "content": "In this work, we introduce Mutually-Aware FEAture\nlearning (MAFEA) to compute query and exemplar fea-\ntures mutually-aware of each other throughout the feature\nextractor, while the previous methods compute these features\nwithout any explicit feedback to each other, as illustrated\nin Figure 2. Specifically, MAFEA considers the co-relations\nbetween query and exemplar images in addition to the self-\nrelations of each image. Moreover, we introduce a learnable\nbackground token to prevent undesired interactions between\nthe exemplar and background features in co-relations. As\na result, MAFEA can produce highly target-aware features\nthat differentiate target object features from the background\nfeatures, including the non-target object features.\n3.1. Overall Pipeline\nThe architecture of MAFEA consists of a ViT encoder,\nrelation learner, and CNN decoder. Firstly, given a query\nimage \\(I^{Q} \\in \\mathbb{R}^{3\\times H^{Q}\\times W^{Q}}\\) and a set of M exemplar images\n\n\\(I^{E} = {I^{E}_{i} \\in \\mathbb{R}^{3\\times H^{E}\\times W^{E}}} \\; i \\in {1,2, ..., M}\\)\nas input, they\nare split into \\(N^{Q}\\) and \\(N^{E}\\) image patches of resolution S\u00d7S\nrespectively, where \\(N^{Q} = H^{Q}W^{Q}/S^{2}\\) for the query image\nand \\(N^{E} = H^{E}W^{E}/S^{2}\\) for the exemplar images. Then,\nimage patches are converted into the query features \\(z^{Q} \\in\n\\mathbb{R}^{N^{Q}\\times C}\\) and exemplar features \\(z^{E} \\in \\mathbb{R}^{N^{E}\\times C}\\) by a projection\nfunction \\(\\mathbb{R}^{3\\times S\\times S} \\rightarrow \\mathbb{R}^{C}\\). Also, position embedding is added\nto \\(z^{Q}\\) and \\(z^{E}\\) to retain positional information, and \\(z^{E}\\) are\nconcatenated to define \\(\\hat{z}^{E} \\in \\mathbb{R}^{N_{E}\\times C}\\) where \\(N_{E}\\) denotes\nthe sum of \\(N^{E}\\). After that, \\(z^{Q}\\) and \\(\\hat{z}^{E}\\) are refined by the\nViT encoder, which incorporates mutual relation modeling.\nFinally, the relation learner and CNN decoder sequentially\nreceive the refined query and exemplar features and produce\na density map \\(y \\in \\mathbb{R}^{1\\times H^{Q}\\times W^{Q}}\\). The number of objects in\nthe query image is computed as the sum of density map.\n3.2. Mutual Relation Modeling\nThe core idea of MAFEA is to consider the mutual\nrelationship between query and exemplar features from the\noutset. MAFEA encodes features based on two types of rela-\ntionships: self-relations within each image, and co-relations\nbetween different images. The refined query \\(s^{Q}\\) and ex-\nemplar features \\(s^{E}\\), reflecting self-relations, are defined as\nfollows:\n\\[\ns^{Q} = MHA(Q^{Q}, K^{Q}, V^{Q}),\ns^{E} = MHA(Q^{E}, K^{E}, V^{E}),\n\\]\nwhere Q, K, and V represent queries, keys, and values fed\nto the multi-head attention block (MHA). We compute \\(Q^{Q}\\),\n\\(K^{Q}\\), and \\(V^{Q}\\) by applying linear projections to the given\nquery features \\(z^{Q}\\) respectively, and produce \\(Q^{E}\\), \\(K^{E}\\), and\n\\(V^{E}\\) using exemplar features \\(z^{E}\\). The self-relation modeling\nguides the query and exemplar features to capture self-\nsimilarity within each image. Unlike the previous works only\ndefine self-relations throughout the feature extractor, we also\nrefine query \\(c^{E\\rightarrow Q}\\) and exemplar features \\(c^{Q\\rightarrow E}\\) using co-\nrelations as follows:\n\\[\nc^{E\\rightarrow Q} = MHA(Q^{Q}, K^{E},V^{E}),\nc^{Q\\rightarrow E} = MHA(Q^{E}, K^{Q}, V^{Q}).\n\\]\nThe correlation modeling enables bi-directional interaction\nbetween query and exemplar features. Firstly, the exemplars\ninfluence the query by identifying the difference between\nthe target and non-target object features. Secondly, the query\ncontributes to refining the exemplars, enabling them to ag-\ngregate diverse target object features. As a result, the encoder\nrefines the query features to focus more on target-specific\ntraits rather than general object characteristics. With self-\nrelations and co-relations, the output sequences of the l-th\nencoder layer are derived as follows:\n\\[\nz_{l+1}^{Q} = z_{l}^{Q} + s_{l}^{Q} + c_{l}^{E\\rightarrow Q}, l = 1, 2, ..., L - 1,\nz_{l+1}^{E} = z_{l}^{E} + s_{l}^{E} + c_{l}^{Q\\rightarrow E}, l = 1, 2, ..., L - 1,\n\\]"}, {"title": "3.3. Background Token", "content": "When computing \\(c^{E\\rightarrow Q}\\) in Eq. 2, MAFEA utilizes only\nexemplar features to produce keys and values. Although\nthe attention mechanism intrinsically mitigates improper\nco-relations, the background features, including non-target\nobject features, might be represented by the exemplar fea-\ntures. In this case, it obscures the difference between the\ntarget object and background features, thus, it confuses in\nprecisely identifying the actual target in the query features.\nIn this regard, we introduce the background token which\nis designed to learn the general features of the background\nregion. The background token \\(z^{B} \\in \\mathbb{R}^{1\\times C}\\) is concatenated\nwith the exemplar features and then fed into the self-relation\nand co-relation modeling as follows:\n\\[\ns^{[E;B]} = MHA([Q^{E}; Q^{B}], [K^{E}; K^{B}], [V^{E}; V^{B}]),\nc^{[E;B]\\rightarrow Q} = MHA(Q^{Q}, [K^{E}; K^{B}], [V^{E}; V^{B}]),\nc^{Q\\rightarrow [E;B]} = MHA([Q^{E}; Q^{B}], K^{Q}, V^{Q}),\n\\]\nwhere \\(Q^{B}\\), \\(K^{B}\\), and \\(V^{B}\\) are obtained by linear projections\non the background token, respectively. \\(s^{[E;B]}\\), \\(c^{[E;B]\\rightarrow Q}\\), and\n\\(c^{Q\\rightarrow [E;B]}\\) substitute \\(s^{E}\\), \\(c^{E\\rightarrow Q}\\), and \\(c^{Q\\rightarrow E}\\) defined in Eq. 1\nand Eq. 2, individually. By incorporating the background\ntoken into those relations, we can prevent the background\nfeatures from being expressed by the exemplar features in\nthe computation of \\(c^{[E;B]\\rightarrow Q}\\)."}, {"title": "3.4. Target-Background Discriminative Loss", "content": "Although the background token is designed to handle the\nbackground features of the query, it is not guaranteed without\nan explicit objective. In this regard, we define a target-\nbackground discriminative (TBD) loss which encourages the\nbackground token to align with background features. We first\ncompute alignment score AS, which represents the degree\nof alignment between i-th query feature and the background\ntoken, as follows:\n\\[\nAS_{i} = \\frac{\\sum_{j=1}^{N_{B}} exp(Q_{i}^{Q} \\cdot K_{j}^{B})}{\\sum_{j=1}^{N_{E}+N_{B}} (exp (Q_{i}^{Q} \\cdot [K^{E}; K^{B}]_{j}))}\n\\]\nwhere \\(Q^{Q}_{i}\\) is i-th \\(Q^{Q}\\), and \\(K^{B}_{j}\\) and \\([K^{E}; K^{B}]_{j}\\) denote j-th\n\\(K^{B}\\) and \\([K^{E}; K^{B}]\\), respectively. Then, to align the back-\nground token only with background features, we divide the\nquery features into positive set P, comprising features that\nspatially contain one or more ground-truth (GT) points, and\nnegative set N which consists of features not including any\nGT points. As a result, we define TBD for the i-th query\nfeature, as follows:\n\\[\nL_{i}^{TBD} = -1_{z^{Q}_{i} \\in P}log(1 - AS_{i}) - 1_{z^{Q}_{i} \\in N}log(AS_{i}),\n\\]\nwhere \\(z^{Q}_{i} \\in P\\) and \\(z^{Q}_{i} \\in N\\) mean i-th query features belong\nto the positive set and or not, respectively. Also, \\(L^{TBD}\\) is the\naverage value of \\(L_{i}^{TBD}\\) over all query features."}, {"title": "3.5. Training Loss", "content": "Once obtaining the target-aware features, \\(z^{Q}\\) and \\(z^{E}\\), we\nproduce the correlation volume using the relation learner\nand convert it to the density map with the decoder. Fol-\nlowing Djuki\u0107 et al. (2023), the relation learner performs\niterative adaptation to produce intermediate correlation vol-\numes, subsequently processed by auxiliary decoder blocks\nas follows:\n\\[\nC = Relation\\text{-}Learner(z^{Q}, z^{E}),\ny^{k} = Decoder^{k}(c^{k}), k = 1, 2, ..., K,\n\\]\nwhere \\(C = {c^{k}}\\_{k=1}^{K}\\) is the set of correlation volumes and \\(y^{k}\\)\nis the output of the k-th decoder.\nWe adopt the object-normalized \\(l2\\) loss (\\(\\mathcal{L}^{count}\\)), which\nis the mean squared error between the predicted and ground\ntruth density map normalized by the number of objects. The\nobject-normalized \\(l2\\) loss is formulated as follows:\n\\[\n\\mathcal{L}^{count} = \\frac{1}{M} \\sum \\|y - \\hat{y}\\|_{2}^{2},\n\\]\nwhere y and \\(\\hat{y}\\) are the predicted density and ground-truth\ndensity maps, respectively. M is the number of objects in\nmini-batch. Also, we utilize the auxiliary loss (\\(\\mathcal{L}^{aux}\\)) for the\nintermediate density maps as follows:\n\\[\n\\mathcal{L}^{aux} = \\sum_{M} \\sum_{k=1}^{K-1} \\|y - y^{k}\\|_{2}^{2},\n\\]\nwhere \\(y^{k}\\) is the intermediate density map of the k-th decoder\nand K - 1 is the number of intermediate density maps. The\nfull objectives are defined as follows:\n\\[\n\\mathcal{L} = \\mathcal{L}^{count} + \\lambda_{1}\\mathcal{L}^{aux} + \\lambda_{2}\\mathcal{L}^{TBD},\n\\]\nwhere \\(\\lambda_{1}\\) and \\(\\lambda_{2}\\) are the weights of the auxiliary loss and\nTBD loss, respectively."}, {"title": "4. Experiments", "content": "In this section, we first describe the experimental set-\ntings. We then compare it with the current state-of-the-art\nmethods. Finally, we provide in-depth analyses of the results\nthrough the various ablation studies.\n4.1. Implementation Details\n4.1.1. Architecture.\nOur framework comprises a ViT encoder, relation learner,\nand CNN decoder. The patch size is set to 16 \u00d7 16, and both\nthe kernel and stride of the projection head are set to 16\u00d716\ncorresponding to the patch size. The ViT encoder comprises\n12 transformer encoder blocks where the hidden dimension\nof each block is 768, and the multi-head attention of each\nblock consists of 12 heads. The relation learner, inspired by"}, {"title": "4.2. Datasets and Metrics.", "content": "4.2.1. Datasets.\nWe experiment on three benchmark datasets: FSCD-\nLVIS Nguyen, Pham, Nguyen and Hoai (2022), FSC-147 Ran-\njan et al. (2021), and CARPK Hsieh, Lin and Hsu (2017).\nFSCD-LVIS is designed for few-shot object counting and\ndetection in complex scenes with multiple class objects. It\ncontains 6195 images across 372 classes, split into 3953 train\nand 2242 test images. FSC-147, a few-shot object counting\ndataset, consists of simpler scenes where most images\ncontain only a target class. It includes 6135 images of 147\nclasses, divided into 3659 train, 1286 validation, and 1190\ntest images. Both datasets include three randomly selected\nexemplars per image to depict the target objects. Note that,\nthere is no shared object class between the sets. Furthermore,\nwe validate our models' generalization capability on the\ntest set of CARPK, a dataset tailored for counting cars,\ncomprising 459 drone-captured images.\n4.2.2. Configuration of Multi-Class Subset.\nAlthough the FSC-147 contains a large number of ob-\njects in each image, the scene of each image is mostly com-\nposed of single-class objects. Due to this inherent charac-\nteristic, the evaluation of the FSC-147 might not accurately\nassess the ability of the model to identify the target class\nwithin an image containing diverse object categories. For a\nquantitative assessment of whether the model suffers from\nthe target confusion problem, we construct a multi-class\nsubset of the FSC-147 (FSC-147-Multi). We selectively\nremove images where objects from other classes amount to\nless than 20% of the target class objects, to exclude single-\nobject predominant images in the multi-class experiments.\nThe indices of the images that make up the FSC-147-Multi\ncan be found in Table 1, and experimental results are detailed\nin Sec. 4.3.\n4.2.3. Metrics.\nGenerally, the counting methods are evaluated using\nMean Absolute Error (MAE) and Root Mean Squared Er-\nror (RMSE). These metrics are defined as follows:\n\\[\nMAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_{i} - \\hat{y_{i}}|,\n\\]\n\\[\nRMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_{i} - \\hat{y_{i}})^{2}},\n\\]\nwhere n denotes the number of images, \\(y_{i}\\) and \\(\\hat{y_{i}}\\) are the\nground truth and predicted counts for i-th image, respec-\ntively."}, {"title": "4.3. Comparison with the State-of-the-Art", "content": "To evaluate the model's robustness against target confu-\nsion, assessments need to be conducted in multi-class scenes\nwhere diverse class objects coexist. Given that the FSC-147\ndataset mainly consists of simple scenes with only target\nclass objects, its limited multi-class scenes are not suitable\nfor target confusion evaluation. Thus, we construct a multi-\nclass subset of the FSC-147 (FSC-147-Multi), where images\ncontain non-target objects more than 20% of the number of\nexisting target class objects. Initially, we compare our model\nwith state-of-the-art (SOTA) methods in a complex multi-\nclass scenario, and then extend the evaluation to a simpler\nsingle-class scenario.\n4.3.1. Evaluation on a multi-class scenario\nWe compare our method with SOTA methods on multi-\nclass datasets: FSCD-LVIS and FSC-147-Multi, as shown\nin Table 2. For a fair comparison on FSCD-LVIS, we re-\nproduce all baseline models using an image size of 512\nby utilizing official codes. Regarding Counting-DETR, we\nreport its official performance. On the FSCD-LVIS, our\nmethod outperforms all baselines, showing an improvement\nof 11% in MAE and 4.6% in RMSE compared to the second-\n      best performer. Similarly, on the FSC-147-Multi, our results\ndemonstrate a significant performance gap compared to the"}, {"title": "4.4. Cross-Dataset Generalization", "content": "We evaluate the generalization ability of our model on\na car counting dataset, CARPK. To avoid overlap between\nthe train and test sets, the tested models are pre-trained\nwith FSC-147 by excluding its car category. The results are\nsummarized in Table 4. Note that, we do not fine-tune our\nmodel on the CARPK. As reported, our method outperforms\nthe current state-of-the-art methods. It demonstrates the\nrobustness of our method in cross-dataset generalization."}, {"title": "4.5. Ablation Study", "content": "To verify the effectiveness of our approach, we conduct\nextensive ablation studies on FSCD-LVIS and FSC-147.\nSpecifically, we begin with component-level analysis on\nmulti-class and single-class scenarios and then investigate\nthe impact of the number of exemplars. Additionally, we\nvisualize the attention maps of the Alignment Score (AS)\nmap to validate the role of the background token.\n4.5.1. Component-level analysis on multi-class\nscenario\nFirstly, we verify the effect of integrating mutual relation\nmodeling into the feature extractor. In Table 5 and Table 6,\nthe 1st row presents the result of the model that indepen-\ndently computes query and exemplar features by a shared\nfeature extractor. Compared with it, MRM shows noteworthy"}, {"title": "4.5.2. Component-level analysis on single-class\nscenario", "content": "We further assess our proposed module in the validation\nand test sets of FSC-147. As shown in Table 7, MRM\nachieves a performance gain of 12.0% MAE and 11.5% MAE\nfor the validation and test sets, respectively. The ablation of\nBT brings an improvement of 8.2% MAE and 5.7% MAE\nfor the validation and test sets, respectively. Also, TBD\nbrings an improvement in MAEs of 11.2% and 8.7% for\nvalidation and test sets, respectively. These results confirm\nthe effectiveness of our method even in the single-class\nscenario."}, {"title": "4.5.3. The number of exemplars", "content": "We further investigate the impact of the exemplar's num-\nbers in Table 8. From the zero-shot where exemplar fea-\ntures are replaced by learnable tokens, to the 3-shot, our\nmethod provides better performance as more exemplars are\nprovided. These results validate that the exemplar features\ncontribute to refining the query features in a mutually-aware\nmanner."}, {"title": "4.5.4. Influence of the background token", "content": "To validate whether the background token highlights the\nbackground region, we visualize the alignment score (AS)\nmap in Fig. 5. We also compute the AS map for the exemplar\nfeatures by summing up the scores of these features. As\nshown in the Fig. 5, the exemplar features activate only the\ntarget areas, while the background token highlights the back-\nground areas including the non-target class objects. These\nresults show that our model can effectively differentiate\nbetween target objects and background regions."}, {"title": "4.6. Additional Qualitative Results", "content": "We show more qualitative results. As reported in Fig-\nure 6a, we can observe that our model makes precise pre-\ndictions from sparse to dense scenes for the novel classes.\nHowever, as seen in Figure 6b, accurate predictions are\nchallenging for images with over 1000 instances, which are\nvery dense. This issue is not unique to MAFEA. Given the\npractical application of few-shot object counting models, we\nbelieve that having 1000 objects within a single image is an\nuncommon scenario."}, {"title": "5. Conclusion", "content": "In this work, we discover and solve the target confusion\nproblem, which arises when multiple object classes coex-\nist in the query image, resulting in inaccurate identifica-\ntion of the target objects. To settle this problem, we intro-\nduce a novel perspective, Mutually-Aware Feature Learn-\ning (MAFEA), which encourages interaction between query\nand exemplar features from the outset. By incorporating\nmutual relation modeling into the feature extractor, our\nmodel produces highly target-aware query features, facili-\ntating the distinction between target and non-target objects.\nAdditionally, the background token with Target-Background\nDiscriminative (TBD) loss enables the model to effectively\ndifferentiate target objects from background features. We\ndemonstrate the robustness of our method against the target\nconfusion problem through evaluations in a complex multi-\nclass scenario, such as FSCD-LVIS and FSC-147-Multi.\nMoreover, we validate its effectiveness even in a single-class\nscenario. Experimental results validate the unique merits of\nour proposed method by achieving state-of-the-art perfor-\nmances and demonstrating its effectiveness in addressing the\ntarget confusion problem."}]}