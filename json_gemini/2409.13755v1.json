{"title": "Entity-Aware Self-Attention and Contextualized GCN for Enhanced Relation Extraction in Long Sentences", "authors": ["Xin Wang", "Xinyi Bai"], "abstract": "Relation extraction as an important natural Language processing (NLP) task is to identify relations between named entities in text. Recently, graph convolutional networks over dependency trees have been widely used to capture syntactic features and achieved attractive performance. However, most existing dependency-based approaches ignore the positive influence of the words outside the dependency trees, sometimes conveying rich and useful information on relation extraction. In this paper, we propose a novel model, Entity-aware Self-attention Contextualized GCN (ESC-GCN), which efficiently incorporates syntactic structure of input sentences and semantic context of sequences. To be specific, relative position self-attention obtains the overall semantic pairwise correlation related to word position, and contextualized graph convolutional networks capture rich intra-sentence dependencies between words by adequately pruning operations. Furthermore, entity-aware attention layer dynamically selects which token is more decisive to make final relation prediction. In this way, our proposed model not only reduces the noisy impact from dependency trees, but also obtains easily-ignored entity-related semantic representation. Extensive experiments on various tasks demonstrate that our model achieves encouraging performance as compared to existing dependency-based and sequence-based models. Specially, our model excels in extracting relations between entities of long sentences.", "sections": [{"title": "I. INTRODUCTION", "content": "There has been major interest in relation extraction, which aims to assign a relation among a pair of entity mentions from plain text. Relation extraction is the basis for answering knowledge queries [1], [2], building knowledge base [3], [4], and also forming an important supporting technology for information extraction [5], [6]. Recent models for relation extraction are primarily built on deep neural networks, which encode the entire sentence to obtain relation representations and have made great progress [7], [8].\nFrom the example given in Fig. 1 for cross-sentence n- ary task, there is a relation \"sensitivity\" between the three entities within the two sentences, which expresses that \u201ctumors with L858E mutation in EGFR gene respond to gefitinib treat- ment\". The edges connecting different tokens identify their dependency labels. Prior efforts show that models utilizing dependency parsing of input sentences (i.e., dependency-based models) are very effective in relation extraction, because their superiority lies in drawing direct connections between distant syntactically correlated words. Xu et al. [9] first applied LSTM on the shortest dependency path (SDP) between the entities in the full tree. Miwa et al. [10] reduced the full tree into the subtree below the lowest common ancestor (LCA) of the entities. Both patterns prune the dependency trees between the entities to cover relevant information and discard noises. However, if only the dependency structure (i.e., SDP, LCA) shown in Fig. 1 is considered, the tokens \"partial response\" will be neglected, yet, they contribute to the gold relation greatly. Therefore, it is very essential to obtain the interactions of all words, not just the dependency trees of entities. To address this issue, we use a relative position self-attention mechanism, which allows each token to take its left and right context into account while calculating pairwise interaction scores with other tokens.\nRecently, combining entity position features with neural networks has greatly improved the performance of relation extraction. Zhang et al. [11] combined sequence LSTM model with a position-attention mechanism and got a competitive results. Lee et al. [12] proposed a novel entity-aware BiLSTM with Latent Entity Typing (LET), and obtained state-of-the-art performance on SemEval dataset. From their experiments, we conclude that the words that determine the relation frequently related to the target entities. However, these methods only utilize the semantic representations and position features, ignoring the dependency syntax of the words. Unlike previous efforts, which focus on either dependency parsing or the se- mantic features, we synthesize syntactic dependency structure and entity-related sequential semantic context into an attention mechanism, both of which are crucial for relation extraction.\nIn this paper, we first utilize relative position self-attention mechanism to encode semantic interactions of the sequential sentence, which ignores the distance between words to cal- culate the compatibility scores and relative position scores. Then contextualized graph convolution module encodes the dependency trees between the entities to capture contextual long-range dependencies of words. Afterwards, entity-aware attention mechanism combines these two modules to get final relation representations. The contributions of our work can be summarized as follows:\n1) We propose a ESC-GCN model to learn relation repre- sentations. Compared with previous methods, our method not only utilizes semantic features but also considers dependency features.\n2) Our proposed model prove to be very competitive on the"}, {"title": "II. RELATED WORK", "content": "Recently, deep neural models have shown superior perfor- mances in the field of NLP. Compared with traditional hand-crafted models, deep neural models can automatically learn latent features and greatly improve performances.\nRelation extraction has been intensively studied in a long history, and most existing neural relation extraction mod- els can be divided into two categories: sequence-based and dependency-based. Zeng et al. [7] first applied CNN with manual features to encode relations. Wang et al. [13] pro- posed attention-based CNN, which utilizes word-level atten- tion to better determine which parts of the sentence are more influential. Variants of Convolutional Neural Networks (CNNs) methods have been proposed, including CNN-PE [14], [15], CR-CNN [16] and Attention-CNN [17]. Besides CNN-based architecture, the RNN-based models are another effective approaches. Zhang et al. [8] first applied RNN to relation extraction and got competitive performance. Zhang et al. [18] employed BiLSTM to learn long-term dependencies between entity pairs. Moreover, variants RNNs methods such as Attention-LSTM [19], [20] and Entity-aware LSTM [12] have been proposed. However, these models only considered the sequential representations of sentences and ignored the syntactic structure. Actually, these two features complement each other. By combining C-GCN [21], [22] and PA-LSTM [11], Zhang et al. [21] obtained better results on TACRED dataset.\nCompared with the sequence-based models, incorporating dependency syntax into neural models has proven to be more successful, which captures non-local syntactic relations that are only implicit in the surface from alone [23]\u2013[25]. Xu et al. [9] proposed SDP-LSTM that leverages the shortest dependency path between two entities. Miwa et al. [10] reduced the full tree into the subtree below the lowest common ancestor, which combined a Tree-LSTM [26] and BiLSTMs on tree structures to model jointly entity and relation extraction. Peng et al. [27] proposed a graph-structured LSTM for cross- sentence n-ary relation extraction, which applied two directed acyclic graphs (DAGs) LSTM to capture inter-dependencies in multiple sentences. Song et al. [28] proposed a graph- state LSTM model which employed a parallel state to model each word, enriching state scores via message passing. Zhang et al. [21] presented C-GCN for relation extraction, which uses graph convolution and a path-centric pruning strategy to selectively include relative information. Vashishth et al. [29], [30] utilized GCN to incorporate syntactic and semantic information of sentences to learn its word embedding. Sahu et al. [31] proposed Self-determined GCN (S-GCN) which determines a weighted graph using a self-attention mechanism. Vaswani et al. [32] proposed an attention-based model called Transformer. It's a mainstream that combined attention mechanism with CNNs [7], [33], [34] or RNNs [8], [35] in the past few years. Recently, attention mechanism has been proven to capture helpful information for relation extraction [17], [36]. Tran et al. [37] utilized Segment-Level Attention-based CNN and Dependency-based RNN for relation classification, which got a comparable result on SemEval dataset. Verga et al. [38] used self-attention to encode long contexts spanning multiple sentences for biological relation extraction. Zhang et al. [11] employed a position-attention mechanism over LSTM outputs for improving relation extraction. Bilan et al. [39] substituted the LSTM layer with the self-attention encoder for relation extraction. Yu et al. [40] proposed a novel segment attention layer for relation extraction, and achieved competitive results on TACRED dataset."}, {"title": "III. THE PROPOSED MODEL", "content": "Following the existing studies [21], we define relation extraction as a multi-class classification problem, which can be formalized as follows: Let $S = [w_1,w_2,...,w_n]$ denote a sentence, where $w_i$ is the i-th token. A subject entity and an object entity are identified: $W_s = [w_{s1}, w_{s2}, ..., w_{sn}]$ and $W_o = [w_{o1}, w_{o2}, ..., w_{on}]$. Given $S$, $W_s$ and $W_o$, the goal of relation extraction is to predict a relation $r \\in R$ or \u201cno relation\u201d."}, {"title": "A. Preliminary", "content": "GCNs are neural networks that operate directly on graph structures [41], which are an adaptation of convolutional networks. Given a graph with n nodes, we generate the graph with an n\u00d7n adjacency matrix A where $A_{ij} = 1$ if there is an edge going from node i to node j, otherwise $A_{ij} = 0$. Similar to Marcheggiani et al. [42], we extend GCNs for encoding dependency trees by incorporating opposite of edges into the model. Each GCN layer takes the node embedding from the previous layer $g_i^{(l-1)}$ and the adjacency matrix $A_{ij}$ as input, and outputs updated node representation for node i at the l-th layer. Mathematically, the induced representation $g_i^{(l)}$ can be defined as :\n$g_i^{(l)} = \\rho \\left( \\sum_{j=1}^{n} A_{ij}W^{(l)}g_j^{(l-1)} + b^{(l)} \\right)$"}, {"title": "B. Input Representation", "content": "Distributed representation of words in a vector space is helpful to achieve better performance in NLP tasks. Accord- ingly, we embed both semantic information and positional information of words into their input embeddings, respectively.\nIn our model, the input representation module first trans- forms each input token $w_i$ into a comprehensive embedding vector $x_i$ by concatenating its word embedding $word_i$, en- tity type embedding $ner_i$ and part-of-speech (POS) tagging embedding $pos_i$. Embedding vector $x_i$ formally defined as follow:\n$x_i = [word_i; ner_i; pos_i]$.\nIt has been proved that the words close to the target enti- ties are usually more informative in determining the relation between entities [39], we modify the position representation originally proposed by [11], and convert it into binary position encoding. Consequently, we define a binary-position sequence $[p_1,...,p_n]$ that relative to the subject entity:\n$p_i =\\begin{cases}\n-[log_2(s_1-i)] - 1, & i < s_1\\\\\n0, & s_1 \\le i \\le s_2\\\\\n[log_2(i-s_2)] + 1, & i > s_2\n\\end{cases}$,\nwhere $s_1, s_2$ represent the start index and end index of the subject entity respectively, $p_i \\in Z$ can be viewed as the relative distance of token $x_i$ to the subject entity.\nSimilarly, we also obtain a position sequence $[p_1,\u2026\u2026\u2026, p_n]$ relative to the object entities. By concatenating the position embeddings, we get a unified position embedding $p_i \\in R^{d_p\\times2}$, and $d_p$ indicates the dimension of position embedding."}, {"title": "C. Relative Position Self-attention Mechanism", "content": "The self-attention mechanism proposed by Vaswani et al. [32], which allows words to take its context into account. Following Bilan et al. [11], we apply several modifications to the original self-attention layer. Firstly, we simplify the residual connection that directly goes from the self-attention block to the normalization layer. Then we substitute the layer normalization with batch normalization. In our experiments, we have observed improvements with these setting, and a more detailed overview of the results can be seen in the subsection V-A.\nTraditionally, a self-attention layer takes a word repre- sentation at position i as the query (a matrix Q holds the queries for position i) and computes a compatibility score with representations at all other positions (represented by a matrix V). The score w.r.t. position i is reformed to an attention distribution over the entire sentence, which is used as a weighted average of representations E at all positions. Shaw et al. [43] exploited the relative positional encoding to improve the performance of self-attention. Similarly, we mod- ify our self-attention layer, together with a position attention that takes into account positions of the query and the object in the sentence. For one attention head a, our self-attention head $sa$ obtain its representation by summing pairwise interaction scores and relative position scores together, formally defined as follows:\n$sa = softmax(\\frac{QK^T + RM^T}{\\sqrt{d_w}})V$,\nwhere $Q = W_q^{(a)}e_i$, $K = W_k^{(a)}E$, $V = W_v^{(a)}E$ wherein $W_q^{(a)}$, $W_k^{(a)}$, $W_v^{(a)}$ are linear transformations, which map the input representation into lower-dimensional space. M is relative position embedding matrix:\n$M_i = [m_{1-i},..., m_{-1}, m_0, m_1, ..., m_{n-i}]$,\nwhere n is the length of the input sentence and the matrix $M_i$ is the relative position vectors, $m_0$ is at position i and other $m_j$ are ordered relative to position i.\nSimilar to Q, we obtain a query vector $R = W_r^{(a)}e_i$ to obtain position relevance. The position attention scores result from the interaction of R with the relative position vectors in $M_i$. As show in Fig. 3, we associate position attention scores with the pairwise interaction scores, which incorporates position features into overall dependencies of sequence."}, {"title": "D. Contextualized GCN Layer", "content": "In this section, we construct a contextualized GCN model which takes the output from subsection III-B as input $h^{(0)}$ of this module. A BiLSTM layer is adopted to acquire the context of sentence for each word $w_i$. For explicitly, we denote the operation of LSTM unit as LSTM(xi). The contextualized word representations is obtained as follows:\n$h_i = [LSTM (x_i); LSTM (x_i)], i \\in [1, n]$,\nwhere $h_i \\in R^{2\\times d_h}$ and $d_h$ indicates the dimension of LSTM hidden state. Then we obtain hidden representations of all tokens $h^{(L_1)}$, which represents the input $g^{(0)}$ for graph con- volution, where $L_1$ represents the layer number of RNN.\nThe GCN model [41] has been popularly used for learning graph representation. Dependency syntax has been recognized as a crucial source of features for relation extraction [23], and most of the information involved relation within the subtree rooted at the LCA of the entities. Miwa et al. [10] has shown that discarding those noises outside of LCA can help relation extraction. Before applying graph convolution operation, we do some tricks on the dependency parsing tree, which keeps the original dependency path in the LCA tree and incorporates 1-hop dependencies away from the subtree. Accordingly, we cover the most relevant content and remove irrelevant noise as much as possible.\nOriginally applying the graph convolution in (1) could bring about node representations with obviously different scale [21], since the degree of tokens varies a lot. Furthermore, Equation (1) is never carried the nodes themselves. To cope with the above limitations, we resolve these issues by normalizing the activations in the graph convolution, and add self-loop into each node in adjacency matrix A, modified graph convolution operation as follows:\n$g_i^{(l)} = \\sigma \\left( \\sum_{j=1}^n \\frac{\\overline{A}_{ij} W^{(l)} g_j^{(l-1)}}{d_i}+b^{(l)} \\right)$,\nwhere $\\overline{A} = A + I$, I is the n \u00d7 n identity matrix, and $d_i = \\sum_{j=1}^n \\overline{A}_{ij}$ is the degree of token i. This operation updates the representation of node i by aggregating its neighborhood via a convolution kernel. After L2 iterations, we obtain the hidden outputs of graph convolution $g^{(L_2)}$, where L2 represents the layer number of GCN."}, {"title": "E. ESC-GCN for Relation Extraction", "content": "After applying the L2-layer contextualized GCN model, we obtain hidden representation of each token, which is directly influenced by its neighbors (no more than L2 edges apart in the dependency trees). To make use of graph convolution for relation extraction, we first obtain a sentence representation as follows:\n$I_{sent} = f (g^{(L_2)}) = f (GCN (g^{(0)}))$,"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we evaluate our ESC-GCN model with four datasets on two tasks, namely cross-sentence n-ary relation extraction and sentence-level relation extraction."}, {"title": "V. ANALYSIS", "content": "In order to study the contribution of each component, we conducted an ablation study on the TACRED. Table VI shows the results. Instead of default residual connections described by [32], the optimized residual connection contributes 0.2 points. Similarly, layer normalization contributes 0.4 points improve- ment. The entity-aware module and self-attention module contribute 0.5 and 0.7 points respectively, which illustrates that both layers promote our model to learn better relation repre- sentations. When we remove the feedforward layers and the entity representation, $F_1$ score drops by 0.9 points, showing the necessity of adopting \"multi-channel\u201d strategy. We also notice that the BiLSTM layer is very effective for relation extraction, which drops the performance mostly ($F_1$ relatively drops 2.3 points)."}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose a novel neural model for relation extraction that is based on graph convolutional networks over dependency trees. By incorporating the context of the words related to entities with inter-dependencies of input sentence, our model can capture the long-distance dependency relation between target entities more effectively, especially in long sentences. The experimental results demonstrate that our pro- posed model outperforms most baseline neural sequence-based models and dependency-based models. We further visualize the attention of our model to show how our relative position self-attention layer affects the model. In summary, our model effectively combines syntactic and semantic representations, which significantly improves the performance of relation ex- traction."}, {"title": "SUPPLEMENTAL MATERIAL", "content": "We tune the hyper-parameters according to the validation sets. Following previous work [21], we employ the \"entity mask\" strategy where subject (object similarly) entity with special NER-(SUBJ\u3009 token, which can avoid overfitting and provide entity type information for relation extraction. We use the pre-trained 300-dimensional GloVe [47] vectors as the initialization for word embeddings.3 We randomly initialize all other embeddings (i.e., POS, NER, position embeddings) with 30-dimension vectors. We set LSTM hidden size to 200 in all neural models. We set self-attention hidden size to 130 and the number of heads to 3. We also use hidden size 200 for the GCN layers and the feedforward layers. We employ the ReLU function for all nonlinearities, and apply max-pooling operations in all pooling layers. We use the dependency parsing, POS and NER sequences in the original dataset, which was generated with Stanford CoreNLP [48]. For regularization, we apply dropout with p = 0.5 after the input layer and before the classifier layer. The model is trained using stochastic gradient descent optimizer for 100 epochs and decay rate of 0.9."}]}