{"title": "COLLAP: Contrastive Long-form Language-Audio Pretraining with Musical Temporal Structure Augmentation", "authors": ["Junda Wu", "Warren Li", "Zachary Novack", "Amit Namburi", "Carol Chen", "Julian McAuley"], "abstract": "Modeling temporal characteristics plays a significant role in the representation learning of audio waveform. We propose Contrastive Long-form Language-Audio Pretraining (CoLLAP) to significantly extend the perception window for both the input audio (up to 5 minutes) and the language descriptions (exceeding 250 words), while enabling contrastive learning across modalities and temporal dynamics. Leveraging recent Music-LLMs to generate long-form music captions for full-length songs, augmented with musical temporal structures, we collect 51.3K audio-text pairs derived from the large-scale AudioSet training dataset, where the average audio length reaches 288 seconds. We propose a novel contrastive learning architecture that fuses language representations with structured audio representations by segmenting each song into clips and extracting their embeddings. With an attention mechanism, we capture multimodal temporal correlations, allowing the model to automatically weigh and enhance the final fusion score for improved contrastive alignment. Finally, we develop two variants of the CoLLAP model with different types of backbone language models. Through comprehensive experiments on multiple long-form music-text retrieval datasets, we demonstrate consistent performance improvement in retrieval accuracy compared with baselines. We also show the pretrained CoLLAP models can be transferred to various music information retrieval tasks, with heterogeneous long-form multimodal contexts.", "sections": [{"title": "I. INTRODUCTION", "content": "The ability to effectively model temporal characteristics is essential in the representation learning of audio waveforms, especially for complex and full-length music tracks. Music information retrieval works [1], [2] have studied approaches to extract musical temporal and structural information, which can be further used to augment models' music understanding abilities [3]. The recent contrastive learning approaches [4]\u2013[6] enable to extract such information as latent audio representations, which are trained to distinguish between matched text-audio pairs and other mismatched pairs by capturing distinctive features in the audio data (illustrated in Figure 1a). However, such methods have focused on relatively short segments, limiting the model's ability to handle longer, more nuanced sequences.\nTo address these challenges, we introduce Contrastive Long-form Language-audio Pretraining (CoLLAP), which extends the perception window to handle both long-form audio inputs and detailed language-descriptions. We illustrate the comparison between the conventional CLAP model and our proposed COLLAP model in Figure 1. The COLLAP model uses a feature extractor to segment music tracks into frames and encode each by a kernel function. Then kernel-wise and temporal attention mechanisms are employed to measure global and temporal alignment between audio and text. Finally, the model is optimized with contrastive learning using weighted similarity scores from both kernel-wise and temporal attention. CoLLAP effectively extends the perception window for both the input audio (up to 5 minutes) and the language descriptions (exceeding 250 words), which enables retrieval of full-length music tracks with fine-grained music descriptions.\nTo enable large-scale contrastive pretraining of CoLLAP, we leverage a Music-LLM augmented dataset of 51.3K audio-text pairs and 4,109 hours of audio, derived from the large-scale AudioSet training data, with an average audio length of 288 seconds and an average text length of 256 words. In addition, we develop two variants of COLLAP based on two different backbone language models, Roberta-base [7] and GPT2 [8].\nFinally, we conduct comprehensive experiments on multiple long-form music-text retrieval datasets and observe consistent improvement in retrieval accuracy of CoLLAP compared with baseline models. We also evaluate CoLLAP's transfer learning ability in various music information retrieval tasks that involve heterogeneous long-form multimodal contexts, including speech audio and Wikipedia free-form long-context. In addition, we also observe better generalizability in the CoLLAP-GPT2 variant compared to RoBERTa model backbone due to the GPT2 model's better language modeling ability of long-context. We summarize our contributions as follows:\n\u2022 We propose the Contrastive Long-form Language-audio Pre-training (CoLLAP) model for multimodal fusion and representation learning of long-form audio and language descriptions.\n\u2022 We design a novel fusion mechanism that combines structured audio and language representations, leveraging attention to capture and weigh multimodal temporal correlations for improved contrastive alignment.\n\u2022 We augment a dataset of 4,109 hours of long-form full-length music tracks, paired with musical structural augmented captions generated by Music-LLMs.\n\u2022 Through comprehensive experiments we demonstrate that CoLLAP consistently outperforms baseline models in long-form text-audio retrieval, and show its generalizability across different tasks."}, {"title": "II. COLLAP: MODEL DESIGN AND LEARNING", "content": "We illustrate our CoLLAP model design in Figure 2, where full-length music track waveform is processed with a dual-feature extractor, while textual representations are extracted from musical structure augmented captions. We split music tracks of variable lengths into frames to enable audio temporal attention with texts, which extracts and measures both the global and temporal multimodal alignment scores. With the temporal attention augmented alignment scores, we follow the conventional contrastive learning scheme [4], [6], [9], [10], where the contrastive loss will be propagated back to both the temporal attention and the feature extractors.\nGiven N input audio-text pairs {(Xi, Yi)}i<N, we extract the textual embeddings T\u00bf \u2208 RD, musical embeddings O\u00bf \u2208 RD, and speech embeddings Si \u2208 RD as follows:\nTi = fr(Yi; \u04e9\u0442), Oi = fo(Xi;0o), Si = fs(Xi;0s),\nwhere the model parameters of the text encoder \u04e9\u0442 are initialized from a pre-trained language model (e.g., RoBERTa [7] and GPT-2 [8]), while the music encoder and speech encoder are adapted from BEATS [11] and Whisper [12] models. We fuse the musical and speech embeddings by an audio feature adapter linear layer ha,\nUi = ha ([Oi, Si]), I < N.\nThen, we split the unified audio representation with a length of T into consecutive frames with a kernel function with a kernel size of H and stride step of ST,\n$H=\\frac{\\eta_k}{\\eta_s}*30, S_T=\\frac{\\eta_s}{\\eta_s}*30$\nwhere \u03b7\u03ba is pre-defined to determine how many seconds per frame, and \u03b7\u03c2 determines seconds per stride. Finally, the processed audio representation is unfolded and reshaped to I\u2081 = {I}w,H \u2208 RH\u00d7W\u00d7D\n$I_1 = Unfold(U_i, H, S_T), where W_1 = \\frac{T-H}{S_T} + 1$\\label{1}.\nWith the audio tokenized with fixed-length frames I\u2081 = {I}W,H, we can calculate kernel-wise attention and temporal attention to augment the multimodal alignment estimation."}, {"title": "B. Multimodal and Temporal Attention Augmentation", "content": "Given the audio representation I\u2081 = {I}w\u2081, and the text representation Tj, we calculate their cosine similarity\n$M_{i,j} = \\{(I_1^{v,h})^T T_j\\}_{W_1,H}$,\\label{2}\nin each frame v < Wi and each kernel h < H. To further measure the text's attention on the individual frame and kernel, we calculate the kernel-wise attention A and temporal attention Aj,\n$A_i^{K}(v,h) = \\frac{M_{i,j}(v,h)}{\\sum_{k<H} M_{i,j} (v,k)}$,\\label{3}\n$A_j^{T}(v,h) = \\frac{M_{i,j} (v,h)}{\\sum_{l<W_i} M_{i,j} (l,h)}$,\\label{4}\nwhere Mi,j (v, h) is the corresponding cosine similarity score of the v-th frame and h-th kernel in Mi,j."}, {"title": "C. Temporal Attention Fused Contrastrive Learning", "content": "Then we use the calculated kernel-wise attention A and temporal attention Aj to weigh and sum the original cosine similarity matrix Mi,j. To obtain the global similarity between the text and audio, Mi,j is weighted by the kernel-wise attention AK, with an average pooling,\n$\\overline{r}_{ij}^{K} = \\frac{1}{H*W_i} \\sum_{k<H} \\sum_{l<W_i} M_{ij} (k, l) \\cdot A_i^{K} (k, l)$.\\label{5}\nTo capture the temporal attention-weighted similarity between text and audio, we further derive the similar similarity score,\n$\\overline{r}_{ij}^{T} = \\frac{1}{H*W_i} \\sum_{l<W_i} \\sum_{k<H} M_{ij} (k, l) \\cdot A_j^{T} (k, l)$.\\label{6}\nFinally, we compose the two weighted similarity scores with two scalers YK and \u04af\u0442 for balance. Therefore, each pairwise cosine similarity score ri,j \u2208 RN\u00d7N in the mini-batch is calculated as\n$r_{i,j} = \\gamma_K \\cdot \\overline{r}_{ij}^{K} + \\gamma_T \\cdot \\overline{r}_{ij}^{T}$.\\label{7}\nFollowing [4], [5], [9], we adopt the conventional contrastive loss function to derive the final loss,\n$L = - \\sum_{i<N} log \\frac{e^{r_{i,i}}}{\\sum_{j<N} e^{r_{i,j}}}$ \\label{8}\nwhere the contrastive loss will be propagated back to both the temporal attention and the feature extractors.\nWe collect a large-scale long-form audio waveform dataset derived from the full-length tracks from the training subset of AudioSet [13]. We filter out audio tracks whose lengths are either less than 2 minutes or longer than 5 minutes, accumulating to a total of 51.3K and 4, 109.50 hours of audio tracks with an average length of 288.25 seconds per track. To further pair the full-length audio tracks with long-form and fine-grained captions that comprehensively describe the entire track, we leverage the FUTGA model [3] to generate dense captions, which provides both global caption and temporally-aware structural information. Therefore, the generated dense captions have an average of 256.94 words for each caption."}, {"title": "IV. EXPERIMENTS", "content": "We implement the CoLLAP model using PyTorch 2.2 framework, leveraging pre-trained RoBERTa and GPT-2 models for the text encoder and adapting BEATS and Whisper models for the music and speech encoders, respectively. We collect 51.3K long-form audio-text pairs derived from the original AudioSet-train dataset [13], with an average audio duration of 288 seconds and a text length of 257 words. We initialize RoBERTa or GPT-2 for the text encoder with pre-trained weights. The music and speech encoders are respectively adapted from BEATS and Whisper models and concatenated as the fused audio embedding. The fused textual and audio embedding sizes are set to 512. We fine-tuned the full parameters of both the text encoder and audio embeddings, using an AdamW optimizer with a learning rate of 1e \u2013 4 and weight decay of le \u2013 5. We use a batch size of 50 and enable in-batch contrastive learning loss implemented by a cross-entropy loss function. The contrastive learning process is set for 20 epochs, with a linear learning rate scheduling. The training process leverages 2 NVIDIA A100 GPUs with 40GB of memory.\nWe evaluate the COLLAP model on three text-audio retrieval tasks, where four datasets, SongDescriber [17], MusicCaps [15], AudioSet-Eval [13], and HarmonixSet [18], are used for general long-form text-to-audio retrieval. To test the retrieval accuracy in the speech domain, we evaluate the VCTK dataset [19] for long-context transcript to full speech retrieval. Finally, we further evaluate the model's zero-shot generalizability in free-form music context collected from Wikipedia pages and enable wiki-to-music retrieval.\nWe compare our proposed CoLLAP model with three contrastive learning baselines for the main experiment in Table II: HSTAT (ROBERTa) [6] employs RoBERTa for textual encoding and incorporates the feature fusion mechanism and keyword-to-caption augmentation; Larger CLAP [6] further enhances the model performance on music and speech domains by expanded pre-training; Cacophony [5] enhances by a hierarchical attention mechanism and advanced fusion techniques to dynamically combine multi-scale features from both modalities. For our method, we develop two model variants COLLAP (RoBERTa) and COLLAP (GPT2) using two different language model backbones.\nThe long-form text-audio retrieval experiments are designed to evaluate the effectiveness of the CoLLAP model in aligning extended audio tracks with their corresponding textual descriptions. Retrieval performance is measured using recall at ranks 5, 20, and 100 for both text-to-audio (T2A) and audio-to-text (A2T) retrieval tasks.\nAs presented in Table II, the CoLLAP variants outperform the baseline models across all datasets, particularly on SongDescriber and HarmonixSet. The attention mechanisms in COLLAP enable the model to effectively capture the temporal and multimodal correlations, leading to significant improvements in retrieval accuracy. The RoBERTa-based COLLAP variant demonstrates slightly higher performance, especially in A2T retrieval tasks."}, {"title": "D. Zero-shot Transcript-speech Retrieval", "content": "We also evaluate CoLLAP's zero-shot transfer performance on transcript-speech retrieval tasks using the VCTK dataset. This experiment assesses the model's capability to align spoken content with corresponding transcripts without additional fine-tuning. Table III reports retrieval performance for both T2A and A2T tasks at various recall ranks.\nThe results indicate that the CoLLAP model variants maintain robust retrieval accuracy in this zero-shot setting. The GPT-2 based variant outperforms the RoBERTa-based variant, suggesting that GPT-2's generative capabilities may better handle the variability in spoken language. These findings highlight CoLLAP's potential for applications in speech recognition and audio-text alignment."}, {"title": "E. Zero-shot Wiki-music Retrieval", "content": "Finally, we assess CoLLAP's generalizability in retrieving music-related content from textual descriptions in a zero-shot manner using the Wiki-music dataset. This dataset includes Wikipedia articles paired with audio clips, and the task involves retrieving the correct audio clip given a text query and vice versa. The retrieval performance is detailed in Table IV.\nCOLLAP achieves significant gains over the baseline models in the Wiki-SD and Wiki-MC tasks. The model's attention mechanisms allow it to effectively align long-form text with corresponding audio segments, leading to improved retrieval accuracy. These results suggest that CoLLAP can be effectively transferred to diverse music-related information retrieval tasks, making it a versatile tool for exploring large-scale multimodal datasets."}, {"title": "V. CONCLUSION", "content": "In this paper, we introduce CoLLAP, a novel contrastive learning framework designed for long-form language-audio representation learning. Our model leverages dual-feature extraction and a multimodal attention mechanism to effectively capture both global and temporal alignments between lengthy audio tracks and detailed textual descriptions. Through comprehensive experiments across multiple datasets, including SongDescriber, MusicCaps, AudioSet-Eval, HarmonixSet, and Wiki-music, we demonstrate that COLLAP significantly improves retrieval performance over existing baseline models."}]}