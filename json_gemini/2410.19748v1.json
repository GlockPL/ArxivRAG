{"title": "C2DA: Contrastive and Context-aware Domain Adaptive Semantic Segmentation", "authors": ["Md. Al-Masrur Khan", "Zheng Chen", "Lantao Liu"], "abstract": "Unsupervised domain adaptive semantic segmentation (UDA-SS) aims to train a model on the source domain data (e.g., synthetic) and adapt the model to predict target domain data (e.g. real-world) without accessing target annotation data. Most existing UDA-SS methods only focus on inter-domain knowledge to mitigate the data-shift problem. However, learning the inherent structure of the images and exploring the intrinsic pixel distribution of both domains are ignored; which prevents the UDA-SS methods from producing satisfactory performance like the supervised learning. Moreover, incorporating contextual knowledge is also often overlooked. Considering these issues, in this work, we propose a UDA-SS framework that learns both intra-domain and context-aware knowledge. To learn the intra-domain knowledge, we incorporate contrastive loss in both domains, which pulls pixels of similar classes together and pushes the rest away, facilitating intra-image-pixel-wise correlations. To learn context-aware knowledge, we modify the mixing technique by leveraging contextual dependency among the classes to learn context-aware knowledge. Moreover, we adapt the Mask Image Modeling (MIM) technique to properly use context clues for robust visual recognition, using limited information about the masked images. Comprehensive experiments validate that our proposed method improves the state-of-the-art UDA-SS methods by a margin of 0.51% mIoU and 0.54% mIoU in the adaptation of GTA-V\u2192Cityscapes and Synthia Cityscapes, respectively. We open-source our C\u00b2DA code. Code link: github.com/Masrur02/C-Squared-DA", "sections": [{"title": "1 INTRODUCTION", "content": "Deep segmentation models trained by existing datasets [1-3] are not always sufficient to segment accurately in novel and difficult environments. Most importantly when it comes to the case of different domains, for example, simulation-real, day-night, summer-fall, and so on, it is difficult to boost the model to be generalized in unseen data of other domains because there is a non-trivial domain shift between the trained data and the data to be predicted. Unsupervised Domain Adaptation (UDA) is an effective framework for solving the problem of limited annotated data and the problem of domain shift in semantic segmentation. In UDA, a model is trained to transfer the knowledge in the annotated"}, {"title": "2 RELATED WORKS", "content": "Unsupervised Domain Adaptation: In UDA a deep learning model is trained on annotated source data to predict on label-scarce target domain data. Over the years, UDA has been employed in almost all branches of computer vision including classification [10], segmentation [5], object detection [11] due to its ability to solve domain gaps. Mainstream UDA methods are mainly grouped into two categories: adversarial training [12] and self-training [13]. Adversarial training focuses on learning domain-invariant knowledge through the alignment of features from the source domain with those of the target domain. It mitigates the domain gaps by using entropy minimization [14], correlation alignment [15], etc. Adversarial learning focuses on aligning features between two domains on a global scale rather than aligning features at the class level and leads to a negative transfer problem during semantic segmentation tasks. As a result, the training process becomes unstable and yields suboptimal performance. On the other hand, self-training adapts a student-teacher [5] framework to tackle the data shift problem which is typical in UDA. In this strategy, pseudo-labels are generated by a teacher model trained on the source domain data. Later, the student model is trained on the target images by leveraging those pseudo-labels as ground truth data. However, due to significant differences in data distributions between the two domains, pseudo-labels inherently possess noise. To create reliable pseudo-labels, strategies utilized often include adopting pseudo labels with high confidence [16], learning from the future [17], employing uncertainty-aware pseudo labeling [18], and so on. Other applied strategies to increase the robustness of UDA methods are utilizing consistency regularization [19], domain-mixup [20], multi-resolution inputs [13], etc.\nContrastive Learning: Contrastive learning is one of the notable methods for learning discriminative feature representations. It learns the inherent structure of the images by contrasting positive data pairs against the negative"}, {"title": "3 Methodology", "content": "To explain the overall method of our work, we first provide preliminary knowledge of the UDA methods in Sect. 3.1. Then we provide the general structure of our proposed model in Sect. 3.2. In Sect. 3.5, we discuss the technique of generating masked images and learning from context clues. In Sect. 3.3, we discuss the technique for obtaining intra-domain knowledge. Finally, in Sect. 3.4 we describe the context-aware mixing scheme for reducing the domain shift."}, {"title": "3.1 Preliminaries of UDA-SS", "content": "We consider a source domain S and a target domain T in space $x\\times y$, where x, y denote the input space and label space, respectively. In the source domain, we have $N_s$ images with labels $(x_s=\\{x_i\\}_{i=1}^{N_s}, y_s=\\{y_i\\}_{i=1}^{N_s})$ that belongs to C categories, while for the target domain, we only have access to $N_t$ images $x_T=\\{x_j\\}_{j=1}^{N_t}$. A neural network consisting of a feature extractor $g_\\theta$ and a segmentation head $h_{\\phi}$ is used as the adaptation model. The model is first trained on the source domain data $x_s$ with labels $y_s$. Therefore segmentation loss function for the source domain becomes"}, {"title": "3.2 Model Framework", "content": "We build our proposed UDA framework by using the teacher-student framework (see Fig. 2). The architecture of the teacher model is the same as the student model, and both utilize a pre-trained SegFormer transformer. Each iteration consists of four stages. At the beginning of each iteration, we update the teacher model's weight by using an Exponential Moving Average (EMA), to ensure that the teacher model remains in sync with the student model. In the first stage, we train the student model with the source data and source label. After that, in the second stage, we use the teacher model to predict the target domain images and generate pseudo-labels without backpropagation. In the third stage, we use a cross-domain mix module (described in Sect. 3.4) to generate a new image pair (xmix, ymix) from both domains and train the student model again. Hence, Eq. (2) gets updated as:\n$L_{CE} = -E[p_{mix}^i log h_{cls} (g_\\theta(x_{mix}))]$,\nwhere $p_{mix}^i$ is scalar value from the probability vector of the mixed label $y_{mix}$, $p_{mix}^i$ is already one-hot encoded as we use copy-paste based mixing strategy. Finally, in the fourth stage, we mask out random patches from the target domain images (described in Sect. 3.5) and generate the masked images $x^{ma} =\\{x_j^{ma}\\}_{j=1}^{N_t}$. Then we train the student model again on the masked images supervised by the pseudo-labels. Based on the training of mixing and masking, the adaptation objective of our model is:\n$min_{\\theta,\\phi}L_{CE}(\\theta,\\phi) + L_{TE}(\\theta,\\bar{\\phi}) + L_{E}(\\theta, \\phi)$,\nwhere $L_E$ is the masked loss."}, {"title": "3.3 Learning Inherent Structure", "content": "The adopted segmentation losses do not consider learning the inherent context within the images, which is important for local-focused segmentation tasks. So, to learn the intra-domain knowledge, we opt to utilize pixel-wise contrastive learning. Specifically, along with the classification head $h_{cls}$, we use a projection head $h_{proj}$ that generates an embedding space $e_s = h_{proj}g_{\\theta}(x)$ of the pixels. Contrastive learning facilitates learning the correlation between the labeled pixels by pulling the positive pairs of pixels together and pushing the negative pairs of pixels away. Considering the pixels of the same class C as positive pairs and the other pixels in x as negative pairs, contrastive loss $L_{pix}$ can be derived as\n$L_{pix} = \\frac{\\sum log \\frac{d(e_{s_i}, e_{s_j})}{\\sum_{k=1}^{N_{pix}}d(e_{s_i}, e_{s_k})}}{C(i)=C(j)}$,\nwhere $N_{pix}$ is the total number of pixels, $e_{s_i}$ is the feature map of $i^{th}$ pixel in the embedding space, and d denotes the similarity between the pixel features which can be measured using metrics like Euclidean distance or cosine similarity. In particular, we utilize the exponential form of cosine similarity $d(e_{s_i}, e_{s_j}) = exp(s(e_{s_i}, e_{s_j})/\\tau)$, where s represents the cosine similarity between two-pixel features $e_{s_i}$ and $e_{s_j}$, and $\\tau$ is the temperature parameter. The temperature parameter $\\tau$ modulates the distribution sharpness of similarities. Effective"}, {"title": "3.4 Context-aware Mixing", "content": "Exploiting contextual knowledge is another important concept for mitigating the domain shift in UDA as both the source domain and target domain share similar semantic contexts. Domain shift refers to the discrepancy between the data distributions of the source and target domains, which can hinder model performance when applied to the target domain. By leveraging these shared semantic contexts, we create more meaningful training examples that help the model generalize better across domains. We use mixed images to calculate the target domain loss instead of the pseudo-labels as the mixture efficiently guides the model to learn from the supervision signals of both domains. According to ClassMix [9], we first randomly choose and copy 50% of the classes from the source ground truth $y_s$ and then we paste them over the pseudo-label $\\tilde{y}_T$ to generate mixed label $y_{mix}$. Similarly, we also paste the same class areas of $x_s$ over the $x_T$ to generate mixed image $x_{mix}$. To be specific, we generate a mask, M, by selecting random classes, and then apply this mask to the images from both domains, blending them to generate the mixed images. Formally,\n$x_{mix} = M \\otimes x_S + (1 - M) \\otimes x_T\\\\\ny_{mix} = M \\otimes y_S + (1 - M) \\otimes \\tilde{y}_T$.\nHowever, in this way, the cross-domain mixture module overlooks the shared"}, {"title": "3.5 Masking Module", "content": "A masking module withholds local information from target images, encouraging the learning of context relations for robust recognition of classes with similar appearances by randomly masking out patches of target domain images. For masking out the random patches from target domain images, we generate a random mask M from a uniform distribution\n$M_{pa+1:(p+1)a, qb+1:(q+1)a} = [u > t] with u ~ R(0,1)$,\nwhere the superscript of M indicates the specific region of the image from rows pa +1 to (p+1)a and columns qb+1 to (q+1)b. Here, a denotes the patch size, and t represents the mask ratio. The indices p and q range from 0 to W - 1, where W is the width of the image. The Height H is not shown explicitly here because the mask is applied in a symmetric manner across both the width and height of the image. Later we perform element-wise multiplication between the M and target image $x_T$. Specifically,\n$x^{ma} = M x_T$.\nThe student model is then retrained on masked images using pseudo-labels $\\tilde{y}$ to predict masked target images $\\hat{y}^{ma}$. In this way, the model can only access limited information from the unmasked regions of the target images, making the prediction more difficult. Hence, the model is forced to learn from the remaining context clues to reconstruct the pseudo-label $\\tilde{y}$.\n$\\hat{Y}^{ma} = h_{cls} (g_\\theta(x^{ma}))$.\nAs the pseudo-labels guide the model to generate masked target prediction, the masked loss $L^{TE}$ can be formulated, as:\n$L^{TE} = -E[p^i log h_{cls} (g_\\theta(x^{ma}))]$."}, {"title": "4 Experiments", "content": "Evaluation Setup\nDatasets: We have tested our model in five datasets. (1) GTA-V is a simulation dataset collected in the city environment. The dataset consists of 24,966 synthetic images with a resolution of 1914\u00d71052. The dataset has annotated labels"}, {"title": "4.2 Comparison", "content": "We compare our proposed UDA framework with the baseline method MIC in both quantitative and qualitative manner. Besides the baseline, we perform a quantitative comparison with other SOTA methods as well. First, we show the quantitative comparison of GTA V Cityscapes adaptation in Table 1. The"}, {"title": "4.3 Ablation Studies", "content": "Effect of Each Module In the proposed UDA framework, we adopt three different modules, i.e., a Prior-Guided ClassMix module, a contrastive learning"}, {"title": "4.4 Navigation Missions", "content": "We combine our proposed C2DA model (trained with RUGD\u2192MESH setup) with POVNav planner [46] to show the effectiveness of our model in real-world deployment. We examine the behavior of our navigation system in two different forest scenarios (see Fig. 6), where the first scenario consists of grass and trees and the second scenario consists of mulch and trees."}, {"title": "5 CONCLUSIONS", "content": "We present a model named C2DA to improve unsupervised domain adaptive semantic segmentation. Our approach is a self-training framework that explores both the inherent structures of the images and the contextual clues of the target domain images. To achieve this, we incorporate contrastive loss along with traditional cross-entropy loss. Furthermore, we modify the ClassMix technique by leveraging the contextual dependency of the classes and applying the masking technique to the target domain images to ensure context-aware learning. Evaluation of benchmark datasets demonstrates that our model outperforms the SOTA by a slight margin. Moreover, we deployed our framework in a robotic vehicle to navigate in unstructured environments."}]}