{"title": "CYBERHOST: TAMING AUDIO-DRIVEN AVATAR DIFFUSION MODEL WITH REGION CODEBOOK ATTENTION", "authors": ["Gaojie Lin", "Jianwen Jiang", "Chao Liang", "Tianyun Zhong", "Jiaqi Yang", "Yanbo Zheng"], "abstract": "Diffusion-based video generation technology has advanced significantly, catalyzing\na proliferation of research in human animation. However, the majority of these\nstudies are confined to same-modality driving settings, with cross-modality human\nbody animation remaining relatively underexplored. In this paper, we introduce\nCyberHost, an end-to-end audio-driven human animation framework that ensures\nhand integrity, identity consistency, and natural motion. The key design of Cyber-\nHost is the Region Codebook Attention mechanism, which improves the generation\nquality of facial and hand animations by integrating fine-grained local features with\nlearned motion pattern priors. Furthermore, we have developed a suite of human-\nprior-guided training strategies, including body movement map, hand clarity score,\npose-aligned reference feature, and local enhancement supervision, to improve\nsynthesis results. To our knowledge, CyberHost is the first end-to-end audio-driven\nhuman diffusion model capable of facilitating zero-shot video generation within\nthe scope of human body. Extensive experiments demonstrate that CyberHost\nsurpasses previous works in both quantitative and qualitative aspects.", "sections": [{"title": "1 INTRODUCTION", "content": "Human animation aims to generate realistic and natural human videos from a single image and control\nsignals such as audio, text, and pose sequences. In audio-driven settings, previous works (Prajwal\net al., 2020; Yin et al., 2022; Wang et al., 2021; Ma et al., 2023; Zhang et al., 2023; Chen et al.,\n2024; Xu et al., 2024c;b; Tian et al., 2024; Wang et al., 2024a) have primarily focused on generating\nportrait videos from various modalities, often overlooking the challenges associated with animating\nthe human body below the shoulders. Recently, advancements in diffusion models have led some\nstudies (Karras et al., 2023; Wang et al., 2024b; Hu et al., 2023; Zhang et al., 2024; Xu et al., 2023;\nHuang et al., 2024; Corona et al., 2024) to explore their potential for enhancing full-body human\nvideo generation. However, these diffusion models are predominantly tailored for video-driven\nsettings and do not seamlessly translate to audio-driven scenarios. In the realm of portrait animation,\nworks like EMO (Tian et al., 2024) have demonstrated that an end-to-end audio-driven diffusion\nmodel can generate highly expressive results, yet this approach remains unexplored for full-body\nanimation. This paper aims to address this gap.\nCompared to portrait, the challenge of audio-driven body animation primarily lies in two aspects: (1)\nCritical human body parts such as the face and hands occupy only a small portion of the frame, yet\nthey carry the majority of the identity information and semantic expression. Unfortunately, neural\nnetworks often fail to spontaneously prioritize learning in these key regions, making them more prone\nto artifacts."}, {"title": "2 RELATED WORK", "content": "Video Generation. Benefiting from the advancements in diffusion models, video generation has made\nsignificant progress. Some works (Singer et al., 2022; Blattmann et al., 2023a; Zhou et al., 2022; He\net al., 2022; Wang et al., 2023a) have extended the 2D U-Net pretrained on image-to-text task to 3D,\nachieving temporal video generation. AnimateDiff (Guo et al., 2023) trained a pluggable temporal\nmodule on large-scale video data, which can be integrated into other text-to-image diffusion models\nfined on specific datasets to achieve temporally smooth video generation. For controllability,\nVideoComposer (Wang et al., 2024c) trained a composer fusion encoder used to combine flexible\nconditions. Compared with UNet-based methods, DiT-based approaches have shifted the model\narchitecture from convolutional networks to transformers. Some works, such as EasyAnimate (Xu\net al., 2024a), CogVideX (Yang et al., 2024), and Sora (Tim et al., 2024), have expanded the 2D\nDiT framework to 3D for video generation by incorporating a specialized motion module block.\nAdditionally, to efficiently model video data, some works have designed 3D Variational Autoencoders\n(VAEs) (Kingma & Welling, 2013) to compress videos along both spatial and temporal dimensions.\nTo accelerate video generation, some methods (Podell et al., 2023; Ren et al., 2024) have proposed\nlightweight distillation schemes, achieving real-time performance in specific domains.\nBody Animation. Existing body animation approaches mainly (Hu et al., 2023; Wang et al., 2024b;\nXu et al., 2023; Karras et al., 2023; Zhou et al., 2022) focus on video-driven settings, where the\ndriving signal is a video sequence. DreamPose (Karras et al., 2023) uses DensePose (G\u00fcler et al.,\n2018) as a control signal and trains a diffusion model to perform pose transfer for any given image,\nthereby generating human video frames sequentially. MagicAnimate (Xu et al., 2023) extends a 2D\nU-Net to 3D and fine-tunes it on human body data, thereby enhancing the temporal smoothness of\nhuman video generation. AnimateAnyone (Hu et al., 2023) uses skeleton maps as control signals\nfor its diffusion model and employs a dual U-Net architecture to maintain consistency between the\ngenerated video and the reference images. Some speech-driven body animation works, GAN-based\nor Diffusion-based (Liao et al., 2020; Ginosar et al., 2019; Wang et al., 2023b; Corona et al., 2024)\ndo exist, but these methods are typically two-stage processes. Speech2Gesture (Ginosar et al., 2019)\nfirst predicts body gesture sequence and then utilizes a pre-trained GAN to render it to final video.\nVlogger (Corona et al., 2024) proposed two diffusion models for human-to-3d-motion and motion-to-\n2d-video. Two-stage methods require explicit representations as intermediate variables. While the\nuse of intermediate representations can reduce the complexity of training, these representations are\noften not detailed enough, and the introduced errors lower the performance ceiling. Unfortunately,\nend-to-end approaches have not been widely explored so far."}, {"title": "3 METHOD", "content": "This section begins by introducing the fundamentals of diffusion models and outlining the overall\nstructure of our proposed CyberHost framework in Section 3.1. Next, in Section 3.2, we detail the\nkey designs of Region Code Attention, and explain how it is applied to the hand and face regions.\nFollowing this, in Section 3.3, we present our proposed training strategies aimed at enhancing the\nquality of generated videos in half-body conversational scenes."}, {"title": "3.1 PRELIMINARY", "content": "We develop our algorithm based on the Latent Diffusion Model (LDM) (Blattmann et al., 2023b),\nwhich utilizes a Variational Autoencoder (VAE) Encoder (Kingma & Welling, 2013) E to transform\nthe image I from pixel space into a more compact latent space, represented as $z_0 = E(I)$. This\ntransformation significantly reduces the computational load. During training, random noise is"}, {"title": "3.2 REGION CODEBOOK ATTENTION", "content": "As shown in Figure 3, our proposed Region Codebook Attention employs a spatio-temporal memory\nbank to learn motion codebook and injects identity descriptors extracted from cropped local images.\nThe former aims to learn identity-agnostic features, while the latter focuses on extracting identity-\nspecific features. This design can be utilized to enhance synthesis results across any region of the\nhuman body. In subsequent sections, we specifically apply it to the face and hands, two areas that\npresent significant challenges, and have confirmed its effectiveness.\nMotion Codebook. While the popular dual U-Net architecture effectively maintains overall visual\nconsistency between the generated video and reference images, it struggles with generating fine-\ngrained texture details and complex motion patterns in local areas like the face and hands. This\nchallenge is further exacerbated in the task of audio-driven human body animation due to the absence\nof explicit control signals. To address this, we introduce a spatio-temporal memory bank to learn\nshared local structural priors, including common texture features, topological structures, and motion\npatterns. We refer to this learned spatio-temporal memory bank as motion codebook and leverage\nits learned structural priors to prevent local motion degradation. The motion codebook is composed\nof two sets of learnable basis vectors: $C_{spa} \\in R^{1\\times n \\times d}$ for spatial features and $C_{temp} \\in R^{1\\times m \\times d}$\nfor temporal features, where n and m denote the number of basis vectors and c denotes the channel\ndimension. We consider the combination of $C_{spa}$ and $C_{temp}$ as a pseudo 3D memory bank, endowing\nit with the capability to learn spatio-temporal features jointly. This capability facilitates the modeling\nof 3D characteristics such as hand motion. Furthermore, we constrain the basis vectors of the\nmemory bank to be mutually orthogonal to maximize the learning capacity of the motion codebook.\nSpecifically, the Gram-Schmidt process is applied to these vectors during each forward pass.\nThe regional motion codebook is integrated into the U-Net through a spatio-temporal cross-attention\nas shown in Figure 3. Given the backbone feature $F_{imet}$ from U-Net, we apply cross attention with\n$C_{spa}$ in the spatial dimension and with $C_{temp}$ in the temporal dimension. The final output $F_{motion}$ is\nformulated as the sum of two attentions' result,\n$F_{motion} = Attn(F_{imet}, C_{spa}, C_{spa}) + Attn(F_{imet}, C_{temp}, C_{temp})$\n$= softmax(\\frac{QK^T}{\\sqrt{d}}) \\cdot V_{spa} + softmax(\\frac{QK^T}{\\sqrt{d}}) \\cdot V_{temp}$\nwhere Attn(*, *, *) denotes the cross attention, Q, K and V are the query, key, and value, respectively,\nprojected from the input. We aim for $F_{motion}$ to fully utilize the spatio-temporal motion priors of the\nlocal region learned within the 3D memory bank, refining and guiding the U-Net features through\nresidual addition. To effectively focus the memory bank on feature learning for the target local region\nwhile filtering out gradient information from unrelated areas, we require a regional mask to weight\nthe residual addition process. To achieve this, and to avoid introducing additional regional mask\nas input, we employ auxiliary convolutional layers to directly predict a regional attention mask $M_r$\nusing U-Net feature $F_{imet}^{unet}$\nIdentity Descriptor. It is worth noting that the process of learning motion codebook is identity-\nagnostic. It relies on the statistical analysis of regional common structure and motion patterns\nwithin dataset. However, identity-specific features such as hand size, skin color or textures may be\noverlooked. To addressing this, we employ a Regional Image Encoder R to extract identity-aware\nregional features from the cropped region image $I_r$. The extracted feature is referred to as the identity\ndescriptor. For clarity, we illustrate this process in the left bottom of Figure3 using the hand as\nan example. Combining the identity-independent motion codebook and the identity descriptor, the\nmathematical formulation of the overall region codebook attention can be expressed as follows:\n$F_{id} = Attn(F_{imet}^{unet}, R(I_r), R(I_r))$\n$F_{out}^{unet} = (F_{motion} + F_{id}) * M_r + F_{imet}^{unet}$\nApplication to Hand and Facial Region. Note that both hand and facial features can be divided\ninto identity-independent common structural features and identity-dependent appearance features.\nTherefore, the design principle of region codebook attention ensures its applicability to feature\nmodeling for both hand and facial regions. In terms of implementation details, we use a structure\nsimilar to the Pose Encoder but with deeper blocks for the Hand Encoder to enhance its feature"}, {"title": "3.3 HUMAN-PRIOR-GUIDED TRAINING STRATEGY", "content": "The performance of diffusion models is closely tied to the quality of the training data. Recent Text-to-\nVideo studies (Podell et al., 2023; Blattmann et al., 2023a) demonstrate that designing condition inputs\nsuch as resolution and cropping parameters can enhance the model's robustness to varied data and\nimprove the output controllability. Inspired by them, we design the Body Movement Map and Hand\nClarity Score conditions to decouple hard cases in the dataset while also reducing the uncertainty\ncaused by the weak correlation between audio and body motion. Additionally, we designed the\nPose-Aligned Reference Feature and Local Enhancement Supervision to guide the model in fully\nconsidering the skeletal topology information during the video generation process.\nBody Movement Map. Frequent body movements, including translations and rotations, are present\nin the talking body video data, which increase the training difficulty. To address this issue, we design\na Body Movement Map to serve as a control signal for the movement amplitude of body root in\ngenerated videos. Specifically, we determined a rectangular box representing the motion range of the\nthorax point over a video segment. To avoid a strong correlation between the motion trajectory of\nthe thorax point and the boundaries of the rectangular box, we augmented the size of the rectangular\nbox by 100% - 150%. The body movement map is down-sampled and encoded through a learnable\nPose Encoder and added as a residual to the noised latent. During testing, we typically input a body\nmovement map of fixed size to ensure the stability of the overall generated results.\nHand Clarity Score. Blurry hand images tend to lose structural details, weakening the model's ability\nto learn hand structures and causing the model to generate indistinct hand appearances. Therefore,\nwe introduce a Hand Clarity Score to indicate the clarity of hand regions in the training video frames.\nThis score is used as a conditional input to the denoising U-Net, enhancing the model's robustness\nto blurry hand data during training and enabling control over the clarity of the hand images during\ninference. Specifically, for each frame in the training data, we crop the pixel areas of the left and right\nhands based on key points and resize them to a resolution of 128 \u00d7 128. We then use the Laplacian\noperator to calculate the Laplacian standard deviation of the hand image frames. A higher standard"}, {"title": "4 EXPERIMENTS", "content": "In this section, we first provide the implementation details of our method and the experimental setup.\nFollowing this, we conduct quantitative and qualitative comparisons with state-of-the-art methods to\nvalidate the superior performance of our approach. We also perform ablation studies to analyze the\neffectiveness of our modules and training strategies. Finally, we explore the effects of our method in\na multimodal-driven setting and examined its generalization ability on open-set test images."}, {"title": "4.1 IMPLEMENT DETAILS", "content": "The training process is divided into two stages. The first stage aims to teach the model how to\nmaintain visual consistency between the generated video frames and the reference images. In this\nstage, two arbitrary frames from the training video clips are sampled as the reference frame and\ntarget frame, respectively. The primary training parameters include those of the Reference Net, Pose\nEncoder, and basic modules within the Denoising U-Net. The training was conducted for a total of 4\ndays on 8 A100 GPUs, with a batch size of 12 per GPU and a resolution of 640 \u00d7 384. In the second\nstage, we begin end-to-end training for the task of generating videos from images and audio. During\nthis phase, the parameters of modules such as the temporal layers, audio attention layers, and region\ncodebook attention layers are also optimized. Each video clip has a length of 12 frames, with the\nmotion frames' length set to 4. We use a total of 32 A100 GPUs to train for 4 days, with each GPU\nprocessing one video sample. This setup allows us to train with different resolutions on different\nGPUs. We constrain these different resolutions to have an area similar to the 640 \u00d7 384 resolution,\nwith both the height and width being multiples of 64 to ensure compatibility with the LDM structure.\nEach stage is trained with the learning rate set to le\u00af5. The classifier-free guidance (CFG) scale for\nthe reference image and audio is set to 2.5 and 4.5, respectively. We used video data collected from\nthe internet featuring half-body speech scenarios for training, amounting to a total of 200 hours and\nmore than 10k unique identities. We designated 269 video segments from 119 identities as the test\nset for quantitative evaluation."}, {"title": "4.2 COMPARISONS WITH STATE-OF-THE-ARTS", "content": "Due to the limited comparable works in the Audio-Driven Talking Body setting, we modified our\nalgorithm slightly for comparison in the Video-Driven Body Reenactment and Audio-Driven Talking\nHead experiment settings. This allows us to validate the advanced nature and generalizability of\nthe CyberHost framework against some of the current state-of-the-art algorithms. For evaluation\nmetrics, we use Fr\u00e9chet Inception Distance (FID) to assess the quality of the generated video frames\nand Fr\u00e9chet Video Distance (FVD) (Unterthiner et al., 2019) to evaluate the overall coherence of the\ngenerated videos. To assess the preservation of facial appearance, we calculate the cosine similarity\n(CSIM) between the facial features of the reference image and the generated video frames. We use\nSyncC and SyncD, as proposed in (Prajwal et al., 2020), to evaluate the synchronization quality\nbetween lip movements and audio signals in audio-driven settings. Additionally, Average Keypoint\nDistance (AKD) is used to measure the accuracy of actions in video-driven settings. Because the\nAKD cannot be used to evaluate hand quality in audio-driven scenarios, we compute the average of\nHand Keypoint Confidence (HKC) as a reference metric for evaluating hand quality. Similarly, we\ncalculated the standard deviation of hand keypoint coordinates within a video segment as the Hand\nKeypoint Variance (HKV) metric to represent the richness of hand movements.\nAudio-driven Talking Body Currently, only a few works such as Dr2 (Wang et al., 2023b),\nDiffTED (Hogue et al., 2024), and Vlogger (Corona et al., 2024) have adopted two-stage approaches\nto achieve audio-driven talking body video generation. However, these methods are not open-sourced,\nmaking it difficult to conduct direct comparisons. To better compare the effectiveness with the dual-\nstage method, we constructed a dual-stage audio-driven talking body baseline based on the current\nstate-of-the-art audio2gesture and pose2video algorithms. Specifically, we trained DiffGesture (Zhu\net al., 2023a) on our dataset to generate subsequent driving SMPLX (Pavlakos et al., 2019) pose\nsequences based on input audio and an initial SMPLX pose. Finally, the SMPLX meshes were\nconverted into DWPose (Yang et al., 2023) key points, and MimicMotion (Zhang et al., 2024) was\nused for video rendering based on these key points.\nAs shown in Table 1, our proposed CyberHost significantly outperforms the two-stage baseline in\nterms of image quality, video quality, facial consistency, and lip-sync accuracy in the audio-driven\ntalking body (A2V-B) setting. Figure 4 also presents a visual comparison between CyberHost and the\ntwo-stage baseline. Additionally, we utilized reference images and audio from 30 demos displayed\non the Vlogger homepage to conduct both quantitative and qualitative comparisons with Vlogger.\nNotably, since most of Vlogger's test videos exhibit minimal motion, the HKC indicator is relatively\nhigh, whereas the HKV indicator, which measures the diversity of movements, is very low, as shown\nin Table 1. As depicted in Figure 4, our proposed CyberHost surpasses Vlogger in both generated\nimage quality and the naturalness of hand movements.\nVideo-driven Body Reenactment We adapt our method to perform video-driven human body\nreenactment (V2V-B) by utilizing DWPose (Yang et al., 2023) to extract full-body keypoints from"}, {"title": "4.3 ABLATION STUDY", "content": "Analysis of Region Codebook Attention. As shown in Table 1, we conducted ablation experiments\nto analyze the structure and effectiveness of Region Codebook Attention. As we can see, the motion\ncodebook significantly improves metrics related to image quality, such as FID and FVD, as well\nas metrics related to the quality of hand generation, such as HKC. The identity descriptor, on the\nother hand, is more closely associated with the CSIM metric, demonstrating its effectiveness in\nmaintaining identity consistency. Additionally, we separately investigated the overall effectiveness of\nface codebook attention and hand codebook attention. Face codebook attention significantly improves\nfacial-related metrics such as CSIM and Sync score. Hand codebook attention effectively reduces\nartifacts caused by hands, thereby enhancing image quality metrics such as FVD and FID.\nAnalysis of Human-prior-guided Training Strategies. We also validated the effectiveness of\nvarious human-prior-guided training strategies in Table 1. The body movement map enhances the\nstability of the generated human body videos, leading to improved overall video quality. This\nsubsequently boosts metrics such as SSIM, PSNR, FID, and FVD. The hand clarity score can reduce\nartifacts caused by rapid hand movements and enhance hand clarity, thus significantly impacting the"}, {"title": "4.4 MULTIMODAL-DRIVEN VIDEO GENERATION", "content": "Our proposed CyberHost also supports combined control signals from multiple modalities, such\nas 2D hand keypoints and audio. As shown in Figure 6, the hand keypoints from Hand Pose\nTemplate are used to control hand movements and audio information is used to drive head movements,\nfacial expressions, and lip synchronization. This driving setup leverages the explicit structural\ninformation provided by hand pose templates to enhance the stability of hand generation, while\nsignificantly improving the correlation and naturalness of head movements, facial expressions, and\nlip synchronization with the audio."}, {"title": "4.5 AUDIO-DRIVEN RESULTS IN OPEN-SET DOMAIN", "content": "To validate the robustness of the CyberHost algorithm, we tested the audio-driven talking body\nvideo generation results on open-set test images. As shown in the Figure 7, our proposed method\ndemonstrates good generalization across various characters and is capable of generating complex\ngestures, such as hand interactions."}, {"title": "5 CONCLUSION", "content": "This paper introduces an one-stage audio-driven talking body generation framework, CyberHost,\ndesigned to produce human videos that match the input audio with high expressiveness and realism.\nCyberHost features an innovative Region Codebook Attention module to enhance the generation\nquality of key local regions, such as hands and faces. This module uses a spatio-temporal memory\nbank as a motion book to provide implicit guidance for maintaining coherent topological structures\nand natural motion patterns. Additionally, it injects appearance features from locally cropped images\nas identity descriptors to ensure local identity consistency. Combined with a suite of human-prior-\nguided training strategies suited to reduce the motion uncertainty in audio-driven setting, including\nthe body movement map, hand clarity score, pose-aligned reference feature, and local enhancement"}]}