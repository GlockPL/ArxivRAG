{"title": "Multivariate Time Series Anomaly Detection by Capturing Coarse-Grained Intra- and Inter-Variate Dependencies", "authors": ["Yongzheng Xie", "Hongyu Zhang", "Muhammad Ali Babar"], "abstract": "Multivariate time series anomaly detection is essential for fail-\nure management in web application operations, as it directly in-\nfluences the effectiveness and timeliness of implementing reme-\ndial or preventive measures. This task is often framed as a semi-\nsupervised learning problem, where only normal data are avail-\nable for model training, primarily due to the labor-intensive na-\nture of data labeling and the scarcity of anomalous data. Existing\nsemi-supervised methods often detect anomalies by capturing intra-\nvariate temporal dependencies and/or inter-variate relationships to\nlearn normal patterns, flagging timestamps that deviate from these\npatterns as anomalies. However, these approaches often fail to cap-\nture salient intra-variate temporal and inter-variate dependencies\nin time series due to their focus on excessively fine granularity,\nleading to suboptimal performance. In this study, we introduce\nMtsCID, a novel semi-supervised multivariate time series anom-\naly detection method. MtsCID employs a dual network architec-\nture: one network operates on the attention maps of multi-scale\nintra-variate patches for coarse-grained temporal dependency learn-\ning, while the other works on variates to capture coarse-grained\ninter-variate relationships through convolution and interaction\nwith sinusoidal prototypes. This design enhances the ability to\ncapture the patterns from both intra-variate temporal dependen-\ncies and inter-variate relationships, resulting in improved perfor-\nmance. Extensive experiments across seven widely used datasets\ndemonstrate that MtsCID achieves performance comparable or su-\nperior to state-of-the-art benchmark methods. Our code is available\nat https://github.com/ilwoof/MtsCID/.", "sections": [{"title": "1 Introduction", "content": "Modern society increasingly relies on web-based application sys-\ntems integrated into distributed systems, cloud computing plat-\nforms, and Internet of Things (IoT) devices [18, 22, 33]. These\nsystems function across various sectors, including finance, trans-\nportation, telecommunications, media, and industrial operations.\nDowntime or service interruptions in these critical infrastructures\ncan disrupt daily life, create chaos in business operations, and result\nin significant financial losses [8, 19, 39].\nTo ensure high reliability and availability of these systems, nu-\nmerous Al-based methods [3-6, 13, 14, 16, 17, 22, 28, 29, 31, 32, 34,\n36, 37, 42] have been proposed to detect anomalies from system op-\nerational data, such as service KPIs, as well as system runtime status\ndata like CPU and memory usage. This data often takes the form\nof Multivariate Time Series (MTS). MTS anomaly detection aims to\nidentify whether time steps in a series are normal or abnormal.\nDespite the vast amount of time series generated daily in these\nsystems, supervised learning methods often face challenges in this\ndomain due to the labor-intensive data labeling process and the\nscarcity of anomalous instances [14, 36]. As a result, MTS anomaly\ndetection is typically framed as a semi-supervised learning task,\nwhere only normal data is available for model training.\nTraditional machine learning methods, such as Local Outlier\nFactor (LOF) [4], One Class Support Vector Machine (OCSVM) [28],\nIsolation Forest (iForest) [16], have been widely used for anomaly\ndetection tasks. These methods treat multivariate observations at\ntime steps as points in a feature space. Distance or density metrics\nare used to assess the proximity of these points to each other. The\npoints that deviate significantly from the majority are flagged as\nanomalies. We refer to this approach as proximity-based. Recently,\nproximity-based methods that combine deep representation learn-\ning, such as DeepSVDD [26] and DAGMM [42], have also emerged.\nHowever, this approach often struggles with high accuracy due to\ntheir inability to effectively capture dynamic intra-variate temporal\ndependencies and complex inter-variate relationships.\nTo address the aforementioned issues, many temporal-based\nand spatiotemporal-based methods have been developed [5, 14, 15,\n29, 30, 36, 37]. For instance, THOC [29] employs a differentiable\nhierarchical clustering mechanism to integrate temporal features\nacross various scales and resolutions for effective normal pattern\nlearning. AT [36] models prior and series associations between\ntime steps in series for temporal pattern learning. DCdetector [37]\nutilizes two patch-based attention networks for contrastive learning\nto capture temporal dependencies in the given time series. We\nclassify these approaches as temporal-based methods since they\nprimarily rely on temporal dependencies for anomaly detection.\nIn contrast, InterFusion [15] leverages a hierarchical Variational"}, {"title": "2 Proposed Approach", "content": "2.1 Problem Definition\nGiven a set of subsequences $D = {X_1, . . ., X_N }$, where N represents\nthe total number of subsequences and each $X_i \\in R^{T\\times C}$ denotes\na subsequence of observations $[x_1, . . ., x_L ]$, with L indicating the\nlength of the subsequence. Here, $x_t \\in R^C$ represents the multivari-\nate observation vector at time t, with C indicating the total number\nof variates. Semi-supervised time series anomaly detection aims to\nidentify anomalies at the individual time step level within specified\nsubsequences, assuming that the training subsequences consists\nsolely of normal observations.\n2.2 Approach Overview\nWhen MTS subsequences are input into MtsCID, they are processed\nthrough two branches: the upper branch for learning temporal de-\npendencies and the lower branch for learning inter-variate rela-\ntionships. As shown in Fig 1, the two branches comprises three\ncomponents: the temporal autoencoder network (t-AutoEcoder),\nthe inter-variate dependency encoder network (i-Encoder), and the\nsinusoidal prototypes interaction module (p-i Module).\nIn the upper branch, each variate in the subsequence is initially\ntransformed into its frequency components. The fc-Linear, a fre-\nquency component-based Linear layer, and fc-Transformer, a fre-\nquency component-based Transformer network, are employed to\nlearn the dependencies of these frequency components. The de-\nrived representations are subsequently transformed back to the\ntime domain and passed through a set of intra-variate ts-Attention\n(time-series Attention) networks to learn temporal dependencies\nfrom the attention maps of multi-scale patches. Finally, these repre-\nsentations are fed into the decoder to reconstruct input sequences.\nIn the lower branch, each subsequence is first processed in the\ntime domain using a convolutional layer with a specific kernel size\nto capture local temporal dependencies in the variates. The output\nis then fed into the inter-variate fc-Transformer networks to learn\ninter-variate relationships in the frequency domain. The resulting\nrepresentations are subsequently interacted with the sinusoidal\nprototypes in the p-i Module for inter-variate relationship learning.\nDuring training, the reconstruction loss between the input and\nthe generated sequences from the upper branch is combined with\nthe output from the lower branch to form a comprehensive loss that\nguides model training. During inference, the reconstruction loss\nfrom the upper branch interacts with the output from the lower\nbranch to produce an anomaly score for each time step in the series,\nindicating whether a specific timestamp is anomalous.\nFigure 2 provides an overview of how the building blocks func-\ntion. Next, we will elaborate on each major component of MtsCID."}, {"title": "2.3 Temporal AutoEncoder Network\n(t-AutoEncoder)", "content": "The temporal autoencoder network is designed to capture temporal\ndependencies between time steps within the series. When time se-\nries subsequences $X \\in R^{B\\times L\\times C}$ are input\u2014where B represents the\nbatch size, L is the subsequence length, and C denotes the number\nof series-each series is first transformed into its frequency compo-\nnents $H \\in R^{B\\times f\\times C}$. Here, H encompasses the real and imaginary"}, {"title": "2.4 Inter-variate Dependency Encoder Network\n(i-Encoder)", "content": "Prior studies have demonstrated that inter-variate relationships can\nenhance anomaly detection in multivariate time series [5, 15, 30].\nIn this study, we employ an independent inter-variate dependency\nencoder network to capture these relationships from normal time\nseries. Since each variate measures different aspects of the moni-\ntored system, they often exhibit varying periodicities, making it\ndifficult to learn their normal combination patterns over time.\nTo address this challenge, we first input the time series subse-\nquences $X \\in R^{B\\times L\\times C}$ into a 1D-convolution network. The kernel\nsize of the convolution network is assumed as k. This process yields\ncorresponding representations, $T \\in R^{B\\times C\\times L}$, which capture lo-\ncal temporal dependencies in variates. The derived representation\noffers two key benefits: 1) Capturing coarse-grained temporal de-\npendencies enhances the semantic representations of individual\ntime steps, making it more robust to noise interference. 2) It can\nmitigate the issues related to potential misalignment of time steps\nacross variates that may occur during data collection, as well as\nchallenges posed by unsynchronized variates.\nThe derived representations T is then transformed into its fre-\nquency components, $E \\in R^{B\\times C\\times f}$. Then, a fc-Transformer network\nis employed to capture inter-variate relationships in the frequency\ndomain, generating the representations $J \\in R^{B\\times C\\times f}$. Notably, the\ninter-variate fc-Transformer processes data along the variate di-\nmension, while the intra-variate fc-Transformer in the temporal\nautoencoder operates along the time step dimension. A detailed\ncomparison can be seen in Figure 2.\nThe derived representations J, which capture inter-variate re-\nlationships, are then transformed back into the time domain. A\nresidual connection and layer normalization are applied, result-\ning in the final representations $O \\in R^{B\\times L\\times C}$. The mathematical\nexpressions summarizing this process are presented as follows:\n$T = Conv1d(X)$\n$E = DFT(T)$\n$J = Softmax(\\frac{EE^T}{\\sqrt{f}})E$\n$O = LayerNorm(iDFT(J) + X)$"}, {"title": "2.5 Sinusoidal Prototypes Interaction Module\n(p-i Module)", "content": "In the inter-variate dependency encoder network, while the derived\nrepresentations O capture inter-variate relationships, the combina-\ntions of variables at time steps remain complex and challenging for\nlearning salient normal patterns. To address this issue, we aim to\nsimplify these complex combinations into a limited set, making it\neasier to learn normal inter-variate relationship patterns.\nIn the study by Song et al. [30], the authors demonstrate that\nincorporating memory items, also known as prototypes, can en-\nhance the learning of inter-variate normal patterns. Inspired by\ntheir work, we develop a sinusoidal Prototypes Interaction Module.\nUnlike their dynamic memory updating mechanism, we utilize a\nset of fixed memory items $M \\in R^{C\\times L}$, derived from sinusoidal\nfunctions with varying periodicity, defined as follows:\n$M_{i,j} = cos(\\frac{2\\pi}{L}.i.j)$\n$M \\in R^{L\\times C}$ for $i = 0, 1, ..., L - 1$ and $j = 0, 1, ..., C - 1$\nBy using fixed memory items, we avoid the instability issue in\nmodel training mentioned in [30]. Furthermore, since these memory\nitems are derived from sinusoidal functions with different periodic-ity, their combinations along the time step dimension approximate\na limited set. As shown in the experimental section, this approach\nenhances the salience of patterns across inter-variate relationships,\nthereby improving both robustness and detection accuracy, even\nwithout the need for additional two-phase training and clustering\nprocesses as described in [30].\nFollowing the practice in [30], we multiply representations O\n, generated from the inter-variate dependency encoder network,\nwith our fixed sinusoidal prototypes through dot product, followed\nby the SoftMax operation, as follows:\n$W_{ti} = \\frac{exp(<O_{:,t,:}, M_{i,:} > /\\tau)}{\\Sigma_{j=1} exp(< O_{:,t,:}, M_{j,:} > /\\tau)}$"}, {"title": "2.6 Learning Tasks", "content": "MtsCID utilizes two learning tasks, i.e., temporal dependency recon-\nstruction task and prototype-oriented learning task, to effectively\nguide model optimization during training. These tasks corresponds\nto two specific losses: the $L_{t-rec}$, and the $L_{i-ent}$.\n2.6.1 Temporal Dependency Reconstruction Task. For the temporal\ndependency learning branch, i.e. the upper branch, a reconstruction\nloss is utilized between the input and the reconstructed ones to\ndirect its network for optimization. The reconstruction loss $L_{t-rec}$\nis defined as L2 loss between X and X:\n$L_{t-rec} = \\frac{1}{B} \\Sigma_{s=1}^{B} \\| X - \\hat{X} \\|_2^2$\n2.6.2 Prototype-Oriented Learning Task. For the inter-variate de-\npendency learning branch, i.e. the lower branch, we adopt the\npractice from the study [30] that an entropy loss $L_{i-ent}$ as our"}, {"title": "2.7 Anomaly Detection", "content": "During inference, the deviations from the learned normal patterns\nin the two branches, specifically the Temporal Deviation (TD) and\nRelationship Deviation (RD), are combined to generate anomaly\nscores for each time step in the input. The $TD(X_{t,:}, \\hat{X}_{t,:})$ is defined\nas the distance between the input $X_{t,:} \\in R^{C}$ and the reconstructed\ninput $\\hat{X}_{t,:} \\in R^{C}$ at time t. The $RD(O_{t,:}, M_{.,:})$ is defined as the distance\nbetween each $O_{t,:}$ and its nearest memory step $m_{s,:}$. The formal\ndefinitions of the Anomaly Scores (AScore) are as follows:\n$AScore(X) = Softmax([RD(O_{t,:}, M_{.,:})]) \\circ [TD(X_{t,:}, \\hat{X}_{t,:})]$\nwhere $\\circ$ is element-wise multiplication and and $AScore(X) \\in R^{L}$\nis anomaly score at each time step. These anomaly scores with\nhigher scores indicating a higher likelihood of the corresponding\ntime steps as anomalies."}, {"title": "3 Experimental Setting", "content": "3.1 Datasets\nWe evaluate MtsCID on two groups of seven widely used real-world\nMTS datasets. A summary of these datasets is provided in Table 1,\nalong with further descriptions below.\n\u2022 SMAP (Soil Moisture Active Passive) [11] is a dataset of soil\nsamples and telemetry information using the Mars rover\nby NASA, while MSL (Mars Science Laboratory) [11] corre-\nsponds to the sensor and actuator data for the Mars rover\nitself. SMD (Server Machine Dataset) [31] is a five-week\nlong dataset of stacked traces of the resource utilization of\n28 machines from a compute cluster. PSM (Pooled Server\nMetrics) [1] is collected internally from multiple application\nserver nodes at eBay with 25 dimensions. SWaT (Secure\nWater Treatment) [20] is gathered from a real-world water\ntreatment plant, including 7 days of normal operations and\n4 days of abnormal operations.\n\u2022 NIPS-TS-GECCO [21], referred to as GECCO, comprises\ndrinking water quality data for the Internet of Things and\nwas published at the 2018 Genetic and Evolutionary Compu-\ntation Conference. NIPS-TS-SWAN [2, 12], known as SWAN,\nis an openly accessible, comprehensive MTS benchmark de-\nrived from solar photospheric vector magnetograms in the\nSpace Weather HMI Active Region Patch series. Both datasets\nare sourced from [37]."}, {"title": "3.2 Evaluation Metrics", "content": "In this study, we employ three groups of evaluation metrics.\n\u2022 The first group of metrics includes point-adjustment Preci-\nsion, Recall, and F1-score, which are widely used in time\nseries anomaly detection [30, 31, 34-38, 40]. This approach\nacknowledges that anomalies typically manifest as contigu-\nous segments rather than isolated points. Consequently, if\nany point within a contiguous segment is detected as anoma-\nlous, the entire segment is deemed correctly identified. Fol-\nlowing the methodology in [5], we utilize the best F1 score\nto mitigate biases from threshold settings.\n\u2022 The second group is Affiliation Precision, Recall and F1\nScore [10], which are also used in recent studies [5, 22, 37]:\nThis set of metrics incorporates duration measures between\nground truth and predictions, addressing limitations of other\nmetrics that ignore temporal adjacency and event duration.\nDue to space constraints, we present only the Affiliation\nF1-score (AF-F1).\n\u2022 VUS-ROC/VUS-PR [23] are used as the third group of metrics\nin our study, which are also used in recent studies [5, 22, 37].\nThese metrics extend the ROC-AUC and PR-AUC measures.\nVUS-ROC (Volume Under the ROC Surface) and VUS-PR\n(Volume Under the PR Surface) address biases introduced\nby point adjustment by evaluating the overall volume under\nthe respective curves."}, {"title": "3.3 Implementation and Environment", "content": "Following the approach in [30], we generate sub-sequences using\na non-overlapping sliding window of length 100 to create fixed-\nlength inputs for each dataset. The training data is then divided\ninto 80% for training and 20% for validation. In the t-AutoEncoder\nnetwork, we set the number of multi-scale patches to m = 2, with\npi taking values from [10, 20]. The i-Encoder network is configured\nwith a 1D convolution layer featuring a kernel size of (k = 5).\nWe implemented MtsCID using PyTorch 1.11.0. The model was\ntrained with the AdamW optimizer and employed polynomial learn-\ning rate decay, starting at 2 \u00d7 10-3 and gradually decreasing to\n5\u00d710-5. A batch size of 64 was used, with training up to 20 epochs\nand early stopping if performance didn't improve for 10 iterations.\nAll experiments were conducted on a Linux server Ubuntu 20.04\nequipped with an AMD Ryzen 3.5GHz CPU, 96 GB of memory, and\nan RTX2080Ti with 11 GB of GPU memory."}, {"title": "4 Results and Analysis", "content": "4.1 The Effectiveness of MtsCID\nTo evaluate the effectiveness of our proposed method, we compare\nMtsCID with nine state-of-the-art semi-supervised methods. The\nbaseline methods include proximity-based approaches-iForest [16],\nDeepSVDD [26], and DAGMM [42]; temporal-based methods-\nTHOC [29], AT [36], and DCdetector [37]; spatio-temporal-based\nmodels-InterFusion [15], MEMTO [30], and STEN [5]. In our com-\nparative analysis, the implementations of the baseline approaches\nwere obtained from their public repositories. To ensure consistency,\nwe adhered to the parameters provided by their respective imple-\nmentations unless otherwise specified. Each method was executed\nfive times for each dataset, and the resulting values were averaged\nto report the final results.\nWe first evaluate MtsCID against all the aforementioned base-\nlines using point-adjustment metrics across five datasets, with re-\nsults presented in Table 2. Since recent methods, including AT,\nDCdetector, MEMTO, and STEN, outperform other baselines, we\nfocus our multi-metric comparison of MtsCID primarily on these\nmodels. This comparison includes the aforementioned five datasets,\nas well as two additional, more challenging datasets (GECCO and\nSWAN) that feature a wider variety of anomalies.\nThe experimental results in Tables 2 and 3 demonstrate that\nMtsCID achieves robust and superior performance across all datasets.\nSpecifically, MtsCID secures the highest F1 and AF-F1 scores in six\nout of seven datasets, and the second-best F1 score in the remain-\ning dataset, outperforming all baseline methods. Notably, on the\nchallenging GECCO dataset, MtsCID demonstrates a significant\nadvantage in the F1 metric, outperforming the second-best baseline\nby 42.12%. MEMTO also achieves consistently excellent results, indi-\ncating that leveraging prototypes enhances detection performance\nby providing additional information. While DCdetector and STEN\nperform well, closely matching MtsCID's effectiveness across most\ndatasets, there is a notable disparity in detection effectiveness on\nthe SMD and GECCO dataset.\nFurthermore, We can see from Table 2 and Table 3 that the\nmethods leveraging temporal and spatiotemporal dependencies\nconsistently outperform all proximity-based methods, underscoring\nthe importance of capturing dependencies among features in time\nseries. However, spatiotemporal-based methods like STEN do not\nalways surpass temporal-based methods such as AT and DCdetector,\nlikely because temporal dependencies are the primary indicators for\nidentifying anomalies. This suggests that spatial features require\ncareful design for improved performance."}, {"title": "4.2 Ablation studies", "content": "In this section, we aim to thoroughly examine the effectiveness\nof each major component within MtsCID on the final results. To"}]}