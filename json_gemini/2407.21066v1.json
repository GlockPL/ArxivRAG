{"title": "ELP-Adapters: Parameter Efficient Adapter Tuning for Various Speech Processing Tasks", "authors": ["Nakamasa Inoue", "Shinta Otake", "Takumi Hirose", "Masanari Ohi", "Rei Kawakami"], "abstract": "Self-supervised learning has emerged as a key approach for learning generic representations from speech data. Despite promising results in downstream tasks such as speech recognition, speaker verification, and emotion recognition, a significant number of parameters is required, which makes fine-tuning for each task memory-inefficient. To address this limitation, we introduce ELP-adapter tuning, a novel method for parameter-efficient fine-tuning using three types of adapter, namely encoder adapters (E-adapters), layer adapters (L-adapters), and a prompt adapter (P-adapter). The E-adapters are integrated into transformer-based encoder layers and help to learn fine-grained speech representations that are effective for speech recognition. The L-adapters create paths from each encoder layer to the downstream head and help to extract non-linguistic features from lower encoder layers that are effective for speaker verification and emotion recognition. The P-adapter appends pseudo features to CNN features to further improve effectiveness and efficiency. With these adapters, models can be quickly adapted to various speech processing tasks. Our evaluation across four downstream tasks using five backbone models demonstrated the effectiveness of the proposed method. With the WavLM backbone, its performance was comparable to or better than that of full fine-tuning on all tasks while requiring 90% fewer learnable parameters.", "sections": [{"title": "I. INTRODUCTION", "content": "In the field of audio and speech processing, self-supervised learning using large-scale unlabeled datasets has become a leading approach for extracting generic representations from speech data [1]\u2013[7]. The main idea of this approach is to leverage the inherent structures and patterns within the speech data to train models via representation learning loss such as contrastive loss [8]\u2013[11]. This significantly reduces the need for manually labeled data, making model training more scalable and efficient. Examples of models trained by self-supervised learning, which we refer to as self-supervised models, include wav2vec [1], [2], HuBERT [4], and WavLM [6]. These models have demonstrated the ability to extract task-independent features with transformer-based architectures.\nIn recent years, the range of speech processing tasks that can be covered by self-supervised models has been steadily expanding beyond automatic speech recognition. For example, a number of studies have proposed methods that utilize speech embeddings extracted from self-supervised models for discriminative tasks such as speaker verification [12]\u2013[15] and speech emotion recognition [16], [17]. Some pioneering studies have demonstrated the effectiveness of self-supervised models in addressing more complex and generative tasks. For example, spoken question answering is an important line of research focused on developing models capable of understanding and responding to questions posed in natural spoken language, where recent studies leverage self-supervised models [18]\u2013[23]. It has also been demonstrated that self-supervised models can perform voice conversion effectively and efficiently by integrating a decoder and a vocoder [24]\u2013[28]. These studies highlight the potential of self-supervised models across various speech tasks.\nTo apply self-supervised models to downstream tasks, fine-tuning on task-specific labeled datasets is often required. This process enables the models to adapt and specialize in specific tasks, leading to excellent results not only in speech recognition but also in various speech tasks. However, one limitation is the substantial number of parameters involved. When fine-tuning is conducted for each downstream task, multiple models must be stored, one for each task. This can lead to storage inefficiencies in real-world application settings, such as when each user wants to fine-tune the model with their private data and task.\nA parameter-efficient method for adapting self-supervised models to various downstream tasks is thus desirable. Learning task-specific downstream head modules, such as a linear classification head, with frozen self-supervised models is an efficient solution; however, it often degrades the final performance compared to that obtained by fine-tuning all parameters because the optimal features can differ substantially depending on each task. For instance, linguistic features that include phoneme information are crucial for speech recognition, whereas non-linguistic features are crucial for speaker verification.\nRecently, learning with adapter modules that can be inserted into the intermediate encoder layers of a frozen model has emerged as a promising approach for parameter-efficient fine-tuning. The first adapter tuning method [29] was proposed for BERT [30] in the field of natural language processing, where two adapter modules are inserted into each encoder layer of BERT. Each adapter module consists of two linear layers with an activation between them and a skip connection. This approach requires fewer parameters (the frozen parameters are shared among all downstream tasks) without degrading accuracy. A number of follow-up studies have used adapters for various natural language processing tasks [31]\u2013[33].\nFor speech recognition, Kannan et al. [34] integrated adapter modules into recurrent neural network transducers. Hou et al. [35], [36] proposed the use of adapters for cross-lingual speech adaptation. Winata et al. [37] proposed the adapt-and-"}, {"title": "II. CONVENTIONAL METHODS", "content": "The goal of self-supervised learning is to learn features from unlabeled data by leveraging the intrinsic structure of the data itself. This approach involves creating tasks where the input data serve as their own supervision data. Below, we review five self-supervised models for speech signal processing that we use as the backbones in our experiments.\n1) wav2vec2.0 [2]: This model consists of a convolutional neural network (CNN) encoder followed by multiple transformer encoders. The CNN encoder extracts low-level features from raw waveform inputs via a sequence of several blocks, each with a temporal convolution layer, layer normalization [47], and a Gaussian error linear unit (GELU) [48] activation function. The transformer encoders apply attention modules to the extracted features. We employ the wav2vec2.0 base model trained on the Librispeech [49] corpus, which contains 960 hours of speech with contrastive loss and diversity loss. The number of parameters is 95.04M.\n2) HuBERT [4]: This model aims to discover hidden acoustic units to provide frame-level targets in self-supervised learning using masked prediction. The architecture consists of a CNN encoder and transformer encoders, similar to wav2vec2.0. We employ the HuBERT base model, which is trained on the Librispeech corpus with masked prediction loss using the acoustic unit discovery module. The number of parameters is 94.68M.\n3) ContentVec [7]: This model aims to disentangle speaker variations during self-supervised learning by incorporating three disentanglement mechanisms into HuBERT, namely disentanglement in teachers, students, and speaker conditioning. The architecture is the same as that of the HuBERT base model. We employ the ContentVec model trained on Librispeech.\n4) WavLM [6]: This model is a self-supervised model for addressing various downstream speech tasks. The architecture consists of a CNN encoder and transformer-based encoders using gated relative position bias [50]. We employ two models, namely WavLM Base and WavLM Base+. The former model is trained on Librispeech. The latter model, which we refer to as WavLM+, is trained on a union set of Librispeech, GigaSpeech [51], and VoxPopuli [52], which contains a total of 96k hours of audio data. The number of parameters for each model is 94.70M."}, {"title": "B. Fine-tuning methods", "content": "Given a self-supervised model, the goal of fine-tuning is to adjust the model parameters for a specific downstream task, typically using a relatively small amount of labeled data and a task-specific loss function. Assuming that self-supervised models share a common architecture, which consists of a CNN encoder followed by multiple transformer-based encoders as shown in Fig. 1(a), below we provide details on five fine-tuning methods that we use as baselines in our experiments.\n1) Full fine-tuning: This method updates all model parameters for each downstream task. Typically, a small downstream head such as a linear head or a multi-layer perceptron (MLP) with few layers is added to the self-supervised model to apply a task-specific loss function such as CTC loss for ASR [53] and cross entropy loss for SIC. In general, full fine-tuning is less parameter-efficient, but it often achieves high performance on downstream tasks.\n2) Weight tuning: This method utilizes the weighted sum of features extracted from the encoder layers, where the weight coefficients are learnable and the other parameters of the self-supervised model are frozen as shown in Fig. 1(b). More specifically, it is formulated as\n$X = \\sum_{l=1}^{L} \\omega_l X_l,$\nwhere $X_l \\in \\mathbb{R}^{n \\times d}$ is the output of the l-th encoder layer given as a sequence of d-dimensional vectors of length $n \\in \\mathbb{N}$, $\\omega_l \\in \\mathbb{R}$ is a learnable weight, and $L \\in \\mathbb{N}$ is the number of encoder layers. When applying weight tuning to downstream tasks, a learnable downstream head that takes $X \\in \\mathbb{R}^{n \\times d}$ as the input is added to the frozen self-supervised model. As discussed in [6], this method is significantly more parameter-efficient than full fine-tuning because most parameters are frozen and shared among all downstream tasks. However, performance on downstream tasks is often degraded.\n3) LoRA tuning [42]: This method injects rank decomposition matrices into a frozen self-supervised model. When applying LoRA tuning to self-attention modules, the key, value, and query matrices are computed with injected low-rank matrices as shown in Fig. 1(c). More specifically, given an input sequence $X \\in \\mathbb{R}^{n \\times d}$, the self-attention module of LoRA tuning is given as\n$f_{attn}(X) = softmax(\\frac{Q(K)^T}{\\sqrt{d}})V,$\nwhere K, V, and Q are the key, value, and query matrices, respectively, given by\n$K = X(W_k + A_k B_k),$\n$V = X(W_v + A_v B_v),$\n$Q = X(W_q + A_q B_q).$\nHere, $W_k, W_v, W_q \\in \\mathbb{R}^{d \\times d'}$ are pre-trained frozen weights, $A_k, A_v, A_q \\in \\mathbb{R}^{d \\times r}$ and $B_k, B_v, B_q \\in \\mathbb{R}^{r \\times d'}$ are learnable low-rank matrices. The rank r is chosen such that $r < min(d, d')$. We apply LoRA tuning to all self-attention modules and the fully connected layers after each self-attention module with r = 128.\n4) Prefix tuning [46]: This method prepends pseudo tokens to each encoder layer by concatenating new learnable embeddings to the key and value matrices of each self-attention module as shown in Fig. 1(d). Specifically, the key, value, and query matrices to compute self-attention are given by\n$K = [P_k; XW_k] \\in \\mathbb{R}^{(n+m) \\times d'},$\n$V = [P_v; XW_v] \\in \\mathbb{R}^{(n+m) \\times d'},$\n$Q = XW_q \\in \\mathbb{R}^{n \\times d'}.$\nHere, $W_k, W_v, W_q \\in \\mathbb{R}^{d \\times d'}$ are pre-trained frozen weights, $P_k, P_v \\in \\mathbb{R}^{m \\times d'}$ are newly added learnable matrices, and $[;]$ indicates the concatenation operation. We apply prefix tuning to all self-attention modules with m = 5.\n5) Efficient adapter tuning [40]: In the field of natural language processing, Houlsby et al. [29] proposed efficient adapter modules for transformers. This was applied to wav2vec2.0 by Thomas et al. [40] for speech recognition. We refer to this method as efficient adapter tuning. Let $X_o \\in \\mathbb{R}^{n \\times d}$ be the output of the CNN encoder, where n is the length, which depends on the time length of the audio input, and d is the"}, {"title": "III. PROPOSED ADAPTER ARCHITECTURE", "content": "This section presents ELP-adapter tuning, a novel fine-tuning method for various speech processing tasks. Given a frozen self-supervised model (e.g., WavLM), ELP-adapter tuning incorporates three types of adapter, namely E-adapters, L-adapters, and a P-adapter, into the model, as shown in Fig. 2. The E-adapters $g_e^{(l)}$ are integrated into the transformer-based encoder layers. This helps to obtain fine-grained linguistic representations that are effective for speech recognition. The L-adapters $g_l^{(l)}$ create paths from each encoder layer to the downstream head. This helps to extract features from intermediate encoder layers; such features are effective for emotion recognition and speaker verification. The P-adapter $g_p$ appends learnable embeddings to input tokens to further enhance training effectiveness and efficiency.\nThe amount of storage required to store fine-tuned models is $O(N + K(M + H))$, where K is the number of downstream tasks and N, M, and H are the numbers of parameters of the frozen backbone model, learnable adapter modules, and downstream head, respectively. Compared to full fine-tuning, for which the amount of storage required is $O(K(N + H))$, ELP-adapter is more efficient when $M \\ll N$. In practice, we need M to be roughly 10 percent of N to achieve performance comparable to that of full fine-tuning. For example, with the WavLM backbone and our ELP-adapter modules, we have $N = 94.7M$ and $M = 9.52M$. In the following, we provide detailed descriptions of each adapter module and downstream head."}, {"title": "A. E-adapters", "content": "The E-adapters $g_e^{(l)}: \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$ are incorporated into each encoder layer to obtain fine-grained representations via fine-tuning as shown in Fig. 2(a). Specifically, they are formulated as follows:\n$\\hat{Z_l} = f_{norm}(f_{mhsa}(\\hat{X}_{l-1}) + X_{l-1}),$\n$\\hat{X_l} = f_{norm}(g_e^{(l)}(f_{ffn}(\\hat{Z_l})) + Z_l),$\nwhere $f_{ffn}$ is a frozen feedforward network, $f_{mhsa}$ is a frozen multi-head self-attention module, $f_{norm}$ is a normalization function, and $\\hat{X_l}$ indicates the adapted output of the l-th encoder layer. Each E-adapter is given by\n$g_e^{(l)}(X) = f_{norm}(f_{fc2}(\\sigma(f_{fc1}(X)))) + X$\nwhere $f_{fc1}$ and $f_{fc2}$ are learnable fully connected layers and $\\sigma$ is an activation function. Compared to the conventional efficient adapter tuning in Eqs. (12) and (11), the adapter module for MHSA is omitted. When the E-adapters are used with the L-adapters presented in the next subsection, this omission does not lead to a decrease in performance and improves parameter efficiency. For activation function $\\sigma$, the rectified linear unit (ReLU) is used for ASV and SER and GELU is used for ASR and IC."}, {"title": "B. L-adapters", "content": "The L-adapters make paths from each encoder layer to the downstream head to utilize the intermediate representations from the early phases of fine-tuning as shown in Fig. 2(b). Let $X_l$ be the output of the l-th encoder layer. The L-adapters $g_l^{(l)}$ are applied to each $X_l$ to obtain adapted features as\n$\\hat{A_l} = g_l^{(l)}(X_l)$\nfor $l = 1,2,...,L$. Each L-adapter is given by\n$g_l^{(l)}(X) = f_{norm}( \\sigma(f_{fc}(X))),$\nwhere $f_{fc}$ is a learnable fully connected layer, $\\sigma$ is an activation function, and $f_{norm}$ is a layer normalization function. The weighted sum of the adapted features\n$\\hat{A} = \\sum_{l=1}^{L} \\omega_l \\hat{A_l}$\nis then fed into the downstream head, where $\\omega_l \\in \\mathbb{R}$ represents learnable weights. This L-adapter is simpler than the conventional adapter module in Eq. (13), resulting in better parameter efficiency. The activation function $\\sigma$ is the same as that used for the E-adapters. The L-adapters are key components of our proposed method to cover various speech processing tasks, such as automatic speaker verification, where features extracted from lower layers are effective."}, {"title": "C. P-adapter", "content": "Let $X_o \\in \\mathbb{R}^{n \\times d}$ be the output of the CNN encoder, where n is the length and d is the dimension of each feature vector. The P-adapter injects pseudo features into it as shown in Fig. 2(c). We introduce four variants of P-adapters.\n1) Prefix P-adapter: The prefix P-adapter $g_{pre}$ prepends a new learnable matrix $P \\in \\mathbb{R}^{m \\times d}$ as follows:\n$g_{pre}(X_o) = [P; X_o] \\in \\mathbb{R}^{(m+n) \\times d},$\nwhere $[;]$ indicates the concatenation operation. Then, $Q_o = g_{pre}(X_o)$ is fed into the transformer encoders to obtain the encoder outputs $Q_l$ as\n$Q_l = f_{enc}^l(Q_{l-1}),$\nwhere $f_{enc}^l$ indicates the l-th encoder. At the final layer, the first m vectors corresponding to P are omitted:\n$\\hat{X_L} = g_{pre}^{-1}(\\hat{Q_L}) \\in \\mathbb{R}^{n \\times d}$\nwhere $g_{pre}^{-1}$ is the inverse operation of $g_{pre}$ for omitting the vectors. This restores the sequence length to n. The feature $\\hat{X_L}$ is fed into the downstream head.\nWhen the P-adapter is utilized with the E-adapters, the E-adapters are applied to $Q_l$:\n$\\hat{Q_l} = f_{norm}(g_e^{(l)}(f_{ffn}(\\hat{Z_l})) + Z_l),$\n$\\hat{Z_l} = f_{norm}(f_{mhsa}(\\hat{Q}_{l-1}) + Q_{l-1}).$\nWhen the P-adapter is utilized with the L-adapters, the inverse operation is inserted into each L-adapter as follows:\n$\\hat{A_l} = g_l^{(l)}(g_{pre}^{-1}(\\hat{Q_l})).$"}, {"title": "2) Suffix P-adapter:", "content": "The suffix P-adapter $g_{suf}$ appends a new learnable matrix $P \\in \\mathbb{R}^{m \\times d}$ to $X_o$ as follows:\n$g_{suf}(X_o) = [X_o; P] \\in \\mathbb{R}^{(n+m) \\times d}.$\nThis can be utilized with the E- and L-adapters in the same way as done for the prefix P-adapter.\n3) Nonlinear P-adapter: To facilitate learning through pseudo features, the nonlinear P-adapter applies a small MLP $f_{mlp}$ to learnable embeddings P. Specifically, we introduce two variants of the nonlinear P-adapter, namely prefix nonlinear P-adapter $g_{nl-pre}$ and suffix nonlinear P-adapter $g_{nl-suf}$, as follows:\n$g_{nl-pre}(X_o) = [f_{mlp}(P); X_o] \\in \\mathbb{R}^{(m+n) \\times d},$\n$g_{nl-suf}(X_o) = [X_o; f_{mlp}(P)] \\in \\mathbb{R}^{(n+m) \\times d}.$\nThe best P-adapter configuration depends on the task, as we will discuss in Section VIII-B. We use the suffix P-adapter as the default P-adapter because it offers a good balance between performance and efficiency."}, {"title": "D. Downstream head", "content": "The downstream head is a minimal learnable module that is used to apply task-specific loss function. This paper considers four tasks, ASR, ASV, SER and SIC, which belong to the four different aspects of speech [54]: content, speaker, paralinguistics, and semantics, respectively. As shown in Fig. 2(d), a single fully connected layer is used for ASR. A small network that consists of two fully connected layers with an average time pooling layer in between is used for ASV, SER, and SIC. During fine-tuning, we also make all LayerNorm parameters learnable in the backbone self-supervised model, resulting in an addition of 0.037M learnable parameters. This approach is applied to all fine-tuning methods (weight tuning, prefix tuning, LoRA tuning, efficient adapter tuning, and our ELP-adapter tuning) in our experiments."}, {"title": "IV. AUTOMATIC SPEECH RECOGNITION", "content": "ASR aims to convert speech signals into text transcriptions. For this task, speaker-independent features that distinguish phonemes often help improve performance. In this section, we conduct experiments to demonstrate the effectiveness of ELP-adaptor tuning for the ASR task."}, {"title": "A. Datasets and evaluation metrics", "content": "The LibriLight dataset (train-10h) [55] is used for training. It is a supervision training set that consists of 10 hours of audio data derived from open-source English audiobooks in the LibriVox project. The number of speakers is 24 (12 male, 12 female). The LibriSpeech (dev-clean) [49] dataset is used for testing. It consists of 5.4 hours of audio data with 40 speakers (20 male, 20 female).\nThe evaluation metric is the word error rate (WER), defined as\n$WER = \\frac{S + D + I}{N}$\nwhere S is the number of substitutions, D is the number of deletions, I is the number of insertions, and N is the number of words in the reference."}, {"title": "B. Implementation details", "content": "The downstream head for ASR consists of a single fully connected layer. CTC loss [53] is used as the loss function. All models are fine-tuned with the Adam optimizer for $N_{total} = 34,600$ iterations with a batch size of 8. The learning rate is scheduled with a linear warmup scheduler:\n$N_t = \\begin{cases}\n  \\frac{t}{N_{warm}} \\cdot (n_{max} - n_0) + n_0 & \\text{if } t \\leq N_{warm}\\\\\n  n_{max} + \\frac{(N_{total} - t)}{N_{total} - N_{warm}} \\cdot (n_0 - n_{max}) & \\text{if } t > N_{warm}\n\\end{cases}$\nwhere t is the time step, $N_{warm} = 5,000$ is the number of warmup steps, $n_0 = 10^{-7}$ is the initial and final learning rate, and $n_{max}$ is the maximum learning rate after warmup. For each fine-tuning method, the best learning rate for $n_{max}$ is chosen from {$1.0 \\times 10^{-3}, 5.0 \\times 10^{-4}, 1.0 \\times 10^{-4}, 5.0 \\times 10^{-5}, 1.0 \\times 10^{-5}$}."}, {"title": "C. Comparison with conventional methods", "content": "Table I compares ELP-adapter tuning with the conventional fine-tuning methods described in Section II-B. As shown, our method outperformed the conventional methods for the five self-supervised models while having fewer learnable parameters than that for the conventional efficient adapter tuning. This shows the effectiveness and parameter efficiency of ELP-adapter tuning. It is also worth noting that ELP-adapter tuning achieved WERs lower than those obtained by full fine-tuning for two models (HuBERT and WavLM+). This is because ELP-adapter allows for quick adaptation while avoiding overfitting.\nRegarding the self-supervised models, WavLM showed the best performance among the four models pre-trained on LibriSpeech (wav2vec2.0, HuBERT, ContentVec, and WavLM), with the exception of the prefix tuning result. This is because the gated relative position bias used in WavLM is particularly effective for ASR. With prefix tuning, wav2vec2.0 provided the best fit. This is because when adding new elements to the key and value matrices of attention, a simpler architecture works better. In addition, WavLM+ outperformed WavLM in all cases. This shows that increasing the amount of pre-training data improves performance, regardless of the fine-tuning method."}, {"title": "D. Ablation study", "content": "Table II shows the results of the ablation study for various adapter types. As shown, E-adapter tuning outperformed L-adapter tuning for the five self-supervised models. This indicates that the adaptation of encoders plays a more crucial role in ASR than the fusion of outputs from multiple layers via L-adapters. For ASR, features from layers closer to the last layer,"}, {"title": "E. Trade-off analysis", "content": "We performed experiments in which the numbers of layers used to fine-tune and insert adapters was varied to find cases where a smaller number of parameters would perform well. Figure 3 compares the results obtained with full fine-tuning, conventional efficient adapter tuning, and the proposed ELP-adapter tuning. As shown, all error curves decrease quickly, showing that adjustments of only the top three layers are sufficient. This suggests that encoders in the lower layers are already effective at extracting features for ASR and that freezing them to avoid overfitting can enhance performance. We also confirmed that our method had the best performance in all cases."}, {"title": "V. AUTOMATIC SPEAKER VERIFICATION", "content": "ASV aims to verify the identity of speakers. Given an enrollment utterance and a test utterance, the goal is to determine whether these two utterances are from the same speaker. This paper focuses on text-independent speaker verification, which has no constraints on speech content. For this task, speaker-dependent features that are robust to changes in speech content and background noise often help improve performance. This section applies ELP-adapter tuning to the ASV task."}, {"title": "A. Datasets and evaluation metrics", "content": "The VoxCeleb1 dataset [56] is used for training and testing. It consists of 351 hours of audio data extracted from videos uploaded to YouTube. The training set consists of 148,642 audio utterances from 1,211 speakers. The test set consists of 37,611 trials built from 4,874 audio utterances from 40 speakers.\nThe evaluation metric is the equal error rate (EER), which is the error rate at which the false alarm rate (FAR) and the false rejection rate (FRR) are equal. Here, FAR and FRR are given by\n$FAR = \\frac{FP}{FP + TN}, FRR = \\frac{FN}{TP + FN},$\nwhere TP is the number of true positives, FP is the number of false positives, and FN is the number of false negatives."}, {"title": "B. Implementation details", "content": "The downstream head for ASV consists of a small MLP, which has two fully connected layers and an average time pooling layer in between. The number of hidden units is set to 768. The cross-entropy loss with speaker ID labels is used as the loss function, by which models learn to classify speakers. All models are fine-tuned with the Adam optimizer for 20.8k iterations. The batch size is determined adaptively at each iteration to fit as much data as possible into 16 GB of GPU memory. The learning rate is scheduled with the linear warmup scheduler with $N_{warm} = 5,000$. In the verification phase, speaker embeddings are extracted from the average time pooling layer by omitting the final fully connected layer. For each trial, the cosine similarity between the speaker embeddings for the enrollment and test utterances is measured to determine whether the two utterances are from the same speaker, with the threshold set such that FAR and FRR are"}, {"title": "C. Comparison with conventional methods", "content": "Table III compares ELP-adapter tuning with the four conventional fine-tuning methods. As shown, our method has the best performance for HuBERT, ContentVec, WavLM, and WavLM+. This advantage is derived from the use of L-adapters, which connect the outputs of each layer to the downstream head. As discussed in the ASR experiments, features in the upper layers tend to be speaker-independent. Therefore, connecting the lower layers to the downstream head helps to improve the extraction of speaker-dependent features, which is essential for ASV. With wav2vec 2.0, speaker information and prosodic information could be entangled even in the upper layers, making simple encoder adaptation with conventional efficient adapter tuning the best solution.\nWith full fine-tuning, HuBERT performed the best and WavLM+ performed the worst, in contrast to the results for ASR. This indicates that the features of WavLM+, especially those of the last layer, are highly speaker-independent. ELP-adapter tuning effectively addresses this limitation, achieving the best performance with WavLM+."}, {"title": "D. Ablation study", "content": "Table IV shows the results of the ablation study. As shown, L-adapter tuning outperformed E-adapter tuning for HuBERT, ContentVec, WavLM, and WavLM+. Notably, L-adapter tuning significantly improves the performance of WavLM+, with a 1.96 point decrease in EER. This supports the above discussion about the effectiveness of L-adapters for ASV. The combination of E-adapters and L-adapters improved performance for all models, and the addition of P-adapter further improved performance for four of the five models (Wav2vec2.0, ContentVec, WavLM and WavLM+). This result is consistent with that for the ASR task."}, {"title": "E. Trade-off analysis", "content": "Figure 4 shows the results obtained with various numbers of fine-tuned layers for full fine-tuning, conventional efficient adapter tuning, and proposed ELP-adapter tuning. As shown, EER decreases as the number of fine-tuned layers increases. In contrast to the ASR results in Figure 3, fine-tuning lower layers improves performance because these layers facilitate the extraction of speaker-dependent non-linguistic features. We also confirmed that our method had the best performance in all cases."}, {"title": "VI. SPEECH EMOTION RECOGNITION", "content": "SER aims to identify the affect of the speaker based on audio utterances. It is often formulated as a classification problem, where the goal is to classify the input audio utterance into predefined emotion classes. For this task, audio features that effectively capture emotional cues in speech, such as tone, pitch, and rhythm, are crucial for enhancing accuracy. This section applies ELP-adapter tuning to the SER task."}, {"title": "A. Datasets and evaluation metrics", "content": "The IEMOCAP dataset [59] is used for training and testing. It consists of 12 hours of audio data collected from 10 actors (5 male, 5 female) performing scripted and spontaneous dialogues. Following previous studies [60], four emotion categories, namely \u201cneutral\u201d, \u201chappy\u201d, \u201csad\u201d, and \u201cangry\u201d, are used for evaluation, where \u201cexcited\u201d is merged into \u201chappy\u201d. The evaluation metric is the error rate (ER), which is given by\n$ER = 1 - \\frac{1}{C} \\sum_{i=1}^{C} ACC_i,$\nwhere C = 4 is the number of emotion classes and $ACC_i$ is the accuracy for the i-th class. Five-fold cross-validation is performed to measure ER."}, {"title": "B. Implementation details", "content": "The downstream head for SER consists of a small MLP, which has two fully connected layers and an average time pooling layer in between. The number of hidden units is set to 256. The cross-entropy loss is used as a loss function. All models are fine-tuned with the Adam optimizer for 2,750 iterations. The batch size is set to 32. The learning rate is scheduled with a step scheduler, which is given by\n$n_t = n_0 \\cdot (\\gamma^{\\lfloor \\frac{t}{s} \\rfloor}),$\nwhere $\\gamma = 0.1$ and $s = 10.$"}, {"title": "C. Comparison with conventional methods", "content": "Table V shows that ELP-adapter tuning outperforms the four conventional fine-tuning methods. For SER, it is necessary to comprehensively extract prosodic information such as tone, pitch, and rhythm. Similar to the case for ASV, L-adapters helped to extract non-linguistic features from lower encoder layers.\nBecause most self-supervised models are trained to find hidden audio units in an unsupervised manner on clean non-emotional speech data, which often leads to the formation of"}, {"title": "D. Ablation study", "content": "Table VI shows the results of the ablation study. As shown, L-adapter tuning outperformed E-adapter tuning for all models. Similar to ASV, this result indicates that features extracted from lower layers are useful for SER because they often represent low-level features such as pitch and tone, which help to discriminate emotions. The combination of multiple types of adapter further improved the performance. This result is consistent with those for ASR and ASV."}, {"title": "E. Trade-off analysis", "content": "Figure 5 shows the results obtained with various numbers of fine-tuned layers. ER decreases as the number of fine-tuned layers increases. This tendency is similar to that observed for ASV, but unlike for ASV, the error curve does not exhibit a sharp bend. This indicates that the high-level linguistic features in upper layers effective for ASR are also beneficial for SER. It was also confirmed that the proposed method outperforms the conventional methods in all cases."}, {"title": "VII. SPEECH INTENT CLASSIFICATION", "content": "SIC aims to identify the purpose behind an audio input. It requires understanding and categorizing the intent into predefined classes. For this task, features that capture semantic information often play a critical role in improving performance. In this section, we apply ELP-adaptor tuning to the IC task."}, {"title": "A. Datasets and evaluation metrics", "content": "The Fluent Speech Commands dataset [61"}]}