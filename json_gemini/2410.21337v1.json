{"title": "Fine-tuned Large Language Models (LLMs): Improved Prompt Injection Attacks Detection", "authors": ["Md Abdur Rahman", "Fan Wu", "Alfredo Cuzzocrea", "Sheikh Iqbal Ahamed"], "abstract": "Large language models (LLMs) are becoming a popular tool as they have significantly advanced in their capability to tackle a wide range of language-based tasks. However, LLMs applications are highly vulnerable to prompt injection attacks, which poses a critical problem. These attacks target LLMs applications through using carefully designed input prompts to divert the model from adhering to original instruction, thereby it could execute unintended actions. These manipulations pose serious security threats which potentially results in data leaks, biased outputs, or harmful responses. This project explores the security vulnerabilities in relation to prompt injection attacks. To detect whether a prompt is vulnerable or not, we follows two approaches: 1) a pre-trained LLM, and 2) a fine-tuned LLM. Then, we conduct a thorough analysis and comparison of the classification performance. Firstly, we use pre-trained XLM-ROBERTa model to detect prompt injections using test dataset without any fine-tuning and evaluate it by zero-shot classification. Then, this proposed work will apply supervised fine-tuning to this pre-trained LLM using a task-specific labeled dataset from deepset in huggingface, and this fine-tuned model achieves impressive results with 99.13% accuracy, 100% precision, 98.33% recall and 99.15% F1-score thorough rigorous experimentation and evaluation. We observe that our approach is highly efficient in detecting prompt injection attacks.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) are increasingly utilized due to their impressive capacity to handle a broad spectrum of linguistic undertakings, as documented in several studies [1-5]. This evolution introduces an exciting prospect for application development, blending traditional programming techniques with LLMs functionalities [6-9]. The popular transformer-based LLMs are GPT-3 [10], GPT-4 [11-12], and PaLM-2 [13] which possess amazing generative power that are frequently employed in the backend of many LLM based Integrated Applications, which are mostly popular real-world applications. Nonetheless, these models are especially prone to prompt injection attacks, a serious vulnerability highlighted in recent research for LLMs.\nThese attacks occur when an application employs a LLM based application to interpret a query comprising both an instruction (prompt) and additional data, and the malicious data is nefarious enough to manipulate LLM's operational output. This form of exploitation can alter the intended responses which enable unreliable sources to manipulate the outcomes.\nSuch threats are particularly severe in applications incorporating LLMs, as they allow malicious inputs to dominate the response mechanism of the LLM either entirely or partially. Highlighting the severity of this security issue, the Open Web Application Security Project (OWASP) has ranked malicious prompt exploitation at the forefront of their concerns in their latest evaluation of the top 10 risks facing applications that integrate LLMs [14].\nOur focus is on safeguarding applications through efficient detection that integrate LLMs against rapid injection attacks. Typically, LLMs are utilized either through applications in various ways like an API or through online chat interfaces on websites. This constraint narrows our scope, as the common query structure from an application to an LLM is denoted as P + D, where P represents a variable input which could originate from any source and D is a stable prompt devised by the developer. In this setup, while D may vary during execution, P remains constant and is embedded within the application's source code.\nIn the past, researchers have explored numerous methods, such as traditional machine learning and more advanced techniques like quantum machine learning, to detect and mitigate cyberattacks effectively [15-20]. Moreover, many present diverse approaches for efficient data management, including event-based lossy compression for data streams, distributed query systems for sensor networks, privacy-preserving frameworks [21-26]. These approaches have demonstrated success in identifying vulnerabilities and preventing threats. However, the focus has now shifted toward integrating Large Language Models (LLMs) into cybersecurity solutions [26-28]. LLMs are being leveraged for their ability to process"}, {"title": "II. OWASP TOP 10 LLM VULNERABILITIES", "content": "The OWASP Top 10 list for Large Language Models (LLMs) identifies critical vulnerabilities that can pose significant security risks to applications utilizing these advanced technologies. Key vulnerabilities include Prompt Injection, where attackers manipulate input prompts to gain unauthorized access, and Insecure Output Handling, which results from inadequate validation of LLM-generated outputs. Additionally, Training Data Poisoning involves tampering with training data, leading to biased or harmful responses. Other threats, such as Model Denial of Service and Supply Chain Vulnerabilities, can disrupt services or compromise system integrity through malicious components or datasets. Mitigating these risks is crucial to ensure the reliability, accuracy, and ethical standards of LLMs.\nOther vulnerabilities include Sensitive Information Disclosure, where LLMs inadvertently expose confidential data, and Insecure Plugin Design, which allows untrusted inputs to exploit system weaknesses. Excessive Agency highlights the risks of granting too much autonomy to LLMs, potentially resulting in unintended outcomes. Moreover, Overreliance on LLM outputs without critical assessment can lead to flawed decision-making and security issues. Finally, Model Theft poses a severe threat, as unauthorized access to proprietary models can result in the loss of competitive advantage and sensitive information. This list serves as a comprehensive guide for developers and organizations to recognize these vulnerabilities and implement necessary countermeasures to secure LLM applications effectively."}, {"title": "III. PROMPT INJECTION", "content": "In addition to direct prompt injection, LLMs can also be vulnerable to indirect prompt injection attacks that manipulate the external environment or context of model. Addressing this challenge requires fine-tuning strategies that focus on secure data source interactions.\nFine-tune the LLM to be more discerning of external data sources, such as websites or files, that may contain embedded malicious prompts. Develop robust input validation mechanisms to identify and block potentially compromised external content.\nFine-tune the LLM to be more aware of potential prompt leakage, where the model inadvertently reveals sensitive information about its internal prompt or training data. Incorporate detection mechanisms to identify and mitigate such instances of unintended disclosure.\nThe landscape of prompt injection attacks is rapidly evolving, requiring a continuous effort to fine-tune and improve the security of pre-trained LLMs. Ongoing research, community collaboration, and technological advancements are essential to stay ahead of these sophisticated threats and ensure the safe and trustworthy deployment of LLMs.\nIndirect prompt injection attacks exploit the design of large language models (LLMs), which process external inputs alongside instructions without distinguishing between them. Attackers insert malicious code into these inputs, leading LLMs to generate harmful or misleading outputs. This type of vulnerability is particularly insidious, as it involves subtle manipulation of input data rather than direct tampering with the LLM's core or code. Detecting and mitigating these attacks requires advanced techniques to identify harmful content within inputs, emphasizing the need for robust security measures focused on both the model and the integrity of incoming data to ensure reliable outputs (Fig. 1)."}, {"title": "IV. LARGE LANGUAGE MODELS", "content": "Large language models (LLMs) have significantly advanced in their capability to tackle a wide range of language-based tasks. They leverage sophisticated architectures, primarily the transformer model, to process vast amounts of textual data, capturing intricate relationships among words and phrases. This capability allows LLMs to perform various tasks. The training of LLMs occurs through unsupervised and self-supervised learning techniques, utilizing extensive datasets to refine their language understanding. These capabilities make LLMs highly effective in applications such as chatbots, content"}, {"title": "V. FINE-TUNING LLM", "content": "Large language model (LLM) fine-tuning is a process that takes pre-trained models and adapts them for specific tasks or domains using smaller, specialized datasets. While initial LLM training is typically unsupervised (using unlabeled data), fine-tuning is a supervised process that uses labeled data to refine the model's performance and better align it with human expectations.\nThe goal of fine-tuning is to transform general-purpose language models into specialized models suited to unique applications. By bridging the gap between generic pre-trained models and specific requirements, fine-tuning ensures that models can handle particular tasks effectively. Supervised fine-tuning involves updating the model with labeled data, which allows it to learn and perform tasks more accurately. The process of preparing data for fine-tuning often involves converting general datasets into instruction-based datasets. For instance, a large set of Amazon product reviews can be restructured as instruction prompts. Once the data is organized, it is typically divided into training, validation, and test splits, similar to standard supervised learning methods.\nDuring the fine-tuning process, the model is exposed to prompts from the labeled training data. It generates responses and compares them to the actual labels to calculate an error, which is then used to adjust the model's internal parameters (weights). These adjustments are guided by optimization algorithms, such as gradient descent, to minimize the error and refine the model's understanding of the task. With each iteration over the training data, the model improves by learning the nuances and specific patterns of the new dataset. This helps in adapting the model's general knowledge to become more specialized and effective for the desired task."}, {"title": "VI. DATASET", "content": "We have used two datasets and sourced from HuggingFace which are specifically developed for analyzing prompt injection attacks. The training dataset and test dataset have 546 instances, 116 instances respectively. Both datasets contains two attributes: a text attribute representing the malicious and legitimate prompt texts (string) and a label attribute (int-64) indicating whether the prompt is malicious or legitimate. The label is typically assigned a value of 0 for legitimate prompts and 1 for malicious prompts. The dataset serves as the foundation for training and evaluating the model to detect malicious prompt injections. By providing a diverse range of prompt examples with corresponding labels, the dataset enables the model to learn and distinguish between malicious and legitimate prompts effectively, which enhances the ability to identify and mitigate security risks in AI systems."}, {"title": "VII. METHODS", "content": "In this study, we focus on fine-tuning large language models (LLMs) to classify prompt injection attacks-adversarial inputs designed to manipulate LLM-based systems. Our goal is to create a reliable classification system that distinguishes between legitimate and malicious prompts. We utilize the XLM-ROBERTa model for its strong performance in text classification tasks and ability to handle multilingual data. The fine-tuning process involves several steps (Fig. 2). First, we load a dataset from the HuggingFace library and apply BERT tokenizer, and early stopping to prevent over-fitting. This helps standardize input data which ensures consistent performance. Next, we train the model on the labeled dataset over multiple epochs, and adjust hyperparameters such as learning rate and batch size to optimize accuracy.\nAfter training, the model is evaluated using 116 test samples to measure its ability to classify both legitimate and injection prompts. Standard metrics like accuracy, precision, recall, and F1-score are used to assess the performance of model. Fig. 2 illustrates the full architecture, and further details are discussed in the experiments section. Finally, We will evaluate the accuracy, precision, recall, and F1-score of fine-tuned model as well as non-fine-tuned model on a test set to scale its effectiveness in detecting malicious injected prompts."}, {"title": "VIII. RESULTS AND DISCUSSION", "content": "Accuracy is calculated using the following formula:\nAccuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$\nwhere:"}, {"title": "A. Accuracy Metrics", "content": "$\\frac{TP}{TP + FP}$\nRecall assesses the model's ability to identify actual positive cases:\nRecall = $\\frac{TP}{TP + FN}$\nThe F1-Score is a metric that combines precision and recall to provide a balanced measure of a model's performance, with values ranging from 0 (poor) to 1 (excellent):\nF1-Score = 2 $\\times$ $\\frac{Precision \\times Recall}{Precision + Recall}$\nThese metrics are critical in evaluating the effectiveness of the prompt injection detection system in identifying malicious inputs."}, {"title": "B. Results", "content": "For the experiment, we used the pre-trained XLM-ROBERTa model which is an enhanced version of BERT from the HuggingFace library without fine-tuning. The zero-shot classification pipeline was used to evaluate how it performs prompt classification on the testing dataset without any fine-tuning. After assessing the model, we analyzed the classification results. The testing dataset yielded an accuracy of 55.17%, precision of 55.13%, recall of 71.67%, and an F1 score of 62.32%. These metrics indicate the model's ability to classify prompts is not effective across multiple languages.\nSecondly, the LLM, XLM-RoBERTa, model was fine-tuned on the training dataset across 50 epochs and then evaluated using the testing dataset which has 116 samples. Fig. 3. illustrates the accuracy of the proposed fine-tuned BERT for prompt injections detection over epochs. The results from the first 10 epochs demonstrate a rapid improvement in performance metrics, with accuracy, precision, recall, and F1 score increasing notably in the initial epochs. During the first epoch, the model achieved an accuracy of 99.11%, precision of 100%, recall of 98.33%, and an F1 score of 99.14%. By the second epoch, the accuracy remained stable at 99.11%, but precision reached 100%, and recall slightly dropped to 98.28%, giving an F1 score of 98.30%. In the third epoch, both accuracy and F1 score peaked at 99.14% and 99.16%, respectively, with perfect precision (100%) and recall of 98.33%. This high performance continued through the following epochs, with minimal fluctuations in the precision and recall values. Fig. 4. shows the four line plots to express the performance metrics of the proposed fine-tuned BERT over 50 epochs. Also, confusion matrix is provided values of true positive, true negative, false positive, and false negative.\nBy the 10th epoch, the model achieved its highest performance, with an accuracy of 99.14%, precision of 100%, recall of 98.33%, and an F1 score of 99.16%. These results demonstrate that the model had converged to a high level of classification accuracy early in the training process, with only slight improvements in subsequent epochs.\nIn the final 10 epochs (epochs 41-50), the model's performance metrics stabilized. From epoch 41 onward, accuracy remained constant at 98.27%, with precision, recall, and F1 score all plateauing at 98.33%. Despite the training loss continuing to decrease marginally, these core performance metrics showed no further improvement. This indicates that the model had fully converged by this point, with no significant gains in classification performance in the later stages of fine-tuning. Overall, the fine-tuned XLM-ROBERTa model significantly outperformed its zero-shot counterpart, achieving higher accuracy and more stable performance in prompt classification tasks after fine-tuning. Moreover, as we implemented different models like ML, Non-Fine-tuned and Fine-tuned Models, Fig. 5. shows the bar diagram to express the performance metrics.\nIn our work, we compared the performance of the XLM-ROBERTa model for prompt classification in both non-fine-tuned, fine-tuned settings, and existing simillar models in similar tasks of other researchers in TABLE II. The non-fine-tuned XLM-ROBERTa achieved an accuracy of 55.17%, which is lower compared to models like Deep Learning (DistilBERT) (63.76%). Similarly, models like TCNN (88.08%) and TCNN-URG (89.84%) on the Weibo dataset also outperformed the non-fine-tuned version of our model.\nHowever, when we fine-tuned XLM-ROBERTa, the accuracy improved dramatically, outperforming models such as Multi-lingual BERT with embedded dataset which achieved accuracy 96.55% [33] when applied to prompt-injection same datasets. Fig. 6 illustrates the accuracy of our implemented models with existing works. This result highlights the importance of fine-tuning in improving model performance, especially for specific classification tasks. Our fine-tuned model demonstrates significant advancements over previous works and making it one of the top-performing models in prompt-based classification tasks. This indicates that fine-tuning large language models (LLMs) is essential for achieving highest performance in OWASP recommended vulnerabilities detection and prevention."}, {"title": "IX. CONCLUSION", "content": "In this work, we study the impacts of using various BERT for prompt injection attacks prevention. We propose a method to fine-tune pre-trained XLM-ROBERTa model to detect prompt injections using test dataset without any fine-tuning and evaluate it by zero-shot classification. Then, this proposed work will apply supervised fine-tuning to this pre-trained LLM using a task-specific labeled dataset from deepset in huggingface, and this fine-tuned model demonstrates impressive results with 99.13% accuracy. From a efficiency perspective, dataset could be more robust that we used from Hugging Face. But we showed the ways of applying LLM for the detection and defense with qualityful training samples. We hope our work can raise the awareness of researchers for ensuring the perfect detection with more samples of the training dataset, and we aim more works for the defence of OWASP recommended vulnerabilities."}]}