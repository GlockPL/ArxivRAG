{"title": "CoT-ICL Lab: A Petri Dish for Studying Chain-of-Thought Learning from In-Context Demonstrations", "authors": ["Vignesh Kothapalli", "Hamed Firooz", "Maziar Sanjabi"], "abstract": "We introduce CoT-ICL Lab, a framework and methodology to generate synthetic tokenized datasets and systematically study chain-of-thought (CoT) in-context learning (ICL) in language models. CoT-ICL Lab allows fine grained control over the complexity of in-context examples by decoupling (1) the causal structure involved in chain token generation from (2) the underlying token processing functions. We train decoder-only transformers (up to 700M parameters) on these datasets and show that CoT accelerates the accuracy transition to higher values across model sizes. In particular, we find that model depth is crucial for leveraging CoT with limited in-context examples, while more examples help shallow models match deeper model performance. Additionally, limiting the diversity of token processing functions throughout training improves causal structure learning via ICL. We also interpret these transitions by analyzing transformer embeddings and attention maps. Overall, CoT-ICL Lab serves as a simple yet powerful testbed for theoretical and empirical insights into ICL and CoT in language models. The code is available at: https://github.com/kvignesh1420/cot-icl-lab.", "sections": [{"title": "Introduction", "content": "Transformer-based language models [1] have demonstrated remarkable capabilities in tasks requiring emergent reasoning behaviors, such as few-shot ICL [2, 3] and CoT prompting [4, 5, 6]. In-context learning refers to a phenomenon wherein language models generalize to new tasks by conditioning on a small number of input-output examples without explicit parameter updates [7]. Meanwhile, chain-of-thought prompting augments the input with explicit intermediate reasoning steps that can guide the model's generative process toward more accurate solutions [4]. Despite the substantial performance gains witnessed in various natural language processing tasks [8], the precise mechanisms and architectural factors driving ICL and CoT remain only partially understood.\nRecent studies have ventured into controlled synthetic tasks to understand how transformers learn in-context [9, 10, 11]. These works often rely on real values examples consisting of single-input and single-output pairs, and study if the transformers can learn linear/non-linear function classes. While these toy tasks facilitate theoretical analysis, they leave open the question of whether the findings readily extend to more complex or compositional settings, especially pertaining to natural language. In a parallel line of research, investigations of CoT prompting often rely on short, human-annotated explanations or heuristics, thereby limiting the variety and control of \"reasoning\u201d processes considered [12, 13, 14, 15]. Although such strategies have yielded valuable insights, there does not exist a setup that unifies ICL and CoT and facilitates systematic probing of different aspects of complexity-ranging from vocabulary size and chain length (i.e, the number of tokens involved in reasoning) to the shape and sparsity of dependencies between tokens.\nIn this work, we introduce CoT-ICL Lab, a tokenized synthetic dataset generation framework, that is specifically designed as a \"petri dish\" for studying how transformer-based models acquire chain-of-thought reasoning in-context. Our framework differs from existing works in three main ways:\n1. Tokenized setup akin to language. Unlike many purely numeric toy tasks, we consider inputs and chain tokens in a discrete token space (i.e, a custom vocabulary V). This setup aligns closely with natural language prompting and facilitates complexity control via vocabulary size."}, {"title": null, "content": "2. Decoupled structure and token processing functions. We represent the causal structure of the chain via a directed acyclic graph (DAG) and implement token processing via arbitrary MLP transformations of the corresponding \u2018unknown' data embeddings $E_{data} \\in \\mathbb{R}^{|V| \\times d}$. This separation grants flexibility in controlling problem difficulty \u2014e.g., by manipulating chain length, number of edges in the DAG, depth and activation functions in MLPs and the dimension of data embeddings d.\n3. Multi input-output ICL examples. A majority of the efforts which study ICL in transformer models rely on (real valued) single input-output examples in-context [9, 11, 16]. Our setup addresses such limitations and allows researchers to use tokenized multi input-output examples in-context, which is closer to practice. To the best of our knowledge, this is the first work to introduce and analyze transformer models in such controlled settings.\n4. Ablation-friendly design. By varying one component at a time (vocabulary, number of input tokens or chain tokens per example, DAG connectivity, MLP complexity, or the underlying transformer architecture), researchers can precisely identify which facets of the problem most challenge the model's ICL and CoT capabilities.\nIn addition to proposing the CoT-ICL Lab framework, we also showcase how it can be used to gain insights into the abilities of decoder-only transformer models in ICL with and without CoT. Specifically:\n\u2022 Transformer-models undergo phase transitions in accuracy while training on ICL problems. Such transitions are facilitated by model size, availability of more examples in-context and CoT prompting.\n\u2022 We empirically show that the phase transition correlates with the alignment between model's token embeddings and the data/language embeddings $E_{data}$. Furthermore, when reducing problem complexity by utilizing a finite token processing functions, we noticed that the attention maps of the model capture the underlying reasoning DAG and model excels at ICL.\n\u2022 In essence, we highlight an interplay between the problem complexity induced due to diverse prompt processing functions and the DAG structure. As DAG sparsity reduces and the number of prompt processing functions increase, we observed that larger models tend to adapt to such diversity in ICL problems and leverage CoT to outperform the smaller models. Thus, showcasing the intricacies involved in scaling the model size for ICL performance."}, {"title": "2 Related Work", "content": "In-Context Learning. Initially popularized by GPT-3 [2], in-context learning has garnered extensive attention for its surprising ability to generalize with just a few example prompts. Many investigations center on how transformers might implicitly perform gradient descent or implement other adaptation mechanisms in their hidden activations [9, 10, 11, 17]. See [7, 18] for surveys on the topic. However, these analyses often assume real-valued examples and very simple data distributions, leaving room to explore richer compositional structures that can align with natural language tasks.\nChain-of-Thought. CoT prompting [4, 5, 6, 19] has emerged as an effective technique for eliciting more interpretable (and sometimes more accurate) intermediate reasoning from large language models. Despite empirical successes, debate persists as to whether models truly learn a generalized reasoning algorithm or simply latch onto superficial features [12]. While some efforts [13, 15] systematically study CoT's potential, they often rely on limited or handcrafted tasks that do not fully capture the complexity of multi-step compositional processes.\nSynthetic Tasks for Controlled Model Analysis. Synthetic tasks provide controlled environments that enable precise interventions, ablation studies, and theoretical insights into the model behavior and training dynamics [9, 10, 11]. However, existing synthetic settings generally remain numeric and follow overly restrictive Markovian assumptions [20] (e.g., a single parent for each token). Our proposed CoT-ICL Lab extends these efforts by decoupling the causal structure from token-processing functions. We leverage directed acyclic graphs (DAGs) to control the branching factor in the chain generation, and MLPs for varied levels of token transformations. This design grants extensive configurability, encompassing vocabulary size, multi-input example length, chain length, DAG sparsity, MLP depth, activations and more."}, {"title": "3 Preliminaries and Setup", "content": "Notation. Let {1,\u2026, K} = [K]. We consider a vocabulary V to represent the tokens of a synthetic language. Let F denote a class of functions that are compositional in nature. Formally, a function f \u2208 F is composed of C sub-functions as: $f = f_c \\circ f_{c-1} \\circ ... \\circ f_1$. Given N input tokens $x = (x_1,.., x_N) \\in V^N$, the function f recursively generates C chain tokens $y = (y_1,...,y_C) \\in V^C$ as follows:\n$y_c = f_c(x_1,......,x_N, y_1,......,y_{c-1}), \\forall c \\in [C].$   (1)\nHere $y_{:C-1} = (y_1,..., y_{C-1})$ are treated as the intermediate tokens and $y_c$ as the answer token. The recursive process involves all the input and intermediate tokens to generate the answer token and presents a generalized setup to study CoT. The full notation list is presented in Table 3."}, {"title": "3.1 Compositional Nature of F", "content": "Let G, H denote the causal structure and token processing function classes respectively. In this work, we consider the sub-functions involved in the composition of $f = f_c \\circ f_{c-1} \\circ f_1$ to be formulated as $f_c = h_c \\circ g_c$, where $g_c \\in G, h_c \\in H$. Given input tokens $(x_1,......,x_N)$, the chain tokens in (1) are decomposed into:\n$\\begin{aligned}\ny_c &= f_c (x_1,\u00b7\u00b7\u00b7, x_N, y_1,\u2026\u2026, y_{c-1})\\\\\n&= h_c (g_c(x_1,\u00b7\u00b7\u00b7, x_N, y_1,\u2026\u2026, y_{c-1})), \\forall c\\in [C]\n\\end{aligned}$    (2)"}, {"title": "3.2 Multi-Input ICL and CoT", "content": "Prompt design. We consider a generalized ICL problem of learning $f \\in F$ with (multi) input-output pairs in the token space. An example is defined as a vector of N input tokens and the corresponding answer token, as per (1). A collection of $K \\in \\mathbb{N}$ such examples results in a prompt $p_k(f)$ as follows:\n$p^K(f) = (x_1^{(i)},...,x_N^{(i)}, y_c^{(i)})_{i=1}^K $   (3)\nBy including the intermediate tokens in an example, we obtain a CoT example, which is now a vector of N input tokens, and all the C chain tokens. The corresponding prompt $p^{K}_{COT}(f)$ is given as follows:\n$p^{K}_{COT}(f) = (x_1^{(i)},...,x_N^{(i)}, y_1^{(i)},...,y_C^{(i)})_{i=1}^K$   (4)"}, {"title": "4 CoT-ICL Lab: Data Generation", "content": "In this section, we present details about the synthetic data generation using CoT-ICL Lab and draw parallels with NLP tasks.\nLanguage vocabulary embedding. To create synthetic training and evaluation datasets via the CoT-ICL Lab framework, we consider a vocabulary V of arbitrary size and associate with it a common data embedding matrix $E_{data} \\in \\mathbb{R}^{|V| \\times d}$. Here d denotes the data embedding dimension and the entries are sampled i.i.d from $N(0,1)$. In particular, $E_{data}$ will be leveraged by $h\\in H$ to process embeddings of the tokens and return a new token (see Figure 1).\nCausal structure via DAGs. G is selected to be a class of topologically sorted DAGs whose (1) edge connectivity represents the causality involved in chain generation and (2) whose sparsity controls the usage of input and intermediate tokens. For notational simplicity, we represent DAGs pertaining to our setup as G(M, N, C). We sample one DAG per prompt and use it in the creation of all (CoT-) examples within the prompt. For instance, given input tokens $x_1,x_2,x_3,x_4$ and chain tokens $y_1, y_2, y_3$, we illustrate in Figure 1 a DAG which maps $y_1 \\leftarrow \\{x_1, x_2\\}, y_2 \\leftarrow \\{x_3, x_4\\}$ and $y_3 \\leftarrow \\{y_1, y_2\\}$. The possible structures that can be sampled using a particular choice of M, N, C controls the diversity of causal structures in our dataset of prompts. In the following sections, we experiment with different choices of M, N, C and analyze its impact on model behavior.\nToken processing via MLPs. The function class H is selected to be MLPs whose complexity is controlled by the choice of activations \u03c6 such as ReLU, SiLU, LeakyReLU, Identity, and the depth ranging from $l \\in \\{1,2,3,4,5\\}$. To generate a single chain token, we randomly initialize an MLP based on l, \u03c6 and use it to process the embeddings of the M parent tokens. We take the mean of the M final layer features, apply the activation function again and multiply with $E_{data}$ to obtain a chain token via arg-max. For notational simplicity, we represent MLPs of depth l and activation \u03c6 as $H(l, \u03c6)$ (see algorithm in Appendix A.1). Thus, we sample C MLPs per prompt (one for each chain token) and use them to generate the chain tokens of all K examples within the prompt. In essence, these token processing functions are shared across the ICL examples in each prompt but differ across prompts. Note that unlike prior work [16], we do not consider the intermediate features of the MLPs and only rely on the generated token (see also Table 2 in Appendix E).\nWe present comprehensive details about the (1) distribution of tokens, and (2) the flexibility of our setup in terms of simulating the complexity of real world datasets [8] in Appendix A."}, {"title": "5 Model Training and Evaluation", "content": "Training. We employ NLP style next-token prediction training of decoder only transformer (TF) models [22] with Cross-Entropy (CE) loss. We employ the supervised fine-tuning strategy to compute the CE loss only on the K answer tokens for $p^K$ and on all the K \u00d7 C chain tokens for $p_{COT}^K$ [9, 11, 16].\nEvaluation via accuracy. To measure the ICL ability of a TF model, we measure the accuracy of predicting the answer token of the query (final) example. Formally, we generate the evaluation prompt prefix with K \u2212 1 in-context examples and append the query input tokens $x = (x_1,\u00a8\u00a8\u00a8,x_N) \\in V^N$ at the end. For the query input x, the ground truth chain tokens $\u1ef9 = (\u1ef9_1,\u2026,\u1ef9_C) \\in V^C$ are generated by the recursive formulation given in (1) using a test function \u0192 \u2208 F. Note that there is a difference in model predictions w/ and w/o CoT as follows:\n$\\begin{aligned}\n\u0177_{pred} &:= TF (p^{K-1}(f),x) \t \\text{w/o CoT},\\\\\n\u0177_{pred} &:= TF^{\u25e6C} (p^{K-1}_{COT}(f),x) \t \\text{w/ CoT}.\n\\end{aligned}$   (5)\nHere $TF^{\u25e6C}(.)$ represents the C-step auto-regressive greedy token generation by the model (without any teacher-forcing) as follows [16]:\n$\u0177_C = TF \\big(p^{K-1}_{COT}(f), x, \\underbrace{\u0177_1,..., \u0177_{c-1}}_{previous\\ step\\ outputs} \\big)$   (6)\nIntuitively, when using CoT prompts, we allow the model to generate C - 1 intermediate tokens, followed by the final token $\u0177_{pred} = \u0177_C$. Given an evaluation dataset of T prompts, the accuracy is formulated as accuracy $= \\frac{1}{T} \\sum_{t=1}^T \ud835\udd40_{\u0177_{pred}=\u1ef9_C}.\nOn intermediate tokens. Since we have access to all the ground truth chain tokens using f, we measure the accuracy of predicting them based on \u0177c, \u2200c\u2208 [C]. In fact, in Section 5.2, we show a gradual error propagation phenomenon which results in higher accuracy values on tokens at the beginning of the chain and lower accuracy at the end.\nModels. We create three models TF-4, TF-8, TF-12 with varying depth based on the Llama-3 architecture [23] for our experiments (see Table 1). We ensure that depth is the only varying design factor in the architecture to facilitate a systematic study of the model performance.\nLearning the \u2018unknown' embeddings $E_{data}$. Recall that the fixed embedding matrix $E_{data} \\in \\mathbb{R}^{|V| \\times d}$ which was used to generate the training sequences is unknown to the TF models. To understand the effect of training on the learnable embeddings $E_{TF}$ of the TF models, we measure the subspace similarity between the left singular bases of $E_{data}$ and ETF [24]. Let $d < |V|$ and denote SVD of $E_{data}$ and Err as follows to obtain:\n$\\begin{aligned}\nU_{data} S_{data} V_{data}^T &= E_{data}; \\\\\nU_{TF} S_{TF} V_{TF}^T &= E_{TF}\\\\\nsim(E_{data}, E_{TF}) := \\frac{1}{d} ||U_{data}[:d]U_{TF}[:d]||_F.\\\\\n\\end{aligned}$   (7)\nHere $sim(.,.)$ allows us to measure how well the subspaces of $E_{data}, E_{TF}$ are aligned, and a higher value indicates that the TF model is better learning the token space of the target language embeddings.\nExperimental setup. To present our empirical findings based on training TF models on CoT-ICL Lab, we use the following common parameters to create our datasets. We create a training dataset of size $T = 32 \\times 10^5$, evaluation dataset of size $T = 10^4$ and use $d = 10$ along with H(1, LeakyReLU). In particular, as also mentioned in Section 4, we do not put limitations on the cardinality of G, H. See Appendix B for details on the hardware, training resources and hyper-parameters used for experiments."}, {"title": "6 Conclusion", "content": "In this paper, we introduced CoT-ICL Lab, a framework to generate synthetic Chain-of-Thought prompts and systematically study the role of CoT for In-Context Learning tasks. We used the flexibility and controllability of this framework to (1) generate synthetic multi input-output ICL datasets, (2) design controlled experiments to gain better insights into the role of CoT for ICL and (3) interpret the model behavior via embeddings and attention map analysis. We believe these insights, and many more that could be extracted by experimenting with CoT-ICL Lab, would play a crucial role in better understanding CoT for ICL in NLP, which is of utmost importance for the success of large language models."}, {"title": "7 Limitations", "content": "While CoT-ICL Lab is designed to closely mirror the chain-of-thought process in in-context learning for NLP, we acknowledge that its synthetic nature does not fully capture the linguistic properties of natural language. Specifically, CoT-ICL Lab tokens are not grounded in real-world concepts and therefore do not inherently align with the priors that govern natural language tokens. Consequently, researchers utilizing COT-ICL Lab for experimentation should carefully weigh its advantages such as flexibility and controllability\u2014against its limitations, particularly its synthetic nature, and consider the potential impact on their results."}, {"title": "A.1 Algorithm for Token Processing", "content": "Algorithm 1 formalizes the token processing via H(l, \u03c6) by utilizing the corresponding data embedding matrix $E_{data}$ and generating a single chain token. Formally, given a token embedding $e \\in \\mathbb{R}^d$, the output of the MLP is formulated as:\n$h(e) = W_L (\u03c6 (W_{L-1}(\u2026 (W_1 (e))))), \\t h\\in \u0397(l, \u03c6)$.   (9)\nHere $W_\u03b9 \\in \\mathbb{R}^{d \\times d}, \u2200l \\in [L]$ denote the linear layers whose width is kept constant (d) across layers, and whose entries are sampled from $N(0, 1)$. The usefulness of CoT-ICL Lab lies in its flexibility to modify Algorithm 1. For instance:\n1. Future efforts can explore non-random $E_{data}$ and also modify Step 3 to employ more complex function classes beyond just MLPs.\n2. One can also explore feature aggregation techniques (Step 5-6) when scaling to $(|V| > 1024)$.\nAlgorithm 1 Generate a single chain token yc\nRequire: M parent token embeddings ${e_{data}^i}_{i=1}^M$ from the data embedding matrix $E_{data}$, choices of depth l and activation functions \u03c6,\n1: Initialize MLP $h_c \\in H(l, \u03c6)$\n2: for i = 1 to M do\n3:   $h^i \\leftarrow h_c(e^i_{data})$   {Process each parent embedding}\n4: end for\n5: $h_{mean} \\leftarrow \\frac{1}{M} \\sum_{i=1}^M h^i$   {Mean of final layer features}\n6: $h_{act} \\leftarrow \u03c6(h_{mean})$   {Apply activation again}\n7: $y_c \\leftarrow argmax(E_{data} h_{act})$   {Compute chain token}\n8: return $y_c$"}, {"title": "A.2 Synthetic Datasets and Token Distributions", "content": "To create synthetic training and evaluation prompt datasets via the CoT-ICL Lab framework, we consider a vocabulary V of arbitrary size and the data embedding matrix $E_{data} \\in \\mathbb{R}^{|V| \\times d}$. To create a single prompt, we randomly sample a DAG from G(M, N, C) and sample C MLPs from H(l, \u03c6). The N input tokens per example are sampled uniformly from V and are then used to generate the C chain tokens using (2). Note that $g_c(.)$ corresponds to the M edges of the DAG that map the parent tokens to the chain token (i.e, the filtering function) and $h_c(.)$ corresponds to the token processing function via a MLP (as per Algorithm 1). Creating K such examples/CoT examples gives us a prompt and creating T such prompts gives us the desired synthetic dataset."}, {"title": "A.3 Token Distribution Fits", "content": "To understand the distribution of tokens in our synthetic datasets, we sample 10,000 CoT prompts with |V| = 64, $d = \\{10, 20, 30, 40\\}$, H(1, LeakyReLU), N = 4, M = 4, C = 2, K = 40 and plot the distribution of input tokens and chain tokens in Figure 10. Observe that the distribution of chain tokens in the sequences exhibit a decay that depends on d. By utilizing the powerlaw python package [27] to fit power-law and log-normal distributions to the chain tokens frequencies, we quantify that the distribution is more likely to be log-normal. See Figure 11 for the d = 40 case with varying \u03c6 as well.\nTo quantify the impact of varying factors (such as V, d, l, \u03c6), we measure the TokenCoverage over all chain tokens in the dataset as follows:"}, {"title": "A.4 TokenCoverage by MLP depth l", "content": "To illustrate the role of depth l of the MLPs in generating the chain tokens, we sample T = 10,000 CoT prompts with |V| = 1024, $d = \\{10,20,30,40\\}$ and H(l, \u03c6). We choose $l = \\{1,2,3,4,5\\}$ and \u03c6 = \\{ReLU, SiLU, LeakyReLU, Identity\\}. Each CoT prompt uses N = 4, M = 4, C = 2, K = 40. Figure 12 illustrates that ReLU, LeakyReLU, Identity functions have approximately the same token coverage for varying l. However, SiLU tends to exhibit lower TokenCoverage for larger l values."}, {"title": "A.5 TokenCoverage by MLP Activation \u0444.", "content": "We vary \u03c6 = {ReLU, SiLU, LeakyReLU, Identity} and plot the TokenCoverage for a similar dataset with T = 10,000 CoT prompts in Figure 13. A key-takeaway is that ReLU and SiLU lead to highly skewed chain token distributions with relatively smaller TokenCoverage for any given dimension d. Nonetheless, all \u03c6 exhibit an increasing trend for larger values of d."}, {"title": "A.6 TokenCoverage in the CoT-Collection NLP Dataset", "content": "To further show that our setup is flexible enough to resemble realistic NLP scenarios in token distribution, we analyze CoT prompts from the CoT-Collection dataset [8] which collates \u2248 1.8 million prompts from a diverse pool of 216 tasks. We analyze the TokenCoverage by tokenizing the reasoning text of the prompts pertaining to each task with the Llama-3.1 tokenizer (|V| = 128256). From Figure 14, notice that the tasks span a wide range of the TokenCoverage values with the lowest being < 0.2 and the highest being 1. From the above analysis of simulating chains via different configurations of H, d, we can notice that our setup is flexible enough to replicate the complexity of real-world datasets (in terms of input token lengths and chain lengths) and offer flexibility of simulating even complex datasets which might not be easy to curate."}, {"title": "B Hardware and Hyper-Parameters for Training and Evaluation", "content": "We use the DistributedDataParallel APIs in PyTorch [28] to run each training job on 4 H100 NVIDIA GPUs. Furthermore, since our modeling code leverages HuggingFace APIs, we also employ Liger-Kernels [29] for faster training. We created 3 different models based on the Llama-3 architecture whose details are presented in Table 1. The smallest model TF-4 has \u2248 240M parameters without the embedding layer and the largest model TF-12 has \u2248 730M parameters. For all the training runs, we use a batch size of 64 per rank and the AdamW optimizer with a learning rate 5 \u00d7 10-5. We employ the GenerationConfig API in the transformers library to greedily generate the model predictions without teacher forcing. This API is used for evaluations on checkpoints during the training runs. For larger scale and on-demand evaluations, we provide code examples to leverage VLLM [30] and SGLang [31] based inference."}, {"title": "C Additional Experiments", "content": "C.1 Varying Activation Functions (\u03c6)\nSetup. Considering a vocabulary |V| = 64, d = 10, we employ H(1, \u03c6), N = 4, M = 4, C = 2, K = 40 with varying \u03c6 = {ReLU, SiLU, LeakyReLU}. All models are trained on a dataset with T = 32 \u00d7 105 prompts and evaluated on T = 10,000 prompts.\nLower TokenCoverage leads to higher accuracy. We previously observed from Figure 13 that the activation function (\u03c6) corresponding to H plays a key role in determining the TokenCoverage of chain tokens in the prompts. For instance, when |V| = 64, the TokenCoverage with Identity and SiLU is relatively larger than ReLU. This implies that the number of unique tokens that a model would have to correctly predict is relatively lower in the ReLU case. We can observe from Figure 15 that such smaller coverage can indeed result in higher evaluation accuracy across all model sizes.\nC.2 Vary Data Embedding Dimension (d)\nBased on the token distribution analysis, we have noticed that the TokenCoverage increases monotonically with d for various choices of H(l, \u03c6) in Figure 12. As a result, Figure 16 shows that larger coverage makes it difficult for the TF models to attain high evaluation accuracy as they have to now correctly predict a larger fraction of tokens in V."}, {"title": "C.3 Vary Number of (CoT-) examples K", "content": "In Section 5.1, we observed that more examples in-context can help smaller models to perform on-par with bigger models. To this end, we plot in Figure 17 the accuracy by varying K = {10,20,30}. For simplicity we choose a smaller vocab size of |V| = 64 and show that smaller K can hurt performance even when CoT is employed across all model sizes. Additionally, larger models tend to outperform the smaller one TF-4 in the extreme case of K = 10 without any CoT. Next, by increasing the vocabulary size to |V| = 1024, we observe from Figure 18 that TF-8, TF-12 leverage their depth and outperform TF-4 by utilizing CoT when K = 30. For K = 10, 20, the problem turns out to be harder even for these bigger models."}, {"title": "C.4 Longer Training", "content": "We follow the same setup of Section 5.1 and explore the impact of longer training on the accuracy of the TF models. In particular, we train on 3\u00d7 more steps, which results in 96 \u00d7 105 (CoT-) prompts per dataset. Observe from Figure 19a that by training beyond 32 \u00d7 105 prompts, even the TF-4 model exhibits"}, {"title": "C.5 Ablations with fixed DAGs and Token Processors", "content": "Vary V. Following the same setup as Section 5.1 with varying vocabularies, we consider the ablations with a fixed DAG and C MLPs. We observe from Figure 20 that when the DAG is fixed across all prompts, there is a slight increase in the evaluation accuracy when compared to the random DAG per prompt case in Figure 3. However, notice from Figure 21 that fixing the C MLPs for all prompts facilitates the models to learn the causal structure with ease and results in very high accuracy when CoT is enabled. Furthermore, notice that for non-CoT datasets, TF-8 (Figure 21b) and TF-12 (Figure 21c) reach higher a accuracy than the smaller TF-4 model (Figure 21a) for |V| = {512, 1024}.\nVary M. As a follow up of Section 5.4, we show in Figure 22 that when the DAG is fixed across all prompts, the TF-12 model exhibits a transition to higher accuracy (with M = 1) towards the end of training (Figure 22c), which is not observed in the case of TF-4 (Figure 22a) and TF-8 (Figure 22b). On the other hand, when we fix the C token processors, Figure 23 shows that all models can achieve a perfect accuracy of 1 with M = 1 and CoT. Interestingly, similar to the previous case of varying V, we observe that the TF-8 (Figure 23b) and TF-12 (Figure 23c) models reach a higher accuracy on the non-CoT datasets pertaining to M = {2,3}, when compared with the smaller TF-4 model (Figure 23a)."}, {"title": "D Interpreting the Attention Maps", "content": "To interpret the attention maps, we consider the same ablation setup as Section 5.4 in the main text with fixed token processing functions and consider M = 1, |V| = 1024 and the TF-4 model for simplicity. Let the tokens $x_1,x_2,x_3, x_4$ and $y_1, y_2, y_3, y_4$ represent the input and chain tokens of a CoT example respectively. We consider a validation prompt with the DAG structure (i.e the parent tokens) for the 4 chain tokens given by: $y_1 \\leftarrow \\{x_4\\}, y_2 \\leftarrow \\{x_1\\}, y_3 \\leftarrow \\{y_1\\}, y_4 \\leftarrow \\{y_2\\}$ for the analysis. Now, given such a validation prompt prepared with K = 40, we auto-regressively generate the 4 chain tokens and consider the attention maps used for generating the last chain token (i.e the answer token). We take the mean of all attention maps across the heads in a layer and plot the last 64 rows and columns in Figure 24. Furthermore, we also plot such attention maps for the M = 2 case in Figure 25 and the M = 3 case in Figure 26 with the DAGS described in the captions.\nQualitatively, we observed that even for the relatively difficult setup of M = {2,3}, larger attention scores for the chain tokens in any example are placed on the parent tokens in that particular example itself (similar"}, {"title": "E Comparison with Related Work", "content": "ICL with real-valued examples. Analyzing the ICL capabilities of transformer models with synthetic data has gained massive attention in recent years. In particular, the notion of using these models as \"statisticians\" which can learn and approximate arbitrary function classes on real valued inputs has been widely explored [9, 10, 11, 32, 33, 34", "7": ".", "K": ".", "16": "explored the role of CoT for learning the MLP function classes. More importantly, they evaluate the CoT outputs of the transformers without teacher-forcing and show how this decomposition can facilitate a hierarchical layer-wise learning of the MLPs. On the other hand, several works have explored simple tokenized settings which employ markov chain like causal structures to study the attention maps and training dynamics of shallow transformers [17, 20, 26, 35"}]}