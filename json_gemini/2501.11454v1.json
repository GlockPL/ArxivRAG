{"title": "Improving thermal state preparation of Sachdev-Ye-Kitaev model with reinforcement learning on quantum hardware", "authors": ["Akash Kundu"], "abstract": "The Sachdev-Ye-Kitaev (SYK) model, known for its strong quantum correlations and chaotic behavior, serves as a key platform for quantum gravity studies. However, variationally preparing thermal states on near-term quantum processors for large systems (N > 12, where N is the number of Majorana fermions) presents a significant challenge due to the rapid growth in the complexity of parameterized quantum circuits. This paper addresses this challenge by integrating reinforcement learning (RL) with convolutional neural networks, employing an iterative approach to optimize the quantum circuit and its parameters. The refinement process is guided by a composite reward signal derived from entropy and the expectation values of the SYK Hamiltonian. This approach reduces the number of CNOT gates by two orders of magnitude for systems N > 12 compared to traditional methods like first-order Trotterization. We demonstrate the effectiveness of the RL framework in both noiseless and noisy quantum hardware environments, maintaining high accuracy in thermal state preparation. This work contributes to the advancement of a scalable, RL-based framework with applications for computations of thermal out-of-time-order correlators in quantum many-body systems and quantum gravity studies on near-term quantum hardware.", "sections": [{"title": "Introduction", "content": "The holographic duality [1] establishes a profound connection between a specific class of quantum field theories in d dimensions and quantum gravity in d + 1 dimensions. This strong/weak duality allows the exploration of strongly coupled field theories via classical supergravity and vice versa. Despite its power, there are no known cases where both sides of the duality can be simultaneously and analytically solved. The Sachdev-Ye-Kitaev (SYK) model, a simplified variant of the Sachdev-Ye model [2], was introduced in a series of seminal talks by Kitaev [3, 4]. This model exhibits holographic properties and is particularly amenable to study in the strong-coupling limit.\nThe SYK model consists of N Majorana fermions in 0+1 dimensions with random couplings between q fermions, drawn from a Gaussian distribution with zero mean and variance proportional to $J^2/N^{q-1}$. In the large-N and low-temperature limit N > BJ \u226b 1, where \u03b2 is the inverse temperature and J the disorder strength-the model displays an approximate conformal symmetry and is related to near-extremal black holes with a nAdS2 (near-AdS2) geometry. Meanwhile, the SYK model saturates the chaos bound [5], a key feature of holographic systems. Moreover, SYK model is a prominent example of a fast scrambler [6], known for its logarithmic growth of complexity and its saturation of the bounds proposed by the Brown-Susskind conjecture regarding quantum information scrambling in black holes [7]. Its computational tractability in 0+1 dimensions has enabled extensive numerical studies, with systems comprising up to 60 Majorana fermions analyzed [8, 9]. Over the past decade, the SYK model has been extensively studied in both condensed matter and high-energy physics [10, 11, 12]. For a comprehensive review, see refs. [13, 14, 15].\nGiven its significance in holography, detailed investigations of both pure and thermal (Gibbs) states of the SYK model are imperative. The ground state exhibits volume-law entanglement entropy with a known coefficient [16], rendering it classically challenging to prepare [17]. Additionally, a defining feature of the SYK model is the behavior of thermal four-point correlators, particularly out-of-time-order correlators (OTOCs), which saturate the Lyapunov exponent in the large-N and low-temperature regime [5]. Accurate computation of these OTOCs requires preparing the corresponding thermal state, which is a central focus of this study. While low-energy states have been explored via large-scale classical simulations, our research aims to advance the state-of-the-art by employing hybrid quantum-classical algorithms [18, 19]."}, {"title": "Preliminaries", "content": ""}, {"title": "The problem: Dense SYK model", "content": "The Hamiltonian for the full or dense SYK model with N Majorana fermions and q-fermion interaction terms is:\n$H = \\frac{(i)^{q/2}}{q!} \\sum_{i,j,k,...,q=1}^{N} J_{i1i2...iq} X_{i}X_{j}X_{k}*** X_{q},$\nwhere Xi are Majorana fermions satisfying the anticommutation relation {Xi,Xj} = XiXj + XjXi = dij. We focus on the case where q = 4, which corresponds to random all-to-all quartic interactions"}, {"title": "Variational thermal state preparation", "content": "Thermal states, represented by a density matrix p\u00df, are critical for studying quantum systems at finite temperatures. These states are challenging to prepare compared to ground states (\u03b2\u2192 \u221e) or high-temperature states (\u03b2 \u2192 0), which are computationally simpler. The finite inverse temperature regime (3) is particularly important for analyzing quantum many-body systems such as the SYK model.\nTo prepare a thermal state, we minimize the Helmholtz free energy F, given by\n$F = \\langle H \\rangle_\\beta - TS_\\upsilon,$\nwhere T = 1/\u1e9e is the temperature, $\\langle H \\rangle_\\beta = Tr[\\rho_\\beta H]$ is the energy expectation value, where p\u00df is the thermal state at temperature T, and $S_\\upsilon = -Tr[\\rho_\\beta log \\rho_\\beta]$ is the von Neumann entropy. We adopt the algorithm from Ref. [39] whose subroutines are defined in the following."}, {"title": "Variational thermal state preparation with reinforcement learning", "content": "We give an overview of the variational thermal state preparation of the SYK model within the reinforcement learning (RL) framework, for details of the framework see Appendix A. The algorithm is illustrated in Fig. 1. The framework is used to estimate the parameters of the PQC1 and the structure as well as parameters of the PQC2 elaborated in Sec. 2.2. Wherein we present state, the action representations, and the reward function used in this work."}, {"title": "Results", "content": "We employ a reinforcement learning (RL) agent-driven variational quantum thermal state algorithm introduced in Ref. [39]. For the dense SYK model, we consider the first circuit (PQC1, which calculates the entropy) to be constructed using strongly entangling layers [45]. The second circuit (PQC2 which calculates the expectation value) is built using the agent, as described also in the previous section. For the numerical simulations, we use QULACS [46] implementation in a hybrid CPU and GPU interface\nThroughout the paper we use the gradient-free optimizer COBYLA [47] with 103 iterations to optimize the parameters of the PQC1 and the PQC2. The RL framework is running for a maximum of 5 \u00d7 103 episodes the RL environment interacts with the agent for a predefined number of steps. Furthermore, the agent is optimized by the ADAM optimzier [48] with 10-3 learning rate. Further details of the RL-agent can be found in Appendix C and the environment that contains the tensor-based state and the reward function is elaborated in Section 3."}, {"title": "Sampling the best-performing circuits", "content": "As discussed earlier, during the training of 5 \u00d7 103 episodes, the RL agent generates a class of parameterized quantum circuits with varying depths and gate counts, meeting predefined thresholds for free energy and fidelity. Selecting a specific circuit that accurately approximates the free energy, Hamiltonian energy expectation, and entropy is a challenging task. To streamline this process, we employ a filtering subroutine to identify the best-performing circuit. This filter applies a threshold on the errors, defined as:\n$\\epsilon = \\Delta F + w_a \\Delta\\langle H\\rangle_\\beta + w_b \\Delta S,$\nwhere AF represents the error in free energy, $ \\Delta\\langle H\\rangle_\\beta$, scaled by the weight factor wa, corresponds to the error in the Hamiltonian expectation, and AS, scaled by w\u044c, denotes the error in entropy. Among the circuits proposed by the RL agent, the one minimizing \u025b is selected as the best-performing candidate.\nThe parameters wa and w\u044c are dependent on the reward function and the number of qubits."}, {"title": "Noiseless simulation", "content": "In Fig.3, we present the results for the dense SYK model simulations for N = 8,10 Majorana fermions, the results are scaled up to N = 14 in Fig. 4. Each simulation was carried out for five instances, and to benchmark the performance of the RL framework, the agent was retrained for each instance. In each panel of Fig.3 and Fig. 4, we report the free energy, the expectation value of the Hamiltonian, and the entropy. The solid line represents the mean of the exact values, while the green (yellow) region indicates the 16 (20) deviation from the mean. Red dots correspond to the performance of the RL-agent parameterized circuit optimized using the free energy error as a reward signal (Eq. 4). Meanwhile, blue crosses depict the performance of the RL circuit using both the free energy error and the fidelity of the thermal state as reward signals (see Eq. 5). The error bars indicate one standard deviation for the given number of model instances. For 8 < N < 12, the fidelity achieved was in the range 0.80 \u2264 F(p(\u2642)) \u2264 0.98, whereas for N = 14, the fidelity decreased below 0.80 in the range 0.4 < F(p()) \u2264 0.80.\nIn the RL framework, the scaling of the number of gates required is substantially lower compared to the scaling in first-order Trotterization. This efficient scaling enables the preparation of thermal states for increasing numbers of Majorana fermions using a minimal number of 1- and 2-qubit gates. Consequently, this study achieves, for the first time, a variational preparation of thermal states for N = 14 Majorana fermions, demonstrating good agreement in estimating the free energy. Notably, at \u1e9e = 5.2, the RL agent provided the most accurate approximation of the free energy and expectation value compared to other temperatures."}, {"title": "Noise robustness on QPU", "content": "In this section, we study the robustness of the circuits selected by the RL-agent under realistic noisy conditions. The RL-agent is trained in a noiseless environment to optimize the parameterized circuits for minimizing the free energy. After training, the best circuit is selected according to how accurately it can find the free energy and then the circuit is executed on the IBM Eagle r3 processor to assess its practical performance. In the absence of any sophisticated error-mitigation techniques. See Appendix E for further information on the quantum hardware noise and limitations.\nThe results were compared with exact classical calculations and experimental outcomes on the QPU for different temperatures."}, {"title": "Noisy training with constrained qubit connectivity", "content": "In this section, we consider the variational framework is noisy, i.e., each gate in the action space is subjected to quantum noise, leading to a noisy PQC1 and PQC2 where the noise values correspond to the median noise of IBM_brisbane device. As the action space consists of 1- and 2-qubit gates, we add bit flip noise of strength 2.342 \u00d7 10-4 after each application of a 1-qubit gate, and 2-qubit depolarizing noise of strength 8.043 \u00d7 10-3 is applied after each CNOT gate. Hence the RL agent interacts with a noisy environment and receives a noisy reward signal.\nThe results presented in Fig. 6 highlight the impact of noise during the simulation for N = 8 Majorana fermions, executed under the constrained connectivity of the IBM Eagle r3 processor. A key observation is the improved accuracy of the energy expectation value and the entropy in the noisy scenario compared to the noiseless simulations. This improvement may be attributed to the adaptive optimization of the RL agent, which dynamically accounts for noise during the construction of the parameterized quantum circuit.\nHowever, the cumulative accuracy of the free energy estimation diminishes noticeably in the presence of noise. This reduction is due to the cumulative effects of noise during the execution of the circuits. Unlike the energy expectation value and entropy, which are influenced by local noise-induced fluctuations, the free energy calculation depends on a more global accuracy of the thermal state, making it more susceptible to cumulative errors introduced by gate noise and connectivity constraints."}, {"title": "Improvement in CNOT count", "content": "This section presents the improvement in the number of CNOT gates in the RL-agent proposed circuit compared to first-order Trotterization. The improvement is quantified by\n$Improvement = \\frac{CNOTs_{Trotterized}}{CNOTs_{RL-agent}}$\nfor different qubit counts and values of \u03b2. In Tab. 2 the results show that for 5 and 6 qubits, the RL-agent offers over a 102-fold improvement in the number of CNOT gates compared to first-order Trotterization-based Hamiltonian variational ansatz [25], particularly for higher values of \u03b2. This demonstrates the ability of the RL-agent to significantly reduce circuit complexity as the qubit count increases, making it favorable for scaling to higher qubits."}, {"title": "Conclusion and future work", "content": "Summary In this work, we present a novel approach to preparing thermal states of the Sachdev-Ye-Kitaev (SYK) model using a variational quantum algorithm in the reinforcement learning (RL) framework. In the variational algorithm, which plays the role of the environment in the RL framework, the parameterized quantum circuits are encoded in a 3D tensor. In the encoding, the X-axis represents the position of gates, the Y-axis denotes the type of gate, and the Z-direction is the depth. To process such higher-dimensional data, we integrate the environment with a 3D convolutional neural network (CNN) to optimize the preparation of the thermal states for different temperatures.\nThe 3D-CNN is guided by a e-greedy policy, and a composite reward signal helps the agent to choose an action (i.e., a quantum gate) for a specific state of the environment. In this framework we operate in two key scenarios: (1) A fully noiseless environment for systems with up to N = 14 Majorana fermions. (2) A noisy environment with constrained qubit connectivity as per the IBM Eagle r3 processor for N = 8 Majorana fermions, using a noiseless agent.\nThis RL framework has proven highly effective in identifying optimal parameterized quantum circuits for thermal state preparation across a wide range of temperatures. Crucially, our approach significantly reduces the computational overhead associated with this task, making it feasible to prepare thermal states for larger SYK systems, N\u2265 14, which was previously not feasible. Our method significantly reduces the computational overhead associated with preparing thermal states for large SYK systems (N > 12 Majorana fermions) on near-term quantum devices. Key findings of the paper include:\n1. A two order of magnitude reduction in the number of CNOT gates for systems N \u2265 12 compared to traditional methods like first-order Trotterization.\n2. Effective thermal state preparation in both noiseless and noisy quantum hardware environments, maintaining high accuracy.\n3. Successful integration of reinforcement learning with convolutional neural networks to iteratively refine quantum circuits and their parameters.\nThis approach represents a significant advancement in RL-based quantum circuit optimization, with potential applications extending beyond the SYK model to broader quantum simulations and quantum gravity studies on near-term quantum hardware.\nFuture work Building on the success of this study, several promising directions for future research emerge:\n1. Scaling to larger systems: Investigate the scalability of this approach for SYK models with N > 14 Majorana fermions, advancing the limits of current quantum simulations. This can be achieved by leveraging the recently proposed gadget reinforcement learning [54], which enables the learning of composite gate sets from smaller Majorana fermionic SYK models and subsequently applying them to solve models with a larger number of Majorana fermions efficiently.\n2. Out-of-time-order correlations: Apply this method to accurately compute out-of-time-order correlations (OTOCs) for the SYK model, which are crucial for studying quantum chaos and holographic properties.\n3. Exploring reward engineering: Explore other forms of reward function to benchmark the performance of the CNN agent. Such as we can encode the information corresponding to the properties of the thermal state (e.g., the entropy, purity, etc.) to improve the thermal state preparation further.\n4. Quantum information-theoretic analysis Recent quantum informatic research suggests analyzing RL-agent-produced circuits can significantly enhance agent performance [55]. By applying quantum information-theoretic principles to circuit analysis, we can optimize circuit architectures, identify performance bottlenecks, and refine the RL framework's efficiency.\n5. Advanced error mitigation: Implement sophisticated error mitigation techniques such as [56, 50, 53, 57, 51] to improve the accuracy of results in noisy quantum environments, particularly for free energy calculations.\n6. Generalization to other models: Extend the RL framework to optimize quantum circuits for other strongly correlated quantum systems beyond the SYK model."}, {"title": "Reinforcement learning framework", "content": "In the context of Reinforcement Learning (RL), an agent interacts with its environment to learn an optimal policy through a process of trial and error [58]. This interaction can be formally modeled as a Markov Decision Process (MDP), defined by the tuple (S, A, P, R), where:\n\u2022 S represents the state space\n\u2022 A represents the action space\n\u2022P:S\u00d7S\u00d7 A \u2192 [0, 1] defines the transition dynamics\nR:SA \u2192 R describes the reward function of the environment\nIn our work, we consider both the action space A and the state space S to be finite and discrete sets.\nAgent behavior and performance metrics\nThe agent's behavior within the environment is governed by a stochastic policy \u03c0(a|s) : S \u00d7 A \u2192 [0, 1], where a \u2208 A and s\u2208 S. To assess the agent's performance, we use a metric called the return, which is calculated as a discounted sum:\n$G(T) = \\sum_{j=0}^{T-1} \\gamma^{j} r_{t+1},$\nHere, T = (so, ao, ro,..., sT\u22121, aT\u22121, rT\u22121) \u2208 (S \u00d7 A \u00d7 R)T represents the interaction sequence T is a fixed length called the horizon, and y is an environment-specific discount factor. The primary objective of the agent is to determine the optimal policy that maximizes the expected return."}, {"title": "Tensor-based encoding of quantum circuits", "content": "We adopt a binary encoding scheme, as introduced in [28, 29], to capture the structure of PQC2, focusing on the order and arrangement of gates to provide the agent with a complete circuit description. Unlike previous approaches, which flattened the 3D encoding into one dimension for use in feedforward neural networks, our method leverages the full spatial structure of the encoding by employing a 3D convolutional neural network (CNN). This allows us to extract richer spatial features from the encoded representation as shown in the Appendix C.."}, {"title": "CNN outperforms FNN", "content": "We utilize 3D convolution, whose theory can be found elaborately in [61, 27]."}, {"title": "Resource quantization", "content": ""}, {"title": "The IBM Eagle r3 processor", "content": "The free energy and the entropy is calculated on ibm_kyiv, ibm_sherbrooke and ibm_brisbane. They are all 127 qubit devices with the Eagle r3 processor. The qubit connectivity of these devices is shown in Fig. 9. All these machines use the same set of basis gates, which consist of ECR and single-qubit gates ID, RZ, SX, X."}, {"title": "The agent and environment hyperparameters", "content": "The hyperparameter tuning of the reinforcement learning agent corresponds to the tuning of the following parameters:"}, {"title": "The art of sampling best circuits", "content": "The process of selecting optimal quantum circuits from those proposed by the Reinforcement Learning (RL) agent involves a filtering threshold defined as:\n$\\epsilon = \\Delta F + w_a \\Delta\\langle H\\rangle_\\beta + w_b \\Delta S,$\nwhere AF, $ \\Delta\\langle H\\rangle_\\beta$, and AS represent errors in free energy, Hamiltonian expectation, and entropy, respectively. Weights wa and wo adjusts the relative importance of these errors. The best-performing circuit is selected by:\n1. Evaluating each proposed circuit using \u025b.\n2. Selecting the circuit that minimizes \u025b."}]}