{"title": "Vision-in-the-loop Simulation for Deep Monocular Pose Estimation of UAV in Ocean Environment", "authors": ["Maneesha Wickramasuriya", "Beomyeol Yu", "Taeyoung Lee", "Murray Snyder"], "abstract": "This paper proposes a vision-in-the-loop simulation environment for deep monocular pose estimation of a UAV operating in an ocean environment. Recently, a deep neural network with a transformer architecture has been successfully trained to estimate the pose of a UAV relative to the flight deck of a research vessel, overcoming several limitations of GPS-based approaches. However, validating the deep pose estimation scheme in an actual ocean environment poses significant challenges due to the limited availability of research vessels and the associated operational costs. To address these issues, we present a photo-realistic 3D virtual environment leveraging recent advancements in Gaussian splatting, a novel technique that represents 3D scenes by modeling image pixels as Gaussian distributions in 3D space, creating a lightweight and high-quality visual model from multiple viewpoints. This approach enables the creation of a virtual environment integrating multiple real-world images collected in situ. The resulting simulation enables the indoor testing of flight maneuvers while verifying all aspects of flight software, hardware, and the deep monocular pose estimation scheme. This approach provides a cost-effective solution for testing and validating the autonomous flight of shipboard UAVs, specifically focusing on vision-based control and estimation algorithms.", "sections": [{"title": "I. INTRODUCTION", "content": "Unmanned Aerial Vehicles (UAVs) have become integral to modern technology, with applications in military operations, maritime surveillance, search and rescue, and offshore logistics. Despite their advantages such as cost-effectiveness, versatility, and enhanced safety-operating UAVs in complex and dynamic environments like oceans presents significant challenges. Autonomous takeoff, navigation, and landing on moving platforms such as ships remain particularly difficult due to unpredictable maritime conditions, including ship motion, variable lighting, and air wakes [1].\nA crucial factor in enabling safe UAV operations on ships is accurately estimating the UAV's relative 6D pose (position and orientation in 3D space) with respect to the vessel. Traditional methods often rely on Real-Time Kinematic Global Positioning System (RTK-GPS), which provides precise positioning through differential corrections. However, GPS-based systems have limitations, including dependence on continuous communication between the UAV and the ship, susceptibility to signal jamming or spoofing, and the risk of revealing the ship's location through radio transmissions [2]. These constraints highlight the need for alternative approaches, such as vision-based methods that use monocular cameras for relative pose estimation.\nVision-based methods\u2014including fiducial markers like ArUco [3], laser patterns, and feature-based visual-inertial navigation (VIN) [4]\u2014have been explored for UAV landing operations. While effective under controlled conditions, these methods are sensitive to occlusions, lighting variations, and visibility constraints, limiting their robustness in maritime environments. To address these challenges, prior research demonstrated that deep learning-based approaches using a Transformer Neural Network Multi-Object (TNN-MO) architecture outperform conventional methods in estimating a UAV's relative pose using monocular images [5]. This approach improved accuracy in both synthetic and real-world datasets, overcoming several limitations of traditional techniques.\nThe UAV-mounted camera captures images of the real ocean environment, which are then processed by the TNN-MO model to estimate the camera's relative 6D pose with respect to the ship. However, testing and validating these deep learning models in actual ocean environments poses significant challenges due to the limited availability of research vessels, high operational costs, and the risk of UAV failures-potentially leading to costly equipment losses and further complicating research efforts.\nTo address these challenges, this paper presents a vision-in-the-loop simulation environment for testing deep monocular pose estimation models and vision-based control algorithms. The framework leverages advancements in Gaussian splatting, a novel technique for photorealistic 3D scene reconstruction that represents a scene using 3D Gaussian functions instead of traditional meshes or point clouds [6], which enables real-time rendering of highly detailed and realistic environments. By constructing a photorealistic 3D virtual environment, this approach enables cost-effective evaluation of flight software and hardware under realistic maritime conditions in indoor lab settings, reducing reliance on research vessel deployments or scaled-down indoor vessel prototypes.\nIn short, the main contribution of this paper is the development of a UAV test framework that enables comprehensive indoor validation of UAV deep pose estimation methods and autonomous flight maneuvers, addressing key challenges in real-world testing. By integrating advanced simulation techniques with deep learning, this study enhances the robustness and reliability of UAV operations in ocean environments."}, {"title": "II. DEEP TRANSFORMER NETWORK FOR MONOCULAR POSE ESTIMATION", "content": "To estimate the 6D pose of UAV from a monocular camera, we employed the Transformer Neural Network for Multi-Object Pose Estimation (TNN-MO) architecture proposed in our previous work [5]. This architecture is specifically designed to estimate 6D poses (position and orientation) from a single RGB image with high accuracy and robustness in complex, real-world environments."}, {"title": "A. Network Architecture", "content": "The TNN-MO architecture combines a CNN, a transformer module, and probabilistic fusion for precise pose estimation. It processes a 480 \u00d7 640 single red, green, blue (RGB) image through a ResNet50 backbone, reducing the resolution to 15 \u00d7 20 and generating a 2048-channel feature map. A 1 \u00d7 1 convolution reduces the number of channels to 256. These features are flattened, enriched with positional encodings, and processed by a 6-layer transformer encoder with skip connections. A 6-layer transformer decoder, using 7 object queries (for 6 ship parts and a no-object class), outputs embeddings for class prediction and 2D keypoints via FFNs. By estimating 2D keypoints, the model recovers the relative 6D poses of objects using the Efficient Perspective-n-Point (EPnP) [7] algorithm. EPnP solves the 2D-to-3D correspondence problem by leveraging known 2D keypoints and their associated 3D object models, enabling accurate estimation of each object's relative position and orientation. Finally, Bayesian fusion integrates pose estimates from multiple ship parts, weighting contributions by confidence to enhance robustness and accuracy."}, {"title": "B. Synthetic Data and Model Training", "content": "To ensure effective neural network training, we developed a synthetic data generation pipeline within a simulated virtual environment, eliminating the cost and time associated with real-world data collection. A detailed 3D CAD model of a research vessel was constructed using structure-from-motion techniques, with missing elements manually added to enhance fidelity. Environmental features such as dynamic ocean waves, sunlight, shadows, and human figures were incorporated to improve realism. Domain randomization was applied to textures (e.g., sea, sky, hull) and lighting conditions to ensure the model generalizes across diverse scenarios. Camera poses were sampled from realistic UAV trajectories, capturing various viewpoints and distances. The dataset included both horizon and non-horizon scenarios to improve robustness under challenging conditions. To mitigate occlusion issues and enhance detection accuracy, the ship was decomposed into six parts (e.g., stern, superstructure). The model was trained on 435k synthetic images (332k with horizon, 102k without) using the TNN-MO architecture in PyTorch. Training spanned 350 epochs with the AdamW optimizer, a batch size of 48, and gradient clipping at a maximum norm of 0.1, leveraging an NVIDIA A100 GPU."}, {"title": "C. Validation", "content": "The TNN-MO model's performance was validated using synthetic and real-world datasets to ensure robustness in estimating a UAV's 6D pose relative to a ship."}, {"title": "1) Validation with Synthetic Data:", "content": "The model was tested on a synthetic dataset, as shown in Figure 1, which was excluded from training. It achieved a mean absolute position error of 0.204 m and an attitude error of 0.91\u00b0 across 5,500 images under varying conditions, and the position error is about 0.8% of the maximum range (25 m)."}, {"title": "2) Real-World Validation:", "content": "Real-world validation, as shown in Figure 2, was conducted using a UAV equipped with a Data Collection System (DCS) on the research vessel YP689. The model handled variable lighting, occlusions, and partial visibility, achieving position MAE between 0.089 m and 0.177m (0.66%-0.97% of 18.2 m range) and rotation errors of 1.1\u00b0 to 4.0\u00b0. Results aligned with RTK-GPS measurements, as shown in Figure 3, demonstrating robustness for autonomous UAV operations."}, {"title": "III. PHOTO-REALISTIC VIRTUAL ENVIRONMENTS", "content": "The presented TNN-MO model is capable of estimating the 6D pose of a UAV in an ocean environment, as validated by real-world data. This suggests its potential to enable vision-based autonomous UAV flights, ensuring safe and reliable launch and recovery from a research vessel. However, developing a fully autonomous UAV system that utilizes deep monocular pose estimation models in real-world ocean environments requires substantial additional engineering effort. This includes integration with estimation and control architectures, flight hardware and software development, tuning, testing, and validation. In practice, these tasks pose additional challenges due to the limited availability of research vessels, high operational costs, and the risk of UAV failures, which could lead to costly equipment losses and further complicate research efforts.\nRather than developing these algorithms directly in real-world ocean environments, a more practical approach is to construct an indoor setup that emulates the considered scenarios. Traditionally, indoor setups involve building a downscaled ship prototype and simulating environmental conditions to resemble actual ocean scenarios. However, achieving realistic visual conditions for camera systems and scaling the model accurately are particularly difficult due to lighting constraints, a challenge further exacerbated when the available flight space is significantly limited.\nAnother widely used approach is to generate photorealistic computer-simulated environments using 3D graphics software. However, constructing an accurate 3D model of a ship is challenging due to the confidential nature of ship designs, and creating a realistic ocean environment requires specialized expertise in computer graphics. Additionally, emulating various outdoor conditions at sea is a key challenge. While the ship model itself may remain unchanged, ocean conditions and lighting vary significantly depending upon the weather, time of day, and season. Incorporating these dynamic changes into a simulated environment is both complex and computationally demanding. Real-time rendering of such scenes requires significant graphical processing power and optimization to generate images efficiently.\nTo address these challenges, we propose a novel simulation framework that leverages recent advancements in Gaussian splatting to generate photorealistic images in real time for real-world pose estimation. This approach aims to overcome the limitations of traditional methods and provide a more efficient and realistic testing environment for autonomous UAV flight algorithms."}, {"title": "A. Gaussian Splatting", "content": "Gaussian splatting is a computer graphics technique for real-time rendering of 3D radiance fields, offering a bridge between traditional point-based rendering and neural radiance field methods [6]. It uses 3D Gaussians as primitives for scene representation, optimizing properties such as position, covariance, opacity, and spherical harmonics (SH) coefficients to enhance rendering quality. These 3D Gaussians provide a differentiable volumetric representation that maintains computational efficiency by avoiding redundant calculations in empty spaces, enabling a compact, high-quality representation without relying on neural networks.\nA key advantage of Gaussian splatting is its efficient, visibility-aware rendering algorithm, which leverages a tile-based rasterizer. This approach supports anisotropic Gaussian splatting, allowing for fast scene optimization and rendering. By incorporating adaptive density control, it dynamically adjusts the number and distribution of Gaussians, enabling precise representation of complex geometries. The method achieves state-of-the-art visual quality while maintaining real-time performance, rendering at over 30 frames per second (FPS) at 1080p resolution. These capabilities make Gaussian splatting highly suitable for novel-view synthesis in intricate environments.\nGiven its strengths, Gaussian splatting is particularly advantageous for generating photorealistic indoor environments to simulate real-world conditions for autonomous UAV testing. Unlike traditional methods or computationally intensive 3D graphics pipelines, Gaussian splatting enables the creation of dynamic and realistic environments such as ocean simulations\u2014with minimal computational overhead. This real-time rendering capability ensures a high-fidelity testing environment for UAV algorithms, including those integrating deep monocular pose estimation models. Consequently, Gaussian splatting provides an efficient, flexible, and repeatable solution for indoor experimentation, overcoming the limitations of traditional setups and enabling precise validation of autonomous UAV flight systems."}, {"title": "B. Data Collection", "content": "To collect real-world image data for constructing 3D Gaussian Splatting (3DGS), we utilized a GoPro Hero 13 Black action camera mounted on the octocopter UAV, as shown in Figure 5. This UAV was previously used for deploying the DCS [8]. Initially, 4K video footage was recorded with image stabilization enabled. However, generating images from the video introduced motion blur, which significantly degraded the quality of the 3DGS reconstruction.\nTo mitigate motion blur and improve image quality, we switched to the time-lapse mode, enabling the camera to capture high-resolution images (5546 \u00d7 4851 pixels) at fixed intervals of 0.5 seconds. This approach ensured sharper still frames, thereby enhancing the accuracy and reliability of the 3DGS reconstruction process.\nThe UAV was flown closer to the ship to capture detailed data, focusing on the back and both sides of the ship, as illustrated in the first row of Figure 4. Particular attention was given to the ship's landing area to ensure comprehensive coverage of the flight deck, thereby improving the robustness of the 3DGS reconstruction, especially near the stern of the ship where the UAV operates more often.\nImage datasets were captured across multiple days, at varying times of day, and under different vessel headings to construct diverse 3D Gaussian Splatting (3DGS) environments with distinct lighting conditions. To construct 3DGS environment 1 shown in Figure 4, a dataset of 1,084 images was used. During the process, the images were downscaled to a resolution of 1600 \u00d7 1399 pixels to manage GPU memory limitations. The reconstruction was performed using an NVIDIA A100-PCIE-80GB GPU, ensuring sufficient computational power for efficient processing."}, {"title": "C. Virtual Ocean Environment", "content": "After training the 3D Gaussian Splatting (3DGS) model using the collected GoPro data, we successfully generated photo-realistic virtual environments with different light conditions. A comparison between the real-world photos and the virtual images generated by the proposed 3DGS is presented in Figure 4, which demonstrates that the virtual scene closely resembles the actual environment, achieving a high level of realism. Specifically, the sky and horizon are rendered realistically. However, the ocean appears blurry in the generated images due to the inherent noise in feature extraction caused by non-stationary ocean waves. Despite this, we trained the model to prioritize the ship's structure while minimizing the impact of the ocean and sky by employing domain randomization techniques [5]. As a result, the ocean would have minimal impact on ship pose estimation accuracy.\nTo seamlessly integrate the output of 3DGS with the TNN-MO model and other parts of the flight software, we developed custom software named the \"Photo-Realistic Ocean Environment Simulator,\" as shown in Figure 6. In developing the software, we integrated key components from the Splatviz Viewer [9], which is specifically designed for real-time visualization and interaction with 3D Gaussian Splatting (3DGS) scenes. Built using the Python GUI library PyImGui, the simulator provides an intuitive interface and enhanced functionalities tailored to our application requirements. It achieves real-time rendering capability, generating images at a resolution of 640 \u00d7 640 with a frame rate ranging from 75 to 110 FPS on a laptop equipped with an NVIDIA GeForce RTX 3060 GPU.\nThis software has a direct interface with a Vicon motion capture system. Specifically, the pose of a UAV in the indoor flight experiment area captured by Vicon is transmitted to the 3DGS model such that a photorealistic scene is created, where the Vicon pose in the indoor area corresponds to the pose of the UAV relative to the flight deck in the virtual ocean environment. In other words, this system creates a visual scene that makes it appear as if the UAV in the indoor test area is flying in an ocean environment. The photo-realistic images can be saved up to 60 FPS alongside their corresponding Vicon pose data as ground truth. These data are valuable for testing, validating, and analyzing vision-based models, such as TNN-MO inference, in offline scenarios.\nAdditionally, the simulator includes an optional real-time 6D pose estimation module that integrates the TNN-MO model to estimate camera poses directly from the rendered images. This module provides real-time visualization of the keypoints predicted by the TNN-MO model, as shown in Figure 9. It is particularly useful for evaluating the tracking performance and robustness of vision-based models. The software's real-time 6D pose estimation capability facilitates the development and testing of online UAV state estimators in indoor environments. By simulating realistic conditions, it increases the likelihood of success while mitigating challenges encountered in real-world scenarios."}, {"title": "IV. VISION-IN-THE-LOOP SIMULATION", "content": null}, {"title": "A. Flight Hardware and Software", "content": "The UAV, shown in Figure 7, consists of a Vector-Nav VN100 IMU, T-Motor 700KV brushless motors, and MS1101 propellers, controlled by MikroKopter BL-Ctrl v2 ESCs to ensure stable and precise flight. A custom PCB handles power distribution, communication interfaces (I2C, UART), and sensor integration, powered by a 14.8V 6000mAh Li-Po battery.\nThe flight software runs on an onboard NVIDIA Jetson TX2, employing a multi-threaded architecture for real-time sensor processing, state estimation, and motor control. A delayed Kalman filter integrates IMU and GPS data for precise state estimation [4], while an adaptive geometric controller ensures robust trajectory tracking [11], [12]. A GTKmm-based GUI enables real-time monitoring, mission planning, and data visualization, with Wi-Fi ensuring seamless communication between the UAV and the ground station.\nFor indoor flight experiments, position and attitude measurements are obtained via a Vicon motion capture system with twelve Valkyrie VK-8 cameras, which can achieve sub-millimeter accuracy (often around 0.1 mm to 1 mm) under optimal conditions, transmitting data at 200 Hz over Wi-Fi. For outdoor operations, a SwiftNav Piksi Multi RTK GPS provides centimeter-level positioning."}, {"title": "B. Vision-In-The-Loop Test Framework", "content": "The vision-in-the-loop simulation framework integrates flight hardware, software, and the TNN-MO model for monocular pose estimation, enabling comprehensive testing of autonomous UAV flight algorithms with integrated vision-based pose estimation schemes. An overview of the proposed simulation framework is illustrated in Figure 8.\nFor a quadrotor UAV flying in an indoor environment, its pose is measured by an external Vicon motion capture system, providing the position and attitude pair $(x, R) \\in \\mathbb{R}^{3} \\times \\mathrm{SO}(3)$, where the special orthogonal group is denoted by $\\mathrm{SO}(3) = {R \\in \\mathbb{R}^{3 \\times 3}, |, R^{T} R = I_{3 \\times 3}, \\text{det}[R] = 1}$. The Vicon measurements are considered ground truth.\nThis data is then transferred to the estimation thread of the onboard flight software, where it is integrated with IMU measurements to estimate the UAV's complete state, namely $(x, R, v, \\Omega) \\in \\mathbb{R}^{3} \\times \\mathrm{SO}(3) \\times \\mathbb{R}^{3} \\times \\mathbb{R}^{3}$, which includes the additional linear velocity v and angular velocity $\\Omega$. Based on the estimated state, the control thread of the flight software computes throttle commands for each rotor, ensuring the UAV follows the desired trajectory safely.\nBoth the estimator and control algorithms are optimized for deployment on an onboard Nvidia Jetson platform, ensuring real-time applicability. Meanwhile, the pose measured by the Vicon motion capture system is sent to the proposed 3DGS framework to generate photo-realistic images corresponding to (x, R) in the virtual environment. These images are then fed into the TNN-MO model to estimate the UAV's 6D pose relative to the ship $(\\hat{x}, \\hat{R})$, as detailed in Section III. The TNN-MO model operates alongside the Photorealistic Ocean Environment Simulator at a frequency of 10 Hz in a ground station, with estimated poses recorded for analysis.\nIn the current setup, the UAV is controlled based on the pose measured by the Vicon system. For future implementations, the Vicon system will be replaced with TNN-MO-based pose estimations $(\\hat{x}, \\hat{R})$, fused with IMU measurements to estimate the full state (x, R, \u03c5, \u03a9) for the flight controller, as indicated by the dashed line in Figure 8."}, {"title": "C. Flight Test Results", "content": "To evaluate the performance and effectiveness of the vision-in-the-loop simulation, a series of flight experiments were conducted in an indoor laboratory environment, as shown in Figure 9. The UAV was tasked with executing takeoff, path-following, and landing maneuvers. The pose estimates produced by the TNN-MO model were analyzed and compared with the ground truth Vicon pose data to assess accuracy and robustness.\nWe consider two test scenarios with different virtual environments, as shown in Figure 10. The first scenario (Test 1) involves a zig-zag flight path on a bright day. The resulting actual flight trajectory (blue) and the estimated trajectory (orange) are illustrated with respect to the ship in Figures 11 and 12. The position x is plotted with respect to time in Figure 13 along with a $2\\sigma$ bound, and the attitude error is presented in terms of the errors in the Euler angles in Figure 14. These results show that the estimated trajectories are consistent with the true pose measured by Vicon.\nSpecifically, as summarized in Table I, the model achieves a mean absolute error (MAE) of 0.105 m for position, with a standard deviation (std.) of 0.3 m, and the position error is about to 0.88% of the maximum operational range (12 m). The MAE of the attitude is 1.78\u00b0. These values are consistent with other state-of-the-art techniques in vision-based localization. More importantly, these errors result from both the generation of the photo-realistic scene with 3DGS and the estimation of the pose with TNN-MO.\nThe second scenario (Test 2) involves a straight path on a cloudy day. The corresponding results, illustrated in Figures 15 to 18, and the errors summarized in Table I, are consistent with those of Test 1 or slightly improved. These results underscore the model's consistency in delivering accurate pose estimations under varying simulated maritime conditions.\nThese findings demonstrate the feasibility of validating vision-based control and estimation algorithms in an indoor environment under realistic maritime conditions, eliminating the need for costly and high-risk real-world experiments.\nOnce robust vision-based state estimation algorithms are successfully validated within the vision-in-the-loop simulation framework, the system can transition to real-world experiments. By leveraging this framework, researchers gain a cost-effective and scalable platform for rigorous validation of deep monocular pose estimation models. This approach accelerates the development of reliable UAV systems tailored for maritime applications while systematically addressing challenges such as environmental variability and sensor noise. Consequently, the framework significantly advances the deployment of vision-driven autonomous systems in complex, dynamic environments."}, {"title": "V. CONCLUSIONS", "content": "This study presents a vision-in-the-loop simulation framework for validating deep monocular pose estimation and vision-based control of UAVs in maritime environments. By leveraging 3D Gaussian Splatting (3DGS), a photo-realistic virtual ocean environment is constructed, enabling indoor testing of UAV flight algorithms without the need for costly real-world deployments. The proposed Transformer Neural Network Multi-Object (TNN-MO) model achieves mean absolute position errors of 0.105 m (Test 1) and 0.089 m (Test 2), with attitude errors of 1.8\u00b0 and 1.54\u00b0, respectively, across two distinct 3DGS environments. These results demonstrate the framework's ability to simulate dynamic maritime conditions while ensuring accurate 6D pose estimation.\nFuture work will focus on integrating TNN-MO with an IMU through the onboard estimation scheme to validate the complete flight hardware and software without relying on an external motion capture system, as well as conducting real-world flight experiments."}]}