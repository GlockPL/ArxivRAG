{"title": "EXPLORING THE CAMERA BIAS OF\nPERSON RE-IDENTIFICATION", "authors": ["Myungseo Song", "Jin-Woo Park", "Jong-Seok Lee"], "abstract": "We empirically investigate the camera bias of person re-identification (ReID)\nmodels. Previously, camera-aware methods have been proposed to address this\nissue, but they are largely confined to training domains of the models. We measure\nthe camera bias of ReID models on unseen domains and reveal that camera bias\nbecomes more pronounced under data distribution shifts. As a debiasing method for\nunseen domain data, we revisit feature normalization on embedding vectors. While\nthe normalization has been used as a straightforward solution, its underlying causes\nand broader applicability remain unexplored. We analyze why this simple method\nis effective at reducing bias and show that it can be applied to detailed bias factors\nsuch as low-level image properties and body angle. Furthermore, we validate its\ngeneralizability across various models and benchmarks, highlighting its potential\nas a simple yet effective test-time postprocessing method for ReID. In addition,\nwe explore the inherent risk of camera bias in unsupervised learning of ReID\nmodels. The unsupervised models remain highly biased towards camera labels\neven for seen domain data, indicating substantial room for improvement. Based on\nobservations of the negative impact of camera-biased pseudo labels on training, we\nsuggest simple training strategies to mitigate the bias. By applying these strategies\nto existing unsupervised learning algorithms, we show that significant performance\nimprovements can be achieved with minor modifications.", "sections": [{"title": "1 INTRODUCTION", "content": "Person re-identification (ReID) is a process of retrieving images of a query identity from gallery\nimages. With recent advances in deep learning, a wide range of challenging ReID scenarios have been\ncovered, including object occlusion (Miao et al., 2019; Somers et al., 2023), change of appearance (Jin\net al., 2022), and infrared images (Wu et al., 2017; Wu & Ye, 2023). In general, the inter-camera\nsample matching is not trivial since the shared information among images from the same camera can\nmislead a model easily. This phenomenon is known as the problem of camera bias, where samples\nfrom the same camera tend to gather closer in the feature space. This increases the false matching\nbetween the query-gallery samples since the samples of different identities from the same camera can\nbe considered too similar. To address the issue, camera-aware ReID methods (Luo et al., 2020; Wang\net al., 2021; Chen et al., 2021; Cho et al., 2022; Lee et al., 2023) have been proposed, aiming to learn\ncamera-invariant representations by leveraging camera labels of samples during training.\nHowever, the previous works on camera bias of ReID models have mainly focused on seen domains\nof the models, while the camera bias of ReID models on unseen domains has been overlooked.\nWe observe that existing ReID models exhibit a large camera bias for unseen domain data. For\nexample, Figure 1 describes the feature distance distributions between samples of a camera-aware\nmodel (Cho et al., 2022) trained on the Market-1501 (Zheng et al., 2015) dataset, using samples from\nthe MSMT17 (Wei et al., 2018) dataset. Compared to the distance distributions of the seen domain\nsamples, the distance distributions of the unseen domain samples are more separable.\nIn this paper, we first investigate the camera bias of existing ReID models on seen and unseen domain\ndata. We observe that, regardless of the model types, there is a large camera bias in distribution shifts,\nand unsupervised models are vulnerable to camera bias even on seen domains. As a straightforward"}, {"title": "2 RELATED WORK", "content": "In traditional person ReID methods, the convolutional neural networks (CNN) architectures have\nbeen popularly adopted with cross-entropy and triplet loss (Zheng et al., 2017; Hermans et al., 2017;\nLuo et al., 2019; Ye et al., 2021). When identity labels of training data are unavailable, the pseudo\nlabels are used instead based on clustering on the extracted features (Fan et al., 2018; Lin et al., 2019;\nYu et al., 2019; Zhang et al., 2019; Dai et al., 2022). Recently, the transformer backbones (He et al.,\n2021; Luo et al., 2021b; Chen et al., 2023) and self-supervised pretraining (Fu et al., 2021; 2022;\nLuo et al., 2021b; Chen et al., 2023) significantly improve the ReID performance. To enhance the\ngeneralization ability of the models, a variety of domain generalizable techniques have been also\nproposed (Dai et al., 2021; Song et al., 2019; Liao & Shao, 2021; Ni et al., 2023; Dou et al., 2023).\nHowever, it has been found that the ReID models are biased towards the camera views of given data.\nThe camera-aware methods have been proposed to alleviate this problem, where camera labels of\nthe samples are utilized in model training as auxiliary information (Luo et al., 2020; Zhuang et al.,\n2020; Zhang et al., 2021; Wang et al., 2021; Chen et al., 2021; Cho et al., 2022; Lee et al., 2023).\nFor example, an inter-camera contrastive loss is proposed to minimize the variations of the features\nfrom different cameras within the same class (Wang et al., 2021; Cho et al., 2022). Zhuang et al.\n(2020) replace batch normalization layers of a model with camera-based batch normalization layers\nconditional to the camera labels of inputs to reduce the distribution gap. Some other studies (Gu et al.,\n2020; Luo et al., 2021a) post-process a feature by subtracting the mean feature within its camera"}, {"title": "3 QUANTITATIVE ANALYSIS ON CAMERA BIAS", "content": "In this section, we quantitatively investigate the camera bias in existing ReID models. The camera\nbias is the phenomenon where the feature distribution is biased towards the camera labels of the\nsamples, which degrades ReID performance. Many camera-aware methods have been proposed to\naddress this problem. However, the scope of the discussion has been primarily limited to training\ndomain and the camera bias on unseen domains has not been thoroughly explored. We focus on\nthe camera bias of ReID models on unseen domains, examining various types of models including\ncamera-aware/agnostic, supervised/unsupervised, and domain generalizable approaches, with the\nwidely used backbones such as ResNet (He et al., 2016) and ViT (Dosovitskiy et al., 2021).\nTo measure the bias, we utilize Normalized Mutual Information (NMI) which quantifies the shared\ninformation between two clustering results. We extract the features of samples and perform clustering\nto them using InfoMAP (Rosvall & Bergstrom, 2008). Then, the camera bias is computed by NMI\nbetween cluster labels and camera labels of the samples. The accuracy of the clusters are measured\nby NMI between the cluster labels and the identity labels. The results on MSMT17, Market-1501,\nCUHK03-NP (Zhong et al., 2017a), and PersonX (Sun & Zheng, 2019) are shown in Table 1, where\nthe bias of the ground truth (i.e., NMI between the identity labels and the camera labels) indicates the\ninherent imbalance in a dataset. All models except ISR (Dou et al., 2023) are trained on MSMT17,\nhence the other datasets are unseen domains for them. For ISR, all datasets are unseen domains.\nWe make two notable observations from the results. First, the existing ReID models have a large\ncamera bias on the unseen domains, regardless of their training setups or backbones. Second, the\nunsupervised models have a large camera bias on the seen domain, even on their training data. These\nimply that debiasing methods for unseen domains are needed in general, and there is room for\nperformance improvement of unsupervised methods by reducing the camera bias during training.\nRelatively, the recent supervised models exhibit less debiased results on the training domain."}, {"title": "4 UNDERSTANDING CAMERA BIAS AND FEATURE NORMALIZATION", "content": ""}, {"title": "4.1 CAMERA-SPECIFIC FEATURE NORMALIZATION", "content": "In Section 3, we observed that the ReID models have a large camera bias on unseen domains.\nAs a straightforward debiasing method, we introduce camera-specific feature normalization which\npostprocesses embedding vectors leveraging camera labels at test time. It is performed as follows.\nSuppose that a test dataset $X = \\{(x_1,y_1), (x_2,y_2),\\ldots, (x_N, y_N)\\}$ with $N$ samples is given, where\n$x_i$ and $y_i$ denote the image and camera label of each sample, respectively. A pretrained encoder"}, {"title": "4.2 ANALYSIS ON FEATURE SPACE", "content": "We dive deeply into the feature space of a ReID model (Luo et al., 2021b) trained on MSMT17 using\nCUHK03-NP samples, to understand why the normalization can play a role of debiasing.\nSensitivity to camera variations differs across dimensions We first find that the sensitivity of\neach dimension of the feature space to camera variations is quite different from each other. We\ncompute mean features of each camera view and present the element-wise variances of the mean\nfeatures in the descending order in Figure 2(a). It is shown that some dimensions have a relatively\nlarge variation, which might be largely related to the camera bias of the model.\nMovements of features due to camera variations We indirectly investigate features, movements\ndue to camera changes using the identity labels and camera labels of the samples. We obtain\ndisplacement vectors from feature pairs of two different cameras with the same identities (details in\nAppendix B.1) and compute their average cosine similarity in selected dimensions, with increasing\nthe number of selected dimensions. Three selecting orders are used: (1) \u201cDimension index\u201d follows\nthe original index order of the dimensions, (2) \u201cCamera sensitive\u201d follows the descending order of the\nelement-wise variances of the camera means, and (3) \"Camera insensitive\" follows the reverse order\nof (2). From Figure 2(b), we observe that the similarities of the displacement vectors in the camera-\nsensitive dimensions are relatively large. In other words, the features tend to move consistently in the"}, {"title": "4.3 ANALYSIS ON DETAILED BIAS FACTORS", "content": "We explore the feature normalization for detailed bias factors of ReID models, including image\nproperties and body angle of images. The model (Luo et al., 2021b) trained on MSMT17 is used.\nMovements of features due to image transformations Given the fine-grained nature of person\nReID, the camera bias of a model might be closely related to the difference in low-level image\nproperties between cameras. Here, we analyze the changes of features due to image transformations\napplied to samples from CUHK03-NP, using eight low-level transformation functions with four levels\nof transformation strength as shown in Figure 10. The feature of the i-th image and the feature of its\ntransformed image at level k are denoted by $f_i(0)$ and $f_i(k)$, respectively. For example, for a blurring\nfunction, $f_i(4)$ denotes the feature when the i-th image is most strongly blurred. Then, we compute the\naverage cosine similarity between displacement vectors of the features after applying a transformation\nto the images for each level k, which is given by $E_{i,j}[Sim(f_i(k) - f_i(k-1), f_j(k) - f_j(k-1))]$. The\nresult is shown in Figure 3(a). We observe that, for certain transformations such as decreasing\nbrightness, the displacement vector $(f_i(k) - f_i(k-1))$ due to the transformation is similar across\ndifferent images to some extent, which is analogous to the effect of camera variations.\nNormalization for image properties Then, can we reduce biases of the model towards the low-\nlevel properties by utilizing the feature normalization? To find out, we calculate the brightness,\nsharpness, contrast, and area of all samples, as visualized in Figure 11. Note that all samples in the\ndataset have almost same contrast values. We divide the samples into N groups of equal size for each\nproperty. For example, when dividing the samples into N = 2 groups based on the brightness, we\nuse the median brightness value as the threshold for group assignment. Here, a small but meaningful\ncorrelation between these group labels and camera labels is observed as shown in Figure 12. Then,\nwe perform a group-specific feature normalization on the features using the property group labels. As\npresented in Figure 3(b), the normalization on the features based on the property groups is effective\nfor brightness, sharpness, and area. It does not work for contrast since all contrast values are almost\nequal. In addition, we subdivide each property group into multiple groups based on the camera labels"}, {"title": "4.4 MORE EMPIRICAL RESULTS", "content": "Generalizability We present the evaluation results of the camera-specific feature normalization\non multiple ReID models in Table 3. The mean average precision (mAP) and cumulative matching\ncharacteristics (CMC) Rank-1 (R1) are used for evaluation. The NMI scores of clustering results are\nalso reported as in Section 3. Note that ISR and PAT (Ni et al., 2023) are domain generalized methods.\nThe feature normalization significantly improves the performance of all models on the unseen\ndomain (white background in the table), regardless of training methods or backbones architectures.\nFor example, on Market-1501, CC (Dai et al., 2022) exhibits about 7.5% improvement in mAP\nand about 5.9% reduction in camera bias, and TransReID (He et al., 2021) shows about 9.4%\nimprovement in mAP and about 2.7% reduction in camera bias. For the seen domain, the camera-\nagnostic unsupervised models show slight improvement (gray background), while the camera-aware\nor supervised models exhibit no improvement (red background). It is likely because the camera bias\nof these models is already relatively small on the seen domain, e.g., SOLIDER (Chen et al., 2023) has\nan almost identical bias value to the ground truth. The normalization results of Figure 1(b) are shown\nin Figure 4, where the less separable distributions are observed. The feature visualization result is\nillustrated in Figure 15.\nAblation study The camera-specific feature normalization consists of (1) camera-specific, (2) mean\ncentering, and (3) scaling by standard deviation on the features. We investigate the effectiveness\nof each component for CUHK03-NP in Table 4, using TransReID-SSL (Luo et al., 2021b) trained\non MSMT17. ZCA whitening is also evaluated to check the effectiveness of rotation related to\ncovariance across feature dimensions. There are some gains from the entire transforms, but the\ncamera-specific transforms outperform them. It is observed that the camera-specific mean centering\nhas a dominant effect and the scaling operation provides a small but additional gain. The rotation by\nthe ZCA whitening does not exhibit definite gains compared to the normalization."}, {"title": "5 RISK OF CAMERA BIAS IN UNSUPERVISED LEARNING", "content": ""}, {"title": "5.1 RISK OF BIASED PSEUDO LABELS", "content": "In Section 3, we observed that the ReID models learned in unsupervised manners have a large camera\nbias even on their training data. We argue that the existing USL algorithms have two limitations\nintroducing the camera bias into the models. First, the pseudo labels of training data are biased\ntowards the camera labels. In USL, a model is supervised by the pseudo labels of the training samples\nwhich are usually generated by clustering of the features extracted by the model. However, as we\nhave seen, the clustering result is already camera-biased, hence using them for training would make\nthe model dependent on the camera-related information. Second, camera-biased clusters with few\ncameras are used in training without sufficient consideration. For example, consider a cluster only\nconsisting of samples from one camera. Since most of the samples of this cluster may share similar\ncamera-related information (e.g., background), utilizing them as positive training samples can lead\nthe model to pay more attention to the common camera-related information. Also, the samples in\nthat cluster were likely grouped together incorrectly due to the shared camera information, which is\nexpected to be more common in the early stage of model training."}, {"title": "5.2 \u03a4\u039f\u03a5 EXAMPLE RESULTS", "content": "We investigate the risks of biased training data toward cameras using toy examples. ResNet50 models\nare trained on the toy datasets using the cluster contrastive loss (Dai et al., 2022) in the experiments.\nFigure 6(a) compares the training results with the different levels of camera bias and accuracy of\npseudo labels for the same training samples. We constructed a dataset of 7500 samples by randomly\nselecting 500 identities from Market-1501, where each identity has 5 samples per camera with 3\ncameras. To generate pseudo labels with varying degrees of camera bias at the same accuracy, a\ncertain proportion of identities were divided into three equally-sized clusters for each identity based\neither on camera labels (\u201cCamera\u201d) or random selection (\u201cRandom\u201d). Five splitting ratios of 0%,\n25%, 50%, 75%, and 100% were used. For example, the pseudo labels generated by splitting 50% of\nthe identities consist of 250 original clusters and 750 split clusters, totaling 1000 clusters. The bias\nof the pseudo labels is measured by calculating the mean entropy of the camera labels within each\ncluster (Lee et al., 2023). It is observed that, at the same pseudo label accuracy, models trained with\n\u201cCamera\u201d consistently perform worse than those trained with \u201cRandom\u201d. Moreover, \u201cRandom\u201d with\n91.9% accuracy outperforms \u201cCamera\u201d with 93.8% accuracy, despite having lower accuracy. These\nresults suggest that greater camera bias of pseudo labels has a detrimental effect on model training,\nand pseudo labels with lower accuracy but less camera bias can provide more benefits than those with\nhigher accuracy but greater camera bias.\nFigure 6(b) illustrates the impact of camera diversity of training data, using ground truth labels. We\nconstructed five datasets of 11821 samples of 1041 identities from MSMT17, where the maximum\nnumbers of cameras per identity are different. As expected, the model performance declines as the\nmaximum number of cameras decreases. Notably, a significant performance drop is observed when\nthe model is trained with samples from only a single camera for each identity. This suggests that using\nsingle-camera clusters for training can degrade the model performance. In addition, the influence of\nclustering parameter on the camera bias is investigated in Appendix I."}, {"title": "5.3 SIMPLE STRATEGIES FOR DEBIASED UNSUPERVISED LEARNING", "content": "To reduce the explored risk of camera bias in unsupervised learning, we present two simple training\nstrategies applicable to existing USL algorithms; (1) debiased pseudo labeling: clustering on the\ndebiased features computed by Equation 2 instead of the original features when generating pseudo\nlabels, and (2) discarding biased clusters: discarding the single-camera clusters in training data.\nWe present an example of applying the proposed strategies to unsupervised learning in Algorithm 1.\nWith these minor modifications, we observe significant performance improvements in next section."}, {"title": "5.4 EMPIRICAL RESULTS", "content": "We validate the suggested training strategies on the SOTA camera-agnostic methods, CC and PPLR,\nand the SOTA camera-aware method, PPLR-CAM. A vehicle ReID dataset, VeRi-776 (Liu et al.,\n2016), is additionally used and the person and vehicle images are resized to 384 \u00d7 128 and 256 \u00d7 256,\nrespectively, following the setup of PPLR. The models are trained on a H100 GPU with batch size 256\nand 100 training epochs, with DBSCAN (Ester et al., 1996) to obtain pseudo labels. Our strategies\neffectively improves all methods as presented in Table 6. In particular, outstanding performance gains\nare obtained on the challenging benchmark, MSMT17, e.g., 19.3% mAP increase for CC. The gains\nfor PPLR-CAM are relatively small, which is likely because it uses a camera-aware loss function. In\naddition, the number of discarded training samples by our strategy is discussed in Appendix H.\nAblation study Table(a) of Figure 7 investigates the individual effect of our strategies, using CC.\nBoth of the suggested strategies contribute to the performance improvements by reducing the camera\nbias. The ratio of the single-camera clusters and clustering accuracy during training are illustrated in\nthe plot of Figure 7. It is observed that the baseline has unusually high single-camera cluster rates\n(from about 80% to about 35%), which is effectively mitigated by the proposed methods."}, {"title": "6 CONCLUSION", "content": "We revisited the debiasing effects of normalization on embedding vectors of ReID models and\nexplored the risk of camera bias inherent in unsupervised learning for ReID models. We found that\nthe existing ReID models are biased towards camera labels on unseen domain, and the unsupervised\nmodels even have a large camera bias to their training data. We analyzed why the camera-specific\nfeature normalization has debiasing effects and explored its potential and applicability for ReID tasks\nin comprehensive empirical studies. It was observed that, for a camera variation, the sensitivity of\neach feature dimension is quite different and features tend to move consistently in sensitive dimen-\nsions. Then, it was shown that the feature normalization is a simple but effective bias elimination\nmethod for ReID models in general, including biases towards low-level properties and body angle.\nAlso, we empirically showed the detrimental effects of biased pseudo labels using toy examples and\nachieved significant performance improvements with simple modifications to the existing unsuper-\nvised algorithms. We hope that the insights from this work will serve as an insightful foundation for\nresearching biases of ReID models and developing debiasing techniques for ReID models."}]}