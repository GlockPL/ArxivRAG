{"title": "KOLMOGOROV-ARNOLD NETWORKS FOR TIME SERIES\nGRANGER CAUSALITY INFERENCE", "authors": ["Meiliang Liu", "Yunfang Xu", "Zijin Li", "Zhengye Si", "Xiaoxiao Yang", "Xinyue Yang", "Zhiwen Zhao"], "abstract": "We introduce Granger Causality Kolmogorov-Arnold Networks (GCKAN), an innovative architecture\nthat extends the recently proposed Kolmogorov-Arnold Networks (KAN) to the domain of causal\ninference. By extracting base weights from KAN layers and incorporating the sparsity-inducing\npenalty along with ridge regularization, GCKAN infers the Granger causality from time series\nwhile enabling automatic time lag selection. Additionally, we propose an algorithm leveraging time-\nreversed Granger causality to enhance inference accuracy. The algorithm compares prediction and\nsparse-inducing losses derived from the original and time-reversed series, automatically selecting the\ncasual relationship with the higher score or integrating the results to mitigate spurious connectivities.\nComprehensive experiments conducted on Lorenz-96, gene regulatory networks, fMRI BOLD signals,\nand VAR datasets demonstrate that the proposed model achieves competitive performance to state-of-\nthe-art methods in inferring Granger causality from nonlinear, high-dimensional, and limited-sample\ntime series.", "sections": [{"title": "Introduction", "content": "Granger causality is a statistical framework for analyzing the causality between time series. It offers a powerful\ntool to investigate temporal dependencies and infer the direction of influence between variables [Seth(2007),\nMaziarz(2015), Friston et al.(2014)Friston, Bastos, Oswal, van Wijk, Richter, and Litvak, Shojaie and Fox(2022)].\nBy examining the past values of a series, Granger causality seeks to determine if the historical knowledge\nof one variable improves the prediction of another [Bressler and Seth(2011), Barnett and Seth(2014)]. Reveal-\ning inner interactions from observational time series has made Granger causality useful for the investigation\nin many fields, such as econometrics [Mele et al.(2022)Mele, Magazzino, Schneider, Gurrieri, and Golpira],\nneuroscience [Chen et al.(2023)Chen, Ginoux, Carbo-Tano, Mora, Walczak, and Wyart],\nclimate science\n[Ren et al.(2023)Ren, Li, He, and Lucey], etc.\nRecently, there has been a growing interest in incorporating neural networks into the study of Granger causality due to\ntheir inherent nonlinear mapping capabilities. For now, a variety of neural Granger causality models have been pro-\nposed, mainly based on multi-layer perceptron (MLP) [Tank et al.(2022)Tank, Covert, Foti, Shojaie, and Fox,\nBussmann et al.(2021)Bussmann, Nys, and Latr\u00e9, Zhou et al.(2024)Zhou, Bai, Yu, Zhao, and Chen], recur-\nrent neural network (RNN) [Khanna and Tan(2019), Tank et al.(2022)Tank, Covert, Foti, Shojaie, and Fox],\nconvolutional neural network (CNN) [Nauta et al.(2019)Nauta, Bucur, and Seifert], or their combination\n[Cheng et al.(2024)Cheng, Li, Xiao, Li, Suo, He, and Dai]. These models have achieved significant improve-\nments in inferring nonlinear Granger causality but still have certain limitations: (1) RNN-based models are more\nsuitable for processing long time series, but experience decreased causal inference performance in the limited\ntime-samples scenario. Additionally, these models have only an explicit dependence on one past timepoint, which\nmakes them unable to select time lags automatically; (2) MLP-based models can automatically select time lags but face\nthe challenge of low inference efficiency when dealing with high-dimensional and noisy time series. (3) CNN-based\nmodels have been shown to perform ineffectively on many nonlinear datasets."}, {"title": "Background and Related Works", "content": null}, {"title": "Background: Neural network-based Granger causality", "content": "Recently, inferring Granger causality from nonlinear time series via neural networks has attracted widespread at-\ntention. [Tank et al.(2022)Tank, Covert, Foti, Shojaie, and Fox] proposed the cMLP and cLSTM, which extract the\nfirst-layer weights of multi-layer perceptron (MLP) and long short-term memory (LSTM), respectively, and impose\nthe sparsity-inducing penalty to infer Granger causality. [Bussmann et al.(2021)Bussmann, Nys, and Latr\u00e9] proposed\nthe Neural Additive Vector Autoregression (NAVAR) model based on MLP and LSTM, called NAVAR(MLP) and\nNAVAR(LSTM), for Granger causality inference. [Khanna and Tan(2019)] proposed the economy-SRU (eSRU) model,\nwhich extracts weights from statistical recurrent units (SRU) and regularizes them to infer Granger causality. In\naddition, [Nauta et al.(2019)Nauta, Bucur, and Seifert] proposed a temporal causal discovery framework (TCDF) based\non temporal convolutional network (TCN) and causal verification steps to infer Granger causality and select time lags.\n[Cheng et al.(2023)Cheng, Yang, Xiao, Li, Suo, He, and Dai] proposed Causal discovery from irregUlar Time-Series\n(CUTS), which can effectively infer Granger causality from data with random missing or non-uniform sampling\nfrequency. Subsequently, to solve the problems of large causal graphs and redundant data prediction modules of CUTS,\n[Cheng et al.(2024)Cheng, Li, Xiao, Li, Suo, He, and Dai] proposed the CUTS+, which introduced a coarse-to-fine\ncausal discovery mechanism and a message-passing graph neural network (MPGNN) to achieve more accurate causal\nreasoning. [Marcinkevi\u010ds and Vogt(2021)] proposed a generalised vector autoregression (GVAR) based on the self-\nexplaining neural network model, which effectively inferred causal relationships and improved the interpretability of the\nmodel. [Zhou et al.(2024)Zhou, Bai, Yu, Zhao, and Chen] proposed a neural Granger causality model based on Jacobi\nregularization (JRNGC), which only needs to construct a single model for all variables to achieve causal inference."}, {"title": "Related Works", "content": null}, {"title": "Component-wise nonlinear autoregressive (NAR)", "content": "Assume a p-dimensional nonlinear time series $x_t = [x_{t1},..., x_{tp}]$, where $x_{ti} = (..., x_{(t\u22122)i}, x _{(t\u22121)i})$. In the\nnonlinear autoregressive (NAR) model, the $t^{th}$ time point $x_t$ can be denoted as a function g of its past time values:\n$x_t = g(x_{t1},..., x_{tp}) + e_t$ (1)\nFurthermore, in the component-wise NAR model, it is assumed that the $t^{th}$ time point of each time series $x_{ti}$ may\ndepend on different past-time lags from all the series:\n$x_{ti} = g_i (x_{t1},..., x_{tp}) + e_{ti}$ (2)"}, {"title": "Time reversed Granger causality", "content": "The time-reversed Granger causality was initially introduced by [Haufe et al.(2013)Haufe, Nikulin, M\u00fcller, and Nolte],\nwhich was used to reduce spurious connections caused by volume conduction effects in analyzing Elec-\ntroencephalogram (EEG) signals [van den Broek et al.(1998)van den Broek, Reinders, Donderwinkel, and Peters,\nNunez et al.(1997)Nunez, Srinivasan, Westdorp, Wijesinghe, Tucker, Silberstein, and Cadusch]. Subsequently,\n[Winkler et al.(2016)Winkler, Panknin, Bartz, M\u00fcller, and Haufe] demonstrated that, in finite-order autoregressive\nprocesses, causal relationships would reversed in time-reversed time series. Moreover, comparing the causal relationship\ninferred from the original data and the time-reversed data can enhance the robustness of causal inference against\nnoise. However, the proof of [Winkler et al.(2016)Winkler, Panknin, Bartz, M\u00fcller, and Haufe] is mainly for the linear\nsystem. However, the findings of [Winkler et al.(2016)Winkler, Panknin, Bartz, M\u00fcller, and Haufe] primarily apply\nto linear systems. Recent research indicates that in nonlinear chaotic systems, causal relationships inferred from\ntime-reversed time series generally align with those from the original data, with perfect causal relationship reversal\noccurring only under specific conditions [Ko\u0159enek and Hlinka(2021)]."}, {"title": "Kolmogorov-Arnold Networks (KAN)", "content": "[Liu et al.(2024)Liu, Wang, Vaidya, Ruehle, Halverson, Solja\u010di\u0107, Hou, and Tegmark] proposed KAN, which has gar-\nnered attention as a compelling alternative to the multilayer perceptron (MLP). The theoretical foundation of MLP is\nrooted in the universal approximation theorem, which demonstrates that neural networks can approximate any continu-\nous function under appropriate conditions [Pinkus(1999)]. By contrast, KAN is grounded in the Kolmogorov-Arnold\n(KA) representation theorem, which states that any multivariate continuous function can be represented by the sum of a\nfinite number of univariate functions [Schmidt-Hieber(2021)].\nTheorem 2.1 Let $f:[0,1]^n \\rightarrow R$ be a continuous multivariate function. There exist continuous univariate functions\n$\\Phi_q$ and $\\phi_{q,p}$ such that:\n$f(x_1, x_2,...,x_n) = \\sum_{j=1}^{2n+1} \\Phi_q ( \\sum_{i=1}^{n} \\phi_{q,p} (x_p) )$\nwhere $\\Phi_i: R \\rightarrow R$ and $\\phi_{q,p} :[0,1] \\rightarrow R$ are continuous functions.\nAlthough the KA representation theorem is elegant and general, its application in deep learning remained limited\nbefore the work of [Liu et al.(2024)Liu, Wang, Vaidya, Ruehle, Halverson, Solja\u010di\u0107, Hou, and Tegmark]. This limi-\ntation can be attributed to two primary factors: (1) the function $\\phi_{q,p}$ is typically non-smooth; (2) the theorem is\nconstrained to construct shallow neural networks with two-layer nonlinear architectures with limited hidden layer\nsize. [Liu et al.(2024)Liu, Wang, Vaidya, Ruehle, Halverson, Solja\u010di\u0107, Hou, and Tegmark] do not strictly constrain the\nneural network to fully adhere to Theorem 2.1, but instead extended the network to arbitrary width and depth to be used\nin deep learning. Due to this alternation, KAN and its variants have been extensively applied across various domains,\nincluding computer vision, natural language processing, and time series forecasting. In this study, we develop our\nGranger causality model based on the code of efficientKAN 1."}, {"title": "Model Architecture", "content": null}, {"title": "Component-wise KAN", "content": "To extract the influence from input to output, we model each component $g_i$ using a separate KAN. Let $g_i$ take the form\nof a KAN with $L \u2013 1$ layers, and $h^l$ are denoted as the $l^{th}$ hidden layer. The trainable parameter of KAN including\n$W_{base}, W_{spline}$ on each layer, $W_{base} = \\{W^0_{b},W^1_{b},...,W^{L-1}_{b}\\}$ and $W_{spline} = \\{W^0_{s},W^1_{s},...,W^{L-1}_{s}\\}$. We separate"}, {"title": "Applying sparsity-inducing penalty and ridge regularization on KAN to infer Granger causality", "content": "According to Eq.3, the inference of Granger causality in Equation 8 uses component-wise NAR combined with sparsity-\ninducing penalty. In our study, we extract the base weight $W_b^0$ of the first hidden layer and apply a group lasso penalty\nto the columns of the $W_b^1$ matrices for each $g_i$, which is denoted as:\n$GroupLasso (W_b^1(:)) = || W_b^1(:,j) ||_F$ (9)\nwhere $W_b^1(:,j)$ is the j column of the $W_b^1$ corresponding to the time series $j$. $|| \u00b7 ||_F$ is denoted as the Frobenius matrix\nnorm. The sparse-inducing loss $L_s$ is defined as:\n$L_s = \\lambda \\sum_{j=1}^{p} ||W_b^1(:,j)||_F$ (10)\n$\\lambda > 0$ is the group lasso hyperparameter that controls the penalty strength. For the base weight $W_b^l$ of other hidden\nlayers except the first hidden layer, we apply ridge regularization to them, which is denoted as:\n$RidgeRegularization (W_b^{1:L-1}) = \\sum_{l=1}^{L-1} ||W_b^l||_2$ (11)\nwhere $|| \u00b7 ||_2$ is denoted as the L2 norm. The ridge regularization loss $L_r$ is defined as:\n$L_r = \\gamma \\sum_{l=1}^{L-1} ||W_b^l||_2$ (12)\n$\\gamma > 0$ is the ridge regularization hyperparameter that controls the regularization strength. Finally, the predicted loss is\ndefined as:\n$L_p = \\sum_{i=1}^{p} (x_{ti} - g_i(x_t))^2$ (13)\nTherefore, the loss function is defined as:\n$L = L_p + L_s + L_r$ (14)\nSince the proposed model is a component-wise architecture, a total of p models are needed to construct the complete\nGranger causality matrix. We extract the first hidden layer weight $W_b^1$ to compute the $i^{th}$ row of the Granger causality\nmatrix $G$, which is denoted as:\n$G(i,:) = ||W_b^1(:,i) ||_F$ (15)"}, {"title": "Fusion of origin and time reversed time series", "content": "During the experiment, we observed that, in certain simulation trials, the causality scores derived from the original and\nreversed time series exhibit considerable divergence. Specifically, there were cases where the causality score from the\noriginal time series was higher, while in other cases, the reversed time series yielded a better score. Consequently, our\nobjective is to develop an algorithm that can automatically select the matrix with the higher causality score from either\nthe original or reversed time series or obtain more accurate inference results by fusing both of them.\nAlgorithm 1 summarizes the proposed procedure for fusing the original and reversed time series. In the Granger\ncausality inference stage, a total of 2p GCKAN models are required, with the first p models applied to the original time\nseries and the next p models to the reversed time series (lines 3-4 in Algorithm 1). We use Equation (12) to calculate the\nGranger causality matrix of the original and reversed time series (line 5 in Algorithm 1). Subsequently, we compare the\nlosses to determine whether to select a single matrix or fuse both matrices. Specifically, when the prediction loss and\nsparsity-induced loss of the original time series are both lower than those of the reversed time series, it suggests that the\nmodel's prediction and sparsity performance are superior for the original time series. Therefore, the Granger causality\nmatrix derived from the original time series is chosen as the final matrix. Conversely, if the reversed time series exhibits"}, {"title": "Experiment", "content": "In this section, we show the performance of the proposed model (GCKAN) on four widely\nused benchmark datasets: Lorenz-96 model, fMRI BOLD signals, Gene regulatory net-\nworks, and VAR. Comparative experiments\nwere conducted against several state-of-the-\nart models, including CMLP &\nCLSTM [Tank et al.(2022)Tank, Covert, Foti, Shojaie, and Fox],\nTCDF [Nauta et al.(2019)Nauta, Bucur, and Seifert],\ne-SRU [Khanna and Tan(2019)],\nGVAR\n[Marcinkevi\u010ds and Vogt(2021)], CUTS+ [Cheng et al.(2024)Cheng, Li, Xiao, Li, Suo, He, and Dai], JGC\n[Suryadi et al.(2023)Suryadi, Chew, and Ong],\nand JRNGC [Zhou et al.(2024)Zhou, Bai, Yu, Zhao, and Chen].\nAdditionally, the proposed model's experimental hyperparameters are detailed in the Appendix. Our codes are provided\nin the supplementary material.\nIn alignment with prior studies, the model performances are evaluated using the area under the receiver operating\ncharacteristic curve (AUROC). Notably, in the evaluation of the gene regulatory networks, only the off-diagonal\nelements of the Granger causality adjacency matrix are considered since the gold standard provided by the gene\nregulatory networks does not account for self-causality. In contrast, for the Lorenz-96, fMRI BOLD, and VAR datasets,\nall elements of the adjacency matrix are included in the evaluation."}, {"title": "Lorenz-96", "content": "The Lorenz-96 model is a mathematical model employed to investigate the dynamics of simplified atmospheric systems.\nIts behavior is governed by the following ordinary differential equation:\n$\\frac{\\partial x_{t,i}}{\\partial t} = -x_{t,i-1} (x_{t,i-2} - x_{t,i+1}) - x_{t,i} + F$ (16)\nWhere F represents the external forcing term in the system, and p denotes the spatial dimension of the system. An\nincrease in F results in heightened system chaos, while an increase in p enhances the spatial complexity of the system.\nWe numerically simulate R = 5 replicates under the following three conditions : (1) F = 10, P = 10, T = 1000 (low\ndimensionality, weak nonlinearity); (2) F = 40, P = 40, T = 1000 (high dimensionality, strong nonlinearity); (3)\nF = 40, P = 40, T = 500 (limited observations)."}, {"title": "Gene regulatory networks", "content": null}, {"title": "Dream-3", "content": "The second dataset is the DREAM-3 in Silico Network Challenge, available at https://gnw.sourceforge.net/\ndreamchallenge.html. This dataset provides a complex and nonlinear framework for evaluating the performance of\nGranger causality models. It consists of five sub-datasets: two corresponding to E.coli (E.coli-1, E.coli-2) and three to\nYeast (Yeast-1, Yeast-2, Yeast-3). Each sub-dataset has a distinct ground-truth Granger causality network and includes\np=100 time series, which represents the expression levels of n=100 genes. Each time series comprises 46 replicates,\nsampled at 21 time points, yielding a total of 966 observations.\nThe results of the Dream-3 dataset are shown in Table 2. The performance of all models drops significantly compared\nto the Lorenz-96 dataset since the Dream-3 dataset contains 100 channels, carries additional noise and has few\nobservations, which leads to frequent overfitting of the models. Our model emerged as the top-performance model\namong its counterparts in four out of five gene regulatory networks. Specifically, the AUROC of the proposed model in\nE.coli-1, E.coli-2, Yeast-1, and Yeast-3 are 0.762, 0.680, 0.667 and 0.562, respectively. Compared with the baseline, the\nperformance is improved by 9.6%, 0.2%, 1.5% and 0.2%, respectively. This further proves the effectiveness of our\nmethod in identifying sparse Granger causality in high-dimensional, noisy time series."}, {"title": "Dream-4", "content": "The third dataset is the DREAM-4 in silico challenge. Analogous to the DREAM-3 dataset, it consists of five sub-\ndatasets, each containing p = 100 time series. However, unlike DREAM-3, each time series in DREAM-4 includes only\n10 replicates sampled at 21 time points, yielding a total of 210 observations. This is substantially fewer than the 966\nobservations provided by the DREAM-3 dataset. Therefore, Dream-4 dataset challenges the inference performance of\neach model in scenarios with a limited number of time series observations."}, {"title": "fMRI BOLD signals", "content": "The fourth dataset is the simulated fMRI BOLD signals generated using the dynamic causal model (DCM) with the\nnonlinear balloon model for vascular dynamics. Each data includes multiple time series corresponding to different\nbrain regions of interest (ROIs). Notably, the fMRI BOLD dataset contains 28 sub-datasets, each comprising 50\nsubjects and including distinct features. However, previous studies have typically utilized few subjects from few\nsimulations (e.g., sim-3, sim-4) for model evaluation, which is inadequate for comprehensively assessing model\nperformance on the fMRI dataset. In this study, we address this limitation by conducting a thorough evaluation\nusing the complete set of subject data from all simulations (a total of 1,400 subjects). The dataset is shared at\nhttps://www.fmrib.ox.ac.uk/datasets/netsim/index.html. Table 4 presents the comparison results of the\nfirst four simulations. The complete model performances and the summary of all the simulations' specifications are\nprovided in the Appendix A Table 6, 7.\nComparative experiments conducted on the fMRI BOLD signal dataset demonstrate that only TCDF, JGC, JRNGC,\nCUTS+, and GCKAN effectively infer Granger causality across all simulations and subjects. Among these methods,\nGCKAN achieved superior performance in 22 out of 28 simulations, covering various complex scenarios such as global\nmean confusion, mixed time series, shared inputs, backward connections, cyclic connections, and time lags. In contrast,\nJRNGC and CUTS+ exhibited better performance in simulations with varying connection strengths (e.g., Sim 15,\n22, 23). Furthermore, given the inclusion of noise and randomness (with a standard deviation of 0.5 seconds in the\nhemodynamic response function delay) and the limited sampling points (T=200) in most cases, the proposed model can\nmore effectively infer Granger causality under noisy and data-constrained conditions compared to existing baseline\nmodels."}, {"title": "VAR", "content": null}, {"title": "experimental results", "content": "The fifth dataset is the VAR model. For a p-dimensional time series $x_t$, the VAR model is given by:\n$x_t = A^{(1)}x_{t-1} + A^{(2)}x_{t-2}+,...,+A^{(k)}x_{t-k} + U_t$ (17)\nwhere $(A^{(1)}, A^{(2)}, . . ., A^{(k)}$ are regression coefficients matrices and $u_t$ is a vector of errors with Gaussian distribution.\nWe define sparsity as the percentage of non-zero coefficients in $A^{(i)}$, and different sparsity represent different\nquantities of Granger causality interaction in the VAR model. The comparison results of the VAR dataset are presented\nin Table 5."}, {"title": "Conclusion", "content": "In this study, we propose a novel neural network-based Granger causality model, termed Granger Causal Kolmogorov-\nArnold Networks (GCKAN). The model leverages the base weights of the KAN layer, incorporating sparsity-inducing\npenalties and ridge regularization to infer the causal relationship. In addition, we develop an algorithm grounded\nin time-reverse Granger causality to mitigate spurious connections and enhance inference performances. Extensive\nexperiments validate that GCKAN can effectively infer interaction relationships in time series, outperforming the\nexisting baselines. These results suggest that GCKAN brings a new avenue for advancing Granger causality inference.\nWe anticipate that this model will inspire subsequent research to design more accurate and computationally efficient\nframeworks for Granger causality inference."}]}