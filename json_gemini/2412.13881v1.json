{"title": "Understanding and Analyzing Model Robustness and Knowledge-Transfer in Multilingual Neural Machine Translation using TX-Ray", "authors": ["Vageesh Saxena", "Dr. Sharid Lo\u00e1iciga", "Nils Rethmeier"], "abstract": "Compared to the conventional phrase-based statistical machine translation techniques, neural networks have shown promising results in the field of Neural Machine Translation. However, despite their success on massive translation datasets, Multi-lingual Neural Machine Translation in an extremely low-resource setting has not been adequately explored. Through this research, we demonstrate how different languages leverage knowledge-transfer to improve Multi-lingual Neural Machine Translation in an extremely low-resource setting. Utilizing a minimal amount of parallel data to learn useful mappings between different languages, we use the Tatoeba translation challenge dataset from Helsinki NLP [J\u00f6rg Tiedemann and Thottingal, 2020] to perform English-German, English-French, and English-Spanish translations.\nUnlike most of the conventional approaches that leverage heavy pre-training from source to target language-pair, we pre-train our model to perform English-English translations, where English is the source language for all our translations. We then finetune our pre-trained model to perform English-German, English-French, and English-Spanish translations, using joint multi-task and sequential transfer learning approaches. Through this research, we investigate questions such as :\n\u2022 RQ1: How can we effectively utilize knowledge-transfer within different languages to improve Multi-lingual Neural Machine Translation in an extremely low-resource language setting?\n\u2022 RQ2: How does selectively pruning neuron-knowledge affects model's generalization and robustness? Additionally, how does pruning affects catastrophic forgetting in our Multi-lingual Neural Machine Translation system?\n\u2022 RQ3: How can we effectively utilize TX-Ray [Rethmeier et al., 2019] to interpret, visualize, and quantify knowledge-abstractions and knowledge-transfer, for the trained models in RQ1-2?", "sections": [{"title": "Introduction", "content": "Artificial neural networks are a series of interconnected neurons within multiple layers that mimics sensory processing of the brain [Priddy and Keller, 2005]. This subfield of machine learning, based on training deep(multi-layered) artificial neural networks is called Deep Learning or Deep neural learning. These neural networks acquire their power and ability to learn from data through an algorithm called backpropagation. The role of this algorithm is to optimize the network output by iteratively adapting the weight for each neuron(aka hidden units), thereby optimizing the learning process for that batch of data. However, this performance comes at a considerable price. Due to backpropagation, neural networks are sensitive to new information, and they often overwrite the learned knowledge. In other words, when something new is learned, it wipes out the old information. Nevertheless, the real world consist of an infinite number of novel scenarios and training a model on such data is nearly impossible.\nThe ability of neural networks to transfer knowledge used on new scenarios is commonly known as transfer learning. Transfer Learning is a traditional approach where a previously trained neural network is reinitialized as the starting point for a new neural network that learns an additional secondary task. In other words, learning from the first model is used as pretrained-weights to the second model. The conventional supervised learning paradigm, on the other hand, breaks down if we do not have enough labelled data for the end-task or domain we care about, for a reliable model. Transfer learning enables us to deal with such scenarios by leveraging the already existing labelled data on any related task or domain. We try to transfer this knowledge from the source task in the source domain and apply it to our problem of interest in the target domain. Unlike classical machine learning setups, this transfer of knowledge from one domain to another through limited data is essential to semi-supervised learning under domain shift.\nWhile transfer learning aids in generalizing to the scenarios that are not encountered during training, it partially configures the model to prioritize on the second end-task. Inspired by the idea behind learning in the biological human brain, lifelong learning is a methodology focused on the ability of cortical and subcortical circuits to add new information on the fly. Multi-task training is one of the recognized techniques that allows us to train neural networks across many tasks. However, this is only possible if all tasks are available at the time of primary training. In other words, for each new task, we would generally need to retrain our model on all the tasks again. In the real world, however, we would like a model that can gradually leverage from its experience. For that reason, we need to facilitate a model that can"}, {"title": "Research Questions", "content": "Through this work, we investigate on questions such as:\n\u2022 RQ1: How can we effectively utilize knowledge-transfer within different languages to improve Multilingual Neural Machine Translation, in an extremely low-resource language setting? Through this research question, we demonstrate how Multilingual Neural Machine Translation leverage knowledge-transfer within different languages (like English, German, French, and Spanish), in an extremely low-resource setting. Additionally, we also investigate how languages with similar roots affect this multilingual-training process.\n\u2022 RQ2: How does selectively pruning neuron-knowledge affects model's generalization and robustness? Additionally, how does pruning affects catastrophic forgetting in neural networks?; Following the research from our original paper [Rethmeier et al., 2019], we extend the scope of TX-Ray by examining the effects of selective pruning neuron-knowledge on multilingual translation quality. In this experiment, we selectively prune neuron-knowledge from the max, most-n, and least-n active neurons and examine their effects on Catastrophic forgetting.\n\u2022 RQ3: How can we interpret, visualize, and quantify knowledge-abstractions for the trained models in RQ1-2? Through this research question, we :\n(A): Visualize knowledge-abstractions to understand the evolution of positive and negative knowledge while training, for different translation pairs (namely English-English, English-German, English-French, and English-Spanish). Additionally, we also examine the effects of transfer and pruning on the knowledge-abstractions and translation quality.\n(B): Employ TX-Ray [Rethmeier et al., 2019]; an interpretable framework that utilizes visualizations as a means to express knowledge-transfer and different linguistic features learnt by the trained models."}, {"title": "Related Research", "content": "Over the years, Deep Neural Networks (DNNs) have achieved exceptional accomplishment by learning numerous intricate tasks in the fields of Computer Vision and Natural Language Processing(NLP). In this work, we utilize the learning capabilities of such state-of-the-art networks to perform the task of Machine Translation in an extremely low-resource setting, aka Neural Machine Translation(NMT). Neural Machine Translation is a subfield of computational linguistics focused on translating one language to another, through the means of a Neural Network. During the training, a sequence of words(tokens) is converted from the source language to one or many target languages. In case of latter, such a task is widely known as Multilingual Neural Machine Translation (Figure 2.1).\nNeural Machine Translation operates on sequential data over several time-steps. Translating one language to another language consists of a sequence of words of variable length, both from source language as well as the target language. However, deep learning architectures like Feed-Forward Neural Networks, aka Multi-layer perceptron networks(MLP) and Convolutional Neural Networks(CNNs) need a fixed input vector(size) to produce a sized fix output. In general, practices like"}, {"title": "Multi-task and Continual Learning", "content": "Multitask training techniques [Johnson, Schuster, Quoc V Le, et al., 2017b] have shown promising results in the field of Multilingual Neural Machine Translation. The de-facto approach here is to not only share the input vocabulary across all the tasks but also introduce an artificial token at the beginning of the input sentence to identify the target language. The intention here is to enable the knowledge-transfer between all the target languages while training on parallel batches. Whereas in transfer learning, the standard approach is to pre-train a sequence encoder and fine-tune it to a set of end-tasks. This fine-tuning is performed either by freezing the weights or by directly fine-tuning the pretrained model [Rethmeier et al., 2019]. Following the work of Sebastian Ruder in [Ruder, 2019], we design a neural machine translation system that uses sequential-transfer learning to perform Multilingual Neural Machine Translation. Sequential transfer learning is a setting where the training between the source and target tasks is committed sequentially. Therefore, unlike multitask learning, instead of optimizing learning jointly, it is optimized sequentially. The goal of sequential-transfer learning, as part of continual learning, is to transfer information from the pre-trained model to several domains, in a sequential fashion. In other words, a sequential-transfer setup is just a multi-step transfer learning setup.\nDespite the differences, sequential transfer learning (or continual learning) and Multi-task learning are closely related (Figure 2.2). They both utilize the transfer of knowledge from one domain to improve the performance in others. However, based on the research [Ruder, 2019], sequential-transfer learning-based approaches are helpful in scenarios where:"}, {"title": "TX-Ray : Transfer eXplainability as pReference of Activations analysis", "content": "Despite the tremendous success of sequential-transfer learning in natural language processing, neural-learning remains a deep mystery. The networks internal working is shielded from human eyes, concealed in layers of computations, making it hard to diagnose errors, activations, and biases. Understanding them will not only provide transparency in deep learning but will also help in further improving the performance. In this research, we attempt to interpret the transfer of knowledge through various visualizations, from one domain to another in the setting of Multilingual Neural Machine Translation. However, To accomplish that, there is a need to construct deep abstractions and instantiate them in rich interfaces. As mentioned in [Rethmeier et al., 2019], methods such as [Carter et al., 2019 and Yosinski et al., 2015] focus on decision-understanding [Gehrmann et al., 2019] by analyzing supervised probing tasks [Belinkov and Glass, 2019] using either performance metrics [Conneau and Kiela, 2018; Wang et al., 2019] or laborious per-instance explainability [Arras, Osman, et al., 2019; T. Wu et al., 2019]. These approaches analyze input-output relations for decision-understanding, whereas methods [Gehrmann et al., 2019] like [Carter et al., 2019; Pezzotti et al., 2018] visualize interpretable model abstractions learned via supervision.\nWith the tremendous success of transfer learning, NLP relies heavily on pre-trained data. The field currently lacks methods that allow one to explore, analyse, and quantify knowledge transfer mechanisms for pretraining or fine-tuning stages. Inspired by [Olah et al., 2018], that uses techniques like feature visualisation, attribution, and dimensionality reduction, we use TX-Ray [Rethmeier et al., 2019]; an interpretability framework that evaluates the knowledge transfer in a transfer-learning setup. Our framework not only helps us visualise what the network learns but also explain how it developed its understanding while keeping the amount of information to human-scale. From analysing recent model and explainability methods [Belinkov and Glass, 2019; Gehrmann et al., 2019; Gilpin et al., 2018], two"}, {"title": "Catastrophic forgetting in Continual Learning", "content": "The ability to learn in a sequential-transfer learning fashion is critical in natural language processing. Not only it helps to solve the problem of limited labelled data but the ability to transfer knowledge from previous experience also enables the network to learn and generalize better. While continual learning shows the promising results towards model generalization in natural language processing, it suffers from the problem of catastrophic forgetting. Forgetting in neural networks or humans is defined as the inability to extract information from memory earlier. Although forgetting in humans comes in many forms, the process is normal, adaptive, and a crucial aspect of the learning. Neurons in neural networks allow a similar mechanism of forgetting information while trained continually, to solve specialized end-tasks.\nWhile this helps in improving the performance of a specific end-task, is catastrophic from the generalized learning in such systems. Catastrophic Forgetting in Continual learning is a phenomenon due to which the network forgets crucial information while updating its memory to improve the performance on a specific task [C. V. Nguyen et al., 2019].\nSeveral approaches like Elastic Weight Consolidation(EWC), Incremental Moment Matching (IMM), and Neural Pruning (CLNP) [Golkar et al., 2019; Kirkpatrick et al., 2017; Lee et al., 2017] have shown promising results towards improving training in continual learning setup. These approaches focus on methods that enable selective freezing of the weights of the network to minimize catastrophic forgetting. However, to keep things simple, we choose to manually freeze the weights of our pre-trained network. Similar to the experiments performed by [G. Nguyen et al., 2020], we aim to improve transfer in the neural network through the means of interpretability techniques and visualizations, we implement our continual learning system by simply freezing the weights of our pre-trained network at every step [Soselia et al., 2018]. Furthermore, in this work, we experiment with various approaches by pruning specialized, generalized, and noise-sensitive knowledge to show its effects on model performance."}, {"title": "BLEU-4 as the Evaluation Metric", "content": "Inspired by the research in [Papineni et al., 2002], we choose Bilingual Evaluation Understudy Score, or in short BLEU, as our evaluation metrics. The metric calculates matching n-grams between the source and target sequences, where n represents the number of tokens/words compared in pairs, regardless of their order. In this work, we propose using BLEU-4 cumulative score that calculates the individual n-grams, weighted in all orders between 1 to n via geometric mean. In BLEU-4, the weights are uniformly distributed, i.e. 1/4 or 25%, between each of the 1-gram, 2-gram, 3-gram and 4-gram scores.\nEven though BLEU-4 score is not perfect, it is widely recognized as an easy, inexpensive, and language-independent technique for evaluating predictions in neural machine translation. An ideal BLEU-4 score lies between 0.0 and 1.0, where 1.0 represents a perfect match and 0.0 represents perfect mismatch."}, {"title": "Network Architectures", "content": "Over the years, sequence to sequence (seq2seq) models has shown promising results in the field of Neural Machine Translation [Sutskever et al., 2014]. Usually, seq2seq models are encoder-decoder models, that use a recurrent neural network (RNN)-based architecture to encode a source(input) sentence into a fixed-length vector otherwise known as context vector. This abstract representation of a given input sentence is decoded by a second RNN to produce the target(output) sentence, by generating one word(token) at a time.\nExploiting pre-trained Transformers(like BERT) have recently gained their popularity in the field of Natural Language Processing. Various researchers [Clinchant et al., 2019, Zhu et al., 2020, and Klein et al., 2017] have demonstrated extraordinary results by fine-tuning such pre-trained models for the task of Neural Machine Translation. However, Since one of our focus is in training a Multilingual Neural Machine Translation system in an extremely low-resource setting(i.e. with small training data and a uni-layered neural network model), we train a Long Short-Term Memory (LSTM), a Gated Recurrent Unit (GRU), and an Attention-based Bidirectional-Gated Recurrent Unit (A-GRU) sequential models to determine the architecture that performs the best for machine translation on our corpora [J\u00f6rg Tiedemann and Thottingal, 2020]. We briefly introduce each of these next:"}, {"title": "Long Short-Term Memory (LSTM)", "content": "Following the research by [Sutskever et al., 2014], we train an LSTM-based neural architecture to perform translation from one source language (x) to the target language (y). We first append start of sequence (sos) and end of sequence (eos) tokens to the source (English) sentence. This helps our model to understand the content between the start and end of every sentence in the dataset."}, {"title": "LSTM Encoder", "content": "The new input, \"sos good morning eos\", is then passed from the embedding layer (yellow) to the encoder unit (blue) of our network. At each time-step of the LSTM Encoder (Figure 2.5), the input to the encoder of the network is the embedding of the current token (e(xt)) and hidden state (ht\u22121) from the previous time-step. Together, LSTM Encoder can be represented as a function of e(xt) and ht\u22121 as:\n$h_t = LSTMEncoder(e(x_t), h_{t-1})$\nWhere $h_t$ represent the intermediatory hidden state, which is the vector representation of the sentence so far. Once the final token (eos) from the source sentence has been passed to the LSTM unit through the embedding layer, we use this final hidden state as the context vector (z), i.e. the vector representation for the entire source sentence."}, {"title": "LSTM Decoder", "content": "Once we have the final context vector (z), we pass it to the LSTM Decoder (Figure 2.6) to obtain our appended target sentence, \"sos guten morgen eos\". Now, at each time-step of the LSTM Decoder (blue), we have the embedding of the current token (d(yt)) and the hidden state (st\u22121) from the previous time-step. The initial hidden state (so) of the decoder is assigned as the context vector (z) obtained from the network's encoder. Please note that although the input/source embedding layer and the output/target embedding layer are both shown in yellow in Figure 2.6, they are two different embedding layers with their own initialized parameters. Similar to the LSTM Encoder, LSTM Decoder can be expressed as a function of d(yt) and st\u22121 as:\n$s_t = LSTMDecoder(d(y_t), s_{t-1})$\nHere, st represents the intermediatory hidden state, which is the vector representation of the sentence generated so far. The final hidden state is then passed through a linear layer(purple) to predict the next token in the sequence(\u0177t), in the supervised fashion."}, {"title": "Gated-Recurrent Units (GRU)", "content": "The Decoder in the LSTM-based architecture described earlier, has a significant downside. Since the hidden states need to contain information from the entire source sequence along with all the tokens decoded so far, the LSTM decoder compresses tons of information into these hidden states. Following the research by [Cho et al., 2014], we train a GRU based architecture, to achieve improved test perplexity whilst only using a layer in both the encoder and the decoder units. Researchers have demonstrated that both LSTMs and GRU's performs similarly, however, GRUs are found to be faster than the LSTMs and they abstract information better [Chung et al., 2014]."}, {"title": "GRU Encoder", "content": "The architecture implemented for GRU Encoder (Figure 2.7) is similar to the LSTM Encoder described previously, except the fact that the LSTM units are swapped with the GRUs. Similar to LSTMs, GRUs have several gating mechanisms that control the information flow. However, GRUs only requires and returns hidden states since there are no cell states. Similar to the LSTM Encoder, the GRU Encoder takes in a sequence of inputs, $X = \\{x_1, x_2, ..., x_T\\}$, and passes it through the embedding layer. A context vector, $z = h_t$, is then extracted after computing the hidden states, $H = \\{h_1, h_2, ..., h_T\\}$, obtained from the inputs. In other words, the operations inside the GRU Encoder can be represented through the following mathematical equation:\n$h_t = EncoderGRU(e(x_t), h_{t-1})$"}, {"title": "GRU Decoder", "content": "In the Decoder (Figure 2.8), however, instead of taking just the embedding target token d(yt) and previous hidden state st\u22121 as inputs, GRU units also takes the context vector z (dotted-red lines). Note that this context vector from the GRU Encoder is independent of the time-step is re-used as the same, for any time-step in the GRU Decoder. In the end, we pass the embedding of the current token, d(yt), along with the context vector, z, to the linear layer for predicting the next token in the supervised fashion.\n$s_t = GRUDecoder(d(y_t), s_{t-1},z)$\n$\\hat{y}_{t+1} = f(d(y_t), s_t,z)$"}, {"title": "Attention-based Bidirectional Gated Recurrent Units (A-BGRU)", "content": "Despite the better information compression achieved through GRUs, the context vector obtained through the encoder still needs to contain the entire information from the source sentence. To overcome this problem, we train an attention-based bidirectional GRU model to perform our task. The attention model of our networks calculates an attention vector, a, which represents the length of the source sentence. Since every element of this attention vector lies in between 0 and 1 (so that the entire vector space sums to 1), we then calculate a weighted sum of the hidden states, H, to get a weighted source vector while decoding, w. We use this as an input to our networks Decoder and linear layer to make the prediction."}, {"title": "Bidirectional GRU Encoder", "content": "Following the research by [Bahdanau et al., 2014], we implement our Encoder (Figure 2.9) using a layer of Bidirectional GRU unit. This enables the forward GRU to embedded the source sentence from left to right(light blue) and the backward GRU to embedded the same source sentence from right to left(dark blue). Therefore, just like before, the GRU can be formulated for both the directions by the below-mentioned equations. The direction of the arrows represents the direction of GRU Encoder. While the \u2192 represents forward direction, the \u2190 represents backward direction. Therefore, unlike before, we receive two context vectors, \u2192, and, \u2190, from the forward and backward directions respectively. The hidden state thus obtained from the GRU Encoder is the concatenation of the forward and backward hidden states, i.e. $h_1 = [\\overrightarrow{h_1};\\overleftarrow{h_T}], h_2 = [\\overrightarrow{h_2};\\overleftarrow{h_{T-1}}]$ and therefore, the encoder hidden states are represented as $H = \\{h_1,h_2, ..., h_T\\}$.\n$\\overrightarrow{h_t} = GRUEncoder(\\overrightarrow{e(x_t)},\\overrightarrow{h_{t-1}})$\n$\\overleftarrow{h_t} = GRUEncoder(\\overleftarrow{e(x_t)},\\overleftarrow{h_{t+1}})$"}, {"title": "Attention Layer", "content": "The attention layer (Figure 2.10) uses the previous hidden states of the decoder, St\u22121, along with the stacked forward and backwards hidden states from the encoder, H. An attention vector, at, equal to the length of the source sentence is then generated, where every element of the vector is between 0 and 1, and the entire vector sums to 1. This attention vector represents the features(token/words) in the source sentence that the model should emphasize on, to correctly predict the next token, \u0177t+1.\nTo determine the attention vector, we first calculate the energy between the previous decoder and the encoder hidden states. Since the encoder hidden states are sequences of N tensors, we duplicate the previous decoder hidden state T times."}, {"title": "GRU Decoder", "content": "The Decoder utilizes attention layer that takes the previous hidden states, St-1, along with the encoded hidden states, H, to return an attention vector, at. This attention vector, at, is concatenated with the weighted sum of the encoder hidden states, H, to create a weighted source vector, w\u2081. As shown in Figure 2.11, the embedded input,"}, {"title": "Dataset", "content": "Since one of the goals of this work is to train a generalized Multilingual Neural Machine Translation System, we train our network on the dataset provided by the Tatoeba Translation Challenge [J\u00f6rg Tiedemann and Thottingal, 2020]. The dataset covers over 500 languages for the task of Machine Translation in real-world scenarios, under realistically low-resource settings. The translation challenge includes training data from OPUS parallel corpus [J\u00f6rg Tiedemann and Nygaard, 2004] and test data from the Tatoeba corpus via the aligned dataset in OPUS. In this work, we show the performance of our network only on English-German, English-French, and English-Spanish datasets from [J\u00f6rg Tiedemann and Thottingal, 2020]. Additionally, since the source language for our system is always consistent i.e. English, we pre-train our network on English-German dataset from Tatoeba, with the source and target languages both being English. For more details, check sec."}, {"title": "Data-insights", "content": "As shown in the Fig. 3.1, the original Tatoeba Translation Challenge datasets contains 284768, 250098, and 194977 bilingual sentences for English-German, English-French, and English-Spanish in the training dataset respectively. Whereas, there are only 10000 bilingual sentences in each test dataset. For the pre-training(i.e. English-English), we have used the Tatoeba English-German bilingual corpus, which has"}, {"title": "Pre-processing and data cleaning", "content": "To keep things simple, we have avoided heavy data pre-processing/cleaning. Not only we removed all the special symbols and characters to keep the data consistent, but spelling checks and proper contractions (like you're -> you are, won't -> will not) are performed for the English language in every corpus. Also, we decided not to use any pre-trained embeddings during our analysis. For tokenization purposes, we have used pre-trained Spacy tokenizers."}, {"title": "Experiments", "content": "Since the objective of the experiments mentioned above is to find the most suitable architecture for Neural Machine Translation in a low-resource setting, we decided to retain our hyperparameters setting almost identical across all the architectures.\nThe implementation of the networks has been carried out in Pytorch. For optimization purpose, we have used Adam; a method of Stochastic Optimization with L2 penalty. Since our end task is to perform machine translation, we presume it as an N-class classification problem, where N represents the unique number of tokens/classes present in the target vocabulary. Therefore, we use CrossEntropy as our loss function that contains a combination of the logarithmic function of Softmax, and negative log-likelihood to solve a classification problem of N classes.\nWe obtain better results by training a multi-layered architecture. However, due to computational and low-resource setting reasons, we have focused our research by training and comparing uni-layered models only. During our experiments, we manually set the padding index to 0 and make sure that our loss function remains unaffected from the padded tokens. We achieve this by ignoring this padding index, i.e. 0 while computing the loss function. We use a standard split of 70-30 to divide the pretrained data, i.e. Tatoeba English-English data [J\u00f6rg Tiedemann and Thottingal, 2020] into training and test dataset."}, {"title": "Architectural details", "content": "To have a baseline model, we train all the architectures mentioned in section 2.6 to perform English-English, and English-German, English-French, and English-Spanish translations from [J\u00f6rg Tiedemann and Thottingal, 2020] and [J\u00f6rg Tiedemann and Nygaard, 2004] datasets respectively. The results in table 5.1, help us in identifying the best performing model for all our datasets. The details about the hyperparameters used while training is mentioned in table 4.1.\nAll the architectures in table 5.1 are trained in end-to-end fashion described in section 4.2.1. As can be observed in the table 5.1, the Attention-based Bidirectional Gated Recurrent Units (A-BGRU) architecture performs best on all our datasets. Therefore, for all of our further research, we only use A-BGU as our standard architecture."}, {"title": "Baselines", "content": "As mentioned in section 4.2, we perform end-to-end and 1-hop transfer training to establish a benchmark for the transfer models described in section 4.3. To give a quick recap, we perform translations in end-to-end, i.e., between the source and target languages directly (Figure 4.1) without any transfer. Whereas, in 1-hop transfer training, we first pre-train our translation model to perform English-English translations. We then fine-tune this pre-trained model to perform English-German, English-French, or English-Spanish translations in transfer-learning fashion (Figure 4.2 and 4.3)."}, {"title": "End-to-End training", "content": "For the end-2-end baseline (Figure 4.1), we train four different networks to perform English-English, and English-German, English-French, and English-Spanish translations. We take the training and test corpora for these networks from English-German dataset in [J\u00f6rg Tiedemann and Nygaard, 2004], and English-German, English-French, and English-Spanish from [J\u00f6rg Tiedemann and Thottingal, 2020], respectively.\nWe generate these results to have a rough estimation of how the network performs on our extracted parallel-dataset, for direct source-target translation. Please note that since we do not apply any transfer in our end-to-end networks, we find it unnecessary to share the vocabulary across all the different datasets. Also, for English-English translations, we take the source sequence from the from English-German dataset in [J\u00f6rg Tiedemann and Nygaard, 2004] and copy the same as the target sequence. Further details and explanations regarding the same are discussed in section 4.2.2."}, {"title": "1-hop Transfer", "content": "While End-to-End baseline shows how well our model performs for direct source-target translations, 1-hop transfer shows the effect of the direct transfer, from our pre-trained knowledge on the fine-tuned language. Since we perform all the translations from the English language, we argue that the model must develop an understanding of how to represent the source language perfectly. Therefore, we pre-train our Encoder-Decoder model to perform translations from English to the English language from English-German dataset in [J\u00f6rg Tiedemann and Nygaard, 2004] first (Figure 4.2).\nWhile most of the low-resource research in neural machine translation focuses on heavy pre-training [Johnson, Schuster, Quoc V. Le, et al., 2017a and Aharoni et al.,"}, {"title": "Step 2: 1-hop transfer", "content": "For the 1-hop transfer, we take the English-English dataset from English-German dataset in [J\u00f6rg Tiedemann and Nygaard, 2004] and share its input vocabulary with English-German, English-French, and English-Spanish dataset in [J\u00f6rg Tiedemann and Thottingal, 2020]. As shown in Figure 4.3, we then transfer the knowledge from English-English dataset to English-German, English-French, and English-Spanish datasets by using the pre-trained model from step 1 and separately fine-tuning it for English-German, English-French, and English-Spanish translation models respectively.\nAs mentioned earlier, to keep things simple and minimize catastrophic forgetting, we choose to freeze the weights of our pre-trained model before applying it for fine-tuning. Since the end-task for pre-trained and fine-tuned models are different, i.e. translation to different languages, we only share the Encoder from our pre-trained model during fine-tuning."}, {"title": "RQ1: Utilizing Knowledge-transfer between different languages", "content": "To understand the influence of knowledge-transfer from one/many languages to another, we train our network using both joint multi-task and sequential transfer learning approaches. In this section, we describe each one of them separately:"}, {"title": "Joint Multi-task transfer Learning", "content": "Unlike sequential-transfer learning (described in section 4.3.2) where the transfer of knowledge is only in the forward direction, we allow the knowledge-transfer to flow from all the target languages. The idea is to compare and observe how the knowledge-flow from all the target languages destroys and build knowledge on the top of each other in a Multilingual-Neural Machine Translation system."}, {"title": "Step 1: Pre-training", "content": "Similar to the training performed in Figure 4.2, we perform pre-training on English-German dataset from [J\u00f6rg Tiedemann and Nygaard, 2004]. As mentioned earlier, we duplicate the source sequences to target sequences and train a neural machine translation model to perform English-English translations for pre-training. The idea here is to let again the model have an understanding of how to represent the source language before applying transfer-learning."}, {"title": "Step 2: Multi-task transfer", "content": "To enable the knowledge-transfer flow from every target language, we first combine the training and test datasets from English-German, English-French, and English-Spanish corpora in [J\u00f6rg Tiedemann and Thottingal, 2020]. Like earlier, we share the input vocabulary from our extracted English-English dataset with the new combined dataset. The idea is to generate multilingual translations by fine-tuning on our pre-trained English-English translation model. Therefore, we call this training as the joint multi-task transfer training, as all the end-tasks are combined and provided at the get-go.\nAs shown in Figure 4.4, the Encoder from the pre-trained English-English translation model is shared while fine-tuning our model on the new combined dataset. We argue that since our objective is to understand how knowledge-transfer from multiple languages destroys and build to model learning, we optimize our loss together for multilingual translations. Note that this is completely different from the conventional multitask training, where all the losses are optimized separately to find a common local minima."}, {"title": "Step 3: Testing", "content": "Since we save our model on the basis of combined validation loss, we test our trained model in two different circumstances (Figure 4.5). In the first case, we try to obtain the combined BLEU-4 score on the combined test dataset from the English-German (red colour), English-French (green colour), and English-Spanish (blue colour) dataset in [J\u00f6rg Tiedemann and Thottingal, 2020]. This shows our models capability to perform multilingual translations. In the second case, we evaluate all the three datasets"}, {"title": "Sequential Transfer Learning", "content": "In our sequential-transfer learning setup, we transfer knowledge only in the forward direction. In other words, the fine-tuned model does not affect the knowledge-abstraction of the pre-trained model at any stage of training. At each point of"}]}