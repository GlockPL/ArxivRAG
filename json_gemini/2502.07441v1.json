{"title": "SensPS: Sensing Personal Space Comfortable Distance between Human-Human Using Multimodal Sensors", "authors": ["Ko Watanabe", "Nico F\u00f6rster", "Shoya Ishimaru"], "abstract": "Personal space, also known as peripersonal space, is crucial in human social interaction, influencing comfort, communication, and social stress. Estimating and respecting personal space is essential for enhancing human-computer interaction (HCI) and smart environments. Personal space preferences vary due to individual traits, cultural background, and contextual factors. Advanced multimodal sensing technologies, including eye-tracking and wristband sensors, offer opportunities to develop adaptive systems that dynamically adjust to user comfort levels. Integrating physiological and behavioral data enables a deeper understanding of spatial interactions. This study aims to develop a sensor-based model to estimate comfortable personal space and identify key features influencing spatial preferences. Here we show that multimodal sensors, particularly eye-tracking and physiological wristband data, can effectively predict personal space preferences, with eye-tracking data playing a more significant role. Our experimental study involving controlled human interactions demonstrates, that the Transformer model achieves the highest predictive accuracy (F1 score: 0.87) for estimating personal space. Eye-tracking features, such as gaze point and pupil diameter, emerge as the most significant predictors, while physiological signals from wristband sensors contribute marginally. These findings highlight the potential for AI-driven personalization of social space in adaptive environments. Our results suggest that multimodal sensing can be leveraged to develop intelligent systems that optimize spatial arrangements in workplaces, educational institutions, and public settings. Future work should explore larger datasets, real-world applications, and additional physiological markers to enhance model robustness.", "sections": [{"title": "1 Introduction", "content": "Personal space [11, 14, 31], also known as Peripersonal space, is a fundamental aspect of human social interaction, representing the comfortable distance indi-"}, {"title": "2 Related Work", "content": "This section reviews the related work of eye-tracking technologies, wristband sensors, cognitive state estimation with multimodal sensors, and personal space estimation."}, {"title": "2.1 Eye-Tracking Technologies", "content": "Research on eye-tracking has been carried out across various fields, including HCI and psychology. Studies have demonstrated that eye-tracking can be utilized to estimate factors such as confidence [5], personality [3], attention [41], and cognitive load [46]. Additionally, eye-tracking is employed in diverse interaction tasks like gaze-based typing [10, 26], menu navigation [23], and object selection [13]. The choice of eye-tracking device varies depending on the data type and task. Examples include PC-mounted eye-tracking [5, 16, 17], eye-tracking glasses [9, 28], head-mounted displays [29], and webcam-based eye-tracking [4, 34, 35]."}, {"title": "2.2 Wristband Sensors", "content": "Wristband sensors are wearable devices that can measure physiological signals such as heart rate [40], skin conductance [27], and galvanic skin response [6]. These sensors have been widely used in research and practical applications due to their ability to provide continuous, real-time data in a non-invasive manner. Studies have demonstrated that wristband sensors are used for various tasks, such as estimating stress [27], concentration [40], workload [7], and mind wandering [8]. Additionally, wristband sensors are used for interaction tasks, such as object interaction [24], text entry [19], and step aware voice instructions [1]. This study uses a wristband sensor to measure physiological signals such as heart rate and skin conductance. These features are significant for estimating cognitive state, which is a key factor for estimating personal space."}, {"title": "2.3 Personal Space", "content": "Personal space, also known as peripersonal space, refers to the comfortable distance individuals maintain from others, which varies subjectively among individuals [20, 31].\nCoello et al. [14] explored the concepts of Interpersonal Space (IPS). IPS is the area individuals maintain between themselves and others during social interactions. When this space is encroached upon, it often leads to discomfort, prompting individuals to increase the distance to regain comfort. Our study emphasizes PPS to investigate whether physiological signals can predict personal space preferences.\nCandini et al. [11] examined the direct link between physiological responses and the regulation of interpersonal space. Their study employed an ecological experimental setup where participants' skin conductance response (SCR) was measured as a confederate approached or withdrew from them at varying distances. The results showed a significant increase in SCR when participants were"}, {"title": "3 Methodology", "content": "In this section, we describe the device selection, data preprocessing, and machine learning and deep learning models."}, {"title": "3.1 Device Selection", "content": "In this study, we employ the Pupil Core eye-tracking glasses 4 and the Empatica E4 wristband sensor 5. Figure 1 illustrates these devices.\nThe Pupil Core is a high-performance eye-tracking device renowned for its accuracy and dependability in capturing gaze data. It features binocular eye tracking with a sampling rate of up to 200 Hz, ensuring precise and real-time data collection. It is equipped with high-resolution cameras (1920x1080 pixels) and a 90-degree field of view, which provides comprehensive eye movement tracking. The Pupil Core supports 2D and 3D eye tracking, making it versatile for various research applications. Weighing only 35 grams, it is lightweight and comfortable for extended use. The adjustable headband ensures a secure fit across different head sizes. The device is compatible with various software tools from Pupil Labs, such as Pupil Capture and Pupil Player, facilitating data recording, visualization, and analysis. It offers robust connectivity options, including USB and Wi-Fi, allowing seamless integration with other devices and systems. This enables easy real-time data transfer and processing, enhancing research workflow efficiency. Overall, the Pupil Core's advanced features and user-friendly design make it"}, {"title": "3.2 Data Preprocessing", "content": "Data Cleaning: This section details the preprocessing steps for the eye-tracking and wristband sensor data. First, we synchronized the data with the task duration by trimming it to the relevant period. For each trial, we recorded the start and end timestamps of the task. These timestamps were used to trim the data from the Pupil Core and Empatica E4 sensors to match the task duration. After trimming, we applied preprocessing to eliminate noise and artifacts. For the eye-tracking data, we used Pupil Labs' built-in tools for noise and artifact removal. For the Empatica E4 sensor data, we removed NaN values and normalized the dataset. We also discarded any incomplete or missing data resulting from wireless disconnections.\nFeature Extraction: We extracted features from the data across statistical, temporal, and frequency domains. Features such as fixation, saccade, and blinks were extracted for the eye-tracking data. For the Empatica E4 wristband data, features included EDA, skin temperature, and accelerometer readings.\nSliding Window: A sliding window approach was employed to extract features from the data. The features extracted are akin to those used in Discaas [45] and Waistonbelt [30]. The window size was set to ten seconds, with a five-second overlap. Features were extracted from each window segment. These features were then used to train machine learning and deep learning models.\nLabeling: In this study, we categorized comfortable distance into two classes: comfort and discomfort. This was achieved by converting the Likert scale results into binary labels based on a predefined threshold. Specifically, we classified data"}, {"title": "3.3 Machine Learning and Deep Learning Models", "content": "Machine Learning: For machine learning, we use support vector machine (SVM), decision tree (DT), and random forest (RF) to perform binary classification. A support vector machine (SVM) is a supervised learning model that finds the optimal hyperplane that maximizes the margin between the two classes. Decision tree (DT) is a non-parametric supervised learning method for classification and regression. It splits the data into subsets based on the value of input features, creating a tree-like model of decisions. Random forest (RF) is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes for classification. It helps improve accuracy and control overfitting. All three methods are applied to classify the data into comfort and discomfort.\nDeep Learning: VGG16 [37] is a popular neural network for image classification, with 16 layers: 13 for convolution and 3 for fully connected tasks. It uses small filters to capture image details and pooling layers to simplify data, making it efficient and effective.\nMobileNet [22] is a lightweight neural network for mobile, using fewer parameters to save resources. It includes a base model, pooling, and a dense layer for binary classification, optimized with Adam and binary cross-entropy loss. MobileNet V2 [32] and V3 [21] are similar but more efficient, with V3 offering better performance for tasks like image classification and object detection.\nThe Transformer architecture [42] is a deep learning model widely used for various tasks, including natural language processing. The Transformer model consists of several key components:\nMulti-Head Attention Layer: This component allows the model to focus on different parts of the input sequence simultaneously. It enhances the model's ability to capture complex dependencies within the data.\nFeed-Forward Network (FFN): This is a simple neural network with two layers, where the first layer uses a ReLU activation function. It processes the output from the attention layer to further transform the data.\nLayer Normalization: This technique normalizes inputs across features, which helps stabilize and accelerate the training process by ensuring consistent input distributions.\nDropout Layers: These layers are used as a regularization technique to prevent overfitting. They work by randomly setting a fraction of input units to zero during training, which helps the model generalize better to new data.\nCreating a Transformer model involves constructing and compiling the model to effectively capture dependencies and relationships in the input data, making it suitable for various tasks."}, {"title": "4 Data Collection", "content": "In this section, we describe the experimental setup and data collection process."}, {"title": "4.1 Participants", "content": "We recruited ten participants for this study, aged between 21 and 28 years (Mean = 25.1). The sample consisted of seven males and nine females. Participants came from a diverse range of countries, including Albania, India, Turkey, USA, China, Russia, Azerbaijan, Brazil, Germany, and Iran. This diversity was intentional to minimize cultural bias in the results. All participants were either undergraduate or graduate students at the University of Kaiserslautern-Landau. Informed consent was obtained from all participants, in compliance with GDPR regulations, explaining the use of their data."}, {"title": "4.2 Experimental Procedure", "content": "This study employs the Pupil Core eye-tracking glasses and the Empatica E4 wristband sensor. Figure 2 illustrates the experimental setup. One participant is designated as stationary, while the other wears eye-tracking glasses and a wristband sensor. The Pupil Core glasses are positioned on the forehead, with cameras aligned to capture both eyes. The Empatica E4 wristband connects to the computer via a USB cable.\nParticipants first review and sign the consent form. They then wear the Pupil Core eye-tracking glasses and Empatica E4 wristband sensor and sit in front of a computer screen. A calibration and validation process follows to ensure the correct alignment of the devices. For the eye-tracking glasses, participants focus on a fixation point for alignment. For the wristband sensor, participants press a button to initiate data collection.\nAfter calibration and validation, participants perform a task involving estimating comfortable personal space. They are seated in a chair. During each trial,"}, {"title": "5 Result and Discussion", "content": "Table 1 indicates that the Transformer model significantly outperforms other models with an F1 score of 0.87, demonstrating its superior capability in capturing complex dependencies in the data. Random Forest follows with a respectable F1 score of 0.67, making it this study's best-performing traditional machine-learning model. MobileNet and its variants show moderate performance. MobileNet achieves an F1 score of 0.63, while MobileNet V2 and V3 lag behind."}, {"title": "6 Limitations and Future Work", "content": "The number of participants in the study is limited, which may affect the generalizability of the results. In this study, we only used ten participants. The number of participants is not enough to generalize the results to the general population. Additionally, the study does not account for individual differences such as personality traits, cultural background, gender, and situational context, which could influence the outcomes. This may allign to the bias of the participants.\nThe trial size in this study is also relatively small, as we only collected two minutes of data per trial for each participant. This limited duration may not capture the full range of variability in the participants' responses and could affect the robustness and reliability of the findings. The data size can be larger by asking participants to perform the task for longer periods.\nTo improve the robustness and reliability of the findings, future studies should consider applying a sliding window approach with longer durations and varying overlap intervals. This method would allow for a more comprehensive data analysis by capturing a wider range of variability in the participants' responses over extended periods. The larger data size can achieve in applying different size of window for the sliding window approach.\nThe task conducted in this study does not reflect natural conditions, as participants were fully aware that they were part of an experiment, which may have influenced their behavior and responses. Ideally, such experiments should be conducted in more naturalistic settings, often called \"in the wild,\" to obtain more genuine and ecologically valid data. Additionally, the experiment did not consider the different types of discussions that participants might engage in, which could have varying impacts on their physiological and psychological responses.\nThe sensors utilized in this study are restricted to two types: eye-tracking glasses, which monitor and record the participants' gaze and pupil diameter, and a wristband sensor, which measures various physiological signals such as electrodermal activity (EDA) and heart rate. This limitation in sensor variety may affect the comprehensiveness of the data collected, as other potentially relevant physiological and behavioral signals are not captured.\nTo enhance the generalizability of the results, it is essential to increase the sample size and include a more diverse participant pool in future work. Incorporating a broader range of individual differences, such as personality traits and cultural backgrounds, will provide a more nuanced understanding of the factors influencing outcomes. Extending the data collection duration and employing advanced data analysis techniques, like the sliding window approach, will help capture a more comprehensive range of participant responses. Conducting experiments in naturalistic settings will improve ecological validity, and expanding"}, {"title": "7 Conclusion", "content": "This study explored the estimation of comfortable personal space using multimodal sensors, integrating eye-tracking and wristband-based physiological data. Our findings indicate that deep learning models, particularly the Transformer model, effectively predict personal space preferences, achieving an F1 score of 0.87. Eye-tracking data plays a more significant role than wristband sensor data. These results highlight the potential for intelligent systems to personalize spatial arrangements in workplaces, educational institutions, and public settings, enhancing user comfort and reducing social stress. However, the study has limitations, including a small participant pool and controlled experimental conditions that may not fully reflect real-world scenarios. Future research should expand participant diversity, explore naturalistic settings, and integrate additional physiological and behavioral markers to improve model robustness. By advancing sensor-based estimations of personal space, this research contributes to the development of adaptive HCI that dynamically respond to individual comfort needs, paving the way for more socially aware intelligent systems."}]}