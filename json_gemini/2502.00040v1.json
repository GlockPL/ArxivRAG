{"title": "Multi-Objective Reinforcement Learning for Power Grid Topology Control", "authors": ["Thomas Lautenbacher", "Ali Rajaei", "Davide Barbieri", "Jan Viebahn", "Jochen L. Cremer"], "abstract": "Transmission grid congestion increases as the electrification of various sectors requires transmitting more power. Topology control, through substation reconfiguration, can reduce congestion but its potential remains under-exploited in operations. A challenge is modeling the topology control problem to align well with the objectives and constraints of operators. Addressing this challenge, this paper investigates the application of multi-objective reinforcement learning (MORL) to integrate multiple conflicting objectives for power grid topology control. We develop a MORL approach using deep optimistic linear support (DOL) and multi-objective proximal policy optimization (MOPPO) to generate a set of Pareto-optimal policies that balance objectives such as minimizing line loading, topological deviation, and switching frequency. Initial case studies show that the MORL approach can provide valuable insights into objective trade-offs and improve Pareto front approximation compared to a random search baseline. The generated multi-objective RL policies are 30% more successful in preventing grid failure under contingencies and 20% more effective when training budget is reduced - compared to the common single objective RL policy.", "sections": [{"title": "I. INTRODUCTION", "content": "The energy transition and the shift toward renewable energy sources are crucial steps for mitigating climate change and ensuring a sustainable energy future. However, this transition poses significant operational challenges for system operators, including congestion management. Transmission network topology control is an under-utilized and non-costly source of flexibility. Adjusting the network topology, such as line switching or modifying busbar connections within substations, can reroute power flows to prevent line overloads and mitigate cascading outages [1]\u2013[3]. In addition to maintaining continuous electricity supply, power systems must address other objectives, such as minimizing asset wear, reducing operational cost, and mitigating environmental impacts. Achieving these objectives requires a multi-objective approach to decision-making that maintains grid security while addressing other operational objectives [4].\nTransmission network topology control problem can be modeled as a mixed-integer non-linear optimization problem which is computationally challenging to approach. The so-called combinatorial explosion of possible topologies and the complex nonlinear nature of power systems [1] makes this problem challenging. To address these challenges, heuristic and expert rule-based approaches, such as in [5]\u2013[8] are developed to determine corrective topological actions to relieve congestion. However, these approaches do not provide a sequence of control actions and may lead to sub-optimal solutions. To provide sequences of actions, recently researchers explored the use of reinforcement learning (RL) and Artificial Intelligence (AI) more broadly for topological control [3], [9]\u2013[11]. Studies such as [12]\u2013[15] explore RL-based approaches, including the deep duelling Q-network (DDQN) initialized with imitation learning [12], the Semi-Markov actor-critic algorithm [13], the cross-entropy method with importance sampling [14], and the proximal policy optimization (PPO) [15]. Additionally, [16] develops an AlphaZero-based approach using Monte-Carlo tree search to simulate future outcomes, and guide the agent toward long-term strategies, while [17] presents a curriculum-based approach to improve learning efficiency and stability. Building on these ideas, [18] combines curriculum learning with tree search to benefit from long-term strategies as well as the efficiency and stability. Some studies focus on addressing the combinatorial explosion of the topology control problem through hierarchical RL [19] and multi-agent RL [20]. Furthermore, [21] proposes a reward design using multiple metrics to reduce overloads. However, the developed approaches in [5]\u2013[8], [12]\u2013[21] focus on single operational objectives and providing only a single policy. This limits their application to address the trade-offs inherent in the multi-objective nature of power systems and to provide a set of policies for operators to select from.\nThis paper proposes a multi-objective RL (MORL) approach to address the network topology control problem. Despite"}, {"title": "II. METHODOLOGY", "content": "This paper aims to provide a decision-support approach for transmission system operators to perform topological control considering multiple objectives. The proposed approach integrates a multi-objective adaptation of the PPO algorithm [22], [23] (MOPPO) with deep optimistic linear support [24]. To capture different operational objectives, we design custom reward functions that address line loading, topological deviation and switching frequency. The proposed approach considers a multi-policy MORL [25], which results in a set of optimal solutions rather than a single solution. This allows system operators to better understand the trade-offs among objectives and select the most appropriate policy. Fig. 1 depicts the proposed approach. By using DOL as an outer loop method within the MORL approach, the convex coverage set of solutions is constructed iteratively. In each iteration, the DOL generates a new set of weight vectors w and gives one weight vector with the highest priority to MOPPO, that is then trained in the multi-objective environment. MOPPO uses w to account for the multiple rewards $R_t$. After training, the MOPPO is evaluated. If the average value vector over the evaluation episodes $V$, found by the MOPPO, is Pareto optimal, it is added to the convex coverage set (CCS). The detailed methodology is explained in the following."}, {"title": "A. Single-Policy Multi-Objective PPO", "content": "In order to learn on multiple rewards, the agent needs to receive a reward signal for each of the objectives. To this end, we extend the original RL grid environment into a multi-objective environment, allowing the agent to receive a d-dimensional reward vector $r_t \\in R^d$, where $d$ is the number of objectives. In this paper, the rewards reflect the operational objectives of reducing the line loading, decreasing the topological deviation, and reducing the switching frequency, explained in detail in section II-B."}, {"title": "B. Reward functions for operational objectives", "content": "1) Line Loading Reward: The L2RPNReward, referred to here as Line Loading Reward, is the conventional reward"}, {"title": "C. Deep Optimistic Linear Support", "content": "To generate a set of policies, we employ the deep optimistic linear support (DOL) algorithm to iteratively construct the optimal solution set of policies [24]. DOL functions as an outer-loop MORL approach [25], leveraging optimistic linear support (OLS) [27] to iteratively construct the convex coverage set (CCS) of a multi-objective problem by generating scalarization functions in the form of weight vectors w and updating the solution set accordingly [25]. Additionally, here DOL facilitates integration of MOPPO as a multi-objective single policy RL Algorithm.\nAlgorithm 2 provides details of the proposed DOL approach. In the first iteration, DOL assigns the extrema weights to the queue with infinite priority, ensuring these weights are processed first by the MOPPO algorithm (line 7). For each"}, {"title": "D. Policy Selection", "content": "Despite the multi-objective nature of the problem, the ultimate goal of the system operator is maintain secure system operation as long as possible [4]. To this end, Algorithm 3 is proposed to identify the best-performing policy as a recommendation for the system operator. We assess policy quality using a metric independent of the rewards, referred to as Episode Duration (E) [26]. E measures how long the agent can prevent the power system from premature grid failure. After generating the complete set of policies, we select the best-performing policy ($\\pi_{MO}$) for each seed run based on (E). Policies trained solely on extreme weights are excluded from consideration, as the focus is on selecting policies optimized for multiple rewards."}, {"title": "III. CASE STUDIES", "content": "All case studies are performed on the RTE 5-bus system in the Grid2Op environment [26], providing initial insights into a MORL approach for topology control. Figure 2 shows the 5-bus system. Each environment scenario lasts a week with a 5-min resolution (2016 time steps), which corresponds to the maximum episode duration (E). We use 16 scenarios for training, 2 scenarios for validation, and 2 scenarios for testing. The experiments are performed on up to 20 random seeds initializing the environment. The PPO neural network architecture consists of 2 fully connected layers with 64-dimensional hidden features. Training is performed using the Adam optimizer with a learning rate of $5 \\times 10^{-4}$ and a batch size of 512. The MOPPO algorithm assumes 4 update cycles. In the case study on robustness to contingencies (Section III-C), an adversarial agent is considered to simulate N-1 contingency states by randomly targeting power lines. Additionally, a set of common expert rules are considered to improve the performance and ensure safety, as detailed in [8], [14]. All computations are performed using DelftBlue's super-computer, equipped with Intel XEON E5-6248R 24C 3.0GHz CPU cores [28]. We use Grid2Op 1.10, LightSim2Grid 0.8, pandapower 2.14, gymnasium 0.29, mo-gymnasium 1.1 and the morl-baselines 1.0 package. The code for this study is publicly available in [29].\nIn Section III-B, we compare our approach to a random sampling (RS) benchmark, which replaces the DOL component by randomly selecting weight vectors from a uniform distribution. These randomly selected weights are then provided to the MOPPO, following the same process as in the DOL-based approach. To enhance this baseline, we incorporate the extrema weights [27], as exploring these weights is expected to yield significant gains in the objective space."}, {"title": "B. Pareto Front Approximation", "content": "Table I presents the results for hypervolume, sparsity and inverted generational distance (IGD) of the proposed DOL approach and RS. The hypervolume metric evaluates the spread and distribution of the solution space, the sparsity metric quantifies the density of the solution set, and IGD measures how accurately the generated solution set approximates the"}, {"title": "C. Robustness to N-1 Contingencies", "content": "This case study investigates the robustness of MO policies that are trained on multiple rewards compared to SO policies under N-1 contingency states. The contingency states are generated by an adversarial attacker, which disconnects power lines at random. We use the same settings for the adversarial attacks as in [19]. The multi-objective policies (MO policies) are trained on $R^L$, $R^D$, and $R^F$ rewards, while the SO policy is trained on the common $R^L$ reward. The MO policies are selected based on Algorithm 3 considering the Episode Duration metric E. The following scenarios are considered:\n\u2022\nNo Contingencies: The environment does not include any unplanned contingencies (baseline).\n\u2022 Moderately Frequent Contingencies: line disconnection randomly at maximum twice a day.\n\u2022 Highly Frequent Contingencies: line disconnection randomly at maximum four times a day."}, {"title": "D. Efficient Training", "content": "This case study investigates the training efficiency of MO and SO policies when computational resources are limited. Similar to the previous case study, we evaluate performance using the average episode duration and select the best MO policies according to Algorithm 3. Table III compares MO and SO policies considering the following training scenarios:\n\u2022\nFull Training Budget: The agent is trained with the default number of interactions (2048 training samples, 4 update cycles).\n\u2022 Moderate Training Budget: The agent is trained on 75% of the training (1536 training samples, 3 update cycles).\n\u2022 Low Training Budget: The agent is trained on 50% of the training (1024 training samples, 2 update cycles)."}, {"title": "IV. DISCUSSION AND CONCLUSION", "content": "This paper presents the first investigation into multi-objective reinforcement learning (MORL) for power grid topology control. We demonstrated trade-offs exist among conflicting operational objectives in the underlying problem"}], "equations": ["V^{\\pi} (s_t) = E_{\\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t | s_t = s \\right],", "A_t = w \\cdot A_t,", "Margin_{l,t} = \\begin{cases} \\frac{F_l - |F_{l,t}|}{F_l} & \\text{if } |F_{l,t}| < F_l \\\\ 0 & \\text{if } |F_{l,t}| \\geq F_l \\end{cases},", "R_t^L = \\sum_{l=1}^{L} (\\text{Margin}_{l,t})^2,", "D_{i,t} = \\begin{cases} 1, & \\text{if any element in substation } i \\text{ is assigned to a different bus}, \\\\ 0, & \\text{otherwise}. \\end{cases}", "D_t = \\sum_{i=1}^{N} D_{i,t},", "R_t^D = \\begin{cases} r_{\\text{default}}, & \\text{if } D_t = 0, \\\\ r_{\\text{minor}}, & \\text{if } D_t < d_{\\text{threshold}}, \\\\ r_{\\text{major}}, & \\text{otherwise}, \\end{cases}", "F_t = \\sum_{t \\in T_m} \\text{switching actions in interval } m \\text{ up to } t", "R_t^F = \\begin{cases} r_{\\text{DoNothing}}, & \\text{if } F_t = 0, \\\\ r_{\\text{low}}, & \\text{if } F_t < F_{\\text{low}}, \\\\ r_{\\text{high}}, & \\text{if } F_t \\geq F_{\\text{high}}, \\end{cases}"]}