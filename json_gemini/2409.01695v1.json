{"title": "USTC-KXDIGIT System Description for ASVspoof5 Challenge", "authors": ["Yihao Chen", "Haochen Wu", "Nan Jiang", "Xiang Xia", "Qing Gu", "Yunqi Hao", "Pengfei Cai", "Yu Guan", "Jialong Wang", "Weilin Xie", "Lei Fang", "Sian Fang", "Yan Song", "Wu Guo", "Lin Liu", "Minqiang Xu"], "abstract": "This paper describes the USTC-KXDIGIT system submitted to\nthe ASVspoof5 Challenge for Track 1 (speech deepfake de-\ntection) and Track 2 (spoofing-robust automatic speaker veri-\nfication, SASV). Track 1 showcases a diverse range of techni-\ncal qualities from potential processing algorithms and includes\nboth open and closed conditions. For these conditions, our sys-\ntem consists of a cascade of a front-end feature extractor and\na back-end classifier. We focus on extensive embedding engi-\nneering and enhancing the generalization of the back-end clas-\nsifier model. Specifically, the embedding engineering is based\non hand-crafted features and speech representations from a self-\nsupervised model, used for closed and open conditions, respec-\ntively. To detect spoof attacks under various adversarial con-\nditions, we trained multiple systems on an augmented training\nset. Additionally, we used voice conversion technology to syn-\nthesize fake audio from genuine audio in the training set to en-\nrich the synthesis algorithms. To leverage the complementary\ninformation learned by different model architectures, we em-\nployed activation ensemble and fused scores from different sys-\ntems to obtain the final decision score for spoof detection. Dur-\ning the evaluation phase, the proposed methods achieved 0.3948\nminDCF and 14.33% EER in the close condition, and 0.0750\nminDCF and 2.59% EER in the open condition, demonstrating\nthe robustness of our submitted systems under adversarial con-\nditions. In Track 2, we continued using the CM system from\nTrack 1 and fused it with a CNN-based ASV system. This ap-\nproach achieved 0.2814 min-aDCF in the closed condition and\n0.0756 min-aDCF in the open condition, showcasing superior\nperformance in the SASV system.", "sections": [{"title": "1. Introduction", "content": "Automatic Speaker Verification (ASV) systems has been widely\nused in many human-machine interfaces, where a person's iden-\ntity is automatically verified in biometric authentication. Mean-\nwhile, with recent advances in deep learning based text-to-\nspeech (TTS) and voice conversion (VC), speech synthesis [1]\ncan readily generate extremely near-realistic and human-like\nspeech, which however is often indistinguishable to human\nears and poses a serious threat to state-of-the-art ASV systems.\nTherefore, to deal with these issues, it is urgent to develop ef-\nfective and robust spoof speech detection systems.\nIn practice, speech deepfake detection has drawn great at-\ntention in the AI community and the technology industry. To\nbenchmark the progress of research in speech deepfake detec-\ntion, the ASVspoof Challenge [2-5] challenge releases a series\nof spoofing datasets to imitate diversified and challenging at-\ntack situations in realistic applications, including TTS, VC and\nspeech replay attacks. The ASV spoof 2021 challenge [5] has in-\ncluded more data to simulate quite practical and realistic scenar-\nios of different spoofing attacks. There are three sub-challenges:\nphysical access (PA), logical access (LA) and deepfake detec-\ntion (DF). LA and DF both aim to detect the spoofing speech\ngenerated by various advanced text-to-speech (TTS) and voice\nconversion (VC) techniques. The ASV spoof 2021 LA aimed to\nevaluate the robustness of the spoofing detection model against\ndifferent channel effects by adding various codec and channel\ntransmission effects to the evaluation data. As a extended from\nthe LA track, the ASVspoof 2021 DF task aims to evaluate\nthe spoofing detection system against different unknown con-\nditions. In addition to the series of ASVspoof challenges, the\nAudio Deepfake Detection Challenge (ADD) [6] extends the at-\ntack situations of fake audio detection to further address diver-\nsified and challenging attack situations in realistic applications.\nTo address the issue of voice spoofing, the first Spoofing-Aware\nSpeaker Verification (SASV) Challenge [7] aims to integrate re-\nsearch efforts in speaker verification and anti-spoofing.\nRecently, self-supervised pre-trained model has achieved\nsignificant advances in the fields of speech deepfake detec-\ntion [8-10], which far exceeds the performance of conventional\nmethods. It was shown that building a general pre-trained model\nbased on the exploitation of a large mount of unlabeled data can\nbe quite essential to boost the performance, reduce data labeling\nefforts and lower entry barriers for speech deepfake detection.\nIn this work, we also apply the open-sourced wav2vec2-large\npre-trained model as the feature extractor to help build a ro-\nbust detection system. However, deepfake speech exhibits lo-\ncal/global artifacts [11], which vary across different TTS and\nVC algorithms, and speech could be of varied technical qual-\nity stemming from the application of different processing al-\ngorithms. These both imply the impossibility of being fully\ncovered by the training set of an SSD system. Generally, the\nmost intuitive way to address the abovementioned issues is data\naugmentation [10, 12, 13]. Another solution is to provide multi-\nscale information for model training [14, 15], which can avoid\ninformation loss in the training process by increasing model ca-\npacity. In addition, a potential direction for improving gen-\neralization ability is one-class learning. OC-softmax and its\nvariants [16, 17] were proposed for speech deepfake detection."}, {"title": "2. Task Description and Datasets", "content": "ASVspoof5 [18] is centred around two subtasks: stand-alone\nspeech deepfake detectors (Track 1), and spoofing-robust auto-\nmatic speaker verification solutions (Track 2). There are two\nconditions, open and closed, for both Track 1 and Track 2. For\nclosed condition, participants commit to using data within the\nASVspoof5 training partition only, and however participants\nmay use external data and pre-trained models in the open con-\ndition.\nFor the Track 1 Closed task, we only used the training data from\nASVspoof5. For Track 1 Open task, the datasets used for train-\ning included:\n\u2022 ASVspoof5 train data\n\u2022 ASVspoof2021 LA, ASV spoof2021 DF data\n\u2022 Our own business data\nFor the Track 2 tasks, our CM system remained consistent with\nTrack 1, while the ASV system used only the VoxCeleb2 data."}, {"title": "3. Augmentation techniques", "content": "The number of utterances in the evaluation data set was much\nlarger than in training or developing sets. Therefore, to re-\nduce over-fitting and bias caused by diversified noise, we ap-\nply different augmentation methods for different conditions to\nincrease system robustness.\nFor the closed condition, we apply a variety of codec algo-\nrithms [5], including MP3, OGG, AAC OPUS, a-law and u-law.\nBesides, to mock the telephony transmission loss, audio sam-\nples are first downsampled to 8kHz and then upsampled back to\n16kHz.\nFor the open condition, first, we utilized recorded noises\nfrom MUSAN [19] and the RawBoost method [12] to add\nnoises. Based on a variety of convolutional and additive noises,\nRawBoost models nuisance variability stemming such as encod-\ning, transmission, microphones and amplifiers, as well as linear\nand nonlinear distortions. Thus we can add different nuisance\nnoises dependent on the corresponding raw waveform inputs.\nSecond, we randomly selected audio from the public room im-\npulse response (RIR) set [20] and convoluted it with the tar-\nget audio to generate new audio for reverberation simulation of\ndifferent room sizes. Besides, we apply a variety of codec al-\ngorithms [5], including MP3, OGG, AAC, OPUS, a-law, u-law\nand so on. The training data of all models in the experiment\nwere augmented using the above-mentioned methods in the on-\nthe-fly way.\nMost DA methods focus on improving the generalization of the\nsystem in real scenes, such as the methods in Section 3.1, which,\nhowever, cannot cover the speech synthesis algorithms in the\ntraining set. To tackle this issue, we use several vocoders to syn-\nthesize fake audio. In this work, we select three neural vocoders\nfor the closed condition and one neural vocoder for the open\ncondition. We utilized the bona fide speech from the ASV spoof\n5 training dataset to train the FastDiff [21], FreeV [22], and\nHiFi-GAN [23] models for the closed condition. For the open\ncondition, we used the VCTK corpus to train the VITS [24]\nmodel.\nHiFi-GAN: As one of the state-of-the-art neural vocoders,\nHiFi-GAN generates audio based on generative adversarial net-\nworks (GANs) using the true mel-spectrum. It includes one\ngenerator and two discriminators: multi-scale and multi-period\ndiscriminators, which can achieve efficient and high-fidelity\nspeech synthesis. The generator and discriminators are trained\nadversarially by incorporating two additional losses to improve\nthe training stability and model performance.\nFastDiff: FastDiff is a speedy conditional diffusion model\ndesigned for high-quality speech synthesis. To enhance au-\ndio quality, FastDiff employs a series of time-aware location-\nvariable convolutions with various receptive field patterns. This\napproach efficiently models long-term temporal dependencies\nusing adaptive conditions.\nFreeV: A vocoder framework that simplifies the model's\npredictive complexity by utilizing the estimated amplitude spec-\ntrum. This vocoder is composed of PSP and ASP, with Con-\nvNeXtV2 serving as the foundational building block. The\nPSP features an input convolutional layer, eight ConvNeXtV2\nblocks, and two convolutional layers for a parallel phase esti-\nmation structure.\nVITS: The VITS model includes a text encoder that con-\nverts text into linguistic features, a flow-based posterior encoder\nthat captures the complex distribution of these features, and a\nHiFi-GAN-based decoder that synthesizes the final audio wave-\nform. By leveraging these components, VITS is capable of pro-\nducing high-quality speech with rich prosody and naturalness,\nmaking it a powerful tool for text-to-speech applications. We\nuse the VCTK corpus to train the VITS model. During infer-\nence, the ASR (Automatic Speech Recognition) results from\na LibriSpeech-based model and the speaker representation ex-\ntracted from ASVspoof5 training data are used to decode the\nwaveform."}, {"title": "4. Systems Description", "content": "We used the baseline-based AASIST and RawNet2 [26] net-\nworks. Furthermore, we employ the S\u00b2pecNet [27] method for\naudio spoofing detection, which harnesses complementary in-\nformation from multi-order spectrograms. This approach fea-\ntures a TSF module that merges the two spectral representa-\ntions in a coarse-to-fine manner. To minimize information loss,\nthe fused representation is reconstructed back into the original\nspectrograms. On the other hand, we used AMSoftmax [28] and\nCircle Loss [29] in place of the standard Softmax loss function\nfor training. Additionally, we experimented with using differ-\nent fixed-length of durations for inference and then fused the\nresults.\nTo construct final system for closed condition, we used 4 sin-\ngle subsystems. Fusion was performed on the score level. The\noptimal weights are estimated using COBYLA [30] toolkit min-\nimizing the minDCF metric on development set.\nWav2vec2 [31] is a self-supervised pre-trained model, which\ncan extract speech representations or embeddings from raw\nwaveform. It has shown an impressive performance on speech\ndeepfake detection. Therefore, we use Wav2vec2-large as the\nfeature extractor for the open condition.\nFine-tuning Wav2vec2-large model mainly includes three\nstages. Firstly, the raw waveform is sent into a feature encoder\ncomposed of several convolutional layers (CNN). The feature\nencoder extracts vector representations of size 1024 every 20ms\nand the receptive field is 25ms. Secondly, these encoder em-\nbeddings are fed into the context encoder, which contains 24\ntransformer block layers and is used to explore the contextual\ninformation contained in the input speech. Finally, the context\nencoder embeddings are used for downstream task.\nEven though the features extracted by Wav2vec2 can in-\nclude rich discriminative spoofing information during fine-\ntuning process, they still have a tendency to overlook some prin-\nciples of speech pronunciation or hearing. On the contrary, Lin-\near frequency cepstral coefficients (LFCCs) obtains great de-\ntection performance in the hand-craft features, which reflects\nLFCCs are data-independent and robust even in the face of un-\nknown attacks [32]. Therefore, it would be interesting to see\nif leveraging the complementarity of these two sets of features\ncan help SDD.\nAASIST [33] is an end-to-end system using Integrated Spectro-\nTemporal graph attention networks, which mainly includes\nthree innovations: 1) a novel heterogeneous stacking graph at-\ntention layer, which models artefacts spanning heterogeneous\ntemporal and spectral domains with a heterogeneous attention\nmechanism and a stack node, 2) a max graph operation that in-\nvolves a competitive selection of artefacts, and 3) a modified\nreadout scheme. AASIST uses a sinc convolutional layer based\nfront-end, and thus can extract representations directly from raw\nwaveform inputs. To integrate AASIST with self-supervised\nmodel when fine-tuning, the sinc convolution layer of AASIST\nis replaced by the aforementioned wav2vec2 model as the same\nmodel architecture as in [9].\nMeanwhile, there exists a diversity in the spoof utterances\ngenerated by different text-to-speech and voice conversion al-\ngorithms, resulting in a poor generality of an SSD system to\nunseen spoofing attacks. To address this problem, we adopt\nres2net-based system as the same model architecture as in [10],\nwhich integrate multi-scale feature aggregation (MFA) and dy-\nnamic convolution operations into the anti-spoofing framework.\nThis additional multi-scale reception improves the system's ca-\npacity and helps the system perform better when generalized to\nunseen spoofing attacks\nBesides, to leveraging the complementarity of LFCCs and\nwav2vec2 embedding, we apply the aggregation methods as\nin [34]. The wav2vec2 process each speech segment with a\n20ms frame-shift, whereas LFCCs utilizes a 10ms frame-shift.\nTherefore, it is crucial to align the two features in prior to being\napplied to aggregation modules. Specifically, the extracted 60-\ndimensional LFCCs features are first processed using Convld\nto align both the time and feature dimensions with wav2vec2\nfeatures."}, {"title": "4.1.1. Single models for closed condition", "content": "4.1. CM system"}, {"title": "4.1.2. Fusion for closed condition", "content": ""}, {"title": "4.1.3. Embedding extraction for open condition", "content": ""}, {"title": "4.1.4. Single models for open condition", "content": ""}, {"title": "4.1.5. Training Protocol", "content": "In the closed condition, during the training stage, the speech\nis cropped or concatenated to a fixed length of 4 seconds. We\nconfigure the sinc layer with 70 filters, each having fixed cut-in\nand cut-off frequencies. The Adam optimizer, with a learning\nrate of 8 \u00d7 10-4, is utilized. The training batch size is set to 32.\nIn the test phase, we use different fixed durations (2 seconds, 4\nseconds, and 6 seconds) and fuse the results at the score level.\nFor open condition, all the single models trained were based\nof wav2vec2 embedding extractor. Besides, a fully connected\nlayer after the pre-trained model is used to reduce the represen-\ntation dimension from 1024 to 256. During fine-tuning, the pre-\ntrained wav2vec2 model is optimized jointly with these back-\nend network via the back-propagation. We use the standard\nAdam optimizer, which adopts a mini-batch size of 16 and a\nlearning rate of 10-6 with a weight decay of 10-4 to avoid\nover-fitting. Considering the imbalance between the genuine\nand fake audios in the training set, we use the weighted cross\nentropy to minimize the training loss. The weights are associ-\nated with the number of the genuine and fake categories."}, {"title": "4.1.6. Fusion for open condition", "content": "To construct final system for open condition, we used 12 single\nsubsystems which can be divided into three parts according to\nthe architecture they are based on. These are 4 Light\nRes2Net-like, 4 AASIST-like models and 4 Res2Net-LFCCS\nmodels. The details of final system is demonstrated in Table\n1. Fusion was performed on the score level. Fusion weights\nwere selected manually with respect to the performance of each\nsingle system on the progress subset of evaluation set."}, {"title": "4.2. ASV system", "content": "As one of the most classical ConvNets, ResNet [35] has\nproved its power in speaker verification. In our systems,\nbottleneckblock-based ResNet (deeper structures:ResNet-242)\nare adopted. Base channels of all these ResNets are 64. Similar\nto the classic ResNet, the network structure begins with a 3x3\nconvolution layer as the initial layer, followed by four convo-\nlution blocks. Each block contains stacked bottleneck layers.\nThe strides for these blocks are (1, 2, 2, 2), and the number of\nchannels in each block is (64, 128, 256, 512). The number of\nbottlenecks in each block of ResNet-242 is (3, 10, 64, 3).\nThe pooling layer aims to aggregate the variable sequence to\nan utterance level embedding. We used the multi-query multi-\nhead attention pooling mechanism layer (MQMHA) [36] in the\nResNet242 model. The number of query was set to 4 while the\nnumber of head was 16.\nRecently, margin based softmax methods have been widely used\nin speaker recognition works. To make a much better perfor-\nmance, we strengthen the AM-Softmax and AAM-Softmax [37]\nloss functions by Sub-Center [38] method.\nQuality Measure Functions (QMF) [39] was applied to cali-\nbrate the scores, and it greatly enhanced the performance. For\nQMF, we combined two qualities, speech duration and magni-\ntude of embeddings. We selected dev trials from the asvspoof5\nas the training set of QMF. Then a Logistic Regression (LR)\nwas trained to serve as our QMF model. For speech duration,\nwe used duration of enroll and test as quality. We used QMF to\ncalibrate the system score.\nWe conducted our experiments using PyTorch, training the ASV\nmodel in two stages. In the first stage, we used the SGD opti-\nmizer with a momentum of 0.9 and a weight decay of le-3. The\nmodels were trained on 8 GPUs with a mini-batch size of 1,024\nand an initial learning rate of 0.08. For each batch, 200 frames\nper sample were used. The ReduceLROnPlateau scheduler was\nemployed, validating every 2,000 iterations with a patience of 2.\nThe minimum learning rate was set to 1.0e-6, and the decay fac-\ntor was 0.1. Additionally, the margin was gradually increased\nfrom 0 to 0.2. We utilized the SubCenter loss function in this\nstage, with the number of SubCenters (K) set to 3.\nIn the Large-Margin Fine-Tuning stage (LM-FT) [40], set-\ntings are slightly different from the first stage. First, we re-\nmoved the speed augmentation from the training set to avoid\ndomain mismatch. Next, we changed the frame size from 200\nto 600 and increased the margin exponentially from 0.2 to 0.8.\nThe AM-Softmax loss was replaced with AAM-Softmax loss.\nThird, we selected the center with the largest norm as the \"dom-\ninant center\" and discarded the other sub-centers, using only the\ndominant center as the speaker center throughout the fine-tuning\nprocess. Finally, we used a smaller fine-tuning learning rate of\n8e-5 with a batch size of 256. The learning rate scheduler re-\nmained mostly unchanged, except the decay factor was adjusted\nto 0.5."}, {"title": "4.2.1. Backbone", "content": ""}, {"title": "4.2.2. Pooling method", "content": ""}, {"title": "4.2.3. Loss function", "content": ""}, {"title": "4.2.4. Backend", "content": ""}, {"title": "4.2.5. Training Protocol", "content": ""}, {"title": "4.2.6. Fusion for CM and ASV system", "content": "We use a cascaded system to integrate the CM (countermea-\nsure) and ASV (automatic speaker verification) systems. The\ncascaded system is composed of two tandem modules:\na) The first module generates a hard decision based on a\nthreshold, which is set according to the a-DCF on the develop-\nment set.\nb) If the decision from the first module is positive, the sec-\nond module directly outputs the raw scores.\nc) If the decision from the first module is negative, the sec-\nond module assigns a fixed minimum score based on the devel-\nopment set."}, {"title": "5. Results and Discussion", "content": "The final results of the evaluation set system fusion submit-\ntted for the Track 1 open challenge are shown in Table 4. With\nthe enhancement of self supervised pre training models, the de-\ntection performance of various spoof algorithms has improved\nsignificantly, achieving a final min a-DCF of 0.0750."}, {"title": "5.1. ASVSpoof5 Track1", "content": "The final results of the evaluation set system submitted for\nthe Track 1 closed challenge are presented in Table 3. Notably,\nthe A19 and A28 spoofing algorithms underperformed, yielding\na final minDCF of 0.3948 and an EER of 14.33%."}, {"title": "5.2. ASVSpoof5 Track2", "content": "The final results of the fused evaluation set system submit-\ntted for the Track 2 closed challenge are presented in Table 5.\nInfluenced by the CM system from Track 1, the A19 and A28\nalgorithms also performed poorly in the Closed mode of Track\n2, resulting in a final min a-DCF of 0.2814."}, {"title": "6. Conclusion", "content": "In this paper, we present our system for the ASVspoof5 chal-\nlenge. For Track 1, we utilized handcrafted features and self-\nsupervised speech representations to address the closed and\nopen conditions, respectively. Multiple systems were trained on\nan augmented dataset to effectively detect spoof attacks under\nvarious adversarial conditions. Our system achieved a minDCF\nof 0.3948 and an EER of 14.33% in the closed condition, and\na minDCF of 0.0750 and an EER of 2.59% in the open con-\ndition, demonstrating robustness under adversarial conditions.\nFor Track 2, we implemented a two-stage ASV system, em-\nploying the QMF method to calibrate the system score. By in-\ntegrating the CM system from Track 1 with the ASV system in\na cascaded manner, we achieved a min-aDCF of 0.2814 in the\nclosed condition and 0.0756 in the open condition, indicating\nsuperior performance in the SASV system."}]}