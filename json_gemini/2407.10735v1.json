{"title": "Transforming Agency On the mode of existence of Large Language Models", "authors": ["Xabier E. Barandiaran", "Lola S. Almendros"], "abstract": "This paper investigates the ontological characterization of Large Language Models (LLMs) like ChatGPT. Between inflationary and deflationary accounts, we pay special attention to their status as agents. This requires explaining in detail the architecture, processing, and training procedures that enable LLMs to display their capacities, and the extensions used to turn LLMs into agent-like systems. After a systematic analysis we conclude that a LLM fails to meet necessary and sufficient conditions for autonomous agency in the light of embodied theories of mind: the individuality condition (it is not the product of its own activity, it is not even directly affected by it), the normativity condition (it does not generate its own norms or goals), and, partially the interactional asymmetry condition (it is not the origin and sustained source of its interaction with the environment). If not agents, then ... what are LLMs? We argue that ChatGPT should be characterized as an interlocutor or linguistic automaton, a library-that-talks, devoid of (autonomous) agency, but capable to engage performatively on non-purposeful yet purpose-structured and purpose-bounded tasks. When interacting with humans, a \"ghostly\u201d component of the human-machine interaction makes it possible to enact genuine conversational experiences with LLMs. Despite their lack of sensorimotor and biological embodiment, LLMs textual embodiment (the training corpus) and resource-hungry computational embodiment, significantly transform existing forms of human agency. Beyond assisted and extended agency, the LLM-human coupling can produce midtended forms of agency, closer to the production of intentional agency than to the extended instrumentality of any previous technologies.", "sections": [{"title": "1. Introduction", "content": "The recent emergence of Large Language Models (LLMs hereafter) (see Brown et al., 2020) with their wide availability' and their human-like generative capabilities are (re)opening the question around the ontological status of Artificial Intelligence. Are these systems genuinely intelligent? Do they possess mindful capacities? The responses are often polarized (Mitchell & Krakauer, 2023). Inflationary views (fuelled by research enthusiasm and commercial interest alike) tend to amplify AI properties, assimilating or approximating them to the human (and the superhuman). Deflationary views (typically trying to mitigate the harms of inflationary marketing), tend to downplay capacity attributions, and bring Al systems closer to dumb mathematical or mechanical devices. Deflationary categorizations typically revolve around treating LLMs as statistical processors, \u201cstochastic parrots\u201d (Bender et al., 2021), a \u201cblurry JPEG of the web\" (Chiang, 2023), \u201clumbering statistical engine for pattern matching\u201d (Chomsky et al., 2023), or simply \u201cbullshit\u201d (Hicks et al., 2024, although this technical characterization is more informative and insightfull than the previous ones). The most inflationary characterizations range from considering LLMs as \"human-brain equivalents\" (Ge et al., 2023), \u201cgenuine authors\u201d and \u201caccountable\u201d entities (Miller, 2023), up to a \u201cfully sentient person\u201d (Lemoine, 2022). Somewhere in the middle stand more technical characterizations like \u201cartificial reasoners\u201d (Wei et al., 2023), \u201clearners\u201d (Brown et al., 2020), \u201cgeneral pattern machines\u201d (Mirchandani et al., 2023), \u201csparky artificial general intelligence\u201d (Bubeck et al., 2023) or simply \u201clanguage models\" with the slippery temptation to be turned into \u201cworld models\u201d (K. Li et al., 2023).\nAn increasing danger of some deflationary views of LLMs is that, from their point of view, most of the risks can be attributed to the influence of misguided inflationary conceptions. It is often assumed that these can be mitigated if inflationary views are conclusively shown to be wrong: \u201cBehind the smog of the hype and the marketing\" the argument goes \u201cthere is no genuine intelligence or understanding behind LLMs, they are simple statistical processors, the only problem (besides their energy consumption and biases) is that other humans take them at face value\u201d. Moreover, the argument continues, \u201cif we treat them as the stupid machines they are\u201d, the conclusion follows, \u201ceven the issues of bias and energy should fade away\u201d. Or, as Chomsky et al. conclude, \u201cGiven the amorality, faux science and linguistic incompetence of these systems, we can only laugh or cry at their popularity\" (2023). The real capacity of LLMs is thus left disregarded by such deflationary views, both as a potential risk to society and as a genuine source of positive sociotechnical transformation that needs to be more deeply thought out. If we do not fine-tune our conceptualization of what LLMs are, we will not\""}, {"title": "2. When computers can", "content": "Benchmarks, particularly when out of reach for the available technology, have often helped to reach agreement on the (lack of) capacities of AI systems. Now, many defend that the Turing Test is outdated (Bayne & Williams, 2023; Biever, 2023; Srivastava et al., 2023; Tikhonov & Yamshchikov, 2023). Generic conversations with LLMs are indistinguishable from those we can enjoy with other humans (Jones & Bergen, 2024). More systematic variations of the Turing Test, directed at capturing the capacity of AI to display common sense, like the Winograd schema challenge (Levesque et al., 2012), have been declared obsolete (Kocijan et al., 2023). More sophisticated common sense reasoning tests like Winogrande (Sakaguchi et al., 2019) and HellaSwag (Zellers et al.,"}, {"title": "3. How do Large Language Models work?", "content": "Large Language Models (LLMs) are so-called \u201cartificial intelligence\u201d systems (Norvig & Russell, 2021), part of current NLP (Natural Language Processing) technologies, that belong to the family of \"machine learning\" and the sub-category of \u201cdeep learning\" systems. They are designed to process and generate \u201cnatural\u201d language through a large number (on the order of billions) of processing steps. Transformers (Vaswani et al., 2017), in turn, are one kind of recently very successful type of LLMs, and GPT (Generative Pre-trained Transformer) is a specific type of implementation of Transformer technology (Brown et al., 2020; Radford et al., 2019). ChatGPT, in turn, is a specifically tuned and interfaced version of GPT (and increasingly a platform to connect GPT to other tools and to deliver personalized services with GPT technology).\nChatGPT uses different versions of GPT models to produce human-like text (GPT-3.5, GPT-4, etc.). It is a computational language processing system designed to generate sequences of words, codes or other data (more recently, images) from an input sequence called \u201cprompt\u201d. Thus, given a prompt, GPT produces the text that would have been more statistically expected on the training data. For example, if the sequence \"Elephants don't play\u201d is entered as a prompt, the ChatGPT offers \u201cElephants don't play chess\" as a response. The system has a heat parameter that increases less likely variations on the output. So, for instance, the system might respond to the original prompt with \"Elephants don't play video games\u201d or could simply output \u201cElephants don't play.\". This basic functioning is what made so popular the characterization of ChatGPT as simply a complicated auto-complete tool (Floridi, 2023).\nHowever, the simplicity of the general task of optimizing to predict the following word, and its recursive iteration, is the key for the emergence of complex capacities in LLMs. Moreover, optimization alone provides no ground to understand the working of a system, its capabilities and limits, its mode of existence. Appealing to partial aspects of how they operate, Transformers are often qualified as stochastic, probabilistic and statistical (Bender et al., 2021; Chomsky et al., 2023; Floridi, 2023). Stochasticity refers to\""}, {"title": "3.1. Architecture and processing", "content": "Figure 1 illustrates step-by-step the processing of the input text as it goes along the GPT architecture. We explain each step in detail below.\n1. Tokenization and encoding: The first operation that takes place as we enter text into a LLM like ChatGPT is tokenization. The input stream is chopped into tokens (small syllable-like or small word text chunks, including punctuation marks). On average, each token is about 3/4 of a word in English, with a mean of 4 characters per token. Nevertheless, we are going to use the terms \u201cword\u201d and \u201ctoken\u201d interchangeably. Then"}, {"title": "3.2. Training", "content": "GPT and other LLM configuration is typically carried out in various stages. The first is, somewhat paradoxically, called \u201cpre-training\u201d but constitutes the main training (understood as the process by which one improves or acquires new capacities). During this process, the parameters of each processing transformation just described gradually change until a given level of accuracy is reached, pre-training ends, and they remain fixed until new training procedures start. Then comes fine-tuning, with two basic stages: task specific fitting and human reinforcement learning. Finally, prompt learning is often used, which is more of an instructional form of directing the system."}, {"title": "Pre-training", "content": "Explaining first the way of functioning of the whole architecture, as we just did, is essential to understand training. Contrary to other approaches, each processing block is not trained in isolation to perform a specific task (e.g. 1st grammatically articulate the input, then build a general abstract representation, next, carry out inferences and take an output decision), but the entire system is trained at once, through back-propagation (Rumelhart et al., 1986).\nThe basic mechanism is simple: the system is initialized with random parameters. A chunk of input (e.g. the beginning of a sentence) is then chosen among a training dataset. It is then processed as we explained above. This is called a forward pass. This pass finishes when the system provides the result array: that which indicates the probabilities of all the words to be the next one (the step before selecting the final output). The result will be nonsense at the beginning. For example, to the input \"Elephants don't play\" the highest probability of the result array could be \u201cpurple\u201d, followed by \u201cFodor\u201d, \u201cmisuse\u201d, \u201cchain\u201d, \u201ccat\u201d, etc. Now, this is compared to the correct result: an array that gives o probability to all the words except 1 for \u201cchess\u201d. But \u201cchess\u201d might be very down on the assigned probabilities. Yet, it is now possible to compute an error (or loss): the difference between the assigned probabilities on the result array and the target one.\nNext, this loss will be backpropagated through the network (the backward pass). By means of an optimization algorithm, small changes are made all throughout the whole network in the direction of minimizing this error: The algorithm calculates the response to \u201cwhat change should I do to this parameter so that the resulting output reduces the error?\u201d and makes the change accordingly, for each parameter on each block, backwards.\nThis process is iterated once and again, until the forward process produces no or little errors. All three major components of the LLMs are trained in this way: embeddings, attention mechanisms and feed-forward networks. Although the overall procedure is locally relatively simple, the amount of little changes is vast and the effect is the performance capacity we can witness today. The computational cost of training GPT-3"}, {"title": "Fine-tuning", "content": "Additional training procedures are used to fine-tune the transformer for specific tasks, like summarization, translation, or conversation. This time the instruction (e.g. summarize) and the task input (e.g. a whole Wikipedia article) are provided, and the system is trained with back-propagation to match a model output (e.g. Wikipedia's summary entry for that article), instead of just the next token. This is considered supervised learning, no human intervenes yet, but the task is not simply to \u201cguess\" the following word but to match a specific target goal, pairs of input and target-output are required to complete this training.\nTransformers are usually further trained to include Reinforcement Learning with Human Feedback or RFHF (Ziegler et al., 2020). The pre-trained and fine-tuned LLM is let to interact with humans. Then, based on how humans have positively or negatively evaluated the output of the model, it is trained to produce outputs that are more likely to be positively rated or less likely to be negatively valued; according to the past corrections made by human interactors. This is where the system is often trained on ethical or moral values, together with a number of other quality checks.\nFinally, we have one-shot or few-shot learning procedures that operate basically at the prompt level, providing examples or specific instructions that the LLMs take as input to produce new examples or follow the instructions provided (Brown et al., 2020)."}, {"title": "3.3. Anthropomorphising GPT", "content": "Calls to avoid anthropomorphizing GPT are recurrent (Bender et al., 2021; Butkus, 2020; Coeckelbergh, 2021; Jebari & Lundborg, 2021; Kubes & Reinhardt, 2022; Shardlow & Przyby\u0142a, 2023). But anthropomorphizing is only referred to as projecting human qualities, particularly cognitive or emotional ones, to the machine. Something that is perceived as a risky strategy, since understanding (or experiencing) the interaction with ChatGPT through the human or intentional stance (Dennett, 1989) as if it truly had genuine human capacities, would make us falsely attribute a set of properties it certainly lacks. Properties that are essential to the human social world-making: commitment, trust, responsibility, empathy, etc. Important as it is, the emphasis of this type of anthropomorphisation shadows other important forms. There are at least two more types of anthropomorphisation that are relevant to understand GPT. And their analysis is perhaps more revealing of its mode of existence than the attribution of"}, {"title": "3.4. Towards LLM based agents", "content": "At a first sight, nothing in this architecture qualifies properly as agency. Not even for the most optimistic or naive engineers. The system is fully driven by the prompt and directly driven or steered when output completion has taken place by a new prompt. Moreover, the system has no internal states, no (internal) memory, no potential desires, goals, or purposes. When operating (after training is completed), not even a trace of what is processed is left within the system. Except for the history of outputs that is continuously fed-back into the system auto-regressively. In a sense, GPT operates like Leonard Shelby, the protagonist of Christopher Nolan's celebrated film Memento (2001). Devoid of the capacity to create new memories (yet able to use its knowledge), Leonard externalizes instructions (goals, instrumental steps, etc.) and contextual information (pictures, notes, etc.) to regain the agency that he lost due to his amnesia.\nThe lack of agentive capacities of the raw GPT is apparent in the type of digital embodiments that Al engineers are providing to enhance GPT and develop so-called \u201cautonomous GPT agents\u201d (Andreas, 2022; Huang et al., 2024; Wang et al., 2023; Weng, 2023; Xi et al., 2023; W. Zhou et al., 2023). March to June 2023 saw a rapid increase of projects trying to deploy digital agents based on GPT and other LLMs: AutoGPT (Significant Gravitas, 2023/2023), AutoGen (Q. Wu et al., 2023/2023), DemoGPT (\u00dcnsal, 2023/2023), SuperAGI (admin_sagi, 2023), MiniAGI (Mueller, 2023/2023). A number of initiatives have followed that promise to deliver fully operational agents for programming (S. Wu, 2024; Yang et al., 2024) and tech giants seem to be betting on LLM-driven agents to make generative-AI services profitable (Holmes, 2024; Knight, 2014).\nThere are 5 kinds of LLM enhancement strategies that are being developed to move from ChatBots to the so-called \u201cagents\u201d: a) extended memory systems, b) planning strategies, c) reflexive evaluations, d) the use of tools, and, e) multi-agent interactions."}, {"title": "4. LLMs are not (autonomous) agents", "content": "We have seen how LLMs based on transformer architecture internally operate and how their capacities have been expanded with a series of additions to the foundational trained models. Moving below the surface of performance-level measurement to characterize agency requires a certain commitment to theoretical or philosophical frameworks regarding the nature of actions, purpose, and cognitive properties. In this section, we first approach the issue from the point of view of computational representationalism (from which LLMs can be comfortably be characterized as agents). We then move to alternative so called 4E frameworks, whose requirements severely problematize agency attribution to LLMs.\nFrom the philosophical perspectives that have given credit and have contributed to the Al research program, it is difficult to rule out genuinely agentive capacities from ChatGPT-like systems. Representational computationalism is one such approach (Carruthers, 2006; Newell, 1980; Putnam, 1965). It is a type of functionalism that defines mental properties (intelligence, knowledge, learning, or agency) in terms of the input-output functional (internal transition) states of a system representing states of affairs of the environment. The essential feature of the mind is the capacity to reason or to draw inferences upon representations of the world; i.e., information processing. For instance, you take the umbrella because you just read it will be raining today, and you know that umbrellas are a good way to cover from the rain. According to representational functionalism, this is the kind of inference that is characteristic of"}, {"title": "5. What are transformers then, if not agents, and how do they transform agency?", "content": "If transformer architectures, and LLMs as we know then, are not (autonomous) agents, then we are left with the task of adequately characterizing their mode of existence. What are LLMs that have such a strong impact on our digital environment and the way we live in them? If we are not to embrace transformers as members of our familiar way of being (as agents) in the world, we need to start somewhere else. First, it might be useful to distinguish operations from actions. An operation is a sequence of occurrences that can be interpreted functionally; that is, in (instrumental means-ends) relation to a final goal state. Mechanisms carry out operations. A digital operation is a logical transformation executed by a computer. Operations are externally defined, whereas actions are internally defined by the agent that carries them out (in the sense outlined in the previous section). Note that an action can externally be defined as an operation and translated into a machine. But it is more convenient to use a specific name to label those processes that can be described both as an operation when carried out by a machine and as an action when done by a human: we might call those tasks (see Table 1).\nWith these distinctions at hand, we can now proceed to properly characterize LLMs. From the point of view of their organization, transformers are automata, as opposed to autonomous systems. This distinction is still relevant and crucial. Automatic\u00b9\u00ae systems do not need human intervention to carry out certain operations, but are not autonomous. They cannot define their own identity and norms. However, as structured and identifiable digital instruction sets in physical memory and processors, they can carry out complex sequences of operations in the real world. They can transform energy into operations without human supervision or intervention during the process. They are automata.\nHowever, transformers are not any kind of automata, they are of a very special digital kind, and operate in a very special type of environment, with an even more singular relationship to human life: they are digital language automata operating in multiple language-supporting and language-driven digital networks we continuously inhabit as linguistic animals (together with many other digital and physical objects around us).\nMoreover, LLM-powered chatbots, like ChatGPT, are specifically constituted by the way they couple with their associated milieu: other humans. As such, they often become phantasmic language automata. In some sense, ChatGPT is certainly the ghost of the text"}, {"title": "5.1. The language automaton and the ghost in the human-machine interaction", "content": "If transformer architectures, and LLMs as we know then, are not (autonomous) agents, then we are left with the task of adequately characterizing their mode of existence. What are LLMs that have such a strong impact on our digital environment and the way we live in them? If we are not to embrace transformers as members of our familiar way of being (as agents) in the world, we need to start somewhere else. First, it might be useful to distinguish operations from actions. An operation is a sequence of occurrences that can be interpreted functionally; that is, in (instrumental means-ends) relation to a final goal state. Mechanisms carry out operations. A digital operation is a logical transformation executed by a computer. Operations are externally defined, whereas actions are internally defined by the agent that carries them out (in the sense outlined in the previous section). Note that an action can externally be defined as an operation and translated into a machine. But it is more convenient to use a specific name to label those processes that can be described both as an operation when carried out by a machine and as an action when done by a human: we might call those tasks (see Table 1).\nWith these distinctions at hand, we can now proceed to properly characterize LLMs. From the point of view of their organization, transformers are automata, as opposed to autonomous systems. This distinction is still relevant and crucial. Automatic\u00b9\u00ae systems do not need human intervention to carry out certain operations, but are not autonomous. They cannot define their own identity and norms. However, as structured and identifiable digital instruction sets in physical memory and processors, they can carry out complex sequences of operations in the real world. They can transform energy into operations without human supervision or intervention during the process. They are automata.\nHowever, transformers are not any kind of automata, they are of a very special digital kind, and operate in a very special type of environment, with an even more singular relationship to human life: they are digital language automata operating in multiple language-supporting and language-driven digital networks we continuously inhabit as linguistic animals (together with many other digital and physical objects around us).\nMoreover, LLM-powered chatbots, like ChatGPT, are specifically constituted by the way they couple with their associated milieu: other humans. As such, they often become phantasmic language automata. In some sense, ChatGPT is certainly the ghost of the text"}, {"title": "5.2. Transformer embodiments", "content": "Cognitive science has turned from abstract symbolic computations into the (historically neglected) role of the material body in the production of mindful experience and capacities (Calvo & Gomila, 2008; Gallagher, 2023; Shapiro, 2019; Varela et al., 1991). Cognitive agency is said to be embodied, extended beyond the brain as the \u201cmere\u201d hardware of the mind executing the genuine mindful \u201cinmaterial\u201d software, into"}, {"title": "5.3. Assisted, extended and midtended agency", "content": "What is ChatGPT as an extension of human cognitive agency? Is it an assistant? Most technologies that have been thought of as extensions or embodiments of human activity have been thought of as bodily prostheses. The extended mind hypothesis (Clark & Chalmers, 1998) and its later developments, including material engagement theories (Malafouris, 2016), have focused on the way in which beyond-the-skull extended material or computational processes should be understood as constitutive of cognition or brain processes. According to this view, mindful thinking processes must often be understood as extended into the material environment that they shape and, in turn, also shapes cognitive processes. Thinking involves thinging. When we make pottery, we don't print or carve an internal mental 3D model into the clay, we mold it. The materiality of the clay guides us, we bring the jar into form through a continuous reciprocal interaction between brain, body and world. Digital technologies constitute a branch of these phenomena: we offload memories, drafting procedures, image manipulation, etc. into our PC and mobile devices. Cognitive gadgets are organs of our mind extended beyond the skull.\nSocial interaction also extends and assists human agency: crew and team members interact (coordinatively or subordinatively), to achieve levels of agency (collective or directive) that are unreachable to a single individual (Lewis-Martin, 2022). The interlocutionary capacity of LLMs brings human autonomous agency to a level not-unlike that of intersubjectively augmented agency, where the machine takes the role of an assistant (or that of a master). Recent human-computer interfaces have been dominated by action directed design, we tell computers what they have to do (by programming a specific function, by pushing a button, by dragging a file, by selecting a menu item). LLMs, instead, make possible to prompt or command an intention (Nielsen, 2023): \u201cI want a summary of this text in French so that the 5-year-old child of my visiting friend can understand it\u201d, \u201cI need an impressionist style picture for a book cover on philosophy of mind\". This is certainly going to bring us to unprecedented forms of master-slave dialectical relationships with machines (and the corporations they\""}, {"title": "6. Discussion and conclusions", "content": "The irruption of LLMs on our digital world, and through it on our lives (digital or otherwise), is breaking (again) what we thought human artifacts could never do. They perform operations that would require high levels of complex, common-sense, unstructured, and creative intelligence if performed by humans. We are forced to question their ontological status and the deep way in which they can transform ours. In this paper, we have focused on responding to this question, focusing on agency.\nWe started identifying the polarized take on LLMs ontological status: from their inflationary characterization as fully sentient beings to the deflationary one as mere stochastic parrots. Next, we delved into the faulty yet outstanding capacities that LLMs display as measured by different human-level intelligence benchmarks. We concluded that a deeper delving into their organization is necessary to properly determine their mode of being. A detailed explanation of the complex and powerful architecture and"}, {"title": "6.1. Autonomous agents, interlocutionary automata, and deeply embodied midtension", "content": "You are enmeshed in a thick web of recurrent attention-intention loops, of which you are both cause and effect. Through the growth and arrangement of this web, you have developed a sensitivity to navigate and stir your behavioral world so as to care for its deep precariousness. Along this open process, what is continuously guiding your action is not the anticipation of the next token of a pre-given text (or data-stream), but the sensitivity to the consequences of your actions at different nested scales: on the task at hand, on your goals, plans and, ultimately, on your own identity (itself the result of your own actively sustained and stirred encounters within the world). You are a genuine agent. LLMs are not. But they perform, historically-unprecedented, extraordinary tasks. And they will continuously intersect with the way in which we navigate our (linguistic) worlds.\nIf by autonomous agent we mean an automatic system that is efficient on a sequence of multiple tasks, then LLMs (with important extensions and, most probably, internal architectural improvements) might soon deserve the name. If, by agency, we mean the sense of agency we experience as autonomous self-defining and self-governing systems, then there are good reasons to believe that the LLM architecture as we know it falls currently short to meet the demands. This might not be a bug but an intended safety feature of transformer architectures. Systems that display complex intentional capacities might be a powerful assistant at a high prize. Autonomous agents of this kind are the most powerful and yet most dangerous assistants. Being capable of creating your own norms, and being adaptively capable of displaying complex strategies to meet and transform them, is compatible with accepting external commands and making external norms your own. But it is also open to revolt. And this, in turn, opens a whole set of problems of AI alignment and safety.\nA fundamental question remains: is it possible to achieve efficient and automatic multistep task-completion without genuine agency? Some strong requirements (like deep material living embodiment) might never be met by transformer-like systems. But it is still possible to envision variations on the current architecture that could bring the system's operations closer to living actions.\nThe deep transformation we are witnessing bears some relevant parallels with the"}, {"title": "6.2. On the dangers of the \u201cstochastic parrot\u201d metaphor and the \u201cagency without intelligence\u201d conceptualization", "content": "Deflationary accounts of AI tend to forget that human agency can, at a very fundamental level of quantum mechanics or the less fine-grained level of neuronal modelling, be characterized simply as a collection of dumb \u201cprobabilistic\" and \u201cstochastic\u201d processes. It is not the description of the basic mechanisms that compose a complex system that defines its properties, but the organization of interrelated processes (both internal and interactive) that such basic mechanism make possible to emerge. This is as true of us (living humans) as it is of any machine. In order to assess the genuine capacities of a system, we need to look at their internal workings, and the emergent capacities they can display when organized and coupled in specific manners.\""}, {"title": "6.3. Prospects for transformed agencies in the era of deep digitality", "content": "Some of the properties that are essential to agency (most notably individuality and normativity) emerge from the deep materiality of natural agency. However, the recent course of Al explosion, with the gigantic investment of data and computational capacity (and the related energy demands) is revealing a deep digitality whose consequences are still to be fully unpacked. The complexity and scale of the operations involved in LLMs training and execution are huge. Prompt processing operations that, carried by an aware and conscious human, would take billions of years, challenge our intuitions and conceptual resources. By a digitality that deep, it is reasonable to hold that the boundary between invention and discovery, between artifact and nature, between engineering and science is somewhat blurred. We (humans) have built LLM as much as we have discovered their emergent capabilities25. And avenues for a genuine\ndigital agency might still be open for discovery. The way in which deep and wide materiality has revealed agentive capacities in natural history might well be somewhat replicated in the digital realm. If deep materiality brings with it the capacity to make difference emerge (Anderson, 1972) we have no reason to preclude the increasingly deep digitality of artificial devices to reveal new forms of agency, yet to come. And even deeper transformations of the existing forms of agency.\nBut depth alone does not bring matter (or digitality) to life. It is ultimately the organization of processes, their interaction with their environments, their interdependence with the rest of beings, that needs to be scrutinized to disclose the mode of existence of any device. No benchmark or general description (stochastic, statistical, probabilistic, syntactic, or otherwise), is sufficiently informative of the potential transformative capacities of machines. LLMs are no exception. Their mode of existence is highly dependent on human (and other) forms of life. And the deeper our materiality and digitality merge, the deeper will be the transformations to come. This is why transparency and openness regarding LLMs (and AI in general) is much more than a private ethical imperative and turns into a collective political concern: how these systems work and get coupled to our social fabric, on how they feed on the human heritage and care, how they suck planetary resources and affect social inequalities. To shape this future, we need a better conceptual understanding of how the mode of existence of LLMs transforms real agency."}]}