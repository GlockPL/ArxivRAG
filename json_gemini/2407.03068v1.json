{"title": "xApp Distillation: AI-based Conflict Mitigation in B5G O-RAN", "authors": ["Hakan Erdol", "Xiaoyang Wang", "Robert Piechocki", "George Oikonomou", "Arjun Parekh"], "abstract": "The advancements of machine learning-based (ML) decision-making algorithms created various research and industrial opportunities. One of these areas is ML-based near-real-time network management applications (xApps) in Open-Radio Access Network (O-RAN). Normally, xApps are designed solely for the desired objectives, and fine-tuned for deployment. However, telecommunication companies can employ multiple xApps and deploy them in overlapping areas. Consider the different design objectives of xApps, the deployment might cause conflicts. To prevent such conflicts, we proposed the xApp distillation method that distills knowledge from multiple xApps, then uses this knowledge to train a single model that has retained the capabilities of previous xApps. Performance evaluations show that compared conflict mitigation schemes can cause up to six times more network outages than xApp distillation in some cases.", "sections": [{"title": "I. INTRODUCTION", "content": "Open radio access networks (O-RAN) in 5G and beyond led to a number of new research and implementation opportunities for the telecommunication sector. One of the key features of O-RAN is xApp applications on beyond the fifth generation (B5G) cellular networks, and the capacity to host machine learning operations. xApp is a part of the RAN intelligent controller (RIC) that takes near-real-time actions as a third-party application to control the network parameters for the desired objectives. The RIC platform has the capability of storing and running multiple xApps [1]. ML-based xApps and their capabilities have been studied and proved to be beneficial for next-generation (NextG) cellular networks [2]. However, most of the studies assume there can be only a single xApp in the wireless communication environment and ignore the risk of conflicts [3]. For sequential decision-making applications, deep reinforcement learning (DRL)-based solutions are among the popular choices for ML-based xApps [4].\nO-RAN alliance defined several types of conflict situations [5]. 1) Direct conflicts are easy to detect due to the observability of the conflict before actions. Direct conflicts occur when multiple xApps try to control the same parameter in the network. 2) Indirect conflicts cannot be detected until actions are taken in the environment. These situations occur when different xApps adjust different parameters to optimise the same metric according to the respective objective. As an example of indirect conflicts, one xApp manages the resource block allocation while the other controls transmit power. Both of them have effects on the throughput value, but it is not always possible to predict how those actions will affect the performance. Besides the number of potential reasons, effects of implicit conflicts can only be observed on future KPIs which is not possible to avoid at the time of action [5].\nTo cope with conflicting xApps, the O-RAN Alliance proposes two different conflict mitigation schemes in the RIC architecture technical reports [5]. Since the issue is having multiple sets of actions for a single operation, conflict mitigation schemes aim to eliminate all actions and apply the best action among the set. O-RAN conflict mitigation decides on the optimised action, and disables all of the xApp actions except for the optimised one. This approach doesn't use the information of disabled actions.\nTo improve joint coordination of network, [6] proposed team-learning. Their approach proposes a joint learning scheme for multiple xApps by sharing actions with each other. Their results show improvements over individually trained DRL-based models in terms of overall throughput and packet drop rates. However, in their scenario, there are only indirect conflicts, and xApps perform only single operations. Moreover, the O-RAN conflict mitigation procedure [5] is not applied after RL agents take indirectly conflicting actions.\nIn this paper, we propose xApp distillation, a conflict mitigation scheme that learns from conflicting xApps to create an enhanced xApp with improved performance by using policy distillation. The major contributions of this letter are summarized as follows:\n\u2022 A novel conflict mitigation scheme that utilises deployed xApps (either ML-based or heuristics) for distilling policy to a better performing, non-conflicting xApp. Current mitigation schemes proposed by O-RAN alliance have shortcomings in service quality. Proposed method provides higher QoS among users.\n\u2022 Proposed xApp distillation forms an xApp that provides computationally efficient and more reliable network management. The method reduces interrupts that are caused by mitigation delay and action rollbacks.\n\u2022 Proposed method provides a knowledge base by using policy distillation from multiple xApps. Thus, it can be"}, {"title": "II. SYSTEM MODEL", "content": "This paper considers a connected urban system model with multiple users moving in a 250-by-250-meter area. The simulation is built on the mobile-env [7] Gymnasium environment to deploy multiple xApps. We add new resource types and a new reward design to optimise the network. The simulation has B base stations (BSs) and K mobile users. The designed environment uses an Okumura-Hata-based propagation model [8] to simulate an urban area.\nWe consider B BSs to provide connectivity to K users across the area. We defined the downlink data rate $r_{i,j}$ transmitted from BS j to user i according to calculated signal interference noise ratio (SINR) as follows:\n$r_{i,j} = R_{i,j}log(1+\\frac{\\alpha_{j}P_{j}h_{i,j}}{N_{o} + \\sum_{k \\neq j}h_{i,k}P_{k}})$ (1)\n\u2022 \u03c3 is the bandwidth of a single RB,\n\u2022 $R_{i,j}$ is the number of RBs allocated from BS j to user i,\n\u2022 $\u03b1_{j}$ is a binary indicator of connection of BS j,\n\u2022 $P_{j}$ is the transmit power level of BS j,\n\u2022 $h_{i,j}$ is the channel coefficient between BS j and user i.\nThe designed environment enables RL agents to manage the network by controlling three type of resources in the network. These network operations in the environment are handover, RB allocation and cell transmit power control.\n1) Handover: We consider K users to be mobile in a rectangular area. Due to the change in received signal strength, the controller handovers users from one BS to another to seek better service availability.\n2) Resource block allocation: RB allocation represents the number of RB that is being used by the corresponding user. Although the number of available RBs in LTE was constant, in 5G new radio (NR) it varies depending on the guard bandwidth and single RB bandwidth. We considered using parameters given in Table I [9].\nAs a typical profile mode of 5G, we consider one RB to be 360 kHz and with the double guard band, there are 273 RBs in total.\n3) Transmit power level control: Each BS in the environment has various transmit power levels. The power levels directly affect the datarate, hence it creates an indirect conflict possibility with RB allocation actions. Moreover, we used SINR values instead of SNRs to penalise RL agents which frequently maximise the transmit power level. In the simulation, we consider changes in cell power level between 25 dBm to 35 dBm."}, {"title": "III. XAPP DISTILLATION", "content": "The conflict mitigation schemes proposed by O-RAN ignore some of the xApps' actions to mitigate conflict by choosing one of the options and discarding others. Therefore, the chance to take a sub-optimal action in the environment becomes higher. Hence, we proposed the xApp distillation method that utilises all xApps by distilling their policies to form a single xApp that consists of all action capabilities of the candidate xApps. We build a knowledge base using experiences from all xApps's transition memories. These memories are collected in a replay buffer memory to train new xApps. Policy distilling is applied by having a teacher network i.e., pre-trained xApps to train a new model for the student network i.e., new xApp.\nIn this paper, we designed two xApps that have multiple operation capabilities. In xApp 1, we developed handover and RB allocation while xApp 2 operates handover and transmit power level control of BSs. The objective of this design is to introduce both direct and indirect conflicts simultaneously. Hence, we could analyse how xApp distillation conducts the mitigation for these xApps.\nAs a network controller we employed deep Q-network because of its advantages over tabular Q-learning on continuous state spaces. In the simulated environment according to users' trajectories, there can be an indefinite number of state-action transitions. Therefore, using a tabular-based method may be less likely to converge to an optimal solution.\nIn this paper, we designed a multi-headed multilayer perceptron (MLP) network model for RL agents. Therefore, each DQN model can perform multiple tasks in a single production of inference. For the xApps each head of MLP model corresponds to an operation such as a handover for users. The main reason to choose a multi-headed network structure is to avoid a sequential decision making process for users. Because in sequential decisions first users tend to take greedy actions while last ones are left with suboptimal options due to BS capacity or depleted RBs at cells.\n1) State: State includes BS connection indicator, SINR values for neighbouring BSs and a utility parameter for each user. The connection indicator is a one-hot encoded array of BS connections a = [\u03b11, ..., \u03b1\u03bd].\nSINR value for user i is calculated according to their distances to BSs. For BS j, it is calculated as follows:"}, {"title": null, "content": "$\\,SINR_{i,j} = \\frac{P_{j}h_{i,j}}{N_{o} + \\sum_{k \\neq j}h_{i,k}P_{k}}$ (2)\nThe last parameter of the state information is the utility value of users. Utility parameter provides an interface between state and reward to fulfill the Markov decision process (MDP). Utility corresponds to the logarithmic representation of downlink data rate $U_{i} = 10[\\frac{ln(r_{i}) - ln(w)}{U - L}]$\nwhere ri is the data rate, w is a coefficient to scale the utility parameter. U and L are the upper and lower limit respectively. The overall state vector is given as follows:\n$State = [a, SINR_{j=1:B}, U_{i=1:K}]$ (3)\n2) Action: The action space represents the decisions that xApps produce according to the state of the environment. The type of action defines the ability of xApps. There are three different operations namely, handover, RB allocation and cell transmit power control. In this study, we wanted to simulate both direct and indirect conflicts as classified in [5] through the actions of RL agents.\nTo simulate direct conflict, we created two xApps, both employ handover capability. For the indirect conflict, xApp 1 allocates RBs among the users while xApp 2 controls the cell transmit power in addition to the handover operation. Both the number of allocated RBs and the transmit power level affect the overall channel capacity of users. Since these are different types of actions observing such conflict prior to action is almost impossible [5].\nIn the simulation, there are three types of actions and each xApp has a different ability to manage the network.\n\u2022 xApp 1: Handover and RB allocation\n\u2022 xApp 2: Handover and cell power level control\n\u2022 Final xApp: Handover, RB allocation and cell power level control\nThe action vectors of DQN models are designed as follows:\n$\u03b1 = [[DC, \u03b1_{1,1}, ..., \u03b1_{1,B}], ..., [DC, \u03b1_{K,1}], ..., [\u03b1_{K,B}]]$\n$R = [[R_{1,1}, ..., R_{1,MR}], ..., [R_{K,1}, ..., R_{K,MR}}]$\n$T = [[PL_{1,1}, ..., PL_{1,MT}], \u2026\u2026\u2026, [PL_{B,1}, ..., PL_{B,MT}}]$\n$Action = [a, R,T]$ (4)\nwhere R indicates the number of RB assigned to K users up to MR blocks, T is the transmission power levels for B BSs up to Mr level. We added \"DC\" disconnect action as a handover option to analyse how often RL agents cannot find an optimal BS to connect. Each operation in the action vector corresponds to the output head of the designed multi-headed DQN network.\n3) Reward: Reward design plays a crucial role in DRL applications that train a model to perform actions for specific tasks. Because it defines the task to which the model is trained to perform.\nTo avoid greedy actions for specific users and poor performance for the rest of the users, we chose to use proportional fairness as the reward function. While it increases with the throughput, it penalises too high or too low data rates by using a logarithmic data rate. The reward function for K users is calculated as follows:\n$PF = \\sum_{i=1}^{K} log(r_{i})$ (5)\nFor a fair service availability for each user, we employed proportional fairness where downlink data rate ri is calculated as in equation 1."}, {"title": "C. Policy distillation", "content": "Policy distillation, originally proposed for transferring a pre-trained action policy into an untrained Q-Network [10]. In this paper, we consider a scenario in which the host purchases multiple xApps from different vendors and deploys them into the network that provides service on partially or fully overlapping areas. Since the host cannot intervene with the training stage of those xApps, our proposed method uses these pre-trained xApps to distil knowledge to a student policy that will take action on behalf of all teacher networks. To simplify the process, we divided it into four stages.\nfinal xApp will operate. At this point, pre-trained xApps act as teacher models. Thus, individually deployed xApps generate data to fill a replay buffer memory. Note that we save the information before it is applied to the network. Otherwise, the network controller can change the action when applying due to O-RAN conflict mitigation. When sufficient state, action pairs are stored in replay buffer, stage 2 ends.\nIn stage 3, we use replay buffer memory to extract state and action data for distillation. xApp distillation is essentially a supervised learning scheme which uses well-trained xApps i.e., teacher model, and trains an untrained deep Q-network i.e., student model. These well-trained xApps do not necessarily have to be an ML-based method. As long as they produce an action regarding to state, they can be heuristics or a mix of both as well. Note that the action space of teacher networks and student policy can be different. As shown in Figure 2 loss is calculated by only using the matching heads of the teacher DQN model and the student policy.\nThe stage 4 of the xApp distillation is the deployment of the student policy to a DQN model for test and evaluation. In our simulations we experimented with distilling policies from heuristics, however distilling policy from another DQN model performed better.\nWhen distilling policy from a teacher network, choosing distillation loss plays a crucial role in the process. As proposed in [10] we employed KL divergence loss:\n$L_{KL}(D^{T}, \\theta_{S}) = \\sum_{i=1}^{\\vert D \\vert} \\sum_{j} softmax(q_{j}^{\\tau})ln \\frac{softmax(q_{j}^{\\tau})}{softmax(q_{f}^{\\tau})}$ (6)\nwhere D is replay buffer memory, \u03c4 is the temperature, $q_{j}^{\\tau}$ and $q_{f}^{\\tau}$ are the Q-values from the teacher and student networks respectively."}, {"title": "D. Conflict mitigation scheme", "content": "O-RAN conflict mitigation method [5] essentially proposes two types of mitigation namely against direct and indirect conflicts.\nFor direct conflicts, it is easy to observe and avoid conflicts. O-RAN alliance proposes simply ignoring one of the xApps and applying the other for the network. However, for indirect conflicts, it is almost impossible to observe conflicts prior to the action. Therefore, they propose to monitor the network performance, and if the network performance deteriorates after the performed action, it rolls back the action and applies previous actions. These are the essential mitigation scheme for inter-xApp conflicts.\nIn both cases, it ignores some of the xApps and applies an action taken by different xApps or taken for the previous state of the network which leads to possible sub-optimal actions. However, the xApp distillation utilises all xApps' abilities by using their experiences on the policy distillation. Moreover, after xApp distillation, a single xApp which is the result of distillation is deployed on the environment. Thus, it eliminates inter-xApp conflicts."}, {"title": "IV. PERFORMANCE EVALUATION", "content": "In this paper, we use the \"mobile-env\" [7] wireless communication environment wrapped in the Gymnasium framework as a base of our simulations. To implement conflicts we added new resource types to control such as resource block allocation among users and base station transmit power level control.\nWe compared our results with individually trained and deployed xApps, and xApps that are trained using the team learning method [6]. Both methods are subject to O-RAN conflict mitigation due to the deployment of multiple xApps in a single environment.\nWe simulated this method using DQN models as the target of distillation. However, this is an open method to other DRL-based controllers. The DQN model structure we used for xApps is relatively lightweight. The structure of the model is an MLP, consisting of 4 layers, and hidden layers have 50 and 100 neurons respectively. We used SGD optimiser to update teacher xApp models throughout 100,000 episodes of pre-training with a learning rate of 0.01. After pre-training was completed we used 10,000 post-training steps to save S, A, R transitions that will be used to distil knowledge to the student model. The temperature value for distillation is set to 20."}, {"title": "B. Numerical results", "content": "O-RAN conflict mitigation applies one of the xApps during the conflicts and eliminates the rest. Therefore, alternating network control options creates inconsistent service for the users. The figure compares the xApp distillation with O-RAN conflict mitigation applied to individually trained agents and the agents trained by using the team-learning method [6]. While the team-learning method improves the performance of individually trained and deployed xApps, xApp distillation shows overall the highest data rates.\nTo evaluate these results, we compared these methods in terms of network outage in 50,000 time steps in the environment. The network outage for 5 Mbps to 100 Mbps for all methods is given in Figure 4.\nThe figure demonstrates the network outage ratio for Individual Learning, Team Learning, and our xApp Distillation method across different bandwidths. Notably, xApp Distillation significantly outperforms the other methods. For instance, at 10 Mbps, it achieves an 83.3% reduction in network outage compared to Team Learning. Even at higher bandwidths like 50 Mbps, xApp Distillation maintains a 33.3% improvement over Individual Learning. This highlights xApp Distillation's effectiveness in consistently reducing network outages and enhancing reliability."}, {"title": "V. CONCLUSION", "content": "In this paper, we show that xApp distillation eliminates inter-xApp conflicts and it provides significantly better performance than state-of-the-art studies in terms of reliable and consistent service. Since indirect and implicit conflicts are almost impossible to detect or mitigate, our method takes advantage of using the experience of pre-trained xApps and take actions from a single decision-maker instead of applying multiple xApps actions simultaneously which can cause such conflicts. xApp distillation provided a consistent 10 Mbps downlink data rate where compared methods failed to provide up to more than 30 percent simulation runtime.\nIn our future work, we plan to extend this xApp distillation method to use more scalable RL controllers than DQN method such as Multi-agent RL and decomposed action-space methods. Moreover, we will increase the number of KPIs to analyse network in a more comprehensive way."}]}