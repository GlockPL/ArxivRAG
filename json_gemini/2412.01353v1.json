{"title": "Su-RoBERTa: A Semi-supervised Approach to Predicting Suicide Risk through Social Media using Base Language Models", "authors": ["Chayan Tank", "Shaina Mehta", "Sarthak Pol", "Vinayak Katoch", "Avinash Anand", "Raj Jaiswal", "Rajiv Ratn Shah"], "abstract": "In recent times, more and more people are posting about their mental states across various social media platforms. Leveraging this data, AI-based systems can be developed that help in assessing the mental health of individuals, such as suicide risk. This paper is a study done on suicidal risk assessments using Reddit data leveraging Base language models to identify patterns from social media posts. We have demonstrated that using smaller language models, i.e., less than 500M parameters, can also be effective in contrast to LLMs with greater than 500M parameters. We propose Su-ROBERTa, a fine-tuned RoBERTa on suicide risk prediction task that utilized both the labeled and unlabeled Reddit data and tackled class imbalance by data augmentation using GPT-2 model. Our Su-RoBERTa model attained a 69.84% weighted F1 score during the Final evaluation. This paper demonstrates the effectiveness of Base language models for the analysis of the risk factors related to mental health with an efficient computation pipeline.", "sections": [{"title": "1. Introduction", "content": "Suicide is one of the major public health problems with profound effects on the lives of individuals, families, and communities at large. The World Health Organization reports that an estimated 720,000 people die each year from suicide. It is one of the leading causes of death among young people and other vulnerable groups, such as refugees or migrants, as well as other sexual orientations [1]. Factors that result in suicidal behavior are multifaceted and complex. They include financial difficulties, interpersonal conflicts, chronic illness, and mental health disorders such as depression and anxiety. Knowledge of these contributing factors is critical in the development of effective prevention strategies. The traditional methods of suicide assessment rely on structured interviews and assessments, which consume a lot of time and are prone to personal biases.\nRecently, a few studies show that people who are having suicidal thoughts use social media more frequently for sharing their mental state as well as suicidal thoughts [2]. As a result, social media has become the major source for collecting data related to suicide for developing several solutions for suicide risk assessment. Using anonymity features in Social media Platforms such as Twitter and Reddit, users are encouraged to openly discuss their emotional states and mental health issues, which can provide rich insights for understanding suicidal ideation and facilitating timely interventions.\nOver the years, many researchers have leveraged online data to model various aspects of mental health tasks, such as depression detection and suicidal ideation, by employing large language models (LLMs) for a deeper contextual understanding and reasoning [3] [4]. Through these researches, it is clear that LLMs show superior performance for text-based classification tasks in the mental health domain, but They are also computationally expensive to perform if fine-tuning is required for a specific task, our aim for this paper is to demonstrate that simpler language models with fewer parameters, being efficient can also provide a decent performance on same tasks, clearly not as well as bigger LLMs, but certainly more deployable on mobile compute or edge devices. With simplicity, efficiency, and deployability in mind, we propose Su-RoBERTa, our semi-supervised fine-tuned RoBERTa model on the suicide risk dataset revealed in the IEEE BigData 2024 Detection of Suicide Risk on Social Media Competition. To train this model, we also had to perform a compute-efficient way of data augmentation to tackle the data imbalance of this dataset. For this, we used the GPT-2-124M parameter model, leveraging its Superior generative capabilities for upsampling the sensitive suicide"}, {"title": "2. Literature Review", "content": "In recent years, to prevent suicide, several researchers have started working on AI-based solutions for suicidal risk prediction using social media posts from several social media websites such as Twitter, Reddit, etc. Considering the work done by several researchers on suicidal risk assessment, such as Mirtaheri et al., [5] proposed an AL-BTCN model consists of LSTM (Long Short Term Memory), Bi-TCN (Bidirectional Temporal Convolutional Neural Network) and self attention layer for suicide ideation detection among users. They extracted word embeddings from BERT (Bidirectional Encoder Representations from Transformers) model on Twitter and Reddit datasets obtained from Github and Kaggle and fed into AL-BTCN model for training. They achieved F1 score of 92 percent, 92 percent and 95 percent from two Twitter datasets and one Reddit dataset respectively. Another approach is proposed by Oyewale et al. [6] uses CNN and Bi-LSTM network for suicidal risk assessment. They extracted the Word2Vec and FastText word embeddings on a Reddit dataset proposed by Aldhyani et al. [7] and used it to train their proposed architecture and achieved an F1 score of 90.30 percent and 93.56 percent using Word2Vec and FastText embeddings respectively.\nSquires et. al [8] leveraged semi-supervised learning and proposed deep label bayesian smoothing method for capturing uncertainties during assessment of suicidal risk and using this technique to train CNN based network. They achieved accuracy and F1 Score of 52.333 percent and 47.77 percent on Reddit C-SSRS dataset proposed by Gaur et al. [9]. Sawhney et. al [10] considered suicidal risk prediction as ordinal Regression problem and proposed SISMO model which consists of Bi-directional Long Short Term Memory (Bi-LSTM) and attention mechanism, for this purpose. They extracted the textual embeddings from the dataset proposed by Gaur et al. [9] and used it to train their proposed archi-"}, {"title": "3. Dataset Description", "content": "The Dataset used for the IEEE BigData 2024 Detection of Suicide Risk on Social Media Competition was proposed by Li et al. [20] and it consists of 2100 Reddit posts, out of which 2000 are reserved for model training and validation and 100 are reserved for testing purposes. The training set consists of 500 labeled Reddit posts and 1500 unlabeled Reddit posts, and the test set contains 100 posts whose labels are unknown to us as per the competition guidelines. The leaderboard position was determined by the evaluation of the revealed test set without labels, and the Final evaluation was done on an unrevealed test set.\nIn the Labeled training samples, Each post is categorized into 4 categories which are:\n\u2022 Ideation These 190 posts contain suicide content, but there is no intention to die by suicide. They can speak about thoughts, feelings, or ideation for suicide, but they do not have the act or a more detailed suicide plan.\n\u2022 Behavior These 140 posts contain suicide content that speaks about suicide with a clear plan or intent to commit suicide. This category contains a more severe level of risk compared to ideation, as the user might have details on how they intend to act in this regard.\n\u2022 Indicator These 129 posts do not contain contents that simply have suicidal intent. The language or tone may involve distress or general emotional issues, but nothing"}, {"title": "4. Data Augmentation", "content": "The class distribution in the dataset was imbalanced, as seen in Figure 3, with 'Attempt' being the minority class and 'Ideation' being the Majority class. The preliminary results were impacted by this imbalance. Hence, data augmentation was necessary for the dataset distribution and further modeling. We performed data augmentation using a RoBERTa-base and GPT-2-base model to balance the dataset, which is mentioned in section 4.1 and 4.2 respectively and can also be seen as Stage 1 in Figures 1 and 5 respectively."}, {"title": "4.1. GPT-2", "content": "A GPT-2-Base 124M parameter language model was chosen, keeping computational efficiency and faster text generation in mind. As seen in Figure 4, we up-sampled the minority classes 'Behavior' and 'Indicator' to 190 samples, equating them to 'Ideation', which was the majority class. The 'Attempt' class was upsampled to 82 samples as there were only 41 original samples of it; up-sampling it to 190 samples degraded the quality of the samples, so we augmented it to double the original size. The GPT-2 model was fine-tuned to generate synthetic posts for these Minority classes. This augmented dataset increased 152 samples using upsampling, and the final labeled training set contained 652 samples for training instead of 500 initially. This Tackled the class imbalance and increased the labeled training samples as well."}, {"title": "4.2. ROBERTa", "content": "ROBERTa model proposed by Liu et al. [21], an extension of BERT model mainly used for only Masked Language Modeling task and handles noisy datasets effectively. We performed the data augmentation by generating similar sentences from the minority classes using the ROBERTa base model, inspired by the approaches mentioned in [22] [23], which is available in NLPAug 1 library in Python. We have upsampled 'Behavior', 'Indicator' and 'Attempt' class to 190, 190 and 82 samples respectively, leading to final labeled training set contained 652 samples for training instead of 500 initially which is shown in Figure 4."}, {"title": "5. Proposed Methodology", "content": "The Objective of the competition was to classify the 100 test set samples into one of these 4 classes based on the patterns learned by leveraging the 500 labeled and 1500 unlabeled samples from the training dataset.\nFor our preliminary experimentations, We initially tried a classical semi-supervised approach by using NLPAug and SVM classifiers for data augmentation and modeling, respectively. This approach, although highly compute efficient is not that powerful, and its performance on evaluation is not that impressive. This compelled us to shift towards language models, mainly base language models such as BERT and RoBERTa, which are still more efficient than LLMs such as Llama-8B, Mistral models, GPT-3.5, etc. The augmentation for this case has also been done by a GPT-2 language model, which is a better generative model than the base language models."}, {"title": "5.1. Classical Semi-supervised approach: Support Vector classifier (SVC)", "content": "Given the size of the data and the large number of unlabeled training data, initially, we opted to use a classical semi-supervised learning approach inspired by [24] [25], keeping computational efficiency as the goal. For the preprocessing of data, we have replaced emojis, converted the text to lowercase, removed special characters, HTML tags, and accented tags, and expanded acronyms.\nTo remove the data imbalance, we performed data augmentation by generating similar sentences from the minority classes as mentioned in section 4.2. Then, textual embeddings are extracted using Sentence BERT proposed by Reimers et al. [26] using Sentence Transformers library in Python 2, which captures the semantics of the sentences. The labeled and unlabeled training data is combined and fed into the SVM classifier, and the model is trained using a semi-supervised algorithm from Scikit-learn library in python 3 whose implementation is based on [27]. This pipeline can be seen in Figure 5 in 3 Stages."}, {"title": "5.2. Base Language Model approach: Su-RoBERTa", "content": "Even while leveraging the capabilities of Language models, our primary goal was to demonstrate comparable results to Large Language models greater than 500M parameters while minimizing computational demands. By using Base language models, we aimed to design an efficient, lightweight classifier pipeline capable of delivering predictions with reduced resource requirements, making it suitable for deployment on low-compute devices like mobile phones.\nAs discussed in section 4.1, we first up-sampled the minority classes and then used the augmented data along with the original samples to Finetune the ROBERTa model, Keeping computational complexity in mind, in an iterative semi-supervised approach presented in Figure 1. The RoBERTa model is generally better at understanding deep contextual meaning, while GPT-2 excels at generating text.\nInitially, the RoBERTa 355M parameter model is fine-tuned on 4 class classification task using the augmented dataset. After fine-tuning the model on the labeled training samples, it is made to predict the classes of unlabeled training samples, which were 1500 in the count. Using these predictions, we pseudo-labeled the training data by following a greater than '0.33' probability policy, i.e., we only pseudo-labeled the samples that were predicted as a certain class with confidence of more than 0.33 probability, this probability threshold was decided by empirical experimentations. This ensured that the low-confidence predictions were not included as noise to further training of the model.\nThese new Pseudo-labeled samples, along with the original samples, are now used as a training corpus to fine-tune the ROBERTa models again from scratch for better generalization. This approach is once again repeated, making a total of 2 iterations. The final Su-ROBERTa model was trained on the output pseudo-labeled data along with the original data; the sample count for final training included the whole of 2000 samples in the dataset available for training. This Fine-tuning pipeline has been shown in Figure 6. The finetuning of the Model was performed using AdamW optimizer, with a batch size of 8 samples running for 10 epochs on a 24 GB NVIDIA GeForce RTX 3090. The whole semi-supervised finetuning pipeline took 30 minutes to complete.\nThis demonstrates the Low compute required to fine-tune such a model for mental health prediction tasks. In the future, such training pipelines can also be deployed on mobile devices that are not that compute-heavy to train or finetune LLMs."}, {"title": "6. Evaluation", "content": null}, {"title": "7. Results", "content": "This section discusses the Final results achieved from both the approaches discussed in section 5. Both the Su-ROBERTa model and the Support vector classifier are evaluated using the weighted F1 scores as per the competition guidelines. The results of both approaches are given in table 2. The test set of the competition was not revealed, and the evaluations have been done on the leaderboard of the challenge.\nReferring to the evaluation table 2, the SVM model achieved a Weighted F1 Score of 50.52 percent, whereas the Su-ROBERTa model achieved a Weighted F1 Score of 61.31 percent during preliminary evaluation. Based on the Weighted F1 Scores of both models, we have submitted the Su-ROBERTa model for final evaluation. On final evaluation, the Su-ROBERTa model achieved the Weighted F1 Score of 69.84 percent, which led us to the 10th rank in the leaderboard of the competition."}, {"title": "8. Conclusion", "content": "The paper demonstrates the superiority of language models for social media suicidal risk prediction, leveraging smaller language models, i.e., base language models such as GPT-2 and RoBERTa, to be made into efficient language classifiers that are low-compute and deployable. These models achieved a decent performance on the challenge leaderboard, however, these still need to be improved for real-time mobile applications. In the future, we will focus on using techniques such as prompt engineering to enhance the LLMs' performance and use some advanced Large language models such as GPT-40, OpenAI-01 and Llama-3.2 models for data augmentation, labeling, and finetuning to improve the predictions on such mental health tasks."}, {"title": "9. Future Scope", "content": "For a richer understanding, the inclusion of Multi-modal datasets for suicide and mental health analysis is crucial, Combining audio and visual data along with text for a complete Contextual overview. Social media platforms have evolved significantly in recent years. The shift towards multimedia content to platforms such as TikTok, Instagram, and YouTube Shorts popularizing the use of short video clips has provided new opportunities for analyzing social media data. To effectively assess the mental health of individuals based on these rich, multi-modal data forms, developing new architectures capable of understanding and processing multiple modalities is now essential. More metadata can be included in the analysis of mental health, such as sleep and stress detected through a wearable tracker and heart rate detection systems.\nExplainability analysis of these architectures and models is also crucial for mental health tasks as this type of data is sensitive, and the predictions need an underlying reason for the classification so that they can be verified by trained clinicians."}]}