{"title": "Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt Generation for Enhanced LLM Content Moderation", "authors": ["Daniel Schwartz", "Dmitriy Bespalov", "Zhe Wang", "Ninad Kulkarni", "Yanjun Qi"], "abstract": "We present a modular pipeline that automates the generation of stealthy jailbreak prompts derived from high-level content policies, enhancing LLM content moderation. First, we address query inefficiency and jailbreak strength by developing Graph of Attacks with Pruning (GAP), a method that utilizes strategies from prior jailbreaks, resulting in 92% attack success rate on GPT-3.5 using only 54% of the queries of the prior algorithm. Second, we address the cold-start issue by automatically generating seed prompts from the high-level policy using LLMs. Finally, we demonstrate the utility of these generated jailbreak prompts of improving content moderation by fine-tuning Prompt-Guard, a model trained to detect jailbreaks, increasing its accuracy on the Toxic-Chat dataset from 5.1% to 93.89%.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have shown impressive abilities in generating human-like text, but this power comes with vulnerabilities that can be exploited to produce harmful, biased, or misleading content, raising significant challenges for safe and responsible deployment. Although content moderation guardrails can filter and block undesirable outputs, they are continually challenged by 'jailbreaking' techniques that bypass these safety measures. Table 1 highlights the limitations of current guardrails, as evidenced by their low true positive rates when detecting modern jailbreaking techniques Prompt Guard, Llama Guard , and a perplexity-based machine learning classifier. In the pursuit of continually improving content moderation guardrails for safe deployment of LLM applications, this research develops a repeatable,\nmodular pipeline to efficiently generate diverse, stealthy jailbreak prompts for robust moderation system evaluation and fine-tuning, contributing to a more secure LLM ecosystem. Our pipeline addresses shortcomings in existing pipelines such as manual seed prompt creation, vulnerability to detection, and inefficiency. The primary contribution integrates two key components:\n1. Efficient and Stealthy Jailbreaks. Section 3.1: Existing jailbreak techniques jailbreak each seed prompt independently, resulting in more queries to the target LLM. We propose Graph of Attacks (GAP), which improves jailbreak efficiency and stealth by utilizing dynamic, interconnected reasoning, refining attack vectors, reducing queries, and evading detection.\n2. Improved Content Moderation. Section 3.2: Our pipeline leverages the generated jailbreak prompts to fine-tune content moderation systems, substantially enhancing their detection capabilities across diverse types of harmful content. This approach significantly improves key performance metrics, demonstrating increased robustness against evolving jailbreak"}, {"title": "2 Related Studies", "content": "Jailbreaking techniques exploit vulnerabilities in large language models (LLMs) to bypass safety mechanisms and produce harmful or unintended outputs. Traditional methods such as the Tree of Attacks with Pruning (TAP) employ linear structures for exploring attack vectors but are limited in efficiency and adaptability. Approaches such as GPTFuzzer and GCG rely on heuristic rules and template-based methods, which, while effective, require substantial manual intervention and often generate limited prompt diversity. AutoDAN introduces automation via pre-trained models to streamline prompt generation, but it struggles to evade advanced content moderation systems such as Llama Guard and Prompt Guard. Despite achieving high success rates in bypassing these guardrails, these methods often depend on high query counts and lack scalability. Recent benchmarks, such as JailbreakBench, highlight the growing sophistication of jailbreak techniques while also exposing their vulnerabilities to advanced moderation systems."}, {"title": "3 Methodology", "content": "3.1 Efficient Generation of Stealthy Jailbreak Prompts\nExisting methods for generating adversarial prompts, such as GCG, GPT-Fuzzer, and AutoDAN, often face efficiency limitations and detection by content moderation systems. To address these challenges, we extend the Tree of Attacks with Pruning (TAP) method to introduce the Graph of Attacks with Pruning (GAP). We chose TAP as our foundation due to its superior performance in evading detection by state-of-the-art guardrails, as evidenced by its consistently low True Positive Rates across multiple\nsecurity measures (refer to Table 1). Inspired by the Graph-of-Thoughts (GoT) structure, GAP employs a dynamic, graph-based sampling strategy for more diverse and effective prompt generation. GAP builds on TAP by replacing its linear structure with interconnected prompt nodes, allowing for more comprehensive exploration.\nGAP advances beyond TAP by implementing a graph structure instead of a tree, enabling more complex relationships between prompts. The key distinction lies in GAP's use of a global context derived from all conversation histories, sorted by maximum refinement score, rather than relying solely on individual path histories (as shown in Algorithm 1, Appendix A.1.1). This allows GAP to leverage information from multiple branches when generating and selecting new candidate prompts based on estimated effectiveness.\nThe graph-based approach helps identify synergies and patterns across branches, creating a self-improving knowledge repository that adapts flexibly to different jailbreaking scenarios. While both systems use a two-phase pruning strategy for efficiency, GAP's interconnected structure enables more comprehensive exploration of the prompt space. This advancement in prompt refinement techniques demonstrates the potential for more effective adversarial attacks, highlighting the ongoing need for research in AI safety and defensive measures against evolving language model vulnerabilities.\n3.2 Improving Content Moderation Using Learned Jailbreaks as Augmentation\nOur research aims to enhance the safety and security of LLM applications by strengthening content moderation guardrails, particularly in detecting stealthy jailbreak attacks. As demonstrated by our results in Table 1, the TAP method achieved the lowest True Positive Rate, bypassing multiple guardrails across many seeds, including just 22.0% for Prompt Guard, highlighting the need for improved detection mechanisms. To address this challenge, we have developed a pipeline that generates diverse and stealthy jailbreak prompts, leveraging our proposed GAP method, which can be used to evaluate and fine-tune existing moderation systems more effectively.\nA critical component of this pipeline is the use of two complementary datasets: the Seed Prompt Dataset (SPD) and the GAP-Seed Prompt Dataset (GAP-SPD). The SPD provides a balanced"}, {"title": "4 Experiments and Results", "content": "4.1 Graph of Attacks with Pruning (GAP) Performance on AdvBench\nWe begin our evaluation by assessing the effectiveness of our method, Graph of Attacks with Pruning (GAP), in generating efficient and stealthy jailbreak prompts. To provide a comprehensive analysis, we compare GAP's performance against the Tree of Attacks with Pruning (TAP) method using the AdvBench dataset. Table 2 presents the results of this comparative analysis.\nIn our experiments, we evaluate two versions of GAP: GAP-M, which utilizes a larger, more capable model (Mistral Large 2407) as the attacker\nLLM, and GAP-V, which employs Vicuna-13B-v1.5 for a direct, equivalent comparison with TAP. This dual approach allows us to showcase both the full potential of our method and its effectiveness when using comparable resources.\nThe results show that GAP-M consistently outperforms TAP across all evaluated models. For GPT3.5, GAP-M achieved a remarkable 96% attack success rate (ASR) with only 10.4 queries on average, significantly surpassing TAP's 78% ASR and 26.3 queries. We observed similar superior performance for other models, with GAP-M achieving 100% ASR for both Gemma-2-9B (4.22 queries) and Qwen2.5-7B (6.72 queries), compared to TAP's lower ASR and higher query counts.\nImportantly, even when using the same attacker model as TAP (Vicuna-13B-v1.5), GAP-V still demonstrates superior performance. For GPT3.5, GAP-V achieved a 92% ASR using 14.2 queries, outperforming TAP's 78% ASR and 26.3 queries. This trend continues across other models, with GAP-V consistently achieving higher ASR with fewer queries than TAP."}, {"title": "4.2 Automated Seed Generation and Evaluation Datasets", "content": "Robust content moderation systems require diverse and comprehensive datasets to ensure effective training and evaluation. To this end, we contribute two primary datasets, the Seed Prompt Dataset (SPD) and the GAP-Seed Prompt Dataset (GAP-SPD), which address limitations in existing benchmarks by providing balanced, diverse, and stealthy prompts. The SPD serves as the foundation for the GAP-SPD, capturing a wide range of benign and harmful behaviors, while the GAP-SPD refines harmful prompts into stealthy adversarial cases optimized to bypass content moderation systems.\nWhile existing benchmarks such as AdvBench and JBB provide valuable baselines, they primarily focus on predefined harmful behaviors or simplistic adversarial cases. In contrast, the SPD offers balanced pairing of benign and harmful prompts across fine-grained behaviors, and the GAP-SPD integrates stealthy adversarial cases optimized for real-world attack scenarios. These contributions address key gaps in existing datasets, making them uniquely suited for fine-tuning and evaluating content moderation systems."}, {"title": "4.2.4 Diversity and Utility of GAP-SPD", "content": "To validate the effectiveness of our automated seed generation process, we evaluated the diversity of prompts in GAP-SPD using established metrics from , including Unique N-grams, Entropy of N-grams, and Self-BLEU. Table 4 demonstrates that GAP-SPD outperforms AdvBench and JBB-Behaviors across all metrics, achieving the highest diversity and lowest similarity between prompts. This diversity ensures that GAP-SPD provides a comprehensive and challenging benchmark for training and evaluating content moderation systems.\nThe Seed Prompt Dataset (SPD) ensures balanced and diverse coverage of harmful and benign prompts, while the GAP-Seed Prompt Dataset (GAP-SPD) integrates stealthy adversarial cases, challenging content moderation systems with realistic attack scenarios. Combined with established benchmarks, these datasets contribute to a comprehensive evaluation framework for fine-tuning content moderation tools. By bridging theoretical advancements and practical applications, this framework enables robust defenses against evolving"}, {"title": "4.3 Fine-tuning Prompt Guard and Performance Evaluation", "content": "Using the GAP-Seed Prompt Dataset (GAP-SPD), we designed a comprehensive experimental setup to evaluate the effectiveness of fine-tuning Prompt Guard. Our approach leverages the streamlined training requirements of Prompt Guard-86M, which allows for fine-tuning using only prompt inputs and their corresponding binary safety labels.\nTo assess the performance improvements and generalization capabilities of our fine-tuned model, we employed a multi-faceted evaluation approach. We used several key metrics, including True Positive Rate (TPR), accuracy, and F1 score, to measure the model's effectiveness in identifying harmful content. The evaluation was conducted on three distinct datasets: a held-out test set from the GAP-SPD, the Toxic Chat dataset, and the OpenAI Moderation dataset.\nTable 5 showcases the substantial improvements in True Positive Rate (TPR), accuracy, and F1 score across all three test sets. The fine-tuned model consistently outperforms the base model across all metrics and datasets. Notably, on the GAP-SPD test set, the accuracy increased dramatically from 34.9 to 90.6, while the TPR improved from 64.6 to 86.1. Similar significant improvements were observed in the Toxic Chat and OpenAI Moderation datasets, demonstrating the model's enhanced ability to generalize across different types of harmful content. For a more comprehensive analysis of the fine-tuning results, including additional metrics such as precision, recall, and false positive rate (FPR), please refer to Table 7 in Appendix A.3.1.\nAdditionally, we performed a comparative analysis to benchmark our fine-tuned Prompt Guard against other guardrails, including Perplexity, Llama Guard, and the base Prompt Guard model. This comparison, presented in Table 6, focused on the models' abilities to detect various jailbreak methods, including Seeds, GPTFuzzer, GCG, TAP, and GAP. The results further underscore the effectiveness of our fine-tuning approach, with the fine-tuned Prompt Guard showing markedly improved performance in detecting various jailbreak methods compared to other guardrails. Particularly noteworthy are the improvements in detecting TAP and GAP-generated jailbreaks, with TPR increasing from 22.0 to 66.0 and 16.0 to 70.0, respectively."}, {"title": "5 Conclusion", "content": "This research offers a comprehensive framework to improve LLM security by automating the generation of diverse and stealthy prompts. We introduce the Graph of Attacks with Pruning (GAP) methodology, which refines harmful prompts into stealthy jailbreak examples optimized for bypassing moderation systems. By overcoming the limitations of manual prompt creation and linear models like TAP, GAP enables a more dynamic exploration of attack vectors, achieving higher success rates and fewer queries, making attacks more efficient and harder to detect.\nIn addition to advancing LLM security offensively, we highlight the value of integrating GAP's outputs into defensive systems. The development of the Seed Prompt Dataset (SPD) and the GAP-Seed Prompt Dataset (GAP-SPD) demonstrates the practical utility of our methodology. The SPD provides a balanced and diverse collection of benign and harmful prompts for exploring vulnerabilities, while the GAP-SPD embodies both diversity and stealth, offering a robust benchmark for evaluating and fine-tuning moderation systems. By leveraging GAP's query-efficient graph-based approach, the GAP-SPD bridges theoretical advancements with real-world applications, enabling more effective training of content moderation tools. Fine-tuning the Prompt Guard tool with adversarial examples from GAP-SPD significantly improved its ability to detect harmful content across multiple domains. These improvements in key metrics such as accuracy, precision, recall, and F1 score highlight the effectiveness of integrating adversarial insights into moderation frameworks, leading to more robust defenses against evolving threats.\nWhile our research demonstrates promising results, there are limitations to consider. The effectiveness of our approach depends on the quality and diversity of training data, as well as models' adaptability to new jailbreaks. Additionally, while GAP performs well in controlled environments, its scalability and efficiency in real-time applications require further exploration. Future work will focus on expanding adversarial techniques, assessing GAP's performance with additional LLM architectures, and addressing scalability in real-world settings. We also aim to investigate the ethical implications of LLM security, particularly regarding biases in content moderation systems.\nThis research underscores the vulnerabilities in"}, {"title": "A Appendix", "content": "A.1 GAP Algorithm and Performance\nA.1.1 GAP Algorithm Pseudocode\nThis section presents pseudocode (Algorithm 1) for a graph-based prompt refinement algorithm, Graph of Attacks with Pruning (GAP), which builds upon the Tree of Attacks with Pruning (TAP) approach. The key differences, highlighted in yellow, include the use of a graph structure instead of a tree, allowing for more complex relationships between prompts. The algorithm introduces a global context derived from all conversation histories, sorted by maximum refinement score, rather than relying solely on individual path histories. This enables the Attacker to consider information from multiple branches when generating new prompts. The prompt selection process is also modified, choosing the prompt with the highest estimated score based on the global context. These changes aim to create a more interconnected and adaptive exploration of the prompt space compared to the linear approach of TAP.\nA.1.2 GAP vs TAP Performance Analysis\nOur comprehensive analysis of the Graph of Attacks with Pruning (GAP) method in comparison to the Tree of Attacks with Pruning (TAP) method reveals consistent and significant performance improvements across multiple dimensions. Figures 3, 4, and 5 illustrate these improvements for three target language models (GPT-3.5, Gemma2-9B, and Qwen2.5-7B) and three distinct LLM query budgets: Target, Attacker, and Evaluator. Across all scenarios, GAP demonstrates superior efficiency and effectiveness in generating jailbreak prompts, achieving higher success rates with fewer queries compared to TAP.\nFigure 3, focusing on the Target LLM query budget, showcases GAP's ability to achieve higher success rates with fewer queries across all models. This efficiency is particularly evident for Gemma2-9B, where GAP reaches near-perfect success rates even at low query budgets. Figures 4 and 5, examining the Attacker and Evaluator LLM query budgets respectively, further reinforce GAP's superior performance. The consistent outperformance across different query types highlights GAP's versatility and robust design, enabling it to optimize the jailbreaking process at various stages.\nInterestingly, the performance variations across models suggest differing levels of robustness to"}, {"title": "A.2 Dataset Generation and Evaluation", "content": "A.2.1 Automated Seed Generation Process\nThe development of our Seed Prompt Dataset (SPD) begins with an \"Attacker\" LLM , which generates diverse prompts designed to explore less obvious attack vectors. These prompts are then evaluated by an \"Evaluator\" LLM , which selects the most promising ones for further refinement.\nFigure 6 illustrates the two-phase process we designed to construct the SPD, a unique and diverse collection of prompts aimed at evaluating LLM safety. In the first phase, we expand upon existing benchmarks, such as the JailbreakBench-Behaviors dataset , by applying our own metaprompting techniques with Mistral Large 2 (2407) and in-context learning to generate 100 fine-grained behaviors for each of ten top-level categories (e.g., disinformation, economic harm, privacy). This phase results in 1,000 distinct behaviors tailored to explore potential vulnerabilities.\nIn the second phase, we use similar metaprompting techniques to generate specific seed prompts for each fine-grained behavior, creating 1,000 harmful seed prompts designed to elicit harmful content. To ensure balance, we repeat the process to generate 1,000 benign seed prompts, each corresponding to a non-harmful request aligned with the same behaviors. This deliberate and balanced approach ensures that the SPD is broader and more comprehensive than existing datasets, with equal representation of benign and harmful prompts. The result is a dataset that reflects the full spectrum of potential LLM vulnerabilities while providing a robust foundation for evaluating and improving content moderation systems."}, {"title": "A.3 Prompt Guard Evaluation", "content": "A.3.1 Fine-tuning Results\nTable 7 presents a comprehensive comparison of performance metrics between the base Prompt Guard model and its fine-tuned version across three distinct test domains: GAP-Seed Prompt Dataset (GAP-SPD), Toxic Chat, and OpenAI Moderation. The results demonstrate significant improvements across all key metrics after fine-tuning with prompts from the GAP-SPD. Notably, the True Positive Rate (TPR) shows a substantial increase, indicating enhanced detection of harmful content. Accuracy and F1 scores also exhibit marked improvement, reflecting a better overall performance in distinguishing between benign and harmful prompts. The fine-tuned model also demonstrates improved precision and recall, suggesting a more balanced and effective approach to content moderation. These results collectively underscore the effectiveness of leveraging the GAP-SPD in fine-tuning Prompt Guard, resulting in a more robust and accurate content moderation system."}]}