{"title": "Addressing Vulnerabilities in AI-Image Detection: Challenges and Proposed Solutions", "authors": ["Justin Jiang"], "abstract": "The rise of advanced AI models like Generative Adversarial Networks (GANs) and diffusion models such as Stable Diffusion has made the creation of highly realistic images accessible, posing risks of misuse in misinformation and manipulation. This study evaluates the effectiveness of convolutional neural networks (CNNs), as well as DenseNet architectures, for detecting Al-generated images. Using variations of the CIFAKE dataset, including images generated by different versions of Stable Diffusion, we analyze the impact of updates and modifications such as Gaussian blurring, prompt text changes, and Low-Rank Adaptation (LoRA) on detection accuracy. The findings highlight vulnerabilities in current detection methods and propose strategies to enhance the robustness and reliability of AI-image detection systems.", "sections": [{"title": "1. Introduction", "content": "The rapid advancement of artificial intelligence (AI) has led to significant improvements in image generation techniques, resulting in AI-generated images that are increasingly indistinguishable from real photographs. Models such as Generative Adversarial Networks (GANs) and diffusion models like Stable Diffusion have made it possible to create highly realistic images with minimal input from users. The accessibility of these tools has expanded, with open-source implementations and user-friendly interfaces making them available to a broader audience. While these developments have numerous beneficial applications in fields such as entertainment, art, and design, they also pose significant risks. The ease with which realistic images can be generated raises concerns about their potential misuse in activities like blackmail, manipulation, and the spread of misinformation.\nDetecting AI-generated images has thus become an essential area of research. Convolutional Neural Networks (CNNs) have shown promise in image classification tasks, including the detection of manipulated or synthesized images. However, as AI-generated images become more realistic, existing detection methods require enhancement to maintain their effectiveness.\nThis paper focuses on evaluating the effectiveness of using CNNs to detect AI-generated images, particularly those produced by Stable Diffusion-based generators. We explore vulnerabilities in current detection approaches, such as susceptibility to adversarial attacks and overfitting to specific data distributions. Additionally, we investigate the use of DenseNet architectures to improve the accuracy and robustness of detecting AI-generated images. DenseNets, known for their efficient feature propagation and reduced parameter count, may offer advantages over traditional CNNs in this context.\nThe datasets used in this study are variations of the CIFAKE dataset, a widely referenced resource for training and evaluating AI-image detectors. The original CIFAKE dataset comprises 120,000 images: 60,000 real images sourced from the CIFAR-10 dataset and 60,000 synthetic images generated using Stable Diffusion 1.4. The synthetic images replicate the categories in CIFAR-10 (e.g., airplanes, cats, and trucks) using prompts such as \"A photograph of [object],\" supplemented with context-specific modifiers to enhance realism. All images were resized to 32x32 pixels for computational efficiency. To assess the generalizability and limitations of the CNN-based models, two extended datasets-CIFAKE-SD2.1 and CIFAKE-SD3.0-were created using Stable Diffusion 2.1 and 3.0, respectively. These datasets preserve the structure and composition of the original CIFAKE dataset but feature AI-generated images from updated versions of Stable Diffusion, providing a robust testbed for evaluating the impact of model updates on detection accuracy.\nIn addition to examining the impact of different Stable Diffusion versions on model detection accuracy, this study investigates various other factors that could influence detection performance, including alterations and modifications to image generation or the generated images themselves. Specifically, factors such as Gaussian blurring, variations in prompt text, and adjustments to the image generation model using Low-Rank Adaptation (LoRA) are analyzed. Corresponding datasets, including CIFAKE-SD2.1-Blurred, CIFAKE-SD2.1-GPT40, and CIFAKE-SD2.1-LoRA, were systematically generated to facilitate these evaluations."}, {"title": "2. Related Work", "content": "The rapid advancement of generative models has led to the proliferation of highly realistic AI-generated images, raising concerns about authenticity and the potential for misuse. Detecting these synthetic images has become a critical area of research, with various methodologies proposed to address the challenge. Convolutional Neural Networks (CNNs) have been extensively employed for image classification and forgery detection tasks. For instance, [8] introduced the DenseNet architecture, which enhances feature propagation and reduces redundancy by connecting each layer to every other layer in a feed-forward manner. This architecture has proven effective in image recognition tasks due to its efficient use of parameters and improved gradient flow. Building on the strengths of CNNs, [5] proposed a CNN-based approach specifically for detecting AI-generated images, demonstrating notable accuracy in distinguishing synthetic content. Generative Adversarial Networks (GANs) have been at the forefront of generating realistic images. The work by delves into the mechanisms of GANs, highlighting how the generator and discriminator networks compete to produce lifelike images. While GANs excel in image synthesis, they also present challenges in detection due to the high quality of generated images. To address the evolving sophistication of generative models, provided a comprehensive review of text-to-image synthesis techniques, including GANs and diffusion models. The paper compares various models, discussing their advantages and limitations, and underscores the need for robust detection methods as generative models continue to improve. Image forgery detection has also been approached through the analysis of compression artifacts. introduced a method combining Error Level Analysis (ELA) and CNNs to identify inconsistencies in image compression levels, effectively detecting manipulated images. This technique leverages the fact that edited regions often exhibit different compression characteristics compared to the rest of the image. In terms of protecting the integrity and ownership of AI-generated images, watermarking techniques have been explored. proposed embedding watermarks into Stable Diffusion Models (SDMs) to assert ownership and safeguard intellectual property. Their method involves fine-tuning the SDM to generate specific watermarks in response to predefined prompts, thereby proving model ownership without compromising performance. Robustness of detection methods under image alterations is another critical aspect. conducted a performance comparison of AI-generated image detection methods, evaluating their resilience to image manipulations such as JPEG compression and Gaussian blurring. They utilized tools like Grad-CAM and t-SNE for visualization, providing insights into the methods' effectiveness under challenging conditions."}, {"title": "2.1. CIFAKE Dataset and Classifier", "content": "Bird and Lotfi introduced the CIFAKE dataset and proposed a Convolutional Neural Network (CNN) to classify images as either real or AI-generated. The classifier processes 32x32 pixel RGB images and outputs a binary decision, with values above 0.5 classified as real. The optimal network architecture comprises two convolutional layers with 32 filters each and two fully connected layers, achieving an accuracy of 92.93% with a binary cross-entropy loss of 0.18. Despite its success, the study did not provide details on key training parameters, such as optimizers and learning rates, leaving room for further exploration.\nThe CIFAKE dataset includes 120,000 images, evenly split between real and AI-generated categories. The real images are sourced from CIFAR-10, spanning 10 categories such as airplanes and cats. The AI-generated images were created using Stable Diffusion 1.4 with prompts like \u201cA photograph of [object],\u201d along with category-specific modifiers, and resized to 32x32 pixels for consistency. This dataset has been instrumental in evaluating the performance of detection models under controlled conditions.\nThis paper builds upon CIFAKE by replicating its methodology and further evaluating the effectiveness and vulnerabilities of the proposed CNN-based classifier. Specifically, this research examines its robustness against variations in image generation, such as newer versions of Stable Diffusion, Gaussian blurring, and modifications using Low-Rank Adaptation (LoRA). The CIFAKE study provides the foundational framework and motivation for this work, enabling a deeper investigation into the resilience and limitations of AI-generated image detectors."}, {"title": "3. Methods", "content": "To evaluate and enhance the effectiveness of AI-generated image classifiers, this study first focused on generating a diverse set of datasets to comprehensively test the robustness of classifiers under various conditions. In real-world application scenarios, AI-generated image classifier services typically have no control over the methods used to generate the provided images. To simulate such unconstrained scenarios, we generated several datasets by introducing diverse alternations to the original Stable Diffusion image generation methods.\nFor datasets representing outputs from advanced AI models, newer versions of Stable Diffusion were utilized, reflecting the evolution of AI generation capabilities over time. To mimic real-world image degradation, Gaussian blurring was applied to artificially introduce imperfections such as those caused by camera focus or resolution issues. To simulate scenarios where bad actors use fine-tuned Stable Diffusion models, datasets were generated with Stable Diffusion fine-tuned via Low-Rank Adaptation (LoRA) [7], allowing for the creation of highly photorealistic images with reduced detectable artifacts. Additionally, recognizing that the original CIFAKE dataset was limited by a fixed set of prompts, we introduced datasets generated using a broader and more diverse set of prompts created by large language models. This allowed us to evaluate vulnerabilities related to overfitting on fixed prompt templates and expose potential weaknesses when classifiers encounter unseen prompt variations.\nThese varied dataset generation approaches ensured a comprehensive evaluation of the classifiers' robustness and their ability to adapt to challenging scenarios and diverse inputs.\nBuilding on the foundation provided by the Nottingham-Trent CNN-based classifier, we also explored ways to improve its performance and robustness. A DenseNet121 architecture [8] was adapted for this task, leveraging its densely connected layers to enhance gradient flow, improve feature reuse, and reduce the number of trainable parameters compared to traditional CNNs of similar depth. DenseNet's architecture is particularly suited for image classification tasks where capturing fine-grained patterns and preserving information across layers are critical.\nTo accommodate the computational constraints of this study and the dataset characteristics, the DenseNet model was modified to process 32x32 pixel images, similar to the input size used in the CIFAKE dataset, and was tailored for binary classification tasks. The modified architecture retained DenseNet's core strengths while adapting to the dataset's size and format.\nThe general approach involved training and testing both CNN and DenseNet models on the original CIFAKE dataset and the newly generated alternated datasets. This two-pronged strategy-first creating challenging datasets to test the classifiers' resilience and then introducing an advanced neural network architecture-enabled a thorough evaluation of the original classifier and allowed for the proposal"}, {"title": "4. Experiments", "content": "To thoroughly evaluate the robustness and effectiveness of the classifiers, we conducted a series of experiments across multiple datasets, including the original CIFAKE dataset and its alternated versions generated with diverse approaches. \nTo extend the evaluation, two additional datasets were created using newer versions of Stable Diffusion: CIFAKE-SD2.1 and CIFAKE-SD3.0. These datasets maintained the same structure as CIFAKE but used Stable Diffusion 2.1 [2] and 3.0 [3] to generate the synthetic images. Additional datasets were generated to test specific vulnerabilities, including: CIFAKE-SD2.1-Blurred: Gaussian-blurred images to obscure detection-relevant patterns; CIFAKE-SD2.1-P2 and CIFAKE-SD2.1-P3: Images generated with slightly altered prompts to test sensitivity to prompt variations; CIFAKE-SD2.1-GPT40: Images generated using highly specific prompts created by GPT-4; CIFAKE-SD2.1-Negative: Images generated with negative prompts to suppress visual artifacts; and CIFAKE-SD2.1-LoRA: Images generated using Stable Diffusion fine-tuned with Low-Rank Adaptation (LoRA) for enhanced photorealism. A completed list of datasets can be found in Table 1."}, {"title": "4.2. Results", "content": "The results of this study are presented to evaluate the performance, robustness, and limitations of AI-generated image classifiers under various experimental conditions, including modifications to datasets and model architectures.\nReplicating CIFAKE Method. In order to establish a reliable baseline for subsequent experiments, this paper evaluated the CNN model's accuracy and binary cross-entropy loss across different training durations (5, 10, and 15 epochs) while keeping all other variables constant. As shown in Table 2, the model trained for 15 epochs achieved an accuracy of 93.67% and a binary cross-entropy loss of 0.1706, closely matching the 92.93% accuracy and 0.18 loss reported in the original study. This alignment validates the robustness of the original approach. Consequently, all subsequent experiments in this paper were conducted using models trained for 15 epochs. Notably, despite the reduced resolution of 32\u00d732 pixels used for classification, the classifier successfully distinguished between real and Stable Diffusion-generated images, which remain indistinguishable to human vision."}, {"title": "Evaluation Across Stable Diffusion Versions", "content": "The original CIFAKE dataset's AI-generated component was created using Stable Diffusion 1.4 (Diffusers, trained by CompVis ). Since the publication of the study, newer versions of Stable Diffusion, specifically 2.1 and 3.0, have been released, offering enhanced image generation capabilities. To evaluate whether the CNN model trained on Stable Diffusion 1.4 would perform less accurately on images generated by these newer versions, we generated two additional datasets: CIFAKE-SD2.1 and CIFAKE-SD3.0.\nBoth CIFAKE-SD2.1 and CIFAKE-SD3.0 followed the structure of the original CIFAKE dataset, comprising 60,000 real images from the CIFAR10 dataset and 60,000 AI-generated images. The primary difference lay in the Stable Diffusion version used for generating synthetic images, which incorporated models developed by Stability AI through the Diffusers library. While Stable Diffusion 1.4 produces images of size 512\u00d7512 pixels by default, the newer versions-2.1 and 3.0-generate images with higher resolutions of 768\u00d7768 and 1024\u00d71024 pixels, respectively. For consistency, all synthetic images in CIFAKE-SD2.1 and CIFAKE-SD3.0 were resized to 512\u00d7512 pixels before downscaling to 32x32 for training and evaluation.\nTo test the impact of version differences, we trained three CNN models on CIFAKE, CIFAKE-SD2.1, and CIFAKE-SD3.0, and then evaluated their performance on all three"}, {"title": "Evaluation on Gaussian Blur", "content": "To explore if the CNN model relies on specific version-dependent patterns of Stable Diffusion-generated images, this paper tested its performance on a modified dataset where these patterns were obscured through Gaussian blurring. This experiment aimed to simulate real-world imperfections such as focus issues or resolution degradation, which are commonly introduced by adversarial or accidental manipulations. The resulting dataset, CIFAKE-SD2.1-Blurred, retains the structure of CIFAKE-SD2.1 but applies Gaussian blur to all images.\nGaussian blurring was chosen for its ability to degrade image quality smoothly and realistically, mimicking conditions that make it harder for both humans and models to distinguish between real and AI-generated images. The blur was applied with a radius of 5 pixels and a standard deviation (\u03c3) of 1.1, calculated using OpenCV's formula:\n$\\sigma = 0.3 (\\frac{kernel\\_size - 1}{2} - 1)^{-1}$", "latex": ["\\sigma = 0.3 (\\frac{kernel\\_size - 1}{2} - 1)^{-1}"]}, {"title": "Evaluation on Image Size Sensitivity", "content": "The default image sizes generated by Stable Diffusion versions differ, with Stable Diffusion 2.1 producing 768\u00d7768 pixels and Stable Diffusion 3.0 generating 1024\u00d71024 pixels. In previous experiments, images were standardized to 512\u00d7512 pixels to align with the default size of Stable Diffusion 1.4 and to isolate the Stable Diffusion version as the primary variable. However, this standardization might obscure potential model sensitivities to variations in image size.\nTo evaluate the impact of image size on classification accuracy, additional datasets\u2014CIFAKE-SD2.1-768 and CIFAKE-SD3.0-1024-were generated. CIFAKE-SD2.1-768 contains images generated using Stable Diffusion 2.1 at its default resolution of 768\u00d7768 pixels, while CIFAKE-SD3.0-1024 includes images generated by Stable Diffusion 3.0 at 1024x1024 pixels. The CNN model trained on CIFAKE-SD2.1 was tested on CIFAKE-SD2.1-768, and the model trained on CIFAKE-SD3.0 was tested on CIFAKE-SD3.0-1024."}, {"title": "Evaluation on Prompt Variability", "content": "The CIFAKE dataset relies on a fixed and limited set of prompts to generate its AI-generated image component, specifically in the format \"A photograph of a/an ...\". This approach may fail to represent the diverse and flexible ways prompts can be structured in real-world applications. Bad actors are likely to manipulate prompts to produce images that are more challenging to detect. To address this limitation, a series of experiments was conducted using datasets generated with varied and more specific prompts to evaluate the robustness of the CNN model.\nTo test the effects of slight variations in prompts, two additional datasets were created: CIFAKE-SD2.1-P2, using the prompt \"A photo of ..., real,\" and CIFAKE-SD2.1-P3, using the prompt \"Realistic photo of ...\". These datasets maintained the same structure and resolution (512\u00d7512 pixels) as CIFAKE-SD2.1.\nTo simulate real-world scenarios where prompts may be more detailed, the CIFAKE-SD2.1-GPT4o dataset was created. This dataset used 125 unique prompts per category, generated using OpenAI's GPT-40, resulting in a total of 60,000 AI-generated images. Prompts included detailed scenarios like \"A plane flying low over a beach with sunbathers watching\" or \"A commercial airplane parked at an airport gate at night.\"\nAnother potential adversarial tactic involves using negative prompts to refine AI-generated images, avoiding characteristics that might make them detectable as fake. To test this, the CIFAKE-SD2.1-Negative dataset was created by adding negative prompts to avoid traits such as \"blurry, distorted, low quality, surreal, or cartoonish.\""}, {"title": "Evaluation on Low-Rank Adaptation (LoRA) Alternation", "content": "Stable Diffusion can produce a wide range of images, from illustrations and art to highly photorealistic imagery. One method to tune Stable Diffusion for specific styles or themes is Low-Rank Adaptation (LoRA), a fine-tuning technique that enables targeted modifications without requiring extensive computational resources. LORA modifies the pre-trained model's architecture by introducing low-rank matrices, which capture task-specific features while preserving the broader generative capabilities of the original model . This technique is particularly beneficial for adapting Stable Diffusion to generate highly photorealistic images, potentially avoiding detectable \"fingerprinting\" patterns.\nTo evaluate the impact of LoRA tuning on Stable Diffusion and the CNN model's performance, we used the MIT-Adobe FiveK dataset to train a photorealism-oriented LORA. The dataset comprises 5,000 images labeled with detailed textual descriptions excluding general quality characteristics, such as lighting conditions."}, {"title": "Using DenseNets to Detect Stable Diffusion-Generated Images", "content": "DenseNet (Dense Convolutional Network) offers an advanced deep learning architecture that enhances feature reuse and gradient flow, making it especially effective for image processing tasks. Unlike traditional convolutional neural networks (CNNs), DenseNet employs dense connectivity, connecting each layer to all subsequent layers. This approach ensures that early layers' features, such as edges and textures, are readily accessible to later layers, enabling better decision-making and reducing redundancy.\nDenseNet consists of dense blocks and transition layers. Each dense block contains several layers with outputs concatenated to all preceding layers, allowing the model to retain detailed representations. Transition layers compress accumulated features using convolution and pooling, balancing computational efficiency with detailed feature retention. For this study, the DenseNet121 architecture was selected and modified to process 32\u00d732 pixel images and output a single binary classification value. Adjustments included reducing the kernel size, stride, and padding in the first convolution layer to match the input size and modifying the final layer to output a single classification value."}, {"title": "5. Discussion", "content": "The findings highlight both the strengths and vulnerabilities of current AI-generated image classification methods, offering insights into their practical applications and areas for improvement in handling diverse and evolving generative techniques.\nStable Diffusion Version Overfitting. From these results, three key observations emerge. First, models trained on a dataset generated with a specific version of Stable Diffusion performed poorly when tested on datasets generated with different versions, highlighting a lack of generalization. Second, models struggled significantly to identify images generated by older versions of Stable Diffusion, with substantial accuracy drops when tested on earlier datasets. Finally, models trained on newer versions (e.g., CIFAKE-SD3.0) performed better in identifying images generated by their respective versions compared to those trained on older versions.\nInterestingly, while images from newer versions of Stable Diffusion appear more realistic to humans, the CNN model demonstrated better accuracy in detecting these images compared to older versions. This suggests that newer Stable Diffusion models introduce more distinct patterns or \"fingerprints,\" which the CNN classifier can more effectively learn. However, the sensitivity of CNN models to version-specific patterns underscores a significant limitation: their reliance on consistent generative models and their vulnerability to evolving AI generation techniques. This observation emphasizes the need for classifiers that generalize well across varying generative methods.\nGaussian Blur. From Table 4, the CNN model's overall accuracy drops significantly from 95.23% to 71.13% when tested on blurred images. More notably, Table 4(b) reveals that the fake image accuracy plummets to 49.90%, which is effectively random guessing for a binary classification problem. These results confirm that Gaussian blurring disrupts the patterns or \"fingerprints\" the CNN model relies upon for detection.\nThis experiment highlights the vulnerability of CNN-based classifiers to simple image modifications like Gaussian blurring. Such manipulations can significantly impair the classifier's ability to identify AI-generated images, underlining the need for more robust detection models capable of adapting to diverse real-world scenarios.\nImage Size Sensitivity. From Table 5, it is evident that image size has a moderate effect on model accuracy. The CNN model trained on CIFAKE-SD2.1 showed a relatively minor accuracy drop of 1.33 percentage points when tested on CIFAKE-SD2.1-768. In contrast, the CNN model trained on CIFAKE-SD3.0 exhibited a more significant decrease of 8.08 percentage points when tested on CIFAKE-SD3.0-1024.\nThis discrepancy suggests that larger differences in image resolution between training and testing datasets may disproportionately affect model performance. The model trained on CIFAKE-SD3.0 may be more impacted due to the greater disparity between the training (512\u00d7512) and testing (1024x1024) image sizes. These findings highlight the importance of considering image resolution consistency in training and evaluation pipelines for AI-generated image classifiers.\nPrompt Variability. The results in Table 6 indicate that minor modifications to the prompt have little to no effect on the model's accuracy. This suggests that the CNN model"}, {"title": "Low-Rank Adaptation (LoRA) and Stable Diffusion", "content": "The results demonstrate that LoRA can effectively tune Stable Diffusion to produce images that challenge the CNN model's ability to distinguish real from AI-generated images. The significant drop in accuracy suggests that LoRA introduces modifications that reduce detectable \"fingerprinting,\" increasing the photorealistic quality of generated images. These findings emphasize the importance of incorporating LoRA-tuned datasets into classifier training to enhance robustness against advanced AI image generation techniques."}, {"title": "Using DenseNets", "content": "The DenseNet model demonstrated robust performance, outperforming the CNN model across all evaluation scenarios. DenseNet consistently achieved higher accuracy compared to the CNN model, both for real and AI-generated images.\nDenseNet's performance was particularly notable for blurred images, a challenging scenario where patterns indicative of Stable Diffusion may be obscured. When tested on the CIFAKE-SD2.1-Blurred dataset, DenseNet demonstrated a significant advantage over CNN, achieving a 15.75% higher overall accuracy and a 23.29% higher accuracy in detecting AI-generated images. This resilience to Gaussian blurring underscores DenseNet's capacity to maintain strong performance even under conditions where image clarity is compromised.\nDenseNet's architecture, with its dense connectivity and efficient feature reuse, consistently outperformed traditional CNN models across all scenarios. Its ability to excel on blurred datasets highlights its potential as a robust model for detecting AI-generated images, even in challenging real-world conditions."}, {"title": "6. Conclusion", "content": "The application of Convolutional Neural Networks (CNNs) has proven effective in distinguishing images generated by Stable Diffusion from authentic photographs. Notably, the performance of CNN-based detectors remains largely invariant to variations in image size, input prompts, and negative prompts provided to the Stable Diffusion model. However, these detectors exhibit significant sensitivity to the specific version of Stable Diffusion employed. Additionally, adversarial techniques, such as the application of Gaussian blurring or the use of Low-Rank Adaptation (LoRA), pose challenges to the accurate detection of AI-generated images. To mitigate these vulnerabilities, the adoption of DenseNet architectures has shown promise. DenseNet demonstrates improved robustness against Gaussian blurring and achieves superior overall performance compared to CNNs in the detection of Stable Diffusion-generated images. This highlights its potential as a more reliable framework for addressing adversarial modifications in AI-generated content."}]}