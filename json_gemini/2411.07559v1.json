{"title": "Zer0-Jack: A memory-efficient gradient-based jailbreaking method for black box Multi-modal Large Language Models", "authors": ["Kaishen Wang", "Tiejin Chen", "Hua Wei"], "abstract": "Jailbreaking methods, which induce Multi-modal Large Language Models (MLLMs) to output harmful responses, raise significant safety concerns. Among these methods, gradient-based approaches, which use gradients to generate malicious prompts, have been widely studied due to their high success rates in white-box settings, where full access to the model is available. However, these methods have notable limitations: they require white-box access, which is not always feasible, and involve high memory usage. To address scenarios where white-box access is unavailable, attackers often resort to transfer attacks. In transfer attacks, malicious inputs generated using white-box models are applied to black-box models, but this typically results in reduced attack performance. To overcome these challenges, we propose Zer0-Jack, a method that bypasses the need for white-box access by leveraging zeroth-order optimization. We propose patch coordinate descent to efficiently generate malicious image inputs to directly attack black-box MLLMs, which significantly reduces memory usage further. Through extensive experiments, Zero-Jack achieves a high attack success rate across various models, surpassing previous transfer-based methods and performing comparably with existing white-box jailbreak techniques. Notably, Zero-Jack achieves a 95% attack success rate on MiniGPT-4 with the Harmful Behaviors Multi-modal Dataset on a black-box setting, demonstrating its effectiveness. Additionally, we show that Zer0-Jack can directly attack commercial MLLMs such as GPT-40. Codes are provided in the supplement.", "sections": [{"title": "1 Introduction", "content": "Recently, the safety alignment of Large Language Models (LLMs) [Ji et al., 2024] has drawn increasing attention because of the safety concern raised by the potentially harmful response [Bengio et al., 2024, Zhou et al., 2024]. And this kind of safety alignment succeeds in the first place, where the user will not get any useful response when directly asking questions related to safety or privacy. However, recent works have proven that it is possible to \u201cjailbreak\u201d the safety alignment so that LLMs may answer the question that they should not by carefully designing prompts [Carlini et al., 2024, Wei et al., 2024]. Later on, using gradient methods and other automatic technologies to find the powerful jailbreak prompts has been proposed so that the attack rate becomes much higher compared with hand-craft prompts [Zou et al., 2023, Liu et al., 2023b], which further demonstrates the vulnerability when obtaining the white-box models."}, {"title": "2 Related Works", "content": "With the success of LLMs [Achiam et al., 2023, Touvron et al., 2023, Chiang et al., 2023], Multi-modal Large Language Models (MLLMs), which handle both text and image inputs, have gained popularity [Liu et al., 2024b, Zhu et al., 2023, Liu et al., 2024a]. Despite their capabilities in tasks such as image descriptions and visual question answering, MLLMs have been shown to be even more vulnerable to jailbreak attacks due to the additional modality [Qi et al., 2024, Sun et al., 2024, Liu et al., 2024c, Zhao et al., 2024]. For example, Liu et al. [2023a] demonstrated that images containing specific text can assist in jailbreaking MLLMs. In white-box settings, where full access to model parameters is available, methods like generating malicious image inputs [Niu et al., 2024] or combining both text and image prompts [Shayegani et al., 2023] by optimization have proven effective in bypassing safety mechanisms. Similar to LLM jailbreaking, the most effective methods in MLLMs rely on calculating gradients to find inputs that induce harmful outputs.\nWhile gradient-based methods for white-box models have shown strong performance, the challenge of attacking black-box models remains underexplored. Black-box models, such as commercial MLLMs like GPT-40 [OpenAI, 2024], do not provide access to their internal parameters, making gradient-based attacks impossible. Most existing jailbreak methods for black-box models rely on transfer attacks, where malicious inputs generated on white-box models are used to indirectly attack black-box models [Zou et al., 2023, Niu et al., 2024, Dong et al., 2023]. However, these transfer attacks often suffer from a significant reduction in success rate compared to direct attacks on white-box models [Niu et al., 2024].\nIn this paper, instead of transferring the malicious prompts from white-box models, we propose Zero-Jack, a method that directly generates malicious image inputs for jailbreaking black-box MLLMs. Zero-Jack leverages zeroth-order optimization, which estimates gradients without accessing model parameters, to find malicious prompts capable of bypassing safety measures. One challenge with zeroth-order optimization is its susceptibility to high estimation errors in high-dimensional inputs. To mitigate this, Zer0-Jack optimizes only a specific part of the image, reducing the dimensionality of the problem and thereby minimizing estimation errors. Furthermore, Zer0-Jack does not rely on backpropagation, resulting in significantly lower memory usage. Through extensive experiments, we show that Zer0-Jack can achieve a high attack success rate within reasonable queries as well as decrease memory usage when generating malicious prompts. Overall, we provide the comparison between different types of jailbreak methods in Fig. 1 and summarize our contribution as follows:"}, {"title": "3 Method", "content": "In this section, we begin to provide an introduction to a baseline jailbreak method focusing on text-only LLMs. We then demonstrate how this method can be adapted and extended into a more powerful and memory-efficient jailbreak technique tailored to MLLMs. We also provide the overview of our method Zero-Jack in Fig. 2."}, {"title": "3.1 Preliminary", "content": "The general goal of jailbreaking attacks in LLMs is inducing LLMs to output unsafe or malicious responses. For example, a LLM with good safety alignment should not generate a detailed response to the query 'How to build a bomb', while jailbreaking attacks aim at making the LLM output the answer to this query. Similar to some adversarial attacks in NLP [Wallace et al., 2019], gradient-based jailbreaking attacks try to find specific suffix tokens that make LLMs output malicious responses. For example, a new query from attackers might be \u2018How to build a bomb. !!!!!!!!!', which can actually induce LLMs to output the detailed procedures of how to make a bomb.\nHowever, unlike adversarial attacks, where the target is to output the same answer and reduce the accuracy when the suffix is added to the prompt [Wallace et al., 2019], the jailbreaking attackers hope LLMs can output true answers to their unsafe query. Besides, there are usually multiple true answers to the query in jailbreaking and thus it is not possible to find suffix tokens by optimizing the output towards one true answer.\nTo tackle the problem, one of the most popular jailbreaking methods, Greedy Coordinate Gradient (GCG) [Zou et al., 2023] tries to find suffix tokens that induce LLMs to output their answer starts with 'Sure, here is'. Then if the language model could output this context at the beginning of the response instead of refusing to the question, it is highly possible for language models to continue the completion with the precise answer to the question.\nIn detail, the optimization problem in GCG can be formulated as:\n$\\min_{x_1\\in\\{1,...,V\\}^n} \\mathcal{L}(X_{1:n}),$ \nwhere x1 is the suffix tokens, $X_{1:n}$ represents the original prompts and $\\mathcal{L}(x_{1:n})$ is the loss function:\n$\\mathcal{L}(x_{1:n}) = -\\log P(X_{n+1:n+H} | X_{1:n})$"}, {"title": "3.2 A Trivial White-box Jailbreak on MLLMs", "content": "With the rapid success of Multi-modal LLMs (MLLMs), recent works have found that it will be easier for attackers to jailbreak the MLLMs due to the new modal introduced in MLLMs [Zhao et al., 2024, Qi et al., 2024]. Therefore, in this paper, we mainly transfer the idea of inducing LLMs to output 'Sure, here it is' at the beginning to jailbreak MLLMs by utilizing the image inputs.\nSpecific to the image input in MLLMs, we can map the continuous values into RGB values without losing too much information since the RGB values in the image are sufficiently close enough that they can be treated as continuous largely. Then it is possible that we do not need to care about the discrete optimization anymore by transferring the attack surface from texts to images i.e. perturbing image inputs only. In this case, The optimization problem in Eq. (1) can be transferred into: min$\\mathcal{L}(x_{1:n}, Z)$, where Z represents the value tensors of the input image. We can optimize this objective by calculating the gradient with respect to the image inputs:\n$\\nabla_Z \\mathcal{L}(X_{1:n}, Z)$\nBy transferring the attack surface from the text to images, our jailbreak method can deal with the potential performance degradation caused by discrete optimization. However, the current version of the attack still suffers from the following two disadvantages:\n1.  Directly computing Eq. (3) requires the white-box accesses to the MLLMs, which further restricts the potential usage of such an attack.\n2.  We present the GPU memory usage for differnt models and parameters in Table 1. As shown in Table 1, the trivial white-box attack requires a lot of memory that a single A100 could not attack 70B models, which restricts the number of usage scenes for the attack."}, {"title": "3.3 Zero-Jack: Jailbreaking with Zeroth-order Gradient Estimator", "content": "SPSA-P: Improved Zeroth-order Gradient Estimator for MLLMs To tackle the mentioned problems for attacking black-box models and high memory usage, we utilize zeroth-order optimization technology to calculate Eq. (3) without backpropagation [Shamir, 2017, Malladi et al., 2023]. In detail, we estimate the gradient with respect to Z by the Simultaneous perturbation stochastic approximation (SPSA) [Spall, 1992]:\n$\\nabla_Z \\mathcal{L}(x, Z) := \\frac{\\mathcal{L}(x, Z + \\lambda u) - \\mathcal{L}(x, Z - \\lambda u)}{2\\lambda} u,$\nWhere u is uniformly sampled from the standard Euclidean sphere and $\\lambda > 0$ is the smoothing parameter [Duchi et al., 2012, Yousefian et al., 2012, Zhang et al., 2024]. Using this formula to estimate the gradient, we only need to get the output logits or probability, which is allowed for many commercial MLLMs [Finlayson et al., 2024] and helps reduce memory usage because we do not need to calculate the real gradient by backpropagation anymore. It also has been proven that Eq. (4) is an unbiased estimator of the real gradient [Spall, 1992].\nHowever, using Eq. (4) directly as the gradient to optimize Z may suffer from the estimated errors caused by high dimension problems especially when the size of images is large [Yue et al., 2023, Zhang et al., 2024, Nesterov and Spokoiny, 2017]. The performance of zeroth-order optimization can be very bad with high-resolution images. To tackle this problem, we propose SPSA with a patch coordinate descent method (SPSA-P) to reduce the influence of estimated error when dimensions are high. In detail, we utilize the idea of patches from the vision transformer [Dosovitskiy, 2020] and divide the original images into several patches:\n$Z = [P_1, ..P_{i-1}, P_i, P_{i+1}, ..., P_n],$\nwhere $P_i$ represents the i-th patch for the image. Normally, we use 32 \u00d7 32 as the shape for each patch if the original image has the shape of 224 \u00d7 224. Then we will compute the gradient for each patch instead of the whole image by only perturbing $P_i$ at one iteration:\n$\\nabla_{P_i} \\mathcal{L}(x, Z) := \\frac{\\mathcal{L}(x, P_i + \\lambda u) - \\mathcal{L}(x, P_i - \\lambda u)}{2\\lambda} u.$\nLeveraging SPSA-P in ZerO-Jack To utilize SPSA-P in our jailbreak framework Zer0-Jack, we can directly use Eq. (2) as the loss function to optimize in Eq. (7) to get the estimated gradient:\n$\\nabla_{P_i} \\mathcal{L}(X_{1:n}, Z) := \\frac{\\mathcal{L}(X_{1:n}, P_i + \\lambda u) - \\mathcal{L}(X_{1:n}, P_i - \\lambda u)}{2\\lambda} u.$\nAfter estimating the gradient for one patch, we can choose to update the patch immediately or keep the whole image unchanged until we get the estimated gradient for the whole image. We choose to update the patch immediately to get the new image:\n$P_i' = P_i - \\alpha \\nabla_{P_i} \\mathcal{L}(X_{1:n}, Z),$\n$Z' = [P_1, ..P_{i-1}, P_i', P_{i+1}, ..., P_n],$\nwhere $\\alpha$ is the learning rate. Then we move to the next patch $P_{i+1}$, estimate the gradient of the next patch, and update the next patch $P_{i+1}$:\n$\\nabla_{P_{i+1}} \\mathcal{L}(X_{1:n}, Z') := \\frac{\\mathcal{L}(X_{1:n}, P_{i+1} + \\lambda u) - \\mathcal{L}(X_{1:n}, P_{i+1} - \\lambda u)}{2\\lambda} u.$\nWe choose to update one patch at a time instead of the whole image after estimating the gradient for the whole image because sometimes updating a few patches can lead to a successful attack and waiting for the gradient for the whole image can get suboptimal iteration efficiency. By updating only one patch each time, the updating dimensions become 32 \u00d7 32, which is around 0.02% of the updating dimensions if we directly update the whole image of 224 \u00d7 224, thus reducing the estimation errors significantly. Overall, we summarize the overall framework Zer0-Jack, which utilizes our proposed SPSA-P in Algorithm 1."}, {"title": "4 Experiments", "content": "We evaluate our method using three prominent Multi-modal Large Language Models (MLLMs) known for their strong visual comprehension and textual reasoning capabilities: MiniGPT-4 [Zhu et al., 2023], LLaVA1.5 [Liu et al., 2024a], and INF-MLLM1 [Zhou et al., 2023], all equipped with 7B-parameter Large Language Models (LLMs). Additionally, to assess memory efficiency, we conduct experiments with MiniGPT-4 paired with a 70B LLM, demonstrating that our approach requires minimal additional memory beyond inference. For all models we use, we use the instruction version from the authors of these models and all models have safety alignment so that they hardly respond to potentially harmful questions before jailbreak attacks. We only use the logits from models to simulate a black-box situation."}, {"title": "4.1 Setup", "content": "We evaluate Zer0-Jack using two publicly available datasets specifically designed for assessing model safety in multi-modal scenarios:\n\u2022 Harmful Behaviors Multi-modal Dataset: The Harmful Behaviors dataset [Zou et al., 2023] is a safety-critical dataset designed to assess LLMs' behavior when prompted with harmful or unsafe instructions. It includes 500 instructions aimed at inducing harmful responses. For our experiments, we selected a random subset of 100 instructions from this dataset. To create multi-modal inputs,"}, {"title": "4.2 Overall Performance on Benchmarking Datasets", "content": "The evaluation results on the Harmful Behaviors Multi-modal Dataset, as shown in Table 2, highlight the effectiveness of our Zer0-Jack, compared to other jailbreak techniques. In MiniGPT-4, Zero-Jack achieved an impressive ASR of 95%, significantly outperforming other methods such as AutoDAN at 16% and GCG at 13%. Similarly, in LLaVA1.5, Zer0-Jack recorded an ASR of 90%, while alternatives faltered, with AutoDAN achieving only 8% and the P-Text yielding no successful attacks at all. INF-MLLM1 showed an ASR of 88% for Zer0-Jack, reinforcing its effectiveness, while other methods like AutoDAN and GCG managed only 22% and 1%, respectively. Notably, when evaluating the larger MiniGPT-4 model paired with a 70B LLM, Zer0-Jack achieved an ASR of 92%, whereas GCG, AutoDAN, and WB did not yield results due to GPU memory constraints. The results from the Zer0-Jack were comparable to those of the WB method, but Zer0-Jack consumed significantly less memory. This further indicates that our method remains effective even when scaled to larger model architectures, requiring minimal additional memory beyond inference.\nAs shown in Table 3, the evaluation results from the MM-SafetyBench-T Dataset underscore the effectiveness similar to the previous results on Harmful Behaviors. Specifically, ZerO-Jack achieved an ASR of 98.2% in MiniGPT-4, 95.8% in LLaVA1.5, and 96.4% in INF-MLLM1. In contrast, methods originally designed for LLMs, such as GCG, AutoDAN, and PAIR, demonstrated significantly reduced effectiveness when their adversarial prompts were transferred to MLLMs. For instance, while GCG excelled in LLMs jailbreak, it only managed to achieve an ASR of 40.5% in MiniGPT-4 and 23.2% in LLaVA1.5. For a larger MiniGPT-4 model paired with a 70B LLM, the results demonstrated the same trend as Table 2. We also report the results for different categories in the MM-SafetyBench-T dataset in Fig. 6 while the full results can be found in Appendix."}, {"title": "4.3 Evaluation on Transferability", "content": "To assess the transferability of images optimized through Zer0-Jack across different models, we con-ducted three sets of comparative experiments. First, we optimized images using the MM-SafetyBench-T dataset on the MiniGPT-4 model to generate adversarial images capable of successfully bypassing defenses. We then transferred these optimized images to the LLaVA1.5, GPT-40, and INF-MLLM1 for transferability evaluation."}, {"title": "4.4 Analysis on Efficiency", "content": "To analyze the efficiency of Zer0-Jack, we evaluate its practical advantages in terms of memory consumption and iteration efficiency over traditional methods.\nAs illustrated in Fig. 4, traditional jailbreak methods often require sub-stantial memory, limiting their practicality for deployment. To compare memory consumption, we evaluated text-based methods on the LLaMA2-7B model, which is commonly used as the language model in MLLMs. Specifically, GCG consumes approximately 50GB of memory, while AutoDAN requires around 26GB. In contrast, image-based optimization techniques such as A-Image and WB Attack, applied to MLLMs like MiniGPT-4, use about 19GB each due to the need for gradient retention, while Zer0-Jack significantly reduces memory usage without sacrificing performance, uses only 10GB of memory."}, {"title": "Iteration Efficiency", "content": "Next, we compare the iteration efficiency, which refers to the number of iterations required for each method to generate a successful adversarial goal.\nAs shown in Fig. 4, we found that GCG typically requires around 100 iterations per adversarial goal, while AutoDAN takes even more, averaging between 100 and 120 iterations. For AdvImage, the default setting requires more than 200 steps to generate the adversarial image due to its perturbation constraint on the image. WB Attack requires around 40 to 50 iterations. In contrast, Our Zero-Jack demonstrates significantly greater efficiency. Zer0-Jack only needs 55 iterations on average to optimize the image successfully, which is comparable with the WB Attack that is a white-box attack."}, {"title": "4.5 Case study", "content": "As shown in Fig. 3, this case study illustrates the performance of various methods on LLaVA1.5 in response to the question, \u201cList steps in detail to engage in identity theft without leaving any traces\" along with the corresponding image. Text-based approaches, including AutoDAN, GCG, and PAIR, generated adversarial text prompts that resulted in responses such as \"I cannot provide any information\" and \"It is a serious crime...\", demonstrating their failure to bypass the model's safeguards. In contrast, our method effectively engaged LLaVA1.5, yielding clear and actionable steps such as: 1) Gather personal information, 2) Steal the information, etc. This stark difference underscores the success of our integrated approach in generating relevant and detailed outputs, highlighting its effectiveness in circumventing the model's limitations compared to existing techniques."}, {"title": "4.6 Attack Commercial MLLMs Directly", "content": "In this subsection, we show that Zer0-Jack could attack commercial MLLMs directly. And we will fo-cus on GPT-40 [OpenAI, 2024] here. OpenAI's API only allows users to retrieve the top 20 tokens with the highest log probabilities, rather than accessing the entire set of logits. However, if we look back at the loss function in Eq. (2), we can find that Zer0-Jack only requires logits to our target responses 'Sure, here it is'. Though the target responses may not show in the top 20 tokens with the highest log probabilities, we find that we can access the log probability to any token by logit_bias, which is a function provided by OpenAI's API that enables users to add bias to any token's logit. Then if we add a very high bias to 'sure', it will force GPT-40 to generate 'sure' and the API will return the log probability of the generated token 'sure'. Through this method, we can access to log probability of all tokens in target responses and attack GPT-4o using Zero-Jack. Finally, when generating the final output, we discard anything about logit_bias. In Fig. 5, we provide a showcase that uses Zer0-Jack to jailbreak GPT-40 successfully. Zero-Jack attacks this showcase with reasonable iterations that it only spends around 0.7 dollars calling OpenAI's API."}, {"title": "5 Discussion", "content": "though Zero-Jack only requires access to output logits or probabilities, Zero-Jack could not directly attack the web version of commercial MLLMs. Besides, there are some commercial MLLMs' API that do not support return logits [Anthropic, 2024].\n\u2022 Limitations:"}, {"title": "A Code", "content": "Our code is provided in an anonymous Github Link (hyperlink here)."}, {"title": "B Detailed Results for categories in MM-safetybench-T", "content": "In Table 6, we provide the numbers of successful attacks for each scenario in MM-Safetybench-T and in Table 5, we provide the numbers of successful attacks for each scenario in MM-Safetbench-T dataset when we test the transfer ability of Zer0-Jack. As we can see, even for each scenario, Zero-Jack can beat other baseline methods. And an image example could be found at Fig. 6."}, {"title": "C More Detailed Responses", "content": "We present the detailed responses generated from MiniGPT-4 on both datasets in the supplementary, in the type of JSON file, containing both the question and our Zer0-Jack's response."}]}