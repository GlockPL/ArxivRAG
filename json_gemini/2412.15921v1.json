{"title": "Less is More: Towards Green Code Large Language Models via Unified Structural Pruning", "authors": ["Guang Yang", "Yu Zhou", "Xiangyu Zhang", "Wei Cheng", "Ke Liu", "Xiang Chen", "Terry Yue Zhuo", "Taolue Chen"], "abstract": "The extensive application of Large Language Models (LLMs) in generative coding tasks has raised concerns due to their high computational demands and energy consumption. Unlike previous structural pruning methods designed for classification models that deal with low-dimensional classification logits, generative Code LLMs produce high-dimensional token logit sequences, making traditional pruning objectives inherently limited. Moreover, existing single-component pruning approaches further constrain the effectiveness when applied to generative Code LLMs. In response, we propose Flab-Pruner, an innovative unified structural pruning method that combines vocabulary, layer, and Feed-Forward Network (FFN) pruning. This approach effectively reduces model parameters while maintaining performance. Additionally, we introduce a customized code instruction data strategy for coding tasks to enhance the performance recovery efficiency of the pruned model. Through extensive evaluations on three state-of-the-art Code LLMs across multiple generative coding tasks, the results demonstrate that Flab-Pruner retains 97% of the original performance after pruning 22% of the parameters and achieves the same or even better performance after post-training. The pruned models exhibit significant improvements in storage, GPU usage, computational efficiency, and environmental impact, while maintaining well robustness. Our research provides a sustainable solution for green software engineering and promotes the efficient deployment of LLMs in real-world generative coding intelligence applications.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have demonstrated outstanding performance and been deployed across numerous domains [75, 24, 74]. Software engineering is no exception [22, 30, 50, 10], with LLMs excelling in tasks like code generation [19], summarization [46], and vulnerability detection [39]. However, the substantial scale and intensive computational requirements of these models pose challenges, particularly in resource-constrained environments [3, 21, 34]. Moreover, the energy consumption associated with training and inference leads to high carbon emissions, raising concerns about environmental sustainability [51, 52].\nTo enhance energy efficiency and sustainability, green software engineering has spurred exploration of model pruning, quantization, and knowledge distillation [35, 79, 63, 61], aimed at reducing the computational and environmental impact of LLMs. Quantization speeds up inference by converting high-precision weights to lower precision. Knowledge distillation transfers knowledge from larger to smaller models, requiring additional computational resources. Among these, model pruning has emerged as a promising strategy, with two main approaches: unstructured pruning that merely zeros out specific weights while maintaining the original parameter count, and structured pruning [62] that effectively reduces model size by removing entire structural components (e.g., neurons, layers) while preserving model integrity and functionality. For instance, unstructured pruning, such as SparseGPT [13], targets individual weights, achieving sparsity but not significantly reducing hardware demands, thus limiting their use in constrained settings. In contrast, structured pruning, such as ShortGPT [44], reduces parameter counts and hardware dependencies, enhancing operational efficiency while maintaining model integrity.\nAlthough these pruning methods can be theoretically applied to code generation models, our extensive experiments (cf. RQ1 in Section 5.1) reveal their significant limitations when applied to generative coding tasks. For example, when applying ShortGPT [44] to the LLAMA model, we observe a complete performance collapse on the HumanEval code generation benchmark. Through careful analysis, we identify two fundamental limitations in existing approaches:\n1.  Misaligned Pruning Objectives: Current pruning methods [44, 72, 29] primarily focus on layer-wise similarity metrics (e.g., angle distance, cosine similarity, and Taylor score between layers), which aim to preserve the model's general language modeling capabilities while overlooking the specific requirements of downstream tasks.\n2.  Limited Pruning Scope: Existing approaches typically adopt single-component pruning strategies (e.g., solely focusing on layer redundancy reduction), failing to leverage the potential synergies that could be achieved through an integrated, multi-granular pruning approach across different model components.\n3.  Lack of Code-specific Post-tuning: Existing pruning methods rely on generic supervised fine-tuning on downstream datasets for performance recovery, without considering domain-specific adaptations for code-related tasks. This generic approach fails to leverage the unique characteristics and requirements of code generation tasks, potentially limiting the effectiveness of the post-pruning recovery process.\nWe provide a detailed analysis of these limitations and their implications in Section 5.1.\nBeyond performance considerations, the reliability and robustness of pruned models in coding tasks raise critical concerns for real-world deployment. These concerns encompass several key aspects: How well do pruned models maintain their robustness against adversarial inputs? Despite the crucial nature of these questions for practical applications, existing research has largely focused on performance metrics while leaving these reliability aspects unexplored. We present a comprehensive analysis of these critical concerns in Section 5.3.\nMethod. To minimize model parameters while ensuring that pruned models maintain high standards in performance and robustness, First, we define the pruning objectives: since the model generates code through probability distributions over the vocabulary, we use KL divergence to ensure that the pruned model maintains similar token generation probabilities as the original model, directly optimizing for code generation behavior. Then we introduce Flab-Pruner, a unified structural pruning method designed for the combination of three components, i.e., FFN Pruning, Layer Pruning and Vocabulary Pruning. In particular, the vocabulary pruning component reduces the model's embedding size by eliminating tokens that are absent in the given programming corpus. The FFN pruning component targets specific neurons within the FFN block, reducing the model's size by eliminating certain neurons. The layer pruning component reduces the number of layers in the model by assessing the redundancy between layers. To consider the consistency between pruning objectives and downstream task performance, the above pruning components are all designed to minimize the KL divergence between the pruned model and the original model.\nAdditionally, we introduce a customized code instruction tuning strategy specifically designed for generative coding tasks. In contrast to directly train the pruned model on the original dataset, we purposefully replace the code generated by the original model into the training dataset by evaluating the performance of the original and pruned models on the training set. Compared to using the original dataset for performance recovery, our approach enhances efficiency by achieving better performance recovery.\nEvaluation. We undertake a comprehensive evaluation of Flab-Pruner spanning three widely studied code intelligence tasks, i.e., code generation, CoT generation and code output prediction. The goal of code generation is to convert requirement in natural language into code, bridging the gap between description and execution. CoT generation is about creating thought sequences from prompts, showcasing advanced reasoning. Code output prediction assesses the model's understanding of code and its ability to predict outcomes. By considering these diverse tasks, our evaluation aims to provide a holistic evaluation of Flab-Pruner's capabilities in various code intelligence domains. Our evaluation"}, {"title": "2. Background", "content": "2.1. Transformer\nA standard LLM, such as the Transformer [58], processes input sequences through several key components, including an embedding layer, multiple self-attention layers, feed-forward networks, and an output layer.\nGiven an input sequence of token indices $X = (X_1,X_2, ..., X_n)$, the LLM first maps these discrete indices into continuous vectors using an embedding layer:\n$E = (e_1, e_2, ..., e_n) \\text{ where } e_i = W_e[x_i]$\nHere, $W_e \\in R^{|V| \\times d}$ is the embedding matrix, $|V|$ is the vocabulary size, and d is the dimension of the embeddings. Each token $x_i$ is mapped to its corresponding embedding vector $e_i$. The embedded sequence E is then processed"}, {"title": "3. Our Method", "content": "The workflow of Flab-Pruner is shown in Figure 1. There are three major pruning steps, i.e., vocab pruning, layer pruning and FFN pruning. For a given sense model, we first perform vocab pruning, then layer pruning, and finally FFN pruning.\n3.1. Pruning Objective\nTo maintain the pruned model's performance, defining an appropriate pruning objective is crucial. In natural language processing, existing pruning methods typically focus on layer-level similarity. For instance, ShortGPT [44] employs angle distance between layers, while Laco [72] utilizes cosine similarity to identify and remove redundant layers.\nHowever, for generative coding tasks, we argue that layer-level similarity alone is insufficient. The key challenge lies in ensuring that the pruned model generates code $Y'$ that remains consistent with the original model's output Y. Note that the model generates code through a probability distribution over the vocabulary, obtained by applying a linear transformation followed by a softmax function to the hidden representations $H^{(L)}$ from the last layer. Therefore, preserving the output probability distribution is critical for maintaining generation quality."}, {"title": "3.2. Vocabulary Pruning", "content": "Vocabulary pruning, typically applied in tasks like text classification [5] and machine translation [45], involves removing these seldom-used tokens, thus reducing the overall model size without significantly affecting its capabilities.\nIn the context of Code LLMs, many models have expanded their vocabularies to accommodate a wide range of programming languages. This expansion often leads to a large vocabulary size as the models strive to cover various syntaxes and language-specific terminologies. However, in practice, developers usually work with a limited set of programming languages, which means that a significant portion of the vocabulary is rarely used. This observation leads to the hypothesis that the absence of these infrequent tokens might not substantially impact the model's ability to capture the nuances of any particular language effectively [73].\nFormalization. Formally, given a vocabulary V and a usage statistic U, the pruned vocabulary $V'$ is defined as:\n$V' = {v \\in V | U(v) > \\tau}$"}, {"title": "3.3. Layer Pruning", "content": "Layer pruning involves the removal of entire layers to reduce its depth and computation, which is theoretically justified by the observed redundancy [7] within deep neural networks and the notable similarity [16] across layers. In NLP, studies [44, 6, 54, 29] reveal that the performance of LLMs remains largely intact even after the removal of many layers, indicating an underlying redundancy that pruning can exploit.\nAlgorithms. Our layer pruning approach diverges from existing methods that typically rely on layer similarity or language modeling capabilities as the primary criterion for pruning. We assert that the primary purpose of pruning should be to ensure that the model continues to deliver satisfactory performance, which we discussed in Section 3.1.\nFurthermore, existing methods [42, 44, 16, 48, 54] primarily focus on one-shot removal strategies, which determine the redundancy of each layer in a single computation pass and select the top_k layers or a contiguous block of k layers for removal. These methods are straightforward but may not account for the dynamic changes in the model's state"}, {"title": "3.4. FFN Pruning", "content": "FFN pruning is based on the observation that not all neurons in transformer models are equally vital [79]. Each layer in Transformer is composed of a GQA module and an FFN module, the GQA's inherent characteristics necessitate a specific ratio between key_value_heads and attention_heads. Pruning any can lead to a significant degradation in model performance. Given this constraint, our focus shifts to the intermediate size of the FFN, which represents the expansion of the model's representational capacity and presents a more flexible target for pruning.\nAlgorithm. To streamline the FFN pruning process and minimize its computational overhead, we have devised a set of four heuristic rules to determine which neurons within the FFN to eliminate:\n\u2022\n\u2022\n\u2022\n\u2022\nFollowing these heuristics, we generate a structured pruning mask that guides the pruning process. Finally, we still choose the best rule through computing the similarity.\nAnalysis. Our approach to FFN pruning leverages heuristic rules, which significantly reduces the computational cost. The time required for this process is merely a matter of minutes, making it a highly efficient method for model optimization."}, {"title": "3.5. Performance Recovery", "content": "After pruning some model parameters, it is crucial to implement effective strategies to recover the model's performance. The following pseudocode describes our proposed Performance Recovery Algorithm.\nAlgorithm 3 presents the pseudocode for the Performance Recovery process. The algorithm aims to improve the performance of the pruned model by leveraging the code generation capabilities of the original model. The process begins by iterating through the provided training dataset D, which consists of samples including input data, expected outputs, and associated test cases (Lines 2-5). For each sample, the algorithm uses the original model Mori to generate code based on the input data. If the generated code passes the corresponding test cases, the expected output in the dataset is replaced with this generated code.\nBy replacing outputs with semantically correct code generated by the original model, the overall quality of the dataset is consistently high, ensuring the training data is always reliable. Furthermore, the pruned model is able to fit the training data more efficiently, reducing loss more quickly and effectively during training. This enhances the convergence speed and helps the pruned model achieve better performance faster.\nFinally, the pruned model Mpru is retrained on the updated dataset. This training aims to equip the pruned model with the necessary knowledge to handle previously challenging scenarios. The outcome is a recovered version of the"}, {"title": "4. Experiment Setup", "content": "To assess the effectiveness of Flab-Pruner, we design the following three research questions (RQs):\nRQ1 (Performance Comparison) This RQ is designed to evaluate the performance of Flab-Pruner compared to dense models and other structured pruning methods. Furthermore, we investigate the impact of three components proposed in Flab-Pruner and conducts a hyperparameter analysis.\nRQ2 (Efficiency Comparison) This RQ is designed to examine the resource deployment efficiency of Flab-Pruner compared to dense models.\nRQ3 (Robustness Analysis) This RQ is designed to assess the robustness of pruned models compared to dense models, particularly under various prompt perturbations.\n4.1. Subject Models\nIn our research, we select three state-of-the-art CodeLLMs, i.e., CodeQwen-1.5 and its two variants (NxCode and CodeSlerp), which are top-performing 7B-parameter models on the HumanEval leaderboard. These models are chosen for their strong performance and all three models share the same transformer-based architecture, making them ideal candidates for our comparative study of pruning techniques.\n4.2. Downstream Tasks\nIn our research, we focus on the generation tasks in code intelligence. We delve into the following three key tasks, each with its unique set of challenges and methodologies:\n\u2022 Code Generation. This task [4] involves the generation of code snippets from a given natural language description and signature, utilizing a zero-shot learning strategy."}, {"title": "5. Experiment Result", "content": "5.1. RQ1: Performance Comparison\nOne of the primary objectives of this study is to evaluate the effectiveness of Flab-Pruner in reducing model size while preserving the performance. To this end, we design a series of experiments to compare the performance of Flab-Pruner against other structured pruning methods as well as the original dense models. We consider five representative pruning methods, including ShortGPT, UIDL, Linearity, SLEB and LLM-pruner as the baselines. Each of these methods has a unique focus and explores different aspects of model pruning. For each baseline, we control it to have a similar number of model parameters as Flab-Pruner after pruning to ensure fairness.\n\u2022 ShortGPT [44] prunes layers guided by the cosine similarity between layer representations.\n\u2022 UIDL [16] prunes layers by evaluating the angular distance between layer representations.\n\u2022 Linearity [48] prunes layers by evaluating the linear relationship of layer representations.\n\u2022 SLEB [54] prunes layers that have the least impact on the model by calculating the perplexity metric.\n\u2022 LLM-pruner [42] prunes layers that have the least impact on the model by utilizing gradients derived from Taylor's formula.\n(1) Compared with single component pruning methods: Our empirical results, shown in Table 2, demonstrate the performance of Flab-Pruner across various tasks. Without post-training, Flab-Pruner achieves the highest performance in the code generation task. Taking HumanEval as an example, the Pass@1 of Flab-Pruner on the CodeQwen model is 75.00%, while the best Pass@1 of other baseline methods is 42.68%, compared to the relative performance improvement of 75.73%. In the CoT generation task, Flab-Pruner also performs better, with a BLEU-4 of 31.42% on the CodeQwen model, while the best BLEU-4 of other baseline methods is 15.92%, compared to the relative performance improvement of 97.98%. For the code output prediction task, Flab-Pruner also outperforms other methods. Taking Crux-O as an example, the EM of Flab-Pruner on the CodeQwen model is 31.75%, while the best EM of other baseline methods is 27.63%, compared to the relative performance improvement of 14.93%.\n(2) Compared with other pruning objectives: As shown in Figure 2, we compare the effect of pruning different number of layers on the model performance and find that the more layers are pruned, the more the model performance decreases. Furthermore, we conduct an in-depth analysis of our proposed pruning objectives in Layer Pruning and find that our pruning objectives can maintain higher performance compared to other objectives proposed by baselines.\nThese performance differences are primarily attributed to the inherent limitations in the layer selection and importance assessment strategies of other methods. SLEB's reliance on perplexity metrics may not fully correspond to code functionality accuracy. ShortGPT, UIDL, and Linearity's isolated layer evaluations fail to capture the collective"}, {"title": "5.2. RQ2: Efficiency Analysis", "content": "In this RQ, we first compare the efficiency of the pruned model to the original model, focusing on key metrics such as GPU usage, speed, CO2 emissions, and FLOPs. These metrics are crucial for practical deployment considerations in"}, {"title": "5.3. RQ3: Robustness Analysis", "content": "To thoroughly evaluate the robustness of the pruned models, we design a comprehensive experimental framework that builds upon the pioneering ReCode work of Wang et al. [60]. Leveraging the code generation task as a critical benchmark, we aim to assess model performance under various perturbed conditions. Four token-level perturbation methods are constructed in HumanEval: format, func_name, natgen, and nlaugenter. Each of these perturbation methods is specifically tailored to probe the model's resilience to specific types of noise, offering a comprehensive perspective on the models' ability to withstand real-world disturbances.\n\u2022 Format. This perturbation method introduces noise in the code format, such as insert the newline or replace space indent with tab.\n\u2022 Func_name. This perturbation method alters function names in the code, including applying character-level or word-level natural text transformations on component words.\n\u2022 Natgen. This perturbation method introduces code syntax noise, such as inserting the deadcode or swaping operand.\n\u2022 Nlaugenter. This perturbation method introduces natural language noise in the docstrings, such as applying SynonymSubstitution or BackTranslation.\nThe empirical results, as shown in Table 6, indicate that the pruned models exhibit a slight decrease in performance under partially token-level perturbed conditions compared to the dense model. The performance of the three pruned models under the four perturbation methods is slightly lower than that of the dense model, with a maximum decrease of less than 10%. However, after post-training, the performance of the pruned models is even better than the dense model under certain perturbations. Taking the CodeQwen model as an example, after post-training, the performance of Flab-Pruner under the Func_Name perturbation is 77.44%, while the performance of the dense model is 75.61%. This indicates that our post-training strategy can enhance the robustness of the model.\nFurthermore, we also conduct experiments using the EvoEval dataset [66]. This dataset includes a variety of semantic-altering operations:\n\u2022 Tool_use. This perturbation method introduces a new problem with a main issue and additional helper functions that can assist in solving it.\n\u2022 Combine. This perturbation method merges two distinct problems by integrating concepts from both."}, {"title": "6. Threats to Validity", "content": "Threats to Internal Validity. The first internal threat involves the potential for implementation errors in Flab-Pruner. We counteract the possibility of errors in Flab-Pruner with comprehensive code reviews and trusted libraries like PyTorch and Transformers. The second internal threat pertains to the accuracy of the implemented baselines. To mitigate this risk, we reproduced all baselines using their shared scripts. Furthermore, to ensure a fair evaluation and uphold the integrity of the model architectures, we only focused on structured pruning methods in our comparative analysis.\nThreats to External Validity. Our primary external validity concern is the representativeness of the datasets used. We selected high-quality datasets to reflect the domain accurately, focusing on Python due to its widespread support in code LLMs. We plan to expand to other languages and levels in future work to improve the generalizability of our results. Furthermore, we considered three state-of-the-art Code LLMs in our study, which may limit the generalizability of our findings to other models. We will expand the assessment to include additional models to improve the external validity of the results.\nThreats to Construct Validity. The main challenge in construct validity is the choice of metrics for automated evaluation. We included diverse metrics such as Pass@1, BLEU and EM to comprehensively assess model performance, providing different perspectives on their capabilities."}, {"title": "7. Related Work", "content": "7.1. Code Intelligence\nThe success of models like BERT [9] in NLP has inspired the creation of pre-trained models for code processing. Models such as CodeBERT [11], GraphCodeBERT [20], and CuBERT [28] are designed for generating informative code embeddings vital for tasks like software defect detection [71]. Building on the success of GPT [12] in NLP, models like CodeGPT [40], trained on datasets like CodeSearchNet [25], have shown promise in coding tasks. Recent advancements in deep learning have led to models like CodeGeeX [77] and CodeLlama [49], excelling in complex programming challenges and demonstrating superior performance.\nCode Generation. The code generation task, where code is produced from natural language, is a hot topic [56, 33, 69, 76], with models trained for specific goals like next-token prediction or the \"filling in the middle\" technique for contextual code completion, as seen in InCoder [15] and StarCoder [32, 38].\nThe field has seen the rise of diverse models, each with unique training approaches and strengths. Notable examples include WizardCoder [41], OpencodeInterpreter [78], and Magicoder [64], all aimed at improving precision and efficiency in code generation challenges."}, {"title": "CoT Generation.", "content": "CoT generation is about crafting logical steps in natural language to reach a code solution, improving output reliability through clear reasoning. Researchers [80] have adapted this for code intelligence, with approaches like Jiang's self-planning [27] and Li's structured CoT [31] to tackle complex coding challenges."}, {"title": "Code Output Prediciton.", "content": "Predicting code output from inputs is a tough test of a language model's comprehension skills [26]. Gu et al. [17] found top models did well in HumanEval but not in output prediction using CRUXEval."}, {"title": "7.2. Model Compression", "content": "Model compression reduces model size, boosts transformer efficiency, and maintains performance through techniques like knowledge refinement, quantization, and pruning.\nKnowledge distillation. Knowledge distillation trains a compact student model to emulate a larger teacher model. In software engineering, Compressor [53] employs task-specific distillation to enhance transformer efficiency with neural architecture search. Yang et al. [70] introduced COTTON to boost lightweight models by transferring reasoning skills from larger models using rules and agent alignment.\nQuantization. Quantization trims neural network precision to optimize memory and efficiency. Wei et al. [63] exam-ined quantized models in code generation, noting performance trade-offs. Xiao et al. [68] introduced SmoothQuant for weight and activation quantization. Gptq [14] utilizes second-order info for quantization, and Qlora [8] backpropagates through a 4-bit model into Low Rank Adapters. Quantization can enhance efficiency but may affect accuracy.\nPruning. Pruning boosts model efficiency by creating sparser or smaller models. Unstructured pruning zeros parameters for sparsity, like SparseGPT [13], treating it as a sparse regression issue, but risking irregular structures. Structured pruning removes components based on criteria, with tools like LLM-Pruner [42] using gradients to cut less critical parts, ShearedLLaMA [67] applying targeted pruning and dynamic loading, and ShortGPT [44] removing whole layers.\nConsidering that existing pruning methods may compromise model performance, we propose Flab-Pruner, a comprehensive pruning approach that maintains model efficiency and performance in code intelligence tasks. Moreover, Flab-Pruner utilizes the LoRA technique for effective post-training, guaranteeing that the pruned models attain performance levels akin to the original dense models."}, {"title": "8. Conclusion and Future Work", "content": "The development and evaluation of Flab-Pruner underscore the viability of structural pruning for LLMs in the generative coding tasks. Our approach has demonstrated that it is possible to significantly reduce the computational footprint of LLMs without compromising their core capabilities. By pruning 22% of parameters, Flab-Pruner retains 97% of the original performance, which further achieves the same or even better performance after post-training. The pruned models also exhibit enhanced efficiency in GPU usage, Flops, CO2 emissions, and token processing speed, aligning with the goals of green software engineering. Moreover, the comprehensive evaluation, including robustness analysis, assures that the pruned models maintain a high standard of performance.\nIn future work, we plan to explore more Code LLMs and evaluate the effectiveness of Flab-Pruner on a broader range of generative coding tasks. We will release more pruned models on the open source platform to facilitate the deployment of pruned models in real-world software engineering scenarios. Moreover, the current work focuses on Python code. We plan to expand Flab-Pruner to support a broader range of programming languages in the future."}, {"title": "CRediT authorship contribution statement", "content": "Guang Yang: Data curation, Methodology, Software, Writing - Original draft preparation. Yu Zhou: Conceptu-alization, Methodology, Writing - Review & Editing. Xiangyu Zhang: Data curation, Writing - Review & Editing. Wei Cheng: Validation, Writing - Review & Editing. Ke Liu: Validation, Writing - Review & Editing. Xiang Chen: Validation, Writing - Review & Editing. Terry Yue Zhuo: Methodology, Writing - Review & Editing. Taolue Chen: Conceptualization, Methodology, Writing - Review & Editing, Validation."}]}