{"title": "UNRAVELING THE COMPLEXITY OF MEMORY IN RL AGENTS: AN APPROACH FOR CLASSIFICATION AND EVALUATION", "authors": ["Egor Cherepanov", "Nikita Kachaev", "Artem Zholus", "Alexey K. Kovalev", "Aleksandr I. Panov"], "abstract": "The incorporation of memory into agents is essential for numerous tasks within the domain of Reinforcement Learning (RL). In particular, memory is paramount for tasks that require the utilization of past information, adaptation to novel environments, and improved sample efficiency. However, the term \u201cmemory\u201d encompasses a wide range of concepts, which, coupled with the lack of a unified methodology for validating an agent's memory, leads to erroneous judgments about agents' memory capabilities and prevents objective comparison with other memory-enhanced agents. This paper aims to streamline the concept of memory in RL by providing practical precise definitions of agent memory types, such as long-term versus short-term memory and declarative versus procedural memory, inspired by cognitive science. Using these definitions, we categorize different classes of agent memory, propose a robust experimental methodology for evaluating the memory capabilities of RL agents, and standardize evaluations. Furthermore, we empirically demonstrate the importance of adhering to the proposed methodology when evaluating different types of agent memory by conducting experiments with different RL agents and what its violation leads to.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement Learning (RL) effectively addresses various problems within the Markov Decision Process (MDP) framework, where agents make decisions based on immediately available information (Mnih et al., 2015; Badia et al., 2020). However, there are still challenges in applying RL to more complex tasks with partial observability.\nTo successfully address such challenges, it is essential that an agent is able to efficiently store and process the history of its interactions with the environment (Ni et al., 2021). Sequence processing methods originally developed for natural language processing (NLP) can be effectively applied to these tasks because the history of interactions with the environment can be represented as a sequence (Hausknecht & Stone, 2015; Esslinger et al., 2022; Samsami et al., 2024).\nHowever, in many tasks, due to the complexity or noisiness of observations, the sparsity of events, the difficulty of designing the reward function, and the long duration of episodes, storing and retrieving important information becomes extremely challenging, and the need for memory mechanisms arises (Graves et al., 2016; Wayne et al., 2018; Goyal et al., 2022). Nevertheless, in the existing literature on RL, where the concept of \u201cmemory\u201d is discussed, the definitions of memory are only defined in terms of the specific problem under consideration.\nFor example, in some works, memory is defined as the ability of an agent to effectively establish and use dependencies between events within a fixed-size sequence of tokens (context) in decision making (Esslinger et al., 2022; Ni et al., 2023; Grigsby et al., 2024). In other works, the term \"memory\" refers to the agent's ability to use out-of-context information through the use of various memory mechanisms (Parisotto et al., 2020; Lampinen et al., 2021; Cherepanov et al., 2024). In"}, {"title": "2 PARTIALLY OBSERVABLE MARKOV DECISION PROCESS", "content": "The Partially Observable Markov Decision Process (POMDP) is a generalization of the Markov Decision Process (MDP) that models sequential decision-making problems where the agent has incomplete information about the environment's state. POMDP can be represented as a tuple $M_P = (S, A, O, P, R, Z)$, where S denotes the set of states, A is the set of actions, O is the set of observations and $Z = P(0_{t+1} | S_{t+1}, a_t)$ is an observation function such that $o_{t+1} \\sim Z(s_{t+1}, a_t)$. An agent takes an action $a_t \\in A$ based on the observed history $h_{0:t-1} = \\{(o_i, a_i, r_i)\\}$ and receives a reward $r_t = R(s_t, a_t)$. It is important to note that state $s_t$ is not available to the agent at time t. In the case of POMDPs, a policy is a function $\\pi(a_t | o_t, h_{0:t-1})$ that uses the agent history $h_{0:t-1}$ to obtain the probability of the action $a_t$. Thus, in order to operate effectively in a POMDPs, an agent must have memory mechanisms to retrieve a history $h_{0:t-1}$. Partial observability arises in a variety of real-world situations, including robotic navigation and manipulation tasks, autonomous vehicle tasks, and complex decision-making problems."}, {"title": "3 RELATED WORKS", "content": "Researchers' interest in memory-enhanced RL agents is evident in the abundance of works proposing architectures with memory mechanisms and benchmarks (Osband et al., 2019; Morad et al., 2023; Pleines et al., 2023) for their validation (see Appendix C for details). However, despite the rather large number of works devoted to this topic, the term \u201cmemory\u201d in RL still has multiple senses, and the selection of benchmarks and experiments is not always done correctly.\nThus, for instance, in Oh et al. (2016), memory is understood as the ability of an agent to store recent observations into an external buffer and then retrieve relevant information based on temporal context. In Lampinen et al. (2021), memory is the ability to store and recall desired information at long intervals. In Fortunato et al. (2020), memory refers to working and episodic memory (with short-term and long-term nature, respectively) from cognitive psychology and neuroscience, which allows an intelligent agent to use information from past events to make decisions in the present and future. Ni et al. (2023) describes two distinct forms of temporal reasoning: (working) memory and (temporal) credit assignment, where memory refers to the ability to recall a distant past event at the"}, {"title": "4 MEMORY OF HUMANS AND AGENTS", "content": "Most works related to the concept of memory in RL use various principles from cognitive psychology and neuroscience such as long-term memory (Lampinen et al., 2021; Ni et al., 2023; Grigsby et al., 2024), working memory (Graves et al., 2014; Fortunato et al., 2020), episodic memory (Pritzel et al., 2017; Fortunato et al., 2020), associative memory (Parisotto & Salakhutdinov, 2017; Zhu et al., 2020), and others to introduce it. Despite the fundamental differences in these concepts, works on memory in RL often simplify these concepts to their inherent temporal scales (short-term memory and long-term memory). Regardless, the temporal scales are often presented qualitatively without clearly defining the boundaries between them. For example, many studies assume that remembering a few steps within an environment represents short-term memory, while remembering hundreds of steps represents long-term memory, without considering the relative nature of these concepts. This ambiguity between short-term and long-term memory can lead to a misattribution of an agent's memory capabilities and to an incorrect estimation of them when conducting experiments. To address this ambiguity, in this section we introduce formal definitions of agent memory in RL and its types, and propose an algorithm for designing an experiment to test agent memory in a correct way."}, {"title": "4.1 MEMORY IN COGNITIVE SCIENCE", "content": "Human cognitive abilities that ensure adaptive survival depend largely on memory, which determines the accumulation, preservation, and reproduction of knowledge and skills (Parr et al., 2020; 2022). Memory exists in many forms, each of which relies on different neural mechanisms. Neuroscience and cognitive psychology distinguish memory by the temporal scales at which information is stored and accessed, and by the type of information that is stored. Abstracting from this distinction, a high-level definition of human memory is as follows: \u201cmemory \u2013 is the ability to retain information and recall it at a later time\"."}, {"title": "4.2 \u039c\u0395\u039cMORY IN RL", "content": "The interpretation of memory in RL varies across studies. In some POMDPs, agents need to retain crucial information to make future decisions within a single environment. Here, memory typically encompasses two aspects: 1) the efficiency of establishing dependencies between events within a fixed time interval (e.g., transformer context (Esslinger et al., 2022; Ni et al., 2023)); and 2) the efficiency of establishing dependencies between events outside a fixed time interval (Parisotto et al., 2020; Sorokin et al., 2022).\nBased on the neuroscience definitions outlined in subsection 4.1, the first interpretation aligns with short-term memory, while the second corresponds to long-term memory. Both interpretations are also closely related to declarative memory. In Meta-RL, memory typically refers to an agent's ability to leverage skills from different environments/episodes Team et al. (2023); Kang et al. (2024a), akin to procedural memory.\nHowever, many studies fail to differentiate between agents with declarative and procedural memory, often treating Meta-RL tasks as a whole rather than focusing on decision-making based on past"}, {"title": "5 MEMORY DECISION MAKING", "content": "POMDP tasks that use agent memory can be divided into two main classes: Meta Reinforcement Learning (Meta-RL), which involves skill transfer across tasks, and Memory Decision-Making (Memory DM), which focuses on storing and retrieving information for future decisions.\nThis distinction is crucial: agents in Meta-RL use something like the procedural memory of subsection 4.1 to facilitate rapid learning and generalization, while those in Memory DM rely on something like declarative memory for current decision-making within the same environment. Despite these differences, many studies overlook behavioral manifestations and focus solely on temporal scales.\nTo introduce a definition for Memory DM tasks, we first need to introduce the definition of agent context length:\nDefinition 1. Agent context length ($K \\in \\mathbb{N}$) \u2013 is the maximum number of previous steps (triplets of (o, a, r)) that the agent can process at time t.\nFor example, an MLP-based agent processes one step at a time (K = 1), while a transformer-based agent can process a sequence of up to $K = K_{attn}$ triplets, where $K_{attn}$ is determined by attention. Using the introduced Definition 1 for agent context length, we can introduce a formal definition for the Memory DM framework we focus on in this paper:\nDefinition 2. Memory Decision-Making (Memory DM) \u2013 is a class of POMDPs in which the agents decision-making process at time t is based on the history $h_{0:t-1} = \\{(o_i, a_i, r_i)\\}$ if t > 0 otherwise $h_0 = \\O$. The objective is to determine an optimal policy $\\pi^*(a_t | o_t, h_{0:t-1})$ that maps the current observation of and history $h_{0:t-1}$ of length t to an action $a_t$, maximizing the expected cumulative reward within a single POMDP environment $M_P$: $J^{\\pi} = E_{\\pi} [\\sum_{t=0}^{T-1} \\gamma^t r_t]$, where T - episode duration, $\\gamma \\in [0, 1]$ \u2013 discount factor.\nIn the Memory DM framework (Definition 2), memory refers to the agent's ability to recall information from the past within a single environment and episode. In contrast, in the Meta-RL framework (see Appendix, Definition 7), memory involves recalling information about the agent's behavior from other environments or previous episodes. To distinguish these concepts, we adopt the definitions of \u201cDeclarative memory\u201d and \u201cProcedural memory\u201d from subsection 4.1:\nDefinition 3 (Declarative and Procedural memory in RL). Let $N_{envs}$ be the number of training environments and $N_{eps}$ the number of episodes per environment. Then,\n1. Declarative Memory \u2013 a type of agent memory when an agent transfers its knowledge within a single environment and across a single episode within that environment:\nDeclarative Memory $\\implies N_{envs} \\times N_{eps} = 1$ ."}, {"title": "5.1 \u039c\u0395\u039cORY-INTENSIVE ENVIRONMENTS", "content": "To effectively test a Memory DM agent's use of short-term and long-term memory, it is crucial to design appropriate experiments. Not all environments are suitable for assessing agent memory; for example, omnipresent Atari games (Bellemare et al., 2013) with frame stacking or MuJoCo control tasks (Fu et al., 2021) may yield unrepresentative results. To facilitate the evaluation of agent memory capabilities, we formalize the definition of memory-intensive environments:\nDefinition 5 (Memory-intensive environments). Let $M_P$ be POMDP and $\\Xi = \\{\\xi_n\\} = \\{(t_r - t_e - \\Delta t + 1)_n\\}$ set of correlation horizons $\\xi$ between for all event-recall pairs. Then $M_P$ memory-intensive environment $\\iff max_{n} \\xi_n > 1 \\land min_{n} \\xi_n > 1$.\nCorollary: $max_{n} \\xi_n = 1 \\implies M - MDP$.\nUsing the definitions of memory-intensive environments (Definition 5) and agent memory types (Definition 4), we can configure experiments to test short-term and long-term memory in the Memory DM framework. Notably, the same memory-intensive environment can validate both types of memory, as outlined in Theorem 1:\nTheorem 1 (On the context memory border). Let $M_P$ be a memory-intensive environment and K be an agents context length. Then there exists context memory border $\\overline{K} > 1$ such that if $K < \\overline{K}$ then the environment $M_P$ is used to validate exclusively long-term memory in Memory DM framework:\n$\\exists \\overline{K} \\geq 1 : \\forall K \\in [1, \\overline{K}] : K < min \\Xi$ .\nProof. Let $\\overline{K} = min \\Xi - 1$. Then $\\forall K < \\overline{K}$ is guaranteed that no correlation horizon $\\xi$ is in the agent history $h_{t-K+1:t}$, hence the context length $K < min \\Xi - 1$ generates the long-term memory problem exclusively. Since context length cannot be negative or zero, it turns out that $1 \\leq K < \\overline{K} = min \\Xi - 1$, which was required to prove."}, {"title": "5.2 LONG-TERM MEMORY IN MEMORY DM", "content": "As defined in Definition 4, Memory DM tasks with short-term memory occur when event-recall pairs in the memory-intensive environment $M_P$ are within the agent's context ($\\xi < K$). Here, memory involves the agent's ability to connect information within a context, regardless of how large K is. Examples include works like Esslinger et al. (2022); Ni et al. (2023); Grigsby et al. (2024). Validating short-term memory is straightforward by simply setting a sufficiently large context length K. However, validating long-term memory capabilities is more complex and of greater interest.\nMemory DM tasks requiring long-term memory occur when event-recall pairs in the memory-intensive environment $M_P$ are outside the agent's context ($\\xi > K$). In this case, memory involves the agent's ability to connect information beyond its context, necessitating memory mechanisms (Definition 6) that can manage interaction histories h longer than the agent's base model can handle.\nDefinition 6 (Memory mechanisms). Let the agent process histories $h_{t-K+1:t}$ of length K at the current time t, where $K \\in \\mathbb{N}$ is agents context length. Then, a memory mechanism $\\mu(K) : \\mathbb{N} \\rightarrow \\mathbb{N}$ is defined as a function that, for a fixed K, allows the agent to process sequences of length $K_{eff} > K$, i.e., to establish global correlations out of context, where $K_{eff}$ is the effective context.\n$\\mu(K) = K_{eff} \\geq K$ .\nMemory mechanisms are essential for addressing long-term memory challenges (processing out-of-context information) in the Memory DM framework.\nExample of memory mechanism. Consider an agent based on an RNN architecture that can process K = 1 triplets of tokens (observations, actions, and rewards) at all times t. By using memory mechanisms $\\mu(K)$ (e.g., as in Hausknecht & Stone (2015)), the agent can increase the number of tokens processed in a single step without expanding the context size of its RNN architecture. Therefore, if initially in a memory-intensive environment $M_P : \\xi > K = 1$, it can now be represented as $M_P : \\xi < K_{eff} = \\mu(K)$. Here, the memory mechanism $\\mu(K)$ refers to the RNNs recurrent updates to its hidden state.\nThus, validating an agent's ability to solve long-term memory problems in the Memory DM framework reduces to validating the agent's memory mechanisms $\\mu(K)$. To design correct experiments in such a case, the following condition must be met:"}, {"title": "6 EXPERIMENTS", "content": "To illustrate the importance of following a consistent methodology (Algorithm 1) when evaluating an agent's long-term and short-term memory capabilities, as well as to highlight the ambiguity in results that can arise from experimental misconfigurations, we conducted a series of experiments with memory-enhanced agents in memory-intensive environments within the Memory DM framework.\nFor our experiments, we chose two memory-intensive environments: Passive-T-Maze (Ni et al., 2023) and Minigrid-Memory (Chevalier-Boisvert et al., 2023) (see Appendix, Figure 6). In the Passive-T-Maze, the agent starts at the beginning of a T-shaped maze and observes a clue, which it must use to make a turn at a junction at the end of the maze. The Minigrid-Memory environment presents a similar challenge to the Passive-T-Maze; however, the agent must first reach a room containing a clue before walking down a corridor and making a turn. A detailed description of these environments can be found in Appendix, subsection E.1.\nAs memory-enhanced baselines, we chose Deep Transformer Q-Networks (DTQN) (Esslinger et al., 2022), DQN with GPT-2 (DQN-GPT-2) (Ni et al., 2023), and Soft Actor-Critic with GPT-2 (SAC-GPT-2) (Ni et al., 2023)."}, {"title": "6.1 IMPACT OF EXPERIMENT CONFIGURATION ON MEMORY TYPE TESTED", "content": "In subsection 5.1, we identified intervals of agent context length to separate the impact of long-term memory (LTM) and short-term memory (STM). However, the transition between LTM and STM creates an intermediate range where their contributions cannot be clearly distinguished, as some correlation horizons fall inside the agent's context and others do not.\nWithout standardized definitions or validation methods for LTM and STM, experiments often occur in this transitional interval, making it impossible to assess LTM memory. This ambiguity can lead to misinterpretations of the agent's LTM capabilities, as demonstrated below.\nTo illustrate this, we conducted experiments with the transformer-based agent SAC-GPT-2 in the MiniGrid-Memory environment, setting the map size to L = 21. Two experimental configurations were used: fixed-length corridors with $\\xi = L + 1$ (fixed mode) and variable-length corridors with $\\xi \\in [7, L + 1]$ (variable mode). If the methodology proposed in Algorithm 1 for testing LTM and STM within the Memory DM framework is not followed, the agent's context length K might be set arbitrarily as K = 14 (representing LTM, since K < L) or K = 22 (representing STM, since K > L).\nThe results of this experiment are shown in Figure 4. The solid line represents STM (K = 22), the dashed line represents LTM (K = 14), while green indicates variable mode and red indicates fixed mode. In variable mode (green), the agent achieves a success rate (SR) almost 1.0 for both LTM and STM validation experiments. This might incorrectly suggest that the agent possesses both memory types. Conversely, in fixed mode (red), the results reveal a discrepancy: the agent demonstrates STM memory but fails to exhibit LTM memory.\nThis discrepancy arises because SAC-GPT-2 lacks memory mechanisms to solve LTM problems; it can only leverage information within its context K. The confusion occurs due to a naive experimental setup, where K was chosen relative to L based solely on environmental documentation, without consideration for the interaction of LTM and STM. In variable mode, the agent's performance reflects a mix of LTM and STM capabilities, making it impossible to isolate LTM memory explicitly. In contrast, the fixed mode, tested according to the methodology outlined in Algorithm 1, clearly identifies STM memory while confirming the absence of LTM memory.\nIn this section, we have demonstrated that a naive approach to testing an agent's memory can result in misinterpreting its true capabilities. In contrast, our proposed methodology enables the design of"}, {"title": "6.2 THE RELATIVE NATURE OF AN AGENT'S MEMORY", "content": "According to the Algorithm 1, the experimental setup for testing agent memory types (LTM and STM) relies on two parameters: the agent's context length K and the context memory border $\\overline{K}$, which depends on the environment properties, $\\Xi$. Verifying an agent's LTM or STM requires adjusting K or $\\Xi$ while keeping the other fixed. This section explains how these parameters interact in memory testing experiments.\nWe evaluate two memory-enhanced agents, DTQN and DQN-GPT-2, in the Passive T-Maze environment by varying K and $\\Xi$. The results are shown in Figure 5.\nFirst, we test STM by setting K = $\\Xi$ = 15. In this configuration, all relevant information stays within the agent's context. As shown in Figure 5 (left), both agents achieve a return of 1.0, confirming STM capabilities. To test LTM, we use $\\Xi$ = 15 and adjust the setup so that key event-recall pairs fall outside the agent's context. By reducing K from 15 to 5, as shown in Figure 5 (center), the return of both agents drops to 0.5, indicating that they cannot recall the cue information, which confirms that LTM is not LTM. Next, we further assess STM by reducing $\\Xi$. With K = 5 and $\\Xi$ reduced from 15 to 5, as shown in Figure 5 (right), the agent's return returns to 1.0. This shows that agents can effectively use memory when all relevant information is within their context.\nIn summary, verifying LTM and STM can be done by adjusting K or $\\Xi$ while keeping the other fixed. The Passive T-Maze is an effective testbed due to its parameterizable corridor length L, which relates to $\\xi$ as $\\xi = L + 1$. However, in many environments where $\\xi$ is fixed, varying K remains a viable approach for memory evaluation."}, {"title": "7 CONCLUSION", "content": "In this study, we formalize memory types in RL, distinguishing long-term memory (LTM) from short-term memory (STM), and declarative from procedural memory, drawing inspiration from neuroscience. We also separate POMDPs into two classes: Memory Decision-Making (Memory DM) and Meta Reinforcement Learning (Meta-RL).\nThe formalization, along with the methodology for validating LTM and STM in the Memory DM framework, provides a clear structure for distinguishing between different types of agent memory. This enables fair comparisons of agents with similar memory mechanisms and highlights limitations in memory architecture, facilitating precise evaluations and improvements.\nAdditionally, we demonstrate the potential pitfalls of neglecting this methodology. Misconfigured experiments can lead to misleading conclusions about an agent's memory capabilities, blurring the lines between LTM and STM. By following our approach, researchers can achieve more reliable assessments and make informed comparisons between memory-enhanced agents.\nThis work provides a significant step toward a unified understanding of agent memory in RL. Our definitions and methodology offer practical tools for rigorously testing agent memory, ensuring consistent experimental design. By addressing common inconsistencies, our approach guarantees reliable results and meaningful comparisons, advancing research in RL."}, {"title": "A APPENDIX \u2013 GLOSSARY", "content": "In this section, we provide a comprehensive glossary of key terms and concepts used throughout this paper. The definitions are intended to clarify the terminology proposed in our research and to ensure that readers have a clear understanding of the main elements underpinning our work.\n1. M-MDP environment\n2. Mp-POMDP environment\n3. Mp \u2013 memory-intensive environment\n4. $h_{0:t-1} = \\{(o_i, a_i, r_i)\\}$ \u2013 agent history of interactions with environment\n5. K-agent base model context length\n6. $\\overline{K}$ \u2013 context memory border of the agent, such that $K \\in [1, \\overline{K}] \\iff$ strictly LTM problem\n7. $\\mu(K)$ \u2013 memory mechanism that increases number of steps available to the agent to process\n8. $K_{eff} = \\mu(K)$ \u2013 the agent effective context after applying the memory mechanism\n9. $a_t = \\{(\\alpha_{Oi}, a_i, r_i)\\}_{t_e}^{t_e+\\Delta t}$ - an event starting at time $t_e$ and lasting $\\Delta t$, which the agent should recall when making a decision in the future\n10. $\\beta_{t_r} = \\beta_{t_r}(a_t) = a_t | (o_t, a_t)$ \u2013 the moment of decision making at time $t_r$, according to the event $a_t$\n11. $\\xi = t_r - t_e - \\Delta t + 1$ \u2013 an event's correlation horizon"}, {"title": "B APPENDIX \u2013 ADDITIONAL NOTES ON THE MOTIVATION FOR THE ARTICLE", "content": "B.1 WHY USE DEFINITIONS FROM NEUROSCIENCE?\nDefinitions from neuroscience and cognitive science, such as short-term and long-term memory, as well as declarative and procedural memory, are already well-established in the RL community, but do not have common meanings and are interpreted in different ways. We strictly formalize these definitions to avoid possible confusion that may arise when introducing new concepts and redefine them with clear, quantitative meanings to specify the type of agent memory, since the performance of many algorithms depends on their type of memory.\nIn focusing exclusively on memory within RL, we do not attempt to exhaustively replicate the full spectrum of human memory. Instead, our goal is to leverage the intuitive understanding of neuroscience concepts already familiar to RL researchers. This approach avoids the unnecessary introduction of new terminology into the already complex Memory RL domain. By refining and aligning existing definitions, we create a robust framework that facilitates clear communication, rigorous evaluation, and practical application in RL research.\nB.2 ON PRACTICAL APPLICATIONS OF OUR FRAMEWORK\nThe primary goal of our framework is to address practical challenges in RL by providing a robust classification of memory types based on temporal dependencies and the nature of memorized information. This classification is essential for standardizing memory testing and ensuring that RL agents are evaluated under conditions that accurately reflect their capabilities.\nIn RL, memory is interpreted in various ways, such as transformers with large context windows, recurrent networks, or models capable of skill transfer across tasks. However, these approaches often vary fundamentally in design, making comparisons unreliable and leading to inconsistencies in testing. Our framework resolves this by providing a clear structure to evaluate memory mechanisms under uniform and practical conditions."}, {"title": "D APPENDIX \u2013 META REINFORCEMENT LEARNING", "content": "In this section, we explore the concept of Meta-Reinforcement Learning (Meta-RL), a specialized domain within POMDPs that focuses on equipping agents with the ability to learn from their past experiences across multiple tasks. This capability is particularly crucial in dynamic environments where agents must adapt quickly to new challenges. By recognizing and memorizing common patterns"}, {"title": "E APPENDIX \u2013 EXPERIMENT DETAILS", "content": "E.1 APPENDIX \u2013 ENVIRONMENTS DESCRIPTION\nThis section provides an extended description of the environments used in this work.\nPassive-T-Maze (Ni et al., 2023). In this T-shaped maze environment, the agent's goal is to move from the starting point to the junction and make the correct turn based on an initial signal. The agent can select from four possible actions: a \u2208 left, up, right, down. The signal, denoted by the variable clue, is provided only at the beginning of the trajectory and indicates whether the agent should turn up (clue = 1) or down (clue = -1). The episode duration is constrained to T = L+1, where L is the length of the corridor leading to the junction, which adds complexity to the task. To facilitate navigation, a binary variable called flag is included in the observation vector. This variable equals 1 one step before reaching the junction and 0 at all other times, indicating the agent's proximity to the junction. Additionally, a noise channel introduces random integer values from the set -1,0, +1 into the observation vector, further complicating the task. The observation vector is defined as o = [y, clue, flag, noise], where y represents the vertical coordinate.\nThe agent receives a reward only at the end of the episode, which depends on whether it makes a correct turn at the junction."}]}