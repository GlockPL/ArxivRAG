{"title": "Graph neural networks and non-commuting operators", "authors": ["Mauricio Velasco", "Kaiying O'Hare", "Bernardo Rychtenberg", "Soledad Villar"], "abstract": "Graph neural networks (GNNs) provide state-of-the-art results in a wide variety of tasks which typically involve predicting features at the vertices of a graph. They are built from layers of graph convolutions which serve as a powerful inductive bias for describing the flow of information among the vertices. Often, more than one data modality is available. This work considers a setting in which several graphs have the same vertex set and a common vertex-level learning task. This generalizes standard GNN models to GNNs with several graph operators that do not commute. We may call this model graph-tuple neural networks (GtNN).\nIn this work, we develop the mathematical theory to address the stability and transferability of GtNNs using properties of non-commuting non-expansive operators. We develop a limit theory of graphon-tuple neural networks and use it to prove a universal transferability theorem that guarantees that all graph-tuple neural networks are transferable on convergent graph-tuple sequences. In particular, there is no non-transferable energy under the convergence we consider here. Our theoretical results extend well-known transferability theorems for GNNs to the case of several simultaneous graphs (GtNNs) and provide a strict improvement on what is currently known even in the GNN case.\nWe illustrate our theoretical results with simple experiments on synthetic and real-world data. To this end, we derive a training procedure that provably enforces the stability of the resulting model.", "sections": [{"title": "Introduction", "content": "Graph neural networks (GNNs) [45, 3, 24, 16] are a widely-used and versatile machine learning tool to process different kinds of data from numerous applications, including chemistry [15], molecular geometry [48], combinatorial optimization [21, 36], among many other. Such networks act on functions on the vertices of a graph (also called signals or vertex features) and use the structure of the graph as a powerful inductive bias to describe the natural flow of information among vertices. One of the most common graph neural networks are based on graph convolutions [14], which generalize the notion of message passing. The typical architecture has building blocks which are polynomial functions of the adjacency matrix (or more generally of the shift operator) of a graph composed with componentwise non-linearities. Therefore, such networks implement the idea that the values of a function at a vertex are related with the values at the immediate neighbors of the vertex and also with the values at the neighbors of its neighbors, etc.\nDue to the significant practical success and diversity of applications of GNNs, there is a growing interest in understanding their mathematical properties. Researchers have delved into various theoretical aspects of MPNNs, including, for instance, expressivity [35, 47, 9, 1, 8], oversmoothing [44], multi-scale properties [18, 5], and model relaxations [12, 17]. One of the fundamental properties of graph neural networks is their remarkable transferability property, which intuitively refers to their ability to perform well in large networks when trained in smaller networks, thus transfering knowledge from one to the other. This is in part possible because the number of parameters that defines a GNN is independent of the size of the input graphs. The idea is conceptually related to the algebraic notion of representation stability that has been recently studied in the context of machine learning models [28]. More precisely, if two graphs describe similar phenomena, then a given GNN should have similar repercussions (i.e. similar effect on similar signals) on both graphs. In order to describe this property precisely, it is necessary to place signals and shift operators on different graphs (of potentially different sizes) in an equal footing to allow for meaningful comparisons and to characterize families of graphs describing \"similar\" phenomena. The seminal work [40] has used the theory of graphons to carry out these two steps, providing a solid theoretical foundation to the transferability properties of GNNs. The theory was further developed in [27, 41, 32, 23, 10], and was extended to other models in [6, 26, 43]. The transferability theory is very related to the stability or perturbation theory of GNNs that studies how GNN outputs change under small perturbations of the graph input or graph signal [42, 7, 13, 22], and conceptually related to the theory of generalization for GNNs [46, 11, 29, 33, 31] though the techniques are different.\nIn many practical situations a fixed collection of entities serves as common vertices to several distinct graphs simultaneously that represent several modalities of the same underlying object. This occurs, for instance, in recommendation systems where the items can be considered as vertices of several distinct similarity graphs. It occurs in the analysis of social networks because individuals often participate in several distinct social/information networks simultaneously and in a wide array of multimodal settings.\nThe goal of this paper is to extend the mathematical theory of GNNs to account for multimodal graph settings. The most closely related existing work is the algebraic neural network theory of Parada-Mayorga, Butler and Ribeiro [38, 37, 4] who pioneer the use of algebras of non-commuting operators. The setting in this paper could be thought of as a special case of this theory. However, there is a crucial difference: whereas the main results in the articles above refer to the Hilbert-Schmidt norm, we define and analyze block-operator-norms on non-commutative algebras acting on function spaces. This choice allows us to prove stronger stability and transferability bounds that when restricted to classical GNNs improve upon or complement the state-of-the-art theory. In particular, we complement work in [42] by delivering bounds that do not exhibit no-transferable energy, and we complement results in [32] by providing stability bounds that do not require convergence. Our bounds are furthermore easily computable in terms of the networks' parameters improving on the results of [37] and in particular allow us to devise novel training algorithms with stability guarantees.\nOur contributions. The main contribution of this work is a theoretical analysis for graph neural networks in the multimodal framework where each graph object (or graph tuple) can have several adjacency matrices on a fixed set of vertices. We call this model graph-tuple neural networks (GtNNs). It generalizes GNNs and is naturally suited for taking into account information flows along paths traversing several distinct graphs. This architecture replaces the polynomials $h(X)$ underlying graph convolutional neural networks with non-commutative polynomials $h(X_1, ..., X_k)$ on the adjacency matrices of the k graphs in our tuple. More generally our approach via operator networks gives a general and widely applicable parametrization for such networks. Our approach is motivated by the theory of switched dynamical systems, where recent algorithmic tools have improved our understanding of the iterative behaviour of non-commuting operators [34]. Our main results are tight stability bounds for GtNNs and GNNs.\nThe second contribution of this article is the definition of graphon-tuple neural networks (WtNNs) which are the natural limits of (GtNNs) as the number of vertices grows to infinity. Graphon-tuple neural networks provide a good setting for understanding the phenomenon of transferability. Our"}, {"title": "Preliminary definitions", "content": "For an integer n we let $[n] := \\{1, 2, . . ., n\\}$. By a graph $G$ on a set $V$ we mean an undirected, finite graph without self-loops with vertex set $V(G) := V$ and edge set denoted $E(G)$. A shift matrix for $G$ is any $|V| \\times |V|$ symmetric matrix $S$ with entries $0 < S_{ij} \\leq 1$ satisfying $S_{ij} = 0$ whenever $i \\neq j$ and $(i, j) \\notin E(G)$.\nOur main object of study will be signals (i.e. functions) on the common vertices $V$ of a set of graphs so we introduce notation for describing them. We denote the algebra of real-valued functions on the vertex set $V$ by $\\mathbb{R}[V]$. Any function $f : V \\rightarrow \\mathbb{R}$ is completely determined by its vector of values so, as a vector space, $\\mathbb{R}[V] \\simeq \\mathbb{R}^{|V|}$ however, as we will see later, thinking of this space as consisting of functions is key for understanding the neural networks we consider. Any shift matrix $S$ for $G$ defines a shift operator $T_G : \\mathbb{R}[V] \\rightarrow \\mathbb{R}[V]$ by the formula $T_G(f)(i) = \\sum_{j \\in V} S_{ij}f(j)$.\nThe layers of graph neural networks (GNNs) are built from univariate polynomials $h(x)$ evaluated on the shift operator $T_G$ of a graph composed with componentwise non-linearities. If we have a $k$-tuple of graphs $G_1,..., G_k$ with common vertex set $V$ then it is natural to consider multivariate polynomials evaluated at their shift operators $T_{G_i}$. Because shift operators of distinct graphs generally do not commute this forces us to design an architecture which is parametrized by noncommutative polynomials. The trainable parameters of such networks will be the coefficients of these polynomials.\nNoncommutative polynomials. For a positive integer $k$, let $\\mathbb{R}\\langle X_1, ..., X_k \\rangle$ be the algebra of non-commutative polynomials in the variables $X_1,...,X_k$. This is the vector space having as basis all finite length words on the alphabet $X_1,..., X_k$ endowed with the bilinear product defined by concatenation on the basis elements. For example in $\\mathbb{R}\\langle X_1, X_2 \\rangle$ we have $(X_1 + X_2)^2 = X_1^2 + X_1X_2 + X_2X_1 + X_2^2 \\neq X_1^2 + 2X_1X_2 + X_2^2$.\nThe basis elements appearing with nonzero coefficient in the unique expression of any element $h(X_1,..., X_k)$ are called the monomial words of $h$. The degree of a monomial word is its length (i.e. number of letters). For example there are eight monomials of degree three in $\\mathbb{R}\\langle X_1, X_2 \\rangle$, namely: $X_1^3, X_1^2X_2, X_1X_2X_1, X_2X_1^2, X_2^2X_1, X_1X_2^2, X_2X_1X_2, X_2^3$. More generally there are exactly $k^d$ monomial words of length $d$ and $\\frac{k^{d+1}-1}{k-1}$ monomial words of degree at most $d$ in $\\mathbb{R}\\langle X_1,..., X_k \\rangle$.\nNoncommutative polynomials have a fundamental structural relationship with linear operators which makes them suitable for transference. If $W$ is any vector space let $End(W)$ denote the space of linear maps from $W$ to itself. If $T_1,...,T_k \\in End(W)$ are any set of linear maps on $W$ then the individual evaluations $X_i \\rightarrow T_i$ extend to a unique evaluation homomorphism $\\mathbb{R}\\langle X_1,..., X_k \\rangle \\rightarrow End(W)$, which sends the product of polynomials to the composition of linear maps. This relationship (known as universal freeness property) determines the algebra $\\mathbb{R}\\langle X_1,..., X_k \\rangle$ uniquely. This proves that noncommutative polynomials are the only naturally transferable parametrization for our networks. For a polynomial $h$ we denote the linear map obtained from evaluation as $h(T_1, ...,T_k)$."}, {"title": "Operator filters and non-commuting operator neural networks", "content": "Using noncommutative polynomials we will define operator networks, an abstraction of both graph and graphon neural networks. Operator networks will provide us with a uniform generalization to graph-tuple and graphon-tuple neural networks and allow us to describe transferability precisely.\nThe domain and range of our operators will be powers of a fixed vector space $F$ of signals. More formally, $F$ consists of real-valued functions on a fixed domain $V$ endowed with a measure $\\mu_V$. The measure turns $F$ into an inner product space (see [25, Chapter 2] for background) via the formula $\\langle f,g \\rangle := \\int_V fgd\\mu_V$ and in particular gives it a natural norm $||f|| := \\sqrt{\\langle f, f \\rangle}$ which we will use throughout the article. In later sections the set $F$ will be either $\\mathbb{R}[V]$ or the space $L := L^2([0, 1])$ of square integrable functions in $[0, 1]$ but operator networks apply much more generally, for instance to the spaces of functions on a manifold $V$ used in geometric deep learning [2]. By an operator $k$-tuple on $F$ we mean a sequence $T := (T_1, . . ., T_k)$ of linear operators $T_i : F \\rightarrow F$. The tuple is nonexpansive if each operator $T_j$ has norm bounded above by one.\nIf $h \\in \\mathbb{R}\\langle X_1,..., X_k \\rangle$ is a noncommutative polynomial then the operator filter defined by $h = \\sum_{\\alpha} C_{\\alpha}X^{\\alpha}$ and the operator tuple $T$ is the linear operator $\\Psi(h,T) : F \\rightarrow F$ given by the formula\n$h(T_1,...,T_k)(f) = \\sum_{\\alpha} C_{\\alpha}X^{\\alpha}(T_1,...T_k)(f)$\nwhere $X^{\\alpha}(T_1, ..., T_k)$ is the composition of the $T_i$ from left to right in the order of the word $\\alpha$. For instance if $h(X_1, X_2) := -5X_1X_2X_1 + 3X_1^2X_2$ then the graph-tuple filter defined by $h$ applied to a signal $f \\in F$ is $\\Psi(h, T_1, . . ., T_k)(f) = -5T_1(T_2(T_1(f))) + 3T_1^2(T_2(f))$.\nMore generally, we would like to be able to manipulate several features simultaneously (i.e. to manipulate vector-valued signals) and do so by building block-linear maps of operators with blocks defined by polynomials. More precisely, if $A, B$ are positive integers and $H$ is a $B \\times A$ matrix whose entries are non-commutative polynomials $h_{b,a} \\in \\mathbb{R}\\langle X_1,...,X_k \\rangle$ we define the operator filter determined by $H$ and the operator tuple $T$ to be the linear map $\\Psi(H,T) : F^A \\rightarrow F^B$ which sends a vector $x = (x_a)_{a \\in [A]}$ to a vector $(z_b)_{b \\in [B]}$ using the formula\n$z_b = \\sum_{a \\in [A]} h_{b,a}(T_1,...,T_k)(x_a)$\nAn operator neural layer with ReLU activation is an operator filter composed with a pointwise non-linearity. This composition $\\sigma \\circ \\Psi(H, T)$ yields a (nonlinear) map $\\Phi(H,T) : F^A \\rightarrow F^B$.\nFinally an operator neural network (ONN) is the result of composing several operator neural layers. More precisely if we are given positive integers $a_0, ..., a_N$ and $N$ matrices $H^{(i)}$ of noncommutative polynomials $H_{b,a}^{(j)} := h_{b,a}(X_1,..., X_k)$ for $(b,a) \\in [a_{j+1}] \\times [a_j]$ and $j = 0, . . ., N - 1$, the operator neural network (ONN) determined by $H := (H^{(i)})$ and the operator tuple $T$ is the composition $F^{a_0} \\rightarrow F^{a_1} \\rightarrow \\cdot \\rightarrow F^{a_N}$ where the $j$-th map in the sequence is the operator neural layer with ReLu activation $\\Phi(H^{(j)},T) : F^{a_j} \\rightarrow F^{a_{j+1}}$. We write $\\Phi(H,T) : F^{a_0} \\rightarrow F^{a_N}$ to refer to the full composite function. See Appendix A for a discussion on the trainable parameters and the transfer to other $k$-tuples. We conclude the Section with a key instance of operator networks:\nAn Example: Graph-tuple neural networks (GtNNs). Henceforth we fix a positive integer $k$, a sequence $G_1,..., G_k$ of graphs with common vertex set $V$ and a given set of shift operators $T_{G_1},..., T_{G_k}$. We call this information a graph-tuple $G := (G_1, . . ., G_k)$ on $V$.\nThe graph-tuple filter defined by a noncommutative polynomial $h(X_1, ..., X_k) \\in \\mathbb{R}\\langle X_1,..., X_k \\rangle$ and $G$ is the operator filter defined by $h$ evaluated at $T := (T_{G_1},...,T_{G_k})$ denoted $\\Psi(h,T) : \\mathbb{R}[V] \\rightarrow \\mathbb{R}[V]$. Exactly as in Section 2 and using the notation introduced there, we define graph-tuple filters, graph-tuple neural layers with ReLu activation and graph-tuple neural networks (GtNN) on the graph-tuple $G$ as their operator versions when evaluated at the tuple $T$ above."}, {"title": "Perturbation inequalities", "content": "In this Section we introduce our main tools for the analysis of operator networks, namely perturbation inequalities. To speak about perturbations we endow the Cartesian products $F^A$ with max-norms\n$||z||_* := \\max_{a \\in [A]} ||z_a|| \\text{ if } z = (z_a)_{a \\in [A]} \\in F^A$.\nwhere the norm $|| \\cdot ||$ on the right-hand side denotes the standard $L^2$-norm on $F$ coming from the measure $\\mu_V$ as defined in the previous section. Fix feature sizes $a_0,...,a_N$ and matrices $H := ((H^{(i)}))_{j=0,...,N-1}$ of noncommutative polynomials in $k$-variables of dimensions $a_{j+1} \\times a_j$ for $j = 0,..., N - 1$ and consider the operator-tuple neural networks $\\Phi(H,T) : F^{a_0} \\rightarrow F^{a_N}$ defined by evaluating this architecture on $k$-tuples $T$ of operators on the given function space $F$. A perturbation inequality for this network is an estimate on the sensitivity (absolute condition number) of the output when the operator-tuple and the input signal are perturbed in their respective norms, more precisely perturbation inequalities are upper bounds on the norm\n$|| \\Phi (H, W) (f) - \\Phi (H, Z) (g)||_{*}$ (1)\nin terms of the input signal difference $||| f - g|||_{*}$ and the operator perturbation size as measured by the differences $||Z_j - W_j||_{op}$. The main result of this Section are perturbation inequalities that depend on easily computable constants, which we call expansion constants of the polynomials appearing in the matrices $H$, allowing us to use them to obtain perturbation estimates for a given network and to devise training algorithms which come with stability guarantees. A key reason for the success of our approach is the introduction of appropriate norms for computations involving block-operators: If $A, B$ are positive integers and $z = (z_a)_{a \\in [A]} \\in F^A$ and $R : F^A \\rightarrow F^B$ is a linear operator then we define\n$||R||_{op} = \\sup_{z: ||z||_* \\leq 1} ||R(z)||_{*}$ (2)\nIf $h \\in \\mathbb{R}\\langle X_1,...,X_k \\rangle$ is any noncommutative polynomial then it can be written uniquely as $\\sum_{\\alpha} C_{\\alpha}x^{\\alpha}$ where $\\alpha$ runs over a finite support set of sequences in the numbers $1,...,k$. For any such polynomial we define a set of $k + 1$ expansion constants via the formulas\n$C(h) := \\sum_{\\alpha} |c_{\\alpha}|$ and $C_j (h) := \\sum_{\\alpha}q_j (\\alpha)|c_{\\alpha}| for $j = 1, ..., k$\nwhere $q_j (\\alpha)$ equals the number of times the index $j$ appears in $\\alpha$. Our main result is the following perturbation inequality, which proves that expansion constants estimate the perturbation stability of nonexpansive operator-tuple networks (i.e. those which satisfy $||T_j||_{op} \\leq 1$ for $j = 1, . . ., k$).\nTheorem 1. Suppose $W$ and $Z$ are two nonexpansive operator $k$-tuples. For positive integers $A, B$ let $H$ be any $B \\times A$ matrix with entries in $\\mathbb{R}\\langle X_1,...,X_k \\rangle$. The operator-tuple neural layer with ReLu activation defined by $H$ satisfies the following perturbation inequality: For any $f,g \\in F^A$ and for $m := \\min( || f ||_*, ||g||_* )$, we have\n$|| \\Phi(H, W) (f) - \\Phi(H, Z) (g)||_{*} \\leq ||f - g||_* \\max_{b \\in [B]} C(h_{b,a}) + m \\max_{b \\in [B]} \\sum_{a \\in [A]} \\sum_{j=1}^k C_j (h_{b,a}) ||W_j - Z_j||_{op}$\nThe proof is in Appendix C. We apply the previous argument inductively to obtain a perturbation inequality for general graph-tuple neural networks by adding the effect of each new layer to the bound. More concretely if $a_0,..., a_N$ denote the feature sizes of such a network and $R_W$ and $R_Z$ denote the network obtained by removing the last layer then"}, {"title": "Graphons and graphon-tuple neural networks (WtNNs).", "content": "In order to speak about transferability precisely, we have to address two basic theoretical challenges. On one hand we need to find a space which allows us to place signals and shift operators living on different graphs in equal footing in order to allow for meaningful comparisons. On the other hand objects that are close in the natural norm in this space should correspond to graphs describing \"similar\" phenomena. As shown in [41], both of these challenges can be solved simultaneously by the theory of graphons. A graphon is a continuous generalization of a graph having the real numbers in the interval $[0, 1]$ as vertex set. The graphon signals are the space $L$ of square-integrable functions on $[0, 1]$, that is $L := L^2([0, 1])$. In this Section we give a brief introduction to graphons and define graphon-tuple neural networks (WtNN), the graphon counterpart of graph-tuple neural networks. Our first result is Theorem 4 which clarifies the relationship between finite graphs and signals on them and their induced graphons and graphon signals respectively allowing us to make meaningful comparisons between signals on graphs with distinct numbers of vertices. The space of graphons has two essentially distinct natural norms which we define later in this Section and review in Appendix B. Converging sequences under such norms provide useful models for families of \u201csimilar phenomena\u201d and Theorem 5 describes explicit sampling methods for using graphons as generative models for graph families converging in both norms.\nComparisons via graphons. A graphon is a function $W: [0, 1] \\times [0,1] \\rightarrow [0, 1]$ which is measurable and symmetric (i.e. $W (u, v) = W(v, u)$). A graphon signal is a function $f \\in L := L^2([0,1])$. The shift operator of the graphon $W$ is the map $T_W : L \\rightarrow L$ given by the formula\n$T_W(f)(u) = \\int_0^1 W(u, v) f (v)dv$\nwhere $dv = d\\mu(v)$ denotes the Lebesgue measure $\\mu$ in the interval $[0, 1]$.\nA graphon-tuple $W_1,..., W_k$ consists of a sequence of $k$ graphons together with their shift operators $T_{W_i}: L \\rightarrow L$. Exactly as in Section 2 and using the notation introduced there, we define (A, B) graphon-tuple filters, (A, B) graphon-tuple neural layers with ReLu activation and graphon-tuple neural networks (WtNN) as their operator versions when evaluated at the $k$-tuple $W := (T_{W_1}, ...,T_{W_k})$.\nFor instance, if we are given positive integers $a_0,...,a_N$ and matrices $H^{(i)}$ with entries given by noncommutative polynomials $H_{b,a}^{(j)} := h_{b,a}^{(j)} \\in \\mathbb{R}\\langle X_1,..., X_k \\rangle$ for $(b,a) \\in [a_{j+1}] \\times [a_j]$ and"}, {"title": "Universal transferability", "content": "Our next result combines perturbation inequalities for graphon-tuple networks and Theorem 4 which compares graph-tuple networks and their induced graphon-tuple counterparts resulting in a transferability inequality. As a corollary of this inequality we prove a universal transferability result which shows that every architecture is transferable in a converging sequence of graphon-tuples, in the sense that the transferability error goes to zero as the index of the sequence goes to infinity. This result is interesting and novel even for the case of graphon-graph transferability (i.e. when $k = 1$)."}, {"title": "Training with stability guarantees", "content": "Following our perturbation inequalities (i.e., Theorem 1 and Corollary 3) we propose a training algorithm to obtain a GtNN that enforces stability by constraining all the expansion constants $C(h)$ and $C_j(h)$. Consider a GtNN $\\Phi(H,T_G)$ and nonexpansive operator $k$-tuples $T_G$. Denote the set of $k + 1$ expansion constants for each layer $d = 0, ..., N - 1$ as\n$C(H^{(d)}) := \\max_{b \\in [a_{d+1}]} \\sum_{a \\in [a_d]} C(h_{b,a})$ and $C_j(H^{(d)}) := \\max_{b \\in [a_{d+1}]} \\sum_{a \\in [a_d]} C_j(h_{b,a})$ for $j = 1,..., k,\nand write $C(H) = (C(H^{(d)}))_{d=1}^{N-1}$ and $C_j(H) = (C_j(H^{(d)}))_{d=1}^{N-1}$ for $j = 1, . . ., k$. Given $k + 1$ vectors of target bounds $\\hat{C} := (\\hat{C}^{(d)})_{d=0}^{N-1}$ and $\\hat{C}_j := (\\hat{C}_j^{(d)})_{d=0}^{N-1}$ for $j = 1, . . ., k$, and training data $(x_i, y_i) \\in F^{a_0} \\times F^{a_N}$ for $i \\in I$, we train the network by a constrained minimization problem\n$\\min_{c} \\frac{1}{|I|} \\sum_{i \\in I} l(\\Phi(\\hat{H}(c), T_G)(x_i), y_i)$ s.t. $C(\\hat{H}(c)) \\leq \\hat{C}, C_j(\\hat{H}(c)) \\leq \\hat{C}_j$ for $j = 1, ..., k,$\nwhere $l(.,.)$ is any nonnegative loss function depending on the task, and $c$ denotes all the polynomial coefficients in the network. If we pick $\\hat{C}$ to be an all ones vector (or smaller), by Corollary 3, the perturbation stability is guaranteed to scale linearly with the number of layers $N$.\nTo approximate the solution of the constrained minimization problem we use a penalty method,\n$\\min_{c} \\frac{1}{|I|} \\sum_{i \\in I} l(\\Phi(\\hat{H}(c), T_G)(x_i), y_i) + \\lambda[p(C(\\hat{H}(c)) - \\hat{C}) + \\sum_{j=1}^k p(C_j(\\hat{H}(c)) - \\hat{C}_j)],$ (6)\nwhere $p(\\cdot)$ is a componentwise linear penalty function $p(C) = (p(C^{(d)}))^{\\dagger}$ with $p(C^{(d)}) = \\max(0, C^{(d)})$. The stable GtNN algorithm picks a fixed large enough penalty coefficient $\\lambda$ and trains the network with local optimization methods."}, {"title": "Experimental data and numerical results", "content": "We perform three experiments\u00b9: (1) we test the tightness of our theoretical bounds on a simple regression problem on a synthetic dataset consisting of two weighted circulant graphs (see Figure 1 and Appendix D.1 for details) (2) we assess the transferability of the same model (Appendix D.2), and (3) we run experiments on a real-world dataset of a movie recommendation system where the information is summarized in two graphs via collaborative filtering approaches [19] and it is combined to infer ratings by new users via the GtNN model (see Figure 2 and Appendix D.3)."}, {"title": "Conclusions", "content": "In this paper, we introduce graph-tuple networks (GtNNs), a way of extending GNNs to a multimodal graph setting through the use tuples of non-commutative operators endowed with appropriate block-operator norms. We show that GtNNs have several desirable properties such as stability to perturbations and a universal transfer property on convergent graph-tuples, where the transferabil-"}]}