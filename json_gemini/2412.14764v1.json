{"title": "CodeRepoQA: A Large-scale Benchmark for Software Engineering Question Answering", "authors": ["Ruida Hu", "Chao Peng", "Jingyi Ren", "Bo Jiang", "Xiangxin Meng", "Qinyun Wu", "Pengfei Gao", "Xinchen Wang", "Cuiyun Gao"], "abstract": "In this work, we introduce CodeRepoQA, a large-scale benchmark specifically designed for evaluating repository-level question-answering capabilities in the field of software engineering. CodeRepoQA encompasses five programming languages and covers a wide range of scenarios, enabling comprehensive evaluation of language models. To construct this dataset, we crawl data from 30 well-known repositories in GitHub, the largest platform for hosting and collaborating on code, and carefully filter raw data. In total, CodeRepoQA is a multi-turn question-answering benchmark with 585,687 entries, covering a diverse array of software engineering scenarios, with an average of 6.62 dialogue turns per entry.\nWe evaluate ten popular large language models on our dataset and provide in-depth analysis. We find that LLMs still have limitations in question-answering capabilities in the field of software engineering, and medium-length contexts are more conducive to LLMs' performance. The entire benchmark is publicly available at https://github.com/kinesiatricssxilm14/CodeRepoQA.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are increasingly being integrated into tools such as chatbots and coding assistants to assist developers, showcasing their potential to solving various software engineer-ing tasks [7, 10]. As a result, the research community has begun exploring how LLMs can be further leveraged to assist with more complex repository-level tasks encountered in software development [1, 11, 12, 14, 15]. GitHub issues provide rich, real-world data that includes bug reports, feature requests, and usage questions, making them an ideal source for constructing diverse and realistic question-answering tasks for evaluating LLMs. Their collaborative nature and technical content allow models to be assessed on practical software development challenges.\nExisting question-answering (QA) benchmarks, such as MMLU [6], evaluate models in zero-shot and few-shot settings and assess them in answering question across 57 subjects spanning science, tech-nology and mathematics. CodeQA [9] contains a benchmark of 119,778 Java and 70,085 Python QA pairs. CS1QA [8] contains 9,237 question-answer pairs extracted from chat logs of introductory Python classes. CodeApex [2] evaluates LLMs on C++ code genera-tion and correction.\nAdditionally, these benchmarks do not focus on repository-level question-answering and cannot reflect the complexity of real-world scenarios. Specifically, developers often engage in multi-turn di-alogues to resolve issues. Multi-turn dialogues better reflect real-world conversations, as solving software engineering issues often requires multiple interactions [3]. These benchmarks are limited to"}, {"title": "2 Data Construction", "content": "We employ two steps to produce the final benchmark\u00b9: (1) Raw Data Crawling: we crawl and extract metadata related to issues from GitHub; (2) Data Filtering: we select and clean the gathered data to ensure its relevance and quality. After completing the above"}, {"title": "2.1 Raw Data Crawling", "content": "To ensure sufficient diversity within CodeRepoQA, we select 30 popular repositories from GitHub, encompassing five widely-used programming languages: Python, Java, JavaScript, TypeScript, and Go. To assure the quality of the benchmark, we select repositories with over 5,000 stars that are widely utilized within their respective fields. GitHub, as the largest platform for hosting and managing software projects, contains a wealth of metadata. The GitHub REST API [4] allows developers to interact with GitHub services through HTTP requests. We use the GitHub REST API to crawl all issues from 30 repositories on GitHub, totalling over 636,000 entries. To ensure the quality of CodeRepoQA, we conducted the following filtering."}, {"title": "2.2 Data Filtering", "content": "After completing the Raw Data Crawling process, we obtain approx-imately 636,000 issues. These issues are intended for constructing the QA benchmark. We apply a filtering process to ensure high-quality data automatically. We apply the following criteria to filter the issue entries:\nComment scale: Prioritize character count; remove entries un-der 200 characters or those exceeding 10MB to balance sufficient information and model input limits.\nRedundancy: According to the GitHub documentation [5], we exclude duplicate issues by detecting markers such as \"Duplicate of #\" to ensure unique and valuable entries.\nExternal check: Filter out conversations that contain external links, and only retain issues containing internal GitHub links to avoid complications from external content.\nEvent count: Influenced by methodologies from research like StarCoder [13], we exclude issues with more than ten events to avoid auto-generated text and robot-produced records, maintaining high-quality data.\nRobot detection: Identify and remove issues and responses created by robots through inspection of the author information and some common robot response patterns.\nParticipant count: Exclude issues with only one participant, after removing robot users, to ensure the benchmark contains gen-uine and effective interactions.\nAfter completing the data filtering process, we obtained 585,687 issue entries containing numerous real-world code-related QA pairs."}, {"title": "3 Experiment", "content": "The following experiments use filtered QA pairs as the benchmark. For our constructed benchmark, CodeRepoQA, we propose the following two Research Questions (RQs) based on the specified aspects:\nRQ1: How do models perform in answering questions?\nRQ2: How does the length of question affect the models' an-swering performance?"}, {"title": "3.1 Model Selection", "content": "We selected ten different large language models (LLMs) to evaluate their performance on coding tasks. These models are broadly used and represent some of the most advanced technologies available, in-cluding both commercial and open-source options. The commercial models include the GPT and Gemini (GM) series, such as GPT-40, GPT-4, Gemini-1.5-Flash, and Gemini-1.5-Pro. The open-source models include Mistral, CodeQwen (CQ), and the DeepSeek Coder (DSC) family, encompassing a range of model sizes. These include Mistral-large-2 (123B), CodeQwen-1.5-Chat (7B), DeepSeek-Coder-V2 (236B), DeepSeek-Coder-V2-Lite (16B), DeepSeek-Coder (33B), and DeepSeek-Coder (6.7B). To ensure the validity of all experi-ments, we strictly controlled the length of the input data to ensure it did not exceed the maximum input tokens allowed by the selected models."}, {"title": "3.2 Evaluation Design", "content": "To comprehensively assess the capability of LLMs in question-answering in software engineering scenarios, we designed a specific QA task. For this evaluation, we use the historical dialogue turns as input, ensuring that the model understands the context of the conversation. The last response from a repository maintainer (such as MEMBER, AUTHOR, or CONTRIBUTOR) is used as the ground truth. This approach allows us to accurately answer the LLMs' ability to answer the questions in software engineering scenarios."}, {"title": "3.3 Evaluation Metrics", "content": "We employ the following metrics to evaluate LLMs, including BLEU, ROUGE-L, ROUGE-1, and Edit Similarity (ES), all of which range from 0 to 1.\nBLEU: Measures the n-gram precision between the generated text and reference texts, incorporating a brevity penalty to discour-age overly concise outputs.\nROUGE-L and ROUGE-1: These metrics assess the quality of text generated by models compared to reference texts. ROUGE-L evaluates sequence-level similarity using the Longest Common Subsequence (LCS) method, while ROUGE-1 measures the overlap of single words (1-grams). The F1 score is the primary metric for a balanced evaluation of content accuracy and completeness.\nEdit Similarity: Evaluates the similarity between the gener-ated text and the reference text by comparing the number of edits required to transform one text into the other."}, {"title": "3.4 Performance of LLMs", "content": "We evaluate the performance of ten models listed in Section 3.1. The results for the evaluation are presented in the respective sections of Table 3. We can derive the following observations:\nCommercial models do not always outperform open-source ones in software engineering, nor do larger models consis-tently show superior performance. Although the two models from the Gemini series achieved the best overall performance, GPT-4, a famous commercial model, underperforms compared to open-source models like CodeQwen (7B). Additionally, the largest and newest model in the DeepSeek series, DeepSeek-Coder-V2, is out-performed by its smaller counterparts, 33B and 6.7B. This suggests that having more parameters does not necessarily lead to better per-formance, highlighting that larger models are not always superior across different tasks.\nLLMs still demonstrate limitations in software engineer-ing QA scenarios. Even with Gemini-1.5-Pro, which has the best overall performance, there is a significant gap between its output and the actual feedback. The combined score across four metrics is only 0.1826, with a BLEU score of 0.1208. Specifically, Gemini-1.5-Pro achieved the highest scores in ROUGE-L (0.1551) and ROUGE-1 (0.2470). This indicates that the vocabulary overlap between the generated text and real-world responses is relatively low, suggest-ing that the generated content may not effectively capture key information from the actual responses."}, {"title": "3.5 Impact of Question Length on LLMs' Performance", "content": "To investigate the impact of question length on performance of LLMs, we equally divide the benchmark into five groups based on the question length. We then calculate the scores for each group across four metrics. The detailed results are shown in Table 4.\nLLMs perform better on questions of medium length in software engineering QA scenarios. Based on the data in Table 4, it is evident that LLMs perform better on questions of medium length. The 40% context length group achieves the highest scores in BLEU (0.1196), ROUGE-1 (0.2348), and Edit Similarity (0.1806), and maintains the highest average score (0.1698) among all groups. Similarly, the 60% context length group also performs well, particu-larly in BLEU, ROUGE-L, and ROUGE-1 metrics, with an average score of 0.1589. In contrast, the shortest (20%) and longest (100%) context length groups show relatively lower performance across most metrics, indicating that extremely short and long contexts are less effective. These findings suggest that medium-length contexts are more conducive to generating high-quality answers from LLMs in software engineering QA scenarios."}, {"title": "4 Conclusion", "content": "In this paper, we present CodeRepoQA, a large-scale benchmark for assessing the question-answering capabilities of LLMs in the field of software engineering. CodeRepoQA is a multi-turn question-answering benchmark, containing 585,687 entries from 30 well-known GitHub repositories and covers five distinct programming languages. CodeRepoQA provides a comprehensive evaluation of LLMs' abilities in answering questions of repository-level compared to previous benchmarks. Our experiments show that LLMs still demonstrate limitations in software engineering QA scenarios, and medium-length questions are found to yield better performance in generating high-quality answers. Our benchmark and detailed description are available at: https://github.com/kinesiatricssxilm14/CodeRepoQA."}]}