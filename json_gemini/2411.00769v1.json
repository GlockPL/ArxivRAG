{"title": "GAMEGEN-X: INTERACTIVE OPEN-WORLD GAME VIDEO GENERATION", "authors": ["Haoxuan Che", "Xuanhua He", "Quande Liu", "Cheng Jin", "Hao Chen"], "abstract": "We introduce GameGen-X, the first diffusion transformer model specifically designed for both generating and interactively controlling open-world game videos. This model facilitates high-quality, open-domain generation by simulating an extensive array of game engine features, such as innovative characters, dynamic environments, complex actions, and diverse events. Additionally, it provides interactive controllability, predicting and altering future content based on the current clip, thus allowing for gameplay simulation. To realize this vision, we first collected and built an Open-World Video Game Dataset (OGameData) from scratch. It is the first and largest dataset for open-world game video generation and control, which comprises over one million diverse gameplay video clips sampling from over 150 games with informative captions from GPT-40. GameGen-X undergoes a two-stage training process, consisting of foundation model pre-training and instruction tuning. Firstly, the model was pre-trained via text-to-video generation and video continuation, endowing it with the capability for long-sequence, high-quality open-domain game video generation. Further, to achieve interactive controllability, we designed InstructNet to incorporate game-related multi-modal control signal experts. This allows the model to adjust latent representations based on user inputs, unifying character interaction and scene content control for the first time in video generation. During instruction tuning, only the InstructNet is updated while the pre-trained foundation model is frozen, enabling the integration of interactive controllability without loss of diversity and quality of generated video content. GameGen-X represents a significant leap forward in open-world video game design using generative models. It demonstrates the potential of generative models to serve as auxiliary tools to traditional rendering techniques, effectively merging creative generation with interactive capabilities. The project will be available at https://gamegen-x.github.com/.", "sections": [{"title": "INTRODUCTION", "content": "Generative models (Croitoru et al. (2023); Ramesh et al. (2022); Tim Brooks & Ramesh (2024); Rombach et al. (2022b)) have made remarkable progress in generating images or videos conditioned on multi-modal inputs such as text, images, and videos. These advancements have benefited content creation in design, advertising, animation, and film by reducing costs and effort. Inspired by the success of generative models in these creative fields, it is natural to explore their application in the modern game industry. This exploration is particularly important because developing open-world video game prototypes is a resource-intensive and costly endeavor, requiring substantial investment in concept design, asset creation, programming, and preliminary testing (Anastasia (2023)). Even early development stages of games still involved months of intensive work by small teams to build functional prototypes showcasing the game's potential (Wikipedia (2023)).\nSeveral pioneering works, such as World Model (Ha & Schmidhuber (2018)), GameGAN (Kim et al. (2020)), R2PLAY (Jin et al. (2024)), Genie (Bruce et al. (2024)), and GameNGen (Valevski et al. (2024)), have explored the potential of neural models to simulate or play video games. They have primarily focused on 2D games like \u201cPac-Man\", \"Super Mario\u201d, and early 3D games such as \u201cDOOM (1993)\u201d. Impressively, they demonstrated the feasibility of simulating interactive game environments. However, the generation of novel, complex open-world game content remains an open problem. A key difficulty lies in generating novel and coherent next-gen game content. Open-world games feature intricate environments, dynamic events, diverse characters, and complex actions that are far more challenging to generate (Eberly (2006)). Further, ensuring interactive controllability, where the generated content responds meaningfully to user inputs, remains a formidable challenge. Addressing these challenges is crucial for advancing the use of generative models in game content design and development. Moreover, successfully simulating and generating these games would also be meaningful for generative models, as they strive for highly realistic environments and interactions, which in turn may approach real-world simulation (Zhu et al. (2024)).\nIn this work, we provide an initial answer to the question: Can a diffusion model generate and control high-quality, complex open-world video game content? Specifically, we introduce GameGen-X, the first diffusion transformer model capable of both generating and simulating open-world video games with interactive control. GameGen-X sets a new benchmark by excelling at generating diverse and creative game content, including dynamic environments, varied characters, engaging events, and complex actions. Moreover, GameGen-X enables interactive control within generative models, allowing users to influence the generated content and unifying character interaction and scene content control for the first time. It initially generates a video clip to set up the environment and characters. Subsequently, it produces video clips that dynamically respond to user inputs by leveraging the current video clip and multimodal user control signals. This process can be seen as simulating a game-like experience where both the environment and characters evolve dynamically.\nGameGen-X undergoes a two-stage training: foundation model pre-training and instruction tuning. In the first stage, the foundation model is pre-trained on OGameData using text-to-video generation and video continuation tasks. This enables the model to learn a broad range of open-world game dynamics and generate high-quality game content. In the second stage, InstructNet is designed to enable multi-modal interactive controllability. The foundation model is frozen, and InstructNet is trained to map user inputs\u2014such as structured text instructions for game environment dynamics and keyboard controls for character movements and actions\u2014onto the generated game content. This allows GameGen-X to generate coherent and controllable video clips that evolve based on the player's inputs, simulating an interactive gaming experience. To facilitate this development, we constructed Open-World Video Game Dataset (OGameData), the first large-scale dataset for game video generation and control. This dataset was collected from over 150 next-generation games and was built by using a human-in-the-loop proprietary data pipeline that involves scoring, filtering, sorting, and structural captioning. OGameData contains one million video clips from two subsets including OGameData-GEN and OGameData-INS, providing the foundation for training generative models capable of producing realistic game content and achieving interactive control, respectively.\nIn summary, GameGen-X offers a novel approach for interactive open-world game video generation, where complex game content is generated and controlled interactively. It lays the foundation for a new potential paradigm in game content design and development. While challenges for practical application remain, GameGen-X demonstrates the potential for generative models to serve as a scalable and efficient auxiliary tool to traditional game design methods. Our main contributions are summarized as follows: 1) We developed OGameData, the first comprehensive dataset specifically curated for open-world game video generation and interactive control, which contains one million video-text pairs. It is collected from 150+ next-gen games, and empowered by GPT-40. 2) We introduced GameGen-X, the first generative model for open-world game game content, combining a foundation model with the InstructNet. GameGen-X utilizes a two-stage training strategy, with the foundation model and InstructNet trained separately to ensure stable, high-quality content generation and control. InstructNet provides multi-modal interactive controllability, allowing players to influence the continuation of generated content, simulating gameplay. 3) We conducted extensive experiments comparing our model's generative and interactive control abilities to other open-source and commercial models. Results show that our approach excels in high-quality game content generation and offers superior control over the environment and character.\""}, {"title": "OGAMEDATA: LARGE-SCALE FINE-GRAINED GAME DOMAIN DATASET", "content": "OGameData is the first dataset designed for open-world game video generation and interactive control. As shown in Table 1, OGameData excels in fine-grained annotations, offering a structural caption with high text density for video clips per minute. It is meticulously designed for game video by offering game-specific knowledge and incorporating elements such as game names, player perspectives, and character details. It comprises two parts: the generation dataset (OGameData-GEN) and the instruction dataset (OGameData-INS). The resulting OGameData-GEN is tailored for training the generative foundation model, while OGameData-INS is optimized for instruction tuning and interactive control tasks. The construction details, dataset analysis, and captioning prompt designs are in Appendix B."}, {"title": "DATASET CONSTRUCTION PIPELINE", "content": "As illustrated in Figure 2, we developed a robust data processing pipeline encompassing collection, cleaning, segmentation, filtering, and structured caption annotation. This process integrates both AI and human expertise, as automated techniques alone are insufficient due to domain-specific intricacies present in various games.\nData Collection and Filtering. We gathered content from more than 150 next-gen games and game engine direct outputs to ensure diversity. These data are from the internet and local game engines, specifically focusing on gameplay footage that minimizes UI elements. Despite the rigorous collection, some low-quality videos were included, and these videos lacked essential metadata like game name, genre, and player perspective. Low-quality videos were manually filtered out, with human experts ensuring the integrity of the metadata, such as game genre and player perspective. To prepare videos for clip segmentation, we used PyScene and TransNetV2 (Sou\u010dek & Loko\u010d (2020)) to detect scene changes, discarding clips shorter than 4 seconds and splitting longer clips into 16-second segments. To filter and annotate clips, we sequentially employed models: CLIP-AVA (Schuhmann (2023)) for aesthetic scoring, UniMatch (Xu et al. (2023)) for motion filtering, VideoCLIP (Xu et al. (2021)) for content similarity, and CoTrackerV2 (Karaev et al. (2023)) for camera motion.\nStructured Text Captioning. The OGameData supports the training of two key functionalities: text-to-video generation and interactive control. These tasks require distinct captioning strategies. For OGameData-GEN, detailed captions are crafted to describe the game metadata, scene context, and key characters, ensuring comprehensive textual descriptions for the generative model foundation training. In contrast, OGameData-INS focuses on describing the changes in game scenes for interactive generation, using concise instruction-based captions that highlight differences between initial and subsequent frames. This structured captioning approach enables precise and fine-grained generation and control, allowing the model to modify specific elements while preserving the scene."}, {"title": "DATASET SUMMARY", "content": "As depicted in Table 1, OGameData comprises 1 million high-resolution video clips, derived from sources spanning minutes to hours. Compared to other domain-specific datasets (Caba Heilbron et al. (2015); Zhou et al. (2018); Sanabria et al. (2018); Anne Hendricks et al. (2017)), OGameData stands out for its scale, diversity, and richness of text-video pairs. Even compared with the latest open-domain generation dataset Miradata (Ju et al. (2024)), our dataset still has the advantage of providing more fine-grained annotations, which feature the most extensive captions per unit of time. This dataset features several key characteristics: OGameData features highly fine-grained text and boasts a large number of trainable video-text pairs, enhancing text-video alignment in model training. Additionally, it comprises two subsets-generation and control-supporting both types of training tasks. The dataset's high quality is ensured by meticulous curation from over 10 human experts. Each video clip is accompanied by captions generated using GPT-40, maintaining clarity and coherence and ensuring the dataset remains free of UI and visual artifacts. Critical to its design, OGameData is tailored specifically for the gaming domain. It effectively excludes non-gameplay scenes, incorporating a diverse array of game styles while preserving authentic in-game camera perspectives. This specialization ensures the dataset accurately represents real gaming experiences, maintaining high domain-specific relevance."}, {"title": "GAMEGEN-X", "content": "GameGen-X is the first generative diffusion model that learns to generate open-world game videos and interactively control the environments and characters in them. The overall framework is illustrated in Fig 3. In section 3.1, we introduce the problem formulation. In section 3.2, we discuss the design and training of the foundation model, which facilitates both initial game content generation and video continuation. In section 3.3, we delve into the design of InstructNet and explain the process of instruction tuning, which enables clip-level interactive control over generated content."}, {"title": "GAME VIDEO GENERATION AND INTERACTION", "content": "The primary objective of GameGen-X is to generate dynamic game content where both the virtual environment and characters are synthesized from textual descriptions, and users can further influence the generated content through interactive controls. Given a textual description T that specifies the initial game scene\u2014including characters, environments, and corresponding actions and events\u2014we aim to generate a video sequence V = {V_i}_{i=1}^N that brings this scene to life. We model the conditional distribution: p(V_{1:N} | T, C_{1:N}), where C_{1:N} represents the sequence of multi-modal control inputs provided by the user over time. These control inputs allow users to manipulate character movements and scene dynamics, simulating an interactive gaming experience.\nOur approach integrates two main components: 1) Foundation Model: It generates an initial video clip based on T, capturing the specified game elements including characters and environments. 2) InstructNet: It enables the controllable continuation of the video clip by incorporating user-provided control inputs. By unifying text-to-video generation with interactive controllable video continuation, our approach synthesizes game-like video sequences where the content evolves in response to user interactions. Users can influence the generated video at each generation step by providing control inputs, allowing for manipulation of the narrative and visual aspects of the content."}, {"title": "FOUNDATION MODEL TRAINING FOR GENERATION", "content": "Video Clip Compression. To address the redundancy in temporal and spatial information (Lab & etc. (2024)), we introduce a 3D Spatio-Temporal Variational Autoencoder (3D-VAE) to compress video clips into latent representations. This compression enables efficient training on high-resolution videos with longer frame sequences. Let V \\in \\mathbb{R}^{F\\times C\\times H\\times W} denote a video clip, where F is the number of frames, H and W are the height and width of each frame, and C is the number of channels. The encoder \\mathcal{E} compresses V into a latent representation z = \\mathcal{E}(V) \\in \\mathbb{R}^{F'\\times C'\\times H'\\times W'}, where F' = F/s_f, H' = H/s_h, W' = W/s_w, and C' is the number of latent channels. Here, s_f, s_h, and s_w are the temporal and spatial downsampling factors. Specifically, 3D-VAE first performs the spatial downsampling to obtain frame-level latent features. Further, it conducts temporal compression to capture temporal dependencies and reduce redundancy over frame effectively, inspired by Yu et al. (2023). By processing the video clip through the 3D-VAE, we can obtain a latent tensor z of spatial-temporally informative and reduced dimensions. Such z can support long video and high-resolution model training, which meets the requirements of game content generation."}, {"title": "Masked Spatial-Temporal Diffusion Transformer", "content": "GameGen-X introduces a Masked Spatial-Temporal Diffusion Transformer (MSDiT). Specifically, MSDiT combines spatial attention, temporal attention, and cross-attention mechanisms (Vaswani (2017)) to effectively generate game videos guided by text prompts. For each time step t, the model processes latent features z_t that capture frame details. Spatial attention enhances intra-frame relationships by applying self-attention over spatial dimensions (H', W'). Temporal attention ensures coherence across frames by operating over the time dimension F', capturing inter-frame dependencies. Cross-attention integrates guidance of external text features f obtained via T5 (Raffel et al. (2020)), aligning video generation with the semantic information from text prompts. As shown in Fig. 4, we adopt the design of stacking paired spatial and temporal blocks, where each block is equipped with cross-attention and one of spatial or temporal attention. Such design allows the model to capture spatial details, temporal dynamics, and textual guidance simultaneously, enabling GameGen-X to generate high-fidelity, temporally consistent videos that are closely aligned with the provided text prompts.\nAdditionally, we introduce a masking mechanism that excludes certain frames from noise addition and denoising during diffusion processing. A masking function M(i) over frame indices i \\in \\mathcal{I} is defined as: M(i) = 1 if i > x, and M(i) = 0 if i < x, where x is the number of context frames provided for video continuation. The noisy latent representation at time step t is computed as: \\tilde{z}_t = (1-M(\\mathcal{I})) \\odot z + M(\\mathcal{I}) \\odot \\epsilon_t, where \\epsilon_t \\sim \\mathcal{N}(0, \\mathcal{I}) is Gaussian noise of the same dimensions as z, and \\odot denotes element-wise multiplication. Such a masking strategy provides the support of training both text-to-video and video continuation into one foundation model."}, {"title": "Unified Video Generation and Continuation", "content": "By integrating the text-to-video diffusion training logic with the masking mechanism, GameGen-X effectively handles both video generation and continuation tasks within a unified framework. This strategy aims to enhance the simulation experience by enabling temporal continuity, catering to an extended and immersive gameplay experience. Specifically, for text-to-video generation, where no initial frames are provided, we set x = 0, and the masking function becomes M(i) = 1 for all frames i. The model learns the conditional distribution p(V | T), where T is the text prompt. The diffusion process is applied to all frames, and the model generates video content solely based on the text prompt. For video continuation, initial frames V_{0:x} are provided as context. The masking mechanism ensures that these frames remain unchanged during the diffusion process, as M(i) = 0 for i < x. The model focuses on generating the subsequent frames V_{x+1:N} by learning the conditional distribution p(V_{x+1:N} | V_{1:x}, T). This allows the model to produce video continuations that are consistent with both the preceding context and the text prompt. Additionally, during the diffusion training (Song et al. (2020a;b); Ho et al. (2020); Rombach et al. (2022a)), we incorporated the bucket training (Zheng et al. (2024b), classifier-free diffusion guidance (Ho & Salimans (2021)) and rectified flow (Liu et al. (2023b)) for better generation performance. Overall, this unified training approach enhances the ability to generate complex, contextually relevant open-world game videos while ensuring smooth transitions and continuations."}, {"title": "INSTRUCTION TUNING FOR INTERACTIVE CONTROL", "content": "InstructNet Design. To enable interactive controllability in video generation, we propose InstructNet, designed to guide the foundation model's predictions based on user inputs, allowing for control of the generated content. The core concept is that the generation capability is provided by the foundation model, with InstructNet subtly adjusting the predicted content using user input signals. Given the high requirement for visual continuity in-game content, our approach aims to minimize abrupt changes, ensuring a seamless experience. Specifically, the primary purpose of InstructNet is to modify future predictions based on instructions. When no user input signal is given, the video extends naturally. Therefore, we keep the parameters of the pre-trained foundation model frozen, which preserves its inherent generation and continuation abilities. Meanwhile, the additional trainable InstructNet is introduced to handle control signals. As shown in Fig. 4, InstructNet modifies the generation process by incorporating control signals via the operation fusion expert layer and instruction fusion expert layer. This component comprises N InstructNet blocks, each utilizing a specialized Operation Fusion Expert Layer and an Instruct Fusion Expert Layer to integrate different conditions. The output features are injected into the foundation model to fuse the original latent, modulating the latent representations based on user inputs and effectively aligning the output with user intent. This enables users to influence character movements and scene dynamics. InstructNet is primarily trained through video continuation to simulate the control and feedback mechanism in gameplay. Further, Gaussian noise is subtly added to initial frames to mitigate error accumulation."}, {"title": "EXPERIMENTS", "content": ""}, {"title": "QUANTITATIVE RESULTS", "content": "Metrics. To comprehensively evaluate the performance of GameGen-X, we utilize a suite of metrics that capture various aspects of video generation quality and interactive control, following Huang et al. (2024b) and Yang et al. (2024). These metrics include Fr\u00e9chet Inception Distance (FID), Fr\u00e9chet Video Distance (FVD), Text-Video Alignment (TVA), User Preference (UP), Motion Smoothness (MS), Dynamic Degrees (DD), Subject Consistency (SC), and Imaging Quality (IQ). It's worth noting that the TVA and UP are subjective scores that indicate whether the generation meets the requirements of humans, following Yang et al. (2024). By employing this comprehensive set of metrics, we aim to thoroughly evaluate model capabilities in generating high-quality, realistic, and interactively controllable video game content. Readers can find experimental settings and metric introductions in Appendix C.1."}, {"title": "Generation and Control Ability Evaluation", "content": "As shown in Table 2, we compared GameGen-X against four well-known open-source models, i.e., Mira (Zhang et al. (2023)), OpenSora-Plan1.2 (Lab & etc. (2024)), OpenSora1.2 (Zheng et al. (2024b)) and CogVideoX-5B (Yang et al. (2024)) to evaluate its generation capabilities. Notably, both Mira and OpenSora1.2 explicitly mention training on game data, while the other two models, although not specifically designed for this purpose, can still fulfill certain generation needs within similar contexts. Our evaluation showed that GameGen-X performed well on metrics such as FID, FVD, TVA, MS, and SC. It implies GameGen-X's strengths in generating high-quality and coherent video game content while maintaining competitive visual and technical quality. Further, we investigated the control ability of these models, as shown in Table 3. We used conditioned video clips and dense prompts to evaluate the model generation response. For GameGen-X, we employed instruct prompts to generate video clips. Beyond the aforementioned metrics, we introduced the Success Rate (SR) to measure how often the models respond accurately to control signals. This is evaluated by both human experts and Pllava (Xu et al. (2024)). The SR metric is divided into two parts: SR for Character Actions (SR-C), which assesses the model's responsiveness to character movements, and SR for Environment Events (SR-E), which evaluates the model's handling of changes in weather, lighting, and objects. As demonstrated, GameGen-X exhibits superior control ability compared to other models, highlighting its effectiveness in generating contextually appropriate and interactive game content. Since IQ metrics favor models trained on natural scene datasets, such models score higher. In generation performance, CogVideo's 8fps videos and OpenSora 1.2's frequent scene changes result in higher DD."}, {"title": "Ablation Study", "content": "As shown in Table 4, we investigated the influence of various data strategies, including leveraging MiraData (Ju et al. (2024)), short captions (Chen et al. (2024)), and progression training (Lab & etc. (2024)). The results indicated that our data strategy outperforms the others, particularly in terms of semantic consistency, distribution alignment, and user preference. The visual quality metrics are comparable across all strategies. This consistency implies that visual quality metrics may be less sensitive to these strategies or that they might be limited in evaluating game domain generation. Further, as shown in Table 5, we explored the effects of our design on interactive control ability through ablation studies. This experiment involved evaluating the impact of removing key components such as InstructNet, Instruct Captions, or the decomposition process. The results demonstrate that the absence of InstructNet significantly reduces the SR and UP, highlighting its crucial role in user-preference interactive controllability. Similarly, the removal of Instruct Captions and the decomposition process also negatively impacts control metrics, although to a lesser extent. These findings underscore the importance of each component in enhancing the model's ability to generate and control game content interactively."}, {"title": "QUALITATIVE RESULTS", "content": "Generation Functionality. Fig. 5 illustrates the basic generation capabilities of our model in generating a variety of characters, environments, actions, and events. The examples show that the model can create characters such as assassins and mages, simulate environments such as Sakura forests and rainforests, execute complex actions like flying and driving, and reproduce environmental events like snowstorms and heavy rain. This demonstrates the model's ability to generate and control diverse scenarios, highlighting its potential application in generating open-world game videos.\nInteractive Control Ability. As shown in Fig. 6, our model demonstrates the capability to control both environmental events and character actions based on textual instructions and keyboard inputs. In the example provided, the model effectively manipulates various aspects of the scene, such as lighting conditions and atmospheric effects, highlighting its ability to simulate different times of day and weather conditions. Additionally, the character's movements, primarily involving navigation through the environment, are precisely controlled through input keyboard signals. This interactive control mechanism enables the simulation of a dynamic gameplay experience. By adjusting environmental factors like lighting and atmosphere, the model provides a realistic and immersive setting. Simultaneously, the ability to manage character movements ensures that the generated content responds intuitively to user interactions. Through these capabilities, our model showcases its potential to enhance the realism and engagement of open-world video game simulations.\nOpen-domain Generation, Gameplay Simulation and Further Analysis. As shown in Fig., we presented initial qualitative experiment results, where GameGen-X generates novel domain game video clips and interactively controls them, which can be seen as a game simulation. Further, we compared GameGen-X with other open-source models in the open-domain generation ability as shown in Fig. 7. All the open-source models can generate some game-like content, implying their training involves corresponding game source data. As expected, the GameGen-X can better meet the game content requirements in character details, visual environments, and camera logic, owing to the strict dataset collection and building of OGameData. Further, we compared GameGen-X with other commercial products including Kling, Pika, Runway, Luma, and Tongyi, as shown in Fig. 8. In the left part, i.e., the initially generated video clip, only Pika, Kling1.5, and GameGen-X correctly followed the text description. Other models either failed to display the character or depicted them entering the cave instead of exiting. In the right part, both GameGen-X and Kling1.5 successfully guided the character out of the cave. GameGen-X achieved high-quality control response as well as maintaining a consistent camera logic, obeying the game-like experience at the same time. This is owing to the design of a holistic training framework and InstructNet."}, {"title": "CONCLUSION", "content": "We have presented GameGen-X, the first diffusion transformer model with multi-modal interactive control capabilities, specifically designed for generating open-world game videos. By simulating key elements such as dynamic environments, complex characters, and interactive gameplay, GameGen-X sets a new benchmark in the field, demonstrating the potential of generative models in both generating and controlling game content. The development of the OGameData provided a crucial foundation for our model's training, enabling it to capture the diverse and intricate nature of open-world games. Through a two-stage training process, GameGen-X achieved a mutual enhancement between content generation and interactive control, allowing for a rich and immersive simulation experience. Beyond its technical contributions, GameGen-X opens up new horizons for the future of game content design. It suggests a potential shift towards more automated, data-driven methods that significantly reduce the manual effort required in early-stage game content creation. By leveraging models to create immersive worlds and interactive gameplay, we may move closer to a future where game engines are more attuned to creative, user-guided experiences. While challenges remain, GameGen-X represents an initial yet significant leap forward toward a novel paradigm in game design. It lays the groundwork for future research and development, paving the way for generative models to become integral tools in creating the next generation of interactive digital worlds."}, {"title": "RELATED WORKS", "content": ""}, {"title": "VIDEO DIFFUSION MODELS", "content": "The advent of diffusion models, particularly latent diffusion models, has significantly advanced image generation, inspiring researchers to extend their applicability to video generation (Liu et al. (2023a; 2024)). This field can be broadly categorized into two approaches: image-to-video and text-to-video generation. The former involves transforming a static image into a dynamic video, while the latter generates videos based solely on textual descriptions, without any input images. Pioneering methods in this domain include AnimateDiff (Guo et al. (2023)), Dynamicrafter (Xing et al. (2023)), Modelscope (Wang et al. (2023a)), AnimateAnything (Dai et al. (2023)), and Stable Video Diffusion (Rombach et al. (2022b)). These techniques typically leverage pre-trained text-to-image models, integrating them with various temporal mixing layers to handle the temporal dimension inherent in video data. However, the traditional U-Net based framework encounters scalability issues, limiting its ability to produce high-quality videos. The success of transformers in the natural language processing community and their scalability has prompted researchers to adapt this architecture for diffusion models, resulting in the development of DiTs (Peebles & Xie (2023). Subsequent work, such as Sora (Tim Brooks & Ramesh (2024)), has demonstrated the powerful capabilities of DiTs in video generation tasks. Open-source implementations like Latte (Ma et al. (2024)), Opensora (Zheng et al. (2024b), and Opensora-Plan (Lab & etc. (2024)) have further validated the superior performance of DiT-based models over traditional U-Net structures in both text-to-video and image-to-video generation. Despite these advancements, the exploration of gaming video generation and its interactive controllability remains under-explored."}, {"title": "GAME SIMULATION AND INTERACTION", "content": "Several pioneering works have attempted to train models for game simulation with action inputs. For example, UniSim (Yang et al. (2023)) and Pandora (Xiang et al. (2024)) built a diverse dataset of real-world and simulated videos and could predict a continuation video given a previous video segment and an action prompt via a supervised learning paradigm, while PVG (Menapace et al. (2021)) and Genie (Bruce et al. (2024)) focused on unsupervised learning of actions from videos. Similar to our work, GameGAN (Kim et al. (2020)), GameNGen (Valevski et al. (2024)), and DIAMOND (Alonso et al. (2024)) focused on the playable simulation of early games such as Atari and DOOM, and demonstrates its combination with a gaming agent for interaction (Zheng et al. (2024a)). However, they didn't explore the potential of generative models in simulating the complex environments of next-generation games. Instead, GameGen-X can create intricate environments, dynamic events, diverse characters, and complex actions with a high degree of realism and variety. Additionally, GameGen-X allows the model to generate subsequent frames based on the current video segment and player-provided multi-modal control signals. This approach ensures that the generated content is not only visually compelling but also contextually appropriate and responsive to player actions, bridging the gap between simple game simulations and the sophisticated requirements of next-generation open-world games."}, {"title": "DATASET", "content": ""}, {"title": "CONSTRUCTION DETAILS", "content": "Data Collection. Following Ju et al. (2024), we selected online video websites and local game engines as one of our primary video sources. Prior research predominantly focused on collecting game cutscenes and gameplay videos containing UI elements. Such videos are not ideal for training a game video generation model due to the presence of UI elements and non-playable content. In contrast, our method adheres to the following principles: 1) We exclusively collect videos showcasing playable content, as our goal is to generate actual gameplay videos rather than cutscenes or CG animations. 2) We ensure that the videos are high-quality and devoid of any UI elements. To achieve this, we only include high-quality games released post-2015 and capture some game footage directly from game engines to enhance diversity. Additionally, we record the gameplay videos locally, to collect the keyboard control signals. Following the data collection stage, we collected 32,000 videos from more than 150 next-generation video games."}, {"title": "Video-level Selection and Annotation", "content": "Despite our rigorous data collection process, some low-quality videos inevitably collected into our dataset. Additionally, the collected videos lack essential metadata such as game name, genre, and player perspective. This metadata is challenging to annotate using AI alone. Therefore, we employed human game experts to filter and annotate the videos. In this stage, human experts manually review each video, removing those with UI elements or non-playable content. For the remaining usable videos, they annotate critical metadata, including game name, genre (e.g., ACT, FPS, RPG), and player perspective (First-person, Third-person). After this filtering and annotation phase, we curated a dataset of 15,000 high-quality videos complete with game metadata."}, {"title": "Scene Detection and Segmentation", "content": "The collected videos, ranging from several minutes to hours, are unsuitable for model training due to their extended duration and numerous scene changes. We employed TransNetV2 (Sou\u010dek & Loko\u010d (2020)) and PyScene for scene segmentation, which can adaptively identify scene change timestamps within videos. Upon obtaining these timestamps, we discard video clips shorter than 4 seconds, considering them too brief. For clips longer than 16 seconds, we divide them into multiple 16-second segments, discarding any remainder shorter than 4 seconds. Following this scene segmentation stage, we obtained around 1,000,000 video clips, each containing 4-16 seconds of content at 24 frames per second."}, {"title": "Clips-level Filtering and Annotation", "content": "Some clips contain game menus, maps, black screens, low-quality scenes, or nearly static scenes, necessitating further data cleaning. Given the vast number of clips, manual inspection is impractical. Instead, we sequentially employed an aesthetic scoring model, a flow scoring model, the video CLIP model, and a camera motion model for filtering and annotation. First, we used the CLIP-AVA model (Schuhmann (2023)) to score each clip aesthetically. We then randomly sampled 100 clips to manually determine a threshold, filtering out clips with aesthetic scores below this threshold. Next, we applied the UniMatch model (Xu et al. (2023)) to filter out clips with either excessive or minimal motion. To address redundancy, we used the video-CLIP (Xu et al. (2021)) model to calculate content similarity within clips from the same game, removing overly similar clips. Finally, we utilized CoTrackerV2 (Karaev et al. (2023)) to annotate clips with camera motion information, such as \"pan-left\" or \"zoom-in.\""}, {"title": "Structural Caption", "content": "We propose a Structural captioning approach for generating captions for OGameData-GEN and OGameData-INS. To achieve this, we uniformly sample 8 frames from each video and stack them into a single image. Using this image as a representation of the video's content, we designed two specific prompts to instruct GPT-40"}]}