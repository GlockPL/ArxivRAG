{"title": "A Survey of Mamba", "authors": ["Haohao Qu", "Liangbo Ning", "Rui An", "Wenqi Fan", "Tyler Derr", "Xin Xu", "Qing Li"], "abstract": "Deep learning, as a vital technique, has sparked a notable revolution in artificial intelligence (AI), resulting in a great change in human lifestyles. As the most representative architecture, Transformers have empowered numerous advanced models, especially the large language models (LLMs) that comprise billions of parameters, becoming a cornerstone in deep learning. Despite the impressive achievements, Transformers still face inherent limitations, particularly the time-consuming inference resulting from the quadratic computation complexity of attention calculation. Recently, a novel architecture named Mamba, drawing inspiration from classical state space models, has emerged as a promising alternative for building foundation models, delivering comparable modeling abilities to Transformers while preserving near-linear scalability concerning sequence length. This has sparked an increasing number of studies actively exploring Mamba's potential to achieve impressive performance across diverse domains. Given such rapid evolution, there is a critical need for a systematic review that consolidates existing Mamba-empowered models, offering a comprehensive understanding of this emerging model architecture. In this survey, we therefore conduct an in-depth investigation of recent Mamba-associated studies, covering from three main aspects: the advancements of Mamba-based models, the techniques of adapting Mamba to diverse data, and the applications where Mamba can excel. Specifically, we first recall the foundational knowledge of various representative deep learning models and the details of Mamba-1&2 as preliminaries. Then, to showcase the significance of Mamba for AI, we comprehensively review the related studies focusing on Mamba models' architecture design, data adaptability, and applications. Finally, we present an discussion of current limitations and explore various promising research directions to provide deeper insights for future investigations.", "sections": [{"title": "1 Introduction", "content": "Over the past two decades, deep learning, as the most prominent artificial intelligence (AI) technique, has brought about a revolution in various domains such as healthcare [88], autonomous systems [37, 60], recommender systems [104, 228], and financial services [144, 216]. This period has witnessed the emergence of numerous deep neural networks (DNNs) that have significantly altered human lifestyles, offering immense convenience to individuals. One notable example is U-Net [151, 164], a robust deep learning model within the field of vision, which is extensively employed in medical imaging for the examination of radiology scans like MRI and CT scans. Its application assists in the identification and diagnosis of diseases, showcasing its effectiveness in this critical healthcare domain [112, 191]. Moreover, Graph Neural Networks (GNNs) are employed in handling graph-structure data to support smart services, such as recommender systems that suggest personalized content, products, or services to users [41, 42, 194]. Furthermore, Recurrent Neural Networks (RNNs) are extensively adopted in machine translation due to their ability to capture the sequential and contextual information essential for accurate translations [119, 167], empowering individuals from diverse linguistic backgrounds to effectively communicate and comprehend each other's ideas, opinions, and information.\nAmong the various deep learning architecture, Transformers have recently stood out and established its dominance across a broad spectrum of applications [34, 175]. For instance, as the most representative large foundation models, large language models (LLMs) like ChatGPT are fundamentally built on the Transformer architecture [2, 146, 228]. By scaling their model sizes to billions and training on a mix of diverse data sources, these Transformer-based models have demonstrated human-level intelligence with its impressive capabilities in language understanding, common sense reasoning, and in-content-learning [44, 217]. This remarkable success is bolstered by the attention mechanism [174], which enables the Transformer-based models to concentrate on relevant parts of the input sequence and facilitate better contextual understanding. However, the attention mechanism also introduces a significant computational overhead that increases quadratically with the input size [124, 233], which presents challenges in processing lengthy inputs. For example, the rapid growth in computational cost makes Transformers impractical or infeasible to process substantial sequences, thereby limiting their applicability in tasks like document-level machine translation [131] or long document summarization [94].\nRecently, a promising architecture, structured state space sequence models (SSMs) [58], have emerged to efficiently capture complex dependencies in sequential data, becoming a formidable competitor to Transformer. These models, inspired by classical state space models [89], can be considered as a fusion of recurrent neural networks and convolutional neural networks. They can be computed efficiently using either recurrence or convolution operations, achieving linear or near-linear scaling with sequence length, thus significantly reducing the computational costs. More specifically, as one of the most successful SSM variants, Mamba achieves comparable modeling capabilities to Transformers while maintaining linear scalability with sequence length [55], propelling it into the realm of focal topics. Mamba first introduces a simple yet effective selection mechanism that enables the model to filter out irrelevant information while retaining necessary and relevant data indefinitely by parameterizing the SSM parameters based on the input. Then, Mamba proposes a hardware-aware algorithm to compute the model recurrently with a scan instead of convolution, achieving up to 3xfaster computation on A100 GPUs. As shown in Figure 1, the powerful modeling capabilities for complex and lengthy sequential data, along with near-linear scalability, position Mamba as an emerging foundation model, poised to revolutionize various domains of research and applications, such as computer vision [199, 234], natural language processing [111, 226], healthcare [152, 179, 198], etc. For example, Zhu et al. [234] propose Vim, which is 2.8\u00d7faster than DeiT [173] and saves 86.8% GPU memory when extracting features for high-resolution images. Dao and\nGu [29] show the connections between SSMs and variants of attention and propose a new architecture that refines selective SSM, achieving 2-8\u00d7 faster on language modeling.\nMotivated by the powerful long-sequence modeling capabilities of Mamba and its great efficiency, a substantial body of literature has emerged, focusing on employing and improving Mamba on various downstream tasks. Given this significant surge in studies related to Mamba, it is crucial to conduct a comprehensive review of existing literature and deliberate on potential directions for future research. In this survey, we thus conduct a comprehensive review of Mamba from several perspectives to provide newcomers with a fundamental understanding of Mamba's inner workings while helping experienced practitioners stay abreast of its latest developments. Specifically, the remaining survey is organized as follows: In Section 2, we recall the background knowledge of various representative deep neural networks, including RNNs, Transformers, and State Space Models, while the details of Mamba are introduced in Section 3. Subsequently, we summarize the recent advancements in Mamba-based studies from the perspectives of block design, scanning mode, and memory management in Section 4. Then, Section 5 presents the techniques of adapting Mamba to diverse data, including sequential and non-sequential data. Besides, representative applications of Mamba models are introduced in Section 6, while the challenges and future directions are presented in Section 7. Finally, we summarize the whole survey in Section 8.\nConcurrent with our survey, several related surveys have been released, purely focusing on state space models [137, 183] and Vision Mamba [120, 199, 218]. Diverging from these surveys, this paper is centered on the associated research concerning Mamba. It systematically analyzes existing literature from a novel standpoint to explore the evolution of Mamba architecture and the data adaptation methods utilized in Mamba-based models."}, {"title": "2 Preliminary", "content": "Mamba is deeply intertwined with the recurrent framework of Recurrent Neural Networks (RNNs), the parallel computation and attention mechanism of Transformers, and the linear property of State Space Models (SSMs). Therefore, this section aims to present an overview of these three prominent architectures, serving as a prerequisite for comprehending Mamba thoroughly.\n2.1 Recurrent Neural Networks\nRecurrent Neural Networks (RNNs) excel in processing sequential data due to their capability to retain internal memory [54]. Such networks have demonstrated remarkable effectiveness in a multitude of tasks that involve analyzing and predicting sequences, e.g., speech recognition, machine translation, natural language processing, and time-series analysis [69, 169]. In order to grasp the foundations of recurrent models, this section will offer a brief overview of the standard RNN formulation.\nSpecifically, at each discrete time step k, the standard RNN specifically processes a vector $x_k \\in R^D$ along with the previous step's hidden state $h_{k-1} \\in R^N$ to produce an output vector $o_k \\in R^O$ and update the hidden state to $h_k \\in R^N$. The hidden state serves as the network's memory and retains information about the past inputs it has seen. This dynamic memory allows RNNs to process sequences of varying lengths. Formally, it can be written as\n$h_k = tanh(W_{hx}x_k + W_{hh}h_{k-1} + b_h)$, (1)\n$o_k = W_{oh}h_k + b_o$, (2)\nwhere $W_{hx} \\in R^{N\\times D}$ is the weight matrix responsible for processing model inputs into hidden states, $W_{hh} \\in R^{N\\times N}$ is the recurrent connections between hidden states, $W_{oh} \\in R^{O\\times N}$ represents the weight used to generate outputs derived from hidden states, $b_h \\in R^N$ and $b_o \\in R^O$ correspond the biases, and tanh denotes the hyperbolic tangent activation function introducing non-linearity to the RNN model. In other words, RNNs are nonlinear recurrent models that effectively capture temporal patterns by harnessing the historical knowledge stored in hidden states.\nHowever, there are several limitations associated with RNNs. First, RNNs have a restricted capability to effectively extract long-range dynamics within input sequences. As information traverses through successive time steps, the repeated multiplication of weights in the network can lead to dilution or loss of information. Consequently, it becomes challenging for RNNs to retain and recall information from earlier time steps while making predictions. Second, RNNs process sequential data incrementally, restricting their computational efficiency since each time step relies on the preceding one. This makes parallel computations challenging for them. Furthermore, conventional RNNs lack built-in attention mechanisms, which allow the network to capture global information within input sequences. This absence of attention mechanisms hinders the network's ability to selectively model the crucial segments of the data. o overcome these constraints, Transformers and State Space Models have emerged, each tackling these challenges from different perspectives. These two approaches will be further elaborated upon in the subsequent subsections.\n2.2 Transformers\nThe Transformer [174] is a groundbreaking model in the realm of deep learning, revolutionizing various artificial intelligence applications. Its introduction marked a significant departure from traditional sequence-to-sequence models by employing a self-attention mechanism, facilitating the capture of global dependencies within model inputs. For instance, in natural language processing, this self-attention capability allows the model to comprehend relationships"}, {"title": "A Survey of Mamba", "content": "between various positions in a sequence. It achieves this by assigning weights to each position based on its significance relative to other positions. More specifically, a sequence of input vectors x is first transformed into three types of vectors: Query Q, Key K, and Value V by utilizing linear transformations of the original input, defined by:\n$Q = x \\cdot W^Q, K = x \\cdot W^K, V = x \\cdot W^V$, (3)\nwhere $W^Q$, $W^K$, and $W^V$ are the trainable parameters. The attention scores are computed by calculating the dot product of Q and K, then scaling the result by $\\sqrt{d_k}$, where $d_k$ is the dimension of the key vectors. Such procedures are then passed through a Softmax function to normalize the scores S and produce attention weights, defined by:\n$S = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V$, (4)\nApart from performing a single attention function, multi-head attention is introduced to enhance the model's ability to capture different types of relationships and provide multiple perspectives on the input sequence. In multi-head attention, an input sequence is processed in parallel by multiple sets of self-attention modules. Each head operates independently, performing the exact computations as in the standard self-attention mechanism. The attention weights from each head are then combined to create a weighted sum of the value vectors. This aggregation step allows the model to leverage information from multiple heads and capture diverse patterns and relationships present in the input sequence. Mathematically, the multi-head attention is computed as follows:\n$MultiHead(Q, K, V) = (S_1 \\oplus S_2... \\oplus S_m) \\cdot W^O$,\nwhere $S_i = Softmax(\\frac{Q_iK_i^T}{\\sqrt{d_K}})V_i, i \\in [1, m]$, (5)\nwhere m is the number of attention heads. $\\oplus$ is the concatenation operation, and $W^O$ is the linear transformation to project the multi-head attention scores to the final values.\n2.3 State Space Models\nState Space Models (SSMs) are a traditional mathematical framework utilized to depict the dynamic behavior of a system over time [89]. Recent years have found the widespread applications of SSMs in diverse fields like control theory, robotics, and economics [58, 59]. At its core, SSMs embody the system's behavior through a collection of hidden variables referred to as \"states\", enabling it to capture temporal data dependencies effectively. Different from RNNs, SSMs are linear models characterised by their associative property. To be specific, in a classical state space model, two key equations are formulated, i.e., state equation and observation equation, to model the relationships between input x(t) \u2208 R and output y(t) \u2208 R at current time t, through a N-dimensional hidden state h(t) \u2208 RN. The process can be written by\n$h'(t) = Ah(t) + Bx(t)$, (6)\n$y(t) = Ch(t) + Dx(t)$, (7)\nwhere h'(t) is the derivative of current state h(t), A \u2208 RN\u00d7N is the state transition matrix that describes how states change over time, B \u2208 RN\u00d71 is the input matrix that controls how inputs affect state changes, C \u2208 R1\u00d7N denotes the output matrix that indicates how outputs are generated based on current states, and D \u2208 R represents the command"}, {"title": "Qu et al.", "content": "coefficient that determines how inputs affect outputs directly. In general, most SSMs exclude the second term in the observation equation, i.e., set Dx(t) = 0, which can be recognized as a skip connection in deep learning models.\n2.3.1 Discretization. To adhere to the requirements of machine learning settings, SSMs must undergo a process of discretization that transforms continuous parameters into discrete parameters. Discretization methods generally aim to partition continuous time into K discrete intervals with equal integration area as possible. To achieve the goal, as one of the most representative solutions, Zero-Order Hold (ZOH) [138, 223] is successfully employed in SSMs, which assumes that the function value remains constant over the interval \u2206 = [tk\u22121, tk]. After ZOH discretization, the SSM equations can be rewritten by\n$h_k = Ah_{k-1} + Bx_k$, (8)\n$Y_k = Ch_k$, (9)\nwhere A = exp(\u0394A), and B = (\u0394A)-1 (exp(\u2206A) \u2013 I) \u00b7 AB, k is the discrete time step. From these formulas, it is clear that the discrete SSM has a similar structure to recurrent neural networks and, therefore, discrete SSMs can accomplish inference processes with higher efficiency compared to Transformer-based models that compute attention on all inputs in each auto-regressive decoding iteration.\n2.3.2 Convolutional Computation. The discrete SSM, being a linear system, possesses the associated property and therefore integrates seamlessly with convolutional computation. More specifically, it can calculate the output at each time step independently as follows:\n$Yo = CA^0Bx_0$, (10)\n$y_1 = CA^1Bx_0 + CA^0Bx_1$, (11)\n$y_2 = CA^2Bx_0 + CA^1Bx_1 + CA^0Bx_2$, (12)\n$y_k = CA^kBx_0 + CA^{k-1}Bx_1 + ... + CA^1Bx_{k-1} + CA^0Bx_k$ (13)\nBy creating a set of convolutional kernels K = (CB, ..., CAB, ...), the recurrent computation can be converted to a convolutional form as:\ny = x * K, (14)\nwhere x = [x0, x1, ...] and y = [yo, y1, ...] \u2208 RL denote the input and output sequences, respectively, while L is the sequence length. This convolutional computation allows SSMs to take full advantage of modern matrix computation hardware (e.g., GPUs) to enable parallel computing during training process, which is impossible with RNNs utilizing nonlinear activation functions. Notably, given an input x(k) with D dimensions, the SSM computation will be calculated separately for each dimension to produce a D-dimensional output y(t). In this case, the input matrix B \u2208 RN\u00d7D, the output matrix C\u2208 RD\u00d7N, and the command matrix D \u2208 RD\u00d7D, while the state transition matrix remains unchanged, i.e., A \u2208 RNXN\n2.3.3 Relationship among RNN, Transformer, and SSM. The computation algorithms of RNN, Transformer, and SSM are depicted in Figure 2. On the one hand, the conventional RNN operates within a non-linear recurrent"}, {"title": "Qu et al.", "content": "framework where each computation depends solely on the previous hidden state and the current input. While this format allows RNNs to quickly generate outputs during auto-regressive inference, it hampers their ability to fully exploit GPU parallel computing, leading to slower model training. On the other hand, the Transformer architecture performs matrix multiplications in parallel across multiple query-key pairs, which can be efficiently distributed across hardware resources, which enables faster training of attention-based models. However, when it comes to generating responses or predictions from Transformer-based models, the inference process can be time-consuming. For instance, the auto-regressive design of language models entails generating each token in the output sequence sequentially, which requires repetitive calculations of attention scores at each step, leading to slower inference times. Unlike RNNs and Transformers, which are limited to supporting only one type of computation, discrete SSMs have the flexibility to support both recurrent and convolutional computations, given its linear property. This unique capability allows SSMs to achieve not only efficient inference but also parallel training. However, it should be noted that the most conventional SSMs are time-invariant, meaning that their A, B, C, and A are unrelated to the model input x. This would limit context-aware modeling, which leads to inferior performance of SSMs in certain tasks such as selective copying [55].\n3 Mamba\nTo address the aforementioned drawback of traditional SSMs in terms of their inferior context-aware capabilities, Mamba is proposed by [55] as a potential alternative that promises to be a general sequence foundation model backbone. More recently, Mamba-2 [29] proposes Structured Space-State Duality (SSD) that establishes a robust theoretical framework connecting structured SSMs and various forms of attention, allowing us to transfer algorithmic and systems optimizations originally developed for Transformers to SSMs. In this section, we will give a concise and clear introduction to Mamba and Mamba-2."}, {"title": "Qu et al.", "content": "3.1 Mamba-1: Selective State Space Model with Hardware-aware Algorithms\nConventional SSMs have shown limited effectiveness in modeling text and other information-dense data [55], impeding their progress in deep learning. In the pursuit of empowering SSMs with Transformers' modeling capabilities, Gu and Dao [55] introduces three innovative techniques based on Structured State Space Models, i.e., High-order Polynomial Projection Operator (HiPPO)-based Memory Initialization, Selection Mechanism, and Hardware-aware Computation, as illustrated in Figure 3. These techniques aim to enhance the capabilities of SSMs in long-range linear-time sequence modeling. In particular, the initialization strategy establishes a coherent hidden state matrix, effectively facilitating long-range memory. Then, the Selection Mechanism empowers SSMs to acquire content-aware representations. Lastly, Mamba crafts two hardware-aware computation algorithms, Parallel Associative Scan and Memory Recomputation, to enhance training efficiency.\n3.1.1 HiPPO-based Memory Initialization. Modeling and learning from sequential data represent foundational challenges in contemporary machine learning, forming the bedrock for various tasks, including language modeling, speech recognition, and video processing. A fundamental component for modeling intricate and long-term temporal dependencies lies in memory, encompassing the ability to store and integrate information from preceding time steps [73]. Similar to RNNs, preserving and forgetting the historical hidden states (i.e., the matrix A) plays a critical role in SSMs to achieve satisfying performances. In previous structured state space sequence models (SSMs), there have been suggestions for special initializations, especially in the case of complex-valued models. These special initializations have proven beneficial in various scenarios, including situations with limited data availability. Similarly, Mamba focuses primarily on the initialization of the hidden state matrix A to capture complex temporal dependencies. This is accomplished through the utilization of the HiPPO theory [56] with an innovative scaled Legendre measure (LegS), ensuring a comprehensive consideration of the complete historical context rather than a limited sliding window. To be specific, the HiPPO-LegS"}, {"title": "A Survey of Mamba", "content": "assigns uniform weight to all historical data points, which can be expressed as:\n$A_{nk}^{HiPPO} = \\begin{cases}  (2n + 1) \\sqrt{(2k+1)} & \\text{if } n > k \\\\  -n+1 & \\text{if } n = k, \\\\ 0 & \\text{if } n < k  \\end{cases}$ (16)\nwhere n is the number of polynomials, and k denotes the particular discrete time steps. Building upon the HiPPO theory, Mamba introduces two simple initialization methods for the complex and real cases, i.e., S4D-Lin and S4D-Real [57], as presented in\n$A_{dn} = \\begin{cases}  -\\frac{n}{i} & \\text{S4D-Lin} \\\\  -\\frac{n}{n+1} & \\text{S4D-Real}  \\end{cases}$ (17)\nwhere n is the n-th element of A for all input dimensions d = 1, 2, ..., D. Given such an initialization, the model can learn long-dependent memory that experiences smaller degradation of newer steps and larger degradation of older steps by compressing and reconstructing the input information signal. According to the formulas, HiPPO-LegS possesses advantageous theoretical properties: it remains consistent across input timescales and offers rapid computation [56]. Additionally, it has bounded gradients and approximation errors, facilitating the parameter learning process.\n3.1.2 Selection Mechanism. Conventional state space models are unable to produce personalized outputs based on specific model inputs (i.e., the content-aware modeling ability) due to the property of Time Invariance. To provide SSMs with such a capability similar to the attention mechanisms, Mamba designs a time-varying selection mechanism that parameterizes the weight matrices according to model input. Such innovation empowers SSMs to filter out extraneous information while retaining pertinent details indefinitely. Formally, the selection mechanism involves setting the interval A, and matrices B, C as functions of the input $x \\in R^{B\\times L\\times D}$, which can be formulated as:\n$B \\rightarrow S_B = W_Bx$, (18)\n$C \\rightarrow S_C = W_Cx$, (19)\n$\\triangle \\rightarrow S_{\\triangle} = \\tau_{\\triangle} \\cdot BroadCast_D(W_{\\triangle}x)$, (20)\nwhere $S_B \\in R^{B\\times L\\times N}$, $S_C \\in R^{B\\times L\\times N}$, and $S_{\\triangle} \\in R^{B\\times L\\times D}$ are the selective space matrices that function of the input to achieve content-aware modeling. B, L, D, and N represent the batch size, input length, input feature size, and hidden channel number, respectively. Notably, $W_B \\in R^{N\\times D}$, $W_C \\in R^{N\\times D}$, and $W_{\\triangle} \\in R^{D\\times 1}$ are the selection weights (i.e., linear parameterized projections) for corresponding components, and BroadCast means to broadcast the result to all the dimensions d = 1, 2, .., D. Subsequently, the selective SSMs undergo discretization using a common statistical technique, Zero-Order Hold (ZOH) [138], as presented in\n$A \\rightarrow S_A = exp(S_{\\triangle}A)$, (21)\n$B \\rightarrow S_B = (S_{\\triangle}A)^{-1}(exp(S_{\\triangle}A) - I) \\cdot S_{\\triangle}S_B$, (22)\nwhere $S_A \\in R^{B\\times L\\times D\\times N}$ and $S_B \\in R^{B\\times L\\times D\\times N}$ are the selective state transition matrix and the input matrix, respectively, which become the functions of input x. By doing so, the discrete SSM has changed from time-invariant to time-varying"}, {"title": "A Survey of Mamba", "content": "(i.e., content-aware) as\ny = SSM(A, B, C) (x), (23)\nwhich generates output $y \\in R^{B\\times L\\times D}$ depending on the input x. Note that the time-varying selection mechanism in Mamba has a similar structure to the attention mechanism in Transformer, i.e., both perform operations based on inputs and their projections, which allows Mamba's SSM to achieve a flexible content-aware modeling. Nevertheless, it loses the equivalence to convolutions, which negatively impacts its efficiency.\n3.1.3 Hardware-aware Computation. The selection mechanism is crafted to surpass the limitations of linear time-invariant models. Still, it challenges efficient training: SSMs' convolutional kernels become input-dependent, resulting in the inability to perform parallel computations. To tackle the problem, Mamba utilizes two computation techniques, i.e., Parallel Associative Scan (also called Parallel Prefix-Sum) [64] and Memory Recomputation. First, the Parallel Associative Scan leverages the property of linear associative computation and the parallelism of modern accelerators (GPU and TPU) to perform the calculation of selective SSMs in a memory-efficient manner. More specifically, the parallel associative scan reduces the computation complexity of model training from $O(N^2d)$ to $O(N/t)$. At its core, the scan revolves around constructing a balanced binary tree on the given input and sweeps it to and from the root. In other words, the parallel associative scan begins by traversing from the leaves to the root (i.e., Sweep-Up), creating partial sums at the internal nodes of the tree. Then, it reverses the traversal, moving from the root back up the tree to construct the whole scan using the partial sums (i.e., Sweep-Down).\nOn the other hand, Mamba leverages the traditional approach of recomputation to diminish the overall memory demand for training selective SSM layers. In particular, Mamba abstains from storing intermediate states of size (B, L, D, N) during the forward pass of the Parallel Associative Scan to prevent memory expansion. Instead, it recomputes those intermediate states in the backward pass for gradient computation. By doing so, recomputation sidesteps the necessity of reading O(BLND) elements between GPU memory cells. In addition to optimizing the memory needs of the scan operation, Mamba-1 extends its use of recomputation to enhance the efficiency of the entire SSM layer. This optimization encompasses projections, convolutions, and activations, which typically demand significant memory resources but can be rapidly recomputed.\n3.2 Mamba-2: State Space Duality\nTransformers, which have played a crucial role in the success of deep learning for various areas, have inspired the development of various techniques, such as Parameter-efficient Fine-tuning [95], Catastrophic Forgetting Mitigation [96], and Model Quantization [195], aimed at improving model performance from diverse perspectives. To enable state space models to access and benefit from the valuable techniques initially developed for Transformers, Mamba-2 [29] have introduced a comprehensive framework called Structured State-Space Duality (SSD), which establishes theoretical connections between SSMs and different forms of attention. Formally,\ny = SSD(A, B, C) (x) = Mx, (24)\nwhere M denotes the matrix form of SSMs that uses the sequentially semi-separable representation, and Mji = CAj:iBi. Notably, Cj and Bi represent the selective space state matrices associated with input tokens xj and xi, respectively. Aj:i denotes the selective matrix of hidden states corresponding to the input tokens ranging from j to i. In essence, SSD demonstrates that both the attention mechanism used by Transformers and the linear time-variant system employed in"}, {"title": "A Survey of Mamba", "content": "SSM can be seen as semi-separable matrix transformations. Furthermore, Dao and Gu [29] also proves that the selective SSM is equivalent to a structured linear attention mechanism implemented with a semi-separable masking matrix. Based on SSD, Mamba-2 has devised a more hardware-efficient computation through a block decomposition matrix multiplication algorithm. Specifically, by viewing state space models as semi-separable matrices through the matrix transformation, Mamba-2 decomposes the computation into matrix blocks, in which diagonal blocks represent intra- chunk computations. In contrast, the off-diagonal blocks represent inter-chunk computations factored through the SSM's hidden state. This approach enables Mamba-2 to achieve a 2-8\u00d7 faster training process than Mamba-1's parallel associative scan while remaining competitive with Transformers.\n3.3 Mamba Block\nIn this subsection, we provide a summary of the block design for Mamba-1 and Mamba-2. Figure 4 illustrates the comparison of these two architectures. Mamba-1 is motivated by an SSM-centric point of view where the selective SSM layer is tasked with conducting a map from input sequences X to Y. In this design, the linear projections of (A, B, C) are applied after the initial linear projection that creates X. The input tokens and state matrices are then passed through the selective SSM cell, utilizing the parallel associative scan, to produce the output Y. After that, Mamba-1 employs a skip connection to encourage feature reuse and alleviate the degradation problem often occurring during the model training process. Finally, the Mamba model is constructed by stacking this block interleaved with standard normalization and residual connections.\nAs for Mamba-2, it introduces the SSD layer aiming to create a map from [X, A, B, C] to Y. This is achieved by simultaneously processing [X, A, B, C] with a single projection at the beginning of the block, similar to how standard attention architectures generate the Q, K, V projections in parallel. In other words, the Mamba-2 block simplifies"}, {"title": "Qu et al.", "content": "the Mamba-1 block by removing sequential linear projections. This enables faster computation of the SSD structure compared to the parallel selective scanning in Mamba-1. Additionally, a normalization layer is added after the skip connection, aiming to improve training stability.\n4 Advancements in Mamba Models\nState Space Models and Mamba have been recently explored and become one promising alternative as the foundational model backbone. While Mamba demonstrates proficiency in natural language processing, it still encounters challenges, such as memory loss, generalization to diverse tasks, and inferior capability to capture complex patterns to Transformer- based language models. To overcome these challenges, plenty of efforts have been made to improve the Mamba architecture. Existing research studies primarily concentrate on modifying the block design, scanning mode, and memory management aspects. This section will introduce several vital techniques from these three aspects, and a summary of related studies is presented in Table 1.\n4.1 Block Design\nThe design and structure of the Mamba block have a significant impact on the overall performance of Mamba models, making it an emerging research focus. As illustrated in Figure 5, based on different approaches to constructing new Mamba blocks, existing research can be categorized into three categories: a) Integration methods aim to integrate the Mamba block with other well-known models, so as to strike a balance between effectiveness and efficiency; b) Substitution methods attempt to utilize Mamba block as a substitution for main layers in advanced model frameworks; and c) Modification methods focus on modifying the components within the classical Mamba block. Accordingly, we will present a detailed review of these methods in the following subsections."}, {"title": "A Survey of Mamba", "content": "4.1.1 Integration. Given Mamba's exceptional ability to capture long-term dynamics, it has been extensively integrated with other models, leveraging their own strengths to deliver a robust framework tailored to specific scenarios. The integration specifically encompasses advanced models like Transformers, Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), Recurrent Neural Networks (RNNs), and Spiking Neural Networks (SNNs). Specific examples are described below.\n\u2022 Transformer-based models have exhibited remarkable performance in numerous tasks, but their quadratic computational complexity still hampers them during inference process [58]. In the pursuit of efficient generation, some researchers have proposed incorporating Mamba blocks with Transformer-based models. For example, Jamba [111] combines blocks of Transformer and Mamba layers to tackle long-content Natural Language Processing tasks, capitalizing on the advantages of both model families. The Attention-Mamba hybrid model demonstrated superior performance compared to the standalone Transformer and Mamba models, achieving better throughput than the vanilla Transformer model. Mambaformer [200] utilizes the hybrid framework to forecast multiple time series, including exchange rates, hourly electricity consumption, and power load, which internally combines Mamba blocks and Transformer layers for long- and short-range dependencies, respectively. Due to the integration of Mamba and Transformer, Mambaformer outperforms Transformer-based predictors in long-short range time series forecasting.\n\u2022 CNN-based methods are constrained by local receptive fields, resulting in suboptimal performance capturing global and long-range semantics [55]. Known for the superior capability of state space models to learn long-range patterns, some studies [107, 187, 204] have explored the potential of utilizing Mamba blocks to enhance CNN-based models, especially in the field of computer vision. For instance, MedMamba [214] and nnMamba [53] showcase how the integration of visual Mamba blocks improves the performance of CNNs in image analysis tasks.\n\u2022 GNN has demonstrated promising potential in capturing neighboring relationships through message-passing mech- anisms, where information is propagated over a connection graph through stacked layers. Nonetheless, these"}, {"title": "Qu et al.", "content": "models face a significant limitation known as over-smoothing [20", "203": ".", "9": "reformulates graph-structured data into sequential tokens in a particular order and leverages a selective SSM layer within the Mamba block to construct a novel Graph Mamba Network (GMN) architecture, which achieves superior graph representation learning capabilities, particularly in the datasets that requires high-order dependencies between nodes.\n\u2022 RNN-based models have yielded outstanding results in capturing temporal dynamics. Nevertheless, RNNs still face significant challenges, including time-consuming recurrent training and limitations in memory capacity for hidden states. Inspired by the emergence of recent Mamba-based architectures, some researchers have developed a fusion of Mamba blocks and RNNs. For instance, VMRNN [170"}]}