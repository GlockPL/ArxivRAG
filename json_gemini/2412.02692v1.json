{"title": "Taming Scalable Visual Tokenizer for Autoregressive Image Generation", "authors": ["Fengyuan Shi", "Zhuoyan Luo", "Yixiao Ge", "Yujiu Yang", "Ying Shan", "Limin Wang"], "abstract": "Existing vector quantization (VQ) methods struggle with scalability, largely attributed to the instability of the code-book that undergoes partial updates during training. The codebook is prone to collapse as utilization decreases, due to the progressively widening distribution gap between non-activated codes and visual features. To solve the problem, we propose Index Backpropagation Quantization (IBQ), a new VQ method for the joint optimization of all codebook embeddings and the visual encoder. Applying a straight-through estimator on the one-hot categorical distribution between the encoded feature and codebook, all codes are differentiable and maintain a consistent latent space with the visual encoder. IBQ enables scalable training of visual tokenizers and, for the first time, achieves a large-scale codebook (2^{18}) with high dimension (256) and high utilization. Experiments on the standard ImageNet benchmark demonstrate the scalability and superiority of IBQ, achieving competitive results on both reconstruction (1.00 rFID) and autoregressive visual generation (2.05 gFID). The code and models are available at https://github.com/TencentARC/SEED-Voken.", "sections": [{"title": "1. Introduction", "content": "Given the remarkable scalability and generalizability of autoregressive transformers [2, 29] in large language models [17, 26], recent efforts have been made to extend this success to visual generation [6, 24, 25, 28]. The common practice is to discretize images into tokens using vector quantization (VQ) before applying autoregressive transformers for discrete sequence modeling through next-token prediction.\nExisting studies on autoregressive visual generation primarily aim to improve the reconstruction performance of visual tokenizers [6, 31, 32] or to refine the autoregressive modeling of transformers [14, 16, 24, 25]. The improvement of visual tokenizers is crucial, given that it can markedly amplify the potential performance of ensuing autoregressive models. Intuitively, scaling visual tokenizers by increasing the codebook size and embedding dimension could help bridge the gap between discrete and continuous representations, thereby mitigating the information loss associated with discrete tokens. However, it is noteworthy that current visual tokenizers [24, 31] have not demonstrated such scaling properties.\nEmpirical research [6, 24] has revealed that current VQ methods struggle with scalability due to the inherent tendency of the codebook to collapse. This arises because these methods only optimize limited selected codes during each backpropagation. Such a widely-adopted partial update strategy gradually broadens the distribution gap between non-activated codes and the visual encoder's representation space, making the non-activated codes further less likely to be selected. As shown in Fig. 2, VQGAN [6] almost fails when scaling both codebook size (i.e., 16, 384) and embedding dimension (i.e., 256) simultaneously. Only a small amount of codes share the same distribution with the visual encoder, and the codebook usage degrades from 68% to 0.002% after training one epoch.\nTo tackle the challenge, we introduce a new VQ method, namely, Index Backpropagation Quantization (IBQ). It globally updates the entire codebook in each backward process to ensure consistency with the distribution of the visual encoder. In such a way, all codes have the same probability of being selected, resulting in a high utilization of the code-book throughout the training process. Specifically, rather than directly applying the straight-through estimator [1] to the selected codes, we employ this reparameterization approach on the categorical distribution between visual features and all codebook embeddings, thereby rendering all codes differentiable. As shown in Fig. 2, the sampled visual features and the codebook embeddings from IBQ are evenly mixed. IBQ keeps a high codebook usage (~96%) throughout the training process. Fully leveraging the code-book effectively enhances the representation capacity, as demonstrated by the superior reconstruction of IBQ (1.37 rFID) compared to VQGAN (3.98 rFID).\nExperiments on the standard ImageNet benchmark validate the scalability and superiority of our method in both reconstruction and generation. We conducted an in-depth study on the scaling law of the IBQ tokenizer from three aspects: codebook size, code dimension, and model size. We observed that the reconstruction performance or codebook usage significantly increases when scaling up tokenizer training. To our knowledge, IBQ is the pioneering work to train an extremely large codebook (i.e., 262, 144) with a relatively large code dimension (i.e., 256). This achievement has resulted in state-of-the-art reconstruction performance, achieving a 1.00 rFID. We employed the IBQ tokenizer in a vanilla autoregressive transformer for visual generation, training models of varying sizes from 300M to 2.1B, and achieved competitive results."}, {"title": "2. Related Work", "content": "2.1. Vector Quantization\nAt the core of visual tokenizers is vector quantization, which maps the visual signals into discrete tokens. VQ-VAE [28] proposes an encoder-quantizer-decoder structure and uses a learnable codebook as the discrete representation space. VQ-VAE2 [19] introduces multi-scale hierarchical VQ-VAE to enhance local features. VQGAN [6] further uses adversarial loss and perceptual loss for good perceptual quality. RQ-VAE [13] and DQ-VAE [9] improve VQGAN by residual quantization and dynamic quantization respectively. To improve codebook utilization for large-size codebooks, some works try to decrease code dimension [24, 31]. Following this observation, MAGVIT-v2 [32] reduces the code dimension to zero, and significantly enlarges the codebook size to 2^{18} with Lookup-Free Quantization. Instead of joint optimization of the model and codebook, VQGAN-LC [36] extends the codebook size to 100,000 by utilizing a frozen codebook with a trainable projector.\nExisting VQ methods suffer from codebook collapse when scaling up tokenzier, and usually use a small code-book size or low code dimension, which limits the representational capacity of the codebook. In contrast, our proposed IBQ shows consistent improvements when scaling up code dimension, codebook size and model size.\n2.2. Tokenized Visual Generation\nTokenizers map continuous visual signals into a discrete token sequence. For subsequent visual generation, there are two approaches, including non-autoregressive (NAR) and autoregressive (AR) generation. NAR [3, 32] usually adopts BERT-style transformers to predict masked tokens. For inference, these methods generate all tokens of an image simultaneously, and iteratively refine the generated images conditioned on the previous generation. In contrast, AR models perform next-token prediction in a raster-scan manner. VQGAN [6] adopts GPT2-medium architecture, while LlamaGen [24] employs Llama [26] for scalable im-"}, {"title": "3. Method", "content": "Our framework consists of two stages. The first stage is to learn a scalable visual tokenizer with high codebook utilization via Index Backpropagation Quantization. In the second stage, we use an autoregressive transformer for visual generation by next-token prediction.\n3.1. Preliminary\nVector quantization (VQ) maps continuous visual signals into discrete tokens with a fixed-size codebook \\(C \\in \\mathbb{R}^{K \\times D}\\), where K is the codebook size and D is the code dimension. Given an image \\(I \\in \\mathbb{R}^{H \\times W \\times 3}\\), VQ first utilizes an encoder to project the image into the feature map \\(Z \\in \\mathbb{R}^{h \\times w \\times D}\\), where \\(h = H/p, w = W/p\\), and p is the downsample ratio. Then the feature map is quantized into \\(Q \\in [\\mathbb{R}^{h \\times w \\times D}\\) as discrete representations with the codebook. Finally, the decoder reconstructs the image given the quantized features.\nFor quantization, previous methods typically calculate the Euclidean distance between each visual feature \\(z \\in \\mathbb{R}^{D}\\) and all codes of the codebook, and then select the closest code as its discrete representation [6, 28]. Since the arg min operation in quantization is non-differentiable, they apply the straight-through estimator on selected codes to copy the gradients from the decoder, to optimize the encoder simultaneously. The quantization process is as follows:\n\\begin{equation}\nq = \\arg \\min_{C_k \\in C} ||z - C_k|| \\in \\mathbb{R}^{D},\n\\end{equation}\n\\begin{equation}\nz_q = z + sg[q - z],\n\\end{equation}\nwhere sg[] is stop-gradient operation."}, {"title": "3.2. Index Backpropagation Quantization", "content": "Quantization. To ensure the consistent distribution between the codebook and encoded features through the training, we introduce an all-codes updating method, Index Backpropagation Quantization (IBQ). At the core of IBQ is passing gradients backward to all codes of the codebook, instead of the selected codes only. Algorithm 1 provides the pseudo-code of IBQ.\nSpecifically, we first perform dot product between the given visual feature z and all code embeddings as logits and get probabilities (soft one-hot) by softmax function.\n\\begin{equation}\nlogits = [z^T C_1, z^T C_2,\u2026\u2026, z^T C_K]^T \\in \\mathbb{R}^{K},\n\\end{equation}\n\\begin{equation}\nInd_{soft} = softmax(logits),\n\\end{equation}\n\\begin{equation}\nInd_{hard} = One-Hot(argmax(Ind_{soft})).\n\\end{equation}\nThen we copy the gradients of soft one-hot categorical distribution to hard one-hot index:\n\\begin{equation}\nInd = Ind_{hard} - sg[Ind_{soft}] + Ind_{soft}.\n\\end{equation}\nGiven the index, the quantized feature is obtained by:\n\\begin{equation}\nz_q = Ind^T C.\n\\end{equation}\nIn this way, we can pass the gradients to all codes of the codebook via index. By Index Backpropagation Quantization, the distribution of the whole codebook and encoded features remains consistent throughout completed training, thus gaining a high codebook utilization.\nTraining Losses. Similar to VQGAN [6], the tokenizer is optimized with a combination of losses:\n\\begin{equation}\nL = L_R + L_Q + L_P + L_G + L_E,\n\\end{equation}\nwhere \\(L_R\\) is reconstruction loss of image pixels, \\(L_Q\\) is quantization loss between the selected code embeddings and encoded features, \\(L_P\\) is perceptual loss from LPIPS [34], \\(L_G\\) is adversarial loss with PatchGAN discriminator [10] to enhance the image quality, and \\(L_E\\) is entropy penalty to encourage codebook utilization.\nWe further introduce double quantization loss, to force the selected code embeddings and given encoded visual features towards each other.\n\\begin{equation}\nz_q' = Ind_{hard}^T [C_1, C_2,\u2026\u2026\u2026,C_K],\n\\end{equation}\n\\begin{equation}\nL_Q = ||z_q - z||^2 + \\beta ||sg[z] - z_q'||^2 + \\beta||z - sg[z_q']||^2.\n\\end{equation}"}, {"title": "3.3. Autoregressive Transformer", "content": "After tokenization, the visual feature is quantized into discrete representations which are subsequently flattened in a raster-scan manner for visual generation. Given the discrete token index sequence \\(X = \\{x_t\\}_{t=1}^T\\), where \\(T = h' \\times w'\\), we leverage an autoregressive transformer to model the sequence dependency via next-token prediction. Specifically, the optimization process is to maximize the log-likelihood:\n\\begin{equation}\np(x_1,\u2026, x_T|c) = \\prod_{t=1}^T P(x_t|x_1,\u2026, x_{t-1}, c),\n\\end{equation}\nwhere c is the condition such as class label.\nNote that, since our focus is on the visual tokenizer, we adopt the vanilla architecture of autoregressive transformers akin to Llama [26] with AdaLN [18] for visual generation."}, {"title": "4. Experiment", "content": "4.1. Datasets and Metrics\nThe training of the visual tokenizer and autoregressive transformer are both on 256 \u00d7 256 ImageNet [4]. For visual reconstruction, the reconstruction-FID, denoted as rFID [7], codebook utilization, and LPIPS [34] on ImageNet 50k validation set are adopted to measure the quality of reconstructed images. For visual generation, we measure the quality of image generation by the prevalent metrics FID, IS [21] and Precision/Recall [12].\n4.2. Implementations Details\n4.3. Main Results\nVisual Generation. In Tab. 7, we compare IBQ with other generative models, including Diffusion models, AR models, and variants of AR models (VAR [25] and MAR [14]) on class-conditional image generation task. With the powerful visual tokenizers of IBQ, our models show consistent improvements when scaling up the model"}, {"title": "4.4. Scaling Up IBQ", "content": "Existing VQ methods struggle to scale up due to the codebook collapse. For example, the usage and rFID of LlamaGen[24] significantly drop when increasing the code dimension from 8 to 256 (97%\u21920.29%, 2.19 rFID\u21929.21 rFID), as shown in Tab. 1. This is due to their partial updates during training which progressively widens the distribution gap between non-activated codes and encoded features."}, {"title": "4.5. Ablation Studies", "content": "Key Designs. To validate the effectiveness of our method, we conduct ablation studies on several key designs, as shown in Tab. 2. The re-implemented VQGAN performance is 3.98 rFID and 5.3% codebook utilization. Different from previous methods, the replacement from VQ to IBQ achieves consistent distribution between encoded features and the whole codebook by rendering all code differentiable, which brings a clear improvement of both code-book usage (5.3%\u2192 98%) and reconstruction quality (3.98 rFID\u21921.67 rFID). By incorporating double quantization loss to force the selected code embeddings and encoded visual features toward each other, IBQ guarantees more precise quantization. Following MAGVIT-v2 [32], we enlarge the model size for better compacity, and the reconstruction performance gets improved correspondingly."}, {"title": "5. Conclusion", "content": "In this paper, we identify the bottleneck in scaling tokenizers arising from the partial-update strategy in current VQ methods, which progressively broadens the distribution gap between encoded features and non-activated codes, ultimately leading to codebook collapse. To tackle this challenge, we propose a simple yet effective vector quantization method, coined as Index Backpropagation Quantization (IBQ), for scalable tokenizer training, which updates all codes by applying the straight-through estimator on the categorical distribution between visual features and all codebook embeddings, thereby maintaining consistent distribution between the entire codebook and encoded features. Experiments on ImageNet demonstrate that IBQ enables a high-utilization, large-scale visual tokenizer with improved performance in both reconstruction (1.00 rFID) and generation (2.05 gFID), confirming the scalability and effectiveness of our method."}, {"title": "Appendix", "content": "A. Autoregressive Model Configurations\nWe show the detailed autoregressive model configurations in Tab. 8. We scale up the AR models from 342M to 2.1B following the scaling rules proposed in VAR [25].\nB. Comparison with Soft Vector Quantization\nTo comprehensively illustrate the rationality of our IBQ, we compare it with another global update method, Soft Vector Quantization dubbed as Soft VQ. During training, it adopts the weighted average of all code embeddings as the quantized feature va and incorporates a cosine decay schedule of the temperature ranging from 0.9 to 1e-6 for one-hot vector approximation. As for inference, it switches back to the original VQGAN way, which selects the code with the highest probability for hard quantization.\nAs shown in Tab. 9, Soft VQ is far behind IBQ in both reconstruction quality and codebook usage. In the experiments, we observe that the training process of Soft VQ corrupts within a few epochs (< 10). This may stem from the unstable adversarial training where the adaptive weight of the GAN loss appears enormous and ends up with NAN. In addition, the soft-to-hard manner for one-hot vector approximation brings more difficulty in optimization and incurs inconsistency of quantization between training and inference, as demonstrated by a significant reconstruction quality drop (16.17rFID \u2192 233.17rFID).\nMoreover, we provide an in-depth investigation by visualizing the distribution between the codebook and encoded features of Soft VQ. As shown in Fig. 6, although all-code updating strategy is enabled, the inappropriate quantization process tends to cluster codes mistakenly which subsequently suffers from low codebook usage (2.5%). We speculate that the force of the weighted average of code embeddings toward the encoded feature will smooth the codebook representation and result in similar and less informative code embeddings. In contrast, IBQ adopts hard quantization with index backpropagation. The hard quantization only involves the selected codes toward the encoded features for discriminative representation, thus ensuring precise quantization, while index backpropagation performs joint optimization of all codebook and the visual encoder to achieve consistent distribution. Considering the factors above, our proposed IBQ shows dominance in both reconstruction quality and codebook utilization.\nC. Additional Visualizations\nWe provide more qualitative reconstruction and generation samples in Fig. 7 and Fig. 8, respectively."}, {"title": "D. Limitation and Future Work", "content": "In this work, we mainly focus on visual tokenizers through scalable training, and adopt vanilla autoregressive transformers for visual generation. We believe that combining our powerful visual tokenizers with advanced AR models is a promising way to improve autoregressive visual generation. In addition, our models are trained on ImageNet [4], limiting the generalization ability. Pretraining tokenizers on more data and training AR models on text-image pairs would be helpful. We leave these for future work."}]}