{"title": "MAMBAQUANT: QUANTIZING The Mamba Family\nWITH VARIANCE ALIGNED ROTATION METHODS", "authors": ["Zukang Xu", "Yuxuan Yue", "Xing Hu", "Zhihang Yuan", "Zixu Jiang", "Zhixuan Chen", "Jiangyong Yu", "Chen Xu", "Sifan Zhou", "Dawei Yang"], "abstract": "Mamba is an efficient sequence model that rivals Transformers and demonstrates\nsignificant potential as a foundational architecture for various tasks. Quantization\nis commonly used in neural networks to reduce model size and computational\nlatency. However, applying quantization to Mamba remains underexplored, and\nexisting quantization methods, which have been effective for CNN and Trans-\nformer models, appear inadequate for Mamba models (e.g., Quarot suffers a 21%\naccuracy drop on Vim-T\u2020 even under W8A8). We have pioneered the exploration\nof this issue and identified several key challenges. First, significant outliers are\npresent in gate projections, output projections, and matrix multiplications. Sec-\nond, Mamba's unique parallel scan further amplifies these outliers, leading to\nuneven and heavy-tailed data distributions. Third, even with the application of the\nHadamard transform, the variance across channels in weights and activations still\nremains inconsistent. To these ends, we propose MambaQuant, a post-training\nquantization (PTQ) framework consisting of: 1) Karhunen-Lo\u00e8ve Transformation\n(KLT) enhanced rotation, rendering the rotation matrix adaptable to diverse chan-\nnel distributions. 2) Smooth-Fused rotation, which equalizes channel variances\nand can merge additional parameters into model weights. Experiments show that\nMambaQuant can quantize both weights and activations into 8-bit with less than\n1% accuracy loss for Mamba-based vision and language tasks. To the best of our\nknowledge, MambaQuant is the first comprehensive PTQ design for the Mamba\nfamily, paving the way for further advancements in its application.", "sections": [{"title": "1 INTRODUCTION", "content": "Mamba (Gu & Dao, 2023) is a modern sequence model that competes with the Transformer (Vaswani\net al., 2017), particularly noted for its ability to handle extremely long sequences. The model's design\nis inspired by the Structured State Space model (S4) (Gu et al., 2021) and integrates features from\nrecurrent, convolutional, and continuous-time models to effectively capture long-term periodic depen-\ndencies. Expanding upon the S4 paradigm, Mamba brings about several noteworthy improvements,\nespecially in handling time-variant operations. These enhancements enable the effective and efficient\nprocessing of lengthy data sequences, positioning Mamba as a promising foundational architecture\nfor vision (Zhu et al., 2024; Liu et al., 2024), language (Gu & Dao, 2023; Li et al., 2024), and\nmulti-modality tasks (Zhao et al., 2024)."}, {"title": "2 RELATED WORK", "content": "Mamba Models Mamba (Gu & Dao, 2023) is a selective structured state space model that sub-\nstantially improves the performance of state space models (SSM) in handling sequential data. It\ntransforms parameters in the structured state space model (S4) (Gu et al., 2021) into learnable func-\ntions and proposing a parallel scanning method. By overcoming the local perception limitations of\nconvolutional neural networks (CNNs) and the quadratic computational complexity of Transform-\ners (Vaswani et al., 2017), Mamba-based networks (Xu et al., 2024) are widely applied in various\ntasks. For instance, the original Mamba (Gu & Dao, 2023) demonstrates comparable performance\nto Transformers in language modeling, audio generation, and DNA sequence prediction. Vision\nMamba (Vim) (Zhu et al., 2024) marks the first introduction of Mamba to the field of computer"}, {"title": "3 PRELIMINARIES", "content": "3.1 STATE SPACE MODELS\nThe state space models (SSMs) are typically re-\ngarded as contiguous linear time-invariant (LTI)\nsystems (Kalman, 1960), which map an input sig-\nnal $x(t) \\in \\mathbb{R}$ to its output $y(t) \\in \\mathbb{R}$ through a\nhidden state $h(t) \\in \\mathbb{R}^{d \\times 1}$.\n$\\dot{h}(t) = Ah(t \u2212 1) + Bx(t)$,\n(1)\n$Y_{ssm}(t) = Ch(t) + Dx(t)$,\n(2)\nwhere $A \\in \\mathbb{R}^{d \\times d}$, $B \\in \\mathbb{R}^{d \\times 1}$, $C\\in \\mathbb{R}^{1 \\times d}$, $D\\in\n$\\mathbb{R}^{1 \\times 1}$ are weighting parameters, $t \\in \\mathbb{Z}^{+}$, and $h(0)$\nis an initial hidden state.\n3.2 MAMBA ARCHITECTURE\nSince the usage of LTI system, the model parame-\nters remain unchanged, decreasing the performance\nwhen representing changing inputs. To tackle this\nissue, Mamba (Gu & Dao, 2023) propose an implementation of selective SSM (Gu et al., 2021),\nwhich formulates parts of the parameters as functions of a specific input sequence:\nx' = \u03c3(DWConv(State_Projection(x'))),\n(3)\n\u0100 = $A$, B = B-Projection(x') \u2299 \u0394, C = C_Projection(x'),\n(4)\nwhere x' denotes the transformed input and o represents the SiLU activation. Those input-dependent\nparameters and \u00e6' are used by the Parallel Scan (PScan) operator to generate y'ssm, The calculation"}, {"title": "3.3 QUANTIZATION", "content": "of PScan can be expressed as:\n$\\dot{h}(t) = \\overline{A}h(t \u2212 1) + \\overline{B}x(t), \\overline{y}_{ssm}(t) = \\overline{C}h(t)$,\n(5)\nThis temporary output is then element-wisely multiplied with a gated variable z to generate better\noutputs:\nz = \u03c3(Gate_Projection(x)),\n$\\overline{y}_{out} = \\overline{y}_{ssm} \\odot z$.\n(6)\n3.3 QUANTIZATION\nQuantization is generally performed to obtain a low-precision representation (e.g., 4-bit integer)\nfrom a high-precision variable (e.g., 16-bit floating points). For a tensor \u00e6 to be quantized, it can be\nuniformly quantized to b-bits as follows (Jacob et al., 2018):\n$\\hat{x} = (clamp( \\lfloor \\frac{x}{s} + z \\rceil , 0, 2^{b} \u2013 1) \u2013 z) \\cdot s$,\n$s = \\frac{max(x) \u2013 min(x)}{2^{b} -1}$ , $z= \\frac{-min(x)}{s}$,\n(7)\nwhere z is the zero point, s is the scale factor, [.] denotes the rounding-to-nearest operator, clamp is\nthe clipping function."}, {"title": "4 METHOD", "content": "4.1 DIMINISHED EFFECTIVENESS OF HADAMARD TRANSFORMATION\nHadamard transformation is a promising quantization method for LLMs, recognized for its effective-\nness in handling outliers and its computational simplicity and speed. It provides robust performance\nwhile efficiently managing data variability.\nHadamard matrices are square matrices with orthogonal rows and columns, where each element is\neither $\\frac{1}{\\sqrt{m}}$ or $-\\frac{1}{\\sqrt{m}}$ (m is the order of the Hadamard matrix). By multiplying with such a uniformly\ndistributed matrix, each row contributes relatively equally to a given channel, thereby making\nthe extreme values of the channels closer together (Tseng et al., 2024). Additionally, due to the\northogonal nature, the Hadamard matrix can be well integrated into model weights while ensuring\nthe computational consistency.\nWe initially attempt to directly apply this method to Mamba models, particularly to the gate projection,\noutput projection, and the matmul layer. However, the Hadamard transformation is not sufficiently\neffective in normalizing the hard layers mentioned in Figure 1 of the Mamba architecture with\nsignificant outliers, as illustrated in Figure 2(b)(e).\nTo this end, we conduct a thorough analysis of this issue and find that this method fails to align the\nchannel variance of quantization variables, thereby overlooking the distribution consistency between\ndifferent channels. In detail, given a centered data matrix (the columns of the matrix are zero-mean.)\nX (weights or activations) with dimensions (n, m) and the Hadamard transformation matrix H with\ndimensions (m, m), the covariance matrix $C_{XH}$ of the transformed matrix XH can be expressed\nas:\n$C_{XH} = \\frac{1}{n - 1}(XH)^TXH = \\frac{1}{n - 1}H^TX^T XH = \\frac{1}{n - 1}H^T K\u039bK^T H$,\n(8)\nwhere $X^TX = K\u039bK^T$ represents the eigenvalue decomposition, K is the eigenvectors matrix, and\n\u039b is the diagonal eigenvalues matrix. Considering that $H^T K$ and $K^T H$ are transposed matrices of\neach other, the l-th diagonal elements of $C_{XH}$ can be expressed as:\n$(C_{XH})_{ll} = \\frac{1}{n - 1} \\sum_{j=1}^{m} (H^T K)_{lj}^2 \\Lambda_j = \\frac{1}{n - 1} \\sum_{j=1}^{m} \\sum_{i=1}^{m} H_{il} K_{ij} \\Lambda_j$,\n(9)\nwhere $\u039b_j$ is the j-th eigenvalue of \u039b. The complete derivation from Equation 8 to Equation 9 is\nprovided in Appendix A.2. For a given value of l, Equation 9 represents the variance of the l-th\nchannel. As the vector $H_{i,j}$ varies with the l, the channel variances cannot be proven to be numerically\nclose in most cases refers to Appendix A.3. Further, considering that H is a fixed matrix while both\nK and \u039b are input-dependent, it is not feasible for the Hadamard transformation to uniformly adjust\nthe channel variances across all scenarios. This property of Hadamard transformation inevitably\nformulates a distinct distribution for each channel, thus leading to sub-optimal quantization."}, {"title": "4.2 KLT-ENHANCED ROTATION FOR OFFLINE TRANSFORMATION", "content": "To overcome the constrain stated in Section 4.1, we introduce the Karhunen-Lo\u00e8ve Transformation\n(KLT) (Dony et al., 2001) to equalize channel variances. KLT identifies principal components in the\ndata and projects it onto these components, retaining the most critical information by focusing on\ndirections of maximum variance. In practical, the mean value for each channel of the Mamba weights\nand activations is typically close to zero, meeting the applicable conditions of KLT. Specifically, We\napply KLT by performing eigenvalue decomposition on the covariance matrix $C_X$ of the centered\nmatrix X derived from the calibration data.\n$C_X = \\frac{1}{n - 1}X^T X = \\frac{1}{n - 1} K\u039bK^T$.\n(10)\nNext, the KLT-Enhanced rotation matrix $\\overline{H_K}$ can be obtained by applying the KLT to the Hadamard\nmatrix H, as described in Equation 11, and the Equation 8 turns into Equation 12:\n$\\overline{H_K} = KH$,\n(11)\n$\\begin{aligned}C_{X\\overline{H_K}} &= \\frac{1}{n - 1} \\overline{H_K}^T K\u039bK^T \\overline{H_K} = \\frac{1}{n - 1} H^T K^T K\u039bK^T KH\\\\\n&= \\frac{1}{n - 1}H^T I \u039b IH = \\frac{1}{n - 1}H^T \u039b H,\n\\end{aligned}$\n(12)\nwhere I denotes the identity matrix. Consequently, the Equation 9 thus turns to Equation 13:\n$\\begin{aligned}(C_{X\\overline{H_K}})_{ll} &= \\frac{1}{(n - 1)m} \\sum_{j=1}^{m} H_{il} ^2 \\Lambda_j = \\frac{1}{(n - 1)m} \\sum_{j=1}^{m} \\Lambda_j.\n\\end{aligned}$\n(13)\nIn this way, the variance of each channel becomes the same, making quantization much easier. This\ntransformation serves a dual purpose: it not only equalizes the variance among different channels\nbut also embodies the distinctive property of Hadamard matrices, which is their ability to balance\nmaximum values. We also provide detailed steps for the formula of performing KLT rotation\nfollowed by Hadamard rotation in Appendix A.4 to achieve variance balancing. In practice the KLT\nis offline performed by using the calibration data to avoid extra computational costs. Still it can be\nwell-generalized to wider range of inputs (detailed in Appendix A.7).\nTo apply this KLT-Enhanced rotation matrix, we modify the offline transformation in QuaRot (Ashk-\nboos et al., 2024b) for the Mamba structure. As shown in Figure 4, we employ this strategy for the\nLORA module and the inter-block connection (where the output projection, gate projection and the\nstate projection is transformed)."}, {"title": "4.3 SMOOTH-FUSED ROTATION FOR ONLINE TRANSFORMATION", "content": "To mitigate the shortcoming of the Hadamard rotation discussed in Section 4.1 where the online\ntransformation is applied, we introduce the smoothing technique prior to its execution. The moti-\nvation of employing this method is to uniform the channel variances through a smoothing vector.\nTypically, the smoothing factors can be absorbed into the neighbored layers with the quantization of\nT-LLMS (Xiao et al., 2022; Shao et al., 2023). This operation effectively circumvents the demand for\nadditional memory allocation and computational overhead that would arise from the incorporation\nof extra parameters. However, this approach does not align with the Mamba modules due to the\nnon-linear SiLU operation and the complex loop structure of PScan. To this end, two distinct designs\nare proposed for output projection and matrix multiplication, respectively."}, {"title": "5 EXPERIMENTS", "content": "For the output projection layer: We improve the traditional SiLU activation function with Smooth\nSiLU (S-SiLU) (Hu et al., 2024) to meet the needs of smooth-fused quantization:\n$S-SiLU(x, s) = x \\cdot \u03c3(x \\cdot s)$,\n(14)\nwhere x is an activation variable, \u03c3(\u00b7) represents the Sigmoid function, s denotes the introduced\nsmoothing parameter, and ''represents element-wise multiplication. Depicted in Figure 5(a), the\napplication of the S-SiLU function on the gate projection described by Equation 6 can be expressed\nas follows:\n$y_{out} = [y_{ssm} \\odot \u03c3(x_gW_g)]W_o = [y_{ssm} \\odot S-SiLU(x_gW_g, S_{out})]W_o^*$,\n(15)\nwhere $y_{ssm}$ denotes the output activation of the SSM, $W_g^* = \\frac{W_g}{s_{out}}$ and $W_o^* = s_{out} \\cdot W_o$ are\ntransformed weights of the gate projection (denoted with subscript 'g') and the output projection\n(denoted with subscript 'o'), '\u25c7'represents element-wise division, $s_{out}$ is the absorbed smoothing\nfactor, $x_g$ is the input of the gate projection, and $y_{out}$ represents the final output of the Mamba block.\nFor the matrix multiplication layer: We also design a scheme to absorb the smoothing factor for\nthe matrix multiplication operator within the Mamba block. One input stream of the multiplication\nis the output of the C projection in Equation 4, which can directly fuse the smoothing factor $S_{mm}$\ninto the weight of C projection (Wc) as shown in Figure 5(b). Another input stream comes from\nthe output of the parallel scan operator. As shown in Equation 5, the calculation of PScan includes\naddition operator, and the smoothing factor 8mm will be transmitted along two routes on both sides of\nthe addition operator. One route is transmitted through B and absorbed by the weight of B projection\n(WB), and the other route is transmitted through A and absorbed by \u2206, which defined in Equation 3.\nBecause of the existence of exponential calculation in Equation 4, $1/s_{mm}$ becomes \u2013ln(smm) when\ntransmitted to A, and is absorbed by applying the addcmul operator (PyTorch, 2023) to \u2206(1) in\nEuation 16. It is solely applied to the first token of \u2206 ((1)).\naddcmul(\u2212ln(smm), \u0394(1), A) = A\u0394(1) \u2013 ln(smm).\n(16)\nAfter smoothing, the channel variances of activations for the output pro-\njection and the matrix multiplication becomes relatively uniform. Subse-\nquently, we modify and apply the online Hadamard rotation (Ashkboos\net al., 2024b) for the Mamba structure as shown in Figure 6. The\nHadamard matrix H is dynamically applied to the input activation of\nthe output projection and the matrix multiplication, while the transposed\n$H^T$ can be absorbed into corresponding weights."}, {"title": "6 CONCLUSION", "content": "In this paper, we focus on introducing the quantization techniques into the realm of Mamba models.\nFirstly, we identify that significant outliers which challenge the quantization process are present in\ngate projection, output projection, and matrix multiplication, while the unique PScan operator further\namplifies the numerical differences. Secondly, we find that Hadamard transformation method widely"}, {"title": "A APPENDIX", "content": "A.1 LIMITATIONS\nRecently, Mamba models demonstrates superior accuracy across various vision and language tasks,\nindicating its strong capabilities in feature extraction and pattern recognition. However, its deploy-\nment with the quantization methodology still remains largely under explored. We thus propose\nMambaQuant, an accurate and efficient post training quantization framework especially desined for\nthe Mamba family.\nWhile this approach can effectively quantize the weights and activations of Mamba models into 8-bit\nwith less than a 1% drop in accuracy, it struggles to maintain such a high level of accuracy when\nquantizing weights to 4-bit. In addition, we note that the proposed Karhunen-Lo\u00e8ve Transformation\n(KLT) enhanced rotation is efficiently constrained if applied to the online Hadamard rotation. This is\nprimarily due to the additional computation steps introduced by the eigenvalue decomposition (as\nstated in Equation 10) and the application to the Hadamard matrix (as stated in Equation 11). Despite\nthe constrains, we hope that our work could inspire the research interest on Mamba quantization\nwithin the community. We are also committed to extending the KLT-Enhanced rotation method to\nonline transformation in order to achieve better performance in low-bit quantization.\nA.2 DERIVED THE INABILITY OF HADAMARD ROTATION TO ENSURE CONSISTENCY OF\nCOLUMN VARIANCE.\nHadamard properties. An orthogonal matrix Q is a square matrix such that $QQ^T = I$. In this\nwork, we consider only real orthogonal matrices. A rotation matrix is an orthogonal matrix with\n|Q| = 1. A Hadamard matrix is an orthogonal matrix with each element is either $\\frac{1}{\\sqrt{m}}$ or $-\\frac{1}{\\sqrt{m}}$. A\n2x2 Hadamard matrix is defined as follows:\n$H = \\frac{1}{\\sqrt{2}} \\begin{pmatrix}\n1 & 1 \\\\\n1 & -1\n\\end{pmatrix}$\n(17)\nCovariance Calculation. Given a matrix X with dimensions (n, m) with zero mean across each\ncolumn. Alculate the covariance matrix Cx of X:\n$C_X = \\frac{1}{n - 1}X^T X = \\frac{1}{n - 1} K\u039bK^T$,\n(18)"}, {"title": "A.3 THE VARIANCE AFTER HADAMARD ROTATION IS STILL INCONSISTENT", "content": "where K is the eigenvectors matrix, \u039b is the diagonal eigenvalues matrix, n denotes the number of\nrows. We provide a proof based on the above properties of Hadamard that Hadamard cannot achieve\ncolumn variance consistency. In detail, given a Hadamard transformation matrix \u0397 with dimensions\n(m, m).the covariance matrix $C_{XH}$ of the transformed matrix XH can be expressed as:\n$C_{XH} = \\frac{1}{n - 1}(XH)^TXH = \\frac{1}{n - 1} H^T X^T XH$,\n(19)\nSubsituate Equation 18 into Equation 19:\n$C_{XH} = \\frac{1}{n - 1} H^T X^T XH = \\frac{1}{n - 1} H^T K\u039bK^T H$,\n(20)\nHadamard matrix expansion:\n$\\begin{aligned}H = \\begin{pmatrix}\nH_{11} & H_{12} & \\cdots & H_{1m} \\\\\nH_{21} & H_{22} & \\cdots & H_{2m} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\nH_{m1} & H_{m2} & \\cdots & H_{mm}\n\\end{pmatrix},\n\\end{aligned}$\n(21)\nKLT matrix expansion:\n$\\begin{aligned}K = \\begin{pmatrix}\nK_{11} & K_{12} & \\cdots & K_{1m} \\\\\nK_{21} & K_{22} & \\cdots & K_{2m} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\nK_{m1} & K_{m2} & \\cdots & K_{mm}\n\\end{pmatrix},\n\\end{aligned}$\n(22)\nWe define the matrix $P = H^T K$:\n$\\begin{aligned}P = H^T K = \\begin{pmatrix}\n\\sum_{i=1}^m H_{i1} K_{i1} & \\sum_{i=1}^m H_{i1} K_{i2} & \\cdots & \\sum_{i=1}^m H_{i1} K_{im} \\\\\n\\sum_{i=1}^m H_{i2} K_{i1} & \\sum_{i=1}^m H_{i2} K_{i2} & \\cdots & \\sum_{i=1}^m H_{i2} K_{im} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n\\sum_{i=1}^m H_{im} K_{i1} & \\sum_{i=1}^m H_{im} K_{i2} & \\cdots & \\sum_{i=1}^m H_{im} K_{im}\n\\end{pmatrix},\n\\end{aligned}$\n(23)\nSubstitute Equation 23 into Equation 20:\n$C_{XH} = \\frac{1}{n - 1}P\u039bP^T$\n$\\begin{aligned}&= \\frac{1}{n - 1} \\begin{pmatrix}\n\\sum_{i=1}^m P_{i1}^2 \\Lambda_i &  & \\cdots &  \\\\\n & \\sum_{i=1}^m P_{i2}^2 \\Lambda_i &  & \\\\\n\\vdots &  & \\ddots &  \\\\\n &  &  & \\sum_{i=1}^m P_{im}^2 \\Lambda_i\n\\end{pmatrix}\n\\end{aligned}$\n$\\begin{aligned}&= \\frac{1}{n - 1} \\begin{pmatrix}\n(\\sum_{i=1}^m (H_{i1} K_{ij})^2) \\Lambda_j &  & \\cdots &  \\\\\n & (\\sum_{i=1}^m (H_{i2} K_{ij})^2) \\Lambda_j &  & \\\\\n\\vdots &  & \\ddots &  \\\\\n &  &  & (\\sum_{i=1}^m (H_{im} K_{ij})^2) \\Lambda_j\n\\end{pmatrix},\n\\end{aligned}$\n(24)\n$\\begin{aligned}(C_{XH})_{ll} &= \\frac{1}{n - 1} \\sum_{i=1}^m (H_{i1} K_{ij})^2 \\Lambda_j = \\frac{1}{n - 1} \\sum_{j=1}^m \\sum_{i=1}^m H_{il} K_{ij} \\Lambda_j,\n\\end{aligned}$\n(25)\nSince the values of $P_{ij}$ are not equal, the variance for each column of matrix XH (which is the\nvalue on the diagonal of $C_{XH}$) is also not equal. Because the Hadamard transform is a fixed\northogonal transformation, a fixed orthogonal transformation cannot uniformly adjust the variance in\nall directions, resulting in the variance after transformation still being uneven."}, {"title": "A.5 THE CALCULATION PROCESS OF INCREASING PARAMETERS AND ADDING\nCOMPUTATIONAL LOAD", "content": "A.3 THE VARIANCE AFTER HADAMARD ROTATION IS STILL INCONSISTENT\nWe provide an example of a simple random 4x4 matrix after it has undergone the Hadamard transform.\nExample of a simple random 4x4 matrix R and a 4x4 Hadamard matrix H::\n$R = \\begin{pmatrix}\n 3 & 1 & 0 & 1 \\\\\n -2 & -3 & -3 & -3 \\\\\n 1 & 4 & 4 & -1 \\\\\n -2 & 1 & -6 & 6\n\\end{pmatrix}$\n(26)"}, {"title": "A.6 ADDITIONAL RESULTS", "content": "$\\begin{aligned}H = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}\n 1 & 1 & 1 & 1 \\\\\n 1 & 1 & -1 & -1 \\\\\n 1 & -1 & 1 & -1 \\\\\n 1 & -1 & -1 & 1\n\\end{pmatrix}\n\\end{aligned}$\n(27)\nCalculate the covariance matrix CRH of the matrix RH after the Hadamard transform:\n$\\begin{aligned}C_{RH} = \\begin{pmatrix}\n 1.83 & -4.83 & -2.83 & 2.00 \\\\\n -4.83 & 30.50 & 2.17 & -5.67 \\\\\n -2.83 & 2.17 & 8.75 & -1.50 \\\\\n 2.00 & -5.67 & -1.50 & 2.17\n\\end{pmatrix}\n\\end{aligned}$\n(28)\nIn the random example we provided, it can be seen that after the Hadamard rotation, there is a\nsignificant imbalance in the variance across the columns (which are the diagonal values of the\ncovariance matrix $C_{RH}$).\nWe observe the distribution within the actual Mamba network. In the Vim-T network, we compare\nthe variances before and after the Hadamard rotation for the output projection inputs of the 1st, 10th,\nand 20th blocks, as shown in Fig. 9. Additionally, Hadamard rotation is applied to the gate projection\ninputs of the 1st, 10th, and 20th blocks in Fig. 10, as well as to the gate projection weights of the 1st,\n10th, and 20th blocks in Fig. 11."}, {"title": "A.7 GENERALIZATION EVALUTATION OF THE KLT-ENHANCED ROATION", "content": "We estimate the original computational load of the Mamba-2.8b model to be 2.8 TFlops. In Method\n4.2, we employed the online Hadamard technique, which has a complexity of O(nlog2(n)). The\ncomplexity of the online Hadamard transformation is provided in Equation (1) of the paper by (Fino\n& Algazi, 1976). Within the Mamba block, we inserted an online Hadamard transformation of size\n[16,16] for the matmul smoothing and similarly used a [5120,5120] Hadamard transformation for the\noutput projection smoothing. Thus, the percentage increase in computational load is calculated as:\n$\\begin{aligned} (1024 \\times 5120 \\times 16 \\times log_2(16) + 1024 \\times 5120 \\times log_2(5120)) \\times 64 \\approx 0.91\\% \\\\\n 2.8 \\times 10^{12}\n\\end{aligned}$\n(38)\nThis analysis demonstrates that our proposed enhancements introduce minimal increases in both\nparameter count and computational load, making them practical for efficiency-critical applications."}, {"title": "A.8 COMPARISON OF NUMERICAL DISTRIBUTIONS BETWEEN VIT AND VIM MODELS", "content": "4.5 THE CALCULATION PROCESS OF INCREASING PARAMETERS AND ADDING\nCOMPUTATIONAL LOAD\nIn this section, we take the Mamba-2.8b model as an example to illustrate the increase in parameter\ncount and computational load. The Mamba-2.8b model comprises 64 blocks, and we assume a\ntoken quantity of 1024 for our calculations. We detail the computational overhead introduced by our\nsmoothing process. The original parameter count of the Mamba-2.8b model is 2.8 billion. In our\nMethod 4.3, we integrated a smoothing process by replacing the SiLU activation function with the S-\nSiLU activation function, which added 5,120 smoothing scale parameters. Additionally, we modified\nthe mul operation to an addcmul operation, contributing an additional 16 parameters. Consequently,\nthe percentage increase in parameter count is calculated as: .\n$\\frac{((5120 + 16) \\times 64)}{2.8 \\times 10^9} \u2248 0.01\\%$.\n(37)\nWe estimate the original computational load of the Mamba-2.8b model to be 2.8 TFlops. In Method\n4.2, we employed the online Hadamard technique, which has a complexity of O(nlog2(n)). The\ncomplexity of the online Hadamard transformation is provided in Equation (1) of the paper by (Fino\n& Algazi, 1976). Within the Mamba block, we inserted an online Hadamard transformation of size\n[16,16] for the matmul smoothing and similarly used a [5120,5120] Hadamard transformation for the\noutput projection smoothing. Thus, the percentage increase in computational load is calculated as:\n$\\frac{(1024 \\times 5120 \\times 16 \\times log_2(16) + 1024 \\times 5120 \\times log_2(5120)) \\times 64}{2.8 \\times 10^{12}} \u2248 0.91\\%$\n(38)\nThis analysis demonstrates that our proposed enhancements introduce minimal increases in both\nparameter count and computational load, making them practical for efficiency-critical applications."}]}