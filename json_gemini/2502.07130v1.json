{"title": "Unconstrained Body Recognition at Altitude and Range: Comparing Four Approaches", "authors": ["Blake A Myers", "Matthew Q Hill", "Veda Nandan Gandi", "Thomas M Metz", "Alice J O'Toole"], "abstract": "This study presents an investigation of four distinct approaches to long-term person identification using body shape. Unlike short-term re-identification systems that rely on temporary features (e.g., clothing), we focus on learning persistent body shape characteristics that remain stable over time. We introduce a body identification model based on a Vision Transformer (ViT) (Body Identification from Diverse Datasets, BIDDS) and on a Swin-ViT model (Swin-BIDDS). We also expand on previous approaches [26] based on the Linguistic and Non-linguistic Core ResNet Identity Models (LCRIM and NLCRIM), but with improved training. All models are trained on a large and diverse dataset of over 1.9 million images of approximately 5k identities across 9 databases. Performance was evaluated on standard re-identification benchmark datasets (MARS [42], MSMT17 [34], Outdoor Gait [31], DeepChange [35]) and on an unconstrained dataset [3] that includes images at a distance (from close-range to 1000m), at altitude (from an unmanned aerial vehicle, UAV), and with clothing change. A comparative analysis across these models provides insights into how different backbone architectures and input image sizes impact long-term body identification performance across real-world conditions.", "sections": [{"title": "I. INTRODUCTION", "content": "Face recognition algorithms are highly accurate at establishing the unique identity of individuals (e.g., [5], [19], [33]). In natural viewing conditions, however, facial identity information is commonly degraded or obscured (e.g., viewing from a far distance or at an extreme angle). When the face is unusable or inaccessible, information about the shape of the body can constrain identity decisions. Body shape can contribute to person identification by supporting/vetoing uncertain face identifications and/or by establishing a plausible identity match to a gallery image. As such, it can serve as a valuable biometric, even if it provides information that does not uniquely identify an individual.\nEarly attempts to use the body for identification focused on re-identification in a closed-world setting, which aims to track a person in a constrained environment like an airport or train station (for a review, see [39]). In a closed-world environment, algorithms can rely on cues such as clothing that make the problem easily tractable with deep learning. In an open-world, clothing change makes the problem more difficult [1], [7], [14], [16], [22], [23], [26], [36], [38], [26], [23], [16]. As algorithms capable of overcoming clothing change have matured (for a review, see [39]), long-term body identification models have aimed more broadly at identification in highly challenging scenarios [16], [18], [17], [26]. This emphasis coincides with the development of the BRIAR dataset, which contains whole person images, captured at long-range (e.g., 300+ meters), through atmospheric turbulence, and/or from elevated sensor platforms [3]. Images and videos in this dataset are taken across multiple views (yaw, pitch) and with clothing changes that sideline short-term cues that are useful in re-identification scenarios [40]. Long-term body identification with unconstrained data presents a number of unique challenges. Body shape can deform via the movements of the limbs (e.g., arms up or down, leg extended) and/or by changes in posture (e.g., bending, reaching). Texture and albedo information, which are critically important for face identification, have only limited value for clothed bodies. Perhaps most challenging is the complicating factor of clothing change that alters the overall shape of the body (e.g., pants/skirt, running gear/winter coat). Despite the difficulty of the task, there is ample evidence that humans use body information for person identification when the face is unavailable or insufficiently resolved for identification [8], [29], [41].\nThe goal of the present study was to compare four machine learning approaches to real-world (longer-term) body identification. We evaluated two Vision Transformer models (ViT) [6], [24] and two ResNet neural networks [10]. All four models were trained to identify bodies from input images, using a very large dataset compiled from 9 feeder datasets. The use of a common dataset for training allowed us to compare the models on an equal footing.\nIn the first part of the work, we compared the models on four common re-ID datasets. In the second part of the work, we tested the models on the BRIAR test set (BTS) [3]. We also test performance on subsets of data that measure body identification with face included and restricted, at long range, and from overhead. In the third part of the work, we dissect the advantage of the best model to determine whether architecture or image size accounts for its superiority.\nThe contributions of the paper are:\n\u2022\tWe show that ViT models are superior to equivalently-trained ResNet models for body identification.\n\u2022\tWe show that a Swin-ViT model is superior to the other tested models across metrics. This was true for the benchmark datasets and the unconstrained dataset.\n\u2022\tLinguistic pre-training in a ResNet model showed only a small performance advantage over an equivalent non-linguistically trained model.\n\u2022\tWe show that both architecture and image size contributed to the superior performance of the Swin-ViT, but that image size was the critical factor in its high performance."}, {"title": "A. Related Work", "content": "Long-term body identification models can be categorized according to the approach they take to representing body shape information.\n1) 2D Body Shape from Images: The most direct approach is to learn a mapping from variable images of bodies (view, clothing, illumination, distance) to identity [9], [26]. The greatest challenge in this approach has been the limited availability of training data with sufficient variability (especially clothing sets) to learn the task of long-term body identification. Using a ResNet-50 model pretrained on ImageNet [30], the Clothing-Change Feature Augmentation (CCFA) approach [9] augments model training to form meaningful clothing variations in the feature space. The augmented features maximize the change of clothing and minimize the change of identity by adversarial learning. The effectiveness of CCFA was demonstrated with two standard CC-ReID datasets (PRCC-ReID [36] and LTCC-ReID [28].\nThe Non-linguistic Core ResNet Identity Model (NLCRIM) [26] was built on a ResNet-101 backbone pretrained with ImageNet [30]. NLCRIM was trained to map body images to identities using the BRIAR Research Set (BRS) [3]. It was evaluated with the BRIAR Test Set (BTS), which contained identities viewed at multiple distances (up to 1000 meters) that varied widely in yaw and pitch. Extreme pitch conditions were captured from unmanned aerial vehicles (UAVs). All test items included a change of clothing. NLCRIM performed well across all probe distance/pitch conditions. An improved version of this model, with enhanced training and substantially more training data, is tested in the present study.\nA similar direct approach to learning a mapping between whole body images and identity was taken in [15]. A ResNet-50 model was trained from scratch with BRS data. This body encoder was embedded in an end-to-end system that included a trained detector model. The combined model performed well on the unconstrained BTS data.\nThe causality-based autointervention model (AIM1) was proposed to mitigate clothing bias for robust clothes-changing person ReID (CC-ReID) [37]. Specifically, the effect of clothing on model inference was analyzed. A dual-branch structure of clothing and ID was utilized to simulate the causal intervention process and was penalized by a causality loss. Progressively, clothing bias was automatically eliminated with model training, as AIM learned more discriminative identity clues that are independent of clothing. The superiority of the AIM approach over other approaches was demonstrated with two standard CC-ReID datasets (PRCC-ReID [36] and LTCC-ReID [28].\n2) 3D Body Shape Features: To overcome reliance on short-term cues in body images, several models have attempted to reconstruct 3D body shapes for identification ([1], [22]). In the 3D Shape Learning (3DSL) approach, a texture-insensitive 3D shape embedding is extracted from a 2D image by adding 3D body reconstruction as an auxiliary task and regularization [1]. The use of the 3D reconstruction regularization forces a decoupling of the 3D body shape from the visual texture, enabling the model to acquire discriminative 3D shape ReID features. An adversarial self-supervised projection (ASSP) model is used to provide a 3D shape ground truth. The effectiveness of the approach was demonstrated with common person ReID datasets (e.g., Market1501 [43]) and clothes-changing datasets (e.g., PRCC-ReID [36] and LTCC-ReID [28]).\nIn other work, the 3DInvarReID model [22] begins by disentangling identity from non-identity components (pose, clothing shape, and texture) of 3D clothed humans. Next, accurate 3D clothed body shapes are reconstructed, and discriminative features of naked body shapes for person ReID are learned. The model was found to be effective for disentangling identity and non-identity features in 3D clothed body shapes, using a dataset (CCDA [22]) that contains a wide variety of human activities and clothing changes.\n3) Linguistic Models: Body models based on linguistic descriptors (e.g., \u201ccurvy,\u201d \u201clong-legged\u201d) encode shape via the complex myriad of features captured by single and small groups of words [26]. Work in psychology [13] and computer graphics [32] has demonstrated that a linear mapping can be learned from human-generated body descriptions (27 words) to the coefficients of a PCA trained with 3D body scans [25]. Motivated by this finding, the Linguistic Core ResNet Identity Model (LCRIM) was developed using an ImageNet pretrained ResNet augmented with linguistic annotation pretraining. This linguistic core was then trained to map images to identity [26]. Although the LCRIM model performed at a level similar to NLCRIM, the fusion of the two models performed substantially better than either model alone. This suggests that the two models encode complementary information about body shape.\nIn related work, linguistic body descriptions were leveraged for ReID in CLIP3DReID [23]. This was done by integrating human descriptions with visual perception using a pretrained CLIP model. CLIP was used to automatically label body shapes with linguistic descriptors. A student model's local visual features were then aligned with shape-aware tokens derived from CLIP's linguistic output. The CLIP image encoder and the 3D SMPL [25] identity spaces were used in combination to align the global visual features. The effectiveness of CLIP3DReID was demonstrated using PRCC-ReID [36] and LTCC-ReID [28]."}, {"title": "II. METHODS", "content": "We examined four distinct approaches to body shape recognition: two vision transformer models (BIDDS and Swin-BIDDS) and two ResNet-based models (LCRIM and NLCRIM) [26]. Each architecture employs a unique strategy for capturing body shape features across varying conditions.\n1) Vision Transformer Models: The BIDDS model is built on a Vision Transformer architecture. We used a ViT-B/16 pre-trained on ImageNet-1k. The core model processes 224 x 224 sized images with patch size 16. We modified the original ViT architecture by replacing the classification head with a custom fully connected layer that maps to a 2048-dimensional embedding. This embedding space is designed to capture essential body shape features crucial for person identification. Following core training, we fine-tune the model on the BRS1-5 datasets (cf. [3]), increasing image size to 384\u00d7384 to capture more detailed features of the fine-tuning BRIAR data, while maintaining the same architectural structure.\nSwin-BIDDS is based on the hierarchical vision transformer, which uses shifted windows (Swin Transformer, [24]). This type of transformer was developed to better adapt transformers from the language domain to the vision domain, by accommodating large variations in the scale of visual entities. The shifted windowing scheme of the Swin Transformer is more efficient than a standard ViT, because it limits self-attention computation to non-overlapping local windows, while supporting cross-window connections. This hierarchical structure progressively merges patches and is well-suited to modeling at various scales.\n2) ResNet-Based Models: These models leverage the ResNet architecture with different core training strategies. Both are pre-trained with ImageNet-1k [30]. Additionally, LCRIM incorporates semantic body descriptors into its training process (See Section II-C.1 for details). Its architecture consists of a ResNet-50 base augmented with an encoder-decoder structure that maps to a linguistic feature space before the final identification layers. The encoder pathway compresses the representation (2048 \u2192 512 \u2192 64 \u2192 16), while the decoder pathway (16\u219224 \u2192 30) reconstructs linguistic body attributes. NLCRIM is identical to LCRIM, but without linguistic training.\nBy comparison to the published version of NLCRIM and LCRIM [26], there were three changes: 1.) a new training regime with hard triplet mining was added [12]; 2.) there was a substantial increase in the quantity of training data; and 3.) the ResNet-101 was replaced by a ResNet-50 (see below for details).\nB. Training Methods\nAll models employed hard triplet loss with negative mining [12]. This operates on image triplets: an anchor image, a positive sample (same identity), and a negative sample (different identity). The loss calculation measures the Euclidean distances between the anchor and positive samples and between the anchor and negative samples. We selected the most challenging negative samples (i.e., those closest to the anchor in the embedding space) within each batch. This hard negative mining encourages the model to learn features that effectively differentiate between similar body shapes. We also ensured that each batch included pairs or small sets of images from the same person. All four models use the Adam optimizer and incorporate dynamic sampling, whereby triplet selection is adapted based on the current state of the embeddings. This ensures that the models continuously encounter challenging examples throughout training. The training process employs a low learning rate (10-5) and weight decay (10-6) to prevent over-fitting while maintaining stability. We applied standard augmentations during training, including random horizontal flip, color jitter, random grayscale, and gaussian blur.\nC. Training Data\nAn important feature of our approach is the use of a large and diverse collection of training datasets (see Table I). The datasets include over 1.9 million images across 4,788 identities. To benchmark the models for the experiments (see Section III), a subset of test data were withheld from three of the training sets (MSMT17 [34], MARS [42], and DeepChange [35]), and a fourth set was designated solely for testing purposes (Outdoor Gait [31]), with none of its data included in the training phase. This diverse collection spans multiple scenarios, from ground-level views to aerial perspectives. The training images were primarily derived from video files, with bodies cropped and processed to maintain their aspect ratios while being placed on a 224x224 (384x384 for Swin-BIDDS) black background. Some of the datasets include clothing change and some do not.\n1) Additional Pre-training: Only LCRIM had additional pre-training. This comprised a specialized linguistic pre-training phase using the HumanID [27] and MEVA [4] datasets. The HumanID dataset provides diverse viewing scenarios of 297 identities, including approach sequences, walking perpendicular to the camera, and elevated viewpoints. Each identity was annotated by 20 human observers using 30 standardized body descriptors, with the final descriptors averaged across annotators (cf., [26]). The MEVA dataset, comprising over 9,300 hours of video across varied activities and scenarios, contributed an additional 158 identities. Images from these datasets were used to train LCRIM's initial ability to map between visual features and linguistic body descriptions. The model was subsequently tuned for mapping image to identity using the datasets listed in Table I.\n2) Additional Fine-tuning: Subsequent to the large-scale training using the datasets in Table I, both the BIDDS and Swin-BIDDS were fine-tuned using the BRIAR training data (BRS1-5), which contained 697,348 images of 995 unique identities. Note: this training data was included in the large scale training and repeated in the fine-tune stage. During this fine-tuning, BIDDS processes images at an increased size of 384 x 384, allowing for more detailed feature extraction. The Swin-BIDDS model used the 384 \u00d7 384 images for both the large-scale training and the fine-tuning.\nIn summary, the strategy across models combines specialized linguistic pre-training, extensive foundation-model training, and targeted fine-tuning to fully exploit the capabilities of each architectural approach. The processing of video-derived images and standardization of input sizes ensures consistent training conditions across the models."}, {"title": "III. EXPERIMENTS", "content": "A. Benchmark Dataset Tests\n1) Methods: The models were evaluated first with the test data from four benchmark Re-ID datasets (MSMT17 [34], MARS [42], Outdoor Gait [31], and DeepChange [35]). DeepChange is a clothes change database; MSMT17, MARS, and Outdoor Gait are not. For MSMT17, MARS, and Outdoor Gait the test data were split into a gallery (half of the items) and a probe set (remaining items). Because the data are derived from video, the split was made to assure minimally similar gallery and probe items. Images were processed by the models, and identity templates were formed for gallery items by averaging embeddings of the images for each identity. Identification was measured by comparing probe image embeddings to the gallery templates. For DeepChange, all identities had multiple clothing sets. The original DeepChange dataset, however, used similar clothing for each identity across both the probe and gallery sets. Thus, to ensure that clothing did not become a dominant cue, we restructured the partitioning of the probe and gallery sets. Specifically, we designated a single clothing set for all probe instances and then ensured that each identity's gallery templates had different clothing.\n2) Results: Table II summarizes the verification (TAR@FAR) and identification (Rank) performance for the benchmark datasets. The Swin-BIDDS model performed best on all metrics and for all datasets. At a general level, the transformer-based models (BIDDS and Swin-BIDDS) performed more accurately than the ResNet-based models (NLCRIM and LCRIM). This was consistent across all metrics and for all datasets. Although comparisons with benchmarks in the literature are not always possible (or transparent), the rank 1 performance of NLCRIM, BIDDS and Swin-BIDDS exceeded the state-of-the-art (SOTA) for MARS (cf. previous Rank 1 SOTA: MARS (.908) [11]). The Swin-BIDDS exceeded the SOTA for MSMT17 (cf. previous Rank 1 SOTA: MSMT (.917) [2]) and DeepChange (previous Rank 1 SOTA (.48) [35]).\nIt is worth noting that our models were trained on multiple datasets in which clothing was not a reliable cue to identity. The strong performance of BIDDs and Swin-BIDDS on the no-clothes-change datasets indicates that the models utilize body shape cues, and other identifying information not linked to clothing (head structure). The strong performance of the Swin-BIDDS model on DeepChange (clothes-change) is consistent with this conclusion.\nB. Identification in Unconstrained Datasets\n1) Methods: Next, the models were evaluated using the most challenging of the datasets. Specifically, we used the BRIAR Test Set (BTS) summarized in Table III. The first test was conducted on the entire dataset and subsequent tests were done on targeted partitions of the data into probe items. These partitions included: a.) face-included items; b.) face-restricted items, c.) long-range items taken at distance, and d.) items captured from overhead using an UAV.\nTo test identification, gallery embedding templates were formed by averaging the embeddings across all still images for each identity. Probe embedding templates were derived from video segments, indicating the specific frames to be used from the video. The embedding for each probe was computed by averaging the frame-level embeddings across this subset of frames. As a result, the videos for a given identity each contributed multiple probe embeddings (one per segmented clip).\n2) Results: Table IV shows that Swin-BIDDS performed substantially better than the other models on nearly all metrics. The table also shows the consistency of this advantage across the test partitions. As for the benchmark datasets, the ViT-based models (BIDDS and Swin-BIDDS) were clearly superior to the ResNet models. Although it is difficult to compare across partitions, especially given the different numbers of items in each set, Rank 1 and Rank 20 performance suggest that Swin-BIDDS provides consistently strong identity information for probes with and without a visible face, probes at a distance, and probes taken from overhead (UAV). Moreover, despite differences in the overall performance of the four models, none collapsed on the partition tests, highlighting the diversity and quantity of the training data in the success of the models.\nC. Ablation Experiments: Architecture vs. Input Size?\nThe Swin-BIDDS model performed best on the benchmark datasets and the challenging BRIAR data. It was also consistently best on all partitions of the BRIAR data. In these experiments, we test factors that might account for the superior performance of Swin-BIDDS over its closest competitor, BIDDS. The models differed in two ways. The backbone architecture changed from a ViT model (BIDDS) to a Swin-ViT (Swin-BIDDS) and the input image size for the core model training changed from 224x224 sized images (BIDDS) to 384 \u00d7 384 sized images (Swin-BIDDS). Both models were fine-tuned with 384 \u00d7 384 images.\nTechnically, changes in architecture and image size are independent. However, a simultaneous change in both is not uncommon, due to the fact that the Swin-ViT scales far more efficiently than ViT with increasing input image size. It does this by implementing a hierarchical structure of shifting attention windows, giving Swin-ViT the desirable feature of linear complexity as a function of image size [24]. Thus, a change from a ViT to Swin-ViT is often undertaken with the goal of minimizing the computational resources required for an increase in image size.\n1) Methods: To tease apart whether architecture, image size, or both were responsible for the performance boost, we trained additional models as comparators. To test directly for the effects of image size independent of architecture, we compared two Swin-ViT models. The first was the Swin-BIDDS model tested in the previous experiments. This is a Swin-ViT model that used image size 384 \u00d7 384 for core training and fine-tuning. For clarity, we refer to this as Swin-BIDDS(384,384). For the comparison model, we trained Swin-BIDDS(224,224). This is a Swin-ViT model that used 224 x 224 images for core training and fine-tuning.\nTo determine the impact of the image size independent of architecture we compared a ViT model with a Swin-ViT model, keeping image size constant. Specifically, the Swin-BIDDS(224,224) model was compared with a BIDDS(224,224). This latter is a version of the BIDDS(224,384) model used in previous experiments, but with the image size for fine-tuning lowered to 224.\n2) Results: Plots showing the CMC and ROC of these models appear in Figure 2. Both plots indicate that the performance of the Swin-BIDDS(384,384) surpasses the other models primarily based on its processing of the larger image size. There is also a smaller contribution of the Swin-ViT architecture, which is seen more clearly in Table V. Metrics for each model appear in the top rows. At the bottom of the table, the first row shows the effects of changing architecture from the ViT to Swin-ViT. This is computed as the difference between the two architecture comparator networks. All 4 metrics increase with that change. The final row of Table V shows the effects of changing image size from the smaller (224, 224) size to larger (384, 384) size. This is computed as the difference between the two image size comparator networks. Again, all 4 metrics increase, but by a substantially larger margin.\nAs a final check on the consistency of image size as the critical factor in the superior performance of Swin-BIDDS over BIDDS, we conducted the same comparisons on the benchmark datasets. The results appear in Figure 3 and show again that image size is the driving factor in the superior performance of Swin-BIDDS over BIDDS. In these benchmark datasets, changing the backbone architecture had variable effects for different datasets."}, {"title": "IV. CONCLUSIONS AND FUTURE WORKS", "content": "We implement four long-term body identification models based on ResNet (LCRIM, NLCRIM) and ViT (BIDDS, Swin-BIDDS) architectures. The models are tested on their ability to identify bodies in benchmark re-identification datasets and in a highly challenging unconstrained dataset that includes people viewed at a distance, from elevated vantage points, and with clothing variability. An important aspect of our approach is that we train the models on a large-scale, diverse dataset of nearly two million images of nearly 5,000 identities. We showed that vision transformer architectures consistently outperformed ResNet architectures."}, {"title": "ETHICAL IMPACT STATEMENT", "content": "We have read the guidelines for the Ethical Impact Statement. The development of body identification models does not involve direct contact with human subjects, and therefore does not require approval by an Institutional Review Board. Instead, images/videos of human subjects are incorporated as training and test data for body identification models. We used only datasets (videos and images of people) that have been pre-screened and approved for ethical data collection standards by a United States government funding agency, XXXX. The standards applied for dataset approval require consent from the subjects who are depicted in the images/videos for use in research. Specifically, the standards are set in accordance with Health Services Research and applicable privacy policies, statutes, and federal regulations. Images/videos of subjects who appear in publications require additional consent. We followed these guidelines carefully. Images displayed in the paper have been properly consented and are displayed according to the published instructions for use of the dataset.\nThe development and study of biometric identification algorithms entails risk to individuals and societies. It is clear that these systems can have negative impacts if they are misused. They can potentially threaten individual privacy and can impinge on freedom of movement and expression in a society. The goal of our work is to better understand how these systems work. The results of this work can have both positive and negative societal impacts. On the positive side, knowing the types of representations created by body identification networks can help to minimize person identification errors. It can also help to set reasonable performance expectations thereby limiting the scope of use. On the negative side, the knowledge gained can potentially be used to manipulate a system in unethical ways and to create synthetic images that can be misused or misinterpreted.\nThese risks are mitigated by the potential for positive societal impact. Body identification algorithms can be used to locate missing people (including children). They can also be used in law enforcement to identify individuals implicated in crimes. Legitimate and societally-approved use can protect the general public from harm. Of note, body identification systems can be used in combination with face identification systems to improve identification accuracy, thereby minimizing erroneous identifications."}]}