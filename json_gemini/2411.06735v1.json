{"title": "MULTI-MODAL FORECASTER: JOINTLY PREDICTING TIME SERIES AND TEXTUAL DATA", "authors": ["Kai Kim", "Howard Tsai", "Rajat Sen", "Abhimanyu Das", "Zihao Zhou", "Abhishek Tanpure", "Mathew Luo", "Rose Yu"], "abstract": "Current forecasting approaches are largely unimodal and ignore the rich textual data that often accompany the time series due to lack of well-curated multimodal benchmark dataset. In this work, we develop TimeText Corpus (TTC), a carefully curated, time-aligned text and time dataset for multimodal forecasting. Our dataset is composed of sequences of numbers and text aligned to timestamps, and includes data from two different domains: climate science and healthcare. Our data is a significant contribution to the rare selection of available multimodal datasets. We also propose the Hybrid Multi-Modal Forecaster (Hybrid-MMF), a multimodal LLM that jointly forecasts both text and time series data using shared embeddings. However, contrary to our expectations, our Hybrid-MMF model does not outperform existing baselines in our experiments. This negative result highlights the challenges inherent in multimodal forecasting. Our code and data are available at https://github.com/Rose-STL-Lab/Multimodal_Forecasting.", "sections": [{"title": "INTRODUCTION", "content": "Deep learning has become the predominant method in forecasting large-scale time series Zhou et al. (2022); Wang et al. (2022); Woo et al. (2023), but most existing methods consider time series as a single data modality. In practice, time series data do not exist in isolation and there are rich text meta-data available. Large Language Models (LLM) such as GPT Brown et al. (2020) and LLaMA Touvron et al. (2023) have demonstrated marvelous success in processing and understanding text data. It is intriguing to exploit text as an new modality to improve forecasting. On one hand, text can provide context to the dynamics governing the associated time series. On the other hand, generating text alongside numerical forecasts also offer valuable interpretations for the predictions.\nHowever, effectively combining and forecasting these two data modalities remains a complex challenge Zhang et al. (2024). The differences in structure and content between numerical time series and text data pose challenges to their simultaneous integration in forecasting models. Traditionally, the community has studied time series and natural language processing separately. To the best of our knowledge, there is no large-scale, well-curated, paired time series and text dataset for forecasting. The only exception is Time-MMD Liu et al. (2024) but their text data is synthetic.\nSeveral early attempts have been made at multimodal forecasting. Kumar et al. (2022); Obst et al. (2019) propose to augment time series forecasting with textual meta-data, but only predict time series. Other works proposed to treat time series forecasting as a \"modified language task\". For example, LLMTime Gruver et al. (2023) proposes a zero-shot approach by representing the time series as a string of numerical digits inputs to LLMs. Time-LLM Jin et al. (2023) transforms the time series input into text prototype representation and projects the output back to numerical forecasts. Others use a pre-trained language model with fine-tuning or prompt engineering for forecasting Zhou et al. (2023); Cao et al. (2023). While these approaches show promise in leveraging LLMs for time series forecasting, they primarily focus on fitting numerical data into the language model paradigm rather than addressing the broader challenge of multimodal forecasting."}, {"title": "RELATED WORK", "content": "Traditional Time Series Forecasting. Classical time series models such as AR, ARIMA, the VAR models (Zivot & Wang, 2006) and exponential smoothing models (McKenzie, 1984) rely on over-simplified modeling assumptions and often cannot cope with the complexity of real-world data. The rise of deep learning has shown great success in forecasting time series at scale. For example, DeepAR (Salinas et al., 2020), Koopman Neural Forecaster (Wang et al., 2022) and PatchTST (Nie et al., 2022) are all recent models that meet or exceed traditional ARIMA and ETS forecasting methods on many data domains.\nDeep Learning and Transformers. MLPs have been used for time-series forecasting in the popular N-BEATS model (Oreshkin et al., 2019). TiDE (Das et al., 2023) uses a Multi-layer Perceptron (MLP)-based encoder-decoder framework. Gated recurrent units like GRU and LSTM were introduced to handle the autoregressive nature of time-series data, though these methods are typically data hungry and have complex parameterizations (Parmezan et al., 2019).\nTransformers have recently seen a surge in popularity for time series forecasting. LongTrans (Li et al., 2019) uses an attention layer with LogSparse design, reaching near-linear space and time complexity. Informer (Zhou et al., 2021) uses the ProbSparse self-attention mechanism to achieve sub-quadratic dependency on the length of the context. Autoformer (Wu et al., 2021) employs trend and seasonal decomposition with a sub-quadratic self-attention mechanism. FEDFormer (Zhou et al., 2022) incorporates a frequency-enhanced structure, while Pyraformer (Liu et al., 2021) introduces pyramidal self-attention that achieves linear complexity and can attend to different granularities.\nTransformer architectures must be parameterized well in order to be effective (Nie et al., 2022). Zeng et al. (2023) recently observed that a simple linear projection model can outperform transformer based architectures on long horizon time series forecasting tasks. DLinear (Zeng et al., 2023) learns a linear mapping from context to the horizon, pointing to deficiencies in sub-quadratic approximations to the self-attention mechanism. PatchTST (Nie et al., 2022) resolved this by showing that feeding contiguous patches of time-series as tokens to the vanilla self-attention mechanism can beat the performance of DLinear in long-term forecasting benchmarks.\nAll of the works mentioned above only use a single time series modality and benchmark on time series datasets that are mostly stationary. Our work builds on the success of deep learning methods by building a multi-modal model that forecasts both text and time."}, {"title": "LLM and Time Series.", "content": "The use of Large Language Models (LLMs) as natural time-series predictors have been explored in varying levels of sophistication, from zero-shot forecasting to incorporating time series embeddings in various levels within the language model Zhang et al. (2024).\nPrompt-based methods. LLMTime (Gruver et al., 2023) uses LLM for time series forecasting in a zero-shot manner. They represent the time series as a string of numerical digits and view time series forecasting as next-token prediction in text. Time-LLM (Jin et al., 2023) proposes to reprogram pre-existing LLMs by translating time series data into a text-like format. Zhou et al. (2023) use pretrained Large Models from vision and language to fine-tune on all major types of tasks involving time series. Chang et al. (2024) forecast textual events, rather than time series data, using a RAG-based method.\nQuantization and Alignment. Quantization based methods convert numerical data into discrete representations for input to LLMs, whereas alignment-based methods train a separate encoder for time series data and align the encoded time series to the semantic space of language models (Zhou et al., 2023). Many different models across different domains adopt one or both of these techniques. Auto-TTE (Chung et al., 2023) quantizes ECG graphs into discrete formats and forecasts ECG signals conditioned on text reports. Cao et al. (2023) propose TEMPO, a novel integration of seasonal and trend decomposition into the pre-trained transformers.\nOverall, these techniques focus on using or tuning pre-trained LLMs for time-series forecasting. We build on these methods by predicting text as an additional modality, and use techniques from papers like LLMTime (Gruver et al., 2023) to benchmark our results.\nMultimodal Forecasting. This work is the first investigation of a multimodal forecasting model that incorporates textual and numerical time-series data. Previous works have mainly focused on study in specific domains and single modality. Kumar et al. (2022) explored a multi-modal sales forecasting network that combines data from news articles with numerical data such as historical sales and holiday information. Obst et al. (2019) leveraged daily weather reports to predict time series of national electricity consumption, average temperature, and wind-speed.\nRecently, Liu et al. (2024) jointly proposed Time-MMD, a new multimodal dataset, and MM-TSFlib, a new multimodal forecasting model. MM-TSFlib, forecasts time series by first modeling the time and textual series independently, then combining the outputs using a linear weighting mechanism. techniques differ from ours in two important ways. First, their time series pipeline relies on a frozen LLM to generate the textual embeddings (Liu et al., 2024), whereas our hybrid model can learn the textual embeddings. Second, Time-MMD does not study how multi-modal inputs can be used to create textual predictions, which we show beats naive text-only prediction models and can serve as a powerful tool for event forecasting."}, {"title": "DATASET", "content": "Weather. National weather service dataset 1 consists of 2 separate sources of data: a series of text discussions about weather forecasting from the National Weather Service and a series of numerical weather recordings from Visual Crossing. Using the National Weather Service archives, we obtain daily forecast discussions about the different regions across the United States. In conjunction with the text data, we collect daily weather recordings in Washington D.C. by Visual Crossing Weather API. The finalized dataset, which includes weather recordings and forecast discussions, ranges from Jan 1st, 2014 to December 1st, 2023, for a total of 3621 days. The time series categories include daily maximum temperature, minimum temperature, average temperature, dew point, humidity, precipitation, and average windspeed. Different time series on the same day are paired with the same text data. We use these daily weather recordings to forecast the future values in Washington D.C., with forecasting horizons of 1 days through 7 days.\nMedical. MIMIC-III\u00b2 comprises of hourly data from over 58,000 hospital admissions of more than 38,000 patients. It spans a period of 12 years, from 2001 to 2012, enabling researchers to study"}, {"title": "\u041c\u0415\u0422\u041dOD", "content": "We present Hybrid Multi-Modal Forecaster (Hybrid-MMF), a novel framework to enhance both text and time series forecasting accuracy by encoding both historial time series and correlated text and projecting them into different output heads.\nVARIABLE DEFINITIONS\nThese variables are used in the figure presentation below.\n\u2022 I: Input window size or number of time steps.\n\u2022 C: Number of input time-series variables.\n\u2022 N: Length of tokenized text per time step.\n\u2022 E: Embedding dimension for the text data.\n\u2022 H: Hidden dimension after projection.\n\u2022 O: Number of output time-series variables or text tokens.\n\u2022 V: Vocabulary size for LLM tokenization.\nPROBLEM DEFINITION\nGiven the time series dataset of length T, denoted as $X = [X_1,\\ldots, X_T]$ with each $X_t|_{t\\in[1,T]} \\in \\mathbb{R}^d$ being a state vector at time t, as well as the text data $Y = [Y_1,\\ldots, Y_T]$ with $Y_t$ representing the text corpus collected at time t, our goal is to learn a functional mapping f such that\n$[X_{t-L}, ..., X_{t-1}, Y_{t-L},\\ldots, Y_{t-1}] \\rightarrow [X_{t-L},..., X_{t-1}, Y_{t-L},\\ldots, Y_{t-1}]$\nwhere L is the input sequence's time lag and H is the forecasting horizon. L is chosen to ensure the temporal dependencies and trends in the input sequences are considered. H is task-dependent."}, {"title": "HYBRID MODEL", "content": "Our hybrid model leverages Llama 3.1 8B, a state-of-the-art pre-trained language model, to embed text inputs. Llama 3.1 8B was chosen for its robust capability in handling complex natural language processing tasks, especially in generating high-quality embeddings that capture deep semantic and contextual relationships. This allows our model to understand and predict textual events with high precision.\nFor numerical inputs, we project the data into the textual embedding space using either the Llama 3.1 8B model or the BAAI/bge-small-en-v1.5 model. The motivation behind this choice is that both models are highly effective in generating embeddings that can represent multiple modalities. By chosing these embeddings, the model can effectively handle the multimodal nature of the forecasting task, predicting both time-series data and textual events seamlessly. The final combined embedding forms a comprehensive representation, enhancing the model's ability to capture dependencies across different modalities."}, {"title": "STAGE 1: PRETRAINING TIME-SERIES WITH TEXT EMBEDDINGS", "content": "In the first stage, the model processes two types of inputs:\nInput Time Series: $X_{time} \\in \\mathbb{R}^{I\\times C}$, where I is the number of time steps, and C represents the number of variables or channels in the time-series data.\nTokenized Input Text: $X_{text} \\in \\mathbb{R}^{I\\times N}$, where N is the tokenized text length per time step.\nThe text data is converted into embeddings $E_{text} \\in \\mathbb{R}^{I\\times E}$ using either the LLaMA tokenizer or the BAAI/bge-small-en model from Sentence Transformers. The text embeddings are projected to"}, {"title": "EXPERIMENTS", "content": "We conducted a series of experiments to evaluate our proposed hybrid model against many text-only and numerical-only forecasting models, which served as our baseline. A full explanation of the evaluated models can be found in ??. The textual metrics and numerical metrics were evaluated independently. A complete collection of the results can be found in Appendix C."}, {"title": "BASELINE MODELS", "content": "We implement the following baseline models for comparison. For language model-based approaches (Text2Text, TextTime2Text, TextTime2Time, and TextTime2TextTime), we use the unsloth/Meta-Llama-3.1-8B-Instruc model under zero-shot, in-context learning (with one example), and fine-tuned settings. The context length varies from 900 to 5500 tokens depending on the input-output prediction days and specific dataset characteristics. All other baselines are evaluated in zero-shot settings only.\n\u2022 Input_Copy: A simple baseline that copies the input as the output, serving as a performance floor for both text and time-series predictions.\n\u2022 NLinear: A linear projection time series forecasting model proposed by (Zeng et al., 2023), focused solely on numerical predictions.\n\u2022 Linear_Text_Embedding: An extension of the NLinear model that incorporates text embeddings as covariates, using the bge-small-en-v1.5 embedding model.\n\u2022 Text2Text: Language models that process and forecast textual events only using unsloth/Meta-Llama-3.1-8B-Instruc.\n\u2022 TextTime2Text: Language models that take both text and time-series inputs (with time-series data converted to text) but forecast only textual events.\n\u2022 TextTime2Time: Language models that process both text and time-series inputs but predict only future time-series values. The textual input provides context for numerical forecasting.\n\u2022 TextTime2TextTime: Language models that handle both modalities for input and output, where numerical data is stringified and concatenated with text. Capable of predicting both text and time-series data."}, {"title": "EXPERIMENT SETUP", "content": "We employed different evaluation metrics for text and time-series predictions. For time-series predictions, we used Root Mean Squared Error (RMSE). To evaluate the forecast text, we use the following metrics.\n\u2022 Cosine Similarity: We calculated cosine similarity as the dot product between the sentence embeddings of the generated and reference texts.\n\u2022 METEOR: METEOR (Banerjee & Lavie, 2005) is a metric that considers synonyms, stemming, and paraphrases when comparing generated and reference texts.\n\u2022 ROUGE: ROUGE (Lin, 2004) is a family of metrics which assess similarity by comparing the number of overlapping n-grams.\n\u2022 GPT Semantic Score: We ask ChatGPT-40 to rank the similarity between the the reference text and the predicted text using a scale from 1-10.\n\u2022 GPT F1 Score: As a last textual evaluation metric, we prompt ChatGPT-4o to first compare line by line the ground truth and prediction text. Then, we count up the number of true positive, false positive, and false negative facts to compute the Precision, Recall, and F1 scores. Full details can be found in Appendix A.\nIn our forecasting setting, the time lag is the same as forecasting horizon. For Climate, we forecast up to 7 days and (1-1) means 1 day ahead forecasting with lag 1. For Medical, we forecast 6 time steps ahead. We tested the model's performance across increasing horizons for all tasks. Our experiments are based on univariate forecasting. We chose patient heartrate and temperature in Washington D.C. as our forecasting variables for the climate and medical dataset, respectively."}, {"title": "QUANTITATIVE RESULTS", "content": "complexity of predicting future outcomes and underscores the need for larger datasets and further research to fully tap into the potential of multimodal forecasting."}, {"title": "DISCUSSION", "content": "In this study, we explored different strategies for multimodal forecasting by combining both text and numerical data to improve prediction accuracy. Our results showed that the fine-tuned model consistently outperformed both the few-shot and zero-shot settings. Our hybrid model matched but did not exceed these baselines. We conjecture that this is due to the inherent difficulty in improving on textual forecasting and the inability for models to adapt well to the time-series token in the model's embedding space.\nWe also found that incorporating text embeddings improved the numerical predictions in both the NLinear and fine-tuned models. This suggests that textual context can help enhance the model's ability to recognize patterns in numerical data. However, despite these improvements, our hybrid model\u2014which was designed to forecast both text and numbers\u2014did not meet expectations and failed to surpass the NLinear baseline.\nA significant challenge we encountered is the inherent difficulty of textual event forecasting. Taking example from our data domains, tomorrow's weather is a highly dynamic, unpredictable system. The data we used may not have been large or varied enough to enable the hybrid model to learn effectively.\nIn conclusion, while the fine-tuned model and the addition of text embeddings led to clear improvements, the hybrid model didn't achieve the breakthrough we were aiming for. This highlights the"}, {"title": "ADDITIONAL EXPERIMENTAL DETAILS", "content": "LLM FINETUNES\nWe fine-tune the unsloth/Meta-Llama-3.1-8B-Instruct model using Low-Rank Adaptation (LORA) (Hu et al., 2021) to improve computational efficiency. LoRA allows us to update only specific layers of the model, such as the attention and feed-forward layers, while freezing the remaining parameters. This approach significantly reduces the memory and computational cost associated with traditional fine-tuning.\nThe text data is tokenized using the LLaMA tokenizer (Touvron et al., 2023), while the numerical data is fed directly into the model without normalization. During fine-tuning, the model learns to predict both text and numerical outputs, effectively handling multimodal input. This approach allows the model to generate coherent text and accurate numerical predictions, which are subsequently compared to baseline models.\nFEW-SHOT AND ZERO-SHOT EVALUATION\nIn our few-shot learning setup, the model is given only one random input-output example from training dataset. This approach allows us to evaluate the model's ability to generalize with minimal data, which is crucial in real-world scenarios where labeled data may be scarce. By measuring the performance on such limited examples, we can assess the model's adaptability and effectiveness in low-data environments.\nFor zero-shot evaluation, the model is tested without any task-specific fine-tuning or exposure to the target data. This setting evaluates the pre-trained model's ability to generalize to entirely new tasks based solely on its prior knowledge. Zero-shot results provide insight into the model's flexibility and robustness, which is especially valuable when no task-specific data is available for model training or fine-tuning."}, {"title": "PROMPT", "content": "PROMPT FOR WEATHER DATASET\nPrompt:\nGiven the weather information of the first 1 day, predict the weather information of the next 1 day. Output the result strictly in the following JSON format and no additional text:\n{\n\"day_2_date\": \"YYYY-MM-DD\",\n\"day_2_weather_forecast\": \"Weather description\",\n'day_2_temp\": \"A Float Number\"\n}\nInput\n{\n\"day_1_date\": \"2014-01-05\",\n\"day_1_weather_forecast\": \"{weather information}\",\n\"day_1_temp\": 37.1\n}\nModel Response:\n{\n\"day_2_date\": \"2014-01-06\",\n\"day_2_weather_forecast\": \"weather information\",\n\"day_2_temp\": 36.6\n}\nPROMPT FOR MEDICAL DATASET\nPrompt:\nGiven the medical information of the first 1 day, predict the medical information of the next 1 day. Output the result strictly in the following JSON format and no additional text:\n{\n\"day_2_date\": \"YYYY-MM-DD\",\n\"day_2_medical_notes\": \"Medical description\",\n\"day_2_Heart_Rate\": \"A Float Number\"\n}\nInput:\n{\n\"day_1_date\": \"2131-05-06\",\n\"day_1_medical_notes\": \"{medical information}\",\n\"day_1_Heart_Rate\": 170.5\n}\nModel Response:\n{\n\"day_2_date\": \"2131-05-07\",\n\"day_2_medical_notes\": \"medical information\",\n\"day_2_Heart_Rate\": 169.815\n}"}, {"title": "PROMPT FOR GPT SCORE", "content": "Prompt:\nYou are a helpful assistant capable of evaluating the semantic similarity between two summaries. The semantic score you provide should be a number between 1 and 10, where 10 represents the highest level of semantic similarity (meaning the summaries convey almost identical information), and 1 represents the lowest level of semantic similarity (meaning the summaries convey entirely different or unrelated information). The score should reflect how closely the meanings and key details of the two summaries align. You should only give me the number, nothing else.\nGround truth: {ground_truth}\nOutput: {output}\nModel Response:\n{\n\"score\": 5\n}"}, {"title": "PROMPT FOR GPT4 TEXT EVALUATOR", "content": "Prompt:\nYou are a helpful assistant capable of evaluating the similarity, accuracy, and consistency of a ground truth weather forecast, and predicted weather forecast for the same day. By comparing the semantic meaning of each sentences line by line, count the number of\n1) (True Positive): Correct information presented in the prediction,\n2) (False Postive): Incorrect information presented in the prediction,\n3) (False Negative): Information not presented in the prediction.\nMake sure to include explanation for each step. The end of the output should contain the counts of true positives, false positives, and false negatives.\nThe total count:\nTP total count:\n- FP total count:\n- FN total count:\nGround truth: {ground_truth}\nOutput: {output}\nModel Response:\n1. **Identifying True Positives (TP)**\n- Heavy rainfall expected in the Mid-Atlantic aligns with both forecasts.\nHeavy precipitation general trend noted in both forecasts.\nSnow and ice possibly in the Midwest and Northeast matches the mention of snow/ice in northern regions.\n2. **Identifying False Positives (FP)**\nThe predicted forecast assigns specific amounts of rainfall (2 to 4 inches), which is not indicated in the ground truth forecast, leading to revelation of specific metrics not originally stated.\nThe predicted forecast introduces a slight risk of excessive rainfall which is not mentioned at all in the ground truth forecast.\n3. **Identifying False Negatives (FN)**\nThe ground truth mentions the Ohio Valley and Central Appalachians but the predicted forecast shifts focus more towards southern Plains and Mid-South, dropping some regions.\nRisks of isolated flash flooding due to saturated soils and specifics about heavy precipitation in areas like the Northern Rockies and Great Basin mentioned in the ground truth are neglected in the predicted forecast.\nThe reference to much below normal temperatures affecting parts of Alaska is not mirrored in the predicted forecast."}]}