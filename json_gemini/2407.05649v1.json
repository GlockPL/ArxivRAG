{"title": "Graph Attention with Random Rewiring", "authors": ["Tongzhou Liao", "Barnab\u00e1s P\u00f3czos"], "abstract": "Graph Neural Networks (GNNs) have become fundamental in graph-structured\ndeep learning. Key paradigms of modern GNNs include message passing, graph\nrewiring, and Graph Transformers. This paper introduces Graph-Rewiring Atten-\ntion with Stochastic Structures (GRASS), a novel GNN architecture that combines\nthe advantages of these three paradigms. GRASS rewires the input graph by super-\nimposing a random regular graph, enhancing long-range information propagation\nwhile preserving structural features of the input graph. It also employs a unique\nadditive attention mechanism tailored for graph-structured data, providing a graph\ninductive bias while remaining computationally efficient. Our empirical evalua-\ntions demonstrate that GRASS achieves state-of-the-art performance on multiple\nbenchmark datasets, confirming its practical efficacy.", "sections": [{"title": "Introduction", "content": "Graph neural networks (GNNs) have emerged as a key innovation in machine learning for tasks\ninvolving complex relational data represented as graphs [65, 70]. Central to the advancement of\nGNNs are message passing neural networks (MPNNs), exemplified by the Graph Convolutional\nNetwork (GCN) [38]. By aligning computation with the structure of the graph, MPNNs offer a\nstrong inductive bias for graph-structured data. However, despite recent advances, limitations persist\nin challenges such as underreaching, which occurs when distant nodes cannot communicate [1],\noversmoothing, which occurs when node representations become indistinguishable [51, 14], and\noversquashing, which occurs when task-relevant information is excessively compressed [6, 62].\nGraph rewiring techniques, aimed at modifying the graph structure to enhance information flow, have\nshown promise in overcoming these limitations [2]. However, it comes with the risk of distorting the\nstructure of the graph, leading to loss or misinterpretation of relational information [20].\nBuilding GNNs with Transformers [63] presents another promising solution to the above-listed\nproblems. However, many Graph Transformers (GTs) face scalability challenges as a result of\ntheir reliance on quadratic-time Transformers [55]. This makes their applicability to large graphs\ncomputationally challenging. To address these issues, the General, Powerful, Scalable (GPS) Graph\nTransformer [55] combines linear-time Transformers with MPNNs to offer a method with O(|V|+|E|)\ntime complexity, where |V| denotes the number of vertices and |E| the number of edges. Exphormer\n[60], which generalizes sparse Transformers [74] to graphs, represents another promising approach.\nAll these architectures, however, inherit their attention mechanisms from Transformers that are\nintended for sequences, which are often not optimal for graph-structured data.\nOur Contribution. We aim to design a GNN that encompasses the graph inductive bias of message\npassing, the improved information flow through graph rewiring, and the representation power of\nthe attention mechanism. Towards this end, we propose Graph-Rewiring Attention with Stochastic\nStructures (GRASS), a novel GNN inspired by these three paradigms. GRASS rewires the input\ngraph by superimposing a random regular graph. At each layer, it applies an attentive node aggregator"}, {"title": "Related Work", "content": "In this section, we briefly summarize some of the prior work related to the main concepts of GRASS.\nGraph Rewiring. Graph rewiring involves the strategic modification of graph structures by adding,\nremoving, or reallocating edges. Techniques include deterministic adjustment of the graph structure\nbased on spectral properties [28, 62, 2, 20] and probabilistic rewiring of the graph based on learned\ndistributions [53]. DRew [31] innovatively introduces layer-dependent rewiring, demonstrating\noutstanding performance on tasks that depend on long-range relationships. While graph rewiring\nenhances MPNNs by improving graph connectivity, and thus information propagation, they risk\ncompromising informative structural properties of graphs, degrading its inductive bias [20].\nGraph Attention and Graph Transformers. Graph Attention Network (GAT) [66] and its suc-\ncessors [8, 76, 68, 10] have leveraged similar attention-based approaches to achieve remarkable\nresults [8, 25]. Despite their success, these MPNNs are intrinsically limited by the structure of the\ninput graph, leading to persistent challenges such as underreaching [1], oversmoothing [51, 14], and\noversquashing [6, 62]. At a theoretical level, the expressivity of MPNNs is often no more powerful\nthan the Weisfeiler-Lehman graph isomorphism test [71, 59, 40].\nGraph Transformers [73, 22, 39, 72, 47, 35] employ the Transformer architecture [63] to transcend\nthese limitations of MPNNs. Instead of relying on message passing within neighborhoods of the\ninput graph, GTs facilitate simultaneous attention between all nodes, unrestricted by the structure of\nthe graph. However, its O(|V|2) time complexity becomes prohibitive for large graphs. Linear and\nsparse Transformers, exemplified by Performer [18] and BigBird [74], respectively, achieve linear\nruntime with respect to the number of tokens. However, integrating a graph inductive bias into these\nTransformers can be challenging, since they are designed for Euclidean data [46].\nThe General, Powerful, Scalable (GPS) Graph Transformer [55] represents a hybrid approach,\nmerging MPNNs' inductive bias with the global perspective of Linear-Time Transformers. Exphormer\n[60] presents a natural generalization of BigBird to graphs by incorporating local, global, and\nrandom attention. It employs random regular expander graphs as an alternative to BigBird's random\nattention patterns on sequential data. However, these models, along with prior GTs, focus on\nadapting Transformer-style dot-product attention to graph-structured data, rather than investigating\ngraph-specialized alternatives. Their performance does not exceed that of more tailored attention\nmechanisms, such as GRIT [46].\nGraph Encoding. By integrating structural information directly into nodes and edges, graph\nencoding significantly improves the expressivity of GNNs, especially for GTs [25]. Techniques\nsuch as graph Laplacian positional encoding (LapPE) [25] and random walk structural encoding\n(RWSE) [23] have been instrumental in this regard. The Graph Inductive Bias Transformer (GRIT)\n[46] introduces a novel encoding strategy-relative random walk probabilities (RRWP)\u2014which is\nparticularly powerful in capturing structural properties [46]. However, like its GT predecessors, GRIT\nsuffers from O(V|2) complexity, limiting its applicability to large graphs.\nIn summary, while existing methodologies have significantly advanced the field of graph neural\nnetworks, they are often constrained by limited expressivity within the MPNN framework, compu-\ntational inefficiencies, or challenges in incorporating graph inductive biases. GRASS emerges as a"}, {"title": "Methods", "content": "In this section, we introduce the design of GRASS. We begin by examining the desirable qualities of\na GNN, which guide our architectural design. Subsequently, we introduce the components of GRASS\nby following the order of data processing in our model, and also describe the role of each component.\nDesign Goals. We center our design around what we consider to be the key characteristics of an\neffective GNN. We will focus on the processing of nodes (N1-N3) and edges (E1-E2), as well as the\nscalability (SI) of the proposed model.\nN1. Permutation Equivariance. Unlike tokens in a sentence or pixels in an image, nodes in a graph\nare unordered, and therefore the model should be permutation equivariant by construction. Since\nreordering the nodes of a graph does not change the graph, permuting the nodes of the input\ngraph of a GNN layer should result in the same permutation of its output [65]. Formally, let\nf(X, E, A) be the function computed by a layer, where X : \\mathbb{R}^{|V| \\times n_{\\text{node}}} represents node features\nwith $n_{\\text{node}}$ dimensions, E : \\mathbb{R}^{|V|\\times|V|\\times n_{\\text{edge}}} represents edge features with $n_{\\text{edge}}$ dimensions, and\nA: {0,1}^{|V|\\times|V|} represents the adjacency matrix (edge weights are considered scalar-valued\nedge features). If the layer is permutation equivariant and (Xout, Eout) = f(Xin, Ein, A), then\n(PXout, PEoutPT) = f(PXin, PEinPT, PAPT) for an arbitrary permutation matrix P.\nN2. Effective Communication \u2013 The model should facilitate long-range communication between\nnodes. Numerous real-world tasks require the GNN to capture interactions between distant\nnodes [24]. However, MPNN layers, which propagate information locally, frequently fail in this\nregard [46]. A major challenge is underreaching, where an MPNN with l layers is incapable of\nsupporting communication between two nodes xi, xj with distance d(xi, xj) > I [1]. Another\nchallenge is oversquashing, where the structure of the graph forces information from a large\nset of nodes to squash through a small set of nodes to reach its target [62]. A node with a\nconstant-size feature vector may need to relay information from exponentially many nodes\n(with respect to model depth), leading to excessive compression of messages in deep MPNNS\n[1]. Addressing these issues is crucial for effectively capturing long-distance relationships.\nN3. Selective Aggregation \u2013 The model should only aggregate information from relevant nodes and\nedges. MPNN layers commonly update node representations by unconditionally summing or\naveraging messages from neighboring nodes and edges [65]. In deep models, this can lead to\noversmoothing, where the representation of nodes from different classes becomes too similar in\nthe process of repeated aggregation [14]. MPNN like GCN [38] have been shown to behave\nas low-pass filters in feature vectors [51]. To address this issue, nodes should only aggregate\ninformation from relevant neighbors, instead of doing so unconditionally, in order to maintain\ndistinguishability of node representations when required by the task.\nEl. Relationship Representation \u2013 The model should effectively represent the relationships between\nnodes with edges. Edges in graph-structured data often convey meaningful information about\nthe relationship between the nodes it connects [29]. However, many contemporary GNNs do not\nuse edge features [32, 71, 47], only support scalar-valued edge features (edge weights) [38, 21],\nor never update edge features across layers [72, 60]. In addition to the semantic relationship\nrepresented by edge features of the input graph, structural relationship can be represented by\nedge encodings added by the model [55]. To inform the model about the relationship between\nnodes, edge representations should incorporate information from both edge features and edge\nencodings, and be meaningfully utilized by layers.\nE2. Directionality Preservation \u2013 The model should preserve and utilize information carried by\nedge directions. Many graphs representing real-world relationships are inherently directed\n[57]. Although edge directionality has been shown to carry important information for various\ntasks, many GNN variants require undirected graphs as input, to prevent edge directions from\nrestricting information flow [57]. It would be beneficial for the model's expressivity if edge\ndirectionality information could be preserved without severely limiting communication.\nS1. Efficient Computation \u2013 The model should have O(|V| + |E|) time complexity for its non-\nprecomputable operations. The number of edges |E| in a connected directed graph with |V|"}, {"title": "Graph Encoding", "content": "Extracting structural information plays an important role in graph-structured learning and is crucial\nfor Relationship Representation. To this end, GRASS applies relative random walk probabilities\n(RRWP) encoding [46] and degree encoding [72] to represent structural relationships.\nRRWP Encoding. RRWP encoding has been shown to be very expressive both theoretically and\npractically [46], serving as a major source of structural information for the model. To calculate random\nwalk probabilities, we first obtain the transition matrix T, where $T_{ij}$ represents the probability of\nmoving from node i to node j in a random walk step. It is defined as T = D-1A : [0,1]^{|V|\\times|V|},\nwhere A \u2208 {0,1}^{|V|\\times|V|} is the adjacency matrix of the input graph G, and D \u2208 \\mathbb{N}^{|V|\\times|V|} is\nits degree matrix. The powers of T are stacked to form the RRWP tensor P, with $P_{h,i,j}$ representing\nthe probability that a random walker who starts at node i lands at node j at the h-th step. Formally,\nP = [T, T2, ..., Tk] : [0,1]^{k\\times|V|\\times|V|}, where k is the number of random walk steps. The diagonal\nelements $P_{:,i,i}$ where i \u2208 VG are used as node encodings, similarly to RWSE [23]. The rest are\nused as edge encodings when the corresponding edge is present in the rewired graph H. All node\nencodings undergo Batch Normalization (BN) [36] to improve their distribution, while batch noise\nalso acts as a regularizer that increases the model's resistance to noise in the random walk probabilities\nproduced by structural noise in the input graph. Here, n denotes the dimensionality of hidden layers.\n$x^{RW}_i = W_{\\text{node-enc}}BN(P_{:,i,i}) : \\mathbb{R}^n$\n$e^{RW}_{ij} = W_{\\text{edge-enc}}BN(P_{:,i,j}) : \\mathbb{R}^n$\nAs required by Efficient Computation, operations slower than O(|V| + |E|) time must be precom-\nputable. P only needs to be computed once per dataset because it depends only on the adjacency\nmatrix A of the input graph. To further improve performance through pipelining, the CPU looks up\nP:,i,i for each i \u2208 |VG| and P:,i,j for each edge (i, j) \u2208 Eg to hide latency and conserve VRAM.\nDegree Encoding. Degree encoding is another simple yet powerful way to represent structural\ninformation [72]. For graphs in which the number of possible degrees is limited, the model could\nprecompute a degree embedding tensor E : \\mathbb{R}^{D+\\times D-\\times n}, where D+ and D- denote the maximum\nout- and in-degree of the dataset, while $d^+(i)$ and $d^-(i)$ represent the out- and in-degree of node i.\n$x^{\\text{deg}}_i = E_{d^+(i),d^-(i),:} : \\mathbb{R}^n$\nFor graphs in which degrees can take a wide range of values, it could be more efficient to obtain\ndegree encoding with a linear layer Wdeg, at the cost of expressivity. Here, || denotes concatenation.\n$X^{\\text{deg}}_i = W_{\\text{deg}}BN(d^+(i) || d^-(i)) : \\mathbb{R}^n$"}, {"title": "Random Rewiring", "content": "To address the issues of underreaching and oversquashing for Effective Communication, GRASS\nrewires the input graph by superimposing a random regular graph. While deterministic rewiring and\nnon-regular graphs may also improve connectivity, we present the theoretical advantages of using\nrandom regular graphs. We also provide details on random graph generation and input graph rewiring.\nEffects on Diameter. The diameter of the input graph upper-bounds the distance between any\ntwo nodes, and thus the number of layers for an MPNN to propagate messages between them.\nSuperimposing a random regular graph can drastically decrease the diameter of the input graph. The\nleast integer d that satisfies $(r-1)^{d-1} \\ge (2 + \\varepsilon)r|V|\\log |V|$ is the upper bound of the diameter\nof a random r-regular graph with |V| nodes with r > 3 and \u025b > 0 [7]. Since adding edges to a\ngraph never increases its diameter, the diameter d of the rewired input graph is upper-bounded by\n$d \\in O(\\frac{\\log |V|}{\\log r})$ when r \u2265 3. Subsequently, all nodes would be able to communicate with each other\nwith $O(\\frac{\\log |V|}{\\log r})$ message passing layers, significantly reducing the risk of underreaching even on\nlarge graphs. In addition, the diameter of a graph is a trivial upper bound of its effective resistance\n[26], which has been shown to positively correlate with oversquashing [6]. Intuitively, it upper bounds\nthe \"length\" of the bottleneck through which messages are passed.\nEffects on Internally Disjoint Paths. Since oversquashing can be attributed to squeezing too many\nmessages through the fixed-size feature vector of a node [1], increasing the number of internally\ndisjoint paths between two nodes can reduce oversquashing by allowing information to propagate\nthrough more nodes in parallel. Intuitively, it increases the \u201cwidth\u201d of the bottleneck. A random\nr-regular graph with r > 2 almost certainly has a vertex connectivity of r as $|V| \\to \\infty$ [27]. Menger's\nTheorem then lower-bounds the number of internally disjoint paths by a graph's vertex connectivity\n[30]. This provides another perspective on how the random regular graph reduces oversquashing.\nEffects on Spectral Gap. Oversquashing has been shown to decrease as the spectral gap of a graph\nincreases, which is defined as \u5165\u2081, the smallest positive eigenvalue of the graph's Laplacian matrix\n[37]. It has been proven that a random r-regular graph sampled uniformly from the set of all r-regular\ngraphs with |V| nodes almost certainly has $\\mu < 2\\sqrt{r} - 1 + 1$ as $|V| \\to \\infty$, where \u03bc is the largest\nabsolute value of nontrivial eigenvalues of its adjacency matrix [52]. Since the graph is r-regular, its\ni-th adjacency matrix eigenvalue \u03bc\u2081 and i-th Laplacian matrix eigenvalue \u5165\u2081 satisfy \u5165\u2081 = r - \u03bc\u03b5 [45],\nlower-bounding the spectral gap with $\\lambda_1 > r - 2\\sqrt{r} - 1 - 1$. Therefore, the random regular graph\nalleviates oversquashing on large input graphs by increasing the spectral gap of the rewired graph.\nGenerating Random Regular Graphs. We generate random regular graphs with the Permutation\nModel [52] that we describe here and with pseudocode in Appendix A. For a given positive and even\nparameter r \u2265 2, and for the input graph G = (VG, EG), we randomly generate a corresponding\nr-regular graph by independently and uniformly sampling random permutations \u03c31, \u03c32, ..., \u03c3r from\nS|VG|, the symmetric group defined over the nodes of the graph G. Using these random permutations,\nwe construct a random pseudograph R = (VG, E\u00f1), where the edge set E\u00f1 of the graph R is\n$E_R = \\bigsqcup_{i \\in V_G, j \\in [r]} {\\{ i, \\sigma_j(i) \\}}$\nwhere \u2210 denotes the disjoint union of sets. The resulting graph R is a random regular pseudograph,\nand the probability that R is any regular pseudograph with |VG| nodes and degree r is equal [7]. Being\na pseudograph, R might not be simple: it might have self-loops and multi-edges. Even when |VG| is\nlarge, the probability that R is simple\u2014that it does not have multiple-edges and self-loops\u2014would"}, {"title": "Attention Mechanism", "content": "Many GTs emulate the structure of Transformers designed for Euclidean data [22, 39, 72, 35, 60].\nMeanwhile, GRASS uses attentive node aggregators with attention scores computed by MLP edge\naggregators, which is a more tailored attention mechanism for graph-structured data. Figure 3\nillustrates its structure in detail. The proposed attention mechanism is unique in the way it uses\nedge representations as the medium of attention weights. To satisfy the needs of Relationship\nRepresentation, edge representations must be updated alongside node representations. Unlike a node,\nwhose neighborhood is a set of nodes, a directed edge always has one head and one tail which is an\nordered pair. An undirected edge can be represented by two directed edges with opposite directions,\nwhich allows us to generalize this observation to undirected graphs. With an ordered input, an edge\ncan use an MLP as its aggregator, offering better expressivity by MLP's universal approximation [34]\nwhile preserving Permutation Equivariance. Assuming Relationship Representation is satisfied, edge\nfeatures would already represent node relationships, which we can use as attention weights ai,j after\napplying a linear layer and taking element-wise softmax [9] to obtain a probability distribution.\nThe proposed attention mechanism is defined as follows, where \u2299 denotes the Hadamard product, d\ndenotes in-degree after random rewiring, and N- denotes in-neighbors.\ns_{i,j} = \\text{dropout} \\left( \\log \\left( d^{-}(j) \\right) \\exp \\left( W_{\\text{attn-edge}} \\cdot e_{i,j}^{l-1} \\right) \\right) : \\mathbb{R}^{+n}\na_{i,j} = \\frac{s_{i,j}}{\\sum_{h \\in N^-(j)} s_{h,j} + \\varepsilon} : \\mathbb{R}^{+n}\nx^l_j = W_{\\text{tail-tail}} x^{l-1}_j + \\sum_{i \\in N^-(j)} a_{i,j} \\odot \\left( W_{\\text{tail-head}} x^{l-1}_i + W_{\\text{edge-tail}} e_{i,j}^{l-1} \\right) : \\mathbb{R}^{n}\ne_{i,j}^l = W_{\\text{edge-head}} x^{l-1}_i + W_{\\text{edge-tail}} x^{l-1}_j + W_{\\text{attn-edge}} s_{i,j} + W_{\\text{edge-edge}} e_{i,j}^{l-1} : \\mathbb{R}^{n}\nIn calculating attention weights, GRASS scales the input to the softmax operation by the logarithm\nof the number of keys, which is equal to the in-degree of that node after the random edges are added,"}, {"title": "Interpretations of GRASS", "content": "A Message Passing Perspective. GRASS is an MPNN on a noisy graph. In an MPNN, information\nis propagated along the edges of the input graph, defined by its adjacency matrix [65]. GRASS can\nbe seen as an MPNN that injects additive and multiplicative noise into the adjacency matrix, through"}, {"title": "Conclusion", "content": "GRASS demonstrates the effectiveness of random rewiring and our graph-tailored attention mecha-\nnism by achieving state-of-the-art performance, surpassing more elaborate methods of graph rewiring,\nas well as more computationally demanding Graph Transformers. To improve the reproducibility of\nour results, our codebase will be made publicly available on GitHub.\nLimitations. The output of GRASS is inherently random due to random rewiring. The relationship\nbetween performance variance and r is demonstrated in Table 4. In scenarios that strictly require\ndeterministic model output, the random number generator used for random rewiring should be seeded\nwith the input graph, making the deployment of GRASS more complicated in these scenarios."}]}