{"title": "Explainable Distributed Constraint Optimization Problems", "authors": ["Ben Rachmut", "Stylianos Loukas Vasileiou", "Nimrod Meir Weinstein", "Roie Zivan", "William Yeoh"], "abstract": "The Distributed Constraint Optimization Problem (DCOP) formulation is a powerful tool to model cooperative multi-agent problems that need to be solved distributively. A core assumption of existing approaches is that DCOP solutions can be easily understood, accepted, and adopted, which may not hold, as evidenced by the large body of literature on Explainable AI. In this paper, we propose the Explainable DCOP (X-DCOP) model, which extends a DCOP to include its solution and a contrastive query for that solution. We formally define some key properties that contrastive explanations must satisfy for them to be considered as valid solutions to X-DCOPs as well as theoretical results on the existence of such valid explanations. To solve X-DCOPs, we propose a distributed framework as well as several optimizations and suboptimal variants to find valid explanations. We also include a human user study that showed that users, not surprisingly, prefer shorter explanations over longer ones. Our empirical evaluations showed that our approach can scale to large problems, and the different variants provide different options for trading off explanation lengths for smaller runtimes. Thus, our model and algorithmic contributions extend the state of the art by reducing the barrier for users to understand DCOP solutions, facilitating their adoption in more real-world applications.", "sections": [{"title": "1. Introduction", "content": "The Distributed Constraint Optimization Problem (DCOP) (Modi, Shen, Tambe, & Yokoo, 2005; Petcu & Faltings, 2005; Fioretto, Pontelli, & Yeoh, 2018) formulation is a powerful tool to model cooperative multi-agent optimization problems. DCOPs are used for modeling many problems that are distributed by nature and where agents need to coordinate their decisions (represented by value assignments) to minimize the aggregated constraint costs. This model is widely employed for representing distributed problems such as meeting scheduling (Maheswaran, Tambe, Bowring, Pearce, & Varakantham, 2004b), wireless sensor networks (Yeoh, Felner, & Koenig, 2010), multi-robot teams coordination (Zivan, Yedidsion, Okamoto, Glinton, & Sycara, 2015; Pertzovskiy, Zivan, & Agmon, 2023), smart grids (Miller, Ramchurn, & Rogers, 2012), smart homes (Fioretto, Yeoh, & Pontelli, 2017; Rust, Picard, & Ramparany, 2022), and large-scale satellite constellations (Zilberstein, Rao, Salis, & Chien, 2024).\nOne of the core assumptions of the existing approaches is that DCOP solutions can be easily understood, accepted, and adopted in these various applications. Unfortunately, this assumption is often invalid and there is often a strong need for AI systems to explain their recommendations or solutions to human users, as evidenced by the large body of literature on Explainable AI (XAI) (Gunning, Stefik, Choi, Miller, Stumpf, & Yang, 2019; Miller, 2019; Sreedharan, Chakraborti, & Kambhampati, 2020; Byrne, 2023). This need is even more important in applications where DCOP agents represent human users who are affected by the choice of value assignments in the DCOP solution. For example, in a meeting scheduling problem, it is important for human users to be able to query their agents to understand why their meetings are scheduled at particular times.\nTo address this limitation, in this paper, we propose the Explainable DCOP (X-DCOP) model, which extends a DCOP to include its solution, and a query regarding that solution. Motivated by insights from the broader XAI literature, we consider contrastive queries due to their relevance in decision-making scenarios (Krarup, Krivic, Magazzeni, Long, Cashmore, & Smith, 2021; Vasileiou, Xu, & Yeoh, 2023; Zehtabi, Pozanco, Bolch, Borrajo, & Kraus, 2024). Contrastive queries are of the form \"Why is solution A chosen over an alternative solution B?\" or, in the case of X-DCOPs, \u201cWhy is variable $x_1$ assigned value $d_1$ instead of an alternative value $d_2$?\u201d. A solution to an X-DCOP is a contrastive explanation that provides counterfactual reasoning on the impact of assigning the alternative value instead of the original value (i.e., the increase in constraint cost and the source of this increase), justifying why the original value is better than the alternative.\nTo solve X-DCOPs, we propose a distributed framework called CEDAR as well as several optimizations and suboptimal variants to compute the explanations. We also include a human user study to evaluate the user understanding and satisfaction of the explanations; the study shows that users, not surprisingly, prefer shorter explanations over longer ones. Finally, our empirical computational results on random graphs and meeting scheduling benchmarks show that CEDAR can scale to large X-DCOP problems with 50 agents, and the different versions of CEDAR trade off explanation lengths for smaller runtimes."}, {"title": "2. Background: DCOPs", "content": "A Distributed Constraint Optimization Problem (DCOP) (Modi et al., 2005; Petcu & Faltings, 2005; Fioretto et al., 2018) is a tuple $(A, X, D, F, \\alpha)$, where:\n\u2022 $A = \\{a_i\\}_{i=1}^n$ is a set of agents.\n\u2022 $X = \\{x_i\\}_{i=1}^n$ is a set of variables.\n\u2022 $D = \\{D_x\\}_{x \\in X}$ is a set of finite domains and each variable $x \\in X$ can be assigned values from the set $D_x$.\n\u2022 $F = \\{f_i\\}_{i=1}^m$ is a set of constraints, each defined over a set of variables: $f_i: \\Pi_{x \\in x_{f_i}} D_x \\rightarrow \\mathbb{R} \\cup \\{\\infty\\}$, where infeasible configurations have $\\infty$ costs and $x_{f_i} \\subset X$ is the scope of $f_i$.\n\u2022 $\\alpha : X \\rightarrow A$ is a mapping function that associates each variable to one agent.\nA solution $\\sigma$ is a value assignment for a set of variables that is consistent with their respective domains. We use the term $var(\\sigma)$ to denote the set of variables whose values are assigned in $\\sigma$. A grounded constraint $f\\downarrow_{\\sigma}$ is the constraint $f$ grounded for the value assignments in the solution"}, {"title": "3. Explainable DCOP", "content": "The Explainable DCOP (X-DCOP) model extends the standard DCOP model to allow a user to ask an agent a contrastive query for a given solution of the DCOP. Specifically, an X-DCOP is defined by a tuple $(P, \\sigma, Q)$, where:\n\u2022 $P$ is a DCOP instance.\n\u2022 $\\sigma$ is a complete solution to the DCOP $P$.\n\u2022 $Q = (a_Q, \\sigma_Q, \\hat{\\sigma}_Q)$ is a query defined by a tuple, where $a_Q$ is the agent that is asked the query; $\\sigma_Q \\subseteq \\sigma$ is the value assignment in the solution $\\sigma$ for a subset of variables $var(\\sigma) \\supseteq var(\\sigma_Q)$; and $\\hat{\\sigma}_Q$ is an alternative value assignment for that same subset of variables $var(\\sigma_Q) = var(\\hat{\\sigma}_Q)$. In other words, for each variable $x \\in var(Q), \\sigma_Q(x) \\neq \\hat{\\sigma}_Q(x)$."}, {"title": "4. X-DCOP Framework: CEDAR", "content": "We now describe Constraint-based Explanation via Distributed Algorithms and Reasoning (CEDAR), our framework for solving X-DCOPs. We first start with a fairly naive and straight-forward framework (pseudocode presented in Algorithm 1), before describing optimizations and variants that we can also consider. At a high level, when an agent $a_Q$ receives a query $Q = \\langle a_Q, \\sigma_Q, \\hat{\\sigma}_Q \\rangle$, it first identifies all of its own grounded constraints that are relevant to the query through the getOwnGroundedConstraints($\\cdot$) function for both the original and alternative solutions $\\sigma_Q$ and $\\hat{\\sigma}_Q$, respectively. Those grounded constraints are stored in their respective sets $F^{a_Q}_{\\mu_{\\sigma_Q}}$ and $F^{a_Q}_{\\mu_{\\hat{\\sigma}_Q}}$ (Lines 1-2). Specifically, the getOwnGroundedConstraints($\\cdot$) function accepts a solution $\\bar{\\sigma}$ (that is either the original or alternative solution) as its argument and returns exactly all the constraints whose scope includes a variable $x$ associated with the agent $a_Q$ (i.e., $\\alpha(x) = a_Q$) and is also a variable in the query (i.e., $x \\in var(\\bar{\\sigma})$) grounded with $(\\sigma \\setminus \\sigma_Q) \\cup \\bar{\\sigma}_Q$.\nNext, it identifies all of the other grounded constraints that it is unaware of (because the scope of those constraints do not include any of its variables). To do so, it sends a REQUEST message to each agent whose variables are in the query (Lines 4-6). When an agent receives such a REQUEST message, it also identifies all its own grounded constraints for the query and returns them to the sender (Lines 13-14).\nOnce agent $a_Q$ receives all the grounded constraints from all the other agents, it stores all those constraints in the appropriate sets $F^{a_Q}_{\\mu_{\\sigma_Q}}$ or $F^{a_Q}_{\\mu_{\\hat{\\sigma}_Q}}$ (Lines 15-18), which are then combined with its own grounded constraints in the respective sets $F_{\\downarrow_{\\sigma_Q}}$ and $F_{\\downarrow_{\\sigma_Q}}$ (Lines 8-9). These sets then contain all grounded constraints whose scope includes a variable in the query. Finally, it evaluates the costs $F_{\\downarrow_{\\sigma_Q}}(\\sigma_Q)$ and $F_{\\downarrow_{\\hat{\\sigma}_Q}}(\\hat{\\sigma}_Q)$ of the solution $\\sigma_Q$ and the alternative $\\hat{\\sigma}_Q$, respectively, and returns them (Lines 10-12).\nCEDAR is guaranteed to terminate and will return a valid explanation if one exists (see Theorems 2 and 3)."}, {"title": "4.1 Optimizations", "content": "We now describe two optimizations O1 and O2 that can be applied within CEDAR to improve it.\nOptimization O1 (Returning Shortest Explanations): For an explanation to be valid, it is required that $F_{\\hat{Q}} (\\hat{Q}) \\ge F_{\\downarrow_{Q}} (\\sigma_{Q})$. However, as illustrated in Example 4, it can be the case that there exist smaller subsets of grounded constraints $F_{\\hat{Q}}^* \\subseteq F_{\\hat{Q}}$ whose cost $F_{\\mu_{\\hat{\\sigma}_Q}}^* (\\hat{\\sigma}_Q) \\ge F_{\\downarrow_{\\sigma_Q}}(\\sigma_Q)$ is also greater than or equal to the cost of the original solution. Motivated by our user study, which shows that human users prefer shorter explanations (see Section 6), our first optimization seeks to find a minimal subset of grounded constraints $F_{\\hat{Q}}^*$ for the alternative solution $\\hat{\\sigma}_Q$ that forms a valid explanation.\nTo compose this minimal subset, CEDAR sorts the grounded constraints in $F_{\\downarrow_{\\hat{\\sigma}_Q}}$ in decreasing cost order. Then, the minimal subset $F_{\\hat{Q}}^*$ is constructed by incrementally adding the remaining grounded constraint with the highest cost to the set until either (1) $F_{\\mu_{\\hat{\\sigma}_Q}}^* (\\hat{\\sigma}_Q) \\ge F_{\\downarrow_{\\sigma_Q}}(\\sigma_Q)$, in which case a valid explanation is found; or (2) $F_{\\mu_{\\hat{\\sigma}_Q}}^* (\\hat{\\sigma}_Q) < F_{\\downarrow_{\\sigma_Q}}(\\sigma_Q)$ even after adding all the grounded constraints in $F_{\\downarrow_{\\hat{\\sigma}_Q}}$, in which case a valid explanation does not exist."}, {"title": "4.2 Suboptimal Variants", "content": "We now describe two subvariants V1 and V2 for CEDAR that trade off explanation lengths for shorter runtimes.\nSuboptimal Variant V1: The runtime of optimization O2 is dominated by the priority heap operations to identify the grounded constraint with the highest cost. In this variant, we speed up this process at the cost of losing the guarantee that the constraint with the highest cost will be returned. Specifically, agent $a_Q$ merges the sorted lists of each of the other agents into a single pseudo-sorted list; the first set of $|A_Q|$ elements of the pseudo-sorted list are the first elements from each of the $A_Q$ sorted lists of the other agents; the second set of $|A_Q|$ elements are the second elements from each of the sorted lists; and so on. The agent $a_Q$ iteratively adds the grounded constraints from the pseudo-sorted list into a set $F_{\\hat{Q}}^*$. until either (1) $F_{\\mu_{\\hat{\\sigma}_Q}}^* (\\hat{\\sigma}_Q) \\ge F_{\\downarrow_{\\sigma_Q}}(\\sigma_Q)$, in which case a valid explanation is found; or (2) $F_{\\mu_{\\hat{\\sigma}_Q}}^* (\\hat{\\sigma}_Q) < F_{\\downarrow_{\\sigma_Q}}(\\sigma_Q)$ even after adding all grounded constraints from the pseudo-sorted list, in which case a valid explanation does not exist.\nSuboptimal Variant V2: While optimizations O1 and O2 aim to find shortest explanations, they can come at a high runtime and communication cost as all the relevant grounded constraints in $F_{\\downarrow_{\\hat{\\sigma}}}$ for the alternative solution $\\hat{\\sigma}_Q$ need to be considered and communicated to agent $a_Q$. In this variant, we consider an anytime extension whereby agent $a_Q$ sends REQUEST messages to only a subset of agents to request for their relevant grounded constraints with the hope that the grounded constraints received, together with its own relevant grounded constraints, are sufficient to form a valid explanation. If the grounded constraints received are insufficient, additional REQUEST messages are sent to other agents. This process repeats until either a valid explanation can be formed or grounded constraints are received from all agents. Although this approach may return a valid explanation faster and with fewer messages compared CEDAR with Optimizations 1 and 2, note that it is no longer guaranteed to return explanations that are shortest."}, {"title": "4.3 Theoretical Results", "content": "We assume that messages are never lost and are received in the order that they are sent a standard assumption in the DCOP literature (Fioretto et al., 2018).\nTheorem 2. CEDAR and all its optimized versions and suboptimal variants are guaranteed to terminate.\nProof. (Sketch) The main CEDAR framework illustrated in Algorithm 1 is guaranteed to terminate since the agent will receive REPLY messages from all agents that it sent REQUEST messages to, under the assumption that messages are never lost. Optimizations O1 and O2 as well as Variants V1 and V2 do not affect the termination guarantee because, in the worst case, they will add all relevant grounded constraints into either the set $F_{\\hat{Q}}$ for O1 and O2 or the set $F_{\\downarrow_{\\hat{\\sigma}}}$ for V1 and V2, after which they terminate.\nTheorem 3. CEDAR and all its optimized versions and suboptimal variants are guaranteed to return a valid explanation if one exists.\nProof. (Sketch) The main CEDAR framework illustrated in Algorithm 1 computes $F_{\\downarrow_{\\sigma_Q}}, F_{\\hat{\\sigma}}, F_{\\downarrow_{\\sigma_Q}} (\\sigma_Q)$, and $F_{\\downarrow_{\\hat{\\sigma}}} (\\hat{Q})$ exactly as defined in Definition 1. Thus, if a valid explanation exists, CEDAR will return it. Optimizations Ol and O2 as well as Variants V1 and V2 do not affect the correctness because they either return a valid explanation after finding it or, in the worst case, guarantee that no valid explanation exists after adding all relevant grounded constraints."}, {"title": "5. Computational Experimental Evaluation", "content": "We now describe our computational evaluations of CEDAR.\nTo demonstrate the generality of CEDAR, we evaluate it on two DCOP types, commonly used in the literature:\n\u2022 Random Graphs: Unstructured, abstract, constraint graph topologies with density $p_1 = \\{0.2,0.7\\}$. Each variable has 10 values in its domain. The constraint costs were uniformly selected between 1 and 100.\n\u2022 Meeting Scheduling: Using the EAV formulation (Maheswaran et al., 2004b), meetings are represented by variables and time slots form their domain. Each user $u_i$ has a preferred time slot $d^*_i$, chosen randomly from a uniform distribution. The cost for a user attending a meeting in time slot $d_j$ is $2|d_j-d^*_i|$. Binary constraints arise when a user needs to attend two meetings, with equal time slots incurring an infinite cost and non-equal time slots incurring a cost that reflects the preferences of all users of the meetings. Instances were created where each user attends two meetings and the number of users were varied to maintain a graph density $p_1$ of 0.5.\nTo investigate the impact of the properties of DCOP solutions, we evaluated CEDAR with both optimal complete solutions and 1-opt solutions. To construct queries with respect to a solution $\\sigma$, we randomly selected the agent $a_Q$ receiving the query and the variables $var(\\sigma_Q)$ queried. These variables are also constrained with at least another variable in that set. We consider two approaches to select alternative values for the variables queried:\n\u2022 Random Baseline: In experiments involving both optimal and 1-opt solutions, for each variable $x \\in var(\\sigma_Q)$, an alternative value $d \\in D_x \\setminus \\{d^*, d^1\\}$ is randomly selected, where $d^*$ and $d^1$ are the values of $x$ in the optimal and 1-opt solutions, respectively. In experiments involving only optimal solutions, an alternative value $d \\in D_x \\setminus \\{d^*\\}$ is randomly selected.\n\u2022 Best Alternative: Inspired by the fact that queries from human users typically involve alternative values that they think are best, this approach identifies the best alternative value to include in queries. To do this, we solved an alternate DCOP, where the domain of each variable $x' \\notin var(\\sigma_Q)$ is exactly its value in the solution $\\sigma$ and the domain of each variable $x \\in var(\\sigma_Q)$ is its original domain $D_x$ without its value in the solution $\\sigma$.\nFinally, to measure distributed runtimes, we use Non-Concurrent Logic Operations (NCLOs) (Zivan & Meisels, 2006; Netzer, Grubshtein, & Meisels, 2012), which avoid double-counting concurrent computations of agents in a distributed environment."}, {"title": "6. Human User Study", "content": "We conducted a human user study to examine whether and how explanation length affects user comprehension and satisfaction in DCOP-based systems. We used meeting scheduling problems, as it is an application domain that users are likely to be very familiar with. We now briefly describe our study.\nStudy Design: We created a synthetic meeting scheduling scenario involving four participants (the user and three others \u2013 Bob, Charlie, and David) coordinating two meetings (M1 and M2) over four possible time slots: Morning, noon, afternoon, and evening. Bob and the user needed to attend both meetings, while Charlie and David needed to attend only one.\nThe scheduler proposed holding M1 in the afternoon and M2 in the evening. Participants were presented with a contrastive query that compared this schedule with an alternative where M1 is at"}, {"title": "7. Related Work", "content": "The grounded constraints in the explanations of X-DCOP bear some similarity to nogoods in Constraint Satisfaction Problems (CSPs) and Constraint Optimization Problems (COPs) including their distributed versions (DCSPs and DCOPs) (Schiex & Verfaillie, 1994; Yokoo & Hirayama, 2000; Silaghi & Yokoo, 2009). Nogoods are value assignments to subsets of variables that cannot lead to a solution because they violate one or more constraints. However, the role of nogoods differ substantially. Nogoods are used to speed up the search of solutions by pruning portions of the search space that can be ignored. However, grounded constraints are used as solutions themselves.\nThere is also a great deal of literature on Explainable CSP (Liffiton & Sakallah, 2008; Laborie, 2014; Gupta, Genc, & O'Sullivan, 2021). A common theme across many of these methods is their use of hitting sets to find minimal unsatisfiable sets (MUSes) to explain why a CSP is unsatisfiable. An MUS is a set of constraints that is unsatisfiable but any of its subsets are satisfiable. Therefore, similar to our goal of finding shortest explanations, MUSes are subset-minimal justifications for why a CSP is unsatisfiable. Our approach can thus be viewed as a generalization of MUSes to weighted constraints, where constraints are no longer Boolean, but have associated costs instead. The approach to use MUSes as explanations have also been explored in other explainable subareas, such as explainable planning and scheduling (Vasileiou, Previti, & Yeoh, 2021; Vasileiou et al., 2023), where MUSes correspond to sets of logical rules and constraints that must be satisfied by a plan or schedule. Finally, it is important to note that the existing explainable CSP, planning, and scheduling approaches discussed above are generally centralized approaches, while CEDAR is a distributed framework."}, {"title": "8. Conclusions", "content": "In this paper, we introduced the Explainable Distributed Constraint Optimization Problem (X-DCOP) model, an extension of the DCOPs that incorporates contrastive queries and explanations to enhance human interpretability. By addressing a critical gap in the adoption of DCOPs in the real world, X-DCOP facilitates understanding of distributed solutions, particularly in domains where human stakeholders are directly impacted. We defined key properties for valid explanations, presented theoretical guarantees for their existence, and proposed CEDAR, a distributed framework with optimized and suboptimal variants for computing explanations. Our empirical evaluations demonstrated the scalability of the framework and the trade-offs between explanation length and runtime. Furthermore, a user study confirmed the preference for concise explanations, underscoring the practical utility of the model. By bridging the fields of explainable AI and multi-agent systems, X-DCOP paves the way for more transparent, trustworthy, and user-aligned distributed optimization solutions, enabling broader application in real-world scenarios."}]}