{"title": "INFLUENTIAL LANGUAGE DATA SELECTION VIA\nGRADIENT TRAJECTORY PURSUIT", "authors": ["Zhiwei Deng", "Tao Li", "Yang Li"], "abstract": "Curating a desirable dataset for training has been the core of building highly capa-\nble large language models (Touvron et al., 2023; Achiam et al., 2023; Team et al.,\n2024). Gradient influence scores (Pruthi et al., 2020; Xia et al., 2024) are shown to\nbe correlated with model performance and are commonly used as the criterion for data\nselection. However, existing methods are built upon either individual sample rankings\nor inefficient matching process, leading to suboptimal performance or scaling up issues.\nIn this paper, we propose Gradient Trajectory Pursuit (GTP), an algorithm that per-\nforms pursuit of gradient trajectories via jointly selecting data points under an L0-norm\nregularized objective. The proposed algorithm highlights: (1) joint selection instead\nof independent top-k selection, which automatically de-duplicates samples; (2) higher\nefficiency with compressive sampling processes, which can be further sped up using\na distributed framework. In the experiments, we demonstrate the algorithm in both\nin-domain and target-domain selection benchmarks and show that it outperforms top-k\nselection and competitive algorithms consistently, for example, our algorithm chooses\nas low as 0.5% data to achieve full performance on the targeted instruction tuning tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models encompasses an enormous amount of knowledge acquired through pretraining\ncorpus (Brown et al., 2020; Jiang et al., 2023; Touvron et al., 2023). A crucial component that makes\nlanguage models practically useful is the post-training phase, which empower the model with a variety\nof extra abilities and skills, such as aligning language models to follow human instructions (Ouyang et al.,\n2022; Taori et al., 2023; Wang et al., 2023b; Xu et al., 2023), teaching models the tool use (Schick et al.,\n2023) or exploring environments (Carta et al., 2023; Lin et al., 2023).\nServing as the core of model training, choosing and utilizing desirable datasets are critical for both\nquickly adapting models given a fixed budget (Xie et al., 2023) and maximizing the performance for\na targeted task (Wang et al., 2023a; Xia et al., 2024). An automatic data selection algorithm that chooses\na subset based on certain information is gaining its needs given the massive amount of corpora available\nonline. Gradient information, which reflects the optimization process and has direct correlation with\nthe final model performance, is one of the most common criterion for data selection (Mirzasoleiman\net al., 2020; Killamsetty et al., 2021; 2022; Yang et al., 2023; Pan et al., 2024; Xia et al., 2024). However,\nhow to effectively utilize gradient for actual selection process is still a challenging problem. Top-k\nselection is fast and the most straightforward way, where data quality is ranked via the similarity scores\nof individual gradient vectors and the main gradient trajectory (Xia et al., 2024). Nonetheless, due to its\nindependence assumption, its performance is often suboptimal compared to joint selection (Evans et al.,\n2024). Orthogonal Matching Pursuit (Cai & Wang, 2011; Killamsetty et al., 2021) can perform joint\nselection through interatively removing subspace projections, but is highly time-inefficient and requires\nsolving a non-negative least square for as many iterations as the target subset sample size.\nIn this paper, we introduce Gradient Trajectory Pursuit (GTP), an algorithm that selects data samples\nthrough matching the trajectory on a gradient subspace. As discussed in previous works, a model's\noptimization process can be seen as the Langevin dynamic of an energy-based model (Welling & Teh,\n2011), and matching the optimization process can lead to model weights with similar performance (Zhao\net al., 2021; Cazenavette et al., 2022). This sets the base rationale for our algorithm. Further drawing\ninspiration from studies demonstrating that the optimization process happens in a subspace (Gur-Ari et al.,"}, {"title": "2 RELATED WORKS", "content": "Coreset data selection. Building an effective training dataset has evolved to the cornerstone of foundation\nmodel construction. Due to the heavy cost of human annotation and filtering, a flux of works are developed\nto perform automated data selections. Features of data carry information that discriminates samples and\nare often used for selecting representative points (Sener & Savarese, 2017; Kaushal et al., 2019; Xia et al.,\n2020; Wang et al., 2020; Xia et al., 2022; Xie et al., 2023). N-gram features (Xie et al., 2023) are used for\nre-weighting and selecting samples for pre-training, demonstrating strong correlations with target domains.\nDeep learning features (Zhang et al., 2018; Hanawa et al., 2020; Xia et al., 2024) are common choices\nfor effective baselines measuring data similarities. Although frequently adopted, feature-based methods are\nnot guaranteed to have correlation with model optimization. Gradient information is another natural choice\nfor selecting influential data (Yu et al., 2020; Mirzasoleiman et al., 2020; Mindermann et al., 2022; Han\net al., 2023; Xia et al., 2024). Different from features, gradients often reflect more information about the"}, {"title": "3 OUR METHOD", "content": "In this section, we introduce our algorithm Gradient Trajectory Pursuit (GTP), a framework designed\nto select influential language data through joint data selection. We start from introducing the problem\nformulation in section 3.1, discussing the base rationale behind trajectoy matching, then explain the main\nalgorithm and the distribtued variants in section 3.2. Finally, we discuss the complexity and computation\ntime of our algorithm in section 3.3."}, {"title": "3.1 PROBLEM STATEMENT", "content": "Problem definition. Given a large collection of training samples $D_{tr} = \\{(x_i,y_i)\\}_{i=1}^N$, the goal is to select\na small subset $D_s = \\{(x_{s_i},y_{s_i})\\}_{i=1}^M$ that maximizes the generalization performance of models trained on it\nwhen evaluated on a test set $D_{te}$. This test set may either be drawn from the same distribution as the training\ndata or from a different, task-specific distribution, leading to in-domain or targeted-domain data selection.\nMain idea. We approach the subset selection problem from considering model training dynamics. With\na training set, the gradient descent process converges to the posterior distribution $p(\\theta|D) \\propto p(D|\\theta)$ over\nthe model parameter $\\theta$. Assuming the existence, if a selected subset $D_s$ has a gradient vector field w.r.t.\n$p(\\theta|D_s)$ that closely matches the gradient vector field of the full dataset, a model trained using the subset\ncan converge to the same distribution over $\\theta$. Considering the intensive computation cost for computing\nand matching over the whole gradient vector field\u00b9, we only focus on a part of parameter states obtained\nfrom teacher training and perform matching in a subspace. This draws inspiration from findings that the\noptimization process actually happens in a subspace (Gur-Ari et al., 2018; Frankle & Carbin, 2018; Singhal\net al., 2023) where a random guess can already perform decently well. We use parameter states from early\ntrajectories, assuming that the early training provides the most diversity and larger gradient subspace since\noptimization is an information collapse process. To minimize the matching objective, we collectively solve\n$D_s$ samples, and perform joint selection among all data points. This differs from previous works, where\nKillamsetty et al. (2021) focuses on online settings (ours is offline) and uses OMP to select one sample\nper iteration (leading to scaling issue in offline cases), while compared to Xia et al. (2024) which uses"}, {"title": "3.2 GRADIENT TRAJECTORY PURSUIT", "content": "We denote the parameter states on the optimization trajectory as $\\tau = (\\theta^{(0)}, \\theta^{(1)}, ..., \\theta^{(T-1)})$. T can be\nmanually set. To select the index subset S that matches the gradient of target dataset in a subspace, we\ndefine the following objective function:\n$L_t(S)=||U_t^T \\nabla_{\\theta^{(t)}}logp(\\theta^{(t)} \\vert D_{tar}) - U_t^T \\nabla_{\\theta^{(t)}}logp(\\theta^{(t)}\\vert D_{s})||_2$ (1)\nwhere $L_t$ is the per-step matching loss, $U^T_t \\nabla$ denotes the projection of vector on a subspace $U_t$, $D_{tar}$\nis the full dataset for in-domain case and target-domain dataset otherwise. We would like to minimize\nthe summation across steps as the final loss: $L(S) = \\sum_t L_t(S)$. To formulate the selection set S\ninto the objective, we introduce a set of weights $w_i$ for each data point and define the gradient as\n$\\nabla_{\\theta}logp(\\theta\\vert D_s) = \\sum_{i=1}^N w_i \\nabla_{\\theta}logp((x_i, y_i);\\theta)$, $(x_i,y_i) \\sim D_{tr}$, where $w = (w_1,...,w_n)$ is regularized\nthrough L0 norm $||w||_0$. Due to sparsity enforced in L0 norm, the indices of final weights w that are\nnon-negative is the final selected set S. The objective function with index set incorporated is then:\n$L(S) = \\sum_t ||U_t^T \\nabla_{\\theta^{(t)}}logp((\\theta^{(t)}\\vert D_{tar}) -  \\sum_{i=1}^N w_i U_t^T \\nabla_{\\theta^{(t)}}logp((x_i, y_i);\\theta^{(t)}||_2 + ||w||_0$ (2)\nNote that directly optimizing the above objective function requires a proper algorithm to minimize the\nnon-differentiable LO norm in equation 2. We will discuss the core solver variants that jointly minimize\nequation 2 in later this section.\nThe evolving subspace. We choose to project the model gradients onto a subspace before matching, both\nto reduce the storage and computation cost and to rule out potential noise signals (Singhal et al., 2023).\nAs training progresses, parameter state at each step t can have a different meaningful subspace $U_t$. We\nparallelize the subspace computation across and obtain a series of evolving subspace $(U^d, ..., U^{T-1})$.\nTo compute the subspace, we use the full training set $D_{tr}$ for in-domain and use target dataset $D_{target}$\nfor target-domain case. Note that different subspace analysis algorithms are applicable here. We adopts\nstandard PCA for computing the principal components for the subspace."}, {"title": "3.3 COMPUTATION TIME ANALYSIS", "content": "As pointed out in Xia et al. (2024), the computation time cost for all steps should be considered. Our\nalgorithm consists of warmup model training, gradient subtraction and storing, subspace computation,\nand data selection, which follows the design in Xia et al. (2024). The main difference come from our\nsubspace computation and selection algorithm, where we adopt PCA instead of random projection and\nuse GTP instead of top-k for data selection."}, {"title": "4 EXPERIMENTS", "content": "We evaluate our data selection algorithm on standard benchmarks against gradient-based prior arts. In\nsection 4.1, we perform in-domain data selections on large language model agent tasks. In section 4.2,\nwe compare algorithms on targeted instruction tuning, a benchmark focusing on empowering language\nmodel to adeptly follow human instructions."}, {"title": "4.2 TARGETED INSTRUCTION TUNING", "content": "Task setup. We follow the setup in Xia et al. (2024) and evaluate our algorithm on the targeted instruction\ntuning task. Compared to standard instruction tuning where mixed datasets are used to finetune large\nlanguage model to follow human instructions, targeted instruction tuning individually selects data samples\nfor different target benchmarks for finetuning language models. The selection algorithm hence plays an\nimportant role for improving the final trained model performance. For example, multiple works find that\nwhen various data sources are mixed together, it can negatively impact the model performance for different\ntarget tasks Wang et al. (2023a); Xia et al. (2024), while Xia et al. (2024) achieves equivalent or better\nresults with only 5% of the full dataset.\nDatasets and models. We follow the settings from Xia et al. (2024) and use a diverse and mixed collection\nof public instruction tuning datasets as the main training set to select from. The datasets include: Flan\nV2 (Longpre et al., 2023), CoT (Wei et al., 2022), Dolly (Conover et al., 2023), and Open Assistant (K\u00f6pf\net al., 2024). The four datasets are commonly used in various models and benchmarks. For the language\nmodel being tested upon, we use Mistral-7B (Jiang et al., 2023)2.\nImplementation details. To build the initial parameter states for extracting gradients, we warmup train\nMistral-7B (also with LoRA) for 4 epochs using 5% of the full dataset. Data gradient extractions are\nperformed in parallel across machines. We use the selected data from each benchmark as the target dataset\nand compute subspace and target gradient b. These are exactly following the standard setups in Xia et al.\n(2024). The GTP iterations are performed 5 times to select the final subset.\nEvaluation benchmarks and results. There are\nthree targeted benchmarks considered in the task:\nMMLU (Hendrycks et al., 2020), BBH (bench\nauthors, 2023), and TydiQA (Clark et al., 2020).\nMMLU is a de-facto benchmark for evaluating\nlarge language model's capability on 57 subsets\nacross elementary mathematics, humanities, law,\nsocial sciences and more. BBH evaluates the rea-\nsoning capability of large language models. It is\ncurated as a subset of the BIG-Bench benchmark,\nfocusing on 27 tasks where current models struggle\nto match human performance. TyDiQA is a mul-\ntilingual question-answering benchmark covering\n11 diverse languages, and features questions from"}, {"title": "5 CONCLUSIONS", "content": "In this paper, we propose an algorithm (GTP) that can both perform joint data selection and has the\nscalability towards larger target sample sizes. The algorithm automatically de-duplicates samples and\ncan potentially serve as a standard selection technique other than top-k. Both in-domain selection and\ntarget-domain selection are applicable for GTP. For potential limitations, gradient-based methods still rely\non a few steps of model training, which can be more expensive than pure representation-based methods or\ninfluence function methods. It will be worth attempting to connect matching pursuit with representation-\nbased or influence function methods and develop an algorithm that does not require additional model\ntraining for data selection."}]}