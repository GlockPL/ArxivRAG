{"title": "Almost Sure Convergence of Linear Temporal Difference Learning with Arbitrary Features", "authors": ["Jiuqi Wang", "Shangtong Zhang"], "abstract": "Temporal difference (TD) learning with linear function approximation, abbreviated as linear TD, is a classic and powerful prediction algorithm in reinforcement learning. While it is well understood that linear TD converges almost surely to a unique point, this convergence traditionally requires the assumption that the features used by the approximator are linearly independent. However, this linear independence assumption does not hold in many practical scenarios. This work is the first to establish the almost sure convergence of linear TD without requiring linearly independent features. In fact, we do not make any assumptions on the features. We prove that the approximated value function converges to a unique point and the weight iterates converge to a set. We also establish a notion of local stability of the weight iterates. Importantly, we do not need to introduce any other additional assumptions and do not need to make any modification to the linear TD algorithm. Key to our analysis is a novel characterization of bounded invariant sets of the mean ODE of linear TD.", "sections": [{"title": "1 Introduction", "content": "Function approximation is crucial in reinforcement learning (RL) algorithms when the problem involves an intractable discrete or continuous state space (Sutton and Barto, 2018). The idea is to encode the states into finite-dimensional real-valued vectors called features. A parameterized function of the features with some learnable weights is then used to approximate the desired function. For instance, linear function approximation takes the dot product between the feature and the weight to compute the approximated value. Most existing convergence results with linear function approximation assume the features are linearly independent (Tsitsiklis and Roy, 1996; Konda and Tsitsiklis, 1999; Sutton et al., 2008, 2009; Maei, 2011; Yu, 2015; Sutton et al., 2016; Lee and He, 2019; Nachum et al., 2019; Zou et al., 2019; Carvalho et al., 2020; Zhang et al., 2020a,b, 2021a,b; Zhang and Whiteson, 2022; Qian and Zhang, 2023; Zhang et al., 2023). The linear independence assumption is, however, not desired for at least four reasons. First, many well-known empirical successes of RL with linear function approximation (Liang et al., 2016; Azagirre et al., 2024) do not have linearly independent features, thus leaving a gap between theories and practices. Second, in the continual learning setting (Ring, 1994; Khetarpal et al., 2022; Abel et al., 2023), the observations made by an agent are usually served one after another. There is no way to verify whether the features used in the observations are linearly independent or not. Third, the features are sometimes constructed via neural networks (Chung et al., 2019). Usually, it is impossible to guarantee those neural network-based features are linearly independent. Fourth, sometimes the features are gradients of another neural network, e.g., in the compatible feature framework for actor critic algorithms (see Sutton et al. (1999); Konda and Tsitsiklis (1999); Zhang et al. (2020b) for details). One cannot guarantee those features are linearly independent either. Despite the fact that the linear independence assumption greatly simplifies the theoretical analysis, it is unrealistically restrictive.\nThis work contributes towards this challenge using linear temporal difference (TD) learning (Sutton, 1988) as an example, since linear TD is arguably one of the most fundamental RL algorithms. In particular, this work is the first to establish the almost sure convergence of linear TD without requiring linearly independent features. In fact, we do not make any assumptions on the features. We prove that the approximated value function converges to a unique point and the weight iterates converge to a set of TD fixed points. We also establish a notion of local stability of the weight iterates. Importantly, we do not need to introduce any other additional assumptions and do not need to make any modifications to the linear TD algorithm."}, {"title": "2 Background", "content": "Notations. A square complex matrix (not necessarily symmetric) $M \\in \\mathbb{C}^{n \\times n}$ is said to be positive definite if, for any non-zero vector $x \\in \\mathbb{C}^{n}$, it holds that $\\Re(x^H M x) > 0$, where $H$ denotes the conjugate transpose of $x$ and $\\Re(\\cdot)$ denotes the real part. A matrix $M$ is negative definite if $-M$ is positive definite. Likewise, a matrix $M$ is positive semi-definite if $\\Re(x^H M x) \\geq 0$ for any non-zero $x \\in \\mathbb{C}^{n}$. A matrix $M$ is negative semi-definite if $-M$ is positive semi-definite. Given a vector $x \\in \\mathbb{C}^{n}$, we define the $l_2$ norm $||x|| = \\sqrt{x^H x}$. The $l_2$ norm $||\\cdot||$ also induces a matrix norm. Given a matrix $M \\in \\mathbb{C}^{m \\times n}$, the induced matrix norm is defined as\n$||M|| = \\sup_{\\substack{x \\in \\mathbb{C}^n, \\\\ x \\neq 0}} \\frac{||Mx||}{||x||}$\nWe now restrict ourselves to real vectors and matrices. A real symmetric positive definite matrix $D \\in \\mathbb{R}^{n}$ induces a vector norm $||\\cdot||_D$, where $||x||_D = \\sqrt{x^\\top D x}$ for $x \\in \\mathbb{R}^{n}$. We overload $||\\cdot||_D$ to also denote the induced matrix norm.\nWe consider a Markov Decision Process (Bellman, 1957; Puterman, 2014) with a finite state space $\\mathcal{S}$ and a finite action space $\\mathcal{A}$. The agent implements a policy $\\pi : \\mathcal{A} \\times \\mathcal{S} \\to [0,1]$. The dynamics of the environment is characterized by a transition probability function $p: \\mathcal{S} \\times \\mathcal{S} \\times \\mathcal{A} \\to [0,1]$. The environment also adopts a bounded reward function $r : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$. At time step $t$, the agent is at state $S_t$. It interacts with the environment by supplying an action $A_t \\sim \\pi(\\cdot|S_t)$ according to its policy and transitions to the next state $S_{t+1} \\sim p(\\cdot|S_t, A_t)$ following the transition probability function. At the same time, the environment emits a"}, {"title": "3 TD Fixed Points", "content": "With feature linear independence, linear system (4) has a unique solution, referred to as the TD fixed point. Without assuming linear independence, matrix A is merely negative semi-definite, so linear system (4) can potentially adopt infinitely many solutions. In light of this, we refer to all solutions to the linear system as TD fixed points. Namely, we define\n$W_* = \\{ w_* | Aw_* + b = 0 \\}$\nand refer to $W_*$ as the set of TD fixed points. A few questions arise naturally from this definition.\n(Q1) Is $W_*$ always non-empty?\n(Q2) If $W_*$ contains multiple weights, do those weights give the same value estimate?\n(Q3) Do all weights in $W_*$ minimize the MSPBE?\n(Q4) Do the iterates $\\{w_t\\}$ generated by linear TD (1) converge to $W_*$?\nWe shall give affirmative answers to all the questions in the rest of the paper.\nWe start by relating $W_*$ with the MSPBE. We first notice that without making assumptions on features, the canonical projection operator in (2) is ill-defined. In particular, if $X$ does not have full column rank, the $\\arg \\min$ in (2) may return a set of weights instead of a unique one. In light of this, we redefine $\\Pi$ always to select the weight with the smallest norm. Namely, we redefine $\\Pi$ as\n$\\Pi v = X \\arg \\min_{w} \\{ ||w|| | w \\in \\arg \\min_{w} ||Xw - v||_D^2 \\}$\nIn the rest of the paper, we always use this more general definition of $\\Pi$. It turns out that this new definition of $\\Pi$ also enjoys a closed-form expression.\nLemma 1 Let Assumption 2.2 hold. Let $(\\cdot)^\\dagger$ denote the pseudo-inverse. Then,\n$\\Pi = X (D^{1/2} X)^\\dagger D^{1/2}$"}, {"title": "4 ODE Solutions", "content": "We use $w(t; w_0)$ to denote the solution to the ODE (5) with the initial condition $w(0; w_0) = w_0$. With linearly independent features, matrix $A$ is negative definite. It follows from standard dynamical system results (Khalil, 2002) that\n$\\lim_{t \\to \\infty} w(t; w_0) = -A^{-1}b$\nIn other words, regardless of the initial condition $w_0$, a solution always converges to the globally asymptotically stable equilibrium $-A^{-1}b$. Without assuming linear independence, matrix $A$ is merely negative semi-definite. It is then impractical to expect all solutions to converge to the same point. However, can we expect each $w(t; w_0)$ to converge to a $w_0$-dependent limit? The answer is affirmative. To proceed, we first prove that at least the value estimate converges."}, {"title": "4.1 Value Convergence", "content": "Theorem 2 Let Assumption 2.2 hold. Then, for any $w_0 \\in \\mathbb{R}^d$, it holds that\n$\\lim_{t \\to \\infty} Xw(t; w_0) = v_*$\nwhere we recall that $v_*$ is defined in (7).\nProof To simplify notations, we first perform a change of variable. Fix any $w_0 \\in \\mathbb{R}^d$ and any $w_* \\in W_*$. Define $z(t) = w(t; w_0) - w_*$. It can then be easily verified that\n$\\frac{dz(t)}{dt} = \\frac{dw(t; w_0)}{dt} = A w(t; w_0) + b = A z(t)$\nIn other words, $w(t; w_0) - w_*$ is a solution to ODE (9) starting from $w_0 - w_*$. Therefore, we study solutions to ODE (9) in the rest of the proof, . We use $z(t; z_0)$ to denote a solution to (9) with the initial condition $z(0; z_0) = z_0$. When it does not confuse, we write $z(t; z_0)$ as $z(t)$ for simplicity. Define\n$Z_* = \\{ z | Xz = 0 \\}$\nWe recall that $Xz = 0 \\iff Az = 0$. Fix an arbitrary $z_* \\in Z_*$. We define\n$U(z) = ||z - z_*||^2$\nThen, for any $z_0 \\in \\mathbb{R}^d$, we have\n$\\frac{dU(z(t))}{dt} = (z(t) - z_*)^\top A z(t) = (z(t) - z_*)^\top A (z(t) - z_*) = (Xz(t) - Xz_*)^\top D (\\gamma P_\\pi - I) (Xz(t) - Xz_*) \\leq 0$\nwhere the inequality holds because $D(\\gamma P_\\pi - I)$ is negative definite. Furthermore, it holds that\n$\\frac{dU(z(t))}{dt} = 0 \\iff z(t) \\in Z_*$\nClearly, $U$ is not a Lyapunov function without making additional assumptions about $X$. This $U$ is, however, sufficient for LaSalle's invariance principle.\nLemma 4 (LaSalle's theorem, Theorem 4.4 of Khalil (2002)) Let $\\Omega \\subset \\mathbb{R}^d$ be a compact set that is positively invariant with respect to (9). Let $U : \\mathbb{R}^d \\to \\mathbb{R}$ be a continuously differentiable function such that $\\frac{dU(z(t))}{dt} \\leq 0$ whenever $z(t) \\in \\Omega$. Let $E$ be the set of all"}, {"title": "4.2 Weight Convergence", "content": "The value convergence in Theorem 2 immediately implies that any solution $w(t; w_0)$ would eventually converge to the set $W_*$ as time progresses. But is it possible that $w(t; w_0)$ keeps oscillating within $W_*$ or in neighbors of $W_*$ without converging to any single point? In this section, we rule out this possibility and prove that any solution will always converge to some fixed point.\nTheorem 3 For any $w_0 \\in \\mathbb{R}^d$, there exists a constant $w_\\infty(w_0) \\in W_*$ such that\n$\\lim_{t \\to \\infty} w(t; w_0) = w_\\infty(w_0)$\nProof It is well-known that $z(t; z_0)$ has a closed-form solution (Khalil, 2002) as\n$z(t; z_0) = \\exp(At) z_0$\nThe standard approach to work with matrix exponential is to consider the Jordan normal form (Horn and Johnson, 2012). Namely, we let\n$A = P J P^{-1}$"}, {"title": "4.3 Bounded Invariant Sets", "content": "In the ODE methods for stochastic approximation, if the mean ODE of the stochastic approximation algorithm (cf. ODE (5) for linear TD (1)) is not globally asymptotically stable, usually one can only expect that the iterates of the stochastic approximation converge to a bounded invariant set of the ODE. In light of this, we now study the bounded invariant sets of ODE (5). We first study the bounded solutions to the ODE on $(-\\infty, +\\infty)$.\nTheorem 4 Let $w(t)$ be a bounded solution to ODE (5) on $(-\\infty, +\\infty)$, i.e.,\n$\\sup_{t \\in (-\\infty, +\\infty)} ||w(t)|| < \\infty$\nIt then holds that $w(t)$ is constant and is in $W_*$, i.e., there exists some $w_* \\in W_*$ such that $w(t) = w_*$ holds for any $t \\in (-\\infty, +\\infty)$.\nThe proof is in Appendix C.4. This theorem leads to the following characterization of a bounded invariant set.\nCorollary 1 If $W$ is a bounded invariant set of ODE (5), then $W \\subset W_*$"}, {"title": "5 Convergence of Linear TD", "content": "Having fully characterized the mean ODE (5), we are now ready to connect the linear TD update (4) with the mean ODE. We start with stability.\nTheorem 5 Let Assumptions 2.1 and 2.2 hold. Then, the iterates $\\{w_t\\}$ generated by linear TD in (1) is stable, i.e.,\n$\\sup_t ||w_t|| < \\infty \\text{ a.s.}$\nThe proof is in Appendix D.1. The proof is based on Theorem 17(a) of Benveniste et al. (1990), where we will use\n$U(w) = ||w - w_*||^2 + ||w_*||^2$\nas an energy function. Here, $w_*$ is any fixed point in $W_*$. In the canonical analysis with linearly independent features, $||w - w_*||^2$ is commonly used as the Lyapunov function. Without"}, {"title": "6 Related Work", "content": "The seminal work of Sutton (1988) formalizes the idea of temporal difference learning. The linear TD update (1) in this paper is referred to as TD(0) in Sutton (1988), which is a special case of the more general TD algorithm with eligibility trace, referred to as TD(\u03bb) in Sutton"}, {"title": "7 Conclusion", "content": "This work contributes towards RL with arbitrary features using linear TD as an example, where the commonly used linear independence assumption on features is lifted. The insight and techniques in this work can be easily used to analyze other RL algorithms, e.g., SARSA (Rummery and Niranjan, 1994; Zou et al., 2019; Zhang et al., 2023), gradient TD methods (Sutton et al., 2008, 2009; Maei, 2011; Zhang et al., 2021a; Qian and Zhang, 2023), emphatic TD methods (Yu, 2015; Sutton et al., 2016; Zhang and Whiteson, 2022), density ratio learning methods (Nachum et al., 2019; Zhang et al., 2020a), TD with target networks (Lee and He, 2019; Carvalho et al., 2020; Zhang et al., 2021b), and actor-critic methods with compatible features (Sutton et al., 1999; Konda and Tsitsiklis, 1999; Zhang et al., 2020b), as well as their in-context learning version (Wang et al., 2024). This work is also closely related to overparameterized neural networks, where the linearization of the neural network at the initial weights naturally results in features that are not necessarily linearly independent (Cai et al., 2019)."}, {"title": "Appendix A. Mathematical Background", "content": "We first provide the definition of the Moore-Penrose pseudo-inverse for completeness.\nDefinition A.1 (Definition 23.1 of Gallier and Quaintance (2019)) Given any nonzero $m \\times n$ matrix A of rank r, if $A = V \\Sigma U^\\top$ is a singular value decomposition of A such that\n$\\Sigma = \\begin{bmatrix}\n    \\Lambda & 0_{r,n-r} \\\\\n    0_{m-r,r} & 0_{m-r,n-r}\n\\end{bmatrix}$\nwhere\n$\\Lambda = \\begin{bmatrix}\n    \\lambda_1 & & \\\\\n    & \\ddots & \\\\\n    & & \\lambda_r\n\\end{bmatrix}$\nis an $r \\times r$ diagonal matrix consisting of the nonzero singular values of A, then if we let $\\Sigma^\\dagger$ be the $n \\times m$ matrix\n$\\Sigma^\\dagger = \\begin{bmatrix}\n    \\Lambda^{-1} & 0_{r,m-r} \\\\\n    0_{n-r,r} & 0_{n-r,m-r}\n\\end{bmatrix}$"}, {"title": "Appendix B. Proofs in Section 3", "content": "B.1 Proof of Lemma 1\nProof We first note that\n$||Xw - v||_D^2 = ||D^{1/2} (Xw - v)||^2$\nThen, by Theorem A.1, it holds that\n$X \\arg \\min_{w} \\{ ||w|| | w \\in \\arg \\min_{w} ||Xw - v||_D^2 \\} = X \\arg \\min_{w} \\{ ||w|| | w \\in \\arg \\min_{w} ||D^{1/2} (Xw - v)||^2 \\} = X (D^{1/2} X)^\\dagger D^{1/2} v$\nHence, we have $\\Pi = X (D^{1/2} X)^\\dagger D^{1/2}$ by (6).\nB.2 Proof of Lemma 2\nProof Lemma 4 of Tsitsiklis and Roy (1996) proves that T is a contraction mapping w.r.t. $|| \\cdot ||_D$ under Assumption 2.2. Next, we show that $\\Pi$ is nonexpansive w.r.t. $|| \\cdot ||_D$. Define $Z = D^{1/2} X$, then we have\n$||\\Pi v||_D = ||X (D^{1/2} X)^\\dagger D^{1/2} v||_D = ||Z Z^\\dagger D^{1/2} v|| \\leq ||Z Z^\\dagger|| ||D^{1/2} v|| \\leq ||D^{1/2} v|| = ||v||_D$\nIt then follows immediately that $T \\Pi$ is a contraction w.r.t. $|| \\cdot ||_D$.\nB.3 Proof of Lemma 3\nProof Our proof relies on the fact $D(\\gamma P_\\pi - I)$ is negative definite (Sutton et al., 2016). For $w, w' \\in W_*$, we have\n$Aw + b - (Aw' + b) = 0 = A (w - w') = 0 = X^\\top D (\\gamma P_\\pi - I) X (w - w') = 0 = (w - w')^\\top X^\\top D (\\gamma P_\\pi - I) X (w - w') = 0 \\implies X (w - w') = 0 = Xw = Xw'$"}, {"title": "Appendix C. Proofs in Section 4", "content": "Lemma C.1 For all $w_0 \\in \\mathbb{R}^d$, it holds that $\\sup_{t \\in [0, \\infty)} ||w(t; w_0)|| < \\infty$.\nProof Fix an arbitrary $w_* \\in W_*$. We first show that $\\frac{d ||w(t; w_0) - w_*||^2}{dt} < 0$ for any $w_0 \\in \\mathbb{R}^d$.\n$\\frac{d ||w(t; w_0) - w_*||^2}{dt} = 2 (w(t; w_0) - w_*)^\top (A w(t; w_0) + b) = 2 (w(t; w_0) - w_*)^\top (A w(t; w_0) + b - (A w_* + b)) = 2 (w(t; w_0) - w_*)^\top A (w(t; w_0) - w_*) = 2 (w(t; w_0) - w_*)^\top X^\\top D (\\gamma P_\\pi - I) X (w(t; w_0) - w_*) = 2 (X (w(t; w_0) - w_*))^\top D (\\gamma P_\\pi - I) (X (w(t; w_0) - w_*)) \\leq 0$\nThis implies $||w(t; w_0) - w_*||$ is monotonically decreasing for any $w_0 \\in \\mathbb{R}^d$. Thus, $||w(t; w_0) - w_*|| < ||w_0 - w_*||$ for $t > 0$, and we get\n$\\sup_{t \\in [0, \\infty)} ||w(t; w_0)|| \\leq ||w_*|| + ||w_0 - w_*||$\nwhich completes the proof."}, {"title": "Appendix D. Proofs in Section 5", "content": "We note that linear TD in (1) is a special case of (17) where\n$Y_t = (S_t, A_t, S_{t+1})$\n$H(w, y) = (r(s, a) + \\gamma w^\\top x(s') - w^\\top x(s)) x(s)$"}]}