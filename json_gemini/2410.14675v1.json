{"title": "ENHANCING LARGE LANGUAGE MODELS' SITUATED FAITHFULNESS TO EXTERNAL CONTEXTS", "authors": ["Yukun Huang", "Sanxing Chen", "Hongyi Cai", "Bhuwan Dhingra"], "abstract": "Large Language Models (LLMs) are often augmented with external information as contexts, but this external information can sometimes be inaccurate or even in-tentionally misleading. We argue that robust LLMs should demonstrate situated faithfulness, dynamically calibrating their trust in external information based on their confidence in the internal knowledge and the external context. To benchmark this capability, we evaluate LLMs across several QA datasets, including a newly created dataset called RedditQA featuring in-the-wild incorrect contexts sourced from Reddit posts. We show that when provided with both correct and incorrect contexts, both open-source and proprietary models tend to overly rely on external information, regardless of its factual accuracy. To enhance situated faithfulness, we propose two approaches: Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning (RCR). SCR enables models to self-access the confidence of external information relative to their own internal knowledge to produce the most accurate answer. RCR, in contrast, extracts explicit confidence signals from the LLM and determines the final answer using predefined rules. Our re-sults show that for LLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR outperforms RCR, achieving improvements of up to 24.2% over a direct input augmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR outperforms SCR. Fine-tuning SCR with our proposed Con-fidence Reasoning Direct Preference Optimization (CR-DPO) method improves performance on both seen and unseen datasets, yielding an average improvement of 8.9% on Llama-3-8B. In addition to quantitative results, we offer insights into the relative strengths of SCR and RCR. Our findings highlight promising avenues for improving situated faithfulness in LLMs. The data\\u00b9 and code\\u00b2 are released.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) can be enhanced by incorporating relevant external context (Chen et al., 2017; Lewis et al., 2020) provided by automated retrieval systems, tools, users, or another model (Mialon et al., 2023). However, external information can often be unreliable, containing errors due to online misinformation, intentional disinformation stemming from malice (Zou et al., 2024), noisy human inputs, and outdated data (Kasai et al., 2023). Prior research has demonstrated that LLMs can be too faithful to the external context, making them susceptible to being misled by unreliable contexts (Petroni et al., 2020; Xie et al., 2023; Shi et al., 2023; Shaier et al., 2024).\nInstead of blindly following external information, a robust LLM should exhibit situated faithful-ness, dynamically balancing its reliance on internal knowledge and external context. This means the model should trust its internal knowledge when it is correct or when external information is un-reliable, while conversely leaning on external data when uncertain or when the external evidence is convincing. The key objective is to maximize the accuracy of the model's responses, irrespective of whether the external context is correct or incorrect. To achieve this, it requires strong reading com-prehension skills to interpret and extract accurate information as well as confidence reasoning-the"}, {"title": "2 RELATED WORK", "content": "Language models with input augmentation (Lewis et al., 2020; Asai et al., 2024) are valuable for many application scenarios but are susceptible to noisy contexts (Petroni et al., 2020; Shi et al., 2023). Past research have explored tailoring LMs to utilize only relevant contexts (Li et al., 2023a; Weston & Sukhbaatar, 2023; Yoran et al., 2024). These approaches however do not account for contexts that are relevant but contradict LLMs' parametric knowledge. While Pan et al. (2023) touch on the issue of directly incorrect contexts, their focus remains on the RAG setup and lacks a dedicated exploration of the LLM's internal behavior.\nFacing such knowledge conflicts(Xu et al., 2024), LLMs exhibit enigmatic behaviors. They can be too receptive to external evidence (Xie et al., 2024; Tan et al., 2024) and over-reliant on parametric knowledge (Longpre et al., 2021; Jin et al., 2024) at the same time. Shi et al. (2024) propose a fusion-in-decoder approach to trust context when sources are known to be reliable. Yu et al. (2024) resolve knowledge conflict by training a separate classifier with in-distribution data to remove untruthful information from context as a pre-processing step. However, it relies on in-distribution data and the model's hidden states, limiting its practical use. Moreover, Doing so at the level of surface forms fails to consider various semantic granularities. Concurrent to our work, ClashEval (Wu et al., 2024), FaithEval (Ming et al., 2024) and DynamicQA (Marjanovi'c et al., 2024) also benchmark this problem but don't focus on providing an effective solution.\nIt has been shown that LLMs are capable of estimating the confidence in the correctness of their own response (Kadavath et al., 2022; Xiong et al., 2023). In-context knowledge conflicts additionally re-quire the model to corroborate the external with the internal to make reasoned comparisons to the final conclusion. Sharing perhaps the most similar idea to ours, concurrent work AstuteRAG (Wang et al., 2024) proposes to iteratively consolidate internal and external knowledge while considering source reliability. Their major setup leverages actual results from a commercial search engine, which presents an arbitrary mixture of correct, incorrect, and irrelevant evidence in a single context. Fo-cusing on a clean evaluation of LLMs' ability to handle a specific context, our setup controls the type of evidence the model sees at an instance level. We further explore various methods, includ-ing confidence-based, self-evaluation-based, and DPO methods, to elicit models' ability to resolve knowledge conflicts-all of which perform effectively. Another concurrent work, KnowPO (Zhang et al., 2024), also employs DPO to teach models how to resolve knowledge conflicts. However, their approach uses preference pairs based solely on final answers, while our CR-DPO method extends this by incorporating COT reasoning before the final answer. This not only enhances the model's reasoning ability but also provides explainability regarding its final decision."}, {"title": "3 PROBLEM SET-UP", "content": ""}, {"title": "3.1 FORMULATION", "content": "Let M represent a QA model built on top of an LLM G (i.e. M can be either a standalone LLM or an LLM-based pipeline). Given a QA instance x = (q, c), where q denotes a question and c denotes a context that contains an answer to q,\\u00b3 the model M generates an answer a = M(q, c).\nWe define the correctness of the model's answer a with the function y(a) \\u2208 {0,1}, where y(a) = 1 indicates that the answer a is correct (e.g., an exact match with the ground truth) and y(a) = 0 indicates that the answer is incorrect. Let ac be the answer entailed by the context c, we define the correctness of the context c with respect to the question q using y(c, q) \\u2208 {0,1}, where y(c, q) = 1 when y(ac) = 1, y(c, q) = 0 when y(ac) = 0. Similarly, let ain be the internal answer from the base LLM G without any context, where ain = G(q).\nSituated Faithfulness refers to the ability of the model M to provide a correct answer regardless of whether the external context c is correct or incorrect. Therefore, improving situated faithfulness means maximizing the accuracy of the model's answer, regardless of whether the context is correct or not. Specifically, this involves maximizing the following probabilities:\n\\u2022 Pr(y(a) = 1 | y(c,q) = 1,y(ain) = 1): The probability of providing a correct answer when both the context and the model's internal answer are correct."}, {"title": "3.2 DATASETS", "content": "To comprehensively benchmark the situated faithfulness problem, we select a diverse set of question-answering datasets across various domains. Each question is tested twice, paired with both a correct and an incorrect context, incorporating contexts generated by both language models and humans."}, {"title": "3.2.1 REDDITQA", "content": "While many QA datasets are accompanied with correct human-generated documents containing valuable supporting evidence, no existing dataset focuses on human-written documents with inac-curacies, raising questions on how models react to natural false contexts. To fill in this gap, we contribute a new dataset sourced from existing Reddit posts, which often contain unverified, inac-curate claims. Concretely, we first apply a claim detector to Reddit post summaries ((V\u00f6lske et al., 2017)), filtering out those without significant factual claims. Then, GPT-4o evaluates whether the claims may contain inaccuracies. For claims flagged as incorrect or uncertain, GPT-4o generates a self-contained, multiple-choice world-knowledge question, providing the correct answer, a wrong answer, and two plausible alternatives. Next, we use a natural language inference model to check if the incorrect post indeed provides the wrong answer to the question.\nFollowing the automated process, we conduct a human evaluation to ensure the validity of each question by verifying the following criteria: 1) Is the question concise, self-contained, and asking for verifiable factual information based on world knowledge? 2) Is the answer confirmed by rep-utable sources, such as authoritative websites (e.g., government websites, Wikipedia) or textbooks? 3) Does the Reddit post contain inaccuracies that lead to an incorrect answer? 4) Are the other plau-sible answer choices clearly wrong and potentially misleading? 5) Is verified evidence from online sources correctly paired with the context for the QA instance? If any of these criteria are not met, annotators attempt to correct the errors. For non-correctable data points, annotators will discard them. Step by Step guidance and other annotation details can be found in \\u00a7G."}, {"title": "3.2.2 OTHER QAS", "content": "We benchmark the ability of current LLMs to perform situated faithful reasoning across several question-answering tasks, covering diverse domains and varying difficulty levels.\n\\u2022 NaturalQA (Kwiatkowski et al., 2019): An open-domain dataset designed to simulate real-world search processes by focusing on naturally occurring questions.\n\\u2022 TriviaQA (Joshi et al., 2017): Another open-domain dataset that consists of relatively easy questions, as the required knowledge is already memorized by most LLMs."}, {"title": "4 METHODS", "content": "We examine two sets of approaches to enhance LLMs' situated faithfulness. The first is self-guided confidence reasoning (SCR), where the LLM is aware that the external context may be incorrect and estimates the confidence of both its internal knowledge and the provided context, reasoning through to the final answer. In contrast, the second approach, rule-based confidence reasoning (RCR), also involves estimating confidence for internal and external knowledge, but the final decision is made according to predefined external rules rather than the model's own reasoning."}, {"title": "4.1 SELF-GUIDED CONFIDENCE REASONING", "content": "Implicit Self-Guided Confidence Reasoning (ImplicitSCR) The model is prompted that the pro-vided context may be incorrect and must rely on its own judgment to assess the reliability of the context before providing a final answer. To encourage the use of internal knowledge, the context is presented after the question. Our pilot experiments (\\u00a7E) indicate that this ordering structure biases the model to rely more on prior knowledge, reducing susceptibility to misleading context. After that, the model implicitly reasons about confidence during the decision-making process and outputs only the final answer. (See \\u00a7F.4 for the actual prompts used).\nExplicit Self-Guided Confidence Reasoning-Explicit (ExplicitSCR) the model first generates separate answers based on its internal knowledge (\\u00a7F.1) and completely faithful to the provided context. (\\u00a7F.3). The model then performs verbalized confidence reasoning using a chain-of-thought process. It begins by evaluating the confidence in its internal answer, reflecting on how it derived this answer based on known facts. Next, it assesses the reliability of the external context by cross-referencing it with the facts supporting its internal answer. The final answer is chosen by reasoning through both perspectives, balancing internal and contextual confidence. (See \\u00a7F.5 for the details of the prompt.)\nConfidence Reasoning Preference Optimization (CR-DPO) We train the model to learn verbal-ized confidence reasoning by optimizing its preferences between a pair of self-sampled reasoning paths. First, when the model's internal answer is incorrect, we provide it with a correct external context and ask it to reason why the context is correct and its internal answer is wrong, resulting in a chosen reasoning path. Conversely, we lie to the model by telling it the context is incorrect and its internal answer is right, asking it to reason accordingly, generating a rejected reasoning path. When the model's internal answer is correct, we repeat the same process, comparing correct in-ternal reasoning with external context. (See \\u00a7F.9 Prompts for these two processes)This approach allows the model to learn preferences between reasoning paths through direct preference optimiza-tion (Rafailov et al., 2023). Additionally, to encourage diversity in reasoning, we use dual sampling: for each instance, we sample two pairs of reasoning paths using different prompts and in-context examples. Following Pang et al. (2024), we apply negative log-likelihood to DPO loss to improve the optimization of the reasoning process. (See \\u00a7B for implementation details)"}, {"title": "4.2 RULE-BASED CONFIDENCE REASONING", "content": "In this set of approaches, we define different types of confidence estimations for both LLMs' internal and context-based answers. The confidence estimation can be represented by a numerical score or a self-evaluation result. The reasoning process is then carried out by predefined heuristics, which determine the final answer by comparing these confidence estimates. All methods here follow the same procedure of 1) generate an internal answer and a context answer (completely faithful to the contexts even if it's wrong) separately; 2) they estimate confidence for each answer; 3) and utilize rules to select.\nInternal Evaluation (InternalEval) The LLM self-evaluates the correctness of its internal answer with the prompt in \\u00a7F.6. If the self-evaluation suggests the answer is correct, the internal answer is selected; otherwise, the context-based answer is used.\nContext Evaluation (ContextEval) The model assesses the correctness of the context relative to the question (See \\u00a7F.7 for the prompt). If the context is deemed correct, the context-based answer is used; otherwise, the internal answer is selected.\nInternal Confidence Thresholding (InternalConf) The LLM evaluates its internal confidence in the internal answer. The reasoning rule here is that: if this confidence exceeds a predefined threshold, the internal answer is selected; otherwise, the model defaults to the context-based answer. The threshold could be prefined as 0.5, or it could be further tuned on a calibration set from the dev split of the same dataset if available. The confidence score could be sequence probability or other confidence elicitation methods like self-consistency.\nContext Confidence Thresholding (ContextConf) The LLM assesses the reliability of the external context. If the confidence in the context surpasses a specified threshold, the context-based answer is chosen; otherwise, the internal answer is used. The threshold could be determined in the same way as Internal Conf. The confidence score could be estimated by the sequence probability of the answer when given the context or self-consistency as well.\n(Calibrated) Token Probability Correction (TPC) (Wu et al., 2024) compares the confidence scores (mean token probabilities) between the model's internal answer and the context answer, se-lecting the one with the higher score as the final answer-referred to as token probability correction. Since internal answer probabilities are generally more uniform, while context-based probabilities tend to be right-skewed, this method can be further improved by comparing percentiles of the con-fidence scores, rather than the raw values, resulting in calibrated token probability correction."}, {"title": "4.3 OTHER BASELINES", "content": "Direct Input Augmentation The simplest and yet common implementation of RAG systems is directly concatenating a question with the corresponding contexts. We follow the same structure of the generative prompt described in Yu et al. (2024) where a simple instruction is inputted first, following the external context concatenated with the question, forming a prompt that is then provided to the model. The model is instructed to leverage the external context in generating the answer. We utilize 3-shot to make sure the answer is in the right format. (See \\u00a7F.2 for the prompt)\nTruth-aware Context Selection Similar to context evaluation, Truth-aware context selection (Yu et al., 2024) filters out incorrect parts of the context at a granular sentence or token level using a classifier. Unlike rule-based methods that choose between internal and context-based answers, this approach feeds the filtered context back into the LLM to generate the final answer. However, it relies on hidden states, which are not accessible in proprietary models like GPT-4, and requires in-distribution training data, making it less comparable to methods that do not. Therefore, we adopt an alternative: LMs remove the untruthful sentences (TACS-LR) (See \\u00a7F.8 for the prompt), and the refined context is used to produce the final answer."}, {"title": "5 RESULTS", "content": ""}, {"title": "5.1 EXPERIMENTAL SET-UP", "content": "We conduct experiments using three models: GPT-4o mini, GPT-4o, and Llama-3 8B. Our base-lines include Direct input augmentation (DIA), Truth-Aware Context Selection (TACS). We com-pare these baselines with both implicit and explicit self-guided confidence reasoning (ImplicitSCR and ExplicitSCR), and a suite of rule-based methods including Token Probability Correction (TPC), Internal Confidence Thresholding (InternalCnf), Context Confidence Thresholding (ContextConf), Internal Evaluation (InternalEval), and Context Evaluation (ContextEval).\nWe also report the results of a closed-book method (i.e., internal answer accuracy) to provide insight into the model's inherent performance on different datasets. We note that this evaluation does not assess the model's situated faithfulness."}, {"title": "5.2 LLMS CAN DO BOTH SELF-GUIDED AND RULE-BASED CONFIDENCE REASONING", "content": "Self-Guided Confidence Reasoning (SCR) Excels on GPT-4o and GPT-4o mini: As shown in Table 1, SCR methods consistently achieve the top-2 accuracies across most datasets for both GPT-4o mini and GPT-4o, outperforming direct input augmentation by significant margins (+17.8 for GPT-40 mini, +24.2 for GPT-4o), as well as TACS and other rule-based methods. This highlights LLMs' ability to evaluate and compare internal knowledge with external context for accurate an-swers. TACS performs poorly because LLMs struggle to filter context effectively, requiring in-distribution training data and limiting its use as an out-of-the-box solution.\nLLMs Perform Well with Rule-Based Confidence Reasoning Rule-based methods also show sub-stantial improvements over direct input augmentation. Among these, \u201cInternalConf\u201d performs the"}, {"title": "5.3 WHAT LIMITS RULE-BASED CONFIDENCE REASONING?", "content": "While RCR methods seem intuitive, their performance is often constrained by errors and biases in rule design and the extraction of confidence signals. We demonstrate these limitations through experiments on the Llama-3-8B, with similar findings observed in the GPT-4o series (\\u00a7D).\nRules can be biased or flawed: For ContextConf and InternalConf, the decision rule-selecting an answer based on whether the confidence score exceeds a fixed threshold can be optimized through threshold tuning on a dedicated calibration set. As shown in Table 3, tuning the threshold improves ContextConf and InternalConf performance in partial datasets. Similarly, Token Probability Correc-tion (TPC) rule can be refined by comparing the percentile of the confidence scores rather than the raw values, resulting in a calibrated token probability correction (CTPC) approach. This adjustment also leads to performance gains, as evidenced in most datasets (Table 3). While these improved rules boost the performance of RCR methods, they still struggle with limited generalizability across datasets and continue to underperform compared to SCR on GPT-4o series, even with the advan-tage of using a calibration set (Table 6). This is partly due to the new biases introduced by the updated rules. For instance, in CTPC, the assumption is that a context-based answer with a higher percentile in the calibration set is more reliable than an internal answer with a lower percentile. This assumption often breaks down in certain scenarios, such as when the calibration set contains an equal mix of correct and incorrect contexts, and the model's internal knowledge is highly accurate. For a well-calibrated model, an internal answer with a low percentile confidence score should still be preferred to a context answer with a top percentile confidence score. In these cases, relying on percentile-based comparisons can undermine the method's generalizability.\nConfidence signal can misalign with the rule Improving the confidence signal itself, such as by calibrating confidence scores InternalConf using isotonic regression, reduces expected calibration error. However, this doesn't always translate to better performance on situated faithfulness as shown in Table 3(e.g. TriviaQA, RedditQA). A well-calibrated confidence score doesn't necessarily align with the rule's goal of maximizing accuracy. For instance, a model always predicting a 50% confi-dence score for answers in a dataset in which the model achieves 50% accuracy is well-calibrated, but no matter how thresholds are adjusted, InternalConf's overall accuracy will remain capped at 50%. Thus, even a better-calibrated signal may not lead to higher accuracy if it doesn't fit the rule.\nConfidence signal can be noisy and biased For InternalEval and ContextEval, the rules are directly aligned with the confidence signal the correctness of the internal or context-based answer. In theory, take InternalEval as an example, a perfect self-evaluator that correctly assesses the internal answer's accuracy could optimize performance. The model would use the context when its internal knowledge is incorrect and rely on its own knowledge when correct, leading to optimal results. However, in practice, self-evaluation is noisy and biased. Our experiments about InternalEval fine-tuning (\\u00a7C) shows that InternalEval is biased and therefore generalize badly even with fine-tuning, highlighting the challenge of extracting reliable confidence signals.\nIn summary, RCR methods are hindered by errors and biases in rule design, confidence extraction, and by misalignment between rules and confidence signals. In contrast, SCR avoids these pitfalls by inherently performing confidence estimation and reasoning directly in the text space, where the model operates more effectively."}, {"title": "5.4 CONFIDENCE REASONING PREFERENCE OPTIMIZATION ENHANCES SCR", "content": "SCR can be further improved with CR-DPO. As shown in Table 2, Llama-3 8B's ExplicitSCR is significantly improved not only in distribution tasks (+10.4% on average) but also generalized to out-of-distribution tasks (+5.9%) including FreshQA and ClashEval.\nCR-DPO allows the model to do better SCR for the following reason:\nCoT enables better confidence reasoning learning CR-DPO allows the model to explicitly learn how to reason through confidence assessments by leveraging CoT. Since the model verbalizes its reasoning process, it learns to identify what constitutes effective confidence reasoning. In contrast, when we remove the explicit COT confidence reasoning (\u201c-COT\u201d in Table 4), which is equivalent to applying DPO to Implicit SCR (DPO-ISCR), where the model directly learns to predict the final answer without an explicit reasoning chain, the results were suboptimal (-7.5% in average). The absence of a clearly articulated reasoning process forces the model to infer the reasoning principles solely from outcomes, making learning less effective and prone to error. This leads to a more challenging and inefficient learning process-similar to relying on inductive guesswork without being able to observe the reasoning steps.\nCR-DPO encourages LLM to explore various reasoning paths CR-DPO allows the model to learn from the preference between different paths through reinforcement learning instead of over-fitting to one single reasoning path. In contrast, supervised fine-tuning on Explicit SCR (\u201c-DPO\u201d in Table 4) leads to a large performance drop (-5.5% in average) because it overfits the model to one correct reasoning path. The CR-DPO can be enhanced by sampling more pairs of chosen and rejected reasoning paths for each question-context pair. As shown in the table (\u201cCR-DPO\u201d versus \u201cCR-DPO-ST\u201d), where each instance have two chosen-rejected reasoning pairs sampled with dif-ferent in-context examples. The performance on the in-distribution datasets are improved (+2.2%) with a slight sacrifice on the out-of-distribution performance (-0.5%).\nMultitask training helps OOD generalization Shown in Table 4, if we remove the RedditQA dataset from training, the performance drops on two unseen datasets (i.e., FreshQA, ClashEval), despite an increase on the in-domain tasks. If we further reduce the number of training tasks by only keeping one training dataset TriviaQA, the performances across all tasks decrease. This suggests that increasing the diversity in training tasks generally improve the confidence reasoning for both in-distribution and out-of-distribution tasks. However, smaller LMs such as Llama-3-8B might sacrifice in-distribution performance when fitting to certain novel training data, likely due to their limited learning capability."}, {"title": "6 CONCLUSION", "content": "In this work, we address the challenge of making LLMs situated faithful to external contexts. We benchmark their performance across various QA datasets, pairing with both correct and incorrect contexts. Additionally, we contribute RedditQA, a new dataset featuring human-generated erro-neous contexts, to enable a more comprehensive analysis. We propose two classes of approaches, Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning (RCR), to help LLMs reconcile external knowledge with internal knowledge for more accurate answers. Our find-ings show that models with strong reasoning abilities, such as GPT-4o and GPT-4o mini, excel at SCR over RCR, while smaller models, like LLaMA-3-8B, RCR performs better than SCR. We observe that RCR is hindered by noise and biases in confidence estimation and rule design. Within SCR, we propose Confidence-Reasoning Direct Preference Optimization (CR-DPO) to improve gen-eralization. Our work provides valuable benchmarks, robust experiments, and insights for advancing LLMs' situated faithfulness for future research."}, {"title": "A DATASETS", "content": ""}, {"title": "A.1 EVALUATION DATASET", "content": "We benchmark the ability of current LLMs to perform situated faithful reasoning across several question-answering tasks, covering diverse domains and varying difficulty levels.\n\\u2022 RedditQA World knowledge multiple-choices QA datasets where in-corrected contexts are source from RedditPost\n\\u2022 NaturalQA (Kwiatkowski et al., 2019): An open-domain dataset designed to simulate real-world search processes by focusing on naturally occurring questions.\n\\u2022 TriviaQA (Joshi et al., 2017): Another open-domain dataset that consists of relatively easy questions, as the required knowledge is already memorized by most LLMs.\n\\u2022 PopQA (Mallen et al., 2023): Features open-domain questions of varying popularity, in-cluding low-popularity questions that models might not have memorized.\n\\u2022 FreshQA (Vu et al., 2023): Contains questions with varying levels of time sensitivity (e.g., fast-changing or static facts) and different complexity (e.g., single-hop or multi-hop), as-sessing both factuality and reasoning. To align with other QA datasets, we only keep data without a false premise.\n\\u2022 ClashEval (Wu et al., 2024): a contemporary dataset with world knowledge questions from multiple domains, each paired with both correct and perturbed incorrect contexts.\nFor TriviaQA, NaturalQA, and FreshQA, we retrieve correct contexts from relevant websites and use a natural language inference model to verify that the context supports the correct answer. If not, GPT-40 generates a supporting context. Wrong contexts are created by asking GPT-40 to modify the correct context to lead to an incorrect answer. We then apply post-processing to filter out artifacts, such as keywords like 'fake' or 'imaginary,' in the wrong contexts.\nFor PopQA, we use contexts from ConflictQA. However, we found many questions to be ambiguous. For example, the question 'Who directs Amy?' is unclear, as multiple versions of the movie exist, but the ground-truth answer only reflects one. To address this, we disambiguate such questions by adding more information using an LLM and augment the ground-truth answers to capture synonyms.\nFor evaluation, we use exact match relaxation for TriviaQA, PopQA, and ClashEval, where the answer is considered correct if the ground-truth appears anywhere in the response. For FreshQA, we apply the LLM-based metric from (Vu et al., 2023). In NaturalQA, since the ground-truth answers are not comprehensive, we combine exact match relaxation with the LLM-based metric. When exact match fails, the LLM metric is used as a fallback. For all datasets except RedditQA, we manually sample 300 examples for testing and 100 for development. Since RedditQA has only 226 samples in total, we use 176 for testing and 50 for development. FreshQA also has only 75 validation samples after filtering the question with false premise."}, {"title": "A.2 TRAINING DATA", "content": "We utilize the similar process to create training data for TriviaQA, PopQA, NaturalQA and Red-ditQA. For RedditQA, we don't have huamn verification so the data would be noisy. We create 5000 datapoints for TriviaQA, RedditQA and NaturaQA. For PopQA, we follow the split 1 from Yu et al. (2024) and keep 2895 training datapoints."}, {"title": "B TRAINING DETAILS", "content": "For the best trained Confidence Reasoning Direct Optimization Models, we utilize Lora training on Llama-3-8B. The training data include TriviaQA, ConflictQA, NaturalQA, RedditQA. The training process was configured with a learning rate of 5e-6 and a maximum gradient norm of 0.3. The batch size per device was set to 1, with gradient accumulation over 4 steps, with distibuted on 4 A6000 GPUs. And the model was trained for 5 epochs. Additionally, 100 warmup steps were used, and the sequence length was constrained to a maximum of 900 tokens, with a prompt length limit of 600"}, {"title": "C INTERNAL EVALUATION FINE-TUNING", "content": "We fine-tune the LLama-3-8B to do Internal Evaluation. We fine-tune it on three tasks including TriviaQA, RedditQA and NaturalQA. Where llama-3-8B first answer the training data and compare with the ground-truth to get the evaluation label. Then we fine-tune the model to do the evaluation. Evaluation Results are reported below\nWe observe that during InternalEval, the model's self-evaluation improves only on in-distribution tasks such as TriviaQA, ConflictQA, and NaturalQA, but performs poorly on OOD datasets like RedditQA, and ClashEval. On these OOD datasets, the evaluation results shift from a bias toward evaluating answers as correct to incorrectly classifying them as wrong. This indicates that the Inter-nalEval signal is noisy and lacks generalization. While the evaluation accuracy is low on datasets like ClashEval, the model's tendency to classify its own answers as incorrect results in an unexpected improvement in accuracy when given the correct document. This bias leads to an overall increase in Acc and the total score, highlighting a discrepancy between the model's internal evaluation ability and its situated faithfulness. This also underscores the limitations of rule-based confidence reason-ing."}, {"title": "D FULL RESULTS", "content": "Full results of GPT4-o are shown in Table.6."}, {"title": "E CONTEXT AND QUESTION ORDER EXPERIMENT", "content": "As shown in Table 7, in our preliminary experiments with GPT-40-mini on incorrect contexts, we found that the language model tended to rely more on its internal knowledge when the question was presented before the context. However, when the context was presented first, the model was more easily misled"}, {"title": "F PROMPTS", "content": "Here are the prompts we use in the experiments. It should be noted that we use the \"document\" to represent the concept of context in real implementations."}, {"title": "F.1 CLOSED BOOK", "content": "It utilizes 3-shot to help it format the answer."}, {"title": "F.2 DIRECT INPUT AUGMENTATION", "content": "It utilizes 3-shot to help it format the answer."}, {"title": "F.3 COMPLETE FAITHFUL", "content": "It utilizes 3-shot to help it format the answer."}, {"title": "F.4 IMPLICITSCR", "content": "It utilizes the same 3-shot as Direct Input Augmentation to format the answer but order of the question and document follow the ImplicitSCR above template."}, {"title": "F.5 EXPLICITSCR PROMPT", "content": ""}, {"title": "GREDDITQA", "content": ""}, {"title": "G.1 HUMAN ANNOTATION GUIDELINE", "content": ""}, {"title": "G.1.1 DATASET OVERVIEW", "content": "This dataset is specifically designed to evaluate the resilience of large language models against misleading information derived from real-world erroneous documents. It features a collection of datapoints", "Note": "A factoid question is a concise, self-contained query that demands a specific, definite, and verifiable piece of factual information, typically resolvable in a single sentence or a brief phrase. Verification of the answer should be possible through reputable sources, including authoritative websites (e.g., government websites, Wikipedia, education institutions, established news organizations), textbooks, peer-reviewed con-ferences or journal papers, widely accepted common sense, or confirmation from at least three sec-ondary sources. For instance, a factoid question could be, \"What is the tallest building in the world as of 2024?\" This question is definite and verifiable. Conversely, a non-factoid question could be, \"What is considered the best movie of all time?\""}]}