{"title": "Rethink Deep Learning with Invariance in Data Representation (Tutorial Proposal)", "authors": ["Shuren Qi", "Fei Wang", "Tieyong Zeng", "Fenglei Fan"], "abstract": "Integrating invariance into data representations is a principled design in intelligent systems and web applications. Representations play a fundamental role, where systems and applications are both built on meaningful representations of digital inputs (rather than the raw data). In fact, the proper design/learning of such representations relies on priors w.r.t. the task of interest. Here, the concept of symmetry from the Erlangen Program may be the most fruitful prior. Informally, a symmetry of a system is a transformation that leaves a certain property of the system invariant. Symmetry priors are ubiquitous, e.g., translation as a symmetry of the object classification, where object category is invariant under translation.\n\nThe quest for invariance is as old as pattern recognition and data mining itself. Invariant design has been the cornerstone of various representations in the era before deep learning, such as the SIFT. As we enter the early era of deep learning, the invariance principle is largely ignored and replaced by a data-driven paradigm, such as the CNN. However, this neglect did not last long before they encountered bottlenecks regarding robustness, interpretability, efficiency, and so on. The invariance principle has returned in the era of rethinking deep learning, forming a new field known as Geometric Deep Learning (GDL).\n\nIn this tutorial, we will give a historical perspective of the invariance in data representations. More importantly, we will identify those research dilemmas, promising works, future directions, and web applications.", "sections": [{"title": "1 TOPIC AND RELEVANCE", "content": "The topic of this tutorial is a historical review of the invariance in data representations. The scope of this tutorial covers 1) the invariance in the era before deep learning, on old-fashioned invariant designs from various hand-crafted representations; 2) the invariance in the early era of deep learning, on the slump of the invariance principle and the success of the data-driven paradigm; 3) the invariance in the era of rethinking deep learning, on the revival of the invariance principle and the emergence of geometric deep learning as a way to bridge the research gap. For the depth within each era, the research dilemmas, promising works, future directions, and web applications will be sorted out. More details are expanded in Section 2.\n\nThe presenters are qualified for a high-quality introduction to the topic. We have extensive research experience and strong publication records in representation backbones and downstream applications of pattern recognition and data mining. More details are expanded in Section 3.\n\nThis tutorial is timely, due to the general limitations of today's intelligent systems and their web applications with respect to being only data-driven. Also, the invariance perspective (technology focus) and the historical perspective (broad horizons) are rarely seen in the tutorial tracks of related conferences.\n\nThis tutorial is relevant to the Web Conference. From a technological perspective, representations play a fundamental role in intelligent systems and their wide range of downstream web applications. From a practical perspective, the"}, {"title": "2 CONTENT", "content": "Over the past decade, deep learning representations, e.g., convolutional neural networks (CNN) and transformer, have led to breakthrough results in numerous artificial intelligence (AI) tasks, e.g., processing human perceptual information, playing board games, and generating realistic media. Without exception, these successful programs are consistent with the principle of empirical risk minimization and rarely involve other realistic factors. More recently, their applications are expanding to more real-world scenarios, e.g., medical diagnostics, self-driving cars, online services, and web platforms. In such scenarios, the robustness, interpretability, and efficiency of AI systems are crucial: 1) robustness means the performance of system is stable for intra-class variations on the input; 2) interpretability means the behavior of system can be understood or predicted by humans; 3) efficiency means the real-time availability and energy cost during human-computer interaction.\n\nIntegrating invariant structures into representations is a principled design towards robust, interpretable, and efficient AI systems [3]. Specifically, representations play a fundamental role, where the system is generally built on meaningful representations of digital inputs (rather than the raw data) [2]. Note that the proper design/learning of such representations in fact relies on priors w.r.t. the task of interest. Here, the concept of symmetry from the Erlangen Program [7] may be the most fruitful prior informally, a symmetry of a system is a transformation that leaves a certain property of system invariant [13]. Symmetry priors are ubiquitous, e.g., translation as a symmetry of the object classification where object category is invariant under translation [5].\n\nWe focus on historical perspectives on the invariance or symmetry in the development of data representations [1].\n\n\u2022 Going back to the era before deep learning, symmetry priors (e.g., invariance and equivariance) w.r.t. geometric transformations (e.g., translation, rotation, and scaling) have been recognized as main ideas in designing representations [9]. However, these hand-crafted representations are all fixed in design, relying on (under)-complete dictionaries, and therefore fail to provide sufficient discriminability at larger scales, e.g., ImageNet classification task [10].\n\n\u2022 As we enter the early era of deep learning, a cascade of learnable nonlinear transformations achieves over-complete representations of strong discriminative power for larger-scale pattern recognition and data mining tasks. As a textbook view now, representations should be learned not designed [8]. Therefore, early learning representations are equipped with very few symmetry priors, typically just translation equivariance. Hence,\nthese representations lack robustness, interpretability, and efficiency guarantees [6], e.g., the presence and understanding of adversarial perturbations [4]. Note that the compatibility between invariance and discriminability has emerged as a tricky problem when moving towards real-world AI [11].\n\n\u2022 Now in the era of rethinking deep learning, the invariance principle has returned, forming a new field known as Geometric Deep Learning (GDL) - endowing a pattern recognition and data mining system with the basic symmetry structure of the physical world, and harmonizing knowledge-driven invariant representations and data-driven deep representations [3]. The GDL research extends the scope of previous invariant theories from geometric, algebraic, and group, while showing the potential for uniformly improving the robustness, interpretability, and efficiency of the deep representation techniques [12].\n\nDespite the above GDL ideals, the current community of invariance faces the following research challenges:\n\n\u2022 At the theoretical level, classical invariance is based on certain global assumptions. As for more informative local and hierarchical invariants in pattern recognition and data mining (i.e., going partial and deep), there is a challenge on corresponding theoretical expansion.\n\n\u2022 At the practical level, classical invariance is often used in certain low-level pattern recognition and data mining tasks. As for higher-level tasks with symmetry prior, there is a challenge on corresponding practical expansion.\n\nWe can respond to the above challenges with a long-term research. One idea is to extend the successful structure of modern deep learning to knowledge approach, exploring the discriminative potential of hand-crafted representations. Another idea is to embed the experience of invariant theory into modern learning representations, replacing the black-box nature of typical learning with controllability. Such theoretical efforts at the representation level are expected to have a wide range of web-related applications.\n\n\u2022 Pattern recognition and data mining on graphs. A uniqueness of geometric deep learning is the applicability to data types, covering typical grids and surfaces, as well as generalized sets and graphs. A good representation on sets or graphs implies, in fact, a full exploitation of their complex geometric priors. Due to the nodes and connectivity properties that are natural to the web, a range of web problems relies on invariant representations of sets or graphs.\n\n\u2022 Recommender systems and social networks. Recommender systems are typically implemented by data mining on data of user-user graph (social networks), user-item graph (interactions), and item-item graph. Invariance is an important prior to achieve compact representations of graph data.\n\n\u2022 Cybersecurity and information forensics. The automated generation and online distribution of misinformation is"}, {"title": "3 ORGANIZERS", "content": "Shuren Qi\nDr. Shuren Qi is currently a Postdoctoral Fellow with Department of Mathematics, The Chinese University of Hong Kong. His research focuses on robust and explainable representations in Geometric Deep Learning, with applications in Trustworthy AI and Science AI. He has authored 14 papers in top-tier venues such as ACM Computing Surveys and IEEE Transactions on Pattern Analysis and Machine Intelligence. His works offer new designs of invariant representations from global to local and hierarchical assumptions.\n\nFei Wang\nDr. Fei Wang is currently an Associate Professor of Health Informatics in Department of Population Health Sciences, Weill Cornell Medicine, Cornell University. His major research interest is data mining and its applications in health data science. He has published more than 250 papers in AI and medicine, which have received more than 21.3K citations (GoogleScholar: link). His H-index is 70. His papers have won 8 best paper awards at top international conferences on data mining and medical informatics.\n\nTieyong Zeng\nDr. Tieyong Zeng is currently a Professor at the Department of Mathematics, The Chinese University of Hong Kong. Together with colleagues, he has founded the Center for Mathematical Artificial Intelligence (CMAI) since 2020 and served as the director of CMAI. His research interests include image processing, optimization, artificial intelligence, scientific computing, computer vision, machine learning, and inverse problems.\n\nFenglei Fan\nDr. Fenglei Fan is currently a Research Assistant Professor with Department of Mathematics, The Chinese University of Hong Kong. His primary research interests lie in NeuroAI and data mining. He has authored 26 papers in flagship AI and medical imaging journals. He was the recipient of the IBM AI Horizon Scholarship. He was also selected as the award recipient for the 2021 International Neural Network Society Doctoral Dissertation Award."}, {"title": "4 SCHEDULE", "content": "\u2022 Introduction (20 min)\nThe bottlenecks of deep learning\nThe potential of invariance\n\n\u2022 Preliminaries of invariance (20 min)\nConcepts of invariance and symmetry in physics and mathematics\nCases of invariance and symmetry in pattern recognition and data mining tasks\nFormalizations of invariance and symmetry\n\n\u2022 Invariance in the era before deep learning (40 min)\nInvariance of global representations\nInvariance of local sparse representations\nInvariance of local dense representations\n\n\u2022 Invariance in the early era of deep learning (40 min)\nHierarchical representations with data driven\nInvariance of hierarchical representations symmetry breaking\n\n\u2022 Invariance in the era of rethinking deep learning (40 min)\nGeometric deep learning as a way to bridge the gap\nThe good, the bad, and the ugly\n\n\u2022 Conclusions and discussions (20 min)\nReview this tutorial\nHighlight research opportunities"}, {"title": "5 STYLE AND AUDIENCE", "content": "This tutorial is lecture style. The intended audience for this tutorial mainly include researchers, graduate students, and industrial practitioners who are interested in exploring a new dimension of data representations.\n\nThe audience is expected to have the basic knowledge on deep learning, pattern recognition, and data mining. However, the tutorial will be presented at college junior/senior level and should be comfortably followed by academic researchers and industrial practitioners.\n\nAfter this tutorial, the audience are expected to 1) have a comprehensive understanding of basic invariance concepts; 2) learn a history of invariance before and after the era of deep learning; 3) know the bottlenecks in the data-driven-only paradigm, the revival of invariance, and the geometric deep learning; and 4) explore novel research opportunities in this area, and master how to use or even design invariance representations for their tasks."}, {"title": "6 TUTORIAL MATERIALS", "content": "This tutorial provides attendees with lecture slides, as well as a reading list of selected papers. We would like to state that such materials are free of any copyright issues."}, {"title": "7 VIDEO TEASER", "content": "This tutorial has a video trailer, available at link."}], "keywords": ["Pattern Recognition", "Data Mining", "Invariance", "Symmetry", "Representation", "Tutorial"]}