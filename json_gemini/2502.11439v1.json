{"title": "An Efficient Row-Based Sparse Fine-Tuning", "authors": ["Cen-Jhih Li", "Aditya Bhaskara"], "abstract": "Fine-tuning is an important step in adapting foundation models such as large language models to downstream tasks. To make this step more accessible to users with limited computational budgets, it is crucial to develop fine-tuning methods that are memory and computationally efficient. Sparse Fine-tuning (SFT) and Low-rank adaptation (LoRA) are two frameworks that have emerged for addressing this problem and have been adopted widely in practice. In this work, we develop a new SFT framework, based on ideas from neural network pruning. At a high level, we first identify \"important\" neurons/nodes using feature importance metrics from network pruning (specifically, we use the structural pruning method), and then perform fine-tuning by restricting to weights involving these neurons. Using experiments on common language tasks, we demonstrate that our method significantly improves the memory efficiency of SFT without increasing training time complexity and implementation complexity, while achieving accuracy comparable to state-of-the-art methods such as LORA and its variants.", "sections": [{"title": "1. Introduction", "content": "The paradigm of pre-training followed by fine-tuning has seen tremendous success in the last few years. Very large models (often referred to as foundation models) are first trained, typically using very large amounts of data and computational resources, using self-supervised learning approaches (Dosovitskiy, 2020; Achiam et al., 2023; Dubey et al., 2024; Zhou et al., 2024). When building a model for a new task (which could be a supervised learning task), the idea is to start with the foundation model and then tune its parameters, possibly after adding additional classification layers, by training using task-specific data. The pre-train then fine-tune paradigm has been shown to have significant advantages over training a new model from scratch for the new task. Often, high accuracy can be obtained using much smaller datasets for the new task.\nDespite the success, fine-tuning a model with billions of parameters requires access to heavy computational resources, even when the task datasets are fairly small. Fortunately, studies (e.g., (Panigrahi et al., 2023) and references therein) show that fine-tuning only a small fraction of parameters can be effective. Parameter-efficient fine-tuning (PEFT) methods have thus been proposed to carry out this idea and address the challenge of making fine-tuning more accessible (Lialin et al., 2023). A leading PEFT approach, Low-Rank Adaptation (LoRA, Hu et al. 2022), achieves memory efficiency by simply making low-rank updates to the weight matrices in the different layers. Another class of PEFT methods is sparse fine-tuning (SFT, Sung et al. 2021; Guo et al. 2021; Ansell et al. 2022; Nikdan et al. 2024), which learns a sparse matrix, typically an unstructured one, for updating the pre-trained weights. However, SFT typically incurs higher memory costs than LoRA during the fine-tuning process, because of the unstructured sparsity. Several works aim to mitigate the memory complexity of SFT (Mofrad et al., 2019; Holmes et al., 2021; Nikdan et al., 2023; 2024), often at the cost of increased running time and more complex implementations of sparse kernels. Besides PEFTs, techniques like Zeroth-Order optimization (Malladi et al., 2023; Guo et al., 2024b) and quantization (Gholami et al., 2022; Dettmers et al., 2022; 2024) can further enhance memory and training efficiency for fine-tuning, including LORA and SFT.\nAs LLMs increase in scale, advancing efficient sparse matrix computation, PEFT, and efficient training remains a crucial problem. Towards this goal, we study the question: Can sparse fine-tuning be improved to create a memory- and parameter-efficient framework, while avoiding additional implementations of sparse operations and without increasing the training time complexity? We answer this question in the affirmative, by proposing a new SFT framework for fine-tuning LLMs and Vision Transformers that achieves memory- and parameter-efficiency while maintaining or even improving performance on downstream tasks. Our approach utilizes NN pruning techniques to identify a subset of fine-tuning parameters and employs a matrix decomposition-based computation for efficient fine-tuning. This design enables the integration of ideas from model compression, SFT, and matrix decomposition methods."}, {"title": "1.1. Our Contributions", "content": "At a high level, our contributions are as follows:\n\u2022 We leverage ideas from network pruning to improve SFT, achieving significant memory efficiency considerably lower than the popular LoRA. Our method uses only standard tensor operations, eliminating the need for custom sparse tensor libraries. Our approach is also modular, and it allows us to integrate several existing pruning techniques (which give different neuron importance scores) and works with all layer types, including LayerNorm and BatchNorm, which LoRA cannot directly handle.\n\u2022 We analyze the memory assignment of several PEFT methods and suggest that model architecture and computation graphs affect memory more significantly than the number of trainable parameters. We validate our methods across diverse fine-tuning tasks (language and vision) and provide practical guidance on training strategies to maximize efficiency and accuracy.\n\u2022 We propose two variants of the Taylor importance for different settings in image and language tasks: class-aware Taylor and Zeroth-Order Taylor. The first one is designed for tasks where class-wise accuracy is important (in addition to overall accuracy), such as image classification. Zeroth-Order Taylor is designed for large language models and requires memory only equal to that of a forward pass. In addition, we show how to effectively reduce the estimation variance of the Zeroth-Order estimator."}, {"title": "2. Background and Related Work", "content": "Parameter-Efficient and Memory-Efficient Fine-Tuning:\nIn various language and vision tasks, the \u201cpre-train then fine-tune\" paradigm has been shown highly effective. PEFT methods (Lialin et al., 2023) fine-tune a small subset of the parameters of a large pre-trained model in order to accelerate the training process. We begin by introducing SFT and LORA, two popular approaches for PEFT."}, {"title": "Sparse Fine-Tuning:", "content": "SFT formulates the fine-tuning process as learning another weight matrix W:\n$$W = W + W_s,$$\n$$h = f(\\hat{W},x) = f(W+W_s,x),$$"}, {"title": "Techniques for Efficient Sparse Computation:", "content": "To reduce memory redundancy in sparse tensor computations, various data formats like compressed sparse column/row (CSC/CSR, Mofrad et al., 2019; Lu et al., 2024) and semi-structured formats (Holmes et al., 2021) have been proposed. These formats enable efficient operations like Sparse Matrix Multiplication (SpMM), which is crucial for dot products and matrix multiplications. Upon these techniques, sparse backpropagation is built to improve training efficiency (Zhang et al., 2020; Gale et al., 2020; Peste et al., 2021; Schwarz et al., 2021; Hoefler et al., 2021; Jiang et al., 2022; Nikdan et al., 2023; Xu et al., 2024). Beyond sparse tensor techniques, NVIDIA also offers memory optimization techniques for efficient training\u00b9.\nHowever, these techniques come with trade-offs, particularly in terms of time complexity and implementation complexity. Achieving memory efficiency often requires a significant increase in time complexity. To mitigate this, some approaches employ optimizations implemented in C++ or lower-level languages, such as those used in (Gale et al., 2020; Nikdan et al., 2023; 2024), to accelerate the training process."}, {"title": "Low-Rank Adaptation (LoRA):", "content": "Instead of requiring Ws"}, {"title": "Neural Network Pruning:", "content": "Besides PEFTs, neural network pruning is another widely applied technique that exploits parameter sparsity to reduce model complexity and speed up inference (LeCun et al., 1989; Han et al., 2015; Han, 2017; Hoefler et al., 2021). Most pruning methods assess importance of neural network weights (or neurons) and remove the least important parameters. Unstructured pruning zeros out individual weights while preserving the network architecture, whereas structured pruning removes parameter groups like channels or neurons, which reduce model size (Liu et al., 2021; Fang et al., 2023; Ma et al., 2023). Both approaches often require retraining to recover lost accuracy during pruning. While effective for classical NNs, pruning LLMs is costly due to high memory demands for computing importance scores and the prohibitive retraining step, making memory-efficient LLM pruning an active research area (Frantar & Alistarh, 2023; Sun et al., 2024)."}, {"title": "3. Number of Trainable Parameters Is Not Everything", "content": "Before introducing our approach, we want to emphasize that in PEFT research, reducing the number of trainable parameters is not the most critical factor for saving memory. Some PEFT methods mentioned in Section 2 focus on reducing the number of trainable parameters to decrease memory consumption. However, once the number of trainable parameters is sufficiently small, this reduction is no longer the most critical factor influencing memory usage.\nIn NN training, backpropagation involves caching numerous intermediate values to compute gradients efficiently for each tensor. The memory cost of these intermediate values is heavily influenced by the computation graph and the model architecture. When the number of trainable parameters is relatively small, the memory consumed by intermediate values far exceeds that of the trainable parameters themselves. We use VeRA, ROSA, and DoRA to demonstrate the influences of these factors."}, {"title": "4. Our Method", "content": "To address the challenges mentioned above, we propose Structured-Pruning-based Sparse Fine-Tuning (SPruFT), as illustrated in Figure 1. This is a novel approach designed to streamline computation graphs and eliminate the need for dropout layers. This method ensures memory efficiency while maintaining competitive fine-tuning performance."}, {"title": "4.1. Proposed Method", "content": "SPruFT utilizes structured neural network pruning to select a subset of the parameters for fine-tuning. NN pruning methods have been studied extensively (see Section 2) with the goal of reducing the size of a network (often fewer neurons) while preserving accuracy. These methods develop techniques for identifying neurons that are important for a given task. Our key insight is to use these importance metrics to indicate which neurons to focus on during fine-tuning. Note that, unlike pruning, where importance often reflects a neuron's role in the original task, here it pertains to the downstream fine-tuning task, which may have a different input distribution and loss. In Section 4.2, we discuss various importance metrics from pruning research and discuss their use in fine-tuning.\nOur method selects the top-r important neurons based on an importance score \u03b7, where r is determined by the desired number of fine-tuning parameters. It follows that the choice of importance matric becomes crucial, which we discuss in Section 4.2. Let the top r neuron indices be i1, i2, ..., ir. After obtaining \u03b7, we next construct a lower-dimensional parameter matrix Wf \u2208 Rr\u00d7din, with the row selection matrix Mizj = 1 for all j \u2208 [r] and zeros elsewhere. Using notations from Section 2, we initialize Wf to zero and define the final parameters \u0174 as Equation 1 where W = MWf.\nLet us now examine how to implement the forward to make backpropagation memory-efficient\u00b2. If the computation graph were to pass through W + MWf (as a na\u00efve implementation would), the gradients would be computed for all din \u00d7 dout parameters, which is redundant. Instead, we use the additivity of the forward function: we have, analogous to the setting of LoRA,\n$$f(\\hat{W},x) = f(W + MW_f,x) = f(W,x) + f (MW_f,x),$$\nAs W remains frozen during fine-tuning, backpropagation only needs to keep track of the derivatives of the second term on the RHS. In addition, M is now a fixed matrix, so the only trainable parameters are those in Wf and f(Wf, x) will not be cached, while LoRA requires the cache of f(A, x) for computing ah (backpropagation, Rumelhart et al. 1986). Besides, as an SFT framework, our method does not rely on any dropout layer, which also saves a huge amount of memory. We explain this in detail in Appendix C.5 and we show that the benefits in terms of memory cost are significant in Section 6.1.\nAn important strength of our approach is its flexibility: it can easily incorporate any desired choice of importance metrics. On the other end, it can also incorporate new ideas in PEFT research. For example, quantization (QLoRA, Dettmers et al. 2024), parameter sharing (VeRA, Kopiczko et al. 2024), and combining SFT with LoRA (ROSA, Nikdan et al."}, {"title": "4.2. Importance Metrics", "content": "Importance evaluation plays a crucial role in our approach, as discussed above. We try various choices in our work: the first is the simple l2 norm of the weight vector corresponding to each neuron; the second is the widely-used Taylor importance (LeCun et al., 1989). By considering the gradients, Taylor importance captures more information about the input distribution as well as the relevance of a neuron for the fine-tuning task of interest (which can be different from the original model). We also consider different variants of Taylor importance, as we discuss below. We remark that norm-based importance can be quite powerful on its own, as is the case with norm-sampling in the matrix approximation literature (Frieze et al., 2004)."}, {"title": "4.2.1. CLASS AWARE TAYLOR IMPORTANCE", "content": "In our experiments on image classification tasks, we also consider a \"class aware\" variant of Taylor importance, which may be of independent interest. The motivation here comes from the observation that the importance of a neuron may depend on the class of an input example (as a toy example, a whisker-detecting neuron may be very important to the cat class, but not much to others; hence not too important on average). Another motivation comes from the observation that when we perform a vanilla (class agnostic) fine-tuning, the accuracy of some classes can be much worse than others an undesirable outcome.\nWe define the class-wise Taylor importance as follows: for neuron i and label t,\n$$\u03b7_i := |L(D^+, \\hat{F}_{c_i})-L(D^+, F)| \\approx |\\bar{w} \\nabla_wL(D^+, F)|,$$"}, {"title": "4.2.2. ZERO-ORDER TAYLOR IMPORTANCE", "content": "As discussed, Taylor importance can incorporate information about the data distribution and the fine-tuning task when evaluating important neurons. However, for large models like Llama-3, it turns out that the computational overhead required for computing Taylor importances is prohibitively large!\u00b3 In these cases, we apply the idea from the memory-efficient zeroth-order optimizer (MeZO, Malladi et al. 2023) to estimate the gradient in Taylor importance. The classical Zeroth-Order (ZO) setting is defined as below.\nDefinition 4.1 (Simultaneous Perturbation Stochastic Approximation or SPSA (Spall, 1992)). Given a model F with parameters \u03b8 \u2208 Rd and a loss function L, SPSA estimates the gradient on a dataset D as\n$$\\nabla L(\\theta, D) = \\frac{L(\\theta + \\epsilon z) \u2013 L(\\theta \u2013 \\epsilon z)}{2\\epsilon}z,$$\nwhere z \u2208 Rd is drawn from z ~ N(0, Id), and \u0454 is the scale of the perturbation. The n-SPSA estimate averages \u25bdL(\u03b8, D) over n randomly sampled z. Note that as e \u2192 0, the estimate above converges to z(z\u00af\u2207L(0, D)), which is equal to VL(0, D) in expectation.\nBy applying SPSA, the Zero-Order Taylor (ZOTaylor) importance can be defined as follows:\n$$\u03b7 := |\\bar{0}g|.$$\nwhere we denote [VL(0, D)] and its estimate as g and g for convenience.\nA na\u00efve implementation of SPSA still requires twice the memory of inference because of the need to store z. However, MeZO uses the trick of regenerating z dynamically using a random seed (of much smaller size than the model), thus eliminating the storage and ensuring memory usage that is equal to that of inference. We now assess the effectiveness of ZOTaylor for our LLM."}, {"title": "Property 4.2.", "content": "n-SPSA is an unbiased and consistent estimator with the variance o\u00b2 where\n$$\\sigma^2 = \\frac{g_i^2}{n} + \\frac{\\sum_{l=1}^d \\bar{g}_l^2}{n}$$"}, {"title": "5. Experimental Setup", "content": "We use multiple datasets for different tasks. For image classification, we fine-tune models on the training split and evaluate it on the validation split of Tiny-ImageNet (Tavanaei, 2020), CIFAR100 (Krizhevsky et al., 2009), and Caltech101 (Li et al., 2022). For text generation, we fine-tune LLMs on 256 samples from Stanford-Alpaca (Taori et al., 2023) and assess zero-shot performance on nine EleutherAI LM Harness tasks (Gao et al., 2021). See Appendix D for details."}, {"title": "5.2. Models and Baselines", "content": "We fine-tune full-precision Llama-2-7B and Llama-3-8B (float32) using our SPruFT, LORA (Hu et al., 2022), VeRA (Kopiczko et al., 2024), DoRA (Liu et al., 2024), and ROSA (Nikdan et al., 2024). ROSA is chosen as the representative SFT method and is the only SFT due to the high memory demands of other SFT approaches, while full fine-tuning is excluded for the same reason. We freeze Llama's classification layers and fine-tune only the linear layers in attention and MLP blocks.\nNext, we evaluate importance metrics by fine-tuning Llamas and image models, including DeiT (Touvron et al., 2021), ViT (Dosovitskiy, 2020), ResNet101 (He et al., 2016), and ResNeXt101 (Xie et al., 2017) on CIFAR100, Caltech101, and Tiny-ImageNet. For image tasks, we set the fine-tuning ratio at 5%, meaning the trainable parameters are a total of 5% of the backbone plus classification layers."}, {"title": "5.3. Training Details", "content": "Our fine-tuning framework is built on torch-pruning5 (Fang et al., 2023), PyTorch (Paszke et al., 2019), PyTorch-Image-Models (Wightman, 2019), and HuggingFace Transformers (Wolf et al., 2020). Most experiments run on a single A100-80GB GPU, while DORA and ROSA use an H100-96GB GPU. We use the Adam optimizer (Kingma & Ba, 2015) and fine-tune all models for a fixed number of epochs without validation-based model selection.\nImage models: The learning rate is set to 10-4 with cosine annealing decay (Loshchilov & Hutter, 2017), where the minimum learning rate is 10-9. All image models used in this study are pre-trained on ImageNet.\nLlama: For LoRA and DoRA, we set \u03b1 = 16, a dropout rate of 0.1, and a learning rate of 10\u20134 with linear decay (0.01 decay rate). For SPruFT, we control trainable parameters using rank instead of fine-tuning ratio for direct comparison."}, {"title": "6. Results and Discussion", "content": "We now present the results of fine-tuning image models and Llamas using our framework. We first apply our SPruFT to fine-tune Llama2 and Llama3 and compare the results with those obtained using LoRA and its variants. Following this, we examine the performance of our approach by utilizing various importance metrics."}, {"title": "6.1. Main Results of LLM", "content": "We apply our SPruFT method to fine-tune Llama2-7B and Llama3-8B, comparing the results with those obtained through LoRA and its variants. We select the magnitude of the neuron vector as the importance metric due to its low memory requirements, simplicity, and widely tested effectiveness. In contrast, gradient-based metrics like Taylor and Hessian are as memory-intensive as full LLM fine-tuning. While Wanda (Sun et al., 2024) offers a memory-efficient metric for pruning LLMs, it still requires one epoch of data forwarding and significantly more memory than inference to compute the input vector's norm6. For epochs choosing, we opt for 5 epochs to balance computational resources and performance."}, {"title": "6.2. Importance Metrics", "content": "We apply various importance metrics to fine-tune Llamas and image models using our approach and report the results to compare their performance. As shown in Table 5 and Table 6, Quantile-Mean Taylor and ZOTaylor offer slight improvements over other importance metrics. For image tasks, while the differences among importance metrics are not substantial, the results consistently indicate that Quantile-Mean Taylor slightly outperforms standard Taylor importance. Additionally, both Quantile-Mean Taylor and standard Taylor importance outperform magnitude-based importance.\nSimilarly, in the cases of Llama2 and Llama3, our findings suggest that ZOTaylor provides a slight performance boost for fine-tuned models. This improvement is likely due to ZOTaylor's ability to capture richer data information, whereas magnitude-based importance tends to focus more on identifying generally important neurons. However, the observed performance gain remains modest, potentially due to the variance of the estimates, as discussed in Section 4.2.2. Beyond these observations, another interesting finding is that fine-tuned models with random row selection perform similarly to VeRA, likely suggesting that this accuracy level could serve as a baseline for other fine-tuning approaches."}, {"title": "7. Conclusions and Future Work", "content": "We propose a parameter-efficient fine-tuning (PEFT) framework that integrates various techniques and importance metrics from model compression research to enhance sparse fine-tuning (SFT). Using our method, we can fine-tune LLMs and vision transformers using significantly less computation resources than the popular LoRA (Low-Rank Adaptation) technique, while achieving similar accuracy. We also explore the effects of using different importance metrics. There are several future directions: (1) For importance metrics, while Quantile-Mean Taylor shows slight improvements, these gains are relatively minor compared to the standard Taylor metric in some cases of DeiT and ViT. We may wish to explore better metrics for classification tasks with a large number of labels. (2) Developing memory-efficient importance metrics for LLMs is another future direction. While Zeroth-Order Taylor is effective for incorporating data-specific information without requiring large memory, the large variance of estimate is a challenge. Although we reduce the variance effectively by increasing the number of estimations, exploring a simple method to reduce variance without increasing estimation times is essential for further advancements in this field. (3) Our results show that fine-tuning a small number of neurons can significantly improve model performance on downstream tasks. This observation naturally raises the question: do the selected neurons play a distinctive role in specific tasks? This question is related to the explainability of neural networks, which is an extensive area of research. It will be interesting to understand if (and how) individual neurons chosen for fine-tuning contribute to the new task."}, {"title": "B. Parameter Dependency", "content": "Dependencies of parameters between neurons or channels across different layers exist in NNs. These include basic layer connections, residual connections, tensor concatenations, summations, and more, as shown in Figure 2. The black neurons connected by real lines represent the dependent parameters that are in the same group. Pruning any black neurons results in removing the parameters connected by the real lines. (Liu et al., 2021) introduced a group pruning method for CNN models that treats residual connections as grouped dependencies, evaluating and pruning related channels within the same group simultaneously. Similarly, (Fang et al., 2023) proposed a novel group pruning technique named Torch-Pruning, which considers various types of dependencies and achieves state-of-the-art results. (Ma et al., 2023) further applied this procedure to pruning LLMs. Torch-Pruning can be applied to prune a wide range of neural networks, including image transformers, LLMs, CNNs, and more, making it a popular toolkit for neural network pruning."}, {"title": "In this study,", "content": "we also evaluate the influences of incorporating parameter dependency in our approach. We put the experimental results of whether incorporating parameter dependency in Appendix C.2. In the experiments, parameter dependency becomes the following process for our approach: first, searching for dependencies by tracing the computation graph of gradient; next, evaluating the importance of parameter groups; and finally, fine-tuning the parameters within those important groups collectively. For instance, if Wa; and W. are dependent, where Wa; is the j-th column in parameter matrix (or the j-th input channels/features) of layer a and W is the i-th row in parameter matrix (or the i-th output channels/features) of layer b, then Wa; and W will be fine-tuned simultaneously while the corresponding Madep for W; becomes column selection matrix and Wat becomes Wf,depMaep. Consequently, fine-tuning 2.5% output channels for layer b will result in fine-tuning additional 2.5% input channels in each dependent layer. Therefore, for the 5% of desired fine-tuning ratio, the fine-tuning ratio with considering dependencies is set to 2.5% for the approach that includes dependencies.\nThe forward function of layer a for column selection mentioned above can be written as the following equation:\n$$f(W^a, x) = f(W^a, x) + f(M^aW^a, x) + f (W^f,depM^a_{dep}, x).$$Note that in this example, as the dependency is connection between the output feature/channel of b and the input feature/channel of a, the dimension dep is equal to dout where Wa \u2208 Rdout\u00d7din, Wb \u2208 Rdout\u00d7din."}, {"title": "C. Ablation Studies and Related Analysis", "content": "In this section, we first discuss the hyperparameter settings. While we do not include DeBERTaV3 (He et al., 2023) in the main context, we fine-tune DeBERTaV3-base (He et al., 2023) on GLUE. The learning rate is set to 2 \u00b7 10-5 with linear decay, where the decay rate is 0.01. The model is fine-tuned on the full training split of 8 tasks from the GLUE benchmark. The maximum sequence length is fixed to 256 with longer sequences truncated and shorter sequences padded. Note that memory efficiency is not emphasized for small-scale models, as dataset-related memory\u2014particularly with large batch sizes-dominates consumption in these cases. The main advantage of our method in these cases is the reduced FLOPs due to fewer trainable parameters.\nFollowing this, we discuss the computational resource requirements for fine-tuning. Figure 4 illustrates the computation and cache requirements during backpropagation. Next, we provide an ablation study on the impact of different rank settings for our approach and LoRA, as shown in Table 12. Finally, Table 14 demonstrates the advantages of freezing self-attention blocks to reduce memory usage while maintaining performance."}, {"title": "C.1. Hyperparameter Settings", "content": "We report the results of three approaches over several epochs as table 7 and table 8. Overall, full fine-tuning over higher epochs is more prone to overfitting, while head fine-tuning shows the exact opposite trend. Except for the results on caltech1019, the loss patterns across all models consistently reflect this trend, and most accuracy results further support this conclusion. However, our approach demonstrates a crucial advantage by effectively balancing the tradeoff between performance and computational resources.\nTable 7 clearly shows that both our approach and full fine-tuning achieve optimal results within a few epochs, while head fine-tuning requires more training. Notably, all models have been pre-trained on ImageNet-1k, which may explain the strong performance observed with head fine-tuning on Tiny-ImageNet. However, even with this advantage, full fine-tuning still outperforms head fine-tuning, and our approach surpasses both. In just 5 epochs, our approach achieves results comparable to full fine-tuning on all datasets with significantly lower trainable parameters.\nIn contrast to Table 7, the results in Table 8 show more variation. Although the validation loss follows a similar trend, we report only the evaluation metrics due to the different patterns observed in these metrics. One potential reason for this variation is the varying amounts of training data across the GLUE tasks. As shown in the table, tasks with fewer samples often require more epochs to achieve better performance for both full fine-tuning and our approach. Conversely, for tasks with large amounts of training data such as 'MNLI', \u2018QNLI', 'QQP', and 'SST-2', the results show tiny improvement from 3 to 10 epochs. Nevertheless, the results still demonstrate that our approach significantly balances the tradeoff between performance and computational resources. Our method achieves near full fine-tuning performance with remarkably less trainable parameters."}, {"title": "C.2. Considering Dependency", "content": "We evaluate our approach with and without considering parameter dependency, as shown in Table 9 and Table 10."}, {"title": "C.3. Memory Measurement", "content": "In this study, we detail the memory measurement methodology employed. The total memory requirements can be categorized into three main components:\n$$mem_{TTL} = mem_{Model} + mem_{FT} + mem_{Aux},$$where:\n1. memTTL is the total memory consumed during training.\n2. memModel represents the memory consumed by the base model itself.\n3. memFT corresponds to the memory required for the fine-tuning parameters and their gradients.\n4. memaux accounts for any additional memory usage, including optimizer states, caching, and other intermediate computations.\nWe yield memby measuring the memory usage during inference on the training data using the pre-trained model. The combined memory usage of memFr and memaux is calculated as the difference between memTTL and memModel. For simplicity, we consistently report memFT + memAux as \u201cmem\" in all comparisons presented in this study."}, {"title": "C.4. Resource Requirements", "content": "Table 11 presents the resource requirements of various PEFT methods. We compare our approach with LoRA and several of its variants that maintain or surpass LoRA's performance. As shown, our method is the most resource-efficient among these approaches. The subsequent ablation study further demonstrates that our approach achieves performance comparable to LoRA. We exclude comparisons with VeRA (Kopiczko et al., 2024), which proposes sharing a single pair of random low-rank matrices across all layers to save memory footprint. While VeRA achieves some memory savings, its performance often deteriorates.\nWe note that while our approach offers significant memory efficiency, this benefit is less pronounced in small-scale models, where the primary memory consumption arises from the dataset-especially with large batch sizes. The main advantage of to cache intermediate outputs for gradient computation. For example, as illustrated in Figure 4, using LoRA, Vout must"}]}