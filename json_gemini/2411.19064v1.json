{"title": "Way to Specialist: Closing Loop Between Specialized LLM and Evolving Domain Knowledge Graph", "authors": ["Yutong Zhang", "Lixing Chen", "Shenghong Li", "Nan Cao", "Yang Shi", "Jiaxin Ding", "Zhe Qu", "Pan Zhou", "Yang Bai"], "abstract": "Large language models (LLMs) have demonstrated exceptional performance across a wide variety of domains. Nonetheless, generalist LLMs continue to fall short in reasoning tasks necessitating specialized knowledge. Prior investigations into specialized LLMs focused on domain-specific training, which entails substantial efforts in domain data acquisition and model parameter fine-tuning. To address these challenges, this paper proposes the Way-to-Specialist (WTS) framework, which synergizes retrieval-augmented generation with knowledge graphs (KGs) to enhance the specialized capability of LLMs in the absence of specialized training. In distinction to existing paradigms that merely utilize external knowledge from general KGs or static domain KGs to prompt LLM for enhanced domain-specific reasoning, WTS proposes an innovative \"LLMOKG\" paradigm, which achieves bidirectional enhancement between specialized LLM and domain knowledge graph (DKG). The proposed paradigm encompasses two closely coupled components: the DKG-Augmented LLM and the LLM-Assisted DKG Evolution. The former retrieves question-relevant domain knowledge from DKG and uses it to prompt LLM to enhance the reasoning capability for domain-specific tasks; the latter leverages LLM to generate new domain knowledge from processed tasks and use it to evolve DKG. WTS closes the loop between DKG-Augmented LLM and LLM-Assisted DKG Evolution, enabling continuous improvement in the domain specialization as it progressively answers and learns from domain-specific questions. We validate the performance of WTS on 6 datasets spanning 5 domains. The experimental results show that WTS surpasses the previous SOTA in 4 specialized domains and achieves a maximum performance improvement of 11.3%.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs), e.g., GPT-4 [1], Gemini [54], and Llama [57] have demonstrated their exceptional performance across a wide range of general domains [1, 10, 13, 37, 56, 57]. Yet, there is a prevalent recognition that LLMs often exhibit limitations when confronted with reasoning tasks that necessitate specialized knowledge [41]. Most explorations of LLMs to date for specific domains, e.g., medical [27, 73] and law [50] fields, have leveraged domain-specialized training or parameter fine-tuning techniques in pursuit of performance enhancement. These processes necessitate the acquisition of high-quality instruction data and sophisticated design of training pipelines, both of which entail substantial efforts [11].\nIn light of these challenges, we focus on steering foundation models via Retrieval-Augmented Generation (RAG) [29, 48] to excel in specialty areas. RAG falls under the category of prompt engineering"}, {"title": "2 RELATED WORKS", "content": "Large Language Models & Knowledge Graphs: The integration of KG and LLM can be categorized into two branches: KG for LLM and LLM for KG. The works in the branch of KG for LLM mainly use Retrieval-Augmented Generation (RAG) [21] to incorporate non-parametric information [26, 46, 52, 63, 76] from KGs to pretrained LLM for addressing knowledge-intensive text generation task. KAPING [4] pioneered KG-augmented LLM by retrieving relevant knowledge triples from KGs and prepending them to the input question as a prompt. KAPING only utilizes a single-layer KG, making it challenging to address questions that require multihop knowledge analysis on KGs. To address this issue, the works [2, 28, 51, 68] conduct multi-hop knowledge retrieval over KGs to improve the reasoning capacity of LLMs. KG-augmented LLMs also"}, {"title": "3 METHODOLOGY OF WAY-TO-SPECIALIST", "content": ""}, {"title": "3.1 Preliminaries", "content": "As WTS rests on a synergistic integration of domain knowledge graph (DKG) and retrieval-augmented LLM, it is essential to provide a brief introduction to these two techniques prior to delineating the design of WTS.\nDomain Knowledge Graph: A DKG is characterized as a collection of knowledge triples $G = {\\{(e_s, r, e_o)|e_s \\in E, r\\in R, e_o \\in E\\}}$,\nwhere $(e_s, r, e_o)$ is a knowledge triple, $\\& $ is the entity set, R is the relation set, $e_s, r$, and $e_o$ correspond to the subject entity, relation and object entity, respectively. WTS maintains DKG in the form of vector databases [47], where entities and relationships within the DKG are represented as high-dimensional feature embedding. This approach enables efficient storage, retrieval, and analysis of the DKG through vector-based operations, drastically improving the efficiency of DKG-based prompting (will be demonstrated in subsequent sections).\nRetrieval-augmented LLM: Retrieval-augmented LLM [4] involves integrating an external knowledge base with LLM to provide relevant information during the generation process. The key idea is"}, {"title": "3.2 Architecture of WTS", "content": "As illustrated in Figure 3, WTS encompasses two closely interconnected components: DKG-Augmented LLM and LLM-Assisted DKG Evolution. DKG-Augmented LLM prompts LLM with domain knowledge in DKG to improve the reasoning capability of LLM for domain-specific questions. LLM-Assisted DKG Evolution employs LLM to generate knowledge triples from questions and answers, and use them to evolve DKG."}, {"title": "3.2.1 DKG-Augmented LLM", "content": "The DKG-Augmented LLM consists of four modules: entity extraction module, retrieval module, pruning module, and reasoning module.\nEntity Extraction Module: Upon receiving a question q, WTS utilizes the entity extraction module to identify the topic entities of the question. While traditional entity extraction methods, e.g., Hidden Markov Models (HMMs) [34, 78] and Conditional Random Fields (CRFs) [40], exist for implementation, they often struggle with handling domain terms. Inspired by the superior performance of LLMs in entity recognition [3, 45, 62], we utilize LLM for entity extraction. Specifically, the entity extraction module prompts LLM (the input prompt is given in Appendix D.1, Prompts 1) to extract N primary entities, denoted by $E_q = {\\{e_{q,1}, e_{q,2}, ..., e_{q,N}\\}}$,\nfrom question q. This process can be mathematically written as\n$E_q = LLM_{Ent} (q)$, where $LLM_{Ent}$ is the prompted LLM. The extracted entities will be fed to the subsequent retrieval module. Note that the number of extracted entities is restricted to N to mitigate the retrieval overhead for non-informative entities.\nRetrieval Module: Given extracted entities, the retrieval module retrieves within the DKG to obtain knowledge triples related to the extracted entities in $E_q$. For ease of description, we for now consider that the retrieval module operates with a non-empty DKG. Subsequent sections will provide a detailed exposition of the construction and evolution of DKG. The retrieval process executes in an iterative manner, with the retrieval depth progressively increasing over successive iterations.\nWe delineate 1st-depth retrieval to facilitate a comprehensive understanding of the retrieval process. The 1st-depth retrieval acquires knowledge triples associated with entities in $E_q$. It is important to note that DKG can be of considerable scale with dense relations between entities. This complexity presents stressing challenges to retrieval efficiency. Our retrieval module employs a dual approach, using Exact Match for coarse-grained retrieval and Similarity Retrieval for fine-grained retrieval, to mitigate the retrieval overhead over large DKGs. We let $E^{(1)}_q$ ($E^{(1)}_q \\leftarrow E_q$) be the input to the 1st-depth retrieval. For each entity $e \\in E^{(1)}_q$, the retrieval module first executes Exact Match as coarse filtering to include knowledge triples in DKG that have e as their subject entity or object entity. Mathematically, the output of Exact Match at 1-st depth can be written as $T^{(1),EM}_q = {\\{t = (e_s, r, e_o) \\in G | e_s = e \\text{ or } e_o = e, \\forall e \\in E^{(1)}_q\\}}$. Subsequently, Similarity Retrieval is implemented to refine knowledge triples in $T^{(1),EM}_q$. Similarity Retrieval uses the pre-trained"}, {"title": "3.2.2 LLM-Assisted DKG Evolution", "content": "LLM-assisted DKG endeavors to enhance the completeness of DKG by progressively incorporating domain knowledge acquired during question answering. The objective of DKG completion is twofold. Firstly, it aims to incorporate new knowledge (entities and relations) previously absent from both DKG and inherent knowledge of LLM. This is pertinent in scenarios where no triples are retrieved for a question or the evaluation of the answer $a^{(D)}_q$ remains negative at the final depth D. Secondly, DKG completion seeks to enhance the characterization of knowledge dependencies by activating knowledge connections previously neglected. This is pertinent to scenarios where a positive answer is obtained after multiple iterations of knowledge retrieval, which indicates relevant knowledge in the current DKG is not adequately connected. In this case, additional relations should be incorporated into DKG to ensure closer association between related entities.\nDomain Knowledge Generation: The fundamental step of DKG evolution is generating knowledge triples based on questions and answers. However, the extraction of knowledge graphs from unstructured data presents significant challenges, primarily due to the inherent variability and complexity of natural language [42]. Given the irregular and unstructured nature of question-answer pairs, our work leverages LLMs to design a schema-free solution for KG triple generation. Firstly, WTS prompts LLM (the input prompt is given in Appendix D.1, Prompt 4) to extract domain knowledge from question-answer pairs, using entities in the previously retrieved knowledge triples, i.e., $T^{(D)}_q$, as references. Then LLMs generate knowledge triples based on the extracted domain knowledge. Given question q and the gold answer $a_q$, the module"}, {"title": "3.3 WTS Formation Pipeline", "content": "In previous sections, we have presented DKG-Augmented LLM and LLM-Assisted DKG Evolution independently. Next, to show how these two components work in tandem to complement each other's capabilities.\nLet us start from the initialization stage. The initial DKG can be either an empty dataset or an established DKG composed of knowledge triples. This enables WTS to support various implementation scenarios, including those where the domain lacks a pre-existing KG or where the available DKG is incomplete.\nAs shown in Figure 3, the formation pipeline of WTS involves cyclical interactions between DKG-Augmented LLM and LLM-Assisted DKG Evolution throughout the process of question answering. The timeline of WTS formation is discretized by the arrival of questions. The DKG possessed by WTS at the reception of question q is denoted by $G_q$, which is used in DKG-Augmented LLM to prompt the LLM for answer generation. Subsequently, WTS calls LLM-Assisted DKG Evolution to generate knowledge triples $T_{q+}$ based on question q and gold answer $a_q$, and retrieved knowledge triples $T^{(D)}_q$. The generated triples $T_{q+}$ are then incorporated to update DKG from $G_q$ to $G_{q+1}$. The updated DKG $G_{q+1}$ will be used to answer the subsequent question $q + 1$.\nFrom a practical perspective, the formation pipeline of WTS can be split into two phases, Apprenticeship and Mastership, contingent upon the availability of gold answers. During Apprenticeship, WTS functions akin to an apprentice, receiving gold answers from a mentor after addressing questions. This scenario is applicable when comprehensive Q&A datasets are established in a particular"}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 Experiment Setup", "content": ""}, {"title": "4.1.1 Datasets and Baselines", "content": "We evaluate each method on 6 Q&A datasets, covering 5 domains, including medical (ChatDoctor5k [23], PubMedQA [20], MedMCQA [38]), natural science, social science, linguistics (SciQ [66], ScienceQA [44]), and one general domain Q&A datasets (Simple Questions [8]). The MedMCQA dataset includes both single-choice and multiple-choice questions, while in the ScienceQA dataset, SOC, NAT, and LAN refer to the social science, natural science, and linguistics domains, respectively.\nFor the dataset ChatDoctor5k, we evaluate the text generation task with BERTScore [77], which computes a similarity score of the LLM-generated answer and the real answer. To calculate the BERTScore, we use the bert-base-uncased [15] model as the embedding model. For the other dataset, we use Accuracy as the metric."}, {"title": "4.1.2 Hyperparameters", "content": "The experiments run WTS with two LLMs, ChatGPT-3.5-turbo and GPT-40, using OpenAI API. The temperature is set to 0.2 and the maximum token length of output is set to 2048. For all datasets, WTS is initialized with an empty vector database as its DKG. With ChromaDB serving as the vector database, we employ the pre-trained sentence transformer, all-mpnet-base-v2, as the embedding model. The embedding similarity is measured by the cosine distance and the maximum permissible similarity gap is set to L = 0.55. To facilitate result evaluation, WTS utilizes prompts to restrict its output format to JSON. This restriction is also applied to GPT-3.5-turbo and GPT-40 baselines for fairness consideration. For the ToG and CoT baselines, minimal instructions are added to their original prompts to ensure consistency in the evaluation process. Further details are provided in Appendix B.3."}, {"title": "4.2 Results and Evaluations", "content": ""}, {"title": "4.2.1 Comparison to Baselines", "content": "Table 1 compares the performance of WTS and baselines. The results indicate that WTS (with GPT-40) outperforms the baselines in 5 of 6 evaluated datasets. The exception occurs on the SimpleQA which is a general domain Q&A dataset, and in this case, ToG achieves the best performance as it includes a well-established general KG, i.e., Freebase KG, for knowledge retrieval.\nWTS demonstrates a substantial performance improvement over the standard I/O prompt methods (i.e., GPT-3.5 and GPT-40) on all 5 domain datasets. The highest performance improvement is achieved on the PubMedQA, providing 103.8% and 126.9% accuracy gain for GPT-3.5 and GPT-40, respectively. However, it is noteworthy that in the ChatDoctor5k dataset, GPT-40 underperforms GPT-3.5. This anomaly is attributed to the conservation of GPT-40, wherein it frequently qualifies its responses as non-professional advice and recommends patients to consult a medical professional.\nWTS exhibits notable advantages in comparison with CoT across all evaluated datasets. The most significant performance improvements are 15.6% and 12.1% on the single-choice and multiple-choice MedMCQA datasets, respectively. These enhancements are attributable to the incorporation of external domain knowledge. In contrast, CoT relies solely on the intrinsic knowledge of the LLM, which lacks domain-specialized information.\nFor commonsense reasoning (i.e., SimpleQA), ToG outperforms other methods as it utilizes commonsense knowledge retrieved from Freebase. However, when applied to specialized domains, the performance of ToG becomes inferior because the knowledge in Freebase is coarse and insufficient for domain-specialized questions.\nTo examine the complexity of the knowledge retrieval process, we calculate the average retrieval time and the execution time of DKG-Augmented LLM (with GPT-3.5) for a single question. The"}, {"title": "4.2.2 Ablation Study", "content": "How does the retrieval mechanism influence the performance of WTS? We conduct ablation studies on the retrieval mechanism by executing WTS with 3 alternative retrieval methods."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced the LLMOKG paradigm, which achieves bidirectional enhancement between LLMs and knowledge graphs. We proposed the flexible plug-and-play Way-to-Specialist (WTS)"}, {"title": "A ALGORITHM FOR WTS", "content": "We present a detailed overview of the algorithmic progress of WTS, as shown on Algorithm 1."}, {"title": "B EXPERIMENT DETAILS", "content": ""}, {"title": "B.1 Dataset", "content": "The statistics of the datasets used in this paper are shown in Table 4. To avoid the influence of context provided by the dataset on the"}, {"title": "B.2 Metrics", "content": "For the dataset ChatDoctor5k, we evaluate the text generation task with BERTScore [77], which computes a similarity score for each token in the candidate sentence with each token in the reference sentence. The complete score matches each token in x to a token in x to compute recall, and each token in x to a token in x to compute precision. We use greedy matching to maximize the matching similarity score, where each token is matched to the most similar token in the other sentence. We combine precision and recall to compute an F1 measure. For a reference x and candidate x, the recall, precision, and F1 scores are:\n$R_{BERT} = \\frac{1}{T} \\sum_{x_i \\in x} max_{x'_j \\in x'} \\Pi_{BERT}(x_i, x'_j)$ (1)\n$P_{BERT} = \\frac{1}{T} \\sum_{x'_i \\in x'} max_{x_j \\in x} \\Pi_{BERT}(x'_i, x_j)$\n$F_{BERT} = 2 X \\frac{P_{BERT}* R_{BERT}}{P_{BERT} + R_{BERT}}$ (2)"}, {"title": "B.3 Evaluation", "content": "To facilitate result evaluation, WTS employs prompts to constrain its output format to JSON. To prevent irrelevant generations from interfering with answer evaluation, the LLMs are instructed to provide the label of their answers in multiple-choice questions. Consequently, the evaluation is based solely on whether the label corresponds to the correct answer. This constraint is also applied to the GPT-3.5-turbo and GPT-40 baselines for the sake of fairness. For the ToG and CoT baselines, to avoid imposing format restrictions that may affect their performance, we use a specific signal by adding '(' before the answer and ')' after it. This method ensures the precise extraction of answers and upholds consistency throughout the evaluation process."}, {"title": "C SUPPLEMENTARY RESULTS", "content": "Table 5 shows the average execution time of WTS for a question, where D represents the maximal retrieval depth. It is apparent that WTS with GPT-40 API incurs a higher time cost compared to GPT-3.5.\nTable 6 shows the performance of the Mastership phase on the ChatDoctor5k dataset. During the Apprenticeship phase, we sampled 800 data to conduct medical Q&A and consistently evolve the DKG. In the Mastership phase, we tested the performance of WTS with another 200 non-overlapping samples. The results show that WTS achieves state-of-the-art performance compared to the four baseline models.\nThe execution time (sec) of the different maximal retrieval depths on three medical domain datasets, ChatDoctor5k, PubMedQA, and MedMCQA, is shown in Table 7. For ChatDoctor5k and PubMedQA, the maximum retrieval depth ranges from 1 to 4, while for MedMCQA, it is set from 2 to 5. The results indicate that as the maximum retrieval depth increases, execution time correspondingly rises.\nWe analyzed the answers across six datasets to investigate the evidence for WTS in generating answers, as illustrated in Figure 8. The results indicate that in the general domain and certain science domains, e.g., social science, which includes a considerable amount of commonsense questions, a significant proportion of the answers rely exclusively on the intrinsic knowledge embedded within LLM's parameters. However, in specialized domains such as the medical domain and linguistics, accurate answer generation requires specific domain knowledge. Consequently, a substantial proportion of these queries rely on a combination of knowledge from both retrieved triples and the LLM's inherent knowledge for answer generation.\nTable 8 shows the size of the knowledge graph constructed in the process of WTS. The results show that advanced models with more inherent knowledge lead to the construction of more concise DKGs. Furthermore, the advanced retrieval mechanisms also enhance the conciseness of DKGs by optimizing the WTS's capability to utilize knowledge efficiently."}, {"title": "D PROMPTS OF METHODS", "content": "Taking MedMCQA, a medical QA dataset with choices as an example, we show the prompt used in the experiments of WTS, ChatGPT, CoT and ToG methods."}, {"title": "D.1 WTS (Way-to-Specialist)", "content": "In WTS, there are four kinds of interaction with the LLMs. The following shows the prompts of WTS including Question Entity Extract Prompt, Triple Score and Prune Prompt, LLM Reason with Triples Prompt, and LLM Generate KG Triple Prompt.\nAs shown in Prompt 1, the \"Question Entity Extraction\" prompt of WTS is designed to extract key entities from questions. The"}, {"title": "D.2 ChatGPT", "content": "For ChatGPT, we prompt it to directly generate the answer to the given question with one shot. The single example given is to assist"}, {"title": "D.4 ToG (Think-on-Graph)", "content": "The prompts for ToG are adapted from [51] where ToG is proposed and tailored to the dataset used in this paper. In [51], the entities in the questions are provided in their datasets. Therefore, we incorporated a Question Entity Extraction module similar to that in WTS."}]}