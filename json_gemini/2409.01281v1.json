{"title": "Path-Consistency: Prefix Enhancement for Efficient Inference in LLM", "authors": ["Jiace Zhu", "Yingtao Shen", "Jie Zhao", "An Zou"], "abstract": "To enhance the reasoning capabilities of large language models (LLMs), self-consistency has gained significant popularity by combining multiple sampling with majority voting. However, the state-of-the-art self-consistency approaches consume substantial computational resources and lead to significant additional time costs due to the multiple sampling. This prevents its full potential from being realized in scenarios where computational resources are critical. To improve the inference efficiency, this paper introduces path-consistency, a method that leverages the confidence of answers generated in earlier branches to identify the prefix of the most promising path. By dynamically guiding the generation of subsequent branches based on this prefix, the path-consistency mitigates both the errors and redundancies from random or less useful sampling in self-consistency. As a result, it can significantly accelerate the inference process by reducing the number of tokens generated. Our extensive empirical evaluation shows that the path-consistency achieves significant acceleration in inference latency ranging from 7.8% to 40.5%, while maintaining or even improving task accuracy across different datasets, including mathematical reasoning, common sense reasoning, symbolic reasoning, and code generation.", "sections": [{"title": "1 Introduction", "content": "The range of tasks that large language models (LLMs) can accomplish is continuously expanding, as the scale and complexity of models continue to grow recently. However, this advancement has not yet endowed LLMs with sufficiently robust reasoning capabilities (Rae et al. 2021; Wu et al. 2016; Guo et al. 2018; Chen et al. 2022). To address this shortcoming and further extend the application scope of LLMs, chain-of-thought (CoT) (Wei et al. 2022) prompting has emerged in response. CoT prompting uses reasoning problems and processes as input prompts to guide language models in generating reasoning paths and final answers, aiming to mimic the thought processes humans might use when solving mathematical or logical problems. This enables LLMs to be applied in an increasing number of specific scenarios, such as mathematical reasoning (Cobbe et al. 2021; Miao, Liang, and Su 2020), logical reasoning (Geva et al. 2021), and code generation (Chen et al. 2021)."}, {"title": "2 Background", "content": "2.1 Reasoning with Self-Consistency\nSelf-consistency (Wang et al. 2023) is a sampling strategy distinct from greedy decoding (Vaswani et al. 2017), significantly enhancing the reasoning performance of language models. This approach generates multiple reasoning paths and aggregates the final output through majority voting.\nEarlier research primarily focused on optimizing reasoning for individual tasks (Andor et al. 2019; Ran et al. 2019; Geva, Gupta, and Berant 2020; Piekos, Malinowski, and Michalewski 2021). More recently, numerous inference strategies based on self-consistency have been proposed, particularly using self-evaluation to calibrate LLMs (Zhang et al. 2023; Shinn, Labash, and Gopinath 2023; Madaan et al. 2024; Paul et al. 2024). For example, self-evaluation guided by random beam search (Xie et al. 2024) leverages additional language models to search for the optimal inference path. Other methods, such as Deductive Beam Search (Zhu et al. 2024), optimize inference by emphasizing the relationships between each reasoning step. However, these approaches often require increased computational resources and sometimes involve extensive model calls to improve task accuracy in LLMs. This results in additional time costs, always making them inefficient in practical applications and contrary to our original intention.\n2.2 Approaches for Efficient Reasoning\nRecent works have attempted to achieve efficient reasoning by using smaller models or increasing inference speed. Distillation (Hinton, Vinyals, and Dean 2015), as an effective model compression technique, has been employed to create smaller models for reasoning tasks (Fu et al. 2023; Ho, Schmid, and Yun 2023; Magister et al. 2023; Li et al. 2023). Additionally, various approaches have sought to improve inference speed by altering inference strategies, thereby avoiding modifications to the model architecture and retraining (Aggarwal et al. 2023; Li et al. 2024). However, these methods often compromise the diversity inherent in self-consistency, which can negatively impact the quality of the generated outputs. The proposed path-consistency is a model-agnostic method that can be applied to any model, including compressed models, to enhance inference efficiency while maintaining task accuracy."}, {"title": "3 Methodology", "content": "3.1 Characterization of Self-Consistency\nSelf-consistency in large language models (LLMs) involves generating multiple branches of reasoning, each potentially leading to different answers. The final answer is determined through aggregation methods, such as majority voting. Unlike greedy decoding, self-consistency avoids the shortcoming of local optimality, and multiple samplings reduce the randomness of single-step sampling.\nHowever, the primary drawback of self-consistency is the significant computational redundancy. With N branches, each answer is derived from N similar but independent inference processes, leading to an N-fold increase in computational cost compared to greedy decoding for a single problem. To improve self-consistency, the goal is to achieve similar sampling effects while reducing the time cost of redundant computations.\nWe propose an intuitive hypothesis: for example, a particular mathematical problem might have five different reasoning paths, p\u2081 to p5. If the model frequently errs on paths P1 to p3 while p4 and p5 are relatively simpler, then full self-consistency wastes significant computational resources on the problematic paths. As shown in Figure 2, statistics on the number of tokens generated during self-consistency across various datasets reveal that over 25%, and sometimes even 50%, of tokens are wasted on incorrect branches. By sampling multiple times only on p4 and p5, we could enhance resource utilization and improve output accuracy. Furthermore, storing limited information from paths p4 and p5 to guide subsequent branch generation could significantly accelerate inference speed and efficiency.\nAdditionally, self-consistency involves extensive redundant processes without yielding intermediate results, with the final answer only emerging at the end. If useful information could be identified early in the generation process to guide subsequent branching, outcomes might improve. Intuitively, when tackling complex problems, using simple criteria to preliminarily assess the quality or correctness of the current generation during intermediate stages can enhance the effectiveness of subsequent steps. Our method aims to reduce the time wasted on incorrect branches while increasing the efficiency of generating correct inference paths."}, {"title": "3.2 Path-Consistency", "content": "Based on the internal mechanisms of self-consistency, we propose an automated dynamic reasoning approach path-consistency that continually seeks the \u201coptimal path\u201d in the form of the \u201cprefix\u201d. Thereby progressively reducing the number of tokens that need to be generated and significantly shortening inference latency. The specific methodology and examples are shown in Figure 3 and Table 1, and can be described as the following \u201cextract-and-sample\" process:\n\u2022 Begin by generating a small number of reasoning paths. Then, using confidence assessment metrics\u00b9 identify the current optimal reasoning path and extract the initial short prefix (e.g., the first step of the current optimal reasoning path).\n\u2022 Sample one from the extracted initial short prefixes as a part of the prompt to guide the model in generating additional reasoning paths. Again, identify the current optimal reasoning path using confidence assessment metrics, and determine the next slightly longer prefix.\n\u2022 Repeat the above steps iteratively, progressively extending the prefix length until the optimal reasoning path is identified and aggregated to produce the final answer.\nTable 1 illustrates the reasoning process of path-consistency with an example. We have taken a reasoning path from different stages of the branching process as samples in Table 1. In the initial branches, the model generates the reasoning paths and answers normally. When a particular answer appears frequently and has a higher confidence level compared to other answers within the current branch, the first step of the corresponding reasoning path is extracted and used as the input prompt for the next stage. As shown in Table 1, as the number of branches increases, the prefix length grows, the generated part subsequently shortens.\nWhen solving complex problems, a LLM takes an input prompt or question, represented as q, and generates a distribution of answers, denoted as P(a|q), after producing a reasoning path. During inference, the model generates a multi-step reasoning path R = [r\u2081,r\u2082,...,rT] = r\u2081:T under the guidance of CoT. This process can also be expressed as\n$P(a | q) = E_{R~P(R|q)}P(a | q, R)$.\nIn basic self-consistency method, the generation process is mechanically repeated numerous times without variation, whereas path-consistency attempts to gather advantageous intermediate information from the early stages of the generation process for subsequent generations. Path-consistency gradually identifies sufficiently confident partial reasoning steps Rprefix = [r\u2081, r\u2082,...rt] and incorporate them as part of the input, thereby better guiding the subsequent reasoning steps and the generation of answer a. Thus, this process can be expressed as\n$P(a | q, R_{prefix}) = E_{R~P(R|q, R_{prefix})} P(a | q, R)$."}, {"title": "3.3 Problems with Truth Is in the Hands of a Few", "content": "In the basic self-consistency method, when generating answers for particularly challenging questions, the problems with \"Truth Is in the Hands of a Few\" may still occur despite the continuous generation of multiple branches. This means that the correct answer may not be the most frequently occurring one, leading to an incorrect final answer in majority voting. In the proposed path-consistency, during the continuous exploration for the optimal path, if the problems with \"truth is in the hands of a few\" are encountered during a specific prefix selection, there's a concern that this undesirable phenomenon may be exacerbated in subsequent branches. We will use the followings to demonstrate that the proposed approach does not worsen this problem.\nTo present the analysis, we make the following assumptions: the probability of generating the correct answer is po, and apart from the correct answer, the model generates only one unique incorrect answer. The total number of branches is set to N, and prefix selection based on majority voting is performed only once at N/2. If the correct answer is the majority at the time of prefix selection, the correct prefix will be selected to guide subsequent generation, increasing the probability of generating the correct answer in the remaining branches to p\u2081 = po + \u2206p; Conversely, if the incorrect answer is the majority at this point, the probability of generating the correct answer in subsequent branches decreases to p2 = po \u2212 \u2206p. Using the binomial distribution formula, the probability of the correct answer being in the majority during the vote is given by:\n$P_{vote} = \\sum_{k=\\lceil N/2 \\rceil}^{N/2} {N/2 \\choose k} p_0^k (1-p_0)^{N/2-k}$.\nAfter prefix selection, the probability of obtaining the correct answer in the subsequent N/2 branches increases or decreases to\n$P_{inc} = \\sum_{k=\\lceil N/2 \\rceil}^{N/2} {N/2 \\choose k} p_1^k (1-p_1)^{N/2-k}$\nand\n$P_{dec} = \\sum_{k=\\lceil N/2 \\rceil}^{N/2} {N/2 \\choose k} p_2^k (1-p_2)^{N/2-k}$.\nTherefore, the final probability of obtaining the correct answer after prefix selection can be expressed as\n$P_{correct} = P_{vote} P_{inc} + (1 - P_{vote})\\cdot P_{dec}$.\nIf no prefix selection is performed, the probability of generating the correct answer in subsequent steps remains unchanged and is equal to the probability at N/2 with majority voting:\n$P_{correct} = P_{vote}$.\nTo ensure that accuracy is not adversely affected, we require\n$P_{correct} > P_{correct}$.\nIt can be simplified to:\n$P_{vote} (P_{inc} - P_{dec} - 1) + P_{dec} \\geq 0$.\nFrom this formula, we observe that the greater the difference between Pinc and Pdec, the more reliable this method becomes, and the smaller the impact on accuracy.\nDuring prefix selection, employing confidence-based criteria can make this process more reliable. For instance, using the beta confidence criteria (Aggarwal et al. 2023)\n$\\int_{0}^{0.5} \\frac{p^{N/2-vm} . (1 \u2212 p)^{vm} dp}{N/2}$,\nwhere \u03c5m represents the number of majority elements. The majority element is considered sufficiently reliable for prefix selection only when this value exceeds a pre-set confidence threshold (Cthreshold).\nIf we roughly assume N = 20, \u03c1\u03bf = 0.4, \u2206p = 0.01, the final accuracy without prefix selection is Pcorrect = 16.62%, and the final accuracy after prefix selection by majority voting is Pvoting = 15.15%, by Beta Stopping Criteria is PC=0.8 = 16.10%. Actually, Pvoting = PC=0. Under the same conditions, if po = 0.6(\u2265 0.5), then both PC=0 and PC=0.8 will be greater than Pcorrect.\nWhile this calculation is a rough estimate and the actual scenario is likely more complex, it provides insight into the impact of path-consistency on problems where \u201cTruth Is in the Hands of a Few\". Essentially, path-consistency amplifies the accuracy of self-consistency: when self-consistency performs well for a particular input, path-consistency may perform even better; when self-consistency performs poorly, the accuracy of path-consistency might decrease, but not significantly. Moreover, prefix selection, guided by a confidence threshold, offers a safer alternative to direct selection through majority voting."}, {"title": "4 Experiments", "content": "We conduct extensive experiments on various tasks to compare and verify the improvements in performance and efficiency of path-consistency over basic self-consistency. Additionally, we demonstrate the efficient process of path-consistency continuously accelerating as prefixes are progressively selected.\n4.1 Setup\nBenchmarks. We will evaluate the task performance and efficiency improvement of path-consistency through the following types of tasks: (1) Arithmetic Reasoning, including GSM8K (Cobbe et al. 2021), SVAMP (Patel, Bhattamishra,"}, {"title": "4.2 Result", "content": "Efficient Inference Process. Table 2 shows the internal behavior and acceleration mechanism of path-consistency, using the GSM8K dataset as an example. The experiments compare the advantages of path-consistency over the basic self-consistency method under different confidence criteria, focusing on task accuracy, inference latency and the number of generated tokens. The results show that, across all confidence criteria, path-consistency not only enhances task accuracy but also provides a maximum acceleration of 28.9%. Observing the acceleration process of path-consistency, it is evident that as the prefix level increases, the speed improvement becomes more significant, reaching up to 78.9% at Level-3 while maintaining at least 27.0% as shown in Figure 7a. It can also lead to a 23.0% to 47.4% decrease in the number of generated tokens as shown in Figure 7b.\nArithmetic Reasoning. Table 3, rows 1-4, show the task performance on mathematical reasoning datasets, along with three additional datasets. Path-consistency ensures task performance that is almost comparable to or even better than"}, {"title": "Commonsense Reasoning", "content": "Table 3, rows 5-7, evaluates the performance of path-consistency on commonsense reasoning datasets. In terms of task accuracy, path-consistency is slightly underperforms compared to the baseline in the StrategyQA and Ruins Names tasks. However, this decline can be mitigated by properly adjusting the confidence threshold. In the Salient Translation task, path-consistency still maintains better task performance compared to the baseline. Regarding acceleration, path-consistency provides up to 34.2%, 10.4%, and 30.5% speed improvements on StrategyQA, Ruins Names, and Salient Translation datasets, respectively.\nAccording to the data, it can be found that the performance of path-consistency on the Ruins Names task is less impressive compared to other datasets. This is because the Ruins Names task is more difficult, with longer input content and longer required prompts, making the benefits of the prefixing behavior less noticeable."}, {"title": "Symbolic Reasoning", "content": "Table 3, rows 8-10, compares the performance of different methods on the symbolic reasoning datasets. Path-consistency performs exceptionally well in terms of task accuracy, especially in the Tracking Shuffled Objects task, where it achieves the highest accuracy of 5.2% among all datasets. Additionally, compared to the baseline, it delivers approximately a 20% speedup of inference latency for tasks such as Boolean Expressions, Tracking Shuffled Objects, and Logical Deduction.\nWe can observe that, in general, lower confidence thresholds indicate a more aggressive prefix selection strategy, often resulting in more significant efficiency improvements. However, in Logical Deduction task, the efficiency improvement is actually higher at a confidence threshold of 0.8 compared to 0.7. This is because a more aggressive prefix selection strategy is more likely to choose suboptimal prefixes, which makes the subsequent branching generation less effective. Specifically, we found that with a confidence threshold of 0.7, the efficiency improvement at prefix level 2 is more significant than at prefix level 3, as prefix level 3 generates a large number of erroneous paths, leading to this phenomenon."}, {"title": "4.3 Scale to Ambiguous Aggregation Scenarios", "content": "Self-consistency baseline aggregates answers at the end of all branches, while the prefix extraction behavior in path-consistency can be seen as an early aggregation. This early aggregation serves as an initial assessment of complex problems, continually accumulating information to aid in the final aggregation. Inference tasks generally have accurate answers, providing confidence metrics for the initial aggregation. However, there are also ambiguous aggregation scenarios. For example, in code generation tasks, there may be no exact answer. We also explore modifying the initial aggregation method in these scenarios, utilizing path-consistency to accelerate inference.\nThe prefix selection strategy for code generation is different from that for clear-cut aggregation scenarios, we need to find different intermediate evaluation metrics tailored to specific situations. In the code generation process, we should not access the test parts in advance since there is no analogous \"answer\" part. Therefore, we use the abstract syntax tree (AST) to check the generated code at any time and simply consider code without syntax errors as correct code.\nAdditionally, code generation does not have the concept of \"confidence\u201d. In reasoning tasks, we extract the \u201cprefixes\" of reasoning paths corresponding to the majority of answers without knowing if the extracted reasoning paths are \"better\". Hence, we need to use constraints like \"majority element\" or \"high confidence\" to limit this behavior. However, In code generation task, code that can be parsed by the AST is certainly a \"better\" choice compared to code with syntax errors, and no additional constraints are needed"}, {"title": "4.4 Analysis", "content": "Error and Redundancy. In Section 3.1, we characterize the challenges of self-consistency. On one hand, it wastes computational resources on incorrect branches, and on the other hand, it mechanically repeats the same computations without obtaining useful intermediate information. Figure 6 shows the changes in the proportion of tokens generated by path-consistency on correct or incorrect reasoning paths."}, {"title": "Hyperparameters Analysis", "content": "We explored the impact of prefix extraction frequency on path-consistency. With the setting of 20 branches, if the highest prefix level is set to level-3, the prefix is extracted every 5 branches; if the highest prefix level is level-4, the prefix is extracted every 4 branches. Our observations indicate that maintaining the highest prefix level at level-3 preserves higher task accuracy. Increasing the highest prefix level enables a more aggressive prefix selection strategy, leading to a more noticeable acceleration in inference."}, {"title": "4.5 Comparison with Similar Work", "content": "Researchers have proposed various methods to enhance the inference efficiency of LLMs. The proposed path-consistency offers several key advantages: (1) It is a model-agnostic prompting method, making it easily deployable across different models and tasks; (2) It does not require additional computation, ensuring that the final results remain consistent with those generated by directly using the model; (3) Furthermore, with the protection of a confidence threshold, path-consistency maintains task accuracy without compromise.\nAggarwal et al. (Aggarwal et al. 2023) introduced the concept of confidence for the first time and applied it in adaptive-consistency to enable adaptive allocation of computational resources, thereby improving inference efficiency. After performing a confidence assessment on early branches' answers similar to path-consistency, adaptive-consistency will directly exit if a significantly high-confidence answer is present, selecting this answer as the final answer and avoiding further branch sampling and inference. Intuitively, adaptive-consistency offers greater acceleration potential; however, it essentially compromises the inherent nature of self-consistency.\nThe diversity of the reasoning paths is the key to a better performance in self-consistency (Wang et al. 2023). Adaptive-consistency, to some extent, undermines this diversity, which may enhance performance for simpler datasets but can be limiting for more complex ones. Path-consistency ensures that the performance of self-consistency remains unaffected while maximizing the acceleration of inference and optimizing the allocation of computational resources, making it a more flexible and broadly applicable technique."}, {"title": "5 Conclusion", "content": "This paper proposes path-consistency, which achieves internal optimization of self-consistency. This technique extracts information from early branches in the form of \"prefixes\", guiding the generation of subsequent branches more efficiently and accurately. The success of our technique across a broad range of tasks, including arithmetic, symbolic, and commonsense reasoning, as well as code generation, demonstrates its robustness in various application areas. In every task, path-consistency effectively enhances the efficiency of LLMs in performing reasoning tasks. Experiments reveal that using an appropriate confidence threshold can improve efficiency while ensuring the quality of the generated output and task accuracy."}, {"title": "5.1 Limitations", "content": "The effectiveness of path-consistency is closely related to the generation quality of LLMs. When the task is challenging and the model struggles to solve it effectively, path-consistency may be less effective. Additionally, when the model generates longer and more complex reasoning steps, the acceleration effect of path-consistency may be less pronounced. However, as the capabilities of the model improve, the potential of path-consistency can be better utilized."}, {"title": "A Appendix", "content": "A.1 Further Analysis\nRobustness to Confidence Threshold. As shown in Figure 7, we examined the relationship between the confidence of the majority element in the tenth branch of the self-consistency baseline and the final answer across all datasets. It was observed that a confidence level around 0.8 serves as an important threshold for determining the correctness of the final answer. Therefore, selecting confidence levels around [0.7, 0.8, 0.9] is a more appropriate choice.\nProtection of High-Confidence Answers As illustrated in the Figure 8, the distribution of confidence for the final answers across all samples is shown. The implementation of path-consistency has led to a significant increase in the number of high-confidence answers, while reducing the number of samples with confidence levels around 0.5. This suggests that path-consistency effectively resolves many ambiguous cases. Consequently, this approach not only avoids compromising task accuracy but may even enhance it.\nA.2 Code Analysis\nSection 3.2 introduces the \u201cextract-and-sample\" process of path-consistency. Following is the code implementation of the key parts:\nSample. Randomly sample a prefix from a list of prefixes."}]}