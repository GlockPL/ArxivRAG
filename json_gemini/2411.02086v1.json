{"title": "Real-time and Downtime-tolerant Fault Diagnosis for Railway Turnout Machines (RTMs) Empowered with Cloud-Edge Pipeline Parallelism", "authors": ["Fan Wu", "Muhammad Bilal", "Haolong Xiang", "Heng Wang", "Jinjun Yu", "Xiaolong Xu"], "abstract": "Railway Turnout Machines (RTMs) are mission-critical components of the railway transportation infrastructure, responsible for directing trains onto desired tracks. Due to frequent operations and exposure to harsh environments, RTMs are susceptible to failures and can potentially pose significant safety hazards. For safety assurance applications, especially in early-warning scenarios, RTM faults are expected to be detected as early as possible on a continuous 7x24 basis. However, limited emphasis has been placed on distributed model inference frameworks that can meet the inference latency and reliability requirements of such mission-critical fault diagnosis systems, as well as the adaptation of diagnosis models within distributed architectures. This has hindered the practical application of current AI-driven RTM monitoring solutions in industrial settings, where single points of failure can render the entire service unavailable due to standalone deployment, and inference time can exceed acceptable limits when dealing with complex models or high data volumes. In this paper, an edge-cloud collaborative early-warning system is proposed to enable real-time and downtime-tolerant fault diagnosis of RTMs, providing a new paradigm for the deployment of models in safety-critical scenarios. Firstly, a modular fault diagnosis model is designed specifically for distributed deployment, which utilizes a hierarchical architecture consisting of the prior knowledge module, subordinate classifiers, and a fusion layer for enhanced accuracy and parallelism. Then, a cloud-edge collaborative framework leveraging pipeline parallelism, namely CEC-PA, is developed to minimize the overhead resulting from distributed task execution and context exchange by strategically partitioning and offloading model components across cloud and edge. Additionally, an election consensus mechanism is implemented within CEC-PA to ensure system robustness during coordinator node downtime. Comparative experiments and ablation studies are conducted to validate the effectiveness of the proposed distributed fault diagnosis approach. Our ensemble-based fault diagnosis model achieves a remarkable 97.4% accuracy on a real-world dataset collected by Nanjing Metro in Jiangsu Province, China. Meanwhile, CEC-PA demonstrates superior recovery proficiency during node disruptions and speed-up ranging from 1.98x to 7.93x in total inference time compared to its counterparts.", "sections": [{"title": "I. INTRODUCTION", "content": "RAILWAY transportation offers a high-capacity, cost-effective, and environmentally friendly solution for long-distance travel, making it a popular choice for passenger and freight services in Europe, Asia, and North America. According to M&M market research [1], the global railway system was valued at $25.1 billion in 2022 and is estimated to reach $30.9 billion by 2027. The Railway Turnout Machines (RTMs), also known as the Railway Point Machines (RPMs), are critical components of the railway transportation infrastructure, responsible for directing trains onto desired tracks. However, RTMs are prone to failures due to wearing caused by frequent operations and exposure to harsh outdoor environments. Statistical analysis reveals RTMs as one of railside equipment that experience the highest failure rates, accounting for 18% of all documented railway system failures occurring between 2011 and 2017 [2]. The malfunction of RTMs can lead to catastrophic accidents such as collisions and train derailments, resulting in severe casualties and property losses. This typically involves the concept of preventive maintenance [3], which calls for regularly scheduled inspections and repairs targeting at the prevention of failures before they occur. For a long time, such condition-based maintenance mainly depends on the expert knowledge and experience of railway workers and thus can be time-consuming and labor-intensive. Therefore, an unsupervised, resilient, and responsive RTM fault early-warning system for train drivers and maintenance groups has raised lots of concern in the industry.\nWith the advent of information technology, Railside Monitoring Units (RMUs) are deployed to collect runtime data during the operation of RTMs. Numerous fault diagnosis methods have been developed utilizing the collected data on vibration [4], current [5], [6], [7], torque and acoustic signals [8], etc. Previous endeavors have been primarily dedicated to enhancing model accuracy, while paying little attention to the performance and reliability issues caused by inappropriate deployment methods [9]. For safety assurance applications, especially in early-warning scenarios, we expect faults to be detected as early as possible to provide drivers and maintenance groups with more response time. The high computational overhead and complex procedures of these fault diagnosis models can make real-time inference challenging on resource-constrained devices such as Personal Computers (PCs). The traditional standalone deployment [10], where all the model components are deployed on a single device or platform, is also susceptible to system-wide unavailability in case of any software or hardware malfunctions on that centralized node [11].\nCloud computing has then become a common approach to wide range of fault diagnostic applications in Industry 4.0 [12], micro-electromechanical systems (MEMS) [13], Cloud Native [14], etc. However, the data gathered must be sent to the cloud to harness the high-performance and elastic advantages of cloud computing. In addition to privacy concerns [15] stemming from the sensitive nature of sensor data (e.g., route schedules and geographical locations), the transmission of data in railway environments like underground tunnels, inevitably leads to data loss and network latency issues [16]. These factors significantly impair the real-time capabilities of cloud-based solutions and hinder their effectiveness in monitoring mission-critical infrastructure [17].\nIn the past decade, academic interest has grown in combining edge computing with fault detection for model deployment, also known as Edge Intelligence (EI) [18]. This novel approach shifts computation from centralized cloud servers to the network edge, offering latency [19], energy consumption [20], Quality of Service (QoS) [21] and mobility [22] enhanced solutions. Federated Learning (FL) [23] has emerged as a potent approach for preserving privacy during model training, which enable each distributed client to train a local replica of the global model with its own dataset before sending updates to aggregate the shared global model.\nHowever, limited emphasis has been placed on distributed model inference frameworks that can meet the latency and reliability requirements of the fault diagnosis model deployment, or on tailoring the diagnosis models to perform optimally within distributed architectures. The inherent complementarity of cloud and edge computing has fostered the concept of cloud-edge collaboration [24], a paradigm that dynamically allocates and coordinates computational tasks across cloud and edge. This collaborative approach has inspired new paradigms for AI-driven real-time and downtime-tolerant monitoring tasks in mission-critical industrial applications [25], where such systems benefit from the high availability characteristic of modern cloud computing infrastructure and the low-latency capabilities afforded by edge computing deployments. Therefore, a RTM fault diagnosis model optimized for distributed deployment, coupled with its edge-cloud collaboration empowered model inference framework is proposed in this paper, where model components are strategically partitioned and offloaded jointly across cloud and edge rather than relying solely on cloud or local to facilitate reliability and faster response.\nThe main contributions of this paper can be summarized as:\n\u2022 A parallel-optimized RTM fault diagnosis model is developed with model integration technique. The model incorporates an enhanced three-stage segmentation scheme as prior knowledge and the outputs of multiple sub-classifiers are fused by a fuzzy-based ensemble mechanism to form the final classification result.\n\u2022 A cloud-edge collaborative framework leveraging pipeline parallelism, namely CEC-PA, is proposed to address the real-time and robustness challenges of distributed fault diagnosis. CEC-PA partitions the integrated model components into pipelines and intelligently schedules them across all worker nodes. Additionally, a downtime-tolerant mechanism is proposed to ensure system robustness.\n\u2022 Extensive experiments are conduced to evaluate the effectiveness of the proposed fault detection model and CEC-PA framework. Results showcase our ensemble-based fault diagnosis model produce accurate predictions across all fault types and CEC-PA outperform other approaches in terms of real-time performance and reliability."}, {"title": "II. RELATED WORK", "content": "Numerous solutions for unmanned intelligent RTM health monitoring have been proposed in the past two decades. Ou et al. [5] proposed a RTM fault diagnosis scheme based on Machine Learning (ML), where a modified Support Vector Machine (SVM) with Gaussian kernel is applied to classify the time-domain and frequency-domain features obtained through Linear Discriminant Analysis (LDA). Ji et al. [26] introduce an adaptive fault diagnosis model for both single and double-action RTMs that utilizes Dynamic Time Warping (DTW) to calculate similarities between input samples and their built-in reference templates. Wang et al. [6] leverage segmentalized Max-Relevance and Min-Redundancy (mRMR) techniques for stage-wise feature extraction. Additionally, a novel classifier named cost-sensitive Extreme Learning Machine (cf-ELM) is complemented in their study, which features bias compensation to enhance classification stability. Deep learning (DL) approaches are also widely adopted due to their strong generalization capabilities. By creating variants of Deep Auto Encoders (DAEs) and Gated Recurrent Units (GRUs), Zhang et al. [10] and Guo et al. [7] propose adaptive latent feature classification method for unsupervised and semi-supervised RTM fault diagnosis, respectively.\nOther than using common data inputs such as power spectral density and current sequence, Cao et al. provide a distinct perspective by focusing on alternative fault diagnosis methodologies leveraging acoustic data [8] and three-dimensional vibration signals [4]. Additionally, to address the Few-shot Fault Diagnosis (FSFD) problem where limited faulty samples are available, Li et al. [27] developed a reweighted regularized prototypical network combined with a novel balance-enforcing regularization (BER) mechanism to hedge against the between-class imbalance and improve classification accuracy."}, {"title": "B. Parallelization Techniques in Distributed AI", "content": "According to Mwase et al. [28], parallelism in Distributed AI can be carried out by breaking down either the data, the model, the stages of the process (i.e., pipeline), or a combination of these. \nData parallelization emerges as a highly effective strategy for accelerating DL on Graphics Processing Units (GPUs), offering versatility and ease of implementation. In this approach, the input batch of dataset is spilt into multiple micro-batches, each allocated to a distinct data-parallel worker. Pandey et al. [29] experimentally demonstrated that the implementation of data parallelization at small scales can achieve near-perfect scaling due to the combination of independent computations and low computational density. Foundation models such as GPT and SAM have demonstrated state-of-the-art performance on various tasks in Natural Language Processing (NLP) and Computer Vision (CV). As a result, such heavyweight models (GPT-3 typically with 175 billion parameters) are too large to fit on a single device and if so still take forever to train.\nTwo parallelization techniques have emerged to mitigate these issues: model parallelism and pipeline parallelism. In contrast to data parallelism, model parallelism (i.e., tensor parallelism) addresses storage limitations via model partitioning and minimizes communication overhead by avoiding complete parameter transfers during each update iteration [28]. Leveraging model parallelism techniques, Xu et al. [30] and Lai et al. [31] proposed SUMMA and DeCNN for the efficient and scalable training of large-scale DL models. However, Gomez et al. [32] point out that model parallelism places extremely high demands on low-latency and high-throughput interconnection between GPUs. Therefore, its usage is restricted by proprietary hardware, such as NVLink, and thus limits the potential for pipeline parallelism to be widely deployed on diverse computing platforms. After analyzing the communication overhead of different parallelization strategies, Oyama et al. [33] concluded that pipeline parallelism divides the layers of the model into stages that only shares activations between neighboring pipeline stages, resulting in even lower communication overhead compared with its model parallelism predecessor. In the context of affordable training of large DNNs, Thorpe et al. introduced Bamboo [34], a distributed system that introduces redundant computations into the training pipeline to provide resilience at a low cost, outperforming traditional checkpointing in training throughput and reducing costs. Additionally, Zhao et al. [35] and Kim et al. [36] demonstrate that pipeline parallelism can accelerate processing without any accuracy loss, as opposed to compression techniques like pruning and quantization."}, {"title": "III. PRELIMINARY", "content": "Turnout machines can be classified into three categories: electro-hydraulic, electro-mechanical, and all-electric [2]. In this paper, we will focus on the most commonly used electro-mechanical modules, which consist of major components including the electric motor, mechanical parts (gear box, friction clamp, locking rod, etc.), and control circuits. Based on its electrical characteristics, we denote the input voltage as U, the input current as I, the three-phase angle as \u03b8, motor's angular velocity as \u03a9, and efficiency as \u03b7. During normal operation, the correlation between power P of the motor and its output torque T can be expressed as\n$T = \\frac{P}{\\Omega} = \\frac{\\sqrt{3} \\eta U I \\cos \\theta}{\\Omega}$                                                                                                                                                                                                                                                           (1)\nAs the sampling interval of MMS (Microcomputer Monitoring System) is typically small (usually less than 100ms), \u03a9 can be approximated as constant over this duration. According to Equation (1), torque T is positively correlated with motor power P and current I. Any variations in resistive forces acting on the motor shaft during switching, such as control system state transitions, mechanical obstructions, or lubrication deficiencies, will manifest as fluctuations in the current waveform I. Therefore, real-time monitoring and analysis of the motor current can provide insights into the working status of the turnout module."}, {"title": "B. Current Pattern Analysis of Three-Stage Turnouts", "content": "According to [10], [5], the current waveform during turnout transitions (i.e., from normal to reverse position and vice versa) follows a characteristic three-stage profile that closely matches the module's operational procedure. As illustrated in Figure 1, these stages can be outlined as follows:\na) Starting Stage: Initially, all three phase currents are zero as the control circuit relay only energizes after a built-in time delay. Upon motor startup, a large current surge rising from 0 occurs due to efforts overcome rotor inertia and the unlocking resistance between the stock rails and closure rails. However, once the motor reaches its operating speed, the current will decline to a relatively constant level.\nb) Transition Stage: As the motor persists in rotating the drive shaft, it engages the rack mechanism, which facilitates the lateral movement of the switch rails until they are securely locked in place. Throughout this stage, the three-phase current remains steady without any sudden fluctuations or overcurrent conditions.\nc) Indication Stage: When RTM has reached its fully locked position, the control circuit relay de-energizes the contactor, disconnecting the motor terminals. This causes the current in one phase to rapidly decrease to zero. However, owing to the RTM's buffering effect, the other two current phases maintain a constant value of approximately 0.6 Amp before eventually dropping to zero."}, {"title": "IV. MODEL FORMULATION AND PROBLEM DEFINITION", "content": "High-speed trains require extensive safety precautions to prevent accidents due to track irregularities. This paper presents a track anomaly early-warning system consisting of high-speed trains, cloud center, RMUs, Base Stations (BSs), and turnout machines. A heterogeneous network paradigm is employed to establish interconnection between these components, as depicted in Figure 2.\nTurnout machines $M = {m_1, m_2,..., m_s}$ are all equipped with MMS current sensing modules to continuously sample operational data during each duty cycle. RMUS $R = {r_1, r_2,..., r_j}$ are strategically distributed along the tracks at regular intervals to collect data from adjacent turnout machines. In addition to aggregating sensory outputs into built-in storage, the RMUs possess moderate on-board computation abilities to serve the purpose of executing computation offloading instructions. Each $r_j$ is assigned to a dedicated BS, which is responsible for transmitting and receiving data within coverage range $g_j$. The cloud center C is capable of high-concurrency task execution while also responsible for task scheduling decisions. Short-range wireless device-to-device (D2D) communication is established between RTMs and BSs leveraging the full-duplex IEEE 802.11p protocol [37]. High-speed trains, denoted as $V = {V_1, V_2, ..., \\upsilon_k}$, are also equipped with onboard electronics to directly communicate with BSs in a D2D manner as they traverse along the tracks. Additionally, in the event of cloud center failures or defective backhaul connections, backup connections between adjacent RMUs can be established to form a self-organized mesh network for uninterrupted data transmission.\nThe proposed early-warning system employs a three-tier architecture where high-speed trains V, RMUs R, and the cloud center C function as end devices, edge nodes, and the cloud, respectively. Fault classification model inference is collaboratively executed across R and C. Upon collecting RTM data, R initiate resource scheduling requests to C. The cloud C subsequently optimizes pipeline partitioning and offloading strategies based on current node status and network congestion. Assigned tasks and parameters are then distributed to R for execution, with results subsequently aggregated in C. Following fault diagnosis model inference completion, detected anomalies trigger network-wide broadcasts. Throughout operations, V maintain continuous D2D communication with R in-range, receiving real-time diagnostic information and responding to fault warnings as necessary."}, {"title": "B. Distributed Task Execution Model", "content": "Traditional fault diagnosis systems [26], [10] typically adopt a centralized approach where data is processed on a single node. This study designs a parallel distributed model where the overall workflow is partitioned into discrete subtasks that can be executed concurrently across edge and cloud. Let $CT(t) = {ct_1, ct_2, ..., ct_n}$ denote the set of computing tasks generated at time slot t. Each task $ct_i$ can be further decomposed into a sequence of fine-grained subtasks ${\\Tau_1, \\Tau_2, ..., \\Tau_{N(ct_i)}}$, where $N(ct_i)$ represents the number of subtasks decomposed from $ct_i$. According to Section IV-A, the network consists of J edge node workers ${Wr_1, Wr_2,..., Wr_j }$ and one cloud worker $W_c$. Subtasks from the overall task pool CT(t) are scheduled adaptively to workers for processing, ensuring that each worker $W_i$ is assigned a subset of tasks $P_j C CT(t)$. After completing $P_j$, worker $W_i$ transmits intermediate outputs to the subsequent workers along the workflow, until reaching the High-speed Trains V. Assuming there is no data dependency between subtasks $\\Tau_2$ and $\\Tau_3$, these independent subtasks are therefore dynamically scheduled in parallel across multiple workers. In contrast, we assume the outputs of $\\Tau_2$ and $\\Tau_3$ must be combined to form the required input for $\\Tau_4$. This introduces an inter-subtask dependency scenario, whereby worker $W_{\\Tau_4}$ assigned with $\\Tau_4$ can only proceed once the predecessors $\\Tau_2$ and $\\Tau_3$ are completed.\nTo efficiently schedule and monitor distributed subtasks, we define the characteristics of subtask $\\Tau_n$ as a quadruple $\\Tau_n = (\\zeta, \\Psi, \\zeta, \\Phi)$, where $\\zeta$, $\\Psi$, $\\zeta$, and $\\Phi$ represent the estimated computational workload, minimal system requirement, predecessor tasks set, and current state quantity, respectively. The life cycle of a successful task involves several key stages, and the computational duration for each stage can be calculated as follows:\na) Queueing Stage: The waiting period from task submission to processing start. Due to the limited hardware capacity and context switching overhead, RMU workers $W_{ier}$ can only process a finite number of tasks concurrently. Queue $Q = {\\Tau_1, \\Tau_2,...}$ is used to store tasks that cannot be immediately processed. Workers are strategically selected from the set $A = {W_i | sys\\_res(r_j) > \\Psi}$ to maximize overall system performance. Subsequently, subtasks are added to the corresponding $W_i$'s queue, awaiting execution until concurrency limit w is no longer exceeded. The queueing time of $\\Tau_n$ scheduled for execution on worker $W_i$ can then be modeled as\n$t_{w\\_tqueue}(\\Tau) = \\begin{cases} 0, & \\text{concurr} < \\omega \\\n\\sum_{k=1}^{\\text{Elen}(Q_i)} \\text{duration}(T_k), & \\text{concurr} > \\omega \\end{cases}$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (2)\nwhere $duration(T_k)$ refers to the estimated processing time of the k-th task in the queue, and $Q_i$ represents the queue of worker $W_{ri}$. Due to the uncertainty in task completion time and to enhance the system robustness, tasks can be competitively queued across multiple workers. Let I denote the time required for decision model inference."}, {"title": "C. Parallel Context Exchange Model", "content": "Within these distributed workflows, computing tasks $ct$ are broken down into subtasks $\\Tau$. These subtasks are then distributed across multiple workers $W_{R\\cup C}$, with the intermediate results of subtask $\\Tau_n$ potentially serving as the input to the downstream subtasks D. Hence, there arises a necessity to exchange context among the distributed computational nodes. In this paper, the geographic position of each node $(.)$ is mathematically characterized using latitude $\\phi(.)$ and longitude $\\lambda(.)$ coordinates. The spatial distance between nodes a and b can be calculated using the Haversine formula:\n$dist_{a,b}(t) = R \\cdot hav^{-1} \\sqrt{hav( \\frac{d \\phi (a,b,t)}{2})} + cos( \\phi(a)) \\cdot cos( \\phi(b)) \\cdot hav(\\frac{d \\lambda (a,b,t)}{2})}$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         (8)\nwhere $R$ is the Earth's radius, and $hav(0)$ is the Haversine function defined as $sin^2(0/2)$. $d \\phi(a,b,t)$ and $d \\lambda(a,b,t)$ denote the changes in latitude and longitude between a and b at time t, respectively.\nAs $dist(a,b)$ increases, it becomes more challenging to maintain reliable data transmission. According to Shannon's theorem, the transmission rate can be computed based on the Signal-to-Noise Ratio (SNR):\n$t_{r(a,b)}(t) = B \\cdot log_2(1 + \\frac{min(P_a, P_r) \\cdot \\sigma \\cdot dist_{a,b(t)}}{N_o})$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (9)\nwhere B is the channel bandwidth, $P(.)$ denotes the transmission power, factor \u03c3 is the path loss exponent, and $N_o$ is the amplitude of the Gaussian background noise.\nCoverage range may vary according to BSs, represented as $g_i$ with i denoting a specific BS. Then, the collection of nodes capable of establishing communication with $r_i$ is denoted as $S = {x | dist(x,r_i) \\leq g_i}$. For unreachable $r_i \\notin S$, mesh networking $M = {a \\leftrightarrow n,...,m \\leftrightarrow b}$ provides an alternative means of connectivity through relaying. As this multi-hop relay solution for two distant nodes incurs higher latency, it's typically utilized as a backup degradation in case of direct link failures. The time consumption to transmit $T_n$'s context from node a to b through M can then be calculated as\n$\\Gamma_{ab}^{mesh}(n) = \\sum_{i=1}^{|M|} t_{r(\\text{a})}^{i\\_n}(t)$.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (10)\nData transfer between RMUs and the cloud leverages wired backhaul link on default, which provides dedicated bandwidth for long-distance data exchange. In this case, the time taken for context exchange transactions primarily depends on the Round-trip Time (RTT), which can be approximately modeled as\n$\\Gamma_{ab}^{backhaul}(T_n) \\approx RTT = 2 \\cdot \\frac{dist_{a,b}}{U_{tran}}$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (11)"}, {"title": "D. Problem Formulation", "content": "Turnout malfunctions can result in catastrophic consequences if not addressed promptly. The real-time detection of these malfunctions is crucial for early-warning of track anomalies, providing more reaction time to train operators and maintenance groups. Our objective is to obtain an optimal policy for task partitioning and offloading that jointly optimizes execution and data transfer to meet real-time constraints. Let $u_e$ and $u_\\Gamma$ be the weighting coefficients of execution time and transmission delay, where $u_e+u_\\Gamma = 1$ and $u_e, u_\\Gamma \\in [0, 1]$. The multi-objective optimization problem can be formulated as\n$\\min \\sum_{t=1}^{T} \\sum_{\\tau_i \\epsilon CT(t)} u_e \\cdot T_{EXEC(cti)} + u_\\Gamma \\cdot T_{TRANS(cti)}$\ns.t.\nC1: TEXECTTRANS <TIMEOUT,\nC2: sys_res(rj) > \\Psi, $ \\forall rj \\in R$ ,\nC3: dist(x,ri) \\leq gi, rj \\in R,\nC4: Tstart(\\Tau_n) \\geq Tend(D), 1 \\leq n \\leq N(cti).\n(13)\nWhere T represents the total run time of the system. Tstart($\\Tau_n$) and Tend($\\Tau_n$) denote the start and completion time of subtask $\\Tau_n$, respectively. Constraint C1 specifies the timeout threshold for individual tasks. Resource constraint C2 guarantees that tasks can only be assigned to workers that have sufficient system resources. Distance constraint C3 ensures that each BS can only communicate with devices within its coverage range. Task dependency constraint C4 specifies the precedence relationships between subtasks."}, {"title": "V. PARALLEL-OPTIMIZED TURNOUT FAULT DIAGNOSIS SCHEME", "content": "Inspired by [10], this paper incorporates multiple sub-models through an ensemble approach to enhance the parallelism of fault diagnosis process. As illustrated in Figure 3, the proposed model has a hierarchical modular structure comprising three main components: a) Segmentation module that partitions turnout operation current sequences into stages. b) Three parallelized fault classification models, each tailored to a particular modeling strategy. c) Late-fusion module to combine previous outputs and form the final result."}, {"title": "Exploiting Phase Segmentation as Prior Knowledge", "content": "A complete turnout transition cycle comprises three distinct stages: starting, transition, and indication. Utilizing the results of stage segmentation as prior knowledge allows downstream models to conduct sequential feature extraction with enhanced effectiveness. Consequently, there exist two segmentation points P\u2081 and P2, which divide the current sequence X = {$X_1,X_2,...,X_n$} into XStage1, XStage2, and X Stage3. Ou et al. [5] leverages second-order difference to identify P\u2081 and P2 in X, as it's particularly sensitive to these inflection points. However, the intense current fluctuations in faulty samples can easily exceed the generalization capabilities of traditional numerical-based algorithms in handling variations, thereby impacting segmentation accuracy. To address this, we employ a GRU (Gated Recurrent Unit) network to analyze three-phase current sequences (A, B, C channels) and assign confidence scores reflecting the likelihood of each point denoting a segmentation boundary. The final confidence score for each potential segmentation point can be computed as\n$Score(i) = \\frac{|xi - \\text{mean}(X_{Stage2})|}{\\mid xi - \\text{mean}(X_{Stage3})\\mid} * d_i \\text{Wi} + \\Upsilon [Score(i+1) + Score(i-1)] + \\Upsilon^2[Score(i+2) + Score(i-2)] + ... $                                                                                                                                                                                                                                                            (14)"}, {"title": "Three-Stage Feature Extraction and Fusion", "content": "In pursuit of a fault classification paradigm exhibiting robustness, parallelizability and interpretability, we adopt an ensemble approach that integrates predictions from distinctive sub-models. Specifically, three sub-classifiers based on a) time-domain feature engineering, b) morphological similarity, and c) deep feature extraction, are developed to address fault classification from different perspectives.\na) Multi-layer Perceptron (MLP): As a Neural Network (NN) model, MLP demonstrates remarkable fitting and generalization capabilities, making it well-suited for classification problems [39]. Time-domain features (Table II) are carefully selected to construct feature set Fstage_x for each segmented stage sequence. These sets are then normalized and combined to form the comprehensive stage-wise features set (Fstage_1, Fstage_2, Fstage_3), which serves as the input for the model. When fewer than two segmented points are recognized, all feature values corresponding to the missing stages will be set to -1. Additionally, when encountering divide-by-zero during feature extraction, the output will be set to 0 to prevent triggering an exception.\nb) Denoising Auto Encoder (DAE): DAE performs non-linear dimensionality reduction while extracting higher-level descriptors of waveform shape, showcasing wide applications in unsupervised time-series anomaly detection [40]. The morphological characteristics of current waveforms, such as subtle variations and local extrema distributions, are challenging to capture numerically. However, such features are proved useful for determining fault types. The DAE operates on a four-dimensional input: three channels allocated for phase current sequences, complemented by a binary segmentation mask channel that uses boolean values (0 and 1) to identify segmentation points. Subsequently, Mean Absolute Error (MAE) is employed to form the total reconstruction error set Lape = {LNormal, LHI,..., LF5}.\nEach element in Louie consists of losses from the three stages, represented as Ltype = (lstage1,1Stage2,1Stage3). Larger loss in-dicates more significant morphological differences, suggesting lower confidence that the sample belongs to that category. Contributions from each stage are aggregated using weight assignments to compute the anomaly score Sae. Subsequently, a numerical inversion and scaling of Sae yields the fault type classification confidence Ck (k \u2208 {Normal, H1, ..., F5}), calculated as\n$C_k = \\frac{e^{-m \\lambda S_k}}{(\\sum_{i=0}^{n} e^{-m \\lambda S_i})^2}$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         (15)\nwhere m is the scaling coefficient and Softmax() normalizes the output to ensure a valid probability distribution across fault types.\nc) Temporal Convolutional Network (TCN): As a one-dimensional Fully Convolutional Network (FCN) designed specifically for sequential data, TCN [41] is regarded as the successor to Recurrent Neural Networks (RNNs). The model's architecture features a four-channel input, consistent with the DAE sub-classifier, and incorporates a linear layer for direct classification result output.\nFuzzy Logic (FL) is a computational paradigm where a value can belong to multiple fuzzy sets, each associated with a membership degree [42]. In this paper, we leverages FL for combining results from multiple sub-classifiers at the decision level. Specifically, we perform fuzzy modeling of outputs from individual classifiers to account for ambiguity inherently associated with classification problems. Membership functions gauge membership to fuzzy set {Negative, Positive} defined over the domain of all fault categories. For a given classifier, Algorithm 1 outlines the process of determining membership functions for each domain with statistical experimental method. The three-dimensional output array W has fuzzy domains on its first dimension and fuzzy sets on the second, with its elements on the third dimension mapping to the corresponding membership function \u03bc(x)."}, {"title": "VI. CEC-PA: A CLOUD-EDGE COLLABORATIVE PIPELINE PARALLELISM FRAMEWORK FOR DISTRIBUTED FAULT DIAGNOSIS", "content": "Monolithic implementations of hybrid fault diagnosis models, where prior knowledge extraction, sub-classifiers, and late-fusion module are executed on a single centralized node, typically exhibit inefficient resource allocation and compromised system responsiveness. To address these limitations, the Cloud-Edge collaborative parallelism-aware scheduling framework, namely CEC-PA, is proposed to intelligently schedule tasks across worker nodes in a parallelized manner. Specifically, CEC-PA operates in conjunction with the previous hierarchical diagnosis model, which is now partitioned at a fine-grained level to fully exploit the distributed computational capabilities across cloud and edge, as depicted in Figure 4.\nIn this section, a partitioning strategy is first presented to divide the overall fault diagnosis model into pipelines. Then, the pipeline offloading problem is formalized as a Markov Decision Process (MDP)"}, {"title": "Parallel Task Partitioning Across Pipelines", "content": "As discussed in section IV-B, parallel tasks may have dependencies on the outputs of prior tasks. Allowing minimal units to be directly scheduled may result in task accumulation and blocking across pipelines. Therefore, assigning coupled tasks to the same pipeline is the key to reduce overheads. We choose to partition at the pipeline-level, rather than neuron-level by the fact that model inference involves both computationally intensive operations and substantial memory access patterns. In contrast, neuron-level parallelism approaches [30] rely heavily on low latency and high bandwidth network environments. The black-box nature at the model component level provides good isolation by exposing only the inputs and outputs. This characteristic aligns seamlessly with our distributed pipeline parallelism approach, where our aim is to minimize context exchange and data throughput for computation tasks scheduled across the network."}, {"title": "B. Formulation of Markov Decision Process", "content": "Once the partitioning of model components has been determined", "43": [44], "comprises": "na) State Space: The state space \u039e encapsulates key observations about the environment to form the foundation for agents' decision-making. Its design jointly considers properties of the distributed pipelines and real-time status of the worker nodes to comprehensively reflect the overall environment. Based on its pending subtasks", "includes": 1, "Factor": "Scaling factor that exponentially escalates $Pj$'s priority based on its waiting time", "Requirement": "The fundamental system resources required for execution", "Encoding": "Obtained by transforming dependencies of its predecessors into a high-dimensional vector using a Graph Neural Network (GNN) encoder. For each worker node Wi", "as": 1, "\u03b3": "A state quantity indicating the communication capability as either wired (\u03b3 = 0) or wireless (\u03b3 > 0). In cases of wireless communication", "37": "within the connection. 2) Workload: The level of computational burden quantified as UP TIME, where TCOMP"}]}