{"title": "Proactive Gradient Conflict Mitigation in Multi-Task Learning: A Sparse Training Perspective", "authors": ["Zhi Zhang", "Jiayi Shen", "Congfeng Cao", "Gaole Dai", "Shiji Zhou", "Qizhe Zhang", "Shanghang Zhang", "Ekaterina Shutova"], "abstract": "Advancing towards generalist agents necessitates the concurrent processing of multiple tasks using a unified model, thereby underscoring the growing significance of simultaneous model training on multiple downstream tasks. A common issue in multi-task learning is the occurrence of gradient conflict, which leads to potential competition among different tasks during joint training. This competition often results in improvements in one task at the expense of deterioration in another. Although several optimization methods have been developed to address this issue by manipulating task gradients for better task balancing, they cannot decrease the incidence of gradient conflict. In this paper, we systematically investigate the occurrence of gradient conflict across different methods and propose a strategy to reduce such conflicts through sparse training (ST), wherein only a portion of the model's parameters are updated during training while keeping the rest unchanged. Our extensive experiments demonstrate that ST effectively mitigates conflicting gradients and leads to superior performance. Furthermore, ST can be easily integrated with gradient manipulation techniques, thus enhancing their effectiveness.", "sections": [{"title": "1. Introduction", "content": "Attaining the status of a generalist agent necessitates addressing multiple tasks within a unified architecture, thereby emphasizing the significance of multi-task learning (MTL) [37], which involves concurrently acquiring proficiency in multiple tasks and striving for superior overall performance compared to learning these tasks separately.\nThe primary concern for MTL lies in the phenomenon of task competition when the model is jointly trained by optimizing the average loss across all tasks. As a result, a subset of tasks demonstrates superior performance while others remain sub-optimized compared to their individual learning counterparts. One of the reasons behind it, from an optimization perspective, is gradient conflict (GC) [35], wherein the direction and magnitude of gradients between tasks differ significantly. This can result in the average gradient biasing towards optimizing one task while providing relatively smaller and sometimes even negative optimization for other tasks when updating the network [18, 35].\nNumerous works have employed the gradient manipulation method to directly or indirectly adjust the gradients of tasks to mitigate the issue of gradient conflict in tasks. The former involves direct alteration of task gradients through manually designed criteria when conflicts arise [4, 17, 35], while the latter modifies task gradients by adjusting weights of loss for each task [18, 19, 26, 28]. Although these methods effectively modify the gradients conflicting with each other, they do not decrease the occurrence of conflicting gradients during training [29].\nA simple approach to mitigate the occurrence of conflicting gradients is to convert those layers in which gradient conflict frequently arises into task-specific layers, thereby reducing the likelihood of gradient conflicts within the remaining shared layers [29]. However, this strategy introduces additional modules and disrupts the internal structure of the original model, resulting in increased computational costs. Furthermore, identifying frequently conflicting layers adds extra computational costs. This becomes prohibitively expensive as the model size continues to expand, and thus prompting our fundamental inquiry:\n(Q) Is there a universally applicable approach to proactively mitigate the occurrence of gradient conflicts as well as preserve architectural integrity for MTL?"}, {"title": "2. Related work", "content": "Multi-task optimization for MTL The recent works [4, 17\u201319, 26, 28, 35] have achieved impressive results in addressing task imbalance issues in MTL by directly or indirectly modifying conflicting task gradients. Specifically, some works [4, 17, 35] propose to form a new update gradient at each training step by directly altering gradients based on certain criteria. Other works [14, 18, 19, 26, 28] learn dynamic loss scale to balance different tasks during training, and thus indirectly altering the gradient of tasks. However, these methods only address GC when it occurs and do not proactively prevent it. In this paper, we sparsely train an MTL model, effectively reducing the incidence of GC.\nTraining with subset of parameters Several methods have already been proposed in single-task learning. Some of them select a subset of parameters based on a certain pre-defined rule, such as gradient [11, 40] and magnitude of parameters [16]. In addition to selecting parameters by hand design, the works in [25, 27, 34] automatically select the subset of parameters through optimization. Although sparse training has been extensively investigated in single-task learning, its application in MTL remains relatively unexplored. Sun et al. [30] and Calandriello et al. [2] learn to share information between tasks using a sparse model instead of sparse training. Differently, we research the gradient conflict via the sparse training perspective."}, {"title": "3. Approach", "content": ""}, {"title": "3.1. Background", "content": "Multi-task learning (MTL) aims to learn multiple tasks simultaneously within a single model. Formally, given {$\\mathcal{T}_t$}$_{t=1}^{T}$ tasks ($T \\geq 2$) and a model $\\Theta$ with parameters $\\Theta$ = ($\\Theta_{sha}$, $\\Theta_{sep}$), where $\\Theta_{sha}$ and $\\Theta_{sep}$ are shared parameter with all tasks and task-specific parameters $\\Theta_{sep}$ respectively, the commonly used optimization method for MTL (referred to as Joint Train) is based on computing the average loss across all tasks with equal weights:\n$\\Theta^* = \\arg \\min_\\Theta \\mathcal{L}(\\Theta) = \\frac{1}{T}\\sum_{t=1}^{T} \\mathcal{L}_t(\\Theta_{sha}, \\Theta_{sep}),$"}, {"title": "Gradient conflict (GC)", "content": "However, optimizing all tasks by aggregating their losses indiscriminately (Eq. (2)) may lead to task competition, wherein certain tasks demonstrate improvement while others exhibit a decline compared to training them separately. From an optimization perspective, one of the reasons stems from conflicts in gradients. Formally, the update of task $T_i$ may potentially exert a detrimental impact on another task $T_j$, namely:\n$\\Delta \\mathcal{L}_j = \\mathcal{L}_j(\\Theta_{sha}, \\Theta_{sep}) - \\mathcal{L}_j(\\Theta'_{sha}, \\Theta_{sep}),$\n$\\Theta'_{sha} = \\Theta_{sha} - \\alpha g_i$\nwhere $g_i = \\nabla_{\\Theta_{sha}} \\mathcal{L}_i(\\Theta_{sha}, \\Theta_{dep})$ is the gradient of loss on task $T_i$ with respect to $\\Theta_{sha}$ and $\\alpha$ is the learning rate. After the first-order Taylor approximation, Eq. (3) can be expressed as $-\\alpha g_i \\cdot g_j + o(\\alpha)$. Gradient conflict arises when $g_i \\cdot g_j < 0$, leading to $\\Delta \\mathcal{L}_j > 0$, indicating that task $T_i$ has a detrimental impact on task $T_j$. Following [35], we provide the definition of gradient conflict:"}, {"title": "Definition 1 (Gradient Conflict)", "content": "If $cos\\phi_{ij} < 0$, where $\\phi_{ij}$ is the angle between gradients of two tasks $g_i$ and $g_j$ ($i \\ne j$), then $g_i$ and $g_j$ are deemed to exhibit gradient conflict."}, {"title": "Gradient manipulation", "content": "To alleviate the issue of gradient conflict, gradient manipulation methods adjust conflicting gradients based on specific criteria and utilize these modified gradients for model updating. Instead of updating the model on the average gradient in Eq. (1) and Eq. (2):\n$\\nabla_{\\Theta_{sha}} \\mathcal{L}(\\Theta) = \\frac{1}{T} \\sum_{t=1}^T \\nabla_{\\Theta_{sha}} \\mathcal{L}_t(\\Theta_{sha}, \\Theta_{sep}),$\nthe gradients of all tasks in gradient manipulation methods are modified as follows:\n$\\nabla_{\\Theta_{sha}} \\mathcal{L}_{gm}(\\Theta) = \\frac{1}{T} \\sum_{t=1}^T w_t \\nabla_{\\Theta_{sha}} \\mathcal{L}_t(\\Theta_{sha}, \\Theta_{sep}),$\n$w_t = f(\\nabla_{\\Theta_{sha}} \\mathcal{L}_1, ..., \\nabla_{\\Theta_{sha}} \\mathcal{L}_T)$\nwhere $w_t$ can be either pre-defined or dynamically computed for tasks via $f$ and thus achieve the aim of adjusting the task gradient [4, 17\u201319, 26, 28, 35]. However, the results of our experiment suggest that these methods can only modify gradients when conflicts occur, rather than proactively reducing the occurrence of GC during training, compared with Joint Train, as shown in Fig. 1."}, {"title": "3.2. Sparse training for multi-task learning", "content": "In this study, we investigate the gradient conflict commonly observed in multi-task learning from a novel perspective: sparse training, which selectively trains only a subset of the model parameters as opposed to full parameter training. This perspective is based on the intuition that by converting a high-dimensional space optimization problem into a lower-dimensional one, the complexity of optimization can be effectively reduced. Additionally, by limiting the impact of gradient updates to only a subset of parameters for each task instead of all parameters, potential interference between tasks can be mitigated.\nSparse training (ST) entails the initial parameter selection from the original model, and then updating only these parameters while keeping other parameters fixed during model training. To clarify potential misunderstandings regarding ST\u2014often confused with sparse networks, where parameters are abandoned for model compression\u2014we provide the following definition to ensure consistency and ease of understanding throughout this paper."}, {"title": "Definition 2 (Sparse Training)", "content": "Given a model $\\Theta$ and a binary mask matrix $M$ indicating whether parameters in $\\Theta$ are selected, where $M \\in \\mathbb{R}^{|\\Theta| \\times |\\Theta|}$, $M_{ii} \\in \\{0, 1\\}$ and $M_{ij} = 0 (\\forall i \\ne j)$, the model is updated by $\\Theta = \\Theta - \\alpha M\\nabla_{\\Theta} \\mathcal{L}(\\Theta)$. We define this training strategy as sparse training.\nTypically, the model architecture in multi-task learning includes a shared encoder as a feature extractor with task-specific decoders for multiple tasks. Therefore, sparse training is used in the encoder, and full parameters training for the decoders. We detail how the mask is computed in section Sec. 3.4. We now apply sparse training for multi-task learning (Joint Train). The visualization of the gradient change can be viewed in Fig. 2 and the update with the reformulated gradient from Eq. (5) is as follows\n$\\nabla_{\\Theta_{sha}} = \\nabla_{\\Theta_{sha}} - M\\nabla_{\\Theta_{sha}} \\mathcal{L}(\\Theta) = \\nabla_{\\Theta_{sha}} - M\\frac{1}{T} \\sum_{t=1}^T \\nabla_{\\Theta_{sha}} \\mathcal{L}_t(\\Theta_{sha}, \\Theta_{sep}).$"}, {"title": "Combination with gradient manipulation methods", "content": "The application of sparse training can be seamlessly and effectively extended to improve various gradient manipulation methods in MTL. The update with the reformulated gradient from Eq. (6) is as follows\n$\\nabla_{\\Theta_{sha}} = \\nabla_{\\Theta_{sha}} - \\nabla_{\\Theta_{sha}} \\mathcal{L}_{gm}(\\Theta) = \\nabla_{\\Theta_{sha}} - M\\frac{1}{T}\\sum_{t=1}^T w_t\\nabla_{\\Theta_{sha}} \\mathcal{L}_t(\\Theta_{sha}, \\Theta_{sep}).$"}, {"title": "3.3. Theoretical analysis for sparse training", "content": "After introducing sparse training into MTL, the optimization objective in Eq. (1) can be formed:\n$\\Theta^* = \\arg \\min_\\Theta \\mathcal{L}(\\Theta), s.t. ||(I - M)(\\Theta_{sha} - \\Theta^0_{sha}) ||^2 = 0,$\nwhere $\\Theta^0_{sha}$ is the initialized original model for $\\Theta$ and $I$ is identity matrix. According to Lagrangian duality, Eq. (10) can be reformulated as:\n$\\mathcal{L} = \\min_\\Theta \\max_\\lambda \\mathcal{L}(\\Theta) + \\lambda ||(I - M)(\\Theta_{sha} - \\Theta^0_{sha}) ||^2.$\nThis can be transformed to optimize the upper bound $\\mathcal{L}$ of regularized problem:\n$\\mathcal{L}_r = \\min_\\Theta \\mathcal{L}(\\Theta) + ||(I - M)(\\Theta_{sha} - \\Theta^0_{sha}) ||^2 \\le \\mathcal{L}.$\nPlease see the supplemental material for proof. Fu et al. [11] demonstrates that Eq. (12) has better stability and smaller generalization bound than only optimizing Eq. (1), resulting in better performance."}, {"title": "3.4. Parameter selection per neuron (PSN)", "content": "Several promising sparse training methods exist for single-task learning, but they are either time-consuming, requiring mask updates at each iteration [25, 27, 34], or memory-intensive due to gradient calculations for all parameters [11, 40]. In MTL, where multiple tasks are trained simultaneously, time efficiency is crucial. Thus, we adopt a one-time selection method, choosing parameters before training and keeping the mask fixed throughout. We consider the following two aspects for selection, magnitude of the parameter and involvement of all neurons in the network.\nThe magnitude of parameters Several studies have focused on model compression through the elimination of parameters with lower magnitudes [10, 13]. This highlights the significance of parameters with larger magnitudes in neural networks, which is consistent with our experimental findings (See Fig. 5c). The intuition behind this phenomenon lies in the fact that parameters with larger magnitudes exert a greater influence on altering neuron activation states through the activation function, wherein a neuron becomes active once the input surpasses a predefined threshold. Therefore, we exclusively select parameters with the highest magnitude for training multiple tasks.\nThe involvement of all neurons A simple idea is to select a certain proportion of parameters with the highest magnitude from the neural network (NN), but this may prevent some neurons from being engaged during training and hinder effective model training due to the dependence of the NN state on neuron activation. Motivated by studies highlighting distinct roles for different components in NN [9, 32, 40], we posit that engaging all neurons is crucial for effective model training. The rationale is that each neuron within the network possesses the inherent capability to finely adjust its activation state, thereby effectively adapting the overall NN state to the tasks, especially for learning multiple tasks simultaneously. Our experiments further substantiate this assertion, as shown in Fig. 5c."}, {"title": "PSN", "content": "By integrating the two aspects, we select the top-K connections (weight/parameters) with the highest magnitude among all input connections for each neuron in the network (Please see Fig. 3 for top-1 example). This approach facilitates the training process for fitting tasks by ensuring that every neuron possesses activation potential, while parameters with higher magnitudes facilitate easier activation of neurons. In this paper, sparse training refers to using this method to select parameters and training the selected parameter, unless otherwise specified."}, {"title": "4. Experiments", "content": "Our experiments are conducted on comprehensive MTL benchmarks to evaluate the effectiveness of sparse training. First, we investigate if sparse training reduces gradient conflict. Subsequently, we examine its impact on performance across various MTL setups. The more details of the experiment are provided in Appendix D."}, {"title": "4.1. EXPERIMENTAL SETUP", "content": "Dateset Our MTL datasets are categorized into three groups: i) Dense prediction tasks: NYUv2 [6]: An indoor scene understanding dataset containing 1449 RGBD images with per-pixel labels across 13 classes, including semantic segmentation, depth estimation, and surface normal prediction. CityScapes [5]: 5000 street-view RGBD images with per-pixel annotations for 7-class semantic segmentation and depth estimation. ii) Multiple binary-classification tasks: CelebA [21]: 200,000 facial images of 10,000 celebrities, each with 40 binary attributes for facial features. We use the first 10 attributes for 10 binary classification tasks due to limited computation. iii) Multiple multi-class classification tasks: VTAB [36]: Containing 24 image understanding tasks with 1000 training examples per task. We use four tasks from it to create two multi-task benchmarks: Clevr: Simple 3D shapes with counting and depth prediction tasks. SmallNORB: Artificial objects with object azimuth and camera elevation prediction tasks.\nBaseline We evaluate our approach using various baselines including i) single-task learning (STL): Each task is trained independently; ii) Joint Train: Training all tasks with average task loss; and 6 gradient manipulation methods including 3 direct and 3 indirect modification techniques. The former includes: iii) PCGrad: Projecting each task gradient onto the normal plane of other tasks [35]; iv) CAGrad: Enhancing the optimization of average loss by explicitly regulating the minimum decrease across tasks [17]; and v) GradDrop: Stochastically dropping specific dimensions of the gradients based on their level of conflict. The latter includes vi) MGDA: Identifying the same descent direction for each task [28]; vii) IMTL-G: Determining the update direction by ensuring equal projections on gradients"}, {"title": "4.2. Incidence of gradient conflict", "content": "We train a MTL model using the Joint Train and 6 state-of-the-art gradient manipulation techniques including PCGrad, CAGrad, GradDrop, MGDA, IMTL-G and NashMTL and then introduce our sparse training strategy to these methods. Throughout the training process, we record instances of GC between any two tasks among all tasks for each training iteration and then calculate the average incidence of GC both over all epochs and the last 50% epochs. The observations of the SAM model on the NYU-v2 dataset are provided below. Similar results on other datasets and models are shown in Appendix F.3, Appendix F.5, Appendix F.7 and Appendix F.6.\nGradient manipulation methods cannot effectively reduce the incidence of gradient conflict The gradient manipulation methods [4, 17\u201319, 26, 28, 35] aim to modify conflicting gradients that are prevalent during the joint training of MTL. As shown in Tab. 1, the average incidence of GC using Joint train is 31.89% across all training epochs and 35.85% over the last 50% epochs. The incidence of GC cannot be effectively reduced by any gradient magnitude methods compared with the Joint train, as shown in Fig. 1 and Tab. 1. The reason is that these methods can only"}, {"title": "4.3. Performance on diverse benchmarks", "content": "It is natural to investigate whether reducing gradient conflict during training through sparsity can enhance performance on common benchmarks. In this section, we present diverse benchmarks to demonstrate the effectiveness of ST.\nSparse training improves the performance for all state-of-the-art methods The performance of Joint Train and all gradient manipulation methods is consistently improved by sparse training, as demonstrated in Tab. 2 for NYUv2 benchmarks. Specifically, sparse training not only enhances overall task performance but also improves individual task performance for the majority of methods. For example, in Tab. 2, Joint Train demonstrates improvements across all individual tasks through sparse training. Similarly, as shown in Tab. 3, all methods exhibit notable improvements by sparse training on CelebA, Clevr, SmallNORB and CityScapes benchmarks.\nEffectiveness on both pre-trained and randomly initialized models Our study primarily focuses on the sparse training for large pre-trained models, because leveraging prior knowledge from these models can be beneficial for MTL and our experimental results demonstrate that larger models exhibit a more severe gradient conflict, as shown in Fig. 5a. However, in order to ensure a fair comparison with related works that manipulate gradients in small and randomly initialized models, we also conduct experiments under the same setting as theirs to further demonstrate the effectiveness of sparse training. As shown in Tab. 3, we observe that even for the small randomly initialized models, the performance of joint training and all gradient manipulation methods is improved by sparse training. Please see Tab. 7 and Tab. 12 for the detailed results in the Appendix.\nGeneralization on different architectures and MTL tasks To evaluate the generalization across diverse architectures and MTL tasks, we conducted experiments on both CNN-based models and transformer-based models with varying visual MTL capabilities. Specifically, our MTL tasks encompassed visual classification (CelebA, Clevr and"}, {"title": "4.4. Ablation study", "content": "The larger the model, the more severe gradient conflicts. In this paper, we focus more on investigating the gradient conflict in the pre-trained large models as larger models demonstrated a more severe phenomenon of gradient conflict. This can be observed in Fig. 5a, where Swin/Tiny demonstrates significantly less gradient conflict compared to Swin/Base and Swin/Large. It is worth noting that although larger models tend to experience more severe gradient conflicts, this does not necessarily lead to inferior performance compared to smaller models with milder gradient conflicts. This discrepancy can be attributed to differences in model capacity and the prior knowledge embedded through pre-training. Nevertheless, this observation underscores the importance of exploring methods to mitigate gradient conflicts in larger models. Within the same model architecture and size, reducing gradient conflicts has been shown to improve performance, as evidenced by works such as [17, 35]. Addressing severe gradient conflicts in larger models may thus unlock their full potential, enabling better utilization of their capacity and capabilities.\nEffortless search for the number of trainable parameters. We explore the effect of trainable parameter numbers for ST. The results in Fig. 5b show that the pre-trained model (SAM) and the randomly initialized model (MTAN) have different optimal trainable parameter numbers. MTAN requires ~60% of the parameters, while SAM needs only ~30%, leveraging information from the pre-trained model. In our paper, most of the experiments use these proportions for ST and achieve better results (please see Tab. 4 in Ap-"}, {"title": "5. Conclusion", "content": "In this paper, the occurrence of gradient conflict in multi-task learning is extensively investigated from a novel perspective: sparse training. Extensive experiments demonstrate that sparse training transferring high-dimensional space into low-dimensional space effectively reduces the incidence of gradient conflict during training while preserving the integrity of the original model. Furthermore, combining sparse training with other gradient manipulation methods significantly improves performance for multi-task learning."}]}