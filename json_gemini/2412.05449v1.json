{"title": "Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications", "authors": ["Raphael Shu", "Nilaksh Das", "Michelle Yuan", "Monica Sunkara", "Yi Zhang"], "abstract": "AI agents powered by large language models (LLMs) have shown strong capabili- ties in problem solving. Through combining many intelligent agents, multi-agent collaboration has emerged as a promising approach to tackle complex, multi-faceted problems that exceed the capabilities of single AI agents. However, designing the collaboration protocols and evaluating the effectiveness of these systems remains a significant challenge, especially for enterprise applications. This report addresses these challenges by presenting a comprehensive evaluation of coordination and routing capabilities in a novel multi-agent collaboration framework. We evaluate two key operational modes: (1) a coordination mode enabling complex task com- pletion through parallel communication and payload referencing, and (2) a routing mode for efficient message forwarding between agents. We benchmark on a set of handcrafted scenarios from three enterprise domains, which are publicly released with the report. For coordination capabilities, we demonstrate the effectiveness of inter-agent communication and payload referencing mechanisms, achieving end-to- end goal success rates of 90%. Our analysis yields several key findings: multi-agent collaboration enhances goal success rates by up to 70% compared to single-agent approaches in our benchmarks; payload referencing improves performance on code-intensive tasks by 23%; latency can be substantially reduced with a routing mechanism that selectively bypasses agent orchestration. These findings offer valuable guidance for enterprise deployments of multi-agent systems and advance the development of scalable, efficient multi-agent collaboration frameworks.", "sections": [{"title": "Introduction", "content": "The rapid advancement of AI agents driven by large language models (LLMs) [3] has opened new frontiers towards solving complex problems. Based on the strong reasoning and tool-use capabilities, an agent can plan and execute multiple steps for actions until the goal of problem solving is reached [23]. However, as the complexity of real-world challenges continues to grow, there is an increasing need for scaling up the agent-based systems by coordinating with multiple agents with diverse specializations [17].\nTowards tackling multi-faceted real-world problems, multi-agent system (MAS) research emerged in the mid-1980s to early-1990s as a critical sub-field of artificial intelligence focused on developing computational systems composed of multiple interacting intelligent agents [18]. Researchers sought to create frameworks where autonomous entities could communicate, coordinate, and solve problems collectively [20]. With the rise of LLM-based AI agents in recent years, the key challenges in MAS research regained focal attention in the new Generative AI (GenAI) era [8]. While earlier MAS work drew inspirations heavily from fields like distributed computing and game theory, new LLM-based GenAI agent research looks further into inter-disciplinary influence from psychology and social science as the AI agents start to demonstrate human-like intelligence as well as social behavior [16]."}, {"title": "Related Work", "content": "Multi-agent GenAI Systems An agent is defined as \"an entity which is placed in an environ- ment and senses different parameters that are used to make a decision based on the goal of the entity.\" [10]. The motivation for multi-agent systems is to have agents collaborate on a complex task that could not have been accomplished by a single agent. With the rise of LLMs, researchers have proposed leveraging the reasoning and planning capabilities of these models to develop more sophisticated multi-agent systems. Examples of such multi-agent LLM systems include MetaGPT [9] and CAMEL [12]. MetaGPT is one of the first multi-agent LLM projects that tries to mimic a software company. Developers can provide a standard operating procedure and MetaGPT tries to assign roles to various agents. CAMEL, or Communicative Agent Framework, promotes independent collaboration between LLM agents. Its key innovation is the use of \"inception prompting,\" a method that steers conversational agents to complete tasks. Beyond its practical applications, CAMEL also functions as a research platform. It enables the creation and analysis of conversational data, offering valuable insights into the behavior and interactions of communicative agents. Across these works, much of the emphasis is on customization and coordination, which was much less prominent in traditional multi-agent systems [18, 20].\nMulti-agent Frameworks and Platforms CrewAI [5] is designed to enhance task execution by organizing agents into specialized roles, similar to team members in a crew. This approach emphasizes task decomposition, where a complex problem is broken down into smaller, manageable subtasks. Each agent is assigned a specific role based on its expertise, allowing for efficient problem- solving. AutoGen [22] represents a significant advancement in enabling multi-agent systems to engage in sophisticated conversations. This framework allows agents to communicate and collaborate by sharing information and refining their outputs through iterative interactions. By simulating human-like dialogues, AutoGen enables agents to negotiate, plan, and execute tasks collaboratively.\nLangGraph [11] introduces an innovative framework for organizing agent interactions using directed acyclic graphs (DAGs). This structure allows for clear visualization and management of dependencies"}, {"title": "Modeling", "content": "Multi-agent collaboration enables developers to combine specialized agents to solve complex prob- lems. Each agent can be independently developed, optimized, and configured to leverage its unique strengths. Compared to single-agent workflows, multi-agent collaboration integrates the complemen- tary capabilities and expertise of agents with different specializations, making it highly effective for addressing complex tasks. Developers can achieve amplified capability, flexibility, and scalability by deploying a team of agents. Moreover, multi-agent solutions can have higher robustness and fault tolerance by using redundant agents in the team. For complex tasks, multi-agent solutions can improve efficiency by distributing the tasks to multiple agents and parallelizing the execution.\nFrom a developer experience perspective, the development process is simplified by dividing func- tionalities among multiple agents. Developers can potentially reuse and compose existing agents for different multi-agent solutions. In some cases, cost-effective solutions can be built by utilizing low-cost orchestrating LLMs for specific agents. In this section, we review the primary features of MAC, which include inter-agent communication, payload referencing, and dynamic agent routing."}, {"title": "Inter-Agent Communication", "content": "We model the inter-agent communication capability as a specialized tool that can be leveraged by the supervisor agent. This approach allows us to seamlessly extend the agents' communication abilities by integrating it with the existing function calling capability. The key aspects of this approach are:\n1. Unified Communication Interface: The user is treated as another agent in the system, allowing for a consistent communication interface across all interactions - whether it's between the user and supervisor agent, or between the supervisor agent and specialist agents.\n2. Parallel Communication: The supervisor agent can engage in parallel communication with multiple specialist agents simultaneously, enabling more efficient task completion through concurrent information exchange (Figure 2).\n3. Leveraging Existing Function Calling Capability: By modeling communication as a tool, the supervisor agent can utilize the same underlying mechanisms for function calling to"}, {"title": "Payload Referencing", "content": "Payload referencing is a specialized mechanism designed to handle the exchange of large content blocks, particularly code snippets (Figure 3). This mechanism aims to reduce the latency of the super- visor agent by allowing direct injection of text extracted from past multi-party communication into the message content. This is an important optimization for inter-agent communication, as the supervisor agent often needs to provide relevant context from previous interactions when communicating with specialist agents.\nFor example, let's say the supervisor agent (Agent A) needs to ask a specialist agent (Agent B) to perform a specific task based on the output of another specialist agent (Agent C). Instead of having Agent A regenerate the full context and details of the message from Agent C, the payload referencing mechanism allows Agent A to simply reference the relevant text from its past interactions. This can significantly reduce the number of output tokens required in the message to Agent B, leading to faster communication and reduced latency.\nWhen a specialist agent generates a message containing structured content (e.g., code blocks), the system automatically detects these sections. For each incoming message to the supervisor agent, the detected content blocks, referred to as payloads, are assigned unique identifiers and wrapped with special tags that include these identifiers before being sent to the supervisor agent. Instead of repeatedly regenerating large static payloads in its outgoing messages to other specialist agents, the supervisor agent is instructed to reference previously detected payloads using the assigned identifiers. This allows the supervisor agent to use a simplified reference tag when sending messages. For every outgoing message from the supervisor agent, the system detects these reference tags and replaces"}, {"title": "Dynamic Agent Routing", "content": "Given a complex problem, a supervisor agent often needs to communicate with multiple specialist agents across several rounds to complete the task. However, when the incoming request is simple and relevant to only a single specialized agent, triggering the full coordination capability of the supervisor agent introduces unnecessary overhead, slowing down the collaboration. In such cases, the supervisor"}, {"title": "End-to-end Automatic Evaluation", "content": "Recent literature have noted the challenges associated with LLM agents benchmarking, which are often due to the dynamic and complex nature of the problem. The definition of success is often unclear as it can refer to either from the user perspective or from the environment/system, as users may not fully know what happens \u201cbehind-the-scenes\". Moreover, benchmarks may not consider that user feedback can help agents achieve their goals. If the user is not properly simulated, then the evaluation undermines the agent's capability to orchestrate tasks with a human in the loop.\nPrior single-agent benchmarking is more static where user inputs and follow-up responses are pre- defined before evaluation. The gold-truth actions would be collected along with the conversations, which would then be compared to the actions generated by the agent. This static setup already has some issues because it assumes that the user goals can only be fulfilled through executing a certain set of actions. In reality, there may be multiple trajectories that enable the agent to fulfill user requests. If those trajectories are not captured in the gold-truth, then the agent is incorrectly penalized.\nTo generalize evaluation metrics for agents, we formally define the success of agent for a given user u and scenario s as X, a Bernoulli random variable that represents success or failure for the user-agent session. A scenario is defined as the setting for a session which includes user goals and task domain. Since user profiles and scenarios vary, we are interested in the expected value of success for any user-agent session:\n$E_{U,S} [X] = P_{success}$", "equations": ["E_{U,S} [X] = P_{success}"]}, {"title": "Benchmarking Data", "content": "There are three artifacts that need to be collected for each scenario. First, the scenario description itself must list the user goals and any background information. The information in the scenario description is critical because the user simulator will be grounded on the description. Then, the second artifact is the input problem, which is the first turn of the conversation from the user. The input problem will begin the benchmarking simulation.\nThe final artifact is a list of assertions that need to be satisfied during the simulation. Assertions are categorized two types: user-side and system-side. User-side assertions cover behaviors of the multi-agent system that can be observed by the user. On the other hand, system-side assertions cover behaviors of the multi-agent system that cannot be observed by the user. These assertions may include tool calling correctness, parameter correctness, inter-agent behavior, or rule compliance. Appendix A shows example artifacts from our publicly released benchmarking data collection."}, {"title": "Simulation", "content": "For each session, we feed the scenario description and the input problem to a user simulator. The user simulator is a LLM that is prompted to follow the scenario description. The simulation begins with the input problem as the first turn of the session. The input problem is delivered to the supervisor agent who then begins to work the specialist agents. If the specialist agents need to invoke an action, the function calls are passed to the action simulator, which is a LLM grounded on the provided tool schemas. The action simulator also has access to past tool invocations so that it can generate results that is aligned to past observations. After the action simulator returns the simulated action, the specialist agents continue to carry out the task.\nDuring the simulation, the user simulator will continue to interact with the supervisor agents. The user simulator can either help clarify any questions from the supervisor agent or keep sending new task requests. Any requests or answers given by the user simulator should be aligned with the information in the scenario. Once the user simulator determines that all the goals in the scenario are met, the user simulator generates a </stop> token to end the simulation. Otherwise, we set a maximum number of user simulation turns to 5 to prevent simulations that fail to end."}, {"title": "Assertion Judgements", "content": "After simulations are completed, we pass the trajectories to a LLM judge to help automate the assertion evaluation. Along with the simulation trajectories, we also pass the scenario and the assertions to the judge. The judge returns whether each assertion is True or False, and includes the reason for their judgement. The reason often exhibits evidence from the conversation, which can"}, {"title": "Metrics", "content": "A conversation is considered overall successful if all assertions, both user-side and system-side, are True. We then measure Goal Success Rate (GSR) as the percentage of conversations that have all assertions evaluated as True. This overall GSR is our primary measure of success. We also compute User GSR and System GSR, which are the variants of GSR where the assertions being evaluated are limited to one type (either user-side or system-side). These metrics are useful to understand whether multi-agent systems are failing from the perspective of the user or the system (Table 1).\nBeyond goal success, latency is a critical metric in multi-agent systems. As these systems involve multiple agents interacting and collaborating to perform complex tasks, the time delay between agent communications and actions can significantly impact user experience. Minimizing latency is crucial for ensuring the system can operate efficiently, especially in time-sensitive applications. Table 2 lists the various latency metrics that are included in this report.\nLastly, we also included metrics to evaluate routing mode. For routing mode, we are concerned with routing classification accuracy, false agent switch rates, turn-level routing overhead, and routing classification latency (Table 3)."}, {"title": "Experimental Results", "content": "To understand how MAC performs for enterprise usage, we choose three business domains to experiment with. For each domain, we set up a set of agents and tools. Then, we manually collect benchmarking data for each domain. In this report, we show experiments on thirty scenarios from each domain. This dataset is also publicly released for others to benchmark their own multi-agent systems (Section 1). The three domains are as follows:\n1. Travel planning: agents help user plan a trip, which includes booking flights, booking hotels, finding local events, getting information about the weather, etc.\n2. Mortgage financing: agents help user with mortgage issues, e.g., submitting loan applica- tions, querying information about properties, retrieving banking information, etc.\n3. Software development: agents help user design, implement, test, review, and/or deploy code.\nThe first two domains are conversational, whereas the third domain is more about automation and requires minimal interaction with user. For the agent profile design, we have made sure to cover the following conditions to showcase adoption of multi-agent collaboration for various developer setups:\n1. Supervisor agents with and without tools: The supervisor agent for Mortgage has access to MortgageLoans tools, whereas the supervisor agents for other domains do not have tools.\n2. Specialist agents with and without tools: Many specialist agents have their own set of tools. However, some specialist agents in Software do not have tools as they should have the capability to complete the tasks without them.\n3. Agent hierarchy with depth more than one: In Software, the supervisor agent can call on Deploy agent, which can then call on Infrastructure agent and Application agent."}, {"title": "Coordination Mode Experiments", "content": "We report the goal success metrics for coordination mode across a variety of settings in Table 5. For evaluating the assertions, we leverage OpenAI's GPT-40 model [15] for providing LLM-based"}, {"title": "Routing Mode Experiments", "content": "To benchmark dynamic agent routing, we curated two additional datasets for evaluation from our original benchmarking data: 1) Mortgage Routing (3 agent layers) and 2) Travel Routing (2 agent layers), and manually annotated around 100 routing classification labels for each datasets. Intrinsic evaluation shows that the LLM-based routing solution with Claude 3.0 Haiku achieves more than 90% routing classification accuracy and less than 3% false agent switching rate (Table 7). Average latency of routing classification is about 350 ms. With end-to-end evaluation, we observe turn-level routing overhead (time taken by the supervisor agents) in 600 ms to 800ms range (Table 8)."}, {"title": "Agreement with Human Evaluation", "content": "Throughout this report, we have used an LLM to provide automatic judgements on assertions. This has helped scale our experiments so that we can quickly prototype and develop improved multi-agent systems. What if we ask a human to judge conversation success purely from their own perspective without any reference to assertions? To understand how well assertion-based evaluation compares with human evaluation, we deliver a batch of ninety trajectories to human annotators during one of our milestone checkpoints. For each conversation, we ask three annotators to provide binary judgements on a different set of guidelines. During this human evaluation, they are never shown assertions or LLM judgements and only instructed to determine success and efficiency from their own judgements. Table 9 show the instructions given to human annotators for each success metric.\nAfter the human annotators finish their evaluation, the aggregated majority judgement is used to compare against LLM evaluations. Note that for this milestone, we use Claude 3.5 Sonnet (20240620) for supervisor agent and Sonnet 3.0 for specialist agents. See Appendix D for the automatic and human evaluation results for this set of experiments. We then compute agreement between human and LLM judges. Since the judgements for automatic and human evaluation are binary, we use agreement ratio to measure alignment between human and automatic evaluation. The agreement ratio is the number of conversations with matching judgements (either both 1 or both 0) over the total number of conversations evaluated."}, {"title": "Communication Mechanism Ablations", "content": "Section 5 shows the main results of our approach with different models. In this section, we provide additional ablations to quantify the impact of MAC communication mechanisms. This includes experiments with and without payload referencing, as well as comparison with open-source frame- works. The additional experiments provide more justification on the utility of MAC for enterprise applications."}, {"title": "Impact of Payload Referencing", "content": "In Table 11, we report the results of an ablation experiment with Payload Referencing for Software domain with Claude 3.5 Sonnet (20241022) as the supervisor agent as well as specialist agents. This experiment reveals that payload referencing significantly improves both the efficiency as well as effectiveness of multi-agent collaboration, particularly in the code-heavy Software domain. The impact is most pronounced here as large code blocks are frequently exchanged between agents.\nEnabling payload referencing results in a 23% relative improvement in overall GSR as well as a 27% relative reduction in the average communication overhead per turn. The latter can be attributed to a 30% relative reduction in the average output tokens per communication of the supervisor agent, as it is able to leverage the payload referencing mechanism to more efficiently reduce the number of generated tokens required for sharing code blocks, while improving the goal success rate.\nWhen enabling payload referencing, we also observe an increase in the average user-perceived turn latency. As this also includes the latency of the specialist agents, it suggests that the specialist agents operating for a smaller number of turns may be detrimental to the overall goal success."}, {"title": "Comparison with Task Automation Framework", "content": "We also conducted a comparative evaluation of our implementation against a widely adopted open- source framework (OSF) for task automation. The OSF implements multi-agent collaboration as a sequential task-automation workflow, wherein the supervisor agent receives a task from the user and is responsible for breaking it up into one-time sub-tasks for its sub-agents. However, as our evaluation is primarily based on more conversational back-and-forth between the specialist agents, the one-time task assignment may not be sufficient. Therefore, we also expanded the OSF to support multiple sub-agent interactions. The OSF also enables tool-use for its agents using ReAct-style prompting [23], as opposed to native function-calling capability of our implementation.\nIn Table 12, we present a comparative analysis with the OSF. Due to the prompting style of the agents in the OSF, they tend to be highly verbose and utilize a much higher number of tokens. To constrain the experiment to a reasonable budget, we employ Claude 3.5 Sonnet (20240620) as the supervisor agent and Claude 3.0 Sonnet as the specialist agents. Since the original prompts in the OSF may have been optimized for GPT-40 (mini), as suggested by the defaults in the code implementation, we also report the performance of GPT-40 (mini) with the OSF in the conversational setting."}, {"title": "Discussion", "content": "The results presented in this technical report demonstrate the effectiveness of MAC in enabling coordinated problem-solving through multiple specialized agents. Across the evaluated domains"}, {"title": "Conclusion", "content": "This paper presents a comprehensive evaluation of the multi-agent collaboration framework, demon- strating its effectiveness in enabling coordinated problem-solving across a variety of enterprise domains. The results highlight the system's ability to leverage specialized agents to tackle complex tasks, achieving impressive goal success rates of up to 90% in the evaluated scenarios. A key aspect of MAC is the focus on optimizing the inter-agent communication mechanisms. The introduction of the payload referencing feature, for example, was shown to provide significant benefits, particularly in code-heavy tasks, by allowing the supervisor agent to more efficiently share and reference large content blocks. This optimization has led to remarkable improvements in both the overall goal success rate and the communication efficiency of the system.\nFurthermore, the evaluation framework that is employed in this study, which combines assertion- based benchmarking with automated LLM-based judgements, has proven to be a reliable and scalable approach for assessing the performance of multi-agent collaborative systems. During development, we observe high agreement rates on goal success between human and automated evaluation. This validates our evaluation framework, which can enable faster prototyping and development of MAC.\nOne key focus for future work will be on further reducing the latency observed in more complex scenarios, such as those in the Software Development domain. Exploring additional optimizations to the inter-agent coordination mechanisms may help to address this challenge and ensure the system can operate efficiently, even in time-sensitive applications. Additionally, expanding the detection and handling of different forms of static long-form payloads could lead to additional performance gains. Investigating automatic prompt optimization techniques may also prove valuable in enhancing the collaboration capabilities of the individual agents.\nFinally, automating the dataset curation process for the benchmarking framework could enable more scalable and iterative development of the multi-agent system, allowing for faster prototyping and deployment of improvements. By building on the solid foundations laid out in this technical report, the future work in these areas promises to further strengthen MAC and its ability to tackle increasingly complex, real-world challenges."}]}