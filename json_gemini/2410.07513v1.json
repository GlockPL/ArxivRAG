{"title": "Evolutionary Contrastive Distillation for Language Model Alignment", "authors": ["Julian Katz-Samuels", "Zheng Li", "Hyokun Yun", "Priyanka Nigam", "Yi Xu", "Vaclav Petricek", "Bing Yin", "Trishul Chilimbi"], "abstract": "The ability of large language models (LLMs) to execute complex instructions is essential for their real-world applications. However, several recent studies indicate that LLMs struggle with challenging instructions (Zhou et al., 2023; Qin et al., 2024; Jiang et al., 2023b). In this paper, we propose Evolutionary Contrastive Distillation (ECD), a novel method for generating high-quality synthetic preference data designed to enhance the complex instruction-following capability of language models. ECD generates data that specifically illustrates the difference between a response that successfully follows a set of complex instructions and a response that is high-quality, but nevertheless makes some subtle mistakes. This is done by prompting LLMs to progressively evolve simple instructions into more complex instructions. When an instruction is made more complex, the original successful response mostly meets the new requirements but misses one or two, thus becoming a \"hard negative\u201d example for the new instruction. By pairing a good response with such a hard negative response, and employing contrastive learning algorithms such as DPO (Rafailov et al., 2023), we improve language models' ability to follow complex instructions. Empirically, we observe that our method yields a 7B model that exceeds the complex instruction-following performance of current state-of-the-art (SOTA) 7B models and is competitive even with open-source 70B models.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated impressive capabilities in a wide range of tasks ranging from creative writing to code generation. In light of these achievements, there has been a surge of interest in building complex data processing systems with LLM-based components (Developers, 2024a,b). Complex instruction-following capability, which is the capability of LLMs to generate outputs consistent with multiple interdependent specifications in the prompt, is critically important for such systems to operate reliably. Consequently, multiple benchmarks have been proposed to capture various aspects of complex instruction-following (Zhou et al., 2023; Qin et al., 2024; Jiang et al., 2023b). Unfortunately, these studies find that there still is a significant gap between proprietary LLMs and open source models on these benchmarks.\nThis raises the question: how shall we effectively distill the complex instruction-following ability of stronger LLMs into smaller language models? After the seminal work by Wang et al. (2023), the usage of proprietary LLMs for the generation of alignment data has been actively studied. As a typical alignment pipeline consists of Supervised Fine-Tuning (SFT) methods and Preference Fine-Tuning (PFT\u00b9) stages (Ouyang et al., 2022), these data generation methods are also largely categorized into SFT data generation methods (e.g., UltraChat (Ding et al., 2023)) and PFT data generation methods (e.g., UltraFeedback (Cui et al., 2023)).\nAmong a wide variety of SFT data generation methods, Evol-Instruct (Xu et al., 2023) and Conifer (Sun et al., 2024) are specifically designed to improve complex instruction-following. They share the common goal: generate highly complex instructions. And they also share the same evolutionary strategy: they first prompt proprietary LLMs to evolve simple seed instructions from ShareGPT to have progressively more complex requirements. Then, proprietary LLMs are again prompted to generate demonstrations on these complex instructions."}, {"title": "2 Related Work", "content": "Instruction-Following. The complex instruction-following ability of LLMs has received significant attention recently with many works proposing new evaluation benchmarks (Zhou et al., 2023; Jiang et al., 2023b; Qin et al., 2024). They consistently find that open source LLMs have significant gaps on following complex instructions compared to proprietary LLMs. At the same time, there has been relatively less work on developing techniques to improve the complex instruction-following ability. Two exceptions include Evol-Instruct (Xu et al., 2023) and Conifer (Sun et al., 2024), which prompt LLMs to evolve the complexity of instructions, and apply SFT on the generated data. There are two main differences between these works and ours. First, we focus on generating preference data for PFT instead of demonstration data for SFT. While SFT data provides only positive feedback to the model on the correct behavior, PFT data provides both positive and negative feedback, which enables teaching the contrast between a successful instruction-following example and a subtle failure example. Second, we propose a fine-grained hierarchical taxonomy of evolution operations to ensure each step of evolution introduces diverse and subtle variations of requirements.\nContrastive Learning. The effectiveness of contrastive learning (Chopra et al., 2005; Hadsell et al., 2006) has been shown across numerous modalities (He et al., 2020; Chi et al., 2020; Radford et al., 2021). In the contrastive learning literature, the importance of the quality of negative samples has been well-established (Robinson et al., 2020). Numerous \"hard\" negative mining techniques have been introduced to improve the quality of negative samples (Schroff et al., 2015; Wu et al., 2018; Xiong et al., 2021). While many alignment methods such as InstructGPT (Ouyang et al., 2022) and DPO (Rafailov et al., 2023) leverage negative samples to facilitate learning (Tajwar et al., 2024), how to obtain sufficiently hard negative samples has not been actively studied. Yan et al. (2024) employs hard negative mining techniques to improve the robustness of the model. Our work instead targets improving complex instruction-following, and uses LLM prompting to generate negative examples rather than mining samples from a fixed dataset.\nData Generation Methods for PFT. Much attention has been given to how to generate preference data for fine-tuning LLMs. Pioneering works focused on collecting feedback from humans (Christiano et al., 2017; Stiennon et al., 2020; Ouyang et al., 2022). Under this approach, human annotators would observe a set of responses to an instruction and rank them according to their preferences. As human annotation is expensive and difficult to scale, however, follow-up works have proposed alternative synthetic data generation approaches.\nThe most popular method of generating PFT data from LLMs is to sample two independent responses on the same instruction, and then ask an LLM to judge which one is of higher quality. This method is often called RLAIF (Reinforcement Learning from AI Feedback) (Bai et al., 2022b). While RLAIF methods have shown encouraging results (Cui et al., 2023; Tunstall et al., 2023; Ivison et al., 2023), obtaining high-quality preference annotations from LLMs have been found challenging (Yang et al., 2023; Sharma et al., 2024). This is because the difference between two responses on the identical instruction can be minor, and LLM-based preference annotation can be subject to many confounding factors, such as self-bias and verbosity bias (Zheng et al., 2023).\nTo avoid quality issues from LLM-based preference annotation, Reinforcement Learning from Contrastive Distillation (RLCD) (Yang et al., 2023) proposes to employ two different prompt templates rather than using the same one. One prompt template is designed to elicit desirable responses, and another is designed to elicit undesirable response. Such a deliberate design of templates allows RLCD to bypass preference annotation. As we discuss in Section 5, however, we find it challenging to prompt-engineer proprietary LLMs to generate undesirable responses. As proprietary LLMs are already aligned to generate helpful and harmless responses (Bai et al., 2022a), very often, their response from the undesirable response template are either as helpful as the response from the desirable response template, or trivially unhelpful (\"No, I can't answer that question.\"), limiting their value as a hard negative example."}, {"title": "3 Evolutionary Contrastive Distillation", "content": "First, we propose the evolutionary data generation process to synthesize preference data for complex instruction-following. Then, we discuss how contrastive learning methods shall be employed to train on this generated data.\nData Generation Framework We assume access to a seed set of instructions $I^{(0)} = \\{I^{(0)}_1, ..., I^{(0)}_n\\}$ and responses on these instructions $R^{(0)} = \\{R^{(0)}_1, ..., R^{(0)}_n\\}$ where $R^{(0)}_i$ is the response on instruction $I^{(0)}_i$. In our experiments, we use ShareGPT (Team, 2023) as the seed set. As instructions which are publicly accessible in scale such as ShareGPT lack the complexity needed for LLM applications (Xu et al., 2023), we iteratively increase their complexity through T rounds of an evolutionary process, and generate the preference dataset D, which consist of (instruction, positive response, negative response) triples.\nAt each round t, for each instruction $I^{(t-1)}_i$, we run the following:\n\u2022 Evolution: Prompt an LLM to evolve $I^{(t-1)}_i$ into proposal instruction $I^{(t)}_i$. The evolution typically increases the complexity of the instruction. We discuss types of evolution operations we consider in Section 4.\n\u2022 Adaptation: Prompt an LLM to generate a proposal response $R^{(t)}_i$ on the new instruction $I^{(t)}_i$.\n\u2022 Elimination: The quality of $R^{(t)}_i$ is checked with another LLM prompt. If the quality is acceptable, the proposal is kept: $(I^{(t)}_i, R^{(t)}_i) \\leftarrow (\\hat{I}^{(t)}_i, \\hat{R}^{(t)}_i)$. Otherwise, $(I^{(t)}_i, R^{(t)}_i) \\leftarrow (I^{(t-1)}_i, R^{(t-1)}_i)$\n\u2022 Contrast: If the proposal was accepted, add the triple $(I^{(t)}_i, R^{(t)}_i, R^{(t-1)}_i)$ into D.\nThe key intuition is that as the original response $R^{(t-1)}_i$ was generated without seeing all of the requirements from the evolved instruction, the evolved response $R^{(t)}_i$ is likely better than the original response $R^{(t-1)}_i$ for the evolved instruction $I^{(t)}_i$. On the other hand, $R^{(t-1)}_i$ is still a hard negative, as it was a desirable response for $I^{(t-1)}_i$, and the difference between $I^{(t-1)}_i$ and $I^{(t)}_i$ is often subtle due to the design of evolution operations.\nWe emphasize that this framework of generating preference data from a evolutionary process is generic, and can accommodate different definitions of the evolutionary process. In Section 5, we show the evolutionary process from Sun et al. (2024) can be successfully adopted in this framework. In order to further improve the quality of the data generated, we propose a fine-grained taxonomy of evolution operations in Section 4.\nContrastive Learning A variety of alignment algorithms can be leveraged to fine-tune LLMs with triplets D generated from the evolutionary process (Ouyang et al., 2022; Rafailov et al., 2023; Azar et al., 2023; Ethayarajh et al., 2024; Hong et al., 2024; Meng et al., 2024). In this work, we focus on Direct Policy Optimization (DPO) (Rafailov et al., 2023) mainly because it is the most well-established in open source LLMs (Tunstall et al., 2023; Ivison et al., 2023), making our experiments and checkpoints easily comparable with existing work. We leave it as a future work to explore the implication of algorithm choice on the complex instruction-following capability.\nFor completeness, we briefly discuss how DPO is adopted for ECD. Let us denote $\\pi_\\theta$ as a language model policy, where $\\pi_\\theta(R|I)$ denotes the probability of generating the response R conditional on the instruction I. We assume that we are given a reference language model $\\pi_{ref}$, and use it to initialize $\\pi_\\theta$. Then, we directly fine-tune the language model $\\pi_\\theta$ on the preference dataset D by solving the following optimization problem:\n$\\operatorname{argmin}_\\theta L_{DPO} (D; \\theta) :=\\mathbb{E}_{I, R_+, R_-\\in D} \\left[\\log \\sigma\\left(\\frac{\\pi_\\theta(R_+|I)}{\\pi_{ref}(R_+|I)} - \\frac{\\pi_\\theta(R_-|I)}{\\pi_{ref}(R_-|I)}\\right)\\right]$\nFollowing Rafailov et al. (2023, 2024), the resulting LLM policy $\\pi_\\theta$ can be associated with the policy which maximizes the reward model learned from D. Due to our construction of D, the reward model would assign higher rewards on responses which meticulously follow complex instructions."}, {"title": "4 Fine-Grained Evolution Taxonomy", "content": "In order to ensure the evolutionary process generates diverse variations of instructions, we define a fine-grained hierarchical taxonomy of evolution operations. Each evolution operation samples a leaf node from this taxonomy to determine the type of operation and the corresponding prompt template. The top level of the taxonomy consists of five categories:\n\u2022 Content: Add a condition that changes the scope or the specificity of the response.\n\u2022 Style: Control the tone, sentiment, or formality of the response.\n\u2022 Format: Impose a formatting or linguistic requirement.\n\u2022 Reasoning: Add a constraint that requires additional steps of logical or numerical reasoning.\n\u2022 Breadth: Come up with a new instruction that matches the domain, length, and complexity of the original instruction.\nThen, for each of the top-level category, we define around five fine-grained evolution types. While Evol-Instruct (Xu et al., 2023) defines only eleven high-level operation types, and Confier (Sun et al., 2024) provides about eleven operation types in in-context examples, our taxonomy defines more fine-grained and hierarchically organized 22 operation types. In Section 5, we find this taxonomy generates higher-quality preference data for ECD.\nFor illustration, consider the following example of an evolution step:\n1. The process starts from a seed instruction \"Write an application letter to my college for immigration certificate.\".\n2. 'Format' is randomly chosen from the top level.\n3. Within the 'Format' category, \u2018Hierarchical: Introduce a hierarchical structure which requires an understanding of a hierarchy of tasks and follow it\u2019 is randomly chosen.\n4. LLM is prompted with the prompt corresponding to 'Hierarchical' type to evolved the original instruction into: \u201cWrite an application letter to my college for immigration certificate. The letter should have the following format: -Date, -Inside address, -Salutation, ...\"."}, {"title": "5 Experiments", "content": "5.1 Models and Datasets\nWe conduct our experiments on the popular base model Mistral-7B-v0.1 (Jiang et al., 2023a). We train our own Instruction Fine-tuned (IFT) model based on the Conifer-7B's SFT recipe (Sun et al., 2024), the SOTA IFT 7B model, mixing in 53k samples from ShareGPT with the Conifer dataset. We call the resulting IFT dataset Conifer-Mix and the resulting IFT model, Conifer-7B-SFT. Conifer-7B-SFT forms the backbone on which we perform PFT experiments using DPO. For details on training, see Appendix B.\nTo test the robustness of our approach, we generate 3 separate ECD datasets: (i) ECD-FineGrained: 30k preference pairs from Fine-Grained Evolutionary Process discussed in Section 4, (ii) ECD-Conifer: ECD data based on the evolutionary process from Conifer (Sun et al., 2024), and (iii) ECD-FineGrained-Conifer: a concatenation of ECD-FineGrained and ECD-Conifer. We add UltraFeedback (Cui et al., 2023) to each of these to improve conversational quality; see the Appendix B.2 for the ablation on its impact.\n5.2 Evaluation\nOur primary goal is to improve the complex instruction-following capability of LLMs, which we measure with the following three benchmarks:\n\u2022 IFEval is a popular benchmark for instruction-following that measures the ability of LLMs to follow programatically checkable instructions such as \"give a response that is more than 400 words\" (Zhou et al., 2023). It uses metrics such as prompt-level accuracy and instruction-level accuracy with a strict version that interprets the requirements very precisely while a loose version gives some leeway.\n\u2022 FollowBench is another instruction-following benchmark that uses GPT-4 to measure the ability of a model to follow fine-grained constraints across 5 different difficulties and types (Format, Content, Style, Situation, and Example) (Jiang et al., 2023b). This benchmark employs two metrics: hard satisfaction rate (HSR) and soft satisfaction rate (SSR). HSR quantifies the average frequency at which all the requirements or constraints are completely met. On the other hand, SSR calculates the average degree to which individual constraints are satisfied across all the given instructions.\n\u2022 InfoBench evaluates the instruction-following of LLMs by breaking down instructions into a set of fine-grained criteria and asks GPT-4 to evaluate the extent to which a model meets the criteria (Qin et al., 2024).\nSince conversational quality is also important in LLM applications, we also evaluate on the following benchmarks:\n\u2022 MTBench is a multi-turn benchmark that measures the conversational quality of a model. It uses GPT-4 to rate the quality of a model's answers across two turns on a scale of 1-10 (Zheng et al., 2023).\n\u2022 AlpacaEval is a single-turn benchmark that measures helpfulness. It uses GPT-4-Turbo to compute the win-rate against a reference model. We use the default reference model, GPT-4-Turbo, and the Length-Controlled win-rate, which has a correlation of 0.98 with ChatBot Arena (Dubois et al., 2024b,a).\nWe benchmark our ECD models against large open-source scale models such as LLaMa-2-70B-Chat (Touvron et al., 2023) and Vicuna-13B-v1.5 (Team, 2023), and strong 7B models like Conifer-DPO-7B (Sun et al., 2024), Deita-7B-v1.0 (Liu et al., 2023), Mistral-7B-Evol-Instruct (Xu et al., 2023), Mistral-7B-ShareGPT-DPO (Sun et al., 2024), and Zephyr-7B-beta (Tunstall et al., 2023).\n5.3 Results\nOur ECD models achieve SOTA performance at the 7B scale for complex instruction-following. For example, consider our ECD model trained on ECD-FineGrained-Conifer. Table 1 shows that it outperforms Conifer-7B-DPO, the latest 7B SOTA in instruction-following, on each metric in IFEval by a substantial margin, improving loose prompt accuracy from 52.3% to 59.3% and loose instruction accuracy from 63.3% to 69.8%, and achieves similar improvements on FollowBench and InfoBench. Similarly, the ECD on ECD-Finegrained-Conifer improves over its initialization Conifer-7B-SFT, improving for example on loose prompt accuracy by over 10pp and even shows competitive performance with LLaMa-2-70B-Chat. ECD-FineGrained-Conifer achieves particularly strong performance on InfoBench Hard, indicating the strength of ECD in improving instruction-following for particularly difficult instructions. While here we discuss specifically ECD-FineGrained-Conifer, these trends uphold across our three ECD data mixtures, ECD-FineGrained, ECD-Conifer, and ECD-FineGrained-Conifer, indicating the robustness of our approach.\nOur ECD models also achieve strong performance on the conversational quality benchmarks: MT-Bench and AlpacaEval. For example, consider ECD-Conifer. It achieves an MT-Bench score of 7.49 and and a length-controlled AlpacaEval score of 25.2%. These are large improvements in comparison to the initialized IFT model Conifer-7B-SFT and the prior SOTA in instruction-following 7B models, Conifer-7B-DPO. As this trend holds across all three ECD models, our data mixture recipe consistently produces SOTA 7B models for complex instruction-following while maintaining strong conversational quality.\nAmong ECD, RLAIF, and RLCD, which is the most effective technique for improving complex instruction-following? We also investigated how ECD compares against RLAIF and RLCD for improving instruction-following. To this end, we generated RLAIF and RLCD data on top of ShareGPT prompts and refer to these as ShareGPT-RLAIF and ShareGPT-RLCD, respectively. Prompts used in the generation can be found in the Appendix C.\nIn order to perform a clean ablation, we used three further data mixtures which do not mix UltraFeedback in: (i) ECD-FineGrained-Pure, all 104,499 preference pairs from the ECD version of the FineGrained synthetic data generation approach, (ii) ECD-Conifer-Pure, the ECD version of the Conifer dataset, and (iii) ECD-FineGrained-Conifer-Pure, a concatenation of ECD-FineGrained and ECD-Conifer-Pure. Since ShareGPT-RLAIF, ShareGPT-RLCD, and ECD-FineGrained-Pure all use the same ShareGPT instructions as seeds and Claude 2 as the teacher LLM, we can directly compare the performance of the models trained on these three data mixtures to assess the effectiveness of ECD, RLAIF, and RLCD."}, {"title": "7 Limitations", "content": "In this work, we focused on improving complex instruction-following capability. However, we envision that ECD can also be useful at improving other LLM capabilities, such as tool usage (Schick et al., 2023), reasoning (Talmor et al., 2018), math (Cobbe et al., 2021), etc. Broadening the definition of our evolutionary process to target a broader set of capabilities is left as a future work.\nAlso, in this work, we focused on having a dependency on a teacher LLM to evolve instructions and generate responses. Therefore, the resulting model is likely to inherit various types of bias the teacher LLM has. However, it is conceivable the teacher model in ECD could be replaced with the student model itself. Such self-improvement (Yuan et al., 2024) will remove dependency on teacher models, and open up an opportunity to surpass them. Future work is required to determine whether it is possible to remove the dependency on a strong teacher LLM."}, {"title": "8 Ethical Considerations", "content": "The focus of our work is on improving the complex instruction-following capability of LLMs, a fundamental capability. The ability to faithfully execute instructions from humans will reduce the risk of LLMs undertaking unintended actions, promoting safer uses of LLMs. On the flip side, this can increase the risk of malicious actors using LLMs towards pernicious ends. A fruitful direction for future research is to continue using alignment to improve complex instruction-following while enhancing the model's capability to refrain from executing harmful tasks."}, {"title": "A Evolution Details", "content": "A.1 Full Taxonomy\nSee Table 6 for the full taxonomy of evolution operations. We also show the prompts for fine-grained evolution in Section C.1. Most of the evolution operations are designed to introduce gradual, nuanced changes to the original instruction. Examples of these evolutions can be found in Table 7. Additionally, since the proposed method leverages DPO, it remains robust to occasional poor examples in the negative responses. For such \"easy\" negative responses, where the margin between positive and negative responses is large, the gradient magnitude will be small. This highlights another advantage of Preference Fine-Tuning over Supervised Fine-Tuning.\nA.2 Evolution Elimination\nWe utilized an evolution eliminator employing heuristic methods such as instruction length variation and deduplication to determine if the evolved instructions and answers are both valid. If unsuccessful, halt the process; otherwise, proceed to the next round of evolution. This precaution is necessary because LLMs like Claude2 may also produce errors, such as omitting code snippets/tables from the original instruction, or generating duplicate instructions."}, {"title": "B Additional Details", "content": "B.1 Training Details\nWe leverage the widely adopted 'The Alignment Handbook' (Tunstall et al., 2023) repository, released by HuggingFaceH4, for fine-tuning. For all our experiments, we use a machine with 8 NVIDIA A100 80GB GPUs. For SFT, we train for 4 epochs with a learning rate of 2e-5 and warm-up ratio of 0.1 with per device batch size 16 and gradient accumulation steps 4. For DPO training, we train for 1 epoch a per device batch size of 8 and a gradient accumulation of 2. We use a learning rate of 5e-7 and a warm-up ratio of 0.1.\nB.2 Ablation Study on Removing UltraFeedback from Data Mixture\nIn this section, we conduct an ablation study on removing UltraFeedback from the ECD-Conifer data mixture. Again, we use the Conifer-7B-SFT model as the IFT initialization. We compare two checkpoints ECD-Conifer and ECD-Conifer-Pure. Whereas ECD-Conifer consists of both the ECD version of Conifer and Ultrafeedback to optimize both instruction-following and conversational quality, ECD-Conifer-Pure only removes UltraFeedback. Table 8 depicts the results for instruction-following and Table 9 depicts the results on conversational quality. On instruction-following, we see that ECD-Conifer-Pure tends to outperform ECD-Conifer, with particularly strong performance on IFEval. For example, it improves the strict prompt accuracy by 5.1%. On the other hand, for conversational quality, ECD-Conifer improves on ECD-Conifer-Pure with a much improve MT-Bench score and LC Win-Rate, indicating the usefulness of UltraFeedback for conversational quality."}, {"title": "C Prompts Used", "content": "C.1 Prompt for Fine-grained Evolution\nC.1.1 Content Evolution\nYou are an Instruction Rewriting Expert. You need to rewrite #Given Instruction# based on #Rewriting Requirement#, in order to obtain a #Rewritten Instruction#.\nBasically, #Rewritten Instruction# should adhere to the following guidelines:\n1. #Rewritten Instruction# must be reasonable and must be understood and responded by humans.\n2. You should try your best not to make the #Rewritten Instruction# become verbose,\n#Rewritten Instruction# can only add 10 to 20 words into #Given Instruction#.\n#Given Instruction#\n{given_instruction}\n#Rewriting Requirement#\nPlease add one proper content constraint to the #Given Instruction#.\nThe content constraints include but are not limited to:\n1. Add a Subtask or Another Related Question.\n2. Narrow Down the Topic: Instead of a general theme or topic, provide a more specific subset.\n3. Set a Higher Standard: Raise the bar for what's considered acceptable or successful.\n4. Limit Resources: Restrict the number or type of resources someone can use.\n5. Introduce Specific Criteria: Mandate particular components or features that must be included.\n6. Specifying Sequence: Dictate the order in which certain steps or actions should be taken.\nPlease start with the sentence \"Here is the new instruction:\" in #Rewritten Instruction#.\nPlease don't add anything related to the #Rewriting Requirement# in the #Rewritten Instruction#.\nIf #Given Instruction# contains no-text parts such as table and code examples ,\n#Rewritten Instruction# should also keep them.\n#Rewritten Instruction#\nC.1.2 Format Evolution\nYou are an Instruction Rewriting Expert. You need to rewrite #Given Instruction# based on #Rewriting Requirement#, in order to obtain a #Rewritten Instruction#.\nBasically, #Rewritten Instruction# should adhere to the following guidelines:\n1. #Rewritten Instruction# must be reasonable and must be understood and responded by humans.\n2. You should try your best not to make the #Rewritten Instruction# become verbose,\n#Rewritten Instruction# can only add 10 to 20 words into #Given Instruction#.\n#Given Instruction#\n{given_instruction}\n#Rewriting Requirement#\nPlease add one proper format constraint that #Given Instruction#\ndoes not have. The format constraints include but are not limited to:\n1. Length: Imposing constraints on the length of individual words, sentences, or paragraphs.\n2. Hierarchical Instructions: Providing instructions that have a hierarchical structure, where the AI\nneeds to understand and follow a hierarchy of tasks to construct a response.\n3. Special Output Format: Asking the AI to respond by using data format like table, json, HTML, LaTeX, etc.\n4. Morphological Constraints: Asking the AI to avoid or use specific morphemes.\n5. Multi-lingual Constraints: Asking the AI to respond\nin multiple languages or switch between languages according to complex patterns.\n6. Incorporation of Specific Literary Devices:\nRequiring the inclusion of specific, and perhaps numerous, literary devices.\n7. Following a Specific Grammatical Structure:\nRequiring the AI to create responses that strictly follow a particular grammatical structure.\nPlease start with the sentence \"Here is the new instruction:\" in #Rewritten Instruction#.\nPlease don't add anything related to the #Rewriting Requirement# in the #Rewritten Instruction#.\nIf #Given Instruction# contains no-text parts such as table and code examples ,\n#Rewritten Instruction# should also keep them.\n#Rewritten Instruction#\nC.1.3 Style Evolution\nYou are an Instruction Rewriting Expert. You need to rewrite #Given Instruction# based on\n#Rewriting Requirement#, in order to obtain a #Rewritten Instruction#.\nBasically, #Rewritten Instruction# should adhere to the following guidelines:\n1. #Rewritten Instruction# must be reasonable and must be understood and responded by humans.\n2. You should try your best not to make the #Rewritten Instruction# become verbose,\n#Rewritten Instruction# can only add 10 to 20 words into #Given Instruction#.\n#Given Instruction#\n{given_instruction}\n#Rewriting Requirement#\nPlease add one proper style constraint that #Given Instruction#\ndoes not have. The style constraints include but are not limited to:\n1. Tone and Emotion: Specify the desired emotional tone for the response.\n2. Writing Style: Ask the AI to mimic a specific author's writing style.\n3. Contradiction: Ask the AI to provide a response that contradicts the previous\nstatement or take a stance opposite to its prior response.\n4. Ambiguity: Instruct the AI to create responses with intentional ambiguity or double meanings.\n5. Humor or Satire: Request that the response be humorous\nor satirical, requiring the AI to generate jokes or witty remarks.\nPlease start with the sentence \"Here is the new instruction:\" in #Rewritten Instruction#.\nPlease don't add anything related to the #Rewriting Requirement# in the #Rewritten Instruction#.\nIf #Given Instruction# contains no-text parts such as table and code examples ,\n#Rewritten Instruction# should also keep them.\n#Rewritten Instruction#\nC.1.4 Breadth Evolution\nYou are an Instruction Creator Expert. You need to draw inspiration from the #Given Instruction#\nto create a brand new #Created Instruction# based on #Creation Requirement#.\n#Given Instruction#\n{given_instruction}\n#Creation Requirement#\n1. #Created Instruction# must be reasonable and must be understood and responded by humans.\n2. #Created Instruction# should belong to the same domain as the #Given Instruction#\nbut be even more rare.\n3. The LENGTH and complexity of the #Created Instruction# should be similar to that of the\n#Given Instruction#.\n4. '#Given Instruction#', '#Created Instruction#', 'given instruction' and 'created instruction' are not\nallowed to appear in #Created Instruction#\n5. #Created Instruction# must be self-contained.\nPlease start with the sentence \"Here is the new instruction:\" in #Created Instruction#.\n Please don't add anything related to the #Creation Requirement# in the #Created Instruction#.\n#Created Instruction#\nC.1.5 Reasoning Evolution\nYou are an Instruction Rewriting Expert. You need to rewrite #Given Instruction# based on\n#Rewriting Requirement#, in order to obtain a #Rewritten Instruction#. Basically,\n#Rewritten Instruction# should adhere to the following guidelines:\n1. #Rewritten Instruction# must be reasonable and must be understood and responded by humans.\n2. You should try your best not to make the #Rewritten Instruction# become verbose,\n#Rewritten Instruction# can only add 10 to 20 words into #Given Instruction#.\n#Given Instruction#\n{given_instruction}\n#Rewriting Requirement#\nPlease add one proper reasoning constraint that #Given Instruction# does not have. The reasoning\nconstraints include but are not limited to:\n1. Explicitly request multiple-step or chain-of-thought reasoning.\n2. Add some numeric reasoning steps.\n3. Add some commonsense reasoning steps.\nPlease start with the sentence \"Here is the new instruction:\" in #Rewritten Instruction#.\n Please don't add anything related to the #Rewriting Requirement# in the #Rewritten Instruction#.\nIf #Given Instruction# contains no-text parts such as table and code examples ,\n#Rewritten Instruction# should also keep them.\n#Rewritten Instruction#\nC.2 Prompt for RLAIF\nWe adopted the RLAIF prompt used from (Yang et al., 2023):\nConsider the following conversation between a human and an assistant:\n$instruction\nPlease choose the response that is more helpful.\nOptions:\n(A) $answer1\n(B) $answer2\nThe answer is: (\nC.3 Prompts for RLCD\nWe adopted the RLCD prompt used from (Yang et al., 2023) for helpful template:\nHuman: $instruction\nAssistant: (giving a helpful response)\nAs well as unhelpful template:\nHuman: $instruction\nAssistant: (giving an unhelpful response)"}, {"title": "D Licences and Terms of Service Compliance", "content": "Below, we give all the artifacts that we used and their respective licenses:\n\u2022 Alignment Handbook: Apache-2.0\n\u2022 Mistral 7B: Apache-2.0\n\u2022 IFEval: Eclipse Public License - v 2.0\n\u2022 FollowBench: Apache 2.0\n\u2022 InfoBench: MIT License\n\u2022 AlpacaEval: Apache 2.0\n\u2022 MT-Bench: Apache 2.0\n\u2022 GPT-4 outputs: since we use this for research, we are in compliance with the GPT-4 terms of service.\n\u2022 Claude outputs: since we use this for research, we are in compliance with the Claude terms of service."}]}