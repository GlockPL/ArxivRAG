{"title": "LLM-Based Offline Learning for Embodied Agents via Consistency-Guided Reward Ensemble", "authors": ["Yujeong Lee", "Sangwoo Shin", "Wei-Jin Park", "Honguk Woo"], "abstract": "Employing large language models (LLMs) to enable embodied agents has become popular, yet it presents several limitations in practice. In this work, rather than using LLMs directly as agents, we explore their use as tools for embodied agent learning. Specifically, to train separate agents via offline reinforcement learning (RL), an LLM is used to provide dense reward feedback on individual actions in training datasets. In doing so, we present a consistency-guided reward ensemble framework (COREN), designed for tackling difficulties in grounding LLM-generated estimates to the target environment domain. The framework employs an adaptive ensemble of spatio-temporally consistent rewards to derive domain-grounded rewards in the training datasets, thus enabling effective offline learning of embodied agents in different environment domains. Experiments with the VirtualHome benchmark demonstrate that COREN significantly outperforms other offline RL agents, and it also achieves comparable performance to state-of-the-art LLM-based agents with 8B parameters, despite COREN having only 117M parameters for the agent policy network and using LLMs only for training.", "sections": [{"title": "1 Introduction", "content": "Developing embodied agents capable of understanding user instructions and executing tasks in physical environments represents a crucial milestone in the pursuit of general AI. Recent advancements in large language models (LLMs) have demonstrated their remarkable reasoning capabilities, paving the way for their application in embodied agents (Yang et al., 2023; Padmakumar et al., 2023; Pantazopoulos et al., 2023; Yun et al., 2023; Logeswaran et al., 2022; Ichter et al., 2022). Yet, deploying an LLM directly as part of an embodied agent presents several inefficiencies, such as the need for sophisticated environment-specific prompt design, substantial computational resource demands, and inherent model inference latency (Hashemzadeh et al., 2024). These factors can limit the practical application of LLMs, particularly in scenarios where embodied agents are required to respond rapidly and efficiently.\nIn the literature of reinforcement learning (RL), data-centric offline learning approaches have been explored (Kumar et al., 2020a). These offline RL approaches are designed to establish efficient agent structures, necessitating datasets that include well-annotated agent trajectories with reward information. However, the characteristics of instruction-following tasks assigned to embodied agents, particularly their long-horizon goal-reaching nature, often conflict with such dense data requirements of offline RL. Embodied agents normally can produce trajectories with sparse reward feedback, because their instruction-following tasks are evaluated based on binary outcomes of success or failure, which directly align with the specific goals of the instructions. In offline RL, this sparse reward setting poses significant challenges in achieving effective agent policies (Park et al., 2023; Ma et al., 2022).\nIn this work, we explore LLMs for offline RL. By employing capable LLMs as a reward estimator that provides immediate feedback on agent actions, we augment the trajectory dataset with dense reward information. This method, LLM-based reward estimation is capable of significantly enhancing the effectiveness of offline RL for embodied agents. To do so, we address the limitations inherent in LLM-based reward estimation. A primary challenge arises from the limited interaction with the environment in an offline setting, which complicates the LLMs' ability to acquire essential environmental knowledge. The offline setting makes it difficult to ensure that the generated rewards are properly grounded in the specific domain of the environment. For instance, without explicit knowledge that a flowerpot is typically stored in a living room in the target environment, an LLM might struggle to accurately assign rewards for actions like \u201cgo to living room\u201d versus \u201cgo to balcony\u201d when tasked with watering plants. While both actions might seem reasonable from a commonsense perspective, the optimal action depends on specific conditions of the target environment that the LLM may not have access to in the offline setting.\nThese challenges, unique to the offline context, differentiate our work from previous works on online LLM-based reward estimation, where LLMs can be fine-tuned or prompts can be refined through repeated interaction with environment or human (Lee, 2024; Xie et al., 2024; Li et al., 2023; Song et al., 2023c). Since these interactions are not available in offline settings, improving the LLM's insufficient spatial reasoning for accurate reward estimation requires a fundamentally different approach.\nIn response, we present COREN, a consistency-guided reward ensemble framework, specifically designed for robust LLM-based reward estimation and effective agent offline learning. It adopts a two-staged reward estimation process, as depicted in Figure 1. (i) An LLM is first queried to estimate several types of rewards for actions, each considering a distinct spatio-temporal consistency criterion of the LLM to have coherent and domain-grounded rewards. (ii) Then, these rewards are further orchestrated, being unified into domain-specifically tuned rewards via an alignment process with the sparse rewards of given trajectories. The resulting agent, trained on the unified dense rewards by offline RL, is capable of performing instruction-following tasks with high efficiency and minimal latency at deployment. This offline RL scheme, enhanced by LLM-based reward estimation, overcomes the limitations faced by the agents that rely on the online exploitation of LLMs.\nThe contributions of our work can be summarized as follows: (i) addressing a practical yet challenging problem of embodied agent offline learning using LLMs for the first time; (ii) proposing a two-staged reward estimation algorithm guided by a spatio-temporal consistency ensemble; and (iii) extensive evaluation on the VirtualHome benchmark, demonstrating performance comparable to state-of-the-art LLM-based online agents."}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Goal-POMDPs", "content": "For an embodied agent that follows user-specified instructions, we model their environment as a goal-conditioned partially observable Markov decision process (Goal-POMDP). A Goal-POMDP is represented by a tuple (S, A, P, R, \u03b3, \u03a9, O, G) (Song et al., 2023a; Singh et al., 2023) with states $s \\in S$, actions $a \\in A$, a transition function $P: S \\times A \\rightarrow \\Delta(S)$, a reward function $R: S \\times A \\times G\\rightarrow \\mathbb{W}^1 \\rightarrow \\mathbb{R}$, a discount factor $\\gamma\\in [0, 1)$, observations $o \\in \\Omega$, an observation transition function $O: S \\times A \\rightarrow \\Omega$, and goal conditions $G \\in G$. Given this Goal-POMDP representation, we consider a user-specified instruction $i$ as a series of goal conditions $G = (G_1,... ) \\subseteq G$ such that the embodied agent is tasked with completing each of the specified goal conditions for the instruction $i$."}, {"title": "2.2 Offline RL", "content": "For a Goal-POMDP, its optimal policy is formulated by\n$\\pi^* = \\underset{\\pi}{\\operatorname{argmax}} \\mathbb{E}_{(s,a)~\\pi, G~G}[\\sum_t \\gamma^t R(s_t, a_t, G)]$  (1)\nTo achieve the optimal policy, we explore offline RL approaches where the policy is derived by optimizing the Bellman error objective, relying on\n1$\\mathbb{X}$ for a set $X$ is all possible finite products of $X$."}, {"title": "3 Our Approach", "content": "LLM-based reward estimation. Offline RL facilitates agent learning without direct environment interaction, but relying solely on sparse rewards to learn long-horizon instruction-following tasks is often inefficient. To improve this, we augment agent trajectories with stepwise intrinsic rewards through LLM-based estimation. Similar to LLM-based task planning (Singh et al., 2023; Ichter et al., 2022), LLMs can be used to approximate the reward of observation-action pairs in the dataset, providing more immediate and actionable dense feedback to enhance the effectiveness of offline learning.\nNot-grounded reward estimation. Intrinsic rewards estimated by LLMs at intermediate steps might not consistently align with the sparse rewards provided at the conclusion of individual instruction-following tasks. This discrepancy arises when the intrinsic rewards are not sufficiently grounded in the environment domain. This issue is exacerbated in a partially observable setting, where LLMs are forced to infer rewards based on incomplete snapshots of the environment."}, {"title": "3.1 Overall Framework", "content": "To tackle the limitations of LLM-based reward estimation, we propose a spatio-temporal consistency-guided reward ensemble framework COREN with a two-stage process. As described in Figure 2, the first stage (i) incorporates contextual, structural, and temporal consistencies to fully harness the LLM's reasoning ability and enhance the groundedness of reward estimates within the specific domain of the embodied environment. In the second stage (ii), COREN orchestrates an ensemble of distinct rewards generated during the first stage based on the trajectories' success. This allows for the derivation of domain-specifically tuned rewards, which can be effectively utilized for the offline learning of embodied agents."}, {"title": "3.2 Spatio-Temporally Consistent Rewards", "content": "For reward estimation, we employ $N$ distinct prompts $P_1, \\ldots, P_N$ with an LLM ($\\Phi_{\\text{LLM}}$), where a prompt is distinguished by its unique explanations, in-context demonstrations, as well as the use of a chain-of-thought (CoT). Specifically, each prompt $P_n$ combined with observation $o$, action $l$, and instruction $i$ is used to generate rewards $R_n$ through $\\Phi_{\\text{LLM}}$ inferences.\n$R_n(o, l|i) = \\Phi_{\\text{LLM}}(P_n, (o, l, i))$  (2)\nSpatial consistency is intended to ensure that the domain-grounded LLM's reward estimation remains consistent across different prompt-induced contexts as well as it is based on a comprehensive understanding of the environmental structure. We achieve this using the implementation of two consistency mechanisms.\nContextual consistency. This mechanism aims to mitigate biases stemming from specific prompt contexts used in LLM-based reward estimation. By employing multiple $N$ prompts, each with a different contextual frame, we ensure that the rewards, which remain consistent across these variations, reflect a consensus in reasoning. For contextually consistent rewards $r_c$, we integrate the responses $R^n(o, l|i)$ of prompts $P_n$ by\n$r^c(o, l|i) = \\underset{r \\in \\mathbb{R}}{\\operatorname{argmax}} \\sum_{n=1}^N \\mathbb{1}(R^n(o, l|i) = r)$  (3)\nwhere $R^n(o, l|i) = \\Phi_{\\text{LLM}}(P_n, (o, l, i))$.\nStructural consistency. This is intended to ensure that the reward estimation incorporates a comprehensive understanding of the environment physical structure, such as objects, their relationships, and their relevance to the given instruction. We inquire $\\Phi_{\\text{LLM}}$ with MDP-specific queries $q(o)$ relevant to observation $o$ such as \u201cWhich objects in $o$ are relevant to the instruction $i$?", "P_n$": "n$r^s(o, l|i) = \\underset{r \\in \\mathbb{R}}{\\operatorname{argmax}} \\sum_{n=1}^N \\mathbb{1}(R^n(o, l|i) = r)$  (4)\nWe rewrite Eq. (2) for query violation cases, obtaining $R^n(o, l|i)$\n$R^n(o, l|i) = \\begin{cases}\n\\O & a(o) \\ne \\Phi_{\\text{LLM}}(P_n, q(o)) \\\\\n\\Phi_{\\text{LLM}}(P_n, (o, l, i)) & \\text{otherwise}.\n\\end{cases}$  (5)\nDetails of prompts $P_n$ and the dataset construction for MDP-specific queries and answers $D_{QA} = \\{(q(o), \\alpha(o)) : o \\in \\tau \\in D\\}$ are in Appendix.\nTemporal consistency. This is designed to ensure that the value assigned to an action remains coherent throughout its whole decision-making process. With temporal consistency, if forward reasoning by the LLM assesses certain actions as having high values, backward verification must confirm that these high-value actions can collectively accomplish the given instruction.\nTo achieve this backward verification, we inquire $\\Phi_{\\text{LLM}}$ with the query $q(i, \\tau, n)$: \u201cIs performing the high-value actions $H_n(\\tau)$ from observation $o$ reasonable to accomplish the instruction $i$?", "l|i)$": "n$R^n(o, l|i) = \\begin{cases}\n\\O & l \\in H_n(\\tau) \\land \\neg \\Phi_{\\text{LLM}}(q(i, \\tau, n)) \\\\\n\\Phi_{\\text{LLM}}(P_n, (o, l, i)) & \\text{otherwise}\n\\end{cases}$  (6)\nfor the cases of query violation, i.e., $l \\in H_n(\\tau) \\land \\neg \\Phi_{\\text{LLM}}(q(i, \\tau, n))$. Here, for all trajectory observations $o \\in \\tau$, high-value actions are defined as\n$H_n(t) = \\{\\underset{l}{\\operatorname{argmax}} \\Phi_{\\text{LLM}}(P_n, (o, l, i))\\}$.  (7)\nGiven $N$ prompts, we then integrate the rewards in Eq. (6) from each by employing the majority voting to establish temporally consistent rewards.\n$r^T(o, l|i) = \\underset{r \\in \\mathbb{R}}{\\operatorname{argmax}} \\sum_{n=1}^N \\mathbb{1}(R^n(o, l|i) = r)$  (8)"}, {"title": "3.3 A Domain-Grounded Reward Ensemble", "content": "From the spatio-temporally consistent rewards $r^C$, $r^S$, and $r^T$ calculated above, we derive domain-grounded rewards through their ensemble based on the alignment with given offline trajectories. We model unified rewards as\n$\\begin{aligned} &\\mathbf{r}(o, l|i) = (r^C(o, l|i), r^S(o, l|i), r^T(o, l|i)) \\\\ &\\mathbf{w}(o, l|i) = (w^C(o, l|i), w^S(o, l|i), w^T(o, l|i)) \\\\ &\\hat{r}(o, l|i) = (\\mathbf{r}(o, l|i), \\mathbf{w}(o, l|i)) \\end{aligned}$  (9)\nwhere $(\\cdot, \\cdot)$ is an inner product and $w^C$, $w^S$ and $w^T$ are learnable weights. These $w$ are generated by the reward orchestrator $\\Psi_\\theta$. It takes observation $o$, action $l$, and instruction $i$ as input, producing a softmax distribution for $w$. The orchestrator $\\Psi$ is used to align the predicted return of a trajectory with the labeled return, i.e., the sparse reward $f_s(i, \\tau)$:\n$w(o_t, l_t | i) = \\Psi_\\theta(o_t, l_t, i)$  (10)\nwhere $\\alpha$ is a hyperparameter.\nFinally, using the augmented trajectory dataset that contains unified rewards $\\hat{r}$ in Eq. (9), an agent can be trained via offline RL algorithms such as CQL (Kumar et al., 2020b). The two-staged reward estimation in COREN is outlined in Algorithm 1."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experiment Settings", "content": "Environment and dataset. For evaluation, we use VirtualHome (VH) (Puig et al., 2018), a widely used realistic benchmark for household activities. VH features a diverse array of interactive objects (e.g., apples, couch) and basic behaviors (e.g., grasp, sit), enabling us to define 58 distinct actions for embodied agents. We use 25 distinct tasks including activities such as sitting on a couch with several fruits, microwaving salmon, and organizing the bathroom counter. To construct a training dataset D for offline RL, we begin with a single expert trajectory for each of these 25 tasks. We then augment each with random actions at intermediate steps that lead to failed trials. For each expert trajectory, a sparse reward of 1 is annotated to indicate success, while for sampled failed trajectories, a sparse reward of 0 is annotated to denote failure. This follows Goal-POMDP representations used in long-horizon instruction-following tasks.\nEvaluation instruction. We employ two distinct instruction types to assess the agent's ability to handle different goal representations. A Fine-grained instruction type provides a detailed task description, often including specific actions performed to achieve certain goal conditions pertinent to the instruction-following task. An Abstract instruction type provides a more abbreviated and generalized task description, focusing on broader objectives without detailing each action. Each of the 25 tasks is assessed using 5 fine-grained and 5 abstract instructions, resulting in a total of 250 distinct instructions being tested. These instructions have not been included within the offline training dataset.\nEvaluation metrics. We use three metrics, consistent with previous works (Singh et al., 2023; Song et al., 2023b). SR measures the percentage of tasks successfully completed, defining success as the completion of all goal conditions for a task; CGC measures the percentage of completed goal conditions; Plan measures the percentage of the action sequence that continuously matches with the ground-truth sequence from the beginning."}, {"title": "4.2 Main Results", "content": "Instruction-following task performance. Table 1 presents a performance comparison of our COREN and the baselines from different categories, including RL agents, LLM-based agents, sLM-based agents across metrics such as SR, CGC, and Plan.\n*   COREN outperforms all the RL agent baselines by a significant margin, achieving average gains of 20.0%, 15.2%, and 5.6% over the most competitive RL agent baseline Self-Consistency in SR, CGC, and Plan, respectively.\n*   Furthermore, the performance of COREN is on par with the LLM-based agents, with only a slight performance drop compared to SayCan-Gemini and ProgPrompt-Gemini, while it surpasses the other LLM-based agents (i.e., all with LLaMA3 and LLM-Planner-Gemini). These results are especially noteworthy, considering the significantly different model sizes between COREN (GPT2-based-117M) and other LLM-based agents (i.e., Gemini, LLaMA-8B). These demonstrate COREN's ability to learn long-horizon instruction-following tasks within specific domains using minimal domain-specific knowledge, such as partially annotated rewards.\n*   We observe that the sLM-based agents using 4-bit quantized LLaMA3 (LLaMA3Q) and GPT2 exhibit lower performance than the others, including our COREN, due to their dependency on the limited reasoning capabilities of sLMs.\n*   Additionally, COREN demonstrates relatively robust performance across different instruction types compared to LLM-based agents. This can be attributed to COREN's ability to learn from a broad range of semantically similar instructions, which are generated by the LLM and included in the offline dataset. This enables the framework to better generalize to abstract instructions.\nCross-domain performance. Here, we extend our evaluation scenarios to include domain shifts in the environment; i.e., the locations of key objects related to the given instructions differ from those in the training dataset. Specifically, we sample a subset of trajectories from the training dataset D and relabel their sparse rewards $f_s(i, \\tau)$ to reflect the altered object locations. While keeping the spatio-temporally consistent rewards unchanged, we then retrain the reward orchestrator in Eq. (10) using these newly labeled sparse rewards. This approach facilitates the generation of domain-specific unified rewards for RL without the need to recalculate the consistency-based rewards themselves through LLM inferences. We also incorporate this newly labeled dataset for the LM agent category. For instance, LLM-Planner adapts to this new environment domain by using the trajectories, which are relabeled as success, as demonstrations for task planning. Since other RL agent baselines, except GCRL, lack the ability to utilize domain information represented as sparse rewards, they are evaluated with the same policy as in the single-domain experiments.\n*   For this cross-domain assessment, as shown in Table 2, COREN outperforms all the RL agent baselines, showing a minimal drop compared to the results in the single-domain experiments (in Table 1). Upon domain shifts, COREN's two-staged process adjusts the reward estimates to align with the target domain by the second stage conducting the adaptive ensemble in Eq. (10). In contrast, the RL agent baselines, which rely solely on the rewards derived from the LLM's commonsense reasoning, exhibit a diminished ability to adapt to specific domains, showing large drops compared to the results in the single-domain experiments.\n*   We also observe that the LLM-based agent baselines experience large degradation in this cross-domain assessment; e.g., LLM-Planner relies on the LLM's knowledge, which is difficult to ground in a specific environment using only a few examples, leading to suboptimal performance."}, {"title": "4.3 Ablation Studies", "content": "Spatio-temporally consistent rewards. To verify that the contextual, structural, and temporal consistencies (in Section 3.2) effectively complement each other in LLM-based reward estimation, we test different combinations of these consistencies in the ensemble of rewards. Table 3 demonstrates that COREN, which utilizes all three, consistently outperforms the others. This specifies that the combination of $w$ and rewards derived from partial consistencies alone is limited in generating unified rewards that significantly benefit RL, while the ensemble weights $w$ can be adjusted via Eq. (10).\nDifferent LLMs for reward estimation. To implement COREN, which uses an LLM for offline reward estimation, we test a variety of LLMs ranging from open-source LLaMA3-8B to proprietary models GPT4 turbo, Gemini 1.0 Pro, and PaLM. In Table 4, we observe that LLaMA3-8B, which has significantly fewer parameters, does not achieve performance comparable to the proprietary models. Among the proprietary models, the more recent and advanced capable LLMs, such as GPT4 turbo and Gemini 1.0 Pro, demonstrate a strong ability in reward estimation that positively impacts agent offline learning."}, {"title": "5 Related Works", "content": "LLMs for embodied environments. Leveraging LLMs as an instruction-following agent in embodied environments becomes a bedrock, capitalizing on LLM's reasoning capabilities (Hu et al., 2023; Singh et al., 2023; Yang et al., 2023; Pantazopoulos et al., 2023; Yun et al., 2023). To overcome the limitation of LLMs' insufficient knowledge about specific domain conditions of the environment, prior works incorporate domain-related information. (Ichter et al., 2022) utilizes an offline dataset to learn the value of actions, which is later combined with the LLM's token generation probability to calibrate the LLM's decision for different domains. (Song et al., 2023a) employs an expert dataset as a knowledge base for retrieval-augmented task planning. Unlike those directly employing LLMs as agent policies and requiring online LLM inferences, our study focuses on leveraging LLMs for reward estimation in offline RL, thus allowing for efficient agent structures.\nLLMs for reward design. In RL, reward engineering is a long-standing challenge, traditionally tackled through manual trial-and-error or by leveraging domain knowledge from human experts. Inverse RL, on the other hand, aims to infer the underlying reward function from reward-free expert demonstrations (Hadfield-Menell et al., 2016; Klein et al., 2012). With the advent of capable foundation models, recent works have exploited them to produce reward functions (Wang et al., 2024; Du et al., 2023; Rocamonde et al., 2023; Baumli et al., 2023). (Kwon et al., 2023) harnesses the in-context learning of LLMs to evaluate the episodes of high-level tasks. (Ma et al., 2023) leverages the code generation ability of LLMs, given environmental programming code, producing multiple code-based reward functions to train RL agents online and enhance them via feedback from agent training statistics. Our COREN framework also leverages LLMs for reward design; however, the framework distinguishes itself by focusing on generating domain-grounded rewards without direct interaction with the environment, particularly in scenarios where the available information about the embodied environment is limited to sparse rewards."}, {"title": "6 Conclusion", "content": "We presented the reward ensemble framework COREN to achieve robust LLM-based reward estimation for offline RL, specifically tailored for embodied instruction-following tasks. The framework utilizes a spatio-temporal consistency-guided ensemble method for reward estimation. It generates multiple stepwise rewards on offline trajectories, with each reward focusing on a specific consistency related to contextual, structural, or temporal aspects, and then it integrates the multiple rewards into more domain-grounded ones via the sparse reward-aligned ensemble. As this work is the first to adopt LLMs for offline learning of embodied agents, we hope it can provide valuable insights into the development of LLM-driven training acceleration techniques. This is particularly significant for embodied agents involved in long-horizon instruction-following tasks, which are typically constrained by sparse reward signals."}, {"title": "7 Limitations", "content": "Despite the robust performance achieved by COREN, we identify that its success heavily depends on the capabilities of LLMs engaging in reward estimation, as shown by the ablation study in Table 4. Our LLM-based reward estimation is conducted in an offline manner, i.e., without direct interaction with the environment. However, the dependency on the capabilities of an LLM can be problematic, especially when the target environment domain significantly differs from the pre-trained knowledge of the LLM and the domain changes continuously over time after agent deployment. In these cases involving dynamic Goal-POMDP environments, the agent policy learned offline by the dense rewards on the training dataset can degrade in terms of its task performance. The benefits of our ensemble method with the notion of spatio-temporal consistency are attributed to the effective alignment with the training dataset, and they can be limited in such non-stationary environment conditions. We leave the exploration of methods to address this limitation as a direction for future work."}, {"title": "8 Acknowledgements", "content": "We would like to thank anonymous reviewers for their valuable comments and suggestions. This work was supported by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (RS-2022-II221045 (2022-0-01045), RS-2022-II220043 (2022-0-00043), RS-2019-II190421 (2019-0-00421)), by the IITP-ITRC(Information Technology Research Center)(IITP-2024-RS-2024-00437633, 10%) grant funded by MSIT, by the National Research Foundation of Korea (NRF) grant funded by MSIT (No. RS-2023-00213118), by BK21 FOUR Project (S-2024-0580-000), and by Samsung Electronics."}, {"title": "A Experiment Settings", "content": ""}, {"title": "A.1 Environment", "content": "We utilize VirtualHome (Puig et al., 2018), an environment and benchmark designed for simulating embodied household tasks. In this environment, actions related to household task activities are established by combining available manipulation behaviors and objects. These actions are executed sequentially to perform complex household tasks. COREN employs a configuration consisting of a house with 4 rooms, utilizing a total of 58 different actions. The actions are derived from the combinations of 8 distinct manipulation behaviors (find, grab, open, close, sit, put, put in, switch on) with various objects present within the environment.\nSingle domain evaluation. For single domain experiments in Table 1, we evaluate each of 25 distinct tasks using a total of 10 instructions per task. These instructions are divided into two categories: 5 fine-grained instructions, which provide detailed descriptions of the task, and 5 abstract instructions, which offer a more general overview. Detailed examples of tasks used are presented in Table 16.\nCross domain evaluation. In the cross-domain setting, we assess tasks within an environment with altered object locations (e.g., relocating an apple from a desk to inside a refrigerator), as described in Table 18. We evaluate a total of 8 tasks from 25 tasks in the single domain evaluation, each with 5 instructions. This is due to the fact that several objects are unable to be relocated in a new layout. Similar to the single domain, each task is assessed with both fine-grained and abstract instructions, totaling 6 instructions per task. Detailed examples of tasks used in the cross-domain evaluation are presented in Table 17."}, {"title": "A.2 Offline Dataset", "content": "To construct a training dataset D for offline RL, we use a single expert trajectory for each of the 25 distinct tasks. Each expert trajectory is augmented with random actions at intermediate steps that lead to failed trials. This process yields a total of approximately 8,000 trajectories for the offline dataset D. For each expert trajectory, a sparse reward of 1 is annotated to indicate its success, while for each sampled failed trajectory, a sparse reward of 0 is annotated to denote its failure. Overall, we utilize one successful and one failed trajectory to establish the sparse rewards."}, {"title": "B Implementation", "content": ""}, {"title": "B.1 COREN Implementation", "content": "In this section, we present the implementation details of our COREN and baselines.\nWe implement our framework using Python v3.9.19 and the automatic gradient framework Jax v0.4.7. The models are trained on a system with an NVIDIA RTX A6000 GPU. The implementation details of COREN include these parts: (i) LLM-based reward estimation, (ii) spatio-temporal consistency consideration for estimated rewards, (iii) domain-grounded reward ensemble, and (iv) offline RL."}, {"title": "B.1.1 LLM-based reward estimation", "content": "The LLM $\\Phi_{\\text{LLM}}$ takes the user instruction $l$, observation $o$, and action $a$ as inputs, along with a prompt $P$ so as to estimate the rewards for $a$ based on how they contribute to accomplishing $i$. We employ multiple $N$ prompts $P_1, \\ldots, P_N$, which differ in their description methods for the reward estimation task, incorporation of in-context demonstrations, or use of chain-of-thought (CoT) prompts. Specifically, we use 5 different types of prompts to create effective rewards: A naive prompt that includes the explanation of reward estimation tasks and required format, three in-context Learning (ICL) prompts that include distinct demonstrations, and a CoT prompt that includes the human-written reasoning path of reward estimation. Each prompt contains the rubric for the reward estimation, including which actions should receive which rewards. For example, a reward of 2 is given for an action that should follow, given the previously completed actions, and a reward of -1 is given for an action that involves searching for objects not related to the task. The prompt examples are provided in Table 19, 20, 21, and 22.\nIn conjunction with the aforementioned prompts, we employ several LLMs: LLama-8B, Gemini 1.0 Pro, PaLM, and GPT4 Turbo. For GPT4 Turbo, the temperature of 0.5 is used, while the other models are set to the temperature of 0.7. The temperature setting is based on the characteristics of each model and aims to balance the trade-off between exploration and exploitation during the reward generation process. Table 6 specifies the LLMs used, their respective model sizes, and the temperature hyperparameters used to conduct the ablation study in Table 4."}, {"title": "B.1.2 Spatio-Temporal Consistency", "content": "Here", "a(o)": "o \\in \\tau \\in D\\"}, ".", "The QA dataset consists of queries $q(o)$ that are easier to answer than the reasoning task of estimating rewards for actions, requiring only observation and instruction. By evaluating the correctness of the responses to these queries, we determine whether the reward estimation has been carried out while properly considering the internal structure of the environment.\nTo create the answers for the MDP-specific dataset $D_{QA}$, we employ GPT4 and use the queries that focus on identifying the objects that play a crucial role in achieving the given instruction. Through this process, we generate a total of 139 QA-pairs. Table 7 shows the examples of QA-pairs.\nGiven observation $o$, $\\Phi_{\\text{LLM}}$ takes a query $q(o')$ along with a prompt $P_n$ as input and generates a response $\\Phi_{\\text{LLM}}(P_n, q(o'))$. Here, $q(o')$ is chosen based on the sentence embedding similarity between $o$ and $o'$ using the sentence transformer model (Reimers and Gurevych, 2019). We integrate the query $q(o)$ into the prompt $P_n$ by directly appending it at the end of the prompt. Table 19 shows the examples. To determine how well the response aligns with the actual answer, we utilize a similarity-based evaluator $E$. Specifically, if the sentence embedding similarity between the response and the ground truth answer $a(o')$ is below a threshold of 0.5, the response is considered incorrect.\nTemporal consistency Temporal consistency involves calculating the sequence of high-value actions $H_n(\\tau)$ for each prompt $P_n$:\n$H_n(\\tau) = \\{\\underset{l}{\\"]}