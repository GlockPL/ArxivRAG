{"title": "Safety Monitoring of Machine Learning Perception Functions: a Survey", "authors": ["Raul Sena Ferreira", "Joris Gu\u00e9rin", "Kevin Delmas", "J\u00e9r\u00e9mie Guiochet", "H\u00e9l\u00e8ne Waeselynck"], "abstract": "Machine Learning (ML) models, such as deep neural networks, are widely applied in autonomous systems to perform complex perception tasks. New dependability challenges arise when ML predictions are used in safety-critical applications, like autonomous cars and surgical robots. Thus, the use of fault tolerance mechanisms, such as safety monitors, is essential to ensure the safe behavior of the system despite the occurrence of faults. This paper presents an extensive literature review on safety monitoring of perception functions using ML in a safety-critical context. In this review, we structure the existing literature to highlight key factors to consider when designing such monitors: threat identification, requirements elicitation, detection of failure, reaction, and evaluation. We also highlight the ongoing challenges associated with safety monitoring and suggest directions for future research.", "sections": [{"title": "INTRODUCTION", "content": "Recent advances in Machine Learning (ML) have allowed autonomous systems to leave the safe environment of research labs to perform complex tasks, where failures can have catastrophic consequences. Examples of such safety-critical systems include self-driving cars, 1 surgical robots,2 and unmanned aerial vehicles in urban environments.3 These autonomous systems frequently use large ML models like neural networks for complex sensor signal interpretation, i.e., perception, or decision-making, i.e., control.5 This paper focuses on safety mechanisms for critical physical systems relying on ML to process sensor signals.\nIn various autonomous system applications, essential perception tasks can only be solved using ML. For example, in highly uncontrolled settings, such as self-driving cars6 or UAV emergency landing,7 deep neural networks must be used to detect pedestrians in RGB images. This information cannot be obtained from other approaches and is crucial to guarantee the system's safety. Despite the great success of modern ML-based perception, it introduces new dependability challenges: 8,9,10 1. The lack of well-defined specification: ML models are learned from examples instead of manually coded, making their operational boundaries elusive, and preventing formal safety guarantees. 2. The black-box nature of the models: traceability and transparency of ML predictions is difficult. 3. The high-dimensionality of data: validation of the complete operational design domain is impossible. 4. The over-confidence of neural networks: output scores cannot be used as is to detect failures since a model can deliver wrong outputs with high confidence.11 Hence, conventional offline safety measures, like fault prevention, removal, and forecasting 12 are often not sufficient to ensure safety and to certify these systems. Online fault tolerance mechanisms, such as Safety Monitors (SM), emerge as a promising alternative to improve safety in critical systems relying on ML perception.\nThis paper focuses on SMs, which aims to keep the system in an acceptable state during operation, despite faults or adverse scenarios. 13 Safety monitors are mentioned in the literature under various terms, including safety kernels, 14 safety managers, autonomous safety systems, 16 checkers, 17 guardian agents, 18 safety bags, 19 or emergency layers. 20"}, {"title": "RELATED WORK", "content": "Recent literature about ML safety has flourished, leading to several surveys about the different facets of this domain. This section aims to delineate the contributions of these works and articulate the unique space our study occupies.\nIn their survey, Xu and Saleh22 categorize the existing research on the use of ML to improve safety across various applications. Similar surveys have been proposed in domain-specific contexts like autonomous driving23 and architecture.24 However, these surveys primarily emphasize the benefits ML brings to safety initiatives, sidestepping the crucial aspect of ensuring the safety of the ML components themselves, which is the focus of our study.\nThe overview proposed by Faria is closer to our objective as it presents the challenges posed by ML models in a safety-relevant context and describes potential solutions. However, it does not present the literature for safety monitoring, a critical pillar for ML safety that our survey delves deep into.\nMohseni et al. 10 also undertook an exhaustive literature review on the safety implications of ML, within the context of autonomous vehicles. Their spectrum of safety mechanisms is composed of three categories: inherently safe design, safety margins, and fail-safe mechanisms, i.e. safety monitors. Regarding SM, they classify detectors by their targeted error types (uncertainty estimation, in-distribution error detection, and out-of-distribution error detection). Likewise, Pandharipande et al. 25 also discusses safety for ML-based automotive perception. For monitoring, they mostly discuss techniques to detect inputs that are outside of the expected operational design domain.\nVarshney and Alemzadeh8 consider ML safety through the lens of risk, epistemic uncertainty, and resultant harm. They discuss four categories of methods to achieve ML safety, including fail-safe strategies, echoing our emphasis on runtime monitoring. Their discussion on monitoring includes classification with rejection and uncertainty estimation techniques. They present several practical examples, but their discussion is mostly focused on the actions that are taken after ML failure has been detected, while not fully covering detection mechanisms. Recently, Mohseni et al. 26 also proposed a taxonomy of dependability techniques for ML design and deployment. Starting from engineering safety requirements, they present safety-related ML research into three categories: inherently safe design, enhancing model performance and robustness, and runtime error detection. However, regarding the specific subfield of ML monitoring, which is the focus of our work, they mostly discuss detection mechanisms (uncertainty estimation, outlier detection, and adversarial attacks detection).\nRahman et al. 27 journeyed closer to our thematic, presenting a taxonomy for runtime monitoring of ML robotic perception. They organize existing approaches to detect runtime failures into three categories: approaches using past examples of failures to predict future ones, approaches detecting inconsistencies in the perception outputs, and uncertainty estimation approaches. Similarly, Zhang et al. 28 discussed detection approaches, classifying them based on their primary objectives: failure rejection, unknown rejection, and fake rejection.\nThe surveys presented above are constructed in a bottom-up fashion: they first identify existing works and organize them into relevant categories. This methodology leads to discussions about ML monitoring that are incomplete, as most of the existing literature deals with the detection of unsafe predictions. Traditional safety engineering, however, recognizes error detection"}, {"title": "WHAT THREATS ARE BEING ADDRESSED BY SAFETY MONITORS?", "content": "The identification of potential threats is pivotal when designing a dependable SM. Should the safety analysis reveal that a threat is likely to occur for a specific application, this should be reflected in the monitor's evaluation protocol. Different kinds of threats can affect the ML-based perception functions. Offline threats, such as poor feature engineering, 30 label noise,31 inadequate ML testing, 32 or bad model maintenance,33 emerge during the ML model development phase. While it is crucial to adhere to robust software engineering practices during SM development, this section mostly focuses on runtime threats, which occur during live operations.\nThe objective of an SM is to detect unsafe ML predictions and implement corrective actions to prevent catastrophic outcomes. Such hazardous predictions can arise from different types of input data, which we term runtime threats. This section presents a taxonomy of runtime threats for perception functions, inspired by existing literature. 34,35,36,37,38,39 Adverse input data are classified as threats if they can be detected or mitigated using similar approaches. Throughout this section, we present runtime threats and discuss their specificities concerning detection, reaction, and evaluation.\nBefore diving into the different threats, we wish to emphasize that, in our recent work, 29 we challenged the prevailing understanding of runtime threats in the context of monitoring. A significant portion of NN monitoring research centers on Out-Of-Distribution (OOD) detection, emphasizing the monitor's role in flagging data diverging from the training distribution. However, we highlighted the inherent ambiguity in defining \u201cOODness\u201d: not all OOD data are inherently dangerous and not all in-distribution data are safe. We advocate for evaluating monitors based on their proficiency in detecting incorrect predictions. Nonetheless, the notion of threats delineated in this section remains crucial for SM design and evaluation. Given that a monitor's efficacy in error detection might fluctuate across diverse threats, it's paramount that every potential threat\u2014identified through a rigorous safety analysis is considered during the monitor's testing phase."}, {"title": "In-distribution errors", "content": "Modern deep-learning architectures have achieved impressive results in many perception tasks. According to the latest leader board on papers with code+, the top-performing model for semantic segmentation on Cityscapes40 has a mean intersection over the union of 84.5%,41 the best model for image classification on ImageNet42 has a top-1 accuracy close to 91%, and the leader for object detection on COCO43 has a mean average precision around 63%.44 These benchmarks were established on the test splits of these datasets, assumed to derive from the same distribution as the training data, denoted as In-Distribution (ID) data. While these results are excellent and allow researchers to build useful applications, even the best computer vision models are not flawless. To guarantee the system's safety, an SM should be able to handle these errors.\nBeyond this fundamental model generalization issue, there is another problem: the data incompleteness. Rare conditions tend to be underrepresented since the training data only account for a small subset of all real-world possibilities.45 Such data, presenting different characteristics than the training data, are considered Out-Of-Distribution (OOD)46 and are discussed as different types of threats in the coming sections. There is no unified naming convention for the threats presented hereafter in the literature, but we strive to propose clear definitions to avoid ambiguity."}, {"title": "Novelty threats", "content": "A new input data encountered at runtime is considered \u201cnovel\" when its category/label does not refer to any of the predefined categories known by the model.46 For example, Blum et al. 47 studied the problem of semantic segmentation of a driving scene and trained their model on the Cityscapes dataset. 40 At runtime, when a dog crosses the road, its corresponding pixels are considered novelty as they do not belong to the predefined set of classes of Cityscapes. Hence, when a novelty input is presented to an ML perception model, it cannot return a correct answer.\nThe above example shows that facing novelty inputs is common for autonomous systems evolving in unstructured environments. Hence, it is crucial to equip ML perception models with defensive mechanisms against this runtime threat. A typical strategy consists of building classification models with rejection,48 with the ability to reject uncertain predictions such as objects outside the network scope. Regarding the recovery after detecting novelty threats, the actions implemented should not rely on the possibility of obtaining a better estimate of the correct prediction. Concrete approaches to detect novelty threats, recover from them, and evaluate the ability of an SM to address them are discussed in Sections 5, 6, and Section 7, respectively."}, {"title": "Distributional shift threats", "content": "A distributional shift occurs when the marginal distribution of the runtime input data is different from the training distribution, while the label generation mechanism remains unchanged. 38 Regarding safety monitoring, we distinguish two types of shifts: covariate and semantic."}, {"title": "Covariate shift", "content": "A covariate shift is a condition that decreases the ML performance through time in dynamic environments. 49 In other words, covariate shift threats are new data presenting different characteristics in their composition but for which the semantic content is not different from training. For images, such threats are also called corruptions or perturbations and were presented and discussed extensively by Hendricks and Dietterich.50 These deteriorated data can come from:\n\u2022 Failure in exteroceptive sensors. These perturbations come from hardware defects and include various errors such as pixel traps, shifted pixels, and Gaussian noise. There are specific approaches to identify sensor faults,51 which can be addressed by tuning the sensor parameters 52 or having a backup sensor system.53\n\u2022 Changes in external conditions. For autonomous systems evolving in unstructured environments, the training data cannot cover all possible real-world conditions. For example, outdoor perception functions should work for different kinds of weather (e.g., snow, fog) and lighting conditions (e.g., night, sunset). As illustrated in the disengagement reports by major companies, such external factors influence the perception performance and can reduce the safety of autonomous vehicles. 54\nTo deal with these two covariate shift types, both traditional signal processing55 and modern deep learning approaches56 have been used to detect and reduce data noise. However, the techniques used against covariate shift threats depend highly on the amount of noise in the data."}, {"title": "Semantic shift", "content": "A semantic shift threat refers to images showcasing objects that:\n\u2022 presents different attributes than known members of this category, such as a pedestrian detector trained in summer encountering individuals wearing winter clothes, 57\n\u2022 displays uncommon interactions between known classes and their surroundings, like an overturned truck. 58\nIn contrast to novelty threats, semantic shift only includes data where the objects align with the model's predefined categories. While one might see a semantic shift as a particular case of covariate shift, its distinct challenges in safety monitoring set it apart. Notably, semantic shifts cannot be handled with denoising or backup sensors. Instead, some studies have explored the detection"}, {"title": "Adversarial threats", "content": "An adversarial input is an intentional modification of in-distribution data to make ML models fail with high confidence. 60,61 In real-world scenarios, these malicious attacks can be made by applying modifications on targeted physical objects such as painting black lines on the road to force the ML model to interpret it as a road lane. 62\nAdversarial threats can lead to serious safety issues if applied against the perception functions of critical systems. Therefore, they should be handled by specific SMs as they are likely to fool generic monitoring approaches. However, specific hardening approaches, such as gradient hiding and defensive distillation, have been developed to identify them or increase model robustness. 35"}, {"title": "HOW TO DERIVE SAFETY MONITORS FROM SAFETY OBJECTIVES?", "content": "Safety monitoring guarantees that some safety properties are not violated, despite potential faults occurring in the main system. The elicitation and modeling of these properties are essential steps in designing safety monitors. For instance, Machin et al. 13 used a HAZOP-UML hazard analysis 63 to identify high-level safety objectives expressed in natural language. These high-level objectives are then converted to low-level safety requirements, expressed formally in the system's state space, and observable by the monitor. For a mobile robotic platform in a standard industrial setting, an example of a high-level safety objective is \"the robot platform must not collide with a human\". A low-level safety requirement can be derived by comparing the braking distance with the distance of any obstacle sensed by a laser. In this example, the low-level requirements are easy to express and implement since the sensor signal can be interpreted in terms of the high-level requirement.\nThe high-level safety objectives can still be identified using standard hazard analysis tools for complex systems involving machine learning perception. However, converting them into low-level monitoring requirements is not straightforward. Indeed, expressing and implementing a high-level requirement in raw sensors can result in solutions that are too conservative or even infeasible to be deployed at runtime. For example, if we consider an emergency braking system (EBS) implemented in an autonomous vehicle:\n\u2022 Using simple sensor signals such as a laser is not enough to capture the semantic information required to distinguish between pedestrians and other moving vehicles. Such semantic information is crucial for EBS to perform two very different low-level requirements: to stop the ego vehicle when the EBS identifies an object as a pedestrian, or slightly decelerate the ego vehicle when the EBS identifies an object as a moving vehicle. Therefore, stopping the car for all sensed objects is too conservative, which would significantly alter the availability of the system.\n\u2022 Using complex sensor signals such as RGB image pixels from camera sensors is not enough to guarantee that a high-level objective is not violated. That is, measuring the pixels alone is infeasible to perform the EBS task since such raw RGB values cannot give insightful information for the EBS to perform a high-level requirement such as avoiding a collision.\nHence, we should specifically monitor the ML function responsible for localizing pedestrians. In other words, the system-level safety objectives should be expressed as variables related to the ML model (input, activation, output).\nAs explained above, most current works on ML monitoring focus on detecting when a model is wrong and should not be trusted. This is a good generic formulation of the problem, agnostic of the system in which the model is embedded. However, we believe that using information from the application context to refine the low-level monitor requirements is a promising research direction. In particular, the hazard analysis of the system could be used to identify safety-critical regions of the ML model input/output space or to understand under which system configuration an ML error is hazardous. In addition, building monitors for specific sub-regions of the state space might allow us to come up with more effective local monitors and better allocation of resources. Although this lead has not yet been explored for ML monitoring, some research from ML safety could serve as a first step towards building better specific monitors. In their work, Dreossi et al. 64,65 propose to identify regions in the state space where a failure of the ML model results in a violation of a formal specification. For an autonomous vehicle use case, they show that errors"}, {"title": "WHICH DETECTION MECHANISMS CAN BE USED FOR SAFETY MONITORING?", "content": "Traditional monitors analyze both exteroceptive (e.g., distance) and proprioceptive (e.g., speed) sensors to detect safety threats. They apply simple rules on sensor data based on formal specifications. 13 However, for complex perception functions, these monitors struggle to interpret raw sensor signals like image pixels. This section discusses recent strategies to detect errors in ML model signals. Yet, even in ML-augmented autonomous systems, traditional monitoring remains indispensable to handle other sensors and ensure the system's proper functioning.\nDespite its significance, the specific field of ML safety monitoring has received limited research attention. Nonetheless, various ML techniques, hailing from sub-fields like uncertainty estimation, anomaly detection, ensemble methods, or multi-modal perception, show potential as SM detection mechanisms. This section delves into such prospective approaches, encompassing those not specifically designed for safety monitors.\nThis section presents a comprehensive taxonomy of detection mechanisms. Our categorization of detectors revolves around the manner in which they are assimilated into the complete system (Figure 3). Within each category, we outline the primary approaches and assess their advantages and limitations. However, it is important to mention that current research does not allow us to reach definitive conclusions regarding the efficacy of these techniques. Indeed, the literature currently presents a varied landscape with differing evaluation methods and conflicting experimental results. Notably, a study by Ferreira et al. 37 revealed unsatisfactory results for several evaluated methods, despite the good results presented in the original papers."}, {"title": "Internal mechanisms", "content": "Internal detection mechanisms are approaches where the ML model itself is trained to predict its failures. In other words, the NN is designed to return both predictions and information regarding the trust in these predictions. We classify internal mechanisms into three families of approaches."}, {"title": "Uncertainty estimation", "content": "Uncertainty estimation in deep learning has been widely studied recently. 111,112,113 Most deep learning models produce a single output value per input data. Uncertainty estimation approaches replace point estimate predictions with a probability distribution over the output space. These probabilities can then be used to evaluate the risk of trusting the prediction. For example, for deep learning classifiers, the outputs of the softmax layer define a probability distribution over the possible classes. However, the community has widely questioned using raw softmax output as an uncertainty proxy. Softmax is merely a normalization technique, not designed to represent meaningful probabilities, and thus often yields overconfident predictions. 114,78\nIn Bayesian Deep Learning, the weights of an NN are treated as random variables, and the objective is to learn their distributions from the training data. Then, using the Bayes rule, one can compute the distribution of the predictions. Traditional Bayesian statistics 115,116 is a natural way to reason about uncertainty in predictive models but comes with a prohibitive computational cost to be used in practice. Recently, various approaches have been proposed to compute approximate Bayesian inference on large ML models, including Variational Inference, 117,118,119,120 Laplace approximation and sampling methods. 121 For a detailed review of Bayesian deep learning, we refer the reader to the following works. 122,111\nBayesian deep learning has been applied to various perception tasks related to autonomous systems, including object detection, 123 semantic segmentation, 124,67 end-to-end vehicle control68 or visual odometry. 69 Of the techniques available, those based on Monte-Carlo Dropout 11,125 are particularly popular due to their simplicity of implementation. This approach retains"}, {"title": "Incorporating domain knowledge", "content": "Domain knowledge can be leveraged to improve the training of ML models. It can be incorporated into the architecture and training process in the form of logical or numerical constraints. 126 For example, a pioneering work proposed to build an object detection model by training a hierarchy of classifiers using lexical-semantic networks to represent prior knowledge about inter-class relationships. 127 This architecture can be used to detect anomalies in the runtime predictions. Likewise, information about the relationship among different superpixels of an image is used in 70 to build a robust classification pipeline. The superpixel relationships are modeled using a graph neural network, which processes the image jointly with a convolutional neural network in the final architecture. This additional information in the model itself can be used to detect incoherence in the final predictions."}, {"title": "Learning with rejection", "content": "In the setup of selective classification \u2013 also called classification with rejection \u2013 input data can either be classified among one of the predefined categories or be rejected, i.e., the system produces no prediction. Such rejection mechanisms can actually be viewed as built-in safety monitors as they allow to reject uncertain predictions at runtime. This kind of approach has been presented as a promising way to control the confidence in the monitored model in critical autonomous driving scenarios. 10 These approaches are internal mechanisms as they consist of modifying the model and learning algorithm to account for rejection. In other words, the predictor and the rejection function are trained jointly and are part of a single unified model. Several approaches have been proposed to integrate rejection options to traditional ML models such as support vector machine, 128 K-nearest neighbors 129, and boosting.74 Recently, Geifman and El-Yaniv75 presented Selectivenet, a neural network architecture optimized to perform classification and rejection simultaneously. Several other approaches have been introduced recently for learning with rejection, as discussed in more detail in the recent survey by Hendrickx et al. 130\nUtilizing an integrated model for prediction and rejection has its advantages. When optimized together, these elements can boost performance for specific tasks, potentially reducing inherent biases due to synergistic training. However, the drawback of such integrated techniques is that any alteration to the monitor requires a complete model retraining, further taxing computational resources. We also believe that learning with rejection does not align well with today's machine learning landscape, where leveraging open-source models as a foundation is common. Indeed, such an approach prevents seamlessly integrating a monitor without the labor-intensive need for total retraining. Finally, while such unified models might diminish bias, there is a counter-argument in favor of diversifying the design. By separating the prediction and monitoring processes, a blend of expertise \u2013 spanning data scientists to safety experts can collaborate, fostering a multi-faceted, bias-resistant system. Designing a good rejection-enabled model requires specific expertise beyond basic machine learning knowledge, which might not always be easy to find."}, {"title": "External mechanisms", "content": "External detection mechanisms are independent components in charge of monitoring the behavior of an ML model during execution. As they are not directly tied to the ML model, they are not required to be trained jointly and can be developed later by specific safety teams. We identified different mechanisms in the literature that differ in their position in the ML perception pipeline. In particular, external detection mechanisms can monitor either the ML model inputs, internal representations, outputs, or even data from other sources."}, {"title": "Monitoring the DNN inputs", "content": "Some approaches predict failures of an ML perception model by monitoring its inputs, e.g., the raw images. These approaches are independent of the monitored ML model, as they characterize the expected operational conditions under which a neural network can be used and discard new abnormal input data before the perception function processes them. Next, we present the existing approaches."}, {"title": "Traditional approaches", "content": "Traditional signal-processing approaches can be used to identify an anomalous sensory input before it enters the ML model. 101 These approaches characterize some statistical patterns of \u201cnormal\u201d data (i.e., from the training set) and compare them to new"}, {"title": "Input reconstruction", "content": "Recent techniques have relied on unsupervised deep learning to identify anomalous images. They start by training an auto-encoder that learns a lower-dimensional latent representation of the images and how to reconstruct the original images from it. Then, at runtime, the auto-encoder can be used to decide if a new image is an outlier by comparing its reconstruction error to a fixed threshold defined during training. 104,105,106,131 Another approach consists of training an outlier detection model on the latent representation of the auto-encoder to predict the nonconformity of new inputs. 107 In practice, this family of approaches was used to identify unexpected conditions, such as weather changes, to anticipate the misbehavior of an autonomous vehicle within a simulation environment. 132 Finally, Feng and Easwaran 133 proposed to detect unusual movements in real-time by combining optical flow operation with representation learning via a variational auto-encoder.\nCompared to conventional techniques, these methods have a higher likelihood of detecting semantic shifts in data, given their reliance on deep representation learning from the training dataset. However, two neural networks trained on the same dataset may not necessarily learn identical features. 134 Relying on such an auto-encoder could introduce an additional bias if its generalization diverges from that of the model being monitored."}, {"title": "Introspection", "content": "A well-explored technique in the robotics domain aims at predicting future failures at runtime. For instance, Gurau et al. 135 proposed two models that predict perception performance from observations gathered over time. Then, the monitor can switch control to a human operator if the robot's perception system is predicted to underperform. Kuhn et al. 136 proposed an introspective approach to predict future disengagements of the car by learning from previous disengagement sequences. They monitor both input images and other state data from the car.\nRelative to other input monitoring methods, these techniques stand out as they strive to assimilate the underlying knowledge of the monitored ML. They learn a fresh data representation to identify unsafe input under the supervision of the model itself. However, it appears that these methods could be enhanced by accessing additional details about the monitored model, like its activations, which might aid in devising superior models for detecting unsafe input data."}, {"title": "Insights regarding input monitoring", "content": "Input monitoring techniques present the advantage of being independent of the monitored neural network, which facilitates the software engineering process. Yet, this very independence can be a double-edged sword, making it challenging to anticipate ML failures on specific inputs without direct model inspection. Such techniques are effective when the operational design domain (ODD) is clearly defined and the predictor is reliably error-free within its ODD. Otherwise, employing techniques tailored to the specific predictor is advantageous, as they are more likely to capture the model's true strengths and vulnerabilities."}, {"title": "Monitoring the DNN internal representations", "content": "Other detection approaches monitor the values from the ML model's hidden layers. The underlying principle is that the training data alone does not fully encapsulate the model's understanding and that crucial information is embedded within the model's weights. This can be justified intuitively by the fact that different models behave differently with new inputs, even when trained on the same dataset. 137,134 This section discusses approaches to monitor neural network activations at runtime."}, {"title": "Continuous layer values", "content": "Several works have proposed to detect unreliable predictions by analyzing the output values of certain layers for novel input data. Rahman et al. 90 trained a binary anomaly classifier on features extracted from a hidden layer of a neural network. Another recent work proposed truncating feature activations from the penultimate layer, before the classification head, to get better uncertainty estimates.91 Adopting a different methodology, Lukina et al. 92 employed a centroid-based clustering technique on the internal representations of a designated layer to characterize known inputs, leveraging the distance to cluster centers as an indicator to"}, {"title": "Binary layer activations", "content": "To reduce the memory usage of internal representation monitors, other works have proposed looking only at the binary activations of a given layer. As ReLU is one of the most popular activation functions in deep neural networks, one can inspect whether a new input is triggering an activation of a specific neuron or not, i.e., non-zero value. The advantage of considering such binary variables is that they can be stored easily using binary decision diagrams 93 or abstraction boxes. 94,95,96 Then, abnormal data are identified by comparing activation patterns encountered at runtime to those recorded during training."}, {"title": "Coherence between layers", "content": "Several works have explored the potential of simultaneously examining multiple hidden layers. Wang et al.97 introduced Dissector, a tool designed to ascertain if the outputs across different layers yield consistent decisions. Schorn and Gauerhof98 proposed an approach called FACER, which builds a feature vector capturing activations from various layers by summing the values of each feature map. Similarly, Lee et al. 99 fitted class conditional Gaussian distributions to both low-level and upper-level features of the deep learning model and defined a confidence score based on the Mahalanobis distance. Another innovative approach consists of tracking the model's internal representations backward to build a saliency map of a given input. 100 The patterns of this map are then compared with the ones obtained for the training set within the same category. Recently, Wang et al. 139 proposed ViM (Virtual-logit Matching), combining a feature-based class-agnostic score and a logits-based class-dependent score, which obtained very good results for detecting out-of-distribution data.\nLeveraging multiple hidden layers often proves advantageous, primarily because it alleviates the challenge associated with optimal monitoring layer selection. Yet, this approach compounds the issues related to computational time. As such, striking a balance between the number of layers to incorporate and the system's inherent constraints becomes pivotal."}, {"title": "Insights regarding internal layers monitoring", "content": "The activations within internal layers provide an invaluable insight into a model's comprehension of the data it processes, making them pivotal for crafting effective monitors. However, the challenge lies in pinpointing the right layer for monitoring to ensure optimal outcomes a decision far from straightforward. Marrying the ideal layer with the most suitable monitoring transformation is equally vital. Owing to these intricate selections, current results showcase significant variance. Different methodologies and layers excel for varying datasets, but the community has yet to converge on definitive guidelines on which techniques and layers best serve specific scenarios. Consequently, formulating an effective internal layer monitor remains a challenging endeavor."}, {"title": "Monitoring the DL outputs", "content": "Some detection approaches monitor the ML outputs. For example, for classification tasks, the output contains information about the target class and the model's confidence associated with this label."}, {"title": "Manipulation of softmax confidence", "content": "A straightforward strategy to monitor deep neural network outputs consists of properly establishing what values of the (softmax) confidence score can be considered reliable.78 Several enhancements over this baseline have been proposed to address the calibration issues from softmax confidence scores. Liang et al.79 presented ODIN, which uses temperature scaling and small input perturbations to separate the softmax scores between in- and out-of-distribution data. Hsu et al. 80 modified this approach to function without requiring out-of-distribution examples and further enhanced detection capabilities using confidence score decomposition. A more recent methodology, DOCTOR, aimed to characterize the optimal discriminator, focusing solely on the softmax probability. 36 Finally, Liu et al. 140 questioned softmax's efficacy for reliable uncertainty estimation, suggesting an alternative energy score for logit transformation.\nThe main advantage of these techniques lies in their straightforward implementation, only requiring attention to the neural network's outputs, and ensuring minimal computational and memory burden. They have shown promise in detecting threats such"}, {"title": "Consistency checking", "content": "Several approaches focus on verifying the spatial or temporal consistency of a sequence of predictions. One primary method involves employing expert knowledge to establish constraints on output sequences. For instance, Kang et al. 81 built a monitor for object detection by identifying flickering, i.e., an object should not keep appearing and disappearing in successive frames of a video. Another strategy, showcased by Harper et al. 82, translates the official highway code's rules and clauses into logical assertions for monitoring.\nAlternatively, these patterns can be learned from data. Chen et al. 83 proposed a logical framework to evaluate both temporal and spatial coherence of bounding box predictions to identify erroneous detections. Temporal coherence focuses on how bounding box labels evolve across sequential frames, while spatial coherence learns standard bounding box sizes at various locations. On another note, Guerin et al. 84 proposed a consistency monitor tailored for objects under periodic motion, like those on production lines. They train a Gaussian process to estimate the probability for a bounding box to be at a particular location at a specific time and use it to discard erroneous detections. Finally, Rabiee and Biswas 141 detect failures of stereo vision-based perception through inconsistencies in plans generated by a vision and a supervisory sensor.\nThese techniques are primarily tailored for object detection tasks. One of the advantages of these methods is the potential to gather samples for subsequent ML training, either through human annotation or weak supervision."}, {"title": "Ensemble methods", "content": "Using an ensemble of ML models is a conventional approach to enhance robustness. An effective ensemble should consist of models that are \u201cgood\u201d, \u201cindependent\u201d and \u201csufficiently numerous\". For complex ML used in perception tasks, such ensembles can be composed of models with the same architecture but trained to identify different aspects of the data or with different architectures trained with the same data. 142 For deep neural networks, Gontijo et al. 137 conducted a study about the influence of different training parameters on what the model knows, which can be used to maximize the ensembling benefits.\nWhile neural network ensembles are commonly used to enhance the robustness of ML systems, they can also serve as effective tools for monitoring predictions. Ensembles provide additional information, such as the level of agreement among individual components, which can be valuable for assessing an ML system's confidence in its predictions. When computational resources are not a limiting factor, building ensembles of deep neural networks presents a promising approach to monitor the coherence between individual models and mitigate the propagation of errors from a single neural network.\nIn practice, different voting strategies were proposed to address anomaly detection. Yahaya et al. 85 weight each model vote based on its performance and a score for what is considered normal. Roitberg et al. 86 leverage the estimated uncertainty of each prediction to measure novelty. Roy et al. 143 present a runtime monitor based on predictive processing and dual-process theory. They developed a bottom-up neural network comprising two layers: 1. a feature density model that learns the joint distribution of the original inputs, outputs, and the model's explanation for its decisions. 2. a graph Markov neural network that captures an even broader context."}, {"title": "Robustness to input perturbation", "content": "To verify if a new input can be considered safe, it was proposed to measure its sensitivity to input perturbations. Such perturbations can be applied either to the data (e.g., image compression87) or to the ML model itself through random mutations. 88 Another possibility is to check the stability of a model within a radius distance calibrated during the training. 89 The underlying hypothesis for these approaches is that, for valid input, the outputs of the neural network should be robust to small perturbations.\nThis approach is mostly used to detect adversarial inputs, but it has also been used for practical scenarios involving critical systems. For example, Zhang et al. 144 proposed DeepRoad, a GAN-based metamorphic testing and input validation framework for autonomous driving systems.\nSimilarly to ensemble methods, a primary limitation of these techniques is their need for multiple"}]}