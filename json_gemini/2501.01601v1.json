{"title": "Few-shot Implicit Function Generation via Equivariance", "authors": ["Suizhi Huang", "Xingyi Yang", "Hongtao Lu", "Xinchao Wang"], "abstract": "Implicit Neural Representations (INRs) have emerged as a powerful framework for representing continuous signals. However, generating diverse INR weights remains challenging due to limited training data. We introduce Few-shot Implicit Function Generation, a new problem setup that aims to generate diverse yet functionally consistent INR weights from only a few examples. This is challenging because even for the same signal, the optimal INRs can vary significantly depending on their initializations. To tackle this, we propose EQUIGEN, a framework that can generate new INRs from limited data. The core idea is that functionally similar networks can be transformed into one another through weight permutations, forming an equivariance group. By projecting these weights into an equivariant latent space, we enable diverse generation within these groups, even with few examples. EQUIGEN implements this through an equivariant encoder trained via contrastive learning and smooth augmentation, an equivariance-guided diffusion process, and controlled perturbations in the equivariant subspace. Experiments on 2D image and 3D shape INR datasets demonstrate that our approach effectively generates diverse INR weights while preserving their functional properties in few-shot scenarios.", "sections": [{"title": "1. Introduction", "content": "Open-sourced models have been the driving force behind the incredible progress of Artificial Intelligence (AI) [27, 50, 62]. Beyond just making powerful models accessible, this openness has created new opportunities for meta-learning: treating model weights themselves as data sources. Recent works have demonstrated the potential of learning from [13, 24, 45, 68, 70] and generating neural network weights [15, 28, 49, 76] to promote further research.\nOne type of neural network well-suited for the weight generation paradigm is the Implicit Neural Representation (INR). These networks use a simple multi-layer perceptron (MLP) to fit continuous signals [8, 52, 59]. INRs serve as powerful tools for representing continuous data, offering several advantages: high-fidelity reconstruction [43, 48], smooth interpolation properties [8, 71], and infinite output resolution [3]. Their consistent and comparative simple architecture makes them ideal candidates for weight-based generative modeling.\nHowever, training generative model on weight comes with its challenges, primarily due to a shortage of data. Currently, collecting large-scale INR data is costly, as it involves time-consuming gradient-based optimization. Existing datasets for INR data are limited from the diversity of sources [39, 45]. This data gap imposes restrictions on further investigation of weight space.\nGiven these constraints, an intuitive motivation is to target on a more practical task of Few-shot Implicit Function Generation. As illustrated in Fig. 1, with only a few new INR checkpoints, we aim to train a generative model that can produce diverse weights while preserving the original distribution's function of these few INR data. These generated checkpoints serve multiple purposes: they can be utilized as new data sources for weight space learning, or they can be directly applicable for practical downstream tasks like 3D shape generation.\nThis new problem comes with two fundamental challenges. First, the curse of dimensionality [4] exacerbates data demands. Since model weights reside in high-"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Implicit Neural Representation", "content": "Implicit Neural Representations (INRs) have demonstrated remarkable efficacy in representing diverse forms of complex signals, including spatial occupancy [42, 56, 59], 3D geometric morphology [5, 9, 20], signed distance functions [34, 48], 3D scene appearence [10, 32, 37, 44, 67] and some other complex signals [8, 38, 41, 69] with the help of a small neural network, usually a MLP with few layers."}, {"title": "2.2. Equivariant Architectures", "content": "A wide range of studies have tried to build equivariant architecture to respect underlying symmetries in the data. This leads to advantages including smaller parameter space, efficient implementation, and better generalization abilities [12, 29, 45, 78]. The standard construction pattern involves identifying basic equivariant functions which are often linear [22, 40, 72], and composing them with pointwise nonlinearities to build deep networks. Of particular relevance to our work are architectures designed for set-structured data, where the input represents ordered elements requiring equivariance to permutations. A recent study incorporates advantages of several previous works to build an efficient and highly expressive architecture [45]. These foundations prove crucial for our work, as neural network weight spaces naturally exhibit similar symmetry structures."}, {"title": "2.3. Neural Network Weight Generation", "content": "Recently, neural network weight generation has gained significant attention, from early meta-learning approaches [19, 21] to recent generative modeling techniques. Existing methods broadly split into two categories: direct weight space diffusion methods [16, 49], which struggle with high-dimensional spaces, and dimensionality reduction approaches using autoencoders [13, 57, 64], which often fail to preserve the inherent structural characteristics of weight spaces. The key distinction of our method lies in its systematic implementation of equivariance, which aims at preserving these inherent characteristics."}, {"title": "3. Preliminaries", "content": "In this section, we formalize the Few-shot Implicit Function Generation problem, we also interpret fundamental properties of equivariance and corresponding equivariant architectures."}, {"title": "3.1. Problem Definition", "content": "Consider a class space F where each f \u2208 F represents a category of signals (e.g., images of a specific digit or 3D shapes from a particular object class such as cars or planes) that can be encoded by implicit neural representations. Let W denotes the weight space of MLPs with a fixed architecture, and \u03a6 : W \u2192 (X \u2192 Y) maps weights to their corresponding implicit functions.\nDefinition 1 (Few-shot INR Generation). Given k example weights Sf = {W1, ..., wk} that encode valid instances of a target class f, the objective is to generate new weights that encode diverse yet valid instances of the same class. Formally, we aim to learn a generator G : Sf \u2192 pf(w) that estimates the weight distribution of class f, where validity requires the reconstructed signal \u03a6(w) to maintain class-specific properties while allowing intra-class variation.\nA comprehensive mathematical formalization is provided in the Appendix."}, {"title": "3.2. Equivariance and Equivariant architecture", "content": "Previous studies have revealed that neural networks exhibit inherent equivariance in their weight space. Different weight configurations can represent the same function due to the permutation invariance of neurons within each layer [40, 53, 65, 70]. Understanding and leveraging this equivariance is crucial for Few-shot Implicit Function Generation, as it enables more efficient learning from limited examples by exploiting the underlying structure of the weight space [12, 17, 29, 77]. Since INR encoding networks typically employ MLPs that inherently possess this property, we present key concepts of it and corresponding equivariant architectures.\nEquivariance. Consider a shallow network with weight matrices W\u2081 and W2. For any permutation matrix P, the transformed weights PW1 and W2PT yield a functionally equivalent network. This symmetry emerges from the equivariance of pointwise activation functions:\n\u03a1\u03c3(x) = \u03c3(Px).\nThis property implies that the space of functionally equiva-"}, {"title": "4. Methodology", "content": ""}, {"title": "4.1. Overview", "content": "Follow the definition of Few-shot Implicit Function Generation problem, our goal is to learn how to generate weights that follow a specific distribution, using only a small number of samples as reference. This problem presents two fundamental challenges as stated in Sec. 1: (1) poor generalization due to the high dimensionality of weight space, and (2) ineffective knowledge transfer and potential mode collapse due to weak element-wise similarity between functionally equivalent weights. We address both challenges through equivariance which is interpreted in Sec. 3.2.\nOur key insight is that by projecting weights into an equivariant latent space, we can generate diverse samples within an equivariance group using only a few reference examples inside this latent space. This projection effectively reduces dimensionality while preserving functional relationships, enabling both better generalization and meaningful knowledge transfer. As illustrated in Fig. 2, we systematically leverage equivariance across three stages:\nEquivariant Feature Learning: We establish a foundation through an equivariant encoder trained via contrastive learning with weight space smooth augmentations, ensuring learned representations preserve essential equivariance property while remaining expressive.\nEquivariance-Guided Distribution Modeling: A diffusion-based generator, conditioned on these equivariant features, models the weight distribution while preserving symmetry properties through a designed explicit equivariance regularization.\nEquivariant Few-shot Adaptation: For k-shot generation,"}, {"title": "4.2. Equivariant Encoder Pre-training", "content": "A compact and expressive latent space of equivariant features is crucial for effective distribution modeling, improving diffusion convergence and stability. To learn such features, we employ a contrastive learning framework specifically adapted for our equivariant architecture. Additionally, we introduce a weight-specific smooth augmentation strategy to enhance the robustness and representation power of the equivariant encoder.\nEquivariant Architecture. Similar to previous work, our equivariant encoder, inspired by the symmetry structure of deep weight spaces, operates directly on weight matrices through a carefully designed architecture that preserves functional equivariance [45, 72]. The encoder decomposes the input weight space into meaningful sub-representations corresponding to different components of a neural network and implements equivariant mappings between these sub-representations through a combination of linear equivariant layers, invariant layers, and efficient pooling and broadcasting operations. Formally, the encoder can be denoted as a mapping E : V \u2192 V parameterized by $ following the canonical form demonstrated in Sec. 3.2. This architecture enables effective projection of the input weight space into an expressive low-dimensional equivariant subspace while preserving the underlying functional relationships. Detailed architecture design can be found in Appendix.\nSmooth Augmentation. The key idea of this encoder is to map weights from an equivariance group to a compact cluster in the equivariant subspace. Yet it can indeed learn from unconstrained weight spaces, recent studies reveal that modern neural networks fundamentally rely on smooth signal modeling [51, 70]. Consequently, to enhance our encoder's feature learning capability, we introduce a smooth augmentation technique for the input weight space. This augmentation could be seen as an optimization of the starting point, and could lead to better representation power of the pre-trained encoder.\nSpecifically, we represent neural networks as dependency graphs G = (V, E) [18], where nodes represent operations with weights and edges denote inter-connectivity. For each subgraph, we find a permutation matrix P that minimizes the total variation (TV) across the network [55]. Leveraging the orthogonal independence of permutations [70], we decompose this optimization into multiple Shortest Hamiltonian Path problems [23], which we efficiently solve using a 2.5-opt local search algorithm [61]. The resulting optimal permutation P* is applied to all weight matrices within each network subdivision, ensuring smooth weight space augmentation while preserving functional equivariance."}, {"title": "Contrastive Learning.", "content": "Following the SimCLR framework [7], we pre-train our equivariant encoder to maximize similarity between different augmented views of the same weight data in the equivariant latent space. The process involves three key steps:\n1) We generate positive pairs by applying stochastic INR-based augmentations to smoothed weights w, obtaining augmented weights w. These augmentations include rotation, translation, scaling, and our proposed color jittering and bias perturbation (see Appendix for details).\n2) Extract equivariant features through pre-trained equivariant encoder E6: \u03c8 = E\u2084(W) and \u03c8 = \u0395\u03c6(\u0175).\n3) Optimize using contrastive loss:\nli,j = -log(exp(sim(E\u03c6(\u1ff6\u2081), E\u03c6(\u1ff6;))/\u03c4))/(\u03a3kti exp(sim(E\u03c6(W\u2081), E\u03c6(\u1ff6\u03ba))/\u03c4)),\nwhere sim(u, v) denotes cosine similarity, \u03c4 is a temperature parameter. {\u1f64\u03ba} contains both the smoothed weights {Wk} and the augmented weights {\u0175k}."}, {"title": "4.3. Equivariance Guided Diffusion", "content": "Within this stage, we employ an equivariance-conditioned diffusion model to reconstruct the input weight distributions, utilizing their corresponding equivariant features. This approach enables a robust reconstruction of the weight space, while maintaining the equivariance inherent in the original data structure.\nWeight-Space Diffusion. The modeling begins by transforming each set of smoothed weights {Wi} into a flattened one-dimensional vector representation. Subsequently, these smoothed weights w\u2081 undergo a diffusion process, yielding noised samples w. For the denoising network, we implement a transformer architecture Go. This choice leverages transformers' established capability to efficiently process long vector sequences, advantageous for weight space manipulation [16, 49]. To highlight the difference with normal diffusion models, our denoising network predicts the denoised weights directly, rather than the noise [49]. The architecture also incorporates cross-attention layers following each transformer block [54], enabling the sampled weights W\u2081 to be conditioned on equivariant features Vi. These features are derived by processing the smoothed weights W\u2081 through the former pre-trained equivariant encoder E6. The comprehensive denoising procedure can be formally expressed as:\n\u1f66\u2081 = Go(\u1ff6\u03c4, \u0395\u03c6(Wi)).\nTraining and Optimization. The primary goal of our approach is to optimize the denoising network to model the input distribution accurately. We apply a simple Mean Squared Error (MSE) loss between the denoised weights wi and the smoothed weight w\u2081 denoted as Lrecon (W, W).\nHowever, in order to leverage the characteristic of equivariance in the following few-shot generation stage, we also designed a specific loss to regulate the approximation of the equivariant features of the generated weights and the original smoothed weights:\nLeq(W,W) = 1/N \u03a3i=1N|\u0395\u03c6(Wi) \u2013 E\u03c6(Wi)||2.\nBy composing these two different objectives, our final target is to minimize the composite objective by sampling all the smoothed weight and generated weight:\nmin Eww [Ltotal] = min Ewi, wi [Lrecon + \u03bb Leq]."}, {"title": "4.4. Few-shot Fine-tuning", "content": "Through latent space guidance, the diffusion model demonstrates adaptability to previously unseen distributions, given only a limited set of weights {w\u2081, ..., w} from the target distribution. This adaptation is facilitated through the utilization of equivariant features {41,...,4} extracted via the pre-trained equivariant encoder, serving as novel guidance vectors that enable knowledge transfer. The adaptation process need only a few iterations of fine-tuning, following the same procedures in Sec. 4.3.\nSubspace Disturbance. To enhance the diversity of generated weights, we introduce an equivariant subspace disturbance strategy during the final generation. This approach is motivated by the observation that the diffusion model's training process inherently embeds knowledge within the equivariant subspace through feature-guided denoising. Consequently, the diversity of the guiding equivariant features directly influences the variability of the generated weights, establishing a foundation for controlled diversity in weight generation.\n~Specifically, we apply a random Gaussian noise e N(0, I) on the equivariant feature & with a controlling parameter \u03b3: \u03c8 = \u03c8 + \u03b3\u03b5. We impose this constraint on the magnitude of the disturbance to make it bounded and controllable. Finally, the generation of target weights is accomplished through a denoising procedure applied to k independent Gaussian noise vectors E = {\u20ac1,...,\u20ack}. This process is guided by disturbed equivariant features {$1,..., \u03a8\u03ba} derived from the support set."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Experimental Setup", "content": "We establish two Few-shot Implicit Function Generation scenarios: 1) Given a set of MLP encoded INRs of different 2D images, we split them into two disjoint parts: the seen categories Ssource and the unseen categories Stest, where category is defined by their rendered image. 2) Similarly, given a set of MLP encoded INRs of different 3D shapes, which are split into two disjoint parts: the seen categories Ssource and the unseen categories Stest, where category is defined by their rendered 3D shape.\nDatasets. For the 2D image scenario, we evaluate our methodology on two benchmark datasets: MNIST (2D greyscale images) [31] and CIFAR-10 (2D RGB images) [30]. We utilize the pre-overfitted INR dataset for MNIST (MNIST-INRs) provided by [45] and the corresponding dataset for CIFAR-10 (CIFAR-10-INRs) from [39]. MNIST-INRs encompasses 50K INR instances, with 5K samples per category, while CIFAR-10-INRs contains 60K INR instances, comprising 6K samples per category. For the 3D shape domain, we evaluate our approach on the ShapeNet dataset [6], focusing on three representative categories: airplane (4045 shapes), car (6778 shapes) and chair (3533 shapes). These shapes are encoded into MLPs, denoted as ShapeNet-INRs. Regarding architectural specifications, CIFAR-10-INRs employs a 3-layer SIREN MLP with a width of 64 [59], while MNIST-INRs utilizes a 3-layer SIREN MLP with a width of 32 [59]. For ShapeNet-INRs, it implements a 3-layer standard MLP with a width of 128, incorporating ReLU activation functions and input positional encoding [16].\nIn the context of few-shot weight generation, we partition each dataset into two distinct subsets [14, 47, 74, 75]. The unseen category support set Stest comprises randomly sampled INR data from a single category, with the sample size varying from 1 to 10 to evaluate different levels of difficulty. The remaining data from other categories constitute the seen category support set Ssource.\nImplementation We provide important configurations here, please refer to Appendix for more detailed information. Our equivariant architecture is implemented with four hidden equivariant layers [45], with the output equivariant feature dimension set to 128. Across the three stages: In the equivariance-guided diffusion stage, we utilize a squared cosine beta scheduler [46] across 1000 timesteps and implement DDIM [60] for sampling. The equivariance loss proportion parameter A is set to 0.1 unless otherwise specified. Finally, during the generation process, the subspace disturbance parameter y, which controls the noise intensity, is set to 0.3 by default.\nMetrics. Since there is no direct evaluation metrics for generated weights, we adopt an indirect evaluation strategy aligned with existing works [13, 15, 16, 57, 64]. For 2d images, a direct rendering is applied and for 3d shape scenario, we extract the underlying isosurface from the MLPs with Marching Cubes [35]. Our evaluation framework assesses both the quality and diversity of generated results for both two scenarios: For the 2D image scenario, we employ FID [25] and LPIPS [73] as the metrics. Specifically, we apply the intra-cluster version of LPIPS to quantify the diversity of generated unseen images in our few-shot generation context like many other few-shot image generation settings do [14, 54, 75]. Lower FID indicates better quality and higher LPIPS indicates enhanced diversity. For the 3D shape scenario, we follow prior works [36, 58, 63, 79] in evaluating MMD, COV, and 1-NNA. For MMD, lower is better; for COV, higher is better; for 1-NNA, 50% is the optimal. We compute these metrics using Chamfer Distance (CD) as the underlying distance measure, with reported CD values scaled by a factor of 102 for clarity.\nBaselines. Our method is benchmarked against two different types of methods: modality-based methods and INR-based methods. In the first category, we benchmark against methods that directly manipulate domain-specific representations. For the 2D image domain, we compare our ap-"}, {"title": "5.2. Main Results", "content": "The empirical results illustrated in Tab. 1 and Tab. 2 demonstrate the consistent superior performance of our method across all evaluation metrics in the 10-shot setting. In"}, {"title": "5.3. Indepth Analysis", "content": "Exploration of equivariant subspace. The efficacy of our proposed method is fundamentally rooted in the expressive power of the equivariant subspace, which enables the incorporation of category-specific knowledge into distribution modeling. To illustrate the learning dynamics within the weight space, we conduct a 2D t-SNE visualization of the equivariant subspace, as shown in Fig. 7. We present comparative visualizations both with and without smooth augmentation of the original weight space. The results demonstrate that the pre-trained equivariant encoder successfully projects the original weight space into a discriminative manifold where categories exhibit clear clustering behavior. Furthermore, the application of smooth augmentation to the original space alleviates the complexity of equivariant subspace learning, resulting in more compact and category-specific embeddings.\nImpact of subspace disturbance. We conduct a systematic analysis of equivariant subspace disturbance intensity and its impact on generation performance, as illustrated in Fig. 8. Our findings demonstrate a clear trade-off between sample diversity and functional fidelity. The intensity of subspace disturbance exhibits a direct correlation with final generation variance. As shown in Fig. 8(a), higher disturbance intensities lead to increased COV, indicating greater diversity among generated samples. However, Fig. 8(b) reveals that this enhanced diversity comes at a cost to generation quality, as measured by increasing MMD.\nAblation study. To evaluate the importance of equivariance, we conduct ablation studies comparing three variants: (1) our full EQUIGEN framework, (2) a variant that omits the equivariant encoder and uses unconditional diffusion, and (3) a variant that replaces equivariant features with class-label conditioning. As shown in Tab. 3, the framework with equivariance modules demonstrates superior performance in both generation quality and diversity. The unconditional variant exhibits mode collapse, and class-label conditioning variant offers only minimal improvement, demonstrating the limitations of direct class conditioning.\nAnother more fine-grained ablation study is conducted to discern the individual contributions of weight space smooth augmentation and equivariant subspace disturbance. The results are presented in Tab. 4. Our analysis reveals that smooth augmentation alone enhances both generation quality and diversity. This empirical evidence supports the idea that starting from an optimized start point, the equivariant encoder could gain better expressive power through contrastive learning. Conversely, applying subspace disturbance yields increased diversity, although a modest decrease in generation quality appears when smooth augmentation is absent. This trade-off can be attributed to the fact that equivariant features with same disturbance may locate at different category clusters with and without the smooth augmentation. The simultaneous employment of both smooth augmentation and subspace disturbance enables our method to achieve optimal performance."}, {"title": "6. Conclusion", "content": "In this work, we present a practical setting named Few-shot Implicit Function Generation and introduce solution to it by systematically leveraging the principle of equivariance. Our three-stage framework effectively addresses both the generalization and mode collapse challenges inherent in weight space generation. Extensive experiments across INR data for 2D image and 3D shape domains validate our approach, demonstrating superior performance in both generation quality and diversity."}]}