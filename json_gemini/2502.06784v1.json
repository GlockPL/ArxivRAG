{"title": "RELGNN: Composite Message Passing for Relational Deep Learning", "authors": ["Tianlang Chen", "Charilaos Kanatsoulis", "Jure Leskovec"], "abstract": "Predictive tasks on relational databases are critical\nin real-world applications spanning e-commerce,\nhealthcare, and social media. To address these\ntasks effectively, Relational Deep Learning (RDL)\nencodes relational data as graphs, enabling Graph\nNeural Networks (GNNs) to exploit relational\nstructures for improved predictions. However, ex-\nisting heterogeneous GNNs often overlook the in-\ntrinsic structural properties of relational databases,\nleading to modeling inefficiencies. Here we intro-\nduce RELGNN, a novel GNN framework specifi-\ncally designed to capture the unique characteris-\ntics of relational databases. At the core of our ap-\nproach is the introduction of atomic routes, which\nare sequences of nodes forming high-order tripar-\ntite structures. Building upon these atomic routes,\nRELGNN designs new composite message pass-\ning mechanisms between heterogeneous nodes,\nallowing direct single-hop interactions between\nthem. This approach avoids redundant aggrega-\ntions and mitigates information entanglement, ul-\ntimately leading to more efficient and accurate\npredictive modeling. RELGNN is evaluated on\n30 diverse real-world tasks from RELBENCH (Fey\net al., 2024), and consistently achieves state-of-\nthe-art accuracy with up to 25% improvement.", "sections": [{"title": "1. Introduction", "content": "Predictive modeling over relational data (multiple tables\nconnected via primary-foreign key relations) is central to\nnumerous real-world applications: e-commerce platforms\nforecast product demand, music streaming services personal-\nize recommendations, and financial institutions assess credit\nrisk. The common strategy to tackle these predictive tasks in-\nvolves classical tabular machine learning approaches (Chen\n& Guestrin, 2016) that often require flattening relational data\ninto a single table through manual feature engineering (Kag-\ngle, 2022). This approach is not only labor-intensive but\nalso leads to a substantial loss of predictive signal, as it\noversimplifies the interconnected structure of relational data\nduring the flattening process.\nTo overcome these limitations, Fey et al. (2024) introduced\nRelational Deep Learning (RDL), a framework that enables\nend-to-end trainable neural networks to perform predictive\nmodeling directly on relational databases by leveraging their\ninherent structure. In RDL, relational data is represented\nas a graph, where each entity is represented as a node, and\nthe primary-foreign key links between entities define the\nedges. This graph-based representation allows Graph Neu-\nral Networks (GNNs) (Gilmer et al., 2017; Hamilton et al.,\n2017) to serve as predictive models, capturing complex\nrelational dependencies that traditional methods overlook.\nComplementing this advancement, RELBENCH (Robinson\net al., 2024) provides the first comprehensive benchmark for\nevaluating and developing RDL models. By addressing the\nshortcomings of tabular approaches, RDL not only enhances\npredictive performance but also establishes a new paradigm\nin machine learning-one where end-to-end learning frame-\nworks are applied to solve predictive problems on relational\ndata.\nDesigning effective GNN models for RDL is essential for\ntackling predictive tasks and supporting critical real-world\napplications. Relational graphs are large-scale heteroge-\nneous networks whose structure evolves dynamically over\ntime. Existing approaches typically apply standard hetero-\ngeneous GNNs directly to these graphs (Robinson et al.,\n2024). However, this can be suboptimal, as these mod-\nels fail to fully capture the unique structural and relational\nproperties inherent in relational databases. Standard hetero-\ngeneous GNNs (Schlichtkrull et al., 2018; Hu et al., 2020)\nare designed for general heterogeneous graphs, where edge\ntypes encode direct semantic interactions between entities.\nIn contrast, relational data graphs are structured around\nprimary-foreign key relationships, which define the connec-\ntivity between tables rather than semantic meaning. This\nfundamental distinction significantly affects how informa-\ntion propagates, highlighting the need for models specifi-\ncally tailored to the structural characteristics of relational\ndatabases.\nHere we propose RELGNN, a novel graph attention frame-\nwork that introduces a composite message passing mech-\nanism to fully exploit the unique structural properties of"}, {"title": "2. Preliminaries", "content": null}, {"title": "2.1. Relational Database", "content": "A relational database (T, L) consists of a set of tables T =\n{T1,..., Tn} and a set of links between them L \u2286 T \u00d7 T.\nEach table is a set T = {v1, ..., Unr}, where the elements\nvi \u2208 T are called rows or entities. Each entity v \u2208 T has\na unique primary key pr that distinguishes it from other\nentities within the table. An entity may also have one or\nmore foreign keys, where each foreign key Kv \u2286 {pv' :\n\u03bd' \u2208 \u03a4' and (T,T') \u2208 L} defines a link between element\nv \u2208 \u03a4 to elements v' \u2208 T', where p is the primary key of\nan entity v' in table T'. Besides, an entity may have several\nattributes xv, which represent the informational content of\nthe entity, and an optional timestamp tv, indicating when an\nevent occurred. Both primary keys and foreign keys, as well\nas attributes and the optional timestamp, are columns of\nthe table. A link L = (Tfkey, Tpkey) between tables exists\nif a foreign key column in Tfkey references a primary key\ncolumn of Tpkey.\nFor example, in Figue 2(a), the TRANSACTIONS table\nhas a primary key (TRANSACTIONID), two foreign keys\n(PRODUCTID and CUSTOMERID), one attribute (PRICE),\nand a timestamp (TIMESTAMP). Similarly, the PRODUCTS\ntable has a primary key (PRODUCTID), no foreign keys,\nthree attributes (DESCRIPTION, IMAGE and SIZE), and no\ntimestamp. The links between tables are illustrated by black\nconnecting lines."}, {"title": "2.2. Relational Deep Learning", "content": "Fey et al. (2024) proposed Relational Deep Learning\n(RDL), a framework that enables end-to-end trainable neu-\nral network models to tackle predictive tasks on relational\ndatabases. In RDL, relational tables are represented as a tem-\nporal, heterogeneous graph, where each table corresponds\nto a node-type, each entity (row) corresponds to a node, and\nprimary-foreign key links define the edges (See Figure 2 b,\nc). The information carried by each entity, specified by the\ncolumns of the table, is extracted as the initial embedding\nfor the corresponding node. This representation preserves\nall the information and predictive signals in relational data.\nIt is important to note that relational data evolves over time\nas events occur. This temporal aspect is captured by the\n(optional) timestamp to attached to each entity v. For in-\nstance, each transaction in the TRANSACTIONS table has a\ntimestamp. Many predictive tasks involve forecasting future\nevents, such as predicting the total sales of a product in the\nnext week. Consequently, it is crucial to treat time as an\nimportant component in RDL. Specifically, temporal neigh-\nbor sampling (Hamilton et al., 2017; Fey et al., 2024) is\nemployed to construct subgraphs around entity nodes at spe-\ncific seed times determined by the corresponding tasks, with\nnodes from future timestamps excluded during the sampling\nto prevent information leakage from future events during\ntraining. After that, GNNs can be trained end-to-end on\nthese temporally sampled subgraphs, eliminating the need\nfor manual feature engineering (See Figure 2 d)."}, {"title": "2.3. Meta-path in Heterogeneous Graphs", "content": "Meta-path (Sun et al., 2011) is widely used in heteroge-\nneous graphs to capture semantic relationships between\ndifferent types of entities (Shang et al., 2016; Dong et al.,\n2017; Hu et al., 2018; Shi et al., 2018; Wang et al., 2019b;\nFu et al., 2020). A meta-path is a sequence of node and\nedge types in a heterogeneous graph, often designed using\nexpert knowledge to capture meaningful relational patterns.\nFor example, in an academic graph, a meta-path can be\ndefined as \"Author-Paper-Author\u201d to model co-authorship\nrelations, or \"Author-Paper-Conference-Paper-Author\" to\ncapture co-conference relations. Despite its widespread use,\nthe meta-path has notable limitations (Shi et al., 2016; Hu\net al., 2020; Shi, 2022). It requires manual selection, which\ndepends on domain expertise and can be biased, leading\nto suboptimal performance if an inappropriate meta-path is\nchosen. Additionally, meta-path lacks flexibility, as it cannot\neasily adapt to changes in graph structure or new relation-\nships. Moreover, designing an effective set of meta-paths\nfor complex graphs can be time-consuming and may fail to\ncapture all relevant interactions, limiting its expressiveness."}, {"title": "3. Method", "content": null}, {"title": "3.1. Challenges in Message Passing for Relational Deep\nLearning", "content": "Message passing in heterogeneous graphs is designed to\naggregate information across nodes and edges of differ-\nent types. However, relational data graphs exhibit distinct\nstructural properties that set them apart from general het-\nerogeneous graphs. Specifically, in a heterogeneous graph,\nthe fundamental unit of structure is a relation type, typically\nrepresented as a triplet of the form (node-type 1, edge-type,\nnode-type 2). For example, consider a scenario where a cus-\ntomer purchases an item. This interaction can be modeled\nas a heterogeneous graph with the triplet (customer, trans-\naction, item), where a message originating from a customer\nnode is propagated to an item node via a transaction edge in\na single hop.\nIn contrast, relational data graphs are structured differently.\nWhile they are heterogeneous and contain multiple node-\ntypes, their edge types are not defined based on semantic\ninteractions but rather by primary-foreign key relationships.\nConsequently, the fundamental structural unit is not neces-\nsarily a triplet but a direct pairwise relationship between\nnodes, represented as (node-type 1, node-type 2). In a rela-\ntional data graph, the (customer, transaction, item) triplet\nis modeled by three nodes: a customer node, a transaction\nnode, and an item node, and pairwise interactions between\nthem i.e., (customer, transaction) and (transaction, item).\nThis distinction is critical in both modeling and analysis and\nrequires further investigation.\nOur first observation is node-types in relational graphs can\nbe broadly classified into two categories: (i) those with zero\nor one foreign key and (ii) those with two or more foreign\nkeys. To illustrate, consider the rel-f1 schema, which\ntracks all-time Formula 1 racing data since 1950 (see Fig. 1).\nIn this schema, Constructor, races, and drivers each have\nzero foreign keys, while circuits has one. By contrast, con-\nstructor_standings, constructor_results, and standings each\nhave two foreign keys, and results and qualifying each have\nthree. Nodes with two or more foreign keys are highlighted\nin purple. Subgraphs containing only node-types with zero\nor one foreign key exhibit no special structural patterns and\ncan be treated as general heterogeneous graphs. However,\nsubgraphs that include node-types with multiple foreign\nkeys reveal unique structures, necessitating further analysis.\nNode-type with two foreign keys (bridge): When a node-\ntype has two foreign keys, it forms a subgraph of the form\n(node-type 3 \u2190 node-type 1 \u2192 node-type 2), creating a lo-\ncal tripartite structure among these three node-types. In\nthis configuration, node-type 1 simply acts as a aggregating\nbridge between node-type 2 and node-type 3. Consequently,\na two-hop communication path can be redundant, leading to\nunnecessary aggregation and modeling inefficiencies. Un-\nder standard heterogeneous GNNs, irrelevant information\noften gets entangled during message passing, which dilutes\nthe predictive signal. For instance, consider how \u201ca driver\nachieved a certain standing in a race\" is passed from a\nsource races node to a destination drivers node via an in-\ntermediate standings node (Fig. 1). In a typical two-hop\nscheme, all neighbors of the standings node\u2014including un-\nrelated constructors nodes-contribute noise during the first\nmessage passing step. Furthermore, information from the\ndrivers node is unnecessarily duplicated when it propagates\nthrough this intermediate node. As we show in the next\nsubsection, modeling this triplet of nodes as an atomic route\nenables direct one-hop message passing between the rel-\nevant node-types-node-type 2 and node-type 3-without\nany loss of information or risk of oversmoothing.\nNode-type with multiple (three or more) foreign keys\n(hub): When node-types have three or more foreign keys,\nthey form star-shaped subgraphs that serve as communi-\ncation hubs, bridging multiple node-types. For example,\nas shown in Fig. 1, the results node mediates interactions\namong constructors-races, races-drivers, and constructors-\ndrivers. These hub nodes inherit the same inefficiencies\nseen in the two-foreign-key case, where two-hop message\npassing often leads to redundant information aggregation.\nFurthermore, the star-shaped connectivity at the schema\nlevel induces a hidden second-order clique structure in the\ndata graph-an aspect that standard modeling approaches\nfail to exploit effectively. Our proposed approach, which\nis explained next, can leverage these hidden cliques, which\nform a critical substructure in many high-impact domain\ngraphs. This transition from star-like to clique-like patterns\nsubstantially increases connectivity density, reshaping in-\nformation propagation and complicating message passing\ndynamics."}, {"title": "3.2. Atomic Routes in Relational Deep Learning", "content": "To address the challenges outlined above, we introduce the\nconcept of atomic routes.\nDefinition 3.1 (Atomic Route). An atomic route is a se-\nquence of node-types that form a composite path between\nthe starting and ending node-type. We distinguish two cases:\n1.  Single foreign key. If a table has exactly one foreign\nkey to another table, the atomic route is an edge con-\nnecting the primary-key node to the foreign-key node.\n2.  Multiple foreign keys. If a table has multiple foreign\nkeys, the atomic route is a hyperedge connecting pairs\nof foreign-key nodes via the primary-key node."}, {"title": "3.3. Composite Message Passing for Relational Deep\nLearning", "content": "In this subsection we build upon the concept of atomic\nroutes and design composite message passing mechanisms-\nfor RDL. We begin this discussion by applying a standard\nheterogeneous GNN on a subgraph encoding tables with\nmultiple foreign keys. We assign src, dst, and mid to\nrepresent nodes corresponding to source, destination, and\nintermediate node-types, respectively. In standard heteroge-\nneous GNNs, it takes two steps to complete the full informa-\ntion exchange. In the first step, each mid node aggregates\ninformation from all its neighbor nodes:\n$h_{mid}^{(l+1)} = UPD({{m_{mid}^{l+1)}|VR = (T, \\phi(mid)) \u2208 R}}),$\nwhere\n$m_{mid}^{(l+1)} = AGGR(h_{dst}^{(l)}, {{h_{srcl}|p(u) = T}}),$\n$h_{v}^{(l)}$ denotes the embedding of node v at the l-th layer, UPD\nand AGGR are arbitrary differentiable functions with opti-\nmizable parameters, {{\u00b7}} denotes a permutation invariant\nset aggregator (e.g. mean, sum), R denotes the edge set\nconsisting of pairs of node-types connected through primary-\nforeign key relationships and $(\u00b7) denotes a function map-\nping a node to its corresponding node-type. Then in the\nsecond step, the message passed from mid to dst is\n$m_{(mid, dst)}^{(l+2)}= AGGR(h_{mid}^{(l+1)}, {{h_{dst}^{(l+1)}}})$\nNote that in Eq. (1), T represents all node-types connected\nto the intermediate node-type. Therefore, in addition to the\ninformation from the source node-type, information from\nother irrelevant node-types connected to the intermediate\nnode-type is also aggregated and entangled during this step.\nSpecifically, information from dst is also passed to mid in\nthis step, and it is subsequently passed back to dst again in\nEq. (2), leading to redundancy, as discussed in Section 3.1.\nTo avoid these modeling inefficiencies we propose a novel\ncomposite message passing scheme based on atomic routes.\n$m_{(dst,mid,src)}^{(l+1)} = AGGR(h_{dst}^{(l)}, {{FUSE(h_{mid}^{(l)}, h_{src}^{(l)})}})$,\nEq. (3) describes an information exchange from src via\nmid to dst that is completed within a single step. As\na result, there is no extraneous information entangled in\nthe process. In summary, our approach effectively tack-\nles the challenges that standard heterogeneous GNNs may\nencounter, such as multiple steps needed for complete infor-\nmation exchange and entangled information during message\npassing."}, {"title": "3.4. RELGNN: Composite Message Passing with\nAtomic Routes", "content": "The introduction of atomic routes and composite message\npassing enables the design of new architectures specifically\ntailored to relational data graphs. Eq. (3) admits multiple\ninstantiations, offering a flexible framework for message\npassing using atomic routes. Here, we propose RELGNN,\na simple yet effective instantiation. When multiple foreign\nkeys are involved, RELGNN instantiates Eq. (3) as follows:\nfor each mid node, it fuses information from each src node\nconnected via a primary-foreign key relationship. Then\nFUSE() is implemented as a linear combination:\n$FUSE(h_{mid}^{(l)}, h_{src}^{(l)}) = W_{1}h_{mid}^{(l)} + W_{2}h_{src}^{(l)}.$\nNote that, due to the nature of foreign keys, each mid node\nis connected to only one src node. Then, RELGNN in-\nstantiates AGGR(\u00b7) with the standard multi-head attention\nmechanism (Vaswani et al., 2017; Shi et al., 2021), where\nembeddings from destination nodes serve as queries, and\nembeddings derived from the fusion operation in Eq. (4)\nserve as keys and values. Let $h_{fuse}^{(l)} := FUSE(h_{mid}^{(l)}, h_{src}^{(l)})$\nas defined in Eq. 4. AGGR(\u00b7) is realized as:\n$AGGR(h_{dst}^{(l)}, {{h_{fuse}^{(l)} }}) = W_{proj}h_{dst} + \\sum_{fuse\\in N(dst)}^{} \\alpha_{dst, fuse} W_{V}h_{fuse}^{(l)},$\nwhere the attention coefficients $\\alpha_{dst, fuse}$ are computed via\nmulti-head attention (with the multi-head notation omitted\nfor brevity):\n$\\alpha_{dst, fuse} = softmax(\\frac{(Wh_{fuse}^{(l)})^{T} (Wh_{dst}^{(l)})}{\\sqrt{d}}).$"}, {"title": "4. Experiments", "content": "We evaluated RELGNN on RELBENCH (Robinson et al.,\n2024), a public benchmark designed for predictive tasks\nover relational databases using GNNS. RELBENCH offers a\ndiverse collection of real-world relational databases and real-\nistic predictive tasks. The benchmark spans 7 datasets, each\ncarefully processed from real-world sources across a wide\nrange of domains, including e-commerce, social networks,\nmedical records, Q&A platform and sports. These datasets\nvary significantly in size, with differences in the number\nof rows, columns, and tables, serving as a challenging and\ncomprehensive benchmark for RDL model evaluation. See\nAppendix A.1 for the description and detailed statistics of\neach dataset.\nRELBENCH introduces 30 predictive tasks covering a wide\nrange of real-world use cases, grouped into three repre-\nsentative types: entity classification (Section 4.1), entity\nregression (Section 4.2), and recommendation (Section 4.3).\nThese tasks are designed to reflect practical applications,\nsuch as predicting event attendance, estimating sales of\nan item, and recommending posts to users. The data is\nsplit temporally, with models trained on data from earlier\ntime periods and tested on data from future time periods.\nThe tasks vary significantly in the number of entities in\nthe train/validation/test split and the proportion of test enti-\nties encountered during training. Description and detailed\ndescription of each task can be found in Appendix A.2.\nWe follow the implementation of RDL in Robinson et al.\n(2024). As introduced in Sec. 2.2, relational data is trans-\nformed into heterogeneous temporal graphs, and tempo-\nral neighbor sampling is employed, creating subgraphs on\nwhich the model is trained. The initial node embeddings are\nextracted from raw table feature using PyTorch Frame (Hu\net al., 2024). These embeddings are subsequently fed into a\nGNN, and the resulting node embeddings are passed into a\nprediction head specific to the type of task to produce the\nfinal output. For baselines, we compare with the heteroge-\nneous GraphSAGE (Hamilton et al., 2017; Fey & Lenssen,\n2019; Robinson et al., 2024) used in the original RELBENCH\npaper. To ensure a fair comparison, we maintain identical\nsettings, including temporal neighbor sampling algorithm,\ninitial node embeddings extraction model, the prediction\nhead, and the loss function. Additionally, we incorporate a\nLight Gradient Boosting Machine (LightGBM) (Ke et al.,"}, {"title": "5. Related Work", "content": "Deep Learning on Relational Data. Several works have\nexplored the use of GNNs for learning on relational data\n(Schlichtkrull et al., 2018; Cvitkovic, 2019; \u0160\u00edr, 2021;\nZahradn\u00edk et al., 2023). These works investigated differ-\nent GNN architectures that utilize the relational structure.\nMore recently, Fey et al. (2024) introduced Relational Deep\nLearning (RDL) (see Sec. 2.2), establishing a new subfield\nof machine learning. RDL has enabled various research\nopportunities, such as advancements in relational graph con-\nstruction algorithms, GNN architectures, and task-specific\nprediction heads. Yuan et al. (2024) focused on improving\ntask-specific prediction heads for recommendation tasks,\naddressing limitations in the currently employed two-tower\nand pair-wise prediction heads. In contrast, our work fo-\ncuses on improving GNN architectures applied to all task\ntypes to generate node embeddings, offering an orthogonal\ncontribution. In addition to GNNs, Wydmuch et al. (2024)\nproposed leveraging large language models (LLMs) to ad-\ndress predictive tasks in RDL.\nDistinction Between RDL and Knowledge Graphs. The\nliterature of knowledge graphs (Bordes et al., 2013; Wang\net al., 2014; 2017) differs from RDL in terms of the tasks\nbeing tackled. Knowledge graph models mainly focus on\ncompletion tasks like predicting missing entities (e.g., Q:\nWho is the author of \"Harry Potter\u201d? A: J.K. Rowling) or\nmissing relationships (Q: Did Yoshua Bengio win a Turing\nAward? A: Yes). In contrast, RDL focuses on making\npredictions about entities or groups of entities (e.g., Will\na customer churn in the next month? How much will a\ncustomer spend in the upcoming week?)"}, {"title": "6. Conclusion", "content": "In this paper, we introduced RELGNN, a novel graph neu-\nral network framework specifically designed to address the\nstructural inefficiencies of existing heterogeneous GNNs for\nrelational databases. By leveraging atomic routes-which\ncapture high-order tripartite structures-we designed a\ncomposite message passing mechanism that enables direct\nsingle-hop interactions between heterogeneous nodes. This\navoids redundant aggregation and mitigates information en-\ntanglement, leading to more efficient and accurate predictive\nmodeling. Through extensive evaluation on RELBENCH, a\ndiverse benchmark covering 30 predictive tasks across seven\nrelational databases, RELGNN consistently outperforms\nstate-of-the-art baselines, achieving up to a 25% improve-\nment in predictive accuracy. Our findings emphasize the\nlimitations of conventional heterogeneous GNNs when ap-\nplied to relational data and highlight the necessity of models\nthat explicitly account for primary-foreign key relationships."}]}