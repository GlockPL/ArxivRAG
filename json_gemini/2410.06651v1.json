{"title": "TOWARD PHYSICS-GUIDED TIME SERIES EMBEDDING", "authors": ["Jiaxi Hu", "Bowen Zhang", "Qingsong Wen", "Fugee Tsung", "Yuxuan Liang"], "abstract": "In various scientific and engineering fields, the primary research areas have revolved around physics-based dynamical systems modeling and data-driven time series analysis. According to the embedding theory, dynamical systems and time series can be mutually transformed using observation functions and physical reconstruction techniques. Based on this, we propose Embedding Duality Theory, where the parameterized embedding layer essentially provides a linear estimation of the non-linear time series dynamics. This theory enables us to bypass the parameterized embedding layer and directly employ physical reconstruction techniques to acquire a data embedding representation. Utilizing physical priors results in a 10\u00d7 reduction in parameters, a 3\u00d7 increase in speed, and maximum performance boosts of 18% in expert, 22% in few-shot, and 53% in zero-shot tasks without any hyper-parameter tuning. All methods are encapsulated as a plug-and-play module.", "sections": [{"title": "INTRODUCTION", "content": "The explosion of real-time sensing data from the physical world opens up new opportunities for data-driven time series analysis, achieving widespread recognition in energy, transportation, education, meteorology, and other domains by leveraging the strong fitting capabilities of neural networks. However, deep time series models struggle to comprehend the underlying physical laws of data, leading to a propensity for overfitting and lacking generalizability to unseen data.\nIn numerous scientific and engineering disciplines, another central focus lies in dynamical systems that evolve over space and time, exampled in fluid mechanics, thermodynamics, and neuroscience. According to the Takens and Whitney theorems, time series can be viewed as observations stemming from underlying dynamical systems, leading to a principal way to model the essence of time series data."}, {"title": "FORMULATION", "content": "Definition 1 (Dynamical System). Let the domain S be an open subset of \\(R^d\\) and set an integer \\(k \\geq 1\\). Define the system state as \\(x : S \\rightarrow R^m\\) where \\(x = (x_1,...,x_m)\\). Then, an expression of:\n\\[F (D^kx(s), D^{k-1}x(s),...,Dx(s), x(s), s) = 0\\]\nis called a kth order system of partial differential equation (or ordinary differential equation when d = 1), where \\(F : R^{md*} \\times R^{md*^{-1}} \\times ... \\times R^{md} \\times R^m \\times S \\rightarrow R^m\\) and \\(s \\in S\\). Continuous systems typically exist on locally differentiable manifold spaces M, where \\(R\\) in the definition can be viewed as the discrete record.\nProblem Statement. Given multivariant historical sampled data \\(U \\in R^{C \\times T}\\), the time series analysis model aims to derive a nonlinear functional mapping \\(f: U \\rightarrow Y\\) for various downstream tasks,"}, {"title": "RELATED WORK", "content": "Parameterized Embedding in Deep Time Series Models. The embedding technique, serving as a space transformation \\(R^n \\rightarrow R^m\\), facilitates the mapping of discrete sparse features into continuous dense vector representations, laying a solid foundation for success across various machine learning domains. In time series analysis benchmark, mainstream methods utilize patch operation to conduct local linear projection, with certain models using convolutions to address inter-block information isolation. Additionally, for audio modeling tasks, windowed spectral transformation techniques, such as short-time Fourier transform, are commonly employed to extract time-frequency representations, thereby addressing concerns related to information density. Grid embeddings are commonly employed to handle the spatial-temporal relationships within the data. Furthermore, several studies leveraged self-supervised learning to obtain enhanced embedding representations.\nEmbedding Theory for Dynamics System Reconstruction. Since significant results proposed by and formalized in , the Embedding Theory has pervaded through almost all aspects of nonlinear dynamical systems (Definition 1). The time series \\(x(t) \\in R\\) can be broadly interpreted as successive, though not always regular, observations of a dynamical system \\(F \\in R^m\\) via a measurement function \\(h : R^m \\rightarrow R^n (n < m)\\). The main goal is to reconstruct the underlying system and explore its properties, which paved the way for developing numerous techniques like derivatives , integrals, time delay, and principal component embedding. The system dynamics is subsequently learned using preferred modeling tools such as recurrent neural networks, state space models, and reservoir computing, etc. Attraos pioneered using the time delay embedding technique in time series forecasting tasks, while our paper extensively explores various embedding techniques (Section 4.1), and provides comprehensive theoretical (Section 4.2) and experimental (Section 5) analysis."}, {"title": "PHYSICS-GUIDED TIME SERIES EMBEDDING", "content": "In this section, we commence by elucidating the implementation of our physic-guided embeddings, including the Time Delay, Principal Component, High-order Derivatives, and Integral-Differential methods. We summarize their properties before progressing to the proposed Embedding Duality."}, {"title": "TECHNICAL DETAILS", "content": "Time Delay Embedding (TD-Emb). As shown in Figure 1(d), time delay embedding augments a scalar time series \\(x \\in R\\) into a higher-dimensional dynamical system \\(F \\in R^{m \\times (T-(m-1)\\Gamma)}\\), where \\(F(t) = (x(t), x(t - \\tau),...,x(t - (m - 1)\\Gamma))\\), by embedding dimension m and time delay \\(\\tau\\). Theoretically, when m exceeds twice the dynamical dimension, the homeomorphic structure can be reconstructed . In this paper, \\(\\tau\\) is set heuristically as a quarter of the most dominant period in the signal\u00b9 and m is determined by the CC method.\nThe sine wave \\(x(t) = sin(\\omega t)\\) yields the most circular embedding in a 2D plane with \\(\\tau = 2\\pi/4\\omega\\)"}, {"title": "Principal Component Embedding (PC-Emb)", "content": "TD-Emb is the most popular method for visualizing the dynamical structures of systems within Euclidean space. However, its performance is highly sensitive to the choice of hyper-parameters. As an alternative, PC-Emb, outlined in Eq. 1, begins by applying TD-Emb to obtain X, followed by the computation of the covariance matrix C from X. Finally, a k-dimensional Principal Component Analysis (PCA) is performed to derive the system representation F. Where \\(X \\in R^{m \\times (T-(m-1))}\\), \\(C \\in R^{m \\times m}\\), and \\(F \\in R^{m \\times k}\\).\n\\[X = TD-Emb(\\tau, \\tau = 1,x) \\qquad C_{ij} = (X_{ij}) \\qquad F = PCA(k, C)\\]"}, {"title": "High-order Derivatives Embedding (HD-Emb)", "content": "In addition to the TD-Emb method, we leverage the multi-order characteristics of the system in Definition 1 by directly concatenating high-order derivatives to construct \\(F \\in R^{(m+1) \\times T}\\). In Eq. 2, we utilize the Forward Differencing technique to approximate this continuous process, with hyper-parameters: order m and discrete step size \\(\\Delta\\).\n\\[F(t) = \\begin{cases} \\left(x(t), \\frac{dx(t)}{dt}, \\dots, \\frac{d^mx(t)}{dt^m}\\right) & \\text{(Continuous)} \\\\ \\left(x(t), \\frac{dx(t)}{dt} = \\frac{x(t+\\Delta) - x(t)}{\\Delta}\\right) & \\text{(Discrete)} \\end{cases}\\]\nAlthough m and \\(\\Delta\\) can still be calculated using numerical methods , our experiment results indicate that \\(m = 3\\) and \\(\\Delta = 1\\) generally yield the best performance. In some studies related to ordinary differential equations and state-space models , \\(\\Delta\\) is defined as a learnable parameter to selectively emphasize important information in the data. In this research, we prioritize the efficiency of non-parameterized physical priors, while the exploration of trainable High-order Derivatives embedding is left for future work."}, {"title": "Integral-differential Embedding (ID-Emb)", "content": "However, in the HD-Emb method, the approximations of successive higher-order derivatives are generally negatively impacted by the signal-to-noise ratio. In Eq. 3, an alternative method is to replace the high-order terms with the integral value by only hyper-parameter \\(\\Delta\\), where continuous integration can be approximated using summation operations.\n\\[F(t) = \\begin{cases} \\left(\\int_{-\\infty}^t x(t)dt, x(t), \\frac{dx(t)}{dt}\\right) & \\text{(Continuous)} \\\\ \\left(\\int_{-\\infty}^t x(t)dt = \\Delta \\sum_{i=1}^T x(t+i)\\right) & \\text{(Discrete)} \\end{cases}\\]"}, {"title": "THEORETICAL JUSTIFICATION FOR EMBEDDING DUALITY", "content": "Proposition 4.1. The embedding method, which uses a shared linear matrix, is an integral transformation \\(h(t) = \\int_{x(s)}\\phi(t,s)d(s)\\) with limited time-invariant measure \\(\\mu\\) and polynomial basis \\(\\phi\\).\nConsidering polynomials as universal approximators for dynamical systems , Proposition 4.1 indicates that parameterized embedding methods essentially projects input time series into polynomial spectral space to represent the dynamical structure, where the measure \\(\\mu\\), or weight function sometimes, is governed by patch length, \\(\\phi\\) is parameterized by the embedding matrix.\nProposition 4.2. For the full-rank embedding matrix, the embedding process is a similarity transformation that maintains the original dynamical properties (system eigenvalues).\nTypically, the dense matrix in deep models is considered to be full rank. Proposition 4.2 shows that the parameterized embedding process is a mere space coordinate transformation, with non-linear time series dynamics linearized by the average Jacobin value within the data patch.\nLemma 4.3. A continuous function f is K-Lipschitz when \\(|| f(x_1) - f (x_2) || \\leq K||x_1 - x_2||\\), then:\n(1) The state space model with the negative diagonal matrix A and normalization layers is 1-Lipschitz.\n(2) The fully connected and convolution neural network with normalization layers is 1-Lipschitz.\n(3) The standard dot-product attention is not Lipschitz. The \\(L_2\\) attention is bounded Lipschitz.\nThe Lipschitz continuity restricts the dynamical structure under small perturbations, ensuring that when K=1, the dynamical properties are generally preserved. Lemma 4.3 allows us to disregard the influence of the encoder architecture in most cases, even though the transformer backbone may not always be optimal, to focus exclusively on the dynamical changes within the embedding layer. Moreover, it provides a solid foundation for flexibly replacing the embedding layer as needed.\nDynamical Feature Space As illustrated in Figure 4(a), for the embedding projection matrix \\(V = eig(v_1,\u2026\u2026\u2026, v_M)\\) initialized with a normal distribution, when the dimension is sufficiently large, its feature space can be considered spherical. For the gradient \\(\\nabla\\) passed into the embedding layer, we can apply the singular value decomposition (SVD) with the diagonal matrix \\(S = diag(s_1,\u00b7\u00b7\u00b7,s_\\mu)\\) and orthogonal matrix \\(U = (u^1,\u2026\u2026\u2026, u^M)\\), specifically, \\(\\nabla v_m = \\sigma u^m\\). This process can be described as the transformation of a spherical feature space (slice) into an ellipsoidal feature space.\nLemma 4.4. The Lyapunov exponents \\(\\Lambda_m = \\lim_{t\\rightarrow\\infty} \\frac{1}{t} \\ln \\sigma_m(t)\\) of the system attractors are the mean logarithmic growth rates of the principal axes lengths of the ellipsoidal feature space."}, {"title": "EXPERIMENTS", "content": "In this section, we focus on addressing the following research questions: RQ1: Does the Embedding Duality theory empirically exist? RQ2: How do physics-guided embeddings perform in expert tasks, how do they achieve this, and do they have the potential to become a new embedding paradigm? RQ3: How do physics-guided embeddings perform in foundation tasks, and do they have the potential to be transferred to large-scale time series foundational models? All Experiments are based on the Time Series Library, details regarding model backbones, experimental settings, full results, technical backgrounds, and further inspirations are reported in Appendix C."}, {"title": "EMPIRICAL EVIDENCE FOR EMBEDDING DUALITY", "content": "Visualization. As illustrated in Figure 5, we present the visual representations of previous parameterized embeddings and our proposed physics-guided embeddings (right: TD-Emb) when utilizing PatchTST as the model backbone. It is evident that the patterns exhibited by these two types of embeddings bear a remarkable resemblance. For instance, in Figure 5(a), the periodic patterns captured by the parameterized embeddings align with the consistent stripes present in dynamical structures, whereas in Figure 5(b), the smooth regions depicted in the parameterized embeddings correspond to the band-like dark regions in the dynamical structure. Notably, the data patterns captured by the physics-guided embedding have been significantly enhanced, highlighting their superior performance in various downstream tasks."}, {"title": "Dim Scaling Law", "content": "In Figure 6, we present the correlation between average model performance and hidden layer dimensions based on PatchTST backbone across three datasets (ETTm2, ETTh2, Weather). Consistent with the proposition 4.5, we observe a decrease followed by an increase in the MSE loss. Moreover, the optimal performance occurs within a specific range, typically 1-2 times the underlying dynamical dimension associated with the dataset. For example, considering a physical prior dimension of 4 for the ETTm2 dataset, where the patch length is 16, yielding a total dimension of 64, the optimal interval lies in 64-128. This remarkable discovery shows that parameterized embeddings have effectively encapsulated the intrinsic dynamical characteristics of the data."}, {"title": "Causal-directional Modeling", "content": "In Table 2, we explore the effects of unidirectional and bidirectional modeling, termed causal-directional modeling, on the performance of the Time-SSM and PatchTST models. Specifically, we adapt the attention mechanism with a causal mask and combine the SSM outcomes bidirectionally using a linear layer, respectively. Our results reveal the following insights: (a) Bidirectional modeling typically outperforms unidirectional modeling by a consistent margin, as supported by Proposition 4.6. (b) The performance gap is more pronounced in the PatchTST model, possibly attributed to SSM's adeptness in capturing dynamical structures, hence mitigating the impact of incidental dynamics. (c) Model variations incorporating physics-guided embeddings contribute to mitigating performance differentials between unidirectional and bidirectional modeling approaches."}, {"title": "PERFORMANCE FOR EXPERT MODELS.", "content": "Forecasting. We maintain the model hyper-parameters (detailed in Appendix C.3) to conduct a fair comparison for previous parameterized and our proposed physics-guided embeddings across diverse model architectures. As depicted in Table 3, the following observations can be made: (a) Generally, the incorporation of physical priors significantly boosts forecasting performance, with the most substantial gain being 11% on the Exchange dataset. (b) The HD-Emb typically delivers top performance and, due to its efficiency, is expected to become the standard embedding technology for expert models. (c) The PatchTST model shows the most significant performance improvement among the three architectures, followed by Time-SSM and ModernTCN. This could be attributed to Theorem 4.3, suggesting that the standard dot-product attention lacks Lipschitz continuity and struggles to adaptively capture the underlying dynamics, while the physical priors effectively resolve this issue. (d) Recently, community efforts have primarily focused on developing more advanced encoder architectures; however, the improvements achieved are minimal . In contrast, our proposed plug-and-play module demonstrates a significant enhancement in performance."}, {"title": "Forecasting w.r.t. Efficiency", "content": "As depicted in Figure 7, we present an efficiency visualization of various model architectures in the ETTh1 dataset. Key observations include: (a) Physics-guided embeddings bypass embedding matrices and reduce model dimensions, leading to a significant reduction in parameter count across various architectures (e.g., the PatchTST model exhibits a 10\u00d7 reduction), alongside performance improvements. (b) The necessity of deep neural networks in time series analysis tasks has long been debated, as some linear models have achieved strong results with greater efficiency . Our proposed method directly aligns the parameter count of existing deep time series models to that of linear models with superior performance, which marks physics-guided Embs, especially HD-Emb, to potentially become the standard embedding method for expert models."}, {"title": "Forecasting w.r.t. Input Length", "content": "In accordance with Table 4, we investigate the impact of input length on performance. It is observed that: (a) Across various lengths, the physics-guided embeddings consistently enhance performance, with HD-Emb exhibiting the best performance. Moreover, as the input length increases, the enhancement becomes more pronounced. This phenomenon is attributed to the fact that in embedding theory, longer input time series can better reconstruct the underlying dynamical structure, with a length of 1000 typically considered sufficient for ideal structural reconstruction. (b) The limitation of the Transformer architecture in modeling long-range dependencies has long been challenged , as model performance tends to degrade with input length over 336. However, our physics-guided embeddings offer a solution to this issue."}, {"title": "Forecasting w.r.t. Testing Curve", "content": "As depicted in Figure 8, we depict the fluctuations in test loss for the original PatchTST model and its variant integrating physics-guided embeddings (TD-test and HD-test). Noteworthy observations include: (a) Generally, data representations originating from physics-guided embeddings exhibit more consistent gradients and attain superior fitting accuracy due to the physical prior. Conversely, parameterized embeddings often grapple with overfitting issues, thereby illuminating the heightened efficacy of physics-guided embeddings. (b) High-order derivatives embedding manifests the most stable gradients and the slowest convergence rate throughout the dataset, enabling a gradual advancement toward the optimal solution."}, {"title": "Forecasting w.r.t. Various Embedding Techniques", "content": "As shown in Figure 9, we evaluate the performance of 10 embedding methods across 7 datasets. The striped bars represent the CD modeling strategy, which combines dynamical structures from multiple variables into a unified system. Key observations include: (a) Physics-guided embeddings (blue) generally outperform parameterized embeddings (red), with the HD-Emb performing the best overall, followed by the TD-Emb and ID-Emb methods. (b) Significant improvements are observed in datasets with fewer variables, like ETT, and those with strong physical characteristics, like sunspots. (c) CD strategy yields substantial improvements in datasets such as ETT#2, Weather, and ECL, but has adverse effects in ETT#1. This is attributed to data characteristics; for instance, Weather variables show similar Lyapunov and mutual information indices, indicating a shared underlying dynamical system, unlike the diverse indices in ETT#1, which favors channel-independent modeling. (d) Grid embedding performs poorly, likely due to the need to concatenate multivariate data in dynamical space, hindering the capture of system dynamics in the temporal domain. (e) Spectral embedding is also suboptimal, as time-series data is less dense than audio and spectral transformations like STFT may disrupt temporal sequencing."}, {"title": "Forecasting w.r.t. Robustness Analysis", "content": "As illustrated in Figure 10, we conduct robustness analyses using five experimental hyper-parameters across four datasets and three input lengths. The key observations include: (a) Compared to parameterized embeddings, physics-guided methods have significantly improved robustness, with this advantage further amplifying as the input length increases. (b) The parameterized embedding struggles to leverage longer time series context. Conversely, physics-guided embeddings demonstrate a more consistently increasing performance with longer input length. (c) Parameterized embeddings exhibit significant variability. Although recent models assert state-of-the-art (SOTA) results, they rely heavily on precise hyper-parameters, whereas our proposed physics-guided embeddings consistently maintain a good performance without meticulous hyper-parameter searching."}, {"title": "Classification", "content": "As shown in Table 5, we conduct an investigation in the realm of classification tasks. Our findings reveal that: (a) Akin to discoveries in the field of neuroscience , dynamical structures play a significant facilitative role in classification tasks, leading to an enhancement in performance compared to parameterized embedding across various architectures, especially for the Time Delay embedding. b) The High-order Derivatives embedding is suboptimal, and we suspect this lackluster performance may stem from the inherent smoothing nature of derivative operations, potentially leading to the loss of information beneficial for classification tasks."}, {"title": "Imputation & Anomaly Detection", "content": "As shown in Figure 11, we present the performance analysis of Imputation and Anomaly Detection tasks. Overall, physics-guided embeddings consistently improve performance on the Imputation task, with particularly notable enhancements for the SSM-based backbone (14.7% in ETTh2 dataset). However, for the Anomaly Detection task, the impact of physics-guided embeddings on performance is minimal, except for the SWAT dataset."}, {"title": "Tasks Summary", "content": "For information-intensive tasks such as forecasting and imputation, physics-guided embeddings can better comprehend the underlying dynamical characteristics and exhibit robustness, leading to significant performance improvements. For non-information-intensive tasks like classification, the dynamical structures constructed by TD-Emb methods can provide physics-related features that deep learning might overlook, thereby enhancing performance, while the HD-Emb method may lose some crucial information. In anomaly detection tasks, which may rely more on periodicity and data distribution, the impact of physics-guided embeddings is less pronounced."}, {"title": "PERFORMANCE FOR FOUNDATION MODELS", "content": "Few-shot Learning. As illustrated in Table 6, it can be observed that physics-guided embeddings yield stable and significant performance improvements, with a maximum performance boost of 21% on the Time-SSM architecture. Consistent with the forecasting task, the HD method demonstrates the best performance, closely followed by the TD method. We attribute this to the more pronounced physical characteristics of the dynamical system compared to temporal data features, as depicted in Figure 5. Therefore, in the few-shot learning, physics-guided embeddings have the capacity to encapsulate richer and more essential information, consequently amplifying the performance."}, {"title": "Zero-shot Learning", "content": "In Table 7, we delve into the performance of zero-shot learning tasks. Generally, the phenomenon of HD dominance, with TD consistently ranking second, remains evident across both intra-domain (e.g., ECL\u2192ETTh1) and cross-domain (Traffic\u2192ETTh1) tasks. As highlighted in Attraos , the underlying dynamical structures of time series display stable patterns, capturing the system's long-term evolutionary behaviors. Unlike numerical statistical information, which depends on specific datasets, the dynamical topological structures provide more fundamental insights with stronger generalization, leading to significant performance improvements."}, {"title": "Zero-shot Learning w.r.t. Input Length", "content": "Table 8 provides an analysis of the impact of varying input lengths, highlighting key trends: (a) Overall, the HD method consistently maintains superior performance, with the TD method closely following. The occasional decline in TD performance may result from its sensitivity to the hyper-parameter in noisy real-world datasets, which can distort the dynamical structures. (b) Physics-guided embeddings exhibit more substantial improvements in the MAE metric, suggesting greater sensitivity to large outliers during forecasting. (c) Except for the ETTh2 dataset, both parameterized and physics-guided embeddings effectively leverage longer contextual information for cross-dataset prediction. Notably, the physics-guided embeddings show a more substantial performance enhancement, achieving an impressive improvement of over 50% at an input length of 720, which indicates their potential to become a new embedding paradigm."}, {"title": "Discussion About Scalability", "content": "The remarkable improvements achieved by physics-guided embeddings in few-shot and zero-shot scenarios suggest their potential application in large-scale time series foundation models (LTSFM) . A crucial aspect of advancing towards physics-guided LTSFM is the necessity of the scaling laws. However, while physics-guided embeddings are available in model depth (layers) expansion, they are constrained by physical priors in model width (hidden dimensions), leading to significant constraints on memory capacity as the dataset size increases. One potential solution is to integrate physics-guided embeddings with a Mixture of Expert techniques. Diverse dynamical dimensions are established to enhance model representation and memory storage during the training stage, with an adaptive selection during the inference stage."}, {"title": "CONCLUSION & FUTURE WORK", "content": "Inspired by embedding theory, this paper demonstrates that the embedding layer in a deep time series model is an estimation of the underlying dynamics of the data. Based on this, we explore replacing parameterized embedding with numerical reconstruction techniques. Experiments show that physics-guided embeddings significantly improve performance across various tasks and backbones. In the future, we aim to advance physics-guided embeddings as a standard embedding technique for expert models and develop physics-guided time series foundation models ."}, {"title": "TECHNICAL BACKGROUND", "content": "A.1 HOW TO DETERMINE THE PHYSICAL HYPER-PARAMETER\nAs mentioned earlier, the theoretical assurances of Takens' theorem falter under finite precision and noise, prompting the exploration of \"optimal\" embedding parameters. The notion of an \"optimal\" set suggests that embeddings vary in quality. Yet, assessing this quality necessitates a metric for comparison. Apart from the empirical selection methods mentioned in the paper, there are also other mainstream approaches available. Generally, these methods can be summarized in two broad categories or arguments: prediction-based and topological arguments.\n\u2022 Prediction-based methods  notions of embedding quality can be seen to be inspired by the application of embeddings in the context of time-series prediction. Fundamentally, good embeddings should enable better predictions. These methods generally try to maximize the amount of new information incorporated in each delay dimension with the aim that it will provide more information about the true system state and aid in time series prediction.\n\u2022 Topological methods  often concentrate on analyzing the attractor structure and the distribution of the manifold within its ambient space. Essentially, a well-structured embedding in terms of topology and geometry should aim to be adequately spread out and unfolded within its ambient setting. This concept of quality aligns with Casdagli's noise amplification arguments. Geometrically-based methods may encompass metrics like the fill factor and displacement from the diagonal. In essence, the considerations for determining the optimal lag and embedding dimension for time delay embedding can be encapsulated by the notions of irrelevance and redundancy.\nA.2 DYNAMICAL ENCODER\nIn the field of machine learning, particularly in the realm of dynamical systems modeling, articles on chaotic dynamical systems primarily focus on employing recurrent neural networks (RNNs) and state-space models for modeling, relying on the autoregressive nature of models to capture underlying dynamics. Additionally, some studies are dedicated to reservoir computing , simulating the problems sensitive to initial values of partial differential equations by maintaining a random vector reservoir. Furthermore, Neural ODEs and some Physics-Informed Neural Networks (PINNs) attempt to uncover underlying patterns in data through a combination of data-driven and physics-constrained approaches. Increasingly, research indicates that deep learning models, i,e., Transformers can also achieve impressive results."}, {"title": "PROOFS", "content": "Proposition B.1. The embedding, which uses a shared linear matrix, is an integral transformation \\(h(t) = \\int_{x(s)}\\phi(t,s)d\\mu(s)\\) with limited time-invariant measure \\(\\mu\\) and polynomial basis \\(\\phi\\).\nProof. This perspective has been extensively discussed in numerous relevant literature (Gu et al., 2020; 2022; Hu et al., 2024c;b), where both patch operations and convolutional neural networks are seen as a parameterized continuous convolution process under a uniform and finite measure window, akin to a polynomial basis function projection. The Hippo theory (Gu et al., 2020) provides a detailed theoretical framework for this. Various extensions can be derived based on different basis functions and measure windows; for instance, trigonometric basis functions lead to Fourier transforms, piecewise polynomial bases result in wavelet transforms, and exponential decay bases yield the recent deep state space model S4 Gu et al. (2021).\nProposition B.2. For the full-rank embedding matrix, the embedding process is a similarity transformation that maintains the original dynamical properties (system eigenvalues)."}, {"title": "Proof", "content": "Let the underlying nonlinear time-variant dynamics is \\(\\dot{z} = g(x(t), t)\\), the dynamics for the (patched) hidden state h is:\n\\[h_{i+1} = WJW^{-1}h_i, \\qquad J = \\frac{1}{P} \\sum_{i=1}^P J (x(t), t), \\qquad J(x) = \\frac{\\partial g(x)}{\\partial x},\\]\nwhich can be regarded as a linearization for non-linear time series. The patch operation averages the Jacobian matrix J that characterizes the system dynamics. As the patch length increases, the embedding will discard more nonlinear features of the data. For a full-rank matrix W, the transformation \\(WJW^{-1}\\), known as a Similarity Transformation, where \\(WJW^{-1}\\) is unitarily equivalent to J, preserves the underlying dynamical properties, serving as a mere space coordinate transformation."}, {"title": "Lemma B.3", "content": "A continuous function f is K-Lipschitz when \\(|| f(x_1) - f (x_2) || \\leq K||x_1 - x_2||\\), then:\n(1) The state space model with the negative diagonal matrix A and normalization layers is 1-Lipschitz.\n(2) The fully connected and convolution neural network with normalization layers is 1-Lipschitz.\n(3) The standard dot-product attention is not Lipschitz. The \\(L_2\\) attention is bounded Lipschitz."}, {"title": "Proof", "content": "The first part can be found with detailed proof in Lemma 2.8 of Time-SSM (Hu et al., 2024b), and matrices A with negative diagonal eigenvalues can also be explained using left half-plane control theory. Descriptions of the second and third parts can be found in (Kim et al., 2021). According to this theorem, SSMs, CNNs, and MLPs (although not suitable for our physics-guided embeddings) can be used to stabilize the modeling of dynamical structures to preserve dynamical characteristics, while the Transformer architecture may potentially disrupt underlying dynamics during modeling. However, some recent articles have shown promising results using the Transformer architecture in modeling PDE dynamical systems , warranting further exploration in the future."}, {"title": "Proposition B.4", "content": "The Lyapunov exponents \\(\\Lambda_m = \\lim_{t\\rightarrow\\infty} \\frac{1}{t} \\ln \\sigma_m(t)\\) of the system attractors are the mean logarithmic growth rates of the principal axes lengths of the ellipsoidal feature space."}, {"title": "Proof", "content": "This is a standard theory in nonlinear dynamical systems, with detailed explanations available in Section 1.2 of the relevant literature ."}, {"title": "EXPERIMENTS", "content": "C.1 ENCODER BACKBONE\nIn this paper, we have selected state-of-the-art models based on the CNN, Transformer, and SSM architectures as the backbone encoders. The specific details are as follows.\n\u2022 Modern-TCN  is a pure convolutional architecture that incorporates both upsampling, downsampling techniques, and patching methods to stack models that separately capture temporal and channel correlations.\n\u2022 PatchTST  is the first transformer architecture to introduce chunking operations, employing a channel-independent strategy to apply the same backbone model to each time variable. It continues to maintain state-of-the-art performance in many tasks to this day.\n\u2022 TimeSSM  is a recent model architecture that applies the SSM kernel, typically utilizing patching operation and channel-independent modeling strategies, particularly excelling in prediction tasks with outstanding performance.\nC.2 DATASETS\nWe perform experiments on 8 mainstream datasets like other papers (Wu et al., 2021; Zhou et al.) to assess our model's performance, with detailed information provided in Table 9. The Dimension signifies the variable count in each dataset. Dataset Size indicates the total time points in the train, validation, and test splits. Forecasting Length specifies the future time points for prediction, with four prediction settings per dataset. Frequency represents the time point sampling interval. To elaborate:\n\u2022 ETT dataset (Zhou et al., 2021) encompasses 7 electricity transformer factors spanning from July 2016 to July 2018. We utilize four subsets: ETTh1 and ETTh2 are hourly recorded, while ETTm1 and ETTm2 are recorded every 15 minutes.\n\u2022 Exchange  compiles daily exchange rate panel data from 8 countries between 1990 and 2016.\n\u2022 Weather  integrates 21 meteorological factors recorded every 10 minutes from the Weather Station of the Max Planck Bio-geochemistry Institute in 2020.\n\u2022 Electricity  records the hourly electricity consumption data of 321 clients.\n\u2022 Traffic  collects hourly road occupancy rates measured by 862 sensors of San Francisco Bay area freeways from January 2015 to December 2016. The train, validation, and test datasets are strictly divided according to chronological order to make sure there are no data leakage.\nC.3 EXPERIMENT SETTING\nAll experiments are conducted on the NVIDIA A6000-48G GPUs. The Adam optimizer is chosen. To ensure a fair and comprehensive comparison of the superiority of our proposed method, we conduct a complete set of experiments on the Time Series Library architecture. Throughout the experimental process, we ensure consistency in the application of physics-guided embeddings and parameterized embeddings, maintaining the same hyper-parameters in both the model architecture and experimental procedures. Specifically, the number of model layers, patch length, and stride are set based on the original paper's configurations, with a learning rate of 0.0001 and a hidden dimension of 256.\nC.4 MORE VISUALIZATION\nIn Figure 12, we present the spectral diagram of embeddings obtained through more parameterized embeddings and physics-guided embeddings.\nC.5 FULL RESULTS\nWe present the full experiment results in the following tables."}]}