{"title": "Dataset Distillation via Committee Voting", "authors": ["Jiacheng Cui", "Zhaoyi Li", "Xiaochen Ma", "Xinyue Bi", "Yaxin Luo", "Zhiqiang Shen"], "abstract": "Dataset distillation aims to synthesize a smaller, representative dataset that preserves the essential properties of the original data, enabling efficient model training with reduced computational resources. Prior work has primarily focused on improving the alignment or matching process between original and synthetic data, or on enhancing the efficiency of distilling large datasets. In this work, we introduce Committee Voting for Dataset Distillation (CV-DD), a novel and orthogonal approach that leverages the collective wisdom of multiple models or experts to create high-quality distilled datasets. We start by showing how to establish a strong baseline that already achieves state-of-the-art accuracy through leveraging recent advancements and thoughtful adjustments in model design and optimization processes. By integrating distributions and predictions from a committee of models while generating high-quality soft labels\u00b9, our method captures a wider spectrum of data features, reduces model-specific biases and the adverse effects of distribution shifts, leading to significant improvements in generalization. This voting-based strategy not only promotes diversity and robustness within the distilled dataset but also significantly reduces overfitting, resulting in improved performance on post-eval tasks. Extensive experiments across various datasets and IPCs (images per class) demonstrate that Committee Voting leads to more reliable and adaptable distilled data compared to single/multi-model distillation methods, demonstrating its potential for efficient and accurate dataset distillation. Code is available at: https://github.com/Jiacheng8/CV-DD.", "sections": [{"title": "1. Introduction", "content": "The rapid growth of large datasets has significantly advanced computer vision and deep learning applications, enabling models to achieve high accuracy and generalization across diverse domains. However, training on massive datasets presents challenges such as high computational cost, memory usage, and long training times, especially for resource-constrained environments. To address these issues, dataset distillation has emerged as an effective technique to condense large datasets into smaller, representative sets, allowing for efficient model training with minimal performance loss. Despite its promise, a key challenge in dataset distillation remains: capturing the essential features of the original data while avoiding overfitting to specific patterns or noise.\nPrior dataset distillation methods [28, 31, 35, 36] often rely on single-model frameworks that may struggle to generalize across complex, diverse datasets and architectures. These approaches can introduce biases specific to the model used, resulting in distilled datasets that may not fully capture the richness of the original data. To overcome these limitations, we propose Committee Voting for Dataset Distillation (CV-DD), a framework that leverages multiple models' perspectives to create a high-quality, balanced distilled dataset. Our first contribution in this work is to identify pitfalls, dis-"}, {"title": "2. Related Work", "content": "Dataset Distillation. Dataset distillation aims to generate a compact, synthetic dataset that retains essential information from a large dataset. This approach facilitates easier"}, {"title": "3. Approach", "content": "The overall framework of our proposed CV-DD is illustrated in Fig. 2. Essentially, CV-DD builds upon the enhanced baseline and employs a Prior Performance Guided Voting Strategy during the optimization of synthetic data, address-ing the limitation of previous ensemble-based method, which"}, {"title": "3.1. Preliminaries", "content": "The goal of dataset distillation is to create a compact synthetic dataset that retains essential information from the original dataset. Given a large labeled dataset D =\n{(U\u2081, V\u2081), ..., (U|D|, V|D|)}, we aim to learn a smaller synthetic dataset Dsyn = {(\u00fb\u2081, \u00db\u2081), ..., (\u00db|Dsyn|, \u00db|Dsyn|)}, where\n|Dsyn| < |D|. The objective is to minimize the performance\ngap between models trained on Dsyn and those trained on D:\n$\\sup_{(u,v)~D} | L (f_{vD} (U), v) \u2013 L (f_{vDsyn} (U), v) | < \u03b4, $\n(1)\nwhere 8 is the allowable gap. This leads to the following\noptimization problem:\n$\\arg \\min_{Dsyn, Dsyn} \\sup_{|(u,v)~D} L (f_{v} (u), v) - L (f_{vDsyn} (u), v)$\n(2)\nThe goal is to synthesize Dsyn while determining the optimal\nnumber of samples per class."}, {"title": "3.2. Pitfalls of Latest Methods", "content": "Diversity and bias issues. SRe2L [36] is a recently proposed optimization-based method that generates distilled data by aligning the Batch Normalization (BN) statistics of synthetic data with those from the training process while simultaneously ensuring the alignment between synthetic data labels and their true labels. The primary limitation of this method is its reliance on a single backbone network for generating distilled data, resulting in limited diversity and increased model-specific bias.\nInformativeness and realistic issues. Prior ensemble-based dataset distillation methods, such as G-VBSM [24] and MTT [1], utilize multiple backbones to generate distilled data. However, these methods assume that all pre-trained models contribute equally to the recovery of synthetic data, failing to prioritize the contributions of more informative models during optimization. Moreover, both MTT and G-VBSM encounter efficiency challenges: MTT matches the training trajectories of multiple models, rendering it highly inefficient and incapable of scaling to large datasets. In con-trast, while G-VBSM is scalable to large datasets, it incurs substantial computational overhead due to the additional alignment of convolutional statistics.\nSuboptimal soft labels. Prior generative dataset distillation methods [1, 24, 36] have overlooked the distributional shift between synthetic and original images, a critical factor that influences the fidelity and representativeness of the distilled data. This oversight has led to the generation of suboptimal soft labels, ultimately resulting in a significant reduction in generalization capability."}, {"title": "3.3. Overview of CV-DD", "content": "Essentially, CV-DD builds upon the enhanced baseline and employs a Prior Performance Guided Voting Strategy during the optimization of synthetic data, address-ing the limitation of previous ensemble-based method, which"}, {"title": "3.4. Building a Strong Baseline", "content": "Many dataset distillation methods use SRe2L [36] as a base-line for performance comparison [24, 25, 28]. However, due to suboptimal design and insufficient hyper-parameter tuning in post-evaluation, some methods appear to surpass SRe2L without truly outperforming it. This subsection intro-duces SRe2L++, a more robust baseline that achieves state-of-the-art performance. The performance improvements of SRe2L++ over SRe\u00b2L are illustrated in Fig. 3.\nReal Image Initialization: The original SRe2L method uses Gaussian noise for initialization during the recover stage. However, EDC [25] shows that initializing with real data improves quality at the same optimization cost. Thus, SRe2L is enhanced by replacing Gaussian noise initialization with real image patches generated by RDED [28].\nData Augmentation for Small Datasets: The original SRe2L omitted data augmentation (e.g., random cropping, re-sizing, flipping) during recovery on small-resolution datasets (e.g., CIFAR-10, CIFAR-100), limiting performance. This has been addressed by incorporating data augmentation, with its impact shown in the Fig. 7.\nBatch-Specific Soft Labeling: To further enhance the per-formance of SRe\u00b2L, we apply the proposed Batch-Specific Soft Labeling technique, which will be elaborated in a later subsection 3.7.\nSmoothed Learning Rate and Smaller Batch Size: Prior studies [25, 28, 35] suggest reducing batch size to increase the number of iterations per epoch, thereby mitigating under-convergence, and adopting a smoothed learning rate sched-uler to avoid convergence to suboptimal minima."}, {"title": "3.5. Committee Choices", "content": "Inspired by ensemble-based dataset distillation methods like MTT [1], FTD [6], and G-VBSM [24], which utilize mul-tiple backbones to enhance performance, CV-DD incorpo-rates a diverse set of five backbones: DenseNet121 [11], ResNet18 [8], ResNet50 [8], MobileNetV2 [23], and Shuf-fleNetV2 [37]. This mix of lightweight and standard archi-tectures improves the diversity and generalization of dis-tilled data. Specifically, CV-DD retains the same backbone throughout the optimization process and switches to differ-ent backbones only when generating new synthetic data, i.e., an approach we refer to as \u201cSwitch Per IPC\u201d. This strategy, as illustrated in Fig. 2, ensures that each distilled dataset is optimized under consistent backbones, thereby promoting a more stable learning process. By utilizing diverse backbones, CV-DD improves the diversity of distilled data. As shown in Fig. 4, CV-DD consistently surpasses SRe\u00b2L++ in data diversity across various classes."}, {"title": "3.6. Committees Voting Strategy", "content": "To address the limitation of previous Ensemble Based Method, we propose a Prior Performance Guided Voting Strategy that ensures the models with stronger prior perfor-mance exert a greater influence on the optimization process. This subsection details the computation of prior performance scores and illustrates how CV-DD effectively utilizes them to optimize the distilled data.\nPrior Performance Assignment: Given a model optimized using the specified loss function in Equation 3, where T represents the Dataset and po(x) denotes the predicted prob-ability distribution from the model parameterized by 0.\n$\\\u04e8_\u0442 = arg \\min_{E(x,y)~T} -\\sum_{i=1}^C y_i log (p_\u03b8(x)_i)$ (3)\nWe leverage the information embedded in the pre-trained model to generate distilled data, with the quality of the dis-"}, {"title": "3.7. Batch-Specific Soft Labeling", "content": "In the post-evaluation stage, a teacher model is commonly employed to pre-generate soft labels [26], thereby enhancing the generalization of the student model [10, 18].\nTypically, the teacher model includes Batch Normaliza-tion layers [20, 24, 28, 36], which utilize running statistics to normalize features. These statistics are progressively up-dated during training, as detailed in Equations 6 and 7.\n$\\Prunning \u2190 & Prunning + (1 \u2212 \u03b1) \u03bc_B$\\n$\\Grunning \u2190 a running + (1 \u2212 \u03b1) \u03c3^2_B$\\n(6)\n(7)\nwhere a is the momentum, and \u00b5\u03b2, \u03c3f are the mean and\nvariance of the current batch, respectively.\nHowever, as shown in Fig. 5, we observe that even if the generated images match the BN distribution during synthesis, there is still a significant gap between the BN distribution of the synthetic images and that of the original dataset, due to the influence of regularization terms and optimization randomness.\nTo address this, we propose Batch-Specific Soft Label-ing (BSSL): Instead of using pre-trained BN statistics from the original images in a real dataset, we recompute the BN statistics directly from each batch of synthetic images, keep-ing all other parameters frozen with the teacher's original pre-trained values each time soft labels are generated. Algo-rithm 1 presents a simple implementation of BSSL. In the post-evaluation phase, this method generates soft labels by setting the teacher model to training mode. This straight-forward adjustment significantly improves the performance of the model during post-training on synthetic data using these soft labels. Specifically, for a given distilled data batch B = {xi | i = 1,2,..., N}, where each xi represents a sample in the batch and is a vector of features, represented as xi \u2208 RC\u00d7H\u00d7W, where C is the number of channels, and H and W are the height and width, respectively. For BN layer, the mean and variance are calculated per channel as:\n$\\\u03bc_{B,c} = \\frac{1}{NHW} \\sum_{i=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} X_{i,c,h,w}$ (8)\n$\\sigma^2_{B,c} = \\frac{1}{NHW} \\sum_{i=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (X_{i,c,h,w} - \u03bc_{B,c})^2 + \u03b5$\n(9)\nwhere \u00b5B, and are the batch-specific mean and vari-ance for each channel c. Here, e is a small constant added"}, {"title": "4. Experiments", "content": "This section evaluates the performance of our proposed method, CV-DD, against state-of-the-art approaches across various datasets, neural architectures, and IPC configurations. Additionally, we conduct comprehensive ablation studies, overfitting mitigation analysis and cross-architecture gener-alization experiments to further validate its effectiveness."}, {"title": "4.1. Dataset and Experimental Configuration", "content": "Detailed configurations, including the hyper-parameters for each stage, are provided in the Appendix.\nDatasets. To comprehensively evaluate the performance of CV-DD, we test it on both low-resolution and high-resolution datasets. The low-resolution datasets include CIFAR-10 [14] and CIFAR-100 [14], both with a resolu-tion of 32\u00d732. For high-resolution datasets, we use Tiny-ImageNet (64\u00d764) [34], ImageNet-1K (224\u00d7224) [4], and ImageNette [4], which is a subset of ImageNet-1K.\nBaseline Methods. We selected RDED [28], a recent state-of-the-art dataset distillation method, as our primary baseline due to its strong performance. Additionally, we incorpo-rated MTT [1] and G-VBSM [24], two ensemble-based ap-proaches, to further evaluate the effectiveness of our tailored ensemble method, CV-DD. Finally, we included CDA [35], a recent optimization-based method, and SRe2L++, which integrates the latest advancements and achieves the best per-formance among these methods, to comprehensively assess CV-DD's effectiveness."}, {"title": "4.2. Main Results", "content": "High-Resolution Datasets. To evaluate the effectiveness of our approach on large-scale and high-resolution datasets, we compare it against state-of-the-art dataset distillation methods on Tiny-ImageNet, ImageNet-1K, and its subset ImageNette. As shown in Table 1, our method, CV-DD, con-sistently outperforms previous SOTA methods across all IPC settings. Notably, on ImageNet-1K at 50 IPC with ResNet18, CV-DD achieves an impressive 59.5%, surpassing our strong baseline SRe2L++ by +1.9%, CDA by +6%, and RDED by +3%. The only exception occurs on Tiny-ImageNet with IPC = 50, where CV-DD falls slightly behind. However, given its substantial improvements on other datasets, this isolated result does not undermine CV-DD's overall effectiveness on high-resolution datasets.\nLow-Resolution Datasets. To demonstrate the applicability of CV-DD beyond high-resolution datasets, we conducted additional experiments on small datasets. As shown in Ta-ble 1, CV-DD consistently delivers outstanding performance across all IPC settings and backbone networks, significantly exceeding the results of prior SOTA baseline methods. For instance, on CIFAR-100 with ResNet18 at IPC=10, CV-DD reaches 53.6% accuracy, outperforming RDED by +11%, SRe2L++ by +1.5%, and CDA by +3.8%. These findings fur-ther validate the robustness and adaptability of CV-DD, em-phasizing its capability to perform effectively across datasets of varying resolutions and complexities.\nComparison with State-of-the-Art Ensemble Methods. To ensure a fair comparison, we trained CV-DD for 1000 epochs on small-resolution datasets e.g., CIFAR-10 and CIFAR-100. Given the substantial variance in training configurations, G-VBSM's results are reported separately from Table 1. In Table 2, we compare the performance of our method (CV-DD) with previous ensemble-based methods across different datasets and resolutions. Notably, CV-DD demonstrates superior performance in IPC=50 set-"}, {"title": "4.3. Analysis", "content": "Overfitting Analysis. CV-DD effectively mitigates over-fitting during the post-training phase. Fig. 6 shows the train and test top-1 accuracy of CV-DD and SRe\u00b2L++ over epochs. Notably, CV-DD maintains lower training accuracy but consistently achieves higher test accuracy compared to SRe2L++. These results demonstrate the effectiveness of CV-DD's Prior Performance Guided Voting Strategy as a regularization method in overfitting-prone scenarios.\nEfficiency analysis. We compared the efficiency of CV-DD"}, {"title": "4.4. Cross-Architecture Generalization", "content": "A key criterion for evaluating distilled data is its ability to generalize across diverse network architectures, ensuring broader applicability in real-world scenarios. To assess this, we compared the performance of CV-DD committee dis-tilled data against RDED, G-VBSM, and SRe2L++ across eleven architectures, from lightweight models like Shuf-fleNetV2 [37] to complex networks such as Wide ResNet50-2 [8]. As shown in Table 4, CV-DD consistently out-performed other methods. Notably, architectures such as RegNet-X-8GF [21], EfficientNet [29], among others, which were not included in the committee, also demonstrated strong performance, further highlighting the robustness and versa-tility of CV-DD. A visualization of the performance trends with respect to parameter size is provided in Appendix."}, {"title": "4.5. Ablation Study", "content": "Effect of the Number of Selected Experts. To validate that using two experts for updating distilled data (N = 2) per gradient step is optimal, we conducted ablation studies to examine how performance varies with different N. Since in-creasing the number of experts adds computational overhead,"}, {"title": "5. Conclusion", "content": "We propose Committee Voting for dataset distillation, a novel framework that synthesizes high-quality distilled datasets by leveraging multiple experts and produces high-quality soft labels through Batch-Specific Soft Labeling. Our approach first establishes a strong baseline that achieves state-of-the-art accuracy through recent advancements and carefully optimized framework design. By combining the distributions and predictions from a committee of models, our method captures rich data features, reduces model-specific biases, and enhances generalization. Complementing this, the gener-ation of high-quality soft labels provides precise supervisory signals, effectively mitigating the adverse effects of distribu-tion shifts and further enhancing model performance. Build-ing on these strengths, CV-DD not only promotes diversity and robustness within the distilled dataset but also reduces overfitting, resulting in consistent improvements across vari-ous configurations and datasets. Our future work will focus on applying the idea of Committee Voting to more modalities and applications of dataset distillation tasks."}]}