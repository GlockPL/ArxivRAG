{"title": "DSSRNN: Decomposition-Enhanced State-Space Recurrent Neural Network for Time-Series Analysis", "authors": ["Ahmad Mohammadshirazi", "Ali Nosratifiroozsalari", "Rajiv Ramnath"], "abstract": "Time series forecasting is a crucial yet challenging task in machine learning, requiring domain-specific knowledge due to its wide-ranging applications. While recent Transformer models have improved forecasting capabilities, they come with high computational costs. Linear-based models have shown better accuracy than Transformers but still fall short of ideal performance. To address these challenges, we introduce the Decomposition State-Space Recurrent Neural Network (DSSRNN), a novel framework designed for both long-term and short-term time series forecasting. DSSRNN uniquely combines decomposition analysis to capture seasonal and trend components with state-space models and physics-based equations. We evaluate DSSRNN's performance on indoor air quality datasets, focusing on CO2 concentration prediction across various forecasting horizons. Results demonstrate that DSSRNN consistently outperforms state-of-the-art models, including transformer-based architectures, in terms of both Mean Squared Error (MSE) and Mean Absolute Error (MAE). For example, at the shortest horizon (T=96) in Office 1, DSSRNN achieved an MSE of 0.378 and an MAE of 0.401, significantly lower than competing models. Additionally, DSSRNN exhibits superior computational efficiency compared to more complex models. While not as lightweight as the DLinear model, DSSRNN achieves a balance between performance and efficiency, with only 0.11G MACs and 437MiB memory usage, and an inference time of 0.58ms for long-term forecasting. This work not only showcases DSSRNN's success but also establishes a new benchmark for physics-informed machine learning in environmental forecasting and potentially other domains. To facilitate further research and application, we provide our datasets and code at https://github.com/ahmad-shirazi/DSSRNN.", "sections": [{"title": "1 INTRODUCTION", "content": "The prediction of complex time series data stands as a critical challenge in machine learning (ML), particularly where accuracy and computational efficiency intersect [17, 19, 41, 43, 43, 44]. Outlier events in time series, such as spikes or dips, can disrupt forecasting models, necessitating robust detection and handling methods to maintain accuracy [3]. Time series are omnipresent in today's data landscape, underpinning critical applications across various domains, from traffic flow estimation [29] and energy management [8] to financial investment [2, 32]. In time series analysis, the challenge isn't just long-term or short-term forecasting [41] but also preparing datasets, often impeded by missing data[7, 11]. The integration of state-space models, founded on physics principles, with advanced ML techniques, presents a versatile solution that transcends domain boundaries [14, 16, 39]. This synthesis transforms physics-based equations into computational algorithms, providing a robust framework capable of capturing the complex temporal dynamics across a range of real-world problems-from forecasting energy consumption in buildings to predicting building energy consumption [28, 31, 34, 42] and weather patterns [4, 15, 24].\nA significant portion of prior work in this area has focused on the use of transformers [37]. However, transformers face several challenges in time series prediction. For long-term forecasting, their self-attention mechanism, which scales quadratically with sequence length, leads to high computational costs and memory usage. Capturing long-range dependencies effectively can be problematic, potentially diluting important signals over time. Transformers also suffer from the vanishing gradient problem and limitations of fixed context windows. In short-term prediction, transformers are prone to overfitting due to their high capacity, struggle with noise sensitivity, and may not generalize well with limited data. Despite shorter sequences, they remain computationally expensive compared to simpler models. Additionally, transformers require extensive hyper-parameter tuning and are less interpretable than traditional models [41]. Our methodology addresses these limitations through the use of state-space models that capture the dynamics of complex systems [13], essentially marrying the precision of physics with the adaptability of machine learning. This integrated approach enables prediction accuracy while optimizing computational resources.\nOur research, though broadly applicable, specifically targets the domain of Indoor Air Quality (IAQ), with a particular focus on predicting carbon dioxide (CO2) levels. This variable is of critical health relevance and presents significant predictive challenges due to its nonlinear behavior and sensitivity to numerous factors, including human occupancy, activity patterns, and ventilation systems [35]. The complexity of CO2 dynamics makes it ideal for demonstrating our models' efficacy in handling intricate dependencies and varying conditions.\nWe validate our work in a range of indoor environments, showing that our model surpasses existing benchmarks in accuracy and efficiency. Its success in forecasting using data with a range of characteristics demonstrates a potential for wider application in time series forecasting.\nThis paper aims to achieve the following objectives:"}, {"title": "2 RELATED WORK", "content": "Given that we explore two crucial aspects: imputation, which addresses missing data to ensure dataset integrity, and prediction, focusing on forecasting methodologies, we present background work on both these topics."}, {"title": "2.1 Review of Imputation Methods", "content": "Accurate imputation in time series is crucial for dataset preparation, as in our case detailed in Table 1. Traditional RNN-based methods, such as GRU-D [7], have introduced time decay factors to address missing data. Approaches like BRITS [5] and M-RNN [40] enhance this with bidirectional RNNs, yet they grapple with the inherent complexities of long-term dependencies and computational intensity. Generative models, including GANs and VAEs, provide alternative strategies [6, 12, 20, 23], but their training complexity and interpretability issues limit their practical scalability.\nThe emergence of self-attention-based models has introduced a new paradigm in time series imputation. Models like DeepMVI [1] and NRTSI [33] leverage Transformer architectures for handling multidimensional and irregularly sampled time series data. Despite their novel approach, these models face challenges in computational efficiency and generalizability. Du et al. (2023) recently advanced this field with SAITS, a model that optimizes imputation through diagonally-masked self-attention blocks, addressing both temporal dependencies and feature correlations with enhanced efficiency [10].\nOur approach diverges significantly from these existing methods. As noted in the introduction, we integrate domain knowledge with ML. This enhances the accuracy of our model while also addressing the limitations of existing approaches. Additionally, leveraging neural networks within this framework allows for efficient processing of larger datasets, balancing robustness with computational agility."}, {"title": "2.2 Review of Prediction Techniques", "content": "Transformer models by Vaswani et al. (2017) [36], have revolutionized time series forecasting through their innovative use of self-attention mechanisms, enabling the model to focus selectively"}, {"title": "3 DECOMPOSITION STATE-SPACE RECURRENT NEURAL NETWORK (DSSRNN)", "content": "Our proposed model, the Decomposition State-Space Recurrent Neural Network (DSSRNN), builds upon two innovative physics-infused ML models: the State-Space Recurrent Neural Network (SS-RNN), Figure 2, and its advanced iteration, the Decomposition State-Space Recurrent Neural Network (DSSRNN), Figure 3. The SS-RNN framework synergizes the robustness of state-space physics with the flexibility of recurrent neural networks, while the DSSRNN employs moving-average decomposition to filter out high-frequency noise inherent in temporal datasets.\nFinally, note that DSSRNN, uses the same framework for the imputation as well as the prediction tasks. This dual functionality underscores the versatility of our model, providing a cohesive approach to managing and forecasting time series data."}, {"title": "3.1 Data", "content": "In our study, we utilized data from Lawrence Berkeley National Laboratory's Building 59 in California, covering the period from"}, {"title": "3.2 Imputation Methods", "content": "In this section, we explore the imputation methods utilized to address the challenge of missing data within our datasets. Table 1 provides a detailed examination of the data absence across various input variables collected from four distinct office environments, spanning from January 2020 to April 2020. To effectively manage this dataset, we segregated it into training, validation, and testing subsets, with all instances of missing data allocated to the testing subset. Our proposed DSSRNN model is uniquely designed to tackle both prediction and imputation tasks using the same architecture. It evaluates performance metrics such as Mean Squared Error (MSE) and Mean Absolute Error (MAE) during the validation phase to accurately impute missing values. Given our development of the DSSRNN model to serve dual purposes-both prediction and imputation-using a unified architecture, we will delve into a comprehensive explanation of our DSSRNN model in the subsequent section dedicated to prediction."}, {"title": "3.3 Prediction Methods", "content": "In the prediction section of the paper, we delve into the architecture of our advanced model, the Decomposition State-Space Recurrent Neural Network (DSSRNN). This section outlines the foundational State-Space Recurrent Neural Network (SS-RNN) structure and its extension into the DSSRNN model, which incorporates decomposition techniques for enhanced prediction capabilities. We will first elaborate on the SS-RNN architecture, explaining its design and how it integrates the robustness of physic-based and state-space models with the adaptability of recurrent neural networks. Subsequently, we will discuss the DSSRNN and how it utilizes decomposition to refine the prediction of time series data, including the identification and forecasting of outlier events. This approach underpins the robustness of our model in predicting critical, rare events within the datasets, which we illustrate through a comparative analysis with leading models in the domain."}, {"title": "3.3.1 State-Space Recurrent Neural Networks (SS-RNN)", "content": "The State-Space Recurrent Neural Network (SS-RNN) architecture represents a novel blend of traditional state-space models with the dynamic capabilities of recurrent neural networks (RNNs). At its core, SS-RNN aims to model time series data by capturing both the underlying physical processes and temporal dependencies. This is achieved through a combination of state variables that embody the physical state of the system and RNN layers that process sequential data, allowing for the integration of prior knowledge and temporal context. The SS-RNN architecture is designed to efficiently handle the flow of information through time, making it particularly adept at predicting complex dynamics in time series data, such as those encountered in IAQ monitoring.\nThe transformation from physic to ML equations begins with the foundational state-space representation of indoor CO2 dynamics [30], as shown in Equation 1. This equation models the temporal change in indoor CO2 concentration by relating it to variables that capture both the physical characteristics of the environment and the dynamics of CO2 exchange. Specifically, the mass flow rate m reflects the air exchange due to ventilation, represented by $m\\times CO2\\_out$, p signifies air density, V stands for the volume of"}, {"title": "3.3.2 Decomposition State-Space Recurrent Neural Networks (DSSRNN)", "content": "DSSRNN architecture advances the SS-RNN framework by incorporating decomposition techniques to dissect and model the temporal data into trend and seasonal components by incorporating a decomposition mechanism to capture and model both global and local temporal structures within the data. Specifically, the trend component, represented as a moving average calculated over a 24-hour window, captures the global patterns. In contrast, the seasonal component-obtained by subtracting this trend from the observation window-reflects local fluctuations. As depicted in Figure 3, the observation window provides input to two parallel pathways: one for isolating seasonal patterns processed by the SS-RNN, and another for delineating the trend via a linear model. The outputs of these pathways are then synergistically combined to yield the final output, capturing both the nuanced cyclicity and underlying directional movement in the data. This bifurcation allows the DSSRNN to adeptly handle the intricacies of time series forecasting, enhancing its ability to identify and predict complex patterns influenced by both long-term trends and short-term seasonal variations."}, {"title": "3.3.3 Predicting outlier events", "content": "In order to demonstrate the robustness of our model in predicting outlier events within the CO2 data, we employ a binary classification approach based on the box-and-whisker plot method. Specifically, CO2 measurements above the upper whisker-calculated as Q3 + 1.5 \u00d7 IQR where Q3 is the third quartile and IQR is the interquartile range-are classified as outliers and encoded as '1', while non-outliers are assigned '0' (See the threshold value for each office in Table 2. This binary transformation allows us to specifically focus on the prediction of outlier occurrences. We then compare the performance of our model in predicting these binary outcomes against SoTA models, showcasing our model's capability to identify significant deviations in air quality. The number of outlier events detected for each office is systematically cataloged in Table 2. The comparative results and analysis are presented in detail in the Result section of the paper."}, {"title": "4 EXPERIMENTS", "content": "In the results section of our paper, we evaluate the DSSRNN's performance using the robust NVIDIA A100 GPU at the Ohio Supercomputer Center (OSC)."}, {"title": "4.1 Imputation Analysis", "content": "In the results section outlined in Section 3.2, we present a comprehensive evaluation of our DSSRNN model in comparison with other imputation methods. The evaluation, focused on the imputation accuracy, is crucial given the presence of missing values documented in Table 1. Our approach, embodied by the DSSRNN model, is meticulously compared against alternative methodologies, including DLinear (a linear-based model) [41] and SAITS (a self-attention-based model) [10], in terms of their ability to handle missing data effectively. The comparative analysis, as presented in Table 4, demonstrates the superior performance of DSSRNN in minimizing both MSE and MAE metrics. For office 1, DSSRNN attains an MSE of 0.357 and an MAE of 0.375, which are the lowest among the methods compared. This pattern of outperformance continues across Office 2, Office 3, and Office 4, with DSSRNN maintaining the lead in minimizing error metrics, signifying its robustness in dealing with incomplete data.\nDLinear, while being a robust model based on linear methodologies, exhibits higher MSE and MAE metrics compared to DSSRNN. This indicates that despite its strengths, DLinear encounters limitations when processing datasets with missing information. In contrast, DSSRNN, which transcends the linear approach of DLinear, captures complex temporal dynamics through a deeper understanding of the data's underlying processes, thereby offering a more comprehensive representation of patterns. However, SAITS, despite embodying the forefront of self-attention-based imputation"}, {"title": "4.2 Prediction Analysis", "content": "In the comprehensive assessment of forecasting methodologies detailed in Table 3 eclipsing a spectrum of advanced models. Our experimental procedures, aligned with the benchmarks set by Zeng et al. (2023) [41], ensured a balanced comparison across the forecast horizons T=96, 192, 336, 720, catering to the input length for long-term time series forecasting. DSSRNN consistently delivered the most accurate forecasts, achieving the lowest MSE and MAE values at all forecast lengths and across all office environments. For example, at the shortest horizon (T=96) in Office1, DSSRNN reported an MSE of 0.378 and an MAE of 0.401, establishing its proficiency in capturing intricate temporal dynamics. Among the linear-based models, Dlinear showed commendable performance, especially at medium to long forecast horizons, marking it as the best within its category. Within the transformer-based models, 'Transformer' excelled at the shortest horizon (T=96), while 'Informer' demonstrated its strength at extended lengths (T=192, 336, and 720), adept at handling long-range data dependencies.\nOur research into short-term time series forecasting evaluates the DSSRNN model across various sequence lengths ranging from 24 to 720-time steps, examining its performance in four office settings for both short-term (e.g., 24-time steps) and long-term forecasting (e.g., 720-time steps). As detailed in Figure 4, DSSRNN performs greatly in most settings and although TIDE has a superior accuracy in some cases, considering its high computational cost, our model can be seen as a better choice, striking a balance between computational cost and accuracy. These findings underscore DSSRNN's adaptability to various forecasting horizons and its transformative potential in time series analysis.\nClassification\nIn an effort to further comprehend the regression capabilities of our DSSRNN model, we transformed the regression task into a classification problem, as meticulously explained in Section 3.3.3. The results of this transformation are depicted in Table 5, where we delineate the thresholds set for event detection, the count of events, and their respective percentages for each office environment.\nThe classification approach allows us to dissect the model's performance in distinguishing between high-concentration and normal event occurrences. While the DLinear model exhibits commendable recall for high-concentration events-likely a result of its decomposition capability, breaking down time series into seasonal and trend components-the Autoformer model shows a preference for normal"}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "In this study, the Decomposition State-Space Recurrent Neural Network (DSSRNN) was introduced, integrating physics and state-space methodologies to adeptly impute missing data and forecast indoor air pollutant concentrations. The architecture of this model is designed for computational efficiency, surpassing the performance of conventional transformer-based models. The DSSRNN has shown exceptional proficiency, outperforming established benchmark models in both MSE and MAE, which highlights its potential for accurate regression and classification predictions and reliable data imputation. The success of the DSSRNN paves the way for the application of physics-informed machine learning models in environmental data analysis and potentially in other predictive domains.\nFor future work, we aim to apply the integration of physics-based concepts and state-space models with ML techniques to broader domains, such as the modeling and forecasting of energy consumption in buildings. This will involve analyzing usage patterns alongside external factors like weather conditions. Additionally, we will explore the optimization of climate control systems, focusing on heating, ventilation, and air conditioning to adapt efficiently to environmental variations. This expansion will harness the DSSRNN model's strengths in addressing both indoor and outdoor pollutant dynamics and energy efficiency challenges."}]}