{"title": "Treat Stillness with Movement: Remote Sensing Change Detection via Coarse-grained Temporal Foregrounds Mining", "authors": ["Xixi Wang", "Zitian Wang", "Jingtao Jiang", "Lan Chen", "Xiao Wang", "Bo Jiang"], "abstract": "Current works focus on addressing the remote sensing change detection task using bi-temporal images. Although good performance can be achieved, however, seldom of they consider the motion cues which may also be vital. In this work, we revisit the widely adopted bi-temporal images-based frame-work and propose a novel Coarse-grained Temporal Mining Augmented (CTMA) framework. To be specific, given the bi-temporal images, we first transform them into a video using interpolation oper-ations. Then, a set of temporal encoders is adopted to extract the motion features from the obtained video for coarse-grained changed region prediction. Subsequently, we design a novel Coarse-grained Foregrounds Augmented Spatial Encoder module to integrate both global and local information. We also introduce a motion augmented strategy that leverages motion cues as an additional out-put to aggregate with the spatial features for improved results. Meanwhile, we feed the input image pairs into the ResNet to get the different features and also the spatial blocks for fine-grained fea-ture learning. More importantly, we propose a mask augmented strategy that utilizes coarse-grained changed regions, incorporating them into the decoder blocks to enhance the final changed prediction. Extensive experiments conducted on multiple benchmark datasets fully validated the effectiveness of our proposed framework for remote sensing image change detection. The source code of this paper will be released on https://github.com/Event-AHU/CTM_Remote_Sensing_Change_Detection.", "sections": [{"title": "1 Introduction", "content": "Remote sensing image change detection targets finding the variable pixel-level regions between given two images. This task can be used in many practical scenarios, including damage assessment, urban studies, ecosystem monitoring, agricultural surveying, and resource management. Although good performance can already be achieved in some simple scenarios, remote sensing change detection is still a challenging task in extreme cases.\nExisting researchers usually adopt Convolutional Neural Networks (CNN) [1] and Transformers [2] to build their backbones for remote sensing image change detection, as illustrated in Fig. 1(a). Specifically, Chen et al. [3] propose a Bitemporal Image Transformer (BIT) that mines the contexts within the spatial-temporal domain effectively. The Visual change Transformer (VcT) proposed by Jiang et al. [4] finds that the mining of the common background information helps the consistent representations which further enhances the visual"}, {"title": "2 Related Work", "content": "In this section, we provide a brief introduction to the works focused on Remote Sensing Image Change Detection [8] and Spatial-Temporal Feature Learning."}, {"title": "2.1 Remote Sensing Image Change Detection", "content": "Remote sensing image change detection is an important application field in remote sensing technology, and its purpose is to detect changes in surface features by comparing remote sensing images from different periods. Most of the existing methods are based on spatial modeling of two images, and direct pairwise difference or splicing of images for change detection. Jiang et al. [4] propose that VCT enables the model to learn a consistent representation between two images by mining spatial contextual information. Goswami et al. [9] used decision tree algorithms and post-classification comparisons of separation matrices, as well as image differencing in algebraic techniques to detect two images. Li et al. [10] propose a novel lightweight network, called A2Net, that recognizes changes by moving network-extracted features in combination with progressive feature aggregation and supervised attention. Zhou et al. [11] proposed a Context Aggregation Network (CANet) for mining cross-image contextual infor-mation between all training images to further enhance contextual representation within a single image. Since the development phase of change detection, a great deal of work has been devoted to enhancing the representation of spatial features. However, specialized modeling of time has long been neglected [7]. Methods focusing on the time dimension include two main categories, recurrent neural network (RNN) [12] based methods and attention-based [13] methods.\nRNNs process sequential data with a memory function that captures the backward and for-ward relationships in a sequence, characterized by accepting inputs and a \"hidden state\" and out-putting a new hidden state at each time step. This design allows information to be passed between time steps, thus capturing long-term dependencies in the sequence, and some improved structures have been developed, such as the Long Short-Term Memory (LSTM) network. Mou et al. [14]presents the first recursive convolutional network architecture for multi-temporal remote sensing image analysis, utilizing a novel recursive convolutional neural network (ReCNN) architecture for change detection in multi-spectral images. The network combines a convolutional neural net-work (CNN) [15] and RNN to learn joint spectral-spatial-temporal feature representations in a uni-fied framework. Chen et al [16] proposed a deep twinned convolutional multilayer recurrent neu-ral network (SiamCRNN) for change detection in multi-temporal VHR images, where a multilayer recurrent neural network stacked with Long Short-Term Memory (LSTM) units is responsible for mapping the extracted spatial-spectral features to new latent feature space and mining the change information in it.\nThe attention mechanism is a technique used to improve the performance of deep learning mod-els, especially when processing sequential data, such as in tasks like natural language processing and image processing. The core idea of Attention Mechanism is to allow the model to selectively focus on certain important parts of the input data when processing, instead of treating all the infor-mation equally. Some researchers have begun to use attention-based methods to model temporal relationships in change detection tasks. Chen et al. [17] designed a change detection self-attention mechanism to model spatial-temporal [18] rela-tions, integrating a new self-attention module in the feature extraction process. Wang et al. [19] proposed a two-temporal-phase attention-sharing"}, {"title": "2.2 Spatial-Temporal Feature Learning", "content": "Spatial-Temporal feature learning [20-22] is a technique for extracting and modeling features of data that vary in spatial and temporal dimensions. It has a wide range of applications in many fields, including computer vision, neuro-science, traffic prediction, and climate research. Spatial-temporal feature learning captures the dynamic behavior of data at different locations and points in time, providing a more compre-hensive understanding and analysis. Aghili et al. [23] proposed Spatial-Temporal Linear Fea-ture Learning (STLFL), which is an improved linear discriminant analysis technique focused on extracting high-level features of P300 event-related brain potentials. Tan et al. [24] proposed a new dynamic spatial-temporal graph data mod-eling framework for constructing spatial-temporal adjacency graphs through the lens of graph prod-ucts. Lu et al. [25] proposed the Spatial-Temporal Fusion (STF) module to learn implicit neural representations from spatial-temporal coordinates and features queried from RGB frames and events. Cui et al. [26] proposed a Transformer-based gait recognition framework that introduces a spatial fusion module (SFM) and a temporal fusion module (TFM) for efficiently fusing spatial and temporal level feature information, respectively. Wang et al. [27] propose an ESTF framework for event-based action recognition based on spatial and temporal Transformer networks. Inspired by these works, in this paper, we also exploit the spatial-temporal features to further augment the performance of changed region detection."}, {"title": "3 The Proposed Method", "content": "In this section, we introduce a novel Coarse-grained Temporal Mining Augmented (C\u0422\u041c\u0410) framework for remote sensing change detection task. As illustrated in Fig. 2, it mainly consists of Temporal Encoder (TE) and Coarse-grained Fore-grounds Augmented Spatial Encoder (CFA-SE). The details will be elaborated below."}, {"title": "3.1 Overview", "content": "As illustrated in Fig. 2, we propose a novel Coarse-grained Temporal Mining Augmented framework for remote sensing change detection, which con-sists of Temporal Encoder (TE) and Coarse-grained Foregrounds Augmented Spatial Encoder (CFA-SE). Given the bi-temporal images, we first employ Temporal Encoder (TE) to yield the fea-ture representations containing temporal infor-mation and generate a preliminary mask. To be concrete, we transform the input bi-temporal images into video frame sequences to enhance the extraction of temporal information. Subse-quently, the temporal encoder processes this video data to derive the preliminary detection results based on temporal modeling, known as coarse change map. To further improve the accuracy of detection, we introduce a novel Coarse-grained Foregrounds Augmented Spatial Encoder (CFA-SE). This encoder not only considers the global and local information of the input image pair but also improves the extraction of motion infor-mation to produce the final probability map for remote sensing change detection. By exploring the reliable local variation regions in the coarse change map and incorporating the threshold seg-mentation technique, the encoder generates an accurate mask. In the spatial modeling phase, our approach integrates global-local branches to encode and decode complete images and images with erased background regions. Finally, by fusing the outputs of these two branches, we obtained a more accurate change map. The whole process is trained end-to-end, ensuring efficient and accurate performance."}, {"title": "3.2 Temporal Encoder", "content": "Given the bi-temporal images, we first refer to work [7] to construct a pseudo-video frame sequence from the input image pair through video transformation technology to obtain a more detailed view of the temporal data. This method not only avoids relying on external data or man-ually using linear interpolation but also directly creates new frames between two known frames, ensuring a uniform distribution of these frames on the timeline. The pseudo-video consists of N frames, where the first and last two frames are directly taken from the original images I\u2081 and I\u2082, while the n-th frame $X_n$ (0 < n < N) is obtained by interpolation. In particular, the generation of the n-th frame $X_n$ follows the following general formula:\n$X_n = I_1 + \\frac{n}{N-1}(I_1 - I_2)$ (1)\nThis process is actually a sampling of frames in a virtual video, which accurately depicts the linear transition of all pixels in time, and the tem-poral resolution (or frame rate) of the video is inversely proportional to the degree of refinement of this transition. In addition, by processing the bi-temporal input as a subset, the constructed frame sequence effectively prevents information loss, ensuring the integrity and accuracy of data.\nAs shown in Fig. 2, we introduce a Temporal Encoder (TE), which mainly consists of a downsampling layer (T-Block I), two temporal blocks (T-Block II), a temporal aggregation mod-ule (TAM) and a convolutional layer. T-Block I is a streamlined convolutional neural network (CNN), which uses a 3 \u00d7 9 \u00d7 9 kernel size and a 1\u00d7 4\u00d74 stride, facilitating a quadruple downsampling of feature maps. This configuration allows the temporal encoder to focus on temporal informa-tion more efficiently. On the other hand, T-Block II focuses on capturing dynamic motion infor-mation closely related to temporal changes. It adopts a series of 3D ResBlock modules similar to those described in [28]. These modules employ 3D convolutional layers with a kernel size of 3, targeting features pertaining to temporal events across different temporal dimensions. Its construc-tion involves a series of 1 x 1 x 1 3D convolutional layers, each followed by a batch normalization (BN) layer [29] and ReLU activation function [30]. Specifically, T-Block II includes an initial 1\u00d71\u00d71 3D convolutional layer followed by a duplication of two identical configurations, aiming to main-tain the simplicity and computational efficiency of the network architecture. Additionally, T-Block II introduces a residual structure, allowing the input of the module to be connected to the output before the final ReLU activation function through a1\u00d71\u00d713D convolutional layer after pass-ing through the BN layer. This design reduces the computational cost [31] by first reducing and then restoring the number of channels through two 1\u00d71 \u00d71 convolutional layers, thus enabling the construction of a broader and deeper network structure.\nIn order to strengthen the interaction between temporal and spatial information, we introduce a temporal aggregation module (TAM) inspired by the previous work [7]. Let the output $H \\epsilon R^{B \\times c \\times T \\times h \\times w}$ of T-Bclock II, where B symbolizes the batch size, c signifies the number of channels, T represents the frame rate, and h and w denote the height and width of the feature map, respectively. We first perform global average pool-ing and global maximum pooling operations on the temporal dimension for 3D features. Following this, the pooled features are connected in channel dimension to yield a 2D feature descriptor with 2c channel. Subsequently, through a 1 \u00d7 1 convolutional layer with c filters, combined with BN and ReLU activation function, the aggregated feature map is transformed pointwise to obtain the final feature representation $\\hat{H} \\epsilon R^{B \\times c \\times h \\times w}$. Finally, through a convolutional layer and applying thresh-old segmentation, we obtain the predicted target"}, {"title": null, "content": "mask matrix M, which can be formulated as follows,\n$M = threshold(f(\\hat{H}))$ (2)\nwhere M\u2208 $R^{B \\times 1 \\times H \\times W}$, H and W correspond to the original image dimensions. f(\u00b7) denotes the a convolutional operation."}, {"title": "3.3 Coarse-grained Foregrounds Augmented Spatial Encoder", "content": "In order to improve the accuracy and efficiency of change detection, we propose a novel Coarse-grained Foregrounds Augmented Spatial Encoder (CFA-SE) module. This module uniquely com-bines both global and local information and utilizes motion-augmented and mask-augmented strategies to obtain more precise change detection. The core components of the proposed CFA-SE module mainly contain bi-temporal image fusion module, motion augmented strategy, and mask augmented strategy.\nInputting the bi-temporal images, how to merge them efficiently in a change detection framework is an important and complex task. In the change detection framework, how to effectively integrate bi-temporal images is a crucial and com-plex task. Traditional fusion methods, such as point-to-point difference and channel-level splic-ing, have their own advantages, but they also have limitations. Although point-to-point difference can directly reveal the difference at pixel level, it overlooks the interactive information between images. On the other hand, channel-level splicing can merge information from two images but may neglect individual image feature extraction. To overcome these limitations, we adopt two comple-mentary strategies in the bi-temporal image fusion module of CFA-SE. As shown in Figure 2, we build two sub-networks for global image pairs. The one branch uses ResNet [31] as the backbone net-work for feature extraction and captures difference features between two images through differential operation, which can be denoted as,\n$F = ResNet(I_1) \u2013 ResNet(I_2)$ (3)\nwhere F\u2208$R^{B \\times c' \\times h' \\times w'}$. The other branch combines the channel dimensions of the two images into a high-dimensional image pair, uti-lizing the Unet [32] network structure to extract the change information. This method effectively combines pixel-level difference features and inter-active information between images, enabling us to comprehensively and deeply detect changes in bi-temporal images.\nTo be specific, we incorporate two types of S-Block (i.e., S-Block I and S-Block II) for Unet architecture, as recommended by [7], in order to decrease the spatial size of the image and increase the channel size of image. Both types of S-Blocks begin with a sequence of convolutional-BN-ReLU-convolutional-BN layers and conclude with a pooling layer, with the exception that S-Block II includes an extra convolutional layer (comprising the corresponding BN and ReLU lay-ers). The use of maximum pooling for downsam-pling enables the capturing of multi-scale spatial context through the stacking of S-Blocks. Further-more, a residual connection is established between the output of the initial ReLU layer and the out-put of the final normalization layer within the block. In order to extract change information from the spatial and temporal cues captured by the encoder, a simple yet efficient decoder is fur-ther designed, consisting of four basic decoding blocks. These decoding blocks predict the proba-bility maps in an asymptotic manner by feeding the features from the previous decoding block or the underlying convolution (i.e., decoding fea-tures) with the features from the corresponding scales of the spatial encoder (i.e., encoding fea-tures). It is worth noting that the initial encoded feature fed into the global decoder is the out-put of S-Block, whereas the other input feature is sourced from the disparity features F extracted by ResNet network, thus establishing the structure of Unet network. In particular, the last decoded feature is first upsampled to fit the size of the encoded feature mapping. Then, the encoded and decoded features are associated by splicing them in the channel dimension, and the fused features are resolved by stacking two convolutional layers. To increase nonlinearity, we employ a ReLU acti-vation function and alternate batch normalization layers to stabilize the training process.\nIn addition, residual concatenation is used in the second convolutional layer to further optimize the learning of features. Special emphasis should be placed on the fact that the original diachronic image is transferred to the final decoding block, which helps to preserve the spatial details of the image. Finally, the probability maps generated by the global-local decoder are fused and bina-rized in a weighted manner with a threshold of 0.5 to obtain the final variogram. By integrating the two subnetworks and implementing interac-tive fusion operations in the decoder, CFA-SE not only improves the accuracy of change detection but also enhances the robustness of model. In this way, although the global and local informa-tion of the bi-temporal image is considered, the importance of motion information and mask guid-ance is ignored. Therefore, in order to generate more accurate change detection results, we intro-duce motion augmented and mask augmented strategies.\nMotion Augmented Strategy. According to the detailed description in Section 3.2, we suc-cessfully utilize T-block II to capture motion infor-mation during temporal changes. Considering that the proposed CFA-SE may overlook such crucial motion information, we implement a motion aug-mented strategy. Specifically, we first extract the 3D output containing motion information from T-block II, and then use temporal aggregation module (TAM) as suggested in [7] to convert these 3D features into 2D features. These transformed 2D features are used as additional inputs for S-block I and S-block II in spatial encoder, thereby effectively facilitating the integration of motion information and enhancing the capability of the model to detect changes in remote sensing images.\nMask Augmented Strategy. To improve the accuracy of change detection, we implement a mask augmented strategy. This strategy dex-terously leverages the preliminary change map acquired through Temporal Encoder (TE) in section 3.2 as prior knowledge, delving deeper into the change information of local areas. Initially,"}, {"title": null, "content": "we apply a customized threshold segmentation onto the coarse variable probability map output by TE, transforming it into a binary image con-taining only 0 and 1. Subsequently, this binary image is upsampled to match the spatial resolu-tion of the original image, producing an accurate mask image. We then utilize this mask image to element-wise multiply the original image pairs (I1, I2). This operation aims to retain the regions indicated by a value of 1 in the mask image (representing potential changes) while eliminating the areas masked with a value of 0 (indicating non-changing regions). This process yields the seg-mented object image pair, which serves as input. The object image pair is then concatenated and fed into Unet [32] network, enabling the network to focus on learning the change regions within images. Finally, we fuse the probability map out-put from Unet with the probability map generated by the global-local decoder to generate the final probability map. This strategy effectively elim-inates the interference of static background in bi-temporal image pair, enabling the model to focus more on detecting changing regions and significantly enhancing the accuracy of detection results."}, {"title": "3.4 Loss Function", "content": "To improve the performance of model during the training process, we add the weighted Binary Cross-Entropy (BCE) loss into the output proba-bility maps of the TE and CFA-SE, respectively. This addition serves as a means of supervision and constraint. Given that the two probability maps are labeled as P\u2081 and P2 respectively, the total loss can be expressed as:\n$L_{total} = \\alpha L_1(P_1, Y) + (1 - \\alpha) L_2(P_2, Y)$ (4)\nwhere L\u2081 and L2 represent the weighted binary cross-entropy loss functions. More detailed infor-mation can be found in [33]. Y denotes the ground-truth change map. The parameter \u03b1 is a balancing hyper-parameter, which is empirically set to 0.5 in all of our experiments."}, {"title": "4 Experiments", "content": "In our experimental study, we use three well-known remote sensing image datasets for remote sense change detection, i.e., SVCD [34], LEVIR-CD [17], and WHU-CD [35]. Each dataset is briefly described as follows:\n\u2022 SVCD [34] is a comprehensive change detec-tion dataset consisting of 11 pairs of accurately aligned remote sensing images obtained from Google Earth. The images exhibit a diverse spatial resolution ranging from 3 cm to 100 cm per pixel. Through careful random cropping and rotation procedures, we generate 16,000 pairs of 256 \u00d7 256 image fragments. These fragments are thought-fully allocated, with 10,000 pairs designated for training, 3,000 pairs for validation, and 3,000 pairs for testing. Each pair of image fragments contains at least one altered pixel, ensuring the richness of changing information in the dataset. It is worth noting that SVCD employs a very strict definition of data change, and only substantial modifications in the semantic class of an object are deemed as changes based on the fundamental annotation. This means that subtle differences such as sea-sonal natural changes in leaves are not reflected in the ground reality label. This feature puts forward higher requirements for change detection meth-ods, requiring algorithms to achieve higher levels in radiation calibration accuracy and semantic information utilization.\n\u2022 LEVIR-CD [17] is specifically designed for detecting changes in buildings, consisting of 637 pairs of remote sensing image tiles, each with a high-resolution size of 1024 \u00d7 1024 pixels. These images possess a detailed spatial resolution of 0.5 meters per pixel, enabling accurate delin-eation of subtle building alterations. The dataset encompasses a total of 31,333 different change instances, covering a diverse range of building categories, thus ensuring comprehensive dataset breadth and representation. To maintain consis-tency and accuracy of training and evaluation, LEVI-CD strictly follows the predefined training/-validation/test data split scheme by the dataset authors. During data processing, a sliding win-dow technique, as described in [7], is employed to extract 256 \u00d7 256 pixel chips from the original image tiles. This approach significantly enhances"}, {"title": null, "content": "the ease of training and evaluating models on large raster images. During the training phase, in order to increase the diversity of samples and improve the generalization ability of the model, the chips are overlapped at 128-pixel strides, resulting in more training samples. Subsequently, during the verification and test phase, in order to ensure the objectivity of the evaluation results, the verification and test chips are extracted in a non-overlapping manner, with a step size configured to 256 pixels. This strategy not only guaran-tees the effectiveness of model training, but also comprehensively addresses the inherent complex-ities associated with processing large-scale raster images.\n\u2022 WHU-CD [35] dataset consists of two high-resolution aerial images, each with dimensions of up to 32507 \u00d7 15354 pixels and an accurate spatial resolution of 0.3 meters per pixel. Compared with the LEVIR-CD [17] dataset, WHU-CD dataset focuses on the detection of building changes. Our research method begins by carefully cropping the original image into non-overlapping 256 \u00d7 256 pixel image chips. Subsequently, these chips are ran-domly divided into three distinct subsets: 6,096 samples for training, 762 samples for validation, and another 762 samples for testing. The data partitioning strategy employed in this dataset is consistent with the method described in the pre-vious work [3], which ensures the integrity and reliability of the experimental results."}, {"title": "4.2 Evaluation Metric", "content": "In our experimental evaluation, we use three key criteria to comprehensively evaluate the perfor-mance of change detection algorithms, namely Precision, Recall, and F1 scores. Each metric plays a different role in the evaluation process and aims to reveal the performance of the algorithm from a different perspective. Precision focuses on the proportion of true positive samples among those predicted by the algorithm, which reflects the accuracy of the algorithm. We can define it as,\n$P = \\frac{N_{tp}}{N_{tp} + N_{fp}}$ (5)\nwhere $N_{tp}$ and $N_{fp}$ represent the number of true positive and false positive samples, respectively. Recall measures the ability of the algorithm to correctly identify all true positive samples, indi-cating the completeness of the algorithm. It can be denoted as follow,\n$R = \\frac{N_{tp}}{N_{tp} + N_{fn}}$ (6)\nwhere $N_{fn}$ represents the number of false negative samples. The F1 score is the harmonic average of Precision and Recall, providing us with a compre-hensive evaluation metric that balances accuracy and completeness. It can be formulated as,\n$F1 = 2 * \\frac{P*R}{P+R}$ (7)\nIn a word, these metrics provide a comprehen-sive overview of the performance of the change detection algorithm, ensuring the objectivity and comprehensiveness of our evaluation."}, {"title": "4.3 Implementation Details", "content": "This section elaborates on the building blocks in the network. The Temporal Encoder (TE) is constructed from three T-Blocks without shared parameters. The local branch in Coarse-grained Foregrounds Augmented Spatial Encoder (CFA-SE) consists of three independent S-Blocks, which do not share parameters. In the S-Block, the out-put channels of each Block is set to 32, 64, and 128. The first two S-Blocks are categorized as S-Block I, while the last one is classified as S-Block II. Given that video frames provide a more abun-dant data source compared to bi-temporal image pairs, the TE module include additional layers and convolutional filters. Consequently, the out-put channels of T-Block I is set to 64, while the terminal convolutional layers of the two T-Blocks (i.e., T-Block II) utilize 256, 256, 512, and 512 fil-ters, respectively. A fixed threshold of 0.5 is set for segmentation when constructing the mask.\nThree datasets, i.e., SVCD [34], LEVIR-CD [17] and WHU-CD [35], are used in the experiment. For the LEVIR-CD dataset, we follow the processing method of the benchmark work [7], which extracts 256 \u00d7 256 image blocks from the original image by sliding the window. In order to ensure the sufficiency of the sample, the training set image blocks are overlapped with a step size of 128 pixels, while the validation set remains non-overlapped with a step size of 256 pixels. In the"}, {"title": null, "content": "testing phase, we also use 256 pixel steps with-out overlapping, and the final prediction score is determined by averaging the prediction scores of all overlapping inference windows for a given pixel.\nAll models are implemented based on the PyTorch framework and trained on an Intel Xeon Silver 4314 CPU and a single NVIDIA GeForce RTX 3090 GPU. On the three datasets, we uni-formly set the batch size to 8 and use the Adam optimizer to update the network parameters. Dur-ing the linear interpolation of video frames, we build 8 frames and set the weight coefficient of auxiliary loss to 0.4. The balance weight of binary cross-entropy loss is fixed at 0.5 for both TE and CFA-SE. To enhance data diversity, training data is randomly flipped, moved, and rotated 90-degrees, followed by training in small batches. We use different learning rates and iterations for dif-ferent datasets. Specifically, for the SVCD dataset, the initial learning rate is set to 0.0004 and reaches convergence in 260,000 iterations. For the LEVIR-CD dataset, the initial learning rate is set to 0.002 and the number of iterations is 220,000. For the WHU-CD dataset, the training starts with a learn-ing rate of 0.0004 and concludes after 160,000 iterations. We employ a step decay strategy to adjust the learning rate, with attenuation rates and step sizes set to 0.1 and 70 for SVCD dataset, 0.2 and 60 for LEVIR-CD dataset, and 0.2 and 105 for WHU-CD dataset, respectively. In addi-tion, we also set different weight factors for the output probability map of CFA-SE according to the characteristics of the dataset. Following each training round, a validation step is undertaken to identify the model with the highest F1 score as the best model to evaluate its performance on the test subset."}, {"title": "4.4 Comparison with State-of-the-Art Methods", "content": "The method was comprehensively validated on three benchmark datasets and compared with 10 other state-of-the-art change detection models, including FC-EF [36], FC-Siam-Conc [36], FC-Siam-Di [36], CDNet [37], STANet [17], BIT [3], L-UNet [38], DSIFN [39], SNUNet [40], and P2V-CD [7]. The quantitative evaluation results for the three datasets are detailed in Table 1, with all data units represented in percentages.\nResults on SVCD dataset. On SVCD dataset [34], our proposed method demon-strates significant improvements over the L-Net method [38], which is based on a UNet-like archi-tecture, with gains of 2.15%, 4.04% and 3.11% in accuracy, recall and F1 score metric, respectively. Compared with DSIFN method [39] that focuses on high-resolution bi-temporal remote sensing images, our proposed method shows enhance-ments of 1.02%, 3.60% and 2.33% in accuracy, recall and F1 score metric, respectively. In addi-tion, compared with P2V-CD method [7] which also employs a pair-to-video transformation strat-egy, our proposed method achieves improvements of 0.12%, 0.42% and 0.27% in accuracy, recall and F1 score metric, respectively. These results show that the proposed CTMA method has better performance on this dataset.\nResults on LEVIR-CD dataset. On the LEVIR-CD dataset [17], our proposed method exhibits improvements in accuracy (0.41%), in recall (3.2%), and the F1 score metric (1.48%) compared to the DSIFN method [39]. When com-pared with the P2V-CD method [7], although the accuracy of our proposed method is slightly lower, it achieves a noteworthy 0.53% enhancement in recall metric. This result could be attributed to the diverse architectural variations and intri-cate environmental contexts present in Levi-CD dataset. In the process of pursuing high recall metric, our proposed method adopts a relatively loose strategy for predicting changing regions, leading to the introduction of some false posi-tive examples that affect the accuracy to some extent. Nevertheless, it should be noted that the F1 score metric is a harmonic average of the accuracy and recall metric, which enables a more comprehensive evaluation of the model's perfor-mance. Although slightly less accurate than the P2V-CD method, our method still performs better on the F1 score metric, which fully demonstrates the overall performance advantage of the proposed CTMA method.\nResults on WHU-CD dataset. On the WHU-CD dataset [35], our proposed method showcases competitive performance. For exam-ple, compared with the L-UNet method, our proposed method achieves a significant improve-ments, including a 16.32% increase in accuracy metric, a 2.61% increase in recall metric, and a significant improvement of 9.75% in F1 score met-ric. Furthermore, compared with SNUNet method based on channel attention mechanism, our pro-posed method exhibits exceptional strength, deliv-ering outstanding results with improvements of 5.42%, 5.17% and 5.29% in accuracy, recall and F1 score metric, respectively. Compared with the P2V-CD method, our proposed method also per-forms well. We achieve a 0.23% enhancement in accuracy metric, a 3.3% increase in recall metric, and a 1.84% rise in F1 score metric. Similarly, our proposed method demonstrates competitive per-formance compared to most other SOTA methods. These results fully validate the effectiveness and superiority of the proposed CTMA method."}, {"title": "4.5 Ablation Study", "content": "In this section, we provide a detailed analy-sis of the different components, hyper-parameter settings, different mask thresholds and different interpolated video frames in the proposed CTMA method on the WHU-CD dataset [35]. By con-ducting a detailed analysis of the effectiveness of these pivotal factors, we aim to elucidate their impact on model performance and provide read-ers with a more comprehensive understanding and reference.\nAnalysis of different components. In order to comprehensively evaluate the contribution of each individual component of the proposed CTMA method on network performance, we conduct an ablation experiment for component analysis on the WHU-CD dataset. The proposed CTMA method mainly combines temporal encoder (TE), spatial encoder (SE), ResNet, and mask augmented (MA) strategy. The experimental findings are visually depicted in Table 2. Specifically, 'TE' captures and learns the dynamic motion information between dense frames after frame insertion, which is the key to understanding temporal changes. 'SE' is the basic element of the CFA-SE module, which focuses on extracting features sensitive to spatial details from spatial dimensions. Meanwhile, 'MA' can enhance the feature representation by intro-ducing mask to further improve the model perfor-mance. The following conclusions can be drawn from the experimental results: 1) Compared #1 with #2, it can be clearly observed that the intro-duction of MA improves the network performance, which proves the effectiveness of MA strategy. 2) Compared #1 with #3, it reveals that integrat-ing ResNet in CFA-SE module not only enhances the network's ability to learn diverse and multi-level information, but also enhances its ability to decipher complex scenarios. 3) The experimen-tal results of #4 show that the CTMA method achieves an optimal level of performance when all components work together. This fully demon-strates the complementary nature of different components, collectively establishing an effective and comprehensive change detection framework. In summary, the results of ablation experiments validate the rationale and necessity behind the design of each component in the proposed CTMA"}, {"title": null, "content": "method. Their collective integration improves the overall performance of the network in the change detection task.\nAnalysis of hyper-parameter settings. In the CFA-SE module of the proposed CTMA method, we introduce an important hyper-parameter \u5165, which controls the contribution of mask augmented branch. In order to further explore the effect of Aon model efficiency, we conduct a series of systematic analysis experi-ments on the WHU-CD dataset. The experimental results are outlined in Table 3. The"}]}