{"title": "Treat Stillness with Movement: Remote Sensing Change Detection via Coarse-grained Temporal Foregrounds Mining", "authors": ["Xixi Wang", "Zitian Wang", "Jingtao Jiang", "Lan Chen", "Xiao Wang", "Bo Jiang"], "abstract": "Current works focus on addressing the remote sensing change detection task using bi-temporal images. Although good performance can be achieved, however, seldom of they consider the motion cues which may also be vital. In this work, we revisit the widely adopted bi-temporal images-based framework and propose a novel Coarse-grained Temporal Mining Augmented (CTMA) framework. To be specific, given the bi-temporal images, we first transform them into a video using interpolation operations. Then, a set of temporal encoders is adopted to extract the motion features from the obtained video for coarse-grained changed region prediction. Subsequently, we design a novel Coarse-grained Foregrounds Augmented Spatial Encoder module to integrate both global and local information. We also introduce a motion augmented strategy that leverages motion cues as an additional output to aggregate with the spatial features for improved results. Meanwhile, we feed the input image pairs into the ResNet to get the different features and also the spatial blocks for fine-grained feature learning. More importantly, we propose a mask augmented strategy that utilizes coarse-grained changed regions, incorporating them into the decoder blocks to enhance the final changed prediction. Extensive experiments conducted on multiple benchmark datasets fully validated the effectiveness of our proposed framework for remote sensing image change detection.", "sections": [{"title": "1 Introduction", "content": "Remote sensing image change detection targets finding the variable pixel-level regions between given two images. This task can be used in many practical scenarios, including damage assessment, urban studies, ecosystem monitoring, agricultural surveying, and resource management. Although good performance can already be achieved in some simple scenarios, remote sensing change detection is still a challenging task in extreme cases.\nExisting researchers usually adopt Convolutional Neural Networks (CNN) [1] and Transformers [2] to build their backbones for remote sensing image change detection, as illustrated in Fig. 1(a). Specifically, Chen et al. [3] propose a Bitemporal Image Transformer (BIT) that mines the contexts within the spatial-temporal domain effectively. The Visual change Transformer (VcT) proposed by Jiang et al. [4] finds that the mining of the common background information helps the consistent representations which further enhances the visual"}, {"title": "2 Related Work", "content": "In this section, we provide a brief introduction to the works focused on Remote Sensing Image Change Detection [8] and Spatial-Temporal Feature Learning."}, {"title": "2.1 Remote Sensing Image Change Detection", "content": "Remote sensing image change detection is an important application field in remote sensing technology, and its purpose is to detect changes in surface features by comparing remote sensing images from different periods. Most of the existing methods are based on spatial modeling of two images, and direct pairwise difference or splicing of images for change detection. Jiang et al. [4] propose that VCT enables the model to learn a consistent representation between two images by mining spatial contextual information. Goswami et al. [9] used decision tree algorithms and post-classification comparisons of separation matrices, as well as image differencing in algebraic techniques to detect two images. Li et al. [10] propose a novel lightweight network, called A2Net, that recognizes changes by moving network-extracted features in combination with progressive feature aggregation and supervised attention. Zhou et al. [11] proposed a Context Aggregation Network (CANet) for mining cross-image contextual information between all training images to further enhance contextual representation within a single image. Since the development phase of change detection, a great deal of work has been devoted to enhancing the representation of spatial features. However, specialized modeling of time has long been neglected [7]. Methods focusing on the time dimension include two main categories, recurrent neural network (RNN) [12] based methods and attention-based [13] methods.\nRNNs process sequential data with a memory function that captures the backward and forward relationships in a sequence, characterized by accepting inputs and a \"hidden state\" and outputting a new hidden state at each time step. This design allows information to be passed between time steps, thus capturing long-term dependencies in the sequence, and some improved structures have been developed, such as the Long Short-Term Memory (LSTM) network. Mou et al. [14]presents the first recursive convolutional network architecture for multi-temporal remote sensing image analysis, utilizing a novel recursive convolutional neural network (ReCNN) architecture for change detection in multi-spectral images. The network combines a convolutional neural network (CNN) [15] and RNN to learn joint spectral-spatial-temporal feature representations in a unified framework. Chen et al [16] proposed a deep twinned convolutional multilayer recurrent neural network (SiamCRNN) for change detection in multi-temporal VHR images, where a multilayer recurrent neural network stacked with Long Short-Term Memory (LSTM) units is responsible for mapping the extracted spatial-spectral features to new latent feature space and mining the change information in it.\nThe attention mechanism is a technique used to improve the performance of deep learning models, especially when processing sequential data, such as in tasks like natural language processing and image processing. The core idea of Attention Mechanism is to allow the model to selectively focus on certain important parts of the input data when processing, instead of treating all the information equally. Some researchers have begun to use attention-based methods to model temporal relationships in change detection tasks. Chen et al. [17] designed a change detection self-attention mechanism to model spatial-temporal [18] relations, integrating a new self-attention module in the feature extraction process. Wang et al. [19] proposed a two-temporal-phase attention-sharing"}, {"title": "2.2 Spatial-Temporal Feature Learning", "content": "Spatial-Temporal feature learning [20-22] is a technique for extracting and modeling features of data that vary in spatial and temporal dimensions. It has a wide range of applications in many fields, including computer vision, neuroscience, traffic prediction, and climate research. Spatial-temporal feature learning captures the dynamic behavior of data at different locations and points in time, providing a more comprehensive understanding and analysis. Aghili et al. [23] proposed Spatial-Temporal Linear Feature Learning (STLFL), which is an improved linear discriminant analysis technique focused on extracting high-level features of P300 event-related brain potentials. Tan et al. [24] proposed a new dynamic spatial-temporal graph data modeling framework for constructing spatial-temporal adjacency graphs through the lens of graph products. Lu et al. [25] proposed the Spatial-Temporal Fusion (STF) module to learn implicit neural representations from spatial-temporal coordinates and features queried from RGB frames and events. Cui et al. [26] proposed a Transformer-based gait recognition framework that introduces a spatial fusion module (SFM) and a temporal fusion module (TFM) for efficiently fusing spatial and temporal level feature information, respectively. Wang et al. [27] propose an ESTF framework for event-based action recognition based on spatial and temporal Transformer networks. Inspired by these works, in this paper, we also exploit the spatial-temporal features to further augment the performance of changed region detection."}, {"title": "3 The Proposed Method", "content": "In this section, we introduce a novel Coarse-grained Temporal Mining Augmented (C\u0422\u041c\u0410) framework for remote sensing change detection task. As illustrated in Fig. 2, it mainly consists of Temporal Encoder (TE) and Coarse-grained Foregrounds Augmented Spatial Encoder (CFA-SE). The details will be elaborated below."}, {"title": "3.1 Overview", "content": "As illustrated in Fig. 2, we propose a novel Coarse-grained Temporal Mining Augmented framework for remote sensing change detection, which consists of Temporal Encoder (TE) and Coarse-grained Foregrounds Augmented Spatial Encoder (CFA-SE). Given the bi-temporal images, we first employ Temporal Encoder (TE) to yield the feature representations containing temporal information and generate a preliminary mask. To be concrete, we transform the input bi-temporal images into video frame sequences to enhance the extraction of temporal information. Subsequently, the temporal encoder processes this video data to derive the preliminary detection results based on temporal modeling, known as coarse change map. To further improve the accuracy of detection, we introduce a novel Coarse-grained Foregrounds Augmented Spatial Encoder (CFA-SE). This encoder not only considers the global and local information of the input image pair but also improves the extraction of motion information to produce the final probability map for remote sensing change detection. By exploring the reliable local variation regions in the coarse change map and incorporating the threshold segmentation technique, the encoder generates an accurate mask. In the spatial modeling phase, our approach integrates global-local branches to encode and decode complete images and images with erased background regions. Finally, by fusing the outputs of these two branches, we obtained a more accurate change map. The whole process is trained end-to-end, ensuring efficient and accurate performance."}, {"title": "3.2 Temporal Encoder", "content": "Given the bi-temporal images, we first refer to work [7] to construct a pseudo-video frame sequence from the input image pair through\n$X_n = I_1 + \\frac{n}{N-1} (I_1 - I_2)$\nvideo transformation technology to obtain a more detailed view of the temporal data. This method not only avoids relying on external data or manually using linear interpolation but also directly creates new frames between two known frames, ensuring a uniform distribution of these frames on the timeline. The pseudo-video consists of N frames, where the first and last two frames are directly taken from the original images I\u2081 and I2, while the n-th frame Xn (0 < n < N) is obtained by interpolation. In particular, the generation of the n-th frame Xn follows the following general formula:\n(1)\nThis process is actually a sampling of frames in a virtual video, which accurately depicts the linear transition of all pixels in time, and the temporal resolution (or frame rate) of the video is inversely proportional to the degree of refinement of this transition. In addition, by processing the bi-temporal input as a subset, the constructed frame sequence effectively prevents information loss, ensuring the integrity and accuracy of data.\nAs shown in Fig. 2, we introduce a Temporal Encoder (TE), which mainly consists of a downsampling layer (T-Block I), two temporal blocks (T-Block II), a temporal aggregation module (TAM) and a convolutional layer. T-Block I is a streamlined convolutional neural network (CNN), which uses a 3 \u00d7 9 \u00d7 9 kernel size and a 1 \u00d7 4 \u00d7 4 stride, facilitating a quadruple downsampling of feature maps. This configuration allows the temporal encoder to focus on temporal information more efficiently. On the other hand, T-Block II focuses on capturing dynamic motion information closely related to temporal changes. It adopts a series of 3D ResBlock modules similar to those described in [28]. These modules employ 3D convolutional layers with a kernel size of 3, targeting features pertaining to temporal events across different temporal dimensions. Its construction involves a series of 1 x 1 x 1 3D convolutional"}, {"title": "3.3 Coarse-grained Foregrounds Augmented Spatial Encoder", "content": "In order to improve the accuracy and efficiency of change detection, we propose a novel Coarse-grained Foregrounds Augmented Spatial Encoder (CFA-SE) module. This module uniquely combines both global and local information and utilizes motion-augmented and mask-augmented strategies to obtain more precise change detection. The core components of the proposed CFA-SE module mainly contain bi-temporal image fusion module, motion augmented strategy, and mask augmented strategy.\nInputting the bi-temporal images, how to merge them efficiently in a change detection framework is an important and complex task. In the change detection framework, how to effectively integrate bi-temporal images is a crucial and complex task. Traditional fusion methods, such as point-to-point difference and channel-level splicing, have their own advantages, but they also have limitations. Although point-to-point difference can directly reveal the difference at pixel level, it overlooks the interactive information between images. On the other hand, channel-level splicing can merge information from two images but may neglect individual image feature extraction. To overcome these limitations, we adopt two complementary strategies in the bi-temporal image fusion\n$F = ResNet(I_1) \u2013 ResNet(I_2)$\nmodule of CFA-SE. As shown in Figure 2, we build two sub-networks for global image pairs. The one branch uses ResNet [31] as the backbone network for feature extraction and captures difference features between two images through differential operation, which can be denoted as,\n(3)\nwhere F\u2208RBxc'xh'xw'. The other branch combines the channel dimensions of the two images into a high-dimensional image pair, utilizing the Unet [32] network structure to extract the change information. This method effectively combines pixel-level difference features and interactive information between images, enabling us to comprehensively and deeply detect changes in bi-temporal images.\nTo be specific, we incorporate two types of S-Block (i.e., S-Block I and S-Block II) for Unet architecture, as recommended by [7], in order to decrease the spatial size of the image and increase the channel size of image. Both types of S-Blocks begin with a sequence of convolutional-BN-ReLU-convolutional-BN layers and conclude with a pooling layer, with the exception that S-Block II includes an extra convolutional layer (comprising the corresponding BN and ReLU layers). The use of maximum pooling for downsampling enables the capturing of multi-scale spatial context through the stacking of S-Blocks. Furthermore, a residual connection is established between the output of the initial ReLU layer and the output of the final normalization layer within the block. In order to extract change information from the spatial and temporal cues captured by the encoder, a simple yet efficient decoder is further designed, consisting of four basic decoding blocks. These decoding blocks predict the probability maps in an asymptotic manner by feeding the features from the previous decoding block or the underlying convolution (i.e., decoding features) with the features from the corresponding scales of the spatial encoder (i.e., encoding features). It is worth noting that the initial encoded feature fed into the global decoder is the output of S-Block, whereas the other input feature is sourced from the disparity features F extracted by ResNet network, thus establishing the structure of Unet network. In particular, the last decoded feature is first upsampled to fit the size of the\nmask matrix M, which can be formulated as follows,\nencoded feature mapping. Then, the encoded and decoded features are associated by splicing them in the channel dimension, and the fused features are resolved by stacking two convolutional layers. To increase nonlinearity, we employ a ReLU activation function and alternate batch normalization layers to stabilize the training process.\nIn addition, residual concatenation is used in the second convolutional layer to further optimize the learning of features. Special emphasis should be placed on the fact that the original diachronic image is transferred to the final decoding block, which helps to preserve the spatial details of the image. Finally, the probability maps generated by the global-local decoder are fused and binarized in a weighted manner with a threshold of 0.5 to obtain the final variogram. By integrating the two subnetworks and implementing interactive fusion operations in the decoder, CFA-SE not only improves the accuracy of change detection but also enhances the robustness of model. In this way, although the global and local information of the bi-temporal image is considered, the importance of motion information and mask guidance is ignored. Therefore, in order to generate more accurate change detection results, we introduce motion augmented and mask augmented strategies.\nMotion Augmented Strategy. According to the detailed description in Section 3.2, we successfully utilize T-block II to capture motion information during temporal changes. Considering that the proposed CFA-SE may overlook such crucial motion information, we implement a motion augmented strategy. Specifically, we first extract the 3D output containing motion information from T-block II, and then use temporal aggregation module (TAM) as suggested in [7] to convert these 3D features into 2D features. These transformed 2D features are used as additional inputs for S-block I and S-block II in spatial encoder, thereby effectively facilitating the integration of motion information and enhancing the capability of the model to detect changes in remote sensing images.\nMask Augmented Strategy. To improve the accuracy of change detection, we implement a mask augmented strategy. This strategy dexterously leverages the preliminary change map acquired through Temporal Encoder (TE) in section 3.2 as prior knowledge, delving deeper into the change information of local areas. Initially,\n$M = threshold(f(H))$\n(2)"}, {"title": "3.4 Loss Function", "content": "To improve the performance of model during the training process, we add the weighted Binary Cross-Entropy (BCE) loss into the output probability maps of the TE and CFA-SE, respectively. This addition serves as a means of supervision and constraint. Given that the two probability maps are labeled as P\u2081 and P2 respectively, the total loss can be expressed as:\n$L_{total} = \\alpha L_1(P_1, Y) + (1 - \\alpha) L_2(P_2, Y)$\nwhere L\u2081 and L2 represent the weighted binary cross-entropy loss functions. More detailed information can be found in [33]. Y denotes the ground-truth change map. The parameter \u03b1 is a balancing hyper-parameter, which is empirically set to 0.5 in all of our experiments.\n(4)"}, {"title": "4 Experiments", "content": "In our experimental study, we use three well-known remote sensing image datasets for remote sense change detection, i.e., SVCD [34], LEVIR-CD [17], and WHU-CD [35]. Each dataset is briefly described as follows:\n\u2022 SVCD [34] is a comprehensive change detection dataset consisting of 11 pairs of accurately aligned remote sensing images obtained from Google Earth. The images exhibit a diverse spatial resolution ranging from 3 cm to 100 cm per pixel. Through careful random cropping and rotation procedures, we generate 16,000 pairs of 256 \u00d7 256 image fragments. These fragments are thoughtfully allocated, with 10,000 pairs designated for training, 3,000 pairs for validation, and 3,000 pairs for testing. Each pair of image fragments contains at least one altered pixel, ensuring the richness of changing information in the dataset. It is worth noting that SVCD employs a very strict definition of data change, and only substantial modifications in the semantic class of an object are deemed as changes based on the fundamental annotation. This means that subtle differences such as seasonal natural changes in leaves are not reflected in the ground reality label. This feature puts forward higher requirements for change detection methods, requiring algorithms to achieve higher levels in radiation calibration accuracy and semantic information utilization.\n\u2022 LEVIR-CD [17] is specifically designed for detecting changes in buildings, consisting of 637 pairs of remote sensing image tiles, each with a high-resolution size of 1024 \u00d7 1024 pixels. These images possess a detailed spatial resolution of 0.5 meters per pixel, enabling accurate delineation of subtle building alterations. The dataset encompasses a total of 31,333 different change instances, covering a diverse range of building categories, thus ensuring comprehensive dataset breadth and representation. To maintain consistency and accuracy of training and evaluation, LEVI-CD strictly follows the predefined training/-validation/test data split scheme by the dataset authors. During data processing, a sliding window technique, as described in [7], is employed to extract 256 \u00d7 256 pixel chips from the original image tiles. This approach significantly enhances"}, {"title": "4.2 Evaluation Metric", "content": "In our experimental evaluation, we use three key criteria to comprehensively evaluate the performance of change detection algorithms, namely Precision, Recall, and F1 scores. Each metric plays a different role in the evaluation process and aims to reveal the performance of the algorithm from a different perspective. Precision focuses on the proportion of true positive samples among those predicted by the algorithm, which reflects the accuracy of the algorithm. We can define it as,\n$P = \\frac{N_{tp}}{N_{tp} + N_{fp}}$\nwhere Ntp and Nfp represent the number of true positive and false positive samples, respectively. Recall measures the ability of the algorithm to\n(5)\n$R=\\frac{N_{tp}}{N_{tp} + N_{fn}}$\ncorrectly identify all true positive samples, indicating the completeness of the algorithm. It can be denoted as follow,\n(6)\nwhere Nfn represents the number of false negative samples. The F1 score is the harmonic average of Precision and Recall, providing us with a comprehensive evaluation metric that balances accuracy and completeness. It can be formulated as,\n$F1 = 2 * \\frac{P*R}{P+R}$\nIn a word, these metrics provide a comprehensive overview of the performance of the change detection algorithm, ensuring the objectivity and comprehensiveness of our evaluation.\n(7)"}, {"title": "4.3 Implementation Details", "content": "This section elaborates on the building blocks in the network. The Temporal Encoder (TE) is constructed from three T-Blocks without shared parameters. The local branch in Coarse-grained Foregrounds Augmented Spatial Encoder (CFA-SE) consists of three independent S-Blocks, which do not share parameters. In the S-Block, the output channels of each Block is set to 32, 64, and 128. The first two S-Blocks are categorized as S-Block I, while the last one is classified as S-Block II. Given that video frames provide a more abundant data source compared to bi-temporal image pairs, the TE module include additional layers and convolutional filters. Consequently, the output channels of T-Block I is set to 64, while the terminal convolutional layers of the two T-Blocks (i.e., T-Block II) utilize 256, 256, 512, and 512 filters, respectively. A fixed threshold of 0.5 is set for segmentation when constructing the mask.\nThree datasets, i.e., SVCD [34], LEVIR-CD [17] and WHU-CD [35], are used in the experiment. For the LEVIR-CD dataset, we follow the processing method of the benchmark work [7], which extracts 256 \u00d7 256 image blocks from the original image by sliding the window. In order to ensure the sufficiency of the sample, the training set image blocks are overlapped with a step size of 128 pixels, while the validation set remains non-overlapped with a step size of 256 pixels. In the"}, {"title": "4.4 Comparison with State-of-the-Art Methods", "content": "The method was comprehensively validated on three benchmark datasets and compared with 10 other state-of-the-art change detection models, including FC-EF [36], FC-Siam-Conc [36], FC-Siam-Di [36], CDNet [37], STANet [17], BIT [3], L-UNet [38], DSIFN [39], SNUNet [40], and P2V-CD [7]. The quantitative evaluation results for the three datasets are detailed in Table 1, with all data units represented in percentages.\nResults on SVCD dataset. On SVCD dataset [34], our proposed method demonstrates significant improvements over the L-Net method [38], which is based on a UNet-like architecture, with gains of 2.15%, 4.04% and 3.11% in accuracy, recall and F1 score metric, respectively. Compared with DSIFN method [39] that focuses on high-resolution bi-temporal remote sensing images, our proposed method shows enhancements of 1.02%, 3.60% and 2.33% in accuracy, recall and F1 score metric, respectively. In addition, compared with P2V-CD method [7] which also employs a pair-to-video transformation strategy, our proposed method achieves improvements of 0.12%, 0.42% and 0.27% in accuracy, recall and F1 score metric, respectively. These results show that the proposed CTMA method has better performance on this dataset.\nResults on LEVIR-CD dataset. On the LEVIR-CD dataset [17], our proposed method exhibits improvements in accuracy (0.41%), in recall (3.2%), and the F1 score metric (1.48%) compared to the DSIFN method [39]. When compared with the P2V-CD method [7], although the accuracy of our proposed method is slightly lower, it achieves a noteworthy 0.53% enhancement in recall metric. This result could be attributed to the diverse architectural variations and intricate environmental contexts present in Levi-CD dataset. In the process of pursuing high recall metric, our proposed method adopts a relatively loose strategy for predicting changing regions, leading to the introduction of some false positive examples that affect the accuracy to some extent. Nevertheless, it should be noted that the F1 score metric is a harmonic average of the accuracy and recall metric, which enables a more comprehensive evaluation of the model's performance. Although slightly less accurate than the P2V-CD method, our method still performs better on the F1 score metric, which fully demonstrates the overall performance advantage of the proposed CTMA method.\nResults on WHU-CD dataset. On the WHU-CD dataset [35], our proposed method showcases competitive performance. For example, compared with the L-UNet method, our proposed method achieves a significant improvements, including a 16.32% increase in accuracy metric, a 2.61% increase in recall metric, and a"}, {"title": "4.5 Ablation Study", "content": "In this section, we provide a detailed analysis of the different components, hyper-parameter settings, different mask thresholds and different interpolated video frames in the proposed CTMA method on the WHU-CD dataset [35]. By conducting a detailed analysis of the effectiveness of these pivotal factors, we aim to elucidate their impact on model performance and provide readers with a more comprehensive understanding and reference.\nAnalysis of different components. In order to comprehensively evaluate the contribution of each individual component of the proposed CTMA method on network performance, we conduct an ablation experiment for component analysis on the"}, {"title": "4.7 Limitation Analysis", "content": "Although the proposed CTMA model has achieved good performance on remote sensing change detection datasets, it still faces several challenges. As illustrated in Fig. 6, the model struggles with detection effectiveness in scenarios where there are either too many or too few regions of change. For instance, the boundary detection is not clear enough (second row), and some change regions are missing (first and third rows).\nUpon conducting a thorough analysis of these shortcomings, we believe that the main possible reasons are as follows: 1) There may be errors in the temporal encoder when predicting change masks, making it inherently difficult to achieve optimal performance based on these inaccurate masks, which guide the overall learning process of the change detection framework. 2) Although the model attempts to integrate global and local information, it lacks an effective supervision mechanism to ensure the successful integration and complementarity of these two types of information. This can result in incomplete or missing information in the predicted change regions. Therefore, we can conduct further exploration around these two aspects in the future."}, {"title": "5 Conclusion", "content": "This work builds upon the existing bi-temporal images-based framework for remote sensing change detection by introducing a novel Coarse-grained Temporal Mining Augmented (C\u03a4\u039c\u0391) framework. Our approach, which incorporates motion cues through the transformation of bi-temporal images into a video and the subsequent extraction of motion features, demonstrates the significance of temporal information in change detection tasks. By integrating these motion features with spatial features and utilizing a ResNet for fine-grained feature learning, our method achieves enhanced performance in predicting changed regions. The segmentation and integration of coarse-grained changed regions into decoder blocks further refine the change prediction process. The extensive experimental results across various benchmark datasets conclusively validate the effectiveness of our proposed framework. The source code for this study is made available allowing for further research and application development in the field of remote sensing image change detection.\nIn our future works, we will further consider adopting more lightweight and hardware-friendly networks to build our framework, such as Mamba [41, 42] or RWKV [43]. It will achieve a better trade-off between the detection performance and computational cost. Also, we will consider introducing semantic information perception modules to mining the locally changed regions well. We believe this will fill the holes in our final prediction as illustrated in Fig. 6."}]}