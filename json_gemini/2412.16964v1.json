{"title": "System-2 Mathematical Reasoning via Enriched Instruction Tuning", "authors": ["Huanqia Cai", "Yijun Yang", "Zhifeng Li"], "abstract": "Solving complex mathematical problems via system-2 reasoning is a natural human skill, yet it remains a significant challenge for current large language models (LLMs). We identify the scarcity of deliberate multi-step reasoning data as a primary limiting factor. To this end, we introduce Enriched Instruction Tuning (EIT), a method that enriches existing human-annotated mathematical datasets by synergizing human and AI feedback to create fine-grained reasoning trajectories. These datasets are then used to fine-tune open-source LLMs, enhancing their mathematical reasoning abilities without reliance on any symbolic verification program. Concretely, EIT is composed of two critical steps: Enriching with Reasoning Plan (ERP) and Enriching with Reasoning Step (ERS). The former generates a high-level plan that breaks down complex instructions into a sequence of simpler objectives, while ERS fills in reasoning contexts often overlooked by human annotators, creating a smoother reasoning trajectory for LLM fine-tuning. Unlike existing CoT prompting methods that generate reasoning chains only depending on LLM's internal knowledge, our method leverages human-annotated initial answers as \u201cmeta-knowledge\" to help LLMs generate more detailed and precise reasoning processes, leading to a more trustworthy LLM expert for complex mathematical problems. In experiments, EIT achieves an accuracy of 84.1% on GSM8K and 32.5% on MATH, surpassing state-of-the-art fine-tuning and prompting methods, and even matching the performance of tool-augmented methods.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are increasingly recognized as a promising stride towards realizing Artificial General Intelligence (AGI) due to (1) their potential for a wide range of intelligent activities and (2) their scaling up of performance as increased dataset size, model size, and computing budget (Bubeck et al., 2023; Kaplan et al., 2020). Surprisingly, all of the current advances in LLMs are established on an embarrassingly \u201csimple\" training paradigm, i.e., Transformer decoder + next-token prediction (Radford et al., 2018). Though promising, the most powerful LLMs, e.g., GPT-4 (Achiam et al., 2023), still frequently make ridiculous mistakes when solving fundamental mathematical problems, e.g., multiplication of four digits (Hagendorff et al., 2022), hindering their usage in scenarios requiring mathematical reasoning, such as autonomous driving (Fu et al., 2024), embodied intelligence (Brohan et al., 2023; Yang et al., 2024; Zhou et al., 2024), and teaching assistance (Hicke et al., 2023). The root of this issue is that LLM-generated responses entirely rely on next-token prediction, a process akin to our intuitive, quick-thinking system-1 reasoning (Daniel, 2017). This is fine for easy tasks but falls short for complex mathematical problems, in which humans usually switch to thoughtful, logical system-2 reasoning, a slower and more deliberate way of thinking while making our answers more accurate and less biased (Daniel, 2017). The deficiency of system-2 mathematical reasoning in LLMs has motivated many recent works that can be divided into two categories: fine-tuning-based and prompting-based methods. Fine-tuning-based methods, such as Phi-GSM (Liu et al., 2023), MetaMath (Yu et al., 2023), MathGenie (Lu et al., 2024), and Orca-Math (Mitra et al., 2024), update LLMs' parameters by distilling privileged LLMs or learning from human's reasoning trajectories. Many of them adopt very complicated augmentation strategies in order to boost performance (see"}, {"title": "2 Related Work", "content": "Mathematical reasoning is a critical aspect of human intelligence and a significant challenge for LLMs, which struggle with complex computations and symbolic manipulations (Lu et al., 2022). Prompt-based methods, such as Chain-of-thought prompting (CoT) (Wei et al., 2022; Yao et al., 2024), aim to improve reasoning by generating intermediate natural language reasoning trajectories. However, only using few-shot examples to prompt LLMs to generate a longer reasoning trajectory may cause them to suffer from severe hallucination issues and instabilities. (Hagendorff et al., 2022; Zhang et al., 2022). Fine-tuning on large-scale datasets of reasoning trajectories is another approach, but it is costly and impractical due to the lack of professional annotators and biased as-"}, {"title": "3 Enriched Instruction Tuning", "content": "According to Daniel Kahneman's theory of fast and slow thinking (Daniel, 2017), system-2 reasoning is crucial for humans or machines to solve complex mathematical problems (refer to our detailed discussion in Sec. 1). Based on the existing proven training paradigm, a straightforward method to incorporate such reasoning capabilities into LLMs is to collect a very large-scale dataset consisting of high-quality multi-step reasoning data from human feedback and to fine-tune a pre-trained LLM on this dataset via supervised or reinforcement learning (Ouyang et al., 2022; Joshi et al., 2023). However, collecting high-quality multi-step reasoning data is highly challenging: it requires well-trained professional human annotators (Wang et al., 2024), and on the other hand, the reliable and scalable assessment of feedback correctness remains an open problem (Huang et al., 2023). An alternative method is learning from AI feedback (LAIF) (Lee et al., 2023; Yu et al., 2023; Lu et al., 2024), which leverages powerful off-the-shelf LLMs to generate complete reasoning trajectories and evaluate their correctness in lieu of human annotators. Despite promising, LAIF cannot master system-2 reasoning capabilities before generating such data. Hence, how to generate high-quality, detailed reasoning data is still an open challenge. To this end, we combine the strengths of the two above methods in a novel way that avoids their limitations. Specifically, for existing human-annotated mathematical datasets, we prompt a powerful LLM (e.g., GPT-4) to fill in missing contextual information on high-quality but usually sparse human-annotated reasoning trajectories, forming a series of enriched and fine-grained \u201cthought chains\", which is then used to fine-tune an open-source LLM. Our approach, named Enriched Instruction Tuning (EIT), comprises two main steps: (1) generation of enriched instruction dataset and (2) LLM fine-tuning, which we will introduce in the follow-\""}, {"title": "3.1 Generation of Enriched Instruction Dataset", "content": "Given human-annotated mathematical instruction datasets such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), the responses provided by human annotators are typically accurate yet sparse, omitting many of the implicit reasoning steps that humans adopt when solving complex tasks. These omitted steps may include the usage of common sense, identification of variable meanings, determination of causal relationships, and even the pre- and re-planning that humans and many animals implicitly execute in their brains (Daniel, 2017; LeCun, 2022; Hagendorff et al., 2022). As demonstrated in Fig. 1 (b), LLMs fine-tuned on such datasets can only learn to imitate intuitive and quick-thinking system-1 reasoning. Moreover, relying on human annotators to write out the comprehensive reasoning trajectories for tackling each complex problem is prohibitively expensive and impractical (Wang et al., 2024). To address this, we leverage powerful off-the-shelf LLMs to enrich the existing mathematical datasets with those missing reasoning contexts through few-shot prompting. The prompts we used can be found in Appendix H.1 and H.2. Example 3.1 provides a detailed comparison between the"}, {"title": "3.2 Finetuning LLMs on Enriched Instruction Dataset", "content": "We train an open-source LLM M on our collected EITMath dataset $D_E = \\{(x, y)\\}_{i=1}^N$ with token-level supervised finetuning (Ouyang et al., 2022). During training, M is expected to produce the corresponding response $y_t$ given the specific instruction $x_t$ as input. The training objective involves next-token prediction, consistent with all decoder-only language models (Radford et al., 2018; Touvron et al., 2023a,b). The likelihood of a target response is modeled as below.\n$P_M(y|x) = \\prod_{i=1}^{L} P_M(t_i | y_{<i}, x)$, (1)\nin which $y_{<i}$ denotes the target response tokens before the current predicted token $t_i$. Eq. 1 can be optimized using the maximum likelihood estimation with mini-batch stochastic gradient descent (Kingma and Ba, 2014)."}, {"title": "4 Experiments", "content": "In order to verify the effectiveness of EIT, we collect EITMath (see Table 2) and use it as the training dataset to fine-tune open-source LLaMA-2 (Touvron et al., 2023b) 13B and 70B models and evaluate their testing performance on two popular benchmarks, MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021)."}, {"title": "4.1 Datasets", "content": "EITMath Table 2 illustrates the composition of our collected EITMath dataset, which contains 15k problems (7.5k from MATH and 7.5k from GSM8K) and the corresponding answers enriched with our proposed ERP and ERS prompting methods. The detailed construction pipeline can be found in Sec. 3.1 and illustrated in Fig. 2. More details of datasets like MATH and GSM8K are provided in Appendix F."}, {"title": "4.2 Experimental Setup", "content": "We use open-source LLMs LLaMA-2 as the base model for fine-tuning. GPT-4-1106-preview is used to generate enriched responses for constructing EIT-Math. Following the prior work (Yu et al., 2023; Xu et al., 2023), we adopt the AdamW optimizer to train the model with 3 epochs, and the learning rate is set to 2e-5. The batch size is 32 for the 70B model and 128 for the 13B model. 32 A100 GPUs are used to fine-tune the above models. We conduct the ablation study on MATH and GSM8K datasets based on LLaMA-2-70B in Table 3. To make a fair comparison, we use the same dataset size of 7.5k as MATH and GSM8K's training dataset. Furthermore, we combine EIT with MetaMath (Yu et al., 2023), a method focusing on augmenting questions themselves, to explore whether combining them results in better performance. Considering other SOTA methods using larger datasets (e.g., MetaMath (Yu et al., 2023) for 395k, MAmmoTH (Yue et al., 2023) for 260k), we also expand EITMath to 70k in order to make a fair comparison. To this end, we construct such a dataset with more diverse responses by sampling GPT-4's output distribution with different temperature coefficients, which slightly improves EIT's performance."}, {"title": "5 Ablation Study", "content": "To evaluate the effectiveness of our proposed Enriching with Reasoning Planning (ERP) and Enriching with Reasoning Step (ERS) prompting methods, we conducted the ablation study on MATH and GSM8K datasets based on LLaMA-2-70B. We construct ERP and ERS datasets separately by prompting GPT4. The prompts can be found in Appendix H.3 and Appendix H.4\nEffects of ERP Table 3 shows the performance of baseline methods and the enhancements achieved by finetuning LLMs on enriched datasets with ERP prompting. The baseline exhibits an accuracy of 14.9% on the MATH dataset and 67.3% on the GSM8K dataset. Incorporating ERP results in a notable accuracy increase to 17.2% on MATH and 72.0% on GSM8K, corresponding to improvements of 2.3% and 4.7%, respectively. These results underscore the significant role of ERP in improving LLM's reasoning capabilities. By generating a high-level plan, ERP effectively decomposes complex instructions into lower-level objectives, thereby enhancing the coherence and direction of the ensuing reasoning process.\nEffects of ERS The performance of the ERS is also quantified in Table 3. ERS leads to a rise in accuracy from 14.9% to 19.5% on the MATH dataset, marking a 4.6% improvement. Similarly,"}, {"title": "6 Comparison with State of the Art", "content": "6.1 Results on MATH and GSM8K\nAs shown in Table 4,\ncompared to the same open-source LLMs that do not use any external tools, such as code verification, EIT achieves the best performance.\nOur EIT-70B model achieves a leading accuracy of 32.5%, with a 2.7% improvement over MetaMath-70B and more than double the accuracy of the baseline LLaMA-2-70B model. For the 13B model, our EIT model also achieves the best performance among models of the same type, achieving 23% accuracy. These results underscore the superiority of our ERP and ERS methodologies in comparison to existing approaches. To further ana-lyze the improvement of our method, we show the accuracy of subtopics on MATH in Table 5. Our EIT exceeds WizardMath and MetaMath across all subtopics. Notably, EIT achieves 15.4% on the most challenging topic, Precalculus, surpassing MetaMath 4.6% and WizardMath 2.8%."}, {"title": "6.2 Analysis from a Scaling Law Perspective", "content": "Better Performance with More Data The scaling law of dataset size is the most attractive property of LLMs. However, recent findings from MetaMath demonstrate that more data is not always better. By combining the existing augmented dataset Rejection sampling Fine-Tuning (RFT) (Yuan et al., 2023) with MetaMathQA of different scales for finetuning, we found that more augmented data hurts performance (Yu et al., 2023). In contrast, our method presents a different perspective. As shown in Fig. 3 left, we conducted a series of fine-tuning experiments on the LLaMA-2-70B model using different subsets of MetaMathQA data\u201420k, 40k, 80k, and 120k samples\u2014combined with our EITMath dataset. When compared to the integra-tion with RFT, our approach yields a markedly better performance. The inclusion of RFT consis-tently led to a decline in performance across all data scales. On the other hand, the incorporation of EIT-Math not only enhanced the model's performance but also demonstrated the benefits of additional data scale positively with performance, in line with the scaling law. This indicates that the quality of data augmentation and the strategic approach to dataset construction are pivotal factors. Our find-ings affirm that, based on EIT, more data can indeed lead to better performance in training LLMs.\nBetter Performance with More Fine-grained Reasoning Steps Observing the performance im-provement brought by scaling dataset size, we raise a question: whether more fine-grained reasoning steps can also enhance performance? To explore this, we curated multiple datasets derived from the MATH benchmark, each featuring a different average token number of responses. These datasets were constructed using the same strategy as EIT-"}, {"title": "6.3 Analysis from a Perplexity Perspective", "content": "Prior work (Yu et al., 2023) demonstrates that the simplicity of the data can better elicit the reasoning abilities of LLMs, because such data are easier for the models to learn from, leading to improved performance. The simplicity can be measured by perplexity (Marion et al., 2023). We calculate the perplexity of enriched responses from our EITMath using the LLaMA-2-7B model without any additional fine-tuning. The results show that the per-plexity of EITMath is lower than all other methods in Fig. 3 right, and the LLM fine-tuned on EITMath also achieves the highest accuracy on GSM8K."}, {"title": "6.4\nComparison between EIT and CoT", "content": "Our method significantly differs from the Chain-of-Thought (CoT) prompting method in several key aspects. While CoT relies on generating inter-mediate natural language trajectories to improve reasoning capabilities using the model's \u201cinternal knowledge\", EIT leverages human-annotated initial answers as \u201cmeta-knowledge\u201d to guide the model in generating more detailed and smoother reason-ing, making the task more similar to a fill-in-the-blank question. This reduces the complexity of the task, as the model only needs to elaborate on the given answer rather than generate the entire"}, {"title": "7 Conclusion", "content": "In this paper, we aim to build an LLM capable of system-2 mathematical reasoning. To achieve this, we introduce Enriched Instruction Tuning (EIT), which leverages the synergy between human and AI feedback to produce enriched instruction datasets consisting of fine-grained reasoning trajectories. These datasets are then utilized to fine-tune open-source LLMs, thereby boosting their inherent ability to execute system-2 mathematical reasoning without any usage of external tools. Compared"}, {"title": "B Quality Analysis of Our Proposed EITMath Dataset", "content": "We also conduct a quality analysis of our EIT-Math by employing perplexity as a metric to evaluate and calculating the accuracy of final answers(Azerbayev et al., 2023b; Brown et al., 2020). As discussed in Sec. 6.3, our analysis indicates that perplexity is inversely proportional to accuracy. The perplexity of our generated data is significantly lower than that of the original answers, demonstrating the high quality of our generated data. Additionally, we compared the final answers of the generated data with the original answers to calculate accuracy. Our results show an accuracy rate of 99.7% for the generated answers, whereas"}, {"title": "C Discussion on Prompting and Fine-tuning Methods", "content": "To further validate the effectiveness of our EIT, we conducted a comparative analysis on fine-tuning and prompting methods, especially the CoT method. As illustrated in Table 6, the performance of prompting with few-shot examples including ERP and ERS is inferior to that of standard CoT prompting. This is because ERP and ERS place higher demands on the model's capabilities, like requiring a more nuanced understanding of com-plex instructions and stronger reasoning abilities to generate more detailed and smoother reason-ing trajectories. Conversely, fine-tuning with stan-dard CoT performs significantly better than stan-dard CoT prompting, particularly on the GSM8K dataset, where it outperforms CoT prompting by 11.5%. This improvement can be attributed to fine-tuning's ability to stimulate the model's CoT capa-bilities while avoiding the instability introduced by complex prompts.\nNotably, our EIT achieves the best performance, with the accuracy of 32.5% on the MATH dataset and 84.1% on the GSM8K dataset, representing nearly a 25.3% and 51.2% improvement compared to prompting with ERP and ERS and over 15% improvement than fine-tuning with standard COT. Unlike data constructed using the CoT method, EIT further leverages human-annotated initial answers as \"meta-knowledge\u201d, which helps LLMs generate more detailed and smoother reasoning reasoning trajectories. This approach effectively mitigates the accumulation of model hallucinations as the length of inference increases, thereby ensuring the accuracy of our constructed data."}, {"title": "D More Results on Self-Consistency", "content": "Self-consistency (Wang et al., 2022) helps to boost models' performance, especially on arithmetic benchmarks such as GSM8K. We investigate the impact of scaling decoding samples and observe that EIT outperforms other methods, as demonstrated in Table 7. ToRA, with external tools, per-forms comparably to EIT without self-consistency decoding. Applying self-consistency decoding"}, {"title": "E Discussion on Datasets Only with Final Answers", "content": "Considering that a small portion of datasets con-tains only the final answers without accompanying reasoning, generating enriched instruction datasets becomes more challenging. We plan to extend our method to such datasets in future work. The potential approach involves a two-stage process. First, LLMs will be prompted to generate sparse responses, those that lead to the correct final an-swer will kept for the second stage. In the sec-ond stage, we enrich the reasoning trajectories pro-gressively in several iterations and use the COT-decoding (Wang and Zhou, 2024) to choose reason-ing trajectories with higher confidence."}, {"title": "F Datasets Details", "content": "MATH dataset collects a total of 12,500 competition-level mathematics problems, which are partitioned into 7,500 for training and 5,000 for"}, {"title": "G Analysis on Case Study", "content": "Example G.1 shows a randomly selected case including responses generated by SFT (supervised fine-tuning using publicly available training sets), MetaMath, and our EIT on the test set of MATH. It is obvious that the first half of the solution from SFT is correct, but there was an error when cal-culating $ \\frac{13}{4} \\times 36$ due to merging calculations. As for MetaMath, unlike SFT, it divides 3 yards into 3 and to calculate inches respectively and find their sum finally. MetaMath aims to convert 1/4 yards into inches by utilizing the yard-foot and foot-inch relationships. However, it neglects the relationship between yards and foot, which leads to hallucinations finally. In contrast to the above methods, firstly, our EIT illustrates ERP accurately, guiding the subsequent steps to convert the mixed number to an improper fraction and then multi-ply by the conversion factor that relates yards to inches. Secondly, EIT demonstrates ERS following the guidance of ERP, which illustrates the conversion factor as the base knowledge and multiplies the converted improper fraction with the conver-sion factor. Furthermore, EIT divides $\\frac{13}{4} \\times 36$ into"}, {"title": "H Prompt For Our EIT", "content": "We present specific instructions and example few-shot prompts of our EIT for querying GPT-4 to generate EITMath."}, {"title": "H.1 Prompt for Enriching GSM8K (ERP and ERS)", "content": "The prompt of EIT for enriching GSM8K dataset can be found in Example H.1"}, {"title": "H.2 Prompt for Enriching MATH (ERP and ERS)", "content": "The prompt of EIT for enriching MATH dataset can be found in Example H.2"}, {"title": "H.3 Prompt for Enriching MATH (ERP)", "content": "The prompt of ERP for generating a high-level plan for MATH dataset can be found in Example H.3, which is used for ablation study in Sec.5."}, {"title": "H.4 Prompt for Enriching MATH (ERS)", "content": "The prompt of ERS for enriching MATH dataset can be found in Example H.4, which is used for ablation study in Sec.5."}]}