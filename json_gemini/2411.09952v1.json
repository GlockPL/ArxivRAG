{"title": "GGAvatar: Reconstructing Garment-Separated 3D Gaussian Splatting Avatars from Monocular Video", "authors": ["Jingxuan Chen"], "abstract": "Avatar modelling has broad applications in human animation and virtual try-ons. Recent advancements in this field have focused on high-quality and comprehensive human reconstruction but often overlook the separation of clothing from the body. To bridge this gap, this paper introduces GGAvatar (Garment-separated 3D Gaussian Splatting Avatar), which relies on monocular videos. Through advanced parameterized templates and unique phased training, this model effectively achieves decoupled, editable, and realistic reconstruction of clothed humans. Comparative evaluations with other costly models confirm GGAvatar's superior quality and efficiency in modelling both clothed humans and separable garments. The paper also showcases applications in clothing editing, as illustrated in Figure 1, highlighting the model's benefits and the advantages of effective disentanglement. The code is available at https://github.com/J-X-Chen/GGAvatar/.", "sections": [{"title": "1 INTRODUCTION", "content": "Reconstructing realistic clothed digital humans and their garments is a significant task in computer graphics and computer vision. This type of work aims to synthesize high-resolution clothed human body images from an unprecedented view or generate human imagery in a novel pose. Previous research has delved into explicit modelling methods under costly capture systems to obtain suboptimal reconstruction outcomes [36, 38]. Recent advancements have shifted towards direct construction from single RGB images or monocular videos, utilizing models with implicit representation such as Neural Radiance Field (NeRF) [27] to capture fine textures on the surface. However, these models [4, 9, 32, 40] require dozens of training hours. Consequently, current studies[11\u201313, 15, 19, 20, 34] are increasingly focused on enhancing rendering speed and modelling efficiency by turning neural rendering techniques into Instant-NGP[28] or 3D Gaussian Splatting (3DGS) [16]. Nevertheless, the lack of disentanglement functions in these existing avatar models may constantly limit their applicability in real-world scenarios. This paper argues that an ideal avatar model should not only produce high-quality, rapid, and thorough reconstruction results, but also possess the decoupling capability necessary for applications such as virtual try-ons.\nUnfortunately, creating a perfect editable and drivable avatar is a demanding task that presents several challenges. Firstly, to effectively disentangle the body and garments, integrity and anti-interference properties must be maintained between distinct components. Specific estimations are required for the unsupervised areas where the human body is obstructed. Secondly, a precise transformation between canonical space and various pose spaces must be established to locate the partitioned point cloud at the target position. Lastly, it is essential to capture diverse and intricate clothing details, including textures, and to achieve high-quality reconstructions from sparse monocular inputs, particularly for loose-fitting attire. However, works such as [6, 8, 14, 18, 22, 23, 33] are limited to recovering geometry without providing corresponding appearance information.\nIn response to these challenges, this paper proposes a novel framework, GGAvatar, designed to construct realistic avatars from monocular videos while effectively and completely separating the garments. Specifically, this paper builds and fits garment templates alongside the corresponding body template to achieve a preliminary state of separation and interference resistance, resulting in partitioned point sets. Phased trainable modules (isolation and joint training) reasonably prevent the intersection of point sets during the training process. Subsequently, the target Gaussian positions are ensured by constructing deformation fields based on a concentric skeleton. Simultaneously, high-quality rendering is accomplished using 3DGS. Notably, GGAvatar enables thorough separation of clothed humans in novel view synthesis tasks from monocular inputs-potentially a first in this field, to my knowledge.\nThe paper evaluates the GGAvatar model by comparing it with baseline approaches and other works on the People Snapshot Dataset [1] or the ZJU Mocap Dataset [32]. The results indicate that GGAvatar demonstrates a high level of reconstruction quality for clothed humans, comparable to that of other 3DGS-based models. Notably, the proposed model outperforms nearly every traditional NeRF-based model while exhibiting significantly faster training speeds-approximately hundreds of times faster than the NeRF counterparts. Furthermore, ablation studies are conducted to validate the effectiveness of each component. To highlight the superiority and practical utility of GGAvatar, this paper compares it with existing non-fully decoupled models on clothing transfer.\nThe contributions are summarized as follows:\n\u2022 This paper proposes the GGAvatar model, based on phased training methods, to achieve high-quality and efficient construction for various viewpoints or pose synthesis tasks of clothed humans.\n\u2022 The method of constructing parameterized templates for garments is introduced to solve the challenge of complex clothes modelling.\n\u2022 The GGAvatar enables a thorough separation between different garments, allowing applications such as colour editing and clothing transfer."}, {"title": "2 RELATED WORK", "content": "Geometry Reconstruction. The Skinned Multi-Person Linear (SMPL) model[25] is a widely used parametric model for human body shape and pose estimation. The extended versions (such as SMPL-H, SMPL-X[31], STAR[29]) can characterize the human body as a deformable mesh in a low-dimensional linear space with learned parameters, allowing for accurate capture of diverse body shapes and poses. Similarly, clothing geometry can be obtained from video sequences using methods [2, 7, 14, 18, 23, 30, 33]. The SMPLicit[6] and Neural-ABC [3] can effectively edit the modelled geometry of fully clothed humans with a latent vector. Additionally, the Implicit Sewing Patterns (ISP) [22] method enables efficient 3D reconstruction of multi-layered garments from a single image. In this approach, each garment mesh is reconstructed by sewing together two individual 2D panels associated with the Signed Distance Function (SDF) value of the 3D surface, alongside the stitching positions generated by neural networks. However, these models focus solely on geometric representation and ignore texture capture.\nDynamic Character Reconstruction. HumanNeRF[40] and Neu-ral Body[32] leverage geometric priors to create implicit neural representations of dynamic humans, synthesizing realistic body details. InstantAvatar [15] combines Instant-NGP[28] (a method that relies on CUDA programming and hash functions) with Fast-SNARF[5] (a spatial transformation algorithm), enabling rapid rendering. For the 3DGS-based techniques[16] (which map Gaussian point clouds directly instead of accumulating the colour values of the pixel blocks) the Gaussian Articulated Template Model (GART)[20], Hu-man Gaussian Splats (HUGS) [19], and GaussianAvatar[12] enhance rendering speed and performance in the reconstruction of animations by managing loose garments with latent bones, constructing avatars and scenes, or performing pose pre-processing, respectively. Although these methods successfully reconstruct avatars, they all overlook the importance of disentanglement.\nDecouplable Human Reconstruction. The existing separable models require training on a large set of 3D clothed human inputs obtained from multi-camera systems [37, 41]. For instance, PhysAvatar [44] and LayGA [24] rely on multi-view video data to achieve excellent reconstruction through reverse and layered rendering frameworks. In contrast, the Segmented Clothed Avatar Radiance Field (SCARF) [9], an innovative hybrid model that merges SMPL-X [31] with NeRF technology, allows for the reconstruction of clothed human avatars and their garments directly from monocular videos. On the basis of SCARF, Disentangled Avatars (DELTA) [8] builds an additional hair component while maintaining the same garment modelling method. However, these approaches have drawbacks, including extensive training time requirements and limited capacity for separation between garments."}, {"title": "3 PARAMETRIC REPRESENTATION", "content": "Body Representation. The SMPL[25] model T(b) (\u03b2, \u03b8) with shape \u03b2\u2208 R10 and pose \u03b8\u2208 R(nk+1)\u00d73 is utilized as a low-cost parametric expression of the human body, where nk (nk = 23) is the number of the joints. Following [8, 9], this paper adds offsets O \u2208 R|V(b)|x3 to all the |V(b)| vertices to capture localized geometric details. The human template (V(b), Face(b)) can be totally represented as follows.\n$$M^{(b)}(\\beta, \\theta) = W (T_{can} (\\beta, \\theta, 0), J(\\beta), 0, W^{(b)}),$$\n$$T_{am} (\\beta, \\theta) = T_{can} (\\beta, \\theta) + B_s(\\beta) + B_\\rho(\\theta) + O.$$"}, {"title": "4 METHOD", "content": "This module offers a comprehensive overview of the GGAvatar model, illustrated in Figure 2. The entire process can be systematically divided into three sections: garment initialization, deformation field processing and rendering."}, {"title": "4.1 Garment Templates Estimation", "content": "Currently, some avatar models [12, 34] adopt a points offset strategy to depict the position of the outermost single layer of clothing. However, reconstructing clothing based solely on a single layer is insufficient for effective garment isolation, as a clothed human typically involves multiple layers of geometry (at least one layer of skin and one layer of garment). Therefore, additional clothing templates should be employed as an initialization stage for GGAvatar, tackling the modelling challenges posed by garment irregularities.\nThe garment template estimation begins with accurately estimating the human pose using FrankMocap[35] to determine the correct parameters \u03b2 and \u03b8. Due to variations in SMPL estimation across different views, the front view of the human body must be selected and aligned with the corresponding segmentation from Self Correction for Human Parsing (SCHP) method [21], serving as the input for the Implicit Surface Prediction (ISP) model[22]. After applying two types of Multi-layer Perceptrons (MLPs), the front and back clothing components, with SDF values and 3D shapes, are merged according to stitch-up order numbers to create a clothing mesh as T(c) (\u03b2, \u03b8).\nIt is essential to establish correct spatial alignment between all garments and the human body. All the garment templates are stored within a unified canonical space, defined as Tcan (\u03b2, \u03b8), which is consistent with the human body's SMPL model. After reshaping and building, vertices are extracted from these standard meshes to serve as the initial positions for garment Gaussian representation."}, {"title": "4.2 Gaussian Representation and Deformation", "content": "Drawing inspiration from 3D Gaussian Mixture Models[20], both garments and the human body reconstruction results should be represented as Gaussians. The Gaussian vertex set V can be V (c), (b) or a combination of these. The i-th Gaussian component in Gaussian set with vertices V can be defined by a 3D mean \u00b5(i), a 3D rotation R(i) for orienting, anisotropic scaling factors s(i) for size adjusting, an opacity factor n (i) and a colour radiance function expanded by spherical harmonics [10, 16]. Additionally, the learnable skinning weights Wk for the k-th joint, with adjustment \u2206wj, can be written as:\n$$W_k(\\mu^{(i)}) = \\bar{W}_k(\\mu^{(i)}) + \\Delta w_j, \\quad i \\in V, k \\in \\{1, 2, ..., n_k \\},$$"}, {"title": "4.3 Rendering with Splatting.", "content": "To preserve the correct shape and orientation of the Gaussian, the covariance is defined as \u03a3(i) = RSSTRT, where S is the scaling matrix and R is the rotation matrix, effectively characterizing the Gaussian ellipsoid. In the context of a camera with an extrinsic E and an intrinsic matrix K, 3DGS[16] achieves notable quality and speed enhancements by employing the mapping operation \u03c0(x; \u0395, \u039a), in contrast to the ray-by-ray calculations used in NeRF [27]. Let Ci) (x, d) denote the colour at querying position x from the i-th Gaussian in the observation space. Following most volume rendering techniques and drawing inspiration from NeRF and Radon transformation theory, the rendered image can be generated from 2D observations as follows.\n$$I (G^{(\\theta)}, d) = \\sum_{i \\in N} c^{(i)} (x, d) \\alpha^{(i)} (1 - a^{(i)}),\\\\ c^{(i)} (x, d) = sph (R'^{(i)} d, f^{(i)}).$$"}, {"title": "4.4 Training Losses", "content": "This paper augments the baseline optimization [16, 20] with additional regularization terms to ensure a smooth effect. In the initial isolation stage, a densify-and-prune strategy, as outlined in 3DGS, is employed and every reconstruction loss for garments and body parts under the SCHP [21] segmentation is summed up. In the subsequent joint training phase, this paper optimizes the Gaussians without adding or removing any components. The primary reconstruction loss [10, 16, 20] between ground truth I (\u03b8) and rendered image \u00ce (\u03b8) can be expressed as follows for each estimated pose \u03b8.\n$$L_{recon} = L_1(\\hat{I} (\\theta), I (\\theta)).$$\nStochastic Structural Similarity Loss. Considering distant or global information, the model replaces the Structural Similarity Index measure (SSIM) loss [39] with the Stochastic Structural Similarity (S3IM) loss [42]. The rendered image is randomly cropped into the patches \\hat{I}_{patch}^{(n)} (\\theta) with the same operation applied to the corresponding ground-truth image patches I_{patch}^{(n)} (\\theta) to calculate SSIM using a K \u00d7 K kernel and a stride of s.\n$$L_{S3IM} = \\frac{1}{N_s} \\sum_{n=1}^{N_s} L_{SSIM} (\\hat{I}_{patch}^{(n)} (\\theta),  I_{patch}^{(n)} (\\theta)),$$\nwhere N, is the number of repetitions.\nDeformation Regularization. Inspired by [26] and [17], this paper leverages the \"as-isometric-as-possible\" constraint to preserve a similar distance and shape after deformation, thereby reducing the occurrence of artefacts.\n$$L_{iso} = \\sum_{i=1}^{|V|} \\sum_{j \\in N(i)} (\\lambda_\\mu |d(\\mu^{(i)}, \\mu^{(j)}) - d(\\mu_0^{(i)}, \\mu_0^{(j)}) | + \\lambda_c |d(c^{(i)}, c^{(j)}) - d(c_0^{(i)}, c_0^{(j)}) |).$$\nIn this equation, the distance function d corresponds to the L2 norm. Subscripts of zero denote attribute values under the canonical pose.\nThe complete loss function comprises several components: an RGB loss Lrecon, a mask loss Lmask for both garments and body, LS3IM, Liso, a Gaussians similarity regularization LG_reg and a collision loss Lcol for the joint training. Overall, the loss function of these two training stages is manifested as\n$$L_{isolation} = \\sum_{c=1}^{n_c} (L_{recon} + \\lambda_1 \\times L_{mask}^{(c)} + \\lambda_2 \\times L_{S3IM}^{(c)} + \\lambda_3 \\times L_{G\\_reg}^{(c)}),\\\\ L_{joint} = L_{recon} + \\lambda_2 \\times L_{mask} + \\lambda_3 \\times L_{S3IM} + \\lambda_4 \\times L_{G\\_reg} + \\lambda_5 \\times L_{iso} + \\lambda_6 \\times L_{col}.$$"}, {"title": "5 EXPERIMENTS", "content": "This paper performs a comprehensive evaluation and application demonstration of the proposed model on monocular datasets. The effectiveness of GGAvatar will be assessed by comparing it with existing Nerf-based and 3DGS-based methods under the same SMPL pose, along with an ablation test."}, {"title": "5.1 Dataset", "content": "The People Snapshot dataset [1] consists of videos of individuals rotating in front of a stationary camera, while the ZJU-MoCap dataset [32] provides detailed motion sequences of various human activities, capturing a range of poses, expressions, and clothing styles. The People Snapshot dataset is split into 80% for training, 10% for validation, and 10% for testing. In the case of the ZJU-MoCap dataset, the experiments use images from 'camera 1' as input and employ the other cameras for evaluation."}, {"title": "5.2 Comparison with SoTA Methods", "content": "Whole Human Synthesis Quality. To compare the proposed model with existing non-couplable methods and assess its adaptability to novel perspectives or poses, numerous avatar methodologies were thoroughly trained for the recommended number of epochs as outlined in the reference papers. Table 1 showcases the improvement in certain image metrics (PSNR, SSIM[39], and LPIPS [43]) for GGAvatar on the People Snapshot dataset, outperforming methods related to NeRF, such as Neural Body[32], InstantAvatar[15], and the baseline GART[20]. Furthermore, Figure 3, qualitatively analyzes the resemblance between the synthetic images and the ground truth on the ZJU-MoCap dataset. Notably, the clarity of the results produced by the proposed model is superior to that of competing methods. In summary, the reconstruction results can reach state-of-the-art performance in certain metrics across specific datasets, positioning GGAvatar on par with other 3DGS-based methods.\nGarment Reconstruction Quality. Considering the scarcity of models for reconstructing texture-based clothing from monocular videos, this paper highlights the quality of clothing reconstruction primarily through comparison with the SCARF[9], which is equivalent to the Delta[8] model. Figure 4 illustrates that clothing reconstruction surpasses both versions of SCARF (the 20-minute and 2-day iterations). Under the same time constraints, SCARF fails to achieve satisfactory clothing modelling within 20 minutes. In comparison to the full model, the results demonstrate rationality at the clothing junctions, while SCARF exhibits a noticeable colour gradient effect. Moreover, human skin processed by SCARF shows significant staining from clothing, which is much more pronounced than in the proposed approach.\nReconstruction Speed. GGAvatar requires a mere 20-minute training duration on a single RTX 3080 Ti (laptop or server), which is significantly faster than NeRF-based methods. For instance, Neural Body[32] demands 14 hours on 4\u00d7 RTX 2080 or a full training day on a 3080 Ti, while HumanNeRF[40] mandates approximately 3 days on the same hardware setup. For the decoupled models, SCARF[9] entails around 2 days (40 hours on NVIDIA V100) for training, which is hundreds of times slower than GGAvatar. Additionally, it achieves remarkable rendering efficiency, delivering approximately 80 FPS for images sized at 540 \u00d7 540."}, {"title": "5.3 Ablation Study", "content": "Table 2 and Figure 5 summarize an ablation study conducted to assess the influence of training type and loss function. The differences are primarily reflected in the metrics (PSNR, SSIM, LPIPS) and the contours around the outputs.\nTraining strategy. The investigation of the multiple training steps, as illustrated in Table 2 and Figure 5, indicates that the model lacking the final joint training exhibits performance deficiencies. This reduction may be attributed to the imperfect merging of clothing boundaries and the human body during the stitching process, resulting in artefacts at the connections.\nLoss function. As depicted in Table 2 and Figure 5, the omission of both LS3IM and Liso leads to a decline in rendering quality. The regularization loss LS3IM, as described in Section 4.4, effectively reduce noise surrounding the 3D contours of the garments in Figure 5."}, {"title": "5.4 Application", "content": "On the application level, in addition to the basic function of synthesising new perspective images, GGAvatar offers three extra features described in this section.\nAnimation. Similar to recent avatar approaches, GGAvatar offers fine-grained control over novel body poses and movements, such as eating, walking, and dancing. Figure 6 shows that the model preserves reconstruction quality even when animations are driven according to predefined action sequences\nClothing Transferring. The model facilitates clothing transfer across avatars by combining Gaussians from diverse individuals. Unlike methods that only support holistic garment transfer, such as SCARF, this approach achieves inter-clothing separation, allowing the transfer of individual clothing items, such as shirts or pants, to another human, as demonstrated in the middle section of Figure 6. Furthermore, there is flexibility in selecting distinct garment components, as shown on the left side of the figure.\nColour Editing. Regarding clothing colour modifications, the model accepts colour keywords or RGB values as input without necessitating manual conversion of spherical harmonics. For example, Figure 6 illustrates the alteration of the pants colour from subject 377 (from the ZJU-MoCap dataset) to crimson (RGB = [80, 0, 0]) and swapping of the two colour channels of male-3-casual (from the People Snapshot dataset) to achieve either localized colour adjustment or a reversed colour effect."}, {"title": "6 CONCLUSION", "content": "This paper presents GGAvatar models to realise the separation of humans and garments while maintaining high-fidelity holistic avatar reconstruction. Unlike the existing decoupling models, GGAvatar improves reconstruction speed, adequacy of garment decoupling and overall quality through clothing initialisation, separation-based training and optimisation. Extensive experiments on the novel view or pose synthesis consistently demonstrate that the proposed model surpasses most implicitly represented clothed human reconstruction models in terms of both quality and efficiency. Additionally, this work proves the effectiveness of clothing editing. In real-world applications, the decoupled garments and novel views display features that make GGAvatar well-suited for virtual reality and virtual try-on scenarios."}]}