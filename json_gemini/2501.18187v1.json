{"title": "In-Context Learning of Polynomial Kernel Regression in Transformers with GLU Layers", "authors": ["Haoyuan Sun", "Ali Jadbabaie", "Navid Azizan"], "abstract": "Transformer-based models have demonstrated remarkable ability in in-context learning (ICL), where they can adapt to unseen tasks from a prompt with a few examples, without requiring parameter updates. Recent research has provided insight into how linear Transformers can perform ICL by implementing gradient descent estimators. In particular, it has been shown that the optimal linear self-attention (LSA) mechanism can implement one step of gradient descent with respect to a linear least-squares objective when trained on random linear regression tasks.\nHowever, the theoretical understanding of ICL for nonlinear function classes remains limited. In this work, we address this gap by first showing that LSA is inherently restricted to solving linear least-squares objectives and thus, the solutions in prior works cannot readily extend to nonlinear ICL tasks. To overcome this limitation, drawing inspiration from modern architectures, we study a mechanism that combines LSA with GLU-like feed-forward layers and show that this allows the model to perform one step of gradient descent on a polynomial kernel regression. Further, we characterize the scaling behavior of the resulting Transformer model, highlighting the necessary model size to effectively handle quadratic ICL tasks. Our findings highlight the distinct roles of attention and feed-forward layers in nonlinear ICL and identify key challenges when extending ICL to nonlinear function classes.", "sections": [{"title": "Introduction", "content": "In recent years, large language models (LLM) based on the Transformer architecture [Vaswani et al., 2017] have achieved great success in numerous domains such as computer vision [Dosovitskiy et al., 2021], speech recognition [Radford et al., 2023], multi-modal data processing [Yang et al., 2023], and even human reasoning tasks [OpenAI, 2023]. Beyond these practical achievements, large-scale Transformers have also demonstrated the remarkable ability to perform in-context learning (ICL) [Brown et al., 2020, Min et al., 2022, Wei et al., 2022b], where a model adapts to different tasks by leveraging a prompt containing a short sequence of examples without requiring updates to its parameters."}, {"title": "Problem setting", "content": "We are interested in the in-context learning problem, where a prompt consists of a sequence of input-output pairs {(xi, Yi)}=1 along with a query Xquery, where yi = f(xi) for an unknown target function f : Rd \u2192 R. Given this prompt, our goal is to make a prediction \u0177 so that \u0177 \u2248 yquery = f(xquery). In this paper, the prompt is represented as a matrix Z\u2208 R(d+1)\u00d7(n+1);\n\nZ = \n1 1\nX1 X1\n0 0\nY1 Y2\n\n..\nX1\n0\nYn\n\n1\nXn+1:= Xquery\n0\n0\n,\n\n(1)\n\nwhere the zeros have dimension d \u2013 d \u2013 1 and the bottommost zero hides the unknown quantity yquery that we aim to predict. In contrast to prior works (e.g. Ahn et al. [2024a], Mahankali et al. [2024], Zhang et al. [2024a]), we include a row of ones to accommodate potential inhomogeneity in the nonlinear target function f. Also, we introduce rows of zeros to embed the input in a higher-dimensional space.\nWe sample the prompts by first drawing i.i.d. Gaussian inputs x1,...,Xn+1 ~ N(0, \u03a3) with"}, {"title": "2.1\nIn-context learning", "content": ""}, {"title": "2.2\nTransformer architecture", "content": "Given a prompt Z\u2208 RD\u00d7N := R(d+1)\u00d7(n+1), the Transformer model is a sequence-to-sequence mapping RDXN \u2192 RD\u00d7N that consists of self-attention and feed-forward layers:\nATTENTION LAYER: The most crucial component of Transformers is the self-attention mechanism [Bahdanau et al., 2015], which can be written as\nSelf-Attention(Z) = Z + \\frac{1}{n}WvZM \\text{softmax}((WKZ) (WQZ)); M =\n\n\n\nI_n\\\\\n0\n0\n\nHere, WQ, WK, Wv \u2208 RD\u00d7D are learnable matrix parameters, and mask matrix M erases the attention score with the last column, which contains \u00e6query, due to the asymmetry that Yquery is not part of the prompt.\nFollowing the conventions established by Von Oswald et al. [2023], we consider a variant called linear self-attention (LSA), which omits the softmax activation and reparameterizes the weights as P := Wy, Q := WvTWQ:\nattn(Z) = Z + \\frac{1}{n}PZMZ\u012aQZ\n(2)\nWe note that LSA is practically very relevant \u2013 Ahn et al. [2024b] found that the optimization landscape of LSA closely mirrors that of full Transformers. Furthermore, there has been a recent line of work on adapting linear attention for more efficient LLMs [Dao and Gu, 2024, Yang et al., 2024a].\nFEED-FORWARD LAYER: The original Transformer model [Vaswani et al., 2017] employs a"}, {"title": "2.3\nIn-context learning objective", "content": "Finally, we define the objective of the in-context learning problem. Given an L-layer Transformer TF, we let its prediction \u0177TF be the (d+1, n+1)th entry of its final layer, i.e. \u00dbTF = Z[d+1, n+1]. Then, its in-context learning loss is the expected squared error with respect to the distribution of the prompts Z:\nL(TF) = Ez[(\u0177TF + Yquery)\u00b2].\n(4)\nSo that the Transformer's prediction \u0177tf approximates -Yquery. Our objective is to find Transformers that minimize this loss L."}, {"title": "3\nSolving quadratic in-context learning", "content": "Recent literature has explored the ability of linear self-attention (LSA) mechanisms to learn linear function classes in context. In this section, we demonstrate that LSA is inherently limited when it comes to nonlinear in-context learning (ICL). To address this limitation, inspired by modern Transformer architectures, we study a mechanism that combines LSA with bilinear feed-forward layers, enabling the model to solve quadratic ICL through kernel regression. Our analysis is quite general, and as we show in Section 5.2, it can be extended to handle ICL of higher-degree polynomials."}, {"title": "3.1 Limitations of linear self-attention in nonlinear in-context learning", "content": "Prior works [Ahn et al., 2024a, Mahankali et al., 2024, Zhang et al., 2024a] have shown that a single layer of linear self-attention (LSA) can learn linear functions in context by implementing one step of gradient descent with respect to a linear least-squares objective. Formally, for an optimal one-layer LSA model, its prediction given prompt Z can be expressed as\n\\hat{y} = \\frac{1}{n} \\sum_{i=1}^{n} X_{\\text{query}}^T \\Gamma x_i y_i,\nwhere \u0393 is a preconditioner associated with the LSA's parameters. This is equivalent to the prediction obtained after one step of preconditioned gradient descent with zero initialization and a loss function L(w) = \\frac{1}{2\\eta} \\sum_i (w^T x_i + y_i)^2.\nA natural question arises: can stacking multiple LSA layers provide enough expressive power to learn nonlinear functions in context? The following proposition answers this question by showing that, even with multiple layers, a linear Transformer cannot effectively predict a nonlinear target function.\nProposition 1. Consider any fixed target function f and inputs x(1),...,x(n+1) i.id. N(0, \u03a3). The in-context learning prompts are sampled according to the form as described in (1). Then, for any linear Transformer TFlin (regardless of the number of layers), its expected prediction loss satisfies\nEx[(YTFlin + yquery)\u00b2] \u2265 min Ex [((x query, \u03b2) + a + Yquery)\u00b2].\n\u03b1,\u03b2\nThis result reveals the limitation that the prediction made by a linear Transformer is at best comparable to solving for a linear regression objective, even if the target function is nonlinear. Therefore, the best prediction a linear Transformer can make is bounded by the performance of linear regression, and it cannot effectively learn a nonlinear target function in context. For a complete proof of this result, please see Appendix A."}, {"title": "3.2 Incorporating bilinear feed-forward layers for quadratic in-context learning", "content": "To overcome the limitation of LSA in handling nonlinear functions, drawing inspiration from modern Transformers, we study a mechanism that augments the linear Transformer architecture with bilinear feed-forward layers. In particular, we consider two layers in a bilinear Transformer and define a bilinear Transformer block BTFB as the composition of a bilinear feed-forward layer followed by a linear self-attention layer, i.e.,\nBTFB(Z) := attn\\circ bilin(Z).\nIn a 2L-layer bilinear Transformer TFbilin there are L bilinear Transformer blocks. We claim that for sufficiently large embedding dimension d and context length n, a single bilinear Transformer block can learn quadratic target functions in context.\nTheorem 2. Given the quadratic function in-context learning problem as described in Section 2.1 and embedding dimension d > \\binom{d+2}{2}, there exists a bilinear Transformer block BTFB such that\nL(BTFB) = O(1/n).\nThe construction that achieves this bound consists of two parts. First, The bilinear feed-forward layer computes quadratic features, expanding the input space to capture the quadratic terms in the target function. Let Z denote the output of the bilinear layer, and let \u017ei be the i-th column of Z (excluding the last row). Then, Zi contains every quadratic monomial involving the entries of xi. Specifically:\nXi = (1, xi[1], ..., xi[d], xi[1]2, ..., Xi [1]xi[d], xi[2]2, . . ., xi[2]xi[d], ..., xi[d]\u00b2).\nXi\n(5)\nThen, the label yi is linear in Zi, and the LSA layer used for linear ICL tasks can be reused here.\nThe bilinear layer achieves the quadratic feature expansion in (5) by taking the element-wise product of two linear projections of its input, so that entries of its output are quadratic in its input. To illustrate how this works, we consider an example where d = 2 and d = 5. The original columns of the prompt have the form (1,xi[1], xi[2], 0, 0, yi). Let\nW\u2081=\n0 0 0 0 0\n0 0 0 0 0\n0 0 0 0 0\n0 0 0 0 0\n0 0 0 0 0\n0 1 0 0 0\n0 1 0 0 0\n,W2 = \n0 0 0 0 0\n0 0 0 0 0\n0 0 0 0 0\n0 0 0 0 0\n0 0 0 0 0\n0 1 0 0 0\n0 0 1 0 0\n\nThen, under definition (3), the i-th column of the output from this bilinear layer has the form (1, xi[1], Xi [2], Xi [1]2, xi[1]xi[2], Yi). Here, W\u2081 and W2 are 0-1 valued matrices that mark the"}, {"title": "4\nNonlinear in-context learning requires larger scales", "content": "While our result in Section 3.2 presents a compelling approach for solving ICL for quadratic functions, it faces several difficulties. First, the construction outlined in Theorem 2 involves a two-layer Transformer with an excessively wide embedding with dimension on the order of \u0472(d2). In Section 4.1, we show that a Transformer block with a narrower design provably cannot learn quadratic functions in context. To address this, we extend our previous ideas and introduce a deep bilinear Transformer, which only requires an embedding dimension linear in d. This construction achieves its goal by performing block-coordinate descent on a quadratic kernel regression. Notably, this approach can be easily generalized to higher-degree polynomial function classes, which we will discuss further in Section 5.2.\nAdditionally, while Theorem 2 indicates that the in-context learning loss decreases as the prompt length n increases, it does not specify the exact values of n needed for effective ICL of quadratic functions. In Section 4.3, we discuss why learning nonlinear functions in-context may require longer prompts compared to linear ones. Specifically, we show that a Transformer's ICL performance is sensitive to the input data distribution, and it can degrade when applied to non-Gaussian distributions. We note that the kernel function computed by the Transformer can introduce non-Gaussian data, and our analysis sheds light on the inherent challenges of learning nonlinear function classes."}, {"title": "4.1 Lower bound on the embedding dimension for quadratic in-context learn-ing", "content": "In Section 3.2, we showed that a bilinear Transformer block with embedding dimension on the order of (d2) can effectively learn quadratic target functions in-context. In the following result, we investigate whether a bilinear Transformer block with a smaller embedding dimension can"}, {"title": "4.2 Deep bilinear Transformers as block-coordinate descent solvers", "content": "An important takeaway of Theorem 3 is that to effectively learn a quadratic target function in context, there is no more efficient way than explicitly write out every possible monomial of the quadratic function (or another set of generators). To achieve this within a single Transformer block, the embedding dimension would need to scale with \u0398(d\u00b2) to accommodate the full kernel. However, this results in a prohibitively wide Transformer network. In this section, we address this limitation by considering a deeper Transformer network. In particular, we show how a deep bilinear Transformer can simulate block-coordinate descent, performing updates on specific subsets of quadratic terms across its layers.\nRecall from Section 3.2 that a single Transformer block corresponds to one step of preconditioned gradient descent with respect to a quadratic kernel. In particular, for the weights w\u2208 R(d+2), we define the quadratic function\nf(x;w) = [1 x] smat(w) \\begin{bmatrix} 1\\\\ x \\end{bmatrix}\nThen, given the prompt's examples (x1,y1),...,(Xn, Yn), the Transformer's prediction corresponds to a gradient descent step with some preconditioner \u0393 so that\n\\hat{y} = f(x_{\\text{query}}; w^+), w^+ = \\Gamma \\nabla_w[L(w)]_{w=0}, L(w) = \\frac{1}{2\\eta} \\sum_{i=1}^{n} (f(x_i; W) + Y_i)^2.\nThis equivalence requires the bilinear layer to output all quadratic monomials. If the embedding dimension is insufficient to fit all of them, we instead try constructing bilinear layers that output"}, {"title": "4.3 Impact of non-Gaussian data on context length of nonlinear in-contextlearning", "content": "In Sections 3.2 and 4.2, we introduced a setting where the incoming data distribution to each attention layer is not Gaussian, but rather a product of two Gaussian random variables. This departure from the Gaussian assumption (as seen in previous works like Ahn et al. [2024a]) has important implications for the difficulty of in-context learning (ICL), particularly when learning quadratic target functions.\nTo illustrate this, we consider a prompt Z where d = d (without the first row of 1's), and the"}, {"title": "5\nNumerical experiments", "content": ""}, {"title": "5.1\nIllustration of block-coordinate descent dynamics", "content": "To illustrate the block-coordinate descent update dynamics that we proposed in Section 4.2, we conduct a numerical experiment with a 12-layer bilinear Transformer, which, as previously shown, can perform up to 6 steps of block-coordinate descent updates. We set the parameters to n = 200, d = 4, and d = 12, resulting in \\binom{d+2}{2} = 15 distinct monomials for the quadratic function. The prompts are drawn according to a quadratic target function as described in Section 2.1. This setup implies that the embedding space is insufficient to compute the entire quadratic kernel with just one bilinear feed-forward layer.\nIn Figure 2, we analyze the in-context learning (ICL) performance of various sub-networks within the bilinear Transformer, where the first 2l layers correspond to l steps of block-coordinate descent. We plot the ICL loss of the bilinear Transformer over 20000 training iterations. We observe that the ICL loss decreases significantly as the number of steps increases, which aligns with the behavior of a gradient-descent-based estimator. Additionally, we note that the ICL prediction after 2 steps is notably poor, indicating that the blocks selected by the trained Transformer have not yet covered all quadratic monomials after passing through two bilinear feed-forward layers. This outcome is expected given that our choice of d < \\binom{d+2}{2} necessitates multiple steps to compute the full quadratic kernel."}, {"title": "5.2\nLearning higher-degree polynomials in-context", "content": "The deep bilinear Transformer construction we discussed in Section 4.2 can also be used to generate higher-degree polynomial kernels. After passing through the first bilinear feed-forward layer, the intermediate prompt contains entries that are quadratic in the original prompt. The subsequent bilinear layer can then multiply these quadratic terms, yielding terms up to degree 4.\nTo illustrate this idea, we consider an example with d = 2,d = 6. Suppose that, after passing through the first bilinear feed-forward layer, the columns of the prompt take the form (1, xi[1], Xi [2], xi[1]xi[2], 0, 0, Yi). Let\nW\u2081 = \n0 0 0 0 0 0\n0 0 0 0 0 0\n0 0 0 0 0 0\n0 0 0 0 0 0\n0 0 0 0 0 0\n0 1 0 0 0 0\n0 0 0 1 0 0\n,W2 = \n0 0 0 0 0 0\n0 0 0 0 0 0\n0 0 0 0 0 0\n0 0 0 0 0 0\n0 0 0 0 0 0\n0 1 0 0 0 0\n0 0 0 1 0 0\nThen, according to definition (3), the i-th column of the output from this bilinear layer takes the form (1,xi[1], xi [2], xi[1]xi[2], xi[1]\u00b2xi[2], xi[1]\u00b2xi[2]2, yi), which includes both cubic and quartic"}, {"title": "6\nConclusion and future work", "content": "In this paper, we investigated the expressive power of Transformers to learn nonlinear target functions in-context. We first showed that previous works on linear Transformer architectures are insufficient for in-context learning (ICL) of nonlinear function classes. To overcome this challenge, we considered a deep bilinear Transformer, which alternates between self-attention and feed-forward layers similar to recent Transformer designs in practice. Using quadratic function class as an example, we showed that a bilinear Transformer can perform nonlinear ICL by mimicking gradient-descent estimators on a kernel regression. Moreover, we discussed how our approach can be generalized to polynomial function classes. Finally, we highlighted challenges"}, {"title": "1.1 Related work", "content": "Motivated by the successes of the Transformer architecture, there has been tremendous progress in understanding its algorithmic power. Independent of the studies on ICL, many works sought to characterize the expressivity of Transformers from an approximation-theoretic perspective. These studies established that Transformers can effectively approximate a wide range of functions and algorithms (e.g., Bai et al. [2024], Edelman et al. [2022], Giannou et al. [2023], Olsson et al. [2022], P\u00e9rez et al. [2021], Wei et al. [2022a]).\nIn the context of ICL, numerous empirical studies have examined the Transformer's capabilities to learn a variety of tasks and settings in context, including linear regression [Garg et al., 2022], discrete boolean functions [Bhattamishra et al., 2024], representation learning [Guo et al., 2024], and reinforcement learning [Lin et al., 2024]. To explain these observations, Aky\u00fcrek et al. [2023], Dai et al. [2023], Li et al. [2023] proposed that Transformers can learn in context by implementing learning algorithms. In particular, for linear regression, Von Oswald et al. [2023] proposed that Transformers implement gradient descent with zero initialization and provided a simple construction for a single-layer linear attention model. Subsequently, Ahn et al. [2024a], Mahankali et al. [2024], Wu et al. [2024], Zhang et al. [2024a] showed that the optimal single-layer linear attention with respect to some suitably chosen in-context learning loss closely corresponds to the construction proposed by Von Oswald et al. [2023].\nOur paper is closely related to this body of research and we draw several inspirations from it. We highlight some of these ideas and how our work differs as follows:\nSection 5 of Mahankali et al. [2024] showed that even for nonlinear target functions, the optimal one-layer LSA minimizing the in-context learning loss still corresponds to solving a linear least-square problem. Our result in Section 3.1 extends this conclusion to an arbitrary number of LSA layers. Additionally, we propose a solution to this limitation by incorporating feed-forward layers."}]}