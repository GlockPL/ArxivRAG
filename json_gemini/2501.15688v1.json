{"title": "Transformer-Based Multimodal Knowledge Graph Completion with Link-Aware Contexts", "authors": ["Haodi Ma", "Dzmitry Kasinets", "Daisy Zhe Wang"], "abstract": "Multimodal knowledge graph completion (MMKGC) aims to predict missing links in multimodal knowledge graphs (MMKGs) by leveraging information from various modalities alongside structural data. Existing MMKGC approaches primarily extend traditional knowledge graph embedding (KGE) models, which often require creating an embedding for every entity. This results in large model sizes and inefficiencies in integrating multimodal information, particularly for real-world graphs. Meanwhile, Transformer-based models have demonstrated competitive performance in knowledge graph completion (KGC). However, their focus on single-modal knowledge limits their capacity to utilize cross-modal information. Recently, Large vision-language models (VLMs) have shown potential in cross-modal tasks but are constrained by the high cost of training. In this work, we propose a novel approach that integrates Transformer-based KGE models with cross-modal context generated by pre-trained VLMs, thereby extending their applicability to MMKGC. Specifically, we employ a pre-trained VLM to transform relevant visual information from entities and their neighbors into textual sequences. We then frame KGC as a sequence-to-sequence task, fine-tuning the model with the generated cross-modal context. This simple yet effective method significantly reduces model size compared to traditional KGE approaches while achieving competitive performance across multiple large-scale datasets with minimal hyperparameter tuning.", "sections": [{"title": "I. INTRODUCTION", "content": "Knowledge graphs (KGs) represent real-world information as structured triples (h,r,t), where h and t are entities and r is a relation connecting them. This representation has proven valuable in various fields, including recommendation systems, question-answering, and drug discovery. KGs are widely used for organizing large-scale data and as auxiliary reasoning sources in systems like retrieval-augmented generation (RAG) [15]. While KGs traditionally store structural and textual information, their extension to incorporate multimodal data, such as images and videos, has led to the development of multimodal knowledge graphs (MMKGs). MMKGs bridge structured knowledge with unstructured content, associating entities with visual data to better ground their properties. Recent advances in natural language processing [19] and multimedia [8] have further highlighted the potential of MMKGs. However, MMKGs face two critical challenges: (1) Incompleteness, where missing links hinder downstream tasks, and (2) Cross-modal integration, where effectively connecting information across modalities remains non-trivial due to noise and inconsistencies in unstructured data.\nTo address these challenges, multimodal knowledge graph completion (MMKGC) approaches aim to predict missing links in MMKGs by learning low-dimensional embeddings for entities and relations from visual, textual, and structural information. These methods use the embeddings to score candidate triples and identify missing links. Early works like IKRL [24] and TransAE [23] attempted to blend visual content into traditional knowledge graph embedding (KGE) models. IKRL integrated image features into KGE models by learning visual embeddings for entities, while TransAE introduced an auto-encoder module for multimodal fusion. More recent works, such as LAFA [17] and Mixture of Modality Knowledge Experts [26], have improved multimodal fusion and reasoning in MMKGC. LAFA focuses on integrating multimodal data by considering link-aware fusion and neighborhood aggregation, demonstrating the potential of entity-specific multimodal reasoning.\nDespite these advances, existing MMKGC methods have several limitations:\n\u2022 Scalability of Embeddings: Traditional KGE and multi-modal KGE models (e.g., TransE [1], LAFA [17]) require a unique embedding for each entity and relation, leading to linear scaling in both model size and inference time as the graph grows.\n\u2022 Uniform Treatment of Modalities: Many MMKGC models uniformly incorporate all associated images for an entity, ignoring their varying relevance to different links. While recent works like RSME [23] and MKGformer [2] consider image relevance, they focus on individual entities rather than the specific links between them.\n\u2022 Insufficient Use of Structural Context: Many existing MMKGC approaches focus on individual triples, neglecting the rich structural information provided by an entity's neighbors. LAFA [17] partially addresses this issue with neighborhood aggregation but relies on KGE-based embeddings, which lack the flexibility and scalability as stated.\nTransformer-based models, such as KGT5 [16], have emerged as promising alternatives for knowledge graph completion. These models frame link prediction as a sequence-to-sequence (seq2seq) task, where the model size is indepen-"}, {"title": "II. RELATED WORK", "content": "A. Knowledge Graph Completion\nKnowledge graph completion (KGC) aims to predict miss-ing links in knowledge graphs by learning representations of entities and relations. Early approaches include embedding-based models like TransE [1], which uses translation-based embeddings, and RotatE [18], which models relations as rota-tions in a complex space. ComplEx [21], as a representative of bilinear models, introduces a diagonal matrix with complex numbers to capture anti-symmetry. DistMult [27] is a bilinear embedding model for knowledge graph completion, scores triples using a dot product, while ComplEx [21] extends this by embedding entities and relations in a complex vector space to handle asymmetric and antisymmetric relations. These methods have successfully handled structural information but lack mechanisms to incorporate multimodal data.\nRecent advancements have explored the use of Transformers for KGC. KG-BERT [28] frames KGC as a binary classifica-tion task, using Transformer models to predict the plausibility of triples. KGT5 [16] extends this idea by treating KGC as a sequence-to-sequence (seq2seq) task, enabling the use of tex-tual descriptions for entities and relations. These approaches demonstrate improved scalability and flexibility compared to traditional KGE models, as the model size is independent of the number of entities in the graph. Transformers also allow for straightforward integration of additional information, such as textual descriptions or external knowledge, without requiring complex model modifications.\nAdditionally, recent work [26] demonstrates the potential of large language models (LLMs) such as Llama-2, ChatGPT, and ChatGLM2-6B in generating diverse perspectives for KGC tasks. These works use LLMs to generate supplementary con-textual information for reasoning and completion, showcasing their utility even though they were not originally designed for KGC.\nB. Multimodal Knowledge Graph Completion and Reasoning\nMultimodal knowledge graph completion (MMKGC) ex-tends KGC by incorporating visual, textual, and structural information. Early models like IKRL [24] and TransAE [23] integrate image features into traditional KGE frameworks. IKRL learns visual embeddings for entities, while TransAE uses an auto-encoder to fuse multimodal features. These methods highlight the potential of visual data in KGC but face challenges in scaling embeddings and integrating multimodal information effectively.\nOther works [3], [11] discuss challenges such as modality-specific relevance and structural integration, emphasizing the importance of adaptive approaches for modality-specific rea-soning. To tackle such challenges, MoMoK [30] employs a mixture of modality knowledge experts to handle the diverse nature of multimodal data, allowing the model to adaptively select relevant modalities for each prediction. RSME [22] introduces a relevance scoring mechanism for multimodal data, prioritizing more relevant visual information when generating entity representations. MMKGR [33], on the other hand, explores multi-hop reasoning over MMKGs, employing a gate-attention mechanism for feature integration and reinforcement learning for complex reasoning tasks. LAFA [17] introduces link-aware fusion and aggregation mechanisms to enhance en-tity representations by considering the relevance of multimodal data to specific links. AIKGC [6] proposes to use a relation-"}, {"title": "III. PRELIMINARIES", "content": "A. Multimodal Knowledge Graphs (MMKGs)\nMultimodal knowledge graphs (MMKGs) extend traditional knowledge graphs by associating entities with multimodal information. Formally, an MMKG is defined as $G_{MM}=\\{(e_i, r_k, e_j, I_{e_i}, I_{e_j})\\}$, where $I_{e_i}$ and $I_{e_j}$ are the sets of mul-timodal data (e.g., images) associated with the head entity $e_i$ and tail entity $e_j$, respectively. The structural information in MMKGs is represented by triples $(e_i,r_k, e_j) \\subset E \\times R \\times E$, similar to traditional KGs, while the multimodal data $I_{e_i}$ and $I_{e_j}$ provide additional context for the entities.\nIn MMKGs, the adjacency tensor $X \\in \\{0,1\\}^{|E|\\times|R|\\times|E|}$ remains the same as in traditional KGs, representing the truth values of triples. The associated multimodal data enhances the semantics of the graph, allowing for richer reasoning over entities and relations.\nB. Multimodal Knowledge Graph Completion (MMKGC)\nThe goal of MMKGC is to predict missing links in MMKGs by leveraging both structural and multimodal information. Given a query $(e_i,r_k,?)$ or $(?, r_k, e_j)$, the objective is to find the most plausible tail entity $e_j$ or head entity $e_i$ to complete the triple, while also utilizing multimodal data $I_{e_i}$ and $I_{e_j}$ to enhance predictions.\nSimilar to KGC, MMKGC relies on a scoring function $f: E \\times R \\times E \\rightarrow R$ that assigns a plausibility score to each"}, {"title": "IV. METHODOLOGY", "content": "A. Overview of MMKG-T5\nIn this section, we provide an overview of our proposed model, MMKG-T5, illustrated in Fig. 1. Building upon the backbone of KGT5, MMKG-T5 incorporates enhancements to address the challenges of multimodal knowledge graph completion (MMKGC). To improve the understanding of the query relation, we introduce additional relation context. Furthermore, we leverage a pre-trained multimodal language model (VLM) to generate link-aware contextual descriptions for images associated with the query entity and its neighbors, ensuring the multimodal context aligns with the structural information in the knowledge graph.\nSection IV-B details the process of generating cross-modal context using pre-trained VLMs, focusing on relevant image selection and neighbor-guided contextualization. Section IV-C describes the use of relation context to enhance head entity prediction, combining multimodal and relational information to predict the tail entity effectively. Finally, Section IV-D explains how MMKGC is framed as a sequence-to-sequence task, utilizing KGT5 to integrate the multimodal and relation context.\nThese enhancements enable MMKG-T5 to address chal-lenges unique to MMKGs, such as integrating multimodal data and understanding complex relations while maintaining computational efficiency. The overall architecture and method-ology are described in detail below.\nB. Multimodal Entity Context\nMultimodal knowledge graphs provide a diverse collection of images for each entity $h_i$, capturing various aspects of the entity in different contexts and scenarios. These visual repre-sentations offer rich information that can complement struc-tural data. However, directly incorporating raw visual informa-tion into the model risks introducing noise or biases, as images often include extraneous or irrelevant details. To address this, we leverage a pretrained multimodal language model (MMLM) [32] to transform visual content into link-aware textual descriptions. The MMLM, fine-tuned on instruction-following tasks, is adept at analyzing and summarizing the content of images in a manner that aligns with the relational structure of the knowledge graph. This conversion enables the integration of visual data in a format that is interpretable and compatible with sequence-to-sequence models like KGT5.\n1) Relevant Image Selection: To ensure the generated tex-tual descriptions are relationally meaningful, we prioritize selecting images that are relevant to both the query entity h and its neighbors. Instead of processing all available images for h, which could introduce irrelevant information, we first identify"}, {"title": "C. Relation Context", "content": "While the multimodal context focuses on the head and tail entities, understanding the role played by the head (tail) entity in the query triple $(h, r,?)$ or $(?, r, t)$ is crucial for accurate prediction. Such understanding requires more than just the name of the query relation r, as relational semantics often depend on the context in which the relation is used. To enrich the model's comprehension of the query relation r, we extract m triples containing the same relation r from the training corpus.\nFor example, for a query \"Sweden | country_of | ?\", we extract triples such as \u201cPhilippines | country_of | Bohol\" and \"Turkmenistan | country_of | Galkyny\u015f_District\u201d. These"}, {"title": "D. MMKG-T5", "content": "Given a query (h,r,?) and its generated context $C_{h,r}$, MMKG-T5 treats link prediction as a sequence-to-sequence (seq2seq) task, following the approach introduced in KGT5 [16]. The architecture of MMKG-T5 is based on the encoder-decoder Transformer model T5 [4], which is well-suited for text-to-text transfer tasks. As multimodal knowledge graphs (MMKGs) provide textual information (mentions) for entities and relations, we leverage this data to verbalize the query into a text sequence of the form:\nquery: <head_entity_mention> | <relation_mention>.\n1) Context Fusion for Query Verbalization: In addition to the mentions provided in the MMKG, we enrich the verbalization of queries with two types of generated contexts:\n\u2022 Link-Aware Multimodal Context: Generated from the entity label and images of the one-hop neighborhood of the query entity, as described in Section IV-B.\n\u2022 Relation Context: Extracted examples of the query rela-tion from the training corpus, as detailed in Section IV-C.\nThe enriched query combines these elements, ensuring the model has access to both multimodal and relational cues. The general structure of the verbalized query is illustrated in Code 1. For multimodal neighbors, we sample up to k = 20 neighbors in a random fashion without replacement, following the neighborhood sampling strategy used in prior works [10]. Similarly, we include up to m = 5 triples for relation context to maintain a manageable input size while providing sufficient relational diversity.\n2) Training Procedure: To train MMKG-T5, we construct training queries based on triples (h, r, t) from the training set. Specifically, for each triple, we generate two queries: (h, r,?) for tail entity prediction and (?, r, t) for head entity prediction."}, {"title": "V. EXPERIMENTS", "content": "A. Experimental Setup\n1) Datasets: We evaluate our model in link prediction tasks on three commonly used MMKG benchmarks: FB15k-237 [20], MKG-W [25], and MKG-Y [25]. All datasets contain image and text modalities. The statistics on the datasets are summarized in Table I.\n2) Evaluation Protocol: We used the train/valid/test splits (from Table I) provided with the MMKG benchmarks. In evaluating the link prediction performance of our model, we utilized standard rank-based evaluation metrics like Mean Reciprocal Rank (MRR) [18] and Hits@K (K=1, 3, 10) [1] over test triples."}, {"title": "D. Performance Comparisons", "content": "In this section, we present the evaluation of MMKG-T5 (Setting 2) on three benchmark datasets: FB15k-237-IMG, MKG-W, and MKG-Y. We compare its performance against traditional KGC models (TransE, DistMult, ComplEx), multimodal KGC models (IKRL, RSME, TransAE), and its backbone model, KGT5. Table II summarizes the results.\nThe results demonstrate that while MMKG-T5 does not achieve state-of-the-art performance across all datasets, it consistently improves over KGT5, showcasing the benefits of incorporating multimodal and relational contexts. These improvements are particularly notable given MMKG-T5's use of a lightweight Transformer-based architecture, avoiding the heavy reliance on entity-specific embeddings employed by traditional KGC and multimodal models. Furthermore, the analysis reveals dataset-specific factors that influence MMKG-T5's performance and highlight areas for further refinement.\n1) FB15k-237-IMG: The FB15k-237-IMG dataset serves as the most multimodal-rich benchmark in our evaluation, with each entity having an average of 12 associated images, of which approximately 8 are utilized for multimodal context generation after filtering. MMKG-T5 achieves an MRR of 0.308 and Hits@10 of 0.488, significantly outperforming its backbone model, KGT5. This improvement underscores the utility of incorporating multimodal and relational contexts for knowledge graph completion.\nCompared to traditional KGC baselines like TransE and DistMult, MMKG-T5 leverages the multimodal information effectively, demonstrating its ability to capture cross-modal interactions. However, state-of-the-art multimodal models such as TransAE and RSME achieve comparable or slightly better results, suggesting that MMKG-T5 could benefit from more advanced multimodal fusion mechanisms.\nThe rich multimodal data in FB15k-237-IMG also facilitates the generation of high-quality link-aware contexts. In setting 2, as shown in Table IV, among the 14,541 entities, 10,865 entities have valid multimodal contexts, with an average of 8 images contributing to the context generation process. The diversity and quality of these images enhance MMKG-T5's ability to reason over multimodal content effectively, resulting in its relative strong performance on this dataset.\n2) MKG-W: On the MKG-W dataset, MMKG-T5 achieves an MRR of 0.270 and Hits@1 of 0.224, demonstrating im-provements over KGT5. However, its performance lags behind advanced multimodal models such as IKRL and RSME. This performance gap highlights areas where MMKG-T5 could be further optimized, particularly in handling datasets with sparser and lower-quality multimodal data.\nAs shown in Table IV, MKG-W presents unique challenges due to its limited multimodal content. Of the 15,000 entities, only 9,302 have associated images, and many of these images are duplicates with varying resolutions. After link-aware fil-tering, in setting 2, valid multimodal contexts are generated for only 4,739 entities, using an average of two images per entity. This scarcity of diverse and relevant multimodal data directly impacts the quality of context generation, thereby"}, {"title": "E. Ablation Study", "content": "To investigate the effectiveness of different multimodal context generation strategies, we conduct an ablation study comparing MMKG-T5 configurations (Setting 1 and Setting 3) with the default configuration (Setting 2) and its backbone model, KGT5. The evaluation is performed on two bench-marks: FB15k-237-IMG and MKG-W. Table III summarizes the results.\n1) Impact of Context Generation Strategies:\n\u2022 Setting 1 (Triple-Specific Context): Setting 1 generates multimodal contexts tightly linked to the specific triple (h,r,t), focusing on the most relevant images and de-scriptions for the queried link. This targeted approach results in improved performance over KGT5 (e.g., MRR increases from 0.247 to 0.283 on FB15k-237-IMG and from 0.248 to 0.282 on MKG-W). The strong alignment"}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this paper, we proposed MMKG-T5, a Transformer-based approach for multimodal knowledge graph comple-tion. By combining link-aware multimodal contexts, entity-centric descriptions, and relation-specific contexts, MMKG-T5 demonstrated competitive performance on diverse bench-marks, highlighting the potential of Transformer architectures for integrating multimodal and relational information. How-ever, challenges such as sparse or redundant multimodal data,"}]}