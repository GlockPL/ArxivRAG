{"title": "FUTGA: TOWARDS FINE-GRAINED MUSIC UNDERSTANDING THROUGH TEMPORALLY-ENHANCED GENERATIVE AUGMENTATION", "authors": ["Junda Wu", "Zachary Novack", "Amit Namburi", "Jiaheng Dai", "Hao-Wen Dong", "Zhouhang Xie", "Carol Chen", "Julian McAuley"], "abstract": "Existing music captioning methods are limited to generating concise global descriptions of short music clips, which fail to capture fine-grained musical characteristics and time-aware musical changes. To address these limitations, we propose FUTGA, a model equipped with fined-grained music understanding capabilities through learning from generative augmentation with temporal compositions. We leverage existing music caption datasets and large language models (LLMs) to synthesize fine-grained music captions with structural descriptions and time boundaries for full-length songs. Augmented by the proposed synthetic dataset, FUTGA is enabled to identify the music's temporal changes at key transition points and their musical functions, as well as generate detailed descriptions for each music segment. We further introduce a full-length music caption dataset generated by FUTGA, as the augmentation of the MusicCaps and the Song Describer datasets. We evaluate the automatically generated captions on several downstream tasks, including music generation and retrieval. The experiments demonstrate the quality of the generated captions and the better performance in various downstream tasks achieved by the proposed music captioning approach.", "sections": [{"title": "1. INTRODUCTION", "content": "Natural language music understanding, which extracts music information and generates detailed music captions, is a fundamental task within the MIR community, beneficial for a series of applications including music generation [1, 2, 3, 4], editing [5, 6], question-answering [7, 8], and retrieval [9, 10, 11]. Existing music understanding and captioning methods focus mainly on model architecture design and data augmentation. Traditional music understanding models are developed for specific purposes and with dedicated model design, including single-track music captioning [12, 13, 14], playlist title generation [15] and conversational music recommendation [16, 17]. Recent developments in music foundation models [18, 19, 20, 21] enable free-form music prompts and multitasking. These foundation models are developed based on pre-trained large language models (LLMs) and aligned with the music modality. Although LLM-powered music understanding models can leverage the abundant pre-trained music knowledge in caption generation, the success of modality alignment still requires a large amount of high-quality music caption data.\nMany previous works have tried to enhance models' music understanding capacity by expanding and improving music annotations on a broader range of music audio files [22, 13], or leveraging more powerful large language models to augment the existing music caption datasets with better language quality [14, 23] via auto-tagging. However, restricted by the current music captioning paradigm, available music caption datasets are limited to two major challenges: (1) Conventional music captions focus only on the global description of a (potentially long) music clip, which cannot efficiently capture a piece of music's fine-grained characteristics nor differentiate it from other music within-genre songs. (2) Key structural information, such as time boundaries of functional music segments and time-aware musical changes, is mostly neglected in traditional music understanding and hard to retrieve due to the limitation in the length of music clips.\nTo address the limitations, we propose FUTGA, a generative music understanding model trained with time-aware music caption data and calibrated with Music Information Retrieval (MIR) features. We first augment the MusicCaps dataset [24] by mixing music clips together into synthetic full-length songs. The corresponding music captions are composed with original short music captions as individual segment descriptions, which are also tagged with temporal segmentation information. To enable more realistic full-length music captioning, we further leverage a text-only LLM for the augmentation of the global music caption, musical changes between segments (e.g., increase of volume, slowing down the tempo, introducing new instruments, etc.), and functional tags of the segments (e.g., intro, verse, chorus, etc.), by paraphrasing and summarizing the template-based captions.\nInspired by existing Large Audio-Language Models (LALMs), we use the open-source SALMONN model [20] as the backbone and fine-tune the model with our developed synthetic full-length music caption data. Using our synthetic data augmentation, FUTGA is able to identify key transition points in musical changes and segment full-length songs according to their musical functions. For example, in Figure 1, we illustrate FUTGA's capacities as a novel form of music captioning. Given a song in full length, FUTGA can generate a global caption that summarizes the whole song's characteristics before identifying the music structure with time segments. Following the flow of music structures, FUTGA can further describe each music clip and musical changes between consecutive music clips. In addition, we also discover that the fine-tuned SALMONN model demonstrates a great instruction-following capacity to generate fine-grained music captions conditioned on given time boundaries and MIR features. Empowered by such model capacities, the model can further be trained with a small-scale MIR dataset Harmonixset [25], where music structure information and some MIR features are available. By injecting the ground-truth information into the instruction prompt, we can accurately guide the model to generate fine-grained music captions corresponding to the time segments. Finally, we ask human annotators to revise the generated captions and fine-tune the model again, which can additionally reduce the sim-to-real gap between synthetic and realistic data distributions.\nWith the final version of FUTGA, we propose automatically annotating the full-length songs in two existing datasets MusicCaps [24] and Song Describer [22]. We evaluate the effectiveness of our fine-grained and temporal-enhanced music caption in several downstream tasks including music captioning, music retrieval, and music generation. We summarize our main contributions as follows:\n\u2022 We propose a synthetic data augmentation method to construct fine-grained and temporally-structured music captions for full-length songs.\n\u2022 We fine-tune the existing large audio-language model with the synthetic dataset, and demonstrate the model's emerging ability in music segmentation and fine-grained music understanding.\n\u2022 Further aligned with human annotations and MIR features, the music understanding model FUTGA is used to automatically augment the MusicCaps and the Song Describer datasets for full-length music captions.\n\u2022 Through evaluation experiments, we demonstrate the proposed music captioning paradigm can benefit several downstream music understanding tasks."}, {"title": "2. TEMPORALLY-ENHANCED GENERATIVE AUGMENTATION", "content": "In this section, we introduce our proposed temporally-enhanced generative augmentation. Due to the limitation of existing music caption datasets, music captioning and understanding models can only generate global music descriptions for short music clips [22, 24, 14]. To address this limitation, we propose the augmentation of synthetic music and caption composition, which empowers music understanding models with capacities of time-aware music segmentation and fine-grained music description generation."}, {"title": "2.1. Synthetic Music Caption Augmentation", "content": "We construct our synthetic music caption augmentation from the existing MusicCaps dataset [24]. Due to the limitation of the length (10 seconds) of the music clips in MusicCaps, the annotated captions only capture the global characteristics of the music, mostly neglecting fine-grained music information, such as musical structures, temporal musical changes and time-boundaries for specific musical segments. To augment MusicCaps and enable fine-grained music understanding with temporally-enhanced music information, we propose a synthetic music composition method that composes multiple music clips into a synthetic piece of music with comparable lengths of complete songs. Given the existing N music clips \\(C = \\{c_i\\}_{i=1}^N\\) and their corresponding music captions \\(T = \\{t_i\\}_{i=1}^N\\), the synthetic music composition method aims to sample a subset of music clips \\(C_k \\subset C\\) and the corresponding subset of music captions \\(T_k \\subset T\\). However, for the music parts in a single song, randomly sampling music clips with heterogeneous musical characteristics may produce incoherent music, which further causes the out-of-distribution (OOD) problem of the constructed synthetic dataset.\nTo address such music incoherent problems in synthetic music samples and make the constructed music and song descriptions more realistic, we propose an important sampling method based on the music characteristic distribution. We extract the semantic embeddings of the original music captions with a pre-trained sentence embedding model f,\n\\[z_i = f(t_i), i = 1,2,..., N,\\]\nin which \\(z_i \\in \\mathbb{R}^{384}\\) and we adopt the MiniLM-L12 model [26] that allows the maximum of 256 tokens. With the extracted sentence embeddings, we can estimate the music description similarity by calculating the pairwise cosine similarity,\n\\[d(z_i, z_j) = \\frac{z_i^T z_j}{||z_i|| ||z_j||}\\]\nwhich can be used as the important scores for relevant music clip sampling. Then, for each music clip \\(c_k \\in C\\), we use it as the seed sample to explore n relevant music clips for synthetic composition,\n\\[C_k = \\{c_{k,1}, c_{k,2},..., c_{k,n}\\},\\]\n\\[C_{k,j} \\sim P(j) = \\frac{exp \\ d(z_k, z_j)}{\\Sigma_i exp \\ d(z_k, z_i)},\\]\nin which the number of music clips is uniformly sampled \\(n \\sim U(3,5)\\), which is consistent to most music structures. By sampling from the neighborhood of each music clip, we obtain a set of synthetic whole-songs \\(\\hat{C} = \\{C_k\\}_{k=1}^K\\), which can both cover the entire music representation space and approximate the representation distribution of realistic songs. We visualize the joint distribution of the original MusicCaps and the synthetic music representations in Figure 2. We observe that the distribution of synthetic samples is perfectly aligned with that of the original MusicCaps. In addition, by augmenting the original dataset, we can bootstrap synthetic samples that are more equally distributed in the feature space, which helps to smooth the learning process. To promote the time variance of different music segments in different songs, we propose to further modify the clip lengths \\(L_k = \\{l_j\\}_{j=1}^n\\) with the uniform distribution \\(l_j \\sim U(6, 10)\\), which can efficiently reduce the model's bias in music segmentation and promote the model to discover the accurate transition points between music segments."}, {"title": "2.2. Temporally-enhanced Music Understanding", "content": "Corresponding to synthetic music augmentation, we also augment the music captions with temporally-enhanced music understanding. For each sampled set of music clips \\(C_k\\), the corresponding music caption set \\(T_k\\) and the clip length information \\(L_k\\) are interleaved and composed by the template,\n\\[\\hat{X}_k = \\frac{\\sum_{j=1}^n (L_{k,j} - L_{k,j-1}, t_{k,j})}{\\sum_{i=1}^n l_{k,i}},\\]\nin which the specific original time-boundaries \\(L_k\\) are transformed into relative time-boundaries, which are always between 0 \u2013 100% (\\(l_{k,0} = 0\\)). We use a relative time-boundary representation approach to minimize training bias towards specific numbers of music lengths in our model. In addition, relative time-boundary representation enhances the model's ability to comprehend music of varying lengths, thereby improving the model's generalizability. By generating these relative time boundaries, our generative music understanding model gains a better awareness of the music's overall progression, which further enhances the model's temporal understanding of music.\nWe further propose to use a text-only large language model (LLM) to augment the template-based caption \\(\\hat{X}_k\\) with natural language descriptions, in which additional information, such as global captions, musical changes, and music structures, can be automatically extracted from the LLM. Since LLMs are pre-trained with abundant domain knowledge including music analysis [20, 18] and music information retrieval [20, 19], with enough context provided, such LLMs can accurately extract music information via language-based summarization and reasoning [14]. Inspired by LLMs' capacities in language reasoning, we propose to paraphrase and augment additional music information with instructions as follows:\nContext: Music Analysis {\\(X_k\\)}. This is a music analysis of a song. Note that the numbers indicate the time-boundaries of functional segments in this song."}, {"title": "3. ALIGNMENT FROM MIR FEATURES AND HUMAN FEEDBACK", "content": "In this section, we design quality control methods for the generative music understanding model. Although the model trained with the synthetic music and captions is capable of generating fine-grained music captions with time-boundary information, the generation quality on realistic whole-songs remains to be evaluated, especially when a sim-to-real domain gap exists. To further validate the generation quality of our proposed model and align the model generation distribution with realistic music samples, we propose to collect a small portion of human annotated ground-truth music captions based on the Harmonixset dataset [25].\nIn the Harmonixset dataset, there are 912 songs annotated with music structures and MIR features (e.g., genre and BPM). We obtain the pseudo-labeled music captions by incorporating human annotations into the prompts:"}, {"title": "4. DATASET CREATION: FUTGA", "content": "Based on the final version of our proposed music captioning model, we automatically generate music captions for whole songs between 2 minutes and 5 minutes in MusicCaps [24] and Song Describer [22]. During inference time, we set the repetition penalty as 1.5 to prevent repetitive descriptions of the same music segments. In addition, we also set the beam search number to 10 to find the statistically best captions. We allow a maximum of 2048 tokens to be generated from FUTGA.\nAs demonstrated in the comparison example in Table 1, FUTGA provides more fine-grained music understanding descriptions with time boundaries indicating music segments, for which the average segment number and the number of musical changes are reported in Table 2. In addition, we can observe relatively longer global captions with more details, which is also verified by the data statistics in Table 2.\nIn terms of music caption diversity, we first show that our captions have significantly larger numbers of tokens and vocabulary size, compared to existing music caption datasets. Second, our dataset still maintains good diversity in terms of unique genre, instrument, and music mood vocabularies, which are comparable to human or GPT-3.5 annotations. Thus, the FUTGA dataset can serve to augment existing music captioning models with strong temporal reasoning abilities without harming the model's generalizability, which will be further evaluated in our evaluation section."}, {"title": "5. AUTOMATIC MUSIC EVALUATION", "content": "We first evaluate the generated data samples' quality by comparing them to existing human annotation datasets, MusicCaps [24] and Song Describer [22]. We follow the previous works [14, 22] and report the metrics, BLEU (B), METEOR (M), ROUGE (R), and BERT-score (B-S), in Table 3. Since our captions are formally different from original music captions, we report the evaluation metrics for the global and the complete captions in our dataset separately. For a fair comparison, we adopt the zero-shot performance of LP-MusicCaps in [14], since our model is only trained on the synthetic dataset and Harmonixset.\nBased on the results in Table 3 on MusicCaps, we observe that the global captions generated from our model consistently show higher quality than the zero-shot results of LP-MusicCaps, which demonstrates that by capturing more details from longer songs, we can obtain more accurate descriptions of the music. In addition, comparing FUTGA and LP-MusicCaps on Song Describer, which is the out-of-domain dataset for both methods, FUTGA shows a significantly larger improvement in the generation results, which demonstrates the model's better capacities in generalizability.\nHowever, the complete music captions generated from FUTGA show relatively inferior performance on MusicCaps, which is mainly due to the different forms of music captions. Since FUTGA focuses on the temporal reasoning of a whole song, the time segment information and musical changes are completely new to both the original MusicCaps captions. Whereas, LP-MusicCaps is directly augmented from MusicCaps, which makes their captions formally more similar. Such observations can motivate future works to explore more fine-grained and complex music caption forms in terms of evaluating the model's generation capacities."}, {"title": "5.2. Music Retrieval", "content": "With segmented music descriptions enabled by FUTGA, we propose a many-to-many music retrieval method by matching multiple music segment descriptions with multiple audio clips. Specifically, we first calculate the intersection-over-union (IoU) score of each music caption's time segment and each audio clip's time segment, which is regarded as the pairwise temporal correlation weight between 0 and 1. Then, we adopt the contrastive multimodal representation model CLAP [29] to extract the text and audio features for each caption and audio segment. Finally, we calculate the IoU weighted average cosine similarity between all the music captions and audio clips. Intuitively, the more temporally aligned caption-audio pairs are gaining more importance scores, whereas non-overlapped pairs will be regarded as irrelevant.\nIn Table 4, we follow the previous works [14, 22] and report the Recall@K scores, where K = 1,5,10, as well as median rank (MedR). On Song Describer, we observe a similar generalization problem across the domain of the LP-MusicCpas baseline, which shows significantly lower performance compared with the original human annotations. Whereas, our method achieves better retrieval results than the human annotation baseline, which demonstrates the effectiveness of such a many-to-many retrieval method enabled by our fine-grained music descriptions.\nOn MusicCaps, we observe a consistent performance drop for both LP-MusicCaps and FUTGA, compared with human-annotated captions. Since LP-MusicCaps and our methods are augmented by LLMs that have a relatively much more complex language variance compared to CLAP text encoders, we argue that the complexity of language introduced by LLMs may be out-of-distribution of CLAP, which further deteriorates the retrieval performance."}, {"title": "5.3. Music Generation", "content": "As most modern text-music datasets include global captions for long audio segments, text-to-music (TTM) generation systems face a \"many-to-one\" problem [2, 30, 4], where overall text relevance may suffer due to learning correlations between global text prompts and weakly related audio segments. Thus, In order to validate FUTGA's ability to provide high-quality time-located audio captions, we propose a music generation task using the open source music generation model MusicLDM [2]. Specifically, we measure the CLAP score [29], which measures audio-text similarity, between MusicLDM generations using both the original and FUTGA-generated captions from SongDescriber [22]. Additionally, to assess whether FUTGA can be used as a method to improve TTM training through improving caption data, we finetune MusicLDM on the FUTGA-generated captions for MusicCaps (which we denote MusicLDM-F), using the standard e-prediction loss [31] with a learning rate of le-6 for 2000 iterations (see [2] for more details).\nWe show results in Tab. 5. We find that without finetuning, MusicLDM's text relevance suffers when provided with more local FUTGA captions compared to global ones. By finetuning on time-aware FUTGA captions however, MusicLDM-F is able to noticeably improve its ability to follow fine-grained captions while sacrificing little ability to handle global controls."}, {"title": "6. CONCLUSION", "content": "In this work, we propose a temporally-enhanced music caption augmentation method through generative large language models. By bootstrapping existing music captions with time boundary tags, MIR features, and musical changes, we fine-tune the pre-trained music understanding model SALMONN-7B, where we observe emerging music segmentation capacities and enable instruction prompting to guide the generation with ground-truth time segments. To further reduce the sim-to-real gap and align the model with realistic music characteristics, we collect human annotations and fine-tune the model again. We use the fine-tuned model to re-annotate the existing MusicCaps and Song Describer datasets with full-length songs. The generated captions are shown to be more fine-grained and beneficial for various downstream tasks.\nFor future works, since our model is the first to enable end-to-end full-length song captioning with significantly longer context provided (10 times more than conventional music captions), we are motivated to further develop a long-context-based CLAP model, which can enable more complex and longer music retrieval tasks. In addition, with more fine-grained details provided by our captions, we propose to further use such captions for more complex music understanding tasks, including music question-answering and whole-song generation."}]}