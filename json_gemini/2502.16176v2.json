{"title": "An End-to-End Homomorphically Encrypted Neural Network", "authors": ["Marcos Florencio", "Luiz Alencar", "Bianca Lima"], "abstract": "Every commercially available, state-of-the-art neural network consume plain input data, which is a well-known privacy concern. We propose a new architecture based on homomorphic encryption, which allows the neural network to operate on encrypted data. We show that Homomorphic Neural Networks (HNN) can achieve full privacy and security while maintaining levels of accuracy comparable to plain neural networks. We also introduce a new layer, the Differentiable Soft-Argmax, which allows the calibration of output logits in the encrypted domain, raising the entropy of the activation parameters, thus improving the security of the model, while keeping the overall noise below the acceptable noise budget. Experiments were conducted using the Stanford Sentiment Treebank (SST-2) corpora on the DistilBERT base uncased finetuned SST-2 English sentiment analysis model, and the results show that the HNN model can achieve up to 82.5% of the accuracy of the plain model while maintaining full privacy and security.", "sections": [{"title": "1 Introduction", "content": "At the same time that neural networks are considered \"black-boxes,\" due to their inherent internal complexity, the need to relinquish privacy to use state-of-the-art neural networks is a well-known issue, especially for companies that deal with sensitive data [5, 24]. In these models, the input of plain text is converted into a vector of features that the model uses to perform inference. If the input could be encrypted, the model could be used without the need to share the plain data with the model owner, providing full privacy and security.\nWe adapt the encryption scheme proposed by [6], which is designed to perform computation on real numbers, to encrypt input data, plus some acceptable level of noise, and then calibrate the model's weights in such a way that the forward pass of the neural network can be performed in the encrypted domain. In the same fashion, the outputs of the model are calibrated so that only the owner of the private key can decrypt the output. The main challenge of this approach is the noise that is introduced in the encrypted domain, which can be mitigated by using a noise budget, which is a parameter that limits the maximum amount of noise. While the base scheme requires specific Add and Mult algorithms to perform operations, they are performed internally in the context of neural networks and the noise budget is controlled by the model's architecture and the training process during backpropagation.\nIt is also commom practice in neural network architectures that the output of the model is a vector of raw logits, upon which the softmax function is applied to calculate the loss function and to perform inference[13, 20, 28, 26]. It is our understanding that a privacy-preserving neural network should maintain the output logits in the encrypted domain, ideally introducing new noise. To address this issue, we propose a new layer, the Differentiable Soft-Argmax, which is a differentiable version of the argmax function. It calibrates the logits in the encrypted domain, raising the entropy of the activation parameters, thus improving the security of the model while keeping the overall noise below the acceptable noise budget."}, {"title": "2 Background", "content": "Homomorphically encrypted neural networks is a relatively new field of research, with the first significant work being published in 2016 [11]. It demonstrated how a network can execute over encrypted data and introduced optimizations to make inference feasible. The encryption scheme chosen was YASHE', described in [2], which does not support floating-point numbers and allows operations on encrypted data if, and only if, the complexity of the arithmetic circuit is known in advance.\nBoth of these limitations were addressed by [6], which introduced a new encryption scheme that allows operations on encrypted data without the need of knowing the complexity of the arithmetic circuit in advance. This scheme is known as HEANN or CKKS, and it is the one we use in this work. Another fundamental difference is the optizations proposed by [11], such as replacing activation functions by polynomi-"}, {"title": "3 Homomorphic encryption", "content": "The objective of using homomorphic encryption $\\varepsilon$ is that, given an algorithm Evaluate and a valid public key pk, a computation can be performed on any circuit C (i.e., a collection of logic gates used to compute a function) and any cyphertexts $\\psi_i \\leftarrow$ Encrypt (pk, $\\pi_i$), such that the output of the computation is an encryption of $\\psi \\leftarrow$ Evaluate$\\varepsilon$ (pk, C, $\\psi_1$, ..., $\\psi_t$) and that Decrypt(sk, $\\psi$) = C($\\pi_1$, ..., $\\pi_t$) for a valid secret key sk, as first described by [22]. The first viable construction of a fully homomorphic encryption scheme was proposed by [9] and may be broken down into three major conceptual steps:\n\u2022 a \"somewhat homomorphic\" scheme, supporting evaluation of low-degree polynomials on encrypted data;\n\u2022 a \"squashing\" decryption mechanism, responsible for expressing outputs as low-degree polynomials supported by the scheme, and;\n\u2022 a \"bootstrapping\" transformation, a self-referential property that makes the depth of the decryption circuit shallower than what the scheme can handle.\nGentry's main insight was to use bootstrapping in order to obtain a scheme that could evaluate polynomials of high-enough degrees while keeping the decryption procedure expressed as polynomials of low-enough degrees, so that the degrees of the polynomials evaluated by the scheme could surpass the degrees of the polynomials decrypted by the scheme [10]. This scheme used \"ideal lattices\", corresponding to ideals in polynomial rings, to perform homomorphic operations. The reason for this is that ideal lattices inherit natural addition and multiplication properties from the ring, which allows for homomorphic operations to be performed on encrypted data. We defer the"}, {"title": "4 Neural network architecture", "content": "Neural network interpretability is a long known and mostly unsolved problem in the field of artificial intelligence [29, 14], sometimes being referred to as black boxes [8]. Therefore, input/output privacy-preserving neural networks allow the use of models on sensitive data without compromising the privacy of the data.\nIn general, neural networks can be considered a regression function that outputs data based on elaborate relationships between high-dimensional input data. As observed by [18], privacy-preserving neural networks tend to suffer from high compu-"}, {"title": "4.1 Rings, Ideals and Lattices", "content": "The requirement of using ideal lattices comes from the necessity of using encryption schemes whose decryption algorithms have low circuit complexity, generally represented by matrix-vector multiplication, with the important caveat that \"code-based constructions would also represent an interesting possibility\" [9].\nA ring is a set R closed under two binary operations + and \u00d7 and with an additive identity 0 and a multiplicative identity 1. An ideal I of a ring R is a subset I \u2286 R such that$\\iota_1 + \\iota_j \\in I$ and $\\sum_{j=1}^t \\iota_j \\times r_j \\in I$ for $\\iota_1, ..., \\iota_t \\in I$ and $r_1,...,r_t \\in R$.\nAs proposed by Gentry, the public key pk contains an ideal I and a plaintext space P, consisting of cosets of the ideal I in the ring R, while the secret key sk corresponds to a short ideal I in R. To encrypt $\\pi \\in P$, the encrypter performs $\\psi \\leftarrow \\pi + I$, where I represents the noise parameter. The decrypter then performs $\\pi \\leftarrow \\psi \u2013 I$ to decrypt the ciphertext $\\psi$. To perform add and multiply operations on the encrypted data, ring homomorphisms are used on the plaintext space P:\nAdd(pk, $\\psi_1$, $\\psi_2$) = $\\psi_1 + \\psi_2 \\in (\\pi_1 + \\pi_2) + I$,\nMult(pk, $\\psi_1$, $\\psi_2$) = $\\psi_1 \\times \\psi_2 \\in (\\pi_1 \\times \\pi_2) + I$.\nThese ciphertexts are essentially \u201cnoisy\u201d lattice vectors, or ring elements, and decrypting them requires knowledge of the basis for a particular lattice [1]. An ideal lattice is formed by embedding a ring ideal into a real n-dimensional coordinate space $R^n$ used to represent the elements of an ideal as vectors [17]. Because R is discrete and has finite rank n, the images of its elements under the embedding form a lattice. Ideal lattices allow ciphertext operations to be performed efficiently using polynomial arithmetic, e.g., Fast Fourier Transform-based multiplication [21].\nConsider plaintext messages $m_1$ and $m_2$ as in Figure 2. After encryption using the public key pk, these messages become vectors $\\psi_1$ and $\\psi_2$, each containing the underlying plaintexts, the ideals $I_1$ and $I_2$ used to obtain the cyphertexts, and some error $e_1$ and $e_2$ stemming from the basis used to derive the secret key sk. A bit-wise homomorphic operation Mult(pk, $\\psi_1$, $\\psi_2$) can then be performed on the cyphertexts to obtain the cyphertext $\\psi_1 \\times \\psi_2$, which contains the product of the plaintexts $m_1 \\times m_2$, the ideals $I_1$ and $I_2$, and the errors $e_1$ and $e_2$."}, {"title": "4.2 Modified HEANN", "content": "HEANN supports approximate addition and multiplication of encrypted messages by truncating a ciphertext into a smaller modulus, which leads to rounding of plaintext and also adds noise to the message. The main purpose of the noise is for security reasons and, given the nature of the scheme, it ends up being reduced during computation due to rescaling. As the authors explain (p. 6), \"the most important feature of our scheme is the rounding operation of plaintexts.\" The output of this scheme, therefore, is an approximate value with a predetermined precision.\nAs per the scheme definition, given the set of complex numbers\nH = {($z_j$)$_{j\\in Z_M}$ : $z_{-j} = \\overline{z_j}$, $\\forall j \\in Z_M$} $\\subset C^{(M)}$,\nand the subgroup T of the multiplicative group $Z_M^*$ satisfying $Z_M/T = {\\pm 1}$, the input of the scheme is a plaintext polynomial constituted by elements of a cyclotomic ring\nR = $Z_t[X]/(\\Phi_M (X))$,\nwhich is mapped to a vector of complex numbers via an embedding map represented by a ring homomorphism. This transformation enables the preservation of the precision of the plaintext after encoding. The decoding procedure is almost the inverse of the encoding, except for the addition of a round-off algorithm for discretization. An important characteristic of HEANN that makes it suitable for neural networks is that the bit size of ciphertext modulus does not grow exponentially in relation to the circuit being evaluated, which allows for the evaluation of deep neural networks without the need for bootstrapping.\nIn our version, based on Gentry's caveat, the inputs to the neural network are encrypted into the polynomial\n$\\psi$ = Enc(m, v),\nwhere $m_i \\in PT$ (mapping to embedding inputs derived from the underlying model during backpropagation), and $\\psi$ is a vector of length K, corresponding to the context window of the model being used as the basis for the interchangeable compute unit."}, {"title": "4.3 Differentiable Soft Argmax Layer", "content": "The softmax function was introduced by [4] as a way to convert raw network outputs (logits) into probabilities, becoming a foundational technique in neural networks. When applied to an n-dimensional vector, the softmax rescales its elements so that the output is a normalized probability distribution. Given an input vector x = ($x_1$,...,$x_n$) $\\in$ $R^n$, it is formally defined as $\\sigma$ : $R^n \\rightarrow (0,1)^n$, where n > 1 and\n$\\sigma(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}$\nThese logits can be calibrated before being ingested by the softmax function by adjusting a hyperparameter defined as temperature [12], effectively raising the entropy of the activation parameters without altering the model's accuracy. This hyperparameter applies a single scalar T > 1 to the logit vector x, yielding $\\sigma_i$ ($x_i/T$). As T $\\rightarrow \\infty$, the probability approaches 1/n, which represents maximum uncertainty. In general, this is a post-processing technique that is applied to the logits after the neural network has been trained.\nBy using a differentiable dedicated layer, we can approximate the temperature using backpropagation and apply the softmax function to the logits in the encrypted domain, after which we can derive the argmax from the resulting probabilities. In order to obtain the final output, we compute the weighted sum of the calibrated logits with respect to a reshaped and broadcasted index vector i that ranges from 1 to n:\n$\\sum_{i=1}^n \\sigma(x_i)i$.\nThus, this layer approximates the argmax function in the encrypted domain, allowing the neural network to perform prediction on encrypted data without any decryption. Because the layer is differentiable, the network can be trained using backpropagation, and it adds only negligible computational overhead to the model."}, {"title": "5 Training", "content": "In this section, we outline the overall training methodology adopted for our privacy-preserving neural network, highlighting the design choices that ensure encrypted data can be processed accurately without revealing sensitive information. Our approach comprises two main training phases: (i) first the base neural network is trained to learn how to operate with encrypted inputs, and (ii) training the differentiable soft argmax layer."}, {"title": "5.1 Training Data and Batching", "content": "For the purpose of this paper, we use the DistilBERT finetuned for sentiment analysis [23] with a shared vocabulary of approximately 30,000 tokens, ensuring consistent text input processing across different sequences. We train our model on a real-world text classification dataset derived from the Stanford Sentiment Treebank (SST-2) [25], containing roughly 67,000 sentences labeled with binary sentiment classes. Before batching, each sentence is encrypted and then grouped according to similar sequence lengths so that each mini-batch contains 32 sentences with a similar number of ciphered tokens. This approach ensures a smooth integration of homomorphic operations while maintaining efficient GPU utilization during the forward and backward passes."}, {"title": "5.2 Hardware and Scheduling", "content": "All experiments were conducted using NVIDIA A100 GPUs allowing for efficient parallelization of the batched tensor operations. We first trained our base neural network for 200 epochs using both an AdamW optimizer and automatic mixed precision (AMP). Each epoch involves a forward and backward pass over all training samples, grouped into mini-batches of 32 encrypted sequences. With both the optimizer and AMP activated, a single step (i.e., processing one batch) typically took about 0.06 seconds on average, culminating in roughly 2 minutes per epoch over 2,105 steps. This schedule allowed the network to converge in approximately 6.5 hours of training time for the full 200 epochs."}, {"title": "5.3 Optimizer and Mixed Precision", "content": "We adopt AdamW with a learning rate of 5 \u00d7 10-6 for its balanced convergence properties and the ability to handle noise from homomorphic encryption. In tandem, Automatic Mixed Precision (AMP) is used to reduce computational overhead. Table 5.3 compares average step times under four configurations, highlighting how each component influences runtime:\n\u2022 Adding AdamW introduces a modest increase in step time (e.g., 0.28 \u2192 0.29 s without AMP, and 0.05 \u2192 0.06 s with AMP).\n\u2022 Enabling AMP drastically shortens average step times (from \u2248 0.28 s to \u2248 0.05 s per step in the absence of AdamW), underscoring the efficiency gains from half-precision arithmetic."}, {"title": "5.4 Training the Differentiable Soft Argmax Layer", "content": "To train this layer effectively, we freeze all previous parameters in order to preserve the already learned representations and optimize only the temperature via a specialized loss function. In many classification tasks, we find that binary cross-entropy or a closely related objective improves the probability estimates at the final stage, ensuring that the calibrated logit space aligns with the desired confidence measure. Because the temperature is a single (or low-dimensional) parameter, this secondary optimization converges swiftly."}, {"title": "6 Performance Metrics", "content": "We evaluated our homomorphically encrypted neural network on the SST-2 validation set [25] and compared its performance to a publicly reported DistilBERT baseline on the same benchmark. Table 6 lists the core metrics for both our encrypted model and DistilBERT. While DistilBERT achieves near-perfect performance, our encrypted approach necessarily operates under more stringent constraints. Encrypted data introduce both computational overhead and additional noise, factors that can modestly reduce performance relative to fully unencrypted pipelines."}, {"title": "7 Experiments", "content": "In this chapter, we present a series of controlled experiments designed to evaluate the impact of key hyperparameters on our homomorphically encrypted neural network. Starting from a baseline configuration (Variation (A)), we systematically adjust pa-"}, {"title": "8 Conclusion", "content": "In this work, we presented a functioning homomorphically encrypted neural network that can operate on encrypted data while maintaining high levels of accuracy. We also introduced a new layer, the Differentiable Soft Argmax, which allows the calibration of output logits in the encrypted domain, raising the entropy of the activation parameters and improving the security of the model. In his seminal thesis, Gentry highlights that constructing an efficient homomorphic encryption scheme without using bootstrapping, or using some relaxations of it, is an interesting open problem (p.9). At least in the context of neural networks, that is what we believe to have accomplished.\nFor sentiment analysis tasks, our experiments show that the HNN model can achieve up to 82.5% of the accuracy of the plain model while maintaining full privacy and security. We believe that this level of accuracy, while significant, can be improved by further refining the training pipelines and methodologies.\nWe are excited about the potential applications of this technology and look forward to further exploring its capabilities. We plan to continue our research by investigating how mechanistic interpretability can be achieved in the context of homomorphically encrypted neural networks and how safe multi-party computation can be integrated into our models and pipelines."}]}