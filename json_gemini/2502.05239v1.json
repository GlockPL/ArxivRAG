{"title": "Enhancing Knowledge Graph Construction: Evaluating with Emphasis on Hallucination, Omission, and Graph Similarity Metrics", "authors": ["Hussam Ghanem", "Christophe Cruz"], "abstract": "Recent advancements in large language models have demonstrated significant potential in the automated construction of knowledge graphs from unstructured text. This paper builds upon our previous work [16], which evaluated various models using metrics like precision, recall, F1 score, triple matching, and graph matching, and introduces a refined approach to address the critical issues of hallucination and omission. We propose an enhanced evaluation framework incorporating BERTScore for graph similarity, setting a practical threshold of 95% for graph matching. Our experiments focus on the Mistral model, comparing its original and fine-tuned versions in zero-shot and few-shot settings. We further extend our experiments using examples from the KELM-sub training dataset, illustrating that the fine-tuned model significantly improves knowledge graph construction accuracy while reducing the exact hallucination and omission. However, our findings also reveal that the fine-tuned models perform worse in generalization tasks on the KELM-sub dataset. This study underscores the importance of comprehensive evaluation metrics in advancing the state-of-the-art in knowledge graph construction from textual data.", "sections": [{"title": "Introduction", "content": "Knowledge Graphs (KGs) play a crucial role in organizing complex information across diverse domains, such as question answering, recommendations, semantic search, etc. However, the ongoing challenge persists in constructing them, particularly as the primary sources of knowledge are embedded in unstructured textual data such as press articles, emails, and scientific journals. This challenge can be addressed by adopting an information extraction approach, sometimes implemented as a pipeline. It involves taking textual inputs, processing them using Natural Language Processing (NLP) techniques, and leveraging the acquired knowledge to construct or enhance the KG.\nIn-context learning, as discussed by [7], coupled with prompt design, involves telling a model to execute a new task by presenting it with only a few demon-"}, {"title": "Background", "content": "The current state of research on knowledge graph construction using LLMs is discussed. Three main approaches are identified: Zero-Shot, Few-Shot, and Fine-Tuning. Each approach has its own challenges, such as maintaining accuracy without specific training data or ensuring the robustness of models in diverse real-world scenarios. Evaluation metrics used to assess the quality of constructed KGs are also discussed, including semantic consistency and linguistic coherence. This section highlight methods and metrics to construct KGs and evaluate the result."}, {"title": "Zero Shot", "content": "Zero Shot methods enable KG construction without task-specific training data, leveraging the inherent capabilities of LLMs. [19] introduce an innovative approach using LLMs for knowledge graph construction, employing iterative zero-shot prompting for scalable and flexible KG construction. [20] evaluate the performance of LLMs, specifically GPT-4 and ChatGPT, in KG construction and reasoning tasks, introducing the Virtual Knowledge Extraction task and the VINE dataset, but they do not take into account open sourced LLMs as LLaMA [12]. [24] address the limitations of existing generative knowledge graph construction methods by leveraging large generative language models trained on structured data. The most of these approaches having the same limitation, which is the use of closed and huge LLMs as ChatGPT or GPT4 for this task. Challenges in this area include maintaining accuracy without specific training data and addressing nuanced relationships between entities in untrained domains."}, {"title": "Few Shot", "content": "Few Shot methods focus on constructing KGs with limited training examples, aiming to achieve accurate knowledge representation with minimal data. [6] introduce PiVe, a framework enhancing the graph-based generative capabilities of LLMs, and the authors create a verifier which is responsable to verifie the results of LLMs with multi-iteration type. [29] investigate LLMs' application in relation labeling for e-commerce Knowledge Graphs (KGs). As ZSP approaches, FSP approaches use closed and huge LLMs as ChatGPT or GPT4 [10] for this task. Challenges in this area include achieving high accuracy with minimal training data and ensuring the robustness of models in diverse real-world scenarios."}, {"title": "Fine-Tuning", "content": "Fine-Tuning methods involve adapting pre-trained language models to specific knowledge domains, enhancing their capabilities for constructing KGs tailored to particular contexts. [4] present a case study automating KG construction for compliance using BERT-based models. This study emphasizes the importance"}, {"title": "Evaluation metrics", "content": "As we employ LLMs to construct KGs, and given that LLMs function as Natural Language Generation (NLG) models, it becomes imperative to discuss NLG criteria. In NLG, two criteria [32] are used to assess the quality of the produced answers (triples in our context).\nThe first criterion is semantic consistency or Semantic Fidelity, which includes:\nHallucination: Presence of information (facts) in the generated text that is absent in the input data.\nOmission: Omission of information present in the input data from the generated text.\nRedundancy: Repetition of information in the generated text (not considered in our evaluation).\nAccuracy: Exact match between the input and generated text without modification.\nOrdering: Sequence of information in the generated text differing from the input data (not considered in our evaluation).\nThe second criterion is linguistic coherence or Output Fluency, which evaluates the fluidity and linguistic correctness of the generated text. This criterion is not considered in our evaluation.\nIn their experiments, [3] calculated three hallucination metrics - subject hallucination, relation hallucination, and object hallucination - using preprocessing steps like stemming. They used the ground truth ontology and test sentence to determine if an entity or relation is present, considering any disparity between them as hallucination.\nThe authors of [6] evaluated their experiments using several metrics, including Triple Match F1 (T-F1), Graph Match F1 (G-F1), G-BERTScore (G-BS) from [33], and Graph Edit Distance (GED) from [35]. The GED metric measures the distance between the predicted and ground-truth graphs by calculating the number of edit operations needed to transform one into the other. To adhere to the semantic consistency criterion, we use the terms \"omission\" and \"hallucination\" instead of \"addition\" and \"deletion,\" respectively."}, {"title": "Propositions", "content": "This section outlines our approach to evaluate the quality of generated KGs using metrics like T-F1, G-F1, G-BS, and GED. We also discuss the use of Op-"}, {"title": "Overall experimentation's process", "content": "In our previous work, we leveraged the WebNLG+2020 and KELM-sub datasets, specifically the version curated by [6]. Their preparation of graphs in lists of triples proves beneficial for evaluation purposes. We utilize these lists and employ NetworkX [39] to transform them back into graphs, facilitating evaluations on the resultant graphs. This step is instrumental in performing ZSP, FSP, and FT LLMs on these datasets. In this work, we will use examples from the training dataset of KELM-sub to do few-shot learning on the original and the finetuned (from our previous work) Mistral model.\nFig. 2 illustrates the different stages of our experimentation process, including data preparation, model selection, training, validation, and evaluation. The process begins with data preparation, where the WEBNLG dataset is preprocessed and split into training, validation, and test sets. Next, the learning type is selected, and different models are trained using the training set. The trained models are then evaluated on the validation set to evaluate their performance. Finally, the best-performing model is selected and validated on the test set to estimate its generalization ability."}, {"title": "Prompting learning", "content": "In this phase, we use ZSP and FSP techniques on LLMs to evaluate their proficiency in extracting triples for KG construction. We merge examples from the KELM-sub test dataset with our adapted prompt, strategically modified for contextual guidance without a support ontology description, as demonstrated by [3]. The prompts for ZSP and FSP are shown in Fig. 3(a) and Fig. 3(b).\nFor ZSP, we started with the method from [6], using the directive \"Transform the text into a semantic graph\" and enhanced it with additional sentences for our LLMS (Fig. 3(a)). For FSP, we used 6-shot learning, corresponding to the maximum KG size in KELM-sub, feeding the prompt with six examples of varying sizes (Fig. 3(b))."}, {"title": "Postprocessing", "content": "To evaluate the generated KGs against ground-truth KGs, we clean the LLM outputs by transforming generated graphs into organized lists of triples and transferring them to textual documents. This rule-based processing removes corrupted text outside the lists of triples, optimizing our evaluation process for metrics like G-F1, GED, and OEP (Section 3.4).\nIn our previous work, instructing LLMs to produce lists of triples sometimes resulted in unstructured text, which we addressed by substituting the generated text with an empty list of triples ('[[\"\",\"\",\"\"]]'). This approach, however, underestimated hallucinations. In the current work, as illustrated in Fig. 1, we calculate the exact hallucination and omission for each generated graph through qualitative evaluation of two randomly generated graphs."}, {"title": "Experiment's evaluation", "content": "To evaluate the generated graphs against ground-truth graphs, we use metrics such as T-F1, G-F1, G-BS [33], and GED [35] as in [6]. We also use Optimal Edit Paths (OEP) to calculate omissions and hallucinations in the generated graphs. Our evaluation follows [6]'s methodology, especially in computing GED and G-F1, and involves constructing directed graphs from lists of triples using NetworkX [39]. Unlike [3], we do not use the ground truth test sentence of an ontology. Instead, we assess omissions and hallucinations using OEP, which provides the precise path of the edit, allowing exact quantification of these errors.\nFor example, Fig. 1 shows 2 omissions ('b)') and 1 hallucination 'a)' in using one of two paths \"OEP for nodes\" or OEP for edges\". Previously, we incremented the global hallucination metric for all graphs if > 1 hallucinations or omissions were found. In the current work, we use OEP to detect the exact percentage of hallucination or omission in a generated graph, experimenting on 2 random examples from the WebNLG+2020 test dataset (Fig. 1).\nDifferent from our previous work, our experiments are evaluated using examples from the KELM-sub test dataset (Table 2 and Table 1). Our primary goal is to improve G-F1, T-F1, G-BS and GM-GBS metrics, while reducing GED, hallucination, and omission."}, {"title": "Mathematical representation of the used metrics", "content": "This study refines the metrics used for evaluating hallucinations and omissions in generated graphs and introduces a new metric, Graph Matching using Graph BERTScore (GM-GBS). In our previous work, we detailed the mathematical representation of all metrics used."}, {"title": null, "content": "The G-BS metric evaluates graph matching by treating edges as sentences and using BERTScore to measure alignment between predicted and ground-truth edges. The F1 score for G-BS is calculated as follows:\n$R_{BERT} = \\frac{1}{|E|} \\sum_{x_i \\in E} max_{x_j \\in E^T} x_j$,\n$P_{BERT} = \\frac{1}{|E^T|} \\sum_{x_j \\in E^T} max_{x_i \\in E} x_j$,\n$F1_{BERT} = \\frac{2 \\cdot P_{BERT} \\cdot R_{BERT}}{P_{BERT} + R_{BERT}}$\nWhere $R_{BERT}$ is the recall, and $P_{BERT}$ is the precision.\nIn this work, we use G-BS to compare generated graphs with ground-truth graphs, defining graph matching with a similarity threshold of 95% to introduce GM-GBS. This approach acknowledges that entities or relations in the generated graph may be synonymous with those in the ground truth graph. Results shown in Fig.4 illustrate that even with 95% BERTScore similarity, the generated graph is nearly identical to the ground truth."}, {"title": null, "content": "To calculate GM-GBS, we follow these steps: Given an array of F1 scores of G-BS $f_1, f_2,..., f_n$ in $f1s_{BS}$, the fraction of F1 scores greater than 0.95 is calculated as follows:\n1. Let $ToGrs$ be the total number of generated graphs.\n2. Let $f_m$ be the count of F1 scores that are greater than 0.95:\n$f_m = \\sum_{i=1}^N 1(f_i > 0.95)$"}, {"title": null, "content": "where $1(.)$ is the indicator function, which is 1 if the condition inside is true and 0 otherwise.\n3. The fraction of F1 scores greater than 0.95 is given by: GM-GBS = $\\frac{f_m}{ToGrs}$\nFor hallucinations and omissions, we use Optimal Edit Paths (OEP) to determine exact counts:\nHallucination: An edit operation is a hallucination if it adds an entity or relation not present in the gold graph. We previously used an overall hallucination metric $Hall. = \\frac{hall}{ToGrs}$, where hall is the number of graphs with hallucinations.\nOmission: An edit operation is an omission if it deletes an entity or relation present in the gold graph. In the previous work the omission was computed by $Omis. = \\frac{omiss}{ToGrs}$, where omiss is the number of graphs with omissions.\nIn this work, we calculate exact percentages of hallucination and omission through qualitative evaluation.\nGiven a list of tuples $lst = [(g_1, p_1), (g_2, p_2), ..., (g_n, p_n)]$, where $g_i$ represents a gold edge and $p_i$ represents a predicted edge:\n1. Let h be the number of hallucinations, where a hallucination is defined as $g_i$ = None:\n$h = \\sum_{i=1}^n 1(g_i = None)$\n2. The exact hallucination rate is then calculated as: $Hall\\_Rate = \\frac{h}{n}$\nWhere n is the total number of edges, and $1(\\cdot)$ is the indicator function, which is 1 if the condition inside is true and 0 otherwise (Same for Omis_rate).\nTo calculate the exact omission rate:\n1. Let o be the number of omissions, where an omission is defined as $p_i$ = None:\n$o = \\sum_{i=1}^n 1(p_i = None)$\n2. The exact omission rate is then calculated as: $Omis\\_Rate = \\frac{o}{n}$"}, {"title": "Experiments", "content": "This section outlines the LLMs used in our experiments for ZSP and FSP and presents the experimental results."}, {"title": "Conclusion and perspectives", "content": "In this study, we evaluated the performance of both the original and fine-tuned Mistral models for Text-to-Knowledge Graph (T2KG) construction tasks using Zero-Shot Prompting (ZSP) and Few-Shot Prompting (FSP). Our analysis incorporated a comprehensive set of metrics, including G-F1, T-F1, G-BS, GED, along with measures for hallucinations and omissions.\nOur results demonstrate that the fine-tuned Mistral model generally outperforms the original Mistral, particularly in Few-Shot scenarios. The fine-tuned Mistral with seven shots achieved superior performance across most metrics, notably improving G-F1 and T-F1 scores, which indicates a higher fidelity in generating ground truth graphs, and reflects its improved ability to produce coherent and contextually relevant outputs.\nDespite these improvements, we observed that fine-tuning on domain-specific data, such as WebNLG, can negatively impact the model's generalization capabilities. This was evident from the comparative performance of the fine-tuned models on the KELM-sub dataset, where the original Mistral model with 7 shots from WebNLG+2020 outperformed the fine-tuned variants. This finding highlights the importance of balancing domain-specific fine-tuning with maintaining broad generalization.\nThe inclusion of the GM-GBS metric provided valuable insights into the semantic similarity between generated and ground truth graphs. Our qualitative analysis of hallucinations and omissions further enhanced our understanding of model performance at the triple level.\nLooking ahead, there are several promising avenues for further research. Refining evaluation metrics to account for synonyms of entities or relations in generated graphs could improve assessment accuracy. Additionally, leveraging LLMs for data augmentation in T2KG construction shows potential, as our experiments suggest that LLMs can maintain consistency in generating results and propose relevant triples.\nExpanding evaluations to a broader range of domains and datasets can provide deeper insights into how various types of data influence model behavior and performance. Combining automated metrics with human evaluation could also offer a richer understanding of model quality, with domain experts providing valuable assessments of the relevance and accuracy of generated graphs. Exploring these directions will contribute to advancing the field of T2KG construction and enhancing the capabilities of language models in producing accurate and contextually appropriate knowledge graphs."}]}