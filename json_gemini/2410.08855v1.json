{"title": "MATCH: Model-Aware TVM-based Compilation for Heterogeneous Edge Devices", "authors": ["Mohamed Amine Hamdi", "Francesco Daghero", "Giuseppe Maria Sarda", "Josse Van Delm", "Arne Symons", "Luca Benini", "Marian Verhelst", "Daniele Jahier Pagliari", "Alessio Burrello"], "abstract": "Streamlining the deployment of Deep Neural Networks (DNNs) on heterogeneous edge platforms, coupling within the same micro-controller unit (MCU) instruction processors and hardware accelerators for tensor computations, is becoming one of the crucial challenges of the TinyML field. The best-performing DNN compilation toolchains are usually deeply customized for a single MCU family, and porting to a different heterogeneous MCU family implies labor-intensive re-development of almost the entire compiler. On the opposite side, retargetable toolchains, such as TVM, fail to exploit the capabilities of custom accelerators, resulting in the generation of general but unoptimized code. To overcome this duality, we introduce MATCH, a novel TVM-based DNN deployment framework designed for easy agile retargeting across different MCU processors and accelerators, thanks to a customizable model-based hardware abstraction. We show that a general and retargetable mapping framework enhanced with hardware cost models can compete with and even outperform custom toolchains on diverse targets while only needing the definition of an abstract hardware model and a SoC-specific API. We tested MATCH on two state-of-the-art heterogeneous MCUs, GAP9 and DIANA. On the four DNN models of the MLPerf Tiny suite MATCH reduces inference latency by up to 60.88\u00d7 on DIANA, compared to using the plain TVM, thanks to the exploitation of the on-board HW accelerator. Compared to HTVM, a fully customized toolchain for DIANA, we still reduce the latency by 16.94%. On GAP9, using the same benchmarks, we improve the latency by 2.15\u00d7 compared to the dedicated DORY compiler, thanks to our heterogeneous DNN mapping approach that synergically exploits the DNN accelerator and the eight-cores cluster available on board.", "sections": [{"title": "I. INTRODUCTION", "content": "The increasing focus on efficiently executing Deep Neural Networks (DNNs) on low-power MCUs has led to the proliferation of domain-specific Systems-On-Chip (SoCs) [1]\u2013[5]. Three crucial features characterize these architectures: first, being designed for the specific task of accelerating DNNs at the extreme edge, they are usually OS-less, in order to minimize both the memory requirements for the software stack and the associated performance overheads. Second, they are usually heterogeneous platforms composed of general-purpose cores in charge of simple control tasks (e.g., receiving input data and sending DNN outputs) and a set of accelerators optimized for DNN workloads. As a last point, they commonly avoid HW caches in favor of SW scratchpad memories to reduce the required silicon area, exploiting the regularity of memory accesses in DNN workloads. Therefore, fully exploiting the hardware resources of these platforms when executing a DNN is a non-trivial task, which requires in-depth knowledge of the SoC and its memory hierarchy.\nAutomated deployment toolchains address these challenges by generating target-dependent code from high-level DNN representations (e.g., Python, ONNX, etc), abstracting hardware details from users [6], [7]. However, existing toolchains for OS-less edge devices are usually either overly generic or overly specific. Generic toolchains, such as the popular TVM [6] or the MLIR-based TinyIREE [8], have extensive support for diverse DNN operators, but only generate code for common CPU and GPU targets, given that they do not have access to in-depth specifics of the target hardware and, therefore, are not able to exploit SoC-specific features such as HW accelerators, ISA extensions, memory hierarchy dimensions, etc. Autotuning engines [9] can partially address this issue by iteratively generating different versions of the code, profiling it, and using search strategies such as reinforcement learning or genetic algorithms to converge to an optimized implementation. On the other hand, they result in very long compile times for large search spaces [10] and usually require the availability of hardware-in-the-loop for profiling.\nOn the other side of the spectrum, target-specific toolchains provide top performance on specific SoCs, fully exploiting the target's accelerators and memory hierarchy. However, their support for DNN operators is limited, and adding a new one requires an in-depth knowledge of both the HW target and the full toolchain. Moreover, they are commonly monolithic software stacks that embed hardcoded hardware-dependent heuristics for memory management, operation scheduling, and tiling [7]. Consequently, adding support for a new DNN accelerator or a new SoC requires a labor-intensive overhaul of the toolchain or, in the worst case, the creation of an almost entirely new toolchain, leading to large delivery delays of the final product.\nA seminal work that combines these two worlds is HTVM [10], which extends TVM with a customizable plug-in for hardware accelerator support [7]. The extension is responsible for the generation of optimized code for operations that can be accelerated on the target SoC's accelerator, while TVM can be used to deploy non-accelerable kernels on the main CPU. However, HTVM still relies on a monolithic tool (DORY), in which hardware-specific cost information is deeply intertwined with the code base, thus suffering from the above-mentioned limitations in terms of extensibility to new targets.\nIn this paper, we describe a step in the direction of solv-"}, {"title": "II. BACKGROUND", "content": "The optimized execution of DNNs at the edge is a complex task that can benefit from both software and hardware op-timizations. On the software side, different solutions, such as neural architecture search, pruning, or quantization, have been proposed to reduce DNNs' complexity while preserving accuracy [13]\u2013[18]. On the hardware side, many solutions have been proposed for accelerating DNN workloads, resulting in a plethora of complex heterogeneous SoCs [1], [2], [4], [19]. To fill the gap between high-level DNN models and such complex hardware targets, AI compilation frameworks [6]\u2013[10], [20]\u2013[24] are used to generate target-specific code that efficiently exploits the available hardware resources.\nOur effort focuses on defining a new AI compiler, MATCH, that not only fulfills its scope of porting DNNs architecture on heterogeneous edge devices but also provides a lightweight in-terface for compiler engineers that simultaneously allows them to achieve three seemingly contrasting goals: i) deployment efficiency, i.e., close-to-optimal performance, by carefully or-chestrating the available HW resources; ii) broad support for existing and upcoming DNN operators; iii) easy extensibility to future hardware targets. In the rest of this section, we first give an overview of existing heterogeneous hardware platforms for DNN execution at the edge and then provide the required background on AI compiler technology."}, {"title": "A. Edge Platforms for DNN Inference", "content": "The execution of DNNs in typical general-purpose OS-less edge devices, such as Microcontrollers (MCUs), is not an easy task due to the requirements these devices need to deliver for an acceptable quality of service, e.g. performance, within a limited power envelope. On the other hand, DNN inference is a highly parallel workload, prevalently composed of operations such as convolutions and General Matrix Mul-tiplications (GEMMs), opening the possibility of aggressive hardware optimization. In this direction, modern SoCs for edge DNN inference are becoming increasingly heterogeneous; they usually include one or more DNN accelerators, usually built as an array of multiply-and-accumulate (MAC) units to paral-lelize tensor processing workloads (i.e. convolutions, matrix multiplication, etc), coupled with general-purpose cores to execute less compute-intensive layers and a multi-level. Cores and accelerators typically exchange data through a multi-level, software-managed memory hierarchy to minimize DNNs tensors access time.\nSeveral examples of heterogeneous MCUs are available both in academia [4], [25], [26] and industry [1], [2], [19], with embedded accelerators that vary from multi-cores Single Instruction Multiple Data (SIMD) / Single Instruction Multiple Threads (SIMT) units to systolic arrays. One notable academic example is the Parallel Ultra-Low Power (PULP) family of devices [27], which is built around a main control core that dispatches digital signal processing (DSP) tasks to a cluster of 8 identical RISC-V cores with specific ISA extensions such as SIMD MACs and memory operations with pointer post-increment. A more aggressively heterogeneous MCU, DIANA, introduced in [4], couples a RISC-V core used for IO-control"}, {"title": "B. AI compilers", "content": "The innovations brought by researchers and vendors on both new HW platforms and DNN models are often bottlenecked by the lack of a sufficiently flexible yet performant middleware layer that allows the execution of new models on newly devel-oped HW. Recently, many works have focused on this layer of the stack, usually referred to as the AI compiler. In the context of an OS-less heterogeneous SoC, an AI Compiler consists of a toolchain to translate HW-unaware DNN formats, such as ONNX or PyTorch models, into HW-specific compiled code for their optimized execution into CPUs, GPUs, and dedicated accelerators. These tools are particularly important in edge computing, where HW architectures are highly heterogeneous with tight memory constraints. Fig.1 depicts the main AI compiler steps, detailed below."}, {"title": "1) Network Transformations:", "content": "the compilation of AI models typically starts from a computational graph representing the workload, where nodes represent operations over tensors, abstracted as edges. Network transformations can be cate-gorized into HW-aware and HW-unaware ones. HW-unaware passes include optimizations such as operator fusion. On the other hand, HW-aware transformations consider the target hardware's characteristics to optimize the computational graph further, enhancing performance and efficiency, for instance, by replacing operations with equivalent ones that are better supported by the hardware, or adapting data layouts and quantization formats."}, {"title": "2) Pattern Matching and HW Dispatching:", "content": "after applying transformations to the network's computational graph, the AI compiler offloads the execution of each node of the optimized graph to a specific hardware module. This step maps high-level operations in the computational graph to low-level hardware-specific operations or kernels. Pattern matching is applied on the graph to identify nodes that are supported by each HW module, to fully leverage SoC-specific capabilities, such as specialized instructions or acceleration units."}, {"title": "3) Memory Scheduling:", "content": "the last graph-level pass is usu-ally memory scheduling, whose goal is minimizing the peak memory occupation for activations based on tensor lifetimes. Effective memory scheduling ensures that data is efficiently allocated and deallocated, reducing memory overhead and making the model execution feasible on devices with limited volatile memory space."}, {"title": "4) Operator-Specific Optimizations:", "content": "Finally, operator-specific optimizations are applied to each subgraph matched in step 2, to enhance the performance of individual operations onto the hardware module they have been assigned to. One crucial optimization technique is loop tiling, which partitions activation and weights tensors into smaller, more manageable blocks or tiles. This enables more efficient data access patterns, particularly by maximizing data reuse within faster memory levels. Ideally, combining memory tiling with loop ordering can enable reaching a performance similar to the one obtained with a theoretically infinite last-level cache. We will call the specific combination of loop tiling and ordering for a layer the layer schedule.\nAfter applying all steps, the final code is generated; here, either a template-based approach [7], [9], [10], [21], [22] or a lowering code-generation [6], [8], [23] pipeline can be used. Depending on the strategy, the kernel's code (i.e., the innermost convolution/GEMM operation, after the application of fusion, tiling, loop ordering, etc.) is imported from HW-specific hand-tuned libraries with optimized performance [7] or produced together with the full layer code through auto-tuning pipelines with HW-in-the-loop [9]."}, {"title": "III. RELATED WORKS", "content": "Table I summarizes the main state-of-the-art AI compilers from academia and industry chronologically. We categorize them into three groups based on the following criteria: i) the ease of extending them to a new HW target, ii) the ease of adding a new DNN operator, and iii) the performance achieved."}, {"title": "IV. MATCH", "content": "To address the dualism between flexible-yet-inefficient and optimized-yet-inflexible toolchains, we introduce MATCH, a novel framework that enhances TVM with HW-aware deploy-ment capabilities using the BYOC framework. Starting from a Python-level DNN model, MATCH generates optimized HW-specific C code to deploy the DNN on OS-less heteroge-neous devices. MATCH exploits TVM-specific features, i.e., the BYOC interface and the Patter Matcher, to be able to simultaneously deploy optimized operators with external user codebases, while falling back to a default unoptimized solution for un-matched operators. MATCH is written in Python >3.8. Fig. 2 shows a high-level overview of the deployment flow using MATCH, which is composed of four main stages. In the rest of this section, we describe the Framework Frontend and Network Transformations in Sec. IV-A, the Pattern Matching and HW-aware Dispatching in Sec IV-B, and the Code Gen-eration in Sec IV-C. In Sec. V, we further detail how to add a new HW target to MATCH, with two specific examples: DIANA and GAP9.\nMATCH currently targets Convolutional Neural Networks (CNNs), since these models are very popular in extreme-edge applications. Its extension to transformers is planned as future work as these newer ML models are just starting to be targeted for tinyML scenarios [31]. This, and other extensions, will be facilitated by the modularity of our approach. Hereinafter, we use the following notation for the hyperparameters of a 2D Convolutional layer: IX/IY/C for the horizontal, vertical, and channel dimensions of the input tensor; OX/OY/K for the re-spective output dimensions; FX/FY for the spatial dimensions of the convolutional weights kernel."}, {"title": "A. Framework Front-End and Network Transformations", "content": "MATCH receives two inputs to generate the target-specific C code: the NN model and the HW target definition. The target definition encompasses a list of hardware execution modules, i.e., units that can be used to execute one or more DNN operators. Each hardware execution module contains the memory description (i.e., hierarchy, dimensions, and connec-tions), a list of offloadable patterns, the set of specific APIs used, and a cost model for each operator. In Sec. V, we detail all the information needed to support a new target."}, {"title": "B. Pattern Matching and Accelerator-aware Dispatching", "content": "After applying the network graph transformation pipeline, MATCH assigns the execution of each part of the DNN to a selected HW module through its Pattern Matcher module. The Pattern Matcher analyzes the intermediate NN graph and tags groups of nodes (patterns) with a reference to the HW module that will execute them. More specifically, MATCH can associate to each pattern either an HW module or a not-matched tag; in the latter case, the TVM default code generation pipeline is invoked, and the layer will be executed on the main CPU of the SoC. If an HW module supporting the layer is found, MATCH exploits a DSE tool to generate an optimized schedule for it. MATCH builds on top of the TVM's internal pattern matching system to facilitate the definition of new SoC-specific patterns. Each HW module definition includes a Pattern Table that lists all patterns that can be offloaded to it. In detail, each pattern comprises the set of nodes to be matched, the replacement (one or more Relay nodes), and an additional Pattern Constraint that defines a set of rules that must be fulfilled for the match to be valid. These rules can verify if the input/output layouts, the quantization formats, the layer hyper-parameters (e.g., the convolutional kernel dimensions), etc, in the candidate graph pattern are compatible with those supported by the HW module.\nNote that compared to other State-of-the-Art (SoA) \u0391\u0399 compilers, such as DORY or HTVM, which separately support accelerators and MCUs but do not allow the orchestration of multiple HW modules on the same SoC, MATCH implements an iterative exploration algorithm to offload patterns to the HW execution unit with the minimum expected latency/energy among those supporting a given pattern, thus enabling the combined usage of multiple HW modules on the same SoC. To do so, MATCH sequentially explores the Pattern Tables of each individual HW module and invokes the DSE tool for each positive match. For each combination of (pattern, node parameters, HW module), the DSE tool provides a twofold output: the best schedule and its predicted latency/energy. If multiple HW modules match a pattern, MATCH selects the one that minimizes one of these metrics and tags the pattern accordingly. To cope with patterns contained within others (e.g., Conv-only versus Conv + Batch Norm + ReLU), we heuristically select the largest one, assuming that node fusion is always convenient to reduce latency and energy."}, {"title": "1) Model-based DSE Engine:", "content": "As a DSE tool, MATCH employs the optimizer included in ZigZag [11], which can search both for the optimal spatial mapping of a DNN layer onto a PE array (optimizing the number of PEs and their parallelism axes) and for the optimal temporal mapping or schedule (i.e., operator's loop ordering and loop tiling). Currently, in MATCH, we do not explore spatial mapping and focus only on temporal mapping optimization. Indeed, the spatial mapping is a fixed input, since we target already manufactured accelerators and hardware devices with pre-existing backend kernels with known parallelization strategies. To optimize the temporal mapping, we parse the provided workload definition and then explore the search space using the LOMA [32] engine. The latter generates all valid and non-equivalent scheduling candidates using the Loop Prime Factors approach [32]. It then allocates each loop's operands to the lowest non-full memory level. An analytical hardware performance cost model guides the selection of the optimal schedule among the generated ones. Importantly, LOMA supports uneven mapping, i.e., with different tensors tiled in different memory levels. We extend ZigZag and LOMA to take into account double-buffering. Further, we extend them to support a more general accelerator model, including systolic"}, {"title": "C. Code Generation", "content": "MATCH includes two code generation \u201cbranches\". For un-matched graph patterns, the fall-back codegen branch is taken, and MATCH generates C code exploiting one of TVM's default targets. The NN node in Relay IR is lowered to Tensor Expressions and then into TIR, a lower-level IR that allows representing information such as loop unrolling and layer tiling. Then, TVM produces generic C code for pre-defined targets such as x86/ARM/RISC-V CPUs. Note that during this lowering step, TVM is unaware of any specific ISA instructions or dedicated HW blocks.\nOn the other hand, if a pattern has been tagged with a specific HW module, the specialized codegen branch is executed. To generate the optimized C code, we provide a generic Layer Template, which is compiled with the i) pattern hyperparameters, ii) the spatial/temporal scheduling provided by the DSE, and iii) platform-specific APIs. We use the Python Mako template library for this step.\nTo generate the network's main file, MATCH uses the de-fault TVM generator that receives the layer function prototypes and the tensor dimensions to allocate the memory. We used TVM's default hill-climbing algorithm for memory allocation."}, {"title": "1) Layer Template Compilation:", "content": "First, default patterns hy-perparameters such as layer geometry, padding, or stride are added to the template. Then, the cached DSE spatio-temporal scheduling is used to define the layer loops order, the loop splitting, and the tensor tiles dimensions. The memory trans-fers are also correctly placed to minimize the latency overhead, following the DSE scheduling and supporting both single- and double-buffering, depending on the provided DSE output. Finally, the pattern's computation kernel (i.e., a wrapper to the target's backend library) is inserted in the innermost loop, where all the input data have already been transferred to the innermost memory level. In the code generated using the template, all function calls are still platform-agnostic: for instance, a memory-copy does not call the SoC-specific DMA function, but a generic MATCH_memory_copy.\nIn the last code generation step, MATCH embeds HW-specific user-defined APIs in the template. These APIs are defined in the HW model for each target, by specifying their name in an API object. We categorized APIs in 4 families:\n\u2022 Platform APIs: they are used to allocate, deallocate, or configure the corresponding HW modules. For instance, for an HW accelerator, these APIs could be used to write on memory-mapped registers in order to configure the module for executing a specific operation.\n\u2022 Memory APIs: these APIs facilitate memory manage-ment by enabling the allocation and deallocation of different memory levels, handling data transfers between them, for instance utilizing DMA calls for efficiency, and computing pointer offsets when employing specific data layouts.\n\u2022 Synchronization APIs: MATCH allows for asyn-chronous and synchronous data management. Therefore, it exposes APIs to synchronize after memory transfers, kernel computation, or both. Thanks to these APIs, we provide the possibility to perform both single- and double-buffered memory transfers.\n\u2022 Computational APIs: these are the backend library calls mentioned above. They receive the whole pattern context to set up and execute the correct computational kernel with its corresponding hyperparameters."}, {"title": "V. ADDING A NEW MATCH HW TARGET", "content": "To extend MATCH to a new HW target, we provide the MatchTarget class, which can encompass one or more HW Execution Modules. Each HW Execution Module contains four key components, as shown in Figure 4."}, {"title": "A. DIANA Model Customization", "content": "The first platform we target is DIANA [4], shown in Fig. 5. It features a RISC-V MCU, and two accelerators: a digital 8-bit accelerator and an Analog-in-Memory Computing (AIMC) ternary one. Our work targets only the digital one since we only target 8-bit integer networks. However, the analog accelerator can be supported as an additional HW Execution Module of the DIANA MatchTarget. The digital accelerator is a 2D SIMD array of 16\u00d716 PEs, that can achieve up to 256 8-bit MAC operations per cycle, allowing execution of convolutions / fully-connected layers with re-quantization, ReLU, and pooling operations directly at the output. The DIANA accelerator implements 2D Convolutions by spatially unrolling the output channels (K) and output spatial width (OX) in the two physical dimensions of the array. Instead, fully connected (FC) layers are spatially unrolled along input and output neurons. We stored the corresponding patterns in the module's Pattern Table. The accelerator has a 256kB L1 activation memory and a 64kB private weight memory, while the SoC L2 main memory is 512kB. Transfers between different memory levels are handled through DMA blocking calls. In order to correctly and fully utilize the PE array, DIANA layer primitives require spatially unrolled dimensions (K and OX) to be multiple of 16 and a custom NCHW-derived data layout. Both these requirements are handled through Network Transformation passes, exploiting spatial padding and static weights reshapes (not adding overhead at runtime).\nThe ZigZag DIANA performance Cost Model includes two main contributions $L_{ops}$ and $L_{mem,i,j}$, respectively, the latency of inner loop computations in L1 and the latency for memory transfers between the i-th and j-th hierarchy levels. $L_{ops}$ is derived automatically from the spatial parallelism information provided as input, coupled with coefficients that account for the number of computation cycles in L1. For DIANA, this corresponds to the latency for reading inputs, performing MACs, and writing outputs (1 cycle each), plus the application of elementwise operators to the outputs, and their storage into the activation memory (23 cycles). $L_{mem,1,2}$ models the memory transfer cycles and the performance overheads caused by L2-to-L1 DMA calls. This component is derived from the memory bandwidth information and the transferred tensors dimensions. Additionally, the shape of the transfers influences the performance overhead, which we established to be 70-cycles for each chunk of data stored contiguously in memory. If a data block is not stored contiguously, the overhead is multiplied by the number of contiguous sub-blocks. Since DIANA transfers data synchronously, ZigZag computes the overall latency as $L = L_{ops} + L_{mem,1,2}$.\nThe DIANA code generator includes the specific HW accel-erator invocation and DMA APIs. No Synchronization APIs are needed, given the blocking nature of the DMA and the presence of a single active accelerator."}, {"title": "B. GAP9 Model Customization", "content": "GAP9 combines a control MCU with a programmable multi-core compute cluster and a dedicated AI accelerator, NE16 (Figure 6). All cores use a RISC-V ISA enhanced with custom DSP-oriented extensions. Noteworthy, GAP9's MatchTarget is composed of two different HW Execution Modules (cluster and NE16), thus showcasing the flexibility of our tool and its management of heterogeneity. Both HW modules communicate with the control MCU through DMA, fully exploiting double-buffering and interleaving memory transfers with computation. The cluster and NE16 share an L1 multi-bank memory of 128kB for both activation and weights, while the SoC includes an L2 main memory of 1.5 MB. The cluster supports a wide range of operators, including pooling, convolutions, additions, and fully-connected layers, all followed by re-quantization. NE16 instead only supports convolutions and fully-connected layers. Note that all patterns present in the NE16 Pattern Table are also included in cluster one, allowing MATCH to choose the best HW Execution Module alternative for these patterns depending on the layer hyperparameters. Network Transformations for GAP9 have already been shown in Table II.\nTwo cost models are defined for the two HW modules: for NE16, we rely on the open-source cost model at\u00b2, which allows us to compute the $L_{ops}$ component. Despite the cluster not being an \"accelerator\u201d in the proper sense, we still define its optimal spatial mapping by considering how the kernels parallelize the innermost loops. For instance, for a convolution, we set the optimal spatial mapping to be equal to the tile dimensions that maximize the parallelization and memory re-use in the internal loop of the adopted kernel library (PULP-NN), i.e., OX = 2, K = 4, and OY = 8 [27]. When this spatial mapping cannot be used, e.g., when the OY dimension is not multiple of 8, MATCH can select between padding and parallelism reduction. For each spatially unrolled dimension (e.g. OY), its largest divider smaller than the optimal unrolling factor is used as a new unrolling factor (e.g., $D$, s.t. $OY = nD$, D < 8). Then, the number of temporal iterations required to process the layer using this new spatial parallelism is computed. If it is identical to the one obtained with the optimal values on a padded version of the input, then the reduced parallelism mapping is kept, as it does not incur a memory overhead. Otherwise, the input is padded. This flexibility is allowed by the structure of the cluster, which does not have a pre-defined spatial unrolling and is supported for all accelerators of this kind in MATCH. Therefore, it is not part of the required target-specific customizations. $L_{ops}$ is finally computed with a model extrapolated from the compiled back-end library, PULP-NN.\nFor both NE16 and Cluster, since DMA transfers are asynchronous, the total latency is computed as $L = max(L_{ops}, L_{mem,1,2})$, where $L_{ops}$ is modeled as detailed above. $L_{mem,1,2}$ models again the memory transfer cycles and the performance overheads caused by L2-to-L1 DMA calls. This component is derived from the memory bandwidth information and the transferred tensors dimensions, as in DIANA. In this case, we established a 27-cycles overhead for each chunk of data transferred that is stored contiguously in memory.\nThe GAP9 code generator includes APIs for both HW modules; further, it also includes the Synchronization APIS to orchestrate NE16 and Cluster execution, and memory transfers. An example of the API functions for NE16 has been previously shown in Fig. 3."}, {"title": "VI. EXPERIMENTAL RESULTS", "content": "In this section, we evaluate MATCH by compiling different DNNs targeting both DIANA and GAP9. For both platforms, we set the operating frequency to 260MHz and used the latest available SDKs and open-source kernel libraries. La-tency measurements were obtained using on-board dedicated performance counters. Our tool is compared with TVM (which only generates code for the main MCU of the two systems) and with HTVM (for DIANA) and DORY (GAP9), which utilize the same platform backend libraries and HW modules but different AI compilation steps. Additionally, on GAP9, we also compare MATCH with the NNTool, a proprietary tool from GreenWaves that utilizes a different backend library. Noteworthy, all the following results are measured on the actual HW platforms.\nIn Sec. VI-A, we first report MATCH's results on single convolutional layers, comparing them with plain TVM. In Sec. VI-B, we benchmark MATCH and SoA competitors on the four networks from the MLPerf Tiny benchmark suite [12]: a ResNet V1 [33] architecture with a backbone of 8 convo-lutional layers, trained for image classification on CIFAR10; a MobileNetV1 [34] with a width multiplier of 0.25 for person detection on the Visual Wake Words dataset; a Depth-wise Separable CNN (DS-CNN) [35] trained for Keyword Spotting (KWS) on the Speech Commands v2 dataset [36], with 105,829 utterances, to be classified into 12 classes; a fully-connected (FC) Autoencoder, which targets the Toy-car data fold included in the DCASE2020 dataset [37], to detect anomalies based on machine operating sounds. Finally, in Sec. VI-C, we perform ablation studies to show the impact of last-level cache memory reduction and demonstrate the performance gain obtained by exploiting SoC heterogeneity."}, {"title": "A. Micro Benchmarking", "content": "In this section, we present the results of our micro bench-marking experiments. We benchmarked a series of convo-"}, {"title": "B. Full Network Execution and Comparison with the State-of-the-Art", "content": "In Tab. III, we extend our experiments to the MLPerf Tiny networks on both GAP9 and DIANA, comparing latency results against plain TVM, HTVM, DORY, and NNTool.\nWhen targeting DIANA, MATCH demonstrates significant improvements over TVM, achieving an average speed-up of 60.87\u00d7 across the tested networks. Specifically, it achieves 169.4x lower latency on ResNet, 6.7\u00d7 on DSCNN, and 6.4\u00d7 on DAE whereas MobileNet cannot be deployed as it exceeds the total on-chip memory. Similarly, for GAP9, MATCH outperformed TVM substantially, with speed-ups of 47.7x for MobileNet, 159.2\u00d7 for ResNet, 53.1\u00d7 for DSCNN, and 11.3\u00d7 for DAE.\nCompared with HTVM, MATCH exhibits a slight perfor-mance deficit for DSCNN and DAE, with latencies of 7.3 ms versus 7.26 ms and 0.4 ms versus 0.36 ms, respectively. This slight disadvantage arises because our layer template is designed to be general across multiple platforms, resulting in a marginally slower execution. However, for MobileNet and ResNet, MATCH improves latency from 6.17 ms and 1.27 ms to 6.08 ms and 0.79 ms, respectively. These gains are attributed to the additional features of MATCH with respect to the tiler included in HTVM, such as the ability to perform loop reordering and uneven tensor mapping, which allows it to find more optimized layer schedules.\nWhen comparing MATCH to DORY, MATCH's ability to leverage the full heterogeneous platform comprising the NE16 accelerator results in an average speed-up across the 4 networks of 2.15\u00d7. Still, even when employing just the cluster (precise numbers reported in Tab. IV below), for MobileNet and ResNet, we achieve latencies of 11.2 ms and 5.48 ms, respectively, outperforming DORY by 1%/5%. For DSCNN and DAE, however, MATCH lags behind, with latencies of 4.25 ms and 0.54 ms, due to the layer template generality.\nFinally, compared to NNTool, MATCH is, on average, 2.5\u00d7 slower. This difference is primarily due to the more efficient and optimized back-end kernels used by NNTool, which are tailored to each specific hyperparameter combina-tion (the library contains hundreds of specialized convolutional kernels), and secondarily to template generalization overheads. Nevertheless, it is important to note that MATCH's framework is designed to be easily extensible, which could in principle allow the integration of alternative back-end kernels, including those used by NNTool.\nMore in general, MATCH is not designed to provide ex-tremely optimized results on a new platform out-of-the-box, but rather, to support deploying workloads to new accelerators"}, {"title": "C. Ablation study", "content": "1) Scheduling and memory scaling: In this section, we explore the impact of L1 memory size on the achieved MAC/cycle across the four networks\u2014MobileNet, ResNet, DSCNN, and DAE, on the GAP9 and DIANA hardware platforms (Fig. 9 and Fig. 10). The goal is to demonstrate how MATCH enables better scheduling and improved performance with respect to competitors, particularly when memory is constrained. For the DAE and DSCNN networks, no notable trends are observed across different L1 memory sizes. This is because these networks do not require tiling, allowing for consistent performance regardless of the available memory. The simplicity of these networks allows maintaining relatively stable MAC/cycle performance across varying L1 sizes, for both MATCH and its comparisons.\nIn contrast, on the MobileNet, at a constrained L1 memory size of 24 kB, on DIANA, we observe that MATCH can still achieve near-full performance, maintaining high MACs per cy-cle. In this scenario, HTVM's performance drops significantly, falling to 3 MAC/cycle, which results in MATCH achieving a speed-up of 1.5\u00d7 over HTVM (versus 1% at full L1 size). This highlights MATCH's ability to optimize layer scheduling effectively, even with limited memory resources.\nFor ResNet, the performance of competing tools declines sharply as L1 memory size decreases on both the DIANA and GAP9 platforms. For example, at 16 kB of L1 memory, HTVM becomes 3.01\u00d7 slower than MATCH, a stark contrast to the 1.61x difference observed when using the full L1"}, {"title": "2) Heterogeneity impact:", "content": "Tab. IV summarizes the perfor-mance results obtained by enabling different HW modules on the GAP9 MatchTarget. This analysis highlights the flexibility of MATCH in handling multi-accelerator systems, demonstrating the effectiveness of our partitioning mechanism, which groups nodes and assigns them to the HW module that the scheduler predicts will deliver the best performance.\nOnly enabling the Cluster HW module reduces the latency of the CPU-only solution by an average of 28.64\u00d7, with the ResNet network achieving a peak speed-up of 62.58x. Enabling the NE16 HW module without the 8-core cluster achieves an average speed-up of 43\u00d7 over the CPU-only baseline, with a maximum speed-up of 118.32\u00d7 for the ResNet network. It is important to note that for the DAE network, the NE16+CPU configuration yields identical results to the CPU-only setup because the DAE model is composed entirely of fully connected layers"}]}