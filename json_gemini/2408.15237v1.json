{"title": "The Mamba in the Llama:\nDistilling and Accelerating Hybrid Models", "authors": ["Junxiong Wang", "Daniele Paliotta", "Avner May", "Alexander M. Rush", "Tri Dao"], "abstract": "Linear RNN architectures, like Mamba, can be competitive with Transformer models in language\nmodeling while having advantageous deployment characteristics. Given the focus on training large-scale\nTransformer models, we consider the challenge of converting these pretrained models for deployment.\nWe demonstrate that it is feasible to distill large Transformers into linear RNNs by reusing the linear\nprojection weights from attention layers with academic GPU resources. The resulting hybrid model,\nwhich incorporates a quarter of the attention layers, achieves performance comparable to the original\nTransformer in chat benchmarks and outperforms open-source hybrid Mamba models trained from scratch\nwith trillions of tokens in both chat benchmarks and general benchmarks. Moreover, we introduce a\nhardware-aware speculative decoding algorithm that accelerates the inference speed of Mamba and hybrid\nmodels. Overall we show how, with limited computation resources, we can remove many of the original\nattention layers and generate from the resulting model more efficiently. Our top-performing model, distilled\nfrom Llama3-8B-Instruct, achieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and\n7.35 on MT-Bench, surpassing the best instruction-tuned linear RNN model.", "sections": [{"title": "1 Introduction", "content": "While Transformers [70] have been an essential architecture in deep learning and have driven the success\nof large language models such as GPT [8], Llama [68], and Mistral [36], they are prohibitively slow for\nvery long sequence generation due to their quadratic complexity with respect to sequence length and large\nkey-value (KV) cache requirement. Recent linear RNN models (Mamba [25], Mamba2 [17], GLA [76], RetNet\n[65], Griffin [18]) beat Transformers in controlled experiments at small to medium scale, although the best\nTransformers still significantly outperform these models on downstream tasks. We note that the training\ntimes of linear RNN models are similar to those of highly optimized Transformers [76], and therefore scaling\nup either of these models requires substantial computational resources.\nThe primary benefit of linear RNN models (Mamba [25], Mamba2 [17]) is that they have faster inference\n(5x higher throughput) than Transformers. Efficient inference is emerging as a critical need for LLM systems\nsuch as new applications currently bottlenecked by the large KV cache of Transformers, e.g. reasoning\nover multiple long documents [29, 54, 62] and files in large codebases [41, 58]). Emerging workflows with\nagents [74, 77] also require large-batch inference to explore more trajectories and long-context to model\ncomplex environments.\nThese properties motivate the goal of distilling a large pretrained Transformer model into a linear\nRNN in order to generate as efficiently as possible. The technical challenges are two-fold: how to map\npretrained Transformer weights to linear RNN weights for distillation, and how to adapt best-practice"}, {"title": "2 From Transformer to Mamba", "content": "Transformer inference techniques, such as speculative decoding, to the new architecture. We make the\nfollowing contributions:\n\u2022 We show that by reusing weights from attention layers, it is possible to distill a large transformer into\na large hybrid-linear RNN with minimal additional compute while preserving much of its generation\nquality. We propose a modified Mamba architecture that can be directly initialized from the attention\nblock of a pretrained model.\n\u2022 We propose a multistage distillation approach that mirrors the standard LLM pipeline combining\nprogressive distillation, supervised fine-tuning [38], and directed preference optimization [56]. This\napproach shows better perplexity and downstream evaluation compared with vanilla distillation.\n\u2022 We develop a hardware-aware speculative sampling algorithm and a fast kernel for speculative decoding\non Mamba and hybrid architectures. We achieve a throughput of over 300 tokens/second for a Mamba\n7B model. Additionally, we show that speculative decoding can be effectively applied to our hybrid\narchitecture.\nOur experiments distill different large-scale open chat LLMs, Zephyr-7B [69], Llama-3 8B [20] to linear\nRNN models (hybrid Mamba and Mamba2), using only 20B tokens of training. Results show that the\ndistilled approach matches the teacher model in standard Chat benchmarks [42, 80]. We also show that it\nperforms on par or better with all similarly sized pretrained-from-scatch Mamba models including Mamba\n7B models [25, 51] trained from scratch with 1.2T tokens or NVIDIA Hybrid Mamba2 models [71] trained\nfrom scratch with 3.5T tokens in multiple tasks (e.g., MMLU [33], TruthfulQA [46]) in the LM evaluation [24]."}, {"title": "2.1 Relationship Between Attention and Linear RNNs", "content": "We begin by reviewing multihead attention to clarify the shapes of intermediate objects. Notationally, we use\nexplicit subscripts for the sequence position instead of matrix representation, to better highlight similarities\nbetween the two models.\nAttention is computed in parallel for multiple differently parameterized heads. Each head takes sequence\no with hidden size D as an argument and computes,\n$Q_t = W^Q o_t$, $K_t = W^K o_t$, $V_t = W^V o_t$ for all t,\n$\\alpha_{1...t} = softmax([Q_t^T K_1 ... Q_t^T K_T]/\\sqrt{D})$\n$y_t = \\sum_{s=1}^t m_{s,t} \\alpha_s V_s$\nwhere $o_t \\in \\mathbb{R}^{D\\times 1}$, $W \\in \\mathbb{R}^{N\\times D}$\n$Q_t, K_t, V_t \\in \\mathbb{R}^{N\\times 1}$ $m_{s,t} = 1(s < t)$\nRecent work has argued that linear RNNs can be serious competitors to attention in large language\nmodels. Several different linear RNN formulations have been proposed with similar formulations. For now,\nwe leave the shapes of the parameters $A_t$, $B_t$, $C_t$ abstract, and note that linear RNNs all take the following\nform, where h is a matrix-valued hidden state,\n$h_t = A_t h_{t-1} + B_t x_t, Y_t = C_t h_t$  (1)\nLinear RNNs have several computational advantages over attention. During training, all yt values can be\ncomputed more efficiently than attention since there is no softmax non-linearity. During inference, each next\nyt can be computed serially without requiring a cache.\nDespite the superficially different form, there is a natural relationship between linear RNNs and attention.\nLinearizing the attention formula by removing the softmax yields:\n$y_t = \\sum_{s=1}^t m_{s,t} \\alpha_s V_s = \\frac{1}{\\sqrt{D}} Q_t \\sum_{s=1}^t (m_{s,t} K_s^T V_s) = \\frac{1}{\\sqrt{D}} Q_t \\sum_{s=1}^t m_{s,t} K_s^T W^V o_s$"}, {"title": "2.2 Distilling to an Expanded Linear RNN", "content": "This implies that there exists a linear RNN form of linearized attention, specifically:\n$h_t = m_{t-1,t} h_{t-1} + K_t V_t$ $Y_t = \\frac{1}{\\sqrt{D}} Q_t h_t$\n$h_t = A_t h_{t-1} + B_t x_t, Y_t = C_t h_t$\n$A_t = m_{t-1,t}, B_t = W^K o_t, C_t = W^Q o_t, x_t = W^V o_t$\nNote though that this version uses a hidden state of size $h \\in \\mathbb{R}^{N\\times 1}$. Effectively tracking only one scalar over\ntime per hidden dimension. Naively applying this transformation leads to poor results. The issue is that\nlinearizing attention produces a degraded representation of the original model, as the softmax nonlinearity\nis critical to attention.\nThe key to improving these models is to increase the capacity of the linear hidden state to better capture\nlong-term structure. For example, previous work has shown the use of kernel methods to improve this\napproximation [35, 60, 79]. These approaches expand the size of the hidden state representation to h to\n$\\mathbb{R}^{N\\times N'}$ to better match the modeling capacity of softmax.\nTo design a effective distilled linear RNN, we aim to stay as close as possible to the original Transformer\nparameterization, while also expanding the capacity of the linear RNN in an efficient manner. We will not\nattempt to have the new model capture the exact original attention function, but instead use the linearized\nform as a starting point for distillation.\nSpecifically, we adapt the parameterization from\nMamba, [25] to increase the hidden state size,\nwhile initializing from the attention representation.\nMamba uses a continuous time state-space model\n(SSM) to parameterize a linear RNN at run time,\ndescribed by the differential equation,\n$h'(k) = Ah(k)+B(k)x(k)$ $y(k) = C(k)h(k)$\nWhere A is a diagonal matrix and other values are\ncontinuous signals. To apply this formulation to\na discrete-time problem like language modeling,\nwe use a neural network to produce a sequence of\nsampling intervals \u2206t and samples of the signals\nat these time steps. Given these sampling intervals,\nand T samples of B, C, Mamba approximates the\ncontinuous-time equation using a linear RNN as a\ndiscretization. We use an overbar to indicate the\ndiscrete-time form, which is reconstructed dynami-\ncally."}, {"title": "2.3 Attention-to-Mamba Initialization and Hybrid Stepwise Training", "content": "Our full approach is shown in Algorithm 1. This algorithm feeds the standard Q, K, V heads from attention\ndirectly into the Mamba discretization, and then applies the resulting linear RNN. As noted above, this can\nseen as roughly initializing with linearized attention and allowing the model to learn richer interactions\nthrough the expanded hidden state.\nFigure 1 shows the resulting architecture. Our version directly replaces Transformer attention heads\ndirectly with fine-tune linear RNN layers. We keep the Transformer MLP layers as is and do not train them.\nThis approach also requires processing additional components like grouped query attention that shares keys\nand values across heads. We note that this architecture differs from the architecture used in many Mamba\nsystems, which combines MLP-SSM layers and uses a single head.\nThis initialization allows us to replace any attention block with a linear RNN block. We experiment\nwith hybrid models where we keep every n attention layers. Empirically we found that replacing layers in a\nstepwise manner was the most effective strategy, i.e. we first keep every 2 layers, distill, and then every 4,\nand continue distillation."}, {"title": "3 Knowledge Distillation for Aligned LMs", "content": "Knowledge distillation (KD) [34] serves as a compression technique aimed at training a smaller network\nthat mimics the behavior of a larger teacher network. After initializing the model from the Transformer\nparameters, we aim to distill it to perform on par with the original language model. We assume that most\nof the knowledge from the transformer is maintained in the MLP layers which were transferred from the"}, {"title": "4 Speculative Decoding Algorithms For Linear RNNs", "content": "original model, and focus on distilling the fine-tuning and alignment steps of the LLM. During this stage, the\nMLP layers are kept frozen and the Mamba layers are trained as in Figure 1.\nSupervised Fine-Tuning We first apply knowledge distillation to redo the supervised fine-tuning (SFT)\nstage of language model adaptation. During this stage, an LLM is trained to maximize the likelihood of a\nresponse y given an input prompt x, i.e. p(y | x). The task looks similar to conditional generation.\nThere are two common approaches for distillation in this setting. One method is to use word-level\nKL-Divergence. In this setting, the full probability distribution of the student model p(\u00b7; 0) is trained to\nmatch the full distribution of the teacher model p(\u00b7; \u04e9\u0442) by minimizing the KL divergence over the entire set\nof next possible tokens at position t. The second method is sequence-level knowledge distillation (SeqKD)\n[38]. SeqKD suggests a simple method for distillation on this style of task, by replacing the ground truth text\ny1...t with the teacher generation output \u01771.\u2026\u2026t, also known as pseudo-labels.\n$L(\\theta) = - \\sum_{t=1}^T  \\alpha  log  p(\\hat{y}_{t+1} | \\hat{y}_{1:t}, x, \\theta) + \\beta  KL [p(\\cdot | \\hat{y}_{1:t}, x, \\theta_T) || p(\\cdot | \\hat{y}_{1:t}, x, \\theta)]$ (2)\nHere @ is trainable parameters of the student model and a and \u03b2 control the weights of sequence and word\nloss term respectively.\nPreference Optimization The second stage of instruction-tuning for LLMs is to align them to a set of user\npreferences. During this stage, a set of desired preference pairs is used to improve the model's output. The\nobjective is to produce outputs y to prompts x that maximize a reward model r while maintaining close to a\nreference model. Typically the reference model is chosen to be the model after supervised fine-tuning. For\ndistillation, we can conveniently utilize the original teacher, i.e.\n$\\max_{\\theta} E_{x\\sim D, y\\sim p(y|x;\\theta)} [r_{\\theta}(x, y)] - \\beta KL[p(y | x; \\theta) || \\pi(y | x; \\theta_T)]$ (3)\nThis preference model is defined by a reward function r\u00f8(x, y) dependent on the method used. Previous\nresearch utilizing AI feedback has primarily focused on employing reinforcement learning methods, such as\nproximal policy optimization (PPO) [61], to optimize & concerning this reward. Recently, methods using\ndirect preference optimization (DPO) [56] have been effective at optimizing this objective with direct gradient\nupdates. Specifically, DPO shows that, if we have access to preferred yw and dispreferred y\u0131 outputs for a\ngiven prompt x, we can reformulate this optimization problem as,\n$\\pi_{\\theta} = \\max_{\\theta} E_{(x,y_w,y_l) \\sim D}  log \\sigma (\\beta  log \\frac{p(y_w|x; \\theta)}{p(y_w|x; \\theta_T)} - \\beta  log \\frac{p(y_l|x; \\theta)}{p(y_l|x; \\theta_T)})$.\n(4)\nThis optimization can be performed at the sequence level by scoring the preferred and dispreferred\noutputs of the model with the teacher and student and then backpropagating to the student. As far as we are\naware this is the first use of DPO as a distillation objective.\nThe main goal of the linear RNN formulation is to improve decoding efficiency. For both attention\nand linear RNNs, the serial dependency of autoregressive generation inherently bottlenecks efficiency.\nSystems cannot utilize all available compute, as they need to wait for the generation of previous tokens to\nproceed [9, 10, 40, 64, 73]. Speculative decoding has emerged as a method for breaking this bottleneck by\nspending extra compute to speculate on future generations. In this section, we consider approaches for\napplying this technique to large Mamba models, which can then be applied to the distilled models."}, {"title": "4.1 Challenges in RNN Speculation", "content": "Speculative decoding uses two models: a draft model, OD, and a verification model, \u03b8v. The fast draft model\nproduces potential future completions, y* = arg maxy1.T P(Y1, \u2026\u2026\u2026, YT; 0D), and the larger verification model\nchecks that these are top ranking at each time step, i.e. checking p(yt|91:t-1;0v). The longer a chain before a\nverification failure the faster the output. If a partial chain matches, we can rewind to the last match."}, {"title": "4.2 Multi-Step Linear RNN Speculation", "content": "We propose a new algorithm for linear RNN spec-\nulative decoding using hardware-aware multi-step\ngeneration. The core to the approach generation\nkernel that computes,\n$Y_{j:k}, h_j, h_k \\leftarrow MULTISTEP(h_i, Y_{1:n}, i, j, k; A, B, C, \\Delta)$\nWhere i is the starting hidden state, i \u2264 j \u2264 k, and\nj... k is the range of y outputs needed. The kernel\nis hardware-aware because it avoids materializing\nkey terms off of the fast GPU memory. Specifically, it\navoids instantiating most h1:n as well as the discrete-\ntime linear RNN parameters. This kernel is aimed"}, {"title": "4.3 Speculation Analysis and Hardware Specific Optimization", "content": "to target the issues presented above. Specifically, it can save a snapshot of the state hj before evaluating the\ndraft tokens. This allows recomputing the correct state on the fly after a token is rejected. The assumption is\nthat decoding is bottlenecked by memory and not by compute, as we can compute multiple steps of decoding\nwith very little overhead over single-step decoding.\nAlgorithm 2 and Figure 2 show the full algorithm. The approach maintains only one RNN hidden\nstate in cache for verification and advances it lazily based on the success of the multi-step kernel. Since\nthe distilled models contain transformer layers, we also extend speculative decoding to Attention/RNN\nhybrid architectures. In this setting, the RNN layers perform verification according to Algorithm 2, while the\ntransformer layers simply perform parallel verification.\nNote that if the draft model is a Mamba or hybrid model, the speculation part of the algorithm gets more\ncomplicated, as the draft model needs to recompute the state for the tokens accepted in the previous iteration.\nThis is done similarly to the verifier model, by caching older entries and recomputing on the fly during\nthe next round of speculation."}, {"title": "5 Results", "content": "4 shows that the performance of our distilled hybrid models matches that of the best open-source\nlinear RNN models on the Open LLM Leaderboard, while outperforming their corresponding open-source\ninstruct models in GSM8K and CRUX."}, {"title": "5.4 Hybrid speculative decoding", "content": "Setup We perform speculative decoding using the distilled hybrid models. We run experiments using both\nHybrid Mamba 50% and Hybrid Mamba 25% as main models. For the draft models, we train 2 and 4-layer\nTransformer Draft models on the OpenHermes2.5 dataset [67], for approximately 3 full epochs, following\nthe \"shrink and fine-tune\" approach from [63]. Specifically, we initialize the draft layers using layers from\nthe Zephyr-7B model (we take layers at indices [0, 31] for the 2-layer model and [0, 10, 20, 31] for the 4-layer\nmodel), and the embeddings and language model head also from the Zephyr-7B model [69]. We perform\nloss masking on the prompt, thus only considering next token prediction loss (cross-entropy) on the chat\ncontinuations from the training set. Speculative decoding experiments are run on a single NVIDIA RTX 3090\non data from OpenHermes2.5."}, {"title": "6 Analysis", "content": "Table 4 shows that the performance of our distilled hybrid models matches that of the best open-source\nlinear RNN models on the Open LLM Leaderboard, while outperforming their corresponding open-source\ninstruct models in GSM8K and CRUX.\nComparison with other distillation approaches Table 6 (left) compares the perplexity of different model\nvariants. We distill using Ultrachat as seed prompt [19] in one epoch and compare the perplexity. We\nfind that removing more layers gets significantly worse. We also compare our distillation approach with\na previous baseline. This approach distills a Transformer model into a Hyena model [55], as proposed in\n[57]. They use a different distillation approach using progressive knowledge transfer, wherein the student\nmodel is trained starting from the first layer and progressively extending to subsequent layers. While it\nis challenging to compare, our distill shows a smaller degradation (1.03 for 50% attention, 1.09 for 25 %%\nattention, 1.22 for 6.35% attention, and 3.36 for no attention), while the Distill Hyena model is trained in\nWikiText [52] dataset with a much smaller model and shows large perplexity degrade.\nDoes distilling from preferences help? In Table 6 (Right), we show the impact of different steps in the\nalignment process of the distillation. We observe that SFT or DPO alone does not yield much improvement,\nwhile SFT + DPO yields the best score. Models are trained using Zephyr as the teacher model and the\nOpenHermes 2.5 [67] dataset as the SFT dataset, and UltraFeedback [16] as the DPO dataset.\nPseudo Label Distillation Ablations. We consider several different model ablation studies in Table 7. For\nthese experiments we consider training for 5k steps using the pseudo-label approaches on the Ultrachat\n[19] dataset. Table 7 (Left) presents the results of distillation with various initializations. According to this\ntable, initializing weights from a transformer is crucial for performance. Without weight initialization from\na transformer, perplexity significantly worsens for both pure Mamba models and hybrid models. Also,\nfreezing MLP layers can help the student model focus on learning the interaction of tokens and better mimic\nattention layers. Table 7 (Right) shows also see smaller benefits from progressive distillation and interleaving\nthe attention layers with Mamba."}, {"title": "7 Related Work", "content": "Table 4 shows that the performance of our distilled hybrid models matches that of the best open-source\nlinear RNN models on the Open LLM Leaderboard, while outperforming their corresponding open-source\ninstruct models in GSM8K and CRUX.\nAttention-free models. Attention-free models offer improved computational and memory efficiency, making\nthem increasingly popular for various language processing tasks, including autoregressive language modeling.\nModels like S4 [27] and its subsequent variants [26, 30] have shown promising results in long-range synthetic\ntasks [66]. Gated SSM architectures, such as GSS [49] and BiGS [72], incorporate a gating mechanism into\nSSMs for (bidirectional) language modeling. The recently introduced Mamba model [25] argues that the\nstatic dynamics of these methods fail to incorporate input-specific context selection within the hidden state,\nwhich could be crucial for tasks like language modeling. Mamba has been shown to outperform Transformers\nacross different model sizes and scales. Additionally, several other sub-quadratic model architectures\n[1, 2, 4, 18, 21, 55, 76] and hybrid architectures [22, 43] have also been proposed.\nDistillation from Transformers. There have been relatively few attempts to distill on to linear RNN\nstyle models. Laughing Hyena [48] proposes to distill the long convolution into a state space representation,\nenabling constant time inference in Hyena [55]. Ralambomihanta et al. [57] introduces a progressive\nknowledge approach to distill small transformer models (70M) into Hyena models.\nSpeculative Decoding. Speculative decoding [9, 10, 40, 64, 73] has recently emerged as a promising\nmethod to accelerate the inference process of large language models, particularly Transformers. This\napproach utilizes a smaller draft model to speculatively generate candidate tokens, which the larger target\nmodel then verifies. Chen et al. [10], Leviathan et al. [40] proposed a rejection sampling scheme to improve\ninference quality, while Spector and Re [64] organized candidate tokens into a tree structure to enable more\nefficient verification. Subsequent work has examined both trained draft models [5, 12, 47] and training-free\ndraft models [23, 31, 75]."}, {"title": "8 Limitations", "content": "A potential limitation of our study is that the evaluation was conducted primarily on large-scale language\nmodels, specifically in the 7B-9B range. As a result, it remains unclear whether our proposed method would\nbe equally effective on smaller-scale models. While large models have demonstrated significant performance\ngains, the applicability of our approach to smaller models, which are often more computationally efficient,\nhas not been fully explored. Future work should investigate the performance of our method on smaller\ntransformer models, including conducting experiments that involve training smaller models from scratch and\napplying the distillation technique to assess their performance across various metrics. This would provide a\nmore comprehensive understanding of the generalizability and limitations of our approach."}, {"title": "9 Conclusion", "content": "We consider the problem of maintaining LLM abilities while increasing decoding speed through a combination\nof distillation and speculative decoding. We first show that a Transformer LLM can be used to effectively\ninitialize a Mamba linear RNN model while maintaining original abilities. We then show that through a\ncombination of distillation on supervised instructions and preferences, we can improve the model's ability\nwith relatively little compute. Finally, we show that the Mamba model can be significantly sped up at\ninference time through the use of a hardware-aware speculative decoding method. The full model nears LLM\nchat accuracy, and is accelerated with speculative decoding. We believe these results show that transformer\nknowledge can be transferred effectively to other architectures, opening up the potential for customizing the\ninference profile of LLMs beyond optimizing attention."}]}