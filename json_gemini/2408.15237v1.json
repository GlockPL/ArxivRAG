{"title": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models", "authors": ["Junxiong Wang", "Daniele Paliotta", "Avner May", "Alexander M. Rush", "Tri Dao"], "abstract": "Linear RNN architectures, like Mamba, can be competitive with Transformer models in language\nmodeling while having advantageous deployment characteristics. Given the focus on training large-scale\nTransformer models, we consider the challenge of converting these pretrained models for deployment.\nWe demonstrate that it is feasible to distill large Transformers into linear RNNs by reusing the linear\nprojection weights from attention layers with academic GPU resources. The resulting hybrid model,\nwhich incorporates a quarter of the attention layers, achieves performance comparable to the original\nTransformer in chat benchmarks and outperforms open-source hybrid Mamba models trained from scratch\nwith trillions of tokens in both chat benchmarks and general benchmarks. Moreover, we introduce a\nhardware-aware speculative decoding algorithm that accelerates the inference speed of Mamba and hybrid\nmodels. Overall we show how, with limited computation resources, we can remove many of the original\nattention layers and generate from the resulting model more efficiently. Our top-performing model, distilled\nfrom Llama3-8B-Instruct, achieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and\n7.35 on MT-Bench, surpassing the best instruction-tuned linear RNN model.", "sections": [{"title": "1 Introduction", "content": "While Transformers [70] have been an essential architecture in deep learning and have driven the success\nof large language models such as GPT [8], Llama [68], and Mistral [36], they are prohibitively slow for\nvery long sequence generation due to their quadratic complexity with respect to sequence length and large\nkey-value (KV) cache requirement. Recent linear RNN models (Mamba [25], Mamba2 [17], GLA [76], RetNet\n[65], Griffin [18]) beat Transformers in controlled experiments at small to medium scale, although the best\nTransformers still significantly outperform these models on downstream tasks. We note that the training\ntimes of linear RNN models are similar to those of highly optimized Transformers [76], and therefore scaling\nup either of these models requires substantial computational resources.\nThe primary benefit of linear RNN models (Mamba [25], Mamba2 [17]) is that they have faster inference\n(5x higher throughput) than Transformers. Efficient inference is emerging as a critical need for LLM systems\nsuch as new applications currently bottlenecked by the large KV cache of Transformers, e.g. reasoning\nover multiple long documents [29, 54, 62] and files in large codebases [41, 58]). Emerging workflows with\nagents [74, 77] also require large-batch inference to explore more trajectories and long-context to model\ncomplex environments.\nThese properties motivate the goal of distilling a large pretrained Transformer model into a linear\nRNN in order to generate as efficiently as possible. The technical challenges are two-fold: how to map\npretrained Transformer weights to linear RNN weights for distillation, and how to adapt best-practice"}, {"title": "2 From Transformer to Mamba", "content": null}, {"title": "2.1 Relationship Between Attention and Linear RNNs", "content": "We begin by reviewing multihead attention to clarify the shapes of intermediate objects. Notationally, we use\nexplicit subscripts for the sequence position instead of matrix representation, to better highlight similarities\nbetween the two models.\nAttention is computed in parallel for multiple differently parameterized heads. Each head takes sequence\no with hidden size D as an argument and computes,\n$Qt = Wot, Kt = WKot, Vt = WV ot $\nfor all t,\n$\u03b1\u2081... \u03b1_T = softmax([Q+K\u2081 ... Q KT]/\u221aD)$\n$Yt = \u2211ms,tas Vs$\ns=1\nwhere ot \u2208 RD\u00d71,\nWERNXD\nQt, Kt, Vt \u2208 RN\u00d71\nms,t = 1(s < t)\nRecent work has argued that linear RNNs can be serious competitors to attention in large language\nmodels. Several different linear RNN formulations have been proposed with similar formulations. For now,\nwe leave the shapes of the parameters At, Bt, C\u0165 abstract, and note that linear RNNs all take the following\nform, where h is a matrix-valued hidden state,\n$ht = Atht-1+ Btxt, Yt = Ctht$\n(1)\nLinear RNNs have several computational advantages over attention. During training, all yt values can be\ncomputed more efficiently than attention since there is no softmax non-linearity. During inference, each next\nyt can be computed serially without requiring a cache.\nDespite the superficially different form, there is a natural relationship between linear RNNs and attention.\nLinearizing the attention formula by removing the softmax yields:\n$Yt = \u2211ms,tas Vs = \u2211 (ms,KTV) = Qims,KWV$\ns=1\ns=1"}, {"title": "2.2 Distilling to an Expanded Linear RNN", "content": "To design a effective distilled linear RNN, we aim to stay as close as possible to the original Transformer\nparameterization, while also expanding the capacity of the linear RNN in an efficient manner. We will not\nattempt to have the new model capture the exact original attention function, but instead use the linearized\nform as a starting point for distillation.\nSpecifically, we adapt the parameterization from\nMamba, [25] to increase the hidden state size,\nwhile initializing from the attention representation.\nMamba uses a continuous time state-space model\n(SSM) to parameterize a linear RNN at run time,\ndescribed by the differential equation,\nh'(k) = Ah(k)+B(k)x(k) y(k) = C(k)h(k)\nWhere A is a diagonal matrix and other values are\ncontinuous signals. To apply this formulation to\na discrete-time problem like language modeling,\nwe use a neural network to produce a sequence of\nsampling intervals \u2206t and samples of the signals\nat these time steps. Given these sampling intervals,\nand T samples of B, C, Mamba approximates the\ncontinuous-time equation using a linear RNN as a\ndiscretization. We use an overbar to indicate the\ndiscrete-time form, which is reconstructed dynami-\ncally.\nA1...T, B1....T, C1...T = Discretize(A, B1...T, C1...\u03a4, \u03941...\u03a4)\nIn this simplest case, with N\u2032 = 1 and an identity discretization, this approach recovers the linear attention\nto linear RNN conversion discussed in the previous section. The benefit of Mamba is that with N' > 1 the\ncontinuous-time parameterization allows the model to learn significantly richer functions, without many\nmore parameters or decreased efficiency. Specifically the only additional learned parameters will be the\nsampling rate A and the dynamic A. These new parameters will control the constructed linear RNN through\nthe discretization function yielding the new matrix valued linear RNN. Specifically, we take in the same\nBt, Ct \u2208 RN\u00d71 and \u2206t \u2208 RN', but output Bt, Ct \u2208 RN'\u00d7N\u00d71, effectively increasing the hidden size by a\nfactor of N' over the naive linear attention."}, {"title": "2.3 Attention-to-Mamba Initialization and Hybrid Stepwise Training", "content": "Our full approach is shown in Algorithm 1. This algorithm feeds the standard Q, K, V heads from attention\ndirectly into the Mamba discretization, and then applies the resulting linear RNN. As noted above, this can\nseen as roughly initializing with linearized attention and allowing the model to learn richer interactions\nthrough the expanded hidden state.\nFigure 1 shows the resulting architecture. Our version directly replaces Transformer attention heads\ndirectly with fine-tune linear RNN layers. We keep the Transformer MLP layers as is and do not train them.\nThis approach also requires processing additional components like grouped query attention that shares keys\nand values across heads. We note that this architecture differs from the architecture used in many Mamba\nsystems, which combines MLP-SSM layers and uses a single head.\nThis initialization allows us to replace any attention block with a linear RNN block. We experiment\nwith hybrid models where we keep every n attention layers. Empirically we found that replacing layers in a\nstepwise manner was the most effective strategy, i.e. we first keep every 2 layers, distill, and then every 4,\nand continue distillation."}, {"title": "3 Knowledge Distillation for Aligned LMs", "content": "Knowledge distillation (KD) [34] serves as a compression technique aimed at training a smaller network\nthat mimics the behavior of a larger teacher network. After initializing the model from the Transformer\nparameters, we aim to distill it to perform on par with the original language model. We assume that most\nof the knowledge from the transformer is maintained in the MLP layers which were transferred from the"}, {"title": "4 Speculative Decoding Algorithms For Linear RNNs", "content": "The main goal of the linear RNN formulation is to improve decoding efficiency. For both attention\nand linear RNNs, the serial dependency of autoregressive generation inherently bottlenecks efficiency.\nSystems cannot utilize all available compute, as they need to wait for the generation of previous tokens to\nproceed [9, 10, 40, 64, 73]. Speculative decoding has emerged as a method for breaking this bottleneck by\nspending extra compute to speculate on future generations. In this section, we consider approaches for\napplying this technique to large Mamba models, which can then be applied to the distilled models."}, {"title": "4.1 Challenges in RNN Speculation", "content": "Speculative decoding uses two models: a draft model, OD, and a verification model, \u03b8v. The fast draft model\nproduces potential future completions, y* = arg maxy1.T P(Y1, \u2026\u2026\u2026, YT; 0D), and the larger verification model\nchecks that these are top ranking at each time step, i.e. checking p(yt|91:t-1;0v). The longer a chain before a\nverification failure the faster the output. If a partial chain matches, we can rewind to the last match."}, {"title": "4.2 Multi-Step Linear RNN Speculation", "content": "We propose a new algorithm for linear RNN spec-\nulative decoding using hardware-aware multi-step\ngeneration. The core to the approach generation\nkernel that computes,\nYj:k, hj, hk\u2190 MULTISTEP(hi, Y1:n, i, j, k; A, B, C, \u0394)\nWhere i is the starting hidden state, i \u2264 j \u2264 k, and\nj... k is the range of y outputs needed. The kernel\nis hardware-aware because it avoids materializing\nkey terms off of the fast GPU memory. Specifically, it\navoids instantiating most h1:n as well as the discrete-\ntime linear RNN parameters. This kernel is aimed"}, {"title": "4.3 Speculation Analysis and Hardware Specific Optimization", "content": "to target the issues presented above. Specifically, it can save a snapshot of the state hj before evaluating the\ndraft tokens. This allows recomputing the correct state on the fly after a token is rejected. The assumption is\nthat decoding is bottlenecked by memory and not by compute, as we can compute multiple steps of decoding\nwith very little overhead over single-step decoding.\nAlgorithm 2 and Figure 2 show the full algorithm. The approach maintains only one RNN hidden\nstate in cache for verification and advances it lazily based on the success of the multi-step kernel. Since\nthe distilled models contain transformer layers, we also extend speculative decoding to Attention/RNN\nhybrid architectures. In this setting, the RNN layers perform verification according to Algorithm 2, while the\ntransformer layers simply perform parallel verification.\nNote that if the draft model is a Mamba or hybrid model, the speculation part of the algorithm gets more\ncomplicated, as the draft model needs to recompute the state for the tokens accepted in the previous iteration.\nThis is done similarly to the verifier model, by caching older entries and recomputing on the fly during\nthe next round of speculation."}, {"title": "5 Results", "content": null}, {"title": "5.1 Experimental Setup", "content": "Target models. We perform experiments using two LLM chat models: Zephyr-7B [69], which is a chat\nfine-tuned Mistral 7B [36], and Llama-3 Instruct 8B [20]. For the linear RNN models, we use hybrid versions of\nMamba and Mamba2 with 50%, 25%, 12.5%, and 0% attention layers. We refer to 0% as a pure Mamba model.\nMamba2 is a variant architecture of Mamba that is designed to be more targeted to recent GPU architectures.\nZephyr-Mamba refers to a distillation from Zephyr [69], while Llama3-Mamba / Llama3-Mamba2 indicates\ndistillation from Llama-3 instruct 8B [68].\nTraining. Distillation does not require any language modeling pretraining data, but instead uses the\npost-training process to adapt the new model. We use a three-stage process. In the first stage, we use\nUltraChat [19] and UltraFeedback [16] as seed prompts and use the teacher model to generate pseudo-labels.\nThe student model is trained in one epoch using the loss Lin Eq 2 with a = 1 and \u03b2 = 0.1. Models are\ntrained using AdamW optimizer with \u03b2 = (0.9, 0.98) with a batch size 64. We use a linear learning rate\nwarm-up (for the first 500 steps) followed by cosine annealing. In the second stage, we use supervised\nfinetuning with our model on the GenQA [11], InfinityInstruct [3] and OpenHermes 2.5 [67] datasets using\nSFT in one epoch, with the same hyperparameters as Zephyr [69]. In the final stage, for models distilled from\nZephyr, we do distilled alignment with our model using DPO on the UltraFeedback [16] dataset which is\nconsistent with teacher model. While models distilled from Llama-3 instructed 8B, we use datasets 2 3 4 from\nSimPO [50] and Zephyr [69]. We only freeze Gated MLP (FFN) in the first stage, while in the second and final\nstage all parameters are trained. The total distillation process for each hybrid model (e.g., Mamba-Llama3\n(50% att)) takes less than five days in 8x80G A100."}, {"title": "5.2 Evaluation on Chat Benchmarks", "content": "We evaluate our models using both single-turn, AlpacaEval [42] and multi-turn chat benchmarks, MT-\nBench [80]. These benchmarks assess the model's ability to follow instructions and respond to challenging\nprompts across a wide variety of domains.\nTable 2 shows the performance of our models on chat benchmarks compared with large transformer\nmodels. The distilled hybrid Mamba model (50%) achieves a similar score in the MT-benchmark as the\nteacher model, and slightly better than the teacher model on the AlpacaEval benchmark in both LC win\nrate and overall win rate. The distilled hybrid Mamba (25% and 12.5%) performance is slightly worse than\nthat of the teacher models in the MT benchmark but still surpasses some large transformers even with more\nparameters in AlpcaaEval. The distilled pure (0%) model does degrade significantly in accuracy. Notably,\nthe distilled hybrid model performs better than Falcon Mamba, which was trained from scratch with more\nthan 5T tokens."}, {"title": "5.3 Evaluation on General Benchmarks", "content": "Zero Shot Evaluation. We utilize the open-source LM Evaluation Harness library [24] (branch big-refactor)\nto assess 10 tasks, with the following evaluation metrics: WinoGrande (WG) accuracy [59], PIQA (PQ)\naccuracy [6], HellaSwag (HS) normalized accuracy [78], ARC-Easy and ARC-Challenge (AE and AC) accuracy\nand normalized accuracy, [13], MMLU (MM), accuracy [32], OpenBookQA (OB) normalized accuracy [53],\nTruthFulQA (TQ) accuracy [45], PubMedQA (PM) accuracy [37], and RACE (RA), accuracy [39]. Each task is\nevaluated by analyzing the probability assigned by the model to each potential answer choice."}, {"title": "5.4 Hybrid speculative decoding", "content": "Setup We perform speculative decoding using the distilled hybrid models. We run experiments using both\nHybrid Mamba 50% and Hybrid Mamba 25% as main models. For the draft models, we train 2 and 4-layer\nTransformer Draft models on the OpenHermes2.5 dataset [67], for approximately 3 full epochs, following\nthe \"shrink and fine-tune\" approach from [63]. Specifically, we initialize the draft layers using layers from\nthe Zephyr-7B model (we take layers at indices [0, 31] for the 2-layer model and [0, 10, 20, 31] for the 4-layer\nmodel), and the embeddings and language model head also from the Zephyr-7B model [69]. We perform\nloss masking on the prompt, thus only considering next token prediction loss (cross-entropy) on the chat\ncontinuations from the training set. Speculative decoding experiments are run on a single NVIDIA RTX 3090\non data from OpenHermes2.5."}, {"title": "6 Analysis", "content": "Table 6 (left) compares the perplexity of different model\nvariants. We distill using Ultrachat as seed prompt [19] in one epoch and compare the perplexity. We\nfind that removing more layers gets significantly worse. We also compare our distillation approach with\na previous baseline. This approach distills a Transformer model into a Hyena model [55], as proposed in\n[57]. They use a different distillation approach using progressive knowledge transfer, wherein the student\nmodel is trained starting from the first layer and progressively extending to subsequent layers. While it\nis challenging to compare, our distill shows a smaller degradation (1.03 for 50% attention, 1.09 for 25 %\nattention, 1.22 for 6.35% attention, and 3.36 for no attention), while the Distill Hyena model is trained in\nWikiText [52] dataset with a much smaller model and shows large perplexity degrade.\nTable 6 (Right), we show the impact of different steps in the\nalignment process of the distillation. We observe that SFT or DPO alone does not yield much improvement,\nwhile SFT + DPO yields the best score. Models are trained using Zephyr as the teacher model and the\nOpenHermes 2.5 [67] dataset as the SFT dataset, and UltraFeedback [16] as the DPO dataset.\nTable 7 presents the results of distillation with various initializations. According to this\ntable, initializing weights from a transformer is crucial for performance. Without weight initialization from\na transformer, perplexity significantly worsens for both pure Mamba models and hybrid models. Also,\nfreezing MLP layers can help the student model focus on learning the interaction of tokens and better mimic\nattention layers. Table 7 shows also see smaller benefits from progressive distillation and interleaving\nthe attention layers with Mamba."}, {"title": "7 Related Work", "content": "Attention-free models. Attention-free models offer improved computational and memory efficiency, making\nthem increasingly popular for various language processing tasks, including autoregressive language modeling.\nModels like S4 [27] and its subsequent variants [26, 30] have shown promising results in long-range synthetic\ntasks [66]. Gated SSM architectures, such as GSS [49] and BiGS [72], incorporate a gating mechanism into\nSSMs for (bidirectional) language modeling. The recently introduced Mamba model [25] argues that the\nstatic dynamics of these methods fail to incorporate input-specific context selection within the hidden state,\nwhich could be crucial for tasks like language modeling. Mamba has been shown to outperform Transformers\nacross different model sizes and scales. Additionally, several other sub-quadratic model architectures\n[1, 2, 4, 18, 21, 55, 76] and hybrid architectures [22, 43] have also been proposed.\nDistillation from Transformers. There have been relatively few attempts to distill on to linear RNN\nstyle models. Laughing Hyena [48] proposes to distill the long convolution into a state space representation,\nenabling constant time inference in Hyena [55]. Ralambomihanta et al. [57] introduces a progressive\nknowledge approach to distill small transformer models (70M) into Hyena models.\nSpeculative Decoding. Speculative decoding [9, 10, 40, 64, 73] has recently emerged as a promising\nmethod to accelerate the inference process of large language models, particularly Transformers. This\napproach utilizes a smaller draft model to speculatively generate candidate tokens, which the larger target\nmodel then verifies. Chen et al. [10], Leviathan et al. [40] proposed a rejection sampling scheme to improve\ninference quality, while Spector and Re [64] organized candidate tokens into a tree structure to enable more\nefficient verification. Subsequent work has examined both trained draft models [5, 12, 47] and training-free\ndraft models [23, 31, 75]."}, {"title": "8 Limitations", "content": "A potential limitation of our study is that the evaluation was conducted primarily on large-scale language\nmodels, specifically in the 7B-9B range. As a result, it remains unclear whether our proposed method would\nbe equally effective on smaller-scale models. While large models have demonstrated significant performance\ngains, the applicability of our approach to smaller models, which are often more computationally efficient,\nhas not been fully explored. Future work should investigate the performance of our method on smaller\ntransformer models, including conducting experiments that involve training smaller models from scratch and\napplying the distillation technique to assess their performance across various metrics. This would provide a\nmore comprehensive understanding of the generalizability and limitations of our approach."}, {"title": "9 Conclusion", "content": "We consider the problem of maintaining LLM abilities while increasing decoding speed through a combination\nof distillation and speculative decoding. We first show that a Transformer LLM can be used to effectively\ninitialize a Mamba linear RNN model while maintaining original abilities. We then show that through a\ncombination of distillation on supervised instructions and preferences, we can improve the model's ability\nwith relatively little compute. Finally, we show that the Mamba model can be significantly sped up at\ninference time through the use of a hardware-aware speculative decoding method. The full model nears LLM\nchat accuracy, and is accelerated with speculative decoding. We believe these results show that transformer\nknowledge can be transferred effectively to other architectures, opening up the potential for customizing the\ninference profile of LLMs beyond optimizing attention."}, {"title": "Acknowledgement", "content": "We thank Together AI for providing compute for some of the experiments. This work has benefited from\nhelpful discussions with Albert Gu at CMU, Fran\u00e7ois Fleuret and Vincent Micheli at the University of Geneva,\nAlbert Tseng and Wen-Ding Li at Cornell University."}]}