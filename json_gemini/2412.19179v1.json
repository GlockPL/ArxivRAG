{"title": "Mask Approximation Net: Merging Feature Extraction and Distribution Learning for Remote Sensing Change Captioning", "authors": ["Dongwei Sun", "Xiangyong Cao"], "abstract": "Remote sensing image change description, as a novel multimodal task in the field of remote sensing processing, not only enables the detection of changes in surface conditions but also provides detailed descriptions of these changes, thereby enhancing human interpretability and interactivity. However, previous methods mainly employed Convolutional Neural Network (CNN) architectures to extract bitemporal image features. This approach often leads to an overemphasis on designing specific network architectures and limits the captured feature distributions to the current dataset, resulting in poor generalizability and robustness when applied to other datasets or real-world scenarios. To address these limitations, this paper proposes a novel approach for remote sensing image change detection and description that integrates diffusion models, aiming to shift the focus from conventional feature learning paradigms to data distribution learning. The proposed method primarily includes a simple multi-scale change detection module, whose output features are subsequently refined using a diffusion model. Additionally, we introduce a frequency-guided complex filter module to handle high-frequency noise during the diffusion process, which helps to maintain model performance. Finally, we validate the effectiveness of our proposed method on several remote sensing change detection description datasets, demonstrating its superior performance. The code available at MaskApproxNet", "sections": [{"title": "I. INTRODUCTION", "content": "REMOTE sensing change captioning has gained significant attention in recent years due to its ability to provide detailed, natural-language descriptions of temporal variations in the Earth's surface. Despite this progress, existing methods, predominantly based on Convolutional Neural Networks (CNNs), have several shortcomings that limit their practical applicability. The most notable issue with these traditional methods is their reliance on complex network architectures specifically designed for feature extraction from bitemporal images, which often results in limited generalizability across datasets. Such methods struggle to adapt to different domains, and their effectiveness diminishes when applied to scenarios or datasets that differ significantly from those used during training. Moreover, CNN-based methods are prone to overfitting, making it difficult to ensure robustness and reliability in real-world applications, especially where data distribution varies. These issues significantly restrict the practical use of remote sensing change captioning in diverse and dynamic environments.\nTo address these limitations, we propose a novel method that leverages diffusion models for remote sensing image change captioning. Diffusion models, which focus on learning data distributions rather than merely extracting features, provide a powerful alternative that can better capture the complex nature of temporal changes in remote sensing imagery. Our method integrates a multi-scale change captioning module to identify changes at various scales and refines these outputs through a diffusion process, resulting in more robust and adaptive caption generation. By shifting the paradigm from feature-based learning to distribution-based learning, our approach offers significant improvements in generalizability and robustness. Furthermore, we introduce a frequency-guided complex filter module to effectively manage high-frequency noise that can degrade model performance during the diffusion process. This combination ensures that the learned representations are not only accurate but also resilient to noise, enhancing the overall quality of generated captions.\nOur approach provides several key advantages over existing methods. First, it enhances the generalizability of change captioning models by focusing on data distribution learning, making it more adaptable to diverse and unseen datasets. Second, the use of a diffusion model enables effective refinement of features, leading to improved caption quality and accuracy. Third, the frequency-guided complex filter module addresses high-frequency noise, thereby maintaining the integrity of the model's output. Extensive experiments conducted on multiple remote sensing change captioning datasets demonstrate that our proposed method consistently outperforms state-of-the-art approaches in terms of captioning accuracy and robustness. These improvements make our framework highly suitable for real-world applications where interpretability and adaptability are crucial.\nIn summary, our work introduces a robust and adaptable framework for remote sensing change captioning by integrating diffusion models and a frequency-guided filtering mechanism. The highlights of our proposed method include enhanced generalizability through data distribution learning, refined feature representation via the diffusion model, and effective noise management to preserve output quality. This comprehensive approach ensures that our model not only performs well on standard datasets but also excels in challenging, real-world scenarios. Summary of the contributions of this paper is as follows:\n\u2022 Enhanced Generalizability: Our method improves generalizability through data distribution learning, enabling better adaptation to diverse datasets.\n\u2022 Refined Feature Representation: The diffusion model"}, {"title": "II. RELATED WORK", "content": "The RSICC task has garnered significant attention in recent years due to its capability to describe differences between bitemporal remote sensing (RS) images using natural language. Hoxha et al. [1]introduced early and late feature fusion strategies to integrate bitemporal visual features, utilizing an RNN and a multiclass SVM decoder for generating change captions. Chouaf et al. [2] were among the first to explore the RSICC task, employing a CNN as a visual encoder to capture temporal scene changes and an RNN as a decoder to produce descriptive change captions.\nTransformer networks [3], incorporating the multi-head attention (MHA) mechanism, have gained prominence in image analysis and achieved notable success in image change captioning. Liu et al. [4] advanced the field by proposing a transformer-based encoder-decoder framework for RSICC. Their method includes a dual-branch transformer encoder for detecting scene changes and a multistage fusion module for combining multilayer features to generate change descriptions. Subsequently, Liu et al. [5] refined their approach by introducing progressive difference perception transformer layers, which effectively capture both high-level and low-level semantic changes. Furthermore, Liu et al. [6] proposed a prompt-based strategy that leverages pretrained large language models (LLMs) for RSICC tasks, using visual features, change classes, and language representations as prompts for a frozen LLM to generate change captions. Chang [7] developed an attentive network for RS change captioning, named Chg2Cap, which harnesses the strengths of transformer models commonly used in NLP. [8] proposes a lightweight Sparse Focus Transformer for remote sensing image change captioning, which significantly reduces parameters and computational complexity while maintaining competitive performance. DiffusionRSCC [9] introduced an innovative diffusion model (Diffusion-RSCC) for RSICC, employing a forward noising and reverse denoising process to learn the probabilistic distribution of the input. SEN [10] proposed a bitemporal pretraining method that leverages self-supervised learning on a large-scale bitemporal RS image dataset. This approach reduces data distribution and input gaps, resulting in more suitable features for RSICC and improved model generalization."}, {"title": "B. Generative Models", "content": "Generative Adversarial Networks (GANs), as a typical generative approach, are widely used in the field of remote sensing imagery, particularly in change detection. [11] this research introduces a Dual Attentive Generative Adversarial Network (DAGAN) for high-resolution remote sensing image change detection. By designing a multi-level feature extractor and a multi-scale adaptive fusion module, DAGAN effectively integrates features at various levels, enhancing the accuracy of change detection. CD-GAN[12]presents an unsupervised change detection method named CD-GAN, tailored for heterogeneous remote sensing images acquired from different sensors. By integrating generative adversarial networks, CD-GAN can detect change regions without the need for image registration, improving robustness and accuracy. By generating better-registered images through GANs, [13] the method mitigates the impact of misregistration on change detection, thereby enhancing detection performance.\nWithin the enormous models for CD, DDPM-based architectures emerge with distinguished advantages over traditional CNNs and transformers. DDPM-CD [14] by pre-training the DDPM on a large set of unlabeled remote sensing images, multi-scale feature representations are obtained. A lightweight change detection classifier is then fine-tuned to detect precise changes. [15] designed to guide the generation of change detection maps by exploiting multi-level difference features. The Similar ideas have also applied in medicine field. The model [9] employs a noise predictor conditioned on cross-modal features to generate human-like descriptions of semantic changes between bi-temporal remote sensing image pairs. The authors [16] propose dynamic conditional encoding to enhance stepwise regional attention and a Feature Frequency Parser (FF-Parser) to mitigate high-frequency noise for medical image segmentation. [17] presents a graph attention-guided diffusion model tailored for liver vessel segmentation. By integrating graph attention mechanisms with diffusion probabilistic models, the approach effectively captures complex vascular structures in liver images."}, {"title": "III. METHOD", "content": "In this section, we propose a novel diffusion model-based approach for the remote sensing image change captioning task, leveraging mask generation for change captioning. The proposed method consists of two key phases: 1) Mask Approximation Phase, and 2) Text Decoder Decoding Phase. The primary objective of the first phase is to construct a mapping from the change distribution to the standard Gaussian distribution by utilizing the designed MaskApproxNet network, which extracts change features from the pre-change image and post-change image. In the reverse denoising process, the prior mask is transformed from the standard Gaussian distribution back to the real data distribution using a denoiser network.\nIn the following parts, we will first introduce the core concept of diffusion models using DDPM as an example. Then, we will provide a detailed explanation of the entire implementation process in the Mask Approximation Phase, including the detailed design of MaskApproxNet and the Frequency Noise Filter. Finally, we will focus on the Text Decoder process."}, {"title": "A. Denoising Diffusion Models", "content": "Diffusion models operate by defining a forward Markov process that gradually transforms data into noise and a reverse process that reconstructs data from noise. Formally, given an initial data distribution $x_0 \\sim q(x_0)$, the forward process generates a sequence of variables $X_1,X_2, ..., X_T$ using a transition kernel $q(x_t | X_{t-1})$. By applying the Markov property and the chain rule, the joint distribution of the sequence conditioned on $x_0$ can be expressed as:\n$q(x_1,..., x_T | x_0) = \\prod_{t=1}^{T} q(x_t | x_{t-1})$.\nIn the context of denoising diffusion probabilistic models (DDPMs), the transition kernel $q(x_t | x_{t-1})$ is designed to gradually transform the data distribution $q(x_0)$ into a simple, tractable prior. A common choice for this kernel is a Gaussian perturbation, defined as:\n$q(x_t | x_{t-1}) = N(x_t; \\sqrt{1 \u2013 \\beta_t}x_{t-1}, \\beta_tI)$,\nwhere $\\beta_t \\in (0,1)$ is a predefined hyperparameter. This Gaussian kernel allows the joint distribution to be marginalized analytically, yielding:\n$q(x_t | x_0) = N(x_t; \\sqrt{\\bar{a_t}}x_0, (1 \u2013 \\bar{a_t})I)$,\nwhere $a_t = 1 \u2212 \u03b2_t$ and $\\bar{a_t} = \\prod_{1=0} a_s$. A sample $x_t$ can then be generated from $x_0$ using the transformation:\n$x_t = \\sqrt{\\bar{a_t}}x_0 + \\sqrt{1 \u2013 \\bar{a_t}}e$,\nwhere $e \\sim N(0,I)$. When $\\bar{a_t} \u2248 0$, the final state $x_T$ approximates a Gaussian distribution, $q(x_T) \u2248 N(0, I)$.\nIntuitively, the forward process adds noise step by step, erasing data structure until only noise remains. To generate new data, DDPMs reverse this process. Starting with a noise sample $x_T \\sim N(0,I)$, the reverse Markov chain iteratively removes noise to reconstruct the data.\nThe reverse process is parameterized by a prior $p(x) = N(0, I)$ and a learnable transition kernel $p_\u03b8 (x_{t-1} | x_t)$, which is modeled as:\n$p_\u03b8(x_{t-1} | x_t) = N(x_{t-1}; \u03bc_\u03b8(x_t, t), \u03a3_\u03b8(x_t, t))$,\nwhere $\u03bc_\u03b8(x,t)$ and $\u03a3_\u03b8(x,t)$ are neural network outputs parameterized by $\u03b8$. The reverse chain starts from $x_T$ and iteratively samples $x_{t-1}$ until reaching $x_0$, yielding a generated data sample."}, {"title": "B. Mask Approximation Phase", "content": "The proposed algorithm model is shown in Figure 1 and is divided into two main stages. The first stage, the Mask Approximation Phase, primarily processes the given remote sensing images $I_{pre}$ and $I_{post}$. It utilizes the designed Mask Approx Module for simple feature processing, combines the results with a Noisy Mask, and sends them to the UNet denoising network to generate change features. These features are then passed to the second stage, the Text Decoder, to generate change detection descriptions. Next, we will introduce each of these two stages in detail."}, {"title": "1) Mask Approx Module", "content": "The proposed framework as illustrated Figure 2, which processes a pair of bi-temporal remote sensing (RS) images, denoted as $I_{pre}$ (pre-change image) and $I_{post}$ (post-change image), through a systematic pipeline. First, a Siamese ResNet backbone is utilized to extract multi-scale features at four levels, represented as $X_{pre}^i$ and $X_{post}^i$ for $i \u2208 {1,2,3,4}$, where:\n$X_{pre}^i = ResNet(I_{pre}, i), X_{post}^i = ResNet(I_{post}, i)$.\nThese features are then input to a difference module, which separately processes the multi-scale features and combines them through a convolution layer to generate rich feature encodings, denoted as $X$:\n$X = Conv(DifferenceModule(X_{pre}^i, X_{post}^i))$.\nThe encoded representations $X$ are passed to a decoder that upsamples the features to match the spatial resolution of the input images. The decoder employs two transpose convolution layers:\n$X_{upsampled} = TransposeConv(X)$,\nfollowed by a residual convolutional block for feature enhancement:\n$X_{refined} = ResidualBlock(X_{upsampled})$.\nFinally, a convolution layer is applied to the refined features to produce the predicted difference image DiffImage:\n$DiffImage = Conv(X_{refined})$.\nThis integrated framework effectively extracts, processes, and decodes change representations to achieve accurate change detection.\nTo achieve denoising, we employ U-Net as the core network. During the forward process, the change label $x_0$ undergoes a sequence of $T$ steps where Gaussian noise is progressively added. Conversely, the reverse process is designed to reconstruct the original data by systematically removing the noise. The reverse process is mathematically defined as:\n$P_\u03b8(X_{0:T-1} | X_T) = \\prod_{t=1}^{T} Po(X_{t-1} | X_t)$\nwhere $\u03b8$ denotes the learnable parameters of the reverse process. The initial state of the reverse process starts from a Gaussian noise distribution $P_\u03b8(X_T) = N(x_T; 0, I_{n\u00d7n})$, where $I$ represents the raw input image. Through this iterative reverse process, the latent variable distribution $p_\u03b8(x_T)$ transitions to the target data distribution $p_\u03b8(X_0)$.\nFigure 2 illustrates the conditioning of the reverse process on temporal remote sensing (RS) images. This process is defined as follows:\n$\u20ac_\u03b8(x_t, I_{pre}, I_{post}, t) = UNet(Cat(I_{pre}, MaskNoise), Cat(I_{post}, MaskNoise), Cat(Diff\\_image, MaskNoise), t)$\nwhere Cat(.) represents the concatenation operation. The variables $I_{pre}$ and $I_{post}$ correspond to the remote sensing images"}, {"title": "2) Frequency Noise Filter", "content": "Frequency-Guided Channel Fusion (FGCF) is a method designed for noise suppression in high-dimensional data as shown in Figure 3, by leveraging frequency-domain analysis. Let $X \u2208 R^{C\u00d7H\u00d7W}$ represent the input feature tensor, where C is the number of channels, and H and W are the spatial dimensions. The method starts by transforming each channel of the input tensor into the frequency domain using the Discrete Fourier Transform (DFT), defined as\n$X_c = F(X_c)$\nwhere F denotes the Fourier transform and $X \u2208 C^{H\u00d7W}$ is the frequency representation of the c-th channel. To suppress high-frequency noise, a frequency-guided weighting function $W_c(f_x, f_y)$ is applied to each frequency component $(f_x, f_y)$ of $X_c$. The weighting function is defined as\n$W_c(f_x, f_y) = exp(-\u03b1\u00b7 ||F(f_x, f_y)||^2)$\nwhere $F(f_x, f_y)$ is a learnable frequency filter and \u03b1 is a scaling parameter controlling the suppression strength. The filtered frequency representation becomes\n$X_c' = W_c \\odot X_c$\nwhere $\\odot$ denotes element-wise multiplication. After filtering, the frequency-domain representation is transformed back to the spatial domain using the Inverse Fourier Transform (IFT):\n$X_c'' = F^{-1}(X_c')$\nTo aggregate complementary information across channels, a channel attention mechanism is applied, where the channel-wise attention weights $A_c$ are computed as\n$A_c = \u03c3 (W_a \u00b7 GAP(X_c'') + b_a)$,\nwith GAP representing Global Average Pooling, $W_a$ and $b_a$ as learnable parameters, and \u03c3 as the sigmoid activation function. The final fused feature tensor is obtained by summing the weighted features from all channels:\n$X_{FGCF} = \\sum_{c=1}^{C} A_c X_c''$\nThe advantages of FGCF include frequency-domain adaptation, which targets specific frequency bands to suppress noise more effectively than purely spatial-domain methods, and channel fusion, which enhances the network's ability to preserve essential features while suppressing noise. The method also ensures parameter efficiency, as the learnable components such as the frequency filter F, the channel attention weights $W_a$, and the bias $b_a$ are lightweight, introducing minimal computational overhead."}, {"title": "C. Text Decoder Phase", "content": "The process of generating sentences using a transformer decoder begins with the conversion of initial token sequences t into word embeddings, represented as $T_{embed}$. This embedding transformation is defined as:\n$T_{embed} = E_{embed}(t) + E_{pos}$,\nwhere $E_{embed}(t)$ represents the token embeddings and $E_{pos}$ is the positional encoding, which captures the sequential order of the tokens. These embeddings serve as the initial input to the transformer decoder as shown in Figure 4."}, {"title": "IV. EXPERIMENTAL", "content": "The LEVIR_MCI dataset [19] is a large-scale benchmark designed for remote sensing image change captioning tasks. It comprises 10,077 pairs of bi-temporal remote sensing images, each with a spatial resolution of 0.5 meters per pixel and a size of 256 \u00d7 256 pixels. These image pairs are sourced from 20 distinct regions in Texas, USA, capturing various urban and suburban developments over time. Each image pair is annotated with five descriptive sentences detailing the changes observed between the two time points, resulting in a total of 50,385 change captions. This dataset facilitates the development and evaluation of models that generate natural language descriptions of changes in remote sensing imagery.\nThe WHU-CDC dataset is derived primarily from the WHU-CD [20] dataset and includes high-resolution (0.075 m) image"}, {"title": "B. Experimental Setup", "content": "In evaluating natural language generation tasks, several automatic metrics are commonly employed to assess the quality of generated text:\nBLEU (Bilingual Evaluation Understudy): Introduced by Papineni et al. [23], BLEU is a precision-based metric for evaluating machine translation quality. It calculates the overlap between n-grams of the candidate and reference translations, with n typically ranging from 1 to 4. The BLEU score is computed as:\n$BLEU = BP \u00d7 exp(\\sum_{n=1}^{N} W_n log P_n)$"}, {"title": "C. Quantitative Results", "content": "Table I summarizes the performance of the proposed Mask Approx Net and competing methods on the LEVIR-CC dataset across multiple evaluation metrics. Mask Approx Net achieves state-of-the-art results, consistently outperforming all baseline methods. Notably, Mask Approx Net achieves the highest scores on key metrics such as BLEU-4 (64.32), METEOR (39.91), ROUGE-L (75.67), and CIDEr-D (137.71), demonstrating its superior ability to generate accurate and semantically rich descriptions. Compared to strong baselines such as SEN, Sparse Focus, and ATTENTIVE, the proposed method shows consistent improvements, particularly in CIDEr-D and BLEU scores, which are critical for assessing semantic relevance and fluency.\nOverall, the results highlight the robustness and effectiveness of Mask Approx Net in capturing detailed semantic and structural information for change captioning, establishing it as the new state-of-the-art on the LEVIR-CC dataset.\nThe experimental results on the WHU-CDC dataset, presented in Table II, highlight the competitive performance of our proposed Mask Approx Net across multiple evaluation metrics. Mask Approx Net achieves the highest scores for"}, {"title": "D. Qualitative Visualization", "content": "The qualitative results in Figure 5 demonstrate the effectiveness of the proposed model in capturing semantic changes within remote sensing images. For instance, in scenarios where \"a ring of houses surrounds the square,\" the model successfully predicts \"many villas are built around the road,\" indicating a reasonable interpretation of structural changes. The predicted captions effectively capture major transitions, such as the replacement of natural features with urban structures or the emergence of roads and buildings. The generated change maps further validate the model's capacity to align spatial transformations with semantic interpretations, showcasing its potential for practical applications in change detection tasks. Overall, the generated change maps and captions reveal the model's capability to produce contextually relevant and visually coherent descriptions."}, {"title": "E. Limitation and Future Work", "content": "For the first time, we applied the diffusion model approach to the task of remote sensing image change captioning. Through this study, we summarize the existing challenges and future directions as follows: 1. The training time of diffusion models, as well as their slow convergence and inference speed during training, remains a fundamental issue. 2. As an application-oriented research field, remote sensing image processing will increasingly focus on the model's size, generalization ability, and robustness. 3. The semantic and logical alignment between images and descriptive text remains a key focus for future research."}, {"title": "V. CONCLUSION", "content": "In this paper, we presented a novel approach to remote sensing image change captioning by integrating diffusion models into the change captioning process. Our method shifts the focus from traditional feature learning paradigms to a data distribution learning perspective, addressing the limitations of existing CNN-based techniques. By incorporating a multi-scale change captioning module and refining the output features through a diffusion model, we have shown significant improvements in the robustness and adaptability of change captioning across different datasets and real-world scenarios. Furthermore, the introduction of the frequency-guided complex filter module ensures that high-frequency noise is effectively managed during the diffusion process, thereby preserving model performance.\nThe experimental results on multiple remote sensing change captioning description datasets validate the effectiveness of our proposed method, demonstrating superior performance compared to existing approaches. Our framework not only improves captioning accuracy but also enhances interpretability, contributing to a more comprehensive understanding of changes in remote sensing imagery. Future work will explore further optimizations of the diffusion process and extend the applicability of our approach to other multimodal tasks in remote sensing, ensuring even broader impact and usability in real-world applications."}]}