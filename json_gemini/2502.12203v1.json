{"title": "An Interpretable Automated Mechanism Design Framework with Large Language Models", "authors": ["JIAYUAN LIU", "MINGYU GUO", "VINCENT CONITZER"], "abstract": "Mechanism design has long been a cornerstone of economic theory, with traditional approaches relying on mathematical derivations. More recently, automated mechanism design approaches have started to be used, most recently including differentiable economics methods that leverage neural networks to design payments and allocations (e.g., RegretNet and GemNet). While both analytical and automated methods have advanced the field, they each face significant weaknesses: mathematical derivations are not automated and often struggle to scale to complex problems, while automated and especially neural-network-based approaches suffer from limited interpretability. To address these challenges, we introduce a novel framework that reformulates mechanism design as a code generation task. Using large language models (LLMs), we generate heuristic mechanisms described in code and evolve them to optimize over some evaluation metrics while ensuring key design criteria (e.g., feasibility, strategy-proofness) through a problem-specific fixing process. This fixing process ensures that any mechanism violating the design criteria is adjusted to satisfy them, albeit with some trade-offs in performance metrics. These trade-offs are factored in during the LLM-based evolution process. The code generation capabilities of LLMs enable the discovery of novel and interpretable solutions, bridging the symbolic logic of mechanism design and the generative power of modern AI. Through rigorous experimentation, we demonstrate that LLM-generated mechanisms achieve competitive performance while offering greater interpretability compared to previous approaches. Notably, our framework can rediscover existing manually designed mechanisms (e.g., Myerson's mechanism in special cases and virtual valuation, reserve price auction, existing redistribution mechanism with theoretical guarantee) and provide insights into neural-network based solutions (i.e., RegretNet) through Programming-by-Example. These results highlight the potential of LLMs to not only automate but also enhance the transparency and scalability of mechanism design, thereby helping to ensure that we can safely deploy the resulting mechanisms in our societies.", "sections": [{"title": "1 Introduction", "content": "Mechanism design is the theory at the heart of market design. Its appeal is due to its rigorous mathematical foundation in game theory, foundational results such as the revelation principle, which allow us to clearly conceptualize what we seek to optimize, and key findings within the framework. These include positive results such as the VCG mechanisms [Clarke, 1971, Groves, 1973, Vickrey, 1961] and Myerson's optimal auction [Myerson, 1981], as well as negative results such as the Myerson-Satterthwaite impossibility [Myerson and Satterthwaite, 1983].\nUnfortunately, one way in which mechanism design has fallen short is that many important problems appear intractable to the analytical approach. Perhaps the best-known example of this is the difficulty of generalizing Myerson's optimal auction in several directions, including multi-item auctions [Daskalakis, 2015], correlated bidder valuations [Papadimitriou and Pierrakos, 2011], and dynamic settings [Doepke and Townsend, 2006, Zhang and Conitzer, 2021]. These extensions often lead to mechanisms that are analytically intractable. As a result, researchers have been forced to explore alternative approaches, such as approximate mechanism design [Hartline, 2013, Procaccia and Tennenholtz, 2013] and data-driven methods [Bergemann et al., 2024], to address more complex settings where analytical solutions remain elusive.\nIn 2002, automated mechanism design (AMD) [Conitzer and Sandholm, 2002] was introduced as a way for computer scientists to help address this challenge. The idea is that, since mechanism design problems are naturally framed as optimization problems, we can approach those problems computationally and thereby solve for the optimal mechanism, at least in special cases. While this approach has seen some successes, in general it faces its own challenges, including both (computational) tractability \u2013 without any structure, linear-programming formulations do not scale well \u2013 and interpretability \u2013 unless we are optimizing only within a well-understood parameterized class of mechanisms, the solution may end up being presented to us as a giant inscrutable table of numbers from which little intuition can be gained.\nA new approach within AMD that promises to make headway on the tractability challenge is differentiable mechanism design [D\u00fctting et al., 2024, Feng et al., 2018, Rahme et al., 2021, Shen et al., 2018, Wang et al., 2024], relying on neural networks. Unfortunately, as neural networks are famous for their lack of interpretability, the other challenge remains.\nIn this paper, we aim to introduce a new automated mechanism design framework that promises to address the interpretability issue. Classically, how do we describe the mechanisms found using the analytical approach-VCG mechanisms, Myerson's auction, etc.? Not as neural networks or giant tables of numbers, but as formulas, as code. This suggests a role for large language models (LLMs): If we can use LLMs to generate an (ideally brief) specification of how the mechanism runs, then we can analyze and understand this specification far more naturally than any giant table of numbers. Rapid progress in LLM, along with advances in associated techniques such as neurosymbolic approaches that combine neural networks with symbolic reasoning and improvements in reasoning abilities, suggests that this approach is likely to grow in success over the years.\nOur primary contribution is to build an interpretable automated mechanism design framework with LLMs. Our method has the following advantages compared to the existing work.\n(1) Our method is automated. For a specific setting, even if it is possible to manually design a good mechanism and analytically derive its expected or worst-case guarantees, in general the methods used to achieve this are highly ad hoc. Even slight variations in the problem setup may require an entirely new analytical technique. In contrast, our techniques, like AMD tech-niques in general, automate the process, generating mechanisms without requiring manual intervention, which are more flexible and applicable to complex and dynamic environments where manual design is infeasible."}, {"title": "2 Related Work", "content": "In this section, we review the various lines of related research on which this paper is based."}, {"title": "2.1 Automated Mechanism Design", "content": "The analytical results of optimal auction design have been limited to simple mechanism design settings such as variations of single-bidder settings. Theoretical studies have not been able to explore the full intricacy of mechanism design; a new direction, automated mechanism design (AMD), was introduced to help in this exploration.\nDiscretizing the players' type spaces allows for the AMD problem to be solved as a linear-programming problem (or, if a deterministic mechanism is desired, a mixed-integer linear pro-gram) [Conitzer and Sandholm, 2002, 2003, 2004]. But the cost of such generality is that it is hard to scale this approach. Several more scalable approaches to AMD have been developed to address scalability issues. One such approach is parametric mechanism design, which restricts the search space to a specific class of parameterized mechanisms. This method has proven effective in various settings, including helping conjecture and proving analytical results. Examples include auction design [Sandholm, 2003, Sandholm and Likhodedov, 2015] and the design of mechanisms that redistribute as much of the auction revenue back to bidders as possible [Cavallo, 2008, Guo, 2011, 2012, 2016, 2024, Guo and Conitzer, 2007, 2008a,b, 2010b, 2014] (for an overview of this approach, see [Guo and Conitzer, 2010a]). On the other hand, in a sense this approach is only partly auto-mated, in that specifying a good parameterized class of mechanisms still requires human insight. Consequently, this approach has been successful only in some domains."}, {"title": "2.2 Differentiable Auction Design and Neurosymbolic Al", "content": "More recently, differentiable methods have been introduced that use neural networks to do auction design [D\u00fctting et al., 2024, Feng et al., 2018, Rahme et al., 2021, Shen et al., 2018, Wang et al., 2024]. The main challenge is to fulfill the expressiveness, strategy-proofness, and multi-bidder requirements. In terms of expressiveness, though the optimal auctions are represented by the neural network, the resulting mechanisms are hard to interpret from the weights of the neural network.\nA recent direction for AI research, neurosymbolic approaches that combine neural networks with symbolic reasoning, is promising for solving the interpretability issue. These methods have shown success in learning interpretable program models, improving efficiency and generalization in various tasks, and enhancing AI system explainability [Hitzler and Sarker, 2022, Sarker et al., 2022, Sheth et al., 2023]. By bridging neural computation with symbolic representation, neurosymbolic AI could potentially address interpretability challenges in mechanism design while maintaining computational efficiency."}, {"title": "2.3 Large Language Models and Code Generation", "content": "Large Language Models (LLMs) have revolutionized natural language processing, demonstrating remarkable capabilities in reasoning, problem-solving, and text generation [Zhao et al., 2023]. Recent advancements have led to models with enhanced abilities in multi-step reasoning and logical deduction, such as GPT-01 and DeepSeek-R1\u00b9 [Guan et al., 2025, Liu et al., 2024a, Mercer et al., 2025, Xiang et al., 2025].\nIn the realm of code generation, LLMs have shown impressive capabilities [Chen et al., 2021, Fried et al., 2022, Nijkamp et al., 2022]. Code generation refers to the automatic creation of source code from high-level descriptions, specifications, or natural language prompts. LLMs have achieved no-table performance in code generation tasks, such as solving problems from coding competitions [Li et al., 2022], debugging [Chen et al., 2023, Jin et al., 2023, Liventsev et al., 2023], and improving the given code [Shypula et al., 2023]. One reason for LLMs' excellent performance is their training on vast datasets of both natural language and code, enabling them to understand context, generate syntactically correct code, and work across multiple programming languages. These works provide the foundations for solving various tasks via LLM-powered code generation. Developers can already take advantage of existing LLM-based code generation tools such as GitHub Copilot\u00b2 to assist with code explanation, debugging, and writing. Given the recent advancements in LLMs, it is natural to explore the application of LLM-based code generation techniques to mechanism design problems."}, {"title": "2.4 Evolutionary Algorithms", "content": "Evolutionary Algorithms (EAs) are a class of optimization techniques inspired by biological evolu-tion. These algorithms leverage principles such as natural selection, mutation, and crossover to solve complex problems in computer science and engineering, iteratively improving a population of candidate solutions through score-based selection (often called \u201cfitness-based\u201d in the EA literature) and stochastic variation operators. EAs excel in navigating high-dimensional, multi-modal, and non-differentiable search spaces where traditional gradient-based methods often get stuck [Slowik and Kwasnicka, 2020, Yu and Gen, 2010].\nRecent works combine EAs with LLMs, introducing EA-based prompt engineering, particularly for mathematical and algorithmic discovery. FunSearch [Romera-Paredes et al., 2024] iteratively generates improved solutions in the form of computer \u201cfunction\u201d code given previous good solutions using LLMs, verifies the generated solutions with an automated \u201cevaluator\u201d, then maintains a pool"}, {"title": "2.5 Programming-by-Example", "content": "While LLMs are known for their expertise in deductive reasoning tasks (following the input instructions and reasoning as told), they also have strong capabilities in inductive reasoning tasks, where they infer underlying patterns from input-output pairs or cause-effect relationships [Cheng et al., 2024]. This capability makes them particularly well-suited for Programming-by-Example (PBE), a technique where a system automatically generates programs or algorithms from example input-output pairs. Instead of manually writing code, users provide examples of the desired behavior, and the system infers the rules to replicate and generalize that behavior for new inputs [Gulwani et al., 2017, Halbert, 1984]. PBE has been successfully applied in various domains, such as automating repetitive tasks in spreadsheets [Wu et al., 2023], and synthesizing data transformations [Feser et al., 2015, Jin et al., 2017]. This method significantly reduces the need for manual coding. Recent advancements in LLMs have expanded PBE's scope, enabling systems to handle more complex tasks with fewer examples. By leveraging their training data and reasoning capabilities, LLMs can infer patterns and generate robust, generalizable programs [Cheng et al., 2024]."}, {"title": "3 Framing Mechanism Design as an Automated Code Generation Workflow", "content": "In the following sections, we denote bidder $i$'s bid as $b_i$, the bids of all other bidders as $b_{-i}$, and the bids of all bidders collectively as $b$. Similarly, we define $p_i$, $p_{-i}$, and $p$ for payments."}, {"title": "3.1 The Automated Mechanism Design with LLM (AMD-LLM) Framework", "content": "3.1.1 Key components of the framework. We automate mechanism design using an LLM code generation workflow that requires three components to be specified:\n(1) Identify the key part of the mechanism design problem and leave it blank as the heuristic function that will be evolved by LLMs. Specifically, we run our experiments in the FunSearch environment [Romera-Paredes et al., 2024]. Example 3.2 below describes a specific auction setting \u2013 Myerson's optimal auction setting [Myerson, 1981], but with bid correlation. In the context of this example, the key part of the mechanism design problem is the allocation func-tion, which is expressed as a Python function that maps the bids to the allocation result. The initial blank version simply does not allocate, or we could use any other trivial initialiazation such as always allocating to bidder 1. This allocation function will go through the LLM-based evolution process. According to Myerson's characterization, given an allocation function, there is only one way to charge the payments without violating either strategy-proofness or individual rationality, so the payment function does not need evolution in itself. That is, the code that derives the correct payments given the current allocation is manually provided as code boilerplate and does not change."}, {"title": "3.1.2 Fixing process", "content": "Although LLMs have strong code generation capabilities, the heuristic func-tions they generate may not always satisfy all mechanism design criteria, particularly for complex problems requiring lengthy, detailed prompts. If the generated mechanism fails to meet even one of the ten requirements, it becomes ineffective. We introduce the fixing process to ensure these important criteria, at the cost of some performance loss in terms of the evaluation metric. The fixing process should be universally applicable to any mechanism in the design space. For example, in the context of Example 3.2, we can always raise the critical price faced by the winner to the point that any bid above it ensures that the winner still wins. This fixing process can fix any allocation heuristic to reach monotonicity, but in some situations, we may need to raise the critical price significantly, which results in lower expected revenue due to high probability of no allocation (no agents can afford the item)."}, {"title": "Definition 3.1 (Fixing process)", "content": "A fixing process (or fixing rule or fix) is a problem-specific post-processing procedure applied to any mechanism within a given design space that fails to satisfy one or more specified design criteria. The fixing process transforms such a mechanism into one that meets all required criteria. This procedure is tailored to the specific problem and design constraints, and it may involve trade-offs, such as performance loss, to achieve the desired criteria.\nFor many mechanism design problems, a na\u00efve fix usually exists. For instance, if the generated mechanism fails to meet the criteria, then the fix can simply change it to always produce a default outcome (e.g., no allocation in auction design), resulting in a zero performance score. More delicate fixes can be found by investigating the properties of the specific problem, as shown in Example 3.2 for optimal single-item auction design.\nWe can similarly fix several other auction design problems. Another example involving fixing the VCG redistribution function is given in Sec. 5."}, {"title": "3.1.3 The workflow", "content": "In each iteration, the evolutionary framework selects one or more existing heuristics from the program database (containing all saved heuristics so far), combines them together using a prompt (i.e., the task description and optionally an evolutionary strategy along the line of \"mutation\u201d, \u201ccrossover\u201d, etc.), and then inputs the combined prompt to the LLM. Next, the LLM outputs a new heuristic. The heuristic will be fixed through the problem-specific fixing process, and evaluated using the evaluation function. The returned value of the evaluation function is called the fitness (or score) in the evolutionary computation literature; it indicates how well the generated heuristic function meets the desired objective, guiding selection and improvement during the evolutionary process. This process continues repeatedly, as shown in Fig. 1 (following the \u201cwithout NN\u201d branch)."}, {"title": "3.1.4 The Prompts", "content": "There are two kinds of prompts for LLMs: system prompts and user prompts.\nA system prompt is an instruction that guides the LLM's behavior, tone, and constraints. In our framework, it includes information such as \u201cyou are a mechanism design expert,\u201d a description of the task, and the required format of the heuristic output. This prompt remains unchanged throughout the workflow.\nA user prompt is the input or query given by the user to generate a response. In our framework, it contains iteration-specific content, including sampled past heuristics and optionally a sampled pre-set evolution strategy (i.e., \u201cmutate\u201d, \u201ccrossover\u201d, etc., details in Sec. 3.1.5). The user prompt varies for each iteration."}, {"title": "3.1.5 The Pre-set Evolution Strategies", "content": "An evolutionary algorithm normally includes the following key genetic operations.\n\u2022 Crossover: combines material from parents to produce offspring, facilitating exploration of the search space by generating new solution combinations.\n\u2022 Mutation: introduces random modifications to individuals, maintaining population diversity, and preventing premature convergence to suboptimal solutions.\n\u2022 Selection and elitism: selects the desirable offspring to retain in the population, and meanwhile, ensures that high-quality solutions are not lost during evolution.\nWe can pre-set the evolution strategies through explicit prompts and ask the LLM to evolve accordingly. Liu et al. [2024b] used this idea to generate combinatorial optimization heuristics, where they designed five strategies, explicitly asking the LLM to create entirely new algorithms, to draw inspiration from existing algorithms, and to adapt or refine existing algorithms by altering their structure, parameters, or components to improve performance or generalization. These prompts well depict the key genetic operations in evolutionary algorithms and can be generally applied. We alter these general strategy prompts to more ad-hoc ones to better fit each of our tested settings (see the examples in later sections).\nIt should be noted that explicitly specifying the evolution strategy in the prompt is optional. If we want to explicitly pre-set an evolution strategy, then in each iteration, the evolutionary framework will select one of the strategies and combine it into the user prompt and input it into the LLM. If"}, {"title": "3.1.6 A Detailed Example", "content": "Example 3.2. Code specification 1 demonstrates an example of our AMD-LLM workflow for solving Myerson's optimal single-item auction design problem [Myerson, 1981], with the additional complexity of allowing bid correlation. In this mechanism design setting, a single item must be deterministically allocated to one of the bidders, or left unallocated. The bidders' valuations can be correlated. The mechanism designer aims to maximize the expected total revenue. The design criteria include feasibility (i.e., no over allocation), strategy-proofness, and ex post individual rationality.\n(1) We first identify the key part of the mechanism and set it as the heuristic function that will evolve through FunSearch. The key part we choose in Code Specification 1 is the allocation function; see lines 36-38. The input of the heuristic function consists of the bidders' bids (a vector of length $n$ where $n$ is the number of bidders). The output of the heuristic function is the allocation function, which is a vector of length $n + 1$ with each element in $[0, 1]$; if the $i$-th $(1 \\leq i \\leq n)$ index of the output vector is the largest, then allocate the item to the $i$-th bidder; if the $(n + 1)$-th index is the largest, then do not allocate to any bidder.\n(2) We need to design a fixing process that ensures all the design criteria. Given a heuristic function (which serves as the allocation function), we need to ensure its monotonicity, which is required for strategy-proofness based on Myerson's characterization. Recall that monotonicity refers to the property that the winner must still win if she increases her bid while the other bids stay the same. We note that under a strategy-proof and individually rational mechanism, a bidder essentially faces a take-it-or-leave-it offer, sometimes referred to as the critical price, which depends on the other bids. Let the winner be bidder $m$. One way to fix a non-monotone allocation is to increase the critical price faced by the winner to the lowest price $p_m$ that satisfies $\\forall b'_m \\geq p_m$, we have $\\text{argmax heuristic}(b'_m, b_{-m}) = m$. The revenue is $p_m$ if bidder $m$ can afford to pay this price, or 0 otherwise. (All other bidders pay nothing as they don't win.) The above fixing process guarantees monotonicity. It is shown in code in lines 19-34 in Code Specification 1.\n(3) For the evaluation function, we sample a batch of bid vectors from the distribution, compute the average revenue of the fixed generated mechanism, and use it as the score for the FunSearch evolutionary update; see lines 3-12.\nWith this code specification, our AMD framework can call the LLM iteratively to evolve the heuristic and evaluate each generated heuristic according to the workflow shown in Sec. 3.1.3."}, {"title": "3.2 Neural Network Assisted Automated Mechanism Design Framework with LLM (NNA-AMD-LLM)", "content": "This section integrates the existing differential economics methods \u2013 neural network mechanism de-sign, such as RegretNet [D\u00fctting et al., 2024] \u2013 into our AMD-LLM framework. Neural networks are often capable of discovering highly effective mechanisms. However, they often lack interpretability, as the deliverables are black-box networks involving intricate arrangements of weights and biases. Building on the AMD-LLM framework introduced in Sec. 3.1, we incorporate an additional RegretNet, which is trained separately in advance to be used as input to our framework, serving as the \"goal\u201d function. Here, the fitness of an LLM-generated heuristic refers to its similarity to the trained RegretNet. The evolutionary process minimizes the difference between the outputs of the LLM-generated heuristic and the trained RegretNet on the same batch of inputs, guiding the generated heuristics to approximate the trained RegretNet. The end result is a Python code segment that serves as an interpretable approximation of RegretNet's black-box solution. This aligns with the Program-by-Example (PBE) technique. Here, the \u201cexamples\u201d are the input-output mappings of RegretNet, and the evolutionary algorithm acts as the program synthesizer. By iteratively generating code candidates and comparing their outputs to RegretNet's, the process distills the neural network's implicit logic into explicit, human-readable Python code. Unlike traditional PBE, which often uses deductive methods, this approach leverages evolutionary search to navigate the vast space of possible programs. The result is a programmatic explanation of RegretNet, bypassing its opaque internal computations while preserving its functional essence.\nSee Fig. 1 (following the \u201cwith NN\u201d branch) for the workflow of NNA-AMD-LLM."}, {"title": "4 An Initial Application \u2013 Rediscovering Virtual Valuations", "content": "We start with a simple setting to see if our framework is effective enough to rediscover an existing mechanism with moderate complexity. We choose Myerson's single-item auction setting, where the optimal auction has been derived by Myerson [1981]. There is a single item for sale and there are multiple bidders with independent private valuations $v_i$. The seller wants to design a deterministic,"}, {"title": "5 An Application to Social Welfare Maximization Problems", "content": "We apply our framework to welfare-maximizing mechanism design. As an example, we tackle the problem of finding optimal VCG redistribution mechanisms with multiple identical items."}, {"title": "5.1 Optimal VCG redistribution mechanism", "content": "We consider the setting of allocating one or more items among several agents. For this setting, one well-known mechanism with desirable properties is the Vickrey-Clarke-Groves (VCG) mechanism, which is efficient, strategy-proof, individually rational, and does not incur a deficit [Clarke, 1971, Groves, 1973, Vickrey, 1961]. However, it is not strongly budget balanced, i.e., it incurs a positive total payment from the agents in general. When there is no party to receive this payment (such as a seller), then this payment is \u201cburned\u201d in the VCG mechanism. The aim of a VCG redistribution mechanism is to redistribute some portion of the collected VCG payments back to the agents, while maintaining the desirable properties of VCG mechanism.\u00b9\u00b2 We require that agent $i$'s redistribution amount received be independent of her own bid to ensure strategy-proofness. In other words, agent $i$'s redistribution takes the form of $r_i(b_{-i})$, where $r_i$ is referred to as the redistribution function and $b_{-i}$ denotes the other agents' bids. For anonymous settings, we omit the subscript and simply use $r(b_i)$ to denote the redistribution amount. The function $r$ is what we aim to evolve using LLM. Our objective is to get as close to budget balance as possible without ever incurring a deficit [Cavallo, 2008, Guo, 2011, 2016, Guo and Conitzer, 2007, 2008a]. In this paper's setting, we aim to redistribute as much as we can in expectation (i.e., maximizing $\\sum_{i} r(b_{-i})$ in expectation), while ensuring that, for all bid profiles, $\\sum_{i} r(b_{-i})$ is never more than the total VCG payment.\nThe design criteria for this problem include strategy-proofness (SP), individual rationality (IR), feasibility, and weakly budget balance (WBB). The design objective is to maximize the expected total redistribution while satisfying these criteria.\nWe consider the case where there are multiple bidders and multiple units of an item, and each bidder wants at most one unit of the item."}, {"title": "5.2 Implementation and the Waterfilling Fix", "content": "We designed specifications for the VCG redistribution mechanism setting described in the previous paragraph. We perform a fix to ensure that any mechanism generated by an LLM in our framework meets all design criteria after the fix.\n\u2022 In our VCG redistribution mechanism, the individual rationality criterion is satisfied by setting the amount redistributed back to each bidder to be nonnegative.\n\u2022 Strategy-proofness is guaranteed by limiting the redistribution function to depend only on the bids of other bidders, not on the bidder's own bid. This restriction guarantees that a bidder manipulating his or her own bid will not influence the redistribution value he or she received, eliminating incentives for strategic manipulation.\n\u2022 No item being allocated to multiple bidders is immediately enforced by using the VCG mechanism, which inherently ensures valid allocations.\n\u2022 To ensure that the total payment (weakly) exceeds the total redistribution, we apply a \"waterfilling\"-style post-processing fix."}, {"title": "Therefore, the fixed redistribution value equals the original redistribution value given by the heuristic function, minus this maximum fix value over all possible bids, i.e.,", "content": "fix_i(b\u2212i) = \\text{heuristic}(b_i) - \\max_{Vb \\in [0,1]} wf(b, b-i).        (1)\nPROPOSITION 5.1. After applying the (further corrected) waterfilling fix, the fixed version of our redistribution mechanism with any LLM-generated heuristic function becomes feasible, individually rational (IR), strategy-proof (SP), and weakly budget balanced (WBB). The resulting fixed mechanism has a redistribution value fixed_redistri, defined in Eq. (1).\nPROOF. We consider the properties respectively.\n\u2022 The (further corrected) fix ensures that the payment for bidder $i$ is independent of bidder $i$'s bid, and thus it maintains the strategy-proofness property of the original redistribution mechanism (the mechanism before the fix).\n\u2022 IR is preserved because in the waterfilling process, the maximum amount of the fix is just the redistribution value from the original mechanism, and thus this will not lead to a negative redistribution.\n\u2022 Feasibility is guaranteed by the VCG mechanism we use, and the waterfilling fix will not change the allocation function.\n\u2022 WBB is met if the total redistribution after the fix is no larger than the total payment. We only need to consider the case where there is a positive fix. In such a case,\n\\sum_{i} wf(b_i, b_{-i}) \\leq \\sum_{i} \\max_{Vb \\in [0,1]} wf(b, b-i) = \\sum_{i} (\\text{heuristic}(b_i) \u2013 \\text{fixed redistri}_{i}(b_{-i})).\nCombined with the facts that Total Redistribution before Fix - Total Payment = $\\sum_{i} wf(b_i, b_i)$, and Total Redistribution before Fix = $\\sum_{i} \\text{heuristic}(b_i)$, we can conclude that\n$\\sum_{i} \\text{fixed redistri}_{i}(b_i) \\leq \\text{Total Payment}$."}, {"title": "6 An Application to Revenue Maximization Problems", "content": "We now move on to another application: maximizing revenue, specifically in the context of corre-lated bidders."}, {"title": "6.1 Single-item auction with multiple correlated bidders", "content": "In this auction, multiple correlated buyers compete for a single item. This setting is more complex than the independent private values model and can lead to different optimal auction designs.\nWe assume that bidders' valuations are drawn from a joint probability distribution. That is, the bidders' valuations are correlated. The optimal auction design can be significantly different from the independent case, because the seller may be able to extract more revenue by leveraging the correlation information. As a result, standard auction formats like second-price auctions are no longer optimal in general [Dobzinski et al., 2015, Dughmi et al., 2014, Morgenstern and Roughgarden, 2016]. In fact, Papadimitriou and Pierrakos [2011] showed that optimal single-item auction design under correlation is NP-hard to approximate."}, {"title": "6.2 Implementation and the Monotonicity Fix", "content": "We experiment in the revenue-maximizing auction setting with 1 item and 2 bidders, where the two bidders' values are correlated. The heuristic function's input consists of the bidders' bids (a vector of length 2). Its output is the allocation function, which is a vector of length 3, where each element should be in range [0, 1]: if the first index of the output vector is the largest, then allocate the item to the first bidder; if the second index is the largest, then allocate to the second bidder; if the third index is the largest, then do not allocate to any bidder. The goal of the mechanism designer is to maximize the revenue while ensuring feasibility, strategy-proofness, and IR. We use the same pre-set evolution strategies as in Sec. 5.3. We use a grid-like correlated distribution in the experiments, detailed in Appendix C. The experiments in this section use the DeepSeek-V3 model. For AMD-LLM, the score used in the evolutionary process is the average total payment. The specification code we use is shown in Code Specification 1. For NNA-AMD-LLM, the score used in the evolutionary process is the average L2 difference between the output of the heuristic function and the output of the trained RegretNet.\nThe fixing process. In this section, we use the monotonicity fix along with the corresponding payment function derived from the allocation heuristic explained in Example 3.2. This fix ensures that the allocation function is monotone in each bidder's valuation, and thus ensures SP."}, {"title": "6.3 Experimental Results", "content": "In the correlated-bidders auction setting, the experiments demonstrate that our frameworks achieve competitive performance while providing interpretable solutions. The AMD-LLM frameworks, both with and without NN assistance, perform on par with Myerson's mechanism with ironing, a well-established and fairly intricate mechanism (noting that ironing involves finding the convex hull of the revenue-demand curve); see Table 2. Notably, while neural-network-based methods (RegretNet) achieve slightly higher performance, they lack interpretability. In contrast, our frameworks generate mechanisms that are both interpretable and near-optimal, enabling a deeper understanding of the underlying logic and making them more practical for real-world applications. This balance of performance and transparency highlights the strength of our approaches in delivering efficient and trustworthy solutions for mechanism design. The evolution dynamics is similar to the one in Fig. 2. Details are included in Appendix C.\nInsights given by LLM-generated heuristics during evolution. The following is one LLM-generated heuristic at iteration 95 in the experiment using NNA-AMD-LLM. It shows that the LLM has learned that the mechanism may choose not to allocate in certain cases when doing so is unprofitable for the seller. The allocation function derived from the following LLM generated heuristic (with sigmoid function) is effectively equivalent to the reserve-price auction with a reserve price of 0.5. However, this LLM generated heuristic outperforms the corresponding reserve-price mechanism in"}, {"title": "7 Conclusion and Future Work", "content": "In this paper, we introduced a novel framework that reformulates mechanism design as a code generation task, leveraging LLMs to automate mechanism design. By generating and evolving heuristic mechanisms described in code, with the additional problem-specific fixing process, our approach bridges the gap between traditional analytical methods and modern automated techniques. This framework ensures that the generated mechanisms are interpretable, and adhere to key design criteria such as feasibility and strategy-proofness. Our experiments demonstrated that LLM-generated mechanisms achieve competitive performance while offering greater interpretability compared to existing neural-network-based approaches. Notably, our framework rediscovered well-known mechanisms, such as Myerson's optimal auction, and provided insights into NN-based solutions like RegretNet. This highlights the potential of LLMs to automate mechanism design while enhancing the transparency and interpretability of the generated mechanisms. The effectiveness of the methodology is likely to increase as LLMs and associated techniques improve, thereby also being able to scale to societal-scale problems that are inaccessible to traditional mechanism design. Meanwhile, unlike alternative neural-network-based approaches, the mechanisms produced are more interpretable and thereby easier to analyze, allowing us to ensure that their deployment is safe.\nThere are several avenues for future research. As mentioned in the experimental sections, LLM-generated heuristics often rely heavily on functions that are commonly used in AI, which may not be"}]}