{"title": "FairPFN: Transformers Can do Counterfactual Fairness", "authors": ["Jake Robertson", "Noah Hollmann", "Noor Awad", "Frank Hutter"], "abstract": "Machine Learning systems are increasingly prevalent across healthcare, law enforcement, and finance but often operate on historical data, which may carry biases against certain demographic groups. Causal and counterfactual fairness provides an intuitive way to define fairness that closely aligns with legal standards. Despite its theoretical benefits, counterfactual fairness comes with several practical limitations, largely related to the reliance on domain knowledge and approximate causal discovery techniques in constructing a causal model. In this study, we take a fresh perspective on counterfactually fair prediction, building upon recent work in in-context-learning (ICL) and prior-fitted networks (PFNs) to learn a transformer called FairPFN. This model is pre-trained using synthetic fairness data to eliminate the causal effects of protected attributes directly from observational data, removing the requirement of access to the correct causal model in practice. In our experiments, we thoroughly assess the effectiveness of FairPFN in eliminating the causal impact of protected attributes on a series of synthetic case studies and real-world datasets. Our findings pave the way for a new and promising research area: transformers for causal and counterfactual fairness.", "sections": [{"title": "1. Introduction", "content": "Algorithmic bias is one of the most pressing AI-related risks, arising when ML-assisted decisions produce discriminatory outcomes towards historically underprivileged demographic groups (Angwin et al., 2016). Despite the topic of fairness receiving significant attention in the ML community, various critics from outside the fairness community argue that statistical measures of fairness and current methods to optimize them are largely misguided in terms of their context-dependence and transferability to effective legislation. Recent work in causal fairness has proposed the popular notion of counterfactual fairness, which provides the intuition that outcomes are the same in the real world as in the counterfactual world where protected attributes \u2013 such as gender, ethnicity, or sexual orientation - take on a different value. According to a recent review contrasting observational and causal fairness metrics (Castelnovo et al., 2022), the non-identifiability of causal models from observational data (Peters et al., 2012) presents a significant challenge in applying causal fairness in practice, as causal mechanisms are often complex due to the intricate nature of bias in real-world datasets. If causal model assumptions are incorrect - for example, when a covariate is assumed not to be influenced by a protected attribute when in fact it is - proposing the wrong causal graph can provide a false sense of security and trust (Ma et al., 2023).\nIn this study, we introduce a novel approach to counterfactual fairness based on the recently proposed TabPFN. Our transformer-based approach coined FairPFN, is pre-trained on a synthetic benchmark of causally generated data and learns to identify and remove the causal effect of protected attributes. In our experimental results across a series of synthetic case-studies and real-world datasets, we demonstrate the effectiveness, flexibility, and extensibility of transformers for causal and counterfactual fairness."}, {"title": "2. Background", "content": "Algorithmic Fairness Algorithmic bias occurs when past discrimination against a demographic group such as ethnicity or sex is reflected in the training data of an ML algorithm. In such cases, ML algorithms are well known to reproduce and even amplify this bias in their predictions (Barocas et al., 2023). Fairness as a topic of research concerns the measurement of algorithmic bias and the development of principled methods that produce non-discriminatory predicted outcomes.\nCausal Fairness Analysis Causal ML is a new and emerging research field that aims to represent data-generating processes and prediction problems in the language of causality, offering support for causal modeling, mediation analysis, and counterfactual explanations. The Causal Fairness Anal-\nysis (CFA) framework (Plecko & Bareinboim, 2022) draws parallels between causal modeling and legal doctrines of direct and indirect discrimination. By categorizing variables into protected attributes A, mediators $X_{med}$, confounders $X_{con f}$, and outcomes Y, the CFA defines the Fairness Cookbook of causal fairness metrics: the Direct Effect (DE), Indirect Effect (IE), and Spurious Effect (SE). These metrics facilitate mediation analysis to assess the remaining causal effects of various bias-mitigation approaches.\nCounterfactual Fairness A related causal concept of fairness is counterfactual fairness (Kusner et al., 2017), which requires that outcomes remain the same in both the real world and a counterfactual world where a protected attribute assumes a different value. Given a causal graph, counterfactual fairness can be obtained either by fitting to observable non-descendants (Level-One), the inferred values of an exogenous unobserved variable (Level-Two) or the noise terms of an Additive Noise Model for observable variables (Level-Three). Counterfactual fairness has gained significant popularity in the fairness community, inspiring recent work on path-specific extensions (Peters et al., 2014) and the application of Variational Autoencoders (VAEs) to achieve counterfactually fair latent representations (Ma et al., 2023).\nA key challenge in the CFA, counterfactual fairness, and causal ML, in general, is the assumption regarding the prior knowledge of causal graphs and models, which relies heavily on domain knowledge and approximate causal discovery techniques. In the context of fairness, (Castelnovo et al., 2022) argue that it is challenging to obtain causal graphs representing complex systemic inequalities. Additionally, (Ma et al., 2023) demonstrate that proposing an incorrect causal graph or model can deteriorate counterfactual fairness and potentially lead to adverse impacts (e.g. fairwashing) if the causal relationships between protected attributes and other variables are incorrectly assumed.\nPrior-Fitted Networks Prior-Fitted Networks (PFNs) are a recent approach to incorporating prior knowledge into neural networks via pre-training on datasets sampled from a prior distribution (M\u00fcller et al., 2021). This allows PFNs to perform well on downstream tasks with limited data.\nTabPFN (Hollmann et al., 2022), a recent application of PFNs to small, tabular classification problems, trains a transformer on a hypothesis of synthetic datasets generated from sparse SCMs, achieving state-of-the-art results by integrating over the simplest causal explanations for the data in a single forward pass of the network."}, {"title": "3. Methodology", "content": "In this section, we introduce FairPFN, a novel bias mitigation technique that synergizes concepts from prior-fitted networks (PFNs) with principles of causal and counterfactual fairness. FairPFN aims to eliminate the causal and counterfactual effects of protected attributes using only observational data.\nSynthetic Prior Data Generation The main methodological contribution of FairPFN is its fairness prior, designed to represent the causal mechanisms of bias in real-world data. FairPFN's fairness prior includes a key addition to the TabPFN hypothesis space, namely the inclusion and specification of protected attributes in the randomly generated SCMs as exogenous variables.\nThe first step of FairPFN is the generation of biased synthetic datasets that realistically represent the causal mechanisms of bias in real-world datasets. We provide a visual overview of this process in (Figure 1). Taking inspiration from TabPFN, we represent SCMs as Multi-Layer-Perceptrons (MLPs) with linear layers serving to represent the structural equation $f = P.W^T x + \\epsilon$ where W are the weights of the activations, $\\epsilon$ is Gaussian Noise, and P is a dropout mask sampled from a log-scale to encourage sparsity of the represented SCM.\nThe exogenous protected attribute is sampled from the input to the MLP as a binary variable $A \\in \\{a_0, a_1\\}$ where $a_i$ is sampled from the same range as non-protected exogenous variables $U_{fair}$ to prevent numeric overflow. We uniformly sample m features X from the second hidden layer on to ensure that they contain rich representations of the causes. Finally, we select the target Y from the output layer. Because Y is a continuous variable, we binarize over a random threshold. We note that without binarizing, future versions of FairPFN are extensible to regression tasks and handling multiple protected attributes.\nVia a forward pass of the MLP, we generate a dataset $D_{bias} = (A, X_{bias}, Y_{bias})$ of n samples and repeat this process throughout training on randomly sampled SCMs, number of features, and number of samples to generate a rich synthetic representation of real-world, biased data.\nFairPFN Pre-training The strategy by which we pre-train the transformer to perform counterfactual fairness is by generating two datasets, $D_{bias}$ and $D_{fair}$. The fair dataset is generated by performing dropout on the outgoing edges of the protected attribute in the sampled MLP. This has the effect of setting the weight to 0 in the represented equation $f = 0. wx + \\epsilon$, meaning that the effect of the protected attribute is reduced to Gaussian noise $\\epsilon$ as visualized in Figure 11. Having generated two datasets, we pass in $D_{bias}$ as context to the transformer, and calculate the loss with respect to the transformer's predictions and the fair outcomes $Y_{fair}$ (Figure 4). It's worth noting that we simply discard $X_{fair}$ in this strategy, but discuss how it could be applied to train FairPFN to be a fairness pre-processsing technique in Section 5.\nFairness Prior-Fitting We train the transformer for approximately 3 days on an RTX-2080 GPU. Throughout training, we vary several hyperparameters, including the size and connectivity of the MLPs, the number of features sampled, and the number of dataset samples generated. To calculate the loss between the predicted and ground truth values of $Y_{fair}$ classification setting, we apply Binary-Cross-Entropy (BCE) loss and a decaying learning rate schedule."}, {"title": "4. Results", "content": "In this section, we evaluate the performance of FairPFN on our benchmark of synthetic and real-world scenarios, with the key message that FairPFN removes the causal and counterfactual effect of protected attributes without any knowledge of the causal model.\nSynthetic Data First, we evaluate FairPFN on our synthetic causal case studies, by visualizing the change in causal effect (DE, IE, or TE) before and after bias-mitigation with FairPFN (Figure 5), with a color gradient of blue to green to represent the increasing amount of noise in each dataset. We observe across all case studies that FairPFN learns to remove the causal effect of the protected attribute with a small variance and highlight two interesting effects.\nFirst, we observe on 5 out of 6 case studies that datasets with higher noise levels can generally be solved while maintaining a lower level of error. This could be due to 1) the lower $Unfair_{TCE}$ in these datasets or 2) the increased identifiability of SCMs with noise and non-linearity (Peters et al., 2014). Additionally, we find that on the Biased case study, FairPFN often achieves an error (1-AUC) less than 0.5. This suggests that FairPFN does not revert to a random classifier when data is only causally influenced by protected attributes as there is still fair information (namely $\\epsilon_X$ and $\\epsilon_Y$) in the data. Instead, FairPFN removes only the causal effect $W_A A^2$ in the corresponding structural equation, allowing the noise terms $\\epsilon_X$ and $\\epsilon_Y$ to influence its predictions.\nWe also observe in Figure 4 that FairPFN dominates EGR in 5 out of 6 case studies, is on the Pareto Front in all 6, and always improves in terms of predictive performance compared to CFP. This is likely attributed to the effect observed in Figure 4 on the Biased case study, where FairPFN learns to remove only the causal effect of the protected attribute, still allowing all remaining information to influence its predictions.\nReal-World Data We also evaluate FairPFN on the Law School Admissions and Adult Census Income datasets, using causal models fit to the structures posed in Figure to measure the TE and MAE 3. We note again that in evaluation FairPFN receives no information about the causal graphs or models. In Figure 6, we measure the causal effect across different baselines, observing that FairPFN shows significant improvement in terms of TCE compared to the Unfair and Unaware baselines. It also demonstrates competative TCE and improved error on the Law School dataset compared to the CFP baselines On the Adult dataset, FairPFN is outperformed by the CFP baseline, which achieves dominating TCE and error. This outcome is likely explained by the fact that the $Unfair_{TCE}$ on the Adult dataset is already quite small (0.03), and thus the four fair noise terms in Figure 3 have a relatively higher representative capacity than in the Law School problem. However, FairPFN still reduces the TCE to less than 0.01, a very acceptable outcome in the broader scope of the problem.\nIn Figure 7, we also measure the MAE between the predictive distributions on the real and counterfactual datasets, $Y_{real}$ and $Y_{aa'}$. We observe that FairPFN achieves competitive MAE with CFP in both scenarios, learning to make counterfactually fair predictions without having access to the causal model or graph. We note that interestingly, EGR performs similarly poorly to Random in both scenarios, aligning with the intuition that randomization is not a counterfactually fair strategy as individuals do not receive consistent outcomes in either the real or counterfactual worlds."}, {"title": "5. Future Work & Discussion", "content": "In this study, we introduce FairPFN, a novel bias-mitigation technique that learns a pre-trained transformer to remove the causal effect of protected attributes in fairness-aware binary classification problems from observational data alone. FairPFN addresses a key limitation in the causal fairness literature by eliminating the need for prior knowledge of the true causal graph in fairness datasets, making it easier for practitioners to apply counterfactual fairness to complex problems where the underlying causal model is unknown. This expands the scope and applicability of causal fairness techniques, enabling their use in a broader range of scenarios. Looking ahead, we believe that FairPFN opens the door to several promising avenues of research.\nReal-World Evaluation A crucial next step in FairPFN would be to train a module to predict the effect of interventions on the protected attribute, producing counterfactual datasets to evaluate on. Doing so with FairPFN could hold advantages in robustness as compared to using causal discovery techniques such as (Lorch et al., 2022), since our pre-trained transformer integrates over the possible causal explanations for the data.\nTransparency and Interpretability In cases where a causal graph or a subset of causal relationships are known, incorporating this domain knowledge as additional input to the transformer could enhance both FairPFN's human-centricity and performance. Additionally, a future direction could involve predicting the causal graphs that explain the data, adding an extra layer of interpretability.\nFairness Preprocessing By modifying FairPFN's output to predict not only fair outcomes but also fair versions of observational variables, we can improve interpretability and transparency while allowing practitioners to use their preferred ML model during deployment. FairPFN could also be repurposed as a generative model to create fair training data, increasing the performance of the selected model.\nBusiness Necessity Incorporating these business-necessity from (Plecko & Bareinboim, 2022) variables into our fairness prior could enable specifying variables through which to allow the causal effect of the protected attribute. This extension is similar to path-specific counterfactual (Peters et al., 2014), which would also open up many more application areas, such as medical diagnosis, where the social effects of protected attributes like sex should be removed, yet their biological effects must be preserved to provide individualized treatment."}, {"title": "A. Baseline Models", "content": "To compare FairPFN to a diverse set of traditional, causal-fairness, and fairness-aware ML algorithms, we also implement several baselines which we summarize below:\n\u2022 Unfair: A TabPFNClassifier is fit the entire dataset (X, A, y)\n\u2022 Unaware: A TabPFNClassifier is fit to non-protected attributes (X, y)\n\u2022 Constant: A \"classifier\" that always predicts the majority class\n\u2022 Random: A \"classifier\" that randomly predicts the target\n\u2022 Level-One: A TabPFNClassifier is fit to non-descendant observables of the protected attribute ($X_{fair}$, y) if any exist\n\u2022 Level-Two: A TabPFNClassifier is fit to non-descendant unobservables of the protected attribute ($U_{fair}$, y) if any exist\n\u2022 Level-Three: A TabPFNClassifier is fit to noise terms of observables ($\\epsilon$, y) if any exist\n\u2022 EGR: Exponentiated Gradient Reduction (EGR) for fairness metric DP as proposed by (Agarwal et al., 2018)\nWe note that these baselines are specifically designed to provide ground truths of the best and worst that can be done in terms of fairness metrics and that certain baselines are only applicable to certain datasets. For example Unfair, Unaware, Random, Constant, and EGR are applicable on all synthetic and real-world datasets. Level-One is only applicable to Direct Effect, Indirect Effect synthetic causal case studies. Level-Two is additionally applicable to the Level-Two synthetic case study, and Level-Three is additionally applicable to the Level-Three synthetic case study as well as the real-world datasets where the causal model is known and noise terms $\\epsilon$ can be estimated."}]}