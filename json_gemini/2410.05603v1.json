{"title": "EVERYTHING EVERYWHERE ALL AT ONCE: LLMS CAN IN-CONTEXT LEARN MULTIPLE TASKS IN SUPERPOSITION", "authors": ["Zheyang Xiong", "Ziyang Cai", "John Cooper", "Albert Gew", "Vasilis Papageorgiou", "Zack Sifakis", "Angeliki Giannou", "Ziqian Lin", "Liu Yang", "Saurabh Agarwal", "Grigorios G Chrysos", "Samet Oymak", "Kangwook Lee", "Dimitris Papailiopoulos"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable in-context learning (ICL) capabilities. In this study, we explore a surprising phenomenon related to ICL: LLMs can perform multiple, computationally distinct ICL tasks simultaneously, during a single inference call, a capability we term \u201ctask superposition\". We provide empirical evidence of this phenomenon across various LLM families and scales and show that this phenomenon emerges even if we train the model to in-context learn one task at a time. We offer theoretical explanations that this capability is well within the expressive power of transformers. We also explore how LLMs internally compose task vectors during superposition. Furthermore, we show that larger models can solve more ICL tasks in parallel, and better calibrate their output distribution. Our findings offer insights into the latent capabilities of LLMs, further substantiate the perspective of \"LLMs as superposition of simulators\u201d, and raise questions about the mechanisms enabling simultaneous task execution.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, with one of the most intriguing being in-context learning (ICL). ICL enables LLMs to perform tasks during inference without the need to fine-tune for that particular task, simply by providing a few examples within the input prompt. This ability has sparked significant interest in the research community, as it suggests that LLMs can adapt to novel tasks on-the-fly, using the capabilities that they acquired during pretraining, and the context provided.\nWhile ICL has been extensively studied from both theoretical and empirical perspectives, many aspects of its underlying mechanisms remain elusive. In this work, we study a surprising phenomenon related to ICL that, to the best of our knowledge, has not been thoroughly studied before: LLMs can perform multiple distinct ICL tasks simultaneously, in a single inference call, a capability we refer to as \"task superposition\u201d.\nOur study suggests that pretrained autoregressive LLMs such as Llama (Touvron et al., 2023) or GPT-3.5 display superposition of tasks purely in-context. When presented with multiple in-context examples from different tasks, in the same prompt, the models can generate outputs that correspond to solutions for all these individual tasks. For instance, given examples of addition and translation, the model can concurrently produce correct answers for both tasks, as well as the composition of these tasks (e.g., the result of addition translated into another language).\nFigure 1 illustrates this phenomenon. In Figure 1a (left), given in-context examples of addition in different languages and the query \"91 + 83 \u2192\", the model generates probabilities for the correct sum in various languages, demonstrating its ability to perform addition and translation concurrently.\nWhile not a mathematically rigorous formulation, we can conceptualize the output of an LLM as a weighted sum of conditional probabilities across possible tasks:\nP(output prompt) \u2248 \u2211task P(output task, prompt)P(task|prompt).\nIn this conceptual model, P(output prompt) represents the probability distribution over possible outputs given the input prompt, a task can be thought of as a latent variable representing different capabilities the model might possess (e.g., arithmetic, translation, sentiment analysis), P(output task, prompt) represents the output probability distribution if the model was specifically attempting to solve a single task, based on the test example in the prompt, and P(task|prompt) represents the model's inferred probability that the prompt specifies a particular task.\nAlthough this mental model is an over-simplification of how an LLM operates, it offers a clean conceptual framework for the task superposition phenomenon we observe. Our findings lend support to the idea that LLMs can simultaneously maintain and utilize multiple task distributions, resulting in outputs that reflect a combination of relevant tasks.\nOur Contributions: Our study makes several key contributions:"}, {"title": "2 RELATED WORK", "content": "Theory and practice of in-context learning. There is rich literature which formalizes in-context learning under diverse definitions. For example, prior works study in-context learning through a Bayesian framework for task retrieval (Xie et al., 2022; Panwar et al., 2023; Zhang et al., 2023), martingales (Falck et al., 2024), optimizers (Aky\u00fcrek et al., 2023; Oswald et al., 2023; Dai et al., 2022) and more (Reddy, 2024; Olsson et al., 2022). Other works confirm the theoretical framing of in-context learning by using it to implement a variety of algorithms and methods (Zhou et al., 2023; Ahn et al., 2023; Giannou et al., 2023; Wu et al., 2024; Laskin et al., 2022; Zhou et al., 2022), or to approximate general-purpose computing machines (Giannou et al., 2023; Wei et al., 2022).\nTo bridge the gap between theory and practice, many works have used these theoretical insights to study in-context learning behaviors, such as in many-shot in-context learning, (Agarwal et al., 2024), long-context (Li et al., 2024), or eliciting personas (Choi & Li, 2024). Other works study the factors that influence how well models can learn through context, such as task diversity (Raventos et al., 2023; Chan et al., 2022), the balance between pre-training priors and in-context (Wei et al., 2023; Lin & Lee, 2024), in-context labels (Min et al., 2022; Lyu et al., 2022), and the in-context format (Lampinen et al., 2022). In-context learning has also been proposed as a means of fine-tuning to improve non-language tasks (Dinh et al., 2022).\nThe development of new architectures such as state space models (Gu & Dao, 2023) has further motivated studying whether in-context learning is prevalent in alternative architectures such as Mamba (Park et al., 2024; Grazzi et al., 2024; Zeng et al., 2024) or in looped transformers (Yang et al., 2023).\nSteering models through in-context learning has been a growing area of interest. Recent work has hypothesized that in-context learning can be encapsulated by a high-dimensional description of a task, which can be used to replace, (Hendel et al., 2023) compose (Todd et al., 2024) or augment (Liu et al., 2024) the latent states of a model, in order to alter its default behavior. Task vectors can be combined via arithmetic operations to solve a variety of tasks (Ilharco et al., 2023). Prior work has also been investigating the power of tokens in defining a task (Bai et al., 2024).\nOther definitions of superposition. Our findings on superposition are inspired by notions of language models as multiverse generators (Reynolds & McDonell, 2021; moire, 2021). One consequence of LLMs as a superposition of tasks is that the outputs may collapse to unintended simulacra, a behavior known as the \"Waluigi effect\" (Nardo, 2023).\nSuperposition has been defined in various related contexts of learning models. Feature superposition (Elhage et al., 2022) refers to a neural network's ability to represent multiple learned concept in a single neuron. Though our discovery of task superposition describes the same abstract idea, we stress that it is distinct from feature superposition because task superposition is most apparent in the final output of a model. Feature superposition is a microscopic-level observation whereas task superposition is a macroscopic-level observation.\nSuperposition is also described as a way to store multiple models in a single set of parameters (Cheung et al., 2019), processing multiple inputs simultaneously (Shen et al., 2024a; Murahari et al.,"}, {"title": "3 LLMS ARE A SUPERPOSITION OF MULTIPLE IN-CONTEXT LEARNERS", "content": "In this section, we want to investigate if existing pre-trained models exhibit superposition of multiple tasks and whether this phenomenon is common (i.e., whether we can observe this phenomenon on a variety set of tasks and different families of LLMs)."}, {"title": "4 TASK SUPERPOSITION IN MODELS TRAINED FROM SCRATCH", "content": "In Section 3 we investigated task superposition in pre-trained LLMs at inference time. In this section, we further investigate how task superposition emerges in LLMs during training. Specifically, if we train the model to in-context learn one task at a time, can it perform task superposition when provided with prompts containing examples of multiple tasks?\nTo answer this question, we train a small GPT-2 model (12 heads, 12 layers and ~86 million parameters) (Radford et al., 2019) to learn a family of retrieval tasks. The input has the form \"{ch1}{ch2}{ch3}{ch4}{ch5}{ch6}{ch7}{ch8}\u2192\" where ch1, ..., ch8 are distinct single characters. We consider 8 retrieval tasks \u2013 ret1, ..., ret8 \u2013 where ret1 is to output ch1 and so on. The model is trained to in-context learn one task (retrieve one of {ch1, ..., ch8}) at a time in training. Namely, during training, the model is only provided with text data such that each prompt only contains in-context examples of a single randomly chosen task (and different prompts can correspond to different tasks).\nConcretely, for each sample, we randomly select task t \u2208 {ret1,...,ret8} and inputs x(1), ..., x(m), where each \u00e6(i) is an eight-character long string. We then form the sequence s = [x(1), gt(x(1)), ..., x(m), gt(x(m))] where gt(x(i)) is the output of performing task t on \u00e6(i). We train the model Me parametrized by @ using ICL training. In particular, we minimize the following objective:\nmin \u03b8 E s { 1 m\u22121 m\u22121 \u2211 j=1 CE(M \u03b8 (sj + x(i+1)), gt(x(j+1))) } , (1)\nwhere sj + x(i+1) = [x(1), gt(x(1)), ..., x(i), gt(x(i)), x(i+1)] and CE is the cross-entropy loss.\nAfter training, we provide the model with prompts containing in-context examples of two tasks (in particular, we choose ret2 and ret 6) and see if the model performs task superposition. We vary the proportion of in-context examples of two tasks and plot the output distributions in Figure 3a.\nSimilarly, we consider a second setting involving 10 tasks. Given a two digit integer input num, task plus0 outputs num, task plus1 outputs num + 1 and so on, up to task plus 9. The model is trained to in-context learn one of plus0,..., plus 9 at a time, following the procedure above. During inference time, the model is tested with prompts containing a mixture of in-context examples from tasks plus2 and plus 6. We vary the mixture ratio and show the output distributions in Figure 3b."}, {"title": "5 TRANSFORMERS HAVE THE CAPACITY TO PERFORM TASK SUPERPOSITION", "content": "In this section, we explore whether Transformers have the inherent expressivity to perform multiple tasks in superposition with a single inference call. To this end, we provide a theoretical construction"}, {"title": "6 TASK SUPERPOSITION THROUGH THE LENS OF TASK VECTORS", "content": "While in Section 5 we provide an existential result by constructing a Transformer that performs task superposition and shows that task superposition is well within the expressive power of Transformers, we would like to further investigate how task superposition manifest in pretrained LLMs internally. In this section we explore the underlying mechanisms that LLMs employ during task superposition. In particular, we focus our empirical study on task vectors (Hendel et al., 2023) where the detailed implementation is in Appendix C. Task vectors are vectors in the embedding space and are found to encode the algorithm that a model internally implements to solve a task given in-context demonstrations."}, {"title": "7 TASK SUPERPOSITION CAPABILITIES AS THE MODEL SCALES", "content": "We want to further investigate how models' task superposition capabilities change as the model size scales. In particular, we investigate two questions: 1) whether larger models can perform more tasks in-context and 2) whether larger models can align their output distribution more closely with the distribution of task examples provided in the prompt. We chose the Qwen-1.5 model family since it contains several model sizes ranging from 0.5B to 14B parameters.\nWe first introduce a quantity which captures the capability of a model to perform multiple tasks. Given a prompt that contains examples of K tasks, we define r to be the number of these tasks whose correct answers appear among the model's top-K most likely outputs. Note that r < K.\nTo see how close the model align the output distribution with the distribution of task examples, we use KL-divergence defined below:\nKL(P||D) = \u2211 x\u2208X P(x) log P(x) D(x) , (2)"}, {"title": "8 LIMITATIONS AND FUTURE DIRECTIONS", "content": "One limitation of our work is the current gap between the demonstrated capability of LLMs to perform task superposition and its practical application in real-world scenarios. While we have shown that LLMs possess the capacity to execute multiple tasks simultaneously, conventional decoding algorithms are not equipped to fully leverage this capability. This limitation stems from what we term \"generation collapse,\" a phenomenon where, after the first token is generated, the model tends to converge on predicting tokens for a single task, effectively negating its ability for multi-task execution.\nThis collapse presents a substantial challenge in harnessing the full power of task superposition. It highlights a critical area for future research: developing decoding strategies that can maintain the model's multi-task state throughout the generation process. Recent work by Shen et al. (2024b) offers some hope that this direction may be fruitful, by proposing a \u201csuperposed decoding\u201d algorithm. Their method efficiently generates multiple streams of tokens from a single inference pass by utilizing superposed token embeddings. While this approach represents a significant step forward, it also highlights the potential for further innovation in this area."}, {"title": "9 CONCLUSION", "content": "We report on the discovery of task superposition, which is the ability of LLMs to simultaneously solve distinct tasks from in-context examples. Task superposition is present in a variety of pretrained models, and becomes more accurate at predicting the distribution of tasks as the model size increases. We also find evidence that while displaying task superposition, models internally mix the task vectors of each individual task. We hope that our findings will contribute to understanding in-context learning mechanisms and enhance our knowledge of LLMs overall."}, {"title": "A NOTATIONS", "content": null}, {"title": "B IMPLEMENTATION DETAILS ON CALCULATING PROBABILITIES", "content": "In this section we provide details on how we calculate probabilities of different outputs given a prompt in our setting.\nLet o be a task answer in string. Let [v1, ..., vM] := tok(I) and let [u1, ..., uN] := tok(o). Then the probability of the task answer o given prompt I can be calculated as\nP(o | [v1, ..., vM]) \u2248 \u220f j=1 N P(uj | [v1, ..., vM, u1, ..., uj\u22121]). (3)"}, {"title": "C IMPLEMENTATION DETAILS ON TASK VECTORS", "content": "We use the task vector definition from Hendel et al. (2023). For example, for task copy (op1) in Figure 1b (left), the procedure to collect the task vector consists of\nl\u2217 = arg max l max acc l , (4)"}, {"title": "D CONSTRUCTION DISPLAYING SUPERPOSITION", "content": "In this section we construct a Transformer that is performing superposition of multiple tasks at inference. For this purpose, we first construct a Transformer that copies from n-tuple in-context examples the i-th one, as well as any function using the ReLU layers. We then create indicator vectors, for each task, which show whether a specific task is present in-context or not. As a last step, we combine these indicator vectors to create the superposition of different tasks. Notice that using the parallel heads of the transformer architecture we can process each task independently until the last step in which the predictions are combined."}, {"title": "D.1 OVERVIEW", "content": "Here we provide a brief overview of how the construction is implemented, while latter we provide the corresponding details."}, {"title": "D.2 TASK IDENTIFICATION", "content": "The first task for performing task superposition based on in-context examples is to define a set of tasks that the model is able to implement."}, {"title": "D.2.1 IDENTIFYING IF TASK'S OUTPUT MATCHES THE IN-CONTEXT EXAMPLE", "content": null}, {"title": "D.3 TASK EXECUTION", "content": null}, {"title": "D.4 SUPERPOSED TASKS WITH PARALLEL HEADS", "content": "The above construction works for a single task, where the output is weighted by the proportions of the task within the context. To complete the construction of a transformer that does superposition of tasks, each of these models needs to be placed within the same overall transformer. This is described here."}], "equations": ["KL(P||D) = \\sum_{x \\in X} P(x) \\log \\frac{P(x)}{D(x)}", "\\min_{\\theta} \\mathbb{E}_{s} { \\frac{1}{m-1} \\sum_{j=1}^{m-1} CE(M_{\\theta}(s_{j} + x^{(i+1)}), g_t(x^{(j+1)})) } ,", "P(\\text{output} | [v_1, ..., v_M]) \\approx \\prod_{j=1}^{N} P(u_j | [v_1, ..., v_M, u_1, ..., u_{j-1}]) .", "||z||_1 = \\sum_{i=1}^{d} \\text{ReLU}(z_i) - \\text{ReLU}(-z_i)", "W_Q = [0 \\space 0 \\space 0]", "W_K = [0 \\space 0 \\space C_I \\space 0]", "W_V = \\begin{bmatrix}  0 & 0 & 0 & 0\\\\ 0 & 1 & 0 & 0\\\\ 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0  \\end{bmatrix}", "(\\mathcal{X} \\space W_Q W_K \\mathcal{X})_{i,j} = p_{n+i}p_{n-s+j}", "\\sigma_s(\\mathcal{X} \\space W_Q W_K \\mathcal{X})_{i,j} = 1_{{n+i = n-s+j}} = 1_{{i = j-s}}", "W_V \\mathcal{X} = \\begin{bmatrix}  * & * & * & * \\\\ \\tilde{g}(x_1^{(i)}) & \\tilde{g}(x_2^{(i)}) &...& \\tilde{g}(y^{(i)})\\\\ & & & \\\\ 0&0&0&0  \\end{bmatrix}", "W_V \\mathcal{X} \\sigma_s(\\mathcal{X} \\space W_Q W_K \\mathcal{X}) = \\begin{bmatrix}  0&0&0&0\\\\ * & * & *& * \\\\ & & & \\\\ 0&0&0&0  \\end{bmatrix}", "\\mathcal{X} + W_V \\mathcal{X} \\sigma_s(\\mathcal{X} \\space W_Q W_K \\mathcal{X}) = \\begin{bmatrix}  * & * & * & * \\\\ 0&0&0&0\\\\ ... & ... & ... & ... \\\\ \\tilde{g}(x_1^{(i)}) & \\tilde{g}(x_2^{(i)}) &...& \\tilde{g}(y^{(i)})\\\\ * & * & *& * \\\\ 1 & 1 & 1&1  \\end{bmatrix}", "\\tilde{g}(b,z) = \\text{ReLU}(b-Cz)", "\\begin{bmatrix}  * & * & * & * \\\\ 0&0&0&0\\\\ ... & ... & ... & ... \\\\ \\tilde{g}(x_1^{(i)}) & \\tilde{g}(x_2^{(i)}) &...& \\tilde{g}(y^{(i)})\\\\  ... & ... & ... & ... \\\\ 1 & 1 & 1&1  \\end{bmatrix} + \\begin{bmatrix}  * & * & * & * \\\\ 0&0&0&0\\\\ ... & ... & ... & ... \\\\ \\tilde{g}(x_1^{(i)}) & \\tilde{g}(x_2^{(i)}) &...& \\tilde{g}(y^{(i)})\\\\  ... & ... & ... & ... \\\\ 1 & 1 & 1&1  \\end{bmatrix}", "W_1\\mathcal{X} \\sigma_s(\\mathcal{X} \\space W_Q W_K \\mathcal{X}) = ... \\begin{bmatrix}  * & * & * & * \\\\ 0&0&0&0\\\\ ... & ... & ... & ... \\\\ \\tilde{g}(x_1^{(i)}) & \\tilde{g}(x_2^{(i)}) &...& \\tilde{g}(y^{(i)})\\\\ ... & ... & ... & ... \\\\ 1 & 1 & 1&1  \\end{bmatrix}", "\\sigma_s(\\mathcal{X} \\space W_Q W_K \\mathcal{X}) \\approx \\begin{bmatrix}  0\\\\ ...\\\\ \\tilde{g}(x_1^{(i)}) \\\\ \\text{fc}(\\|\\tilde{g}(y_i)-\\sigma^{y_i}\\|) = 0 \\\\ \\text{fc}(\\|\\tilde{g}(y_i)-\\sigma^{y_i}\\|)\\\\1&1&1&1\\end{bmatrix}", "x \\gets x + 1 - \\text{ReLU}(x - Cb) - \\text{ReLU}(Cb - C + 1)", "W_Q \\mathcal{X} = \\begin{bmatrix} * \\\\ 0&0 \\\\\\ ...&... \\\\\\ \\tilde{g}(x_1^{(i)}) & \\tilde{g}(x_2^{(i)}) &...& \\tilde{g}(y^{(i)})\\\\ fc(||g(y_i)-\\sigma^{y_i}||)   fc(||g(y_i)-\\sigma^{y_i}||) \\\\ 1&1&1  \\end{bmatrix}", "\\mathcal{X} + W_V \\mathcal{X} W_Q \\mathcal{X} = [ \\frac{-C}{1} ...\\\\  .... \\\\ \\frac{1}{3}  \\\\.... \\\\ \\frac{1}{3}  \\\\... ]", "\\sigma_s(\\mathcal{X} W_Q \\mathcal{X}) = \\begin{bmatrix} 0\\\\ ...  0\\\\ ...  1\\\\ ...  \\\\ ...  \\\\ 1\\\\ ...  \\end{bmatrix}", "W_I \\sigma_s(\\mathcal{X} W_Q \\mathcal{X})= [ \\frac{-1}{1} ...\\\\  .... \\\\ \\frac{1}{3}  \\\\.... \\\\ ... \\\\ \\frac{1}{3}  \\\\ 1\\\\ ... ]", "\\frac{3+1}{2^3} + W_V \\sigma_s(\\mathcal{X} W_Q \\mathcal{X})= [1\\\\ ...\\\\ \\frac{1}{3} f()\\\\ f(2\\\\ \\frac{23}{3} + i \\\\... ]", "\\mathbf{W} = \\text{diag}(W_1, ..., W_T)   \\text{b} = \\begin{bmatrix}b_1 & 11\\\\...\\\\134 T\\end{bmatrix}", "X = \\begin{bmatrix}*\\\\ .012\\frac{m - 2} \\\\\\  34m-1\\end{bmatrix}", "\\alpha = \\begin{bmatrix}c_1 & 11\\\\ .\\\\320T\\end{bmatrix}"]}