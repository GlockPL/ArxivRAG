{"title": "General and Task-Oriented Video Segmentation", "authors": ["Mu Chen", "Liulei Li", "Wenguan Wang", "Ruijie Quan", "Yi Yang"], "abstract": "We present GVSEG, a general video segmentation framework\nfor addressing four different video segmentation tasks (i.e., instance,\nsemantic, panoptic, and exemplar-guided) while maintaining an identical\narchitectural design. Currently, there is a trend towards developing general\nvideo segmentation solutions that can be applied across multiple tasks.\nThis streamlines research endeavors and simplifies deployment. However,\nsuch a highly homogenized framework in current design, where each\nelement maintains uniformity, could overlook the inherent diversity among\ndifferent tasks and lead to suboptimal performance. To tackle this, GvSEG:\ni) provides a holistic disentanglement and modeling for segment targets,\nthoroughly examining them from the perspective of appearance, position,\nand shape, and on this basis, ii) reformulates the query initialization,\nmatching and sampling strategies in alignment with the task-specific\nrequirement. These architecture-agnostic innovations empower GVSEG\nto effectively address each unique task by accommodating the specific\nproperties that characterize them. Extensive experiments on seven gold-\nstandard benchmark datasets demonstrate that GvSEG surpasses all\nexisting specialized/general solutions by a significant margin on four\ndifferent video segmentation tasks.", "sections": [{"title": "1 Introduction", "content": "Identifying target objects and then inferring their spatial locations over time\nin a pixel observation constitute fundamental challenges in computer vision [1].\nDepending on discriminating unique instances or semantics associated with\ntargets, exemplary tasks include: exemplar-guided video segmentation (EVS)\nthat tracks objects with given annotations at the first frame, video instance\nsegmentation (VIS), video semantic segmentation (VSS), and video panoptic\nsegmentation (VPS) which entails the delineation of foreground instance tracklets,\nwhile simultaneously assigning semantic labels to each video pixel. Prevalent\nwork primarily adheres to discrete technical protocols customized for each task,\nshowcasing promising results [2-21]. Nevertheless, these approaches necessitate\nmeticulous architectural designs for each unique task, thereby posing challenges\nin facilitating research endeavors devoting on one task to another. Recently, there\nhave been efforts in shifting the above task-specific paradigm to a general solution"}, {"title": "3 Methodology", "content": "Problem Statement. Video segmentation seeks to partition a video clip V\u2208\n$R^{T\\times H\\times W \\times 3}$ containing T frames of size H\u00d7W into K non-overlap tubes linked\nalong the time axis:\n$\\big\\{Y_k\\big\\}_{k=1}^K = \\big\\{(M_k, C_k)\\big\\}_{k=1}^K,$\nwhere each tube mask $M_k \\in \\{0, 1\\}^{T\\times H\\times W}$ is labeled with a category $c_k \\in \\{1,\\dots ,C'\\}$.\nThe value of K varies across tasks: in VSS, it is consistent with the number of\npredefined semantic categories; in EVS and VIS, it is adjusted in response to the\ninstance count; and in VPS, it is the sum of stuff categories and thing entities.\nTracking by Query Matching. Inspired by the success of query-based object\ndetectors, [4,22,33] propose to associate instances based on the query embeddings.\nSpecifically, given a set of N randomly initialized queries $\\big\\{q_i\\big\\}_{i=1}^N$, we can derive\nthe object-centric representation $\\big\\{a_i\\big\\}_{i=1}^N$ for frame $V_t$ by:\n$\\big\\{q_i^t\\big\\}_{i=1}^N = D\\big(E(V_t), \\big\\{q_i\\big\\}_{i=1}^N\\big),$\nwhere $& and D$ are the Transformer encoder and decoder. Here $a^t_i$ refines rich\nappearance representation for a specific object. The tracking is done by applying\nHungarian Matching on the affinity matrix $S_{ij} = cosine(q_i^t, q_j^{t+1})$ computed\nbetween $a^t_i$ and $a^{t+1}_j$ of two successive frame $V_t$ and $V_{t+1}$. As such, instances\nexhibiting identical attributes across the video sequence are linked automatically."}, {"title": "3.1 GVSEG: Task-Oriented Property Accommodation Framework", "content": "GVSEG seeks to advance general video segmentation through controllable em-\nphasis on instance discrimination and semantic comprehension according to task\nrequirements. Concretely, we first devise a new shape-position descriptor to accu-\nrately reveal the shape and location of targets. Then, by adjusting the engagement\nof above shape-position descriptor during cross-frame query matching, we could\nrealize controllable association for instance and background stuff, respectively.\nFinally, we give an analysis on the limitation of current temporal contrastive\nlearning and devise a task-oriented sampling strategy to tackle encountered issues.\nShape-Position Descriptor. Inspired by shape context [99], a shape-position\ndescriptor is constructed to represent the spatial distribution and shape of\ntarget objects. First, it describes shape cues by encoding the relative geometric\nrelationships of points in object contours relative to the object center. As shown\nin Fig. 2, given the contour $G\\in \\{0,1\\}^{H\\times W}$ of a target object which can be easily\nderived from masks, a set P with M anchor points (i.e., \u2611\u2611) are evenly sampled:\n$P = \\big\\{p_m = (x, y) \\mid G(x, y) = 1, 1 < m < M\\big\\}.$\nAbove anchor points are transformed into polar coordinates with the central\npoint $p_o$ of targets (i.e., \u2729) as the reference point. The polar coordinate is a\nhistogram divided into a grid of uxv bins with u angle divisions and v radius\ndivisions. Next we calculate the number of anchor points falling within each bin:\n$H_{ij} = \\sum_{m=1}^{M} { \\begin{cases}\n      \\frac{1}{\\sqrt{d_{model}}}, & \\text{if } |\\theta_m - \\theta_i| \\le \\frac{\\Delta \\theta}{2} \\text{ and } |r_m - r_j| \\le \\frac{\\Delta r}{2}, \\\\\n      0, & \\text{otherwise}\n    \\end{cases}};\n\n\nwhere \u2206\u03b8, Ar, and (i,j) are the angle span, radius span, and center point\nof each bin, (\u03b8m, rm) is the polar coordinate of anchor point pm, dmodel is the\nembedding dimension of model. As such, H expresses the spatial configuration\nof contour G relative to center point (i.e., po) in a compact and robust way. As\ndepicted in Fig. 2, instances with different shapes (i.e., target A and B) present\nvarying distributions of H which demonstrates the capability to encode the shape\ncues of target objects. Moreover, we equip H with the ability to account for\nthe relative spatial location of target objects by setting $H_{i,j} = -\\frac{1}{\\sqrt{d_{model}}}$ if\nthe center point of a bin (i.e.,\u2729) falls outside of masks. Therefore, instances\nwith similar shapes but different locations (i.e., target B and C) would yield\nsimilar distribution of positive values, but distinct distribution of negative values,\neffectively evolving above shape descriptor into a shape-position descriptor.\nShape- and Position-Aware (SPA) Query Matching. Given the above\nanalysis, a set of shape-position descriptors {Hk}K_1 could be derived from each\nobject k within the mask. We then aim to facilitate the awareness of shape-position\ncues for object association between frames, by integrating such descriptors into\nthe query matching process. To achieve this, as shown in Fig. 3 (c), we draw\ninspiration from the absolute position encoding (APE) which is widely adopted\nin Transformer [100]. Specifically, during mask decoding, N query embeddings\n$\\big\\{q_i\\big\\}_{i=1}^N$ is interacting with the backbone feature F to retrieve object-centric\nfeature in each decoder layer by:\nq^l = CrossAttn(q^{l-1}, F), q^l = SelfAttn(q^{l-1}, q^{l-1})\nWhere l is the layer index. Typically, a Hungarian Matching matrix 1\u00b9\u2208 {0,1}N\u00d7K\nbetween N predictions generated from query embeddings and K ground truth\nobjects can be derived from each decoding layer. Following the principle of APE,"}, {"title": "3.2 Implementation Details", "content": "Network Configuration. GVSEG is a semi-online video segmentation framework\nbuilt upon the tracking by query matching paradigm [4]. It comprises an image-\nlevel segmenter to extract frame-level queries, and an object associator to match\nquery embeddings across frames. The image-level segmenter is implemented as\nMask2Former [92] with both ResNet-50 [104] and Swin-L [105] as the backbone.\nGiven the most recent work typically adopts clip-level inputs for richer temporal\ncues [5, 26, 28], in alignment with this trend, GVSEG takes a clip containing three\nframes as input each time. The size of points set P derived from object contour is\nfixed to 200 to make the shape-position descriptor effectively characterize objects\nof varying scales. We employ u = 36 angle divisions and v = 12 radius divisions\nto capture point distribution in finer granularity.\nTraining. Following the standard protocols [5,23,26,47,106] in video segmentation,\nthe maximum training iteration is set to 10K for OVIS/VSPW/VIPSeg/KITTI\nand 15K for YouTube-VOS18/YouTube-VIS21 with a mini-batch size of 16. The\nAdamW optimizer with initial learning rate 0.001 is adopted. The learning rate is\nscheduled following a step policy, decayed by a factor of 10 at 7K/11K for 10K/15K"}, {"title": "4 Experiment", "content": "Dataset. VIPSeg [37] provides 2,806/323 videos in train/test splits which\ncovers 232 real-world scenarios and 58/66 thing/stuff classes. KITTI-STEP [12]\nis an urban street-view dataset with 12/9 videos for train/val. It includes 19\nsemantic classes, with two of them (pedestrians and cars) having tracking IDs.\nEvaluation Metric. Following conventions [12, 23, 37], we adopt VPQ and STQ\nas metrics. VPQ computes the average panoptic quality from tube IoU across a\nspan of several frames. For VIPSeg [37], we further report the VPQ scores for\nthing and stuff classes (i.e., VPQTh and VPQSt). For KITTI-VPS [12], we divide\nSTQ into segmentation quality (SQ) and association quality (AQ) which evaluate\nthe pixel-level tracking and segmentation performance in a video clip."}, {"title": "5 Conclusion", "content": "We present GVSEG, the first general video segmentation solution that accom-\nmodates task-oriented properties into model learning. To achieve this, we first\nrender a holistic investigation on segment targets by disentangling them into\nthree essential constitutes: appearance, shape, and position. Then, by adjusting\nthe involvement of these three key elements in query initialization and object\nassociation, we realize customizable prioritization of instance discrimination\nor semantic understanding to address different tasks. Moreover, task-oriented\ntemporal contrastive learning is proposed to accumulate a diverse range of infor-\nmative samples that considers both local consistency and semantic understanding\nproperties for tracking instances and semantic/background classes, respectively.\nIn this manner, GVSEG offers tailored consideration for each individual task and\nconsistently obtains top-leading results in four video segmentation tasks."}]}