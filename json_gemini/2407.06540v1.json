{"title": "General and Task-Oriented Video Segmentation", "authors": ["Mu Chen", "Liulei Li", "Wenguan Wang", "Ruijie Quan", "Yi Yang"], "abstract": "We present GVSEG, a general video segmentation framework\nfor addressing four different video segmentation tasks (i.e., instance,\nsemantic, panoptic, and exemplar-guided) while maintaining an identical\narchitectural design. Currently, there is a trend towards developing general\nvideo segmentation solutions that can be applied across multiple tasks.\nThis streamlines research endeavors and simplifies deployment. However,\nsuch a highly homogenized framework in current design, where each\nelement maintains uniformity, could overlook the inherent diversity among\ndifferent tasks and lead to suboptimal performance. To tackle this, GvSEG:\ni) provides a holistic disentanglement and modeling for segment targets,\nthoroughly examining them from the perspective of appearance, position,\nand shape, and on this basis, ii) reformulates the query initialization,\nmatching and sampling strategies in alignment with the task-specific\nrequirement. These architecture-agnostic innovations empower GVSEG\nto effectively address each unique task by accommodating the specific\nproperties that characterize them. Extensive experiments on seven gold-\nstandard benchmark datasets demonstrate that GvSEG surpasses all\nexisting specialized/general solutions by a significant margin on four\ndifferent video segmentation tasks.", "sections": [{"title": "1 Introduction", "content": "Identifying target objects and then inferring their spatial locations over time\nin a pixel observation constitute fundamental challenges in computer vision [1].\nDepending on discriminating unique instances or semantics associated with\ntargets, exemplary tasks include: exemplar-guided video segmentation (EVS)\nthat tracks objects with given annotations at the first frame, video instance\nsegmentation (VIS), video semantic segmentation (VSS), and video panoptic\nsegmentation (VPS) which entails the delineation of foreground instance tracklets,\nwhile simultaneously assigning semantic labels to each video pixel. Prevalent\nwork primarily adheres to discrete technical protocols customized for each task,\nshowcasing promising results [2-21]. Nevertheless, these approaches necessitate\nmeticulous architectural designs for each unique task, thereby posing challenges\nin facilitating research endeavors devoting on one task to another. Recently, there\nhave been efforts in shifting the above task-specific paradigm to a general solution"}, {"title": "3 Methodology", "content": "Problem Statement. Video segmentation seeks to partition a video clip V\u2208\nRH\u00d7W\u00d73 containing T frames of size H\u00d7W into K non-overlap tubes linked\nalong the time axis:\n{Yk}Kk=1 = {(Mk, Ck)}Kk=1,\nwhere each tube mask Mk \u2208 {0, 1}T\u00d7H\u00d7W is labeled with a category ck \u2208 {1,\u2026\u2026,C'}.\nThe value of K varies across tasks: in VSS, it is consistent with the number of\npredefined semantic categories; in EVS and VIS, it is adjusted in response to the\ninstance count; and in VPS, it is the sum of stuff categories and thing entities.\nTracking by Query Matching. Inspired by the success of query-based object\ndetectors, [4,22,33] propose to associate instances based on the query embeddings.\nSpecifically, given a set of N randomly initialized queries {qi}Nn=1, we can derive\nthe object-centric representation {ani}Nn=1 for frame Vt by:\n{ani}Nn=1 = D(E(Vt), {qi}Nn=1),\nwhere & and D are the Transformer encoder and decoder. Here ani refines rich\nappearance representation for a specific object. The tracking is done by applying\nHungarian Matching on the affinity matrix Sij = cosine(qi, qji+1) computed\nbetween ani and an+\u00b9 of two successive frame Vt and Vt+1. As such, instances\nexhibiting identical attributes across the video sequence are linked automatically."}, {"title": "3.1 GVSEG: Task-Oriented Property Accommodation Framework", "content": "GVSEG seeks to advance general video segmentation through controllable em-\nphasis on instance discrimination and semantic comprehension according to task\nrequirements. Concretely, we first devise a new shape-position descriptor to accu-\nrately reveal the shape and location of targets. Then, by adjusting the engagement\nof above shape-position descriptor during cross-frame query matching, we could\nrealize controllable association for instance and background stuff, respectively.\nFinally, we give an analysis on the limitation of current temporal contrastive\nlearning and devise a task-oriented sampling strategy to tackle encountered issues.\nShape-Position Descriptor. Inspired by shape context [99], a shape-position\ndescriptor is constructed to represent the spatial distribution and shape of\ntarget objects. First, it describes shape cues by encoding the relative geometric\nrelationships of points in object contours relative to the object center. As shown\nin Fig. 2, given the contour G\u2208 {0,1}H\u00d7W of a target object which can be easily\nderived from masks, a set P with M anchor points (i.e., pm) are evenly sampled:\nP = {pm = (x, y) | G(x, y) = 1, 1 < m < M}.\nAbove anchor points are transformed into polar coordinates with the central\npoint po of targets (i.e.,) as the reference point. The polar coordinate is a\nhistogram divided into a grid of u\u00d7v bins with u angle divisions and v radius\ndivisions. Next we calculate the number of anchor points falling within each bin:\nHi,j = \u03a3Mm=1 \n{\n1/\u221admodel   if |\u03b8m \u2212 \u03b8i| \u2264 \u2206\u03b8/2 and |rm \u2212 rj| \u2264 \u2206r/2\n 0 otherwise\n };\nwhere \u2206\u03b8, \u2206r, and (i,j) are the angle span, radius span, and center point\nof each bin, (\u03b8m, rm) is the polar coordinate of anchor point pm, dmodel is the\nembedding dimension of model. As such, H expresses the spatial configuration\nof contour G relative to center point (i.e., po) in a compact and robust way. As\ndepicted in Fig. 2, instances with different shapes (i.e., target A and B) present\nvarying distributions of H which demonstrates the capability to encode the shape\ncues of target objects. Moreover, we equip H with the ability to account for\nthe relative spatial location of target objects by setting Hi,j = \u22121/\u221admodel if\nthe center point of a bin (i.e.,\u2729) falls outside of masks. Therefore, instances\nwith similar shapes but different locations (i.e., target B and C) would yield\nsimilar distribution of positive values, but distinct distribution of negative values,\neffectively evolving above shape descriptor into a shape-position descriptor.\nShape- and Position-Aware (SPA) Query Matching. Given the above\nanalysis, a set of shape-position descriptors {Hk}Kk=1 could be derived from each\nobject k within the mask. We then aim to facilitate the awareness of shape-position\ncues for object association between frames, by integrating such descriptors into\nthe query matching process. To achieve this, as shown in Fig. 3 (c), we draw\ninspiration from the absolute position encoding (APE) which is widely adopted\nin Transformer [100]. Specifically, during mask decoding, N query embeddings\n{qi}Nn=1 is interacting with the backbone feature F to retrieve object-centric\nfeature in each decoder layer by:\nqi = CrossAttn(qi'\u22121, F), qi = SelfAttn(qi', qi')\nWhere l is the layer index. Typically, a Hungarian Matching matrix 11\u00b9\u2208 {0,1}N\u00d7K\nbetween N predictions generated from query embeddings and K ground truth\nobjects can be derived from each decoding layer. Following the principle of APE,"}, {"title": "4 Experiment", "content": "4.1 Results for Video Panoptic Segmentation\nDataset. VIPSeg [37] provides 2,806/323 videos in train/test splits which\ncovers 232 real-world scenarios and 58/66 thing/stuff classes. KITTI-STEP [12]\nis an urban street-view dataset with 12/9 videos for train/val. It includes 19\nsemantic classes, with two of them (pedestrians and cars) having tracking IDs.\nEvaluation Metric. Following conventions [12, 23, 37], we adopt VPQ and STQ\nas metrics. VPQ computes the average panoptic quality from tube IoU across a\nspan of several frames. For VIPSeg [37], we further report the VPQ scores for\nthing and stuff classes (i.e., VPQTh and VPQSt). For KITTI-VPS [12], we divide\nSTQ into segmentation quality (SQ) and association quality (AQ) which evaluate\nthe pixel-level tracking and segmentation performance in a video clip."}, {"title": "4.2 Results for Video Semantic Segmentation", "content": "Dataset. VSPW [36] has 2, 806/343 in-the-wild videos with 198, 224/24, 502 frames\nfor train/val, and provides pixel-level annotations for 124 semantic categories.\nEvaluation Metric. Following the standard evaluation protocol [26, 36], we\nadopt the mean Intersection-over-Union (mIoU), and mean video consistency\n(mVC) which evaluates the category consistency among a video clip containing\n8/16 frames (i.e., mVCs and mVC16) as metrics.\nPerformance. As shown in Table 1, based on ResNet-50, GVSEG outperforms all\ncompetitors and achieves 44.5% mIoU. In particular, the 90.5%/86.4% scores\nin terms of mVC8/mVC16 are comparable to MRCFA [85] which utilizes Swin-B\nas the backbone and yields much higher mIoU. This suggests that, benefited by\ntask-oriented temporal contrast learning, GVSEG can produce more consistent\nprediction across frames. When integrated with Swin-B, GVSEG demonstrates\n0.9% gains over Tube-Link [26], confirming the superiority of our approach."}, {"title": "4.3 Results for Video Instance Segmentation", "content": "Dataset. Occluded VIS [35] is specifically designed to tackle the challenging\nscenario of object occlusions. It consists of 607/140 long videos with up to 292\nframes for train/val and spans 25 object categories with a high density of\ninstances. YouTube-VIS21 [2] comprises 2,985/421 high resolution videos for\ntrain/val. It extensively covers 40 object classes with 8,171 unique instances.\nEvaluation Metric. Following the official setup [2, 35], we report the mean\naverage precision (mAP) by averaging multiple IoU scores with thresholds from\n0.5 to 0.95 at step 0.05, and the average recall (AR) given 1/10 segmented\ninstances per video (i.e., AR1, AR10). AP50 and AP75 with IoU thresholds at\n0.5 and 0.75 are also employed for further analysis.\nPerformance. From Table 2 we can observe that GVSEG provides a considerable\nperformance gain over existing methods on Occluded-VIS [35]. Notably, it outper-\nforms the prior specalized/general solution SOTA CTVIS [111]/TarVIS [25] by\n0.4%/4.8% in terms of mAP with ResNet-50 as the backbone. When adopting\nSwin-L, GVSEG showcases far better performance, achieving up to 49.7% mAP\nwhich earns an impressive 2.8% improvement against CTVIS. Moreover, we\nreport performance on YouTube-VIS21 [2]. As seen, GVSEG surpasses the main\nrival (i.e., TarVIS), by 1.3%/0.5% with ResNet-50/Swin-L as backbone."}, {"title": "4.4 Results for Exemplar-guided Video Segmentation", "content": "Dataset. YouTube-VOS18 [112] includes 3, 471/474 videos for train/val. The\nvideos are sampled at 30 FPS and annotated per 5 frames with multiple objects.\nBURST [34] contains 500/993/1, 421 videos for train/val/test. It provides\nmask/point/bounding box as exemplars and averages over 1000 frames per video.\nEvaluation Metric. For YouTube-VOS18, we report region similarity (I) and\ncontour accuracy (F) at seen and unseen classes. For BURST, we assess higher\norder tracking accuracy [115] on common (Hcom) and uncommon (Hunc) classes.\nPerformance. To make a fair comparison with existing work which usually tests\non BURST without training, we train GVSEG on YouTube-VOS18 and randomly\nadopt mask or point exemplars as the guidance. Then the performance is evaluated\nwith mask exemplar on YouTube-VOS18 and point exemplar on BURST. As\nshown in Table 3, GVSEG yields satisfactory performance on YouTube-VOS18,\ni.e., surpassing the general counterpart (i.e., TarVIS [25]) by 2.3%/2.2% in"}, {"title": "4.5 Qualitative Results", "content": "In Fig. 5, we visualize the comparisons of GVSEG against the top-leading methods\non four different tasks (i.e., VPS, VIS, VSS, and EVS). As seen, GVSEG gives\nmore precise and consistent predictions in challenging scenarios."}, {"title": "4.6 Diagnostic Experiment", "content": "For more detailed analysis, we conduct a set of ablative studies on VIPSeg-\nVPS [37] with ResNet-50 as the backbone.\nKey Component Analysis. We investigate the improvements brought by\neach component of GVSEG in Table 4a where 'SPA' indicates 'shape-position\naware'. First, it can be observed that SPA query matching brings a considerable\nimprovement over the Baseline, i.e., 1.8%/1.2% concerning VPQ and STQ. This\nverifies our modeling of segment targets by disentangling them into appearance,\nshape, and position. Moreover, the adoption of task-oriented strategies for query\ninitialization, object association, and temporal contrastive learning (TCL) elevates\nthe results to a new level. Finally, we combine all these designs together which\nresults in GvSEG and obtains the optimal performance. This confirms the\ncompatibility of each component and the effectiveness of our whole algorithm.\nMatching Threshold & Queue Length. The results with different threshold\nTand queue length No utilized in task-oriented TCL are reported in Table 4b.\nThough larger size of samples in the queue contributes to higher scores, we remain\nNo to 100 which gives nearly no impact in training speed and memory usage.\nHistogram Size. In Table 4c, we investigate the impact of the number of bins\nwithin the polar-style histogram for building position-shape descriptor. As seen,\nthere is minor change in performance if u\u00d7v is large enough (e.g., > 200) to\ncapture the fine-grained variation in shape and location."}, {"title": "5 Conclusion", "content": "We present GVSEG, the first general video segmentation solution that accom-\nmodates task-oriented properties into model learning. To achieve this, we first\nrender a holistic investigation on segment targets by disentangling them into\nthree essential constitutes: appearance, shape, and position. Then, by adjusting\nthe involvement of these three key elements in query initialization and object\nassociation, we realize customizable prioritization of instance discrimination\nor semantic understanding to address different tasks. Moreover, task-oriented\ntemporal contrastive learning is proposed to accumulate a diverse range of infor-\nmative samples that considers both local consistency and semantic understanding\nproperties for tracking instances and semantic/background classes, respectively.\nIn this manner, GVSEG offers tailored consideration for each individual task and\nconsistently obtains top-leading results in four video segmentation tasks."}]}