{"title": "Exploring Domain Robust Lightweight Reward Models based on Router Mechanism", "authors": ["Hyuk Namgoong", "Jeesu Jung", "Sangkeun Jung", "Yoonhyung Roh"], "abstract": "Recent advancements in large language models have heavily relied on the large reward model from reinforcement learning from human feedback for fine-tuning. However, the use of a single reward model across various domains may not always be optimal, often requiring re-training from scratch when new domain data is introduced. To address these challenges, we explore the utilization of small language models operating in a domain-specific manner based on router mechanisms. Our three approaches are: 1) utilize mixture of experts to form a single reward model by modularizing an internal router and experts, 2) employing external router to select the appropriate reward model from multiple domain-specific models, and 3) the framework reduces parameter size by loading reward models and router adapters onto a single small language model using adapters. Experimental validation underscores the effectiveness of our approach, demonstrating performance comparable to baseline methods while also reducing the total parameter size.", "sections": [{"title": "Introduction", "content": "Most widely adopted Large Language Models (LLMs) have used the reward model of Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) for fine-tuning. These reward models are trained from various human feedback domains and are subsequently utilized as evaluation metrics during LLM fine-tuning processes. However, training a single reward model across various domains to serve multiple purposes may lead to situations where the model is not fit for specific domains. Additionally, there is a challenge of retraining the reward model from scratch when new dataset from a new domain is introduced.\nIn this paper, we explore various router methods to address these challenges, as summarized"}, {"title": "Related Works", "content": "Recent research focuses on improving LLMs (Chowdhery et al., 2022; Biderman et al., 2023; Touvron et al., 2023) training efficiency. Introducing the reward model serves to evaluate LLM performance in the RLHF fine-tuning method. In (Ouyang et al., 2022), the reward model spans various domains, while (Black et al., 2023) applies the RLHF method to image generation.\nResearch has explored methods for routing language models, such as routing LLMs (Shnitzer et al., 2023; Liu and Liu, 2021; Ravaut et al., 2022; Jiang et al., 2023). Furthermore, various studies are underway to modularize and utilize routers within models(Jiang et al., 2024; Dikkala et al., 2023; Peng et al., 2023), with Mixture of Experts (MoE) (Chen et al., 2022) being one.\nResearch on efficient fine-tuning of language models is ongoing. Low-Rank Adaptation (LoRA) (Hu et al., 2021) attaches adapters to each layer and updates only the adapter parameters, enabling efficient learning. Building upon LoRA, further research explores efficiency improvements(Dettmers et al., 2023; Rajabzadeh et al.; Babakniya et al., 2023) and additional tasks(Zhang et al., 2023; Everaert et al., 2023; Blattmann et al., 2023).\nWe do not train a single reward model across diverse domains. Instead, we utilize adapters to construct multi-reward models and routers, employing a small language model with LoRA, thereby reducing training time and parameters."}, {"title": "Router Based Switching Reward Models", "content": "The reward model assigns rewards to prompt and response. In RLHF, the reward model's loss function calculates the difference between the rewards for the chosen and rejected responses. Reward model dataset has the structure of one input prompt and least two of responses.\nThese reward models cover diverse domains like human preferences and toxic responses, using large-scale models. However, relying solely on one large model may not suit specific domains, and training from scratch for new domains takes time."}, {"title": "Mixture of Reward Experts", "content": "MORE operates by having an internal router select suitable experts among several options, with both the router and experts modularized internally within the model. To implement MoRE, we utilize sparse MoE(Shazeer et al., 2017), applying to small language models to create a single reward model. Maintaining the structure of a single reward model, it processes all dataset together during training, ensuring a training process similar to traditional method.\nSparse MoE, as depicted in Figure 1b, utilizes noisy top-k gating within the router layer directs the output to multiple expert layers before reaching the output layer. These expert layers follow a feed-forward network structure, computing a weighted sum based on the top-k expert outputs, and then the"}, {"title": "Router for Domain-Specific Reward Models", "content": "We introduce RODOS, in Figure 1c, which involves training a small language model for each domain to create multiple domain-specific reward models. The external router is trained to select the reward model suitable for each prompt's domain. This resolves the challenge of a single large reward model trained across multiple domains, which may not be suitable for specific domains.\nFurthermore, RODOS offers a time-efficient solution by training new reward models for new data and retraining the router, rather than restarting the entire reward model training process. This efficiency is attributed to smaller model sizes and shorter router training times relative to reward model training."}, {"title": "Adapter Router Lightweight Integrated Rewards Switching Framework", "content": "Deploying all reward models and router creates deployment challenges for GPU memory. Hosting various models simultaneously results in the total parameter count becoming a multiple of the model parameters, thus demanding a considerable amount of GPU memory.\nIn the ARLISS framework, in Figure 1d, all reward models and routers are trained using adapters, with only the adapter parameters retained, and adapters are dynamically switched during inference. The router adapter selects and switches to the appropriate reward adapter during utilization. This approach consolidates multiple reward models and router into a single language model with multiple adapters, thereby reducing the total size of model parameters, making them lightweight.\nWe utilize Parameter-Efficient Fine-Tuning (Mangrulkar et al., 2022) alongside LoRA, functioning as the adapter mechanism. This enables efficient fine-tuning by updating only adapter parameters, contributing to the overall efficiency of the ARLISS framework."}, {"title": "Experiments Setup", "content": "In this study, we validate the methodology using reward model datasets from five different domains. In cases where the dataset structure is unsuitable for training a reward model, we convert it to a suitable reward dataset structure using only English data.\nAnthropic dataset detects toxic responses and distinguishes whether a response is helpful or harmless (Bai et al., 2022). SHP is a dataset that has two human-written summary responses in a given context (Ethayarajh et al., 2022). HellaSwag is a dataset used for sentence completion tasks, featuring multiple responses to a given prompt (Zellers et al., 2019). Dahoas is a dataset where the model generates two responses to a prompt and humans distinguish between good and bad responses (Alex Havrilla, 2023). Oasst is a dataset that has ranked human-written responses in a given prompt 1.The conversion of each dataset into a reward dataset structure is detailed in Appendix B"}, {"title": "Language Models", "content": "We employed the encoder-only model DeBER-TaV3(DeB) (He et al., 2021), which leverages Transformer's encoder. For our methods, we implement language models such as DeBbase, DeBsmall, and DeBxsmall. The router model is implement with the same language model as the reward model."}, {"title": "Baseline Methods", "content": "In Table 1, the baseline method is a traditional single reward model trained without a router. This method is implemented using DeBlarge and DeBbase for comparison with our proposed approaches. During fine-tuning, all datasets are processed together. Preliminary experiments with other models are detailed in the Appendix E.\nAdditionally, BaseLORA was included in the experiments. This method follows the same training process as the baseline but incorporates LoRA. The purpose is to determine if applying LoRA yields higher performance than the baseline DeBlarge. However, it was observed that BaseLORA exhibited lower performance. BaseLORA were conducted using DeBbase."}, {"title": "Evaluation Metric for Reward Model", "content": "To evaluate the performance of reward model, we utilized binary accuracy. During reward computation for each prompt-response pair, if the reward for the chosen response exceeds that of the rejected response, it is classified as $true$; otherwise, it is classified as $false$."}, {"title": "Experimental Results", "content": "Our study investigates the effectiveness of the proposed router methods through experimental analyses, focusing on key aspects: evaluating the router's impact on application performance, analyzing training time across methods, and comparing total parameters with and without ARLISS integration."}, {"title": "Reward Models Performance", "content": "We analyze the accuracy of our proposed framework compared to other methods. In this regard, we conduct statistical significance analysis for each test dataset. To ensure meaningful evaluation, we conduct evaluations with 5 different seeds.\nTable 2 displays the accuracy for each dataset's test data and the corresponding average. Generally, when the accuracy is less than 0.02, it is considered statistically similar. Excluding the Anthropic dataset, our methods generally outperform the baseline, with RODOS showing the best performance. Moreover, MORE and ARLISS demonstrate a size reduction of approximately half of the baseline. This suggests that our methods offer the potential to replace the baseline with smaller model sizes."}, {"title": "Training Time", "content": "We analyze the implementation time for models with and without the router. For multi-reward model methods, we assess the training time for each reward model and the router. For single reward model methods, only the training time for the reward model across all datasets is considered.\nTable 3 presents the training time for each method per epoch. Overall, our methods show a reduction in time by approximately 63%, with ARLISS demonstrating around a 5% decrease compared to RODOS."}, {"title": "Total Parameter Size", "content": "We analyze our ARLISS framework along with other methods. In this context, we perform parameter size analysis using the same language model.\nTable 3 reveals that the ARLISS framework boasts the smallest parameter size. MoRE showcases a size reduction of about 52%, while ARLISS achieves a reduction of approximately 55% compared to the baseline. Although ARLISS employs a"}, {"title": "Conclusion", "content": "In addressing the limitations of a single large reward model, which can be unsuitable for specific domains and requires retraining when new domain data is introduced, we have implemented router methods. MORE features an internal router alongside a single small reward model, while RODOS incorporates an external router and domain-specific reward models. These methods effectively mitigate challenges related to domain specificity and the need for retraining when new domain data is introduced. Moreover, the ARLISS framework, with adapters for routers and multi-reward models, shows potential for GPU memory optimization by reducing model size.\nFurther research will focus on optimizing the ARLISS framework. Additionally, we plan to investigate the integration of the ARLISS framework into MORE."}, {"title": "Limitation", "content": "The ARLISS framework requires more inference time compared to RODOS, as discussed in Appendix D. This delay arises from the router selecting the reward model and switching the adapter within the same language model, resulting in time consumption during the switching process."}]}