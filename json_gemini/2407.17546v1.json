{"title": "Exploring Domain Robust Lightweight Reward Models based on Router Mechanism", "authors": ["Hyuk Namgoong", "Jeesu Jung", "Sangkeun Jung", "Yoonhyung Roh"], "abstract": "Recent advancements in large language models\nhave heavily relied on the large reward model\nfrom reinforcement learning from human feed-\nback for fine-tuning. However, the use of a\nsingle reward model across various domains\nmay not always be optimal, often requiring re-\ntraining from scratch when new domain data is\nintroduced. To address these challenges, we ex-\nplore the utilization of small language models\noperating in a domain-specific manner based on\nrouter mechanisms. Our three approaches are:\n1) utilize mixture of experts to form a single re-\nward model by modularizing an internal router\nand experts, 2) employing external router to\nselect the appropriate reward model from mul-\ntiple domain-specific models, and 3) the frame-\nwork reduces parameter size by loading reward\nmodels and router adapters onto a single small\nlanguage model using adapters. Experimental\nvalidation underscores the effectiveness of our\napproach, demonstrating performance compa-\nrable to baseline methods while also reducing\nthe total parameter size.", "sections": [{"title": "1 Introduction", "content": "Most widely adopted Large Language Models\n(LLMs) have used the reward model of Reinforce-\nment Learning from Human Feedback (RLHF)\n(Ouyang et al., 2022) for fine-tuning. These reward\nmodels are trained from various human feedback\ndomains and are subsequently utilized as evalua-\ntion metrics during LLM fine-tuning processes.\nHowever, training a single reward model across\nvarious domains to serve multiple purposes may\nlead to situations where the model is not fit for\nspecific domains. Additionally, there is a challenge\nof retraining the reward model from scratch when\nnew dataset from a new domain is introduced.\nIn this paper, we explore various router meth-\nods to address these challenges, as summarized"}, {"title": "2 Related Works", "content": "Recent research focuses on improving LLMs\n(Chowdhery et al., 2022; Biderman et al., 2023;\nTouvron et al., 2023) training efficiency. Intro-\nducing the reward model serves to evaluate LLM\nperformance in the RLHF fine-tuning method. In\n(Ouyang et al., 2022), the reward model spans vari-\nous domains, while (Black et al., 2023) applies the\nRLHF method to image generation.\nResearch has explored methods for routing lan-\nguage models, such as routing LLMs (Shnitzer\net al., 2023; Liu and Liu, 2021; Ravaut et al., 2022;\nJiang et al., 2023). Furthermore, various studies\nare underway to modularize and utilize routers\nwithin models(Jiang et al., 2024; Dikkala et al.,\n2023; Peng et al., 2023), with Mixture of Experts\n(MoE) (Chen et al., 2022) being one.\nResearch on efficient fine-tuning of language\nmodels is ongoing. Low-Rank Adaptation (LoRA)\n(Hu et al., 2021) attaches adapters to each layer\nand updates only the adapter parameters, enabling\nefficient learning. Building upon LoRA, further re-\nsearch explores efficiency improvements(Dettmers\net al., 2023; Rajabzadeh et al.; Babakniya et al.,\n2023) and additional tasks(Zhang et al., 2023; Ev-\neraert et al., 2023; Blattmann et al., 2023).\nWe do not train a single reward model across\ndiverse domains. Instead, we utilize adapters to\nconstruct multi-reward models and routers, employ-\ning a small language model with LoRA, thereby\nreducing training time and parameters."}, {"title": "3 Router Based Switching Reward Models", "content": "The reward model assigns rewards to prompt and re-\nsponse. In RLHF, the reward model's loss function\ncalculates the difference between the rewards for\nthe chosen and rejected responses. Reward model\ndataset has the structure of one input prompt and\nleast two of responses.\nThese reward models cover diverse domains like\nhuman preferences and toxic responses, using large-\nscale models. However, relying solely on one large\nmodel may not suit specific domains, and training\nfrom scratch for new domains takes time."}, {"title": "3.1 Mixture of Reward Experts", "content": "MORE operates by having an internal router se-\nlect suitable experts among several options, with\nboth the router and experts modularized internally\nwithin the model. To implement MoRE, we utilize\nsparse MoE(Shazeer et al., 2017), applying to small\nlanguage models to create a single reward model.\nMaintaining the structure of a single reward model,\nit processes all dataset together during training,\nensuring a training process similar to traditional\nmethod.\nSparse MoE, as depicted in Figure 1b, utilizes\nnoisy top-k gating within the router layer directs\nthe output to multiple expert layers before reaching\nthe output layer. These expert layers follow a feed-\nforward network structure, computing a weighted\nsum based on the top-k expert outputs, and then the"}, {"title": "3.2 Router for Domain-Specific Reward Models", "content": "We introduce RODOS, in Figure 1c, which involves\ntraining a small language model for each domain\nto create multiple domain-specific reward models.\nThe external router is trained to select the reward\nmodel suitable for each prompt's domain. This re-\nsolves the challenge of a single large reward model\ntrained across multiple domains, which may not be\nsuitable for specific domains.\nFurthermore, RODOS offers a time-efficient so-\nlution by training new reward models for new data\nand retraining the router, rather than restarting the\nentire reward model training process. This effi-\nciency is attributed to smaller model sizes and\nshorter router training times relative to reward\nmodel training."}, {"title": "3.3 Adapter Router Lightweight Integrated Rewards Switching Framework", "content": "Deploying all reward models and router creates\ndeployment challenges for GPU memory. Hosting\nvarious models simultaneously results in the total\nparameter count becoming a multiple of the model\nparameters, thus demanding a considerable amount\nof GPU memory.\nIn the ARLISS framework, in Figure 1d, all re-\nward models and routers are trained using adapters,\nwith only the adapter parameters retained, and\nadapters are dynamically switched during infer-\nence. The router adapter selects and switches to the\nappropriate reward adapter during utilization. This\napproach consolidates multiple reward models and\nrouter into a single language model with multiple\nadapters, thereby reducing the total size of model\nparameters, making them lightweight.\nWe utilize Parameter-Efficient Fine-Tuning\n(Mangrulkar et al., 2022) alongside LoRA, func-\ntioning as the adapter mechanism. This enables\nefficient fine-tuning by updating only adapter pa-\nrameters, contributing to the overall efficiency of\nthe ARLISS framework."}, {"title": "4 Experiments Setup", "content": ""}, {"title": "4.1 Datasets", "content": "In this study, we validate the methodology using\nreward model datasets from five different domains.\nIn cases where the dataset structure is unsuitable for\ntraining a reward model, we convert it to a suitable\nreward dataset structure using only English data.\nAnthropic dataset detects toxic responses and\ndistinguishes whether a response is helpful or harm-\nless (Bai et al., 2022). SHP is a dataset that has\ntwo human-written summary responses in a given\ncontext (Ethayarajh et al., 2022). HellaSwag is a\ndataset used for sentence completion tasks, featur-\ning multiple responses to a given prompt (Zellers\net al., 2019). Dahoas is a dataset where the model\ngenerates two responses to a prompt and humans\ndistinguish between good and bad responses (Alex\nHavrilla, 2023). Oasst is a dataset that has ranked\nhuman-written responses in a given prompt 1.The\nconversion of each dataset into a reward dataset\nstructure is detailed in Appendix B"}, {"title": "4.2 Language Models", "content": "We employed the encoder-only model DeBER-\nTaV3(DeB) (He et al., 2021), which leverages\nTransformer's encoder. For our methods, we imple-\nment language models such as DeBbase, DeBsmall,\nand DeBxsmall. The router model is implement\nwith the same language model as the reward model."}, {"title": "4.3 Baseline Methods", "content": "In Table 1, the baseline method is a traditional\nsingle reward model trained without a router.\nThis method is implemented using DeBlarge and\nDeBbase for comparison with our proposed ap-\nproaches. During fine-tuning, all datasets are pro-\ncessed together. Preliminary experiments with\nother models are detailed in the Appendix E.\nAdditionally, BaseLORA was included in the ex-\nperiments. This method follows the same training\nprocess as the baseline but incorporates LoRA. The\npurpose is to determine if applying LoRA yields\nhigher performance than the baseline DeBlarge.\nHowever, it was observed that BaseLORA exhib-\nited lower performance. BaseLORA were conducted\nusing DeBbase."}, {"title": "4.4 Evaluation Metric for Reward Model", "content": "To evaluate the performance of reward model, we\nutilized binary accuracy. During reward compu-\ntation for each prompt-response pair, if the reward\nfor the chosen response exceeds that of the rejected\nresponse, it is classified as true; otherwise, it is\nclassified as false."}, {"title": "5 Experimental Results", "content": "Our study investigates the effectiveness of the pro-\nposed router methods through experimental analy-\nses, focusing on key aspects: evaluating the router's\nimpact on application performance, analyzing train-\ning time across methods, and comparing total pa-\nrameters with and without ARLISS integration."}, {"title": "5.1 Reward Models Performance", "content": "We analyze the accuracy of our proposed frame-\nwork compared to other methods. In this regard,\nwe conduct statistical significance analysis for each\ntest dataset. To ensure meaningful evaluation, we\nconduct evaluations with 5 different seeds.\nTable 2 displays the accuracy for each dataset's\ntest data and the corresponding average. Generally,\nwhen the accuracy is less than 0.02, it is consid-\nered statistically similar. Excluding the Anthropic\ndataset, our methods generally outperform the base-\nline, with RODOS showing the best performance.\nMoreover, MORE and ARLISS demonstrate a size\nreduction of approximately half of the baseline.\nThis suggests that our methods offer the potential\nto replace the baseline with smaller model sizes."}, {"title": "5.2 Training Time", "content": "We analyze the implementation time for mod-\nels with and without the router. For multi-reward\nmodel methods, we assess the training time for\neach reward model and the router. For single re-\nward model methods, only the training time for the\nreward model across all datasets is considered.\nTable 3 presents the training time for each\nmethod per epoch. Overall, our methods show\na reduction in time by approximately 63%, with\nARLISS demonstrating around a 5% decrease com-\npared to RODOS."}, {"title": "5.3 Total Parameter Size", "content": "We analyze our ARLISS framework along with\nother methods. In this context, we perform parame-\nter size analysis using the same language model.\nTable 3 reveals that the ARLISS framework\nboasts the smallest parameter size. MoRE show-\ncases a size reduction of about 52%, while ARLISS\nachieves a reduction of approximately 55% com-\npared to the baseline. Although ARLISS employs a"}, {"title": "6 Conclusion", "content": "In addressing the limitations of a single large re-\nward model, which can be unsuitable for specific\ndomains and requires retraining when new domain\ndata is introduced, we have implemented router\nmethods. MORE features an internal router along-\nside a single small reward model, while RODOS\nincorporates an external router and domain-specific\nreward models. These methods effectively miti-\ngate challenges related to domain specificity and\nthe need for retraining when new domain data\nis introduced. Moreover, the ARLISS framework,\nwith adapters for routers and multi-reward models,\nshows potential for GPU memory optimization by\nreducing model size.\nFurther research will focus on optimizing the\nARLISS framework. Additionally, we plan to in-\nvestigate the integration of the ARLISS framework\ninto MORE."}, {"title": "Limitation", "content": "The ARLISS framework requires more inference\ntime compared to RODOS, as discussed in Ap-\npendix D. This delay arises from the router select-\ning the reward model and switching the adapter\nwithin the same language model, resulting in time\nconsumption during the switching process."}]}