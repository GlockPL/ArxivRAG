{"title": "Adaptformer: Sequence models as adaptive iterative planners", "authors": ["Akash Karthikeyan", "Yash Vardhan Pant"], "abstract": "Abstract-Despite recent advances in learning-based behavioral planning for autonomous systems, decision-making in multi-task missions remains a challenging problem. For instance, a mission might require a robot to explore an unknown environment, locate the goals, and navigate to them, even if there are obstacles along the way. Such behavioral planning problems are difficult to solve due to: a) sparse rewards, meaning a reward signal is available only once all the tasks in a mission have been satisfied, and b) the agent having to perform tasks at run-time that are not covered in the training data (demonstrations), e.g., demonstrations only from an environment where all doors were unlocked. Consequently, state-of-the-art decision-making methods in such settings are limited to missions where the required tasks are well-represented in the training demonstrations and can be solved within a short (temporal) planning horizon. To overcome these limitations, we propose Adaptformer, a stochastic and adaptive planner that utilizes sequence models for sample-efficient exploration and exploitation. This framework relies on learning an energy-based heuristic, which needs to be minimized over a sequence of high-level decisions. To generate successful action sequences for long-horizon missions, Adaptformer aims to achieve shorter sub-goals, which are proposed through an intrinsic (learned) sub-goal curriculum. Through these two key components, Adaptformer allows for generalization to out-of-distribution tasks and environments, i.e., missions that were not a part of the training data. Empirical results in multiple simulation environments demonstrate the effectiveness of our method. Notably, Adaptformer not only outperforms the state-of-the-art method by up to 25% in multi-goal maze reachability tasks, but it also successfully adapts to multi-task missions that the state-of-the-art method could not complete while leveraging only demonstrations (for training) on single-goal-reaching tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "An intelligent autonomous agent must be adaptable to new tasks at runtime, beyond those encountered during training. This is crucial for operating in complex environments that may introduce distractors (i.e., objects the agent has not seen before) and have multiple novel goals.\nExample 1. Consider the mission depicted in Figure 1, where the agent navigates a complex environment. This environment is further complicated by the presence of additional distractors and the requirement to adapt to multiple goals during run-time. Note, that the goal position is not available to the planner, requiring exploratory actions to achieve the objectives. Furthermore, the presence of distractors necessitates adaptive actions, such as unblocking paths, to access doors and goal positions."}, {"title": "II. RELATED WORKS", "content": "Offline RL. Offline RL focuses on learning policies from collected datasets as seen in Figure 2A, without any additional interaction with the environment [6]. This approach faces the challenge of distribution shift between the training demonstrations and run-time distribution. To address the distribution shift, several regularization strategies have been proposed. These include reducing the discrepancy between the learned and behavioral policies [7], [8], [9], as well as implementing value regularization to impose implicit constraints, such as optimizing policies based on conservative value estimations [10]. Despite its potential for learning from training demonstrations, these approaches often encounter difficulties in adapting to new scenarios, as highlighted in Example 1. Offline methods generally employ a pessimistic approach to value function estimation [6], causing instability in training and poor generalization.\nSequence Models in RL. Sequence models in deep learning have been extensively studied in the domain of language modeling, from early sequence-to-sequence models [11] to BERT [12]. In RL, these sequence models have been applied to learn value functions and compute policy gradients, leading to improved performance through model architecture, such as convolutions and self-attention mechanisms, which enable temporally and spatially consistent predictions. More recent works [4], [13], [14] have adopted an autoregressive modeling objective. This approach leverages the conditional generative capabilities of sequence models, where conditioning on desired returns or goal states facilitates the generation of future actions leading to those states or returns, assuming they were observed during training. These strategies aim to answer the question of what action is typically taken next, based on experience, assuming that the desired outcome will occur [4], [15]. Such a behavior cloning approach is designed to map observations to actions, with guidance signals indicating the closeness of the agent's actions to those observed in demonstrations. While effective for behavior cloning tasks, these methods fail in scenarios as seen in Example 1.\nPlanning with Sequence Models. LEAP [5] formulates planning as an iterative energy minimization problem, wherein trajectory-level energy functions are learned via a masked language modeling objective. This method demonstrates generalization to novel test-time scenarios. However, its efficacy is contingent upon an oracle for goal positions essential for generating goal-conditioned trajectories. In the absence of such an oracle, there is a notable decrease in performance, particularly evident in larger mazes, we observe the same in our simulation studies (see Table I). This dependence significantly reduces the model's ability to generalize apriori unseen goals. The major drawback of lack of such conditioning is that it can cause the agent to enter loops, inhibiting exploration and leading to a vulnerability of stalling actions\u2014the agent is stuck at the current position without advancing to the next state. To address the above issues, Adaptformer learns an intrinsic, goal curriculum that generalizes to a diverse goal distribution, which allows it to adapt to apriori unseen goals at run-time. Additionally, adopting a stochastic policy allows for exploration and adaptive skill learning.\nOutline of the paper. In Section III, we formalize the objective we aim to solve. Section IV, introduces the Adaptformer framework an it's components. The overall framework is presented in Figure 2. The training and planning processes are then outlined in Sections IV-E and IV-F, respectively. We present extensive empirical results and discussions in Section V."}, {"title": "III. PRELIMINARIES AND PROBLEM STATEMENT", "content": "A. Goal-Augmented Markov Decision Processes (MDP)\nWe extend the standard MDP framework by incorporating a set of goals G within the state space S. Formally, we consider learning in a Goal-augmented MDP M = (S, A, T, G, R), The MDP tuple comprises discrete states $s \\in S$, each represented by a 2D position, direction and a fully observable image (x, y, d, I) within a finite set of states; discrete actions $a \\in A$; the unknown transition dynamics T(\u00b7|s, a); a set of absorbing goal states G C S and a delayed reward function R : S \u2192 {0,1} [1], [16]. Additionally, we have instructions (INS) that specify the goals of a particular task (see figure 2A). We relabel the reward function Rg(s) to encourage the agent to seek shorter path sequences, defined as $R_g(s) = I\\{s \\in G\\} + c$, where c = -1. The agent gets a reward of 1 when the final goal is reached and -1 otherwise. The agent's objective, I, is to maximize the expected discounted cumulative reward through a goal-conditioned policy $\u03c0 : S \u00d7 G \u2192 A$. The discount factor, $\u03b3 \\in [0,1)$, modulates the importance of immediate versus future rewards.\n$\\mathcal{I}(\\pi)) = \\mathbb{E}_{g \\in G,\\atop a_t \\sim \\pi(\\cdot | s_t, g),\\atop s_{t+1} \\sim T(s_t, a_t)} \\Bigg[\\sum_{t=0}^{\\infty} \\gamma^t R_g(s_t)\\Bigg]$\n(1)\nHere, t corresponds to the timestep. The sparse, binary reward setting often leads to ambiguous guidance and can be uninformative [16]. Moreover, we aim to address test-time adaptability, such as adapting to multiple goals (including unseen apriori) or facilitating long-horizon planning. Therefore, we adopt an RvS-based approach [17], [4], extending policy learning as a conditional generator. To overcome these challenges, we propose to formulate equation 1 as a RvS learning objective, which we aim to optimize via offline RL.\nB. Offline RL\nIn offline RL, we have access to a dataset of near-optimal trajectories collected through demonstrations in the environment as seen in Figure 2A. A trajectory is a sequence of states, actions, and return-to-go $R_t = \\sum_{k=t}R_g(s_k)$, with length n: $T_N = (s_1, a_1, R_1, ..., s_N, a_N, R_N)$. The dataset consists of multiple such trajectories. To solve the policy learning problem via offline RL, we use masked language model to learn energy functions, which allow us to reframe the planning in equation 1 as an iterative optimization procedure [5]. Here, a masked language model, parameterized by \u03b8, is trained to learn a locally normalized energy function $E_\u03b8$. This function assigns a scalar value to each trajectory, such that an optimal sequence of actions $a^*_t$ conditioned on the goals G, receives a low energy score. The policy aims to identify a sequence of actions for a task that minimizes this energy, learned based on a set of offline demonstrations. Here H is a hyper-parameter, which denotes the planning horizon.\n$a^*_{1:H} = \\arg \\min_{a_{1:H}} E_\\theta (a_{1:H} | T_H, G)$    (2)\nChallenges. While the objective in Equation 2 facilitates learning an energy function that acts as a surrogate to rewards equation 1, in turn enabling generation of goal-conditioned trajectories, several challenges remain: (1) Goals at test time could be unknown to the agent\u00b2, (2) Even when goals are specified, goal-conditioned trajectories may offer insufficient guidance for long-horizon tasks, especially when the goal is far from the agent's current position [18], (3) The agent may fail to adapt to multiple goals at test time, potentially hallucinating trajectories observed in training (i.e., generates trajectories that are similar to those seen in demonstrations), (4) Certain skills required for the mission remain unlearned if they are not observed in the training demonstrations (i.e., door opening, or obstacle unblocking)."}, {"title": "IV. ADAPTFORMER: METHODOLOGY", "content": "A. Overview of Adaptformer\nTo address the challenges discussed in Section III-B, we propose Adaptformer. The framework consists of the following key components: a Goal-Augmentation Module to estimate the task-specific goal distribution at test time, discussed in Section IV-B; a State Discriminator that facilitates learning sample-efficient sub-goals to aid planning with long horizon tasks, detailed in Section IV-C; and an Energy based model for an adaptive and generalizable policy in Section IV-D. Section IV-E describes the training of this model, and Section IV-F covers how the model is used for adaptive planning at run-time."}, {"title": "B. Goal Augmentation (GA)", "content": "Adaptformer aims to generate goal-conditioned trajectories. In contrast to existing work, e.g., [5], [4], which relies on an oracle to obtain goal positions during online adaptation, we learn an intrinsic goal curriculum to achieve this. The GA module learns a likelihood distribution across the state space and generates a set of goal proposals, denoted by G. Its objective is defined as follows.\n$max \\log P(G | G_{train}; \\pi_\\omega)$\n(3)\nwhere the goal samples $ \\{g_i\\}_{i=1}^N \\in G_{train} \\sim T$ are drawn from the offline demonstrations. We parameterize the GA module using a fully connected Multi-Layer Perceptron (MLP) with parameters \u03c9, which accepts state, instruction, and image embedding as input [19] as seen in Figure 2B, to generate goal proposals G. In practice, we upper bound the number of expected goal states, enabling Adaptformer to adapt to scenarios with multiple goal positions, even if it was initially trained on tasks with single goal."}, {"title": "C. State Discriminator", "content": "We observe that guidance from goal-conditioned trajectories is often insufficient for long-horizon tasks, particularly when there is a distributional shift between training and test goal distributions, i.e., $P(G_{train}) \\neq P(G_{test})$. For instance, online adaptation might involve multi-goal missions, whereas $G_{train}$ (demonstrations) primarily covers tasks with single-goal tasks. To address these challenges, we introduce learning sub-goal states $S_\\text{CS}$,using a state discriminator [20], [21]. The discriminator assigns confidence values in the range (0,1) to assess whether the generated samples of state sequences $s_{1:H}$ are indistinguishable from the real sequence $S_{1:H}$. Consequently, as the losses in Equation 6 converge, the discriminator learns the true state distribution and forces the policy to generate state sequence that is diverse yet in-distribution to those in demonstrations. This helps in better generalization to new tasks. To achieve this, the discriminator D, modeled as a MLP with parameters \u03c6, learns real state distribution by minimizing $L_\\text{real}$ and discriminating the synthetic state sequence from the policy-induced marginal $p_{\\pi_\\theta}$ by minimizing $L_\\text{fake}$ in Equation 6. Both the discriminator and the generative policy are trained alternately, as is standard in Generative Adversarial Network (GAN). Similar to [18], we find that conditioning actions on achievable sub-goals are more conducive to generating optimal action sequences and preventing stalling. This also results in learning a sample-efficient goal curriculum. Since the discriminator is jointly trained with the policy, unlike hierarchical RL approaches [18], we do not need to specify a number of sub-goal proposals apriori. Thus, during inference, the policy can generate sample-efficient, intrinsic sub-goals as seen in Figure 2B."}, {"title": "D. Energy Based Models for Trajectory Generation", "content": "Once the sub-goals have been established, they facilitate goal-conditioned policy learning. Given demonstrations (see section III-B), we aim to learn the energy of a trajectory E(T), defined as the sum of negative pseudo-likelihood over the horizon $E_\\theta(T) = \\sum_{t=1}^H[-\\log \\pi_\\theta(a_t | T\\backslash t, S_g)]$ [22]. This energy function assigns lower energy to an optimal sequence of actions as seen in Figure 2C. Our approach leverages the masked language model objective [22] to learn a locally normalized energy score, allowing us to score generated rollouts and frame planning as an iterative optimization process [5], [22]. We model a conditional generative policy $\u03c0_\u03b8$, optimized subject to lower bound \u03b2 on its entropy H. This constraint encourages stochasticity and enhances the policy's adaptability to novel environments and tasks.\n$\\min \\sum_{t=1}^H [-\\log \\pi_\\theta (a_t | T\\backslash t, S_g)],  \\text{s.t.} \\newline \\mathbb{E}_{T} \\Bigg[\\sum_{t=1}^H \\mathbb{H}[\\pi_\\theta(\\cdot | T\\backslash t)] \\Bigg] \\ge \\beta$\n(4)\nHere H denotes the planning horizon. This formulation enables the learning of a stochastic policy $\u03c0_\u03b8$ for action prediction at any given timestep. Unlike auto-regressive objectives, the masked language model objective incorporates a bidirectional context of actions across all timesteps, accounting for future trajectories [12]. Our framework thus emphasizes energy minimization across the entire planning horizon rather than focusing on individual timesteps. Next, we define how the Adaptformer is trained."}, {"title": "E. Training Objective", "content": "The objectives outlined in \u00a7 IV B-D can be grouped in two, based on the parameters (\u03b8, \u03c6) being optimized as seen in Figure 2C. Note, in our implementation, the Goal-Augmentation is a sub-module of the policy, hence \u03c9 is contained in \u03b8. Algorithm 1 gives an overview of the training procedure. The training involves alternately minimizing two loss functions: $L_\\uparrow(\u03b8)$ for the policy, and $L_D(\u03c6)$ for the discriminator, described below:\n$L_\\uparrow(\\theta) =  \\mathbb{E}_{T} [- \\log \\pi_\\theta(a_t| T\\backslash t, S_g)]$\n(5a)\n$- \\lambda_1 \\mathbb{E}_{T} [H(\\pi_\\theta(\\cdot| T\\backslash t, S_g))]$\n(5b)\n$+ \\lambda_2 \\mathbb{E}_{(\\hat{S}) \\sim p_{\\theta}} [\\log(1 - D_\\phi(\\hat{S}))]$\n(5c)\n$L_D(\\phi) =  \\mathbb{E}_{(S) \\sim \\tau}[\\log D_\\phi(S)]$\n(6)\n$+ \\mathbb{E}_{(\\hat{S}) \\sim p_{\\theta}} [\\log(1 - D_\\phi(\\hat{S}))]$\nNote that the Equation 4 can be transformed to the dual form, by introducing a Lagrangian multiplier $\u03bb_1 \u2208 (0,\u221e]$.\nWe perform alternate gradient decent steps on \u03bb\u2081 and the $\u03c0_\u03b8$. In practice, we observe that $\u03bb_1 \u2192 0$ and the lower bound B on entropy is eventually satisfied (similar to [23])."}, {"title": "F. Planning: Online Adaptation.", "content": "Algorithm 2 shows how the learnt policy is used at test-time for planning in an iterative manner. Given the trained policy $\u03c0_\u03b8$, we utilize Gibbs sampling [5] to generate plans at run-time. We query the policy to score alternative actions at masked timesteps t \u2208 {0,...,H}. At each iteration i, we query the actions and states $a_t, S_t  \\sim E_\u03b8(T)$ (line 5 in alg. 2) as seen in Figure 2. The positions of the masks are randomized. This strategy allows for minimizing the learnt energy function while also balancing exploration. We highlight a few comments regarding the iterative planning:\n1) The iterative goal-conditioned sampler prevents the model from stalling.\n2) It facilitates trajectory optimization by considering future states.\n3) The sub-goal proposals are influenced by the agent's current location, enabling the dynamic update of new sub-goals that guide the agent.\n4) Provides adaptability to scenarios not encountered in the demonstrations as described in Example 1.\nAs seen in the next section, these are validated via extensive simulation studies."}, {"title": "V. RESULTS AND DISCUSSION", "content": "We assess the performance of our model in modified BABYAI [26] and Minigrid [27] environments, focusing on the following aspects:\n1) Generalization. We evaluate Adaptformer's effectiveness in trajectory planning within maze environments that were not a part of the demonstrations.\n2) Single-goal to Multi-goal Transfer (Adaptability). The model, initially trained on single-goal-reaching demonstrations, leverages this acquired skill at test time to tackle multi-goal-reaching challenges.\n3) Auxiliary Distractors. We introduce additional obstacles and goal states located across various rooms and behind closed doors. The obstacles are strategically placed around the goal state or the door region, requiring the agent to navigate around or reposition the distractor before reaching the goal state as seen in Figure 1. Note that this goal unblocking behavior was not part of the training data (demonstrations).\nWe implemented Adaptformer using Python 3.8 and trained it on a 12-core CPU and an RTX A6000 GPU. We also deployed the policy on an AGILEX LimoBot (Figure 6). See this webpage for more details.\nBaselines for comparison: We compare Adaptformer with LEAP [5] and its variant, LEAP without goal conditioning (LEAPGC) to evaluate the significance of conditioning in generalization to new maps.\nA. Simulation setup\nWe run simulations over an extensive suite of tasks and various environments (Figure 3) across two paradigms (below). The agent can choose from among six actions, left, right, forward, open, drop, or pick up, to navigate and interact with the environment.\n1) Trajectory Planning. This involves the agent moving to one or more goals. The agent is trained through a single goal-reaching task and then evaluated in a multi-goal environment. For our method and the baselines, we evaluate the success rates in reaching goal positions, reporting both the mean and variance. This evaluation is conducted across 50 maps, from 5 different starting positions for each map.\n2) Instruction Completion. Here, the agent operates in a multi-objective environment that will require a higher level of decision-making, such as exploration, picking/dropping objects, key collection, and then reaching the goal behind the door. We evaluate success rates in reaching goal positions across 3 distinct seeds over 50 environment initializations (150 maps).\nB. Results\nTable I presents a summary of the simulation results. Adaptformer outperforms the baselines in trajectory planning, achieving up to 25% increase in success rates. It also has a larger margin over the baselines in challenging long-horizon tasks, demonstrating improved generalizability and adaptability. We also observe that the margin grows as the size of the environment increases. Adaptformer performs on par with LEAP on instruction completion tasks while outperforming LEAPGC. It is also worth noting that LEAP outperforms other popular baselines, such as the Behavior Cloning algorithm (BC), and other model-free RL algorithms like Batch-Constrained deep Q-Learning [7] and Implicit Q-Learning [28], in single-goal tasks in BabyAI environments, as shown in Table 1 of LEAP [5].\nStochastic Environments for trajectory planning. Additionally, we evaluate trajectory planning in stochastic environments without any additional training. In this experiment, the agent has a 20% chance of its chosen action left, right being mapped to one of left, right, forward, pickup, drop, open, with uniform probability. In a stochastic GotoLocal environment, our model performs similar to how it did in the deterministic environment (table I). On the other hand, LEAP performs better in stochastic settings than it did in the deterministic environment. We observed that LEAP exhibits numerous stalling actions, such as in-place turns. When these actions are mapped to other actions, there is a slight increase in success rates in smaller environments. However, a decline in performance is observed in larger mazes, as seen in the results for GotoObjMaze in table I.\nLearning from random trajectories. We assess the model's adaptive capabilities by training it on a single random-walk demonstration of a 100 time-steps. Actions such as open, pickup, or drop were absent from the demonstration. During the evaluation, the agent navigates a multi-room setting with doors as seen in MultiRoomN2S4. Adaptformer outperforms LEAP 71% to 0% due to the stochastic nature of the policy and the diverse sub-goal distribution. On the other hand, LEAP gets stuck in a loop and never solves the task in any of the 150 runs.\nC. Ablation Study\nTo assess the importance of the different components within Adaptformer, we perform a series of experiments by excluding each component in turn. Removing guidance from sub-goals (section IV-B), the discriminator (equations 6 and 5c), or the entropy (term equation 5b) significantly impacts the model's adaptability to test-time scenarios, leading to ~10% decrease in performance (see table II). Qualitatively, excluding goal conditioning and the discriminator results in stalling actions (similar to LEAP). On the other hand, the absence of entropy regularization causes the agent to fail when goal-unblocking is required, as in Example 1. We also observed that by only learning state marginals (see line 2 in Table II), the model experiences a sharp drop in performance, as it fails to learn the transition dynamics.\nEffect of the Size of Training Data. We evaluate success rates as a function of the number of training demonstrations, as shown in Figure 4. There is a steep growth in success rates until 500 training demonstrations, after which learning saturates. It is important to note that our approach utilizes model-free RL and does not explicitly learn dynamics. Moreover, due to the multi-goal setting, the model must generate trajectories that are approximately twice the duration of the training demonstrations. We observe that setting the planning horizon equal to the total sequence length improves scores for complex, long-horizon tasks (such as KeyCorridorS3R3).\nD. Discussion of simulation results\nAs seen in the presented results, Adaptformer outperforms the state-of-the-art across various tasks. In particular, it generalizes to unseen environments and tasks that were not seen during training. We briefly discuss some observations.\nImpact of goal conditioning. Unlike Adaptformer, LEAP's goal conditioning considers only the final goal state as input, obtained through an oracle, which may not always be available. As seen via the ablation studies, goal conditioning, based on the intrinsic goal curriculum (see section IV) is crucial for Adaptformer's adaptation to multiple goals. Without it, the agent tends to get stuck in a local region with no incentive to explore. In Adaptformer, we also iteratively re-initialize the sub-goals, preventing the agent from stalling in the same region, which helps it outperform the baselines on long-horizon tasks. Additionally, we observe that LEAP never takes goal-unblocking actions (such as moving an object in front of a door, see Example 1), as these were not observed during training. Moreover, LEAPGC can only hallucinate trajectories from training and does not generalize. In contrast, our model demonstrates goal-unblocking capabilities and adapts to environments with closed doors, even without explicit demonstrations of door-opening actions.\nCorrelation between energy and task. We explore the energy landscape and observe that states with low energy values are more likely to be visited. As illustrated in Figure 5, Adaptformer implicitly captures the sub-goals associated with the task, unlike LEAP, which conditions solely on the final goal state. The sub-goals effectively identify important state transitions, e.g., doors and keys, assigning them low energy. The iterative planner then estimates energy distributions following Algorithm 2 to generate a trajectory."}, {"title": "VI. CONCLUSION", "content": "Summary. We developed Adaptformer, a generative behavioral planner that can generalize to previously unseen tasks and environments. Extensive simulations demonstrate its capabilities, even in the presence of auxiliary distractors, and an improvement over the state-of-the-art.\nLimitations. (1) We assume access to near-optimal demonstration trajectories for training; however, this might be unrealistic in some settings. Initial results show that Adaptformer performs well in simple tasks even when trained on random demonstrations. However, further studies are required to see how robust it is to sub-optimality in demonstrations. (2) Adaptformer assumes complete information about a given environment. Initial experiments show that it can still complete simple tasks with only local information (such as objects in the agent's field of view); the method needs further development to work successfully in partially known environments. (3) While the distribution of T is stationary in the offline domain, the data distribution is non-stationary during the online adaptation.\nFuture work. To overcome some of the limitations above, we will extend Adaptformer for online fine-tuning via hindsight experience replay [23], [14]. We will also further develop Adaptformer method to perform online information gathering in settings where the agent has a limited field of view."}]}