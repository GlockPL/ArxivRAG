{"title": "END: Early Noise Dropping for Efficient and Effective Context Denoising", "authors": ["Hongye Jin", "Pei Chen", "Jingfeng Yang", "Zhengyang Wang", "Meng Jiang", "Yifan Gao", "Binxuan Huang", "Xinyang Zhang", "Zheng Li", "Tianyi Liu", "Huasheng Li", "Bing Yin"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, they are often distracted by irrelevant or noisy context in input sequences that degrades output quality. This problem affects both long- and short-context scenarios, such as retrieval-augmented generation, table question-answering, and in-context learning. We reveal that LLMs can implicitly identify whether input sequences contain useful information at early layers, prior to token generation. Leveraging this insight, we introduce Early Noise Dropping (END), a novel approach to mitigate this issue without requiring fine-tuning the LLMs. END segments input sequences into chunks and employs a linear prober on the early layers of LLMs to differentiate between informative and noisy chunks. By discarding noisy chunks early in the process, END preserves critical information, reduces distraction, and lowers computational overhead. Extensive experiments demonstrate that END significantly improves both performance and efficiency across different LLMs on multiple evaluation datasets. Furthermore, by investigating LLMs' implicit understanding to the input with the prober, this work also deepens understanding of how LLMs do reasoning with contexts internally.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have exhibited impressive performance across a wide range of natural language processing tasks. As their application scope expands, the input lengths of LLMs are also increasing rapidly (Dubey et al., 2024; Yang et al., 2024; Liu et al., 2024). However, when processing long sequences, LLMs often face a significant challenge from the noise in the contexts which may distract LLMs. This issue arises when models fail to effectively utilize relevant contextual information, becoming distracted by irrelevant or noisy data instead. Consequently, the quality of their output deteriorates, resulting in inaccuracies, hallucinations, incomplete responses, and occasional failures to follow instructions (Anil et al., 2024; Shi et al., 2023).\nSuch distraction is common and not limited to long-sequence tasks. For instance, while retrieval-augmented generation (RAG) can filter out most noise, the prompt delivered to LLMs after retrieval and reranking still contains a significant amount of irrelevant information. Similar challenges arise in table-based question answering (QA), multi-turn dialogue QA, and in-context learning (ICL). In these scenarios, only a small fraction of the provided context is directly relevant to the query, whereas the remaining information-despite often being contextually similar acts as noise, potentially hindering the model's ability to focus on the most relevant content and also affecting efficiency.\nExisting approaches to mitigate this issue primarily fall into two categories. The first involves multi-agent collaboration frameworks (Team, 2024; Lee et al., 2024), where the context is split into segments processed by different agents, followed by inter-agent interaction to produce a final output. While effective, these methods often require complex agent interaction designs and multiple inference steps, resulting in increased latency. The second approach, known as \u201cParallel Context Encoding\u201d (Yen et al., 2024; Merth et al., 2024), either trains an additional encoder to process context segments before feeding them to the LLM or trains a separate module to aggregate outputs from multiple LLM runs on different segments. However, these solutions necessitate non-trivial training of new components of the LLM itself and also incorporates significant changes to LLMs' original forward mechanism.\nThis motivates us to explore whether it's possible to leverage a LLM's inherent ability to handle noisy cases without fine-tuning. Inspired by previous works suggesting that LLMs implicitly know when they are generating incorrect responses (Kadavath et al., 2022; Yin et al., 2023), we hypothesize that LLMs can identify whether the input contains useful information related to the questions before generating the first response token. Our experiments, using a simple linear prober, strongly support this assumption and reveal that such distinguishing abilities emerge in very early layers.\nBased on these findings, we propose \"Early Noise Dropping\u201d (END), a novel approach to mitigate noise distraction in LLMs. END segments the input into multiple chunks and processes them in parallel. A linear prober, operating on hidden states from lower layers, distinguishes between informative and noisy chunks, discarding irrelevant ones early while retaining essential context for the final prediction. The remaining chunks are then combined and processed in a full forward pass. This method significantly reduces computational overhead while preserving key information. Compared to strong baselines, including direct LLM noise discrimination approaches, END achieves over 10% performance improvement and reduces computation by approximately 50%.\nCrucially, the proposed END requires no fine-tuning, instead leveraging LLMs' innate ability to discern task-relevant information. By dropping redundant chunks early, it enhances both efficiency and accuracy while shedding light on LLMs' internal noise discrimination mechanisms. Our main contributions include:\n\u2022 We identify LLMs' noise sensitivity: Even trivial noise distracts LLMs, while noise resembling target information severely degrades performance.\n\u2022 We find that LLMs can internally differentiate relevant and irrelevant context at lower layers, which can be effectively exploited via a linear prober.\n\u2022 We propose END: By selectively processing input chunks, END mitigates noise distraction effectively and efficiently. Comprehensive experiments confirm its superiority over baselines."}, {"title": "Methodology", "content": "In this section, we first show the harmfulness caused by noise in the context and the impact of longer contexts. Then via constructing a linear prober, we elicit the LLMs' inherent capabilities of being aware of whether the input is informative or noisy. Finally, based all previous results, we introduce END, which processes segmented long input with the help of a linear prober, and only retains informative segments for predictions."}, {"title": "The Impact of Noisy Contexts from Longer Contexts.", "content": "In Table 1, we show how the performance of Llama3-8B-instruct on our synthetic task NoisyRetrieval shifts under two key factors: (1) the difficulty of added noise and (2) overall context length. Briefly, for each question, we add a single true segment (containing the answer) plus 12 distraction segments that range in noise difficulty from Level_0 (least confusable) to Level_4 (most confusable). These distraction segments have information similar to the true answer but do not contain the correct answer (Further details on how we generate these segments and control the noise difficulty for this task appear in Section 3.1.1). In this table, we refer to it as the standard context setting for each instance has approximately 4k tokens. Additionally, we test an extended context setting with trivial noise (2x length, \u2248 8k tokens) by adding random texts. Even this seemingly harmless extra material leads to worse performance, particularly when combined with harder noise (e.g., Level_4). We draw two conclusions: (1). The sharp decrease in performance as the noise level increases highlights the critical need for removing noise from the input contexts. (2). The performance gap between the \"Standard\" and \"Extended\" settings demonstrates that even trivial noise in longer context can cause severe performance degradation. This finding underscores the importance of reducing input length and mitigating noise to optimize LLM performance."}, {"title": "A Linear Prober can Effectively Elicit LLMs' Inherent Discrimination Capabilities.", "content": "Previous works on LLMs' implicit abilities suggest that while generating predictions for a task, LLMs inherently perform many functions beyond the task itself. For instance, Slobodkin et al. (2023) found that the last layer's hidden states can be used to reveal whether the generated prediction is hallucination or not.\nThis insight prompts us to ask: Do LLMs inherently check whether the input contains information related to the current queries? We hypothesize that the answer is \u2018Yes', considering that LLMs perform well when directly asked whether a given input can answer a specific question. To validate this hypothesis, we constructed linear probers for four models: Llama3-8B-Instruct, Mistal-Inst-v0.3, Qwen2-7B-Instruct, and Gemma-1.1-7b-it, across three datasets: NoisyRetrieval, NaturalQA, and TriviaQA. Details of these datasets can be found in Section 3.1.1. Following common practice, the linear prober takes the hidden representation of the last input tokens and predicts binary labels for each input: '0' for input irrelevant to the query and '1' for input informative to the query. We also conducted a layer-wise analysis to reveal how these inherent abilities emerge in LLMs. The results are shown in Figure 2. For each query, we set 10 negative inputs and 1 positive input.We can conclude the following from our results:\n\u2022 LLMs can successfully determine whether the input is noisy via a simple linear prober. Although we don't explicitly require the model to discriminate the input in the prompts, the linear prober performs remarkably well.\n\u2022 Such discrimination abilities arise at very early layers. Using the hidden states from layers around layer 13, the linear prober can already achieve a high recall (>0.95) for the positive input (i.e., the non-noise input).\n\u2022 Different models exhibit similar behavior, suggesting that these inherent abilities are universal across LLMs.\nThese findings underscore the potential for leveraging LLMs' innate discrimination capabilities in handling noisy or irrelevant input data."}, {"title": "END: Early Noise Dropping.", "content": "The core idea of the proposed END is to segment the input sequence into multiple chunks, process these chunks in parallel through the lower layers of the LLM, identify and retain only the essential chunks, and finally forward the retained chunks for the final prediction. This approach leverages the LLM's inherent ability to discriminate between informative and noisy input early in its processing stages. The framework of the proposed END is shown in Figure 1. Its details are as follows:\nStep 1: Input Segmentation. We begin by dividing the input sequence into multiple chunks. The optimal chunk size may vary depending on the specific task and model architecture. Most chunks will only contain noise rather than the answer to the task. These chunks are then processed in parallel through the lower layers of the LLM, allowing for efficient computation. During this parallel processing, each chunk is attached to a task-related query.\nStep 2: Noise Discrimination and Chunk Dropping. In this step, we leverage LLMs' inherent noise discrimination capability shown earlier to discriminate noisy chunks and drop them. We employ the previously described linear prober as the discriminator, attached to the lower layers of the LLM. Specifically, we use a logistic regression model as the prober and, based on our previous study, we attach it to layer 13. The prober predicts a score for each chunk, and we drop all chunks with a prediction score lower than a specific threshold. It's not necessary for the linear prober to predict with 100% accuracy whether a segment is noisy or not. As long as it can filter out most noisy segments, we can expect good downstream performance. Hence, in our experiments, we keep chunks with the top 30% predicted scores.\nStep 3: Continue the Forward Pass with Retained Chunks. To obtain the final prediction after noise reduction with the linear prober, we perform a complete forward pass with the remaining segments. We combine all retained chunks and feed them into the LLM in a new forward pass to get the final output. Our approach requires slightly more than one forward pass through the LLM but involves less manipulation of the LLMs. Considering the parallel processing of the initial inputs, the prober applis at early layers, and the largely reduced input length for the final prediction, our approach does not enforce efficiency burden at the inference time."}, {"title": "Experiments", "content": "In this section, we introduce experimental settings first. Then we show the effectiveness of the proposed END and provide further analysis."}, {"title": "Experimental Settings", "content": "We primarily tested the proposed method on question-answering tasks.\nNoisy Retrieval. This synthetic task requires retrieving the passkey of an item characterized by five attributes: NAME, MATERIAL, COLOR, BRAND, and ITEM. Each instance contains a positive segment (which includes the correct answer) and 12 noisy negative segments that serve as distractors. To generate these negative segments, we control the number of shared attributes between the distractor and the positive segment, creating 5 difficulty levels of noise (from Level_0 to Level_4). Here, Level_4 is the most challenging, as the distractor shares 4 attributes with the positive segment, whereas Level_0 is the easiest, with no shared attributes. For example, consider the question, \"What is the passkey of Jack's Green Wooden Samsung Phone?\" The evidence is: \"the password of Jack's Green Wooden Samsung Phone is 12345.\" A Level_0 distractor might be: \u201cthe password of Luke's Red Metal LG Laptop is 54321,\"\nand a Level_4 distractor might be: \"the password of Jack's Green Metal Samsung Phone is 41415.\u201d More details about this task and additional examples can be found in Appendix E.\nNatural QA. This question answering dataset is from DPR (Karpukhin et al., 2020), where each instance has positive and negative segments retrieved in advance. Each instance has negative segments of two noise level 'Weak' and 'Hard' according to the similarity score. Hard negative segments can distract LLMs more easily because they have larger similarity scores.\nTrivia QA. This dataset is also from DPR. Similar to Natural QA, each instance in trivia QA has positive and negative segments retrieved in advance. Following the settings from previous work, each instance has the negative segments with one noise level as 'Hard'."}, {"title": "Baselines", "content": "To ensure a fair comparison across scenarios with varying noise levels and context lengths, we establish two baselines:\nRAG serves as a baseline, simulating the full retrieval-augmented generation (RAG) pipeline. A typical RAG pipeline consists of three main stages: retrieval, reranking, and prediction. During the reranking stage, retrieved chunks or segments are embedded, and similarity scores are computed to reorder them. The top-ranked chunks are retained and passed to the model for prediction. We didn't do real RAG for the proposed END can be attached the prompt after RAG and further reduce noise. To simulate a real-world RAG scenario, we combine positive segments (containing the answer) with negative segments (distractors) as the post-reranking inputs for prediction. For example, the datasets from DPR (e.g., NatureQA, TriviaQA), inherently include negative chunks that have already been filtered and ranked based on similarity scores. This makes our simulation particularly reasonable, as it accurately reflects the reranking and filtering process in the RAG pipeline, ensuring alignment with real-world behavior.\nLLM-Discrim acts as an \"formidable\" baseline and is supposed to surpass all methods by design (although Table 2 shows this is not the case). It has two forward passes. In the first forward pass, it splits the input into chunks and asks the LLMs to determine whether a segment contains the answer. Then, in the second forward pass, it requires the LLMs to provide the prediction using the remaining chunks as input. LLM-Discrim is particularly effective (Zheng et al., 2024; Fu et al., 2023), especially for hard tasks, as it explicitly leverages the ability of LLMs to discriminate input segments, setting a performance benchmark that END method does not aim to exceed."}, {"title": "Main Results & Analysis", "content": "The results for the proposed END are listed in Table 2. In our experiments, we retained the segments with the top 30% of linear prober prediction scores.\nEffectiveness Analysis. As a result, our proposed END significantly outperforms the RAG baseline, it also achieves the best overall performance even when compared to the strong LLM-Discrim baseline. Besides, the superiority of END becomes more pronounced on more challenging tasks. As expected, the recall of this linear prober is sufficiently high. Although there are some instances of inferior performance, we believe these may be attributed to the F1 metric used for evaluation. For example, in the case of NaturalQA (Weak) with Mistral-Inst-v0.3, the recall is nearly perfect (0.998), yet the F1 score is still lower than anticipated. This discrepancy suggests that while our method is highly effective at identifying relevant information, the overall performance metric may not fully capture this advantage in certain scenarios.\nEfficiency Analysis. While END appears to require an additional forward pass due to the extra probing and Chunk Dropping, it actually still processes a significantly reduced input during its second forward pass compared to the RAG baseline, owing to the substantial chunk elimination. Moreover, in comparison with the strong LLM-Discrim baseline, the Chunk Dropping stage consumes less than half of a forward pass. Assuming quadratic complexity for LLM operations, and considering an input segmented into 10 chunks, each of length L, using Llama-3-8B-Instruct (32 layers) as the backbone LLM, and dropping 70% (7) of chunks at layer-13, we can approximate the computational cost as follows: $RAG: \\Theta((10L)^2) = \\Theta(100L^2)$ $LLM-Discrim$ (assuming 2 chunks retained): $\\Theta((10L)^2 + (2L)^2) = \\Theta(104L^2)$ $END$ (30% chunks retained): $\\Theta((10L)^2 + (3L)^2) = \\Theta(49L^2)$ This demonstrates that END achieves superior efficiency compared to both baselines. Real-world wall-clock time analysis also support its efficiency (Appendix D)."}, {"title": "Analysis of the Linear Prober", "content": "The efficacy of END is heavily influenced by the performance of the linear prober. A more accurate and precise linear prober results in shorter final inputs, enabling LLMs to generate predictions more efficiently and to better understand the contexts. This section presents a comprehensive analysis of the linear prober, demonstrating that, as hypothesized in the introduction, it unveils LLMs' inherent capabilities and implicit reasoning processes when addressing factual queries based on the input.\nLinear Prober's Effectiveness Is not from of fitting. To verify that the linear prober's discriminative capability does not stem from the fitting process of training the prober itself, we conducted experiments using randomly initialized LLMs as the backbone while maintaining the original embedding layer to preserve semantic meaning. The performance comparison between these randomly initialized LLMs and their pre-trained counterparts is presented in Table 3. The substantial performance gap of the linear prober demonstrates that this implicit discriminative ability originates from the pre-trained LLMs rather than training the linear prober with a LLM as a general feature extractor.\nThe linear prober is robust to prompt variations. Previous research has shown that LLMS can be sensitive to prompt formats (Sclar et al., 2023). To explore the robustness of the linear prober, we analyze its performance across four different prompt formats: [S|C|Q], [S|Q|C],[Q|S|C], and [SIC], where 'S' represents the system prompt, 'Q' the question, and 'C' the context. The linear prober is trained using the [S|C|Q] format with Llama-3-8B-Instruct as the backbone model. As shown in Table 4, although the linear prober is only trained on [S|C|Q], it generalizes well across [S|C|Q], [S|Q|C] and [Q|S|C] formats. Such kind of generalization strongly supporting the idea that LLMs are inherently prepared to answer a question once the context is fully presented.\nIn contrast, the performance degradation on the [SIC] format suggests that the linear prober relies on more than just distinguishing between noise and informative input\u2014it extracts the model's implicit understanding of the input in relation to the question. The specific prompts used for each dataset and format are provided in Appendix F.\nThe distinguishing capabilities of large and small models arise at similar layers. We conduct probing experiments on both Llama3-70B-Instruct and Llama3-8B-Instruct to investigate how their distinguishing abilities evolve across layers. Figure 3 shows the performance of linear probers applied to both models. Interestingly, despite the significant difference in size-Llama3-70B-Instruct has 80 layers, while Llama3-8B-Instruct has only 32 layers-the performance of the linear prober saturates around similar layers for both models, specifically between layers 10 and 15. This finding suggests that the key distinguishing capabilities in language models is not solely dependent on model size but is instead strongly tied to the depth. Even in the larger Llama3-70B-Instruct, these capabilities arise early in the network, around the same layer range as in the much smaller Llama3-8B-Instruct. This consistency across models of varying sizes implies that these distinguishing features are fundamental properties of the model's internal representations and layer-wise progression.\nThe implicit discrimination ability is as strong as explicitly requiring LLMs to discriminate the input. We conducted an experiment where we replaced the task query in the prompt with a distinguishing query (Appendix F.4) that directly asks the LLM to determine whether the input is relevant to the question, similar to the first forward pass of the baseline method LLM-Discrim. We then attached the linear prober to the representations generated from this input to evaluate whether this explicit requirement would lead to improved performance. The results, shown in Figure 4, are surprising. While the performance with the distinguishing query appears slightly better than with the task query, the overall performance of the linear prober remains nearly the same. This outcome supports our earlier hypothesis: regardless of whether the model is explicitly instructed to distinguish relevant input, LLMs seem to perform this discrimination step implicitly before answering questions related to context."}, {"title": "Related Work", "content": "Enhancing LLMs' Long Context Ability. Zhang et al. (2024) found that positional encoding decay weakens self-attention on relevant parts and proposed modifying key attention head dimensions to improve LLMs' performance on noisy input. Hsieh et al. (2024) addressed positional bias by introducing a calibration mechanism that subtracts baseline attention from a dummy document, preventing overemphasis on input boundaries. He et al. (2024) tackled this via instruction tuning to help LLMs focus on target information. Beyond data-centric approaches, Wu et al. (2024) applied contrastive loss in post-training, enhancing robustness by retrieving similar document pairs.\nRetrieval Augmented Generation (RAG). Different from directly feeding all collected texts to query the LLMs, RAG first performs chunking and then retrieves the most related chunks across all candidate texts (Lewis et al., 2020; Asai et al., 2023). Although RAG can significantly boost the performance of LLMs, it still suffers from the distraction problem as we will show below. This is because there are still only a few useful chunks among all the retrieved chunks, even though they all have scores considering their relation to the query.\nPrompt/Context Compression The original prompts always contain many useless tokens, and this line of works automatically prunes redundant tokens in the prompts so as to reduce the input length. They leverage tailored metrics such as self-information to keep important information (Li et al., 2023). Together, these studies demonstrate that strategic compression of input contexts can lead to computational savings without sacrificing accuracy (Jiang et al., 2023a,b; Xu et al., 2023).\nSeparate Context Processing. This approach decomposes long inputs into separate parts for individual processing, including agent collaboration and parallel context processing. Multi-agent systems iteratively exchange results to refine predictions (Zhao et al., 2024; Team, 2024; Lee et al., 2024). Parallel processing extracts and merges useful information (Yen et al., 2024) or employs trainable selection mechanisms to determine predictions (Merth et al., 2024). Some methods apply KV-cache compression post-segmentation to reduce noise and context length (Kim et al., 2024).\nLinear Probing in Natural Language Processing. Linear probing has been used to extract knowledge from models, including LLMs (Gurnee and Tegmark, 2023). Recent NLP studies show that applying it to the first generated token can aid trust-related tasks like hallucination detection (Slobodkin et al., 2023). Unlike prior work, we perform layer-wise probing and find that LLMs assess input informativeness at an early stage. Moreover, while existing methods probe task-related concepts, our approach explores unrelated concepts."}, {"title": "Conclusion", "content": "In this paper, we introduced Early Noise Dropping (END), a novel method to improve Large Language Models' (LLMs) performance when processing noisy or irrelevant context. END effectively identifies and removes noisy input chunks early in the LLM pipeline, improving performance across various tasks. The method demonstrates versatility by working well across different LLM architectures without requiring fine-tuning. By reducing unnecessary computation on irrelevant information, END offers both performance and efficiency gains.\nWhile END shows significant promise, future work could explore more advanced chunking strategies and dynamic thresholding techniques. The prober could be enhanced with additional trainable parameters or greater complexity."}, {"title": "Limitations", "content": "We acknowledge that the linear prober's generalization across tasks remains challenging (generalization analysis can be found in Appendix \u0421.1). While fitting a simple linear regression requires minimal data, this limitation could potentially be addressed through the implementation of a more sophisticated probing mechanism. Additionally, the mechanism of LLMs' internal behavior is still not entirely clear, requiring further investigation. Besides, due to our limited computational resources, we did not conduct extensive experiments on large LLMs such as Llama-3-70B. However, the prober still performs well (Appendix C.2) at this scale and appears to save more computation, considering that huge models have more layers."}, {"title": "NosiyNeedle", "content": "Noisy Retrieval (Synthetic)\nThe original \"Needle-in-a-Haystack\" style task (gkamradt) such as passkey retrieval (Mohtashami and Jaggi, 2023) is too easy. The passkey retrieval task is to retrieve a simple passkey like \"The passkey is 12345\". Its contexts are some meaningless texts such as repeated \"The grass is green, the sky is blue\". An example looks like:\nThe grass is green, the sky is blue. The grass\nis green, the sky is blue. The passkey is\n12345. The grass is green, the sky is blue.\nThe passkey is 12345. The grass is green,\nthe sky is blue. The grass is green, the sky\nis blue. The grass is green, the sky is\nblue. What is the passkey?\nThis is far from real cases, and any retriever can perfectly find the answer segments. Hence, we design the Noisy Retrieval task as a noisier version. We set five noise levels (level_0 - level_4) for segments, by controlling five attributes in the target sentence: [Name], [Material], [Color], [Brand], [Item]. For example:\n[Jack's] Password to his [Green]\n[Wooden] [Benz] [phone] is 34512:\nWith this sentence as the answerm, some examples are:\n\u2022 level_0: Random but meaningful texts from some papers.\n\u2022 level_1: Random but meaningful texts from some papers + a sentence with one attribute the same as the target:\n[Paul's] Password to his [red]\n[golden] [Apple] [phone] is 51233\n\u2022 level_4: Random but meaningful texts from some papers + a sentence with four attributes the same as the target:\n[Jack's] Password to his [Green]\n[Wooden] [Benz] [laptop] is 12345\nAn example input for NoisyRetrieval:\n[other contexts] [Jack's Password to his Green\nWooden Apple phone is 12345] [other contexts\n] [Paul's Password to his Green Wooden Apple\nphone] [other contexts] [Jack's Password to\nhis red glass Benz phone is 11145] [other\ncontexts] [Jack's Password to his yellow\nglass Apple phone is 32525] [Noise]\nQuestion: What is Jack's Password to his Green\nWooden Benz phone?\nAnswer:\nFor this task, each positive chunks contains the answer inserted randomly into some random texts from random essays. Each negative chunk include distractor content inserted randomly into random texts from random essays. For input context construction, to ensure sufficient difficulty and realism, the chunk containing the answer is placed in the middle of the context. For example, with 10 negative chunks and 1 positive chunk, the positive chunk is positioned as the 6th chunk within the context."}, {"title": "Attribute Value", "content": "The possible values of all attributes are listed below:\nName = [\"John\", \"Emma\", \"Alex\", \"Sophia\",\n\"Michael\", \"Olivia\", \"Liam\", \"Ava\", \"Noah\",\n\"Isabella\", \"Ethan\", \"Mia\", \"Mason\",\n\"Charlotte\", \"William\", \"Amelia\", \"James\",\n\"Harper\", \"Benjamin\", \"Evelyn\"]\nColor = [\"red\", \"blue\", \"green\", \"yellow\",\n\"black\", \"white\", \"purple\", \"orange\", \"pink\",\n\"brown\", \"gray\", \"navy\", \"teal\", \"maroon\",\n\"olive\", \"silver\", \"gold\", \"turquoise\",\n\"lavender\", \"coral\"]\nItem = [\"bag\", \"watch\", \"phone\", \"laptop\",\n\"headphones\", \"sunglasses\", \"shoes\", \"jacket\",\n\"camera\", \"tablet\", \"wallet\", \"backpack\",\n\"earbuds\", \"smartwatch\", \"keyboard\", \"mouse\",\n\"speaker\", \"charger\", \"fitness tracker\",\n\"power bank\"]\nBrand = [\"Apple\", \"Samsung\", \"Nike\", \"Adidas\",\n\"Sony\", \"Gucci\", \"Microsoft\", \"Dell\", \"LG\",\n\"Bose\", \"Lenovo\", \"Asus\", \"Logitech\", \"Prada\",\n\"Canon\", \"Nikon\", \"Fitbit\", \"Fossil\",\n\"JBL\", \"Anker\"]\nMaterial = [\"leather\", \"aluminum\", \"plastic\",\n\"glass\", \"titanium\", \"silicone\", \"ceramic\",\n\"fabric\", \"wood\", \"rubber\", \"nylon\",\n\"polyester\", \"cotton\", \"wool\", \"denim\",\n\"suede\", \"velvet\", \"cork\"]"}, {"title": "Used Prompts", "content": "The prompts used in the probing stage are listed below. [SIC|Q] is the default prompt format without specifying.\nNaturalQA\n[SIC|Q] Setting:\nSystem Prompt: \"You are given some pieces of a story, which can be either a novel or a movie script, and a question. Answer the question as concisely as you can, using a single phrase if possible. Do not provide any explanation.\"\nPrompt Template:\n\"{SYSTEM}\nPieces of the story:\n{CONTEXT}\nNow, if you can find the required information,\nanswer the question based on the story as\nconcisely as you can, using a single phrase\nif possible. Do not provide any explanation.\nQuestion: {QUERY}\nAnswer:\"\n[SIQIC] Setting:\nSystem Prompt: \"You are given some pieces of a story, which can be either a novel or a movie script, and a question. Answer the question as concisely as you can, using a single phrase if possible. Do not provide any explanation.\"\nPrompt Template:\n\"{SYSTEM}\nNow, if you can find the required information,\nanswer the question based on the story as\nconcisely as you can, using a single phrase\nif possible. Do not provide any explanation.\nQuestion: {QUERY}\nPieces of the story:\n{CTX}\nAnswer:\"\n[Q|S|C] Setting:\nSystem Prompt: \"You are given some pieces of a story, which can be either a novel or a movie script, and a question. Answer the question as concisely as you can, using a single phrase if possible. Do not provide any explanation.\"\nPrompt Template:\n\"Now, if you can find the required information,\nanswer the question based on the story as\nconcisely as you can, using a single phrase\nif possible. Do not provide any explanation.\nQuestion: {QUERY}\n{SYSTEM}\nPieces of the story:\n{CTX}\nAnswer:\"\n[SIC] Setting:\nSystem Prompt: \"You are given some pieces of a story, which can be either a novel or a movie script, and a question. Answer the question as concisely as you can, using a single phrase if possible. Do not provide any explanation.\"\nPrompt Template:\n\"{SYSTEM}\nPieces of the story:\n{CTX}\nAnswer:\"\nNoisyNeedle\n[S|C|Q] Setting:\nSystem Prompt: \"There are information about a passkey hidden in input. Please remember it.\"\nPrompt Template:\n\"{SYSTEM}\nPart of the input:\n{CTX}\nRemeber your task: according to all previous\ninput, if there is the answer to: {QUERY},\nanswer the question\""}, {"title": "Setting:", "content": "System Prompt: \"There are information about a passkey hidden in input. Please remember it.\"\nPrompt Template:\n\"{SYSTEM}\nRemeber your task: according to all previous input, if there is the answer to: {QUERY}, answer the question\nPart of the input:\n{CTX}\"\nSetting:\nSystem Prompt: \"There are information about a passkey hidden in input. Please remember it.\"\nPrompt Template:\n\"Remeber your task: according to all previous input, if there is the answer to: {QUERY}, answer the question\n{SYSTEM}\nPart of the input:\n{CTX}\"\nSetting:\nSystem Prompt: \"There are information about a passkey hidden in input. Please remember it.\"\nPrompt Template:\n\"{SYSTEM}\nPart of the input:\n{CTX}\""}, {"title": "Trivia", "content": "[S|C|Q] Setting:\nSystem Prompt: \"Answer the question based on the given passage. Only give me the answer and do not output any other words.\"\nPrompt Template:\n\"{SYSTEM}\nThe following are some passages:\n{CTX}\nNow, if you can find the required information, answer the question based on those passages. Do not provide any explanation.\nQuestion: {QUERY}\nAnswer:\"\n[SIQIC] Setting:\nSystem Prompt: \"Answer the question based on the given passage. Only give me the answer and do not output any other words.\""}, {"title": "Distinguishing Prompt", "content": "The distinguishing prompt directly requires the LLM to discriminate whether the context contains answers to a given question.\nPrompt Template:\n\"You are a helpful assistant. I will give you a query and some contexts. Please help me with my question about the given query and contexts.\nQuestion: {QUERY}\nThe following are some passages:\n{CTX}\nDo the given contexts contain the answers to the given question? Use 'Yes' or 'No' to answer it. Do not provide any explanation."}]}