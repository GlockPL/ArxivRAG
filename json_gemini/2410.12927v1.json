{"title": "SoK: On Finding Common Ground in Loss Landscapes Using Deep Model Merging Techniques", "authors": ["Arham Khan", "Todd Nief", "Nathaniel Hudson", "Mansi Sakarvadia", "Daniel Grzenda", "Aswathy Ajith", "Jordan Pettyjohn", "Kyle Chard", "Ian Foster"], "abstract": "Understanding neural networks is crucial to creating reliable and trustworthy deep learning models. Most contemporary research in interpretability analyzes just one model at a time via causal intervention or activation analysis. Yet despite successes, these methods leave significant gaps in our understanding of the training behaviors of neural networks, how their inner representations emerge, and how we can predictably associate model components with task-specific behaviors. Seeking new insights from work in related fields, here we survey literature in the field of model merging, a field that aims to combine the abilities of various neural networks by merging their parameters and identifying task-specific model components in the process. We analyze the model merging literature through the lens of loss landscape geometry, an approach that enables us to connect observations from empirical studies on interpretability, security, model merging, and loss landscape analysis to phenomena that govern neural network training and the emergence of their inner representations. To systematize knowledge in this area, we present a novel taxonomy of model merging techniques organized by their core algorithmic principles. Additionally, we distill repeated empirical observations from the literature in these fields into characterizations of four major aspects of loss landscape geometry: mode convexity, determinism, directedness, and connectivity. We argue that by improving our understanding of the principles underlying model merging and loss landscape geometry, this work contributes to the goal of ensuring secure and trustworthy machine learning in practice.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning is now ubiquitous in various domains, from natural language processing [1] and computer vision [2] to time-series forecasting [3]. However, the sheer size and complexity of modern neural networks make it difficult to interpret the features they learn. This opacity poses issues for adoption in safety-critical applications such as healthcare [4], autonomous vehicles [5], and nuclear energy [6], where decision-making failures can be catastrophic. Moreover, deep learning practitioners, aware of the risks of harm associated with deploying neural networks, desire strategies to mitigate their potential misuse. Deep learning models are known to hallucinate faulty information [7], aid in creating inappropriate content [8], leak sensitive training data [9, 10, 11], and learn biased decision boundaries [12, 13]. Without a proper understanding of the inner mechanisms of neural networks, and specifically how neural network components relate to behaviors, we cannot develop effective solutions for the safe and ethical deployment of neural networks.\nA subfield of deep-learning that extensively studies proce-dures for associating neural network components (e.g., layers, neurons, or weights) with specific behaviors is model merging. Model merging explores procedures for altering the weights of existing models to produce more performant and general-izable predictors. A common motivation is to reuse existing models, especially as the time, computing infrastructure, and energy required to train models grow. The model merging problem is presented as follows: Given the parameters of several sources models \u03b81, \u03b82,..., \u03b8n, we seek to develop a merging procedure f(\u03b81,\u03b82,..., \u03b8n) : \u0398n \u2194 to produce a new merged model \u03b8* that minimizes some loss L(\u03b8). In particular, the merging methods produce a single model from several previously trained models, in contrast to traditional ensemble methods (see Section IV)\u2014precursors for model merging techniques-that require several models for inference incurring high latency costs. Thus, model merging can act as a cheaper alternative to ensembling. However, ensemble tech-niques need only combine models' prediction logits, whereas model merging techniques must compress several networks into a single model, leading to more potential points of failure. Empirical evidence suggests that neural networks form highly compressed representations of their training data even more so as they are overparameterized-resulting in sparse networks that can be pruned to a fraction of their original size [14, 15]. In principle, we should be able to exploit this sparsity to effectively merge neural networks trained on different tasks, but in practice, merging models trained on different tasks is challenging [16, 17, 18]. By contrast, ensemble methods are consistently successful in this endeavor. Moreover, simply training a monolithic multi-task model often outperforms both ensembling and merging\u2014forming a practical upper bound in performance for both techniques.\nModel merging is complicated by the permutation-invariant nature of neural networks (see Section III-B2), which prevents straightforward averaging between corresponding parameter values in each layer due to potential misalignment between layers of neurons without degrading performance [16, 19, 20]. Additionally, merging methods tend to degrade in performance as the training distributions of source models diverge from one"}, {"title": "B. Contributions", "content": "In this work, we explore model merging techniques through the lens of loss landscape geometry (see Section III-B) to illuminate potential areas of interest for interpretability and security researchers. We make the following contributions:\n1) We propose a taxonomy of model merging techniques focused on enumerating their core algorithmic principles.\n2) We synthesize empirical data from many model merg-ing studies to characterize phenomena that manifest on objective landscapes during training.\n3) We draw meaningful connections between model merging and research in model interpretability and security.\nThis paper is organized as follows: Section III defines some relevant terminology and introduces prerequisite con-cepts from the study of loss landscapes; Sections IV, V, and VI survey model merging research covering ensemble, weight aggregation, and neuron alignment techniques for model merg-ing, respectively; Section VII synthesizes the empirical obser-vations from this survey into descriptions of loss landscape geometry to characterize commonly observed phenomena dur-ing model training; Section VIII discusses the implications of our analysis on model interpretability and security; and Section IX proposes directions for future investigation in light of our analysis."}, {"title": "II. RELATED SURVEYS", "content": "While the model merging literature is extensive, we know of only a handful of other surveys in this area [41, 42, 43, 44]. Li et al. [41] and Yadav et al. [42] focus their discussion of model merging on two particular application areas, namely federated learning and mixture-of-experts regimes, and include techniques that do not attempt to manipulate weights directly, such as knowledge distillation. Meanwhile, Yang et al. [44] survey a range of merging techniques and taxonomize tech-niques according to when they are applied during the training pipeline, focusing on their relevance to application domains such as alignment, continual learning, and multimodal fusion. Li et al. [43] present a survey with a taxonomy similar to ours but focus on their application to federated learning, knowledge distillation, and fine-tuning.\nBy contrast, our work presents a taxonomy of model com-bination techniques organized into just three major categories: ensembling, weight aggregation, and neuron alignment (see Fig. 1). We do not include techniques that require full training runs, such as knowledge distillation. Our work focuses explic-itly on extracting insights into how training proceeds under SGD, connecting the success of model merging techniques to the underlying structure of the loss landscape, and applying these ideas to model interpretability and security."}, {"title": "III. BACKGROUND", "content": "Model merging is intimately connected to the study of loss landscapes and their geometry. Here, we introduce concepts from the loss landscape literature that inform contemporary approaches to model merging."}, {"title": "A. Model Spaces", "content": "Deep learning literature deals with several \"spaces\" over which we can analyze models. For clarity, we define these spaces here. Parameter space or weight space refers to the space of all model parameters. This is generally the domain of model merging techniques, which seek to analyze the weights of models and alter behavior via interventions on weights. Activation space refers to the space of all model activations. That is, the set of possible activation values at each layer in a particular deep learning model. This is generally the domain of interpretability techniques that compute summary statistics describing the relationship between data and model activations to isolate model components of interest."}, {"title": "B. Loss Landscapes", "content": "The loss landscape (also referred to as the objective land-scape) of a deep learning architecture is the high-dimensional surface induced by the loss function when evaluated over the domain of all possible parameter values [45]. Loss landscape geometry refers to the geometric structure of the loss land-scape, including the location of local minima and their relation to one another. A key concept is the idea of an objective mode-a particular local minimum of the loss landscape. An objective mode lies in an objective basin (or loss basin), which is comprised of the objective mode and the surrounding locally convex region of solutions. Model merging methods must account for the structure of the loss landscape to syn-thesize performant solutions from existing models. Therefore, studying model training behavior and loss landscape geometry is crucial to developing effective model merging techniques. Below, we review fundamental loss landscape concepts."}, {"title": "1) Mode Connectivity", "content": "Consider a given path, p(\u03b8\u03b1, \u03b8\u03b9, \u03bb), between two models \u03b8\u03b1 and \u03b8 parameterized by \u03bb\u2208 [0,1]. We will refer to \u03b8\u03b1 and \u03b8 as the endpoints of the path. The loss barrier, defined below in Eq. (1), on the path p is defined as the maximum increase in loss along p over the average loss of both endpoints.\nLp = max \u03bb\u2208[0,1] L(p(\u03b8\u03b1, \u03b8, \u03bb)) \u2013 1/2(L(\u03b8\u03b1) +L(\u03b8b)) (1)\nTwo models are said to exhibit Mode Connectivity if there exists a path p between them where the loss barrier is approx-imately zero [46, 47, 19]. Linear Mode Connectivity (LMC) [48, 47] is a stronger condition asserting that two modes can be connected by a straight line of special interest as linearly connected solutions may lie in the same convex basin. Mode-connecting paths reveal the structure of the loss landscape and allow us to find diverse but equally performant solutions between two known models [49].\nIn practice, we can discover paths of low loss between distinct neural networks via optimization (e.g., by parame-terizing a B\u00e9zier curve) [50, 51, 46, 52, 53]. Surprisingly, solutions are often connected by simple curves, and even linear mode connectivity seems commonplace [50, 47]. Recent work has exploited this fact to ensemble networks that are close in the objective landscape, indicating that despite lying close together, sufficient diversity exists between these solutions to provide an advantage from ensembling [50]. Some empirical evidence suggests that geodesics between neural networks in the distribution space induced by the Fisher-Rao metric may correspond to paths between their parameters in objective space [52]. This finding links the notions of mode connectivity"}, {"title": "2) Permutation Symmetry", "content": "Neural networks inherently have components that are invariant to permutations. For example, in a single linear layer of neurons, any permutation in the order of neurons would still result in the same activations for that layer. Therefore, we can consider any neural network to be a member of an equivalence class of other neural networks whose units are permuted but whose activations are the same. This permutation-invariance property has led some to conjec-ture that all solutions to Stochastic Gradient Descent (SGD) can be linearly connected if the natural permutation invariance of neural networks is accounted for [16]. This conjecture has spurred further investigation into the permutation symmetries inherent in the layers of neural networks and attempts to resolve them to achieve LMC between different solutions.\nSeveral theoretical results demonstrate that the objective landscapes of over-parameterized models are dominated by permutation-equivalent objective modes and that valleys in the loss landscape can be connected through permutation points [55, 56]. Additionally, many merging works provide substan-tial empirical evidence that aligning neurons by learning ap-propriate permutations in their order reveals linearly connected paths between models (see Section VI) [19, 20, 57, 51, 58]. This property holds even when models are trained on slightly differing data distributions [19, 20]. It has been observed that models trained on similar underlying distributions tend to have a similar distribution of singular values but may differ in their corresponding singular directions. This observation has led some to propose that the success of neuron alignment methods can be explained by the resulting alignment of dominant sin-gular directions in the weight matrices by learned permutations of the order of neurons [59]. This observation suggests we can meaningfully associate task-specific performance with just a few singular directions in parameter space [59, 60]."}, {"title": "3) Training Trajectories", "content": "The position of a neural network in the objective landscape is determined by the interaction between loss landscape geometry and gradient descent. There-fore, understanding training trajectories is paramount in model merging. A training trajectory is a sequence of points, each representing the parameters of a neural network, that evolves under some optimization algorithm, such as SGD, during the training process. Numerous efforts have been made to charac-terize how training proceeds from distinct initialization points under SGD and how this relates to the underlying objective landscape. Empirically, researchers have observed that SGD is initially unstable but soon reaches a \"stable point\". After this point, subsequent fine-tuning causes networks to converge to the same loss basin even under different forms of SGD noise (e.g., batch order) [61, 47]. Some work has also demonstrated the converse, that deep neural networks with different random initializations learn functions with distinct properties and lie in distinct regions [62]. These works all support the conclusion that neural networks that share a significant portion of their training trajectory tend to lie in the same linearly connected region of the objective landscape. Conversely, those that are initialized separately lie in distinct regions of the objective landscape. A corollary of this finding is that models in the same objective basin generally do not require any form of neuron alignment for merging [21]."}, {"title": "IV. ENSEMBLING", "content": "Ensembling is the most widely applied and reliable method to combine knowledge between models [63]. Ensemble meth-ods are precursors to modern model merging techniques. In this section, we review the origins of ensembling and several prominent ensembling methods to frame our subsequent dis-cussion of model merging techniques."}, {"title": "A. Overview", "content": "Ensembling combines the predictions (as opposed to pa-rameters) from several trained models to enhance accuracy at inference-time. Ensembles compute a weighted combination of output predictions from each model in the ensemble to form a prediction. The success of ensemble methods is often attributed to a reduction in model bias and variance that occurs when predictions are averaged from several statistical models [62, 63]. Ensembling is closely related to Bayesian Model Averaging techniques [64]. In classic Bayesian Model Averaging, the final prediction is a weighted average of the predictions of each of our models, weighted by the posterior probability of each model [65].\nFor neural networks, the posterior probabilities are calcu-lated over different weight settings:\np(y | x, D) = \u222b p (y | x, \u03b8) p (\u03b8 | D) d\u03b8 (2)\nwhere y is the data label, x the input data, D the train-ing data, and \u03b8 the neural network parameters. Integrating over all weight settings for a neural network is intractable; however, many works observe that neural networks consis-tently converge to solutions lying in large, flat basins in the loss landscape [64, 66, 67, 68, 69]. One explanation for this behavior is that these flat areas occupy large volumes in parameter space, and thus, stochastic gradient descent is exponentially more likely to land in these regions [66]. From a Bayesian perspective, these large, flat areas in parameter space have high probability density, p (\u03b8 | D), in the integral above. Therefore, an ensemble where we take the average predictions of several trained neural networks [70] can be thought of as an approximation of a Bayesian calculation of the posterior probability of y, with the integral estimated by the normalized summation of a few high probability parameter settings."}, {"title": "B. Ensemble Techniques", "content": "Despite the reliability of ensemble methods, they can be impractical because they require training and deploying several"}, {"title": "C. Limitations of Ensembling", "content": "Ensembles of several models have become much more straightforward to obtain thanks to advancements in our un-derstanding of the loss landscape [50, 72, 71]. Still, ensemble techniques suffer from poor scaling in terms of runtime and memory costs, as we must use multiple models for inference. MoE methods are a boon for use cases where inference cost is a limiting factor. Unfortunately, MoE methods are not necessarily a universal solution. While the effective parameter count for a given input in a MoE model is lower, the total number of parameters in the mixture may be the same or even greater [75]. As individual \"experts\" in the mixture specialize in just a few prediction domains, we must train more parameters in total to achieve strong performance across all domains. Consequently, training and serving MoE models requires constantly exchanging large numbers of parameters from memory. This requires a significant investment in com-plex infrastructure to serve models of billions or trillions of parameters. MoE models also exhibit decreased sample efficiency during training; each parameter is not trained during every batch, increasing training costs. Though these methods offer exciting avenues for scaling and deploying ensemble"}, {"title": "V. WEIGHT AGGREGATION", "content": "The fundamental technique underlying most model merging methods is weight aggregation. Given two or more models of the same architecture, one can compute an average or sum over the values for each corresponding parameter to obtain a set of new parameter values that comprise the merged model. These techniques are predicated on the principles of Bayesian Model Averaging, seeking to reduce estimation error from individual model bias and variance by combining several models for inference [76]. Though weight aggregation methods show promise compared to ensemble methods, they also introduce several challenges. Weight aggregation methods usually introduce additional hyperparameters that must be set via trial-and-error or by some heuristic method. Additionally, their failure modes are not fully understood. While some characterizations of their failure cases can be extrapolated from the literature (see Section VI below), more context on loss landscape geometry is necessary to fully understand their modes of failure. Below, we discuss several modes of weight aggregation, including simple averaging, weight steering via task arithmetic, and heuristic weighted averaging."}, {"title": "A. Simple Averaging", "content": "The most straightforward method of merging model pa-rameters is to take a simple average over each corresponding parameter between two or more models, see Eq. (3). This tech-nique is also employed in federated settings as FedSGD [27]. However, simple averaging does not consider each model's relevance to the task at hand. This method also consistently fails if the training distributions or the training trajectories between the two models have diverged [20, 21, 19]. Despite this limitation, simple averaging is particularly useful in the"}, {"title": "1", "content": "* = (\u03a3\u03b8\u03b5) / n (3)\nA host of methods attempt to improve performance by using variations of simple averaging. Stochastic Weight Aver-aging (SWA) [64] averages checkpoints from a single model at different points in its training trajectory, demonstrating comparable performance to FGE [50]. Model Soups [78] are simple averages of fine-tuned models of the same architecture trained with different hyperparameters. Model soups often en-hance Out-Of-Distribution (OOD) and zero-shot performance on downstream tasks. Sparse soups [79] reduce the inference costs associated with model soup methods by enforcing a sparsity constraint via an iterative prune-retrain fine-tuning technique similar to Iterative Magnitude Pruning (IMP) [80]. The similar SWAMP [81] method iteratively performs IMP and simple weight averaging between multiple fine-tuned models to produce a final sparse model. It has been observed that models closer to the center of a group of fine-tuned models stemming from the same pre-trained model display better OOD performance and robustness [82, 83].\nRecent work has also demonstrated that several networks can be trained in tandem such that they form a linearly con-nected subspace of solutions. The midpoint of this subspace approaches the accuracy of an ensemble of independently trained models and demonstrates robustness to label noise [84]. Model Stock takes advantage of this observation by estimating a \"central\" network using only two fine-tuned models, significantly reducing the training effort required to construct model soups [83]."}, {"title": "B. Weight Steering", "content": "Task Arithmetic [85] is a technique for manipulating model weights to enhance task-specific and OOD performance. A task vector-defined as the difference between the pre-trained and fine-tuned model weights for a task, see Eq. (4) can be thought of as the direction in weight space that the pre-trained model must travel to achieve better accuracy on that task.\n\u2206\u03b8task = \u03b8task - \u03b8pre (4)\nInterestingly, combining task vectors via simple arithmetic operations (addition and subtraction) provides predictable con-trol over model behavior. For example, subtracting a task vector from the pre-trained weights reduces performance, while adding the same task vector improves performance on the corresponding task; adding two task vectors for distinct tasks to the pre-trained model weights enhances the resulting model's performance on both tasks. This property is exploited by Model Breadcrumbs [86], a method that takes task vectors from several fine-tuned models, masks outlier directions, and combines them via addition into a single parameter update"}, {"title": "C. Heuristic Weightings", "content": "AdaMerging [97, 98] builds upon Task Arithmetic, learning merging coefficients for task vectors adaptively by minimizing prediction entropy over unlabeled test data. The idea of min-imizing prediction entropy originates in test-time adaptation"}, {"title": "E. Limitations of Weight Aggregation", "content": "Weight aggregation methods have similar advantages to ensembles, such as enhanced generalization, but with a sig-nificantly reduced cost. However, they are only applicable and well-defined for merging models of the same architecture. This property precludes their use for more general forms of knowledge transfer than those studied in this section. Another drawback is that their performance degrades as models diverge in their training trajectory. The studies surveyed generally attempt to merge fine-tuned models from the same pre-trained checkpoint; attempts to perform weight aggregation naively on independently trained models are usually unsuccessful [22]. Similarly, weight aggregation between models trained on tasks with significantly different data distributions tends to be unsuc-cessful. An exception is ZipIt! [101], which gracefully handles divergence in training trajectory and merging between models with different layer sizes this is because it only merges redundant neurons in each layer. ZipIt! also circumvents issues regarding neuron alignment between layers, as they do not naively merge neurons.\nThe tendency for weight aggregation methods to fail when training conditions differ may be explained by underlying permutation misalignment between neural networks [47, 55] or by significant differences in the underlying learned fea-tures [59, 22], which make it infeasible to directly combine corresponding parameters. Still, the constraints necessary for weight aggregation to succeed illustrate interesting properties of model training behavior. The ability to interpolate smoothly between fine-tuned models originating from the same check-point without increases in loss suggests that those models lie in the same linearly connected region. The predictable success of interpolating between fine-tuned models aligns with empirical data indicating that at a certain point in a model's training trajectory, the final linearly connected objective basin to which they will converge is pre-determined even if the training distribution is changed after this point in the trajectory [47].\nA linear path between weights has also been shown to reflect a linear path between learned features in some scenarios [39], particularly under the pre-training and fine-tuning paradigm [111]. This result may explain why some aggregation pro-cedures might succeed without accounting for permutation invariance. Moreover, the success of weight steering methods indicates that at least within an objective basin-we can accurately assign directions in parameter space to task-specific behavior. This property has also been noted by Gueta et al. [82], who observe a clustering in parameter space for LLMs fine-tuned from the same pre-trained checkpoint on semanti-cally similar tasks."}, {"title": "VI. NEURON ALIGNMENT", "content": "As opposed to naive weight aggregation schemes, neuron alignment techniques reduce potential degradation in the re-sulting merged model by first aligning the neurons of each source model and then performing weight aggregation. This prevents weight aggregation from destroying important fea-tures from source models, which may occur if the source"}, {"title": "A. Learning Permutation Matrices", "content": "Entezari et al. [16] conjecture that all objective modes are linearly connected if one accounts for permutation symmetries between modes. To this end, various neuron alignment tech-niques attempt to explicitly determine a permutation matrix that can align the neurons of one model to those of another. Ainsworth et al. [19] present two methods for aligning neurons via permutation matrices: activation matching and weight matching. Activation matching aligns each corresponding layer between two source models by finding a permutation matrix that minimizes the L2 distance of their activation patterns. Weight matching finds a permutation matrix that attempts to maximize the Frobenius product between weight matrices. Intuitively, weight matching aligns weights that are similar to one another and produce similar activations as a result. These methods are set up as a sum-of-bilinear-assignments problem and solved using an iterative approximation algorithm. Both methods are competitive with their third proposed method, which is to learn a permutation matrix directly via a straight-through-estimator (STE). This method updates the weights of the second source model, \u03b8b, to directly minimize the loss barrier (Eq. (1)), projecting the result to the nearest realizable permutation of \u03b8b's parameters. Notably, all three methods observe that their performance degrades as model width decreases and similarly worsens for models in the initial stages of training.\nTatro et al. [51] relate the neuron alignment problem to the assignment problem [112] and optimal transport. They"}, {"title": "B. Learning Alignment Maps", "content": "Some methods are not constrained to use permutation matrices but instead seek to learn unconstrained \"soft\" maps that enable neuron alignment for merging. Optimal Transport Fusion (OTFusion) [20] poses the model merging problem as an optimal transport problem and aligns neurons based on activation and weight similarities using an exact optimal transport solver, requiring only a single pass over all layers as opposed to other iterative neuron alignment methods (see Section VI-A). The optimal transport framework allows for merging between models with differing layer sizes. While previous methods explicitly constrain optimization to return permutation matrices, OTFusion imposes no such constraint. Instead, it finds a Wasserstein barycenter between the dis-tributions encoded by each source model. This amounts to optimization over the Birkhoff polytope [115] in n dimensions whose vertices are the traditional permutation matrices in Sn (if the model layer sizes are equivalent). In this regard, OTFusion is a generalization of prior methods that relate neuron alignment to the discrete assignment problem. Once again, we observe that OTFusion's performance improves with increased model width.\nOTFusion has since been extended to handle transformer architectures [116]\u2014specifically dealing with residual connec-tions, multi-head attention, and layer normalization\u2014as well as to align neurons across layers in heterogenous architectures (as opposed to only within corresponding layers) [117]. De-spite their good performance, even aligned neural networks sometimes demonstrate high loss barriers on the path between them. Jordan et al. [118] attribute this problem to a reduction in the variance of hidden layer activations during interpolation and propose an intermediate scaling layer that improves the performance of merged models."}, {"title": "C. Limitations of Neuron Alignment", "content": "Merging procedures combined with neuron alignment seem to resolve issues raised by prior work associated with merging models from independent training trajectories. Permutation-based methods also empirically reinforce Entezari et al.'s con-jecture [16] that many, if not all, objective modes are linearly connected if one accounts for permutation symmetries. One drawback of the current state-of-the-art for neuron alignment"}, {"title": "VII. INSIGHTS INTO TRAINING", "content": "The overlap between the literature on model merging and loss landscape geometry yields several insights into the nature of loss landscapes and the evolution of model representations during training. Here, we present insights from the model merging literature into the underlying geometric structure of the loss landscape and the effects of model architecture on learned representations."}, {"title": "A. Loss Landscape Macrophenomena", "content": "We outline macrophenomena consistently observed when model merging is performed. We study characterizations of these phenomena from investigations on loss landscapes and relate them to empirical data gathered through merging studies.\nMode Convexity: Objective basins are locally convex, permitting one to travel a short distance and still encounter a meaningful neural network solution with similar behav-ior. More importantly, models with roughly equivalent or lower loss but meaningfully different prediction behavior exist in proximity to any given solution. Fast Geometric Ensembling (FGE) [50] implicitly takes advantage of this phenomenon to rapidly discover quality ensemble members after training only a single model and Stochastic Weight Averaging (SWA) [64] exploits this phenomenon by averaging rather than ensembling several models from the same training trajectory. These studies suggest that local minima in the objective landscape contain models with sufficiently diverse behavior to gain a performance advantage by ensembling or merging them-despite their proximity in parameter space.\nMode Determinism: The final linearly connected objective basin to which deep learning models converge is determined early in the training process. This assertion is supported by experiments on linear connectivity [47] and by visual-izations of model clustering in parameter space [82]. SGD has been observed to occur in two phases: an initial pe-riod of instability, followed by a more stable, linear trajec-tory [47, 120, 121, 122, 123, 124]. Fort et al. [62] show"}, {"title": "B. Implications for Model Training", "content": "The direct comparison of model representations in merging studies allows us to derive information about the effects of model architecture and pre-training objectives on learned representations. Here, we outline some of these findings.\nModel Size: Neuron alignment procedures struggle as model width decreases. Ito et al. [59] investigate this phe-nomenon by studying the distribution of singular values in the weight matrices of source models. They conjecture that the effectiveness of neuron alignment techniques is predicated on their ability to align preferentially the singular directions corresponding to dominant singular values. They find that the proportion of dominant singular values decreases substantially as model width increases, suggesting that wider models are easier to align. Nguyen et al. [117] show empirical results suggesting that over-parameterized models propagate domi-nant principal components of their hidden representations at each layer. These results support the idea that a few dominant feature directions arise in over-parameterized models.\nAghajanyan et al. [15] measure the intrinsic dimensionality of language models and similarly conclude that LLMs form more compressed representations as they scale in size. These works suggest that larger models, despite their increased expressiveness, have fewer dominant directions in feature space. Some work has also attempted to clarify why over-parameterized models may display more amicable optimiza-tion behavior than smaller models. Huang et al. [66] and Li et al. [14] visualize the loss landscapes of models at various sizes and find that overparameterized models often converge to large, flat basins correlated with enhanced generalization capability [64, 66, 67, 68, 69, 126, 127] as opposed to small sharp minima. They explain this tendency by observing that such basins occupy far more volume in objective space as the dimensionality of a model increases. The increased volume of flat basins, coupled with the results presented by Simsek et al. [55] indicating that the number of permutation-equivalent basins increases exponentially as models grow in size, may explain the relative ease with which large models train: the"}, {"title": "VIII. DISCUSSION", "content": "Model merging is closely tied to model interpretability. Techniques in interpretability research, such as activation"}, {"title": "IX. CONCLUSION & FUTURE WORK", "content": "We have surveyed a range of model merging techniques and extracted insights from their empirical observations to characterize several aspects of the underlying structure of loss landscapes-namely mode convexity, mode determinism, mode directedness, and mode connectivity. We discussed the intimate connections between these observations and model interpretability and security, offering analogs for traditional activation space analysis in weight space.\nFuture work exploring the intersection of these fields might focus on understanding the relationship between the objective geometry of hierarchically related tasks, with the goal of understanding why semantically similar tasks produce so-lutions close together in parameter space, even when their raw data distributions (over, say, tokens) might seem quite dissimilar in principle [82]. Such investigations would touch on the ability of neural networks to abstract and generalize across tasks in a meaningful way. One could also attempt to \"orthogonalize\" task-specific directions in weight space to obtain interpretable decompositions over directions in pa-rameter space as an alternative to activation-based methods. Other extensions might further explore methods for continual learning, few-shot learning, or parameter-efficient fine-tuning through model merging techniques.\nA concerning implication of our work is that, due to the principle of mode determinism, models fine-tuned from pre-trained checkpoints\u2014particularly those fine-tuned on similar"}]}