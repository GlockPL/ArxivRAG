{"title": "OpenGeMM: A High-Utilization GeMM Accelerator Generator with Lightweight RISC-V Control and Tight Memory Coupling", "authors": ["Xiaoling Yi", "Ryan Antonio", "Joren Dumoulin", "Jiacong Sun", "Josse Van Delm", "Guilherme Paim", "Marian Verhelst"], "abstract": "Deep neural networks (DNNs) face significant challenges when deployed on resource-constrained extreme edge devices due to their computational and data-intensive nature. While standalone accelerators tailored for specific application scenarios suffer from inflexible control and limited programmability, generic hardware acceleration platforms coupled with RISC-V CPUs can enable high reusability and flexibility, yet typically at the expense of system-level efficiency and low utilization.\nTo fill this gap, we propose OpenGeMM, an open-source acceleration platform, jointly demonstrating high efficiency and utilization, as well as ease of configurability and programmability. OpenGeMM encompasses a parameterized Chisel-coded GeMM accelerator, a lightweight RISC-V processor, and a tightly coupled multi-banked scratchpad memory. The GeMM core utilization and system efficiency are boosted through three mechanisms: configuration pre-loading, input pre-fetching with output buffering, and programmable strided memory access. Experimental results show that OpenGeMM can consistently achieve hardware utilization ranging from 81.89% to 99.34% across diverse CNN and Transformer workloads. Compared to the SotA open-source Gemmini accelerator, OpenGeMM demonstrates a 3.58x to 16.40\u00d7 speedup on normalized throughput across a wide variety of GeMM workloads, while achieving 4.68 TOPS/W system efficiency.", "sections": [{"title": "1 INTRODUCTION", "content": "DNN models have been rapidly integrated into various aspects of our society, bringing a blossom of novel applications. However, they also bring a voracious demand for ever more computing power, presenting significant challenges for efficient execution. This problem is particularly severe when deploying DNNs at the edge, such as in-vehicle and wearable devices, where stringent power and area constraints exist [1].\nA variety of domain-specific DNN accelerators have emerged in recent years, with remarkable performance and energy efficiency [2, 3]. However, these accelerators are typically tailored to specific workloads [4\u20136], hindering their reusability across diverse applications. Furthermore, they are often equipped with dedicated control and data interfaces, posing substantial challenges to smoothly integrating them into standard SoCs. For instance, deploying the NVDLA accelerator [7] requires additional wrappers and a custom compiler for signal translation when communicating to a host system [8, 9].\nAlternatively, hardware acceleration platforms are proposed that include flexible DNN accelerators capable of targeting general computation kernels like general matrix multiplication (GeMM). These platforms integrate with a RISC-V host CPU, aiming to enhance hardware programmability and reusability [10\u201315]. While this solution appears promising, existing platforms face two important challenges: 1.) It is difficult to keep the control overhead negligible,"}, {"title": "2 EFFICIENCY AND VERSATILITY IN GEMM ACCELERATOR GENERATION", "content": "We start by diving into the GeMM accelerator hardware generator, which aims for efficiency across a wide variety of workloads, through a combination of design time configurability and run-time programmability, to always ensure maximal spatial and temporal data reuse."}, {"title": "2.1 GeMM Accelerator Dataflow", "content": "The dataflow of the GeMM accelerator is depicted in Figure 2. The accelerator targets GeMM operations of dimension (M, K, N) as shown in Equation 1:\n\n$C_{M,N} = A_{M,K} \\times B_{K,N}$"}, {"title": "2.2 Exploiting 3D Spatial Unrollings to Maximize Spatial Data Reuse", "content": "GeMM accelerator spatially process a tile matrix A' of size (Mu, Ku) and a tile matrix B' of size (Ku, Nu) to produce a tile matrix C' of size (Mu, Nu), as shown in Figure 3(a). To maximize spatial data reuse, it is important to reuse every fetched data element of A' and B' as much as possible. To ensure this, the GeMM array datapath is conceptualized as a 3D MAC (Multiply-Accumulate) array, as depicted in Figure 3. The 3D MAC array is organized as a (Mu, Nu)-sized mesh of Ku-sized vector dot product units (DotProd as detailed in Figure 3(b)) to spatially unroll all dimensions of matrices A', B', and C'. The 3D MAC array is adapted to match with the 3 nested spatial unrollings for GeMM processing. Specifically, vectors from A' matrix and vectors from B' matrix are broadcasted horizontally and vertically among the DotProd array, maximizing spatial data reuse. Within one DotProd, Ku multiplication results are combinatorially accumulated to get one result of C'.\nOpenGeMM supports design-time configurations of the DotProd array size and the size of each DotProd unit. This feature enables a flexible generation of a wide range of accelerators, such as dot-product units, outer-dot product units, vector-matrix multiplication accelerators, or matrix-matrix multiplication accelerators. Therefore, varied optimized spatial unrollings can be implemented based on these (Mu, Ku, Nu) parameters to accommodate the diverse computational requirements issued from different applications."}, {"title": "2.3 Exploiting Temporal Data Reuse to Minimize Memory Access", "content": "Temporal unrollings should be properly ordered to maximize the local data reuse and avoid unnecessary memory conflicts. Depending on whether the innermost temporal loop is weight-relevant or output-relevant, the dataflow can typically be categorized into weight-stationary and output-stationary. While weight-stationery keeps weight unchanged, output stationary dataflow fits better with GeMM operation, which can be verified through existing DNN dataflow design space exploration (DSE) frameworks [20]. The underlying rationale is the precision of the partial sum is often larger than the weight, leading to higher cost when the partial sum is to be updated every cycle. Take a typical convolutional layer with input tensor size of (Ox, Oy, C) and kernel size of (K, Fx, Fy, C) as an example, after im2col [21], the convolution operation is translated to matrix multiplication with matrix A of the size of (Ox Oy, Fx Fy C) and matrix B of the size of (FxFyC, K). Since Fx. Fy C is typically much larger than Ox Oy, reusing the partial sum in Fx. Fy C dimension temporally saves more data bandwidth and energy. With this observation, OpenGeMM implements an output stationary dataflow supported by an output accumulation register inside each DotProd unit, with the innermost temporal loop iterating from k1 = 0 to K/Ku.\nAll these 6 nested loop processes are handled through a built-in hardware loop controller within the GeMM accelerator, which is in charge of the timely input data request, outputting of result data, and accumulator resets. GeMM accelerator can be programmed at run-time with maximum hardware loop upper bound when the required data amount reaches the on-chip buffer capacity. For even larger matrices, the GeMM accelerator can be called multiple times through software controllers, eg., RISC-V core, to handle extra tiling as more nested temporal loops on higher-level memories."}, {"title": "3 SYSTEM ARCHITECTURE FOR HIGH UTILIZATION AND PROGRAMMABILITY", "content": "To guarantee the GeMM accelerator can be smoothly integrated with standard SoCs, we propose the OpenGeMM platform (Figure 1)"}, {"title": "3.1 OpenGeMM System Architecture", "content": "to enhance programmability and maximize GeMM core throughput. It incorporates the GeMM accelerator mentioned earlier, as well as a compact RV32I host core, a tightly coupled multi-bank scratchpad memory, and three data streamers for efficient memory access. OpenGeMM offers extensive customization capabilities, with design time parameters summarized in Table 1."}, {"title": "3.2 Configuration Pre-loading", "content": "Due to the sequential programming of numerous CSRs, including hardware loop bounds related to workload size (M, K, N), base addresses and strides related to memory access, the programming cycle can be lengthy, as shown in Figure 4(a) 1. To hide the configuration time, we introduce a configuration pre-loading (CPL) mechanism. It allows the host to pre-load the GeMM core configuration for the next computation, while the current computation is being executed, effectively overlapping configuration time with computation time, as depicted in Figure 4(b) 0."}, {"title": "3.3 Input Pre-fetch and Output Data Buffering", "content": "To minimize computation stalls, we incorporate a data buffer within the data streamer to allow continuous pre-fetching of data until the buffer reaches its capacity. As long as there is space in the buffer (data being consumed by the GeMM core), it will actively pre-fetch data. This dynamic producer-consumer mechanism aims to maintain high utilization of the GeMM array. The buffer depth is a configurable parameter set at design time, based on the amount of data that would like to be pre-fetched. Figure 4(b) illustrates that immediately after configuring the streamer, the data buffers begin pre-fetching data while the GeMM core is being configured.\nThe output stationary nature ensures that the GeMM core writes back a result every K/Ku computational cycles, facilitating the reduction of output memory stalls through an output buffering mechanism. Specifically, a configurable number of output data buffers alternate between storing the output from the GeMM core and writing the output matrix to memory in a round-robin fashion. Figure 4(b) illustrates the timing utilization of the write ports. This allows computation to proceed concurrently with writing the output of the last result. Without the input pre-fetch and output data buffering, the GeMM core utilization can be lower because of idle cycles caused by input and output memory stalls, as shown in Figure 4(a) and 3."}, {"title": "3.4 Strided Memory Access", "content": "The compiler can exploit strided data access [24] to optimize the data layout by interleaving access, thereby minimizing bank conflicts and maximizing the utilization of the GeMM core. Figure 4(c) illustrates an example of data layout optimization. Figure 4(c) shows a set of matrices and their sub-matrices for GeMM operation input. These matrices can be organized contiguously in either row-major or column-major order in memory as depicted in Figure 4(c). This layout will lead to bank contentions because sub-matrices A1 and B1 are accessed in the same bank. However, by transforming the data layout as shown in Figure 4(c), bank contentions can be avoided.\nTo address this, we equip each data streamer with a configurable strided address generation [24] unit (AGU) to support flexible data access to relieve the memory bank contentions. Specifically,\n\u2022 At design time, we configure the AGU such that it matches the GeMM core's port width and the address generation pattern, such as how many nested loops are needed for strided address generation, of operands A, B, and C. These are also related to the number of multi-banked SPM read ports Rmem, write ports Wmem, and their width Pword.\n\u2022 At run time, we program the hardware loop bounds, base addresses, and two-dimensional memory strides for each data streamer to generate the corresponding data layout address."}, {"title": "4 OPENGEMM EVALUATION AND SO\u03a4\u0391 COMPARISON", "content": "We evaluate the proposed OpenGeMM system at the register transfer level (RTL), utilizing Chisel [25] for designing the GeMM accelerator hardware generator and SystemVerilog to implement all other platform components built upon [22]. We generate one OpenGeMM instance using the case study parameters listed in Table 1. We choose an 8x8x8 GeMM array to achieve a good balance between spatial utilization and hardware throughput for typical GeMM workloads. For performance and utilization evaluation, we conduct cycle-accurate RTL simulation using Verilator. The system is then synthesized with Synopsys Design Compiler under the TSMC 16nm FFC process technology at a clock frequency of 200MHz and a supply voltage of 0.675V. Power analysis is conducted using Synopsys PrimeTime."}, {"title": "4.1 Evaluation Setup", "content": "4.2 Utilization Analysis\nAn ablation experiment is performed to assess the effectiveness of the proposed three mechanisms in enhancing the GeMM core's utilization. We conduct experiments on the 8x8x8 GeMM accelerator with 500 different computational matrix sizes (M, K, N), randomly"}, {"title": "4.3 Real DNNs Benchmarking", "content": "To evaluate the performance of OpenGeMM in practical scenarios, several typical CNN and Transformer workloads, including ResNet18 [28], MobileNetV2 [29], Vision Transformer [30] and Bert-Base [31] are benchmarked. Our focus is specifically on the energy- and latency-dominant blocks, including convolutional layers (executed via im2col [21]), multi-head attention, multilayer perceptron layers, and fully connected layers within these models."}, {"title": "4.4 Area and Power Evaluation", "content": "The workload for system power estimation involves block matrix multiplication with a size of (32, 32, 32). OpenGeMM system occupies a cell area of 0.531 mm\u00b2 and consumes a total system power of 43.8 mW when operating at 200MHz. Achieving a peak performance of 204.8 GOPS, the OpenGeMM demonstrates a system efficiency of 4.68 TOPS/W.\nThe detailed area and power breakdown of the OpenGeMM system is illustrated in Figure 6, with all other system components encompassed, such as the instruction cache and DMA. The largest area component is the 270KiB multi-banked SPM including the interconnect towards the streamers, occupying 63.47% of the total area breakdown, followed by the GeMM core at 11.86%. In terms of power breakdown, multi-banked SPM accounts for 41.90%, instruction cache for 17.06%, and GeMM core for 13.18%. Data streamers for data movement occupy 2.26% of total area and 6.5% of total system power. It is important to note that the RISC-V control overhead is negligible, around 1.13% of the entire system cost and 2.4% of system power, offering OpenGeMM efficiency with minimal hardware overheads."}, {"title": "4.5 State of The Art Comparison", "content": "We now compare OpenGeMM against the state-of-the-art (SotA) of flexible DNN acceleration systems, as summarized in Table 3. Specifically, we benchmark the throughput of OpenGeMM against the SotA GeMM accelerator generation system Gemmini [12], utilizing performance data from [32].\nFigure 7 illustrates the area-normalized throughput comparison (in GOPS/mm\u00b2) among the Gemmini in output-stationary (OS) mode and weight-stationary (WS) mode, and OpenGeMM across matrix sizes ranging from (8, 8, 8) to (128, 128, 128). Compared to Gemmini OS and WS, OpenGeMM exhibits normalized throughput speedups ranging from 3.75x to 16.40\u00d7 and 3.58x to 15.66\u00d7, respectively. The exceptional performance of OpenGeMM is attributed to the high utilization of the GeMM core, approaching ideal peak performance for these workloads. In contrast, Gemmini falls short of maintaining high temporal utilization (on average 6.25%) on these workloads, because of intensive memory stalls, resulting in lower actual throughput.\nIn summary, OpenGeMM's lightweight RISC-V core and tight memory coupling reduce hardware overhead from control and data access while the GeMM core architecture maximizes spatial and temporal data reuse. Moreover, OpenGeMM introduces three mechanisms to achieve sustained high utilization. This results in OpenGeMM achieving high system energy efficiency and the best operation-area efficiency across all SotAs with int8 data precision. These features make OpenGeMM suitable for efficient edge DNN computing."}, {"title": "5 CONCLUSION", "content": "This paper introduces OpenGeMM, an open-source GeMM acceleration platform targeting edge AI applications. OpenGeMM is built around a Chisel-based GeMM accelerator, associated with a lightweight RISC-V processor and a tightly coupled memory system. To improve the hardware utilization and therefore the processing efficiency, three mechanisms are introduced at the system level. The experiments demonstrate that these mechanisms can improve the MAC array utilization of OpenGeMM consistently up to 81.89%-99.34% across various DNN workloads. Compared with the SotA, OpenGeMM shows 4.68 TOPS/W system power efficiency and 3.58\u00d7 to 16.40\u00d7 normalized throughput speedup."}]}