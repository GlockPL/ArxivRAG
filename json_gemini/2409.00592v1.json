{"title": "Hyper-Compression: Model Compression via Hyperfunction", "authors": ["Feng-Lei Fan", "Juntong Fan", "Dayang Wang", "Jingbo Zhang", "Zelin Dong", "Shijun Zhang", "Ge Wang", "Tieyong Zeng"], "abstract": "The rapid growth of large models' size has far outpaced that of GPU memory. To bridge this gap, inspired by the succinct relationship between genotype and phenotype, we turn the model compression problem into the issue of parameter representation to propose the so-called hyper-compression. The hyper-compression uses a hyperfunction to represent the parameters of the target network, and notably, here the hyperfunction is designed per ergodic theory that relates to a problem: if a low-dimensional dynamic system can fill the high-dimensional space eventually. Empirically, the proposed hyper-compression enjoys the following merits: 1) Preferable compression ratio; 2) No post-hoc retraining; 3) Affordable inference time; and 4) Short compression time. It compresses LLaMA2-7B in an hour and achieves close-to-int4-quantization performance, without retraining and with a performance drop of less than 1%. Our work has the potential to invigorate the field of model compression, towards a harmony between the scaling law and the stagnation of hardware upgradation.", "sections": [{"title": "Introduction", "content": "Recently, the escalating demand for memory and computational resources by large models has presented formidable challenges for their deployment in resource-constrained environments [7], particularly with the stagnation of hardware Moore's Law [22]. Besides technical obstacles, the substantial cost associated with deployment also engenders profound ethical concerns. The dominance of major institutions in the realm of large models introduces risks pertaining to integrity, bias, security, and other critical aspects. Thus, how to serve large models well is not only a technological imperative but also a pursuit for social good.\nOne prevalent way to serve large models is to develop effective model compression approaches that crop the size of large models while maintaining acceptable performance levels. Currently, the landscape of model compression predominantly revolves around four fundamental algorithms: pruning, quantization, knowledge distillation, and low-rank approximation [29] whose basic frameworks have been established years ago. However, the compression efficacy of these methods is either capped or challenging to scale. For instance, even the most extreme case of quantization, represented by 1-bit integers, remains upset because the compression ratio is at best a constant.\nHere, we introduce a novel and general-purpose approach that redefines model compression as a problem of parameter representation. This concept stems from the principles of hyper-networks, wherein smaller networks can generate weights for a significantly larger target network. We extend the concept of hypernets into what we term a 'hyperfunction' (HF), that a small parametric function can generate weights of a large network to advocate hyper-compression. Mathematically, we use a parametric function $W_n = h(n; \\Theta)$ to encode the relationship between locations and weights of the target network, where $W_n$ is the n-th parameter of the target network, and $\\Theta$ is the hyperparameters of h. Should the memory footprint of $\\Theta$ be significantly less than that of W, the function h can be stored on devices, allowing for the dynamic querying of weights during inference and thus facilitating the efficient deployment of large-scale models."}, {"title": "Result and Analysis", "content": "This section illustrates the efficacy of our novel compression methodology on three widely utilized large, middle, and small models: LlaMA2-7B [21], UNet [18], and MobileNetV3 [13]. Distinctively, our compression technique does not necessitate post-hoc retraining, even at high compression ratios and small performance drop. For example, we achieve up to a 7.93\u00d7 reduction in the model size of UNet within 1% performance drop. This is a significant improvement over traditional methods that often require extensive retraining to restore efficacy. This capability to compress without much loss of performance potentially sets a new benchmark in model compression. Detailed descriptions of our algorithm and experiments are provided in Supplementary Information."}, {"title": "Materials and Methods", "content": "Motivation\nA human genome of only approximately 30,000 genes can remarkably encode the development of a human brain that has finally billions of neurons and trillions of synaptic connections [20]. This observation suggests the existence of a concise and implicit mapping from genotype to phenotype, capable of expanding a limited number of genetic instructions into a vast array of biological functionalities. Inspired by this efficient genetic encoding, the idea of hypernet is to use a small network (genotype, called hypernet) to generate weights for the target network (phenotype).\nWe consider the hypernet in the setting of model compression. Furthermore, we generalize the idea of hypernet to a parameterized function (hyperfunction). The biological observation is the existence of a concise"}, {"title": "The Trend of Growth of Large Models", "content": "In recent years, the field of natural language processing (NLP) has witnessed remarkable advancements, particularly with the development of large language models (LLMs). These large models have demonstrated impressive capabilities in various language-related tasks, with a rapid increase in size over the past few years. For instance, GPT-3 consists of a staggering 175 billion parameters. Recently, M6, one of the largest models to date, has over 104 billion parameters. This growth in model size has been driven by the desire to improve the model's complex language understanding by scaling law. However, the growth of GPU memory has not kept pace. GPUs, which are crucial for training and serving these models, have seen relatively modest increases in memory capacity. This disparity between models' size and GPU memory has created significant challenges in the development of large models.\nSpecifically, the disparity between the model size and GPU memory brings the following problems:\n1. Memory Bottleneck: The limited GPU memory poses a bottleneck for serving LLMs. When deploying LLMs for real-world applications, the entire model and its associated parameters need to fit within the GPU memory. However, as LLMs continue to grow in size, it becomes increasingly difficult to accommodate them within the available memory constraints. This limitation hampers the deployment and wide adoption of LLMs in practical scenarios.\n2. Ethic Issues: Since large models are hard to serve, large institutes will gradually dominate the development and deployment of large language models, which raises ethical concerns:\nAlgorithmic Bias: LLMs developed by dominant institutes may reflect the biases present in the data they were trained on. This can perpetuate existing societal biases and discrimination, leading to unfair outcomes in decision-making processes.\nLack of Transparency: Large institutes may not always disclose the full details of their model architectures, training data, or decision-making processes. This lack of transparency can hinder accountability, making it difficult to assess the fairness and reliability of LLMs in real-world applications.\nEthical Use Cases: The ethical implications of deploying LLMs in various applications, such as content moderation, surveillance, or healthcare, need to be carefully considered. Large institutes' control over these technologies raises concerns about how they are used and the potential impact on individuals and society.\nAs the demand for more powerful models and the risk of monopoly by large institutes continue to rise, addressing the disparity between model size and GPU memory is a paramount research topic. Particularly, we need"}, {"title": "Conclusion and Future Work", "content": "In this study, we have proposed hyper-compression, a novel and general-purpose methodology for model compres-sion that leverages hyperfunction to encode the parameters of the target network. We have conducted experiments on various architectures, including LlaMA2-7B, UNet, and MobileNetV3, demonstrating that our approach is both user-friendly and scalable. Nevertheless, it represents merely an initial foray into the whole blueprint. Future directions include 1) theoretically describing the relationship among the error, the length of $\\Theta$, and the dimension K to reveal the essence of using the ergodic theory to realize compression; 2) beyond classical theory, exploring other possibilities such as implicit neural representation [3] to strike a better balance between compression time, inference time, and compression ratio. We believe that hyper-compression can contribute to Moore's law of model compression in the near future, i.e., the compression efficiency of model compression algorithms can be doubled annually, to meet the need of large models's development."}, {"title": "Supplementary Information", "content": ""}, {"title": "I. Limit of the Compression Ratio of Pruning Convolutional Networks", "content": "In the main body, we mention that pruning-based methods are hard to scale. Here, we show that the compression ratio of pruning a convolutional network is constantly bounded by studying a randomly generated infinite network and introducing the percolation theory [10] that characterizes the connectivity of a network when links are removed per certain rules.\nWe study one-dimensional convolutional networks whose topology is shown in Figure 2. We assume any edge of a network is randomly removed with the same probability. This is also equivalent to the following process: Randomly generate weights for edges of a given network. If the weight of an edge is higher than a threshold, the edge is retained; otherwise, it is removed. In the jargon of percolation theory, an edge is open or closed if the edge exists or disappears. When the probability of removing edges reaches a threshold, referred to as the percolation threshold, there will be no connected clusters from the input and output, meaning that the network will always yield 0 for any input. The probability threshold indicates the maximal proportion of a network's edges to prune. This section discusses the upper and lower bounds of the percolation threshold for a class of graphs defined by the one-dimensional convolutional network, thereby quantitatively characterizing the compression ratio.\nLet us first provide relevant definitions below:\nDefinition 1 (Graph, Vertex, Edge). A graph G = (V, E) is an ordered pair of sets, where V is the vertex set, and E is the edge set. In the edge set E, we present the edge between v, w \u2208 V as (v, w).\nThe class of graphs we investigate is $G_r = (V_r, E_r)$, $r \\in \\mathbb{Z}_{>2}$, where $V_r = \\mathbb{Z}^2$, $E_r = \\{((m,n), (m+1, n+i)) \\mid m, n \\in \\mathbb{Z}, i \\in \\{0, 1, \\ldots ,r - 1\\}\\}$, that is, a convolution with the kernel size r and stride 1.\nDefinition 2 (Open and Closed). Let p \u2208 [0,1]. Declare each edge of $G_r$ to be open with probability p and closed otherwise, independently of all other edges.\nDefinition 3 (Open Cluster). Consider the random subgraph of $G_r$ containing the vertex set and the open edges only. The connected components of this graph are called open clusters. We write $C_r(v)$ for the open cluster containing the vertex v, and we call $C_r(v)$ the open cluster at v.\nDefinition 4 (Percolation probability and critical probability). Let $\\theta_r(p)$ being the probability that given vertex, v, belongs to an infinite open cluster in $G_r$, that is $\\theta_r(p) = \\mathbb{P}(| C_r(v) |= \\infty)$. By the translation invariance of those graphs, we lose no generality by taking this vertex as the origin and denoting $C_r(0, 0)$ as $C_r$. The critical probability is defined to be $p_{c,r} = \\sup\\{p \\in [0,1] \\mid \\theta_r(p) = 0\\}$.\nEarlier, researchers have mainly focused on planar graphs, but we observe that $G_r$ is a planar graph only when r = 2, and is non-planar for r\u2265 3, part of $G_2$ and $G_3$ are shown in Figures 2 and 3. Therefore, many methods, such as using the duality of planar graphs and the star-triangular transition, cannot be applied. In the following, we will first solve the critical probability for the case of r = 2, and then study the upper and lower bounds of the critical probability for r > 3. Our method is degenerating a non-planar graph into a planar graph by removing edges.\nProposition 1 (Critical Probability of $G_2$). $p_{c,2} = 0.5$.\nProof of Proposition 1. Let $L^2$ be the square lattice on $\\mathbb{Z}^2$, whose percolation threshold is well-known to be 0.5 [10]. Consider $\\phi: \\mathbb{Z}^2 \\to \\mathbb{Z}^2$ defined by $\\phi(m, n) = (m - n, n)$. We are going to show $\\phi$ is graph isomorphism."}, {"title": "II. The Trend of Growth of Large Models", "content": "In recent years, the field of natural language processing (NLP) has witnessed remarkable advancements, particularly with the development of large language models (LLMs). These large models have demonstrated impressive capabilities in various language-related tasks, with a rapid increase in size over the past few years. For instance, GPT-3 consists of a staggering 175 billion parameters. Recently, M6, one of the largest models to date, has over 104 billion parameters. This growth in model size has been driven by the desire to improve the model's complex language understanding by scaling law. However, the growth of GPU memory has not kept pace. GPUs, which are crucial for training and serving these models, have seen relatively modest increases in memory capacity. This disparity between models' size and GPU memory has created significant challenges in the development of large models.\nSpecifically, the disparity between the model size and GPU memory brings the following problems:\n1. Memory Bottleneck: The limited GPU memory poses a bottleneck for serving LLMs. When deploying LLMs for real-world applications, the entire model and its associated parameters need to fit within the GPU memory. However, as LLMs continue to grow in size, it becomes increasingly difficult to accommodate them within the available memory constraints. This limitation hampers the deployment and wide adoption of LLMs in practical scenarios.\n2. Ethic Issues: Since large models are hard to serve, large institutes will gradually dominate the development and deployment of large language models, which raises ethical concerns:\nAlgorithmic Bias: LLMs developed by dominant institutes may reflect the biases present in the data they were trained on. This can perpetuate existing societal biases and discrimination, leading to unfair outcomes in decision-making processes.\nLack of Transparency: Large institutes may not always disclose the full details of their model architectures, training data, or decision-making processes. This lack of transparency can hinder accountability, making it difficult to assess the fairness and reliability of LLMs in real-world applications.\nEthical Use Cases: The ethical implications of deploying LLMs in various applications, such as content moderation, surveillance, or healthcare, need to be carefully considered. Large institutes' control over these technologies raises concerns about how they are used and the potential impact on individuals and society.\nAs the demand for more powerful models and the risk of monopoly by large institutes continue to rise, addressing the disparity between model size and GPU memory is a paramount research topic. Particularly, we need"}, {"title": "III. Algorithmic Details", "content": "Anchored in the hyperfunction driven by ergodic theory mentioned in the main body, we design a general-purpose model compression method that enjoys the following merits (PNAS): Preferable compression ratio, No post-hoc retraining, Affordable inference time, and Short compression time. In this section, we describe in detail our entire model compression method which is made of four parts, and each part corresponds to one facet of merits.\nPreferable compression ratio\nThe core objective of the algorithm is to identify the smallest $\\theta^*$ for points in hypercubes $[0, 1]^K$ under the condition that $\\epsilon$ is sufficiently small and aiming for the highest feasible dimension K. To this end, we consider the distribution of weights of the original network by binning weights into three groups based on magnitudes of weight values.\n*   First, we flatten all parameters into a one-dimensional vector $W = [W_1,W_2,\\ldots,W_N]$, where N is the total number of weights, and then split it into groups $[W^{(1)}, W^{(2)}, ..., W^{(G)}]$, where $W^{(g)} = [W_{K(g-1)+1},W_{K(g-1)+2},\\ldots, W_{K(g-1)+K}],g = 1,2,\\ldots, G$ is of $R^{1\\times K}$, and G is the number of groups. For any $W^{(g)}$, there exists an integer $\\theta$ such that $|W^{(g)} - \\tau(\\theta \\cdot a)| < \\epsilon ,g = 1,\\ldots,G$ and $a = [a_1, a_2,\\ldots, a_K]$. Finally, the vector $[\\theta_1, \\theta_2, \\cdots, \\theta_G]$ represents a compression to W. Take $K = 2$ and $W = [W_1, W_2, W_3, W_4] = [0.1, 0.2, 0.4, 0.5]$ as an example, we split W as $[W^{(1)}, W^{(2)}]$, where $W^{(1)} = [W_1, W_2]$ and $W^{(2)} = [W_3, W_4]$, and derive $\\theta_1$ and $\\theta_2$ for $W^{(1)}$ and $W^{(2)}$, respectively. As Figure 7 shows, we can find a $\\theta$ for $(x, y) = (\\tau(\\theta/(\\pi + 1))), \\tau(\\theta/(\\pi + 2)))$ that approximates a point in 2D space.\n*   Given a parameter vector W, $[\\theta_1, \\theta_2, \\cdot, \\theta_G]$ is a compression. However, we do not want $max(\\theta)$ to be a large number, as this would require more storage space. Therefore, we define an integer U as the upper bound of $\\theta$ to ensure the preferable compression efficacy. Next, $[\\theta^*_1, \\theta^*_2,\\cdots, \\theta^*_G]$ can be stored using the data type uintm, where $m = [log_2(max(\\theta)) + 1]$, allowing $[\\theta^*_1, \\theta^*_2,\\cdots, \\theta^*_G]$ to be stored using the minimum possible number of bits without causing data overflow. In other words, given a target two-dimensional point $W^{(g)}$ and U, what we are doing is finding an integer $\\theta \\in [0, U]$ such that the Euclidean distance between $\\tau(\\theta \\cdot a)$ and $W^{(g)}$ is minimized. As U increases, potentially, more integers can be explored to make the loss $|W^{(g)} - \\tau(\\theta \\cdot a)|$ smaller. Each layer can select a different U, allowing important layers to choose a larger U to ensure the approximation error remains sufficiently small.\n*   In our algorithm, we set K = 2 and replace the unit square $[0, 1]^2$ with a more flexible box $[a, b] \\times [c, d]$. Specifically, given W, $[a, b] \\times [c, d]$ is defined as a square with side length l, centered at $(\\overline{x_i}, \\overline{y_i})$, where $(\\overline{x_i}, \\overline{y_i})$ is the centroid of two-dimensional points $W^{(1)}, W^{(2)},...,W^{(G)}$. l is a hyperparameter and typically set to 0.01 for optimal results. Consequently, the function $\\tau(x) := x - [x]$ in the ergodic theorem is redefined as $\\tau(z) := f(z) + (\\lfloor\\overline{x_i}, \\overline{y_i}\\rfloor - [\\frac{l}{2}, \\frac{l}{2}])$, where $f(z) = z \\mod l$.\n*   In the ergodic theorem, $a = [a_1, a_2] = d \\cdot l$ is a two-dimensional directional vector, where d and I determine the direction vector and step size for the movement of curves defined by the parametric equation, respectively. Since $\\theta$ is an integer no more than U, at most we have U + 1 points in two-dimensional space $(\\tau(a_1 \\theta), \\tau(a_2 \\theta))$, $\\theta = 0,1,\\ldots,U$. In our experiment, we define an optimal vector a based on U, such that the distribution of U + 1 points is more uniform, thereby minimizing the maximum error as much as possible. Specifically,\n$\\begin{aligned}\na &= d \\cdot l \\\\\n&= \\frac{[\\frac{l}{\\epsilon}, \\frac{l}{\\epsilon}]}{\\sqrt{2}} \\cdot l \\\\\n&= \\frac{[\\frac{l}{\\epsilon}, \\frac{l}{\\epsilon}] \\cdot \\frac{1}{\\sqrt{2}} l}{\\frac{\\frac{l}{\\epsilon}}{\\sin \\alpha} [\\sqrt{U}]} ,\n\\end{aligned}$ (3)\nwhere $tan \\alpha = \\frac{1/U}{1/U} = U = U$. As shown in Fig. 8, it demonstrates that using (3) results in a more uniform distribution of the sample nodes compared to defining a as follows:\n$a = [1/(\\pi + 1), 1/(\\pi + 2)].$ (4)\n*   If we construct a square with its center at the centroid $(\\overline{x_i}, \\overline{y_i})$ and side length l, it is highly probable that many points in $W^{(1)}, W^{(2)}, ..., W^{(G)}$ will fall outside this square. Therefore, we need to first scale these points into the square using a scaling factor s to compress, and then scale the substituted points $\\tau(\\theta \\cdot a)$ back during decompression. While the error rate keeps intact, this inversion will amplify the error $\\epsilon$, where $\\epsilon = |W^{(g)} - \\tau(\\theta^* \\cdot a)|$. It should be noted that, assuming $W^{(F)}$ is the point farthest from the centroid $(\\overline{x_i}, \\overline{y_i})$ in $W^{(1)}, W^{(2)}, ..., W^{(G)}$, we define the farthest distance $l_f$ as $|W^{(F)} - [\\overline{x_i}, \\overline{y_i}]|$, and then the scaling factor $s = \\frac{l}{2l_f}$. The specific formula derivation is as follows:\n$\\begin{aligned}\nW^{(in)} &= W^{(out)} - C \\\\\nCW^{(out)} &= \\frac{l}{2l_f} - S \\\\\n&= CW^{(out)} \\cdot s ,\n\\end{aligned}$ (5)\nwhere C is the centroid node, $W^{(out)}$ is a node ouside the squre, $W^{(in)}$ is the scaled node of $W^{(out)}$.\n*   We denote $\\tau(\\theta^* \\cdot a)$ as $W^{(in)*}$, so that $\\epsilon = W^{(in)*} - W^{(in)}$. The detailed scale-back process is shown as follows:\n$W^{(out)*} = \\frac{CW^{(in)*}}{\\frac{l}{2l_f} - s} + C$(6)\nTherefore, we can derive the error:\n$\\begin{aligned}\nerror &= |W^{(out)} - W^{(out)*}| = |\\frac{CW^{(out)}}{\\frac{l}{2l_f} - s} - \\frac{CW^{(in)*}}{\\frac{l}{2l_f} - s}| \\\\\n&= |\\frac{CW^{(out)} - CW^{(in)*}}{\\frac{l}{2l_f} - s}| \\\\\n&= |\\frac{\\epsilon}{\\frac{l}{2l_f} - s}| \\\\\n&= \\epsilon  \\frac{2 \\cdot l_f}{l}\n\\end{aligned}$ (7)\nFrom (7), we can observe that the farthest distance $l_f$ determines the error of estimating $W^{(out)}$ after scaling back.\nTherefore, as shown in Fig. 9 in our algorithm, we classify the points outside the square into M categories based on their distances from the centroid C, where M is a hyperparameter, so that the points in different categories can utilize different farthest distance $l_{fm}$ to define different scale factor $s_m$. This can mitigate the adverse impact of the global farthest distance $l_f$ on the final error as much as possible. This method is highly effective in practice, as a significant portion of the parameter nodes $W^{(1)}, W^{(2)}, ..., W^{(G)}$ exhibit a pattern that most points are close to the centroid.\nSpecifically, assuming the points outside the square are divided into M categories and $W^{(m)}$ is a point that belongs to the m-th category, the scaling factor $s_m$ for this point is defined as\n$s_m = \\frac{l^{1/2}}{l^{1/2} + (l_f/K) \\cdot m}$ (8)\nThe final compression $\\Theta^*_m$ is given by the following formula:\n$\\Theta^*_m = \\frac{\\Theta_m \\cdot U}{\\Lambda_m} + \\Lambda_m $(9)\nwhere $\\Lambda^*_m$ is the compression of the scaled $W^{(m)}$ in the center square."}, {"title": "No fine-tuning", "content": "Our method does not require fine-tuning of the model, and can directly use the trained model for compression. This greatly simplifies the model compression process and avoids the time and computing resource consumption caused by retraining the model. The reasons why our algorithm requires no retraining are mainly because our algorithm can guarantee the error rate is low by optimizing the direction of a, the side length l, and U. The error rate of points in different classes is around $(\\epsilon / l)/1 = \\epsilon/l$."}, {"title": "Affordable inference time", "content": "At first glance, one may think that model compression based on hyperfunctions suffers from a slow inference time, as this technique needs to restore parameters before inference, which adds another level of computation. Here, we propose a scheme that leverages the intrinsic hierarchical structure of a network to greatly reduce the inference time. Our scheme parallelizes parameter decompression and inference. As shown in Figure 10, while the parameters of later layers (except the first part) are restored, the inference operation in earlier layers is also carried out simultaneously. As long as we can recover the parameters of the current layer before the inference arrives at the current layer, there is no waste of time. Thus, theoretically, the inference time of using our algorithm only increases quite moderately. The increment is the time used to restore the parameters of the first layer."}, {"title": "Short Compression Time", "content": "With the advent of large models, compression time becomes an important facet of evaluating a compression algorithm. Here, we propose a suite of techniques to enable the proposed hyper-compression method to fast complete the model compression process with off-the-shelf tools, for both CPU and GPU inference.\n*   As mentioned earlier, when encoding the parameter vector W, we first convert it into many points $W^{(1)}, W^{(2)}, ..., W^{(G)}$. Then, given $W^{(g)}$, we search $\\mu$ from $(\\tau(1 \\cdot a), ..., (U \\cdot a))$ to approximate"}, {"title": "V. Parameter Sensitivity", "content": "In this section, we use UNet as an example to conduct a parameter sensitivity test on three hyperparameters: M, U, and I in our compression algorithm, where M means the maximum value of categories that can be set for all layers of the model, U means the list of the number of sample nodes in the center square, and I means the length of the side of center square. As shown in Table 9, it can be observed that different values of M, U, and I only moderately impact the model's performance and compression ratio, which means our algorithm is robust to hyperparameters."}]}