{"title": "LLM2: Let Large Language Models Harness System 2 Reasoning", "authors": ["Cheng Yang", "Chufan Shi", "Siheng Li", "Bo Shui", "Yujiu Yang", "Wai Lam"], "abstract": "Large language models (LLMs) have exhibited impressive capabilities across a myriad of tasks, yet they occasionally yield undesirable outputs. We posit that these limitations are rooted in the foundational autoregressive architecture of LLMs, which inherently lacks mechanisms for differentiating between desirable and undesirable results. Drawing inspiration from the dual-process theory of human cognition, we introduce LLM2, a novel framework that combines an LLM (System 1) with a process-based verifier (System 2). Within LLM2, the LLM is responsible for generating plausible candidates, while the verifier provides timely process-based feedback to distinguish desirable and undesirable outputs. The verifier is trained with a pairwise comparison loss on synthetic process-supervision data generated through our token quality exploration strategy. Empirical results on mathematical reasoning benchmarks substantiate the efficacy of LLM2, exemplified by an accuracy enhancement from 50.3 to 57.8 (+7.5) for Llama3-1B on GSM8K. Furthermore, when combined with self-consistency, LLM2 achieves additional improvements, boosting major@20 accuracy from 56.2 to 70.2 (+14.0).", "sections": [{"title": "Introduction", "content": "Large language models (Brown et al., 2020; Chowdhery et al., 2023; OpenAI, 2023) have exhibited remarkable abilities across various tasks that span general assistance (OpenAI, 2022), coding (Chen et al., 2021), vision (Alayrac et al., 2022) and more. However, they still occasionally produce undesirable outputs in many scenarios, e.g., reasoning and planning (Mialon et al., 2023; Hu and Shu, 2023), factual consistency (Min et al., 2023), and human value alignment (Bai et al., 2022), etc. We hypothesize these deficiencies stem from the fundamental design of LLMs. Specifically, the next-token prediction objective optimizes LLMs to maximize the probability of human-generated strings empirically, with no explicit mechanism to distinguish between desirable and undesirable outputs. During the inference stage, LLMs autoregressively generate outputs token-by-token in a single pass, with no awareness of their errors. This procedure is reminiscent of System 1 in the dual-process theory, which postulates that thinking and reasoning are underpinned by two distinct cognitive systems (Stanovich and West, 2000; Evans, 2003; Kahneman, 2011). System 1 operates automatically and subconsciously, guided by instinct and experience. In contrast, System 2, thought to be unique to humans, is more controlled and rational, enabling deliberate thinking for difficult tasks, especially when System 1 may make mistakes (Sloman, 1996). In this paper, we introduce LLM2, which aims to empower LLMs with System 2 reasoning. As shown in Figure 1, LLM2 integrates an LLM (System 1) with a process-based verifier (System 2). During inference, the LLM generates multiple candidates at each time step, and the verifier provides timely feedback on each candidate. By efficiently exploring the generation space based on the verifier's feedback, LLM2 ultimately identifies more effective outputs. During the training stage, the process-based verifier is optimized with a pairwise comparison loss to distinguish between desirable and undesirable tokens. To obtain informative token pairs data for process-supervision, we propose a token quality exploration strategy that generate synthetic data based on the potential impact of tokens on the generated text."}, {"title": "Method", "content": ""}, {"title": "Dual-process LLM", "content": "We aim to build a dual-process LLM (i.e., LLM2), where an LLM serves as System 1 for giving plausible proposals and a verifier functions as System 2 for deliberate thinking, to refine and prevent mistakes introduced by System 1. Specifically, we formalize this procedure as:\n$\\log \\pi^*(x_t|x_{<t}) \\propto \\log \\pi(x_t|x_{<t}) + \\beta s(x_{<t}, x_t), (1)$\nwhere $\\pi$ and $\\pi^*$ represent the policies of the LLM and dual-process LLM, respectively. The verifier steers $\\pi$ during decoding based on the process score $s(x_{<t}, x_t)$, with $\\beta$ controlling the strength. For computational efficiency, we focus verification on the most probable tokens at each time step. Therefore, we filter out low probability tokens using an adaptive plausibility constraint (Li et al., 2022):\n$V_t = \\{v \\in V : z_t[v] > \\log \\alpha + \\max_w z_t[w]\\}, (2)$\nwhere $z_t$ represents the logits of $\\pi$, $V$ is the vocabulary and $V_t \\subset V$ denotes the token set filtered with the hyperparameter $\\alpha \\in [0, 1]$ at time step t."}, {"title": "Process-based Verifier", "content": "We initialize the verifier from an LLM, replacing the unembedding head with a linear head to produce scalar scores. Given a dataset $D = \\{x^i\\}_{i=1}^N$, we synthesize process-supervision $D_p(x) = \\{x_{<t}, x_t^+, x_t^-\\}_{t=1}^T$ for each instance $x$, where $x_t^+$ is more appropriate than $x_t^-$. Accordingly, the training dataset for the verifier is $D_s = \\{x^i, D_p(x^i)\\}_{i=1}^N$. We train the verifier with a pairwise comparison loss (Ouyang et al., 2022):\n$\\mathcal{L}(\\theta, D_s) = -E_{(x, D_p(x)) \\sim D_s} \\sum_{t=1}^T [\\log \\sigma (s_\\theta(x_{<t}, x_t^+) - s_\\theta(x_{<t}, x_t^-))]. (4)$"}, {"title": "Synthetic Process-supervision", "content": "We aim to create $D_p(x) = \\{x_{<t}, x_t^+, x_t^-\\}_{t=1}^T$ for each instance $x$. In particular, we use the ground-truth token $x_t^+$ as $x_t^+$, which is desirable to be correct. Regarding $x_t^-$, our goal is to select tokens that express the undesirable failure modes of LLMs, e.g., reasoning errors, hallucinations and misalignment with human values. Then, through learning to distinguish between $x_t^+$ and $x_t^-$, the verifier can discern desirable and undesirable behaviors."}, {"title": "Experiments", "content": ""}, {"title": "Experimental Setup", "content": "Our experiments are based on the Llama3 model series, specifically using 1B, 3B and 8B instruct versions (Dubey et al., 2024). We leverage these LLMs as the System 1, and utilize them to initialize corresponding verifiers. We use the GSM8K training set as $D$, and employ the LLMs to generate corresponding synthetic datasets $D_p$ for training verifiers. For evaluation, we utilize two benchmarks: GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021). Further details regarding our experimental setup can be found in Appendix A."}, {"title": "Results", "content": "We present a comprehensive comparison of LLM2 against standard vanilla models and various pivotal baselines, including Self-reflection prompting (Madaan et al., 2024), Supervised Fine-tuning (SFT), and Direct Preference Optimization (DPO) (Rafailov et al., 2024). Further elaborations on these baselines are available in Appendix B. As depicted in Figure 2, implementing self-reflection prompting to engage the model in System 2 reasoning does not yield performance enhancements, suggesting a prevailing limitation in self-reflective capabilities for Llama3 models across different scales (1B, 3B, and 8B). Given that Llama3 has undergone extensive post-training with meticulously curated mathematical reasoning data (Dubey et al., 2024), applying GSM8K for either SFT or DPO"}, {"title": "Analysis", "content": ""}, {"title": "Self-generated Answers for Synthetic Process-supervision", "content": "We further refine our methodology by utilizing the model's self-generated correct answers as $D$, replacing traditional golden solutions to formulate $D_s$ for training verifiers. Instances that remain un-correct after multiple samplings are excluded. Our experiments with Llama3-1B, as illustrated in Table 1 indicate that crafting $D$ from self-generated data enhances the efficacy of LLM2. On GSM8K, performance heightens from 57.8 to 59.7, marking an improvement of 9.4 over the vanilla model. On MATH, results improve from 28.8 to 30.2, signifying a 6.0 increase over the baseline."}, {"title": "Self-consistency", "content": "We investigate the potential of integrating LLM2 with self-consistency (Wang et al., 2022), with detailed setup provided in Appendix C. As demonstrated in Figure 3, experiments conducted on Llama3-1B unveil that LLM2, when amalgamated with self-consistency, notably enhances performance. LLM2 trained with self-generated data (i.e., LLM2-SA) elevates Major@20 accuracy on GSM8K from 56.2 to 72.2, and on MATH, the Major@20 accuracy improves from 32.8 to 37.0."}, {"title": "Latency", "content": "We assess the impact of LLM2's decoding latency and compare with vanilla models on Llama3 model series. Specifically, as shown in Table 2, we report the averaged per-instance inference latency on GSM8K. Since the process-based verifier in LLM2 only performs inference when the LLM provides multiple candidate tokens after the adaptive plausibility constraint, LLM2 introduces an additional 1.21x to 1.25x latency. This latency tends to decrease as the modes's parameters increase."}, {"title": "Comparison with PRM Method", "content": "We compare LLM2 with Math-Shepherd (Wang et al., 2024), a representative Process Reward Model (PRM) baseline for Llama3-1B, with the results presented in Table 3. For a fair comparison, we use the GSM8K subset to train a Llama3-1B PRM model as the baseline. The results show that Math-Shepherd's performance converges at Best-of-N (N=20), achieving 57.6 and 27.0 on GSM8K and MATH, respectively, while LLM2 achieves 59.7 and 30.2, demonstrating LLM2's advantages. Additionally, using PRM's Best-of-N for inference potentially introduces an N-fold latency, whereas LLM2 only incurs approximately 1.2x latency. This demonstrates the advantage of LLM2's token-level supervision signals (Lin et al., 2024), which enable more efficient and precise optimization during the generation process."}, {"title": "Employ Qwen2.5", "content": "We further investigate the generalizability of LLM2 across diverse LLM families, conducting experiments on the Qwen2.5-1.5B model (Team, 2024). As illustrated in Table 4, LLM2 emerges as a robust approach to enhance the performance of Qwen2.5-1.5B on both the GSM8K and MATH benchmarks. Specifically, compared to the vanilla model, LLM2 achieves notable improvements in mathematical reasoning, with performance gains of 4.3 and 2.6 on GSM8K and MATH, respectively. In contrast, other methods fail to surpass the vanilla baseline, highlighting the unique efficacy of LLM2. This aligns with our observations on the Llama3 model series, where LLM2 consistently enhanced performance across different model sizes and tasks, reinforcing its potential as a universal enhancement framework for different LLM families."}, {"title": "Related Work", "content": ""}, {"title": "Verifier for LLMs", "content": "Training verifiers to explicitly distinguish between desirable and undesirable outputs has been a promising method to improve the capabilities of LLMs. Existing verifier modeling can be broadly classified into two categories: (1) Outcome-based modeling (Shen et al., 2021; Cobbe et al., 2021), which train verifiers to learn how to distinguish between correct and wrong outputs and selects more optimal ones from a number of candidates at inference time. (2) Process-based modeling (Uesato et al., 2022; Lightman et al., 2023), which supervises each reasoning step of the generation process. To alleviate the reliance on human-annotated process-supervision data, Wang et al. (2024) propose to automatically construct process-supervision data, where the correctness of a mathematical reasoning step is defined as its potential to reach the final answer correctly.\nIn LLM2, we propose a process-based verifier to emulate System 2 reasoning. It is trained on synthetic process-supervision data generated by our token quality exploration strategy. During inference, this verifier can intervene at any time step, providing immediate feedback without waiting for the completion of specific steps or the entire output."}, {"title": "System 2 for LLMs", "content": "Recent works explore the incorporation of System 2 into LLMs, primarily during the inference stage (Weston and Sukhbaatar, 2023; Deng et al., 2023; Saha et al., 2024). These approaches often leverage System 2 mechanisms, such as reflection and planning (Madaan et al., 2024), to generate explicit and verbalized reasoning content, which then guides subsequent token generation. Alternatively, some research focuses on transferring System 2 capabilities to System 1 during the training phase through methods such as distillation (Yu et al., 2024), thereby obviating the need for generating intermediate reasoning tokens during the inference stage.\nLLM2 integrates System 2 during the inference stage. Specifically, LLM2 leverages a process-based verifier as the System 2 to provide real-time feedback at each token generation step without generating auxiliary content."}, {"title": "Conclusion", "content": "In this work, we introduce LLM2, a framework that augments LLMs with a System 2-like reasoning process. By coupling an LLM with a process-based verifier, LLM2 proficiently differentiates between optimal and suboptimal outputs. The framework is empowered by synthetic process-supervision data generated via a novel token quality exploration strategy, which is instrumental in training the verifier. Our empirical results and analyses confirm the efficacy of LLM2 in enhancing LLM performance."}, {"title": "Limitations", "content": "While LLM2 demonstrates significant improvements in mathematical reasoning tasks, our exploration does not extend to other reasoning domains such as commonsense reasoning and code generation, due to computational resource constraints. We are optimistic about the potential of LLM2 to generalize well to these additional tasks. However, applying LLM2 to open-ended tasks, like creative writing, presents challenges due to the lack of definitive supervisory signals for synthetic process-supervision. Addressing these challenges offers a promising direction for future research."}, {"title": "Experimental Setup", "content": "We leverage the training set of GSM8K (Cobbe et al., 2021) as D and use the test set of GSM8K as one of our evaluation set. Although we do not use the MATH (Hendrycks et al., 2021) train set to train the verifier, we utilize the MATH test set as an additional evaluation set to validate the effectiveness of the verifier in improving general mathematical reasoning. Due to computational resource constraints, we randomly sampled 500 examples from the original MATH test set for our evaluation."}, {"title": "Hyperparameter Setting", "content": "We generally set \u03b2 to 0.25 in Equation 1, \u03b1 to 0.1 in Equation 2 and \u03c4 to 0.5 in Equation 6. We set N to 20 in Equation 5. For top-k in Section 2.3, k is set to 5."}, {"title": "Model Details", "content": "We list the Llama3 and Qwen2.5 models used in our experiments along with their corresponding HuggingFace model names in Table 5."}, {"title": "Details of Training Verifiers", "content": "We train our verifiers using 8 NVIDIA A100 80GB GPUs. The training process is conducted over 3 epochs with a batch size of 128. We employ a learning rate of 2e-5 and utilize a cosine learning rate scheduler."}, {"title": "Baselines", "content": "We implement four representative baselines:\nVanilla utilizes the original Llama model directly for inference.\nSupervised Fine-tuning (SFT) fine-tunes LLMs to maximize the log-likelihood of the training data, which in our case is the GSM8K training set. The training process is conducted over 3 epochs with a batch size of 128. We employ a learning rate of 2e-5 and utilize a cosine learning rate scheduler.\nDirect Preference Optimization (DPO) (Rafailov et al., 2024) optimizes language models directly from desirable and undesirable outputs, eliminating the need for an explicit reward model. For desirable data, we use the GSM8K training set; for undesirable data, a randomly sampled incorrect output from the model serves as the undesirable example. The training process is conducted over 1 epoch with a batch size of 128. We set \u03b2 = 0.01 and employ a learning rate of 5e-7 and utilize a cosine learning rate scheduler.\nSelf-reflection Prompting (Madaan et al., 2024) involves first generating an output, followed by prompting the model to assess whether its output is correct and whether to revise the output. This approach can be seen as introducing System 2 reasoning through prompting. The specific prompt is shown in Table 6."}, {"title": "Self-consistency Setup", "content": "For vanilla self-consistency, we use temperature sampling with temperature \u03c4 = 1.0 for instruct models to reach the best baseline performance (Shi et al., 2024b). For combining LLM2 with self-consistency, we simply set \u03b2 to 0.25 in Equation 1, \u03b1 to 0.1 in Equation 2 and do temperature sampling with temperature \u03c4 = 1.0."}, {"title": "Comparison with Token-Level Decoding Methods", "content": "To further demonstrate the effectiveness of our process-based verifier, we compare LLM2 with token-level decoding methods. Specifically, we implement contrastive decoding (CD) (Li et al., 2022) and DoLa (Chuang et al., 2023), and evaluate their performance on the GSM8K and MATH datasets. The results are shown in Tables 7 and 8.\nFor CD, we follow the hyperparameter settings from Li et al. (2022); O'Brien and Lewis (2023); Shi et al. (2024a), using Llama3-1B as the amateur model. For DoLa, we follow the hyperparameter settings from Chuang et al. (2023); Shi et al. (2024b). The results reported for both CD and DoLa represent their best performance across their hyperparameter ranges. As shown, CD does not yield significant improvements, primarily because CD requires an ideal amateur model (O'Brien and Lewis, 2023; Shi et al., 2024b) which may not always exist. As for DoLa, while it proves effective for factual knowledge tasks, it can have adverse effects on reasoning tasks (Chuang et al., 2023; Shi et al., 2024b)."}, {"title": "Accuracy of Process-based Verifier", "content": "We further analyze the accuracy of LLM2's process-based verifier in distinguishing between ground-truth and non-ground-truth tokens. Specifically, using the GSM8K test set, we pair each question q with its answer a. Then we leverage the vanilla models to perform next-token prediction tasks on (q, a<t) and collect the non-ground-truth token with the highest probability as a. Subsequently, we input (q, a<t, at) and (q, a<t, at) into the corresponding verifier. A correct prediction is determined by whether the verifier assigns a higher score to (q, a<t, at). The results, presented in Table 9, demonstrate the verifier's effective token-level accuracy."}, {"title": "Case Study", "content": "We present two representative cases from GSM8K using Llama3-1B to demonstrates how LLM2 improves mathematical reasoning in Table 10 and 11.\nIn Case 1, LLM2 demonstrates its ability to prevent computational errors. While the vanilla model made an arithmetic error in calculating weekly egg production (252 \u00d7 7 = 1754), LLM2 correctly computed 1764 eggs per week, leading to the accurate final answer of 294."}]}